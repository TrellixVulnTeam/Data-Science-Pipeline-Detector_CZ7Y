{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# About this notebook\n\nThis notebook is a simple pipeline using pytorch-tabnet (https://github.com/dreamquark-ai/tabnet) following the original paper's parameters (https://arxiv.org/abs/1908.07442).\n\nIt performs pretraining on test set and standard 5 fold cross validation with voting ensembling of the folds.\n\nAlmost no preprocessing is done (except from removing class 5 row and ignoring trivial columns), no feature engineering is done.\n\nThis is just a very basic starting pipeline.\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-09T11:14:41.254607Z","iopub.execute_input":"2021-12-09T11:14:41.255378Z","iopub.status.idle":"2021-12-09T11:14:53.646769Z","shell.execute_reply.started":"2021-12-09T11:14:41.25522Z","shell.execute_reply":"2021-12-09T11:14:53.645628Z"}}},{"cell_type":"code","source":"# Install pytorch-tabnet : latest develop branch \n!pip install git+https://github.com/dreamquark-ai/tabnet.git@develop","metadata":{"execution":{"iopub.status.busy":"2021-12-28T09:38:17.555723Z","iopub.execute_input":"2021-12-28T09:38:17.556456Z","iopub.status.idle":"2021-12-28T09:38:46.68022Z","shell.execute_reply.started":"2021-12-28T09:38:17.556415Z","shell.execute_reply":"2021-12-28T09:38:46.679345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#importing libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pytorch_tabnet.tab_model import TabNetClassifier\nfrom pytorch_tabnet.pretraining import TabNetPretrainer\nimport torch\nfrom sklearn.metrics import roc_auc_score, accuracy_score\n\nimport copy\nimport torch","metadata":{"execution":{"iopub.status.busy":"2021-12-28T09:38:46.682203Z","iopub.execute_input":"2021-12-28T09:38:46.682481Z","iopub.status.idle":"2021-12-28T09:38:48.75123Z","shell.execute_reply.started":"2021-12-28T09:38:46.682445Z","shell.execute_reply":"2021-12-28T09:38:48.750508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/tabular-playground-series-dec-2021/train.csv')\ntest = pd.read_csv('/kaggle/input/tabular-playground-series-dec-2021/test.csv')\n\n# remove the only 5 cover type target\ntrain = train[train.Cover_Type!=5].reset_index(drop=True)\nprint(train.shape)\nprint(test.shape)\n\n\na = train.nunique().reset_index(drop=False).rename(columns={\"index\": \"feat_name\", 0: \"count\"})\n\n# drop columns with a single value\ndrop_cols = [\"Id\"] + list(a[a[\"count\"] < 2 ].feat_name)\ntarget = [\"Cover_Type\"]\n\n# categorical features are columns with small modalities\ncat_features = [col for col in list(a[a[\"count\"] < 10 ].feat_name) if col not in drop_cols+target]\nnum_features = [col for col in train.columns if col not in drop_cols+target+cat_features]\n\nfeatures = cat_features + num_features","metadata":{"execution":{"iopub.status.busy":"2021-12-28T09:38:48.753346Z","iopub.execute_input":"2021-12-28T09:38:48.753542Z","iopub.status.idle":"2021-12-28T09:39:13.93949Z","shell.execute_reply.started":"2021-12-28T09:38:48.753517Z","shell.execute_reply":"2021-12-28T09:39:13.938737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This is only needed if using embeddings (not used at the moment)\n\nfrom sklearn.preprocessing import LabelEncoder\n\ncategorical_columns = []\ncategorical_dims =  {}\nfor col in cat_features:\n    l_enc = LabelEncoder()\n    train[col] = train[col].fillna(\"VV_likely\")\n    train[col] = l_enc.fit_transform(train[col].values)\n    categorical_columns.append(col)\n    categorical_dims[col] = len(l_enc.classes_)\n    \n    test[col] = l_enc.transform(test[col].values)\n    \ncat_idxs = [] #[ i for i, f in enumerate(features) if f in cat_features]\ncat_dims = [] #[ categorical_dims[f] for i, f in enumerate(features) if f in cat_features]\n\nX_test = test[features].values","metadata":{"execution":{"iopub.status.busy":"2021-12-28T09:39:13.941642Z","iopub.execute_input":"2021-12-28T09:39:13.941932Z","iopub.status.idle":"2021-12-28T09:39:21.905344Z","shell.execute_reply.started":"2021-12-28T09:39:13.941893Z","shell.execute_reply":"2021-12-28T09:39:21.90463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BS = 8192*2\nVBS = BS #512\nmax_epochs=50\n\ntabnet_params = {\"n_d\" : 64,\n                 \"n_a\" : 64,\n                 \"n_steps\" : 5,\n                 \"gamma\" : 1.5,\n                 \"n_independent\" : 2,\n                 \"n_shared\" : 2,\n                 \"cat_idxs\" : cat_idxs,\n                 \"cat_dims\" : cat_dims,\n                 \"cat_emb_dim\" : 1,\n                 \"lambda_sparse\" : 1e-4,\n                 \"momentum\" : 0.3,\n                 \"clip_value\" : 2.,\n                 \"optimizer_fn\" : torch.optim.Adam,\n                 \"optimizer_params\" :dict(lr=2e-2),}\n\n\nparams = copy.deepcopy(tabnet_params)\nparams[\"scheduler_fn\"]=torch.optim.lr_scheduler.StepLR\nparams[\"scheduler_params\"]={\"is_batch_level\":False,\n                            \"gamma\":0.95,\n                            \"step_size\": 2,}","metadata":{"execution":{"iopub.status.busy":"2021-12-28T09:39:21.906656Z","iopub.execute_input":"2021-12-28T09:39:21.906918Z","iopub.status.idle":"2021-12-28T09:39:21.914681Z","shell.execute_reply.started":"2021-12-28T09:39:21.906872Z","shell.execute_reply":"2021-12-28T09:39:21.914018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Pretrain the model on test set\n\nX_unsup_valid = train[features].values\nparams = tabnet_params.copy()\n\nunsupervised_model = TabNetPretrainer(**params)\n\nunsupervised_model.fit(\n    X_train=X_test,\n    eval_set=[X_unsup_valid],\n    pretraining_ratio=0.8,\n    max_epochs=50,\n    patience=5,\n    batch_size=4096,\n    virtual_batch_size=4096\n)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-28T09:39:21.916111Z","iopub.execute_input":"2021-12-28T09:39:21.916419Z","iopub.status.idle":"2021-12-28T09:52:13.268616Z","shell.execute_reply.started":"2021-12-28T09:39:21.916356Z","shell.execute_reply":"2021-12-28T09:52:13.266513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Split for cross validation or single validation\nfrom sklearn.model_selection import StratifiedKFold\n\nN_SPLITS=5\nskf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=0)\n\ncv_preds = np.zeros((X_test.shape[0], N_SPLITS))\n\nfold_idx=0\nfor train_idx, val_idx in skf.split(train, train[target]):\n\n    # Create the numpy datasets\n\n    X_train = train.loc[train_idx, features].values\n    Y_train = train.loc[train_idx, target].values.reshape(-1)\n\n    X_val = train.loc[val_idx, features].values\n    Y_val = train.loc[val_idx, target].values.reshape(-1)\n\n    # Train a tabnet classifier\n\n    params = copy.deepcopy(tabnet_params)\n\n    # Scheduling scheme here is the only part not similar to the original paper\n    # but the dataset is not exactly the same\n\n    # params[\"scheduler_fn\"]=torch.optim.lr_scheduler.StepLR\n    # params[\"scheduler_params\"]={\"is_batch_level\":False,\n    #                             \"gamma\":0.95,\n    #                             \"step_size\": 5,}\n    params[\"scheduler_fn\"]=torch.optim.lr_scheduler.OneCycleLR\n    params[\"scheduler_params\"]={\"is_batch_level\":True,\n                                \"max_lr\":5e-2,\n                                \"steps_per_epoch\":int(X_train.shape[0] / BS),\n                                \"epochs\":max_epochs}\n\n    clf = TabNetClassifier(**params)\n\n    clf.fit(\n        X_train,\n        Y_train,\n        eval_set=[(X_train, Y_train), (X_val, Y_val)],\n        eval_name=['train', 'valid'],\n        eval_metric=['accuracy'],\n        max_epochs=max_epochs,\n        patience=20,\n        drop_last=True,\n        batch_size=BS,\n        virtual_batch_size=VBS,\n    #     weights=1,\n        from_unsupervised=unsupervised_model\n    )\n    \n    preds = clf.predict(X_test)\n    cv_preds[:, fold_idx] = preds\n    fold_idx+=1","metadata":{"execution":{"iopub.status.busy":"2021-12-28T09:52:23.641505Z","iopub.execute_input":"2021-12-28T09:52:23.641766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Voting ensembling\n\nfrom scipy import stats\nfinal_res, _ = stats.mode(cv_preds, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_submission = pd.read_csv('/kaggle/input/tabular-playground-series-dec-2021/sample_submission.csv')\ndf_submission['Cover_Type']= final_res.astype(int)\ndf_submission.to_csv('submission.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_submission.Cover_Type.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}