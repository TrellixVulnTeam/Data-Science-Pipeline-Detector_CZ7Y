{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Version 2**:\n1. Fixed ranges of Aspect and Hillshade columns\n2. Used RobustScaler instead of MinMaxScaler\n\n**Version 3**:\n1. Used 20 folds rather than 10 :p\n2. Used soft voting instead of hard voting","metadata":{}},{"cell_type":"markdown","source":"Please consider **UPVOTING** If you like this notebook :)","metadata":{}},{"cell_type":"markdown","source":" # Part 1: Reading data and preprocessing","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\n\npd.set_option(\"display.max_columns\", None)\n\ntrain_df = pd.read_csv(\"../input/tabular-playground-series-dec-2021/train.csv\")\ntest_df = pd.read_csv(\"../input/tabular-playground-series-dec-2021/test.csv\")\nsub_df = pd.read_csv(\"../input/tabular-playground-series-dec-2021/sample_submission.csv\")\n\ntrain_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dropping the **Id** column from both train and test datasets.","metadata":{}},{"cell_type":"code","source":"train_df.drop(\"Id\", axis=1, inplace=True)\ntest_df.drop(\"Id\", axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dropping columns **Soil_Type7** and **Soil_Type15** because all the rows in these column have the same value.","metadata":{}},{"cell_type":"code","source":"cols = [\"Soil_Type7\", \"Soil_Type15\"]\n\ntrain_df.drop(cols, axis=1, inplace=True)\ntest_df.drop(cols, axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dropping the row with **Cover_Type**=5 because there is only a single row corresponding to it.","metadata":{}},{"cell_type":"code","source":"idx = train_df[train_df[\"Cover_Type\"] == 5].index\ntrain_df.drop(idx, axis=0, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Renaming some columns with long names","metadata":{}},{"cell_type":"code","source":"new_names = {\n    \"Horizontal_Distance_To_Hydrology\": \"x_dist_hydrlgy\",\n    \"Vertical_Distance_To_Hydrology\": \"y_dist_hydrlgy\",\n    \"Horizontal_Distance_To_Roadways\": \"x_dist_rdwys\",\n    \"Horizontal_Distance_To_Fire_Points\": \"x_dist_firepts\"\n}\n\ntrain_df.rename(new_names, axis=1, inplace=True)\ntest_df.rename(new_names, axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Encoding all the labels so that they range from 0 to 5.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\n\nencoder = LabelEncoder()\ntrain_df[\"Cover_Type\"] = encoder.fit_transform(train_df[\"Cover_Type\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Part 2: Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"**Aspect** is the compass direction that a terrain faces. Here, It is expressed in degrees. All the values from 0 to 359 are present. Besides, there are some values greater than 359 and some smaller than 0. It will be better If we make all the values in this column lie in the range (0, 359). Moreover, all the values in this column lies in the range (-360, 720) so adding 360 to angles smaller than 0 and subtracting 360 from angles greater than 359 will do the work.","metadata":{}},{"cell_type":"code","source":"train_df[\"Aspect\"][train_df[\"Aspect\"] < 0] += 360\ntrain_df[\"Aspect\"][train_df[\"Aspect\"] > 359] -= 360\n\ntest_df[\"Aspect\"][test_df[\"Aspect\"] < 0] += 360\ntest_df[\"Aspect\"][test_df[\"Aspect\"] > 359] -= 360","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creating distance based features from **Horizontal_Distance_To_Hydrology** and **Vertical_Distance_To_Hydrology**.","metadata":{}},{"cell_type":"code","source":"# Manhhattan distance to Hydrology\ntrain_df[\"mnhttn_dist_hydrlgy\"] = np.abs(train_df[\"x_dist_hydrlgy\"]) + np.abs(train_df[\"y_dist_hydrlgy\"])\ntest_df[\"mnhttn_dist_hydrlgy\"] = np.abs(test_df[\"x_dist_hydrlgy\"]) + np.abs(test_df[\"y_dist_hydrlgy\"])\n\n# Euclidean distance to Hydrology\ntrain_df[\"ecldn_dist_hydrlgy\"] = (train_df[\"x_dist_hydrlgy\"]**2 + train_df[\"y_dist_hydrlgy\"]**2)**0.5\ntest_df[\"ecldn_dist_hydrlgy\"] = (test_df[\"x_dist_hydrlgy\"]**2 + test_df[\"y_dist_hydrlgy\"]**2)**0.5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creating the following new features:\n1. Sum of all the soil types\n2. Sum of all the wilderness area types\n\nThese features are borrowed from this discussion topic: https://www.kaggle.com/c/tabular-playground-series-dec-2021/discussion/292823","metadata":{}},{"cell_type":"code","source":"soil_features = [x for x in train_df.columns if x.startswith(\"Soil_Type\")]\ntrain_df[\"soil_type_count\"] = train_df[soil_features].sum(axis=1)\ntest_df[\"soil_type_count\"] = test_df[soil_features].sum(axis=1)\n\nwilderness_features = [x for x in train_df.columns if x.startswith(\"Wilderness_Area\")]\ntrain_df[\"wilderness_area_count\"] = train_df[wilderness_features].sum(axis=1)\ntest_df[\"wilderness_area_count\"] = test_df[wilderness_features].sum(axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A quick Google search about **Hillshade** leads to the following result:\n\n> Hillshading computes surface illumination as values from 0 to 255 based on a given compass direction to the sun (azimuth) and a certain altitude above the horizon (altitude). Hillshades are often used to produce maps that are visually appealing.\n\nThus, hillshade is a 3D representation of a terrain which is used to gain insight about its form by measuring luminosity of certain patches of that terrain that results when a source of light is casted at a particular angle.\n\nMore Information about hillshade [here](http://www.geography.hunter.cuny.edu/~jochen/gtech361/lectures/lecture11/concepts/hillshade.htm#:~:text=Hillshading%20computes%20surface%20illumination%20as,maps%20that%20are%20visually%20appealing.)\n\nIn both train and test datasets, there are certain rows with hillshade value more than 255 or less than 0. They must be the result of recording error and should be relpaced with an appropriate value. Perhaps, values less than 0 refer to the darkest shade and replacing them with 0 should be fine. Similarly, we can assume that hillshade values more than 255 refer to the brightest shades and a value of 255 should be good replacement.","metadata":{}},{"cell_type":"code","source":"train_df.loc[train_df[\"Hillshade_9am\"] < 0, \"Hillshade_9am\"] = 0\ntest_df.loc[test_df[\"Hillshade_9am\"] < 0, \"Hillshade_9am\"] = 0\n\ntrain_df.loc[train_df[\"Hillshade_Noon\"] < 0, \"Hillshade_Noon\"] = 0\ntest_df.loc[test_df[\"Hillshade_Noon\"] < 0, \"Hillshade_Noon\"] = 0\n\ntrain_df.loc[train_df[\"Hillshade_3pm\"] < 0, \"Hillshade_3pm\"] = 0\ntest_df.loc[test_df[\"Hillshade_3pm\"] < 0, \"Hillshade_3pm\"] = 0\n\ntrain_df.loc[train_df[\"Hillshade_9am\"] > 255, \"Hillshade_9am\"] = 255\ntest_df.loc[test_df[\"Hillshade_9am\"] > 255, \"Hillshade_9am\"] = 255\n\ntrain_df.loc[train_df[\"Hillshade_Noon\"] > 255, \"Hillshade_Noon\"] = 255\ntest_df.loc[test_df[\"Hillshade_Noon\"] > 255, \"Hillshade_Noon\"] = 255\n\ntrain_df.loc[train_df[\"Hillshade_3pm\"] > 255, \"Hillshade_3pm\"] = 255\ntest_df.loc[test_df[\"Hillshade_3pm\"] > 255, \"Hillshade_3pm\"] = 255","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Scaling with RobustScaler","metadata":{"execution":{"iopub.status.busy":"2021-12-04T14:25:28.016165Z","iopub.execute_input":"2021-12-04T14:25:28.016566Z","iopub.status.idle":"2021-12-04T14:25:28.021173Z","shell.execute_reply.started":"2021-12-04T14:25:28.016538Z","shell.execute_reply":"2021-12-04T14:25:28.019744Z"}}},{"cell_type":"code","source":"from sklearn.preprocessing import RobustScaler\n\n\ncols = [\n    \"Elevation\",\n    \"Aspect\",\n    \"mnhttn_dist_hydrlgy\",\n    \"ecldn_dist_hydrlgy\",\n    \"soil_type_count\",\n    \"wilderness_area_count\",\n    \"Slope\",\n    \"x_dist_hydrlgy\",\n    \"y_dist_hydrlgy\",\n    \"x_dist_rdwys\",\n    \"Hillshade_9am\",\n    \"Hillshade_Noon\",\n    \"Hillshade_3pm\",\n    \"x_dist_firepts\",\n    \"soil_type_count\",\n    \"wilderness_area_count\"\n]\n\nscaler = RobustScaler()\ntrain_df[cols] = scaler.fit_transform(train_df[cols])\ntest_df[cols] = scaler.transform(test_df[cols])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Reducing the size of train and test dataframes","metadata":{}},{"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2\n\n    for col in df.columns:\n        col_type = df[col].dtypes\n\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() / 1024**2\n\n    if verbose:\n        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n \n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = reduce_mem_usage(train_df)\ntest_df = reduce_mem_usage(test_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Part 3: Modelling with Neural Network","metadata":{}},{"cell_type":"markdown","source":"I have used Self-normalizing Neural Networks here which is described in this notebook: https://www.kaggle.com/gulshanmishra/self-normalizing-neural-networks","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, BatchNormalization\n\n\nINPUT_SHAPE = test_df.shape[1:]\nNUM_CLASSES = train_df[\"Cover_Type\"].nunique()\n\ndef build_model():\n    model = Sequential([\n        Dense(units=300, kernel_initializer=\"lecun_normal\", activation=\"selu\", input_shape=INPUT_SHAPE),\n        BatchNormalization(),\n        Dense(units=200, kernel_initializer=\"lecun_normal\", activation=\"selu\"),\n        BatchNormalization(),\n        Dense(units=100, kernel_initializer=\"lecun_normal\", activation=\"selu\"),\n        BatchNormalization(),\n        Dense(units=50, kernel_initializer=\"lecun_normal\", activation=\"selu\"),\n        BatchNormalization(),\n        Dense(units=NUM_CLASSES, activation=\"softmax\")\n    ])\n\n    model.compile(\n        optimizer=\"adam\",\n        loss=\"sparse_categorical_crossentropy\",\n        metrics=[\"accuracy\"]\n    )\n\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Callbacks for early stopping and learning rate reduction when it \"plateaus\"","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n\n\nreduce_lr = ReduceLROnPlateau(\n    monitor=\"val_loss\",\n    factor=0.5,\n    patience=5\n)\n\nearly_stop = EarlyStopping(\n    monitor=\"val_accuracy\",\n    patience=20,\n    restore_best_weights=True\n)\n\ncallbacks = [reduce_lr, early_stop]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"build_model().summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.utils import plot_model\n\n\nplot_model(\n    build_model(),\n    show_shapes=True,\n    show_layer_names=True\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\n\n\nX = train_df.drop(\"Cover_Type\", axis=1).values\ny = train_df[\"Cover_Type\"].values\n\ndel train_df\n\nFOLDS = 20\nEPOCHS = 200\nBATCH_SIZE = 2048\n\ntest_preds = np.zeros((1, 1))\nscores = []\n\ncv = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=42)\n\nfor fold, (train_idx, val_idx) in enumerate(cv.split(X, y)):\n    X_train, X_val = X[train_idx], X[val_idx]\n    y_train, y_val = y[train_idx], y[val_idx]\n\n    model = build_model()\n    model.fit(\n        X_train,\n        y_train,\n        validation_data=(X_val, y_val),\n        epochs=EPOCHS,\n        batch_size=BATCH_SIZE,\n        callbacks=callbacks,\n        verbose=False\n    )\n\n    y_pred = np.argmax(model.predict(X_val), axis=1)\n    score = accuracy_score(y_val, y_pred)\n    scores.append(score)\n\n    test_preds = test_preds + model.predict(test_df)\n    print(f\"Fold {fold} Accuracy: {score}\")\n\nprint()\nprint(f\"Mean Accuracy: {np.mean(scores)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using soft voting strategy to ensemble test predictions","metadata":{}},{"cell_type":"code","source":"test_preds = np.argmax(test_preds, axis=1)\ntest_preds = encoder.inverse_transform(test_preds)\n\nsub_df['Cover_Type'] = test_preds\nsub_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}