{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<font size=\"4\">**Hope you like this ....**\n","metadata":{}},{"cell_type":"markdown","source":"<html> \n \n<body> \n    <h1 style=\"color:green;\">Introduction</h1> \n    <font size=\"4\">ü¶ÅLearning rate hyperparameter is one the  most important hyperparameter which we need to take care while traing an neural network. As it controls how much a model is to be changed in response to the error each time while updating the weights.  \n        \n  üêØIf the learnig rate is too small the algorithm will have to go through many iterations to converge,which will take a long time.\n    <img src=\"https://static.wixstatic.com/media/0cef7a_e11ad46aef61451091fa20c77c8d14a1~mv2.png/v1/fill/w_360,h_192,al_c,q_95/0cef7a_e11ad46aef61451091fa20c77c8d14a1~mv2.webp\" width=\"400\"/>\n       \n  üê∂But if the learnig rate is too high,the algorithm diverges and fail to find a good solution.Because when the learnig rate is high,you might jump across the valley and end up on the other side, possibly even higher up than you were before.\n        <img src=\"https://static.wixstatic.com/media/0cef7a_6a1e7529716344cd8fb9451281c39f21~mv2.png/v1/fill/w_360,h_194,al_c,q_95/0cef7a_6a1e7529716344cd8fb9451281c39f21~mv2.webp\" width=\"400\"/>\n        \n</body> \n  \n</html>","metadata":{}},{"cell_type":"markdown","source":"<font size=\"4\">üëΩDeep learning neural networks can be trained using the\n* Gradient Descent.\n* Stochastic Gradient Descent.\n* Adagrad.\n* Adadelta.\n* RMSprop.\n* Adam. algorithm.   \n<font size=\"4\">üëæ These optimization algorithm estimates the error gradient for the current state of the model using examples from the training dataset, then updates the weights of the model using the back-propagation of errors algorithm, referred to as simply backpropagation.   \n<font size=\"4\">ü§ñ The amount that the weights are updated during training is referred to as the step size or the ‚Äúlearning rate.‚Äù","metadata":{}},{"cell_type":"markdown","source":"# Learning Rate Scheduling","metadata":{}},{"cell_type":"markdown","source":"<font size=\"4\">üíÄFinding a good learning rate is very important.If too high training may diverge.If too low training will converge but take a very long time.\n    \n<font size=\"4\"> üëªWe can find a good learning rate by training the\nmodel for a few hundred iterations, exponentially increasing the learning rate from a\nvery small value to a very large value, and then looking at the learning curve and\npicking a learning rate slightly lower than the one at which the learning curve starts\nshooting back up.  <img src=\"https://i.stack.imgur.com/hhwyC.png\" width=\"400\"/>\n        ","metadata":{}},{"cell_type":"markdown","source":"<font size=\"4\">üê•But we can do better than a constant learning rate : ;if we start with a large learnig rate and then reduce it once traing stops making fst progress, you can reach a good solution faster than with the optimal constant learnig rate.  \n<font size=\"4\">üê£There are many differnt strategies to reduce the learnig rate during training.\nThese strategies are called **Learning schedules** ","metadata":{}},{"cell_type":"markdown","source":"<font size=\"4\">The most commonly used learnig schedules are as following:\n * Power scheduling\n * Exponential scheduling\n * Piecewise constant scheduling \n * Performance scheduling \n * 1cycle scheduling   \n \n\n <img src=\"https://imagehost7.online-image-editor.com/oie_upload/images/8744142cuv65Gsl/wMA77fBrQRhW.png\"/>\n","metadata":{}},{"cell_type":"markdown","source":"<html> \n \n<body> \n    <h1 style=\"color:green;\">1.Power scheduling</h1> \n     <font size=\"4\">ü¶ÜIn this technique we set the learning rate to a function of the iteration number. </font> \n    \n\n<font size=\"4\">**Formula**\n>  <font size=\"4\"> lr = lr0 / (1 + steps / s)**c  \n>     here,  \n>           lr0 ---->  is the initial learning rate  \n>           c   -----> power(typically set to 1)  \n>           s   -----> steps     \n   \n <font size=\"4\">ü¶¢The learning rate drops at each step. After ssteps, it is down to lr0/2 then lr0/3 after some time lr0/5 so on.\n     \n     ","metadata":{"execution":{"iopub.status.busy":"2021-12-07T09:31:16.816951Z","iopub.execute_input":"2021-12-07T09:31:16.817337Z","iopub.status.idle":"2021-12-07T09:31:16.846894Z","shell.execute_reply.started":"2021-12-07T09:31:16.81723Z","shell.execute_reply":"2021-12-07T09:31:16.845881Z"}}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-17T14:02:39.10005Z","iopub.execute_input":"2021-12-17T14:02:39.100393Z","iopub.status.idle":"2021-12-17T14:02:42.861255Z","shell.execute_reply.started":"2021-12-17T14:02:39.100307Z","shell.execute_reply":"2021-12-17T14:02:42.860549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To plot pretty figures\n%matplotlib inline\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport math\n\nimport numpy as np ","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-17T14:02:42.863111Z","iopub.execute_input":"2021-12-17T14:02:42.863415Z","iopub.status.idle":"2021-12-17T14:02:42.870804Z","shell.execute_reply.started":"2021-12-17T14:02:42.863364Z","shell.execute_reply":"2021-12-17T14:02:42.870051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\nX_train_full = X_train_full / 255.0\nX_test = X_test / 255.0\nX_valid, X_train = X_train_full[:5000], X_train_full[5000:]\ny_valid, y_train = y_train_full[:5000], y_train_full[5000:]","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-17T14:02:42.872474Z","iopub.execute_input":"2021-12-17T14:02:42.87307Z","iopub.status.idle":"2021-12-17T14:02:45.343879Z","shell.execute_reply.started":"2021-12-17T14:02:42.873031Z","shell.execute_reply":"2021-12-17T14:02:45.343113Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_epochs = 25\nlearning_rate = 0.01  #initial learning rate\ndecay = 1e-4\nbatch_size = 32\nn_steps_per_epoch = math.ceil(len(X_train) / batch_size)\nepochs = np.arange(n_epochs)\nlrs = learning_rate / (1 + decay * epochs * n_steps_per_epoch)\n\nplt.plot(epochs, lrs,  \"o-\")\nplt.axis([0, n_epochs - 1, 0, 0.01])\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Learning Rate\")\nplt.title(\"Power Scheduling\", fontsize=14)\nplt.grid(True)\nplt.show()","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-12-17T14:02:45.346103Z","iopub.execute_input":"2021-12-17T14:02:45.346404Z","iopub.status.idle":"2021-12-17T14:02:45.542105Z","shell.execute_reply.started":"2021-12-17T14:02:45.346365Z","shell.execute_reply":"2021-12-17T14:02:45.541455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size=\"4.5\">**Concluson**\n\n<font size=\"4\">From the above digram we can see that\n * In this scheduling technique learning rate first drops quickly,then more and more slowly.     \n * It requires tuning of initial learning rate and steps hyper perameter","metadata":{}},{"cell_type":"markdown","source":"<html> \n \n<body> \n    <h1 style=\"color:green;\">2.Exponential scheduling</h1> \n     <font size=\"4\">ü¶ÜIn this technique we set the learning rate to  number. </font> \n    \n\n<font size=\"4\">**Formula**\n>  <font size=\"4\"> lr = lr0 * 0.1**(epoch / s)  \n>     here,  \n>           lr0 ---->  is the initial learning rate  \n>           s   -----> steps     \n \n <font size=\"4\"> ü¶¢The learning rate drops gradually by a factor of 10 every s steps.","metadata":{}},{"cell_type":"code","source":"def exponential_decay_fn(epoch):\n    return 0.01 * 0.1**(epoch / 20)\ndef exponential_decay(lr0, s):\n    def exponential_decay_fn(epoch):\n        return lr0 * 0.1**(epoch / s)\n    return exponential_decay_fn\n\nexponential_decay_fn = exponential_decay(lr0=0.01, s=20)","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-12-17T14:02:45.543379Z","iopub.execute_input":"2021-12-17T14:02:45.543781Z","iopub.status.idle":"2021-12-17T14:02:45.549328Z","shell.execute_reply.started":"2021-12-17T14:02:45.543742Z","shell.execute_reply":"2021-12-17T14:02:45.54861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = keras.models.Sequential([\n    keras.layers.Flatten(input_shape=[28, 28]),\n    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    keras.layers.Dense(10, activation=\"softmax\")\n])\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\nn_epochs = 25","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-12-17T14:02:45.550544Z","iopub.execute_input":"2021-12-17T14:02:45.550858Z","iopub.status.idle":"2021-12-17T14:02:47.687026Z","shell.execute_reply.started":"2021-12-17T14:02:45.550825Z","shell.execute_reply":"2021-12-17T14:02:47.686336Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)\nhistory = model.fit(X_train, y_train, epochs=n_epochs,\n                    validation_data=(X_valid, y_valid),\n                    callbacks=[lr_scheduler])","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-12-17T14:02:47.688108Z","iopub.execute_input":"2021-12-17T14:02:47.688861Z","iopub.status.idle":"2021-12-17T14:04:54.288615Z","shell.execute_reply.started":"2021-12-17T14:02:47.688823Z","shell.execute_reply":"2021-12-17T14:04:54.287902Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.epoch, history.history[\"lr\"], \"o-\")\nplt.axis([0, n_epochs - 1, 0, 0.011])\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Learning Rate\")\nplt.title(\"Exponential Scheduling\", fontsize=14)\nplt.grid(True)\nplt.show()","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-12-17T14:04:54.29009Z","iopub.execute_input":"2021-12-17T14:04:54.290367Z","iopub.status.idle":"2021-12-17T14:04:54.459759Z","shell.execute_reply.started":"2021-12-17T14:04:54.290332Z","shell.execute_reply":"2021-12-17T14:04:54.459006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size=\"4.5\">**Concluson**\n\n    \n<font size=\"4\">From the above digram we can see that\n * In this scheduling technique learning rate  drops by factor of 10 as no. of epochs increase.    \n","metadata":{"execution":{"iopub.status.busy":"2021-12-07T10:27:46.686758Z","iopub.execute_input":"2021-12-07T10:27:46.687075Z","iopub.status.idle":"2021-12-07T10:27:46.693787Z","shell.execute_reply.started":"2021-12-07T10:27:46.68704Z","shell.execute_reply":"2021-12-07T10:27:46.692482Z"}}},{"cell_type":"markdown","source":"<html> \n \n<body> \n    <h1 style=\"color:green;\">3. Piecewise constant scheduling</h1> \n     <font size=\"4\">ü¶ÜIn this technique constant learning rate is used for a number of epoch then a smaller learning rate is used for another number of epochs and so on. </font> \n    \n","metadata":{}},{"cell_type":"code","source":"def piecewise_constant_fn(epoch):\n    if epoch < 5:\n        return 0.01\n    elif epoch < 15:\n        return 0.005\n    else:\n        return 0.001\ndef piecewise_constant(boundaries, values):\n    boundaries = np.array([0] + boundaries)\n    values = np.array(values)\n    def piecewise_constant_fn(epoch):\n        return values[np.argmax(boundaries > epoch) - 1]\n    return piecewise_constant_fn\n\npiecewise_constant_fn = piecewise_constant([5, 15], [0.01, 0.005, 0.001])\n\nlr_scheduler = keras.callbacks.LearningRateScheduler(piecewise_constant_fn)\n\nmodel = keras.models.Sequential([\n    keras.layers.Flatten(input_shape=[28, 28]),\n    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    keras.layers.Dense(10, activation=\"softmax\")\n])\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\nn_epochs = 25\nhistory = model.fit(X_train, y_train, epochs=n_epochs,\n                    validation_data=(X_valid, y_valid),\n                    callbacks=[lr_scheduler])","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-17T14:04:54.461081Z","iopub.execute_input":"2021-12-17T14:04:54.461483Z","iopub.status.idle":"2021-12-17T14:04:54.467705Z","shell.execute_reply.started":"2021-12-17T14:04:54.461445Z","shell.execute_reply":"2021-12-17T14:04:54.467038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.epoch, [piecewise_constant_fn(epoch) for epoch in history.epoch], \"o-\")\nplt.axis([0, n_epochs - 1, 0, 0.011])\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Learning Rate\")\nplt.title(\"Piecewise Constant Scheduling\", fontsize=14)\nplt.grid(True)\nplt.show()","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-12-17T14:07:00.208807Z","iopub.execute_input":"2021-12-17T14:07:00.209044Z","iopub.status.idle":"2021-12-17T14:07:00.383686Z","shell.execute_reply.started":"2021-12-17T14:07:00.209009Z","shell.execute_reply":"2021-12-17T14:07:00.383027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size=\"4.5\">**Concluson**\n\n    \n<font size=\"4\">This technique requires fidling around to figure out the right sequence of learning rates and how long to use each of them.    \n","metadata":{}},{"cell_type":"markdown","source":"<html> \n \n<body> \n    <h1 style=\"color:green;\">4. Performance scheduling</h1> \n     <font size=\"4\">ü¶ÜThis technique measures the validation error every N steps and reduce the learning rate by a factor of lemda when the error stop dropping. </font> ","metadata":{}},{"cell_type":"code","source":"lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\n\nmodel = keras.models.Sequential([\n    keras.layers.Flatten(input_shape=[28, 28]),\n    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    keras.layers.Dense(10, activation=\"softmax\")\n])\noptimizer = keras.optimizers.SGD(learning_rate=0.02, momentum=0.9)\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\nn_epochs = 25\nhistory = model.fit(X_train, y_train, epochs=n_epochs,\n                    validation_data=(X_valid, y_valid),\n                    callbacks=[lr_scheduler])","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-12-17T14:07:00.384915Z","iopub.execute_input":"2021-12-17T14:07:00.385144Z","iopub.status.idle":"2021-12-17T14:09:22.940295Z","shell.execute_reply.started":"2021-12-17T14:07:00.385111Z","shell.execute_reply":"2021-12-17T14:09:22.939293Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.epoch, history.history[\"lr\"], \"bo-\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Learning Rate\", color='b')\nplt.tick_params('y', colors='b')\nplt.gca().set_xlim(0, n_epochs - 1)\nplt.grid(True)\n\nax2 = plt.gca().twinx()\nax2.plot(history.epoch, history.history[\"val_loss\"], \"r^-\")\nax2.set_ylabel('Validation Loss', color='r')\nax2.tick_params('y', colors='r')\n\nplt.title(\"Reduce LR on Plateau\", fontsize=14)\nplt.show()","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-12-17T14:09:22.945027Z","iopub.execute_input":"2021-12-17T14:09:22.945379Z","iopub.status.idle":"2021-12-17T14:09:23.226856Z","shell.execute_reply.started":"2021-12-17T14:09:22.94534Z","shell.execute_reply":"2021-12-17T14:09:23.226047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"<html> \n \n<body> \n    <h1 style=\"color:green;\">5.1cycle scheduling</h1> \n     <font size=\"4\">ü¶ñThis approach contrary to other approaches starts by increasing the initial learnig rate Œ∑0, growing linearly up a learning rate Œ∑1.      \n         \nü¶ïThen it decreases the learning rate linearly to the initial learning rate Œ∑0 again during the second half of training.\n         \nüêâThe maximum learning rate Œ∑1 is chosen using the same approach we used to find the optimal\nlearning rate.\n\nü¶éThe initial learning rate Œ∑0 is chosen to be roughly 10 times lower.","metadata":{"execution":{"iopub.status.busy":"2021-12-07T11:05:41.47368Z","iopub.execute_input":"2021-12-07T11:05:41.473998Z","iopub.status.idle":"2021-12-07T11:05:41.481136Z","shell.execute_reply.started":"2021-12-07T11:05:41.473967Z","shell.execute_reply":"2021-12-07T11:05:41.479904Z"}}},{"cell_type":"code","source":"K = keras.backend\n\nclass ExponentialLearningRate(keras.callbacks.Callback):\n    def __init__(self, factor):\n        self.factor = factor\n        self.rates = []\n        self.losses = []\n    def on_batch_end(self, batch, logs):\n        self.rates.append(K.get_value(self.model.optimizer.learning_rate))\n        self.losses.append(logs[\"loss\"])\n        K.set_value(self.model.optimizer.learning_rate, self.model.optimizer.learning_rate * self.factor)\ndef find_learning_rate(model, X, y, epochs=1, batch_size=32, min_rate=10**-5, max_rate=10):\n    init_weights = model.get_weights()\n    iterations = math.ceil(len(X) / batch_size) * epochs\n    factor = np.exp(np.log(max_rate / min_rate) / iterations)\n    init_lr = K.get_value(model.optimizer.learning_rate)\n    K.set_value(model.optimizer.learning_rate, min_rate)\n    exp_lr = ExponentialLearningRate(factor)\n    history = model.fit(X, y, epochs=epochs, batch_size=batch_size,\n                        callbacks=[exp_lr])\n    K.set_value(model.optimizer.learning_rate, init_lr)\n    model.set_weights(init_weights)\n    return exp_lr.rates, exp_lr.losses\n","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-12-17T14:09:23.228125Z","iopub.execute_input":"2021-12-17T14:09:23.228469Z","iopub.status.idle":"2021-12-17T14:09:23.238775Z","shell.execute_reply.started":"2021-12-17T14:09:23.22843Z","shell.execute_reply":"2021-12-17T14:09:23.238108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_lr_vs_loss(rates, losses):\n    plt.plot(rates, losses)\n    plt.gca().set_xscale('log')\n    plt.hlines(min(losses), min(rates), max(rates))\n    plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 2])\n    plt.xlabel(\"Learning rate\")\n    plt.ylabel(\"Loss\")","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-12-17T14:09:23.239883Z","iopub.execute_input":"2021-12-17T14:09:23.2404Z","iopub.status.idle":"2021-12-17T14:09:23.251409Z","shell.execute_reply.started":"2021-12-17T14:09:23.240361Z","shell.execute_reply":"2021-12-17T14:09:23.250675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.random.set_seed(42)\nnp.random.seed(42)\n\nmodel = keras.models.Sequential([\n    keras.layers.Flatten(input_shape=[28, 28]),\n    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    keras.layers.Dense(10, activation=\"softmax\")\n])\nmodel.compile(loss=\"sparse_categorical_crossentropy\",\n              optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n              metrics=[\"accuracy\"])","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-12-17T14:09:23.252549Z","iopub.execute_input":"2021-12-17T14:09:23.252885Z","iopub.status.idle":"2021-12-17T14:09:23.302036Z","shell.execute_reply.started":"2021-12-17T14:09:23.252846Z","shell.execute_reply":"2021-12-17T14:09:23.301422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 128\nrates, losses = find_learning_rate(model, X_train, y_train, epochs=1, batch_size=batch_size)\nplot_lr_vs_loss(rates, losses)","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-12-17T14:09:23.304171Z","iopub.execute_input":"2021-12-17T14:09:23.304452Z","iopub.status.idle":"2021-12-17T14:09:26.073705Z","shell.execute_reply.started":"2021-12-17T14:09:23.304416Z","shell.execute_reply":"2021-12-17T14:09:26.073022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"<html> \n \n<body> \n      <h1 style=\"color:green;\">Concluding all</h1> \n    \n<font size=\"4\">A 2013 paper by Andrew Senior et al. compared the performance of some of the\nmost popular learning schedules when using momentum optimization to train deep\nneural networks for speech recognition.\n    \n<font size=\"4\">The authors concluded that, in this setting,both performance scheduling and exponential scheduling performed well.\n    \n<font size=\"4\">They favored exponential scheduling because it was easy to tune and it converged slightly faster to the optimal solution.\n    \n<font size=\"4\">But the 1cycle approach seems to perform even better.\n    \n    \nNote: In keras the easiest option is to implement power scheduling(just set the decay hyprparameter)\n    \n    \n    \n    \n    \n    ","metadata":{}},{"cell_type":"markdown","source":"# Thank you for reading if u like please upvoteüòÅ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}