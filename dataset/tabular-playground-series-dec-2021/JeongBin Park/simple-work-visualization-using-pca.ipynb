{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Visualization - Using PCA\n\nThis time, I will Visualization many columns using PCA","metadata":{}},{"cell_type":"markdown","source":"#### Libaries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-12T13:26:52.696016Z","iopub.execute_input":"2021-12-12T13:26:52.696318Z","iopub.status.idle":"2021-12-12T13:26:52.701278Z","shell.execute_reply.started":"2021-12-12T13:26:52.696273Z","shell.execute_reply":"2021-12-12T13:26:52.700261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Load Dataset and Data Preprocessing\n\nI just dropped 'Id' column and, divide 'Cover_Type'","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"../input/tabular-playground-series-dec-2021/train.csv\")\ntrain = train.drop('Id',axis=1)\n\nX_train = train.drop('Cover_Type',axis=1)\ny_train = train['Cover_Type']","metadata":{"execution":{"iopub.status.busy":"2021-12-12T13:34:12.101331Z","iopub.execute_input":"2021-12-12T13:34:12.10162Z","iopub.status.idle":"2021-12-12T13:34:28.171791Z","shell.execute_reply.started":"2021-12-12T13:34:12.101592Z","shell.execute_reply":"2021-12-12T13:34:28.170886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### PCA\n\nPCA is one of the way to decrease column's length <br>\nThen, How we can find best n_components? <br>\nI usually using Eigen Vector <br>\nIt mean that, how much n_component accommodate the data. ","metadata":{}},{"cell_type":"code","source":"pca = PCA(n_components=len(X_train.columns), svd_solver='auto')\npca.fit(X_train)\nplt.figure(figsize=(15,8))\nplt.plot([*range(len(X_train.columns))], pca.explained_variance_ratio_)\nplt.xlim(0,10)\nplt.title(\"Eigen Vector\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-12T13:34:33.213445Z","iopub.execute_input":"2021-12-12T13:34:33.213933Z","iopub.status.idle":"2021-12-12T13:34:50.854821Z","shell.execute_reply.started":"2021-12-12T13:34:33.213863Z","shell.execute_reply":"2021-12-12T13:34:50.853848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('How much n_components accommodate the data : ', sum(pca.explained_variance_ratio_[:2]))","metadata":{"execution":{"iopub.status.busy":"2021-12-12T13:35:57.164943Z","iopub.execute_input":"2021-12-12T13:35:57.165262Z","iopub.status.idle":"2021-12-12T13:35:57.172088Z","shell.execute_reply.started":"2021-12-12T13:35:57.165226Z","shell.execute_reply":"2021-12-12T13:35:57.171233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Look this Eigen Vector chart. If you select 2, You can take the 95% data","metadata":{}},{"cell_type":"code","source":"pca = PCA(n_components=2, svd_solver='auto')\nX_train = pca.fit_transform(X_train)\ntrain = pd.concat([pd.DataFrame(X_train), pd.DataFrame(y_train)],axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T13:27:27.070594Z","iopub.execute_input":"2021-12-12T13:27:27.070929Z","iopub.status.idle":"2021-12-12T13:27:50.042681Z","shell.execute_reply.started":"2021-12-12T13:27:27.070883Z","shell.execute_reply":"2021-12-12T13:27:50.041635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2D Viusailzation\n\nNow, we have only 2 columns and target data('Cover_Type'). <br>\nSo we can visualization it","metadata":{}},{"cell_type":"code","source":"a = train['Cover_Type'].unique()\nfig = plt.figure(figsize=(15,10))\nfor index in a:\n    plt.scatter(x=train[train['Cover_Type']==index][0], y=train[train['Cover_Type']==index][1], alpha=0.3)\n    \nplt.title(\"2D Visualization\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-12T13:27:50.044679Z","iopub.execute_input":"2021-12-12T13:27:50.044917Z","iopub.status.idle":"2021-12-12T13:27:58.611078Z","shell.execute_reply.started":"2021-12-12T13:27:50.04489Z","shell.execute_reply":"2021-12-12T13:27:58.606352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3D Visualization\n\nHow about using 3D visualization. <br>\nI think it's better way than using 2D","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(20, 13))\nax1 = fig.add_subplot(121, projection='3d')\nfor target_class in train['Cover_Type'].unique():\n    x = train[train['Cover_Type'] == target_class][0]\n    y = train[train['Cover_Type'] == target_class][1]\n    z = train[train['Cover_Type'] == target_class]['Cover_Type']\n    ax1.scatter(x, y, z, s= 10, alpha=0.05)\n    \nax2 = fig.add_subplot(122, projection='3d')\nfor target_class in train['Cover_Type'].unique():\n    sample_train = train[train['Cover_Type'] == target_class].sample(min(10000, train.Cover_Type.value_counts()[target_class]))\n    x = sample_train[0]\n    y = sample_train[1]\n    z = sample_train['Cover_Type']\n    ax2.scatter(x, y, z, s= 10, alpha=0.05)\n    \nax1.set_title(\"3D scatter All scatter\")\nax2.set_title(\"3D scatter Sample scatter - max 10000\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-12T13:27:58.612834Z","iopub.execute_input":"2021-12-12T13:27:58.613315Z","iopub.status.idle":"2021-12-12T13:29:10.68143Z","shell.execute_reply.started":"2021-12-12T13:27:58.613259Z","shell.execute_reply":"2021-12-12T13:29:10.680449Z"},"trusted":true},"execution_count":null,"outputs":[]}]}