{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## TPS Dec. 2021 - Baseline XGBM/LGBM/CB with GPU","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"## LightGBM with GPU support","metadata":{}},{"cell_type":"code","source":"%%time\n\n# Refer to https://www.kaggle.com/vinhnguyen/gpu-acceleration-for-lightgbm and\n# https://medium.com/@vipulgote4/how-to-build-and-install-lightgbm-for-gpu-acceleration-2b53f0066c02\n\n!rm -r /opt/conda/lib/python3.6/site-packages/lightgbm\n!git clone --recursive https://github.com/Microsoft/LightGBM","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n!apt-get update && apt-get install -y -qq libboost-all-dev","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n%%bash\ncd LightGBM && rm -rf build\nmkdir build && cd build\ncmake -DUSE_GPU=1 -DOpenCL_LIBRARY=/usr/local/cuda/lib64/libOpenCL.so -DOpenCL_INCLUDE_DIR=/usr/local/cuda/include/ ..\nmake -j$(nproc)","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n!cd LightGBM/python-package/;python3 setup.py install --precompile","metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n!mkdir -p /etc/OpenCL/vendors && echo \"libnvidia-opencl.so.1\" > /etc/OpenCL/vendors/nvidia.icd\n!rm -r LightGBM","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Import libraries","metadata":{}},{"cell_type":"code","source":"%%time\n\nimport os\nimport logging\nimport sys\nimport time\nfrom datetime import timedelta\n\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\nimport gc\ngc.enable()\n\nimport numpy as np\nimport pandas as pd\n\nimport plotly.figure_factory as ff\n\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\n\nfrom xgboost import XGBClassifier\nimport xgboost as xgb\nxgb.set_config(verbosity=0)\n\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier","metadata":{"execution":{"iopub.status.busy":"2021-12-01T16:19:32.428184Z","iopub.execute_input":"2021-12-01T16:19:32.42881Z","iopub.status.idle":"2021-12-01T16:19:37.74519Z","shell.execute_reply.started":"2021-12-01T16:19:32.428721Z","shell.execute_reply":"2021-12-01T16:19:37.743913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load datasets","metadata":{}},{"cell_type":"code","source":"%%time\n\ndata_dir = \"../input/tabular-playground-series-dec-2021/\"\ncov_dir = \"../input/forest-cover-type-dataset/\"\n\ntrain  = pd.read_csv(data_dir  + \"train.csv\")\ntest = pd.read_csv(data_dir + \"test.csv\")\nsubmission = pd.read_csv(data_dir + \"sample_submission.csv\")\n\nTARGET = \"Cover_Type\"\nID = \"Id\"\n\n# Refer to https://www.kaggle.com/lucamassaron/baseline-lightgbm-with-covtype-augmentation/notebook\ncovtype = pd.read_csv(cov_dir + \"covtype.csv\")\ncovtype[ID] = range(len(train), len(train)+len(covtype))\ncovtype = covtype[train.columns].set_index(ID)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T16:19:40.108224Z","iopub.execute_input":"2021-12-01T16:19:40.109018Z","iopub.status.idle":"2021-12-01T16:20:05.814946Z","shell.execute_reply.started":"2021-12-01T16:19:40.108972Z","shell.execute_reply":"2021-12-01T16:20:05.813834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nprint(\"Train shape: \", train.shape)\nprint(\"Test shape: \", test.shape, end=\"\\n\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-12-01T16:20:42.35549Z","iopub.execute_input":"2021-12-01T16:20:42.355784Z","iopub.status.idle":"2021-12-01T16:20:42.364504Z","shell.execute_reply.started":"2021-12-01T16:20:42.355753Z","shell.execute_reply":"2021-12-01T16:20:42.363409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T16:20:44.401143Z","iopub.execute_input":"2021-12-01T16:20:44.401755Z","iopub.status.idle":"2021-12-01T16:20:44.42935Z","shell.execute_reply.started":"2021-12-01T16:20:44.401723Z","shell.execute_reply":"2021-12-01T16:20:44.428138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T16:20:46.47367Z","iopub.execute_input":"2021-12-01T16:20:46.474422Z","iopub.status.idle":"2021-12-01T16:20:46.49694Z","shell.execute_reply.started":"2021-12-01T16:20:46.474386Z","shell.execute_reply":"2021-12-01T16:20:46.495826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Features","metadata":{}},{"cell_type":"code","source":"features = [col for col in train.columns if col not in (ID, TARGET)]\n\nprint(f\"Features ({len(features)}):\")\nfor feature in features:\n    print(feature, end=\", \")","metadata":{"execution":{"iopub.status.busy":"2021-12-01T16:20:49.264007Z","iopub.execute_input":"2021-12-01T16:20:49.264612Z","iopub.status.idle":"2021-12-01T16:20:49.282745Z","shell.execute_reply.started":"2021-12-01T16:20:49.264579Z","shell.execute_reply":"2021-12-01T16:20:49.279327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Reduce memory usage","metadata":{}},{"cell_type":"code","source":"%%time\n\n# Refer to https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\ndef reduce_mem_usage(df, verbose=True):\n    numerics = [\"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print(\"Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)\".format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-01T16:20:51.341191Z","iopub.execute_input":"2021-12-01T16:20:51.341479Z","iopub.status.idle":"2021-12-01T16:20:51.354939Z","shell.execute_reply.started":"2021-12-01T16:20:51.341435Z","shell.execute_reply":"2021-12-01T16:20:51.353381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\ny = train.pop(TARGET)\nX = reduce_mem_usage(train.set_index(ID))\nX_test = reduce_mem_usage(test.set_index(ID))\n\n# Refer to # Refer to https://www.kaggle.com/lucamassaron/baseline-lightgbm-with-covtype-augmentation/notebook\naug_X = reduce_mem_usage(covtype.loc[covtype[TARGET].isin([4, 5]), X.columns])\naug_y = covtype.loc[covtype[TARGET].isin([4, 5]), TARGET]\n\ndel train\ndel test\ndel covtype","metadata":{"execution":{"iopub.status.busy":"2021-12-01T16:20:53.320769Z","iopub.execute_input":"2021-12-01T16:20:53.321286Z","iopub.status.idle":"2021-12-01T16:20:56.489301Z","shell.execute_reply.started":"2021-12-01T16:20:53.321238Z","shell.execute_reply":"2021-12-01T16:20:56.487988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predict","metadata":{}},{"cell_type":"code","source":"%%time\n\ndef predict_with_model(model, verbose=True, splits=5):\n    test_preds = []\n    valid_preds = {}\n    scores = []\n    \n    skf = StratifiedKFold(n_splits=splits, shuffle=True, random_state=42)\n    for fold, (idx_train, idx_valid) in enumerate(skf.split(X, y)):\n        start_time = time.monotonic()\n        \n        X_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n        X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid]\n        \n        valid_ids = X_valid.index.values.tolist()\n        \n        # Refer to https://www.kaggle.com/lucamassaron/baseline-lightgbm-with-covtype-augmentation/notebook\n        X_train = X_train.append(aug_X)\n        y_train = np.concatenate([y_train, aug_y])\n        \n        fit_params = {\n            \"eval_set\": [(X_valid, y_valid)],\n            \"early_stopping_rounds\": 100,\n        }\n        if verbose:\n            # weird, but lightgbm doesn't like this param\n            fit_params[\"verbose\"] = 1000\n\n        model.fit(X_train, y_train, **fit_params)\n        valid_pred = model.predict(X_valid)\n        test_pred = model.predict(X_test)\n        \n        test_preds.append(test_pred)\n        valid_preds.update(dict(zip(valid_ids, valid_pred)))\n\n        score = accuracy_score(y_valid, valid_pred)\n        \n        end_time = time.monotonic()\n        dur = timedelta(seconds=end_time - start_time)\n        print(f\"Fold {fold} | Accuracy: {score} | Took: {dur}\")\n        scores.append(score)\n    \n    test_preds = np.mean(np.column_stack(test_preds), axis=1)\n    valid_preds = pd.DataFrame.from_dict(valid_preds, orient=\"index\").reset_index()\n    \n    return test_preds, valid_preds, scores","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-01T16:20:56.491822Z","iopub.execute_input":"2021-12-01T16:20:56.492267Z","iopub.status.idle":"2021-12-01T16:20:56.505248Z","shell.execute_reply.started":"2021-12-01T16:20:56.492219Z","shell.execute_reply":"2021-12-01T16:20:56.504198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\ndef predict_with_models(models):\n    print(f\"Predicting with {len(models)} models...\", end=\"\\n\\n\")\n    for model_name, model in models:\n        start_time = time.monotonic()\n        \n        verbose = \"lgb\" not in model_name\n        \n        print(\"-\" * 50)\n        print(f\"Using {model_name} model...\")\n        test_preds, valid_preds, scores = predict_with_model(model, verbose=verbose)\n        print(f\"Score: {np.mean(scores)}, Std: {np.std(scores)}\", end=\"\\n\\n\")\n\n        print(\"Saving predictions...\")\n        valid_preds.columns = [ID, model_name]\n        valid_preds.to_csv(f\"{model_name}_train.csv\", index=False)\n\n        test_preds_df = pd.DataFrame({ID: submission[ID], model_name: test_preds})\n        test_preds_df.to_csv(f\"{model_name}_test.csv\", index=False)\n\n        sub = pd.DataFrame({ID: submission[ID], TARGET: test_preds.astype(int)})\n        sub.to_csv(f\"{model_name}_submission.csv\", index=False)\n        \n        end_time = time.monotonic()\n        dur = timedelta(seconds=end_time - start_time)\n        print(f\"Took: {dur}\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-01T16:40:51.424275Z","iopub.execute_input":"2021-12-01T16:40:51.424554Z","iopub.status.idle":"2021-12-01T16:40:51.436005Z","shell.execute_reply.started":"2021-12-01T16:40:51.424523Z","shell.execute_reply":"2021-12-01T16:40:51.434868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nSEED = 42\n\nlgb1_params = {\n    \"random_state\": SEED,\n    \"n_estimators\": 1500,\n    \"objective\" : \"multiclass\",\n    \"verbose\": 0,\n    # gpu\n    \"device\": \"gpu\",\n    \"gpu_platform_id\": 0,\n    \"gpu_device_id\": 0,\n}\n\nxgb1_params = {\n    \"random_state\": SEED,\n    \"n_estimators\": 1500,\n    \"objective\":\"multi:softmax\",\n    \"booster\": \"gbtree\",\n    \"verbose\": 0,\n    # gpu\n    \"gpu_id\": 0,\n    \"tree_method\": \"gpu_hist\",\n    \"predictor\": \"gpu_predictor\"\n}\n\ncb1_params = {\n    \"random_seed\": SEED,\n    \"iterations\": 1500,\n    \"loss_function\": \"MultiClass\",\n    \"verbose\": 0,\n    # gpu\n    \"task_type\" : \"GPU\",\n    \"devices\" : \"0\",\n}\n\n# Model name must be unique\nmodels = [\n    (\"lgb1\", LGBMClassifier(**lgb1_params)),\n    (\"xgb1\", XGBClassifier(**xgb1_params)),\n    (\"cb1\", CatBoostClassifier(**cb1_params)),\n]","metadata":{"execution":{"iopub.status.busy":"2021-12-01T16:40:53.834577Z","iopub.execute_input":"2021-12-01T16:40:53.835262Z","iopub.status.idle":"2021-12-01T16:40:53.848128Z","shell.execute_reply.started":"2021-12-01T16:40:53.83523Z","shell.execute_reply":"2021-12-01T16:40:53.846871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\npredict_with_models(models)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T16:40:57.952238Z","iopub.execute_input":"2021-12-01T16:40:57.952566Z","iopub.status.idle":"2021-12-01T16:48:19.647675Z","shell.execute_reply.started":"2021-12-01T16:40:57.95252Z","shell.execute_reply":"2021-12-01T16:48:19.646583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualize","metadata":{}},{"cell_type":"code","source":"def load_viz_data(submission_files):\n    dfs = []\n    for submission_file in submission_files:\n        df = pd.read_csv(submission_file)\n        dfs.append((submission_file, df))\n        \n    hist_data = []\n    for i in range(len(dfs)):\n        _, df = dfs[i]\n        hist_data.append(df[TARGET])\n        \n    return hist_data","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nsubmission_files = [f\"{model_name}_submission.csv\" for (model_name, _) in models]\nviz_data = load_viz_data(submission_files)\n\nfig = ff.create_distplot(viz_data, submission_files, show_hist=False, show_rug=False)\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}