{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Noise shift at the 3 million mark\n\nBuilding on the work of @rkaveland [here](https://www.kaggle.com/kaaveland/tps202112-ctgan-artifacts-chunks?kernelSessionId=82663352) and @ambrosm [here](https://www.kaggle.com/c/tabular-playground-series-dec-2021/discussion/292381)\n\nThe Wilderness_Area features are noisy. One expects every sample to be in exactly one Wilderness Area, but in the data we see samples that have 0, 1, or more areas assigned. \n\nI noticed the noise model shifts right around the 3 million sample mark in the train data, and seems to remain consistent throughout the test data.\n\nDefine Noise as the sum of the assigned Wilderness Areas minus one. Then plot the cumulative sum. It's pretty dramatic.\n","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ntrain = pd.read_csv(\n         '/kaggle/input/tabular-playground-series-dec-2021/train.csv')\ndel train['Cover_Type']\n\ntest = pd.read_csv(\n         '/kaggle/input/tabular-playground-series-dec-2021/test.csv')\n\ntraintest = pd.concat([train, test], axis=0)\n\nwa_sum = (\n    traintest['Wilderness_Area1'] * 1 +\n    traintest['Wilderness_Area2'] * 1 +\n    traintest['Wilderness_Area3'] * 1 +\n    traintest['Wilderness_Area4'] * 1).values\n\nplt.plot(np.cumsum(wa_sum - 1))\nplt.show()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-19T23:16:45.077508Z","iopub.execute_input":"2021-12-19T23:16:45.077791Z","iopub.status.idle":"2021-12-19T23:16:59.630446Z","shell.execute_reply.started":"2021-12-19T23:16:45.077761Z","shell.execute_reply":"2021-12-19T23:16:59.629156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plotting the individual noise levels.","metadata":{}},{"cell_type":"code","source":"plt.plot(np.cumsum(wa_sum == 0))\nplt.plot(np.cumsum(wa_sum == 2))\nplt.plot(np.cumsum(wa_sum > 2))\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-19T23:22:47.710357Z","iopub.execute_input":"2021-12-19T23:22:47.710627Z","iopub.status.idle":"2021-12-19T23:22:49.18246Z","shell.execute_reply.started":"2021-12-19T23:22:47.710599Z","shell.execute_reply":"2021-12-19T23:22:49.181949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Implications\n\nI'm not sure of the impact. \n- It probably explains why adding the sum of the Wilderness Area features as a feature to your model is helpful. \n- Your CV scores using the entire training set probably aren't going to be representative of the test score. \n- You may actually be better off training on only the final million samples. \n- Perhaps adding some transformation of Id as a feature will improve your model.\n- Tree learners cannot extrapolate, but neural nets can. Extrapolation is essential, as the noise values in test set are beyond the values in training data.","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}}]}