{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"In this version of notebook we are verifying the best model indicaded by GridSearchCV, which was built in my other notebook:\nhttps://www.kaggle.com/christoforum/tps-dec-2021-neural-network-with-gridsearchcv ","metadata":{}},{"cell_type":"markdown","source":"## Import necessary libraries and datasets","metadata":{}},{"cell_type":"code","source":"import math\nimport numpy as np\n\nimport pandas as pd\npd.set_option('display.max_columns', 60)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, BatchNormalization\nfrom tensorflow.keras.optimizers import Adam\nfrom keras.callbacks import EarlyStopping\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom tensorflow.keras.utils import plot_model\n\nfrom sklearn.preprocessing import RobustScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\nfrom sklearn import metrics\nfrom sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix, classification_report, accuracy_score\nfrom scipy import stats","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-08T10:33:06.172724Z","iopub.execute_input":"2021-12-08T10:33:06.173338Z","iopub.status.idle":"2021-12-08T10:33:14.028332Z","shell.execute_reply.started":"2021-12-08T10:33:06.173191Z","shell.execute_reply":"2021-12-08T10:33:14.027232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"INT8_MIN = np.iinfo(np.int8).min\nINT8_MAX = np.iinfo(np.int8).max\nINT16_MIN = np.iinfo(np.int16).min\nINT16_MAX = np.iinfo(np.int16).max\nINT32_MIN = np.iinfo(np.int32).min\nINT32_MAX = np.iinfo(np.int32).max\n\nFLOAT16_MIN = np.finfo(np.float16).min\nFLOAT16_MAX = np.finfo(np.float16).max\nFLOAT32_MIN = np.finfo(np.float32).min\nFLOAT32_MAX = np.finfo(np.float32).max\n\n\ndef memory_usage(data, detail = 1):\n    if detail:\n        display(data.memory_usage())\n    memory = data.memory_usage().sum() / (1024 * 1024)\n    print(\"Memory usage : {0:.2f}MB\".format(memory))\n    return memory\n\n\ndef compress_dataset(data):\n    memory_before_compress = memory_usage(data, 0)\n    print()\n    print('=' * 50)\n    for col in data.columns:\n        col_dtype = data[col][:100].dtype\n\n        if col_dtype != 'object':\n            print(\"Name: {0:24s} Type: {1}\".format(col, col_dtype))\n            col_series = data[col]\n            col_min = col_series.min()\n            col_max = col_series.max()\n\n            if col_dtype == 'float64':\n                print(\" variable min: {0:15s} max: {1:15s}\".format(str(np.round(col_min, 4)), str(np.round(col_max, 4))))\n                if (col_min > FLOAT16_MIN) and (col_max < FLOAT16_MAX):\n                    data[col] = data[col].astype(np.float16)\n                    print(\"  float16 min: {0:15s} max: {1:15s}\".format(str(FLOAT16_MIN), str(FLOAT16_MAX)))\n                    print(\"compress float64 --> float16\")\n                elif (col_min > FLOAT32_MIN) and (col_max < FLOAT32_MAX):\n                    data[col] = data[col].astype(np.float32)\n                    print(\"  float32 min: {0:15s} max: {1:15s}\".format(str(FLOAT32_MIN), str(FLOAT32_MAX)))\n                    print(\"compress float64 --> float32\")\n                else:\n                    pass\n                memory_after_compress = memory_usage(data, 0)\n                print(\"Compress Rate: [{0:.2%}]\".format((memory_before_compress-memory_after_compress) / memory_before_compress))\n                print('=' * 50)\n\n            if col_dtype == 'int64':\n                print(\" variable min: {0:15s} max: {1:15s}\".format(str(col_min), str(col_max)))\n                type_flag = 64\n                if (col_min > INT8_MIN / 2) and (col_max < INT8_MAX / 2):\n                    type_flag = 8\n                    data[col] = data[col].astype(np.int8)\n                    print(\"     int8 min: {0:15s} max: {1:15s}\".format(str(INT8_MIN), str(INT8_MAX)))\n                elif (col_min > INT16_MIN) and (col_max < INT16_MAX):\n                    type_flag = 16\n                    data[col] = data[col].astype(np.int16)\n                    print(\"    int16 min: {0:15s} max: {1:15s}\".format(str(INT16_MIN), str(INT16_MAX)))\n                elif (col_min > INT32_MIN) and (col_max < INT32_MAX):\n                    type_flag = 32\n                    data[col] = data[col].astype(np.int32)\n                    print(\"    int32 min: {0:15s} max: {1:15s}\".format(str(INT32_MIN), str(INT32_MAX)))\n                    type_flag = 1\n                else:\n                    pass\n                memory_after_compress = memory_usage(data, 0)\n                print(\"Compress Rate: [{0:.2%}]\".format((memory_before_compress-memory_after_compress) / memory_before_compress))\n                if type_flag == 32:\n                    print(\"compress (int64) ==> (int32)\")\n                elif type_flag == 16:\n                    print(\"compress (int64) ==> (int16)\")\n                else:\n                    print(\"compress (int64) ==> (int8)\")\n                print('=' * 50)\n\n    print()\n    memory_after_compress = memory_usage(data, 0)\n    print(\"Compress Rate: [{0:.2%}]\".format((memory_before_compress-memory_after_compress) / memory_before_compress))\n    \n    return data","metadata":{"execution":{"iopub.status.busy":"2021-12-08T10:33:14.03037Z","iopub.execute_input":"2021-12-08T10:33:14.030691Z","iopub.status.idle":"2021-12-08T10:33:14.056219Z","shell.execute_reply.started":"2021-12-08T10:33:14.03065Z","shell.execute_reply":"2021-12-08T10:33:14.055504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv('../input/tabular-playground-series-dec-2021/train.csv')\ndf_test = pd.read_csv('../input/tabular-playground-series-dec-2021/test.csv')","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-12-08T10:33:14.05734Z","iopub.execute_input":"2021-12-08T10:33:14.057677Z","iopub.status.idle":"2021-12-08T10:33:39.050323Z","shell.execute_reply.started":"2021-12-08T10:33:14.05765Z","shell.execute_reply":"2021-12-08T10:33:39.049503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train set summary","metadata":{}},{"cell_type":"markdown","source":"Let's see what a train set looks like","metadata":{}},{"cell_type":"code","source":"df_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-08T10:33:39.053809Z","iopub.execute_input":"2021-12-08T10:33:39.054162Z","iopub.status.idle":"2021-12-08T10:33:39.094712Z","shell.execute_reply.started":"2021-12-08T10:33:39.054114Z","shell.execute_reply":"2021-12-08T10:33:39.09382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`Id` column **is redundant**. Let's remove it.","metadata":{}},{"cell_type":"code","source":"df_train.drop('Id', axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T10:33:39.096012Z","iopub.execute_input":"2021-12-08T10:33:39.096232Z","iopub.status.idle":"2021-12-08T10:33:39.909969Z","shell.execute_reply.started":"2021-12-08T10:33:39.096205Z","shell.execute_reply":"2021-12-08T10:33:39.908969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check how big is our data.","metadata":{}},{"cell_type":"code","source":"print(f'Train set shape:   {df_train.shape}')","metadata":{"execution":{"iopub.status.busy":"2021-12-08T10:33:39.912047Z","iopub.execute_input":"2021-12-08T10:33:39.912392Z","iopub.status.idle":"2021-12-08T10:33:39.921984Z","shell.execute_reply.started":"2021-12-08T10:33:39.912347Z","shell.execute_reply":"2021-12-08T10:33:39.920956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our train set has **4 000 000 rows** and **55 columns**.","metadata":{}},{"cell_type":"markdown","source":"Let's find out something more about data. ","metadata":{}},{"cell_type":"code","source":"df_train.info()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-12-08T10:33:39.922948Z","iopub.execute_input":"2021-12-08T10:33:39.92316Z","iopub.status.idle":"2021-12-08T10:33:39.950732Z","shell.execute_reply.started":"2021-12-08T10:33:39.923134Z","shell.execute_reply":"2021-12-08T10:33:39.949842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All columns consist of **integers** and the set is huge - it using a lot of memory.","metadata":{}},{"cell_type":"markdown","source":"Let's see a distribution of each column.","metadata":{}},{"cell_type":"code","source":"df_train.describe()","metadata":{"execution":{"iopub.status.busy":"2021-12-08T10:33:39.952269Z","iopub.execute_input":"2021-12-08T10:33:39.952674Z","iopub.status.idle":"2021-12-08T10:33:46.061708Z","shell.execute_reply.started":"2021-12-08T10:33:39.952642Z","shell.execute_reply":"2021-12-08T10:33:46.060801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Columns `Soil_Type7` and `Soil_Type15` contain only **one value - 0**. They don't introduce a variability, so we can remove them. All remaining columns`Soil_Type` has **two values - 0 and 1**.","metadata":{}},{"cell_type":"code","source":"df_train.drop(['Soil_Type7', 'Soil_Type15'], axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T10:33:46.063152Z","iopub.execute_input":"2021-12-08T10:33:46.063492Z","iopub.status.idle":"2021-12-08T10:33:46.900753Z","shell.execute_reply.started":"2021-12-08T10:33:46.063449Z","shell.execute_reply":"2021-12-08T10:33:46.900114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check dataset has some missing values.","metadata":{}},{"cell_type":"code","source":"df_train.isnull().sum().max() != 0","metadata":{"execution":{"iopub.status.busy":"2021-12-08T10:33:46.903337Z","iopub.execute_input":"2021-12-08T10:33:46.904021Z","iopub.status.idle":"2021-12-08T10:33:47.188771Z","shell.execute_reply.started":"2021-12-08T10:33:46.903985Z","shell.execute_reply":"2021-12-08T10:33:47.187942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are **no missing values** in train dataset.","metadata":{}},{"cell_type":"markdown","source":"At the end let's check dataset has duplicated rows.","metadata":{}},{"cell_type":"code","source":"df_train.duplicated().unique()","metadata":{"execution":{"iopub.status.busy":"2021-12-08T10:33:47.190011Z","iopub.execute_input":"2021-12-08T10:33:47.190255Z","iopub.status.idle":"2021-12-08T10:33:54.295249Z","shell.execute_reply.started":"2021-12-08T10:33:47.19022Z","shell.execute_reply":"2021-12-08T10:33:54.294089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are **no duplicated rows** in train dataset.","metadata":{}},{"cell_type":"markdown","source":"## Test set summary","metadata":{}},{"cell_type":"markdown","source":"We'll carry out exactly the same steps as above.","metadata":{}},{"cell_type":"markdown","source":"Let's see what a test set looks like","metadata":{}},{"cell_type":"code","source":"df_test.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-08T10:33:54.296492Z","iopub.execute_input":"2021-12-08T10:33:54.296749Z","iopub.status.idle":"2021-12-08T10:33:54.329521Z","shell.execute_reply.started":"2021-12-08T10:33:54.29671Z","shell.execute_reply":"2021-12-08T10:33:54.328415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`Id` column **is redundant**. Let's remove it.","metadata":{}},{"cell_type":"code","source":"df_test.drop('Id', axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T10:33:54.330895Z","iopub.execute_input":"2021-12-08T10:33:54.331146Z","iopub.status.idle":"2021-12-08T10:33:54.485034Z","shell.execute_reply.started":"2021-12-08T10:33:54.331115Z","shell.execute_reply":"2021-12-08T10:33:54.48418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check how big is our data.","metadata":{}},{"cell_type":"code","source":"print(f'Test set shape:   {df_test.shape}')","metadata":{"execution":{"iopub.status.busy":"2021-12-08T10:33:54.486008Z","iopub.execute_input":"2021-12-08T10:33:54.486258Z","iopub.status.idle":"2021-12-08T10:33:54.501628Z","shell.execute_reply.started":"2021-12-08T10:33:54.486226Z","shell.execute_reply":"2021-12-08T10:33:54.500431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our test set has **1 000 000 rows** and **54 columns**.","metadata":{}},{"cell_type":"markdown","source":"Let's find out something more about data. ","metadata":{}},{"cell_type":"code","source":"df_test.info()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-12-08T10:33:54.503334Z","iopub.execute_input":"2021-12-08T10:33:54.503604Z","iopub.status.idle":"2021-12-08T10:33:54.610123Z","shell.execute_reply.started":"2021-12-08T10:33:54.50357Z","shell.execute_reply":"2021-12-08T10:33:54.608861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All columns consist of **integers** and the set is huge - it using a lot of memory. Just like in train set.","metadata":{}},{"cell_type":"markdown","source":"Let's see a distribution of each column.","metadata":{}},{"cell_type":"code","source":"df_test.describe()","metadata":{"execution":{"iopub.status.busy":"2021-12-08T10:33:54.611801Z","iopub.execute_input":"2021-12-08T10:33:54.612443Z","iopub.status.idle":"2021-12-08T10:33:56.288421Z","shell.execute_reply.started":"2021-12-08T10:33:54.612396Z","shell.execute_reply":"2021-12-08T10:33:56.287161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Columns `Soil_Type7` and `Soil_Type15` contain only **one value - 0**. They don't introduce a variability, so we can remove them. All remaining columns`Soil_Type` has **two values - 0 and 1**. Just like in train set.","metadata":{}},{"cell_type":"code","source":"df_test.drop(['Soil_Type7', 'Soil_Type15'], axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T10:33:56.289811Z","iopub.execute_input":"2021-12-08T10:33:56.290089Z","iopub.status.idle":"2021-12-08T10:33:56.506641Z","shell.execute_reply.started":"2021-12-08T10:33:56.290058Z","shell.execute_reply":"2021-12-08T10:33:56.505111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check dataset has some missing values.","metadata":{}},{"cell_type":"code","source":"df_test.isnull().sum().max() != 0","metadata":{"execution":{"iopub.status.busy":"2021-12-08T10:33:56.508766Z","iopub.execute_input":"2021-12-08T10:33:56.509189Z","iopub.status.idle":"2021-12-08T10:33:56.592073Z","shell.execute_reply.started":"2021-12-08T10:33:56.509143Z","shell.execute_reply":"2021-12-08T10:33:56.591034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are **no missing values** in test dataset.","metadata":{}},{"cell_type":"markdown","source":"At the end let's check dataset has duplicated rows.","metadata":{}},{"cell_type":"code","source":"df_test.duplicated().unique()","metadata":{"execution":{"iopub.status.busy":"2021-12-08T10:33:56.59364Z","iopub.execute_input":"2021-12-08T10:33:56.594623Z","iopub.status.idle":"2021-12-08T10:33:57.638126Z","shell.execute_reply.started":"2021-12-08T10:33:56.594574Z","shell.execute_reply":"2021-12-08T10:33:57.637195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are **no duplicated rows** in test dataset.","metadata":{}},{"cell_type":"markdown","source":"## Target summary","metadata":{}},{"cell_type":"markdown","source":"Let's check a number of classes in target column.","metadata":{}},{"cell_type":"code","source":"df_train['Cover_Type'].unique()","metadata":{"execution":{"iopub.status.busy":"2021-12-08T10:33:57.639513Z","iopub.execute_input":"2021-12-08T10:33:57.640099Z","iopub.status.idle":"2021-12-08T10:33:57.67267Z","shell.execute_reply.started":"2021-12-08T10:33:57.640016Z","shell.execute_reply":"2021-12-08T10:33:57.671616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have **7** classes. We need to check that classes are balanced or not.","metadata":{}},{"cell_type":"code","source":"sns.countplot(x = df_train['Cover_Type'])\nplt.grid()","metadata":{"execution":{"iopub.status.busy":"2021-12-08T10:33:57.674428Z","iopub.execute_input":"2021-12-08T10:33:57.67482Z","iopub.status.idle":"2021-12-08T10:33:58.240392Z","shell.execute_reply.started":"2021-12-08T10:33:57.674756Z","shell.execute_reply":"2021-12-08T10:33:58.239471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['Cover_Type'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-12-08T10:33:58.241806Z","iopub.execute_input":"2021-12-08T10:33:58.242088Z","iopub.status.idle":"2021-12-08T10:33:58.272493Z","shell.execute_reply.started":"2021-12-08T10:33:58.242056Z","shell.execute_reply":"2021-12-08T10:33:58.271361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Unfortunatelly, **classes are imbalanced**. Class no. 5 appears only once. We'll remove it.","metadata":{}},{"cell_type":"code","source":"df_train.drop(df_train[df_train['Cover_Type'] == 5].index, axis = 0, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T10:33:58.274186Z","iopub.execute_input":"2021-12-08T10:33:58.274713Z","iopub.status.idle":"2021-12-08T10:33:59.519611Z","shell.execute_reply.started":"2021-12-08T10:33:58.27466Z","shell.execute_reply":"2021-12-08T10:33:59.518881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Correlation","metadata":{}},{"cell_type":"code","source":"non_binary_columns = list(df_train.columns[:10])\nsns.heatmap(df_train[non_binary_columns].corr())","metadata":{"execution":{"iopub.status.busy":"2021-12-08T10:33:59.521101Z","iopub.execute_input":"2021-12-08T10:33:59.521628Z","iopub.status.idle":"2021-12-08T10:34:01.605586Z","shell.execute_reply.started":"2021-12-08T10:33:59.521584Z","shell.execute_reply":"2021-12-08T10:34:01.604546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is **no correlation between non-binary feature**.","metadata":{}},{"cell_type":"markdown","source":"## Feature engineering","metadata":{}},{"cell_type":"markdown","source":"Let's look at the `Aspect` column. It is the compass direction that a terrain faces and it is expressed in degrees. Values are contained in range [-33, 407] (train set) and [-33, 400] (test set), so to all values less than 0 we add 360 and to all values greater than or equal to 360 we substract 360.","metadata":{}},{"cell_type":"code","source":"def aspect(x):\n    if x < 0:\n        return x + 360\n    elif (x >= 0) & (x < 360):\n        return x\n    else:\n        return x - 360","metadata":{"execution":{"iopub.status.busy":"2021-12-08T10:34:01.606981Z","iopub.execute_input":"2021-12-08T10:34:01.607233Z","iopub.status.idle":"2021-12-08T10:34:01.612516Z","shell.execute_reply.started":"2021-12-08T10:34:01.607201Z","shell.execute_reply":"2021-12-08T10:34:01.611371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['Aspect'] = df_train['Aspect'].map(aspect)\ndf_test['Aspect'] = df_train['Aspect'].map(aspect)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T10:34:01.614013Z","iopub.execute_input":"2021-12-08T10:34:01.614265Z","iopub.status.idle":"2021-12-08T10:34:07.840422Z","shell.execute_reply.started":"2021-12-08T10:34:01.614234Z","shell.execute_reply":"2021-12-08T10:34:07.839521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's analyse `Hillshade` columns. Hillshading computes surface illumination as values from 0 to 255 based on a given compass direction to the sun (azimuth) and a certain altitude above the horizon (altitude). All our hillshade's values are between -53 and 301 in train set and between -51 and 296 in test set. Therefore we will replace all negative numbers with 0 and all numbers greater than 255 with 255.","metadata":{}},{"cell_type":"code","source":"def hillshade(x):\n    if x < 0:\n        return 0\n    elif (x >= 0) & (x < 256):\n        return x\n    else:\n        return 255","metadata":{"execution":{"iopub.status.busy":"2021-12-08T10:34:07.842143Z","iopub.execute_input":"2021-12-08T10:34:07.842866Z","iopub.status.idle":"2021-12-08T10:34:07.850479Z","shell.execute_reply.started":"2021-12-08T10:34:07.842815Z","shell.execute_reply":"2021-12-08T10:34:07.849377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['Hillshade_9am'] = df_train['Hillshade_9am'].map(hillshade)\ndf_train['Hillshade_Noon'] = df_train['Hillshade_Noon'].map(hillshade)\ndf_train['Hillshade_3pm'] = df_train['Hillshade_3pm'].map(hillshade)\n\ndf_test['Hillshade_9am'] = df_test['Hillshade_9am'].map(hillshade)\ndf_test['Hillshade_Noon'] = df_test['Hillshade_Noon'].map(hillshade)\ndf_test['Hillshade_3pm'] = df_test['Hillshade_3pm'].map(hillshade)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T10:34:07.854908Z","iopub.execute_input":"2021-12-08T10:34:07.855219Z","iopub.status.idle":"2021-12-08T10:34:18.781367Z","shell.execute_reply.started":"2021-12-08T10:34:07.855183Z","shell.execute_reply":"2021-12-08T10:34:18.78069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We create a new columns based on the `Horizontal_Distance_To_Hydrology` and `Vertical_Distance_To_Hydrology` columns. First column will contain L1 distance and second column will contain L2 distance.","metadata":{}},{"cell_type":"code","source":"df_train['L1_distance'] = np.abs(df_train['Horizontal_Distance_To_Hydrology']) + np.abs(df_train['Vertical_Distance_To_Hydrology'])\ndf_test['L1_distance'] = np.abs(df_test['Horizontal_Distance_To_Hydrology']) + np.abs(df_test['Vertical_Distance_To_Hydrology'])\n\ndf_train['L2_distance'] = np.sqrt(np.square(df_train['Horizontal_Distance_To_Hydrology']) + np.square(df_train['Vertical_Distance_To_Hydrology']))\ndf_test['L2_distance'] = np.sqrt(np.square(df_test['Horizontal_Distance_To_Hydrology']) + np.square(df_test['Vertical_Distance_To_Hydrology']))","metadata":{"execution":{"iopub.status.busy":"2021-12-08T10:34:18.782828Z","iopub.execute_input":"2021-12-08T10:34:18.78309Z","iopub.status.idle":"2021-12-08T10:34:18.895334Z","shell.execute_reply.started":"2021-12-08T10:34:18.783059Z","shell.execute_reply":"2021-12-08T10:34:18.894226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, we create one column with sum of `Soil_Type` columns and the other one with sum of `Wilderness_Area` columns.","metadata":{}},{"cell_type":"code","source":"soil_type_cols = [col for col in df_train.columns if 'Soil' in col]\n\ndf_train['Soil_Type_sum'] = df_train[soil_type_cols].sum(axis = 1)\ndf_test['Soil_Type_sum'] = df_test[soil_type_cols].sum(axis = 1)\n\nwilderness_area_cols = [col for col in df_train.columns if 'Wilderness' in col]\n\ndf_train['Wilderness_Area_sum'] = df_train[wilderness_area_cols].sum(axis = 1)\ndf_test['Wilderness_Area_sum'] = df_test[wilderness_area_cols].sum(axis = 1)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T10:34:18.896769Z","iopub.execute_input":"2021-12-08T10:34:18.897024Z","iopub.status.idle":"2021-12-08T10:34:24.418782Z","shell.execute_reply.started":"2021-12-08T10:34:18.896993Z","shell.execute_reply":"2021-12-08T10:34:24.418014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Scaler","metadata":{}},{"cell_type":"markdown","source":"Let's scale and match our datasets. We use RobustScaler algotithm.","metadata":{}},{"cell_type":"code","source":"scale_cols = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',\n              'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways', 'Hillshade_9am', \n              'Hillshade_Noon', 'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points', 'L1_distance',\n              'L2_distance', 'Soil_Type_sum', 'Wilderness_Area_sum']\n\nscaler = RobustScaler()\n\ndf_train[scale_cols] = scaler.fit_transform(df_train[scale_cols])\ndf_test[scale_cols] = scaler.transform(df_test[scale_cols])","metadata":{"execution":{"iopub.status.busy":"2021-12-08T10:34:24.419893Z","iopub.execute_input":"2021-12-08T10:34:24.420104Z","iopub.status.idle":"2021-12-08T10:34:41.352024Z","shell.execute_reply.started":"2021-12-08T10:34:24.420078Z","shell.execute_reply":"2021-12-08T10:34:41.35107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Memory releasing","metadata":{}},{"cell_type":"markdown","source":"Datasets are very large and use huge quantity of memory, so we need to convert type of columns to ones using less memory.","metadata":{}},{"cell_type":"code","source":"df_train = compress_dataset(df_train)\ndf_test = compress_dataset(df_test)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-12-08T10:34:41.355753Z","iopub.execute_input":"2021-12-08T10:34:41.356021Z","iopub.status.idle":"2021-12-08T10:34:59.511819Z","shell.execute_reply.started":"2021-12-08T10:34:41.355989Z","shell.execute_reply":"2021-12-08T10:34:59.510876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CNN","metadata":{}},{"cell_type":"markdown","source":"We need to modify target by encoding labels in following way:\n* 1 -> 0\n* 2 -> 1\n* 3 -> 2\n* 4 -> 3\n* 6 -> 4\n* 7 -> 5","metadata":{}},{"cell_type":"code","source":"le = LabelEncoder()\ndf_train['Cover_Type'] = le.fit_transform(df_train['Cover_Type'])","metadata":{"execution":{"iopub.status.busy":"2021-12-08T10:34:59.513235Z","iopub.execute_input":"2021-12-08T10:34:59.513552Z","iopub.status.idle":"2021-12-08T10:34:59.724841Z","shell.execute_reply.started":"2021-12-08T10:34:59.513507Z","shell.execute_reply":"2021-12-08T10:34:59.723987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's define our features and target.","metadata":{}},{"cell_type":"code","source":"feats = [col for col in df_train.columns if 'Cover_Type' not in col]\nX = df_train[feats]\ny = df_train['Cover_Type']","metadata":{"execution":{"iopub.status.busy":"2021-12-08T10:34:59.726004Z","iopub.execute_input":"2021-12-08T10:34:59.726224Z","iopub.status.idle":"2021-12-08T10:35:00.125027Z","shell.execute_reply.started":"2021-12-08T10:34:59.726197Z","shell.execute_reply":"2021-12-08T10:35:00.123958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's build self-normalizing neural network model.","metadata":{}},{"cell_type":"code","source":"def model_cnn():\n    \n    model = Sequential()\n    model.add(Dense(128, activation = 'relu', kernel_initializer = 'uniform', input_shape = [X.shape[1]]))\n    model.add(Dropout(0.3))\n    model.add(BatchNormalization())\n    model.add(Dense(64, activation = 'relu', kernel_initializer = 'uniform'))\n    model.add(Dropout(0.3))\n    model.add(BatchNormalization())\n    model.add(Dense(32, activation = 'relu', kernel_initializer = 'uniform'))\n    model.add(Dropout(0.3))\n    model.add(BatchNormalization())\n    model.add(Dense(16, activation = 'relu', kernel_initializer = 'uniform'))\n    model.add(Dropout(0.3))\n    model.add(BatchNormalization())\n    model.add(Dense(8, activation = 'relu', kernel_initializer = 'uniform'))\n    model.add(Dropout(0.3))\n    model.add(BatchNormalization())\n    model.add(Dense(6, activation = 'softmax'))\n\n    model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n\n    return model\n\nearly_stop = EarlyStopping(monitor = 'val_accuracy', patience = 10, \n                           verbose = 1, mode = 'max', restore_best_weights = True)\nred_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.5, patience = 5, verbose = 1)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T10:35:00.126316Z","iopub.execute_input":"2021-12-08T10:35:00.126537Z","iopub.status.idle":"2021-12-08T10:35:00.139226Z","shell.execute_reply.started":"2021-12-08T10:35:00.126509Z","shell.execute_reply":"2021-12-08T10:35:00.138327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_cnn().summary()","metadata":{"execution":{"iopub.status.busy":"2021-12-08T10:35:00.141062Z","iopub.execute_input":"2021-12-08T10:35:00.141325Z","iopub.status.idle":"2021-12-08T10:35:00.431544Z","shell.execute_reply.started":"2021-12-08T10:35:00.141294Z","shell.execute_reply":"2021-12-08T10:35:00.430671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = KerasClassifier(build_fn = model_cnn, epochs = 100, batch_size = 1024, \n                        verbose = 1, callbacks = [early_stop, red_lr])","metadata":{"execution":{"iopub.status.busy":"2021-12-08T10:35:00.43293Z","iopub.execute_input":"2021-12-08T10:35:00.433173Z","iopub.status.idle":"2021-12-08T10:35:00.438492Z","shell.execute_reply.started":"2021-12-08T10:35:00.433143Z","shell.execute_reply":"2021-12-08T10:35:00.437655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model(model_cnn(), show_shapes = True, show_layer_names = True)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T10:35:00.439828Z","iopub.execute_input":"2021-12-08T10:35:00.440076Z","iopub.status.idle":"2021-12-08T10:35:01.635909Z","shell.execute_reply.started":"2021-12-08T10:35:00.440045Z","shell.execute_reply":"2021-12-08T10:35:01.634935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = []\naccuracy = []\n\ncv = StratifiedKFold(n_splits = 10, shuffle = True, random_state = 12)\n\nfor fold, (train_idx, test_idx) in enumerate(cv.split(X, y)):\n    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n    \n    history = model.fit(X_train, y_train, validation_data = (X_test, y_test))\n    y_preds = model.predict(X_test)\n    y_preds = le.inverse_transform(y_preds)\n    y_true = le.inverse_transform(y_test)\n    acc = accuracy_score(y_true, y_preds)\n    accuracy.append(acc)\n    \n    df_test_preds = model.predict(df_test)\n    df_test_preds = le.inverse_transform(df_test_preds)\n    preds.append(df_test_preds)\n    \n    print('*' * 20)\n    print(f'*****    Summary of Fold {fold + 1}    *****')\n    print('*' * 20)\n    print(f'Acuuracy: {acc}')\n    \n    print('*' * 20)\n    print(classification_report(y_true, y_preds, zero_division = 0))\n    \n    print('*' * 20)\n    cm = confusion_matrix(y_true, y_preds)\n    fig, ax = plt.subplots(figsize = (10,10))\n    cmd = ConfusionMatrixDisplay(cm, display_labels = set(y_true))\n    cmd.plot(cmap = plt.cm.Blues, ax = ax)\n    plt.show()\n    \n    print('*' * 20)\n    plt.figure(figsize = (15, 5))\n\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n\n    plt.title(f'Fold {fold + 1} - Model Accuracy', size = 16)\n    plt.xlabel('Epoch')\n    plt.legend(['Train accuracy', 'Test accuracy'], loc = 4)\n    plt.grid()\n    plt.show()\n\n    plt.figure(figsize = (15, 5))\n\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n\n    plt.title(f'Fold {fold + 1} - Model loss', size = 16)\n    plt.xlabel('Epoch')\n    plt.legend(['Train loss', 'Test loss'], loc = 7)\n    plt.grid()\n    plt.show()\n    \nprint('*' * 20)\nprint(f' Mean accuracy: {np.mean(accuracy)}')","metadata":{"execution":{"iopub.status.busy":"2021-12-08T10:35:01.638382Z","iopub.execute_input":"2021-12-08T10:35:01.638986Z","iopub.status.idle":"2021-12-08T17:12:06.420233Z","shell.execute_reply.started":"2021-12-08T10:35:01.638937Z","shell.execute_reply":"2021-12-08T17:12:06.419075Z"},"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission","metadata":{}},{"cell_type":"code","source":"sub = pd.read_csv('../input/tabular-playground-series-dec-2021/sample_submission.csv')\nsub['Cover_Type'] = stats.mode(preds, axis = 0)[0].T\nsub.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-08T17:12:06.422327Z","iopub.execute_input":"2021-12-08T17:12:06.422612Z","iopub.status.idle":"2021-12-08T17:12:40.119996Z","shell.execute_reply.started":"2021-12-08T17:12:06.422556Z","shell.execute_reply":"2021-12-08T17:12:40.11928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.to_csv(f'cnn_{np.mean(accuracy):0.7}.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T17:12:40.121738Z","iopub.execute_input":"2021-12-08T17:12:40.122068Z","iopub.status.idle":"2021-12-08T17:12:42.015623Z","shell.execute_reply.started":"2021-12-08T17:12:40.122026Z","shell.execute_reply":"2021-12-08T17:12:42.01485Z"},"trusted":true},"execution_count":null,"outputs":[]}]}