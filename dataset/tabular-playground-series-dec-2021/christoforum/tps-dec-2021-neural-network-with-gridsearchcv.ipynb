{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Introduction","metadata":{"execution":{"iopub.status.busy":"2021-12-06T23:43:08.771486Z","iopub.execute_input":"2021-12-06T23:43:08.771776Z","iopub.status.idle":"2021-12-06T23:43:08.791588Z","shell.execute_reply.started":"2021-12-06T23:43:08.771705Z","shell.execute_reply":"2021-12-06T23:43:08.790496Z"}}},{"cell_type":"markdown","source":"Previous versions of this notebook present models of neural network, which are diferrent from optimizers and initializers. I had an idea to verify quality of these models by GridSearchCV, but it took very long time, so I decided to verify each model separately. And here are the results:\n\n* **V1** - optimizer: **rmsprop**, initializer: **glorot_uniform** -------> _**0.94749**_\n* **V2** - optimizer: **rmsprop**, initializer: **normal** ----------------> _**0.94635**_\n* **V3** - optimizer: **rmsprop**, initializer: **uniform** ---------------> _**0.94638**_\n* **V4** - optimizer: **adam**,    initializer: **glorot_uniform** ----------> _**0.94871**_\n* **V5** - optimizer: **adam**,    initializer: **normal** -------------------> _**0.94897**_\n* **V6** - optimizer: **adam**,    initializer: **uniform** ------------------> _**0.94903**_ <-best\n \nThere is a code below, which contains data preprocessing. Besides, it enable to search the best neural network model by GridSearchCV tool and other things related to evaluation of model.","metadata":{}},{"cell_type":"markdown","source":"## Import necessary libraries and datasets","metadata":{}},{"cell_type":"code","source":"import math\nimport numpy as np\n\nimport pandas as pd\npd.set_option('display.max_columns', 60)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, BatchNormalization\nfrom tensorflow.keras.optimizers import Adam\nfrom keras.callbacks import EarlyStopping\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.wrappers.scikit_learn import KerasClassifier\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn import metrics\nfrom sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix, classification_report","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-07T00:07:54.093462Z","iopub.execute_input":"2021-12-07T00:07:54.093701Z","iopub.status.idle":"2021-12-07T00:08:00.170001Z","shell.execute_reply.started":"2021-12-07T00:07:54.093638Z","shell.execute_reply":"2021-12-07T00:08:00.169003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"INT8_MIN = np.iinfo(np.int8).min\nINT8_MAX = np.iinfo(np.int8).max\nINT16_MIN = np.iinfo(np.int16).min\nINT16_MAX = np.iinfo(np.int16).max\nINT32_MIN = np.iinfo(np.int32).min\nINT32_MAX = np.iinfo(np.int32).max\n\nFLOAT16_MIN = np.finfo(np.float16).min\nFLOAT16_MAX = np.finfo(np.float16).max\nFLOAT32_MIN = np.finfo(np.float32).min\nFLOAT32_MAX = np.finfo(np.float32).max\n\n\ndef memory_usage(data, detail = 1):\n    if detail:\n        display(data.memory_usage())\n    memory = data.memory_usage().sum() / (1024 * 1024)\n    print(\"Memory usage : {0:.2f}MB\".format(memory))\n    return memory\n\n\ndef compress_dataset(data):\n    memory_before_compress = memory_usage(data, 0)\n    print()\n    print('=' * 50)\n    for col in data.columns:\n        col_dtype = data[col][:100].dtype\n\n        if col_dtype != 'object':\n            print(\"Name: {0:24s} Type: {1}\".format(col, col_dtype))\n            col_series = data[col]\n            col_min = col_series.min()\n            col_max = col_series.max()\n\n            if col_dtype == 'float64':\n                print(\" variable min: {0:15s} max: {1:15s}\".format(str(np.round(col_min, 4)), str(np.round(col_max, 4))))\n                if (col_min > FLOAT16_MIN) and (col_max < FLOAT16_MAX):\n                    data[col] = data[col].astype(np.float16)\n                    print(\"  float16 min: {0:15s} max: {1:15s}\".format(str(FLOAT16_MIN), str(FLOAT16_MAX)))\n                    print(\"compress float64 --> float16\")\n                elif (col_min > FLOAT32_MIN) and (col_max < FLOAT32_MAX):\n                    data[col] = data[col].astype(np.float32)\n                    print(\"  float32 min: {0:15s} max: {1:15s}\".format(str(FLOAT32_MIN), str(FLOAT32_MAX)))\n                    print(\"compress float64 --> float32\")\n                else:\n                    pass\n                memory_after_compress = memory_usage(data, 0)\n                print(\"Compress Rate: [{0:.2%}]\".format((memory_before_compress-memory_after_compress) / memory_before_compress))\n                print('=' * 50)\n\n            if col_dtype == 'int64':\n                print(\" variable min: {0:15s} max: {1:15s}\".format(str(col_min), str(col_max)))\n                type_flag = 64\n                if (col_min > INT8_MIN / 2) and (col_max < INT8_MAX / 2):\n                    type_flag = 8\n                    data[col] = data[col].astype(np.int8)\n                    print(\"     int8 min: {0:15s} max: {1:15s}\".format(str(INT8_MIN), str(INT8_MAX)))\n                elif (col_min > INT16_MIN) and (col_max < INT16_MAX):\n                    type_flag = 16\n                    data[col] = data[col].astype(np.int16)\n                    print(\"    int16 min: {0:15s} max: {1:15s}\".format(str(INT16_MIN), str(INT16_MAX)))\n                elif (col_min > INT32_MIN) and (col_max < INT32_MAX):\n                    type_flag = 32\n                    data[col] = data[col].astype(np.int32)\n                    print(\"    int32 min: {0:15s} max: {1:15s}\".format(str(INT32_MIN), str(INT32_MAX)))\n                    type_flag = 1\n                else:\n                    pass\n                memory_after_compress = memory_usage(data, 0)\n                print(\"Compress Rate: [{0:.2%}]\".format((memory_before_compress-memory_after_compress) / memory_before_compress))\n                if type_flag == 32:\n                    print(\"compress (int64) ==> (int32)\")\n                elif type_flag == 16:\n                    print(\"compress (int64) ==> (int16)\")\n                else:\n                    print(\"compress (int64) ==> (int8)\")\n                print('=' * 50)\n\n    print()\n    memory_after_compress = memory_usage(data, 0)\n    print(\"Compress Rate: [{0:.2%}]\".format((memory_before_compress-memory_after_compress) / memory_before_compress))\n    \n    return data","metadata":{"execution":{"iopub.status.busy":"2021-12-07T00:08:00.170978Z","iopub.execute_input":"2021-12-07T00:08:00.171163Z","iopub.status.idle":"2021-12-07T00:08:00.188627Z","shell.execute_reply.started":"2021-12-07T00:08:00.17114Z","shell.execute_reply":"2021-12-07T00:08:00.187987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv('../input/tabular-playground-series-dec-2021/train.csv')\ndf_test = pd.read_csv('../input/tabular-playground-series-dec-2021/test.csv')","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-12-07T00:08:00.191247Z","iopub.execute_input":"2021-12-07T00:08:00.191735Z","iopub.status.idle":"2021-12-07T00:08:17.719836Z","shell.execute_reply.started":"2021-12-07T00:08:00.191711Z","shell.execute_reply":"2021-12-07T00:08:17.719392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train set summary","metadata":{}},{"cell_type":"markdown","source":"Let's see what a train set looks like","metadata":{}},{"cell_type":"code","source":"df_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-07T00:08:17.721634Z","iopub.execute_input":"2021-12-07T00:08:17.722604Z","iopub.status.idle":"2021-12-07T00:08:17.755853Z","shell.execute_reply.started":"2021-12-07T00:08:17.722544Z","shell.execute_reply":"2021-12-07T00:08:17.75488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`Id` column **is redundant**. Let's remove it.","metadata":{}},{"cell_type":"code","source":"df_train.drop('Id', axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-12-07T00:08:17.756998Z","iopub.execute_input":"2021-12-07T00:08:17.757235Z","iopub.status.idle":"2021-12-07T00:08:18.000637Z","shell.execute_reply.started":"2021-12-07T00:08:17.757204Z","shell.execute_reply":"2021-12-07T00:08:17.999755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check how big is our data.","metadata":{}},{"cell_type":"code","source":"print(f'Train set shape:   {df_train.shape}')","metadata":{"execution":{"iopub.status.busy":"2021-12-07T00:08:18.002213Z","iopub.execute_input":"2021-12-07T00:08:18.002645Z","iopub.status.idle":"2021-12-07T00:08:18.017958Z","shell.execute_reply.started":"2021-12-07T00:08:18.002611Z","shell.execute_reply":"2021-12-07T00:08:18.017115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our train set has **4 000 000 rows** and **55 columns**.","metadata":{}},{"cell_type":"markdown","source":"Let's find out something more about data. ","metadata":{}},{"cell_type":"code","source":"df_train.info()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-12-07T00:08:18.019353Z","iopub.execute_input":"2021-12-07T00:08:18.019581Z","iopub.status.idle":"2021-12-07T00:08:18.046044Z","shell.execute_reply.started":"2021-12-07T00:08:18.019551Z","shell.execute_reply":"2021-12-07T00:08:18.045157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All columns consist of **integers** and the set is huge - it using a lot of memory.","metadata":{}},{"cell_type":"markdown","source":"Let's see a distribution of each column.","metadata":{}},{"cell_type":"code","source":"df_train.describe()","metadata":{"execution":{"iopub.status.busy":"2021-12-07T00:08:18.047056Z","iopub.execute_input":"2021-12-07T00:08:18.04724Z","iopub.status.idle":"2021-12-07T00:08:21.519481Z","shell.execute_reply.started":"2021-12-07T00:08:18.047218Z","shell.execute_reply":"2021-12-07T00:08:21.518752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Columns `Soil_Type7` and `Soil_Type15` contain only **one value - 0**. They don't introduce a variability, so we can remove them. All remaining columns`Soil_Type` has **two values - 0 and 1**.","metadata":{}},{"cell_type":"code","source":"df_train.drop(['Soil_Type7', 'Soil_Type15'], axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-12-07T00:08:21.520288Z","iopub.execute_input":"2021-12-07T00:08:21.520438Z","iopub.status.idle":"2021-12-07T00:08:21.75404Z","shell.execute_reply.started":"2021-12-07T00:08:21.520417Z","shell.execute_reply":"2021-12-07T00:08:21.753517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"At the end let's check dataset has some missing values.","metadata":{}},{"cell_type":"code","source":"df_train.isnull().sum().max() != 0","metadata":{"execution":{"iopub.status.busy":"2021-12-07T00:08:21.754956Z","iopub.execute_input":"2021-12-07T00:08:21.755739Z","iopub.status.idle":"2021-12-07T00:08:21.937707Z","shell.execute_reply.started":"2021-12-07T00:08:21.755709Z","shell.execute_reply":"2021-12-07T00:08:21.93707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are **no missing values** in train dataset.","metadata":{}},{"cell_type":"markdown","source":"## Test set summary","metadata":{}},{"cell_type":"markdown","source":"We'll carry out exactly the same steps as above.","metadata":{}},{"cell_type":"markdown","source":"Let's see what a test set looks like","metadata":{}},{"cell_type":"code","source":"df_test.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-07T00:08:21.938549Z","iopub.execute_input":"2021-12-07T00:08:21.939382Z","iopub.status.idle":"2021-12-07T00:08:21.969854Z","shell.execute_reply.started":"2021-12-07T00:08:21.93935Z","shell.execute_reply":"2021-12-07T00:08:21.969334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`Id` column **is redundant**. Let's remove it.","metadata":{}},{"cell_type":"code","source":"df_test.drop('Id', axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-12-07T00:08:21.970706Z","iopub.execute_input":"2021-12-07T00:08:21.97161Z","iopub.status.idle":"2021-12-07T00:08:22.034215Z","shell.execute_reply.started":"2021-12-07T00:08:21.971582Z","shell.execute_reply":"2021-12-07T00:08:22.033613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check how big is our data.","metadata":{}},{"cell_type":"code","source":"print(f'Test set shape:   {df_test.shape}')","metadata":{"execution":{"iopub.status.busy":"2021-12-07T00:08:22.038071Z","iopub.execute_input":"2021-12-07T00:08:22.038291Z","iopub.status.idle":"2021-12-07T00:08:22.043419Z","shell.execute_reply.started":"2021-12-07T00:08:22.038266Z","shell.execute_reply":"2021-12-07T00:08:22.042766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our test set has **1 000 000 rows** and **54 columns**.","metadata":{}},{"cell_type":"markdown","source":"Let's find out something more about data. ","metadata":{}},{"cell_type":"code","source":"df_test.info()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-12-07T00:08:22.044321Z","iopub.execute_input":"2021-12-07T00:08:22.044924Z","iopub.status.idle":"2021-12-07T00:08:22.111036Z","shell.execute_reply.started":"2021-12-07T00:08:22.044893Z","shell.execute_reply":"2021-12-07T00:08:22.110342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All columns consist of **integers** and the set is huge - it using a lot of memory. Just like in train set.","metadata":{}},{"cell_type":"markdown","source":"Let's see a distribution of each column.","metadata":{}},{"cell_type":"code","source":"df_test.describe()","metadata":{"execution":{"iopub.status.busy":"2021-12-07T00:08:22.112568Z","iopub.execute_input":"2021-12-07T00:08:22.112828Z","iopub.status.idle":"2021-12-07T00:08:23.102917Z","shell.execute_reply.started":"2021-12-07T00:08:22.112789Z","shell.execute_reply":"2021-12-07T00:08:23.102252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Columns `Soil_Type7` and `Soil_Type15` contain only **one value - 0**. They don't introduce a variability, so we can remove them. All remaining columns`Soil_Type` has **two values - 0 and 1**. Just like in train set.","metadata":{}},{"cell_type":"code","source":"df_test.drop(['Soil_Type7', 'Soil_Type15'], axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-12-07T00:08:23.104318Z","iopub.execute_input":"2021-12-07T00:08:23.104534Z","iopub.status.idle":"2021-12-07T00:08:23.169149Z","shell.execute_reply.started":"2021-12-07T00:08:23.104506Z","shell.execute_reply":"2021-12-07T00:08:23.168712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"At the end let's check dataset has some missing values.","metadata":{}},{"cell_type":"code","source":"df_test.isnull().sum().max() != 0","metadata":{"execution":{"iopub.status.busy":"2021-12-07T00:08:23.169864Z","iopub.execute_input":"2021-12-07T00:08:23.170592Z","iopub.status.idle":"2021-12-07T00:08:23.227069Z","shell.execute_reply.started":"2021-12-07T00:08:23.170568Z","shell.execute_reply":"2021-12-07T00:08:23.226282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are **no missing values** in test dataset.","metadata":{}},{"cell_type":"markdown","source":"## Target summary","metadata":{}},{"cell_type":"markdown","source":"Let's check a number of classes in target column.","metadata":{}},{"cell_type":"code","source":"df_train['Cover_Type'].unique()","metadata":{"execution":{"iopub.status.busy":"2021-12-07T00:08:23.227979Z","iopub.execute_input":"2021-12-07T00:08:23.228212Z","iopub.status.idle":"2021-12-07T00:08:23.253849Z","shell.execute_reply.started":"2021-12-07T00:08:23.22818Z","shell.execute_reply":"2021-12-07T00:08:23.253252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have **7** classes. We need to check that classes are balanced or not.","metadata":{}},{"cell_type":"code","source":"sns.countplot(x = df_train['Cover_Type'])","metadata":{"execution":{"iopub.status.busy":"2021-12-07T00:08:23.254901Z","iopub.execute_input":"2021-12-07T00:08:23.255065Z","iopub.status.idle":"2021-12-07T00:08:23.612214Z","shell.execute_reply.started":"2021-12-07T00:08:23.255043Z","shell.execute_reply":"2021-12-07T00:08:23.611722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['Cover_Type'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-12-07T00:08:23.613297Z","iopub.execute_input":"2021-12-07T00:08:23.613645Z","iopub.status.idle":"2021-12-07T00:08:23.647374Z","shell.execute_reply.started":"2021-12-07T00:08:23.613615Z","shell.execute_reply":"2021-12-07T00:08:23.646576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Unfortunatelly, **classes are imbalanced**. Class no. 5 appears only once. We'll remove it.","metadata":{}},{"cell_type":"code","source":"df_train.drop(df_train[df_train['Cover_Type'] == 5].index, axis = 0, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-12-07T00:08:23.64858Z","iopub.execute_input":"2021-12-07T00:08:23.648781Z","iopub.status.idle":"2021-12-07T00:08:24.471527Z","shell.execute_reply.started":"2021-12-07T00:08:23.648732Z","shell.execute_reply":"2021-12-07T00:08:24.470613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Correlation","metadata":{}},{"cell_type":"code","source":"non_binary_columns = list(df_train.columns[:10])\nsns.heatmap(df_train[non_binary_columns].corr())","metadata":{"execution":{"iopub.status.busy":"2021-12-07T00:08:24.472927Z","iopub.execute_input":"2021-12-07T00:08:24.473086Z","iopub.status.idle":"2021-12-07T00:08:25.726582Z","shell.execute_reply.started":"2021-12-07T00:08:24.473065Z","shell.execute_reply":"2021-12-07T00:08:25.72613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is **no correlation between non-binary feature**.","metadata":{}},{"cell_type":"markdown","source":"## Standard Scaler","metadata":{}},{"cell_type":"markdown","source":"Let's scale and match our datasets. ","metadata":{}},{"cell_type":"code","source":"columns = list(df_test.columns)\n\nscaler = StandardScaler()\n\ndf_train[columns] = scaler.fit_transform(df_train[columns])\ndf_test = pd.DataFrame(scaler.transform(df_test), columns = df_test.columns)","metadata":{"execution":{"iopub.status.busy":"2021-12-07T00:08:25.727468Z","iopub.execute_input":"2021-12-07T00:08:25.728182Z","iopub.status.idle":"2021-12-07T00:08:35.023296Z","shell.execute_reply.started":"2021-12-07T00:08:25.728154Z","shell.execute_reply":"2021-12-07T00:08:35.022317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Memory releasing","metadata":{}},{"cell_type":"markdown","source":"Datasets are very large and use huge quantity of memory, so we need to convert type of columns to ones using less memory.","metadata":{}},{"cell_type":"code","source":"df_train = compress_dataset(df_train)\ndf_test = compress_dataset(df_test)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-12-07T00:08:35.024306Z","iopub.execute_input":"2021-12-07T00:08:35.024463Z","iopub.status.idle":"2021-12-07T00:08:39.910979Z","shell.execute_reply.started":"2021-12-07T00:08:35.024443Z","shell.execute_reply":"2021-12-07T00:08:39.910299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CNN","metadata":{}},{"cell_type":"markdown","source":"Let's define our features and target.","metadata":{}},{"cell_type":"code","source":"X = df_train[columns]\ny = df_train['Cover_Type']","metadata":{"execution":{"iopub.status.busy":"2021-12-07T00:08:39.911826Z","iopub.execute_input":"2021-12-07T00:08:39.911985Z","iopub.status.idle":"2021-12-07T00:08:40.272441Z","shell.execute_reply.started":"2021-12-07T00:08:39.911964Z","shell.execute_reply":"2021-12-07T00:08:40.271896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We need to modify target column by formatting vector (number of rows, 1) to matrix **(number of rows, number of classes)**. Each column of new target matrix corresponds to one class, so each row consists of 0s (which means no class) and 1 (specific class) ","metadata":{}},{"cell_type":"code","source":"y = pd.get_dummies(y)\ny.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-07T00:08:40.27321Z","iopub.execute_input":"2021-12-07T00:08:40.27336Z","iopub.status.idle":"2021-12-07T00:08:40.347786Z","shell.execute_reply.started":"2021-12-07T00:08:40.273339Z","shell.execute_reply":"2021-12-07T00:08:40.347102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Split data into train set and test set.","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size = 0.2, random_state = 12)\nprint(f'X train shape:  {X_train.shape}')\nprint(f'X test shape:   {X_test.shape}')\nprint(f'y train shape:  {y_train.shape}')\nprint(f'y test shape:   {y_test.shape}')","metadata":{"execution":{"iopub.status.busy":"2021-12-07T00:08:40.348625Z","iopub.execute_input":"2021-12-07T00:08:40.348764Z","iopub.status.idle":"2021-12-07T00:09:18.023991Z","shell.execute_reply.started":"2021-12-07T00:08:40.348744Z","shell.execute_reply":"2021-12-07T00:09:18.022439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's build neural network model. We use GarchSearchCV tool to select the best optimizer and initializer.","metadata":{}},{"cell_type":"code","source":"def model_cnn(optimizer, initializer):\n    model = Sequential()\n    model.add(Dense(128, activation = 'relu', kernel_initializer = initializer, input_shape = [X.shape[1]]))\n    model.add(Dropout(0.3))\n    model.add(BatchNormalization())\n    model.add(Dense(64, activation = 'relu', kernel_initializer = initializer))\n    model.add(Dropout(0.3))\n    model.add(BatchNormalization())\n    model.add(Dense(32, activation = 'relu', kernel_initializer = initializer))\n    model.add(Dropout(0.3))\n    model.add(BatchNormalization())\n    model.add(Dense(16, activation = 'relu', kernel_initializer = initializer))\n    model.add(Dropout(0.3))\n    model.add(BatchNormalization())\n    model.add(Dense(8, activation = 'relu', kernel_initializer = initializer))\n    model.add(Dropout(0.3))\n    model.add(BatchNormalization())\n    model.add(Dense(6, activation = 'softmax', kernel_initializer = initializer))\n\n    model.compile(optimizer = optimizer, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n\n    return model\n\nearly_stop = EarlyStopping(monitor = 'val_accuracy', patience = 10, \n                           verbose = 1, mode = 'max', restore_best_weights = True)\nred_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.5, patience = 5, verbose = 1)","metadata":{"execution":{"iopub.status.busy":"2021-12-07T00:09:18.025989Z","iopub.execute_input":"2021-12-07T00:09:18.026284Z","iopub.status.idle":"2021-12-07T00:09:18.037569Z","shell.execute_reply.started":"2021-12-07T00:09:18.026248Z","shell.execute_reply":"2021-12-07T00:09:18.037004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = KerasClassifier(build_fn = model_cnn, epochs = 100, batch_size = 1024, \n                        verbose = 1, callbacks = [early_stop, red_lr], validation_data = (X_test, y_test))","metadata":{"execution":{"iopub.status.busy":"2021-12-07T00:09:18.03864Z","iopub.execute_input":"2021-12-07T00:09:18.039272Z","iopub.status.idle":"2021-12-07T00:09:18.057112Z","shell.execute_reply.started":"2021-12-07T00:09:18.039243Z","shell.execute_reply":"2021-12-07T00:09:18.056664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params_grid = dict(optimizer = ['adam', 'rmsprop'],\n                   initializer = ['uniform', 'glorot_uniform', 'normal'])\n\ngrid_model = GridSearchCV(estimator = model, param_grid = params_grid, \n                          cv = 10, verbose = 0)\n# grid_model.fit(X_train, y_train)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-12-07T00:09:18.058223Z","iopub.execute_input":"2021-12-07T00:09:18.058932Z","iopub.status.idle":"2021-12-07T00:09:18.070895Z","shell.execute_reply.started":"2021-12-07T00:09:18.058899Z","shell.execute_reply":"2021-12-07T00:09:18.070254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Results of models.","metadata":{}},{"cell_type":"code","source":"# pd.DataFrame(grid_model.cv_results_)[['param_initializer', 'param_optimizer', 'mean_test_score', 'std_test_score', 'rank_test_score']].sort_values('rank_test_score')","metadata":{"execution":{"iopub.status.busy":"2021-12-07T00:09:18.071967Z","iopub.execute_input":"2021-12-07T00:09:18.072192Z","iopub.status.idle":"2021-12-07T00:09:18.083192Z","shell.execute_reply.started":"2021-12-07T00:09:18.072164Z","shell.execute_reply":"2021-12-07T00:09:18.08271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(f'Best: {grid_model.best_score_} using {grid_model.best_params_}')","metadata":{"execution":{"iopub.status.busy":"2021-12-07T00:09:18.084225Z","iopub.execute_input":"2021-12-07T00:09:18.084547Z","iopub.status.idle":"2021-12-07T00:09:18.09657Z","shell.execute_reply.started":"2021-12-07T00:09:18.084518Z","shell.execute_reply":"2021-12-07T00:09:18.096084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Time to predict classes for test data. We need to modify predicted values in following way:\n* 0 -> 1\n* 1 -> 2\n* 2 -> 3\n* 3 -> 4\n* 4 -> 6\n* 5 -> 7\n\nbecause indices of columns don't correspond to values of class.","metadata":{}},{"cell_type":"code","source":"# best_model = grid_model.best_estimator_\n\n# preds = best_model.predict(X_test)\n# preds += 1\n# preds[preds > 4] += 1\n# preds","metadata":{"execution":{"iopub.status.busy":"2021-12-07T00:09:18.097472Z","iopub.execute_input":"2021-12-07T00:09:18.097791Z","iopub.status.idle":"2021-12-07T00:09:18.107955Z","shell.execute_reply.started":"2021-12-07T00:09:18.09776Z","shell.execute_reply":"2021-12-07T00:09:18.107511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We need to extract informations about the values of class from target matrix. We can do it in two steps. First, we take indices of these columns, where no. 1 appears in each rows and then we modify them exactly the same way as before.","metadata":{}},{"cell_type":"code","source":"# y_true = np.argmax(np.array(y_test), axis = 1)\n# y_true += 1\n# y_true[y_true > 4] += 1\n# y_true","metadata":{"execution":{"iopub.status.busy":"2021-12-07T00:09:18.108806Z","iopub.execute_input":"2021-12-07T00:09:18.109635Z","iopub.status.idle":"2021-12-07T00:09:18.123197Z","shell.execute_reply.started":"2021-12-07T00:09:18.109595Z","shell.execute_reply":"2021-12-07T00:09:18.122408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we can evaluate our model.","metadata":{}},{"cell_type":"markdown","source":"## Evaluation","metadata":{}},{"cell_type":"code","source":"# print(classification_report(y_true, preds))","metadata":{"execution":{"iopub.status.busy":"2021-12-07T00:09:18.124341Z","iopub.execute_input":"2021-12-07T00:09:18.124609Z","iopub.status.idle":"2021-12-07T00:09:18.135702Z","shell.execute_reply.started":"2021-12-07T00:09:18.124573Z","shell.execute_reply":"2021-12-07T00:09:18.135087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cm = confusion_matrix(y_true, preds)\n# fig, ax = plt.subplots(figsize = (10,10))\n# cmd = ConfusionMatrixDisplay(cm, display_labels = pd.DataFrame(y_true)[0].sort_values().unique())\n# cmd.plot(cmap = plt.cm.Blues, ax = ax)\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-07T00:09:18.136426Z","iopub.execute_input":"2021-12-07T00:09:18.136615Z","iopub.status.idle":"2021-12-07T00:09:18.149122Z","shell.execute_reply.started":"2021-12-07T00:09:18.136586Z","shell.execute_reply":"2021-12-07T00:09:18.14855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sns.countplot(x = preds)\n# plt.grid()","metadata":{"execution":{"iopub.status.busy":"2021-12-07T00:09:18.1502Z","iopub.execute_input":"2021-12-07T00:09:18.150944Z","iopub.status.idle":"2021-12-07T00:09:18.162642Z","shell.execute_reply.started":"2021-12-07T00:09:18.150909Z","shell.execute_reply":"2021-12-07T00:09:18.162004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = best_model.fit(X_train, y_train, validation_data = (X_test, y_test), \n#                   verbose = 1, callbacks = [early_stop, red_lr])","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-12-07T00:09:18.163884Z","iopub.execute_input":"2021-12-07T00:09:18.164454Z","iopub.status.idle":"2021-12-07T00:09:18.174562Z","shell.execute_reply.started":"2021-12-07T00:09:18.164421Z","shell.execute_reply":"2021-12-07T00:09:18.17383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.figure(figsize = (15, 5))\n\n# plt.plot(model.history['accuracy'])\n# plt.plot(model.history['val_accuracy'])\n\n# plt.title('Model Accuracy', size = 16)\n# plt.xlabel('Epoch')\n# plt.legend(['Train accuracy', 'Test accuracy'], loc = 4)\n# plt.grid()\n# plt.show()\n           \n# plt.figure(figsize = (15, 5))\n\n# plt.plot(model.history['loss'])\n# plt.plot(model.history['val_loss'])\n\n# plt.title('Model loss', size = 16)\n# plt.xlabel('Epoch')\n# plt.legend(['Train loss', 'Test loss'], loc = 7)\n# plt.grid()\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-07T00:09:18.175608Z","iopub.execute_input":"2021-12-07T00:09:18.175802Z","iopub.status.idle":"2021-12-07T00:09:18.188429Z","shell.execute_reply.started":"2021-12-07T00:09:18.175775Z","shell.execute_reply":"2021-12-07T00:09:18.187561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission","metadata":{}},{"cell_type":"code","source":"# sub = pd.read_csv('../input/tabular-playground-series-dec-2021/sample_submission.csv')\n# preds = best_model.predict(df_test)\n# preds += 1\n# preds[preds > 4] += 1\n# sub['Cover_Type'] = preds\n# sub.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-07T00:09:18.192412Z","iopub.execute_input":"2021-12-07T00:09:18.192911Z","iopub.status.idle":"2021-12-07T00:09:18.200614Z","shell.execute_reply.started":"2021-12-07T00:09:18.192885Z","shell.execute_reply":"2021-12-07T00:09:18.200157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sub.to_csv('gs_best.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2021-12-07T00:09:18.201633Z","iopub.execute_input":"2021-12-07T00:09:18.202531Z","iopub.status.idle":"2021-12-07T00:09:18.213178Z","shell.execute_reply.started":"2021-12-07T00:09:18.202493Z","shell.execute_reply":"2021-12-07T00:09:18.21234Z"},"trusted":true},"execution_count":null,"outputs":[]}]}