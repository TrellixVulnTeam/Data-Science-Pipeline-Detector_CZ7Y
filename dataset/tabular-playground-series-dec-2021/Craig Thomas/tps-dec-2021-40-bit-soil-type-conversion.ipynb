{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\nThis kernel is a bit of an exploration to see if we can reduce our dimensionality in a way that tree-based ML algorithms can make use of. In particular, this was inspired by the kernel [TPS122021 Exploiting Sparsity for XGBoost](https://www.kaggle.com/siukeitin/tps122021-exploiting-sparsity-for-xgboost) that was discussed at [https://www.kaggle.com/c/tabular-playground-series-dec-2021/discussion/294808](https://www.kaggle.com/c/tabular-playground-series-dec-2021/discussion/294808). \n\nIn this particular kernel, we'll make use of the fact that the `Soil_Type` fields are binary valued. Because of this fact, we can string all of them together and construct a 40-bit integer representation of the `Soil_Type` field. To explain with a simple example, assume for a moment that we have only 8 `Soil_Type` fields:\n\n```\nSoil_Type1, Soil_Type2, Soil_Type3, Soil_Type4, Soil_Type5, Soil_Type6, Soil_Type7, Soil_Type8\n```\n\nAll of the values are either `0` or `1` like so:\n\n```\n0, 0, 0, 0, 0, 0, 1, 1\n```\n\nIf we concatenate them together, and stuff them into an integer, we would have a value that ranges from 0 to 255. With the example above we would have:\n\n```\nbinary     decimal\n00000011 = 3\n```\n\nWe can do the same thing with all of the `Soil_Type` features, and generate 40-bit integer values. The benefit here is that we would retain all of the original information within the `Soil_Type` bits, but collapse a 40-dimensional space down to 1. Let's see how this looks.","metadata":{}},{"cell_type":"markdown","source":"# Convert To 40-Bit Integer\n\n### Setup Dataframes","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade scikit-learn","metadata":{"_kg_hide-input":true,"_kg_hide-output":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport gc\n\ntrain = pd.read_csv(\"../input/tabular-playground-series-dec-2021/train.csv\")\ntest = pd.read_csv(\"../input/tabular-playground-series-dec-2021/test.csv\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### New Column Definition\n\nHere we'll generate a new field for the `soiltype_label`, and store it in a new column.","metadata":{}},{"cell_type":"code","source":"train[\"soiltype_label\"] = 0\ntest[\"soiltype_label\"] = 0\n\ntrain[\"soiltype_label\"] = train[\"soiltype_label\"].astype(np.int64)\ntest[\"soiltype_label\"] = test[\"soiltype_label\"].astype(np.int64)\n\nsoil_columns = [x for x in train.columns if x.startswith(\"Soil_Type\")]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Row Transformer\n\nThe function below will take a single row of data, and transform the `Soil_Type` fields into a 40-bit integer.","metadata":{}},{"cell_type":"code","source":"def make_40_bit_int_from_soiltype(row):\n    value = 0\n    for column in soil_columns:\n        value |= row[column]\n        value = value << 1\n    return value","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Apply Transformation\n\nWith the function above, all we have to do is `apply` it to our dataframe.","metadata":{}},{"cell_type":"code","source":"train[\"soiltype_label\"] = train.apply(make_40_bit_int_from_soiltype, axis=1)\nprint(\": Number of unique labels: {:,d}\".format(train[\"soiltype_label\"].nunique()))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Comparing Information Representation Methods\n\nNow that we have a single label representing all of the `Soil_Type` features, we should check to make sure we aren't losing any performance from the new features. First, we'll generate a LightGBM model with all of the `Soil_Type` features as they originally appeared.","metadata":{}},{"cell_type":"code","source":"# Drop Cover_Type 5, since we only have one example of it\ntrain = train[(train[\"Cover_Type\"] != 5)]\n\n# Retain all of our features\nfeatures = [\n    'Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology',\n    'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', \n    'Horizontal_Distance_To_Fire_Points', 'Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3',\n    'Wilderness_Area4', 'Soil_Type1', 'Soil_Type2', 'Soil_Type3', 'Soil_Type4', 'Soil_Type5', \n    'Soil_Type6', 'Soil_Type7', 'Soil_Type8', 'Soil_Type9', 'Soil_Type10', 'Soil_Type11', 'Soil_Type12',\n    'Soil_Type13', 'Soil_Type14', 'Soil_Type15', 'Soil_Type16', 'Soil_Type17', 'Soil_Type18', \n    'Soil_Type19', 'Soil_Type20', 'Soil_Type21', 'Soil_Type22', 'Soil_Type23', 'Soil_Type24',\n    'Soil_Type25', 'Soil_Type26', 'Soil_Type27', 'Soil_Type28', 'Soil_Type29', 'Soil_Type30', \n    'Soil_Type31', 'Soil_Type32', 'Soil_Type33', 'Soil_Type34', 'Soil_Type35', 'Soil_Type36',\n    'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'Soil_Type40']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import classification_report\nfrom lightgbm import LGBMClassifier\nfrom lightgbm import early_stopping\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\n\ntarget = train[\"Cover_Type\"]\ncv_rounds = 3\n\nk_fold = StratifiedKFold(\n    n_splits=cv_rounds,\n    random_state=2021,\n    shuffle=True,\n)\n\ntrain_preds = np.zeros(len(train.index), )\ntrain_probas = np.zeros(len(train.index), )\n\nfor fold, (train_index, test_index) in enumerate(k_fold.split(train[features], target)):\n    x_train = train[features].iloc[train_index]\n    y_train = target.iloc[train_index]\n\n    x_valid = train[features].iloc[test_index]\n    y_valid = target.iloc[test_index]\n\n    model = LGBMClassifier(\n        random_state=2021,\n        n_estimators=2000,\n        verbose=-1,\n        metric=\"softmax\",\n    )\n    model.fit(\n        x_train,\n        y_train,\n        eval_set=[(x_valid, y_valid)],\n        callbacks=[early_stopping(50, verbose=False)],\n    )\n\n    train_oof_preds = model.predict(x_valid)\n    train_preds[test_index] = train_oof_preds\n    \n    print(\"-- Fold {}:\".format(fold+1))\n    print(\"{}\".format(classification_report(y_valid, train_oof_preds)))\n    print(\"-- Accuracy: {}\".format(accuracy_score(y_valid, train_oof_preds)))\n\nprint(\"-- Overall:\")\nprint(\"{}\".format(classification_report(target, train_preds)))\nprint(\"-- Accuracy: {}\".format(accuracy_score(target, train_preds)))\n\ntrain[\"unmodified_preds\"] = train_preds\n\n# Show the confusion matrix\nconfusion = confusion_matrix(train[\"Cover_Type\"], train[\"unmodified_preds\"])\ncover_labels = [1, 2, 3, 4, 6, 7]\nfig, ax = plt.subplots(figsize=(15, 15))\nax = sns.heatmap(confusion, annot=True, fmt=\",d\", xticklabels=cover_labels, yticklabels=cover_labels)\n_ = ax.set_title(\"Confusion Matrix for LGB Classifier (Unmodified Dataset)\", fontsize=15)\n_ = ax.set_ylabel(\"Actual Class\")\n_ = ax.set_xlabel(\"Predicted Class\")\n\ndel(train_preds)\ndel(confusion)\n_ = gc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's see what happens if we drop all of those `Soil_Type` columns and focus on using our `soiltype_label` instead. We'll label encode it first however, since huge values of 64-bit integers are going to give LightGBM a problem.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nencoder = LabelEncoder()\ntrain[\"soiltype_label_encoded\"] = encoder.fit_transform(train[\"soiltype_label\"])\ntrain[\"soiltype_label_encoded\"] = train[\"soiltype_label_encoded\"].astype(np.int16)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntarget = train[\"Cover_Type\"]\ncv_rounds = 3\n\nk_fold = StratifiedKFold(\n    n_splits=cv_rounds,\n    random_state=2021,\n    shuffle=True,\n)\n\nfeatures = [x for x in features if not x.startswith(\"Soil_Type\")]\nfeatures.insert(0, \"soiltype_label_encoded\")\n\ntrain_preds = np.zeros(len(train.index), )\ntrain_probas = np.zeros(len(train.index), )\n\nfor fold, (train_index, test_index) in enumerate(k_fold.split(train[features], target)):\n    x_train = train[features].iloc[train_index]\n    y_train = target.iloc[train_index]\n\n    x_valid = train[features].iloc[test_index]\n    y_valid = target.iloc[test_index]\n\n    model = LGBMClassifier(\n        random_state=2021,\n        n_estimators=2000,\n        verbose=-1,\n        metric=\"softmax\",\n        cat_feature=[0],\n    )\n    model.fit(\n        x_train,\n        y_train,\n        eval_set=[(x_valid, y_valid)],\n        callbacks=[early_stopping(50, verbose=False)],\n    )\n\n    train_oof_preds = model.predict(x_valid)\n    train_preds[test_index] = train_oof_preds\n    \n    print(\"-- Fold {}:\".format(fold+1))\n    print(\"{}\".format(classification_report(y_valid, train_oof_preds)))\n    print(\"-- Accuracy: {}\".format(accuracy_score(y_valid, train_oof_preds)))\n\nprint(\"-- Overall:\")\nprint(\"{}\".format(classification_report(target, train_preds)))\nprint(\"-- Accuracy: {}\".format(accuracy_score(target, train_preds)))\n\ntrain[\"collapsed_preds\"] = train_preds\n\n# Show the confusion matrix\nconfusion = confusion_matrix(train[\"Cover_Type\"], train[\"collapsed_preds\"])\ncover_labels = [1, 2, 3, 4, 6, 7]\nfig, ax = plt.subplots(figsize=(15, 15))\nax = sns.heatmap(confusion, annot=True, fmt=\",d\", xticklabels=cover_labels, yticklabels=cover_labels)\n_ = ax.set_title(\"Confusion Matrix for LGB Classifier (Collapsed Dataset)\", fontsize=15)\n_ = ax.set_ylabel(\"Actual Class\")\n_ = ax.set_xlabel(\"Predicted Class\")\n\ndel(train_preds)\ndel(confusion)\n_ = gc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can compare the two runs to see what happened:","metadata":{}},{"cell_type":"code","source":"bar, ax = plt.subplots(figsize=(20, 10))\nax = sns.barplot(\n    x=[\"Unmodified\", \"40-bit Integer\"],\n    y=[\n        float(accuracy_score(target, train[\"unmodified_preds\"])),\n        accuracy_score(target, train[\"collapsed_preds\"]),\n    ],\n)\n_ = ax.set_title(\"Accuracy Score Based on Approach\", fontsize=15)\n_ = ax.set_xlabel(\"Approach\")\n_ = ax.set_ylabel(\"Accuracy Score\")\n_ = ax.set(ylim=(0.90, 1.0))\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(\n        x=p.get_x()+(p.get_width()/2),\n        y=height,\n        s=\"{:.4f}\".format(height),\n        ha=\"center\"\n    )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A general observation can be made here:\n\n1. The performance of the 40-bit integer representation is better, again probably due to dimensionality reduction. We see accuracy improvements in `Cover_Type` classes of `4`, `6`, and `7`.\n\nThe question is however, is can we apply the same integer encoding to the test set and expect to see the same performance?","metadata":{}},{"cell_type":"markdown","source":"# Train / Test Differences\n\nThe first thing to do is check to see if the set of integer representations between the two sets overlap one another. The motivation behind this check is that certain combinations of binary features may not occur in the training set, but occur in the testing set. This is problematic, since the integer encoding will result in two different numeric values. For example, assume again that we have only 8 `Soil_Type` features. In the training set and testing set, let's assume that we have only the following two rows of observations:\n\n| Data Set | Type 1 | Type 2 | Type 3 | Type 4 | Type 5 | Type 6 | Type 7 | Type 8 | Encoded |\n| :------- | :----: | :----: | :----: | :----: | :----: | :----: | :----: | :----: | :-----: |\n| Training | 0      | 0      | 0      | 0      | 0      | 0      | 1      | 1      | 3       |\n| Testing  | 0      | 0      | 0      | 0      | 0      | 0      | 0      | 1      | 1       |\n\nThe problem is that while both the training set and testing set have `Soil_Type8` of `1`, the training set has `Soil_Type7` as `1`. As we can see, the overall encoded values differ because of that. This results in a loss of information - specifically, the loss of `Soil_Type8` being the same between the datasets. Even worse, if we attempt to label encode the two sets, the default SciKit Learn `LabelEncoder` will throw errors that the testing set contains values that were unseen in the original `fit` from the training set (and rightly so, since `LabelEncoder` is only meant to be used to transform `Y` variables, not features). \n\nWe can however, use `OrdinalEncoder` and scope out the size of the differences that occurs between the two datasets.","metadata":{}},{"cell_type":"markdown","source":"### Transform Test Set\n\nFirst thing we need to do, is apply the same 40-bit integer transformer to the testing set.","metadata":{}},{"cell_type":"code","source":"test[\"soiltype_label\"] = test.apply(make_40_bit_int_from_soiltype, axis=1)\nprint(\": Number of unique labels: {:,d}\".format(test[\"soiltype_label\"].nunique()))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Check for Mismatches\n\nLet's use the `OrdinalEncoder` and check to see how many rows in the testing set have values we haven't seen before.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import OrdinalEncoder\n\nencoder = OrdinalEncoder(\n    dtype=np.int16,\n    handle_unknown=\"use_encoded_value\",\n    unknown_value=32766,\n)\ntrain[\"soiltype_label_ordinal\"] = encoder.fit_transform(train[[\"soiltype_label\"]])\ntest[\"soiltype_label_ordinal\"] = encoder.transform(test[[\"soiltype_label\"]])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\": Number of mismatched rows: {:,d}\".format(\n    test[(test[\"soiltype_label_ordinal\"] == 32766)].shape[0]\n))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Given that there are 1,000,000 rows in the testing set, having 6,843 that are mismatched is not bad (~0.7% of the testing data are impacted by this). However, this competition is all about rare cases, so we should examine if we can improve upon this.","metadata":{}},{"cell_type":"markdown","source":"# Encoding Using 5 x 8-bit Integers\n\nInstead of using a single 40-bit integer, we could instead encode the soil types using a series of five 8-bit integers instead. By breaking up the encoded values, we may lose less information about soil types, since we spread out single-bit differences across 5 numerics instead of 1.","metadata":{}},{"cell_type":"code","source":"def make_5_8_bit_ints_from_soiltype(row):\n    integer1 = (np.int64(row[\"soiltype_label\"]) & 0xFF00000000) >> 30\n    integer2 = (np.int64(row[\"soiltype_label\"]) & 0x00FF000000) >> 24\n    integer3 = (np.int64(row[\"soiltype_label\"]) & 0x0000FF0000) >> 16\n    integer4 = (np.int64(row[\"soiltype_label\"]) & 0x000000FF00) >> 8\n    integer5 = (np.int64(row[\"soiltype_label\"]) & 0x00000000FF)\n    return integer1, integer2, integer3, integer4, integer5","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Transform the rows","metadata":{}},{"cell_type":"code","source":"train[[\"soiltype_int1\", \"soiltype_int2\", \"soiltype_int3\", \"soiltype_int4\", \"soiltype_int5\"]] = train.apply(make_5_8_bit_ints_from_soiltype, axis=1, result_type=\"expand\")\ntest[[\"soiltype_int1\", \"soiltype_int2\", \"soiltype_int3\", \"soiltype_int4\", \"soiltype_int5\"]] = test.apply(make_5_8_bit_ints_from_soiltype, axis=1, result_type=\"expand\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"int_encoder = OrdinalEncoder(\n    dtype=np.int16,\n    handle_unknown=\"use_encoded_value\",\n    unknown_value=32766,\n)\ntrain[[\"soiltype_label_int1\", \"soiltype_label_int2\", \"soiltype_label_int3\", \"soiltype_label_int4\", \"soiltype_label_int5\"]] = int_encoder.fit_transform(train[[\"soiltype_int1\", \"soiltype_int2\", \"soiltype_int3\", \"soiltype_int4\", \"soiltype_int5\"]])\ntest[[\"soiltype_label_int1\", \"soiltype_label_int2\", \"soiltype_label_int3\", \"soiltype_label_int4\", \"soiltype_label_int5\"]] = int_encoder.transform(test[[\"soiltype_int1\", \"soiltype_int2\", \"soiltype_int3\", \"soiltype_int4\", \"soiltype_int5\"]])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check to see how many rows in our test set are now mismatched from the training set.","metadata":{}},{"cell_type":"code","source":"def check_for_missing_values(row):\n    return 1 if row[\"soiltype_label_int1\"] == 32766 or row[\"soiltype_label_int2\"] == 32766 or row[\"soiltype_label_int3\"] == 32766 or row[\"soiltype_label_int4\"] == 32766 or row[\"soiltype_label_int5\"] == 32766 else 0\n\ntest[\"missing_8_bit_value\"] = test.apply(check_for_missing_values, axis=1)\nprint(\": Number of mismatched rows: {:,d}\".format(\n    test[\"missing_8_bit_value\"].sum()\n))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We've now reduced the number of mismatches to 33. Let's re-run the LightGBM model and see if there is any accuracy increase with the new encoding, or if the additional dimensions reduces accuracy.","metadata":{}},{"cell_type":"code","source":"%%time\ntarget = train[\"Cover_Type\"]\ncv_rounds = 3\n\nk_fold = StratifiedKFold(\n    n_splits=cv_rounds,\n    random_state=2021,\n    shuffle=True,\n)\n\nfeatures.remove(\"soiltype_label_encoded\")\nfeatures.insert(0, \"soiltype_label_int1\")\nfeatures.insert(1, \"soiltype_label_int2\")\nfeatures.insert(2, \"soiltype_label_int3\")\nfeatures.insert(3, \"soiltype_label_int4\")\nfeatures.insert(4, \"soiltype_label_int5\")\n\ntrain_preds = np.zeros(len(train.index), )\ntrain_probas = np.zeros(len(train.index), )\n\nfor fold, (train_index, test_index) in enumerate(k_fold.split(train[features], target)):\n    x_train = train[features].iloc[train_index]\n    y_train = target.iloc[train_index]\n\n    x_valid = train[features].iloc[test_index]\n    y_valid = target.iloc[test_index]\n\n    model = LGBMClassifier(\n        random_state=2021,\n        n_estimators=2000,\n        verbose=-1,\n        metric=\"softmax\",\n        cat_feature=[0, 1, 2, 3, 4],\n    )\n    model.fit(\n        x_train,\n        y_train,\n        eval_set=[(x_valid, y_valid)],\n        callbacks=[early_stopping(50, verbose=False)],\n    )\n\n    train_oof_preds = model.predict(x_valid)\n    train_preds[test_index] = train_oof_preds\n    \n    print(\"-- Fold {}:\".format(fold+1))\n    print(\"{}\".format(classification_report(y_valid, train_oof_preds)))\n    print(\"-- Accuracy: {}\".format(accuracy_score(y_valid, train_oof_preds)))\n\nprint(\"-- Overall:\")\nprint(\"{}\".format(classification_report(target, train_preds)))\nprint(\"-- Accuracy: {}\".format(accuracy_score(target, train_preds)))\n\ntrain[\"integer_preds\"] = train_preds\n\n# Show the confusion matrix\nconfusion = confusion_matrix(train[\"Cover_Type\"], train[\"integer_preds\"])\ncover_labels = [1, 2, 3, 4, 6, 7]\nfig, ax = plt.subplots(figsize=(15, 15))\nax = sns.heatmap(confusion, annot=True, fmt=\",d\", xticklabels=cover_labels, yticklabels=cover_labels)\n_ = ax.set_title(\"Confusion Matrix for LGB Classifier (8-bit Integers)\", fontsize=15)\n_ = ax.set_ylabel(\"Actual Class\")\n_ = ax.set_xlabel(\"Predicted Class\")\n\ndel(train_preds)\ndel(confusion)\n_ = gc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bar, ax = plt.subplots(figsize=(20, 10))\nax = sns.barplot(\n    x=[\"Unmodified\", \"40-bit Integer\", \"5 x 8-bit Integers\"],\n    y=[\n        float(accuracy_score(target, train[\"unmodified_preds\"])),\n        accuracy_score(target, train[\"collapsed_preds\"]),\n        accuracy_score(target, train[\"integer_preds\"]),\n    ],\n)\n_ = ax.set_title(\"Accuracy Score Based on Approach\", fontsize=15)\n_ = ax.set_xlabel(\"Approach\")\n_ = ax.set_ylabel(\"Accuracy Score\")\n_ = ax.set(ylim=(0.90, 1.0))\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(\n        x=p.get_x()+(p.get_width()/2),\n        y=height,\n        s=\"{:.4f}\".format(height),\n        ha=\"center\"\n    )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The overall accuracy for the 5 x 8-bit integer approach is very slightly less than that of the 40-bit integer approach. However, overall, we only have a mismatch in data of 33 rows. It is probably very worthwhile to go with the 5 x 8-bit integer approach, as we have an impact on only a very small number of testing rows.","metadata":{}},{"cell_type":"markdown","source":"# Conclusions\n\nIt appears that we can reduce the overall dimensionality of the dataset significantly by encoding the `Soil_Type` variables as a series of five 8-bit integers. The encoding compresses the data while at the same time providing a boost to a vanilla LightGBM model's accuracy by nearly 0.5%. Presumably, the reduced dimensionality may also provide some benefit to other approaches, as we collapsed a 40-dimensional vector down to a 5-dimensional vector.\n\nIf you find this kernel useful, please consider upvoting it!","metadata":{}}]}