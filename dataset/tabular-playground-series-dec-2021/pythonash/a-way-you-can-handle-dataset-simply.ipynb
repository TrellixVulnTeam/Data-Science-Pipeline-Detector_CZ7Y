{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Read me!!\n\nHello, welcome to my notebook!!\n\nThis notebook is for you who want to know how you can load and handle dataset.\n\nThe key points are as follows:\n\n1. You have to reduce the memory to allocate your dataset.\n\n> There are so many dataset which have large size.\n\n> So, you may convert each data in row into more small size (byte).\n\n2. EDA will be conducted briefly.\n\n> There are plenty of columns in dataset, but we just identify what column has NaN value or outlier (it will be enough!!).\n\n3. We will divide the columns into dummy cluster and non-dummy cluster.\n\n> This dataset includes some dummy variables and not.\n\n> So, we will divide the dataset into two clusters for our future model (see [Parallel startegy through deep learning])!!\n\n\nIf you have any questions, please leave the comments.\n\nI hope you to gain more imformation about data handling, DNN, CNN, and etc..\n\n## **Knowledge can be improved by being shared.**\n\nPlease upvote!!\n\n\n## [You can learn more skills for handling dataset or neural network.]\n\n### [Parallel strategy through deep learning (0.956%)] - Tabular Playground Series - Dec 2021\n- https://www.kaggle.com/pythonash/parallel-strategy-through-deep-learning-0-956\n\n### [How to use csv and img at the same time] - Pawpularity Contest\n - https://www.kaggle.com/pythonash/how-to-use-csv-and-img-at-the-same-time\n\n### [Parallel combination DNN with CNN] - Pawpularity Contest\n - https://www.kaggle.com/pythonash/parallel-dnn-and-cnn-network-for-beginners\n \n### [Image data handling without memory exploded] - Pawpularity Contest\n - https://www.kaggle.com/pythonash/how-to-handle-dataset-for-beginners\n\n### [Data handling & Deep learning] - Titanic competition (Top 7%)\n - https://www.kaggle.com/pythonash/how-to-handle-raw-dataset-and-analyze-with-dl\n \n### [Deep learning model with SeLU activation function] - Titanic competition\n- https://www.kaggle.com/pythonash/selu-activation-function-in-dl\n\n### [Preparing a completed dataset with proper imputation method] - Titanic competition\n - https://www.kaggle.com/pythonash/making-completed-dataset\n\n**Let's start!**","metadata":{}},{"cell_type":"markdown","source":"# Data loading and handling.","metadata":{}},{"cell_type":"markdown","source":"## Import some useful libraries.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nimport tensorflow as tf\nfrom sklearn.model_selection import StratifiedKFold as SK\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score\n\n\n\ntrain = pd.read_csv('../input/tabular-playground-series-dec-2021/train.csv')\n\ntest = pd.read_csv('../input/tabular-playground-series-dec-2021/test.csv')\n\nsubmission = pd.read_csv('../input/tabular-playground-series-dec-2021/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-12-30T07:52:39.79417Z","iopub.execute_input":"2021-12-30T07:52:39.794874Z","iopub.status.idle":"2021-12-30T07:52:56.366105Z","shell.execute_reply.started":"2021-12-30T07:52:39.794773Z","shell.execute_reply":"2021-12-30T07:52:56.365323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Making your data more light with smaller size.","metadata":{}},{"cell_type":"code","source":"def reduce_mean_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2\n    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        \n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            \n            if str(col_type)[:3] =='int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min> np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() / 1024**2\n    \n    if verbose:\n        print('Memory usage is decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem)/ start_mem))\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2021-12-30T07:52:56.367655Z","iopub.execute_input":"2021-12-30T07:52:56.367943Z","iopub.status.idle":"2021-12-30T07:52:56.383463Z","shell.execute_reply.started":"2021-12-30T07:52:56.367905Z","shell.execute_reply":"2021-12-30T07:52:56.382635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = reduce_mean_usage(train)\ntest = reduce_mean_usage(test)","metadata":{"execution":{"iopub.status.busy":"2021-12-30T07:52:56.385032Z","iopub.execute_input":"2021-12-30T07:52:56.385324Z","iopub.status.idle":"2021-12-30T07:53:17.307294Z","shell.execute_reply.started":"2021-12-30T07:52:56.385287Z","shell.execute_reply":"2021-12-30T07:53:17.306542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Sampling your data with randomness.\n\nIt is crucial part for using deep learning or even predicting because the randomness will make your model be more robust and have more generalization power.","metadata":{}},{"cell_type":"code","source":"# Use sample method with frac =1 .\ntrain = train.sample(frac=1).reset_index(drop=True)\n\ntrain_x = train.drop(['Id','Cover_Type'],axis=1)\n\ntest_x = test.drop(['Id'],axis=1)\n\ntrain_y = train['Cover_Type']\n\ntrain","metadata":{"execution":{"iopub.status.busy":"2021-12-30T07:53:17.309711Z","iopub.execute_input":"2021-12-30T07:53:17.309976Z","iopub.status.idle":"2021-12-30T07:53:19.266023Z","shell.execute_reply.started":"2021-12-30T07:53:17.309938Z","shell.execute_reply":"2021-12-30T07:53:19.265327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Identifying teh dataset whether there is NaN value or not.","metadata":{}},{"cell_type":"code","source":"train_x.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-12-30T07:53:19.267407Z","iopub.execute_input":"2021-12-30T07:53:19.267903Z","iopub.status.idle":"2021-12-30T07:53:19.501803Z","shell.execute_reply.started":"2021-12-30T07:53:19.267865Z","shell.execute_reply":"2021-12-30T07:53:19.501109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## We don't need to imputate any values!!","metadata":{}},{"cell_type":"code","source":"train_y.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-12-30T07:53:19.503119Z","iopub.execute_input":"2021-12-30T07:53:19.503361Z","iopub.status.idle":"2021-12-30T07:53:19.515034Z","shell.execute_reply.started":"2021-12-30T07:53:19.503327Z","shell.execute_reply":"2021-12-30T07:53:19.514362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Identifying the target variable's distribution.\n\nThere are total 7 classes.\n\nHowever, the class 5 is only one in target variable.\n\nSo, you should delete the class because it can be regarded as outlier when the model conducts predicting.\n\n> Your model will compute the weights or train the values with the class 5.","metadata":{}},{"cell_type":"code","source":"train_y.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-12-30T07:53:19.516321Z","iopub.execute_input":"2021-12-30T07:53:19.516606Z","iopub.status.idle":"2021-12-30T07:53:19.539146Z","shell.execute_reply.started":"2021-12-30T07:53:19.516572Z","shell.execute_reply":"2021-12-30T07:53:19.538411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Drop the class 5!!","metadata":{}},{"cell_type":"code","source":"drop_5_index = train_y[train_y == 5].index[0]\n\ntrain_x = train_x.drop([drop_5_index],axis=0).reset_index().drop(['index'],axis=1)\ntrain_y = train_y.drop([drop_5_index],axis=0).reset_index().drop(['index'],axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-12-30T07:53:19.540139Z","iopub.execute_input":"2021-12-30T07:53:19.540571Z","iopub.status.idle":"2021-12-30T07:53:20.277478Z","shell.execute_reply.started":"2021-12-30T07:53:19.540535Z","shell.execute_reply":"2021-12-30T07:53:20.276731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Descriptive statistics for train input.\n\nAs you can see, the variables consist of dummy variable and non-dummy variable.\n\nSo, we will divide the dataset into dummy cluster and non-dummy cluster.","metadata":{}},{"cell_type":"code","source":"train_x.describe()","metadata":{"execution":{"iopub.status.busy":"2021-12-30T07:53:20.278912Z","iopub.execute_input":"2021-12-30T07:53:20.279173Z","iopub.status.idle":"2021-12-30T07:53:23.012178Z","shell.execute_reply.started":"2021-12-30T07:53:20.279138Z","shell.execute_reply":"2021-12-30T07:53:23.011392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Processed target variable distribution.","metadata":{}},{"cell_type":"code","source":"train_y.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-12-30T07:53:23.013839Z","iopub.execute_input":"2021-12-30T07:53:23.014389Z","iopub.status.idle":"2021-12-30T07:53:23.084684Z","shell.execute_reply.started":"2021-12-30T07:53:23.014344Z","shell.execute_reply":"2021-12-30T07:53:23.083961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## For using deep learning, scale your data with min-max scale.","metadata":{}},{"cell_type":"code","source":"scaler = MinMaxScaler()\ntrain_df = scaler.fit_transform(train_x)\ntrain_df = pd.DataFrame(train_df)\ntrain_df.columns = train_x.columns\ntrain_df","metadata":{"execution":{"iopub.status.busy":"2021-12-30T07:53:23.085991Z","iopub.execute_input":"2021-12-30T07:53:23.086414Z","iopub.status.idle":"2021-12-30T07:53:25.776217Z","shell.execute_reply.started":"2021-12-30T07:53:23.086376Z","shell.execute_reply":"2021-12-30T07:53:25.7754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.DataFrame(scaler.transform(test_x))\ntest_df.columns = test_x.columns\ntest_df","metadata":{"execution":{"iopub.status.busy":"2021-12-30T07:53:25.7776Z","iopub.execute_input":"2021-12-30T07:53:25.77852Z","iopub.status.idle":"2021-12-30T07:53:26.129826Z","shell.execute_reply.started":"2021-12-30T07:53:25.778457Z","shell.execute_reply":"2021-12-30T07:53:26.12894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.columns","metadata":{"execution":{"iopub.status.busy":"2021-12-30T07:53:26.132803Z","iopub.execute_input":"2021-12-30T07:53:26.133166Z","iopub.status.idle":"2021-12-30T07:53:26.140053Z","shell.execute_reply.started":"2021-12-30T07:53:26.133124Z","shell.execute_reply":"2021-12-30T07:53:26.139164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Deviding the dataset into dummy and non-dummy cluster.","metadata":{}},{"cell_type":"code","source":"train_non_dummy, train_dummy = train_df.columns[:10], train_df.columns[10:]\n\ntrain_df_dummy = train_df[train_dummy]\ntrain_df_non_dummy = train_df[train_non_dummy]","metadata":{"execution":{"iopub.status.busy":"2021-12-30T07:53:26.141646Z","iopub.execute_input":"2021-12-30T07:53:26.141931Z","iopub.status.idle":"2021-12-30T07:53:26.721192Z","shell.execute_reply.started":"2021-12-30T07:53:26.141889Z","shell.execute_reply":"2021-12-30T07:53:26.720376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df_dummy = test_df[train_dummy]\ntest_df_non_dummy = test_df[train_non_dummy]","metadata":{"execution":{"iopub.status.busy":"2021-12-30T07:53:26.722372Z","iopub.execute_input":"2021-12-30T07:53:26.722682Z","iopub.status.idle":"2021-12-30T07:53:26.84808Z","shell.execute_reply.started":"2021-12-30T07:53:26.722646Z","shell.execute_reply":"2021-12-30T07:53:26.847368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Making the previous data empty.\n\n> For using memory more free, make the of no use data empty.","metadata":{}},{"cell_type":"code","source":"train=[]\ntest=[]\ntrain_x=[]\ntest_x=[]\ntrain_df=[]\ntest_df=[]","metadata":{"execution":{"iopub.status.busy":"2021-12-30T07:53:26.849533Z","iopub.execute_input":"2021-12-30T07:53:26.84981Z","iopub.status.idle":"2021-12-30T07:53:26.854938Z","shell.execute_reply.started":"2021-12-30T07:53:26.849774Z","shell.execute_reply":"2021-12-30T07:53:26.853798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Lastly, encoding the target variable with 6 classes.\n\nIn this case, when you use deep learing with keras, you have to use 'sparse_categorical_crossentropy' and 'sparse_categorical_accuracy' as loss function and metric indicator, repectively.","metadata":{}},{"cell_type":"code","source":"Encoder = LabelEncoder()\n\ny_encoded = Encoder.fit_transform(train_y)\ntrain_y = []\ny_encoded","metadata":{"execution":{"iopub.status.busy":"2021-12-30T07:53:26.85652Z","iopub.execute_input":"2021-12-30T07:53:26.85679Z","iopub.status.idle":"2021-12-30T07:53:27.015008Z","shell.execute_reply.started":"2021-12-30T07:53:26.856755Z","shell.execute_reply":"2021-12-30T07:53:27.014229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# It has done!!\n\nUntil now, you have learned about loading and handling the large data.\n\nThe next step is prepared for predicting and submitting the result.\n\nAnd I will introduce my notebook with 27% rating through deep learing model.\n\nThe reason why we divide dataset into dummy and non-dummy cluster will be described in my next notebook (I will call this as parallel startegy.\n\nIf you get any helps from my notebook, please upvote!!\n\nThank you!","metadata":{}}]}