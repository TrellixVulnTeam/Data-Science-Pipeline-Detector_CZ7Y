{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction","metadata":{}},{"cell_type":"markdown","source":"The objective to predict the cover type of a forest given features like elavation, soil type etc. There are 7 different cover types to predict in total.\n\n![https://th.bing.com/th/id/OIP.PcAN1kc44gDpHowTie715gHaD4?pid=ImgDet&rs=1](https://th.bing.com/th/id/OIP.PcAN1kc44gDpHowTie715gHaD4?pid=ImgDet&rs=1)","metadata":{}},{"cell_type":"markdown","source":"**Acknowledgments:**\n* [Confusion matrices](https://www.kaggle.com/ambrosm/tpsdec21-01-keras-quickstart) by [AmbrosM](https://www.kaggle.com/ambrosm).\n* [Feature engineering](https://www.kaggle.com/c/tabular-playground-series-dec-2021/discussion/293373) by [Gulshan Mishra](https://www.kaggle.com/gulshanmishra).\n* [Memory usage](https://www.kaggle.com/c/tabular-playground-series-dec-2021/discussion/291844) by [Luca Massaron](https://www.kaggle.com/lucamassaron).\n* [Ensembling](https://www.kaggle.com/odins0n/tps-dec-eda-modeling/notebook#Modeling) by [Sanskar Hasija\n](https://www.kaggle.com/odins0n).\n* [Pseudolabelling](https://www.kaggle.com/remekkinas/tps-12-nn-tpu-pseudolabeling-0-95661/notebook) by [Remek Kinas](https://www.kaggle.com/remekkinas).","metadata":{}},{"cell_type":"markdown","source":"This notebook will be essentially the same to my other notebook, except we won't do EDA here to save memory. This will allow use to use more folds in the cross validation stage. \n\nSee below for my main notebook:\n* [EDA, Feature Engineering and Pseudolabelling](https://www.kaggle.com/samuelcortinhas/tps-dec-eda-feat-eng-pseudolab)","metadata":{}},{"cell_type":"markdown","source":"# Libraries","metadata":{}},{"cell_type":"code","source":"# Core\nimport numpy as np\nimport pandas as pd\npd.set_option('display.float_format', lambda x: '%.1f' % x)\npd.get_option(\"display.max_columns\", 55)\nimport seaborn as sns\nsns.set_style('darkgrid')\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom itertools import combinations\nimport statistics\nimport time\n\n# Sklearn\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.metrics import accuracy_score\n\n# Tensorflow\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import callbacks","metadata":{"execution":{"iopub.status.busy":"2021-12-13T20:57:25.794622Z","iopub.execute_input":"2021-12-13T20:57:25.79495Z","iopub.status.idle":"2021-12-13T20:57:31.730662Z","shell.execute_reply.started":"2021-12-13T20:57:25.794878Z","shell.execute_reply":"2021-12-13T20:57:31.729866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data","metadata":{}},{"cell_type":"code","source":"# Save to df\ntrain_data=pd.read_csv('../input/tabular-playground-series-dec-2021/train.csv', index_col='Id')\ntest_data=pd.read_csv('../input/tabular-playground-series-dec-2021/test.csv', index_col='Id')\n\n# save for submission\ntest_index=test_data.index\n\n# Shape and preview\nprint('Training data df shape:',train_data.shape)\nprint('Test data df shape:',test_data.shape)\ntrain_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-13T20:58:18.165948Z","iopub.execute_input":"2021-12-13T20:58:18.166202Z","iopub.status.idle":"2021-12-13T20:58:39.256974Z","shell.execute_reply.started":"2021-12-13T20:58:18.166173Z","shell.execute_reply":"2021-12-13T20:58:39.25616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pseudolabeling","metadata":{}},{"cell_type":"code","source":"# Save to df\npseudo_label_df=pd.read_csv('../input/tps12-pseudolabels/tps12-pseudolabels_v2.csv', index_col='Id')\n\n# Concatenate\nnew_train_data=pd.concat([train_data, pseudo_label_df], axis=0)\n\n# Remove pseudolabel samples from test set\npseudo_label_index=pseudo_label_df.index\nnew_test_data=test_data.drop(pseudo_label_index, axis=0)\n\n# Save for submission\nnew_test_data_index=new_test_data.index\npseudo_label_preds_df=pd.DataFrame({'Id': pseudo_label_index,\n                       'Cover_Type': pseudo_label_df['Cover_Type']}).reset_index(drop=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Drop label 5**","metadata":{}},{"cell_type":"code","source":"new_train_data.drop(new_train_data[new_train_data.Cover_Type==5].index, axis=0, inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature engineering","metadata":{}},{"cell_type":"markdown","source":"**Remove unwanted negative values**","metadata":{}},{"cell_type":"markdown","source":"For the features below it does not make physical sense to include negative numbers.","metadata":{}},{"cell_type":"code","source":"# Specify features to clip\nmask_features=['Slope','Horizontal_Distance_To_Hydrology','Vertical_Distance_To_Hydrology',\n              'Horizontal_Distance_To_Roadways','Horizontal_Distance_To_Fire_Points']\n\n# Clip negative values\nnew_train_data[mask_features]=new_train_data[mask_features].clip(lower=0)\nnew_test_data[mask_features]=new_test_data[mask_features].clip(lower=0)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-12-13T20:58:45.255298Z","iopub.execute_input":"2021-12-13T20:58:45.255581Z","iopub.status.idle":"2021-12-13T20:58:45.374033Z","shell.execute_reply.started":"2021-12-13T20:58:45.255545Z","shell.execute_reply":"2021-12-13T20:58:45.373224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Aspect**","metadata":{}},{"cell_type":"markdown","source":"Aspect values represent angles between 0 and 360 degrees so we should project them onto [0,360] to make any patterns easier to learn.","metadata":{}},{"cell_type":"code","source":"# Project training aspect angles onto [0,360]\nnew_train_data['Aspect'][new_train_data['Aspect'] < 0] += 360\nnew_train_data['Aspect'][new_train_data['Aspect'] >= 360] -= 360\n\n# Project test aspect angles onto [0,360]\nnew_test_data['Aspect'][new_test_data['Aspect'] < 0] += 360\nnew_test_data['Aspect'][new_test_data['Aspect'] >= 360] -= 360","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-12-13T20:58:45.37525Z","iopub.execute_input":"2021-12-13T20:58:45.376714Z","iopub.status.idle":"2021-12-13T20:58:45.722909Z","shell.execute_reply.started":"2021-12-13T20:58:45.376674Z","shell.execute_reply":"2021-12-13T20:58:45.722175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Distance to Hydrology**","metadata":{}},{"cell_type":"markdown","source":"We have the horizontal and vertical distances to Hydrology so we can use these to calculate the l1 or euclidean distance.","metadata":{}},{"cell_type":"code","source":"# l1 (aka Manhattan) distance to Hydrology\nnew_train_data['l1_Hydrology'] = np.abs(new_train_data['Horizontal_Distance_To_Hydrology']) + np.abs(new_train_data['Vertical_Distance_To_Hydrology'])\nnew_test_data['l1_Hydrology'] = np.abs(new_test_data['Horizontal_Distance_To_Hydrology']) + np.abs(new_test_data['Vertical_Distance_To_Hydrology'])","metadata":{"execution":{"iopub.status.busy":"2021-12-13T21:09:15.422982Z","iopub.execute_input":"2021-12-13T21:09:15.423442Z","iopub.status.idle":"2021-12-13T21:09:15.478989Z","shell.execute_reply.started":"2021-12-13T21:09:15.423399Z","shell.execute_reply":"2021-12-13T21:09:15.478309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Euclidean distance to Hydrology (training set)\nnew_train_data[\"ED_to_Hydrology\"] = np.sqrt((new_train_data['Horizontal_Distance_To_Hydrology'].astype(np.int32))**2 + \n                                        (new_train_data['Vertical_Distance_To_Hydrology'].astype(np.int32))**2)\n\n# Euclidean distance to Hydrology (test set)\nnew_test_data[\"ED_to_Hydrology\"] = np.sqrt((new_test_data['Horizontal_Distance_To_Hydrology'].astype(np.int32))**2 + \n                                       (new_test_data['Vertical_Distance_To_Hydrology'].astype(np.int32))**2)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T20:58:45.723964Z","iopub.execute_input":"2021-12-13T20:58:45.724198Z","iopub.status.idle":"2021-12-13T20:58:45.772084Z","shell.execute_reply.started":"2021-12-13T20:58:45.724167Z","shell.execute_reply":"2021-12-13T20:58:45.771384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Hillshade**","metadata":{}},{"cell_type":"markdown","source":"From [ArcMap](https://desktop.arcgis.com/en/arcmap/10.3/manage-data/raster-and-images/hillshade-function.htm): \"A hillshade is a grayscale 3D representation of the surface, with the sun's relative position taken into account for shading the image.\" \n\nThis means all Hillshade values should lie in the range [0, 255] because it corresponds to a greyscale image.","metadata":{}},{"cell_type":"code","source":"# Clips hillshades 0 to 255 index\nhillshades = [col for col in train_data.columns if col.startswith('Hillshade')]\n\n# Clip df's\nnew_train_data[hillshades] = new_train_data[hillshades].clip(0, 255)\nnew_test_data[hillshades] = new_test_data[hillshades].clip(0, 255)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T20:58:45.773395Z","iopub.execute_input":"2021-12-13T20:58:45.77438Z","iopub.status.idle":"2021-12-13T20:58:45.946037Z","shell.execute_reply.started":"2021-12-13T20:58:45.774328Z","shell.execute_reply":"2021-12-13T20:58:45.945329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Number of soil & wilderness types**","metadata":{}},{"cell_type":"markdown","source":"Credit: [Craig Thomas](https://www.kaggle.com/craigmthomas).","metadata":{}},{"cell_type":"code","source":"# Soil type count\nsoil_features = [x for x in new_train_data.columns if x.startswith(\"Soil_Type\")]\nnew_train_data[\"Soil_Type_Count\"] = new_train_data[soil_features].sum(axis=1)\nnew_test_data[\"Soil_Type_Count\"] = new_test_data[soil_features].sum(axis=1)\n\n# Wilderness area count\nwilderness_features = [x for x in new_train_data.columns if x.startswith(\"Wilderness_Area\")]\nnew_train_data[\"Wilderness_Area_Count\"] = new_train_data[wilderness_features].sum(axis=1)\nnew_test_data[\"Wilderness_Area_Count\"] = new_test_data[wilderness_features].sum(axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T20:58:45.94714Z","iopub.execute_input":"2021-12-13T20:58:45.947695Z","iopub.status.idle":"2021-12-13T20:58:48.356759Z","shell.execute_reply.started":"2021-12-13T20:58:45.947658Z","shell.execute_reply":"2021-12-13T20:58:48.35584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Drop features with 0 variance**","metadata":{}},{"cell_type":"code","source":"# Train df\nnew_train_data.drop('Soil_Type7', axis=1, inplace=True)\nnew_train_data.drop('Soil_Type15', axis=1, inplace=True)\n\n# Test df\nnew_test_data.drop('Soil_Type7', axis=1, inplace=True)\nnew_test_data.drop('Soil_Type15', axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T21:01:41.915045Z","iopub.execute_input":"2021-12-13T21:01:41.915312Z","iopub.status.idle":"2021-12-13T21:01:45.095826Z","shell.execute_reply.started":"2021-12-13T21:01:41.915283Z","shell.execute_reply":"2021-12-13T21:01:45.095019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Memory","metadata":{}},{"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-12-13T21:13:45.289117Z","iopub.execute_input":"2021-12-13T21:13:45.289408Z","iopub.status.idle":"2021-12-13T21:13:45.302164Z","shell.execute_reply.started":"2021-12-13T21:13:45.289358Z","shell.execute_reply":"2021-12-13T21:13:45.301238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_train_data=reduce_mem_usage(new_train_data)\nnew_test_data=reduce_mem_usage(new_test_data)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T21:13:47.165645Z","iopub.execute_input":"2021-12-13T21:13:47.166278Z","iopub.status.idle":"2021-12-13T21:14:07.452593Z","shell.execute_reply.started":"2021-12-13T21:13:47.166236Z","shell.execute_reply":"2021-12-13T21:14:07.451822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pre-process data","metadata":{}},{"cell_type":"markdown","source":"**Labels and features:**","metadata":{}},{"cell_type":"code","source":"# Labels\ny=new_train_data.Cover_Type\n\n# Features\nX=new_train_data.drop('Cover_Type', axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T21:14:07.454082Z","iopub.execute_input":"2021-12-13T21:14:07.454505Z","iopub.status.idle":"2021-12-13T21:14:07.691124Z","shell.execute_reply.started":"2021-12-13T21:14:07.454467Z","shell.execute_reply":"2021-12-13T21:14:07.690412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Scale data**","metadata":{}},{"cell_type":"code","source":"scaler = StandardScaler()\nX=scaler.fit_transform(X)\ntest_data_preprocessed = scaler.transform(new_test_data)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T21:14:07.692533Z","iopub.execute_input":"2021-12-13T21:14:07.692769Z","iopub.status.idle":"2021-12-13T21:14:11.881981Z","shell.execute_reply.started":"2021-12-13T21:14:07.692737Z","shell.execute_reply":"2021-12-13T21:14:11.881257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Label encoding**","metadata":{}},{"cell_type":"code","source":"# Encode labels to lie in range 0 to 5\nencoder = LabelEncoder()\ny = encoder.fit_transform(y)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T21:14:11.883748Z","iopub.execute_input":"2021-12-13T21:14:11.884157Z","iopub.status.idle":"2021-12-13T21:14:12.031259Z","shell.execute_reply.started":"2021-12-13T21:14:11.88412Z","shell.execute_reply":"2021-12-13T21:14:12.030526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Save memory**","metadata":{}},{"cell_type":"code","source":"del train_data, test_data, scaler\ndel pseudo_label_df, new_train_data, new_test_data\ndel mask_features, hillshades\ndel soil_features,wilderness_features","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:11:56.441419Z","iopub.execute_input":"2021-12-12T22:11:56.441754Z","iopub.status.idle":"2021-12-12T22:11:56.446784Z","shell.execute_reply.started":"2021-12-12T22:11:56.441711Z","shell.execute_reply":"2021-12-12T22:11:56.445803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Neural network","metadata":{}},{"cell_type":"code","source":"# Define model\ndef build_model():\n    model = keras.Sequential([\n\n        # hidden layer 1\n        layers.Dense(units=256, activation='relu', input_shape=[X.shape[1]], kernel_initializer='lecun_normal'),\n        layers.Dropout(rate=0.3),\n\n        # hidden layer 2\n        layers.Dense(units=256, activation='relu', kernel_initializer='lecun_normal'),\n        layers.Dropout(rate=0.3),\n\n        # hidden layer 3\n        layers.Dense(units=128, activation='relu', kernel_initializer='lecun_normal'),\n        layers.Dropout(rate=0.2),\n        \n        # hidden layer 4\n        layers.Dense(units=64, activation='relu', kernel_initializer='lecun_normal'),\n        layers.Dropout(rate=0.2),\n\n        # output layer\n        layers.Dense(units=6, activation='softmax')\n    ])\n    \n    # Define loss, optimizer and metric\n    model.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n    \n    return model","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-12-12T22:12:30.538966Z","iopub.execute_input":"2021-12-12T22:12:30.539306Z","iopub.status.idle":"2021-12-12T22:12:30.547596Z","shell.execute_reply.started":"2021-12-12T22:12:30.539271Z","shell.execute_reply":"2021-12-12T22:12:30.546271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Callbacks**","metadata":{}},{"cell_type":"code","source":"# Define early stopping callback on validation loss\nearly_stopping = callbacks.EarlyStopping(\n    monitor=\"val_loss\",\n    patience=20,\n    restore_best_weights=True,\n)\n\n# Reduce learning rate when validation loss plateaus\nreduce_lr = callbacks.ReduceLROnPlateau(\n    monitor=\"val_loss\",\n    factor=0.5,\n    patience=5\n)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:12:34.224028Z","iopub.execute_input":"2021-12-12T22:12:34.224616Z","iopub.status.idle":"2021-12-12T22:12:34.230721Z","shell.execute_reply.started":"2021-12-12T22:12:34.224573Z","shell.execute_reply":"2021-12-12T22:12:34.229577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cross validation","metadata":{}},{"cell_type":"markdown","source":"Credit: [Gulshan](https://www.kaggle.com/gulshanmishra/tps-dec-21-tensorflow-nn-feature-engineering).","metadata":{}},{"cell_type":"code","source":"FOLDS = 8\nEPOCHS = 100\nBATCH_SIZE = 250\n\ntest_preds = np.zeros((1, 1))\nscores = []\n\ncv = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=0)\n\nfor fold, (train_idx, val_idx) in enumerate(cv.split(X, y)):\n    # Start timer\n    start = time.time()\n    \n    # get training and validation sets\n    X_train, X_valid = X[train_idx], X[val_idx]\n    y_train, y_valid = y[train_idx], y[val_idx]\n\n    # Build and train model on tpu\n    model = build_model()\n    model.fit(\n        X_train,\n        y_train,\n        validation_data=(X_valid, y_valid),\n        epochs=EPOCHS,\n        batch_size=BATCH_SIZE,\n        callbacks=[early_stopping, reduce_lr],\n        verbose=False\n    )\n\n    # Make predictions and get measure accuracy\n    y_pred = np.argmax(model.predict(X_valid), axis=1)\n    score = accuracy_score(y_valid, y_pred)\n    scores.append(score)\n    \n    # Store predictions\n    test_preds = test_preds + model.predict(test_data_preprocessed)\n    \n    # Stop timer\n    stop = time.time()\n    \n    # Print accuracy and time\n    print(f\"Fold {fold} - Accuracy: {score}, Time: {round((stop - start)/60,1)} mins\")\n    \nprint('')\nprint(f\"Mean Accuracy: {np.mean(scores)}\")","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-12-12T22:12:37.836986Z","iopub.execute_input":"2021-12-12T22:12:37.837808Z","iopub.status.idle":"2021-12-12T22:32:50.79203Z","shell.execute_reply.started":"2021-12-12T22:12:37.837768Z","shell.execute_reply":"2021-12-12T22:32:50.790276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Soft voting**","metadata":{}},{"cell_type":"code","source":"# Soft voting to ensemble predictions\ntest_preds = np.argmax(test_preds, axis=1)\n\n# Recover class labels\npred_classes = encoder.inverse_transform(test_preds)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:32:50.793411Z","iopub.status.idle":"2021-12-12T22:32:50.794175Z","shell.execute_reply.started":"2021-12-12T22:32:50.793925Z","shell.execute_reply":"2021-12-12T22:32:50.793952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"# Save new predictions to df\nnew_test_preds_df=pd.DataFrame({'Id': new_test_data_index, \n                                'Cover_Type': pred_classes})\n\n# Concatenate with pseudolabels\nfinal_preds=pd.concat([new_test_preds_df, pseudo_label_preds_df])\n\n# Sort by id\nfinal_preds=final_preds.sort_values(by='Id', ascending=True)\n\n# Check format\nfinal_preds.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:32:50.797288Z","iopub.status.idle":"2021-12-12T22:32:50.797619Z","shell.execute_reply.started":"2021-12-12T22:32:50.797448Z","shell.execute_reply":"2021-12-12T22:32:50.797464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save to csv\nfinal_preds.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:32:50.798666Z","iopub.status.idle":"2021-12-12T22:32:50.79902Z","shell.execute_reply.started":"2021-12-12T22:32:50.798822Z","shell.execute_reply":"2021-12-12T22:32:50.798844Z"},"trusted":true},"execution_count":null,"outputs":[]}]}