{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ðŸŒ± The following notebook is a simple implementation of a Gradient Boosted Decision Tree using LightGBM.\nThis Notebooks explains my workflow to implement a ML model from basic or a starting configuration to a more complex implementation </br> \nI'm thinking maybe I will need differenent notebooks to avoid confusion explaining other topics, anyways below the steps for a simple implementation\n\n**Notebook steps**\n1. **[Loading Libraries](#1):** Load all the nesessary python modules. \n2. **[Loading the information into DataFrames](#2):** Take the CSV information provided and creates a Pandas Dataframe.\n3. **[Exploring the train & target variable](#3):** Review the distribution of the targets we are trying to predict.\n4. **[Feature Engineering](#4):** Build some simple features to demonstrate the process.\n5. **[Normalizing the training data](#5):** Normalizing the feature to improve the training process.\n6. **[Setting the Cross Validation strategy](#6):** Simple separation of the dataset into train and test datasets.\n7. **[Training a Machine Learning Model, LGBM](#7):** Construction of a plain LGBM model, Utilize Accuracy to evaluate the model performance.\n    * [7.1 Training the Model using a a simple 80/20 train, validation split](#8)\n    * [7.2 Validating Model results](#9)\n8. **[Training a Machine Learning Model, LGBM](#10):** Construction of a plain LGBM model, Utilize Accuracy to evaluate the model performance.\n    * [8.1 Training the Model using a five (5) fold cross validation loop](#11)\n    * [8.2 Validating CV Model results](#12)\n9. **[Submitting the predictions](#13):** Submit the test dataset prediction for evaluation.\n10. **[Document Results](#14):** In this step I document the Notebook score vs. the LB score as the changes made to the model.\n\nI tried to ducument my functions using the `Google Style Python Docstrings` and explain majority of my code with comments </br>\nI also add the `%%time` wrapper to all my cells to understand how much time the code is taking to execute and improve in future iterations.\n\n\n---","metadata":{}},{"cell_type":"markdown","source":"**Model Objective:** </br> \nIn this competition we are trying to classy the type of cover, based on terrain, soil properties and other parameters, The datasets contains the following variables based on the competion description...\n\n**My Strategy:** </br> \nI really like to start simple and build my models from scratch using other notebooks inspirations, adapting some of the code or my code to something that's easy to understand or follow my logic.\n___","metadata":{}},{"cell_type":"markdown","source":"**Dataset Description**\nThis dataset description is from the original competition used to generate the information...\n\nThe study area includes four wilderness areas located in the Roosevelt National Forest of northern Colorado. Each observation is a 30m x 30m patch. You are asked to predict an integer classification for the forest cover type. The seven types are:\n\n1. Spruce/Fir\n2. Lodgepole Pine\n3. Ponderosa Pine\n4. Cottonwood/Willow\n5. Aspen\n6. Douglas-fir\n7. Krummholz\n\nThe training set (15120 observations) contains both features and the Cover_Type. The test set contains only the features. You must predict the Cover_Type for every row in the test set (565892 observations).\n\nThe study area includes four wilderness areas located in the Roosevelt National Forest of northern Colorado. Each observation is a 30m x 30m patch. You are asked to predict an integer classification for the forest cover type. The seven types are:\n\n1. Spruce/Fir\n2. Lodgepole Pine\n3. Ponderosa Pine\n4. Cottonwood/Willow\n5. Aspen\n6. Douglas-fir\n7. Krummholz\n\nThe training set (15120 observations) contains both features and the Cover_Type. The test set contains only the features. You must predict the Cover_Type for every row in the test set (565892 observations).\n\n**Data Fields**\n* Elevation - Elevation in meters\n* Aspect - Aspect in degrees azimuth\n* Slope - Slope in degrees\n* Horizontal_Distance_To_Hydrology - Horz Dist to nearest surface water features\n* Vertical_Distance_To_Hydrology - Vert Dist to nearest surface water features\n* Horizontal_Distance_To_Roadways - Horz Dist to nearest roadway\n* Hillshade_9am (0 to 255 index) - Hillshade index at 9am, summer solstice\n* Hillshade_Noon (0 to 255 index) - Hillshade index at noon, summer solstice\n* Hillshade_3pm (0 to 255 index) - Hillshade index at 3pm, summer solstice\n* Horizontal_Distance_To_Fire_Points - Horz Dist to nearest wildfire ignition points\n* Wilderness_Area (4 binary columns, 0 = absence or 1 = presence) - Wilderness area designation\n* Soil_Type (40 binary columns, 0 = absence or 1 = presence) - Soil Type designation\n* Cover_Type (7 types, integers 1 to 7) - Forest Cover Type designation\n\n**The wilderness areas are:**\n\n1. Rawah Wilderness Area\n2. Neota Wilderness Area\n3. Comanche Peak Wilderness Area\n4. Cache la Poudre Wilderness Area\n\n**The soil types are:**\n\n1. Cathedral family - Rock outcrop complex, extremely stony.\n2. Vanet - Ratake families complex, very stony.\n3. Haploborolis - Rock outcrop complex, rubbly.\n4. Ratake family - Rock outcrop complex, rubbly.\n5. Vanet family - Rock outcrop complex complex, rubbly.\n6. Vanet - Wetmore families - Rock outcrop complex, stony.\n7. Gothic family.\n8. Supervisor - Limber families complex.\n9. Troutville family, very stony.\n10. Bullwark - Catamount families - Rock outcrop complex, rubbly.\n11. Bullwark - Catamount families - Rock land complex, rubbly.\n12. Legault family - Rock land complex, stony.\n13. Catamount family - Rock land - Bullwark family complex, rubbly.\n14. Pachic Argiborolis - Aquolis complex.\n15. unspecified in the USFS Soil and ELU Survey.\n16. Cryaquolis - Cryoborolis complex.\n17. Gateview family - Cryaquolis complex.\n18. Rogert family, very stony.\n19. Typic Cryaquolis - Borohemists complex.\n20. Typic Cryaquepts - Typic Cryaquolls complex.\n21. Typic Cryaquolls - Leighcan family, till substratum complex.\n22. Leighcan family, till substratum, extremely bouldery.\n23. Leighcan family, till substratum - Typic Cryaquolls complex.\n24. Leighcan family, extremely stony.\n25. Leighcan family, warm, extremely stony.\n26. Granile - Catamount families complex, very stony.\n27. Leighcan family, warm - Rock outcrop complex, extremely stony.\n28. Leighcan family - Rock outcrop complex, extremely stony.\n29. Como - Legault families complex, extremely stony.\n30. Como family - Rock land - Legault family complex, extremely stony.\n31. Leighcan - Catamount families complex, extremely stony.\n32. Catamount family - Rock outcrop - Leighcan family complex, extremely stony.\n33. Leighcan - Catamount families - Rock outcrop complex, extremely stony.\n34. Cryorthents - Rock land complex, extremely stony.\n35. Cryumbrepts - Rock outcrop - Cryaquepts complex.\n36. Bross family - Rock land - Cryumbrepts complex, extremely stony.\n37. Rock outcrop - Cryumbrepts - Cryorthents complex, extremely stony.\n38. Leighcan - Moran families - Cryaquolls complex, extremely stony.\n39. Moran family - Cryorthents - Leighcan family complex, extremely stony.\n40. Moran family - Cryorthents - Rock land complex, extremely stony.\n---","metadata":{}},{"cell_type":"markdown","source":"**My Updates:** </br>\n* 12/05/2021: Created the beginner notebook with simple 80/20 model.\n* 12/11/2021: Added a cross validation loop to the model and comparisons against the baseline.\n* 12/11/2021: Created more features and corrected some of the dataset ranges, to improve model performance.\n* 12/12/2021: Improved the notebook structure, added index and majority of the functions.\n\n**Future Ideas:**\n* 12/12/2021: Add Support to track experiments using Neptune.AI (Working On)\n","metadata":{}},{"cell_type":"markdown","source":"**Inspiration:**\nThanks to all the autors of the following notebooks, I used some of the ideas and concepts construct this version of the analysis.\n* https://www.kaggle.com/c/tabular-playground-series-dec-2021/discussion/293373 (Feature Engineering Ideas)\n* https://www.kaggle.com/c/tabular-playground-series-dec-2021/discussion/293612 (Feature Engineering Ideas)\n* https://www.kaggle.com/odins0n/tps-dec-eda-modeling#Table-of-Contents (Visualizations)\n* https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.htmlhttps://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html (Docstrings)","metadata":{}},{"cell_type":"markdown","source":"<a id='1'></a>\n# 1.0- Loading the Requiered Libraries and Configurations.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-12T22:10:17.436554Z","iopub.execute_input":"2021-12-12T22:10:17.436959Z","iopub.status.idle":"2021-12-12T22:10:17.449144Z","shell.execute_reply.started":"2021-12-12T22:10:17.436879Z","shell.execute_reply":"2021-12-12T22:10:17.448314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Import essential libraries for the modeling process.\n\n# Import visualization libraries...\nimport matplotlib.pyplot as plt\nimport plotly.express as px\n\n# Import the lighgbm model library...\nfrom lightgbm import LGBMClassifier\n\n# Import sklearn train, test and split + evaluation metrics.\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import StratifiedKFold\n\n# Import libraries to maintain the notebook performance.\nimport gc\n\n# Tracking experiments (Future notebook releases).\n# import neptune.new as neptune.\n","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:10:17.453518Z","iopub.execute_input":"2021-12-12T22:10:17.454541Z","iopub.status.idle":"2021-12-12T22:10:19.009923Z","shell.execute_reply.started":"2021-12-12T22:10:17.454509Z","shell.execute_reply":"2021-12-12T22:10:19.009195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# I like to disable my Notebook Warnings.\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:10:19.011411Z","iopub.execute_input":"2021-12-12T22:10:19.01187Z","iopub.status.idle":"2021-12-12T22:10:19.017325Z","shell.execute_reply.started":"2021-12-12T22:10:19.011831Z","shell.execute_reply":"2021-12-12T22:10:19.016637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Define some of the notebook parameters for future experiment replication.\nSEED   = 42","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:10:19.019714Z","iopub.execute_input":"2021-12-12T22:10:19.020328Z","iopub.status.idle":"2021-12-12T22:10:19.028124Z","shell.execute_reply.started":"2021-12-12T22:10:19.020161Z","shell.execute_reply":"2021-12-12T22:10:19.027348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Configure notebook display settings to only use 2 decimal places, tables look nicer.\npd.options.display.float_format = '{:,.2f}'.format\npd.set_option('display.max_columns', 15) \npd.set_option('display.max_rows', 25)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:10:19.02943Z","iopub.execute_input":"2021-12-12T22:10:19.029742Z","iopub.status.idle":"2021-12-12T22:10:19.038002Z","shell.execute_reply.started":"2021-12-12T22:10:19.029706Z","shell.execute_reply":"2021-12-12T22:10:19.037067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='2'></a>\n# 2.0- Loading the Train, Test and Submission Datasets.","metadata":{}},{"cell_type":"code","source":"%%time\n# Function to built a dataframe from a CSV.\ndef import_csv(path):\n    \"\"\"\n    Import CSV to a dataframe using the specified path...\n    \"\"\"\n    df = pd.read_csv(path)\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:10:19.039546Z","iopub.execute_input":"2021-12-12T22:10:19.039816Z","iopub.status.idle":"2021-12-12T22:10:19.048442Z","shell.execute_reply.started":"2021-12-12T22:10:19.039781Z","shell.execute_reply":"2021-12-12T22:10:19.047606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Invoke the 'import_csv' funtion to create the train_df dataframe.\nTRAIN_PATH = '/kaggle/input/tabular-playground-series-dec-2021/train.csv'\ntrain_df = import_csv(TRAIN_PATH)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:10:19.049826Z","iopub.execute_input":"2021-12-12T22:10:19.050076Z","iopub.status.idle":"2021-12-12T22:10:31.199152Z","shell.execute_reply.started":"2021-12-12T22:10:19.050044Z","shell.execute_reply":"2021-12-12T22:10:31.198372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Model Predictions for the Test Dataset and Submission for Scoring.\nTEST_PATH = '/kaggle/input/tabular-playground-series-dec-2021/test.csv'\ntest_df = import_csv(TEST_PATH)\n\nSUB_PATH = '/kaggle/input/tabular-playground-series-dec-2021/sample_submission.csv'\nsubmission_df = import_csv(SUB_PATH)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:10:31.200506Z","iopub.execute_input":"2021-12-12T22:10:31.201251Z","iopub.status.idle":"2021-12-12T22:10:34.44707Z","shell.execute_reply.started":"2021-12-12T22:10:31.201212Z","shell.execute_reply":"2021-12-12T22:10:34.446233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Define the memory optimization funtion and simplify the dataframes.\ndef reduce_mem_usage(df, verbose = True):\n    \"\"\"\n    Takes an input dataframe and optimize the variable types to reduce memory consumption.\n    \"\"\"\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2\n\n    for col in df.columns:\n        col_type = df[col].dtypes\n\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() / 1024**2\n\n    if verbose:\n        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem)) \n        \n    return df","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:10:34.448613Z","iopub.execute_input":"2021-12-12T22:10:34.448892Z","iopub.status.idle":"2021-12-12T22:10:34.459421Z","shell.execute_reply.started":"2021-12-12T22:10:34.448855Z","shell.execute_reply":"2021-12-12T22:10:34.458681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Define the memory optimization funtion and simplify the dataframes.\ntrain_df = reduce_mem_usage(train_df)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:10:34.460811Z","iopub.execute_input":"2021-12-12T22:10:34.461301Z","iopub.status.idle":"2021-12-12T22:10:53.018375Z","shell.execute_reply.started":"2021-12-12T22:10:34.461252Z","shell.execute_reply":"2021-12-12T22:10:53.017573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Define the memory optimization funtion and simplify the dataframes.\ntest_df = reduce_mem_usage(test_df)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:10:53.019562Z","iopub.execute_input":"2021-12-12T22:10:53.019913Z","iopub.status.idle":"2021-12-12T22:10:55.850048Z","shell.execute_reply.started":"2021-12-12T22:10:53.019874Z","shell.execute_reply":"2021-12-12T22:10:55.849277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"___","metadata":{}},{"cell_type":"markdown","source":"<a id='3'></a>\n# 3.0- Explore the Train Dataset and Target Variable.","metadata":{}},{"cell_type":"code","source":"%%time\n# Review the first 5 rows on the dataset to get an idea what we are getting into.\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:10:55.851411Z","iopub.execute_input":"2021-12-12T22:10:55.851835Z","iopub.status.idle":"2021-12-12T22:10:55.87108Z","shell.execute_reply.started":"2021-12-12T22:10:55.851796Z","shell.execute_reply":"2021-12-12T22:10:55.870421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Some statistical metrics.\ntrain_df.describe()","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:10:55.874886Z","iopub.execute_input":"2021-12-12T22:10:55.875318Z","iopub.status.idle":"2021-12-12T22:10:58.899822Z","shell.execute_reply.started":"2021-12-12T22:10:55.875271Z","shell.execute_reply":"2021-12-12T22:10:58.89908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Trying to understand better the Hill Shade features.\ntrain_df[['Hillshade_9am','Hillshade_Noon','Hillshade_3pm']].describe()","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:10:58.901183Z","iopub.execute_input":"2021-12-12T22:10:58.901641Z","iopub.status.idle":"2021-12-12T22:10:59.18533Z","shell.execute_reply.started":"2021-12-12T22:10:58.901605Z","shell.execute_reply":"2021-12-12T22:10:59.184486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Trying to understand Soil_Type15 and Soil_Type7, \n# The model think are key to predict but it's the same value for all the.\ntrain_df[['Soil_Type15','Soil_Type7']].describe()","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:10:59.186833Z","iopub.execute_input":"2021-12-12T22:10:59.187088Z","iopub.status.idle":"2021-12-12T22:10:59.281791Z","shell.execute_reply.started":"2021-12-12T22:10:59.187051Z","shell.execute_reply":"2021-12-12T22:10:59.281026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Print all the variables,described in the dataset, it's just easy to read them this way, to generate more features.\nfor feat in train_df.columns:\n    print(feat)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:10:59.283279Z","iopub.execute_input":"2021-12-12T22:10:59.283784Z","iopub.status.idle":"2021-12-12T22:10:59.298297Z","shell.execute_reply.started":"2021-12-12T22:10:59.283745Z","shell.execute_reply":"2021-12-12T22:10:59.297554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Display some metrics of the dataset, will help to understand memory requirements.\ntrain_df.info(max_cols = 15)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:10:59.300208Z","iopub.execute_input":"2021-12-12T22:10:59.300477Z","iopub.status.idle":"2021-12-12T22:10:59.314465Z","shell.execute_reply.started":"2021-12-12T22:10:59.300442Z","shell.execute_reply":"2021-12-12T22:10:59.31369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Identifying if there is any missing information that needs to be fixed.\ntrain_df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:10:59.315656Z","iopub.execute_input":"2021-12-12T22:10:59.315899Z","iopub.status.idle":"2021-12-12T22:10:59.559418Z","shell.execute_reply.started":"2021-12-12T22:10:59.315867Z","shell.execute_reply":"2021-12-12T22:10:59.558524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Notes:** There is no **Null** or **NaN** values in the dataset, this make it easier to start training a model without gettin into filling missing values...","metadata":{}},{"cell_type":"code","source":"%%time\n# Creates a categorical plot for the target variable, help with the visualization of the target distribution.\ntarget_df = pd.DataFrame(train_df['Cover_Type'].value_counts()).reset_index()\ntarget_df.columns = ['Cover_Type', 'count']\nfig = px.bar(data_frame =target_df, x = 'Cover_Type', y = 'count', color = \"count\", color_continuous_scale = \"Emrld\", width=700, height=400)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:10:59.561012Z","iopub.execute_input":"2021-12-12T22:10:59.561304Z","iopub.status.idle":"2021-12-12T22:10:59.974703Z","shell.execute_reply.started":"2021-12-12T22:10:59.561252Z","shell.execute_reply":"2021-12-12T22:10:59.973666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Review how many samples of if class there is available in the dataset.\ntarget_df.groupby('Cover_Type').sum().reset_index()","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:10:59.975885Z","iopub.execute_input":"2021-12-12T22:10:59.976335Z","iopub.status.idle":"2021-12-12T22:10:59.993643Z","shell.execute_reply.started":"2021-12-12T22:10:59.976296Z","shell.execute_reply":"2021-12-12T22:10:59.992963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n**Notes:** The target distribution in the dataset is unbalanced, we will train a baseline model and then build small improvements over time, in the following sections...","metadata":{}},{"cell_type":"markdown","source":"<a id='4'></a>\n# 4.0- Feature Engineering.\nIn this section we will built some simple function, we will construct everything inside funtions for easy appliucation in the future of what we really need, this way the code is more modular","metadata":{}},{"cell_type":"code","source":"%%time\n# Define fuctions to construct new features for the model.\n\ndef euclidean_dist(df, horz_dist, vert_dist, feature_name):\n    \"\"\"\n    Calculates the Euclidean distance based on horz. and vert. distance...\n    Args:\n        df (DataFrame)    : Input dataframe to add the new feature.\n        horz_dist (str)   : Name of the horizontal distance field in the dataframe.\n        vert_dist (str)   : Name of the vertical distance field in the dataframe.\n        feature_name (str): Name of the new created field in the dataframe.\n    \n    Returns:\n        df (DataFrame)    : Dataframe populated with the new feature.\n    \"\"\"\n    \n    df[feature_name] = np.sqrt(df[horz_dist] ** 2 + df[vert_dist] ** 2)\n    return df\n\n\ndef correct_azimut(df):\n    \"\"\"\n    Corrects the azimut to be withing 0 - 360 degrees\n    Args:\n        df (DataFrame): Input dataframe that we need to correct.\n    \n    Returns:\n        df (DataFrame): Dataframe with the feature ranges corrected.\n    \"\"\"\n    \n    df[\"Aspect\"][df[\"Aspect\"] < 0] += 360\n    df[\"Aspect\"][df[\"Aspect\"] > 359] -= 360\n    return df\n\n\ndef correct_hillshade(df):\n    \"\"\"\n    Corrects the Hillshade values from 0 - 255\n    Args:\n        df (DataFrame): Input dataframe that we need to correct.\n    \n    Returns:\n        df (DataFrame): Dataframe with the feature ranges corrected.\n    \"\"\"\n    \n    for feat in df.columns:\n        if 'Hillshade' in feat:\n            train_df.loc[train_df[feat] < 0, feat] = 0\n            train_df.loc[train_df[feat] > 255, feat] = 255\n    return df\n\n\ndef count_diversity(df, feature_group_name = 'soil_type'):\n    \"\"\"\n    Counts the diversity for example different types of soils in the training row\n    Args:\n        df (Dataframe): Input dataframe for the feature creation.\n        feature_group_name (str): Name of the group in the dataframe that we will be counting.\n        \n    Returns:\n        df (Dataframe): Dataframe populated with the new feature.\n    \"\"\"\n    \n    features_group = [x for x in df.columns if x.startswith(feature_group_name)]\n    df[feature_group_name + \"_Count\"] = df[features_group].sum(axis=1)\n    return df\n\n\ndef remove_cover_type(df, cover_value = 5):\n    \"\"\"\n    Remove the selected cover type, Because min. amount of train information.\n    \n    Args:\n        df (Dataframe): Input dataframe to be processed.\n        cover_value (int): Value of the cover that needs to be removed from the DataFrame.\n    \n    Returns:\n        df (Dataframe): Dataframe without the selected cover_value.\n    \"\"\"\n    \n    df = df[df['Cover_Type'] != cover_value]\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:10:59.99507Z","iopub.execute_input":"2021-12-12T22:10:59.99537Z","iopub.status.idle":"2021-12-12T22:11:00.005658Z","shell.execute_reply.started":"2021-12-12T22:10:59.995334Z","shell.execute_reply":"2021-12-12T22:11:00.004844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Build new features for the model. (Training Set).\ntrain_df = euclidean_dist(train_df, 'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology', 'Euclidean_Distance_To_Hydrology')\n# Improve the values of some of the futures for the model.\ntrain_df = correct_azimut(train_df)\ntrain_df = correct_hillshade(train_df)\n# Build other features.\ntrain_df = count_diversity(train_df, feature_group_name = 'Soil_Type')\ntrain_df = count_diversity(train_df, feature_group_name = 'Wilderness_Area')\n\n# Remove conver_type = 5 because there is only 1 example.\ntrain_df = remove_cover_type(train_df)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:11:00.006918Z","iopub.execute_input":"2021-12-12T22:11:00.007679Z","iopub.status.idle":"2021-12-12T22:11:01.161748Z","shell.execute_reply.started":"2021-12-12T22:11:00.00764Z","shell.execute_reply":"2021-12-12T22:11:01.160956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Build new features for the model (Test Set).\ntest_df = euclidean_dist(test_df, 'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology', 'Euclidean_Distance_To_Hydrology')\n# Improve the values of some of the futures for the model.\ntest_df = correct_azimut(test_df)\ntest_df = correct_hillshade(test_df)\n# Build other features.\ntest_df = count_diversity(test_df, feature_group_name = 'Soil_Type')\ntest_df = count_diversity(test_df, feature_group_name = 'Wilderness_Area')","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:11:01.162932Z","iopub.execute_input":"2021-12-12T22:11:01.163658Z","iopub.status.idle":"2021-12-12T22:11:01.313882Z","shell.execute_reply.started":"2021-12-12T22:11:01.163617Z","shell.execute_reply":"2021-12-12T22:11:01.312993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Review some of the features created in the previous step.\ntrain_df.sample(10)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:11:01.315036Z","iopub.execute_input":"2021-12-12T22:11:01.315303Z","iopub.status.idle":"2021-12-12T22:11:01.443866Z","shell.execute_reply.started":"2021-12-12T22:11:01.315251Z","shell.execute_reply":"2021-12-12T22:11:01.443145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"___","metadata":{}},{"cell_type":"markdown","source":"<a id='5'></a>\n# 5.0- Preparing the Dataset for the Training Step.\nIn this section I organize the dataset for the training funtions, for example I will create a list of the futures that are important as identify the training variable. </br>\nI will also focus on identifying categorical vs numerical features to improve the model performance as also apply some normalization to the numerical variables.","metadata":{}},{"cell_type":"code","source":"%%time\n# Prepare the dataset for training, remove some of the not valuable variables.\nremove = ['Id', 'Cover_Type', 'Soil_Type15', 'Soil_Type7'] # Removes the Target from the train dataset.\nfeatures = [values for values in train_df.columns if values not in remove]\nX = train_df[features]\ny = train_df['Cover_Type']\n\nX_test = test_df[features]","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:11:01.445083Z","iopub.execute_input":"2021-12-12T22:11:01.447452Z","iopub.status.idle":"2021-12-12T22:11:01.63742Z","shell.execute_reply.started":"2021-12-12T22:11:01.447411Z","shell.execute_reply":"2021-12-12T22:11:01.636556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Identify categorical and numerical features.\nNUMBER_OF_UNIQUE = 25\ncategorical_features = [col for col in features if train_df[col].nunique() < NUMBER_OF_UNIQUE]\nnumerical_features = [col for col in features if train_df[col].nunique() >= NUMBER_OF_UNIQUE]","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:11:01.638909Z","iopub.execute_input":"2021-12-12T22:11:01.639384Z","iopub.status.idle":"2021-12-12T22:11:04.200473Z","shell.execute_reply.started":"2021-12-12T22:11:01.639342Z","shell.execute_reply":"2021-12-12T22:11:04.199676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Print a list of all the categorical features identified.\nprint(categorical_features)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:11:04.201683Z","iopub.execute_input":"2021-12-12T22:11:04.202393Z","iopub.status.idle":"2021-12-12T22:11:04.207947Z","shell.execute_reply.started":"2021-12-12T22:11:04.202355Z","shell.execute_reply":"2021-12-12T22:11:04.207267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Deletes the original dataframes to save memory\ndel train_df # Deletes the train_df dataframe\ndel test_df  # Deletes the test_df dataframe\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:11:04.209208Z","iopub.execute_input":"2021-12-12T22:11:04.209636Z","iopub.status.idle":"2021-12-12T22:11:04.41326Z","shell.execute_reply.started":"2021-12-12T22:11:04.2096Z","shell.execute_reply":"2021-12-12T22:11:04.412357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Create a funtion to normalize some of the numeric variables.\n# ...","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:11:04.415002Z","iopub.execute_input":"2021-12-12T22:11:04.415414Z","iopub.status.idle":"2021-12-12T22:11:04.421131Z","shell.execute_reply.started":"2021-12-12T22:11:04.415364Z","shell.execute_reply":"2021-12-12T22:11:04.420349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='6'></a>\n# 6.0- Setting a Simple Cross Validation Strategy ","metadata":{}},{"cell_type":"code","source":"# Split the dataset into train and validation for model first calibration...\nX_train, X_val, y_train, y_val = train_test_split(X, y, random_state = SEED, test_size = 0.2)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:11:04.422555Z","iopub.execute_input":"2021-12-12T22:11:04.422975Z","iopub.status.idle":"2021-12-12T22:11:06.14137Z","shell.execute_reply.started":"2021-12-12T22:11:04.422942Z","shell.execute_reply":"2021-12-12T22:11:06.140512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"<a id='7'></a>\n# 7.0- Training a Machine Learning Model, LGBMTraining a Machine Learning Model, LGBM","metadata":{}},{"cell_type":"code","source":"%%time\n# Define some of the model parameters for future experiment replication.\nESTIMATORS = 10 # Default value for the training 2048\nLR = 0.1\nVERBOSE = 16\nEARLY_STOPPING = 250","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:11:06.142757Z","iopub.execute_input":"2021-12-12T22:11:06.143004Z","iopub.status.idle":"2021-12-12T22:11:06.151002Z","shell.execute_reply.started":"2021-12-12T22:11:06.142973Z","shell.execute_reply":"2021-12-12T22:11:06.149986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Define the LGBM most popular model parameters for the training step.\n\nlgb_params = {'n_estimators'     : ESTIMATORS,      # Number of boosting iterations.\n              'random_state'     : SEED,            # Random seed initilizer for the model, helps to replicate the experiments.\n              'learning_rate'    : LR,              # The model learning rate.\n              'subsample'        : 0.95,            # Row subsample from the dataset, like feature_fraction, but this will randomly select part of data without resampling\n              'subsample_freq'   : 1,               # Use or not subsample frequency.\n              'colsample_bytree' : 0.75,            # LightGBM will randomly select a subset of features on each iteration (tree).\n              'reg_alpha'        : 0.5,             # L1 regularization.\n              'reg_lambda'       : 0.5,             # L2 regularization.\n              'min_child_weight' : 1e-3,            # Minimal sum hessian in one leaf, it can be used to deal with over-fitting.\n              'min_child_samples': 32,              # Minimal number of data in one leaf. Can be used to deal with over-fitting.\n              'objective'        : 'multiclass',    # Softmax objective function.\n              'metric'           : 'multi_logloss', # Log loss for multi-class classification.\n              'device_type'      : 'gpu',\n             }   ","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:11:06.152677Z","iopub.execute_input":"2021-12-12T22:11:06.153599Z","iopub.status.idle":"2021-12-12T22:11:06.163219Z","shell.execute_reply.started":"2021-12-12T22:11:06.153548Z","shell.execute_reply":"2021-12-12T22:11:06.162268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='8'></a>\n# 7.1- Training a Simple Model, Gradient Boosted Decision Tree using LightGBM.\n","metadata":{}},{"cell_type":"code","source":"%%time\n# Train a LGBM model using the defined parameterst 'lgb_params'\nlgb_classifier = LGBMClassifier(**lgb_params)\n\nlgb_classifier.fit(X_train, \n                   y_train, \n                   eval_set = [(X_val, y_val)],\n                   categorical_feature = categorical_features,\n                   early_stopping_rounds = EARLY_STOPPING, # will stop training if one metric of one validation data doesnâ€™t improve in last early_stopping_round rounds\n                   verbose = VERBOSE, # Controls the level of LightGBMâ€™s verbosity\n                  )","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:11:06.164852Z","iopub.execute_input":"2021-12-12T22:11:06.165245Z","iopub.status.idle":"2021-12-12T22:11:31.139516Z","shell.execute_reply.started":"2021-12-12T22:11:06.165209Z","shell.execute_reply":"2021-12-12T22:11:31.138035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='9'></a>\n# 7.2- Validating Model Results.","metadata":{}},{"cell_type":"code","source":"%%time\n# Review top 10 features importance for the model.\n# Sorted(zip(clf.feature_importances_, X.columns), reverse=True)\nfeature_imp = pd.DataFrame(sorted(zip(lgb_classifier.feature_importances_,X.columns)), columns=['Value','Feature'])","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:11:31.140759Z","iopub.execute_input":"2021-12-12T22:11:31.14124Z","iopub.status.idle":"2021-12-12T22:11:31.150545Z","shell.execute_reply.started":"2021-12-12T22:11:31.141201Z","shell.execute_reply":"2021-12-12T22:11:31.149854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Review the top five(5) Feature the model believes are important\nprint(feature_imp.head(10), '\\n')\nprint(feature_imp.tail(10))","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:11:31.151876Z","iopub.execute_input":"2021-12-12T22:11:31.152715Z","iopub.status.idle":"2021-12-12T22:11:31.166719Z","shell.execute_reply.started":"2021-12-12T22:11:31.152676Z","shell.execute_reply":"2021-12-12T22:11:31.165832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Results of the model predictions and evaluation against true target...\npreds_valid = lgb_classifier.predict(X_val)\naccuracy = accuracy_score(y_val, preds_valid)\nprint(\"Mean Accuracy :\", accuracy)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:11:31.168467Z","iopub.execute_input":"2021-12-12T22:11:31.168864Z","iopub.status.idle":"2021-12-12T22:11:33.137725Z","shell.execute_reply.started":"2021-12-12T22:11:31.168826Z","shell.execute_reply":"2021-12-12T22:11:33.136956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **Mean Accuracy : 0.96136375** ... Simple baseline model, No Features outside the default variables...\n* **Mean Accuracy : 0.96140750** ... Added Euclidean distance as a Feature to the model...\n* **Mean Accuracy : 0.96140750** ... Added Euclidean distance as a Feature + Corrections to the ranges of the Azimut and the Hillshade...\n* **Mean Accuracy : 0.96057125** ... Added Euclidean distance as a Feature + Corrections to the ranges of the Azimut and the Hillshade, New Train Parameters 128 Estimators...\n* **Mean Accuracy : 0.96060625** ... Added Euclidean distance as a Feature + Corrections to the ranges of the Azimut and the Hillshade, New Train Parameters 128 Estimators, Removed Soil 17 & 7...\n* **Mean Accuracy : 0.96217000** ... Added Euclidean distance as a Feature + Corrections to the ranges of the Azimut and the Hillshade, New Train Parameters 2048 Estimators, Removed Soil 17 & 7...","metadata":{}},{"cell_type":"code","source":"%%time\n# Deletes the train and validation dataframes created for the simple training to save memory\ndel X_train, \ndel X_val, \ndel y_train, \ndel y_val\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:11:33.139151Z","iopub.execute_input":"2021-12-12T22:11:33.139464Z","iopub.status.idle":"2021-12-12T22:11:33.284229Z","shell.execute_reply.started":"2021-12-12T22:11:33.139425Z","shell.execute_reply":"2021-12-12T22:11:33.283475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"<a id='10'></a>\n# 8.0- Training a Machine Learning Model, LGBM","metadata":{}},{"cell_type":"code","source":"%%time\n# Define some of the cross validation model parameters\nFOLDS = 20","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:11:33.285418Z","iopub.execute_input":"2021-12-12T22:11:33.286206Z","iopub.status.idle":"2021-12-12T22:11:33.29445Z","shell.execute_reply.started":"2021-12-12T22:11:33.286163Z","shell.execute_reply":"2021-12-12T22:11:33.29369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='11'></a>\n# 8.1- Training a Simple Model in a Cross Validation Loop, Gradient Boosted Decision Tree using LightGBM.","metadata":{"execution":{"iopub.status.busy":"2021-12-11T23:58:53.116261Z","iopub.execute_input":"2021-12-11T23:58:53.116603Z","iopub.status.idle":"2021-12-11T23:58:53.121655Z","shell.execute_reply.started":"2021-12-11T23:58:53.11657Z","shell.execute_reply":"2021-12-11T23:58:53.120515Z"}}},{"cell_type":"code","source":"%%time\n# Creates a Cross validation loop\n\nfolds = StratifiedKFold(n_splits = FOLDS, shuffle = True, random_state = SEED)\n\noof_preds = np.zeros(X.shape[0])\nsub_preds = np.zeros(X_test.shape[0])\n\nfor n_fold, (trn_idx, val_idx) in enumerate(folds.split(X, y)):\n    trn_x, trn_y = X[features].iloc[trn_idx], y.iloc[trn_idx]\n    val_x, val_y = X[features].iloc[val_idx], y.iloc[val_idx]\n    \n    clf = LGBMClassifier(**lgb_params)\n    clf.fit(trn_x, \n            trn_y, \n            eval_set = [(val_x, val_y)],\n            categorical_feature = categorical_features,\n            verbose=VERBOSE, \n            early_stopping_rounds=EARLY_STOPPING)\n    \n    oof_preds[val_idx] = clf.predict(val_x, num_iteration=clf.best_iteration_)\n    sub_preds += clf.predict(X_test[features], num_iteration=clf.best_iteration_) / folds.n_splits\n    \n    print('Fold %2d Accuracy Score: %.6f' % (n_fold + 1, accuracy_score(val_y, oof_preds[val_idx])))\n    del trn_x, trn_y, val_x, val_y\n    gc.collect()\n\nprint('\\n')    \nprint('Avg. Accuracy Score %.6f' % accuracy_score(y, oof_preds))","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:11:33.295923Z","iopub.execute_input":"2021-12-12T22:11:33.296582Z","iopub.status.idle":"2021-12-12T22:22:05.504879Z","shell.execute_reply.started":"2021-12-12T22:11:33.296536Z","shell.execute_reply":"2021-12-12T22:22:05.503998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='12'></a>\n# 8.2- Validating CV Model Results.","metadata":{}},{"cell_type":"code","source":"%%time\n# Review top 10 features importance for the model.\n# Sorted(zip(clf.feature_importances_, X.columns), reverse=True)\nfeature_imp_cv = pd.DataFrame(sorted(zip(clf.feature_importances_,X.columns)), columns=['Value','Feature'])","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:22:05.510024Z","iopub.execute_input":"2021-12-12T22:22:05.510258Z","iopub.status.idle":"2021-12-12T22:22:05.518741Z","shell.execute_reply.started":"2021-12-12T22:22:05.510228Z","shell.execute_reply":"2021-12-12T22:22:05.517963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Review the top five(5) Feature the model believes are important\nprint(feature_imp_cv.head(10), '\\n')\nprint(feature_imp_cv.tail(10))","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:22:05.520237Z","iopub.execute_input":"2021-12-12T22:22:05.520883Z","iopub.status.idle":"2021-12-12T22:22:05.534818Z","shell.execute_reply.started":"2021-12-12T22:22:05.520839Z","shell.execute_reply":"2021-12-12T22:22:05.534004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"___","metadata":{}},{"cell_type":"markdown","source":"<a id='13'></a>\n# 9.0 - Submitting the Predictions to Kaggle...","metadata":{}},{"cell_type":"code","source":"%%time\n# Calculate the predictions using the 80/20 train / val model...\npreds_test = lgb_classifier.predict(X_test[features])\n\n# Submit predicitons from the CV trained Model...\nsubmission_df['Cover_Type'] = preds_test\nsubmission_df.to_csv('simple_sub_v12122021.csv', index = None)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:22:05.536231Z","iopub.execute_input":"2021-12-12T22:22:05.536994Z","iopub.status.idle":"2021-12-12T22:22:09.73157Z","shell.execute_reply.started":"2021-12-12T22:22:05.536948Z","shell.execute_reply":"2021-12-12T22:22:09.730764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Submit predicitons from the CV trained Model...\nsub_preds = sub_preds.astype(np.int)\nsubmission_df['Cover_Type'] = sub_preds\nsubmission_df.to_csv('cv_sub_v12112021.csv', index = None)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:22:09.733308Z","iopub.execute_input":"2021-12-12T22:22:09.733863Z","iopub.status.idle":"2021-12-12T22:22:11.480312Z","shell.execute_reply.started":"2021-12-12T22:22:09.733823Z","shell.execute_reply":"2021-12-12T22:22:11.478602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='14'></a>\n# 10.0- Results Documentation...\nI'm planning to add a Google Sheets that containts the results of all the iterations completed with this models as the modification added and ideas for improvement...\n...","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}