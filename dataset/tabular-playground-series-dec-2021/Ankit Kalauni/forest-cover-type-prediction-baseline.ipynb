{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\npd.set_option('max_columns',None)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import boxcox, yeojohnson, skew, kurtosis\nfrom imblearn.over_sampling import SVMSMOTE\nimport gc\nfrom tqdm import tqdm\nimport os\nfrom keras import layers\nnp.errstate(divide='ignore')\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\nimport tensorflow as tf\nimport keras\nfrom keras.layers import Input, Dense, BatchNormalization, Concatenate, Dropout\nfrom tensorflow.keras.utils import to_categorical\nimport math\nimport keras.backend as K\nfrom tensorflow.keras.optimizers import Nadam, Adam\n\nfrom tensorflow.keras.optimizers import Adam, Adamax\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.layers import Concatenate\nfrom tensorflow.keras.layers import Input, BatchNormalization\nfrom tensorflow.keras.layers import Dense, Dropout, Multiply","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-14T12:29:49.094825Z","iopub.execute_input":"2021-12-14T12:29:49.095598Z","iopub.status.idle":"2021-12-14T12:29:49.103852Z","shell.execute_reply.started":"2021-12-14T12:29:49.095557Z","shell.execute_reply":"2021-12-14T12:29:49.103184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"debug = False\nplot = False\n\ntrain = pd.read_csv('../input/tabular-playground-series-dec-2021/train.csv')\ntest = pd.read_csv('../input/tabular-playground-series-dec-2021/test.csv')\n\n\n# plabel = pd.read_csv('../input/tps12-pseudolabels/tps12-pseudolabels_v2.csv')\n# df = pd.read_csv('../input/bts-12-external-data/covtype.data',delimiter=',')\n\n# df.columns = train.columns.tolist()\n# train = pd.concat([train,plabel],axis=0)\n\ntrain.drop(['Id'], axis=1, inplace=True)\n\ntrain = train[train['Cover_Type'] !=5].reset_index(drop=True)\n\n\nif debug:\n    train = train.sample(10000)\n    test = test.sample(10000)\n\n\ny = train['Cover_Type']\ntrain.drop(['Cover_Type'], axis=1, inplace=True)\ntest.drop(['Id'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-14T11:51:57.244449Z","iopub.execute_input":"2021-12-14T11:51:57.244646Z","iopub.status.idle":"2021-12-14T11:52:22.363997Z","shell.execute_reply.started":"2021-12-14T11:51:57.244622Z","shell.execute_reply":"2021-12-14T11:52:22.363241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#reduce memory\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df\n","metadata":{"execution":{"iopub.status.busy":"2021-12-14T11:52:22.365179Z","iopub.execute_input":"2021-12-14T11:52:22.365749Z","iopub.status.idle":"2021-12-14T11:52:22.377427Z","shell.execute_reply.started":"2021-12-14T11:52:22.36571Z","shell.execute_reply":"2021-12-14T11:52:22.376761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)","metadata":{"execution":{"iopub.status.busy":"2021-12-14T11:52:22.379228Z","iopub.execute_input":"2021-12-14T11:52:22.379953Z","iopub.status.idle":"2021-12-14T11:52:41.530236Z","shell.execute_reply.started":"2021-12-14T11:52:22.379917Z","shell.execute_reply":"2021-12-14T11:52:41.529332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"to_drop = ['Soil_Type15','Soil_Type7','Soil_Type1']\n\ntrain.drop(to_drop, inplace=True, axis=1)\ntest.drop(to_drop, inplace=True, axis=1)\n\nfrom sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\ny = pd.DataFrame(encoder.fit_transform(y.values.reshape(-1,1)), columns=['Cover_Type'])","metadata":{"execution":{"iopub.status.busy":"2021-12-14T11:52:41.531582Z","iopub.execute_input":"2021-12-14T11:52:41.531911Z","iopub.status.idle":"2021-12-14T11:52:41.987542Z","shell.execute_reply.started":"2021-12-14T11:52:41.531873Z","shell.execute_reply":"2021-12-14T11:52:41.98594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"markdown","source":"![](https://www.photopills.com/sites/default/files/tutorials/2014/2-azimuth-elevation.jpg)","metadata":{}},{"cell_type":"code","source":"horizontal_cols = ['Horizontal_Distance_To_Hydrology','Horizontal_Distance_To_Roadways','Horizontal_Distance_To_Fire_Points']\nhillshade_cols = ['Hillshade_9am','Hillshade_Noon','Hillshade_3pm']\nwilderness_cols = ['Wilderness_Area1','Wilderness_Area2','Wilderness_Area3','Wilderness_Area4']\nSoil_cols = ['Soil_Type2', 'Soil_Type3', 'Soil_Type4', 'Soil_Type5',\n       'Soil_Type6', 'Soil_Type8', 'Soil_Type9', 'Soil_Type10', 'Soil_Type11',\n       'Soil_Type12', 'Soil_Type13', 'Soil_Type14', 'Soil_Type16',\n       'Soil_Type17', 'Soil_Type18', 'Soil_Type19', 'Soil_Type20',\n       'Soil_Type21', 'Soil_Type22', 'Soil_Type23', 'Soil_Type24',\n       'Soil_Type25', 'Soil_Type26', 'Soil_Type27', 'Soil_Type28',\n       'Soil_Type29', 'Soil_Type30', 'Soil_Type31', 'Soil_Type32',\n       'Soil_Type33', 'Soil_Type34', 'Soil_Type35', 'Soil_Type36',\n       'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'Soil_Type40']","metadata":{"execution":{"iopub.status.busy":"2021-12-14T12:11:24.332069Z","iopub.execute_input":"2021-12-14T12:11:24.332623Z","iopub.status.idle":"2021-12-14T12:11:24.33848Z","shell.execute_reply.started":"2021-12-14T12:11:24.332584Z","shell.execute_reply":"2021-12-14T12:11:24.337775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if plot:\n    for col in horizontal_cols:\n        sns.kdeplot(data=train,x=col, fill=True, color='red',alpha=0.5)\n        sns.kdeplot(data=test,x=col, fill=True, color = 'blue',alpha=0.5)\n        plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-14T11:52:41.997257Z","iopub.execute_input":"2021-12-14T11:52:41.998091Z","iopub.status.idle":"2021-12-14T11:52:42.009726Z","shell.execute_reply.started":"2021-12-14T11:52:41.998051Z","shell.execute_reply":"2021-12-14T11:52:42.00887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if plot:\n    for col in hillshade_cols:\n        sns.kdeplot(data=train,x=col, fill=True, color='red',alpha=0.5)\n        sns.kdeplot(data=test,x=col, fill=True, color = 'blue',alpha=0.5)\n        plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-14T11:52:42.010982Z","iopub.execute_input":"2021-12-14T11:52:42.011405Z","iopub.status.idle":"2021-12-14T11:52:42.019836Z","shell.execute_reply.started":"2021-12-14T11:52:42.011368Z","shell.execute_reply":"2021-12-14T11:52:42.019142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(train.head(4))\ntest.head(4)","metadata":{"execution":{"iopub.status.busy":"2021-12-14T11:52:42.021412Z","iopub.execute_input":"2021-12-14T11:52:42.021675Z","iopub.status.idle":"2021-12-14T11:52:42.074646Z","shell.execute_reply.started":"2021-12-14T11:52:42.021641Z","shell.execute_reply":"2021-12-14T11:52:42.073994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"0 - Spruce/Fir\n\n1 - Lodgepole Pine\n\n2 - Ponderosa Pine\n\n3 - Cottonwood/Willow\n\n4 - Douglas-fir\n\n5 - Krummholz","metadata":{}},{"cell_type":"code","source":"train.isnull().sum().sum()","metadata":{"execution":{"iopub.status.busy":"2021-12-14T11:52:42.077637Z","iopub.execute_input":"2021-12-14T11:52:42.07805Z","iopub.status.idle":"2021-12-14T11:52:42.300659Z","shell.execute_reply.started":"2021-12-14T11:52:42.078003Z","shell.execute_reply":"2021-12-14T11:52:42.299853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![](https://miro.medium.com/max/814/0*_9ljPf7RbVI5cVdG.png)","metadata":{}},{"cell_type":"code","source":"def safe_log10(x, eps=1e-10):\n    result = np.where(x > eps, x, -10)\n    np.log10(result, out=result, where=result > 0)\n    return result","metadata":{"execution":{"iopub.status.busy":"2021-12-14T11:52:42.301934Z","iopub.execute_input":"2021-12-14T11:52:42.302687Z","iopub.status.idle":"2021-12-14T11:52:42.307771Z","shell.execute_reply.started":"2021-12-14T11:52:42.302649Z","shell.execute_reply":"2021-12-14T11:52:42.307082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#['original','log1p','sqrt','crbt','boxcox','yeojohnson']\nfrom numpy import log1p\n\nto_trans = {'Elevation':'yeojohnson','Aspect':'yeojohnson','Slope':'yeojohnson','Horizontal_Distance_To_Hydrology':'yeojohnson',\n            'Vertical_Distance_To_Hydrology':'yeojohnson','Horizontal_Distance_To_Roadways':'yeojohnson','Hillshade_9am':'yeojohnson',\n           'Hillshade_Noon':'yeojohnson','Hillshade_3pm':'yeojohnson','Horizontal_Distance_To_Fire_Points':'yeojohnson'}\n\nto_trans_df = pd.DataFrame.from_dict({'keys':to_trans.keys(), 'values':to_trans.values()})\n\nfor i in to_trans_df['keys']:\n    train[i] = locals()[to_trans_df[to_trans_df['keys'] == i]['values'].values[0]](train[i])[0]\n    test[i] = locals()[to_trans_df[to_trans_df['keys'] == i]['values'].values[0]](test[i])[0]\n    print(i,'Done!')","metadata":{"execution":{"iopub.status.busy":"2021-12-14T11:52:42.309047Z","iopub.execute_input":"2021-12-14T11:52:42.309357Z","iopub.status.idle":"2021-12-14T11:53:32.792736Z","shell.execute_reply.started":"2021-12-14T11:52:42.309317Z","shell.execute_reply":"2021-12-14T11:53:32.791911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#adding new columns and iterating over model to check the performance\ndef euclidean(a,b):\n    dis = np.sqrt((a.astype('int64')**2) + (b.astype('int64')**2))\n    \n    return dis\n\ndef aspect(x):\n    if x>359:\n        return x-360\n    elif x<0:\n        return x+360\n    else:\n        return x\n\ndef manhattan(a,b):\n    dis = abs(a.astype('int64')) + abs(b.astype('int64'))\n    \n    return dis\n\n#================================= forest to Hydrology Euclidean ===================================================\ntrain['ForestHydrology_euclidean'] = euclidean(train['Horizontal_Distance_To_Hydrology'],train['Vertical_Distance_To_Hydrology'])\ntest['ForestHydrology_euclidean'] = euclidean(test['Horizontal_Distance_To_Hydrology'],test['Vertical_Distance_To_Hydrology'])\n\n#================================= forest to Hydrology manhattan ===================================================\ntrain['ForestHydrology_manhattan'] = manhattan(train['Horizontal_Distance_To_Hydrology'],train['Vertical_Distance_To_Hydrology'])\ntest['ForestHydrology_manhattan'] = manhattan(test['Horizontal_Distance_To_Hydrology'],test['Vertical_Distance_To_Hydrology'])\n\n#================================= Roadway to Hydrology pytha ===================================================\ntrain['RoadwayHydrology_pytha'] = euclidean(train['Horizontal_Distance_To_Hydrology'],train['Horizontal_Distance_To_Roadways'])\ntest['RoadwayHydrology_pytha'] = euclidean(test['Horizontal_Distance_To_Hydrology'],test['Horizontal_Distance_To_Roadways'])\n\n#================================= Roadway to Fire_Points pytha ===================================================\ntrain['RoadwayFirepoints_pytha'] = euclidean(train['Horizontal_Distance_To_Fire_Points'],train['Horizontal_Distance_To_Roadways'])\ntest['RoadwayFirepoints_pytha'] = euclidean(test['Horizontal_Distance_To_Fire_Points'],test['Horizontal_Distance_To_Roadways'])\n\n#================================= Hydrology to Fire Pytha pytha ===================================================\ntrain['FirepointsHydrology_pytha'] = euclidean(train['Horizontal_Distance_To_Hydrology'],train['Horizontal_Distance_To_Fire_Points'])\ntest['FirepointsHydrology_pytha'] = euclidean(test['Horizontal_Distance_To_Hydrology'],test['Horizontal_Distance_To_Fire_Points'])\n\n\ntrain['Horizontal_sum'] = train[horizontal_cols].astype('int64').sum(axis=1)\ntest['Horizontal_sum'] = test[horizontal_cols].astype('int64').sum(axis=1)\n\ntrain['Aspect'] = train['Aspect'].apply(aspect)\ntest['Aspect'] = test['Aspect'].apply(aspect)\n\neu = ['ForestHydrology_euclidean','RoadwayHydrology_pytha','RoadwayFirepoints_pytha','FirepointsHydrology_pytha']\n\ntrain['Horizontal_min'] = train[horizontal_cols].min(axis=1)\ntrain['Horizontal_mean'] = train[horizontal_cols].mean(axis=1)\ntrain['Horizontal_max'] = train[horizontal_cols].max(axis=1)\n\ntest['Horizontal_min'] = test[horizontal_cols].min(axis=1)\ntest['Horizontal_mean'] = test[horizontal_cols].mean(axis=1)\ntest['Horizontal_max'] = test[horizontal_cols].max(axis=1)\n\ntrain['eu_min'] = train[eu].min(axis=1)\ntrain['eu_mean'] = train[eu].mean(axis=1)\ntrain['eu_max'] = train[eu].max(axis=1)\n\ntest['eu_min'] = test[eu].min(axis=1)\ntest['eu_mean'] = test[eu].mean(axis=1)\ntest['eu_max'] = test[eu].max(axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-12-14T11:53:32.795182Z","iopub.execute_input":"2021-12-14T11:53:32.795652Z","iopub.status.idle":"2021-12-14T11:53:38.305719Z","shell.execute_reply.started":"2021-12-14T11:53:32.795606Z","shell.execute_reply":"2021-12-14T11:53:38.305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape, y.shape","metadata":{"execution":{"iopub.status.busy":"2021-12-14T11:53:38.306965Z","iopub.execute_input":"2021-12-14T11:53:38.307239Z","iopub.status.idle":"2021-12-14T11:53:38.313008Z","shell.execute_reply.started":"2021-12-14T11:53:38.307206Z","shell.execute_reply":"2021-12-14T11:53:38.312316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fe(df):\n    ### source: https://www.kaggle.com/c/tabular-playground-series-dec-2021/discussion/293373\n    df.loc[df[\"Hillshade_9am\"] < 0, \"Hillshade_9am\"] = 0\n    df.loc[df[\"Hillshade_Noon\"] < 0, \"Hillshade_Noon\"] = 0\n    df.loc[df[\"Hillshade_3pm\"] < 0, \"Hillshade_3pm\"] = 0\n    df.loc[df[\"Hillshade_9am\"] > 255, \"Hillshade_9am\"] = 255\n    df.loc[df[\"Hillshade_Noon\"] > 255, \"Hillshade_Noon\"] = 255\n    df.loc[df[\"Hillshade_3pm\"] > 255, \"Hillshade_3pm\"] = 255\n    ########\n    return df\n\ntrain = fe(train)\ntest = fe(test)","metadata":{"execution":{"iopub.status.busy":"2021-12-14T11:53:38.314507Z","iopub.execute_input":"2021-12-14T11:53:38.315053Z","iopub.status.idle":"2021-12-14T11:53:38.590209Z","shell.execute_reply.started":"2021-12-14T11:53:38.314997Z","shell.execute_reply":"2021-12-14T11:53:38.589443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.isnull().sum().sum(), train.shape, test.shape, y.shape","metadata":{"execution":{"iopub.status.busy":"2021-12-14T11:53:38.59158Z","iopub.execute_input":"2021-12-14T11:53:38.591821Z","iopub.status.idle":"2021-12-14T11:53:38.911319Z","shell.execute_reply.started":"2021-12-14T11:53:38.591788Z","shell.execute_reply":"2021-12-14T11:53:38.91038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"to_ignore = ('id','target','Cover_Type','Wilderness_Area','Soil_Type')\n\ncat_cols = [col for col in train.columns if not col.startswith(to_ignore)]\n\ntrain['wilderness_encoder'] = np.argmax(train[wilderness_cols].values,axis=1)\ntest['wilderness_encoder'] = np.argmax(test[wilderness_cols].values,axis=1)\n\ntrain['soil_encoder'] = np.argmax(train[Soil_cols].values,axis=1)\ntest['soil_encoder'] = np.argmax(test[Soil_cols].values,axis=1)\n\ncat_cols.append('soil_encoder')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\ntest['Cover_Type'] = -1\n\ncat_data = pd.concat([train.join(y),test],axis=0)\n\nfor feat in tqdm(cat_data[cat_cols].columns):\n    encoder = LabelEncoder()\n    cat_data.loc[:,feat] = encoder.fit_transform(cat_data[feat].astype('str').fillna('-1').values)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_emb_model(df, categorical_columns):\n    inputs = []\n    outputs = []\n    \n    for c in categorical_columns:\n        num_unique_vals = int(df[c].nunique())\n        embed_dim = int(min(np.ceil(num_unique_vals / 2), 50))\n        inp = Input(shape = (1,))\n        out = layers.Embedding(num_unique_vals+1, embed_dim, name=c)(inp)\n        out = Dropout(0.3)(out)\n        out = BatchNormalization()(out)\n        out = layers.Reshape(target_shape=(embed_dim, ))(out)\n        inputs.append(inp)\n        outputs.append(out)\n    \n    x = Concatenate()(outputs)\n    x = Dense(300, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    x = BatchNormalization()(x)\n\n    \n    model = Model(inputs=inputs, outputs=x)\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-12-14T12:35:51.5688Z","iopub.execute_input":"2021-12-14T12:35:51.569089Z","iopub.status.idle":"2021-12-14T12:35:51.579032Z","shell.execute_reply.started":"2021-12-14T12:35:51.569056Z","shell.execute_reply":"2021-12-14T12:35:51.576385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = get_emb_model(cat_data,cat_cols)","metadata":{"execution":{"iopub.status.busy":"2021-12-14T12:35:51.997519Z","iopub.execute_input":"2021-12-14T12:35:51.997968Z","iopub.status.idle":"2021-12-14T12:35:54.129499Z","shell.execute_reply.started":"2021-12-14T12:35:51.997919Z","shell.execute_reply":"2021-12-14T12:35:54.128761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_data.reset_index(drop=True).to_feather('cat_train_test.feather')\nX.join(y).reset_index(drop=True).to_feather('train.feather')\ntest.reset_index(drop=True).to_feather('test.feather')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"break","metadata":{"execution":{"iopub.status.busy":"2021-12-14T11:53:38.9129Z","iopub.execute_input":"2021-12-14T11:53:38.913194Z","iopub.status.idle":"2021-12-14T11:53:38.920093Z","shell.execute_reply.started":"2021-12-14T11:53:38.913157Z","shell.execute_reply":"2021-12-14T11:53:38.918978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling","metadata":{}},{"cell_type":"code","source":"X = train.copy()\nclasses = y.nunique().values[0]\n\ndel train\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-12-14T12:43:59.547782Z","iopub.execute_input":"2021-12-14T12:43:59.548061Z","iopub.status.idle":"2021-12-14T12:44:00.324932Z","shell.execute_reply.started":"2021-12-14T12:43:59.548008Z","shell.execute_reply":"2021-12-14T12:44:00.324207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_transformer(df,column,rows=2,cols=3,only_og=False):\n    from scipy.stats import boxcox, yeojohnson, skew, kurtosis\n\n    #setting plot theme\n    plt.rcParams['figure.dpi'] = 300\n\n    fig = plt.figure(figsize=(9,6), facecolor='#f6f5f5', constrained_layout='tight_layout')\n    gs = fig.add_gridspec(rows, cols)\n\n    background_color = \"#f6f5f5\"\n    sns.set_palette(['#ff355d','#ffd514',])\n\n    #making multiple ax\n    ax_dict = {}\n    for row in range(rows):\n        for col in range(cols):\n            ax_dict[\"ax%s%s\" %(row,col)] = fig.add_subplot(gs[row, col])\n    \n    if np.any(df[column] <= 0):\n        df_scaled = (df[column] - df[column].min(axis=0)) / (df[column].max(axis=0) - df[column].min(axis=0))\n        df_scaled = df_scaled * (1 - 0.001) + 0.001\n    else:\n        df_scaled = df[column]\n        \n    #dictionary of data transformation\n    pp = {0: df[column],1:np.log1p(df_scaled),2:np.sqrt(df[column]),3:np.cbrt(df[column])\n          ,4:boxcox(df_scaled)[0],5:yeojohnson(df[column])[0]}\n\n    #iterating over subplots and ploting each one by one.        \n    locals().update(ax_dict)\n    count = 0\n\n    trans = ['original','log1p','sqrt','crbt','boxcox','yeojohnson']\n    #setting theme for every ax in local()\n    for indx,row in enumerate(range(rows)):\n        for col in range(cols):\n\n            locals()['ax' + str(row) + str(col)].tick_params(labelsize=3, width=0.5, length=1.5)\n\n            #comment below 2 lines if you are using regplot\n            locals()['ax' + str(row) + str(col)].grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\n            locals()['ax' + str(row) + str(col)].grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\n\n            for s in [\"right\", \"top\"]:\n                locals()['ax' + str(row) + str(col)].spines[s].set_visible(False)\n\n\n            locals()['ax' + str(row) + str(col)].set_facecolor(background_color)\n\n            ax_sns = sns.kdeplot(x=pp[count],ax=locals()['ax' + str(row) + str(col)],zorder=4,fill=True,ec='black')\n\n            locals()['ax' + str(row) + str(col)].set_facecolor(background_color)\n            locals()['ax' + str(row) + str(col)].set_xlabel(column,fontsize=4, weight='bold',)\n            locals()['ax' + str(row) + str(col)].set_ylabel(trans[count],fontsize=4, weight='bold')\n            \n            locals()['ax' + str(row) + str(col)].title.set_text(f'skew: {round(skew(pp[count]),2)} kurtosis: {round(kurtosis(pp[count]),2)}')\n            if only_og:break\n            count+=1\n\n    plt.show()","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-12-14T11:53:38.925861Z","iopub.status.idle":"2021-12-14T11:53:38.92665Z","shell.execute_reply.started":"2021-12-14T11:53:38.926403Z","shell.execute_reply":"2021-12-14T11:53:38.926428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras import Input, layers, Model\nfrom keras.layers import Dense\n\ndef get_model(SEED=42,df=None, categorical_columns=None):\n    #for reuseability\n    keras.backend.clear_session()\n    tf.keras.backend.clear_session()\n    tf.random.set_seed(SEED)\n    np.random.seed(SEED)\n    tf.random.set_seed(SEED)\n    \n    #input\n    input_ = Input(shape=(X.shape[1]))\n    \n    #layers\n    x1 = Dense(units=384, activation='swish')(input_)\n    x1 = Dropout(rate=0.45)(x1)\n    x1 = BatchNormalization()(x1)\n    \n    \n    x2 = Dense(units=192, activation='swish')(x1)\n    x2 = Dropout(rate=0.35)(x2)\n    x2 = BatchNormalization()(x2)\n    \n    x3 = Dense(units=96, activation='swish')(x2)\n    x3 = Dropout(rate=0.25)(x3)\n    x3 = BatchNormalization()(x3)\n    \n    x4 = Dense(units=192, activation='swish')(x3)\n    x4 = BatchNormalization()(x4)\n    x4 = Multiply()([x2, x4])\n    x4 = Dropout(rate=0.35)(x4)\n    x4 = BatchNormalization()(x4)\n    \n    x5 = Dense(units=384, activation='swish')(x4)\n    x5 = BatchNormalization()(x5)\n    x5 = Multiply()([x1, x5])\n    x5 = Dropout(rate=0.45)(x5)\n    x5 = BatchNormalization()(x5)\n    \n    x = Concatenate()([x3, x5])\n    x = Dense(units=128, activation='swish')(x)\n    x = Dropout(rate=0.25)(x)\n    x = BatchNormalization()(x)\n    \n    inputs = []\n    outputs = []\n    \n    for c in categorical_columns:\n        num_unique_vals = int(df[c].nunique())\n        embed_dim = int(min(np.ceil(num_unique_vals / 2), 50))\n        inp = Input(shape = (1,))\n        out = layers.Embedding(num_unique_vals+1, embed_dim, name=c)(inp)\n        out = Dropout(0.3)(out)\n        out = BatchNormalization()(out)\n        out = layers.Reshape(target_shape=(embed_dim, ))(out)\n        inputs.append(inp)\n        outputs.append(out)\n    \n    x1 = Concatenate()(outputs)\n    x1 = Dense(300, activation='relu')(x1)\n    x1 = Dropout(0.3)(x1)\n    x1 = BatchNormalization()(x1)\n    \n    x = Concatenate()([x,x1])\n    \n    x = Dense(units=64, activation='swish')(x)\n    x = Dropout(rate=0.15)(x)\n    x = BatchNormalization()(x)\n    \n    x = Dense(units=32, activation='swish')(x)\n    x = Dropout(rate=0.05)(x)\n    x = BatchNormalization()(x)\n    \n\n    #output\n    output_ = Dense(classes, activation='softmax')(x)\n    \n    model = Model(inputs=[input_,inputs], outputs =[output_])\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2021-12-14T12:44:03.183979Z","iopub.execute_input":"2021-12-14T12:44:03.184531Z","iopub.status.idle":"2021-12-14T12:44:03.201776Z","shell.execute_reply.started":"2021-12-14T12:44:03.184495Z","shell.execute_reply":"2021-12-14T12:44:03.200878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = get_model(42,cat_data, cat_cols)","metadata":{"execution":{"iopub.status.busy":"2021-12-14T12:44:03.31194Z","iopub.execute_input":"2021-12-14T12:44:03.312492Z","iopub.status.idle":"2021-12-14T12:44:05.390786Z","shell.execute_reply.started":"2021-12-14T12:44:03.31246Z","shell.execute_reply":"2021-12-14T12:44:05.39007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.utils.plot_model(model,show_dtype=True, show_layer_names=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-14T12:45:07.328145Z","iopub.execute_input":"2021-12-14T12:45:07.328915Z","iopub.status.idle":"2021-12-14T12:45:09.921568Z","shell.execute_reply.started":"2021-12-14T12:45:07.328865Z","shell.execute_reply":"2021-12-14T12:45:09.92066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"K = keras.backend\n\nclass ExponentialLearningRate(keras.callbacks.Callback):\n    def __init__(self, factor):\n        self.factor = factor\n        self.rates = []\n        self.losses = []\n    def on_epoch_begin(self, epoch, logs=None):\n        self.prev_loss = 0\n    def on_batch_end(self, batch, logs=None):\n        batch_loss = logs[\"loss\"] * (batch + 1) - self.prev_loss * batch\n        self.prev_loss = logs[\"loss\"]\n        self.rates.append(K.get_value(self.model.optimizer.learning_rate))\n        self.losses.append(batch_loss)\n        K.set_value(self.model.optimizer.learning_rate, self.model.optimizer.learning_rate * self.factor)\n        \ndef find_learning_rate(model, X, y, epochs=1, batch_size=32, min_rate=10**-5, max_rate=10):\n    init_weights = model.get_weights()\n    iterations = math.ceil(len(X) / batch_size) * epochs\n    factor = np.exp(np.log(max_rate / min_rate) / iterations)\n    init_lr = K.get_value(model.optimizer.learning_rate)\n    K.set_value(model.optimizer.learning_rate, min_rate)\n    exp_lr = ExponentialLearningRate(factor)\n    history = model.fit(X, y, epochs=epochs, batch_size=batch_size,\n                        callbacks=[exp_lr])\n    K.set_value(model.optimizer.learning_rate, init_lr)\n    model.set_weights(init_weights)\n    return exp_lr.rates, exp_lr.losses\n\ndef plot_lr_vs_loss(rates, losses):\n    plt.plot(rates, losses)\n    plt.gca().set_xscale('log')\n    plt.hlines(min(losses), min(rates), max(rates))\n    plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 2])\n    plt.xlabel(\"Learning rate\")\n    plt.ylabel(\"Loss\")","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-12-14T11:53:38.929842Z","iopub.status.idle":"2021-12-14T11:53:38.930506Z","shell.execute_reply.started":"2021-12-14T11:53:38.930257Z","shell.execute_reply":"2021-12-14T11:53:38.930282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.isnull().sum().sum()","metadata":{"execution":{"iopub.status.busy":"2021-12-14T11:53:38.931726Z","iopub.status.idle":"2021-12-14T11:53:38.932375Z","shell.execute_reply.started":"2021-12-14T11:53:38.932123Z","shell.execute_reply":"2021-12-14T11:53:38.932148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.shape, y.shape, test.shape","metadata":{"execution":{"iopub.status.busy":"2021-12-14T11:53:38.933584Z","iopub.status.idle":"2021-12-14T11:53:38.934223Z","shell.execute_reply.started":"2021-12-14T11:53:38.93397Z","shell.execute_reply":"2021-12-14T11:53:38.933994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import RobustScaler\n\nkf = StratifiedKFold(n_splits=10,shuffle=True,random_state=1)\n\nscores = []\noof_pred = {}\ntest_pred = {}\n\nmodel_name = 'DNN'\ncce = tf.keras.metrics.Accuracy()\n\noof_pred = dict()\ntest_pred = dict()\nscores = dict()\n\nindx = []\n\nmodel_name = 'DNN'\n\noof_pred[model_name] = list()\noof_pred['y_valid'] = list()\ntest_pred[model_name] = list()\nscores[model_name] = list()\n\nfor fold,(train_indx, valid_indx) in enumerate(kf.split(X,y)):\n    print(f\"{'='*10} Fold: {fold} {'='*10}\")\n    \n    #==================== data spliting ====================\n    X_train, X_valid = X.iloc[train_indx],X.iloc[valid_indx]\n    y_train, y_valid = y.iloc[train_indx],y.iloc[valid_indx]\n    #==================== preprocesing ====================\n    scaler = RobustScaler()\n    \n    X_train = scaler.fit_transform(X_train)\n    \n    X_valid = scaler.transform(X_valid)\n    test_scaled = scaler.transform(test)\n    \n    #==================== NOTES: Additional steps ====================\n    \n\n    #====================get model, callbacks and compile ====================\n    \n    model = get_model(SEED=42)\n    \n    kall1 = keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n    patience=20,\n    mode=\"auto\",\n    restore_best_weights=True)\n    \n    kall2 = tf.keras.callbacks.LearningRateScheduler(tf.keras.optimizers.schedules.ExponentialDecay(\n    1e-2,decay_steps=X_train.shape[0]//1024,\n    decay_rate=0.96,\n    staircase=True\n    ))\n    \n    model.compile(optimizer=Nadam(learning_rate = 1e-2),\n    loss='sparse_categorical_crossentropy',\n    metrics='acc')\n    \n    #==================== OneCycleScheduler ====================\n    \n#     batch_size = 1024\n#     rates, losses = find_learning_rate(model, X_train, y_train, epochs=1, batch_size=batch_size)\n#     plot_lr_vs_loss(rates, losses)\n#     plt.show()\n#     break\n    #==================== model ====================\n    \n    history = model.fit(X_train,y_train,epochs=5000, batch_size=3024, validation_data = (X_valid, y_valid), callbacks = [kall1,kall2],\n             verbose=0)\n    \n    #==================== Loss Plot ====================\n    lol = pd.DataFrame(history.history['loss']).plot(label='loss')\n    pd.DataFrame(history.history['val_loss']).plot(ax=lol, label='val_loss')\n    \n    plt.legend(['loss','val_loss'])\n\n    plt.show()\n    \n    print()\n    \n    \n    #==================== Model Prediction ====================\n    \n    y_pred = model.predict(X_valid)\n    oof_pred[model_name].extend(np.argmax(y_pred,axis=-1))\n    \n    score = cce(y_valid, np.argmax(y_pred,axis=-1)).numpy()\n    scores[model_name].append(score)\n    \n    y_pred = model.predict(test_scaled)\n    test_pred[model_name].append(np.argmax(y_pred,axis=-1)) #appending test prediction for every fold\n    \n    oof_pred['y_valid'].extend(y_valid.iloc[:,-1].values)\n    \n    #==================== printing fold number and score ====================\n    print(f\"Fold {fold} Score: {score}\")\n    \n    #==================== deleting some data and clearning ====================\n    del model, X_valid, X_train, y_valid, y_train, y_pred\n    gc.collect()\n    \nprint('\\n','=-='*20,f'Overall Score: {np.mean(scores[model_name])}','=-='*20)","metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"execution":{"iopub.status.busy":"2021-12-14T11:53:38.935517Z","iopub.status.idle":"2021-12-14T11:53:38.936158Z","shell.execute_reply.started":"2021-12-14T11:53:38.935905Z","shell.execute_reply":"2021-12-14T11:53:38.93593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy import stats\nname = 'DNN'\n\ntry:\n    sample_submission.head(1)\nexcept:\n    sample_submission = pd.read_csv('../input/tabular-playground-series-dec-2021/sample_submission.csv')\n\n\nbase_test_predictions = pd.DataFrame(\n    {name: np.mean(np.column_stack(test_pred[name]), axis=1)\n    for name in test_pred.keys()}\n)\n\ndef decode(x):\n    if x<4:\n        return x+1\n    else:\n        return x+2\n\n\nbase_submission = sample_submission.copy()\n\nbase_submission.iloc[:,-1] = base_test_predictions.iloc[:,-1].apply(decode).astype('int8')\nbase_submission.to_csv(f'{name}_base.csv',index=False)\n\noof_predictions = pd.DataFrame({name:oof_pred[name] for name in oof_pred.keys()})\noof_predictions.loc[:,name] = oof_predictions.loc[:,name].apply(decode).astype('int8') \noof_predictions.to_csv(f'{name}_oof_pred.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-14T11:53:38.937402Z","iopub.status.idle":"2021-12-14T11:53:38.938048Z","shell.execute_reply.started":"2021-12-14T11:53:38.937792Z","shell.execute_reply":"2021-12-14T11:53:38.937817Z"},"trusted":true},"execution_count":null,"outputs":[]}]}