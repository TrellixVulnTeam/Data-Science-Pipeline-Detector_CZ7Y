{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Introduction: Tabular Playground Series - Dec 2021\n> The objective of this notebook is to apply step-by-step approach to solve a tabular data competition on Kaggle.\n> \n> The subject of this notebook is [a multi-classification task](https://www.kaggle.com/c/tabular-playground-series-dec-2021/data)\n> \n> The target variable we are predicting consists of 7 different types of forest cover.\n>\n> The training dataset consists of 4 million labeled samples with features like elevation, soil type, etc.\n>\n> The provided dataset was synthetically generated by a GAN that was trained on a the data from the [Forest Cover Type Prediction](https://www.kaggle.com/c/forest-cover-type-prediction/overview). This dataset is (a) much larger, and (b) may or may not have the same relationship to the target as the original data.\n> \n> Please refer to this [data page](https://www.kaggle.com/c/forest-cover-type-prediction/data) for a detailed explanation of the features.\n>\n> For the purposes of this notebook I will refer to held back training data as validation data, and data we have to submit predictions on as test data","metadata":{}},{"cell_type":"markdown","source":"## Changelog\n\n* Version 1 - basic eda and linear model -> 0.209 accuracy on validation dataset\n* Version 2 - established null accuracy baseline -> 0.565 accuracy on train dataset\n* Version 3 - added train-test split, standard scaler, SGD Classifier -> 0.8808 accuracy on public leaderboard (a fraction of the test dataset)\n* Version 3.1 - added Linear SVC -> 0.8805 accuracy on public leaderboard\n* Version 4 - added XGBoost -> 0.91796 accuracy on public leaderboard\n* Version 4.1 - added CatBoost -> 0.94155 accuracy on public leaderboard\n* Version 4.2 - added LightGBM -> 0.92976 accuracy on public leaderboard\n* Version 4.3 - dropped class 5 with only 1 occurance out of 4 million, updated Catboost -> 0.94135 accuracy on public leaderboard\n* Version 5 - forked notebook, to simply adding stratified K fold cross validation for just one model","metadata":{}},{"cell_type":"markdown","source":"## Import","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-16T03:11:32.142235Z","iopub.execute_input":"2021-12-16T03:11:32.142854Z","iopub.status.idle":"2021-12-16T03:11:32.152963Z","shell.execute_reply.started":"2021-12-16T03:11:32.142816Z","shell.execute_reply":"2021-12-16T03:11:32.152167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read datasets to pandas dataframe\ndf_train = pd.read_csv('/kaggle/input/tabular-playground-series-dec-2021/train.csv')\ndf_test = pd.read_csv('/kaggle/input/tabular-playground-series-dec-2021/test.csv')\ndf_sample_submission = pd.read_csv('/kaggle/input/tabular-playground-series-dec-2021/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-12-16T03:11:32.155538Z","iopub.execute_input":"2021-12-16T03:11:32.155889Z","iopub.status.idle":"2021-12-16T03:11:55.857032Z","shell.execute_reply.started":"2021-12-16T03:11:32.155844Z","shell.execute_reply":"2021-12-16T03:11:55.85602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Reduce Memory Usage\n\nI have used a compression function by Guillaume Martin which is discussed here: https://www.kaggle.com/c/tabular-playground-series-dec-2021/discussion/291844\n","metadata":{}},{"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-12-16T03:11:55.858395Z","iopub.execute_input":"2021-12-16T03:11:55.858637Z","iopub.status.idle":"2021-12-16T03:11:55.872172Z","shell.execute_reply.started":"2021-12-16T03:11:55.858607Z","shell.execute_reply":"2021-12-16T03:11:55.870973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = reduce_mem_usage(df_train)\ndf_test = reduce_mem_usage(df_test)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T03:11:55.873781Z","iopub.execute_input":"2021-12-16T03:11:55.874112Z","iopub.status.idle":"2021-12-16T03:12:13.906966Z","shell.execute_reply.started":"2021-12-16T03:11:55.87407Z","shell.execute_reply":"2021-12-16T03:12:13.905974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## EDA","metadata":{}},{"cell_type":"code","source":"# Checking out df_train\ndf_train.describe()","metadata":{"execution":{"iopub.status.busy":"2021-12-16T03:12:13.909285Z","iopub.execute_input":"2021-12-16T03:12:13.909614Z","iopub.status.idle":"2021-12-16T03:12:17.461296Z","shell.execute_reply.started":"2021-12-16T03:12:13.909577Z","shell.execute_reply":"2021-12-16T03:12:17.460443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets see if we have any missing values\nmissing_values_train = df_train.isna().any().sum()\nmissing_values_test = df_test.isna().any().sum()\nprint(f'There are {missing_values_train} missing values in the train dataset')\nprint(f'There are {missing_values_test} missing values in the test dataset')","metadata":{"execution":{"iopub.status.busy":"2021-12-16T03:12:17.462518Z","iopub.execute_input":"2021-12-16T03:12:17.462767Z","iopub.status.idle":"2021-12-16T03:12:17.542568Z","shell.execute_reply.started":"2021-12-16T03:12:17.462735Z","shell.execute_reply":"2021-12-16T03:12:17.541583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# What are the datatypes for our features?\nfor col in df_train:\n    print(df_train[col].dtype, col)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T03:12:17.543929Z","iopub.execute_input":"2021-12-16T03:12:17.546681Z","iopub.status.idle":"2021-12-16T03:12:17.574571Z","shell.execute_reply.started":"2021-12-16T03:12:17.546636Z","shell.execute_reply":"2021-12-16T03:12:17.570438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets see which features are the most correlated with target\ndf_train.corr()['Cover_Type'].sort_values()","metadata":{"execution":{"iopub.status.busy":"2021-12-16T03:12:17.576124Z","iopub.execute_input":"2021-12-16T03:12:17.576346Z","iopub.status.idle":"2021-12-16T03:12:52.878686Z","shell.execute_reply.started":"2021-12-16T03:12:17.576321Z","shell.execute_reply":"2021-12-16T03:12:52.877494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets establish a baseline if we just always predict the target's most common class\n# AKA: null accuracy\ndf_train['Cover_Type'].value_counts(normalize=True).head(1)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T03:12:52.880535Z","iopub.execute_input":"2021-12-16T03:12:52.880925Z","iopub.status.idle":"2021-12-16T03:12:52.9104Z","shell.execute_reply.started":"2021-12-16T03:12:52.88088Z","shell.execute_reply":"2021-12-16T03:12:52.909813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since the accuracy for a model that only predicts class 2 would be 56.5%, we can judge the models we create by how much they can beat this 'dumb model'","metadata":{}},{"cell_type":"code","source":"# How imbalanced are the class distrubutions in our target variable?\ndf_train.groupby('Cover_Type').size()","metadata":{"execution":{"iopub.status.busy":"2021-12-16T03:12:52.911427Z","iopub.execute_input":"2021-12-16T03:12:52.911822Z","iopub.status.idle":"2021-12-16T03:12:53.000757Z","shell.execute_reply.started":"2021-12-16T03:12:52.911782Z","shell.execute_reply":"2021-12-16T03:12:52.999855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since there is only 1 occurrence of class 5 and there are only 377 occurrences of class 4 (out of 4 million samples in the train dataset) we could arguably drop both, for now lets just drop class 5","metadata":{}},{"cell_type":"code","source":"df_train = df_train[df_train['Cover_Type']!=5]","metadata":{"execution":{"iopub.status.busy":"2021-12-16T03:12:53.002669Z","iopub.execute_input":"2021-12-16T03:12:53.003002Z","iopub.status.idle":"2021-12-16T03:12:53.486081Z","shell.execute_reply.started":"2021-12-16T03:12:53.002959Z","shell.execute_reply":"2021-12-16T03:12:53.485134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preprocessing","metadata":{}},{"cell_type":"markdown","source":"If the dataset hadn't already converted categorical features into dummy variables, we would do that here","metadata":{}},{"cell_type":"code","source":"# Create list of features without'id' and target variable 'cover_type'\nfeatures = list(df_train.columns)\nfeatures = features[1:55]","metadata":{"execution":{"iopub.status.busy":"2021-12-16T03:12:53.487994Z","iopub.execute_input":"2021-12-16T03:12:53.488303Z","iopub.status.idle":"2021-12-16T03:12:53.493657Z","shell.execute_reply.started":"2021-12-16T03:12:53.488261Z","shell.execute_reply":"2021-12-16T03:12:53.492584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create feature dataframe and target dataframe for training\nX = df_train[features]\nY = df_train[\"Cover_Type\"]\n# Also create feature dataframe for generating our prediction later\nX_test = df_test[features]","metadata":{"execution":{"iopub.status.busy":"2021-12-16T03:12:53.495069Z","iopub.execute_input":"2021-12-16T03:12:53.495828Z","iopub.status.idle":"2021-12-16T03:12:53.706982Z","shell.execute_reply.started":"2021-12-16T03:12:53.495741Z","shell.execute_reply":"2021-12-16T03:12:53.705963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Do the train test split before standardizing our features (to prevent data leak)\n# Since the dataset is large we could do a smaller test_size than .2,\n# Even better would be to implement StratifiedKFold, ie 5 folds of .2 with class imbalance replicated in each fold\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_validate, Y_train, Y_validate = train_test_split( X, Y, test_size=0.2, random_state=2)\nprint ('Train set:', X_train.shape,  Y_train.shape)\nprint ('Validation set:', X_validate.shape,  Y_validate.shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T03:12:53.710042Z","iopub.execute_input":"2021-12-16T03:12:53.710288Z","iopub.status.idle":"2021-12-16T03:12:56.456922Z","shell.execute_reply.started":"2021-12-16T03:12:53.710257Z","shell.execute_reply":"2021-12-16T03:12:56.456012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Implement StratifiedKFold - it may be easier to simply normalize all training data before this step instead of normalizing within each fold\n# Comment out above cell if this one is being used\nfrom sklearn.model_selection import StratifiedKFold, KFold\n\nFOLDS = 5\n# already defined X\n# already defined Y\n\ncv = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=42)\n\nfor train_index, test_index in cv.split(X, Y):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = Y.iloc[train_index], Y.iloc[test_index]","metadata":{"execution":{"iopub.status.busy":"2021-12-16T03:13:47.678093Z","iopub.execute_input":"2021-12-16T03:13:47.678938Z","iopub.status.idle":"2021-12-16T03:13:50.809714Z","shell.execute_reply.started":"2021-12-16T03:13:47.6789Z","shell.execute_reply":"2021-12-16T03:13:50.808731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nX_train = scaler.fit_transform(X_train)\nX_validate = scaler.transform (X_validate)\nX_test = scaler.fit_transform(X_test)\n\ndel df_train, df_test","metadata":{"execution":{"iopub.status.busy":"2021-12-16T03:12:56.518398Z","iopub.status.idle":"2021-12-16T03:12:56.518826Z","shell.execute_reply.started":"2021-12-16T03:12:56.518627Z","shell.execute_reply":"2021-12-16T03:12:56.518652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modeling","metadata":{}},{"cell_type":"markdown","source":"We are predicting a category, have labled data, and >100K samples\n\ndone-ish:\n* SGD\n* Linear SVC\n* XGBoost\n* CatBoost\n* Light GBM\n\nmay add:\n* Random Forest\n* KNeighbors Classifier\n* SVC","metadata":{}},{"cell_type":"markdown","source":"### SGD Classifier (stochastic gradient descent)\n\nSGD classifier allows you to select a loss function, we will use the default, which is equivalent to a Linear SVM (but faster)","metadata":{}},{"cell_type":"code","source":"# Create SGD model\nfrom sklearn.linear_model import SGDClassifier\nsgdmodel = SGDClassifier(loss='hinge',  penalty='l2')\nsgdmodel.fit(X_train,Y_train)\n# R^2 for training data\nsgdmodel.score(X_train,Y_train)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T03:12:56.520991Z","iopub.status.idle":"2021-12-16T03:12:56.521455Z","shell.execute_reply.started":"2021-12-16T03:12:56.521204Z","shell.execute_reply":"2021-12-16T03:12:56.52123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# R^2 for validation data\nsgdmodel.score(X_validate,Y_validate)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T03:12:56.522962Z","iopub.status.idle":"2021-12-16T03:12:56.523423Z","shell.execute_reply.started":"2021-12-16T03:12:56.523166Z","shell.execute_reply":"2021-12-16T03:12:56.523191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create test data prediction\n# sgdmodel.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T03:12:56.524886Z","iopub.status.idle":"2021-12-16T03:12:56.525546Z","shell.execute_reply.started":"2021-12-16T03:12:56.525258Z","shell.execute_reply":"2021-12-16T03:12:56.525288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Linear SVC","metadata":{}},{"cell_type":"code","source":"# Create Linear SVC model\nfrom sklearn.svm import LinearSVC\nlsvcmodel = LinearSVC(penalty='l2', loss='squared_hinge')\nlsvcmodel.fit(X_train,Y_train)\n# R^2 for training data\nlsvcmodel.score(X_train,Y_train)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T03:12:56.526802Z","iopub.status.idle":"2021-12-16T03:12:56.527318Z","shell.execute_reply.started":"2021-12-16T03:12:56.527086Z","shell.execute_reply":"2021-12-16T03:12:56.527114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# R^2 for validation data\nlsvcmodel.score(X_validate,Y_validate)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T03:12:56.528314Z","iopub.status.idle":"2021-12-16T03:12:56.52901Z","shell.execute_reply.started":"2021-12-16T03:12:56.528675Z","shell.execute_reply":"2021-12-16T03:12:56.528715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### XGBoost","metadata":{}},{"cell_type":"markdown","source":"For this version the hyperparameters are arbitrary, for a future version we could do a grid search to establish the best performing hyperparameters, then we could fit the model again without GPU acceleration to improve accuracy","metadata":{}},{"cell_type":"code","source":"# Create XGBoost model\nfrom xgboost import XGBClassifier # Alternatively there is a sklearn wrapper, from sklearn.ensemble import GradientBoostingClassifier\n\nparams = {\n#             'objective':'binary:logistic',/\n            'objective' : 'multi:softmax',\n            'tree_method': 'gpu_hist',\n            'eval_metric': 'mlogloss',\n            'booster' : 'gbtree',\n            'gamma' : 0.75,\n            'max_depth': 7,\n            'alpha': 10,\n            'learning_rate': .007,\n            'n_estimators':2000,\n            'predictor': 'gpu_predictor'\n        }  \n\nxgbmodel = XGBClassifier(**params)\n\nxgbmodel.fit(X_train,Y_train,\n               early_stopping_rounds=200,\n               eval_set=[(X_validate,Y_validate)],\n               verbose=True)\n\n# R^2 for training data\nxgbmodel.score(X_train,Y_train)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T03:12:56.530158Z","iopub.status.idle":"2021-12-16T03:12:56.530473Z","shell.execute_reply.started":"2021-12-16T03:12:56.530305Z","shell.execute_reply":"2021-12-16T03:12:56.530328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# R^2 for validation data\nxgbmodel.score(X_validate,Y_validate)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T03:12:56.532136Z","iopub.status.idle":"2021-12-16T03:12:56.533063Z","shell.execute_reply.started":"2021-12-16T03:12:56.532606Z","shell.execute_reply":"2021-12-16T03:12:56.53264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### CatBoost","metadata":{}},{"cell_type":"code","source":"# Create CatBoost model\nfrom catboost import CatBoostClassifier\ncatbmodel = CatBoostClassifier(task_type = 'GPU', devices='0')\ncatbmodel.fit(X_train, Y_train)\n\n# R^2 for training data\ncatbmodel.score(X_train,Y_train)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T03:12:56.534209Z","iopub.status.idle":"2021-12-16T03:12:56.534974Z","shell.execute_reply.started":"2021-12-16T03:12:56.534761Z","shell.execute_reply":"2021-12-16T03:12:56.534786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# R^2 for validation data\ncatbmodel.score(X_validate,Y_validate)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T03:12:56.536247Z","iopub.status.idle":"2021-12-16T03:12:56.536999Z","shell.execute_reply.started":"2021-12-16T03:12:56.536722Z","shell.execute_reply":"2021-12-16T03:12:56.536755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### LGBM","metadata":{}},{"cell_type":"code","source":"# Create LightGBM model\nfrom lightgbm import LGBMClassifier\n\nlgb_params = {\n    'objective' : 'multiclass',\n    'metric' : 'multi_logloss',\n    'device' : 'gpu',\n}\n\nlgbmmodel = LGBMClassifier(**lgb_params)\n\nlgbmmodel.fit(X_train,Y_train,\n               early_stopping_rounds=200,\n               eval_set=[(X_validate,Y_validate)],\n               verbose=True)\n\n# R^2 for training data\nlgbmmodel.score(X_train,Y_train)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T03:12:56.537978Z","iopub.status.idle":"2021-12-16T03:12:56.538279Z","shell.execute_reply.started":"2021-12-16T03:12:56.538122Z","shell.execute_reply":"2021-12-16T03:12:56.538139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# R^2 for validation data\nlgbmmodel.score(X_validate,Y_validate)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T03:12:56.539633Z","iopub.status.idle":"2021-12-16T03:12:56.540403Z","shell.execute_reply.started":"2021-12-16T03:12:56.540192Z","shell.execute_reply":"2021-12-16T03:12:56.540225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prepare Submission","metadata":{}},{"cell_type":"code","source":"# View sample submission\ndf_sample_submission","metadata":{"execution":{"iopub.status.busy":"2021-12-16T03:12:56.541595Z","iopub.status.idle":"2021-12-16T03:12:56.541929Z","shell.execute_reply.started":"2021-12-16T03:12:56.541743Z","shell.execute_reply":"2021-12-16T03:12:56.541766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"sgdmodel (public score = 0.88080)","metadata":{}},{"cell_type":"code","source":"# Rename df and replace the cover type column with our predictions\ndf_sgd_submission = df_sample_submission\ndf_sgd_submission['Cover_Type'] = sgdmodel.predict(X_test).astype('int')\ndf_sgd_submission.to_csv(\"sgd_submission.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T03:12:56.543466Z","iopub.status.idle":"2021-12-16T03:12:56.544136Z","shell.execute_reply.started":"2021-12-16T03:12:56.543945Z","shell.execute_reply":"2021-12-16T03:12:56.543972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"xgbmodel (public score = 0.91796)\n","metadata":{}},{"cell_type":"code","source":"# Rename df and replace the cover type column with our predictions\ndf_xgb_submission = df_sample_submission\ndf_xgb_submission['Cover_Type'] = xgbmodel.predict(X_test).astype('int')\ndf_xgb_submission.to_csv(\"xgb_submission.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T03:12:56.545248Z","iopub.status.idle":"2021-12-16T03:12:56.545841Z","shell.execute_reply.started":"2021-12-16T03:12:56.545578Z","shell.execute_reply":"2021-12-16T03:12:56.54561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"lsvcmodel (public score = 0.88050)","metadata":{}},{"cell_type":"code","source":"# Rename df and replace the cover type column with our predictions\ndf_lsvc_submission = df_sample_submission\ndf_lsvc_submission['Cover_Type'] = lsvcmodel.predict(X_test).astype('int')\ndf_lsvc_submission.to_csv(\"lsvc_submission.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T03:12:56.547334Z","iopub.status.idle":"2021-12-16T03:12:56.547827Z","shell.execute_reply.started":"2021-12-16T03:12:56.547573Z","shell.execute_reply":"2021-12-16T03:12:56.547601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"catbmodel (public score = 0.94155)","metadata":{}},{"cell_type":"code","source":"# Rename df and replace the cover type column with our predictions\ndf_catb_submission = df_sample_submission\ndf_catb_submission['Cover_Type'] = catbmodel.predict(X_test).astype('int')\ndf_catb_submission.to_csv(\"catb_submission.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T03:12:56.549741Z","iopub.status.idle":"2021-12-16T03:12:56.550205Z","shell.execute_reply.started":"2021-12-16T03:12:56.549953Z","shell.execute_reply":"2021-12-16T03:12:56.54998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"lgbmmodel (public score = 0.92976)","metadata":{}},{"cell_type":"code","source":"# Rename df and replace the cover type column with our predictions\ndf_lgbm_submission = df_sample_submission\ndf_lgbm_submission['Cover_Type'] = lgbmmodel.predict(X_test).astype('int')\ndf_lgbm_submission.to_csv(\"lgbm_submission.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T03:12:56.551877Z","iopub.status.idle":"2021-12-16T03:12:56.552335Z","shell.execute_reply.started":"2021-12-16T03:12:56.552085Z","shell.execute_reply":"2021-12-16T03:12:56.55211Z"},"trusted":true},"execution_count":null,"outputs":[]}]}