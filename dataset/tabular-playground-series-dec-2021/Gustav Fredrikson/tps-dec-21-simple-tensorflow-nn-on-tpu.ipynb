{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# My application of a simple neural net on playground december 2021\n### Please let me know of any improvements, I'm here to learn\n\n### Ideas for improvement\n* Feature engineering, Cover_Type = 5 is only 1 sample, remove? DONE\n* Encode using sklearn labelencoder (need to use encoder.inverse_transform for test preds later) DONE\n* Scale data using sklearn robustscaler DONE\n* Plot model using tf.keras.utils plot_model\n* Use some tool to do feature importance\n* Can run on TPU, DONE\n* Get lower TPU idle time. Does anyone have any idea how?\n* Would be interesting to see if more folds give more accuracy\n* Try not removing cover_type = 4\n\nUsed https://www.kaggle.com/gulshanmishra/tps-dec-21-tensorflow-nn-feature-engineering as inspiration, please go give that notebook a thumbs up\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport datatable as dt\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold \nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder, RobustScaler\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, BatchNormalization\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom tensorflow.keras.utils import plot_model\nimport tensorflow as tf\n\nplot = False # Plot model or plot summary\nVERBOSE = False # Show all outputs","metadata":{"execution":{"iopub.status.busy":"2021-12-14T09:34:00.648965Z","iopub.execute_input":"2021-12-14T09:34:00.649698Z","iopub.status.idle":"2021-12-14T09:34:00.657577Z","shell.execute_reply.started":"2021-12-14T09:34:00.649648Z","shell.execute_reply":"2021-12-14T09:34:00.656639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Function to reduce memory of dataframes","metadata":{}},{"cell_type":"code","source":"def reduce_memory_usage(df, verbose=True):\n    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n    start_mem = df.memory_usage().sum() / 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if (\n                    c_min > np.finfo(np.float16).min\n                    and c_max < np.finfo(np.float16).max\n                ):\n                    df[col] = df[col].astype(np.float16)\n                elif (\n                    c_min > np.finfo(np.float32).min\n                    and c_max < np.finfo(np.float32).max\n                ):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() / 1024 ** 2\n    if verbose:\n        print(\n            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n                end_mem, 100 * (start_mem - end_mem) / start_mem\n            )\n        )\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-12-14T09:34:00.659389Z","iopub.execute_input":"2021-12-14T09:34:00.659769Z","iopub.status.idle":"2021-12-14T09:34:00.677598Z","shell.execute_reply.started":"2021-12-14T09:34:00.65973Z","shell.execute_reply":"2021-12-14T09:34:00.67678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing training and testing data\nReading using datatable and converting to pandas is often faster than reading directly using pandas","metadata":{}},{"cell_type":"code","source":"train_df = dt.fread(\"../input/tabular-playground-series-dec-2021/train.csv\")\ntest_df = dt.fread(\"../input/tabular-playground-series-dec-2021/test.csv\")\ntest_df = reduce_memory_usage(test_df.to_pandas())\ntrain_df = reduce_memory_usage(train_df.to_pandas())\n\nINPUT_SHAPE = test_df.shape[1:] # Used to decide first layer of nn\nNUM_CLASSES = train_df[\"Cover_Type\"].nunique() # For output layer of nn\n\n# Remove sample with cover_type = 5\nidx_to_drop5 = train_df[train_df[\"Cover_Type\"] == 5].index\nprint(f\"Nr of cover_type = 5: {len(idx_to_drop5)}\")\ntrain_df.drop(idx_to_drop5,\n              axis=0,\n              inplace=True)\n\n# Very few is 4 aswell\n\"\"\"idx_to_drop4 = train_df[train_df[\"Cover_Type\"] == 4].index\nprint(f\"Nr of cover_type = 4: {len(idx_to_drop4)}\")\ntrain_df.drop(idx_to_drop4,\n              axis=0,\n              inplace=True)\"\"\"\n\n\nencoder = LabelEncoder()\ntrain_df[\"Cover_Type\"] = encoder.fit_transform(train_df[\"Cover_Type\"])\n\nbool_features = [i for i in train_df.columns if \"area\" in i.lower() or \"soil\" in i.lower()]\ntest_df[bool_features] = test_df[bool_features].astype(np.int8)\ntrain_df[bool_features] = train_df[bool_features].astype(np.int8)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-14T09:34:00.679303Z","iopub.execute_input":"2021-12-14T09:34:00.679587Z","iopub.status.idle":"2021-12-14T09:34:05.428065Z","shell.execute_reply.started":"2021-12-14T09:34:00.679559Z","shell.execute_reply":"2021-12-14T09:34:05.427252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Scale unscaled data\nGreat article on interesting ways to select pandas columns:\nhttps://towardsdatascience.com/interesting-ways-to-select-pandas-dataframe-columns-b29b82bbfb33","metadata":{}},{"cell_type":"code","source":"cols_to_scale = train_df.loc[:,[(train_df[col] > 7).any() for col in train_df.columns]].columns\nprint(f\"Scaled Columns: {cols_to_scale}\\n\\n  \\\nNumber of scaled Columns: {len(cols_to_scale)}\")\n\nscaler = RobustScaler()\ntrain_df[cols_to_scale] = scaler.fit_transform(train_df[cols_to_scale])\ntest_df[cols_to_scale] = scaler.fit_transform(test_df[cols_to_scale])\n\ny = train_df.pop(\"Cover_Type\").values\nX = train_df.values","metadata":{"execution":{"iopub.status.busy":"2021-12-14T09:34:05.429669Z","iopub.execute_input":"2021-12-14T09:34:05.429903Z","iopub.status.idle":"2021-12-14T09:34:09.228775Z","shell.execute_reply.started":"2021-12-14T09:34:05.429875Z","shell.execute_reply":"2021-12-14T09:34:09.22796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Functions to use when training later\nReduce learningrate when accuracy is plateauing and stop early if accuracy is not improving","metadata":{}},{"cell_type":"code","source":"reduce_lr = ReduceLROnPlateau(\n    monitor=\"val_loss\",\n    factor=0.5,\n    patience=2,\n    verbose=VERBOSE\n)\nearly_stop = EarlyStopping(\n    monitor=\"val_accuracy\",\n    patience=15,\n    restore_best_weights=True,\n    verbose=True # Always show on which fold it stopped early\n)\ncallbacks = [reduce_lr, early_stop]","metadata":{"execution":{"iopub.status.busy":"2021-12-14T09:34:09.230386Z","iopub.execute_input":"2021-12-14T09:34:09.230601Z","iopub.status.idle":"2021-12-14T09:34:09.236762Z","shell.execute_reply.started":"2021-12-14T09:34:09.230576Z","shell.execute_reply":"2021-12-14T09:34:09.235766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define the model and compile it","metadata":{}},{"cell_type":"code","source":"def build_model():\n    # To run on TPU\n    build_with_TPU = False\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n        BATCH_SIZE = 1024\n        # print(f\"Running on TPU: {tpu.master()}\")\n        # print(f\"Batch Size on TPU: {BATCH_SIZE}\")\n        build_with_TPU = True\n    except ValueError:\n        BATCH_SIZE = 1024\n        # print(\"Not running on TPU\")\n        # strategy = tf.distribute.get_strategy()\n        # BATCH_SIZE = 512\n        # print(f\"Running on {strategy.num_replicas_in_sync} replicas\")\n        # print(f\"Batch Size: {BATCH_SIZE}\")\n        \n    if build_with_TPU:\n        with strategy.scope():\n            model = Sequential([\n                Dense(units=300, kernel_initializer='random_normal', activation='gelu',\n                      input_shape=INPUT_SHAPE),\n                BatchNormalization(),\n                Dense(units=200, kernel_initializer='random_normal', activation='gelu'),\n                BatchNormalization(),\n                Dense(units=100, kernel_initializer='random_normal', activation='gelu'),\n                BatchNormalization(),\n                Dense(units=30, kernel_initializer='random_normal', activation='gelu'),\n                BatchNormalization(),\n                Dense(units=6, activation=\"softmax\")\n            ])\n            model.compile(\n                optimizer='adam',\n                loss = 'sparse_categorical_crossentropy',\n                metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n                steps_per_execution=32 # Just a random value, don't know what to use here\n            )\n    else:\n        model = Sequential([\n                Dense(units=300, kernel_initializer='random_normal', activation='gelu',\n                      input_shape=INPUT_SHAPE),\n                BatchNormalization(),\n                Dense(units=200, kernel_initializer='random_normal', activation='gelu'),\n                BatchNormalization(),\n                Dense(units=100, kernel_initializer='random_normal', activation='gelu'),\n                BatchNormalization(),\n                Dense(units=30, kernel_initializer='random_normal', activation='gelu'),\n                BatchNormalization(),\n                Dense(units=6, activation=\"softmax\")\n            ])\n        model.compile(\n            optimizer='adam',\n            loss = 'sparse_categorical_crossentropy',\n            metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n        )\n            \n    return model\n\nif plot:\n    plot_model(\n        build_model(),\n        show_shapes=True,\n        show_layer_names=True\n    )\nelse:\n    build_model().summary()","metadata":{"execution":{"iopub.status.busy":"2021-12-14T09:34:09.238277Z","iopub.execute_input":"2021-12-14T09:34:09.238499Z","iopub.status.idle":"2021-12-14T09:34:20.802797Z","shell.execute_reply.started":"2021-12-14T09:34:09.238473Z","shell.execute_reply":"2021-12-14T09:34:20.801774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train the model\nTrains the model {FOLDS} times, and adds result to predictions to make all models effect result","metadata":{}},{"cell_type":"code","source":"print(\"Num GPUs available: \", len(tf.config.list_physical_devices('GPU')))\n\nFOLDS = 5\nEPOCHS = 200\nBATCH_SIZE = 2048\nSTEPS_PER_EPOCH = 4*BATCH_SIZE # Not used, chosen if wanted faster epochs\ntest_preds = np.zeros((1,1))\nscores = []\n\ncv = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=0)\n\nfor fold, (train_idx, test_idx) in enumerate(cv.split(X,y), start=1):\n    X_train, X_val = X[train_idx], X[test_idx]\n    y_train, y_val = y[train_idx], y[test_idx]\n\n    model = build_model()\n    model.fit(\n        X_train,\n        y_train,\n        validation_data=(X_val, y_val),\n        # steps_per_epoch=STEPS_PER_EPOCH,\n        batch_size=BATCH_SIZE,\n        epochs=EPOCHS,\n        callbacks=callbacks,\n        verbose=VERBOSE\n    )\n\n    y_pred = np.argmax(model.predict(X_val), axis=1)\n\n    score = accuracy_score(y_val, y_pred)\n    print(f\"Fold {fold}/{FOLDS} Validation Accuracy: {score}\")\n    scores.append(score)\n\n    test_preds = test_preds + model.predict(test_df)\n    \nprint(f\"\\n\\nMean accuracy over all folds: {np.mean(scores)}\")","metadata":{"execution":{"iopub.status.busy":"2021-12-14T09:34:20.804143Z","iopub.execute_input":"2021-12-14T09:34:20.804391Z","iopub.status.idle":"2021-12-14T09:53:06.799216Z","shell.execute_reply.started":"2021-12-14T09:34:20.804363Z","shell.execute_reply":"2021-12-14T09:53:06.797218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample = pd.read_csv(\"../input/tabular-playground-series-dec-2021/sample_submission.csv\")\npreds = np.argmax(test_preds, axis=1)\npreds = encoder.inverse_transform(preds)\n\nsample.Cover_Type = preds\nsample.to_csv(\"Submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-14T09:53:06.800426Z","iopub.status.idle":"2021-12-14T09:53:06.800814Z","shell.execute_reply.started":"2021-12-14T09:53:06.800615Z","shell.execute_reply":"2021-12-14T09:53:06.800638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}