{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **1. Introduction**\n\n**Objective:** multi-class classification","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-26T00:30:32.027927Z","iopub.execute_input":"2021-12-26T00:30:32.028964Z","iopub.status.idle":"2021-12-26T00:30:32.06224Z","shell.execute_reply.started":"2021-12-26T00:30:32.02885Z","shell.execute_reply":"2021-12-26T00:30:32.061436Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df=pd.read_csv('/kaggle/input/tabular-playground-series-dec-2021/train.csv')\ntest_df=pd.read_csv('/kaggle/input/tabular-playground-series-dec-2021/test.csv')\nsub_df=pd.read_csv('/kaggle/input/tabular-playground-series-dec-2021/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-12-26T00:30:32.063958Z","iopub.execute_input":"2021-12-26T00:30:32.064456Z","iopub.status.idle":"2021-12-26T00:30:51.831957Z","shell.execute_reply.started":"2021-12-26T00:30:32.064421Z","shell.execute_reply":"2021-12-26T00:30:51.831238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-26T00:30:51.833343Z","iopub.execute_input":"2021-12-26T00:30:51.833597Z","iopub.status.idle":"2021-12-26T00:30:51.856835Z","shell.execute_reply.started":"2021-12-26T00:30:51.833563Z","shell.execute_reply":"2021-12-26T00:30:51.856095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.shape","metadata":{"execution":{"iopub.status.busy":"2021-12-26T00:30:51.857795Z","iopub.execute_input":"2021-12-26T00:30:51.857976Z","iopub.status.idle":"2021-12-26T00:30:51.864253Z","shell.execute_reply.started":"2021-12-26T00:30:51.857953Z","shell.execute_reply":"2021-12-26T00:30:51.863438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **2. EDA**","metadata":{}},{"cell_type":"code","source":"train_df.isnull().sum().sum()","metadata":{"execution":{"iopub.status.busy":"2021-12-26T00:30:51.867375Z","iopub.execute_input":"2021-12-26T00:30:51.867689Z","iopub.status.idle":"2021-12-26T00:30:52.107893Z","shell.execute_reply.started":"2021-12-26T00:30:51.867593Z","shell.execute_reply":"2021-12-26T00:30:52.107108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.info()","metadata":{"execution":{"iopub.status.busy":"2021-12-26T00:30:52.109417Z","iopub.execute_input":"2021-12-26T00:30:52.109674Z","iopub.status.idle":"2021-12-26T00:30:52.129545Z","shell.execute_reply.started":"2021-12-26T00:30:52.10963Z","shell.execute_reply":"2021-12-26T00:30:52.128731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-12-26T00:30:52.131141Z","iopub.execute_input":"2021-12-26T00:30:52.131642Z","iopub.status.idle":"2021-12-26T00:30:52.145374Z","shell.execute_reply.started":"2021-12-26T00:30:52.131605Z","shell.execute_reply":"2021-12-26T00:30:52.144698Z"},"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**PS:** I took this method from [this](https://www.kaggle.com/questions-and-answers/134639#767486) comment, to avoid the error mentioned in this comment.","metadata":{}},{"cell_type":"code","source":"# reduce memory usage\ntrain_df = reduce_mem_usage(train_df)","metadata":{"execution":{"iopub.status.busy":"2021-12-26T00:30:52.146707Z","iopub.execute_input":"2021-12-26T00:30:52.146955Z","iopub.status.idle":"2021-12-26T00:31:10.398988Z","shell.execute_reply.started":"2021-12-26T00:30:52.146923Z","shell.execute_reply":"2021-12-26T00:31:10.398278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.groupby('Cover_Type')['Cover_Type'].count().sort_values(ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-26T00:31:10.400092Z","iopub.execute_input":"2021-12-26T00:31:10.400478Z","iopub.status.idle":"2021-12-26T00:31:10.468119Z","shell.execute_reply.started":"2021-12-26T00:31:10.400442Z","shell.execute_reply":"2021-12-26T00:31:10.467414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nsns.histplot(data=train_df, x='Cover_Type',binwidth=1,stat=\"percent\",discrete=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-26T00:31:10.469253Z","iopub.execute_input":"2021-12-26T00:31:10.469661Z","iopub.status.idle":"2021-12-26T00:31:11.752568Z","shell.execute_reply.started":"2021-12-26T00:31:10.469612Z","shell.execute_reply":"2021-12-26T00:31:11.751883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation:** We have *imbalance* data in our training dataset. Imbalance data makes the classifier model **biased** toward the one or two classes ( which have lot of data ). For example, here Cover_Type 2 and 1 are majority classes.\n\nI tried various data balancing technique like SMOTE and Cost sensitive training. However, I was getting memory issue and/or it takes so much time to training the model. So, I found [this](https://rdcu.be/cDRy9) paper discussing various bossting methods for multi-class imbalanced data classification. Even, the original dataset *Forest Cover Type Prediction* was also part of this paper. They mentioned **LogitBoost** algorithm performs better than other algorithm for big dataset ( i.e., dataset with > 10k instances ). So, I choose this algorithm to built the model. ","metadata":{}},{"cell_type":"markdown","source":"Cover_Type= 5 have only 1 count. So, we can remove it safely.","metadata":{}},{"cell_type":"code","source":"train_df = train_df[train_df['Cover_Type'] != 5]\ntrain_df.shape","metadata":{"execution":{"iopub.status.busy":"2021-12-26T00:31:11.754944Z","iopub.execute_input":"2021-12-26T00:31:11.755598Z","iopub.status.idle":"2021-12-26T00:31:12.266688Z","shell.execute_reply.started":"2021-12-26T00:31:11.755566Z","shell.execute_reply":"2021-12-26T00:31:12.26589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = train_df['Cover_Type']\nX = train_df.drop(columns=['Id','Cover_Type'],axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-12-26T00:31:12.267767Z","iopub.execute_input":"2021-12-26T00:31:12.268003Z","iopub.status.idle":"2021-12-26T00:31:12.393233Z","shell.execute_reply.started":"2021-12-26T00:31:12.267969Z","shell.execute_reply":"2021-12-26T00:31:12.392479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.shape, y.shape","metadata":{"execution":{"iopub.status.busy":"2021-12-26T00:31:12.394633Z","iopub.execute_input":"2021-12-26T00:31:12.394904Z","iopub.status.idle":"2021-12-26T00:31:12.402196Z","shell.execute_reply.started":"2021-12-26T00:31:12.39487Z","shell.execute_reply":"2021-12-26T00:31:12.401413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **3. Modeling**","metadata":{}},{"cell_type":"code","source":"pip install logitboost","metadata":{"execution":{"iopub.status.busy":"2021-12-26T00:31:12.405344Z","iopub.execute_input":"2021-12-26T00:31:12.405771Z","iopub.status.idle":"2021-12-26T00:31:21.455683Z","shell.execute_reply.started":"2021-12-26T00:31:12.405735Z","shell.execute_reply":"2021-12-26T00:31:21.454864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from logitboost import LogitBoost\n\nmodels ={'LB': LogitBoost(random_state=0)}","metadata":{"execution":{"iopub.status.busy":"2021-12-26T01:39:06.501319Z","iopub.execute_input":"2021-12-26T01:39:06.501569Z","iopub.status.idle":"2021-12-26T01:39:06.505193Z","shell.execute_reply.started":"2021-12-26T01:39:06.50154Z","shell.execute_reply":"2021-12-26T01:39:06.504456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for key, value in models.items():\n    print(key)\n    print(value)","metadata":{"execution":{"iopub.status.busy":"2021-12-26T01:39:08.098503Z","iopub.execute_input":"2021-12-26T01:39:08.099195Z","iopub.status.idle":"2021-12-26T01:39:08.105459Z","shell.execute_reply.started":"2021-12-26T01:39:08.099159Z","shell.execute_reply":"2021-12-26T01:39:08.103107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, stratify=y)","metadata":{"execution":{"iopub.status.busy":"2021-12-26T00:31:21.817Z","iopub.execute_input":"2021-12-26T00:31:21.817807Z","iopub.status.idle":"2021-12-26T00:31:25.568933Z","shell.execute_reply.started":"2021-12-26T00:31:21.817771Z","shell.execute_reply":"2021-12-26T00:31:25.568205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I choose test_size=0.3, since we have very large dataset ~4 million instances. For the same reason, I skipped cross-validation ( I tried StratifiedKFold and few others and it took lot of time and/or memory issue ) and split data in a stratified fashion. \n\nFeel free to let me know in comments, how can I use any CV with LogitBoost.\n\n**PS:** LogitBoost runs only on CPU.","metadata":{}},{"cell_type":"code","source":"%%time\n\nlb = models.get('LB')\nlb.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-12-26T00:39:29.137477Z","iopub.execute_input":"2021-12-26T00:39:29.137736Z","iopub.status.idle":"2021-12-26T01:08:08.522713Z","shell.execute_reply.started":"2021-12-26T00:39:29.137706Z","shell.execute_reply":"2021-12-26T01:08:08.521967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score,matthews_corrcoef\n\ny_test_pred = lb.predict(X_test)\nprint(\"Mean Accuracy={}\".format(np.mean(accuracy_score(y_test, y_test_pred))))\nprint(\"Matthews correlation coefficient={}\".format(matthews_corrcoef(y_test, y_test_pred)))","metadata":{"execution":{"iopub.status.busy":"2021-12-26T01:10:38.580876Z","iopub.execute_input":"2021-12-26T01:10:38.581566Z","iopub.status.idle":"2021-12-26T01:11:32.093808Z","shell.execute_reply.started":"2021-12-26T01:10:38.581526Z","shell.execute_reply":"2021-12-26T01:11:32.092937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"IMHO, along with Accuracy score ( which is required as part of the competition ),Matthews correlation coefficient is also required. As per Scikit learn documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html), even for the classes of very different size ( i.e., imbalance data ), it takes into account. Since, I just build the default model and unable to use any techniques to handle imbalance data, I feel this metric is appropriate for this scenario.\n\n> The Matthews correlation coefficient is used in machine learning as a measure of the quality of binary and multiclass classifications. It takes into account true and false positives and negatives and is generally regarded as a balanced measure which can be used ***even if the classes are of very different sizes***.","metadata":{}},{"cell_type":"markdown","source":"# **4. Submission**","metadata":{}},{"cell_type":"code","source":"test_df.drop('Id', axis=1, inplace=True)\n\ntest_df.shape","metadata":{"execution":{"iopub.status.busy":"2021-12-26T01:17:29.355497Z","iopub.execute_input":"2021-12-26T01:17:29.355759Z","iopub.status.idle":"2021-12-26T01:17:29.486714Z","shell.execute_reply.started":"2021-12-26T01:17:29.355729Z","shell.execute_reply":"2021-12-26T01:17:29.485962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = lb.predict(test_df)","metadata":{"execution":{"iopub.status.busy":"2021-12-26T01:19:15.020784Z","iopub.execute_input":"2021-12-26T01:19:15.021281Z","iopub.status.idle":"2021-12-26T01:20:00.449341Z","shell.execute_reply.started":"2021-12-26T01:19:15.021246Z","shell.execute_reply":"2021-12-26T01:20:00.448611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df['Cover_Type']= y_pred\nsub_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-26T01:22:09.456367Z","iopub.execute_input":"2021-12-26T01:22:09.456619Z","iopub.status.idle":"2021-12-26T01:22:09.469347Z","shell.execute_reply.started":"2021-12-26T01:22:09.456591Z","shell.execute_reply":"2021-12-26T01:22:09.468637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df.to_csv('submission.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2021-12-26T01:23:23.197296Z","iopub.execute_input":"2021-12-26T01:23:23.19763Z","iopub.status.idle":"2021-12-26T01:23:24.768256Z","shell.execute_reply.started":"2021-12-26T01:23:23.1976Z","shell.execute_reply":"2021-12-26T01:23:24.767437Z"},"trusted":true},"execution_count":null,"outputs":[]}]}