{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.utils.class_weight import compute_class_weight\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, TensorDataset\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping\nfrom pytorch_lightning.callbacks import LearningRateMonitor\nimport time\nimport gc\nimport torchmetrics\n\n\n# Pandas setting to display more dataset rows and columns\npd.set_option('display.max_rows', 150)\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_colwidth', None)\npd.set_option('display.float_format', lambda x: '%.6f' % x)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-09T08:20:47.067358Z","iopub.execute_input":"2021-12-09T08:20:47.067894Z","iopub.status.idle":"2021-12-09T08:20:48.925238Z","shell.execute_reply.started":"2021-12-09T08:20:47.06781Z","shell.execute_reply":"2021-12-09T08:20:48.92393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Data import**","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/tabular-playground-series-dec-2021/train.csv\", low_memory=False)#, nrows=10000)\ntest = pd.read_csv(\"/kaggle/input/tabular-playground-series-dec-2021/test.csv\", low_memory=False)#, nrows=10000)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T08:20:48.932167Z","iopub.execute_input":"2021-12-09T08:20:48.932503Z","iopub.status.idle":"2021-12-09T08:21:17.546657Z","shell.execute_reply.started":"2021-12-09T08:20:48.932462Z","shell.execute_reply":"2021-12-09T08:21:17.545721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info(memory_usage=\"deep\")","metadata":{"execution":{"iopub.status.busy":"2021-12-09T08:21:17.54833Z","iopub.execute_input":"2021-12-09T08:21:17.549342Z","iopub.status.idle":"2021-12-09T08:21:17.569345Z","shell.execute_reply.started":"2021-12-09T08:21:17.549282Z","shell.execute_reply":"2021-12-09T08:21:17.568196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.info(memory_usage=\"deep\")","metadata":{"execution":{"iopub.status.busy":"2021-12-09T08:21:17.573033Z","iopub.execute_input":"2021-12-09T08:21:17.573721Z","iopub.status.idle":"2021-12-09T08:21:17.693684Z","shell.execute_reply.started":"2021-12-09T08:21:17.573683Z","shell.execute_reply":"2021-12-09T08:21:17.692748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **EDA**","metadata":{}},{"cell_type":"code","source":"# Colors to be used for plots\ncolors = [\"lightcoral\", \"sandybrown\", \"darkorange\", \"mediumseagreen\",\n          \"lightseagreen\", \"cornflowerblue\", \"mediumpurple\", \"palevioletred\",\n          \"lightskyblue\", \"sandybrown\", \"yellowgreen\", \"indianred\",\n          \"lightsteelblue\", \"mediumorchid\", \"deepskyblue\"]","metadata":{"execution":{"iopub.status.busy":"2021-12-09T08:21:17.695073Z","iopub.execute_input":"2021-12-09T08:21:17.695308Z","iopub.status.idle":"2021-12-09T08:21:17.700838Z","shell.execute_reply.started":"2021-12-09T08:21:17.695277Z","shell.execute_reply":"2021-12-09T08:21:17.69991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-09T08:21:17.702408Z","iopub.execute_input":"2021-12-09T08:21:17.702995Z","iopub.status.idle":"2021-12-09T08:21:17.747282Z","shell.execute_reply.started":"2021-12-09T08:21:17.70296Z","shell.execute_reply":"2021-12-09T08:21:17.746559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target = \"Cover_Type\"\n\nfeatures = list(train.columns[1:55])","metadata":{"execution":{"iopub.status.busy":"2021-12-09T08:21:17.748758Z","iopub.execute_input":"2021-12-09T08:21:17.749027Z","iopub.status.idle":"2021-12-09T08:21:17.753511Z","shell.execute_reply.started":"2021-12-09T08:21:17.748995Z","shell.execute_reply":"2021-12-09T08:21:17.752694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[target].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-12-09T08:21:17.754537Z","iopub.execute_input":"2021-12-09T08:21:17.755367Z","iopub.status.idle":"2021-12-09T08:21:17.792531Z","shell.execute_reply.started":"2021-12-09T08:21:17.755329Z","shell.execute_reply":"2021-12-09T08:21:17.791396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(5, 6))\npie = ax.pie([len(train), len(test)],\n             labels=[\"Train dataset\", \"Test dataset\"],\n             colors=[\"salmon\", \"teal\"],\n             textprops={\"fontsize\": 15},\n             autopct='%1.1f%%')\nax.axis(\"equal\")\nax.set_title(\"Dataset length comparison\", fontsize=18)\nfig.set_facecolor('white')\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-12-09T08:21:17.794102Z","iopub.execute_input":"2021-12-09T08:21:17.794455Z","iopub.status.idle":"2021-12-09T08:21:17.935776Z","shell.execute_reply.started":"2021-12-09T08:21:17.794414Z","shell.execute_reply":"2021-12-09T08:21:17.934651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(14, 8))\n\nbars = ax.bar(train[target].value_counts().sort_index().index,\n                  train[target].value_counts().sort_index().values,\n                  color=colors,\n                  edgecolor=\"black\")\nax.set_title(\"Target distribution\", fontsize=20, pad=15)\nax.set_ylabel(\"Count\", fontsize=14, labelpad=15)\nax.set_xlabel(\"Target label\", fontsize=14, labelpad=20)\nax.tick_params(axis=\"x\", pad=20)\nax.bar_label(bars, train[target].value_counts().sort_index().values,\n                 padding=3, fontsize=12)\nax.bar_label(bars, [f\"{x*100:2.1f}%\" for x in train[target].value_counts().sort_index().values/len(train)],\n                 padding=-20, fontsize=12)\nax.margins(0.025, 0.06)\nax.grid(axis=\"y\")\n\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-12-09T08:21:17.937525Z","iopub.execute_input":"2021-12-09T08:21:17.93792Z","iopub.status.idle":"2021-12-09T08:21:18.383562Z","shell.execute_reply.started":"2021-12-09T08:21:17.937843Z","shell.execute_reply":"2021-12-09T08:21:18.382841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[features].describe()","metadata":{"execution":{"iopub.status.busy":"2021-12-09T08:21:18.384848Z","iopub.execute_input":"2021-12-09T08:21:18.385829Z","iopub.status.idle":"2021-12-09T08:21:25.035296Z","shell.execute_reply.started":"2021-12-09T08:21:18.385789Z","shell.execute_reply":"2021-12-09T08:21:25.034231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test[features].describe()","metadata":{"execution":{"iopub.status.busy":"2021-12-09T08:21:25.036758Z","iopub.execute_input":"2021-12-09T08:21:25.037024Z","iopub.status.idle":"2021-12-09T08:21:26.514352Z","shell.execute_reply.started":"2021-12-09T08:21:25.036994Z","shell.execute_reply":"2021-12-09T08:21:26.513386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.concat([train[features], test[features]], axis=0)\ndf.reset_index(inplace=True, drop=True)\n\nunique_values = df[features].nunique() < 10\ncat_features = list(unique_values[unique_values==True].index)\nunique_values = df[features].nunique() >= 10\nnum_features = list(unique_values[unique_values==True].index)\n\nprint(f\"There are {len(cat_features)} categorical features: {cat_features}\")\nprint(f\"\\nThere are {len(num_features)} continuous features: {num_features}\")","metadata":{"execution":{"iopub.status.busy":"2021-12-09T08:21:26.517824Z","iopub.execute_input":"2021-12-09T08:21:26.518184Z","iopub.status.idle":"2021-12-09T08:21:33.401015Z","shell.execute_reply.started":"2021-12-09T08:21:26.518148Z","shell.execute_reply":"2021-12-09T08:21:33.399948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.isna().sum().sum(), test.isna().sum().sum()","metadata":{"execution":{"iopub.status.busy":"2021-12-09T08:21:33.402518Z","iopub.execute_input":"2021-12-09T08:21:33.402777Z","iopub.status.idle":"2021-12-09T08:21:33.806817Z","shell.execute_reply.started":"2021-12-09T08:21:33.402742Z","shell.execute_reply":"2021-12-09T08:21:33.805768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are no missing values in the both datasets.\n\nLet's check feature values distribution in the both datasets.","metadata":{}},{"cell_type":"code","source":"df = pd.concat([train[num_features], test[num_features]], axis=0)\ncolumns = df.columns.values\n\ncols = 3\nrows = len(columns) // cols + 1\n\nfig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(16,20), sharex=False)\n\nplt.subplots_adjust(hspace = 0.3)\ni=0\n\nfor r in np.arange(0, rows, 1):\n    for c in np.arange(0, cols, 1):\n        if i >= len(columns):\n            axs[r, c].set_visible(False)\n        else:\n            hist1 = axs[r, c].hist(train[columns[i]].values,\n                                   range=(df[columns[i]].min(),\n                                          df[columns[i]].max()),\n                                   bins=40,\n                                   color=\"deepskyblue\",\n                                   edgecolor=\"black\",\n                                   alpha=0.7,\n                                   label=\"Train Dataset\")\n            hist2 = axs[r, c].hist(test[columns[i]].values,\n                                   range=(df[columns[i]].min(),\n                                          df[columns[i]].max()),\n                                   bins=40,\n                                   color=\"palevioletred\",\n                                   edgecolor=\"black\",\n                                   alpha=0.7,\n                                   label=\"Test Dataset\")\n            axs[r, c].set_title(columns[i], fontsize=12, pad=5)\n            axs[r, c].set_yticks(axs[r, c].get_yticks())\n            axs[r, c].set_yticklabels([str(int(i/1000))+\"k\" for i in axs[r, c].get_yticks()])\n            axs[r, c].tick_params(axis=\"y\", labelsize=10)\n            axs[r, c].tick_params(axis=\"x\", labelsize=10)\n            axs[r, c].grid(axis=\"y\")\n            if i == 0:\n                axs[r, c].legend(fontsize=10)\n                                  \n        i+=1\n#plt.suptitle(\"Numerical feature values distribution in both datasets\", y=0.99)\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-12-09T08:21:33.80974Z","iopub.execute_input":"2021-12-09T08:21:33.810177Z","iopub.status.idle":"2021-12-09T08:21:39.133325Z","shell.execute_reply.started":"2021-12-09T08:21:33.81012Z","shell.execute_reply":"2021-12-09T08:21:39.132507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.concat([train[cat_features], test[cat_features]], axis=0)\ncolumns = df.columns.values\n\ncols = 4\nrows = len(columns) // cols + 1\n\nfig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(16,40), sharex=False)\n\nplt.subplots_adjust(hspace = 0.3)\ni=0\n\nfor r in np.arange(0, rows, 1):\n    for c in np.arange(0, cols, 1):\n        if i >= len(columns):\n            axs[r, c].set_visible(False)\n        else:\n            hist1 = axs[r, c].hist(train[columns[i]].values,\n                                   range=(df[columns[i]].min(),\n                                          df[columns[i]].max()),\n                                   bins=40,\n                                   color=\"deepskyblue\",\n                                   edgecolor=\"black\",\n                                   alpha=0.7,\n                                   label=\"Train Dataset\")\n            hist2 = axs[r, c].hist(test[columns[i]].values,\n                                   range=(df[columns[i]].min(),\n                                          df[columns[i]].max()),\n                                   bins=40,\n                                   color=\"palevioletred\",\n                                   edgecolor=\"black\",\n                                   alpha=0.7,\n                                   label=\"Test Dataset\")\n            axs[r, c].set_title(columns[i], fontsize=12, pad=5)\n            axs[r, c].set_yticks(axs[r, c].get_yticks())\n            axs[r, c].set_yticklabels([str(int(i/1000))+\"k\" for i in axs[r, c].get_yticks()])\n            axs[r, c].tick_params(axis=\"y\", labelsize=10)\n            axs[r, c].tick_params(axis=\"x\", labelsize=10)\n            axs[r, c].grid(axis=\"y\")\n            if i == 0:\n                axs[r, c].legend(fontsize=10)\n                                  \n        i+=1\n#plt.suptitle(\"Categorical feature values distribution in both datasets\", y=0.99)\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-12-09T08:21:39.135493Z","iopub.execute_input":"2021-12-09T08:21:39.136098Z","iopub.status.idle":"2021-12-09T08:22:01.150258Z","shell.execute_reply.started":"2021-12-09T08:21:39.136059Z","shell.execute_reply":"2021-12-09T08:22:01.149137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It looks like soil types 7 and 15 does not have any examples. Let's check it. If so, they could be dropped from the datasets.","metadata":{}},{"cell_type":"code","source":"print(f\"Rows with soil type 7: {(train['Soil_Type7'] == 1).sum() + (test['Soil_Type7'] == 1).sum()}\")\nprint(f\"Rows with soil type 15: {(train['Soil_Type15'] == 1).sum() + (test['Soil_Type15'] == 1).sum()}\")","metadata":{"execution":{"iopub.status.busy":"2021-12-09T08:22:01.15181Z","iopub.execute_input":"2021-12-09T08:22:01.152669Z","iopub.status.idle":"2021-12-09T08:22:01.180584Z","shell.execute_reply.started":"2021-12-09T08:22:01.152629Z","shell.execute_reply":"2021-12-09T08:22:01.179884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.drop([\"Soil_Type7\", \"Soil_Type15\"], axis=1, inplace=True)\ntest.drop([\"Soil_Type7\", \"Soil_Type15\"], axis=1, inplace=True)\nfeatures.remove(\"Soil_Type7\")\nfeatures.remove(\"Soil_Type15\")\ncat_features.remove(\"Soil_Type7\")\ncat_features.remove(\"Soil_Type15\")","metadata":{"execution":{"iopub.status.busy":"2021-12-09T08:22:01.181836Z","iopub.execute_input":"2021-12-09T08:22:01.182783Z","iopub.status.idle":"2021-12-09T08:22:01.998128Z","shell.execute_reply.started":"2021-12-09T08:22:01.182742Z","shell.execute_reply":"2021-12-09T08:22:01.996901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Numerical features with the least amount of unique values:\")\ntrain[num_features].nunique().sort_values().head(5)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T08:22:01.999782Z","iopub.execute_input":"2021-12-09T08:22:02.000056Z","iopub.status.idle":"2021-12-09T08:22:02.499026Z","shell.execute_reply.started":"2021-12-09T08:22:02.000024Z","shell.execute_reply":"2021-12-09T08:22:02.497924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Some samples could have several wildernes area and soil types as you can see below.","metadata":{}},{"cell_type":"code","source":"display(train[['Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3', 'Wilderness_Area4']].sum(axis=1).value_counts().sort_index())\ndisplay(train[[x for x in train.columns if \"Soil_Type\" in x]].sum(axis=1).value_counts().sort_index())","metadata":{"execution":{"iopub.status.busy":"2021-12-09T08:22:02.500304Z","iopub.execute_input":"2021-12-09T08:22:02.501047Z","iopub.status.idle":"2021-12-09T08:22:03.410909Z","shell.execute_reply.started":"2021-12-09T08:22:02.500989Z","shell.execute_reply":"2021-12-09T08:22:03.409767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check how target distribution differs for samples different amount of said types.","metadata":{}},{"cell_type":"code","source":"print(\"Target distribution per amount of wildernes area types\")\ndf = train[['Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3', 'Wilderness_Area4']].sum(axis=1)\ndf_2 = pd.DataFrame(columns=[str(x) + \" wild_types\" for x in df.value_counts().sort_index().index],\n                    index=list(train[target].value_counts().sort_index().index))\ndf_2.fillna(0, inplace=True)\nfor i in df.value_counts().index:\n    total_samples = len(train.loc[df==i, target]) \n    samples_per_class = train.loc[df==i, target].value_counts().sort_index()\n    for sample_index in samples_per_class.index:\n        df_2.loc[sample_index, str(i) + \" wild_types\"] = round((samples_per_class[sample_index] * 100 / total_samples), 4)\ndf_2","metadata":{"execution":{"iopub.status.busy":"2021-12-09T08:22:03.413314Z","iopub.execute_input":"2021-12-09T08:22:03.413684Z","iopub.status.idle":"2021-12-09T08:22:03.79094Z","shell.execute_reply.started":"2021-12-09T08:22:03.413636Z","shell.execute_reply":"2021-12-09T08:22:03.789904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Target distribution per amount of soil types\")\ndf = train[[x for x in train.columns if \"Soil_Type\" in x]].sum(axis=1)\ndf_2 = pd.DataFrame(columns=[str(x) + \" soil_types\" for x in df.value_counts().sort_index().index],\n                    index=list(train[target].value_counts().sort_index().index))\ndf_2.fillna(0, inplace=True)\nfor i in df.value_counts().index:\n    total_samples = len(train.loc[df==i, target]) \n    samples_per_class = train.loc[df==i, target].value_counts().sort_index()\n    for sample_index in samples_per_class.index:\n        df_2.loc[sample_index, str(i) + \" soil_types\"] = round((samples_per_class[sample_index] * 100 / total_samples), 4)\ndf_2","metadata":{"execution":{"iopub.status.busy":"2021-12-09T08:22:03.794446Z","iopub.execute_input":"2021-12-09T08:22:03.794728Z","iopub.status.idle":"2021-12-09T08:22:04.978391Z","shell.execute_reply.started":"2021-12-09T08:22:03.794693Z","shell.execute_reply":"2021-12-09T08:22:04.977339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see proportion of some classes differs from the amount of wildernes area nad soil types. It's a good idea to add two new features showing the amount of said types per sample.","metadata":{}},{"cell_type":"markdown","source":"To be continued...","metadata":{}},{"cell_type":"code","source":"# %%time\n# from sklearn.decomposition import PCA, SparsePCA, KernelPCA\n# pca = PCA(n_components=2)\n# data_2D = pca.fit_transform(MinMaxScaler().fit_transform(train[cat_features]))\n# pca = SparsePCA(n_components=2)\n# data_2D = pca.fit_transform(MinMaxScaler().fit_transform(train[cat_features]))\n# pca = KernelPCA(n_components=2, eigen_solver='randomized', kernel=\"poly\")\n# data_2D = pca.fit_transform(MinMaxScaler().fit_transform(train[cat_features]).astype(\"float32\"))","metadata":{"execution":{"iopub.status.busy":"2021-12-09T08:22:04.980182Z","iopub.execute_input":"2021-12-09T08:22:04.980819Z","iopub.status.idle":"2021-12-09T08:22:04.984987Z","shell.execute_reply.started":"2021-12-09T08:22:04.980776Z","shell.execute_reply":"2021-12-09T08:22:04.984249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fig, ax = plt.subplots(figsize=(16,16))\n# # fig_colors = train[target].copy()\n# # fig_colors = fig_colors.map({1:\"lightcoral\", 2:\"sandybrown\", 3:\"darkorange\", 4:\"mediumseagreen\", 5:\"cornflowerblue\", 6:\"mediumpurple\"})\n\n# # scatter = ax.scatter(data_2D[:10000, 0], data_2D[:10000, 1],\n# #                  c=train.loc[:9999,target], cmap=\"hsv\",\n# #                  s=4)\n\n# scatter = ax.scatter(data_2D[:, 0], data_2D[:, 1],\n#                  c=train.loc[:,target], cmap=\"hsv\",\n#                  s=4)\n\n# legend = ax.legend(*scatter.legend_elements(),\n#                     loc=\"upper right\", title=\"target\", fontsize=\"large\")\n# ax.add_artist(legend)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T08:22:04.986087Z","iopub.execute_input":"2021-12-09T08:22:04.986778Z","iopub.status.idle":"2021-12-09T08:22:04.998778Z","shell.execute_reply.started":"2021-12-09T08:22:04.986724Z","shell.execute_reply":"2021-12-09T08:22:04.997917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Data preprocessing**","metadata":{}},{"cell_type":"code","source":"# Dropping a row which is the only one example of 5th class\ntrain.drop(train[train[target]==5].index, axis=0, inplace=True)\ntrain.reset_index(drop=True, inplace=True)\nlabel_enc = LabelEncoder()\nNUM_CLASSES = train[target].nunique()","metadata":{"execution":{"iopub.status.busy":"2021-12-09T08:22:04.99999Z","iopub.execute_input":"2021-12-09T08:22:05.000686Z","iopub.status.idle":"2021-12-09T08:22:06.040882Z","shell.execute_reply.started":"2021-12-09T08:22:05.00065Z","shell.execute_reply":"2021-12-09T08:22:06.039729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Transforming Aspect feature to be in [0, 359] range \n# train.loc[train[\"Aspect\"]<0, \"Aspect\"] = train.loc[train[\"Aspect\"]<0, \"Aspect\"] + 360\n# train.loc[train[\"Aspect\"]>=360, \"Aspect\"] = train.loc[train[\"Aspect\"]>=360, \"Aspect\"] - 360\n\n# test.loc[test[\"Aspect\"]<0, \"Aspect\"] = test.loc[test[\"Aspect\"]<0, \"Aspect\"] + 360\n# test.loc[test[\"Aspect\"]>=360, \"Aspect\"] = test.loc[test[\"Aspect\"]>=360, \"Aspect\"] - 360\n\n# train[\"Aspect\"].min(), train[\"Aspect\"].max(), test[\"Aspect\"].min(), test[\"Aspect\"].max()","metadata":{"execution":{"iopub.status.busy":"2021-12-09T08:22:06.04234Z","iopub.execute_input":"2021-12-09T08:22:06.043131Z","iopub.status.idle":"2021-12-09T08:22:06.049347Z","shell.execute_reply.started":"2021-12-09T08:22:06.043091Z","shell.execute_reply":"2021-12-09T08:22:06.047887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Clipping Hillshade features outliers to [0, 255] range \n\n# for df in [train, test]:\n#     df.loc[df[\"Hillshade_9am\"] < 0, \"Hillshade_9am\"] = 0\n#     df.loc[df[\"Hillshade_Noon\"] < 0, \"Hillshade_Noon\"] = 0\n#     df.loc[df[\"Hillshade_3pm\"] < 0, \"Hillshade_3pm\"] = 0\n#     df.loc[df[\"Hillshade_9am\"] > 255, \"Hillshade_9am\"] = 255\n#     df.loc[df[\"Hillshade_Noon\"] > 255, \"Hillshade_Noon\"] = 255\n#     df.loc[df[\"Hillshade_3pm\"] > 255, \"Hillshade_3pm\"] = 255\n    \n# train[[\"Hillshade_9am\", \"Hillshade_Noon\", \"Hillshade_3pm\"]].min(), \\\n# train[[\"Hillshade_9am\", \"Hillshade_Noon\", \"Hillshade_3pm\"]].max(), \\\n# test[[\"Hillshade_9am\", \"Hillshade_Noon\", \"Hillshade_3pm\"]].min(), \\\n# test[[\"Hillshade_9am\", \"Hillshade_Noon\", \"Hillshade_3pm\"]].max()","metadata":{"execution":{"iopub.status.busy":"2021-12-09T08:22:06.051126Z","iopub.execute_input":"2021-12-09T08:22:06.051411Z","iopub.status.idle":"2021-12-09T08:22:06.061147Z","shell.execute_reply.started":"2021-12-09T08:22:06.051376Z","shell.execute_reply":"2021-12-09T08:22:06.060214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # A new feature indicating that the patch is located lower than the closest water source\n# train[\"lower_than_water\"] = (train[\"Vertical_Distance_To_Hydrology\"] < 0).astype(\"int16\")\n# test[\"lower_than_water\"] = (test[\"Vertical_Distance_To_Hydrology\"] < 0).astype(\"int16\")\n# features.append(\"lower_than_water\")\n# cat_features.append(\"lower_than_water\")","metadata":{"execution":{"iopub.status.busy":"2021-12-09T08:22:06.062451Z","iopub.execute_input":"2021-12-09T08:22:06.062726Z","iopub.status.idle":"2021-12-09T08:22:06.075308Z","shell.execute_reply.started":"2021-12-09T08:22:06.062692Z","shell.execute_reply":"2021-12-09T08:22:06.074451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Adding two new features showing amount of different soil and wildernes area types\ntrain[\"wild_areas_sum\"] = train[['Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3', 'Wilderness_Area4']].sum(axis=1)\ntest[\"wild_areas_sum\"] = test[['Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3', 'Wilderness_Area4']].sum(axis=1)\n\ntrain[\"soil_types_sum\"] = train[[x for x in train.columns if \"Soil_Type\" in x]].sum(axis=1)\ntest[\"soil_types_sum\"] = test[[x for x in train.columns if \"Soil_Type\" in x]].sum(axis=1)\n\nfeatures.append(\"wild_areas_sum\")\nfeatures.append(\"soil_types_sum\")\ncat_features.append(\"wild_areas_sum\")\ncat_features.append(\"soil_types_sum\")","metadata":{"execution":{"iopub.status.busy":"2021-12-09T08:22:06.07659Z","iopub.execute_input":"2021-12-09T08:22:06.076992Z","iopub.status.idle":"2021-12-09T08:22:08.900482Z","shell.execute_reply.started":"2021-12-09T08:22:06.076954Z","shell.execute_reply":"2021-12-09T08:22:08.899552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# A new feature showing straight distance to a water source\ntrain[\"straight_dist_to_hydrology\"] = (train[\"Horizontal_Distance_To_Hydrology\"]**2 + train[\"Vertical_Distance_To_Hydrology\"]**2)**0.5\ntest[\"straight_dist_to_hydrology\"] = (test[\"Horizontal_Distance_To_Hydrology\"]**2 + test[\"Vertical_Distance_To_Hydrology\"]**2)**0.5\n\ntrain[\"sum_dist_to_hydrology\"] = train[\"Horizontal_Distance_To_Hydrology\"] + train[\"Vertical_Distance_To_Hydrology\"]\ntest[\"sum_dist_to_hydrology\"] = test[\"Horizontal_Distance_To_Hydrology\"] + test[\"Vertical_Distance_To_Hydrology\"]\n\nfeatures.append(\"straight_dist_to_hydrology\")\nfeatures.append(\"sum_dist_to_hydrology\")\nnum_features.append(\"straight_dist_to_hydrology\")\nnum_features.append(\"sum_dist_to_hydrology\")","metadata":{"execution":{"iopub.status.busy":"2021-12-09T08:25:06.231753Z","iopub.execute_input":"2021-12-09T08:25:06.233147Z","iopub.status.idle":"2021-12-09T08:25:06.47823Z","shell.execute_reply.started":"2021-12-09T08:25:06.233085Z","shell.execute_reply":"2021-12-09T08:25:06.47715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Standardizing and scaling features\ns_scaler = StandardScaler()\nfor col in num_features:\n    train[col] = s_scaler.fit_transform(np.array(train[col]).reshape(-1,1))\n    test[col] = s_scaler.transform(np.array(test[col]).reshape(-1,1))","metadata":{"execution":{"iopub.status.busy":"2021-12-09T08:25:06.48057Z","iopub.execute_input":"2021-12-09T08:25:06.480995Z","iopub.status.idle":"2021-12-09T08:25:15.563094Z","shell.execute_reply.started":"2021-12-09T08:25:06.480933Z","shell.execute_reply":"2021-12-09T08:25:15.561962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reducing datasets memory size due to converting columns into lighter formats\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df\n\ntrain = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T08:25:15.565651Z","iopub.execute_input":"2021-12-09T08:25:15.566081Z","iopub.status.idle":"2021-12-09T08:25:31.233418Z","shell.execute_reply.started":"2021-12-09T08:25:15.56603Z","shell.execute_reply":"2021-12-09T08:25:31.232639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_nn = train[features].copy()\nX_test_nn = test[features].copy()\ny = pd.Series(label_enc.fit_transform(train[target]))\n\n# # Generating OneHot encoded targets\n# ohe = OneHotEncoder(sparse=False)\n# y = ohe.fit_transform(np.array(train[target]).reshape(-1,1))\n# y[:5]","metadata":{"execution":{"iopub.status.busy":"2021-12-09T08:25:31.23526Z","iopub.execute_input":"2021-12-09T08:25:31.235893Z","iopub.status.idle":"2021-12-09T08:25:32.115281Z","shell.execute_reply.started":"2021-12-09T08:25:31.235815Z","shell.execute_reply":"2021-12-09T08:25:32.114286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_nn.columns","metadata":{"execution":{"iopub.status.busy":"2021-12-09T08:25:32.117617Z","iopub.execute_input":"2021-12-09T08:25:32.11797Z","iopub.status.idle":"2021-12-09T08:25:32.126746Z","shell.execute_reply.started":"2021-12-09T08:25:32.117928Z","shell.execute_reply":"2021-12-09T08:25:32.125425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mm_scaler = MinMaxScaler()\nfor col in X_nn.columns:\n    X_nn[col] = mm_scaler.fit_transform(np.array(X_nn[col]).reshape(-1,1))\n    X_test_nn[col] = mm_scaler.transform(np.array(X_test_nn[col]).reshape(-1,1))\n    \n# Transforming test data into tensors\nX_test_nn = torch.tensor(X_test_nn.to_numpy()).float()","metadata":{"execution":{"iopub.status.busy":"2021-12-09T08:25:32.128546Z","iopub.execute_input":"2021-12-09T08:25:32.128977Z","iopub.status.idle":"2021-12-09T08:25:37.774022Z","shell.execute_reply.started":"2021-12-09T08:25:32.128923Z","shell.execute_reply":"2021-12-09T08:25:37.773128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 4096","metadata":{"execution":{"iopub.status.busy":"2021-12-09T08:25:37.77565Z","iopub.execute_input":"2021-12-09T08:25:37.77622Z","iopub.status.idle":"2021-12-09T08:25:37.781348Z","shell.execute_reply.started":"2021-12-09T08:25:37.77617Z","shell.execute_reply":"2021-12-09T08:25:37.780269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_datasets(X_nn, X_valid_nn, y_nn, y_valid_nn, batch_size=BATCH_SIZE):\n    # Transforming data into tensors\n    X_nn = torch.tensor(X_nn.to_numpy(), dtype=torch.float32)\n    y_nn = torch.tensor(y_nn.to_numpy(), dtype=torch.long)\n    X_valid_nn = torch.tensor(X_valid_nn.to_numpy(), dtype=torch.float32)\n    y_valid_nn = torch.tensor(y_valid_nn.to_numpy(), dtype=torch.long)\n    \n    print(\"Using these datasets:\")\n    # Transforming tensors into tensor datasets\n    train_ds = TensorDataset(X_nn, y_nn)\n    valid_ds = TensorDataset(X_valid_nn, y_valid_nn)\n#     test_ds = TensorDataset(X_test_nn)\n    print(f\"Train_ds elements: {len(train_ds)}\")\n    print(f\"Valid_ds elements: {len(valid_ds)}\")\n#     print(f\"Test_ds elements: {len(test_ds)}\")\n\n    # Transforming into dataloader objects using batches\n    \n    train_ds = DataLoader(train_ds, batch_size, drop_last=False, num_workers=4)\n    valid_ds = DataLoader(valid_ds, batch_size, drop_last=False, num_workers=4)\n#     test_ds = DataLoader(test_ds, batch_size=BATCH_SIZE)\n\n    \n    for data, label in train_ds:\n        print(f\"Train_ds batch: {data.shape}, {label.shape}\")\n        break\n    for data, label in valid_ds:\n        print(f\"Valid_ds batch: {data.shape}, {label.shape}\")\n        break\n#     for data in test_ds:\n#         print(f\"Test_ds batch: {data[0].shape}\")\n#         break\n    return train_ds, valid_ds","metadata":{"execution":{"iopub.status.busy":"2021-12-09T08:25:37.783177Z","iopub.execute_input":"2021-12-09T08:25:37.783746Z","iopub.status.idle":"2021-12-09T08:25:37.795234Z","shell.execute_reply.started":"2021-12-09T08:25:37.783695Z","shell.execute_reply":"2021-12-09T08:25:37.79431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Model training**","metadata":{}},{"cell_type":"code","source":"# Computing class weights to be used during training\nclass_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y), y=y)\n# Reduce weight of the smallest classes in order to not give\n# them too much attention\nclass_weights[3] = class_weights[0]\nclass_weights[4] = class_weights[0]\nclass_weights = torch.tensor(class_weights, dtype=torch.float32)\nclass_weights","metadata":{"execution":{"iopub.status.busy":"2021-12-09T08:25:37.797249Z","iopub.execute_input":"2021-12-09T08:25:37.797806Z","iopub.status.idle":"2021-12-09T08:25:38.690578Z","shell.execute_reply.started":"2021-12-09T08:25:37.797757Z","shell.execute_reply":"2021-12-09T08:25:38.6893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# A function to initialize weights using Glorot normal initialization\ndef initialize_weights(m):\n    if isinstance(m, nn.Linear):\n        torch.nn.init.xavier_normal_(m.weight.data)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T08:25:38.692604Z","iopub.execute_input":"2021-12-09T08:25:38.693002Z","iopub.status.idle":"2021-12-09T08:25:38.699088Z","shell.execute_reply.started":"2021-12-09T08:25:38.692954Z","shell.execute_reply":"2021-12-09T08:25:38.697929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# A custom callback to log required parameters like metrics and learning rate\n# since TensorBoard is blocked on Kaggle.\nclass ParamsTracker(pl.callbacks.Callback):\n\n    def __init__(self, verbose=True):\n        self.verbose = verbose\n        # Defining empty lists for metric values\n        self.train_loss = []\n        self.train_acc = []\n        self.val_loss = []\n        self.val_acc = []\n        self.lr_epoch_start = []\n\n    \n    # Getting learning rate from an optimizer params at epoch start\n    def on_train_epoch_start(self, trainer, module):\n        current_learning_rate = trainer.optimizers[0].state_dict()[\"param_groups\"][0][\"lr\"]\n        self.lr_epoch_start.append(current_learning_rate)\n#         print(f\"Epoch start lr {current_learning_rate}\")\n        \n    def on_validation_epoch_end(self, trainer, module):\n        metrics_logs = trainer.logged_metrics\n        self.val_loss.append(metrics_logs[\"val_loss\"].item())\n        self.val_acc.append(metrics_logs[\"val_acc\"].item())\n    \n    # Getting last saved metrics from a trainer object and appending\n    # the required values to the corresponding lists\n    def on_train_epoch_end(self, trainer, module):\n        metrics_logs = trainer.logged_metrics\n        self.train_loss.append(metrics_logs[\"loss_epoch\"].item())\n        self.train_acc.append(metrics_logs[\"train_acc\"].item())\n        \n        # Print all metrics at the end of current epoch if verbose is set to True.\n        # It is done here because on_train_epoch_end event happens after on_validation_epoch_end event.\n        if self.verbose == True:\n            print(f\"Epoch {module.current_epoch} start learning rate: {self.lr_epoch_start[-1]:.6f}, \"\n                  f\"train_loss: {self.train_loss[-1]:.4f}, \"\n                  f\"train_acc: {self.train_acc[-1]:.4f}, \"\n                  f\"val_loss: {self.val_loss[-1]:.4f}, \"\n                  f\"val_acc: {self.val_acc[-1]:.4f}\")  ","metadata":{"execution":{"iopub.status.busy":"2021-12-09T08:25:38.702955Z","iopub.execute_input":"2021-12-09T08:25:38.703316Z","iopub.status.idle":"2021-12-09T08:25:38.714669Z","shell.execute_reply.started":"2021-12-09T08:25:38.703277Z","shell.execute_reply":"2021-12-09T08:25:38.71345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining model parameters\nclass Model(pl.LightningModule):\n    def __init__(self, input_shape, class_weights):\n        super().__init__()\n        \n        # Input layer\n        self.input = nn.Linear(input_shape,128)\n        # Hidden layers\n        self.hidden1 = nn.Linear(128,64)\n        self.hidden2 = nn.Linear(64,32)\n#         self.hidden3 = nn.Linear(32,16)\n        # Output layer\n        self.output = nn.Linear(32,6)\n        \n        # Dropout rate\n        self.dr = 0.05\n        # Activation functions\n        self.activation = F.relu\n        self.softmax = nn.Softmax(dim=1)\n        # Metrics\n        self.train_acc_metric = torchmetrics.Accuracy(num_classes=6, average=\"micro\")\n        self.val_acc_metric = torchmetrics.Accuracy(num_classes=6, average=\"micro\")\n        self.loss = nn.CrossEntropyLoss()#weight=class_weights)\n        \n        self.flag = False\n    \n    def forward(self, x):\n        x = self.activation(self.input(x))\n        x = F.dropout(x,p=self.dr,training=self.training)\n        x = self.activation(self.hidden1(x))\n        x = F.dropout(x,p=self.dr,training=self.training)\n        x = self.activation(self.hidden2(x))\n        x = F.dropout(x,p=self.dr,training=self.training)\n#         x = self.swish(self.hidden3(x))\n#         x = F.dropout(x,p=self.dr,training=self.training)\n#         x = self.softmax(self.output(x))\n        x = self.output(x)\n        return x\n    \n    # Training loop with loss and metric computing for each train data batch\n    def training_step(self, batch, batch_idx):\n        X, y = batch\n        y_hat = self(X).squeeze(1)\n#         if self.flag == False:\n#             print(y_hat)\n#             self.flag = True\n        loss = self.loss(y_hat, y)\n        self.train_acc_metric(torch.argmax(y_hat, dim=1), y)\n        self.log('loss', loss, prog_bar=True, on_epoch=True, logger=True)\n#         self.log('accuracy', self.train_acc_metric, prog_bar=True, on_epoch=False, logger=True)\n        return {'loss': loss,}\n\n    # Uses batch loss and metric values to compute and print\n    # overall train data loss and metric \n    def training_epoch_end(self, outputs):\n        train_acc = self.train_acc_metric.compute()\n        self.log('train_acc', train_acc, prog_bar=True, on_epoch=True, on_step=False, logger=True)\n        self.train_acc_metric.reset()\n\n\n\n    # Computes loss and metric score for each valid data batch\n    def validation_step(self, batch, batch_idx):\n        X, y = batch\n        y_hat = self(X).squeeze(1)\n        val_loss = self.loss(y_hat, y)\n        self.val_acc_metric(torch.argmax(y_hat, dim=1), y)\n        self.log('val_loss',val_loss, prog_bar=True, on_epoch=True, logger=True)\n        return {'val_loss': val_loss}\n\n    # Uses batch loss and metric values to compute and print\n    # overall valid data loss and metric\n    def validation_epoch_end(self, outputs):\n        val_acc = self.val_acc_metric.compute()\n        self.log('val_acc', val_acc, prog_bar=True, on_epoch=True, on_step=False, logger=True)\n        self.val_acc_metric.reset()\n        return {'val_acc': val_acc}\n\n       \n    def predict_step(self, X, batch_idx, dataloader_idx = None):\n        return self.softmax(self(X[0]))\n    \n    # Setting optimizer and learning rate scheduler parameters if any\n    def configure_optimizers(self):\n        optimizer = torch.optim.AdamW(self.parameters(), lr=1e-3, eps=1e-8, weight_decay=1e-2, amsgrad=False)\n#         optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n        #lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.9, last_epoch=-1, verbose=False)\n        lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5,\n                                                                  patience=7, min_lr=1e-04, eps=1e-08,\n                                                                  verbose=False, threshold=0.002, threshold_mode=\"abs\")\n        return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler, \"monitor\": \"val_acc\"}","metadata":{"execution":{"iopub.status.busy":"2021-12-09T08:25:38.716843Z","iopub.execute_input":"2021-12-09T08:25:38.717233Z","iopub.status.idle":"2021-12-09T08:25:38.74232Z","shell.execute_reply.started":"2021-12-09T08:25:38.717184Z","shell.execute_reply":"2021-12-09T08:25:38.741139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Trains the model using given train and valid datasets\n# and returns filepath of the best saved model\ndef train_ann(train_ds, valid_ds, class_weights, Model=Model, input_shape=X_nn.shape[1]):\n    \n    model = Model(input_shape, class_weights)\n    model.apply(initialize_weights)\n\n    # A callback to save the best model\n    checkpoint_callback = pl.callbacks.ModelCheckpoint(\n        dirpath=\"models\",\n        filename=f'model_' + '{val_acc:.4}',\n        monitor='val_acc',\n        mode='max',\n        save_weights_only=True)\n\n    # A callback to stop training when there is no improvement\n    early_stop_callback = EarlyStopping(\n        monitor='val_acc',\n        min_delta=0.0004,\n        patience=20,\n        verbose=False,\n        mode='max'\n    )\n\n#     # A callback to check learning rate if it is not constant\n#     lr_monitor = LearningRateMonitor(logging_interval='epoch')\n    \n    # A custom callback to print required parameters\n    params_tracker_callback = ParamsTracker(verbose=True)\n\n    # print(ModelSummary(model))\n\n    # Setting training parameters\n    trainer = pl.Trainer(\n        fast_dev_run=False,\n        max_epochs=60,\n    #         gpus=1,\n        precision=32,\n        limit_train_batches=1.0,\n        limit_val_batches=1.0, \n        num_sanity_val_steps=0,\n        check_val_every_n_epoch=1,\n        val_check_interval=1.0, \n        callbacks=[checkpoint_callback, early_stop_callback, params_tracker_callback],\n     )\n\n    # Training \n    trainer.fit(model, train_ds, valid_ds)\n\n    # Switching model to evaluation mode\n    model.eval()\n\n    # Getting path of the best saved model\n    best_model_path = checkpoint_callback.best_model_path\n    \n    return best_model_path, params_tracker_callback","metadata":{"execution":{"iopub.status.busy":"2021-12-09T08:25:38.743733Z","iopub.execute_input":"2021-12-09T08:25:38.744222Z","iopub.status.idle":"2021-12-09T08:25:38.760731Z","shell.execute_reply.started":"2021-12-09T08:25:38.744184Z","shell.execute_reply":"2021-12-09T08:25:38.759646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Fold splitting parameters\nsplits = 10\nskf = StratifiedKFold(n_splits=splits, shuffle=True, random_state=42)\n\n# Two zero-filled arrays for out-of-fold and test predictions\nnn_oof_preds = np.zeros((X_nn.shape[0],))\nnn_test_preds = np.zeros((X_test_nn.shape[0],6))\ntotal_mean_acc = 0\n\n# Generating folds and making training and prediction for each of them\nfor num, (train_idx, valid_idx) in enumerate(skf.split(X_nn, y)):\n#     if num > 0:\n#         break\n    print(f\"\\n\\n===Training with fold {num}\")\n    X_train, X_valid = X_nn.loc[train_idx], X_nn.loc[valid_idx]\n    y_train, y_valid = y.loc[train_idx], y.loc[valid_idx]\n    \n    # Preparing datasets\n    train_ds, valid_ds = prepare_datasets(X_train, X_valid, y_train, y_valid)\n    \n    # Training model\n    best_model_path, tracked_values = train_ann(train_ds, valid_ds, class_weights, Model, X_nn.shape[1])\n    \n    # Loading weights of the best model\n    model = Model(X_nn.shape[1], class_weights)\n    model.load_state_dict(torch.load(best_model_path)['state_dict'])\n    model.eval()\n    \n    # Making valid data preds and plotting their histogram\n    preds = np.argmax(model(torch.tensor(X_valid.to_numpy()).float()).detach().numpy(), axis=1)\n#     display(pd.DataFrame(preds).hist(bins=50))\n    print(preds)\n    \n    # Calculating and printing this fold's model ROC AUC score\n    fold_score = accuracy_score(y_valid, preds)\n    print(f\"\\n===Fold {num} valid data accuracy score is {fold_score}\")\n    \n    # Making test data preds and plotting their histogram\n    test_preds = nn.Softmax(dim=1)(model(X_test_nn)).detach().numpy()\n#     display(pd.DataFrame(test_preds).hist(bins=50))\n    \n    # Saving preds in corresponding arrays\n    nn_oof_preds[valid_idx] = preds\n    nn_test_preds += test_preds / splits\n    \n    total_mean_acc += fold_score / splits\n    \nprint(f\"Average accuracy score of all models is {total_mean_acc}\")","metadata":{"execution":{"iopub.status.busy":"2021-12-09T08:25:38.762543Z","iopub.execute_input":"2021-12-09T08:25:38.763103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = pd.DataFrame()\npredictions[\"Id\"] = test[\"Id\"]\npredictions[\"Cover_Type\"] = label_enc.inverse_transform(np.argmax(nn_test_preds, axis=1))\n\npredictions.to_csv('submission.csv', index=False, header=predictions.columns)\npredictions.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(14, 8))\n\nbars = ax.bar(predictions[\"Cover_Type\"].value_counts().sort_index().index,\n                  predictions[\"Cover_Type\"].value_counts().sort_index().values,\n                  color=colors,\n                  edgecolor=\"black\")\nax.set_title(\"Target distribution\", fontsize=20, pad=15)\nax.set_ylabel(\"Count\", fontsize=14, labelpad=15)\nax.set_xlabel(\"Target label\", fontsize=14, labelpad=20)\nax.tick_params(axis=\"x\", pad=20)\nax.bar_label(bars, predictions[\"Cover_Type\"].value_counts().sort_index().values,\n                 padding=3, fontsize=12)\nax.bar_label(bars, [f\"{x*100:2.1f}%\" for x in predictions[\"Cover_Type\"].value_counts().sort_index().values/len(train)],\n                 padding=-20, fontsize=12)\nax.margins(0.025, 0.06)\nax.grid(axis=\"y\")\n\nplt.show();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}