{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom catboost import CatBoostClassifier\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.metrics import accuracy_score\nimport optuna\n\n# Pandas setting to display more dataset rows and columns\npd.set_option('display.max_rows', 150)\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_colwidth', None)\npd.set_option('display.float_format', lambda x: '%.5f' % x)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-09T06:54:59.657686Z","iopub.execute_input":"2021-12-09T06:54:59.658015Z","iopub.status.idle":"2021-12-09T06:55:01.27166Z","shell.execute_reply.started":"2021-12-09T06:54:59.657934Z","shell.execute_reply":"2021-12-09T06:55:01.270237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Data import**","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/tabular-playground-series-dec-2021/train.csv\", low_memory=False)#, nrows=10000)\ntest = pd.read_csv(\"/kaggle/input/tabular-playground-series-dec-2021/test.csv\", low_memory=False)#, nrows=10000)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T06:55:01.273456Z","iopub.execute_input":"2021-12-09T06:55:01.273697Z","iopub.status.idle":"2021-12-09T06:55:32.316957Z","shell.execute_reply.started":"2021-12-09T06:55:01.273662Z","shell.execute_reply":"2021-12-09T06:55:32.316225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reducing datasets memory size due to converting columns into lighter formats\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df\n\ntrain = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T06:55:32.318273Z","iopub.execute_input":"2021-12-09T06:55:32.318565Z","iopub.status.idle":"2021-12-09T06:55:55.047598Z","shell.execute_reply.started":"2021-12-09T06:55:32.31853Z","shell.execute_reply":"2021-12-09T06:55:55.046847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info(memory_usage=\"deep\")","metadata":{"execution":{"iopub.status.busy":"2021-12-09T06:55:55.048692Z","iopub.execute_input":"2021-12-09T06:55:55.049354Z","iopub.status.idle":"2021-12-09T06:55:55.0695Z","shell.execute_reply.started":"2021-12-09T06:55:55.049297Z","shell.execute_reply":"2021-12-09T06:55:55.068676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.info(memory_usage=\"deep\")","metadata":{"execution":{"iopub.status.busy":"2021-12-09T06:55:55.071975Z","iopub.execute_input":"2021-12-09T06:55:55.072718Z","iopub.status.idle":"2021-12-09T06:55:55.196466Z","shell.execute_reply.started":"2021-12-09T06:55:55.072674Z","shell.execute_reply":"2021-12-09T06:55:55.195518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **EDA**","metadata":{}},{"cell_type":"code","source":"# Colors to be used for plots\ncolors = [\"lightcoral\", \"sandybrown\", \"darkorange\", \"mediumseagreen\",\n          \"lightseagreen\", \"cornflowerblue\", \"mediumpurple\", \"palevioletred\",\n          \"lightskyblue\", \"sandybrown\", \"yellowgreen\", \"indianred\",\n          \"lightsteelblue\", \"mediumorchid\", \"deepskyblue\"]","metadata":{"execution":{"iopub.status.busy":"2021-12-09T06:55:55.198137Z","iopub.execute_input":"2021-12-09T06:55:55.198457Z","iopub.status.idle":"2021-12-09T06:55:55.203059Z","shell.execute_reply.started":"2021-12-09T06:55:55.198416Z","shell.execute_reply":"2021-12-09T06:55:55.202339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-09T06:55:55.204286Z","iopub.execute_input":"2021-12-09T06:55:55.20472Z","iopub.status.idle":"2021-12-09T06:55:55.242404Z","shell.execute_reply.started":"2021-12-09T06:55:55.204684Z","shell.execute_reply":"2021-12-09T06:55:55.241668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target = \"Cover_Type\"\n\nfeatures = list(train.columns[1:55])","metadata":{"execution":{"iopub.status.busy":"2021-12-09T06:55:55.24351Z","iopub.execute_input":"2021-12-09T06:55:55.244195Z","iopub.status.idle":"2021-12-09T06:55:55.248627Z","shell.execute_reply.started":"2021-12-09T06:55:55.244158Z","shell.execute_reply":"2021-12-09T06:55:55.24778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[target].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-12-09T06:55:55.25007Z","iopub.execute_input":"2021-12-09T06:55:55.250566Z","iopub.status.idle":"2021-12-09T06:55:55.276829Z","shell.execute_reply.started":"2021-12-09T06:55:55.250529Z","shell.execute_reply":"2021-12-09T06:55:55.275981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(5, 6))\npie = ax.pie([len(train), len(test)],\n             labels=[\"Train dataset\", \"Test dataset\"],\n             colors=[\"salmon\", \"teal\"],\n             textprops={\"fontsize\": 15},\n             autopct='%1.1f%%')\nax.axis(\"equal\")\nax.set_title(\"Dataset length comparison\", fontsize=18)\nfig.set_facecolor('white')\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-12-09T06:55:55.278065Z","iopub.execute_input":"2021-12-09T06:55:55.278463Z","iopub.status.idle":"2021-12-09T06:55:55.401366Z","shell.execute_reply.started":"2021-12-09T06:55:55.278428Z","shell.execute_reply":"2021-12-09T06:55:55.400704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(14, 8))\n\nbars = ax.bar(train[target].value_counts().sort_index().index,\n                  train[target].value_counts().sort_index().values,\n                  color=colors,\n                  edgecolor=\"black\")\nax.set_title(\"Target distribution\", fontsize=20, pad=15)\nax.set_ylabel(\"Count\", fontsize=14, labelpad=15)\nax.set_xlabel(\"Target label\", fontsize=14, labelpad=20)\nax.tick_params(axis=\"x\", pad=20)\nax.bar_label(bars, train[target].value_counts().sort_index().values,\n                 padding=3, fontsize=12)\nax.bar_label(bars, [f\"{x*100:2.1f}%\" for x in train[target].value_counts().sort_index().values/len(train)],\n                 padding=-20, fontsize=12)\nax.margins(0.025, 0.06)\nax.grid(axis=\"y\")\n\n# pie = axs[1].pie(train[target].value_counts(sort=False).sort_index().values,\n#                  labels=train[target].value_counts(sort=False).sort_index().index,\n#                  colors=colors,\n#                  rotatelabels=True,\n#                  textprops={\"fontsize\": 14})\n# axs[1].axis(\"equal\")\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-12-09T06:55:55.402565Z","iopub.execute_input":"2021-12-09T06:55:55.402965Z","iopub.status.idle":"2021-12-09T06:55:55.745912Z","shell.execute_reply.started":"2021-12-09T06:55:55.40293Z","shell.execute_reply":"2021-12-09T06:55:55.745241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[features].describe()","metadata":{"execution":{"iopub.status.busy":"2021-12-09T06:55:55.747149Z","iopub.execute_input":"2021-12-09T06:55:55.747565Z","iopub.status.idle":"2021-12-09T06:55:58.932483Z","shell.execute_reply.started":"2021-12-09T06:55:55.747527Z","shell.execute_reply":"2021-12-09T06:55:58.931537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test[features].describe()","metadata":{"execution":{"iopub.status.busy":"2021-12-09T06:55:58.937077Z","iopub.execute_input":"2021-12-09T06:55:58.9379Z","iopub.status.idle":"2021-12-09T06:55:59.963141Z","shell.execute_reply.started":"2021-12-09T06:55:58.937853Z","shell.execute_reply":"2021-12-09T06:55:59.962201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.concat([train[features], test[features]], axis=0)\ndf.reset_index(inplace=True, drop=True)\n\nunique_values = df[features].nunique() < 10\ncat_features = list(unique_values[unique_values==True].index)\nunique_values = df[features].nunique() >= 10\nnum_features = list(unique_values[unique_values==True].index)\n\nprint(f\"There are {len(cat_features)} categorical features: {cat_features}\")\nprint(f\"\\nThere are {len(num_features)} continuous features: {num_features}\")","metadata":{"execution":{"iopub.status.busy":"2021-12-09T06:55:59.967499Z","iopub.execute_input":"2021-12-09T06:55:59.967718Z","iopub.status.idle":"2021-12-09T06:56:03.379281Z","shell.execute_reply.started":"2021-12-09T06:55:59.96769Z","shell.execute_reply":"2021-12-09T06:56:03.378494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.isna().sum().sum(), test.isna().sum().sum()","metadata":{"execution":{"iopub.status.busy":"2021-12-09T06:56:03.380477Z","iopub.execute_input":"2021-12-09T06:56:03.380812Z","iopub.status.idle":"2021-12-09T06:56:03.707858Z","shell.execute_reply.started":"2021-12-09T06:56:03.380771Z","shell.execute_reply":"2021-12-09T06:56:03.707186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are no missing values in the both datasets.\n\nLet's check feature values distribution in the both datasets.","metadata":{}},{"cell_type":"code","source":"df = pd.concat([train[num_features], test[num_features]], axis=0)\ncolumns = df.columns.values\n\ncols = 3\nrows = len(columns) // cols + 1\n\nfig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(16,20), sharex=False)\n\nplt.subplots_adjust(hspace = 0.3)\ni=0\n\nfor r in np.arange(0, rows, 1):\n    for c in np.arange(0, cols, 1):\n        if i >= len(columns):\n            axs[r, c].set_visible(False)\n        else:\n            hist1 = axs[r, c].hist(train[columns[i]].values,\n                                   range=(df[columns[i]].min(),\n                                          df[columns[i]].max()),\n                                   bins=40,\n                                   color=\"deepskyblue\",\n                                   edgecolor=\"black\",\n                                   alpha=0.7,\n                                   label=\"Train Dataset\")\n            hist2 = axs[r, c].hist(test[columns[i]].values,\n                                   range=(df[columns[i]].min(),\n                                          df[columns[i]].max()),\n                                   bins=40,\n                                   color=\"palevioletred\",\n                                   edgecolor=\"black\",\n                                   alpha=0.7,\n                                   label=\"Test Dataset\")\n            axs[r, c].set_title(columns[i], fontsize=12, pad=5)\n            axs[r, c].set_yticks(axs[r, c].get_yticks())\n            axs[r, c].set_yticklabels([str(int(i/1000))+\"k\" for i in axs[r, c].get_yticks()])\n            axs[r, c].tick_params(axis=\"y\", labelsize=10)\n            axs[r, c].tick_params(axis=\"x\", labelsize=10)\n            axs[r, c].grid(axis=\"y\")\n            if i == 0:\n                axs[r, c].legend(fontsize=10)\n                                  \n        i+=1\n#plt.suptitle(\"Numerical feature values distribution in both datasets\", y=0.99)\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-12-09T06:56:03.709031Z","iopub.execute_input":"2021-12-09T06:56:03.70928Z","iopub.status.idle":"2021-12-09T06:56:07.233455Z","shell.execute_reply.started":"2021-12-09T06:56:03.709245Z","shell.execute_reply":"2021-12-09T06:56:07.232793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.concat([train[cat_features], test[cat_features]], axis=0)\ncolumns = df.columns.values\n\ncols = 4\nrows = len(columns) // cols + 1\n\nfig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(16,40), sharex=False)\n\nplt.subplots_adjust(hspace = 0.3)\ni=0\n\nfor r in np.arange(0, rows, 1):\n    for c in np.arange(0, cols, 1):\n        if i >= len(columns):\n            axs[r, c].set_visible(False)\n        else:\n            hist1 = axs[r, c].hist(train[columns[i]].values,\n                                   range=(df[columns[i]].min(),\n                                          df[columns[i]].max()),\n                                   bins=40,\n                                   color=\"deepskyblue\",\n                                   edgecolor=\"black\",\n                                   alpha=0.7,\n                                   label=\"Train Dataset\")\n            hist2 = axs[r, c].hist(test[columns[i]].values,\n                                   range=(df[columns[i]].min(),\n                                          df[columns[i]].max()),\n                                   bins=40,\n                                   color=\"palevioletred\",\n                                   edgecolor=\"black\",\n                                   alpha=0.7,\n                                   label=\"Test Dataset\")\n            axs[r, c].set_title(columns[i], fontsize=12, pad=5)\n            axs[r, c].set_yticks(axs[r, c].get_yticks())\n            axs[r, c].set_yticklabels([str(int(i/1000))+\"k\" for i in axs[r, c].get_yticks()])\n            axs[r, c].tick_params(axis=\"y\", labelsize=10)\n            axs[r, c].tick_params(axis=\"x\", labelsize=10)\n            axs[r, c].grid(axis=\"y\")\n            if i == 0:\n                axs[r, c].legend(fontsize=10)\n                                  \n        i+=1\n#plt.suptitle(\"Categorical feature values distribution in both datasets\", y=0.99)\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-12-09T06:56:07.234436Z","iopub.execute_input":"2021-12-09T06:56:07.234658Z","iopub.status.idle":"2021-12-09T06:56:21.941724Z","shell.execute_reply.started":"2021-12-09T06:56:07.234625Z","shell.execute_reply":"2021-12-09T06:56:21.940939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It looks like soil types 7 and 15 does not have any examples. Let's check it. If so, they could be dropped from the datasets.","metadata":{}},{"cell_type":"code","source":"print(f\"Rows with soil type 7: {(train['Soil_Type7'] == 1).sum() + (test['Soil_Type7'] == 1).sum()}\")\nprint(f\"Rows with soil type 15: {(train['Soil_Type15'] == 1).sum() + (test['Soil_Type15'] == 1).sum()}\")","metadata":{"execution":{"iopub.status.busy":"2021-12-09T06:56:21.94296Z","iopub.execute_input":"2021-12-09T06:56:21.943662Z","iopub.status.idle":"2021-12-09T06:56:21.963692Z","shell.execute_reply.started":"2021-12-09T06:56:21.943624Z","shell.execute_reply":"2021-12-09T06:56:21.962992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.drop([\"Soil_Type7\", \"Soil_Type15\"], axis=1, inplace=True)\ntest.drop([\"Soil_Type7\", \"Soil_Type15\"], axis=1, inplace=True)\nfeatures.remove(\"Soil_Type7\")\nfeatures.remove(\"Soil_Type15\")\ncat_features.remove(\"Soil_Type7\")\ncat_features.remove(\"Soil_Type15\")","metadata":{"execution":{"iopub.status.busy":"2021-12-09T06:56:21.964753Z","iopub.execute_input":"2021-12-09T06:56:21.965004Z","iopub.status.idle":"2021-12-09T06:56:22.180376Z","shell.execute_reply.started":"2021-12-09T06:56:21.964971Z","shell.execute_reply":"2021-12-09T06:56:22.179692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Numerical features with the least amount of unique values:\")\ntrain[num_features].nunique().sort_values().head(5)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T06:56:22.181822Z","iopub.execute_input":"2021-12-09T06:56:22.182075Z","iopub.status.idle":"2021-12-09T06:56:22.404264Z","shell.execute_reply.started":"2021-12-09T06:56:22.182041Z","shell.execute_reply":"2021-12-09T06:56:22.403585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Some samples could have several wildernes area and soil types as you can see below.","metadata":{}},{"cell_type":"code","source":"display(train[['Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3', 'Wilderness_Area4']].sum(axis=1).value_counts().sort_index())\ndisplay(train[[x for x in train.columns if \"Soil_Type\" in x]].sum(axis=1).value_counts().sort_index())","metadata":{"execution":{"iopub.status.busy":"2021-12-09T06:56:22.405477Z","iopub.execute_input":"2021-12-09T06:56:22.406117Z","iopub.status.idle":"2021-12-09T06:56:22.72287Z","shell.execute_reply.started":"2021-12-09T06:56:22.406073Z","shell.execute_reply":"2021-12-09T06:56:22.722102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check how target distribution differs for samples different amount of said types.","metadata":{}},{"cell_type":"code","source":"print(\"Target distribution per amount of wildernes area types\")\ndf = train[['Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3', 'Wilderness_Area4']].sum(axis=1)\ndf_2 = pd.DataFrame(columns=[str(x) + \" wild_types\" for x in df.value_counts().sort_index().index],\n                    index=list(train[target].value_counts().sort_index().index))\ndf_2.fillna(0, inplace=True)\nfor i in df.value_counts().index:\n    total_samples = len(train.loc[df==i, target]) \n    samples_per_class = train.loc[df==i, target].value_counts().sort_index()\n    for sample_index in samples_per_class.index:\n        df_2.loc[sample_index, str(i) + \" wild_types\"] = round((samples_per_class[sample_index] * 100 / total_samples), 4)\ndf_2","metadata":{"execution":{"iopub.status.busy":"2021-12-09T06:56:22.72422Z","iopub.execute_input":"2021-12-09T06:56:22.724659Z","iopub.status.idle":"2021-12-09T06:56:22.949655Z","shell.execute_reply.started":"2021-12-09T06:56:22.72462Z","shell.execute_reply":"2021-12-09T06:56:22.948956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Target distribution per amount of soil types\")\ndf = train[[x for x in train.columns if \"Soil_Type\" in x]].sum(axis=1)\ndf_2 = pd.DataFrame(columns=[str(x) + \" soil_types\" for x in df.value_counts().sort_index().index],\n                    index=list(train[target].value_counts().sort_index().index))\ndf_2.fillna(0, inplace=True)\nfor i in df.value_counts().index:\n    total_samples = len(train.loc[df==i, target]) \n    samples_per_class = train.loc[df==i, target].value_counts().sort_index()\n    for sample_index in samples_per_class.index:\n        df_2.loc[sample_index, str(i) + \" soil_types\"] = round((samples_per_class[sample_index] * 100 / total_samples), 4)\ndf_2","metadata":{"execution":{"iopub.status.busy":"2021-12-09T06:56:22.950939Z","iopub.execute_input":"2021-12-09T06:56:22.951222Z","iopub.status.idle":"2021-12-09T06:56:23.477931Z","shell.execute_reply.started":"2021-12-09T06:56:22.951179Z","shell.execute_reply":"2021-12-09T06:56:23.477219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see proportion of some classes differs from the amount of wildernes area nad soil types. It's a good idea to add two new features showing the amount of said types per sample.","metadata":{}},{"cell_type":"markdown","source":"To be continued...","metadata":{}},{"cell_type":"markdown","source":"# **Data preprocessing**","metadata":{}},{"cell_type":"code","source":"# Dropping a row which is the only one example of 5th class\ntrain.drop(train[train[target]==5].index, axis=0, inplace=True)\ntrain.reset_index(drop=True, inplace=True)\nlabel_enc = LabelEncoder()\nNUM_CLASSES = train[target].nunique()","metadata":{"execution":{"iopub.status.busy":"2021-12-09T06:56:23.479229Z","iopub.execute_input":"2021-12-09T06:56:23.47966Z","iopub.status.idle":"2021-12-09T06:56:23.918316Z","shell.execute_reply.started":"2021-12-09T06:56:23.479621Z","shell.execute_reply":"2021-12-09T06:56:23.917562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Adding two new features\ntrain[\"wild_areas_sum\"] = train[['Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3', 'Wilderness_Area4']].sum(axis=1)\ntest[\"wild_areas_sum\"] = test[['Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3', 'Wilderness_Area4']].sum(axis=1)\n\ntrain[\"soil_types_sum\"] = train[[x for x in train.columns if \"Soil_Type\" in x]].sum(axis=1)\ntest[\"soil_types_sum\"] = test[[x for x in train.columns if \"Soil_Type\" in x]].sum(axis=1)\n\nfeatures.append(\"wild_areas_sum\")\nfeatures.append(\"soil_types_sum\")\ncat_features.append(\"wild_areas_sum\")\ncat_features.append(\"soil_types_sum\")","metadata":{"execution":{"iopub.status.busy":"2021-12-09T06:56:23.919856Z","iopub.execute_input":"2021-12-09T06:56:23.920117Z","iopub.status.idle":"2021-12-09T06:56:24.275931Z","shell.execute_reply.started":"2021-12-09T06:56:23.920084Z","shell.execute_reply":"2021-12-09T06:56:24.275211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"s_scaler = StandardScaler()\nfor col in num_features:\n    train[col] = s_scaler.fit_transform(np.array(train[col]).reshape(-1,1))\n    test[col] = s_scaler.transform(np.array(test[col]).reshape(-1,1))","metadata":{"execution":{"iopub.status.busy":"2021-12-09T06:56:24.277102Z","iopub.execute_input":"2021-12-09T06:56:24.277841Z","iopub.status.idle":"2021-12-09T06:56:25.259764Z","shell.execute_reply.started":"2021-12-09T06:56:24.277792Z","shell.execute_reply":"2021-12-09T06:56:25.258984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = train[features].copy()\nX_test = test[features].copy()\ny = pd.Series(label_enc.fit_transform(train[target]))","metadata":{"execution":{"iopub.status.busy":"2021-12-09T06:56:25.261152Z","iopub.execute_input":"2021-12-09T06:56:25.261431Z","iopub.status.idle":"2021-12-09T06:56:26.455194Z","shell.execute_reply.started":"2021-12-09T06:56:25.261396Z","shell.execute_reply":"2021-12-09T06:56:26.454265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Hyperparameters optimization**","metadata":{}},{"cell_type":"markdown","source":"Hyperparameters used in this notebook were optimized using Optuna. The code used or that is shown below. They are commented in order to save runtime as optimization has been already done.","metadata":{}},{"cell_type":"code","source":"# def train_model_optuna(trial, X_train, X_valid, y_train, y_valid):\n#     \"\"\"\n#     A function to train a model using different hyperparamerters combinations provided by Optuna. \n#     Loss of validation data predictions is returned to estimate hyperparameters effectiveness.\n#     \"\"\"\n    \n        \n#     #A set of hyperparameters to optimize by optuna\n#     cb_params = {\n#              \"iterations\": trial.suggest_categorical('iterations', [10000]),\n#              \"learning_rate\": trial.suggest_loguniform('learning_rate', 0.15, 1.0),\n#              \"loss_function\": trial.suggest_categorical(\"loss_function\", [\"MultiClass\"]),\n#              \"eval_metric\": trial.suggest_categorical(\"eval_metric\", [\"Accuracy\"]),\n#              \"l2_leaf_reg\": trial.suggest_loguniform('l2_leaf_reg', 1, 100),\n#              \"bagging_temperature\": trial.suggest_loguniform('bagging_temperature', 0.1, 20.0),\n#              \"random_strength\": trial.suggest_float('random_strength', 1.0, 2.0, step=0.01),\n#              \"depth\": trial.suggest_int('depth', 1, 10),\n#              \"grow_policy\": trial.suggest_categorical(\"grow_policy\", [\"SymmetricTree\"]),#, \"Depthwise\", \"Lossguide\"]),\n#              \"leaf_estimation_method\": trial.suggest_categorical(\"leaf_estimation_method\", [\"Gradient\"]),#, \"Exact\", \"Newton\"]),\n#              \"od_type\": trial.suggest_categorical(\"od_type\", [\"Iter\"]),\n#              \"early_stopping_rounds\": trial.suggest_categorical(\"early_stopping_rounds\", [100]),\n#              \"border_count\": trial.suggest_categorical(\"border_count\", [254]),\n#              \"use_best_model\": trial.suggest_categorical(\"use_best_model\", [True]),\n# #              \"bootstrap_type\": trial.suggest_categorical('bootstrap_type', [\"MVS\"]),\n# #              \"subsample\": trial.suggest_float('subsample', 0.1, 1.0, step=0.01),\n# #              \"sampling_frequency\": trial.suggest_categorical('sampling_frequency', [\"PerTree\", \"PerTreeLevel\"]),\n# #              \"sampling_unit\": trial.suggest_categorical('sampling_unit', [\"Object\", \"Group\"]),\n#              \"min_data_in_leaf\": trial.suggest_int('min_data_in_leaf', 1, 300),\n# #              \"rsm\": trial.suggest_float('rsm', 0.05, 1, step=0.05),\n        \n        \n\n# # #                  \"max_leaves\": trial.suggest_int('max_leaves', 1, 64),\n#              \"task_type\": trial.suggest_categorical('task_type', [\"GPU\"]),\n#              \"random_seed\": trial.suggest_categorical('random_seed', [42]),\n#                 }\n    \n\n\n\n\n#     # Model loading and training\n#     model = CatBoostClassifier(**cb_params)\n#     model.fit(\n#                 X_train, y_train,\n#                 eval_set=(X_valid, y_valid),\n#                 verbose=False,\n#             )\n    \n#     print(f\"Number of boosting rounds: {model.tree_count_}\")\n#     oof = model.predict(X_valid)\n    \n#     return accuracy_score(y_valid, oof)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T06:56:26.459906Z","iopub.execute_input":"2021-12-09T06:56:26.460182Z","iopub.status.idle":"2021-12-09T06:56:26.470289Z","shell.execute_reply.started":"2021-12-09T06:56:26.460148Z","shell.execute_reply":"2021-12-09T06:56:26.469686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %%time\n# # Splitting data into train and valid folds using target bins for stratification\n# X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\n# # Setting optuna verbosity to show only warning messages\n# # If the line is uncommeted each iteration results will be shown\n# # optuna.logging.set_verbosity(optuna.logging.WARNING)\n\n# time_limit = 3600 * 4\n\n# study = optuna.create_study(direction='maximize')\n# study.optimize(lambda trial: train_model_optuna(trial, X_train, X_valid,\n#                                                     y_train, y_valid),\n# #                n_trials = 2\n#                timeout=time_limit\n#               )\n\n# # Showing optimization results\n# print('Number of finished trials:', len(study.trials))\n# print('Best trial parameters:', study.best_trial.params)\n# print('Best score:', study.best_value)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T06:56:26.475012Z","iopub.execute_input":"2021-12-09T06:56:26.477485Z","iopub.status.idle":"2021-12-09T06:56:26.484068Z","shell.execute_reply.started":"2021-12-09T06:56:26.477448Z","shell.execute_reply":"2021-12-09T06:56:26.483225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Model training**","metadata":{}},{"cell_type":"code","source":"# Model hyperparameters\ncb_params = {'iterations': 10000,\n             'learning_rate': 0.218904169525507,\n             'loss_function': 'MultiClass',\n             'eval_metric': 'Accuracy',\n             'l2_leaf_reg': 1.6163189485316596,\n             'bagging_temperature': 0.14353551008899088,\n             'random_strength': 1.29,\n             'depth': 10,\n             'grow_policy': 'SymmetricTree',\n             'leaf_estimation_method': 'Gradient',\n             'od_type': 'Iter',\n             'early_stopping_rounds': 300,\n             'border_count': 254,\n             'use_best_model': True,\n             'min_data_in_leaf': 150,\n             'task_type': 'GPU',\n             'random_seed': 42}","metadata":{"execution":{"iopub.status.busy":"2021-12-09T06:56:26.488203Z","iopub.execute_input":"2021-12-09T06:56:26.489173Z","iopub.status.idle":"2021-12-09T06:56:26.497924Z","shell.execute_reply.started":"2021-12-09T06:56:26.489137Z","shell.execute_reply":"2021-12-09T06:56:26.497091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Setting up fold parameters\nsplits = 10\nskf = StratifiedKFold(n_splits=splits, shuffle=True, random_state=42)\n\n# Creating an array of zeros for storing \"out of fold\" predictions\noof_preds = np.zeros((X.shape[0],))\npreds = np.zeros((X_test.shape[0],len(np.unique(y))))\nmodel_fi = 0\ntotal_mean_acc = 0\n\n# Generating folds and making training and prediction for each of 10 folds\nfor num, (train_idx, valid_idx) in enumerate(skf.split(X, y)):\n    X_train, X_valid = X.loc[train_idx], X.loc[valid_idx]\n    y_train, y_valid = y.loc[train_idx], y.loc[valid_idx]\n    \n    model = CatBoostClassifier(**cb_params)\n    model.fit(X_train, y_train,\n              verbose=False,\n              eval_set=(X_valid, y_valid),\n              )\n    \n    # Getting mean test data predictions (i.e. devided by number of splits)\n    preds += model.predict_proba(X_test) / splits\n    \n    # Getting mean feature importances (i.e. devided by number of splits)\n    model_fi += model.feature_importances_ / splits\n    \n    # Getting validation data predictions. Each fold model makes predictions on an unseen data.\n    # So in the end it will be completely filled with unseen data predictions.\n    # It will be used to evaluate hyperparameters performance only.\n    oof_preds[valid_idx] = model.predict(X_valid).flatten()\n    \n    # Getting score for a fold model\n    fold_acc = accuracy_score(y_valid, oof_preds[valid_idx])\n    print(f\"Fold {num} accuracy: {fold_acc}\")\n\n    # Getting mean score of all fold models (i.e. devided by number of splits)\n    total_mean_acc += fold_acc / splits\n    \nprint(f\"\\nOverall ROC AUC: {total_mean_acc}\")","metadata":{"execution":{"iopub.status.busy":"2021-12-09T06:56:26.500879Z","iopub.execute_input":"2021-12-09T06:56:26.502227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Feature importances**","metadata":{}},{"cell_type":"code","source":"# Creating a dataframe to be used for plotting\ndf = pd.DataFrame()\ndf[\"Feature\"] = X.columns\n# Extracting feature importances from the trained model\ndf[\"Importance\"] = model_fi / model_fi.sum()\n# Sorting the dataframe by feature importance\ndf.sort_values(\"Importance\", axis=0, ascending=False, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(13, 30))\nbars = ax.barh(df[\"Feature\"], df[\"Importance\"], height=0.4,\n               color=\"mediumorchid\", edgecolor=\"black\")\nax.set_title(\"Feature importances\", fontsize=30, pad=15)\nax.set_ylabel(\"Feature name\", fontsize=20, labelpad=15)\nax.set_xlabel(\"Feature importance\", fontsize=20, labelpad=15)\nax.set_yticks(df[\"Feature\"])\nax.set_yticklabels(df[\"Feature\"], fontsize=13)\nax.tick_params(axis=\"x\", labelsize=15)\nax.grid(axis=\"x\")\n# Adding labels on top\nax2 = ax.secondary_xaxis('top')\nax2.set_xlabel(\"Feature importance\", fontsize=20, labelpad=13)\nax2.tick_params(axis=\"x\", labelsize=15)\nax.margins(0.05, 0.01)\n\n# Inverting y axis direction so the values are decreasing\nplt.gca().invert_yaxis()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Predictions submission**","metadata":{}},{"cell_type":"code","source":"predictions = pd.DataFrame()\npredictions[\"Id\"] = test[\"Id\"]\npredictions[\"Cover_Type\"] = label_enc.inverse_transform(np.argmax(preds, axis=1))\n\npredictions.to_csv('submission.csv', index=False, header=predictions.columns)\npredictions.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions[\"Cover_Type\"].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(14, 8))\n\nbars = ax.bar(predictions[\"Cover_Type\"].value_counts().sort_index().index,\n                  predictions[\"Cover_Type\"].value_counts().sort_index().values,\n                  color=colors,\n                  edgecolor=\"black\")\nax.set_title(\"Target distribution\", fontsize=20, pad=15)\nax.set_ylabel(\"Count\", fontsize=14, labelpad=15)\nax.set_xlabel(\"Target label\", fontsize=14, labelpad=20)\nax.tick_params(axis=\"x\", pad=20)\nax.bar_label(bars, predictions[\"Cover_Type\"].value_counts().sort_index().values,\n                 padding=3, fontsize=12)\nax.bar_label(bars, [f\"{x*100:2.1f}%\" for x in predictions[\"Cover_Type\"].value_counts().sort_index().values/len(train)],\n                 padding=-20, fontsize=12)\nax.margins(0.025, 0.06)\nax.grid(axis=\"y\")\n\nplt.show();","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}