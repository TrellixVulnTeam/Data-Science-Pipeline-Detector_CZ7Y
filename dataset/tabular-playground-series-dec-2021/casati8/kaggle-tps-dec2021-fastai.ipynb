{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# The december competition with Fastai v2","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"This notebook is a quick demonstration, who to use the Fastai v2 library for a Kaggle tabular competition. Fastai v2 is based on pytorch and allows you, to build a decent machine learning application. \nFor more information please visit the Fastai documentation: https://docs.fast.ai/","metadata":{}},{"cell_type":"code","source":"from fastai.tabular.all import * \nfrom fastai.test_utils import show_install\nfrom sklearn.ensemble import RandomForestRegressor\nshow_install()","metadata":{"execution":{"iopub.status.busy":"2021-12-31T16:43:14.9761Z","iopub.execute_input":"2021-12-31T16:43:14.976481Z","iopub.status.idle":"2021-12-31T16:43:15.063043Z","shell.execute_reply.started":"2021-12-31T16:43:14.97643Z","shell.execute_reply":"2021-12-31T16:43:15.062048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2021-12-31T16:43:15.065637Z","iopub.execute_input":"2021-12-31T16:43:15.06592Z","iopub.status.idle":"2021-12-31T16:43:15.071108Z","shell.execute_reply.started":"2021-12-31T16:43:15.065883Z","shell.execute_reply":"2021-12-31T16:43:15.070285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.random.seed(91)\ntorch.manual_seed(91)","metadata":{"execution":{"iopub.status.busy":"2021-12-31T16:43:15.072855Z","iopub.execute_input":"2021-12-31T16:43:15.073193Z","iopub.status.idle":"2021-12-31T16:43:15.084362Z","shell.execute_reply.started":"2021-12-31T16:43:15.073158Z","shell.execute_reply":"2021-12-31T16:43:15.083407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The data set is located in the follwoing directory ","metadata":{}},{"cell_type":"code","source":"path = Path('../input/tabular-playground-series-dec-2021')\nPath.BASE_PATH = path\npath.ls()","metadata":{"execution":{"iopub.status.busy":"2021-12-31T16:43:15.086195Z","iopub.execute_input":"2021-12-31T16:43:15.086531Z","iopub.status.idle":"2021-12-31T16:43:15.102195Z","shell.execute_reply.started":"2021-12-31T16:43:15.08643Z","shell.execute_reply":"2021-12-31T16:43:15.101551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I use Pandas to import them and to verify, where null values are there or some values are missing. The result shows, that the data set is complete, so that no additional data preparation is needed.  ","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(os.path.join(path, 'train.csv'))\ntest_df = pd.read_csv(os.path.join(path, 'test.csv'))\nsample_submission = pd.read_csv(os.path.join(path, 'sample_submission.csv'))\n\ntrain_df.isna().sum().sum(), test_df.isna().sum().sum(), train_df.isnull().sum().sum(), test_df.isnull().sum().sum()","metadata":{"execution":{"iopub.status.busy":"2021-12-31T16:43:15.104441Z","iopub.execute_input":"2021-12-31T16:43:15.104746Z","iopub.status.idle":"2021-12-31T16:43:29.895153Z","shell.execute_reply.started":"2021-12-31T16:43:15.104707Z","shell.execute_reply":"2021-12-31T16:43:29.894455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can specify whether pseudo lables are added and whether we want to duplicate the rows with specific Cover_Type's.","metadata":{}},{"cell_type":"code","source":"use_pseudo_lables = True\nduplicate_cover_types = True","metadata":{"execution":{"iopub.status.busy":"2021-12-31T16:43:29.896544Z","iopub.execute_input":"2021-12-31T16:43:29.89682Z","iopub.status.idle":"2021-12-31T16:43:29.902852Z","shell.execute_reply.started":"2021-12-31T16:43:29.896782Z","shell.execute_reply":"2021-12-31T16:43:29.902172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if use_pseudo_lables:\n    labels_df = pd.read_csv('../input/tps12-pseudolabels/tps12-pseudolabels_v2.csv')\n    train_df = pd.concat([train_df, labels_df], axis=0)\n    train_df.reset_index(drop=True)\n    labels_df.isna().sum().sum(), labels_df.isna().sum().sum()","metadata":{"execution":{"iopub.status.busy":"2021-12-31T16:43:29.904782Z","iopub.execute_input":"2021-12-31T16:43:29.90535Z","iopub.status.idle":"2021-12-31T16:43:37.667989Z","shell.execute_reply.started":"2021-12-31T16:43:29.905312Z","shell.execute_reply":"2021-12-31T16:43:37.667164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.describe().T","metadata":{"execution":{"iopub.status.busy":"2021-12-31T16:43:37.669422Z","iopub.execute_input":"2021-12-31T16:43:37.669688Z","iopub.status.idle":"2021-12-31T16:43:45.327111Z","shell.execute_reply.started":"2021-12-31T16:43:37.669652Z","shell.execute_reply":"2021-12-31T16:43:45.32632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Cover_Type is the depended value and should be predicted for the test data. I change the Cover_Type from int to category type. With this modification i was able to improve my public score from 0.93057 to 0.95542. The tabular model is unchanged for both runs. Let's see how many different values exists. I will delete the one row with Cover_Type=5. Later on i will combine the predictions of my neural network with some prediction from other other notebooks. These notebooks delete this row.","metadata":{}},{"cell_type":"code","source":"dep_var = 'Cover_Type'\nidx = train_df[train_df[dep_var] == 5].index\ntrain_df.drop(idx, axis = 0, inplace = True)\n\ntrain_df[dep_var] = train_df[dep_var].astype('category')","metadata":{"execution":{"iopub.status.busy":"2021-12-31T16:43:45.328349Z","iopub.execute_input":"2021-12-31T16:43:45.329906Z","iopub.status.idle":"2021-12-31T16:43:48.295221Z","shell.execute_reply.started":"2021-12-31T16:43:45.329855Z","shell.execute_reply":"2021-12-31T16:43:48.294459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.utils import shuffle\nif duplicate_cover_types:\n    print('Duplicate rows with Cover_Type 7')\n    seven_df  = shuffle( train_df.loc[train_df[dep_var] == 7], random_state=2520)\n\n    train_df = pd.concat([seven_df, train_df], axis=0)\n    del seven_df\n    train_df.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-31T16:43:48.296563Z","iopub.execute_input":"2021-12-31T16:43:48.296823Z","iopub.status.idle":"2021-12-31T16:43:50.015056Z","shell.execute_reply.started":"2021-12-31T16:43:48.296789Z","shell.execute_reply":"2021-12-31T16:43:50.01424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nunOfCoverTypes = len(train_df[dep_var].unique())\nnunOfCoverTypes, np.unique(train_df[dep_var], return_counts=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-31T16:43:50.016529Z","iopub.execute_input":"2021-12-31T16:43:50.016827Z","iopub.status.idle":"2021-12-31T16:43:50.177949Z","shell.execute_reply.started":"2021-12-31T16:43:50.016789Z","shell.execute_reply":"2021-12-31T16:43:50.177253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I will drop the column 'Id' fromm the data frames, because the values are unique and the don't add any usefull information to our model. The columns 'Soil_Type7' and 'Soil_type15' contain the value 0. Therefore they don't provide any new information to the model and i can drop them too.","metadata":{}},{"cell_type":"code","source":"train_df.drop(columns=['Id', 'Soil_Type7', 'Soil_Type15'], inplace=True)\ntest_df.drop(columns=['Id', 'Soil_Type7', 'Soil_Type15'], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-31T16:43:50.179272Z","iopub.execute_input":"2021-12-31T16:43:50.179541Z","iopub.status.idle":"2021-12-31T16:43:51.266608Z","shell.execute_reply.started":"2021-12-31T16:43:50.179506Z","shell.execute_reply":"2021-12-31T16:43:51.265836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems, that the values of some columns should be preprocessed. The contian 'strange' value at the first glance. These columns are 'Aspect', 'Hillshade_9am', 'Hillshade_Noon' and 'Hillshade_3pm'. There some other discussion items and notebooks in this competition, where more details described. I will show you my implementation here. To control, whether a preprocessing should be done, i set the following flag doPreprocessing to 'True'\n","metadata":{}},{"cell_type":"code","source":"doPreprocessing=True    ","metadata":{"execution":{"iopub.status.busy":"2021-12-31T16:43:51.267918Z","iopub.execute_input":"2021-12-31T16:43:51.268177Z","iopub.status.idle":"2021-12-31T16:43:51.272206Z","shell.execute_reply.started":"2021-12-31T16:43:51.268141Z","shell.execute_reply":"2021-12-31T16:43:51.271456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The column 'Aspect' stores values of an angle in degree. These values are periodic value with a frequency of 360 degree. I can correct these values to the interval [-360, 360].","metadata":{}},{"cell_type":"code","source":"def clipAspectValues(df):\n    pd.options.mode.chained_assignment = None\n    df[\"Aspect\"][df[\"Aspect\"] < 0] += 360\n    df[\"Aspect\"][df[\"Aspect\"] > 359] -= 360\n    df[\"Aspect_mod_360\"] = df[\"Aspect\"] % 360","metadata":{"execution":{"iopub.status.busy":"2021-12-31T16:43:51.276005Z","iopub.execute_input":"2021-12-31T16:43:51.276479Z","iopub.status.idle":"2021-12-31T16:43:51.282243Z","shell.execute_reply.started":"2021-12-31T16:43:51.276439Z","shell.execute_reply":"2021-12-31T16:43:51.281425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The values for the Hillshade_ columns shouldn't be outside the interval [0,255]. Therefore i will clip values to this interval.","metadata":{}},{"cell_type":"code","source":"def clipHillshadeValues(df):\n    \n    hill_features = [x for x in df.columns if x.startswith(\"Hillshade\")]\n    for col in hill_features:\n        df[col] = np.clip(df[col], a_min=0, a_max=255)\n        \n    df['Hillshade_Noon_is_Bright'] = (df['Hillshade_Noon'] == 255).astype(int)\n    df['Hillshade_9am_is_Zero'] = (df['Hillshade_9am'] == 0).astype(int)\n    df['hillshade_3pm_is_Zero'] = (df['Hillshade_3pm'] == 0).astype(int)","metadata":{"execution":{"iopub.status.busy":"2021-12-31T16:43:51.28354Z","iopub.execute_input":"2021-12-31T16:43:51.283818Z","iopub.status.idle":"2021-12-31T16:43:51.294414Z","shell.execute_reply.started":"2021-12-31T16:43:51.283782Z","shell.execute_reply":"2021-12-31T16:43:51.293676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's calculate the Euclidean and the Manhattan distance based on column values for 'Horizontal_Distance_To_Hydrology' and 'Vertical_Distance_To_Hydrology'","metadata":{}},{"cell_type":"code","source":"def calculateDistance(df):\n    df[\"Hydro_Dist_Eucl\"] = (df[\"Horizontal_Distance_To_Hydrology\"]**2 + \n                                df[\"Vertical_Distance_To_Hydrology\"]**2)**0.5\n    df[\"Hydro_Dist_Manh\"] = np.abs(df[\"Horizontal_Distance_To_Hydrology\"]) + np.abs(df[\"Vertical_Distance_To_Hydrology\"])","metadata":{"execution":{"iopub.status.busy":"2021-12-31T16:43:51.297673Z","iopub.execute_input":"2021-12-31T16:43:51.297888Z","iopub.status.idle":"2021-12-31T16:43:51.304618Z","shell.execute_reply.started":"2021-12-31T16:43:51.297855Z","shell.execute_reply":"2021-12-31T16:43:51.303899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def addCountValues(df):\n    soil_features = [x for x in df.columns if x.startswith(\"Soil_Type\")]\n    df[\"Soil_Type_Count\"] = df[soil_features].sum(axis=1)\n    df[soil_features] = df[soil_features].astype('category')\n\n    \n    wilderness_features = [x for x in df.columns if x.startswith(\"Wilderness_Area\")]\n    df[\"Wilderness_Area_Count\"] = df[wilderness_features].sum(axis = 1)\n    \n    hillshade_features = [x for x in df.columns if x.startswith(\"Hillshade\")]\n    df[\"Hillshade_Count\"] = df[hillshade_features].sum(axis = 1)","metadata":{"execution":{"iopub.status.busy":"2021-12-31T16:43:51.305711Z","iopub.execute_input":"2021-12-31T16:43:51.306003Z","iopub.status.idle":"2021-12-31T16:43:51.313903Z","shell.execute_reply.started":"2021-12-31T16:43:51.305964Z","shell.execute_reply":"2021-12-31T16:43:51.31306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's define a function to duplicate the entries for a specific cover type in the training data frame. I will use the function for oversampling entries with the Cover_Type==4 and Cover_Type==6","metadata":{}},{"cell_type":"code","source":"if doPreprocessing:\n    print(\"Let's start the preprocessing ..\")\n    clipAspectValues(train_df)\n    clipAspectValues(test_df)\n    clipHillshadeValues(train_df)\n    clipHillshadeValues(test_df)\n    calculateDistance(train_df)\n    calculateDistance(test_df)\n    addCountValues(train_df)\n    addCountValues(test_df)\n    print(\"Done ..\")\nelse:\n    print(\"No preprocessing ..\")","metadata":{"execution":{"iopub.status.busy":"2021-12-31T16:43:51.315478Z","iopub.execute_input":"2021-12-31T16:43:51.316235Z","iopub.status.idle":"2021-12-31T16:44:29.095924Z","shell.execute_reply.started":"2021-12-31T16:43:51.316189Z","shell.execute_reply":"2021-12-31T16:44:29.094345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's see how the different cover types are distributed now","metadata":{}},{"cell_type":"code","source":"np.unique(train_df[dep_var], return_counts=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-31T16:44:29.097307Z","iopub.execute_input":"2021-12-31T16:44:29.097863Z","iopub.status.idle":"2021-12-31T16:44:29.291483Z","shell.execute_reply.started":"2021-12-31T16:44:29.097822Z","shell.execute_reply":"2021-12-31T16:44:29.290672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"memory_usage_before = train_df.memory_usage().sum() / 1024**2\ntrain_df = df_shrink(train_df)\ntest_df = df_shrink(test_df)\nmemory_usage_after = train_df.memory_usage().sum() / 1024**2\n\nprint('Memory usage (MByte) before the shrinking:', memory_usage_before, ' , after shrinking: ', memory_usage_after)","metadata":{"execution":{"iopub.status.busy":"2021-12-31T16:44:29.292823Z","iopub.execute_input":"2021-12-31T16:44:29.293322Z","iopub.status.idle":"2021-12-31T16:44:29.931062Z","shell.execute_reply.started":"2021-12-31T16:44:29.293285Z","shell.execute_reply":"2021-12-31T16:44:29.930294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I need a list of the column names, which are candidates for category variables and which are no candidates, also called continous variables. The Fastai library offers the function 'cont_cat_split' to do this for us. You can use the optional parameter 'max_card' to specify the maximum number of unique values a column can have for a category variable. I will use the value 10, which is sufficient for this data set. Both lists are used later, to create a corresponding  model. The category variables are mapped into embeddings, the continous variables are mapped to simple linear model. The value for max_cards specifies the ratio between the category and continous variables. Lower max_card values reduces the number of categroies and and increases the number of continous variables. The value max_card=1 produces an empty continous variable list. All columns of the data frame are handled as continous variables.\nThe parameter dep_var specifies our depended variable 'Cover_Type'. Its column will be skiped when the category and contious variables are determined.","metadata":{}},{"cell_type":"code","source":"cont_vars, cat_vars = cont_cat_split(train_df, dep_var= dep_var,  max_card=10)\nlen(cat_vars), len(cont_vars), cat_vars, cont_vars, ","metadata":{"execution":{"iopub.status.busy":"2021-12-31T16:44:29.932422Z","iopub.execute_input":"2021-12-31T16:44:29.932703Z","iopub.status.idle":"2021-12-31T16:44:30.571902Z","shell.execute_reply.started":"2021-12-31T16:44:29.932666Z","shell.execute_reply":"2021-12-31T16:44:30.571199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for c in cat_vars:\n    print(c, train_df[c].nunique())","metadata":{"execution":{"iopub.status.busy":"2021-12-31T16:44:30.573085Z","iopub.execute_input":"2021-12-31T16:44:30.573466Z","iopub.status.idle":"2021-12-31T16:44:31.911994Z","shell.execute_reply.started":"2021-12-31T16:44:30.573427Z","shell.execute_reply":"2021-12-31T16:44:31.911179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The next step is to create a data loader. The Fastai library offers a powerful helper called 'TabularPandas'. It needs the data frame, list of the category and continous variables, the depened variable and a splitter. The splitter divides the data set into two parts: one for the training and one for the validation and for internal optimization step in each epoch. Let's use a rate of 5 to 1. I need a dataloader also, which is created from this TabularPandas instance. The helper function getData does this job and allows you, to get a small dataloader if you want to do a quick prototyping of your model. ","metadata":{}},{"cell_type":"code","source":"def getData(df, batchSize=1024, randomSplit=True, genSmallDataset=True):\n    \n  if genSmallDataset: \n    example_idx = np.random.choice(range(len(df)), 250000)\n    df = df.iloc[example_idx]\n  \n  splits = null\n  if randomSplit:  \n    splits = RandomSplitter(valid_pct=0.2, seed=718)(range_of(df))\n  else:\n    l = len(df)\n    splits = (L(np.arange(0, 0.8*l), use_list=True),\n              L(np.arange(0.8*l+1, l-1), use_list=True))\n  to_train = TabularPandas(df, \n                           [Categorify,  Normalize],\n                           cat_vars,\n                           cont_vars, \n                           splits=splits,  \n                           device = device,\n                           y_block=CategoryBlock(),\n                           y_names=dep_var) \n\n  return to_train.dataloaders(bs=batchSize)","metadata":{"execution":{"iopub.status.busy":"2021-12-31T16:44:31.913342Z","iopub.execute_input":"2021-12-31T16:44:31.913586Z","iopub.status.idle":"2021-12-31T16:44:31.922016Z","shell.execute_reply.started":"2021-12-31T16:44:31.913551Z","shell.execute_reply":"2021-12-31T16:44:31.921206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dls = getData(train_df, batchSize=4096, randomSplit=True, genSmallDataset=False)\nlen(dls.train), len(dls.valid), type(dls.train), dls.train.device","metadata":{"execution":{"iopub.status.busy":"2021-12-31T16:44:31.923491Z","iopub.execute_input":"2021-12-31T16:44:31.923763Z","iopub.status.idle":"2021-12-31T16:44:47.140277Z","shell.execute_reply.started":"2021-12-31T16:44:31.923727Z","shell.execute_reply":"2021-12-31T16:44:47.139517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"At least i create a learner pasing the dataloader into it. I use the default values for the internal layers as you can see in the reported summary. The model has two hidden layers with 200 and 100 elements as the default. You can change the structure of the hidden layer, using the paramter layers liks this 'layers=[128,64,64,16]'. The hidden layers uses a batch normalization and the ReLU activation function.","metadata":{}},{"cell_type":"code","source":"my_config = tabular_config(ps=0.25, embed_p=0.25, use_bn=True, bn_cont=True, y_range=(1, 8))\nlearn = tabular_learner(dls,\n                        n_out = nunOfCoverTypes,\n                        layers=[512,512,128,128,128,64,64],\n                        # layers=[128, 64, 64, 16], for the best score!\n                        config=my_config,\n                        metrics=[accuracy])\nlearn.summary()","metadata":{"execution":{"iopub.status.busy":"2021-12-31T16:44:47.141477Z","iopub.execute_input":"2021-12-31T16:44:47.141752Z","iopub.status.idle":"2021-12-31T16:44:52.763895Z","shell.execute_reply.started":"2021-12-31T16:44:47.141715Z","shell.execute_reply":"2021-12-31T16:44:52.763045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.lr_find()","metadata":{"execution":{"iopub.status.busy":"2021-12-31T16:44:52.765477Z","iopub.execute_input":"2021-12-31T16:44:52.765852Z","iopub.status.idle":"2021-12-31T16:45:03.438807Z","shell.execute_reply.started":"2021-12-31T16:44:52.765815Z","shell.execute_reply":"2021-12-31T16:45:03.438052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I will use a maximum learning rate of 3e-3. \nStarting the learning process is quite easy, i will run for 100 epochs and i will save the model with the best, with the lowest validation lost value. The Fastai library offers the SaveModelCallback callback. You must specify the file name only. The option with_opt=True stores the values of the optimizer also.\nYou will find the new file under models/kaggle_tps_dec2021.pth","metadata":{}},{"cell_type":"code","source":"learn.fit_one_cycle(150, 2e-3, wd=0.01, cbs=SaveModelCallback(fname='kaggle_tps_dec2021', with_opt=True)) ","metadata":{"execution":{"iopub.status.busy":"2021-12-31T16:45:03.440056Z","iopub.execute_input":"2021-12-31T16:45:03.440929Z","iopub.status.idle":"2021-12-31T18:50:41.341358Z","shell.execute_reply.started":"2021-12-31T16:45:03.440888Z","shell.execute_reply":"2021-12-31T18:50:41.340639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To calculate the predictions for this competition, i will load the best model from the training process. Best model means the model where the validation loss has the lowest value.","metadata":{}},{"cell_type":"code","source":"learn.load('kaggle_tps_dec2021')","metadata":{"execution":{"iopub.status.busy":"2021-12-31T18:50:41.342532Z","iopub.execute_input":"2021-12-31T18:50:41.343016Z","iopub.status.idle":"2021-12-31T18:50:41.387446Z","shell.execute_reply.started":"2021-12-31T18:50:41.342972Z","shell.execute_reply":"2021-12-31T18:50:41.386656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"let's look at the confusion matrix","metadata":{}},{"cell_type":"code","source":"interp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix(normalize=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-31T18:51:00.088999Z","iopub.execute_input":"2021-12-31T18:51:00.089292Z","iopub.status.idle":"2021-12-31T18:51:05.115152Z","shell.execute_reply.started":"2021-12-31T18:51:00.089259Z","shell.execute_reply":"2021-12-31T18:51:05.114463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I will use the test data frame to get the prediction for the submit. I can use a smaller batch, there are less entries in the test data frame.","metadata":{}},{"cell_type":"code","source":"dlt = learn.dls.test_dl(test_df, bs=4096) \nnn_preds, _ = learn.get_preds(dl=dlt) \nnn_preds.min(), nn_preds.max(), nn_preds.shape","metadata":{"execution":{"iopub.status.busy":"2021-12-31T18:51:38.366058Z","iopub.execute_input":"2021-12-31T18:51:38.366321Z","iopub.status.idle":"2021-12-31T18:51:42.185684Z","shell.execute_reply.started":"2021-12-31T18:51:38.366291Z","shell.execute_reply":"2021-12-31T18:51:42.184749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's load the prediction from a XGBoost model and combine them with our own predictions.","metadata":{}},{"cell_type":"code","source":"xgb_preds = pd.read_parquet('../input/reasonable-xgboost-model/reasonable_xgb_test.pq')","metadata":{"execution":{"iopub.status.busy":"2021-12-31T18:50:50.731538Z","iopub.execute_input":"2021-12-31T18:50:50.731891Z","iopub.status.idle":"2021-12-31T18:50:50.85709Z","shell.execute_reply.started":"2021-12-31T18:50:50.731845Z","shell.execute_reply":"2021-12-31T18:50:50.856324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_pred = (nn_preds.numpy() + xgb_preds.to_numpy())\n\nsample_submission[dep_var] = np.argmax(all_pred, axis=1) +1 \nsample_submission.to_csv(\"submission.csv\", index=False)\nsample_submission.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-12-31T18:51:47.857584Z","iopub.execute_input":"2021-12-31T18:51:47.85831Z","iopub.status.idle":"2021-12-31T18:51:49.57452Z","shell.execute_reply.started":"2021-12-31T18:51:47.85827Z","shell.execute_reply":"2021-12-31T18:51:49.5738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls -la ","metadata":{"execution":{"iopub.status.busy":"2021-12-31T18:56:33.577359Z","iopub.execute_input":"2021-12-31T18:56:33.577692Z","iopub.status.idle":"2021-12-31T18:56:34.358869Z","shell.execute_reply.started":"2021-12-31T18:56:33.577602Z","shell.execute_reply":"2021-12-31T18:56:34.358067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The End. You can use this notebook and feel free to modify and expand the model to get a better result. Show me your recommendations and results!😀","metadata":{}}]}