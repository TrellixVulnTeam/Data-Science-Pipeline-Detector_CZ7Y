{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This is my continuation of our base line model: https://www.kaggle.com/tqrahman/baseline-with-tf-and-feature-engineering\n\nIn this notebook, we will focus on the neural network architecture\n* Version 4\n    * implemented PCA to add first five components as additional features\n* Version 6\n    * increased the network size\n    * added Batch Normalization\n    * added Skip Connections\n* Version 8\n    * applied clustering\n    * removed frequncy encoding\n    * feature engineered distance-to-hydrology","metadata":{}},{"cell_type":"code","source":"# Imports\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\nimport tensorflow as tf","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reading in the data\n\ntrain = pd.read_csv('../input/tabular-playground-series-dec-2021/train.csv')\ntest = pd.read_csv('../input/tabular-playground-series-dec-2021/test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to reduce memory\n\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.\n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                #if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                #    df[col] = df[col].astype(np.float16)\n                #el\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB --> {:.2f} MB (Decreased by {:.1f}%)'.format(\n        start_mem, end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reducing memory usage\n\ntrain = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Viewing the number of observations per category in target variable\n\ntrain['Cover_Type'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Getting the all the soil column names\nsoil_columns = [col for col in train.columns if 'Soil_' in col]\n\n# Extracting winderness columns\nwild_columns = [col for col in train.columns if 'Wild' in col]\n\n# Categorical columns\ncat_cols = soil_columns + wild_columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"There are many dummy variables for 'soil_type'. We should check to see if each observation has one type or multiple types of soil. ","metadata":{}},{"cell_type":"code","source":"# Checking if an observation has more than one soil_type\n\ntrain[soil_columns].sum(axis=1).value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on this value count, there are some observations that have multiple soil_types. This might be important for the model to know.","metadata":{}},{"cell_type":"code","source":"# Adding the number of soil_types as an additional feature\n\ntrain['sum_soil_types'] = train[soil_columns].sum(axis=1)\ntest['sum_soil_types'] = test[soil_columns].sum(axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Clustering data based on soil_types","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import KMeans\n\nkmeans = KMeans(n_clusters=7)\ntrain['cluster'] = kmeans.fit_predict(train[soil_columns])\ntest['cluster'] = kmeans.predict(test[soil_columns])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Soil Variables\nIt will be a good idea to remove 'Soil_Type7' and 'Soil_Type15'because it is 0's for all observations. Therefore it is not informative and might add noise to the model.","metadata":{}},{"cell_type":"code","source":"# Remove columns 'Soil_Type7', 'Soil_Type15'\n\ntrain.drop(['Soil_Type7', 'Soil_Type15'], inplace=True, axis=1)\ntest.drop(['Soil_Type7', 'Soil_Type15'], inplace=True, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Hillshade\n\nHillshade is an \"image\" that ranges from 0-255. However some of the hillshade values are less than 0 or greater than 255. We will make an assumption that those were data entry errors and will clip them. If it is less than 0, we will set it to sero. If it is greater than 255, set it to 255.\n\nSome additional thoughts:\n* Set values under 0 to 0 and values greater than 255 to 255 for all Hillshade variables\n* Is clipping the best way to procede? Try just scaling instead of clipping\n* Remove the hillshade data that are NOT within the range between 0, 255","metadata":{}},{"cell_type":"code","source":"# Clipping the hillshade columns between 0 and 255\n\nhillshade_columns = [col for col in train.columns if 'Hillshade' in col]\n\nfor col in hillshade_columns:\n    train[col] = train[col].clip(0,255)\n    test[col] = test[col].clip(0,255)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Aspect\nAspect is in degress. It seems that it should be between 0-360 degrees. However some are below this range and exceeds this range. ","metadata":{}},{"cell_type":"code","source":"# Changing the range of Aspect to fall between 0 and 359\n\ntrain['Aspect'] = train['Aspect'].apply(lambda row: row%360)\ntest['Aspect'] = test['Aspect'].apply(lambda row: row%360)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Getting the features and target variables\n\nfeatures = [col for col in train.columns if col not in ['Id', 'Cover_Type']]\ntarget = 'Cover_Type'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Label encoding the target variable\n\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ntrain[target] = le.fit_transform(train[target])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Removing that single observation that has tree type '5' (or '4' after LabelEncoding)\n\ntrain = train.loc[train['Cover_Type'] != 4,].reset_index(drop=True)\n# train = train.loc[train['Cover_Type'] != 3,].reset_index(drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Getting the all the soil column names\nsoil_columns = [col for col in train.columns if 'Soil_' in col]\n\n# Extracting winderness columns\nwild_columns = [col for col in train.columns if 'Wild' in col]\n\n# Categorical columns\ncat_cols = soil_columns + wild_columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Processing using PCA ","metadata":{}},{"cell_type":"code","source":"# Getting the numerical features\n\nnum_columns = [col for col in train.columns if col not in cat_cols+['Id', 'Cover_Type']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scaling the data before applying PCA\n\nscaler = StandardScaler()\npca_train = scaler.fit_transform(train[num_columns])\npca_test = scaler.transform(test[num_columns])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking to see how many components are needed to explain the most variance\n\npca = PCA()\npca.fit(pca_train)\nprint(pca.explained_variance_ratio_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting scree plots\n\nPC_values = np.arange(pca.n_components_) + 1\nplt.plot(PC_values, pca.explained_variance_ratio_, 'o-', linewidth=2, color='blue')\nplt.title('Scree Plot')\nplt.xlabel('Principal Component')\nplt.ylabel('Variance Explained')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fitting a PCA \n\npca = PCA(n_components=4)\npca_train = pca.fit_transform(pca_train)\npca_test = pca.transform(pca_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Adding 4 components to the data\n\ntrain = pd.concat([train, pd.DataFrame(pca_train,columns=['comp1', 'comp2', 'comp3', 'comp4'])], axis=1)\ntest = pd.concat([test, pd.DataFrame(pca_test,columns=['comp1', 'comp2', 'comp3', 'comp4'])], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Processing for Neural Network","metadata":{}},{"cell_type":"markdown","source":"#### Frequency Encoding for 'Soil' columns\nThere are a lot of 'Soil_' types. It may add a lot of noise for the model. A possiblity is to use a frequency encoding instead of dummying the variable.","metadata":{}},{"cell_type":"code","source":"# # Getting the all the soil column names\n# soil_columns = [col for col in train.columns if 'Soil_' in col]\n\n# # Undummying the Soil_types\n# train['soil_type'] = train[soil_columns].idxmax(axis=1)\n# test['soil_type'] = test[soil_columns].idxmax(axis=1)\n\n# # Calculating the fequency encoding\n# soil_map = pd.Series(train['soil_type'].value_counts()/train.shape[0]).to_dict()\n\n# # Applying the frequency encoding\n# train['soil_type'] = train['soil_type'].map(soil_map)\n# test['soil_type'] = test['soil_type'].map(soil_map)\n\n# # Dropping all the 'Soil-Type' columns\n# train = train.drop(soil_columns, axis=1)\n# test = test.drop(soil_columns, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Wilderness Variables\n#### Frequency Encoding the 'Wilderness' column too","metadata":{}},{"cell_type":"code","source":"# Checking if an observation has more than one soil_type\n\ntrain[wild_columns].sum(axis=1).value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wild_means = KMeans(n_clusters=7)\ntrain['wild_cluster'] = wild_means.fit_predict(train[wild_columns])\ntest['wild_cluster'] = wild_means.predict(test[wild_columns])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_means = KMeans(n_clusters=7)\ntrain['total_cluster'] = total_means.fit_predict(train[cat_cols])\ntest['total_cluster'] = total_means.predict(test[cat_cols])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Adding the number of wild_types as an additional feature\n\ntrain['sum_wild_types'] = train[wild_columns].sum(axis=1)\ntest['sum_wild_types'] = test[wild_columns].sum(axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Distance to Hydrology\nThere is a vertical and horizontal distance. It might be good to combine it to combine the components into one","metadata":{}},{"cell_type":"code","source":"# Function finding the Euclidean distance\n\ndef combine_components(row):\n    return np.sqrt(np.square(row['Horizontal_Distance_To_Hydrology']) + np.square(row['Vertical_Distance_To_Hydrology']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Applying the function\n\ntrain['distance_to_hydrology'] = train.apply(combine_components, axis=1)\ntest['distance_to_hydrology'] = test.apply(combine_components, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Extracting winderness columns\n# wild_columns = [col for col in train.columns if 'Wild' in col]\n\n# # Undummying the wilderness_types\n# train['wild_type'] = train[wild_columns].idxmax(axis=1)\n# test['wild_type'] = test[wild_columns].idxmax(axis=1)\n\n# # Calculating the fequency encoding\n# wild_map = pd.Series(train['wild_type'].value_counts()/train.shape[0]).to_dict()\n\n# # Applying the frequency encoding\n# train['wild_type'] = train['wild_type'].map(wild_map)\n# test['wild_type'] = test['wild_type'].map(wild_map)\n\n# # Dropping all the 'Soil-Type' columns\n# train = train.drop(wild_columns, axis=1)\n# test = test.drop(wild_columns, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Processing for the Model","metadata":{}},{"cell_type":"code","source":"# Getting the features and target variables\n\ncolumns = soil_columns + ['Id', 'Cover_Type', 'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology'] + wild_columns\n\nfeatures = [col for col in train.columns if col not in columns]\ntarget = 'Cover_Type'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Splitting the data into train and test splot\n\n# X_train, X_valid, y_train, y_valid = train_test_split(\n#     train[features], \n#     train[target],\n#     stratify=train[target],\n#     test_size=0.1, \n#     random_state=0\n# )\n# print(f'Shape of X_train: {X_train.shape}')\n# print(f'Shape of y_train: {y_train.shape}')\n# print(f'Shape of X_valid: {X_valid.shape}')\n# print(f'Shape of y_valid: {y_valid.shape}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Scaling the data by fitting on X_train and scaling the rest\n\n# scaler = StandardScaler()\n\n# X_train = scaler.fit_transform(X_train)\n# X_valid = scaler.transform(X_valid)\n# # t = scaler.transform(test[features])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tensorflow Model","metadata":{}},{"cell_type":"code","source":"# Creating the model and compiling\n\ndef get_model(inputs):\n    \n    tf.keras.backend.clear_session()\n    \n    ## Setting the Inputs\n    inputs = tf.keras.Input(shape=(inputs))\n    x = inputs\n    \n    ## Dense Layers\n    \n    ### First layer\n    x = tf.keras.layers.Dense(256, activation='relu')(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    \n    ### Second layer\n    x = tf.keras.layers.Dense(128, activation='relu')(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    \n    ### Creating a skip conection\n    conn1 = tf.keras.layers.Concatenate()([inputs,x])\n    \n    ### Third Layer\n    x2 = tf.keras.layers.Dense(256, activation='relu')(conn1)\n    x2 = tf.keras.layers.BatchNormalization()(x2)\n    \n    ### Fourth layer\n    x2 = tf.keras.layers.Dense(128, activation='relu')(x2)\n    x2 = tf.keras.layers.BatchNormalization()(x2)\n    \n    ### Creating a skip conection\n    conn2 = tf.keras.layers.Concatenate()([x,x2])\n    \n    ### Fourth layer\n    x3 = tf.keras.layers.Dense(256, activation='relu')(conn2)\n    x3 = tf.keras.layers.BatchNormalization()(x3)\n    \n    ### Fifth layer\n    x3 = tf.keras.layers.Dense(128, activation='relu')(x3)\n    x3 = tf.keras.layers.BatchNormalization()(x3)\n    \n#     ### Sixth layer\n#     x3 = tf.keras.layers.Dense(64, activation='relu')(x3)\n#     x3 = tf.keras.layers.BatchNormalization()(x3)\n    \n    ## Output layer\n    output = tf.keras.layers.Dense(7, activation='softmax')(x3)\n\n    model = tf.keras.Model(inputs=inputs, outputs=output)\n    \n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-2),\n        loss=\"sparse_categorical_crossentropy\",\n        metrics=['acc']        \n    )\n    \n    ## Returning the model\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Function that creates a TF sequential model\n\n# def get_model(inputs):\n#     tf.keras.backend.clear_session()\n\n#     ## Creating a Sequential Model\n#     model = tf.keras.Sequential([\n#         tf.keras.layers.Dense(512, input_shape=(None,inputs), activation='relu'),\n#         tf.keras.layers.Dense(256, activation='relu'),\n#         tf.keras.layers.Dense(128, activation='relu'),\n#         tf.keras.layers.Dense(64, activation='relu'),\n#         tf.keras.layers.Dense(7, activation = 'softmax')\n#     ])\n    \n#     ## Compile \n#     model.compile(\n#         optimizer=\"adam\",\n#         loss=\"sparse_categorical_crossentropy\",\n#         metrics=['acc']\n#     )\n    \n#     return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Neural Network parameters\n\nEPOCHS = 50\nBATCH_SIZE = 2048\n\nplateau = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_acc',\n    factor = 0.5,\n    patience = 3,\n    verbose = 0,\n    mode = 'max'\n)\n    \nearly_stopping = tf.keras.callbacks.EarlyStopping(\n    monitor = 'val_acc', \n    min_delta = 1e-06, \n    patience = 6, \n    verbose = 0,\n    mode = 'max', \n    baseline = None,\n    restore_best_weights = True\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # K-fold Cross Validation model evaluation\n\n# X = X_train\n# y = y_train.values\n\n# FOLDS = 5\n# cv = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=0)\n\n# test_preds = np.zeros((1, 1))\n# scores = []\n# for fold, (train_idx, val_idx) in enumerate(cv.split(X, y)):\n    \n#     ## Extracting the training and validation set from a fold\n#     X_t, X_v = X[train_idx], X[val_idx]\n#     y_t, y_v = y[train_idx], y[val_idx]\n    \n#     print('------------------------------------------------------------------------')\n#     print(f'Training for fold {fold} ...')\n    \n#     ## Creating a model\n#     model = get_model(X_train.shape[1])\n\n#     ## Fit data to model\n#     model.fit(\n#         X_t,\n#         y_t,\n#         validation_data=(X_v, y_v),\n#         epochs=EPOCHS,\n#         batch_size=BATCH_SIZE,\n#         verbose=2,\n#         callbacks=[plateau, early_stopping]\n#     )\n    \n#     ## Predicting using the model\n#     y_pred = np.argmax(model.predict(X_v), axis=1)\n#     score = accuracy_score(y_v, y_pred)\n#     scores.append(score)\n#     print(f'>Fold: {fold} --> Accuracy: {score}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Printing the results from K-Fold\n\n# print(f'Accuracy for each fold: {scores}')\n# print(f'Mean of all the folds: {np.mean(scores):.4f}')\n# print(f'Standard Deviation of the folds: {np.std(scores):.4f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After running K-Fold cross validation, the mean was .9485 with a 0.0002 standard deviation. It seems like the model is fitting well. We will rerun the model but with all the data instead of splitting it into train and validation set.","metadata":{}},{"cell_type":"code","source":"# # Creating a model\n\n# model = get_model(train[features].shape[1])\n\n# # Fit data to model\n# model.fit(\n#     X_train,\n#     y_train,\n#     validation_data=(X_valid, y_valid),\n#     epochs=EPOCHS,\n#     batch_size=BATCH_SIZE,\n#     verbose=2,\n#     callbacks=[plateau, early_stopping]\n# )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Analysis\n### Confusion Matrix","metadata":{}},{"cell_type":"code","source":"# y_preds = model.predict(X_valid)\n# y_preds = y_preds.argmax(axis=1)\n# cm = confusion_matrix(y_preds, y_valid)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ## Get Class Labels\n# labels = le.classes_\n# class_names = labels\n\n# # Plot confusion matrix in a beautiful manner\n# fig = plt.figure(figsize=(12, 12))\n# ax= plt.subplot()\n# sns.heatmap(cm, annot=True, ax = ax, fmt = 'g'); #annot=True to annotate cells\n# # labels, title and ticks\n# ax.set_xlabel('Predicted', fontsize=20)\n# ax.xaxis.set_label_position('bottom')\n# plt.xticks(rotation=90)\n# ax.xaxis.set_ticklabels(class_names, fontsize = 10)\n# ax.xaxis.tick_bottom()\n\n# ax.set_ylabel('True', fontsize=20)\n# ax.yaxis.set_ticklabels(class_names, fontsize = 10)\n# plt.yticks(rotation=0)\n\n# plt.title('Refined Confusion Matrix', fontsize=20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predictions","metadata":{}},{"cell_type":"code","source":"# Scaling the train\n\nscaler = StandardScaler()\n\ntrain_scaled = scaler.fit_transform(train[features])\ntest_scaled = scaler.transform(test[features])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating a model\n\nmodel = get_model(train[features].shape[1])\n\n# Fit data to model\nmodel.fit(\n    train_scaled,\n    train[target],\n    epochs=EPOCHS,\n    batch_size=BATCH_SIZE,\n    verbose=2,\n    callbacks=[plateau, early_stopping]\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predicting on the test set\n\npreds = model.predict(test_scaled)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reversing the label encoder\n\nfinal_preds = le.inverse_transform(preds.argmax(axis=1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating a submission file\n\nsubmission = pd.DataFrame({'Id': test['Id'], 'Cover_Type': final_preds })\nsubmission.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}