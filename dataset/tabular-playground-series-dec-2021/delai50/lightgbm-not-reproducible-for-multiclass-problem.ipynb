{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"##### Fixing the seeds during your experimentation stage is important in order to be sure that your improvements are real and not due to randomness. \n\n##### In this problem I just wanted to give a try to the LightGBM model and I bumped into the following reproducibility issue (that drove me crazy):","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport lightgbm as lgb\nimport random\nimport os\nimport numpy as np\n\n# Function to fix some seeds\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n\nSEED = 42\nseed_everything(SEED)\n\n# Read train data\ndf_train = pd.read_csv(\"/kaggle/input/tabular-playground-series-dec-2021/train.csv\")\ndf_train.drop([\"Id\"], axis=1, inplace=True)\n\n# Only sample a smaller fraction of the train data for speed purposes\ndf_train = df_train.sample(frac=0.25, random_state=SEED)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T12:02:13.619784Z","iopub.execute_input":"2021-12-11T12:02:13.620462Z","iopub.status.idle":"2021-12-11T12:02:30.923635Z","shell.execute_reply.started":"2021-12-11T12:02:13.62041Z","shell.execute_reply":"2021-12-11T12:02:30.922641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I used a simple train-test split because it is only to show the no reproducible results:","metadata":{}},{"cell_type":"code","source":"y = df_train[\"Cover_Type\"]\nX = df_train.drop([\"Cover_Type\"], axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T12:02:30.925815Z","iopub.execute_input":"2021-12-11T12:02:30.926175Z","iopub.status.idle":"2021-12-11T12:02:31.785329Z","shell.execute_reply.started":"2021-12-11T12:02:30.92613Z","shell.execute_reply":"2021-12-11T12:02:31.784458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ok, now let's fit the model, predict on the test set, and calculate the accuracy score. Let's do it 3 times and check whether the scores are different:","metadata":{}},{"cell_type":"code","source":"params = {\n    \"objective\": \"multiclass\",\n    \"num_classes\": 7,\n    \"random_state\": SEED,\n}\n\nfor _ in range(3):\n    model = lgb.LGBMClassifier(**params)\n    model.fit(X_train, y_train)\n    y_test_pred = model.predict(X_test)\n    print(accuracy_score(y_test, y_test_pred))","metadata":{"execution":{"iopub.status.busy":"2021-12-11T12:02:31.786742Z","iopub.execute_input":"2021-12-11T12:02:31.787052Z","iopub.status.idle":"2021-12-11T12:03:43.053293Z","shell.execute_reply.started":"2021-12-11T12:02:31.787009Z","shell.execute_reply":"2021-12-11T12:03:43.052535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You (probably) see that the values are not the same, and the gap between those is significant (I said probably because sometimes some of them are the same!). ","metadata":{}},{"cell_type":"markdown","source":"Now we can try to repeat the same process for binary classification. First, convert the target to binary:","metadata":{}},{"cell_type":"code","source":"y = (df_train[\"Cover_Type\"] > 3).astype(int)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T12:03:43.054926Z","iopub.execute_input":"2021-12-11T12:03:43.05574Z","iopub.status.idle":"2021-12-11T12:03:43.805108Z","shell.execute_reply.started":"2021-12-11T12:03:43.055688Z","shell.execute_reply":"2021-12-11T12:03:43.804158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = {\n    \"objective\": \"binary\",\n    \"num_classes\": 1,\n    \"random_state\": SEED,\n}\n\nfor _ in range(3):\n    model = lgb.LGBMClassifier(**params)\n    model.fit(X_train, y_train)\n    y_test_pred = model.predict(X_test)\n    print(accuracy_score(y_test, y_test_pred))","metadata":{"execution":{"iopub.status.busy":"2021-12-11T12:03:43.806267Z","iopub.execute_input":"2021-12-11T12:03:43.806823Z","iopub.status.idle":"2021-12-11T12:04:00.696596Z","shell.execute_reply.started":"2021-12-11T12:03:43.806789Z","shell.execute_reply":"2021-12-11T12:04:00.695872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For binary classification the accuracy scores are the same. What happens with the multiclass problem? Is it doing something under the hood that I'm not aware of?","metadata":{}}]}