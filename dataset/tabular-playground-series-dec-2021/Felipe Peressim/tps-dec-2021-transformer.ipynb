{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-01T15:46:47.718517Z","iopub.execute_input":"2022-01-01T15:46:47.718897Z","iopub.status.idle":"2022-01-01T15:46:47.748605Z","shell.execute_reply.started":"2022-01-01T15:46:47.718789Z","shell.execute_reply":"2022-01-01T15:46:47.747766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\nimport warnings\nimport gc\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nwarnings.filterwarnings(\"ignore\")\n","metadata":{"execution":{"iopub.status.busy":"2022-01-01T15:46:47.750341Z","iopub.execute_input":"2022-01-01T15:46:47.750849Z","iopub.status.idle":"2022-01-01T15:46:48.573894Z","shell.execute_reply.started":"2022-01-01T15:46:47.750791Z","shell.execute_reply":"2022-01-01T15:46:48.573163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed=47","metadata":{"execution":{"iopub.status.busy":"2022-01-01T15:46:48.574958Z","iopub.execute_input":"2022-01-01T15:46:48.575215Z","iopub.status.idle":"2022-01-01T15:46:48.581397Z","shell.execute_reply.started":"2022-01-01T15:46:48.575181Z","shell.execute_reply":"2022-01-01T15:46:48.580567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate_model(model, x, y):\n    y_pred_prob = model.predict(x)\n    acc = accuracy_score(y, y_pred_prob)\n    return {'accuracy' : acc}","metadata":{"execution":{"iopub.status.busy":"2022-01-01T15:46:48.583281Z","iopub.execute_input":"2022-01-01T15:46:48.583955Z","iopub.status.idle":"2022-01-01T15:46:48.589298Z","shell.execute_reply.started":"2022-01-01T15:46:48.583902Z","shell.execute_reply":"2022-01-01T15:46:48.588533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\ndef split_sequences(sequences, n_steps):\n\tX, y = list(), list()\n\tfor i in range(len(sequences)):\n\t\t# find the end of this pattern\n\t\tend_ix = i + n_steps\n\t\t# check if we are beyond the dataset\n\t\tif end_ix > len(sequences):\n\t\t\tbreak\n\t\t# gather input and output parts of the pattern\n\t\tseq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1]\n\t\tX.append(seq_x)\n\t\ty.append(seq_y)\n\treturn np.array(X), np.array(y)","metadata":{"execution":{"iopub.status.busy":"2022-01-01T15:46:48.590806Z","iopub.execute_input":"2022-01-01T15:46:48.591365Z","iopub.status.idle":"2022-01-01T15:46:48.598793Z","shell.execute_reply.started":"2022-01-01T15:46:48.591329Z","shell.execute_reply":"2022-01-01T15:46:48.598002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\n\nfrom tensorflow.keras import layers\nfrom tensorflow import keras\n\n# source https://keras.io/examples/audio/transformer_asr/\n\nclass TransformerEncoder(layers.Layer):\n    def __init__(self, embed_dim, num_heads, feed_forward_dim, dropout_rate=0.1, activation=\"selu\"):\n        super().__init__()\n        self.attn = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.ffn = keras.Sequential(\n            [\n                layers.Dense(feed_forward_dim, activation=activation),\n                layers.Dense(embed_dim),\n            ]\n        )\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = layers.Dropout(dropout_rate)\n        self.dropout2 = layers.Dropout(dropout_rate)\n\n    def call(self, inputs, training):\n        attn_output = self.attn(inputs, inputs)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(inputs + attn_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        return self.layernorm2(out1 + ffn_output)\n\n\nclass Transformer(keras.Model):\n    def __init__(\n            self,\n            num_hid=64,  # embed_dim - num of features\n            time_steps=1,\n            nb_classes=7,\n            num_head=2,\n            num_feed_forward=128,  # pointwise dim\n            num_layers_enc=4,\n            dropout_rate=0.1,\n            activation=\"relu\"\n    ):\n        super().__init__()\n        self.numlayers_enc = num_layers_enc\n        self.enc_input = layers.Input((time_steps, num_hid))\n        self.encoder = keras.Sequential(\n            [self.enc_input]\n            + [\n                TransformerEncoder(num_hid, num_head, num_feed_forward, dropout_rate, activation)\n                for _ in range(num_layers_enc)\n            ]\n        )\n        self.GlobalAveragePooling1D = layers.GlobalAveragePooling1D(data_format='channels_last')\n        self.out = layers.Dense(units=nb_classes, activation='softmax')\n\n    def call(self, inputs):\n        #x =  Time2Vector(x.shape[-1])\n        x = self.encoder(inputs)\n        x = self.GlobalAveragePooling1D(x)\n        y = self.out(x)\n        return y","metadata":{"execution":{"iopub.status.busy":"2022-01-01T15:46:48.600172Z","iopub.execute_input":"2022-01-01T15:46:48.600422Z","iopub.status.idle":"2022-01-01T15:46:53.115313Z","shell.execute_reply.started":"2022-01-01T15:46:48.600389Z","shell.execute_reply":"2022-01-01T15:46:53.114528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/tabular-playground-series-dec-2021/train.csv', sep=',')","metadata":{"execution":{"iopub.status.busy":"2022-01-01T15:46:53.116994Z","iopub.execute_input":"2022-01-01T15:46:53.117516Z","iopub.status.idle":"2022-01-01T15:47:09.331039Z","shell.execute_reply.started":"2022-01-01T15:46:53.117478Z","shell.execute_reply":"2022-01-01T15:47:09.330238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train = train_df.drop(['Id', 'Soil_Type7','Soil_Type15', 'Cover_Type'], axis=1)\ny_train = train_df['Cover_Type']\ny_train = y_train.apply(lambda x : x - 1)\nx_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2, random_state=seed, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-01T15:47:09.332458Z","iopub.execute_input":"2022-01-01T15:47:09.332701Z","iopub.status.idle":"2022-01-01T15:47:14.586452Z","shell.execute_reply.started":"2022-01-01T15:47:09.332667Z","shell.execute_reply":"2022-01-01T15:47:14.585712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://www.kaggle.com/c/tabular-playground-series-dec-2021/discussion/293612\n\ndef r(x):\n    if x+180>360:\n        return x-180\n    else:\n        return x+180\n\ndef fe(df):\n    df['EHiElv'] = df['Horizontal_Distance_To_Roadways'] * df['Elevation']\n    df['EViElv'] = df['Vertical_Distance_To_Hydrology'] * df['Elevation']\n    df['Aspect2'] = df.Aspect.map(r)\n    ### source: https://www.kaggle.com/c/tabular-playground-series-dec-2021/discussion/293373\n    df[\"Aspect\"][df[\"Aspect\"] < 0] += 360\n    df[\"Aspect\"][df[\"Aspect\"] > 359] -= 360\n    df.loc[df[\"Hillshade_9am\"] < 0, \"Hillshade_9am\"] = 0\n    df.loc[df[\"Hillshade_Noon\"] < 0, \"Hillshade_Noon\"] = 0\n    df.loc[df[\"Hillshade_3pm\"] < 0, \"Hillshade_3pm\"] = 0\n    df.loc[df[\"Hillshade_9am\"] > 255, \"Hillshade_9am\"] = 255\n    df.loc[df[\"Hillshade_Noon\"] > 255, \"Hillshade_Noon\"] = 255\n    df.loc[df[\"Hillshade_3pm\"] > 255, \"Hillshade_3pm\"] = 255\n    ########\n    df['Highwater'] = (df.Vertical_Distance_To_Hydrology < 0).astype(int)\n    df['EVDtH'] = df.Elevation - df.Vertical_Distance_To_Hydrology\n    df['EHDtH'] = df.Elevation - df.Horizontal_Distance_To_Hydrology * 0.2\n    df['Euclidean_Distance_to_Hydrolody'] = (df['Horizontal_Distance_To_Hydrology']**2 + df['Vertical_Distance_To_Hydrology']**2)**0.5\n    df['Manhattan_Distance_to_Hydrolody'] = df['Horizontal_Distance_To_Hydrology'] + df['Vertical_Distance_To_Hydrology']\n    df['Hydro_Fire_1'] = df['Horizontal_Distance_To_Hydrology'] + df['Horizontal_Distance_To_Fire_Points']\n    df['Hydro_Fire_2'] = abs(df['Horizontal_Distance_To_Hydrology'] - df['Horizontal_Distance_To_Fire_Points'])\n    df['Hydro_Road_1'] = abs(df['Horizontal_Distance_To_Hydrology'] + df['Horizontal_Distance_To_Roadways'])\n    df['Hydro_Road_2'] = abs(df['Horizontal_Distance_To_Hydrology'] - df['Horizontal_Distance_To_Roadways'])\n    df['Fire_Road_1'] = abs(df['Horizontal_Distance_To_Fire_Points'] + df['Horizontal_Distance_To_Roadways'])\n    df['Fire_Road_2'] = abs(df['Horizontal_Distance_To_Fire_Points'] - df['Horizontal_Distance_To_Roadways'])\n    df['Hillshade_3pm_is_zero'] = (df.Hillshade_3pm == 0).astype(int)\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-01-01T15:47:14.588853Z","iopub.execute_input":"2022-01-01T15:47:14.589108Z","iopub.status.idle":"2022-01-01T15:47:14.600721Z","shell.execute_reply.started":"2022-01-01T15:47:14.589074Z","shell.execute_reply":"2022-01-01T15:47:14.600064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train = fe(x_train)\nx_test = fe(x_test)\n\n# Summed features pointed out by @craigmthomas (https://www.kaggle.com/c/tabular-playground-series-dec-2021/discussion/292823)\nsoil_features = [x for x in x_train.columns if x.startswith(\"Soil_Type\")]\nwilderness_features = [x for x in x_train.columns if x.startswith(\"Wilderness_Area\")]\n\nx_train[\"soil_type_count\"] = x_train[soil_features].sum(axis=1)\nx_test[\"soil_type_count\"] = x_test[soil_features].sum(axis=1)\n\nx_train[\"wilderness_area_count\"] = x_train[wilderness_features].sum(axis=1)\nx_test[\"wilderness_area_count\"] = x_test[wilderness_features].sum(axis=1)\n\nx_train['std'] = np.std(x_train, axis=1)\nx_test['std'] = np.std(x_test, axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-01-01T15:47:14.602109Z","iopub.execute_input":"2022-01-01T15:47:14.602624Z","iopub.status.idle":"2022-01-01T15:47:24.019319Z","shell.execute_reply.started":"2022-01-01T15:47:14.602588Z","shell.execute_reply":"2022-01-01T15:47:24.018579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sc = StandardScaler()\n\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)\nx_train = x_train[:, np.newaxis, :]\nx_test = x_test[:, np.newaxis, :]\nnb_classes = train_df['Cover_Type'].nunique()\ntime_steps = 1\nnum_features = x_train.shape[-1]","metadata":{"execution":{"iopub.status.busy":"2022-01-01T15:47:24.020701Z","iopub.execute_input":"2022-01-01T15:47:24.020986Z","iopub.status.idle":"2022-01-01T15:47:29.825833Z","shell.execute_reply.started":"2022-01-01T15:47:24.020951Z","shell.execute_reply":"2022-01-01T15:47:29.825098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_heads=2\nnum_layers_enc=1\nnum_feed_forward=64\n\nmodel = Transformer(num_hid=num_features,\n                        time_steps=time_steps,\n                        nb_classes=nb_classes,\n                        num_head=num_heads,\n                        num_layers_enc=num_layers_enc,\n                        num_feed_forward=num_feed_forward)\n\nopt = tf.keras.optimizers.Adam()\nloss = tf.keras.losses.SparseCategoricalCrossentropy()\nmodel.compile(optimizer=opt, loss=loss, metrics=['accuracy'])\nmodel.fit(x_train, y_train, epochs=400, batch_size=1024, verbose=1)\nprint()\nresults = model.evaluate(x_test, y_test)[1]\nprint(results)","metadata":{"execution":{"iopub.status.busy":"2022-01-01T15:47:29.826971Z","iopub.execute_input":"2022-01-01T15:47:29.828782Z","iopub.status.idle":"2022-01-01T16:55:13.597812Z","shell.execute_reply.started":"2022-01-01T15:47:29.82875Z","shell.execute_reply":"2022-01-01T16:55:13.597112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"del train_df, x_train, y_train, x_test, y_test\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-01-01T16:55:30.977451Z","iopub.execute_input":"2022-01-01T16:55:30.977719Z","iopub.status.idle":"2022-01-01T16:55:31.192408Z","shell.execute_reply.started":"2022-01-01T16:55:30.977688Z","shell.execute_reply":"2022-01-01T16:55:31.191716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv('/kaggle/input/tabular-playground-series-dec-2021/test.csv', sep=',')\nx_test = test_df.drop(['Id', 'Soil_Type7','Soil_Type15'], axis=1)\nx_test = fe(x_test)\nx_test[\"soil_type_count\"] = x_test[soil_features].sum(axis=1)\nx_test[\"wilderness_area_count\"] = x_test[wilderness_features].sum(axis=1)\nx_test['std'] = np.std(x_test, axis=1)\nx_test = sc.transform(x_test)\nx_test = x_test[:, np.newaxis, :]","metadata":{"execution":{"iopub.status.busy":"2022-01-01T16:55:31.328066Z","iopub.execute_input":"2022-01-01T16:55:31.328561Z","iopub.status.idle":"2022-01-01T16:55:37.667324Z","shell.execute_reply.started":"2022-01-01T16:55:31.328521Z","shell.execute_reply":"2022-01-01T16:55:37.6666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target = model.predict(x_test)\nclass_preds = np.argmax(target, axis=-1) + 1\nids = test_df['Id'].values\nsubmission = pd.DataFrame({'Id' : ids, 'Cover_Type' : class_preds})","metadata":{"execution":{"iopub.status.busy":"2022-01-01T16:55:37.668736Z","iopub.execute_input":"2022-01-01T16:55:37.668986Z","iopub.status.idle":"2022-01-01T16:56:13.346572Z","shell.execute_reply.started":"2022-01-01T16:55:37.668952Z","shell.execute_reply":"2022-01-01T16:56:13.345853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-01T16:56:13.348288Z","iopub.execute_input":"2022-01-01T16:56:13.348539Z","iopub.status.idle":"2022-01-01T16:56:13.362526Z","shell.execute_reply.started":"2022-01-01T16:56:13.348505Z","shell.execute_reply":"2022-01-01T16:56:13.361858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-01T16:56:13.36406Z","iopub.execute_input":"2022-01-01T16:56:13.364367Z","iopub.status.idle":"2022-01-01T16:56:14.910973Z","shell.execute_reply.started":"2022-01-01T16:56:13.364332Z","shell.execute_reply":"2022-01-01T16:56:14.91022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}