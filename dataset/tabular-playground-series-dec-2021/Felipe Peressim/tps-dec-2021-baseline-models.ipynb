{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-12T02:01:21.023764Z","iopub.execute_input":"2021-12-12T02:01:21.023964Z","iopub.status.idle":"2021-12-12T02:01:21.032896Z","shell.execute_reply.started":"2021-12-12T02:01:21.023939Z","shell.execute_reply":"2021-12-12T02:01:21.032174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\nimport warnings\nimport gc\nimport tensorflow as tf\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.pipeline import Pipeline\n\nfrom xgboost import XGBClassifier\n\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2021-12-12T02:01:21.049372Z","iopub.execute_input":"2021-12-12T02:01:21.049884Z","iopub.status.idle":"2021-12-12T02:01:26.12699Z","shell.execute_reply.started":"2021-12-12T02:01:21.049854Z","shell.execute_reply":"2021-12-12T02:01:26.126213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed = 47","metadata":{"execution":{"iopub.status.busy":"2021-12-12T02:01:26.130861Z","iopub.execute_input":"2021-12-12T02:01:26.131072Z","iopub.status.idle":"2021-12-12T02:01:26.135902Z","shell.execute_reply.started":"2021-12-12T02:01:26.131046Z","shell.execute_reply":"2021-12-12T02:01:26.135095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate_model(model, x, y):\n    y_pred_prob = model.predict(x)\n    acc = accuracy_score(y, y_pred_prob)\n    return {'accuracy' : acc}","metadata":{"execution":{"iopub.status.busy":"2021-12-12T02:03:10.810797Z","iopub.execute_input":"2021-12-12T02:03:10.811063Z","iopub.status.idle":"2021-12-12T02:03:10.81514Z","shell.execute_reply.started":"2021-12-12T02:03:10.811029Z","shell.execute_reply":"2021-12-12T02:03:10.814491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_xgboost_model(params=None):\n    if params is None:\n        params = {'colsample_bytree': 0.1,\n                  'eta': 0.12,\n                  'gamma': 5, \n                  'max_depth': 2,\n                  'min_child_weight': 9,\n                  'n_estimators': 1000, \n                  'subsample': 0.9}          \n\n    return XGBClassifier(**params,\n                         objective='multi:softmax',\n                         random_state=seed, \n                         tree_method='gpu_hist', \n                         predictor='gpu_predictor',\n                         early_stopping_rounds=200,\n                         verbosity=0)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T02:03:11.602731Z","iopub.execute_input":"2021-12-12T02:03:11.60357Z","iopub.status.idle":"2021-12-12T02:03:11.610113Z","shell.execute_reply.started":"2021-12-12T02:03:11.603507Z","shell.execute_reply":"2021-12-12T02:03:11.609236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_nn_model(n_layers=None, n_units=32, activation='swish'):\n    model = tf.keras.Sequential()\n    \n    if n_layers is not None and n_layers > 0:\n        for _ in range(n_layers):\n            model.add(tf.keras.layers.Dense(units=n_units, activation=activation))\n    model.add(tf.keras.layers.Dense(units=7, activation='softmax'))\n    model.compile(optimizer=tf.keras.optimizers.Adam(),\n                  loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n                  metrics=[tf.metrics.SparseCategoricalAccuracy()])\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-12-12T02:03:12.249899Z","iopub.execute_input":"2021-12-12T02:03:12.250574Z","iopub.status.idle":"2021-12-12T02:03:12.25705Z","shell.execute_reply.started":"2021-12-12T02:03:12.250531Z","shell.execute_reply":"2021-12-12T02:03:12.256225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_pipelines(model):\n    pipelines = list()\n    # normalize\n    p = Pipeline([('s',MinMaxScaler()), ('m',model)])\n    pipelines.append(('norm', p))\n    # standardize\n    p = Pipeline([('s',StandardScaler()), ('m',model)])\n    pipelines.append(('std', p))\n    # quantile\n    p = Pipeline([('s',QuantileTransformer(n_quantiles=100, output_distribution='normal')), ('m',model)])\n    pipelines.append(('quan', p))\n    # pca\n    p = Pipeline([('s',PCA()), ('m',model)])\n    pipelines.append(('pca', p))\n    # svd\n    p = Pipeline([('s',TruncatedSVD()), ('m',model)])\n    pipelines.append(('svd', p))\n    \n    p = Pipeline([('s',StandardScaler()), ('p', PowerTransformer()), ('m',model)])\n    pipelines.append(('std-power', p))\n    # scale and power\n    p = Pipeline([('s',MinMaxScaler()), ('p', PowerTransformer()), ('m',model)])\n    pipelines.append(('min-max-power', p))\n    \n    p = Pipeline([('p', PowerTransformer()), ('m',model)])\n    pipelines.append(('power', p))\n    \n    return pipelines","metadata":{"execution":{"iopub.status.busy":"2021-12-07T01:55:53.312489Z","iopub.execute_input":"2021-12-07T01:55:53.312751Z","iopub.status.idle":"2021-12-07T01:55:53.325492Z","shell.execute_reply.started":"2021-12-07T01:55:53.312706Z","shell.execute_reply":"2021-12-07T01:55:53.324762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def score_model(x, y, model):\n    # define the cross-validation procedure\n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=seed)\n    # evaluate model\n    scores = cross_val_score(model, x, y, scoring='accuracy', cv=cv, n_jobs=-1)\n    return scores","metadata":{"execution":{"iopub.status.busy":"2021-12-07T01:55:53.327485Z","iopub.execute_input":"2021-12-07T01:55:53.328861Z","iopub.status.idle":"2021-12-07T01:55:53.334662Z","shell.execute_reply.started":"2021-12-07T01:55:53.328822Z","shell.execute_reply":"2021-12-07T01:55:53.333922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"geomean = lambda x, axis : np.exp(np.mean(np.log(x), axis=axis))\nharmonic_mean = lambda x, axis : len(x) / np.sum(1.0/x, axis=axis) \n\nfuncs = {'mean' : np.mean, \n         'std' : np.std, \n         'var' : np.var, \n         'geo_mean' : geomean, \n         'harmonic_mean' : harmonic_mean, \n         'median' : np.median,\n         'None_feature_engineering' : None}","metadata":{"execution":{"iopub.status.busy":"2021-12-07T01:55:53.338468Z","iopub.execute_input":"2021-12-07T01:55:53.339073Z","iopub.status.idle":"2021-12-07T01:55:53.3462Z","shell.execute_reply.started":"2021-12-07T01:55:53.339043Z","shell.execute_reply":"2021-12-07T01:55:53.345513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_df = pd.read_csv('/kaggle/input/tabular-playground-series-dec-2021/train.csv', sep=',')\nrandom.seed(seed)\nn = 4000000\ns = 400000\nskip = sorted(random.sample(range(1, n),n-s))\n\ntrain_df = pd.read_csv('/kaggle/input/tabular-playground-series-dec-2021/train.csv', sep=',', skiprows=skip)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T02:03:26.662162Z","iopub.execute_input":"2021-12-12T02:03:26.662543Z","iopub.status.idle":"2021-12-12T02:03:42.152972Z","shell.execute_reply.started":"2021-12-12T02:03:26.662503Z","shell.execute_reply":"2021-12-12T02:03:42.152253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# XGBoost Baseline","metadata":{}},{"cell_type":"code","source":"x_train = train_df.drop(['Id', 'Soil_Type7','Soil_Type15', 'Cover_Type'], axis=1).values\ny_train = train_df['Cover_Type'].values \nx_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2, random_state=seed, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T18:38:12.602149Z","iopub.execute_input":"2021-12-06T18:38:12.602605Z","iopub.status.idle":"2021-12-06T18:38:12.924975Z","shell.execute_reply.started":"2021-12-06T18:38:12.602556Z","shell.execute_reply":"2021-12-06T18:38:12.92423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = {'colsample_bytree': 0.1,\n          'eta': 0.12,\n          'gamma': 5, \n          'max_depth': 2,\n          'min_child_weight': 9,\n          'n_estimators': 1000, \n          'subsample': 0.9}          \n\nmodel = XGBClassifier(**params, \n                      objective='multi:softmax',\n                      random_state=seed, \n                      tree_method='gpu_hist', \n                      predictor='gpu_predictor',\n                      early_stopping_rounds=200,\n                      verbosity=0)\n\nmodel.fit(x_train, y_train)\nresults = evaluate_model(model, x_test, y_test)\nprint(results)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering\n\nHere wee will experiment creating synthetic features using central tendency statistics.","metadata":{}},{"cell_type":"markdown","source":"<h3>Feature Engineering XGBoost</h3>\n","metadata":{}},{"cell_type":"code","source":"results, names = list(), list()\n\nfor key in funcs.keys():\n    x_train = train_df.drop(['Id', 'Soil_Type7','Soil_Type15', 'Cover_Type'], axis=1)\n    if funcs[key] is not None:\n        x_train[key] = funcs[key](x_train, axis=1)\n    y_train = train_df['Cover_Type']\n    x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size = 0.2, random_state = seed, shuffle=True)\n    model = get_xgboost_model()\n    model.fit(x_train, y_train)\n    result = evaluate_model(model, x_test, y_test)\n    names.append(key)\n    results.append(result['accuracy'])\n    \nfor name, score in zip(names, results):\n    print('>%s: %f' % (name, score))\n\nindex = np.argmax(results)\nprint(\"Best Result: \", names[index], results[index])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# XGBoost - Testing different configurations","metadata":{}},{"cell_type":"code","source":"x_train = train_df.drop(['Id', 'Soil_Type7','Soil_Type15', 'Cover_Type'], axis=1)\ny_train = train_df['Cover_Type']\nx_train['mean'] = np.mean(x_train, axis=1)\nx_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2, random_state=seed, shuffle=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1 - Testing different number of estimators","metadata":{}},{"cell_type":"code","source":"def get_models_n_estimators():\n    models = dict()\n    trees = [10, 50, 100, 150, 200, 250, 300, 350, 400, 450, 500, 1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000]\n    for n in trees:\n        params = {'n_estimators' : n}\n        models[str(n)] = get_xgboost_model(params)\n    return models","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = get_models_n_estimators()\nresults, names = list(), list()\n\nfor i, (name, model) in enumerate(models.items()):\n    model.fit(x_train, y_train, verbose=True)\n    score = evaluate_model(model, x_test, y_test)\n    results.append(score['accuracy'])\n    names.append(name)\n    print(name, 'accuracy: %.3f' % (results[i]))\n\nindex = np.argmax(results)\nn_estimators = int(names[index])\nprint(\"Best number of estimators\", n_estimators)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2 - Testing different max_depth","metadata":{}},{"cell_type":"code","source":"def get_models_n_depths():\n    models = dict()\n    for depth in range(1,20):\n        params = {'n_estimators' : n_estimators, 'max_depth' : depth}\n        models[str(depth)] = get_xgboost_model(params)\n    return models","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = get_models_n_depths()\nresults, names = list(), list()\n\nfor i, (name, model) in enumerate(models.items()):\n    model.fit(x_train, y_train, verbose=True)\n    score = evaluate_model(model, x_test, y_test)\n    results.append(score['accuracy'])\n    names.append(name)\n    print(name, 'accuracy: %.3f' % (results[i]))\n\nindex = np.argmax(results)\nmax_depth = int(names[index])\nprint(\"Best max depth\", max_depth)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3 - Testing different subsamples\n","metadata":{}},{"cell_type":"code","source":"def get_models_subsamples():\n    models = dict()\n    for subsample in np.arange(0.1, 1.1, 0.1):\n        params = {'n_estimators' : n_estimators, 'max_depth' : max_depth, 'subsample' : subsample}\n        key = '%.1f' % subsample\n        models[key] = get_xgboost_model(params)\n    return models","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = get_models_subsamples()\nresults, names = list(), list()\n\nfor i, (name, model) in enumerate(models.items()):\n    model.fit(x_train, y_train, verbose=True)\n    score = evaluate_model(model, x_test, y_test)\n    results.append(score['accuracy'])\n    names.append(name)\n    print(name, 'accuracy: %.3f' % (results[i]))\n\nindex = np.argmax(results)\nsubsample = float(names[index])\nprint(\"Best subsample\", subsample)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4 - Testing different learning rates","metadata":{}},{"cell_type":"code","source":"def get_models_lr():\n    models = dict()\n    rates = [0.0001, 0.001, 0.003, 0.005, 0.01, 0.03, 0.05, 0.1, 0.12, 0.13, 0.3, 0.5, 1.0]\n    for r in rates:\n        params = {'n_estimators' : n_estimators, 'max_depth' : max_depth, 'subsample' : subsample, 'eta' : r}\n        key = '%.4f' % r\n        models[key] = get_xgboost_model(params)\n    return models","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = get_models_lr()\nresults, names = list(), list()\n\nfor i, (name, model) in enumerate(models.items()):\n    model.fit(x_train, y_train, verbose=True)\n    score = evaluate_model(model, x_test, y_test)\n    results.append(score['accuracy'])\n    names.append(name)\n    print(name, 'accuracy: %.3f' % (results[i]))\n\nindex = np.argmax(results)\neta = float(names[index])\nprint(\"Best learning rate\", eta)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5 - Testing different number of features","metadata":{}},{"cell_type":"code","source":"def get_models_nfeatures():\n    models = dict()\n    for i in np.arange(0.1, 1.1, 0.1):\n        params = {'n_estimators' : n_estimators, 'max_depth' : max_depth, 'subsample' : subsample, 'eta' : eta, 'colsample_bytree' : i}\n        key = '%.1f' % i\n        models[key] = get_xgboost_model(params)\n    return models","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = get_models_nfeatures()\nresults, names = list(), list()\n\nfor i, (name, model) in enumerate(models.items()):\n    model.fit(x_train, y_train, verbose=True)\n    score = evaluate_model(model, x_test, y_test)\n    results.append(score['accuracy'])\n    names.append(name)\n    print(name, 'accuracy: %.3f' % (results[i]))\n\nindex = np.argmax(results)\ncolsample_bytree = float(names[index])\nprint(\"Best colsample_bytree\", colsample_bytree)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6 - Testing different number of gamma","metadata":{}},{"cell_type":"code","source":"def get_models_n_gamma():\n    models = dict()\n    # for gamma in range(1,20):\n    for gamma in np.arange(0.0, 1.1, 0.1):\n        params = {'n_estimators' : n_estimators, \n                  'max_depth' : max_depth,\n                  'subsample' : subsample,\n                  'eta' : eta, \n                  'colsample_bytree' : colsample_bytree,\n                  'gamma' : gamma}\n        models[str(gamma)] = get_xgboost_model(params)\n    return models","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = get_models_n_gamma()\nresults, names = list(), list()\n\nfor i, (name, model) in enumerate(models.items()):\n    model.fit(x_train, y_train, verbose=True)\n    score = evaluate_model(model, x_test, y_test)\n    results.append(score['accuracy'])\n    names.append(name)\n    print(name, 'accuracy: %.3f' % (results[i]))\n\nindex = np.argmax(results)\ngamma = float(names[index])\nprint(\"Best gamma\", gamma)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7 - Testing different number of min_child_weight","metadata":{}},{"cell_type":"code","source":"def get_models_n_min_child_weight():\n    models = dict()\n    for min_child_weight in range(1,20):\n        params = {'n_estimators' : n_estimators, \n                  'max_depth' : max_depth,\n                  'subsample' : subsample,\n                  'eta' : eta, \n                  'colsample_bytree' : colsample_bytree,\n                  'gamma' : gamma,\n                  'min_child_weight' : min_child_weight}\n        models[str(min_child_weight)] = get_xgboost_model(params)\n    return models","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = get_models_n_min_child_weight()\nresults, names = list(), list()\n\nfor i, (name, model) in enumerate(models.items()):\n    model.fit(x_train, y_train, verbose=True)\n    score = evaluate_model(model, x_test, y_test)\n    results.append(score['accuracy'])\n    names.append(name)\n    print(name, 'accuracy: %.3f' % (results[i]))\n\nindex = np.argmax(results)\nmin_child_weight = int(names[index])\nprint(\"Best min_child_weight\", min_child_weight)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 8 - Testing different number of reg_alpha","metadata":{}},{"cell_type":"code","source":"def get_models_n_reg_alpha():\n    models = dict()\n    for reg_alpha in [0, 1e-5, 1e-2, 0.1, 0.01, 0.001, 0.003, 1, 10, 100]:\n        params = {'n_estimators' : n_estimators, \n                  'max_depth' : max_depth,\n                  'subsample' : subsample,\n                  'eta' : eta, \n                  'colsample_bytree' : colsample_bytree,\n                  'gamma' : gamma,\n                  'min_child_weight' : min_child_weight,\n                  'reg_alpha': reg_alpha}\n        models[str(reg_alpha)] = get_xgboost_model(params)\n    return models","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = get_models_n_min_child_weight()\nresults, names = list(), list()\n\nfor i, (name, model) in enumerate(models.items()):\n    model.fit(x_train, y_train, verbose=True)\n    score = evaluate_model(model, x_test, y_test)\n    results.append(score['accuracy'])\n    names.append(name)\n    print(name, 'accuracy: %.3f' % (results[i]))\n\nindex = np.argmax(results)\nreg_alpha = int(names[index])\nprint(\"reg_alpha\", reg_alpha)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = {'n_estimators' : n_estimators,\n          'max_depth' : max_depth,\n          'subsample' : subsample,\n          'eta' : eta, \n          'colsample_bytree' : colsample_bytree,\n          'gamma' : gamma,\n          'min_child_weight' : min_child_weight,\n          'reg_alpha' : reg_alpha}\n         \nprint('Best Params: ', params)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = get_xgboost_model(params)\nmodel.fit(x_train, y_train, verbose=True)\nscore = evaluate_model(model, x_test, y_test)\nprint(score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = {'n_estimators' : n_estimators,\n          'max_depth' : max_depth,\n          'subsample' : subsample,\n          'eta' : eta, \n          'colsample_bytree' : colsample_bytree,\n          'gamma' : gamma,\n          'min_child_weight' : min_child_weight,\n          'reg_alpha' : reg_alpha}\nmodel = get_xgboost_model(params)\nmodel.fit(x_train, y_train, verbose=True)\nscore = evaluate_model(model, x_test, y_test)\nprint(score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/tabular-playground-series-dec-2021/train.csv', sep=',')\nx_train = train_df.drop(['Id', 'Soil_Type7','Soil_Type15', 'Cover_Type'], axis=1)\ny_train = train_df['Cover_Type']\nx_train['mean'] = np.mean(x_train, axis=1)\nx_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2, random_state=seed, shuffle=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = {'n_estimators' : n_estimators,\n          'max_depth' : max_depth,\n          'subsample' : subsample,\n          'eta' : eta, \n          'colsample_bytree' : colsample_bytree,\n          'gamma' : gamma,\n          'min_child_weight' : min_child_weight,\n          'reg_alpha' : reg_alpha}\n        \nmodel = get_xgboost_model(params)\nmodel.fit(x_train, y_train, verbose=True)\nscore = evaluate_model(model, x_test, y_test)\nprint(score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del train_df, x_train, y_train, x_test, y_test\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv('/kaggle/input/tabular-playground-series-dec-2021/test.csv', sep=',')\nx_test = test_df.drop(['Id', 'Soil_Type7','Soil_Type15'], axis=1)\nx_test['mean'] = np.mean(x_test, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target = model.predict(x_test).squeeze()\nids = test_df['Id'].values\nsubmission_xgboost = pd.DataFrame({'Id' : ids, 'Cover_Type' : target})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_xgboost.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_xgboost.to_csv('submission_xgboost.csv', index=False) # score 0.95378","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del test_df, x_test\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Neural Network Baseline","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/tabular-playground-series-dec-2021/train.csv', sep=',', skiprows=skip)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T23:13:12.353852Z","iopub.execute_input":"2021-12-06T23:13:12.354769Z","iopub.status.idle":"2021-12-06T23:13:17.686237Z","shell.execute_reply.started":"2021-12-06T23:13:12.354708Z","shell.execute_reply":"2021-12-06T23:13:17.685517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = StandardScaler()\nle = LabelEncoder()\n\nx_train = train_df.drop(['Id', 'Soil_Type7','Soil_Type15', 'Cover_Type'], axis=1)\n\ny_train = train_df['Cover_Type'].values \ny_train = le.fit_transform(y_train)\nx_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2, random_state=seed, shuffle=True)\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.transform(x_test)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T02:04:09.723877Z","iopub.execute_input":"2021-12-12T02:04:09.724237Z","iopub.status.idle":"2021-12-12T02:04:10.421416Z","shell.execute_reply.started":"2021-12-12T02:04:09.724185Z","shell.execute_reply":"2021-12-12T02:04:10.420632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = tf.keras.Sequential()\nmodel.add(tf.keras.layers.Dense(units=7, activation='softmax'))\nmodel.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n\nmodel.compile(optimizer=tf.keras.optimizers.Adam(), \n               loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n                metrics=[tf.metrics.SparseCategoricalAccuracy()])\n\nmodel.fit(x_train, y_train, batch_size=32, epochs=20)\nmodel.evaluate(x_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T02:04:12.58964Z","iopub.execute_input":"2021-12-12T02:04:12.589901Z","iopub.status.idle":"2021-12-12T02:09:34.950856Z","shell.execute_reply.started":"2021-12-12T02:04:12.589874Z","shell.execute_reply":"2021-12-12T02:09:34.950069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>Feature Engineering Neural Network Model</h3>","metadata":{}},{"cell_type":"code","source":"scaler = StandardScaler()\nle = LabelEncoder()\n\nresults, names = list(), list()\n\nfor key in funcs.keys():\n    x_train = train_df.drop(['Id', 'Soil_Type7','Soil_Type15', 'Cover_Type'], axis=1)\n    if funcs[key] is not None:\n        x_train[key] = funcs[key](x_train, axis=1)\n    y_train = train_df['Cover_Type'].values\n    y_train = le.fit_transform(y_train)\n    x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size = 0.2, random_state = seed, shuffle=True)\n    x_train = scaler.fit_transform(x_train.values)\n    x_test = scaler.transform(x_test)    \n    model = get_nn_model()\n    model.fit(x_train, y_train, batch_size=32, epochs=15, verbose=0)\n    result = model.evaluate(x_test, y_test, verbose=0)[1]\n    names.append(key)\n    results.append(result)\n    \nfor name, score in zip(names, results):\n    print('>%s: %f' % (name, score))\n\nindex = np.argmax(results)\nprint(\"Best Result: \", names[index], results[index])","metadata":{"execution":{"iopub.status.busy":"2021-12-06T20:44:00.924639Z","iopub.execute_input":"2021-12-06T20:44:00.9249Z","iopub.status.idle":"2021-12-06T21:05:53.026088Z","shell.execute_reply.started":"2021-12-06T20:44:00.924872Z","shell.execute_reply":"2021-12-06T21:05:53.025331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>Neural Network Pipelines</h3>","metadata":{}},{"cell_type":"code","source":"transformers = {'Min-Max-Scaler': MinMaxScaler(), \n                'Standard-Scaler': StandardScaler(),\n                'QuantileTransformer': QuantileTransformer(n_quantiles=100, output_distribution='normal'),\n                'PCA': PCA(),\n                'TruncatedSVD': TruncatedSVD(),\n                'PowerTransformer': PowerTransformer(),\n                'No-transformer': None}","metadata":{"execution":{"iopub.status.busy":"2021-12-06T23:29:16.124783Z","iopub.execute_input":"2021-12-06T23:29:16.12504Z","iopub.status.idle":"2021-12-06T23:29:16.129774Z","shell.execute_reply.started":"2021-12-06T23:29:16.125011Z","shell.execute_reply":"2021-12-06T23:29:16.128812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results, names = list(), list()\n\nfor key in transformers.keys():\n    x_train = train_df.drop(['Id', 'Soil_Type7','Soil_Type15', 'Cover_Type'], axis=1)\n    y_train = train_df['Cover_Type'].values\n    y_train = le.fit_transform(y_train)\n    x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2, random_state=seed, shuffle=True)\n    \n    if transformers[key] is not None:\n        x_train = transformers[key].fit_transform(x_train.values)\n        x_test = transformers[key].transform(x_test)    \n    \n    model = get_nn_model()\n    model.fit(x_train, y_train, batch_size=32, epochs=15, verbose=0)\n    result = model.evaluate(x_test, y_test, verbose=0)[1]\n    names.append(key)\n    results.append(result)\n    \nfor name, score in zip(names, results):\n    print('>%s: %f' % (name, score))\n\nindex = np.argmax(results)\nprint(\"Best Result: \", names[index], results[index])","metadata":{"execution":{"iopub.status.busy":"2021-12-06T23:31:15.513337Z","iopub.execute_input":"2021-12-06T23:31:15.513904Z","iopub.status.idle":"2021-12-06T23:53:55.414173Z","shell.execute_reply.started":"2021-12-06T23:31:15.513863Z","shell.execute_reply":"2021-12-06T23:53:55.413381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Neural Network - Testing different configurations","metadata":{}},{"cell_type":"code","source":"scaler = StandardScaler()\nle = LabelEncoder()\nx_train = train_df.drop(['Id', 'Soil_Type7','Soil_Type15', 'Cover_Type'], axis=1)\ny_train = train_df['Cover_Type'].values \nx_train['std'] = np.std(x_train, axis=1)\ny_train = le.fit_transform(y_train)\nx_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2, random_state=seed, shuffle=True)\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.transform(x_test)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T02:09:59.168263Z","iopub.execute_input":"2021-12-12T02:09:59.168823Z","iopub.status.idle":"2021-12-12T02:09:59.945708Z","shell.execute_reply.started":"2021-12-12T02:09:59.168784Z","shell.execute_reply":"2021-12-12T02:09:59.944875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1 - Testing different number o layers","metadata":{}},{"cell_type":"code","source":"def get_models_n_layers():\n    models = dict()\n    for n_layers in [0, 1, 2, 3, 4, 5, 6, 7, 8 ,9, 10]:\n        models[n_layers] = get_nn_model(n_layers=n_layers)\n    return models","metadata":{"execution":{"iopub.status.busy":"2021-12-12T02:10:00.463698Z","iopub.execute_input":"2021-12-12T02:10:00.464364Z","iopub.status.idle":"2021-12-12T02:10:00.469052Z","shell.execute_reply.started":"2021-12-12T02:10:00.464325Z","shell.execute_reply":"2021-12-12T02:10:00.467971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models =  get_models_n_layers()\nresults, names = list(), list()\n\nfor i, (name, model) in enumerate(models.items()):\n    model.fit(x_train, y_train, batch_size=32, epochs=15, verbose=0)\n    result = model.evaluate(x_test, y_test, verbose=0)[1]\n    results.append(result)\n    names.append(name)\n    print(name, 'accuracy: %.3f' % (results[i]))\n\nindex = np.argmax(results)\nn_layers = int(names[index])\nprint(\"Best number of layers\", n_layers)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T02:10:02.213002Z","iopub.execute_input":"2021-12-12T02:10:02.213554Z","iopub.status.idle":"2021-12-12T03:03:19.621948Z","shell.execute_reply.started":"2021-12-12T02:10:02.213514Z","shell.execute_reply":"2021-12-12T03:03:19.621232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2 - Testing different number of units","metadata":{}},{"cell_type":"code","source":"def get_models_n_units():\n    models = dict()\n    for n_units in [8, 16, 32, 64, 128, 256, 512, 1024, 2048]:\n        models[n_units] = get_nn_model(n_layers=n_layers, n_units=n_units)\n    return models","metadata":{"execution":{"iopub.status.busy":"2021-12-12T03:03:19.623681Z","iopub.execute_input":"2021-12-12T03:03:19.623935Z","iopub.status.idle":"2021-12-12T03:03:19.631285Z","shell.execute_reply.started":"2021-12-12T03:03:19.623899Z","shell.execute_reply":"2021-12-12T03:03:19.630506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models =  get_models_n_units()\nresults, names = list(), list()\n\nfor i, (name, model) in enumerate(models.items()):\n    model.fit(x_train, y_train, batch_size=32, epochs=15, verbose=0)\n    result = model.evaluate(x_test, y_test, verbose=0)[1]\n    results.append(result)\n    names.append(name)\n    print(name, 'accuracy: %.3f' % (results[i]))\n\nindex = np.argmax(results)\nn_units = int(names[index])\nprint(\"Best number of units\", n_units)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T03:28:55.903856Z","iopub.execute_input":"2021-12-12T03:28:55.90441Z","iopub.status.idle":"2021-12-12T04:09:07.448649Z","shell.execute_reply.started":"2021-12-12T03:28:55.904371Z","shell.execute_reply":"2021-12-12T04:09:07.447883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3 - Testing different activation functions","metadata":{}},{"cell_type":"code","source":"def get_models_n_activations():\n    models = dict()\n    for activation in [\"swish\", \"relu\", \"selu\", \"softplus\", \"elu\"]:\n        models[activation] = get_nn_model(n_layers=n_layers, n_units=n_units, activation=activation)\n    return models","metadata":{"execution":{"iopub.status.busy":"2021-12-12T04:09:17.981753Z","iopub.execute_input":"2021-12-12T04:09:17.981997Z","iopub.status.idle":"2021-12-12T04:09:17.986613Z","shell.execute_reply.started":"2021-12-12T04:09:17.981969Z","shell.execute_reply":"2021-12-12T04:09:17.985537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = get_models_n_activations()\nresults, names = list(), list()\n\nfor i, (name, model) in enumerate(models.items()):\n    model.fit(x_train, y_train, batch_size=32, epochs=15, verbose=0)\n    result = model.evaluate(x_test, y_test, verbose=0)[1]\n    results.append(result)\n    names.append(name)\n    print(name, 'accuracy: %.3f' % (results[i]))\n\nindex = np.argmax(results)\nactivation = names[index]\nprint(\"Best activation function\", activation)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T04:09:18.362075Z","iopub.execute_input":"2021-12-12T04:09:18.362307Z","iopub.status.idle":"2021-12-12T04:30:02.925436Z","shell.execute_reply.started":"2021-12-12T04:09:18.362279Z","shell.execute_reply":"2021-12-12T04:30:02.924698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4 testing different number of batch","metadata":{}},{"cell_type":"code","source":"results, names = list(), list()\nbatches = [8, 16, 32, 64, 128, 256, 512]\n\nfor i, (name, batch_size) in enumerate(zip(batches, batches)):\n    get_nn_model(n_layers=n_layers, n_units=n_units, activation=activation)\n    model.fit(x_train, y_train, batch_size=batch_size, epochs=15, verbose=0)\n    result = model.evaluate(x_test, y_test, verbose=0)[1]\n    results.append(result)\n    names.append(name)\n    print(name, 'accuracy: %.3f' % (results[i]))\n\nindex = np.argmax(results)\nbatch_size = int(names[index])\nprint(\"Best batch_size\", batch_size)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T04:37:06.545673Z","iopub.execute_input":"2021-12-12T04:37:06.54594Z","iopub.status.idle":"2021-12-12T05:18:36.906378Z","shell.execute_reply.started":"2021-12-12T04:37:06.545911Z","shell.execute_reply":"2021-12-12T05:18:36.905624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Best parameters\")\nprint(\"n_layers:\", n_layers)\nprint(\"n_units:\", n_units)\nprint(\"activation:\", activation)\nprint(\"batch_size:\", batch_size)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T05:39:01.274643Z","iopub.execute_input":"2021-12-12T05:39:01.274917Z","iopub.status.idle":"2021-12-12T05:39:01.281647Z","shell.execute_reply.started":"2021-12-12T05:39:01.274886Z","shell.execute_reply":"2021-12-12T05:39:01.280689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/tabular-playground-series-dec-2021/train.csv', sep=',', skiprows=skip)\nscaler = StandardScaler()\nle = LabelEncoder()\nx_train = train_df.drop(['Id', 'Soil_Type7','Soil_Type15', 'Cover_Type'], axis=1)\ny_train = train_df['Cover_Type'].values \nx_train['std'] = np.std(x_train, axis=1)\ny_train = le.fit_transform(y_train)\nx_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2, random_state=seed, shuffle=True)\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.transform(x_test)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T05:18:36.907845Z","iopub.execute_input":"2021-12-12T05:18:36.90868Z","iopub.status.idle":"2021-12-12T05:18:43.338373Z","shell.execute_reply.started":"2021-12-12T05:18:36.90864Z","shell.execute_reply":"2021-12-12T05:18:43.337543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = get_nn_model(n_layers=n_layers, n_units=n_units, activation=activation)\nmodel.fit(x_train, y_train, batch_size=batch_size, epochs=15)\nscore = model.evaluate(x_test, y_test, verbose=0)[1]\nprint(score)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T05:18:43.339754Z","iopub.execute_input":"2021-12-12T05:18:43.340026Z","iopub.status.idle":"2021-12-12T05:21:19.401252Z","shell.execute_reply.started":"2021-12-12T05:18:43.33998Z","shell.execute_reply":"2021-12-12T05:21:19.400331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del train_df, x_train, y_train, x_test, y_test\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-12-12T05:21:19.405727Z","iopub.execute_input":"2021-12-12T05:21:19.406037Z","iopub.status.idle":"2021-12-12T05:21:29.303804Z","shell.execute_reply.started":"2021-12-12T05:21:19.405997Z","shell.execute_reply":"2021-12-12T05:21:29.303078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv('/kaggle/input/tabular-playground-series-dec-2021/test.csv', sep=',')\nx_test = test_df.drop(['Id', 'Soil_Type7','Soil_Type15'], axis=1)\nx_test['std'] = np.std(x_test, axis=1)\nx_test = scaler.transform(x_test)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T05:21:29.306472Z","iopub.execute_input":"2021-12-12T05:21:29.306811Z","iopub.status.idle":"2021-12-12T05:21:33.801263Z","shell.execute_reply.started":"2021-12-12T05:21:29.30677Z","shell.execute_reply":"2021-12-12T05:21:33.800494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = model.predict(x_test)\ntarget = np.argmax(preds, axis=-1)\nids = test_df['Id'].values\nsubmission_nn = pd.DataFrame({'Id' : ids, 'Cover_Type' : target + 1})","metadata":{"execution":{"iopub.status.busy":"2021-12-12T05:50:02.455033Z","iopub.execute_input":"2021-12-12T05:50:02.455741Z","iopub.status.idle":"2021-12-12T05:51:11.610914Z","shell.execute_reply.started":"2021-12-12T05:50:02.455701Z","shell.execute_reply":"2021-12-12T05:51:11.610061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_nn = pd.DataFrame({'Id' : ids, 'Cover_Type' : target + 1})","metadata":{"execution":{"iopub.status.busy":"2021-12-12T05:54:35.82217Z","iopub.execute_input":"2021-12-12T05:54:35.822906Z","iopub.status.idle":"2021-12-12T05:54:35.832821Z","shell.execute_reply.started":"2021-12-12T05:54:35.822866Z","shell.execute_reply":"2021-12-12T05:54:35.831919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_nn.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-12T05:54:41.183435Z","iopub.execute_input":"2021-12-12T05:54:41.183931Z","iopub.status.idle":"2021-12-12T05:54:41.192362Z","shell.execute_reply.started":"2021-12-12T05:54:41.183892Z","shell.execute_reply":"2021-12-12T05:54:41.191642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_nn.to_csv('submission_nn.csv', index=False) # score 0.93079","metadata":{"execution":{"iopub.status.busy":"2021-12-12T05:54:42.770772Z","iopub.execute_input":"2021-12-12T05:54:42.771422Z","iopub.status.idle":"2021-12-12T05:54:44.947426Z","shell.execute_reply.started":"2021-12-12T05:54:42.771377Z","shell.execute_reply":"2021-12-12T05:54:44.946421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del test_df, x_test\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-12-12T05:51:13.359559Z","iopub.execute_input":"2021-12-12T05:51:13.359971Z","iopub.status.idle":"2021-12-12T05:51:13.383112Z","shell.execute_reply.started":"2021-12-12T05:51:13.359932Z","shell.execute_reply":"2021-12-12T05:51:13.381918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ensemble XGBoost and Neural Network","metadata":{}},{"cell_type":"code","source":"df_submission_xgboost = pd.read_csv('submission_xgboost.csv')\ndf_submission_nn = pd.read_csv('submission_nn.csv')\nids = df_submission_xgboost['Id'].values\nsubmission_ensemble = pd.DataFrame({'Id' : ids,\n                           'Cover_Type' : np.array(df_submission_xgboost['Cover_Type'].values + df_submission_nn['Cover_Type'].values)//2})\n","metadata":{"execution":{"iopub.status.busy":"2021-12-12T05:54:19.163582Z","iopub.execute_input":"2021-12-12T05:54:19.164398Z","iopub.status.idle":"2021-12-12T05:54:19.173297Z","shell.execute_reply.started":"2021-12-12T05:54:19.164348Z","shell.execute_reply":"2021-12-12T05:54:19.172263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_ensemble .head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_ensemble.to_csv('submission_ensemble.csv', index=False) # 0.93155","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission Best Model - XGboost","metadata":{}},{"cell_type":"code","source":"submission_xgboost.to_csv('submission.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]}]}