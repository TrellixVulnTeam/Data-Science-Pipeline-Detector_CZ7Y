{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction \n\n**Note:** the main flow of the notebook inherited from https://www.kaggle.com/lucamassaron/lightgbm-with-multiclass-focal-loss\n\nIn this notebook, I demonstrate how to use the multiclass focal loss that should help you score better with such imbalanced classes. The focal loss function is from https://github.com/artemmavrin/focal-loss/blob/master/docs/source/index.rst\n\nThe focal loss is a loss that has been devised for object detection problems where the background is more prominent than the objects to be detected. \n\n![](https://github.com/Atomwh/FocalLoss_Keras/raw/master/images/fig1-focal%20loss%20results.png)\n\nAs you increase the gamma value, you put more emphasis on hard to classify examples. There is clearly a trade-off for this (high gamma values can be detrimental), but overall if you set the right value it should perform much better than using other tricks for imbalanced data.\n\nIn order to implement the multiclass focal loss, I referred to the articles below: \n\n- https://paperswithcode.com/method/focal-loss\n- https://amaarora.github.io/2020/06/29/FocalLoss.html\n- https://medium.com/swlh/focal-loss-what-why-and-how-df6735f26616\n\nThis notebook owes quite a lot of ideas from \"TPSDEC21-01-Keras Quickstart\" (https://www.kaggle.com/ambrosm/tpsdec21-01-keras-quickstart) by @ambrosm .\n\nIt also implements the feature engineering suggested by @aguschin (see the post https://www.kaggle.com/c/tabular-playground-series-dec-2021/discussion/291839 for all the references).\n\n**Note**: the main flow of the notebook inherited from https://www.kaggle.com/lucamassaron/lightgbm-with-multiclass-focal-loss ","metadata":{"papermill":{"duration":0.021668,"end_time":"2021-12-06T08:00:03.133146","exception":false,"start_time":"2021-12-06T08:00:03.111478","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.utils import plot_model\nfrom warnings import filterwarnings\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\nfrom sklearn.preprocessing import QuantileTransformer, LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, KFold\nimport lightgbm as lgbm\n\n\n\nfilterwarnings('ignore')\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'","metadata":{"papermill":{"duration":8.37023,"end_time":"2021-12-06T08:00:11.520556","exception":false,"start_time":"2021-12-06T08:00:03.150326","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-12-06T17:22:40.389139Z","iopub.execute_input":"2021-12-06T17:22:40.389823Z","iopub.status.idle":"2021-12-06T17:22:48.62685Z","shell.execute_reply.started":"2021-12-06T17:22:40.389598Z","shell.execute_reply":"2021-12-06T17:22:48.625911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# main flow\nimport datetime as dt\nstart_time = dt.datetime.now()\nprint(\"Started at \", start_time)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T17:22:48.628618Z","iopub.execute_input":"2021-12-06T17:22:48.628881Z","iopub.status.idle":"2021-12-06T17:22:48.63695Z","shell.execute_reply.started":"2021-12-06T17:22:48.628851Z","shell.execute_reply":"2021-12-06T17:22:48.635792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","metadata":{"papermill":{"duration":0.034518,"end_time":"2021-12-06T08:00:11.574059","exception":false,"start_time":"2021-12-06T08:00:11.539541","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-12-06T17:22:48.640561Z","iopub.execute_input":"2021-12-06T17:22:48.641169Z","iopub.status.idle":"2021-12-06T17:22:48.655693Z","shell.execute_reply.started":"2021-12-06T17:22:48.641121Z","shell.execute_reply":"2021-12-06T17:22:48.654829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy import optimize\nfrom scipy import special\n\nclass FocalLoss:\n    \"\"\"\n    source: https://maxhalford.github.io/blog/lightgbm-focal-loss/\n    \"\"\"\n\n    def __init__(self, gamma, alpha=None):\n        self.alpha = alpha\n        self.gamma = gamma\n\n    def at(self, y):\n        if self.alpha is None:\n            return np.ones_like(y)\n        return np.where(y, self.alpha, 1 - self.alpha)\n\n    def pt(self, y, p):\n        p = np.clip(p, 1e-15, 1 - 1e-15)\n        return np.where(y, p, 1 - p)\n\n    def __call__(self, y_true, y_pred):\n        at = self.at(y_true)\n        pt = self.pt(y_true, y_pred)\n        return -at * (1 - pt) ** self.gamma * np.log(pt)\n\n    def grad(self, y_true, y_pred):\n        y = 2 * y_true - 1  # {0, 1} -> {-1, 1}\n        at = self.at(y_true)\n        pt = self.pt(y_true, y_pred)\n        g = self.gamma\n        return at * y * (1 - pt) ** g * (g * pt * np.log(pt) + pt - 1)\n\n    def hess(self, y_true, y_pred):\n        y = 2 * y_true - 1  # {0, 1} -> {-1, 1}\n        at = self.at(y_true)\n        pt = self.pt(y_true, y_pred)\n        g = self.gamma\n\n        u = at * y * (1 - pt) ** g\n        du = -at * y * g * (1 - pt) ** (g - 1)\n        v = g * pt * np.log(pt) + pt - 1\n        dv = g * np.log(pt) + g + 1\n\n        return (du * v + u * dv) * y * (pt * (1 - pt))\n\n    def init_score(self, y_true):\n        res = optimize.minimize_scalar(\n            lambda p: self(y_true, p).sum(),\n            bounds=(0, 1),\n            method='bounded'\n        )\n        p = res.x\n        log_odds = np.log(p / (1 - p))\n        return log_odds\n\n    def lgb_obj(self, preds, train_data):\n        y = train_data.get_label()\n        p = special.expit(preds)\n        return self.grad(y, p), self.hess(y, p)\n\n    def lgb_eval(self, preds, train_data):\n        y = train_data.get_label()\n        p = special.expit(preds)\n        is_higher_better = False\n        return 'focal_loss', self(y, p).mean(), is_higher_better","metadata":{"papermill":{"duration":0.039752,"end_time":"2021-12-06T08:00:11.63105","exception":false,"start_time":"2021-12-06T08:00:11.591298","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-12-06T17:22:48.658236Z","iopub.execute_input":"2021-12-06T17:22:48.658779Z","iopub.status.idle":"2021-12-06T17:22:48.678588Z","shell.execute_reply.started":"2021-12-06T17:22:48.658715Z","shell.execute_reply":"2021-12-06T17:22:48.677831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from joblib import Parallel, delayed\nfrom sklearn.multiclass import _ConstantPredictor\nfrom sklearn.preprocessing import LabelBinarizer\nfrom scipy import special\n\n\nclass OneVsRestLightGBMWithCustomizedLoss:\n    \"\"\"\n    source: https://towardsdatascience.com/multi-class-classification-using-focal-loss-and-lightgbm-a6a6dec28872\n    \"\"\"\n\n    def __init__(self, loss, n_jobs=3):\n        self.loss = loss\n        self.n_jobs = n_jobs\n\n    def fit(self, X, y, **fit_params):\n\n        self.label_binarizer_ = LabelBinarizer(sparse_output=True)\n        Y = self.label_binarizer_.fit_transform(y)\n        Y = Y.tocsc()\n        self.classes_ = self.label_binarizer_.classes_\n        columns = (col.toarray().ravel() for col in Y.T)\n        if 'eval_set' in fit_params:\n            # use eval_set for early stopping\n            X_val, y_val = fit_params['eval_set'][0]\n            Y_val = self.label_binarizer_.transform(y_val)\n            Y_val = Y_val.tocsc()\n            columns_val = (col.toarray().ravel() for col in Y_val.T)\n            self.results_ = Parallel(n_jobs=self.n_jobs)(delayed(self._fit_binary)\n                                                         (X, column, X_val, column_val, **fit_params) for\n                                                         i, (column, column_val) in\n                                                         enumerate(zip(columns, columns_val)))\n        else:\n            # eval set not available\n            self.results_ = Parallel(n_jobs=self.n_jobs)(delayed(self._fit_binary)\n                                                         (X, column, None, None, **fit_params) for i, column\n                                                         in enumerate(columns))\n\n        return self\n\n    def _fit_binary(self, X, y, X_val, y_val, **fit_params):\n        unique_y = np.unique(y)\n        init_score_value = self.loss.init_score(y)\n        if len(unique_y) == 1:\n            estimator = _ConstantPredictor().fit(X, unique_y)\n        else:\n            fit = lgbm.Dataset(X, y, init_score=np.full_like(y, init_score_value, dtype=float))\n            filtering = ['eval_set', 'early_stopping_rounds', 'verbose_eval', 'num_boost_round']\n            local_fit_params = {item:value for item, value in fit_params.items() if item!='eval_set'}\n            \n            if 'num_boost_round' in fit_params:\n                num_boost_round = fit_params['num_boost_round']\n            else:\n                num_boost_round = 100\n                \n            if 'early_stopping_rounds' in fit_params:\n                early_stopping_rounds = fit_params['early_stopping_rounds']\n            else:\n                early_stopping_rounds = 10\n                \n            if 'verbose_eval'  in fit_params:\n                verbose_eval = fit_params['verbose_eval']\n            else:\n                verbose_eval = 10\n                    \n            if 'eval_set' in fit_params:\n                val = lgbm.Dataset(X_val, y_val, init_score=np.full_like(y_val, init_score_value, dtype=float),\n                                  reference=fit)\n        \n                estimator = lgbm.train(params=local_fit_params,\n                                       train_set=fit,\n                                       valid_sets=(fit, val),\n                                       valid_names=('fit', 'val'),\n                                       fobj=self.loss.lgb_obj,\n                                       feval=self.loss.lgb_eval,\n                                       num_boost_round=num_boost_round,\n                                       early_stopping_rounds=early_stopping_rounds,\n                                       verbose_eval=verbose_eval)\n            else:\n                                   \n                estimator = lgbm.train(params=local_fit_params,\n                                       train_set=fit,\n                                       fobj=self.loss.lgb_obj,\n                                       feval=self.loss.lgb_eval,\n                                       num_boost_round=num_boost_round,\n                                       early_stopping_rounds=early_stopping_rounds,\n                                       verbose_eval=verbose_eval)\n\n        return estimator, init_score_value\n\n    def predict(self, X):\n\n        n_samples = X.shape[0]\n        maxima = np.empty(n_samples, dtype=float)\n        maxima.fill(-np.inf)\n        argmaxima = np.zeros(n_samples, dtype=int)\n\n        for i, (e, init_score) in enumerate(self.results_):\n            margins = e.predict(X, raw_score=True)\n            prob = special.expit(margins + init_score)\n            np.maximum(maxima, prob, out=maxima)\n            argmaxima[maxima == prob] = i\n\n        return argmaxima\n\n    def predict_proba(self, X):\n        y = np.zeros((X.shape[0], len(self.results_)))\n        for i, (e, init_score) in enumerate(self.results_):\n            margins = e.predict(X, raw_score=True)\n            y[:, i] = special.expit(margins + init_score)\n        y /= np.sum(y, axis=1)[:, np.newaxis]\n        return y","metadata":{"papermill":{"duration":0.050856,"end_time":"2021-12-06T08:00:11.698667","exception":false,"start_time":"2021-12-06T08:00:11.647811","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-12-06T17:22:48.680158Z","iopub.execute_input":"2021-12-06T17:22:48.680654Z","iopub.status.idle":"2021-12-06T17:22:48.710288Z","shell.execute_reply.started":"2021-12-06T17:22:48.680618Z","shell.execute_reply":"2021-12-06T17:22:48.709627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from typing import Tuple, List, Dict\n\n# read data\nin_kaggle = True\n\ndef get_data_file_path(is_in_kaggle: bool) -> Tuple[str, str, str]:\n    train_path = ''\n    test_path = ''\n    sample_submission_path = ''\n\n    if is_in_kaggle:\n        # running in Kaggle, inside the competition\n        train_path = '../input/tabular-playground-series-dec-2021/train.csv'\n        test_path = '../input/tabular-playground-series-dec-2021/test.csv'\n        sample_submission_path = '../input/tabular-playground-series-dec-2021/sample_submission.csv'\n    else:\n        # running locally\n        train_path = 'data/train.csv'\n        test_path = 'data/test.csv'\n        sample_submission_path = 'data/sample_submission.csv'\n\n    return train_path, test_path, sample_submission_path","metadata":{"execution":{"iopub.status.busy":"2021-12-06T17:22:48.711886Z","iopub.execute_input":"2021-12-06T17:22:48.712339Z","iopub.status.idle":"2021-12-06T17:22:48.724868Z","shell.execute_reply.started":"2021-12-06T17:22:48.712302Z","shell.execute_reply":"2021-12-06T17:22:48.723921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_path, test_path, sample_submission_path = get_data_file_path(in_kaggle)\n\ntrain = pd.read_csv(train_path)\ntest = pd.read_csv(test_path)\nsubmission = pd.read_csv(sample_submission_path)","metadata":{"papermill":{"duration":22.13377,"end_time":"2021-12-06T08:00:33.849274","exception":false,"start_time":"2021-12-06T08:00:11.715504","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-12-06T17:22:48.726065Z","iopub.execute_input":"2021-12-06T17:22:48.726708Z","iopub.status.idle":"2021-12-06T17:23:12.560141Z","shell.execute_reply.started":"2021-12-06T17:22:48.726667Z","shell.execute_reply":"2021-12-06T17:23:12.559482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"The target class distribution:\")\nprint((train.groupby('Cover_Type').Id.nunique() / len(train)).apply(lambda p: f\"{p:.3%}\"))","metadata":{"papermill":{"duration":0.845793,"end_time":"2021-12-06T08:00:34.712471","exception":false,"start_time":"2021-12-06T08:00:33.866678","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-12-06T17:23:12.561051Z","iopub.execute_input":"2021-12-06T17:23:12.561755Z","iopub.status.idle":"2021-12-06T17:23:13.113487Z","shell.execute_reply.started":"2021-12-06T17:23:12.561708Z","shell.execute_reply":"2021-12-06T17:23:13.112543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Droping Cover_Type 5 and 4 labels, since there is only a few instances of them\ntrain = train[train.Cover_Type != 5]\ntrain = train[train.Cover_Type != 4]","metadata":{"papermill":{"duration":0.926213,"end_time":"2021-12-06T08:00:35.655954","exception":false,"start_time":"2021-12-06T08:00:34.729741","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-12-06T17:23:13.114948Z","iopub.execute_input":"2021-12-06T17:23:13.115256Z","iopub.status.idle":"2021-12-06T17:23:13.927451Z","shell.execute_reply.started":"2021-12-06T17:23:13.115214Z","shell.execute_reply":"2021-12-06T17:23:13.926503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove unuseful features\ntrain = train.drop([ 'Soil_Type7', 'Soil_Type15'], axis=1)\ntest = test.drop(['Soil_Type7', 'Soil_Type15'], axis=1)\n\n# extra feature engineering\ndef r(x):\n    if x+180>360:\n        return x-180\n    else:\n        return x+180\n\ndef fe(df):\n    df['EHiElv'] = df['Horizontal_Distance_To_Roadways'] * df['Elevation']\n    df['EViElv'] = df['Vertical_Distance_To_Hydrology'] * df['Elevation']\n    df['Aspect2'] = df.Aspect.map(r)\n    ### source: https://www.kaggle.com/c/tabular-playground-series-dec-2021/discussion/293373\n    df[\"Aspect\"][df[\"Aspect\"] < 0] += 360\n    df[\"Aspect\"][df[\"Aspect\"] > 359] -= 360\n    df.loc[df[\"Hillshade_9am\"] < 0, \"Hillshade_9am\"] = 0\n    df.loc[df[\"Hillshade_Noon\"] < 0, \"Hillshade_Noon\"] = 0\n    df.loc[df[\"Hillshade_3pm\"] < 0, \"Hillshade_3pm\"] = 0\n    df.loc[df[\"Hillshade_9am\"] > 255, \"Hillshade_9am\"] = 255\n    df.loc[df[\"Hillshade_Noon\"] > 255, \"Hillshade_Noon\"] = 255\n    df.loc[df[\"Hillshade_3pm\"] > 255, \"Hillshade_3pm\"] = 255\n    ########\n    df['Highwater'] = (df.Vertical_Distance_To_Hydrology < 0).astype(int)\n    df['EVDtH'] = df.Elevation - df.Vertical_Distance_To_Hydrology\n    df['EHDtH'] = df.Elevation - df.Horizontal_Distance_To_Hydrology * 0.2\n    df['Euclidean_Distance_to_Hydrolody'] = (df['Horizontal_Distance_To_Hydrology']**2 + df['Vertical_Distance_To_Hydrology']**2)**0.5\n    df['Manhattan_Distance_to_Hydrolody'] = df['Horizontal_Distance_To_Hydrology'] + df['Vertical_Distance_To_Hydrology']\n    df['Hydro_Fire_1'] = df['Horizontal_Distance_To_Hydrology'] + df['Horizontal_Distance_To_Fire_Points']\n    df['Hydro_Fire_2'] = abs(df['Horizontal_Distance_To_Hydrology'] - df['Horizontal_Distance_To_Fire_Points'])\n    df['Hydro_Road_1'] = abs(df['Horizontal_Distance_To_Hydrology'] + df['Horizontal_Distance_To_Roadways'])\n    df['Hydro_Road_2'] = abs(df['Horizontal_Distance_To_Hydrology'] - df['Horizontal_Distance_To_Roadways'])\n    df['Fire_Road_1'] = abs(df['Horizontal_Distance_To_Fire_Points'] + df['Horizontal_Distance_To_Roadways'])\n    df['Fire_Road_2'] = abs(df['Horizontal_Distance_To_Fire_Points'] - df['Horizontal_Distance_To_Roadways'])\n    df['Hillshade_3pm_is_zero'] = (df.Hillshade_3pm == 0).astype(int)\n    return df\n\ntrain = fe(train)\ntest = fe(test)\n\n# Summed features pointed out by @craigmthomas (https://www.kaggle.com/c/tabular-playground-series-dec-2021/discussion/292823)\nsoil_features = [x for x in train.columns if x.startswith(\"Soil_Type\")]\nwilderness_features = [x for x in train.columns if x.startswith(\"Wilderness_Area\")]\n\ntrain[\"soil_type_count\"] = train[soil_features].sum(axis=1)\ntest[\"soil_type_count\"] = test[soil_features].sum(axis=1)\n\ntrain[\"wilderness_area_count\"] = train[wilderness_features].sum(axis=1)\ntest[\"wilderness_area_count\"] = test[wilderness_features].sum(axis=1)","metadata":{"papermill":{"duration":10.730658,"end_time":"2021-12-06T08:00:46.40403","exception":false,"start_time":"2021-12-06T08:00:35.673372","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-12-06T17:23:13.930076Z","iopub.execute_input":"2021-12-06T17:23:13.930666Z","iopub.status.idle":"2021-12-06T17:23:23.937312Z","shell.execute_reply.started":"2021-12-06T17:23:13.930631Z","shell.execute_reply":"2021-12-06T17:23:23.936396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = train.Cover_Type.values - 1\nX = reduce_mem_usage(train.drop(\"Cover_Type\", axis=1)).set_index(\"Id\")\nXt = reduce_mem_usage(test).set_index(\"Id\")","metadata":{"papermill":{"duration":35.101077,"end_time":"2021-12-06T08:01:21.522395","exception":false,"start_time":"2021-12-06T08:00:46.421318","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-12-06T17:23:23.938751Z","iopub.execute_input":"2021-12-06T17:23:23.938993Z","iopub.status.idle":"2021-12-06T17:23:54.691015Z","shell.execute_reply.started":"2021-12-06T17:23:23.938962Z","shell.execute_reply":"2021-12-06T17:23:54.689957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\ndel([train, test])\n_ = [gc.collect() for i in range(5)]","metadata":{"papermill":{"duration":0.933216,"end_time":"2021-12-06T08:01:22.473881","exception":false,"start_time":"2021-12-06T08:01:21.540665","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-12-06T17:23:54.692331Z","iopub.execute_input":"2021-12-06T17:23:54.692636Z","iopub.status.idle":"2021-12-06T17:23:55.628073Z","shell.execute_reply.started":"2021-12-06T17:23:54.692601Z","shell.execute_reply":"2021-12-06T17:23:55.62721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"le = LabelEncoder()\ntarget = le.fit_transform(y)\n\n_, classes_num = np.unique(target, return_counts=True)","metadata":{"papermill":{"duration":0.313083,"end_time":"2021-12-06T08:01:22.805498","exception":false,"start_time":"2021-12-06T08:01:22.492415","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-12-06T17:23:55.629343Z","iopub.execute_input":"2021-12-06T17:23:55.629588Z","iopub.status.idle":"2021-12-06T17:23:55.963088Z","shell.execute_reply.started":"2021-12-06T17:23:55.629557Z","shell.execute_reply":"2021-12-06T17:23:55.962168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N_FOLDS = 5\n\n### cross-validation \ncv = KFold(n_splits=N_FOLDS, shuffle=True, random_state=1)\n\npredictions = np.zeros((len(Xt), len(le.classes_)))\noof = np.zeros((len(X), len(le.classes_)))\nscores = list()\n\nfor fold, (idx_train, idx_valid) in enumerate(cv.split(X, y)):\n    X_train, y_train = X.iloc[idx_train, :], target[idx_train]\n    X_valid, y_valid = X.iloc[idx_valid, :], target[idx_valid]\n    \n    fit_params = {'eval_set': [(X_valid, y_valid)],\n                  'num_boost_round': 1500,\n                  'early_stopping_rounds': 30,\n                  'verbose_eval': 100\n                 }\n    \n    loss = FocalLoss(alpha=0.75, gamma=2.0)\n    model = OneVsRestLightGBMWithCustomizedLoss(loss=loss)\n\n    print('**'*20)\n    print(f\"Fold {fold+1} || Training\")\n    print('**'*20)\n\n    model.fit(X_train, y_train, **fit_params)\n\n    predictions += model.predict_proba(Xt) / N_FOLDS\n    oof[idx_valid] = model.predict_proba(X_valid)\n        \n    scores.append(accuracy_score(y_true=y_valid, y_pred=np.argmax(oof[idx_valid], axis=1)))\n    print(f\"cv accuracy fold {fold+1}: {scores[-1]:0.5f}\")","metadata":{"papermill":{"duration":14037.144551,"end_time":"2021-12-06T11:55:19.968137","exception":false,"start_time":"2021-12-06T08:01:22.823586","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-12-06T17:23:55.964221Z","iopub.execute_input":"2021-12-06T17:23:55.964459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Average cv accuracy: {np.mean(scores):0.5f} (std={np.std(scores):0.5f})\")","metadata":{"papermill":{"duration":0.050658,"end_time":"2021-12-06T11:55:20.046979","exception":false,"start_time":"2021-12-06T11:55:19.996321","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.Cover_Type = le.inverse_transform(np.argmax(predictions, axis=1)) + 1\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{"papermill":{"duration":2.127645,"end_time":"2021-12-06T11:55:22.202781","exception":false,"start_time":"2021-12-06T11:55:20.075136","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oof = pd.DataFrame(oof, columns=[f\"prob_{i}\" for i in le.classes_])\noof.insert(loc=0, column='Id', value=range(len(X)))\noof.to_csv(\"oof.csv\", index=False)","metadata":{"papermill":{"duration":53.258973,"end_time":"2021-12-06T11:56:15.494535","exception":false,"start_time":"2021-12-06T11:55:22.235562","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('We are done. That is all, folks!')\nfinish_time = dt.datetime.now()\nprint(\"Finished at \", finish_time)\nelapsed = finish_time - start_time\nprint(\"Elapsed time: \", elapsed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Revision History\n\n- Initial version of the notebook was based https://www.kaggle.com/lucamassaron/lightgbm-with-multiclass-focal-loss with tiny data manipulation and time tracking extensions (plus dropping class label 4, along with the class label 5). Its score on the public leaderboard was **0.95xxxx**","metadata":{}},{"cell_type":"markdown","source":"# References\n\nWhen running the code of this notebook locally under Andaconda/Python 3.7 on the Windows machine, you may see the page file memory overallocation error. Troubleshooting per https://stackoverflow.com/questions/57507832/unable-to-allocate-array-with-shape-and-data-type can be applied to work it around.","metadata":{}}]}