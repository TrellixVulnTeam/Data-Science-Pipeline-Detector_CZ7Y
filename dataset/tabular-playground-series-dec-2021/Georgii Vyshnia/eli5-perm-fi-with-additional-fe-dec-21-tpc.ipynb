{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\nThe purpose of this notebook is to \n- Perform the feature importance study for the training set of this competition refined with the additional features engineered per the suggestions in https://www.kaggle.com/c/tabular-playground-series-dec-2021/discussion/293612\n- Train a basic catboost model with the important feature subset detected above\n\nThe feature importance study will basically follow the flow of the brilliant experiment in https://www.kaggle.com/lordozvlad/tps-dec-fast-feature-importance-with-sklearnex, by @lordozvlad . The essence of the flow is presented below\n\n- Detecting the feature importance with the permutation importance method, as implemented by ELI5 package, with the random forest trained as a model in the feature permutation rounds\n- Accelerating performance of scikit-learn-based operations with Intelâ€™s accelerator (sklearnex)\n\nThe catboost model is not fully tuned, and its purpose is to demonstrate how good (or equally less than good) the accuracy of the prediction with the important feature subset is.\n","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport datetime as dt\nfrom typing import Tuple, List, Dict\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport plotly.express as px\nimport plotly.offline\n\n\n# read data\nin_kaggle = True\n\ndef get_data_file_path(is_in_kaggle: bool) -> Tuple[str, str, str]:\n    train_path = ''\n    test_path = ''\n    sample_submission_path = ''\n\n    if is_in_kaggle:\n        # running in Kaggle, inside the competition\n        train_path = '../input/tabular-playground-series-dec-2021/train.csv'\n        test_path = '../input/tabular-playground-series-dec-2021/test.csv'\n        sample_submission_path = '../input/tabular-playground-series-dec-2021/sample_submission.csv'\n    else:\n        # running locally\n        train_path = 'data/train.csv'\n        test_path = 'data/test.csv'\n        sample_submission_path = 'data/sample_submission.csv'\n\n    return train_path, test_path, sample_submission_path","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# main flow\nstart_time = dt.datetime.now()\nprint(\"Started at \", start_time)","metadata":{"execution":{"iopub.status.busy":"2021-12-14T23:32:27.024151Z","iopub.execute_input":"2021-12-14T23:32:27.024911Z","iopub.status.idle":"2021-12-14T23:32:27.030332Z","shell.execute_reply.started":"2021-12-14T23:32:27.024876Z","shell.execute_reply":"2021-12-14T23:32:27.029671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# get the training set and labels\ntrain_set_path, test_set_path, sample_subm_path = get_data_file_path(in_kaggle)\n\ntrain = pd.read_csv(train_set_path)\ntest = pd.read_csv(test_set_path)\n\nsubm = pd.read_csv(sample_subm_path)","metadata":{"execution":{"iopub.status.busy":"2021-12-14T23:32:27.032241Z","iopub.execute_input":"2021-12-14T23:32:27.032683Z","iopub.status.idle":"2021-12-14T23:32:53.34698Z","shell.execute_reply.started":"2021-12-14T23:32:27.032636Z","shell.execute_reply":"2021-12-14T23:32:53.345988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Review of Class Labels in Training","metadata":{}},{"cell_type":"code","source":"#### Check the class counts. If any class is too small, just drop those classes\ntarget = 'Cover_Type'\ntrain[target].value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, classes with the labels of 4 and 5 are quite rare in the training set. Therefore we will ignore them in the model training. To achieve it, we are going to drop the training observations with such class labels:","metadata":{}},{"cell_type":"code","source":"print('rows dropped = ', train[((train[target] == 4) | (train[target] == 5))].shape)\ntrain = train[~((train[target] == 4) | (train[target] == 5))]\nprint(train.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Additional Feature Engineering and Data Preprocessing","metadata":{}},{"cell_type":"code","source":"%%time\n# remove useless features\nzero_variance_features = [ 'Soil_Type7', 'Soil_Type15', 'Id']\n\ntrain = train.drop(zero_variance_features, axis=1)\ntest = test.drop(zero_variance_features, axis=1)\n\n# extra feature engineering\ndef r(x):\n    if x+180>360:\n        return x-180\n    else:\n        return x+180\n\ndef fe(df):\n    \n    features_Hillshade = ['Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm']\n    \n    df['EHiElv'] = df['Horizontal_Distance_To_Roadways'] * df['Elevation']\n    df['EViElv'] = df['Vertical_Distance_To_Hydrology'] * df['Elevation']\n    df['Aspect2'] = df.Aspect.map(r)\n    ### source: https://www.kaggle.com/c/tabular-playground-series-dec-2021/discussion/293373\n    df[\"Aspect\"][df[\"Aspect\"] < 0] += 360\n    df[\"Aspect\"][df[\"Aspect\"] > 359] -= 360\n    df.loc[df[\"Hillshade_9am\"] < 0, \"Hillshade_9am\"] = 0\n    df.loc[df[\"Hillshade_Noon\"] < 0, \"Hillshade_Noon\"] = 0\n    df.loc[df[\"Hillshade_3pm\"] < 0, \"Hillshade_3pm\"] = 0\n    df.loc[df[\"Hillshade_9am\"] > 255, \"Hillshade_9am\"] = 255\n    df.loc[df[\"Hillshade_Noon\"] > 255, \"Hillshade_Noon\"] = 255\n    df.loc[df[\"Hillshade_3pm\"] > 255, \"Hillshade_3pm\"] = 255\n    ########\n    df['Highwater'] = (df.Vertical_Distance_To_Hydrology < 0).astype(int)\n    df['EVDtH'] = df.Elevation - df.Vertical_Distance_To_Hydrology\n    df['EHDtH'] = (df.Elevation - df.Horizontal_Distance_To_Hydrology * 0.2).astype(int)\n    df['Euclidean_Distance_to_Hydrolody'] = ((df['Horizontal_Distance_To_Hydrology']**2 + df['Vertical_Distance_To_Hydrology']**2)**0.5).astype(int)\n    df['Manhattan_Distance_to_Hydrolody'] = df['Horizontal_Distance_To_Hydrology'] + df['Vertical_Distance_To_Hydrology']\n    df['Hydro_Fire_1'] = df['Horizontal_Distance_To_Hydrology'] + df['Horizontal_Distance_To_Fire_Points']\n    df['Hydro_Fire_2'] = abs(df['Horizontal_Distance_To_Hydrology'] - df['Horizontal_Distance_To_Fire_Points'])\n    df['Hydro_Road_1'] = abs(df['Horizontal_Distance_To_Hydrology'] + df['Horizontal_Distance_To_Roadways'])\n    df['Hydro_Road_2'] = abs(df['Horizontal_Distance_To_Hydrology'] - df['Horizontal_Distance_To_Roadways'])\n    df['Fire_Road_1'] = abs(df['Horizontal_Distance_To_Fire_Points'] + df['Horizontal_Distance_To_Roadways'])\n    df['Fire_Road_2'] = abs(df['Horizontal_Distance_To_Fire_Points'] - df['Horizontal_Distance_To_Roadways'])\n    df['Hillshade_3pm_is_zero'] = (df.Hillshade_3pm == 0).astype(int)\n    \n    df[\"Hillshade_mean\"] = df[features_Hillshade].mean(axis=1).astype(int)\n    df['amp_Hillshade'] = df[features_Hillshade].max(axis=1) - df[features_Hillshade].min(axis=1).astype(int)\n    return df\n\ntrain = fe(train)\ntest = fe(test)\n\n# Summed features pointed out by @craigmthomas (https://www.kaggle.com/c/tabular-playground-series-dec-2021/discussion/292823)\nsoil_features = [x for x in train.columns if x.startswith(\"Soil_Type\")]\nwilderness_features = [x for x in train.columns if x.startswith(\"Wilderness_Area\")]\n\ntrain[\"soil_type_count\"] = train[soil_features].sum(axis=1)\ntest[\"soil_type_count\"] = test[soil_features].sum(axis=1)\n\ntrain[\"wilderness_area_count\"] = train[wilderness_features].sum(axis=1)\ntest[\"wilderness_area_count\"] = test[wilderness_features].sum(axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-12-14T23:32:53.349453Z","iopub.execute_input":"2021-12-14T23:32:53.349784Z","iopub.status.idle":"2021-12-14T23:33:06.345553Z","shell.execute_reply.started":"2021-12-14T23:32:53.349741Z","shell.execute_reply":"2021-12-14T23:33:06.344339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ndef reduce_memory_usage(df):\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != 'object':\n            c_min = df[col].min()\n            c_max = df[col].max()\n            \n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    pass\n        else:\n            df[col] = df[col].astype('category')\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2021-12-14T23:33:06.346854Z","iopub.execute_input":"2021-12-14T23:33:06.347102Z","iopub.status.idle":"2021-12-14T23:33:06.359195Z","shell.execute_reply.started":"2021-12-14T23:33:06.347058Z","shell.execute_reply":"2021-12-14T23:33:06.358382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrain = reduce_memory_usage(train)\ntest  = reduce_memory_usage(test)","metadata":{"execution":{"iopub.status.busy":"2021-12-14T23:33:06.361411Z","iopub.execute_input":"2021-12-14T23:33:06.361676Z","iopub.status.idle":"2021-12-14T23:33:36.758152Z","shell.execute_reply.started":"2021-12-14T23:33:06.361634Z","shell.execute_reply":"2021-12-14T23:33:36.757175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# IntelÂ® Extension for Scikit-learn\n\nWe are going ot install Intel Extension for  Scikit-learn to accelerate the performance of the usual scikit-learn routines.","metadata":{}},{"cell_type":"code","source":"!pip install scikit-learn-intelex -q --progress-bar off > /dev/null 2>&1","metadata":{"execution":{"iopub.status.busy":"2021-12-14T23:33:36.759329Z","iopub.execute_input":"2021-12-14T23:33:36.759556Z","iopub.status.idle":"2021-12-14T23:33:47.051165Z","shell.execute_reply.started":"2021-12-14T23:33:36.759527Z","shell.execute_reply":"2021-12-14T23:33:47.049557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we are ready to apply the performance acceleration with just two lines of code","metadata":{}},{"cell_type":"code","source":"%%time\nfrom sklearnex import patch_sklearn\npatch_sklearn()\n","metadata":{"execution":{"iopub.status.busy":"2021-12-14T23:33:47.053177Z","iopub.execute_input":"2021-12-14T23:33:47.053495Z","iopub.status.idle":"2021-12-14T23:33:47.060739Z","shell.execute_reply.started":"2021-12-14T23:33:47.053459Z","shell.execute_reply":"2021-12-14T23:33:47.060127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Importancce with ELI5\n\nOne of the most basic questions we might ask of a model is: What features have the biggest impact on predictions?\n\nThis concept is called feature importance.\n\nThere are multiple ways to measure feature importance. In this kernel we consider permutation importance using library ELI5.\nÂ¶\nELI5 provides a way to compute feature importances for any black-box estimator by measuring how score decreases when a feature is not available. It implements permutational feature importance scoring.","metadata":{}},{"cell_type":"code","source":"%%time\nfrom sklearn.model_selection import train_test_split\nfrom timeit import default_timer as timer\n\nX, y = train.drop(['Cover_Type'], axis = 1), train['Cover_Type']\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.3, random_state = 42)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-14T23:33:47.091808Z","iopub.execute_input":"2021-12-14T23:33:47.092178Z","iopub.status.idle":"2021-12-14T23:33:50.109982Z","shell.execute_reply.started":"2021-12-14T23:33:47.092138Z","shell.execute_reply":"2021-12-14T23:33:50.10888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import eli5\nfrom eli5.sklearn import PermutationImportance\nfrom timeit import default_timer as timer\nfrom sklearn.ensemble import RandomForestClassifier","metadata":{"execution":{"iopub.status.busy":"2021-12-14T23:33:50.111884Z","iopub.execute_input":"2021-12-14T23:33:50.112376Z","iopub.status.idle":"2021-12-14T23:33:50.116779Z","shell.execute_reply.started":"2021-12-14T23:33:50.112324Z","shell.execute_reply":"2021-12-14T23:33:50.116139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntimeFirstI  = timer()\nmodelRF     = RandomForestClassifier(random_state = 42).fit(X_train, y_train)\nperm        = PermutationImportance(modelRF, random_state = 42).fit(X_val, y_val)\ntimeSecondI = timer()","metadata":{"execution":{"iopub.status.busy":"2021-12-14T23:33:50.119289Z","iopub.execute_input":"2021-12-14T23:33:50.119834Z","iopub.status.idle":"2021-12-15T00:07:28.067666Z","shell.execute_reply.started":"2021-12-14T23:33:50.119795Z","shell.execute_reply":"2021-12-15T00:07:28.066753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Total time with Intel Extension: {} seconds\".format(timeSecondI - timeFirstI))","metadata":{"execution":{"iopub.status.busy":"2021-12-15T00:07:28.069145Z","iopub.execute_input":"2021-12-15T00:07:28.069379Z","iopub.status.idle":"2021-12-15T00:07:28.101492Z","shell.execute_reply.started":"2021-12-15T00:07:28.069344Z","shell.execute_reply":"2021-12-15T00:07:28.100483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\neli5.show_weights(perm, feature_names = X.columns.tolist())","metadata":{"execution":{"iopub.status.busy":"2021-12-15T00:07:28.102554Z","iopub.status.idle":"2021-12-15T00:07:28.103425Z","shell.execute_reply.started":"2021-12-15T00:07:28.103217Z","shell.execute_reply":"2021-12-15T00:07:28.103242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\npi_features = eli5.explain_weights_df(perm, feature_names = X_train.columns.tolist())\npi_features = pi_features.loc[pi_features['weight'] >= 0.0001]['feature'].tolist()","metadata":{"execution":{"iopub.status.busy":"2021-12-15T00:07:28.104703Z","iopub.status.idle":"2021-12-15T00:07:28.105284Z","shell.execute_reply.started":"2021-12-15T00:07:28.105098Z","shell.execute_reply":"2021-12-15T00:07:28.105119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n#show all important features\npi_features","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature importance notes\n\nAs we can see, the top five features detected to be important as follows\n- 'Elevation', 'EVDtH', 'soil_type_count', 'EHDtH', 'Wilderness_Area3'\n- 'Elevation' is still the most important feature (as in the dataset with the raw features only)\n- Three newly generated derived features take the places from 2 to 4\n- 'Wilderness_Area3', despite its strong negative correlation with 'Wilderness_Area1', shows to be one of the top 5 most important features in the model training\n","metadata":{}},{"cell_type":"code","source":"%%time\n# subset the training and validation sets with the important features only\nX_trainPI = X_train.loc[:, pi_features]\nX_valPI   = X_val.loc[:, pi_features]","metadata":{"execution":{"iopub.status.busy":"2021-12-15T00:07:28.108086Z","iopub.status.idle":"2021-12-15T00:07:28.108686Z","shell.execute_reply.started":"2021-12-15T00:07:28.108505Z","shell.execute_reply":"2021-12-15T00:07:28.108526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Catboost  Prediction with Important Features","metadata":{}},{"cell_type":"code","source":"test_data = test.loc[:, pi_features]","metadata":{"execution":{"iopub.status.busy":"2021-12-15T00:07:28.109718Z","iopub.status.idle":"2021-12-15T00:07:28.110298Z","shell.execute_reply.started":"2021-12-15T00:07:28.110108Z","shell.execute_reply":"2021-12-15T00:07:28.11013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from catboost import CatBoostClassifier\n\ncat_params = {\n    'iterations': 20000,\n    'depth': 7,\n    'task_type' : 'GPU',\n    'l2_leaf_reg': 5,\n    'eval_metric': 'Accuracy',\n}\n\ncat = CatBoostClassifier(**cat_params)\ncat.fit(X_trainPI, y_train, eval_set=(X_valPI, y_val))","metadata":{"execution":{"iopub.status.busy":"2021-12-15T00:07:28.111355Z","iopub.status.idle":"2021-12-15T00:07:28.111894Z","shell.execute_reply.started":"2021-12-15T00:07:28.111717Z","shell.execute_reply":"2021-12-15T00:07:28.111736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predict\npredictions = cat.predict(test_data)\nsubm['Cover_Type'] = predictions\n\nif in_kaggle:\n    submission_path = 'submission.csv'\nelse:\n    submission_path = 'output/catboost_eli5_prediction.csv'\n\nsubm.to_csv(submission_path, index = False)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T00:07:28.11285Z","iopub.status.idle":"2021-12-15T00:07:28.113387Z","shell.execute_reply.started":"2021-12-15T00:07:28.113156Z","shell.execute_reply":"2021-12-15T00:07:28.113183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('We are done. That is all, folks!')\nfinish_time = dt.datetime.now()\nprint(\"Finished at \", finish_time)\nelapsed = finish_time - start_time\nprint(\"Elapsed time: \", elapsed)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T00:07:28.114369Z","iopub.status.idle":"2021-12-15T00:07:28.114885Z","shell.execute_reply.started":"2021-12-15T00:07:28.114653Z","shell.execute_reply":"2021-12-15T00:07:28.114679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# References\n\n- The basic flow of Feature Importance experiment with ELI5, along with using the Intel accelerator for Scikit-learn, inherited from the nice notebook per https://www.kaggle.com/lordozvlad/tps-dec-fast-feature-importance-with-sklearnex, by @lordozvlad\n- The additional feature engineering implemented per the excellent guideline thread [Feature engineering update thread](https://www.kaggle.com/c/tabular-playground-series-dec-2021/discussion/293612) by @lucamassaron\n- The feature importance study from the orginal competition examplified by https://www.kaggle.com/mariannejoyleano/ml-forest-cover-feature-engineering-v01 (please note they used the embedded feature importance of several algorithms, as opposed to the ELI5-backed permutation feature importancce method applied in ths experiment)\n\n\n","metadata":{}}]}