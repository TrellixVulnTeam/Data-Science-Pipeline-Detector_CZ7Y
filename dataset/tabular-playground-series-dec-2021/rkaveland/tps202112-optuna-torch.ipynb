{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"TPS202112 - Optuna torch\n==\n\nIn this notebook, I rely on the downsampled training data from [TPS202112 - Downsample easy cases](https://www.kaggle.com/kaaveland/tps202112-downsample-easy-cases) to be able to run hyper-parameter search over a simple NN architecture with reasonable speed.\n\nThe input data set here, has been cut down to around 1 million samples, from 4 million -- which naturally makes it faster to train.\n\nFirst, let's upgrade torch so we can use the RAdam optimizer, and get some imports out of the way:","metadata":{}},{"cell_type":"code","source":"import os\nimport getpass\nimport random\nimport tempfile\n\nif getpass.getuser() == 'root': # kaggle\n    %pip install -qU scikit-learn torch\n    n_jobs = os.cpu_count()\n    !cp -v ../input/optunatorchsearches/optuna.db .\n    storage = 'sqlite:///optuna.db'\nelse:\n    n_jobs = os.cpu_count() // 2 # hyper threading\n    storage = 'postgresql://localhost/optuna?host=/var/run/postgresql'\n\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nimport seaborn as sns\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import RobustScaler\nimport optuna\n\nimport torch\nfrom torch import nn, optim\nfrom torch.nn import functional as F\nfrom torch.optim import lr_scheduler\nfrom torch.backends import cudnn\nfrom tqdm.notebook import tqdm, trange","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-06T16:07:02.090287Z","iopub.execute_input":"2021-12-06T16:07:02.090823Z","iopub.status.idle":"2021-12-06T16:09:09.858762Z","shell.execute_reply.started":"2021-12-06T16:07:02.09072Z","shell.execute_reply":"2021-12-06T16:09:09.857998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next up, we'll need to set the random seeds for reproducability. \n\nI've already decided on a batch size to use for all my trials, so I'll also set `torch.backends.cuddn.benchmark = True`, for a speedboost. Note that if you're varying batch sizes, this might make your code slower.\n\nWe'll also set up an optuna study, and our CV splits. I'm using the exact same CV setup for all my models, so I can more easily compare results.","metadata":{}},{"cell_type":"code","source":"random.seed(64)\nnp.random.seed(64)\ntorch.manual_seed(64)\ncudnn.benchmark = True\n\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=64)\nstudy = optuna.create_study(\n    storage=storage, study_name='tps202112-mlp', load_if_exists=True, direction='maximize',\n)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-06T16:09:09.860742Z","iopub.execute_input":"2021-12-06T16:09:09.861011Z","iopub.status.idle":"2021-12-06T16:09:09.968128Z","shell.execute_reply.started":"2021-12-06T16:09:09.860975Z","shell.execute_reply":"2021-12-06T16:09:09.967431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I've made a small utility here, that I use to keep track of model weights. At the end of training, I call the `restore()` method on this object to restore the model weights to the ones that had the best validation accuracy. There are probably tons of ways of doing this, but this is simple and explicit:","metadata":{}},{"cell_type":"code","source":"class TrackBestWeights:\n\n    def __init__(self, model):\n        self.weights = tempfile.mktemp()\n        self.best_accuracy = None\n        self.model = model\n\n    def step(self, validation_accuracy):\n        if self.best_accuracy is None or validation_accuracy > self.best_accuracy:\n            self.best_accuracy = validation_accuracy\n            torch.save(self.model.state_dict(), self.weights)\n\n    def restore(self):\n        self.model.load_state_dict(torch.load(self.weights))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-06T16:09:21.260074Z","iopub.execute_input":"2021-12-06T16:09:21.260769Z","iopub.status.idle":"2021-12-06T16:09:21.266986Z","shell.execute_reply.started":"2021-12-06T16:09:21.260729Z","shell.execute_reply":"2021-12-06T16:09:21.266055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we're reading in data. We're using the downsampled training data set, which contains 10% of the easy cases, and all the hard cases. See [TPS202112 - Downsample easy cases](https://www.kaggle.com/kaaveland/tps202112-downsample-easy-cases) for more detail about how that works. \n\nWe're fitting a `RobustScaler` to _both_ train/test data here. Normally you wouldn't do this, but for kaggle TPS, I think it's fine. I tried a few other scalers, but couldn't get as good results with those.\n\nWe're also converting the whole dataset to torch tensors right away, and we're using `LabelEncoder` to ensure that our labels range from `[0, n_classes)`, which is what torch wants to have:","metadata":{}},{"cell_type":"code","source":"data_root = os.environ.get('KAGGLE_DIR', '../input')\nsampled_cases = pd.read_parquet(f'{data_root}/tps202112-downsample-easy-cases/train.pq', columns=['Id'])\ndf = pd.read_parquet(f'{data_root}/tpsdec2021parquet/train_fe.pq').loc[sampled_cases.Id.to_numpy()].assign(\n    Id=sampled_cases.Id.to_numpy()\n)\n\nlabel_encoder = LabelEncoder()\nX, y = df.drop(columns=['Id', 'Cover_Type']), label_encoder.fit_transform(df.Cover_Type)\nscaler = RobustScaler()\ndf_test = pd.read_parquet(f'{data_root}/tpsdec2021parquet/test_fe.pq')\nX_test = df_test\nscaler.fit(pd.concat([X, X_test]))\n\nX_test = torch.from_numpy(scaler.transform(X_test).astype(np.float32))\nX = torch.from_numpy(scaler.transform(X).astype(np.float32))\ny = torch.from_numpy(y)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-06T16:12:44.909015Z","iopub.execute_input":"2021-12-06T16:12:44.909507Z","iopub.status.idle":"2021-12-06T16:13:20.631901Z","shell.execute_reply.started":"2021-12-06T16:12:44.909466Z","shell.execute_reply":"2021-12-06T16:13:20.631034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This notebook is going to run very slowly without a GPU. Let's check which one we've got here, if any:","metadata":{}},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nif torch.cuda.is_available():\n    !nvidia-smi\nelse:\n    print('Running on cpu')","metadata":{"execution":{"iopub.status.busy":"2021-12-06T16:13:20.633406Z","iopub.execute_input":"2021-12-06T16:13:20.633652Z","iopub.status.idle":"2021-12-06T16:13:21.40432Z","shell.execute_reply.started":"2021-12-06T16:13:20.633618Z","shell.execute_reply":"2021-12-06T16:13:21.403502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here are some known good configurations I've already found on my laptop, so I'll enqueue these for optuna trials, to guide the early search:","metadata":{}},{"cell_type":"code","source":"known_good = [\n    {\n    'optim_cls': 'radam', 'weight_decay': 3.04703905856014e-05,\n    'activate': 'selu', 'patience': 10, 'lr': 0.033486004840786954\n},\n    {'optim_cls': 'radam',\n  'weight_decay': 1.4526958400830988e-05,\n  'activate': 'lrelu',\n  'patience': 15,\n  'lr': 0.029178728238702562},\n {'optim_cls': 'radam',\n  'weight_decay': 3.237655170070728e-05,\n  'activate': 'relu',\n  'patience': 5,\n  'lr': 0.030077591918879664},\n {'optim_cls': 'adam',\n  'weight_decay': 1.8181989055907467e-05,\n  'activate': 'silu',\n  'patience': 15,\n  'lr': 0.04638419754304867},\n {'optim_cls': 'adam',\n  'weight_decay': 7.148732191726605e-06,\n  'activate': 'lrelu',\n  'patience': 15,\n  'lr': 0.02068886968901973},\n {'optim_cls': 'radam',\n  'weight_decay': 8.594159652310458e-05,\n  'activate': 'silu',\n  'patience': 15,\n  'lr': 0.037170759932989746},\n {'optim_cls': 'adamw',\n  'weight_decay': 8.902236013675666e-06,\n  'activate': 'lrelu',\n  'patience': 15,\n  'lr': 0.019625012905909813},\n {'optim_cls': 'adam',\n  'weight_decay': 0.00020925505006661127,\n  'activate': 'relu',\n  'patience': 5,\n  'lr': 0.002733050595602287},\n {'optim_cls': 'sgd',\n  'momentum': 0.9671834793043608,\n  'nesterov': False,\n  'weight_decay': 1.1579798382953368e-06,\n  'activate': 'silu',\n  'patience': 15,\n  'lr': 0.033480530452169636},\n {'optim_cls': 'adamw',\n  'weight_decay': 0.0004631111057079639,\n  'activate': 'silu',\n  'patience': 15,\n  'lr': 0.003572919342126105},\n {'optim_cls': 'adamw',\n  'weight_decay': 1.6264423522028222e-06,\n  'activate': 'relu',\n  'patience': 10,\n  'lr': 0.03731454868352696},\n {'optim_cls': 'sgd',\n  'momentum': 0.7985899323111467,\n  'nesterov': True,\n  'weight_decay': 2.1503954623704132e-05,\n  'activate': 'relu',\n  'patience': 5,\n  'lr': 0.033291598506096955}]\n\nif len(study.trials) < 10:\n    for params in known_good: \n        study.enqueue_trial(params)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T16:13:22.797299Z","iopub.execute_input":"2021-12-06T16:13:22.797912Z","iopub.status.idle":"2021-12-06T16:13:22.919245Z","shell.execute_reply.started":"2021-12-06T16:13:22.797867Z","shell.execute_reply":"2021-12-06T16:13:22.918547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here's my model creation code. Note that `make_model` accepts parameters similar to the ones generated by optuna. `set_trial_params` returns a function that will generate a model with selected params on demand -- we want to use that, because we're doing multiple folds, and therefore need to create multiple models.\n\nOur setup is not very advanced. We're letting optuna find the best optimizer, learning rate, activation function and patence for `ReduceLROnPlateau`. In my local testing, `optim.RAdam` consistently gave the best results, so it might make more sense to remove the other ones.\n\nIf you're tuning only a single parameter of a neural network, that parameter should probably be your learning rate. The best value for the learning rate depends on your batch size, so finding a good value is going to take less time if you decide on a batch size, and stick with that. With the kind of tabular data we have here, we can use huge batches, much bigger than what I'm doing here. But I had good results with 4096 right from the start, so I have a good idea of what the learning rate should be for this value.","metadata":{}},{"cell_type":"code","source":"def init(layer):\n    if isinstance(layer, nn.Linear):\n        nn.init.kaiming_normal_(layer.weight, nonlinearity='relu')\n        nn.init.zeros_(layer.bias)\n\ndef make_model(\n        device='cpu', lr=8e-3, optim_cls=optim.Adam, activate=nn.ReLU, patience=5, **optim_params\n):\n    model = nn.Sequential(\n        nn.BatchNorm1d(X.shape[1]),\n        nn.Linear(X.shape[1], 128),\n        activate(),\n        nn.BatchNorm1d(128),\n        nn.Linear(128, 64),\n        activate(),\n        nn.BatchNorm1d(64),\n        nn.Linear(64, 32),\n        activate(),\n        nn.BatchNorm1d(32),\n        nn.Linear(32, 16),\n        activate(),\n        nn.BatchNorm1d(16),\n        nn.Linear(16, len(label_encoder.classes_)),\n    ).to(device)\n    model.apply(init)\n    optimizer = optim_cls(model.parameters(), lr=lr, **optim_params)\n    tracker = TrackBestWeights(model)\n    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.2, patience=patience)\n    return dict(\n        net=model,\n        tracker=tracker,\n        scheduler=scheduler,\n        optimizer=optimizer\n    )\n\ndef set_trial_params(device, trial: optuna.Trial):\n    optim_cls = {\n        'adam': optim.Adam,\n        'adamw': optim.AdamW,\n        'radam': optim.RAdam,\n        'sgd': optim.SGD,\n    }[trial.suggest_categorical('optim_cls', ['adam', 'adamw', 'radam', 'sgd'])]\n    if optim_cls == optim.SGD:\n        optim_kwargs = dict(\n            momentum=trial.suggest_uniform('momentum', .7, .99),\n            nesterov=trial.suggest_categorical('nesterov', [False, True]),\n        )\n    else:\n        optim_kwargs = dict()\n    optim_kwargs['weight_decay'] = trial.suggest_loguniform('weight_decay', 1e-6, 1e-2)\n    activate = {\n        'relu': nn.ReLU,\n        'lrelu': nn.LeakyReLU,\n        'silu': nn.SiLU,\n        'selu': nn.SELU,\n    }[trial.suggest_categorical('activate', ['relu', 'lrelu', 'silu', 'selu'])]\n    patience = trial.suggest_categorical('patience', [5, 10, 15])\n    def return_model():\n        return make_model(\n            device,\n            lr=trial.suggest_loguniform('lr', 5e-4, 6e-2),\n            optim_cls=optim_cls,\n            activate=activate, patience=patience,\n            **optim_kwargs\n        )\n    return return_model","metadata":{"execution":{"iopub.status.busy":"2021-12-06T16:13:30.836136Z","iopub.execute_input":"2021-12-06T16:13:30.836699Z","iopub.status.idle":"2021-12-06T16:13:30.851829Z","shell.execute_reply.started":"2021-12-06T16:13:30.836659Z","shell.execute_reply":"2021-12-06T16:13:30.851071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here's the function we'll ask optuna to optimize for us. It's fairly big, because it contains a whole training loop and loops over the folds too. \n\nIdeally, we'd make this even more complex by logging more metrics, like training loss and training accuracy. We could log them on the `trial` object by doing `trial.set_user_attr('training_losses', training_losses)`.\n\nWe're setting the torch seed for each trial, to ensure the models we create are reproducible. Optionally, we'll return the out of fold predictions, the test predictions and the `cv.n_splits` neural nets we trained. Optuna won't use that, it'll use only the validation accuracy -- but later on, when we're retraining the best model we found, we'll use it.","metadata":{}},{"cell_type":"code","source":"def run_experiment(trial: optuna.Trial, return_preds=False, progress='folds', cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=64)):\n    oof_preds = torch.zeros(len(X), len(label_encoder.classes_), device=device)\n    test_preds = torch.zeros(X_test.shape[0], len(label_encoder.classes_), device=device)\n    n_epochs = 140\n\n    torch.manual_seed(64)\n    make_model = set_trial_params(device, trial)\n    nets = []\n\n    if progress == 'folds':\n        folds = tqdm(cv.split(X, y), total=cv.n_splits)\n    else:\n        folds = cv.split(X, y)\n    for cv_no, (train_idx, val_idx) in enumerate(folds):\n        X_train, y_train = X[train_idx].to(device), y[train_idx].to(device)\n        X_val, y_val = X[val_idx].to(device), y[val_idx].to(device)\n\n        model = make_model()\n\n        it = range(n_epochs) if progress == 'folds' else trange(n_epochs)\n        for epoch in it:\n            x_order = torch.randperm(X_train.shape[0], device=device)\n            x_batches = torch.split(x_order, 4096)\n        \n            model['net'].train()\n        \n            for batch_idx in x_batches:\n                model['optimizer'].zero_grad()\n                X_b, y_b = X_train[batch_idx], y_train[batch_idx]\n                y_hat = model['net'](X_b)\n                loss = F.cross_entropy(y_hat, y_b)\n                loss.backward()\n                model['optimizer'].step()\n            \n            model['net'].eval()\n            accurate = 0\n            loss = 0\n            with torch.no_grad():\n                for batch_idx in torch.split(torch.arange(y_val.shape[0], device=device), 8192):\n                    X_b, y_b = X_val[batch_idx], y_val[batch_idx]\n                    y_hat = model['net'](X_b)\n                    loss += F.cross_entropy(y_hat, y_b, reduction='sum').item()\n                    accurate += (F.softmax(y_hat, dim=1).argmax(axis=1) == y_b).sum().item()\n            accurate = accurate / y_val.shape[0]\n            loss = loss / y_val.shape[0]\n            model['scheduler'].step(loss)\n            model['tracker'].step(accurate)\n            if progress != 'folds':\n                it.set_description(f'acc={accurate:.4f} best_acc={model[\"tracker\"].best_accuracy:.4f} loss={loss:.4f} ')\n\n        model['tracker'].restore()\n        nets.append(model['net'])\n        with torch.no_grad():\n            oof_preds[val_idx] = F.softmax(model['net'](X_val), dim=1)\n            val_acc = (oof_preds[val_idx].argmax(axis=1).cpu() == y[val_idx]).float().mean().item()\n            if progress == 'folds':\n                folds.set_description(f'val_acc = {val_acc:.4f}')\n            trial.report(\n                val_acc, cv_no\n            )\n            test_preds += F.softmax(model['net'](X_test.to(device)), dim=1) / cv.n_splits\n            if trial.should_prune():\n                raise optuna.TrialPruned()\n    with torch.no_grad():\n        if not return_preds:\n            return (oof_preds.argmax(axis=1).cpu() == y).float().mean()\n        else:\n            return oof_preds, test_preds, nets","metadata":{"execution":{"iopub.status.busy":"2021-12-06T16:13:34.026306Z","iopub.execute_input":"2021-12-06T16:13:34.026579Z","iopub.status.idle":"2021-12-06T16:13:34.044821Z","shell.execute_reply.started":"2021-12-06T16:13:34.026548Z","shell.execute_reply":"2021-12-06T16:13:34.044118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we simply need to ask optuna to find the best parameters, and give it a timeout. I'll give it 8 hours here. On kaggle, the folds take around 100 seconds with a GPU, so we should be able to do 36 folds / hour, or about 7 trials per hour. But we're also actively pruning trials that are not impressing us after 1 fold, so we should be able to do many more than just 60 trials:","metadata":{}},{"cell_type":"code","source":"study.optimize(run_experiment, timeout=4 * 60 * 60)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T16:23:20.499601Z","iopub.execute_input":"2021-12-06T16:23:20.502268Z","iopub.status.idle":"2021-12-06T17:42:07.138732Z","shell.execute_reply.started":"2021-12-06T16:23:20.502217Z","shell.execute_reply":"2021-12-06T17:42:07.13753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"At this point, we've found the best parameters, let's fetch the predictions and models for those:","metadata":{}},{"cell_type":"code","source":"oof_preds, test_preds, nets = run_experiment(study.best_trial, return_preds=True, progress='epochs', cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=64))","metadata":{"execution":{"iopub.status.busy":"2021-12-06T16:16:01.838041Z","iopub.execute_input":"2021-12-06T16:16:01.838781Z","iopub.status.idle":"2021-12-06T16:23:10.605974Z","shell.execute_reply.started":"2021-12-06T16:16:01.838734Z","shell.execute_reply":"2021-12-06T16:23:10.605312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will predict on the ~3 million samples we removed by downsampling, to verify that our model isn't terrible on those:","metadata":{}},{"cell_type":"code","source":"all_df = pd.read_parquet(f'{data_root}/tpsdec2021parquet/train_fe.pq').assign(\n    Id=pd.read_parquet(f'{data_root}/tpsdec2021parquet/train.pq', columns=['Id']).Id.to_numpy()\n)\n\nall_df = all_df.loc[all_df.Cover_Type != 5]\nnot_predicted = all_df.loc[~all_df.Id.isin(df.Id)]\n\nnot_predicted_X = torch.from_numpy(scaler.transform(not_predicted.drop(columns=['Id', 'Cover_Type'])).astype(np.float32))\nnot_predicted_y = torch.from_numpy(label_encoder.transform(not_predicted.Cover_Type))\npreds = torch.zeros(len(not_predicted), len(label_encoder.classes_))\n\nwith torch.no_grad():\n    for net in tqdm(nets):\n        net.eval()\n        y_hat = torch.cat([F.softmax(net(X_b.to(device)), dim=1) for X_b in torch.split(not_predicted_X, 16384)], dim=0).cpu()\n        preds += y_hat / len(nets)\n\n(preds.argmax(dim=1) == not_predicted_y).float().mean().item()","metadata":{"execution":{"iopub.status.busy":"2021-12-06T16:15:57.685616Z","iopub.status.idle":"2021-12-06T16:15:57.686314Z","shell.execute_reply.started":"2021-12-06T16:15:57.686043Z","shell.execute_reply":"2021-12-06T16:15:57.68607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As expected, the easy cases are easy and that's why we excluded most of them from training -- we can go much faster like this.\n\nWe should store the complete oof as well, not just the oof we've done on the samples we trained. This is a bit annoying to recombine, can't think of any better way than to sort both predictions by Id:","metadata":{}},{"cell_type":"code","source":"all_oof = torch.cat([preds, oof_preds.cpu()], dim=0).numpy()\ni = np.arange(len(label_encoder.classes_))\ncol_names = [f'Cover_Type={c}' for c in label_encoder.inverse_transform(i)]\n\nall_oof = pd.DataFrame(all_oof, columns=col_names).assign(\n    Id=np.concatenate([not_predicted.Id.to_numpy(), df.Id.to_numpy()], axis=0)\n)\nall_oof = all_oof.sort_values(by='Id')\nall_oof.drop(columns=['Id']).to_parquet('oof_mlp_proba.pq', index=False)\n\noof_pred_label = label_encoder.inverse_transform(all_oof.drop(columns=['Id']).to_numpy().argmax(axis=1))\n\noof_acc = np.mean(oof_pred_label == all_df.Cover_Type)\n\nprint(f'oof_acc = {oof_acc:.4f}')","metadata":{"execution":{"iopub.status.busy":"2021-12-05T19:06:36.530161Z","iopub.execute_input":"2021-12-05T19:06:36.530848Z","iopub.status.idle":"2021-12-05T19:06:38.125494Z","shell.execute_reply.started":"2021-12-05T19:06:36.530813Z","shell.execute_reply":"2021-12-05T19:06:38.124501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It's worth remembering the fact that ~.86 out of fold accuracy on the downsampled data set corresponds to ~.962 accuracy out of fold accuracy on the complete data set. We're ignoring around 73.5% of the data, because we expect to be able to do 99.9% accuracy on it. So the math here is easy enough: `73.5 * 99.9 + oof_acc * (1 - 73.5)`, which comes out to 96.22% for oof_acc = 86%.\n\nLet's store the probabilities for our test predictions so we can easily blend later:","metadata":{}},{"cell_type":"code","source":"test_proba = pd.DataFrame(test_preds.cpu().numpy(), columns=col_names)\ntest_proba.to_parquet('mlp_test_proba.pq', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-05T19:06:43.184536Z","iopub.execute_input":"2021-12-05T19:06:43.18485Z","iopub.status.idle":"2021-12-05T19:06:43.440634Z","shell.execute_reply.started":"2021-12-05T19:06:43.184817Z","shell.execute_reply":"2021-12-05T19:06:43.439536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now I've stored the probabilities of both the out of fold predictions, and test predictions. Let's check if we benefit from blending with the booster from the downsampling notebook:","metadata":{}},{"cell_type":"code","source":"blend = pd.read_parquet('oof_mlp_proba.pq').to_numpy() + pd.read_parquet(f'{data_root}/tps202112-downsample-easy-cases/oof_proba_out.pq').to_numpy()\nblend_pred = label_encoder.inverse_transform(blend.argmax(axis=1))\nnp.mean(blend_pred == all_df.Cover_Type)","metadata":{"execution":{"iopub.status.busy":"2021-12-05T19:06:44.715273Z","iopub.execute_input":"2021-12-05T19:06:44.716208Z","iopub.status.idle":"2021-12-05T19:06:45.458836Z","shell.execute_reply.started":"2021-12-05T19:06:44.716167Z","shell.execute_reply":"2021-12-05T19:06:45.457739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"That's an improvement, so we'll submit the blend, rather than just the NN:","metadata":{}},{"cell_type":"code","source":"blend = (\n    pd.read_parquet('mlp_test_proba.pq').to_numpy() \n    + pd.read_parquet(f'{data_root}/tps202112-downsample-easy-cases/test_proba_out.pq').to_numpy()\n)\nblend_pred = label_encoder.inverse_transform(test_proba.to_numpy().argmax(axis=1))\nsub = df_test[['Id']].assign(Cover_Type=blend_pred)\nsub.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-05T19:07:01.14646Z","iopub.execute_input":"2021-12-05T19:07:01.146761Z","iopub.status.idle":"2021-12-05T19:07:01.362398Z","shell.execute_reply.started":"2021-12-05T19:07:01.146729Z","shell.execute_reply":"2021-12-05T19:07:01.361356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-05T19:07:01.500947Z","iopub.execute_input":"2021-12-05T19:07:01.501424Z","iopub.status.idle":"2021-12-05T19:07:03.264828Z","shell.execute_reply.started":"2021-12-05T19:07:01.501389Z","shell.execute_reply":"2021-12-05T19:07:03.263735Z"},"trusted":true},"execution_count":null,"outputs":[]}]}