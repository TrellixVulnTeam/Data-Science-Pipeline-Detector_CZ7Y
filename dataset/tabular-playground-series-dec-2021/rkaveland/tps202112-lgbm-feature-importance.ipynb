{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Reading parquet data\n==\n\nIn [this notebook](https://www.kaggle.com/kaaveland/tps202112-parquet) I converted the competition data to parquet format, so that I don't need to read the data from csv in future notebooks for this competition.\n\nCSV files have some pretty annoying disadvantages:\n\n- It is slow to read data from CSV -- this is particularly egregious if your data has timestamp columns in it, or if you have strings.\n- They are big. This is part of the reason why they're slow, it simply takes a while to move so much text data from disk into memory.\n- They are untyped. In CSV, everything is a string -- it's up to the reading program to decide how to interpret the strings.\n\nParquet files do much better in all of these aspects, at the cost of not being human-readable text files.\n\nIn this case, our `train.pq` is 77MB, vs 548MB for `train.csv` -- even though it contains the same data!\n\nThis is a pretty normal compression ratio, in my experience -- when there are low cardinality columns, or repeated values, parquet files use tricks like run-length encoding to achieve compression of ratios between 2-10 compared to CSV.\n\nLet's measure how long it takes to read the parquet files:","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport lightgbm\nimport numpy as np\nimport random\nimport plotly.express as px\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nrandom.seed(64)\nnp.random.seed(64)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-01T11:35:31.672495Z","iopub.execute_input":"2021-12-01T11:35:31.673071Z","iopub.status.idle":"2021-12-01T11:35:32.825217Z","shell.execute_reply.started":"2021-12-01T11:35:31.673032Z","shell.execute_reply":"2021-12-01T11:35:32.824522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%time df_test = pd.read_parquet('../input/tpsdec2021parquet/test.pq')\n%time df = pd.read_parquet('../input/tpsdec2021parquet/train.pq')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-01T11:38:40.613943Z","iopub.execute_input":"2021-12-01T11:38:40.614883Z","iopub.status.idle":"2021-12-01T11:38:41.604271Z","shell.execute_reply.started":"2021-12-01T11:38:40.614845Z","shell.execute_reply":"2021-12-01T11:38:41.603592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"That takes about 1 second on kaggle. Let's compare that with the CSV files:","metadata":{}},{"cell_type":"code","source":"%time pd.read_csv('../input/tabular-playground-series-dec-2021/test.csv')\n%time csv_train = pd.read_csv('../input/tabular-playground-series-dec-2021/train.csv')","metadata":{"execution":{"iopub.status.busy":"2021-12-01T11:38:22.345603Z","iopub.execute_input":"2021-12-01T11:38:22.345953Z","iopub.status.idle":"2021-12-01T11:38:40.612611Z","shell.execute_reply.started":"2021-12-01T11:38:22.345918Z","shell.execute_reply":"2021-12-01T11:38:40.611911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"At more than 20 seconds, there's really no contest -- especially, because the csv train file has all the wrong datatypes, whereas the parquet file remembers our selection from [the last notebook](https://www.kaggle.com/kaaveland/tps202112-parquet?scriptVersionId=81264309):","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T11:39:20.2194Z","iopub.execute_input":"2021-12-01T11:39:20.21972Z","iopub.status.idle":"2021-12-01T11:39:20.237402Z","shell.execute_reply.started":"2021-12-01T11:39:20.219686Z","shell.execute_reply":"2021-12-01T11:39:20.236349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Use `lightgbm` to estimate feature importances\n==\n\nHere's a baseline model I typed up to get some feature importance plots:","metadata":{}},{"cell_type":"code","source":"label_encoder = LabelEncoder()\n\nX_train, y_train = df.drop(columns=['Id', 'Cover_Type']), label_encoder.fit_transform(df.Cover_Type)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, shuffle=True, test_size=.2)\nX_test = df_test.drop(columns=['Id'])","metadata":{"execution":{"iopub.status.busy":"2021-12-01T10:34:49.0242Z","iopub.execute_input":"2021-12-01T10:34:49.024449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nsane_defaults = {\n    'objective': 'multiclass',\n    'num_class': len(label_encoder.classes_),\n    'learning_rate': .025,\n    'seed': 64,\n    'boosting': 'goss',\n    'feature_fraction': .5,\n    'force_row_wise': True,\n    'metric': ['multi_logloss', 'multi_error'],\n    'verbosity': -1,\n    'first_metric_only': True,\n}\n\nbooster = lightgbm.train(\n    params=sane_defaults,\n    train_set=lightgbm.Dataset(X_train, label=y_train),\n    num_boost_round=3000,\n    valid_sets=[lightgbm.Dataset(X_val, label=y_val)],\n    early_stopping_rounds=50,\n    verbose_eval=100,\n)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T10:34:49.0242Z","iopub.execute_input":"2021-12-01T10:34:49.024449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"px.bar(\n    x=booster.feature_name(),\n    y=booster.feature_importance(),\n    title='importance_type = \"split\"'\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"px.bar(\n    x=booster.feature_name(),\n    y=booster.feature_importance('gain'),\n    title='importance_type = \"gain\"'\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = df_test[['Id']].assign(\n    Cover_Type=label_encoder.inverse_transform(booster.predict(X_test).argmax(axis=1))\n)\n\nsub.to_csv('submission.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]}]}