{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"CTGAN artifacts? Chunks?\n==\n\nIn TPS202111, kagglers discovered that labels had been flipped at random. As part of that, I (and others) ended up spending a lot of time checking out chunks and streaks of the data in order to try to see if there was a pattern to how the label flips had been chosen. We didn't find any, but we discovered a lot of weirdnesses about the data.\n\nFor the past few days, something hasn't been sitting right with me when I've been working on this TPS. I've been feeling like the cross validation folds almost make a bigger difference than model changes at this point on public LB. And the models I have that do best on local CV are no longer best on public LB.\n\nSo that got me thinking, is there some part of the training data that is somehow closer to the test data, that causes this to happen? And I decided to dig out some tools from the old TPS -- namely, I wanted to check if the data generation / distribution is, as it was back in TPS202111, dependent on ID somehow.\n\nHere's the train class balance by ID -- this plot is based on cumulative counting of each class, we subtract the number of classes a \"dice\" with the total distribution set would've seen. Eg. when the `Cover_Type` line is at around 50 000 below, it means that, the data up until `Id=2.5e6` contains around 50 000 too many instances of that `Cover_Type` compared to the eventual result, which means that for IDs greater than `2.5e6`, we will expect the rate of `Cover_Type=2` to significantly decrease.","metadata":{}},{"cell_type":"code","source":"import random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import StratifiedKFold\nimport lightgbm\n\nrandom.seed(64)\nnp.random.seed(64)\n\nsns.set(style='darkgrid', context='notebook', rc={'figure.figsize': (16, 12), 'figure.frameon': False, 'legend.frameon': False})\ndf = pd.read_parquet('../input/tpsdec2021parquet/train.pq').set_index('Id')\n\nlabel_summary = pd.DataFrame({\n    f'Cover_Type_{cls}': (df.Cover_Type == cls).cumsum() - np.arange(len(df)) * (df.Cover_Type == cls).mean() \n    for cls in df.Cover_Type.unique()}\n)\nlabel_summary.plot.line(title='train class balance by id');","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-18T11:58:05.539863Z","iopub.execute_input":"2021-12-18T11:58:05.540137Z","iopub.status.idle":"2021-12-18T11:58:29.525449Z","shell.execute_reply.started":"2021-12-18T11:58:05.540102Z","shell.execute_reply":"2021-12-18T11:58:29.524427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I thought it was fairly interesting that there's such a strong variation here. If you do folds by id without shuffling/stratifying, it's almost certainly the case that some of the folds have a distribution *much more* like test, than others.\n\nAnother thing I thought might be interesting to study, would be to attempt to see if this sort of thing also happens in the test set. We don't have true labels for that, so I've used the predictions from [this notebook](https://www.kaggle.com/remekkinas/tps-12-nn-tpu-pseudolabeling-0-95690) to compare, since we know that it's pretty close to the real labels.\n\nLet's first have a look at test alone:","metadata":{}},{"cell_type":"code","source":"df_test = pd.read_csv('../input/tps-12-nn-tpu-pseudolabeling-0-95690/tps12-pseudeo-submission.csv').set_index('Id')\nlabel_summary_test = pd.DataFrame({\n    f'Cover_Type_{cls}': (df_test.Cover_Type == cls).cumsum() - np.arange(len(df_test)) * (df_test.Cover_Type == cls).mean() \n    for cls in df_test.Cover_Type.unique()}\n)\nlabel_summary_test.plot.line(title='test class balance by id');","metadata":{"execution":{"iopub.status.busy":"2021-12-18T11:58:29.527272Z","iopub.execute_input":"2021-12-18T11:58:29.528013Z","iopub.status.idle":"2021-12-18T11:58:34.383717Z","shell.execute_reply.started":"2021-12-18T11:58:29.527964Z","shell.execute_reply":"2021-12-18T11:58:34.382843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then, we'll connect train and test:","metadata":{}},{"cell_type":"code","source":"df = pd.concat([df[['Cover_Type']], df_test])\n\nlabel_summary = pd.DataFrame({\n    f'Cover_Type_{cls}': (df.Cover_Type == cls).cumsum() - np.arange(len(df)) * (df.Cover_Type == cls).mean() \n    for cls in df.Cover_Type.unique()}\n)\nlabel_summary.plot.line(title='class balance by id');","metadata":{"execution":{"iopub.status.busy":"2021-12-18T11:58:34.385056Z","iopub.execute_input":"2021-12-18T11:58:34.385527Z","iopub.status.idle":"2021-12-18T11:58:59.832064Z","shell.execute_reply.started":"2021-12-18T11:58:34.385495Z","shell.execute_reply":"2021-12-18T11:58:59.831174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Well, would you look at that? To me, there's no abrubt change in the frequency of generated labels on the border between train and test.\n\nCan we make use of that information? Is there something special that takes place on the inflection points of these curves? Does the same sort of thing happen in feature space? I don't really know. Either way, I thought these line charts were way too pretty to not show them to anybody. I'd be tempted to say from my finding that the dataset hasn't been shuffled after it was generated?","metadata":{}},{"cell_type":"code","source":"df.groupby([(df.index // 20000) * 20000, df.Cover_Type]).size().unstack().fillna(0).plot.line(title='Cover_Type count per 20k Id');","metadata":{"execution":{"iopub.status.busy":"2021-12-18T11:58:59.833857Z","iopub.execute_input":"2021-12-18T11:58:59.834066Z","iopub.status.idle":"2021-12-18T11:59:00.561041Z","shell.execute_reply.started":"2021-12-18T11:58:59.834041Z","shell.execute_reply":"2021-12-18T11:59:00.560213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here's the same kind of cumsum-plot thinking for the bool/categorical features:","metadata":{}},{"cell_type":"code","source":"del df\n\n\ndf = pd.read_parquet('../input/tpsdec2021parquet/train.pq').set_index('Id')\nbool_cols = df.columns[df.dtypes == np.bool_]\ndelta_to_uniform = df[bool_cols].cumsum() - df[bool_cols].mean().to_numpy() * np.arange(len(df)).reshape(-1, 1)\ndelta_to_uniform[::20].plot.line()\nplt.legend(loc='upper left', ncol=3);","metadata":{"execution":{"iopub.status.busy":"2021-12-18T11:59:00.562017Z","iopub.execute_input":"2021-12-18T11:59:00.562239Z","iopub.status.idle":"2021-12-18T11:59:08.602276Z","shell.execute_reply.started":"2021-12-18T11:59:00.562202Z","shell.execute_reply":"2021-12-18T11:59:08.601465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It's easy to see that the incidence rate of these change throughout the dataset.\n\nComparison with Forest Cover Type Dataset\n==\n\nWe know that the data in this TPS was generated from the forest cover type data set, so let's see if the original dataset has these artifacts:","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/forest-cover-type-dataset/covtype.csv', dtype=df.dtypes.to_dict()).assign(\n    Id=lambda df: np.arange(len(df))\n).set_index('Id')\n\nlabel_summary = pd.DataFrame({\n    f'Cover_Type_{cls}': (df.Cover_Type == cls).cumsum() - np.arange(len(df)) * (df.Cover_Type == cls).mean() \n    for cls in df.Cover_Type.unique()}\n)\nlabel_summary.plot.line(title='Original class balance by id');","metadata":{"execution":{"iopub.status.busy":"2021-12-18T11:59:08.603574Z","iopub.execute_input":"2021-12-18T11:59:08.603921Z","iopub.status.idle":"2021-12-18T11:59:14.446466Z","shell.execute_reply.started":"2021-12-18T11:59:08.60388Z","shell.execute_reply":"2021-12-18T11:59:14.445663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bool_cols = df.columns[df.dtypes == np.bool_]\ndelta_to_uniform = df[bool_cols].cumsum() - df[bool_cols].mean().to_numpy() * np.arange(len(df)).reshape(-1, 1)\ndelta_to_uniform[::2].plot.line()\nplt.legend(loc='upper left', ncol=3);","metadata":{"execution":{"iopub.status.busy":"2021-12-18T11:59:14.447862Z","iopub.execute_input":"2021-12-18T11:59:14.448311Z","iopub.status.idle":"2021-12-18T11:59:18.011138Z","shell.execute_reply.started":"2021-12-18T11:59:14.448267Z","shell.execute_reply":"2021-12-18T11:59:18.010174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I would say this actually has the same kind of behaviour as the synthetic dataset. As a control, let's verify that if we shuffle the data, this kind of thing doesn't happen:","metadata":{}},{"cell_type":"code","source":"df = df.sample(frac=1)\n\nlabel_summary = pd.DataFrame({\n    f'Cover_Type_{cls}': (df.Cover_Type == cls).cumsum() - np.arange(len(df)) * (df.Cover_Type == cls).mean() \n    for cls in df.Cover_Type.unique()}\n).set_index(pd.Series(np.arange(len(df)), name='Id'))\nlabel_summary.plot.line(title='Original class balance by id');","metadata":{"execution":{"iopub.status.busy":"2021-12-18T11:59:18.012242Z","iopub.execute_input":"2021-12-18T11:59:18.012955Z","iopub.status.idle":"2021-12-18T11:59:21.494501Z","shell.execute_reply.started":"2021-12-18T11:59:18.012921Z","shell.execute_reply":"2021-12-18T11:59:21.493851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Right, looking a lot more like what we'd expect, at imbalances of only a few hundred. I guess that this implies that both the synthetic dataset and the original are ordered by some attribute that has some sort of interaction with the label?\n\nIs the data ordered?\n==\n\nIn [this post](https://www.kaggle.com/c/tabular-playground-series-dec-2021/discussion/292381), we can see that a classifier can separate data between the train and test data sets. At this point, I have a hunch that the data is ordered by something (other than `Id`), and that whatever it is, causes the features to somehow be ordered too.\n\nJust for fun, let's check if it is predictable whether a sample has `Id` in the upper half of the training data:","metadata":{}},{"cell_type":"code","source":"df = pd.read_parquet('../input/tpsdec2021parquet/train.pq')\ny = df.Id > df.Id.median()\nX = df.drop(columns=['Id', 'Cover_Type'])\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=64)\n\nfor train_idx, val_idx in cv.split(X, y):\n    train_set = lightgbm.Dataset(X.iloc[train_idx], label=y[train_idx])\n    val_set = lightgbm.Dataset(X.iloc[val_idx], label=y[val_idx])\n    booster = lightgbm.train(\n        params={'objective': 'binary', 'metric': ['auc'], 'lambda': 2},\n        train_set=train_set,\n        valid_sets=[val_set],\n        num_boost_round=400,\n        early_stopping_rounds=10,\n        verbose_eval=50,\n    )","metadata":{"execution":{"iopub.status.busy":"2021-12-18T12:01:17.917157Z","iopub.execute_input":"2021-12-18T12:01:17.917463Z","iopub.status.idle":"2021-12-18T12:13:55.531646Z","shell.execute_reply.started":"2021-12-18T12:01:17.917428Z","shell.execute_reply":"2021-12-18T12:13:55.53091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Right, there's a fairly strong indication that the data is ordered in some sort of way. How strongly ordered is it? Can we predict the index?","metadata":{}},{"cell_type":"code","source":"for train_idx, val_idx in cv.split(X, y):\n    train_set = lightgbm.Dataset(X.iloc[train_idx], label=train_idx)\n    val_set = lightgbm.Dataset(X.iloc[val_idx], label=val_idx)\n    booster = lightgbm.train(\n        params={'objective': 'regression', 'metric': ['mae'], 'lambda': 2},\n        train_set=train_set,\n        valid_sets=[val_set],\n        num_boost_round=300,\n        early_stopping_rounds=10,\n        verbose_eval=50,\n    )","metadata":{"execution":{"iopub.status.busy":"2021-12-18T12:15:15.233398Z","iopub.execute_input":"2021-12-18T12:15:15.233744Z","iopub.status.idle":"2021-12-18T12:25:31.169035Z","shell.execute_reply.started":"2021-12-18T12:15:15.233708Z","shell.execute_reply":"2021-12-18T12:25:31.168355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Don't rightly know what to make of this. No single feature column is sorted, not anywhere near sorted. Let's recheck that.\n\nWhat does it mean to be sorted? Well, the next value is smaller than the current value, for each column, for all the rows. Or it could be that the next value is greater than the current value, for each column, for all the rows. Here's a quick check:","metadata":{}},{"cell_type":"code","source":"(X <= X.shift()).mean()","metadata":{"execution":{"iopub.status.busy":"2021-12-18T12:32:38.402735Z","iopub.execute_input":"2021-12-18T12:32:38.40303Z","iopub.status.idle":"2021-12-18T12:32:55.506163Z","shell.execute_reply.started":"2021-12-18T12:32:38.402998Z","shell.execute_reply":"2021-12-18T12:32:55.504916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(X >= X.shift()).mean()","metadata":{"execution":{"iopub.status.busy":"2021-12-18T12:33:00.271501Z","iopub.execute_input":"2021-12-18T12:33:00.271789Z","iopub.status.idle":"2021-12-18T12:33:17.245569Z","shell.execute_reply.started":"2021-12-18T12:33:00.271756Z","shell.execute_reply":"2021-12-18T12:33:17.244497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"None of the columns are actually sorted. The Soil_Type columns that appear sorted just don't have any values. The other Soil Type columns are simply very sparse.","metadata":{}},{"cell_type":"code","source":"pred_diff = val_idx - booster.predict(\n    X.iloc[val_idx]\n)\n\npd.Series(pred_diff).describe()","metadata":{"execution":{"iopub.status.busy":"2021-12-18T12:26:22.671075Z","iopub.execute_input":"2021-12-18T12:26:22.671378Z","iopub.status.idle":"2021-12-18T12:26:42.961286Z","shell.execute_reply.started":"2021-12-18T12:26:22.671347Z","shell.execute_reply":"2021-12-18T12:26:42.960647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.histplot(x=pred_diff, bins=50, kde=True);","metadata":{"execution":{"iopub.status.busy":"2021-12-18T12:27:27.014532Z","iopub.execute_input":"2021-12-18T12:27:27.01487Z","iopub.status.idle":"2021-12-18T12:27:30.514525Z","shell.execute_reply.started":"2021-12-18T12:27:27.014838Z","shell.execute_reply":"2021-12-18T12:27:30.513087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame({\n    'name': booster.feature_name(),\n    'value': booster.feature_importance()\n}).plot.barh(x='name', y='value');","metadata":{"execution":{"iopub.status.busy":"2021-12-18T12:29:02.501483Z","iopub.execute_input":"2021-12-18T12:29:02.502052Z","iopub.status.idle":"2021-12-18T12:29:03.740365Z","shell.execute_reply.started":"2021-12-18T12:29:02.502014Z","shell.execute_reply":"2021-12-18T12:29:03.739696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}