{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-04T00:03:29.063175Z","iopub.execute_input":"2021-12-04T00:03:29.063596Z","iopub.status.idle":"2021-12-04T00:03:30.062105Z","shell.execute_reply.started":"2021-12-04T00:03:29.063514Z","shell.execute_reply":"2021-12-04T00:03:30.061055Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<center style=\"font-family:verdana;\"><h1 style=\"font-size:200%; padding: 10px; background: #87CEFA;\"><b style=\"color:#0000CD;\">Learn with other Kagglers. Come to our Playground.</b></h1></center>\n\nThat code was the first one that I copied in a Competition (Learn With Other Kaggle Users). Then I named my work: \"Adapt and Overcome\" (2019). For obvious reasons I didn't adapted anything. Just changed the file's names and commented some lines where I got errors that I couldn't fix. Overcome? I don't think so. Though my titles are still fun. \n\nThen I got that awful message that pursues me: \"Your notebook tried to allocate more memory than is available. It has restarted.\" That it's not fun even too easy.\n\nAnyway, thank you @TooEzy (Emrecan) for having shared this script in 2015.  See you all in the next Forest Cover Types. In the Roosevelt National Forest of Northern Colorado or anywhere else.","metadata":{}},{"cell_type":"markdown","source":"![](https://i.ytimg.com/vi/m_V6m4snTNM/maxresdefault.jpg)youtube.com","metadata":{}},{"cell_type":"code","source":"data_train = pd.read_csv(\"../input/tabular-playground-series-dec-2021/train.csv\")\ndata_test = pd.read_csv(\"../input/tabular-playground-series-dec-2021/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-12-04T00:07:01.79867Z","iopub.execute_input":"2021-12-04T00:07:01.799505Z","iopub.status.idle":"2021-12-04T00:07:24.934322Z","shell.execute_reply.started":"2021-12-04T00:07:01.799467Z","shell.execute_reply":"2021-12-04T00:07:24.933372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_soil = ['Soil_Type1', 'Soil_Type2', 'Soil_Type3',\n       'Soil_Type4', 'Soil_Type5', 'Soil_Type6', 'Soil_Type7', 'Soil_Type8',\n       'Soil_Type9', 'Soil_Type10', 'Soil_Type11', 'Soil_Type12',\n       'Soil_Type13', 'Soil_Type14', 'Soil_Type15', 'Soil_Type16',\n       'Soil_Type17', 'Soil_Type18', 'Soil_Type19', 'Soil_Type20',\n       'Soil_Type21', 'Soil_Type22', 'Soil_Type23', 'Soil_Type24',\n       'Soil_Type25', 'Soil_Type26', 'Soil_Type27', 'Soil_Type28',\n       'Soil_Type29', 'Soil_Type30', 'Soil_Type31', 'Soil_Type32',\n       'Soil_Type33', 'Soil_Type34', 'Soil_Type35', 'Soil_Type36',\n       'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'Soil_Type40']\ndata_train[\"Soil_Count\"] = data_train[features_soil].apply(sum, axis=1)\ndata_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-04T00:07:58.76643Z","iopub.execute_input":"2021-12-04T00:07:58.766683Z","iopub.status.idle":"2021-12-04T00:09:01.966495Z","shell.execute_reply.started":"2021-12-04T00:07:58.766657Z","shell.execute_reply":"2021-12-04T00:09:01.965636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_wilderness = ['Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3','Wilderness_Area4']\ndata_train[\"Wilderness_Area\"] = data_train[features_wilderness].apply(sum, axis=1)\ndata_train.Wilderness_Area.describe()","metadata":{"execution":{"iopub.status.busy":"2021-12-04T00:09:24.780193Z","iopub.execute_input":"2021-12-04T00:09:24.78206Z","iopub.status.idle":"2021-12-04T00:10:12.138322Z","shell.execute_reply.started":"2021-12-04T00:09:24.782008Z","shell.execute_reply":"2021-12-04T00:10:12.137514Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_train[\"Wilderness_Area\"] = data_train[features_wilderness].apply(np.argmax, axis=1)\n#data_train[\"Wilderness_Area\"] = data_train[\"Wilderness_Area\"].apply(lambda x: x.split(\"Wilderness_Area\")[-1])\ndata_train.Wilderness_Area.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-04T00:15:08.770396Z","iopub.execute_input":"2021-12-04T00:15:08.770757Z","iopub.status.idle":"2021-12-04T00:18:12.662235Z","shell.execute_reply.started":"2021-12-04T00:15:08.77072Z","shell.execute_reply":"2021-12-04T00:18:12.661125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Above I got AttributeError: 'int' object has no attribute 'split'. Then I commented the line in the middle of the snippet above.","metadata":{}},{"cell_type":"code","source":"features = ['Elevation', 'Aspect', 'Slope','Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology',\n       'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points', \"Cover_Type\"]\nsns.heatmap(data=data_train[features].corr(), annot=True, linecolor=\"w\", fmt=\".1\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-04T00:18:54.075712Z","iopub.execute_input":"2021-12-04T00:18:54.076946Z","iopub.status.idle":"2021-12-04T00:18:56.801309Z","shell.execute_reply.started":"2021-12-04T00:18:54.076836Z","shell.execute_reply":"2021-12-04T00:18:56.800344Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot(data_train[\"Hillshade_Noon\"].apply(lambda x: x**4))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-04T00:19:34.418113Z","iopub.execute_input":"2021-12-04T00:19:34.418455Z","iopub.status.idle":"2021-12-04T00:19:54.028672Z","shell.execute_reply.started":"2021-12-04T00:19:34.418421Z","shell.execute_reply":"2021-12-04T00:19:54.027836Z"},"_kg_hide-output":false,"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clear_dataset(dataset):\n    features_soil = ['Soil_Type1', 'Soil_Type2', 'Soil_Type3',\n       'Soil_Type4', 'Soil_Type5', 'Soil_Type6', 'Soil_Type7', 'Soil_Type8',\n       'Soil_Type9', 'Soil_Type10', 'Soil_Type11', 'Soil_Type12',\n       'Soil_Type13', 'Soil_Type14', 'Soil_Type15', 'Soil_Type16',\n       'Soil_Type17', 'Soil_Type18', 'Soil_Type19', 'Soil_Type20',\n       'Soil_Type21', 'Soil_Type22', 'Soil_Type23', 'Soil_Type24',\n       'Soil_Type25', 'Soil_Type26', 'Soil_Type27', 'Soil_Type28',\n       'Soil_Type29', 'Soil_Type30', 'Soil_Type31', 'Soil_Type32',\n       'Soil_Type33', 'Soil_Type34', 'Soil_Type35', 'Soil_Type36',\n       'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'Soil_Type40']\n    features_wilderness = ['Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3','Wilderness_Area4']\n    dataset[\"Soil_Type\"] = dataset[features_soil].apply(np.argmax, axis=1)\n    #dataset[\"Soil_Type\"] = dataset[\"Soil_Type\"].apply(lambda x: x.split(\"Soil_Type\")[-1]).astype(int)\n    dataset = dataset.drop([\"Soil_Type15\", \"Soil_Type7\"], axis=1)\n    dataset[\"Wilderness_Area\"] = dataset[features_wilderness].apply(np.argmax, axis=1)\n    #dataset[\"Wilderness_Area\"] = dataset[\"Wilderness_Area\"].apply(lambda x: x.split(\"Wilderness_Area\")[-1]).astype(int)\n    #dataset = dataset.drop(features_wilderness, axis=1)\n    dataset[\"Hillshade_1\"] = (dataset.Hillshade_Noon * dataset.Hillshade_9am)\n    dataset[\"Hillshade_1_sqrt\"] = np.sqrt(dataset[\"Hillshade_1\"])\n    dataset[\"Hillshade_2\"] = (dataset.Hillshade_3pm * dataset.Hillshade_9am)\n    dataset[\"Hillshade_2_sqrt\"] = np.sqrt(dataset[\"Hillshade_2\"])\n    dataset[\"Hillshade_3\"] = (dataset.Hillshade_3pm * dataset.Hillshade_Noon * dataset.Hillshade_9am)\n    dataset[\"Hillshade_3_sqrt\"] = np.sqrt(dataset[\"Hillshade_3\"])\n    dataset.Hillshade_1 = dataset.Hillshade_1.astype(float)\n    dataset[\"DistanceToHydrology\"] = np.sqrt(dataset.Horizontal_Distance_To_Hydrology ** 2 + dataset.Vertical_Distance_To_Hydrology ** 2)\n    dataset[\"Horizontal_Distance_To_Roadways_sqrt\"]= np.sqrt(dataset[\"Horizontal_Distance_To_Roadways\"])\n    dataset[\"Horizontal_Distance_To_Fire_Points_sqrt\"]= np.sqrt(dataset[\"Horizontal_Distance_To_Fire_Points\"])\n    dataset[\"Slope_sqrt\"] = np.sqrt(dataset[\"Slope\"])\n    dataset[\"Hillshade_9am_cube\"] = dataset[\"Hillshade_9am\"].apply(lambda x: x**3)\n    dataset[\"Hillshade_Noon_cube\"] = dataset[\"Hillshade_Noon\"].apply(lambda x: x**3)\n    dataset[\"Aspect_Slope_cbrt\"] = np.cbrt(dataset.Aspect * dataset.Slope)\n    dataset[\"Elevation_Slope_sqrt\"] = np.sqrt(dataset.Elevation * dataset.Slope)\n    dataset[\"Elevation_Aspect_sqrt\"] = np.sqrt(dataset.Elevation * dataset.Aspect)\n    dataset[\"Elevation_sqrt\"] = np.sqrt(dataset.Elevation)\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2021-12-04T00:24:40.023242Z","iopub.execute_input":"2021-12-04T00:24:40.023557Z","iopub.status.idle":"2021-12-04T00:24:40.038461Z","shell.execute_reply.started":"2021-12-04T00:24:40.023523Z","shell.execute_reply":"2021-12-04T00:24:40.037571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Again  I got AttributeError: 'int' object has no attribute 'split'. Then I commented the line in the middle of the snippet above.","metadata":{}},{"cell_type":"code","source":"data_test = pd.read_csv(\"../input/tabular-playground-series-dec-2021/test.csv\")\nfinal_test = clear_dataset(data_test)","metadata":{"execution":{"iopub.status.busy":"2021-12-04T00:24:48.262967Z","iopub.execute_input":"2021-12-04T00:24:48.263488Z","iopub.status.idle":"2021-12-04T00:26:25.431711Z","shell.execute_reply.started":"2021-12-04T00:24:48.263435Z","shell.execute_reply":"2021-12-04T00:26:25.430732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_train = pd.read_csv(\"../input/tabular-playground-series-dec-2021//train.csv\")\nfinal_train = clear_dataset(data_train)","metadata":{"execution":{"iopub.status.busy":"2021-12-04T00:26:50.992716Z","iopub.execute_input":"2021-12-04T00:26:50.993068Z","iopub.status.idle":"2021-12-04T00:33:23.220573Z","shell.execute_reply.started":"2021-12-04T00:26:50.993031Z","shell.execute_reply":"2021-12-04T00:33:23.219928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_data = final_train.drop([\"Cover_Type\", \"Id\"], axis=1)\ny_data = final_train[\"Cover_Type\"]","metadata":{"execution":{"iopub.status.busy":"2021-12-04T00:33:31.480452Z","iopub.execute_input":"2021-12-04T00:33:31.480915Z","iopub.status.idle":"2021-12-04T00:33:37.317046Z","shell.execute_reply.started":"2021-12-04T00:33:31.480878Z","shell.execute_reply":"2021-12-04T00:33:37.315984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_data = final_train.drop([\"Cover_Type\", \"Id\"], axis=1)\ny_data = final_train[\"Cover_Type\"]","metadata":{"execution":{"iopub.status.busy":"2021-12-04T00:34:42.115594Z","iopub.execute_input":"2021-12-04T00:34:42.116126Z","iopub.status.idle":"2021-12-04T00:34:42.87314Z","shell.execute_reply.started":"2021-12-04T00:34:42.116076Z","shell.execute_reply":"2021-12-04T00:34:42.871129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_val, y_train, y_val = train_test_split(x_data, y_data, test_size=0.25, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-12-04T00:34:47.690736Z","iopub.execute_input":"2021-12-04T00:34:47.691512Z","iopub.status.idle":"2021-12-04T00:34:52.375215Z","shell.execute_reply.started":"2021-12-04T00:34:47.691467Z","shell.execute_reply":"2021-12-04T00:34:52.374337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Your notebook tried to allocate more memory than is available. It has restarted.\n\n#Time to give up. I knew it. It was not Too Easy as I thought. ","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier(n_estimators=100, max_depth=19, max_features=11,n_jobs=-1, random_state=42)\nclf.fit(x_train, y_train)\n\ny_predicted = clf.predict(x_val)","metadata":{"execution":{"iopub.status.busy":"2021-12-04T00:34:58.26821Z","iopub.execute_input":"2021-12-04T00:34:58.268694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n\ndef get_metrics(y_test, y_predicted):  \n    # true positives / (true positives+false positives)\n    precision = precision_score(y_test, y_predicted, pos_label=None,\n                                    average='weighted')             \n    # true positives / (true positives + false negatives)\n    recall = recall_score(y_test, y_predicted, pos_label=None,\n                              average='weighted')\n    \n    # harmonic mean of precision and recall\n    f1 = f1_score(y_test, y_predicted, pos_label=None, average='weighted')\n    \n    # true positives + true negatives/ total\n    accuracy = accuracy_score(y_test, y_predicted)\n    return accuracy, precision, recall, f1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy, precision, recall, f1 = get_metrics(y_val, y_predicted)\nprint(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_feature_importances(clf, X_train, y_train=None, \n                             top_n=10, figsize=(8,8), print_table=False, title=\"Feature Importances\"):\n    '''\n    plot feature importances of a tree-based sklearn estimator\n    \n    Note: X_train and y_train are pandas DataFrames\n    \n    Note: Scikit-plot is a lovely package but I sometimes have issues\n              1. flexibility/extendibility\n              2. complicated models/datasets\n          But for many situations Scikit-plot is the way to go\n          see https://scikit-plot.readthedocs.io/en/latest/Quickstart.html\n    \n    Parameters\n    ----------\n        clf         (sklearn estimator) if not fitted, this routine will fit it\n        \n        X_train     (pandas DataFrame)\n        \n        y_train     (pandas DataFrame)  optional\n                                        required only if clf has not already been fitted \n        \n        top_n       (int)               Plot the top_n most-important features\n                                        Default: 10\n                                        \n        figsize     ((int,int))         The physical size of the plot\n                                        Default: (8,8)\n        \n        print_table (boolean)           If True, print out the table of feature importances\n                                        Default: False\n        \n    Returns\n    -------\n        the pandas dataframe with the features and their importance\n        \n    Author\n    ------\n        George Fisher\n    '''\n    \n    __name__ = \"plot_feature_importances\"\n    \n    import pandas as pd\n    import numpy  as np\n    import matplotlib.pyplot as plt\n    \n    from xgboost.core     import XGBoostError\n    from lightgbm.sklearn import LightGBMError\n    \n    try: \n        if not hasattr(clf, 'feature_importances_'):\n            clf.fit(X_train.values, y_train.values.ravel())\n\n            if not hasattr(clf, 'feature_importances_'):\n                raise AttributeError(\"{} does not have feature_importances_ attribute\".\n                                    format(clf.__class__.__name__))\n                \n    except (XGBoostError, LightGBMError, ValueError):\n        clf.fit(X_train.values, y_train.values.ravel())\n            \n    feat_imp = pd.DataFrame({'importance':clf.feature_importances_})    \n    feat_imp['feature'] = X_train.columns\n    feat_imp.sort_values(by='importance', ascending=False, inplace=True)\n    feat_imp = feat_imp.iloc[:top_n]\n    \n    feat_imp.sort_values(by='importance', inplace=True)\n    feat_imp = feat_imp.set_index('feature', drop=True)\n    feat_imp.plot.barh(title=title, figsize=figsize)\n    plt.xlabel('Feature Importance Score')\n    plt.show()\n    \n    if print_table:\n        from IPython.display import display\n        print(\"Top {} features in descending order of importance\".format(top_n))\n        display(feat_imp.sort_values(by='importance', ascending=False))\n        \n    return feat_imp","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#I don't even know who is George Fisher, the author of the snippet above that Enrecan (@tooezy) has copied and gave credits.","metadata":{}},{"cell_type":"code","source":"a = plot_feature_importances(clf, x_train, y_train, top_n=x_train.shape[1], title=clf.__class__.__name__)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_test = pd.read_csv(\"../input/tabular-playground-series-dec-2021/test.csv\")\nfinal_test = clear_dataset(data_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf = RandomForestClassifier(n_estimators=100,max_depth=11, max_features=21,min_samples_leaf=0.001,criterion=\"entropy\",n_jobs=-1, random_state=42)\nclf.fit(x_train, y_train)\ny_predicted = clf.predict(x_val)\naccuracy, precision, recall, f1 = get_metrics(y_val, y_predicted)\nprint(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_predicted = clf.predict(x_train)\naccuracy, precision, recall, f1 = get_metrics(y_train, y_predicted)\nprint(\"train accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf = RandomForestClassifier(n_estimators=100,max_depth=11, max_features=21,min_samples_leaf=0.001,criterion=\"entropy\",n_jobs=-1, random_state=42)\nclf.fit(x_data, y_data)\ntest_preds = clf.predict(final_test.drop([\"Id\"], axis=1))\noutput = pd.DataFrame({'Id': data_test.Id,\n                       'Cover_Type': test_preds})\noutput.to_csv('rf_submission.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSKR1aGy-dBRtZxwqX1qk0Np170XF88yGvAHQ&usqp=CAU)coloradosun.com","metadata":{}},{"cell_type":"markdown","source":"#Thanks to Enrecan @tooezy. Only the ones who share could be remembered. Even 6 years ago (2015).","metadata":{}}]}