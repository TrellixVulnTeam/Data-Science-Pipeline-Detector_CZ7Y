{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 머신러닝기반데이터분석_이기쁨","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# ****Introduction****\nMercari Interactive EDA + Topic Modelling (https://www.kaggle.com/thykhuely/mercari-interactive-eda-topic-modelling)\n\n일본 쇼핑몰 사이트인 Mercari가 판매자에게 상품 정보에 따른 적정 가격을 제안하고자 한다.\n\n1. 데이터셋 분석\n\n2. 텍스트 처리","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 1. 데이터셋 분석\n\n<연속형 변수>\n* price: 제품 가격\n\n<범주형 변수>\n* train_id: 인덱스\n* name: 제품명\n* item_condition_id: 판매자로부터 제공되는 제품의 상태\n* category_name: 제품의 카테고리\n* brand_name: 제품의 브랜드\n* shipping: 2가지 지표. 0이면 구매자가, 1이면 판매자가 배송비를 부담.\n* item_description: 제품 설명","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nos.getcwd()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.chdir(\"../input/mercari-price-suggestion-challenge\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install py7zr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import py7zr\n\narchive = py7zr.SevenZipFile('./test.tsv.7z', mode='r')\narchive.extractall(path=\"./\")\narchive.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"$ py7zr x test.tsv.7z","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PATH = \"/input/mercari-price-suggestion-challenge\"","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":false,"_kg_hide-input":false,"trusted":true,"collapsed":true},"cell_type":"code","source":"train = pd.read_csv(f'{PATH}/train.tsv', sep='\\t')\ntest = pd.read_csv(f'{PATH}/test.tsv', sep='\\t')\n# tsv: tab으로 구분된 파일. python에서는 csv로 불러오는데 sep='\\t'를 해주면 됨.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nimport string\nimport re\nimport numpy as np\nimport pandas as pd\nimport pickle\n# import lda\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"white\")\n\nfrom nltk.stem.porter import *\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction import stop_words\n\nfrom collections import Counter\nfrom wordcloud import WordCloud\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n%matplotlib inline\n\nimport bokeh.plotting as bp\nfrom bokeh.models import HoverTool, BoxSelectTool\nfrom bokeh.models import ColumnDataSource\nfrom bokeh.plotting import figure, show, output_notebook\n# from bokeh.transform import factor_cmap\n\nimport warnings\nwarnings.filterwarnings('ignore')\nimport logging\nlogging.getLogger(\"lda\").setLevel(logging.WARNING)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# size of training and dataset\nprint(train.shape)\nprint(test.shape)\n# test는 독립변수들을 넣어서 예상 가격을 맞춰야하는 것이기 때문에 price 컬럼이 빠져있음.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# different data types in the dataset: categorical (strings) and numeric\ntrain.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()\n# 종속변수(1): price\n# 독립변수(7): train_id, name, item_condition_id, category_name, brand_name, shipping, item_description","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Price","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.price.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 0과 음수를 피하기 위해서 train['price']+1\n# 왼쪽으로 치우쳐있기 때문에 np.log(train['price'])\n\nplt.subplot(1, 2, 1)\n(train['price']).plot.hist(bins=50, figsize=(20,10), edgecolor='white',range=[0,250])\nplt.xlabel('price+', fontsize=17)\nplt.ylabel('frequency', fontsize=17)\nplt.tick_params(labelsize=15)\nplt.title('Price Distribution - Training Set', fontsize=17)\n\nplt.subplot(1, 2, 2)\nnp.log(train['price']+1).plot.hist(bins=50, figsize=(20,10), edgecolor='white')\nplt.xlabel('log(price+1)', fontsize=17)\nplt.ylabel('frequency', fontsize=17)\nplt.tick_params(labelsize=15)\nplt.title('Log(Price) Distribution - Training Set', fontsize=17)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Shipping","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shipping.value_counts()/len(train)\n# pd.Series.value_counts(): 유일한 값 개수 세기\n# 배송비를 구매자가 부담할때는 0, 판매자가 부담할때는 1로 표기\n# price에서 55%가 구매자가, 나머지 45%는 판매자가 부담하고 있음.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prc_shipBySeller = train.loc[train.shipping==1, 'price']\nprc_shipByBuyer = train.loc[train.shipping==0, 'price']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(20,10))\nax.hist(np.log(prc_shipBySeller+1), color='#8CB4E1', alpha=1.0, bins=50,\n       label='Price when Seller pays Shipping')\nax.hist(np.log(prc_shipByBuyer+1), color='#007D00', alpha=0.7, bins=50,\n       label='Price when Buyer pays Shipping')\nax.set(title='Histogram Comparison', ylabel='% of Dataset in Bin')\nplt.xlabel('log(price+1)', fontsize=17)\nplt.ylabel('frequency', fontsize=17)\nplt.title('Price Distribution by Shipping Type', fontsize=17)\nplt.tick_params(labelsize=15)\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# category_name","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"There are %d unique values in the category column.\" % train['category_name'].nunique())\n# pandas.Series.nunique(): 유일한 값 찾기, 유니크한 value의 개수를 나타냄","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# finished","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Introduction\nThis is an initial Explanatory Data Analysis for the Mercari Price Suggestion Challenge with matplotlib. bokeh and Plot.ly - a visualization tool that creates beautiful interactive plots and dashboards. The competition is hosted by Mercari, the biggest Japanese community-powered shopping app with the main objective to predict an accurate price that Mercari should suggest to its sellers, given the item's information.\n\nUpdate: The abundant amount of food from my family's Thanksgiving dinner has really energized me to continue working on this model. I decided to dive deeper into the NLP analysis and found an amazing tutorial by Ahmed BESBES. The framework below is based on his source code. It provides guidance on pre-processing documents and machine learning techniques (K-means and LDA) to clustering topics. So that this kernel will be divided into 2 parts:\n\nExplanatory Data Analysis\nText Processing\n2.1. Tokenizing and tf-idf algorithm\n2.2. K-means Clustering\n2.3. Latent Dirichlet Allocation (LDA) / Topic Modelling\n\n\n\n\n\n-\nimport nltk\nimport string\nimport re\nimport numpy as np\nimport pandas as pd\nimport pickle\n#import lda\n​\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"white\")\n​\nfrom nltk.stem.porter import *\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction import stop_words\n​\nfrom collections import Counter\nfrom wordcloud import WordCloud\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\n​\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n%matplotlib inline\n​\nimport bokeh.plotting as bp\nfrom bokeh.models import HoverTool, BoxSelectTool\nfrom bokeh.models import ColumnDataSource\nfrom bokeh.plotting import figure, show, output_notebook\n#from bokeh.transform import factor_cmap\n​\nimport warnings\nwarnings.filterwarnings('ignore')\nimport logging\nlogging.getLogger(\"lda\").setLevel(logging.WARNING)\n\n\n\n\n\nExploratory Data Analysis\nOn the first look at the data, besides the unique identifier (item_id), there are 7 variables in this model. This notebook will sequentially go through each of them with a brief statistical summary.\n\nNumerical/Continuous Features\n\nprice: the item's final bidding price. This will be our reponse / independent variable that we need to predict in the test set\nshipping cost\nCategorical Features:\n\nshipping cost: A binary indicator, 1 if shipping fee is paid by seller and 0 if it's paid by buyer\nitem_condition_id: The condition of the items provided by the seller\nname: The item's name\nbrand_name: The item's producer brand name\ncategory_name: The item's single or multiple categories that are separated by \"\\\"\nitem_description: A short description on the item that may include removed words, flagged by [rm]\n\n\n\n\n\nPATH = \"../input/\"\n\n\n\n\n\ntrain = pd.read_csv(f'{PATH}train.tsv', sep='\\t')\ntest = pd.read_csv(f'{PATH}test.tsv', sep='\\t')\n\n\n\n\n\n# size of training and dataset\nprint(train.shape)\nprint(test.shape)\n\n\n\n\n\n# different data types in the dataset: categorical (strings) and numeric\ntrain.dtypes\n\n\n\n\n\ntrain.head()\n\n\n\n\n\nTarget Variable: Price\n\n\n\n\n\nThe next standard check is with our response or target variables, which in this case is the price we are suggesting to the Mercari's marketplace sellers. The median price of all the items in the training is about $267 but given the existence of some extreme values of over $100 and the maximum at $2,009, the distribution of the variables is heavily skewed to the left. So let's make log-transformation on the price (we added +1 to the value before the transformation to avoid zero and negative values).\n\n\n\n\n\n\ntrain.price.describe()\n\n\n\n\n\nplt.subplot(1, 2, 1)\n(train['price']).plot.hist(bins=50, figsize=(20,10), edgecolor='white',range=[0,250])\nplt.xlabel('price+', fontsize=17)\nplt.ylabel('frequency', fontsize=17)\nplt.tick_params(labelsize=15)\nplt.title('Price Distribution - Training Set', fontsize=17)\n​\nplt.subplot(1, 2, 2)\nnp.log(train['price']+1).plot.hist(bins=50, figsize=(20,10), edgecolor='white')\nplt.xlabel('log(price+1)', fontsize=17)\nplt.ylabel('frequency', fontsize=17)\nplt.tick_params(labelsize=15)\nplt.title('Log(Price) Distribution - Training Set', fontsize=17)\nplt.show()\n\n\n\n\n\nShipping\nThe shipping cost burden is decently splitted between sellers and buyers with more than half of the items' shipping fees are paid by the sellers (55%). In addition, the average price paid by users who have to pay for shipping fees is lower than those that don't require additional shipping cost. This matches with our perception that the sellers need a lower price to compensate for the additional shipping.\n\n\n\n\n\n\ntrain.shipping.value_counts()/len(train)\n\n\n\n\n\nprc_shipBySeller = train.loc[train.shipping==1, 'price']\nprc_shipByBuyer = train.loc[train.shipping==0, 'price']\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(20,10))\nax.hist(np.log(prc_shipBySeller+1), color='#8CB4E1', alpha=1.0, bins=50,\n       label='Price when Seller pays Shipping')\nax.hist(np.log(prc_shipByBuyer+1), color='#007D00', alpha=0.7, bins=50,\n       label='Price when Buyer pays Shipping')\nax.set(title='Histogram Comparison', ylabel='% of Dataset in Bin')\nplt.xlabel('log(price+1)', fontsize=17)\nplt.ylabel('frequency', fontsize=17)\nplt.title('Price Distribution by Shipping Type', fontsize=17)\nplt.tick_params(labelsize=15)\nplt.show()\n\n\n\n\n\nItem Category\nThere are about 1,287 unique categories but among each of them, we will always see a main/general category firstly, followed by two more particular subcategories (e.g. Beauty/Makeup/Face or Lips). In adidition, there are about 6,327 items that do not have a category labels. Let's split the categories into three different columns. We will see later that this information is actually quite important from the seller's point of view and how we handle the missing information in the brand_name column will impact the model's prediction.\n\n\n\n\n\n\nprint(\"There are %d unique values in the category column.\" % train['category_name'].nunique())\n\n\n\n\n\n# TOP 5 RAW CATEGORIES\ntrain['category_name'].value_counts()[:5]\n# TOP 5 RAW CATEGORIES\ntrain['category_name'].value_counts()[:5]\n\n\n\n\n\n# missing categories\nprint(\"There are %d items that do not have a label.\" % train['category_name'].isnull().sum())\n\n\n\n\n\n# reference: BuryBuryZymon at https://www.kaggle.com/maheshdadhich/i-will-sell-everything-for-free-0-55\ndef split_cat(text):\n    try: return text.split(\"/\")\n    except: return (\"No Label\", \"No Label\", \"No Label\")\n\n\n\n\n\ntrain['general_cat'], train['subcat_1'], train['subcat_2'] = \\\nzip(*train['category_name'].apply(lambda x: split_cat(x)))\ntrain.head()\n\n\n\n\n\n# repeat the same step for the test set\ntest['general_cat'], test['subcat_1'], test['subcat_2'] = \\\nzip(*test['category_name'].apply(lambda x: split_cat(x)))\n\n\n\n\n\nprint(\"There are %d unique first sub-categories.\" % train['subcat_1'].nunique())\n\n\n\n\n\nprint(\"There are %d unique second sub-categories.\" % train['subcat_2'].nunique())\n\n\n\n\n\nOverall, we have 7 main categories (114 in the first sub-categories and 871 second sub-categories): women's and beauty items as the two most popular categories (more than 50% of the observations), followed by kids and electronics.\n\n\n\n\n\n\nx = train['general_cat'].value_counts().index.values.astype('str')\ny = train['general_cat'].value_counts().values\npct = [(\"%.2f\"%(v*100))+\"%\"for v in (y/len(train))]\n\n\n\n\n\ntrace1 = go.Bar(x=x, y=y, text=pct)\nlayout = dict(title= 'Number of Items by Main Category',\n              yaxis = dict(title='Count'),\n              xaxis = dict(title='Category'))\nfig=dict(data=[trace1], layout=layout)\npy.iplot(fig)\n\n\n\n\n\nx = train['subcat_1'].value_counts().index.values.astype('str')[:15]\ny = train['subcat_1'].value_counts().values[:15]\npct = [(\"%.2f\"%(v*100))+\"%\"for v in (y/len(train))][:15]\n\n\n\n\n\ntrace1 = go.Bar(x=x, y=y, text=pct,\n                marker=dict(\n                color = y,colorscale='Portland',showscale=True,\n                reversescale = False\n                ))\nlayout = dict(title= 'Number of Items by Sub Category (Top 15)',\n              yaxis = dict(title='Count'),\n              xaxis = dict(title='SubCategory'))\nfig=dict(data=[trace1], layout=layout)\npy.iplot(fig)\n\n\n\n\n\nFrom the pricing (log of price) point of view, all the categories are pretty well distributed, with no category with an extraordinary pricing point\n\n\n\n\n\n\ngeneral_cats = train['general_cat'].unique()\nx = [train.loc[train['general_cat']==cat, 'price'] for cat in general_cats]\n\n\n\n\n\ndata = [go.Box(x=np.log(x[i]+1), name=general_cats[i]) for i in range(len(general_cats))]\n\n\n\n\n\nlayout = dict(title=\"Price Distribution by General Category\",\n              yaxis = dict(title='Frequency'),\n              xaxis = dict(title='Category'))\nfig = dict(data=data, layout=layout)\npy.iplot(fig)\n\n\n\n\n\nBrand Name\n\n\n\n\n\nprint(\"There are %d unique brand names in the training dataset.\" % train['brand_name'].nunique())\n\n\n\n\n\nx = train['brand_name'].value_counts().index.values.astype('str')[:10]\ny = train['brand_name'].value_counts().values[:10]\n\n\n\n\n\n# trace1 = go.Bar(x=x, y=y, \n#                 marker=dict(\n#                 color = y,colorscale='Portland',showscale=True,\n#                 reversescale = False\n#                 ))\n# layout = dict(title= 'Top 10 Brand by Number of Items',\n#               yaxis = dict(title='Brand Name'),\n#               xaxis = dict(title='Count'))\n# fig=dict(data=[trace1], layout=layout)\n# py.iplot(fig)\n\n\n\n\n\nItem Description\n\n\n\n\n\nIt will be more challenging to parse through this particular item since it's unstructured data. Does it mean a more detailed and lengthy description will result in a higher bidding price? We will strip out all punctuations, remove some english stop words (i.e. redundant words such as \"a\", \"the\", etc.) and any other words with a length less than 3:\n\n\n\n\n\n\ndef wordCount(text):\n    # convert to lower case and strip regex\n    try:\n         # convert to lower case and strip regex\n        text = text.lower()\n        regex = re.compile('[' +re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]')\n        txt = regex.sub(\" \", text)\n        # tokenize\n        # words = nltk.word_tokenize(clean_txt)\n        # remove words in stop words\n        words = [w for w in txt.split(\" \") \\\n                 if not w in stop_words.ENGLISH_STOP_WORDS and len(w)>3]\n        return len(words)\n    except: \n        return 0\n\n\n\n\n\n# add a column of word counts to both the training and test set\ntrain['desc_len'] = train['item_description'].apply(lambda x: wordCount(x))\ntest['desc_len'] = test['item_description'].apply(lambda x: wordCount(x))\n\n\n\n\n\ntrain.head()\n\n\n\n\n\ndf = train.groupby('desc_len')['price'].mean().reset_index()\n\n\n\n\n\ntrace1 = go.Scatter(\n    x = df['desc_len'],\n    y = np.log(df['price']+1),\n    mode = 'lines+markers',\n    name = 'lines+markers'\n)\nlayout = dict(title= 'Average Log(Price) by Description Length',\n              yaxis = dict(title='Average Log(Price)'),\n              xaxis = dict(title='Description Length'))\nfig=dict(data=[trace1], layout=layout)\npy.iplot(fig)\n\n\n\n\n\nWe also need to check if there are any missing values in the item description (4 observations don't have a description) andl remove those observations from our training set.\n\n\n\n\n\n\ntrain.item_description.isnull().sum()\n\n\n\n\n\n# remove missing values in item description\ntrain = train[pd.notnull(train['item_description'])]\n\n\n\n\n\n# create a dictionary of words for each category\ncat_desc = dict()\nfor cat in general_cats: \n    text = \" \".join(train.loc[train['general_cat']==cat, 'item_description'].values)\n    cat_desc[cat] = tokenize(text)\n​\n# flat list of all words combined\nflat_lst = [item for sublist in list(cat_desc.values()) for item in sublist]\nallWordsCount = Counter(flat_lst)\nall_top10 = allWordsCount.most_common(20)\nx = [w[0] for w in all_top10]\ny = [w[1] for w in all_top10]\n\n\n\n\n\ntrace1 = go.Bar(x=x, y=y, text=pct)\nlayout = dict(title= 'Word Frequency',\n              yaxis = dict(title='Count'),\n              xaxis = dict(title='Word'))\nfig=dict(data=[trace1], layout=layout)\npy.iplot(fig)\n\n\n\n\n\nIf we look at the most common words by category, we could also see that, size, free and shipping is very commonly used by the sellers, probably with the intention to attract customers, which is contradictory to what we have shown previously that there is little correlation between the two variables price and shipping (or shipping fees do not account for a differentiation in prices). Brand names also played quite an important role - it's one of the most popular in all four categories.\n\n\n\n\n\n\nText Processing - Item Description\nThe following section is based on the tutorial at https://ahmedbesbes.com/how-to-mine-newsfeed-data-and-extract-interactive-insights-in-python.html\n\n\n\n\n\n\nPre-processing: tokenization\nMost of the time, the first steps of an NLP project is to \"tokenize\" your documents, which main purpose is to normalize our texts. The three fundamental stages will usually include:\n\nbreak the descriptions into sentences and then break the sentences into tokens\nremove punctuation and stop words\nlowercase the tokens\nherein, I will also only consider words that have length equal to or greater than 3 characters\n\n\n\n\n\nstop = set(stopwords.words('english'))\ndef tokenize(text):\n    \"\"\"\n    sent_tokenize(): segment text into sentences\n    word_tokenize(): break sentences into words\n    \"\"\"\n    try: \n        regex = re.compile('[' +re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]')\n        text = regex.sub(\" \", text) # remove punctuation\n        \n        tokens_ = [word_tokenize(s) for s in sent_tokenize(text)]\n        tokens = []\n        for token_by_sent in tokens_:\n            tokens += token_by_sent\n        tokens = list(filter(lambda t: t.lower() not in stop, tokens))\n        filtered_tokens = [w for w in tokens if re.search('[a-zA-Z]', w)]\n        filtered_tokens = [w.lower() for w in filtered_tokens if len(w)>=3]\n        \n        return filtered_tokens\n            \n    except TypeError as e: print(text,e)\n\n\n\n\n\n# apply the tokenizer into the item descriptipn column\ntrain['tokens'] = train['item_description'].map(tokenize)\ntest['tokens'] = test['item_description'].map(tokenize)\n\n\n\n\n\ntrain.reset_index(drop=True, inplace=True)\ntest.reset_index(drop=True, inplace=True)\n\n\n\n\n\nLet's look at the examples of if the tokenizer did a good job in cleaning up our descriptions\n\n\n\n\n\n\nfor description, tokens in zip(train['item_description'].head(),\n                              train['tokens'].head()):\n    print('description:', description)\n    print('tokens:', tokens)\n    print()\n\n\n\n\n\nWe could aso use the package WordCloud to easily visualize which words has the highest frequencies within each category:\n\n\n\n\n\n\n# build dictionary with key=category and values as all the descriptions related.\ncat_desc = dict()\nfor cat in general_cats: \n    text = \" \".join(train.loc[train['general_cat']==cat, 'item_description'].values)\n    cat_desc[cat] = tokenize(text)\n​\n​\n# find the most common words for the top 4 categories\nwomen100 = Counter(cat_desc['Women']).most_common(100)\nbeauty100 = Counter(cat_desc['Beauty']).most_common(100)\nkids100 = Counter(cat_desc['Kids']).most_common(100)\nelectronics100 = Counter(cat_desc['Electronics']).most_common(100)\n\n\n\n\n\ndef generate_wordcloud(tup):\n    wordcloud = WordCloud(background_color='white',\n                          max_words=50, max_font_size=40,\n                          random_state=42\n                         ).generate(str(tup))\n    return wordcloud\n\n\n\n\n\nfig,axes = plt.subplots(2, 2, figsize=(30, 15))\n​\nax = axes[0, 0]\nax.imshow(generate_wordcloud(women100), interpolation=\"bilinear\")\nax.axis('off')\nax.set_title(\"Women Top 100\", fontsize=30)\n​\nax = axes[0, 1]\nax.imshow(generate_wordcloud(beauty100))\nax.axis('off')\nax.set_title(\"Beauty Top 100\", fontsize=30)\n​\nax = axes[1, 0]\nax.imshow(generate_wordcloud(kids100))\nax.axis('off')\nax.set_title(\"Kids Top 100\", fontsize=30)\n​\nax = axes[1, 1]\nax.imshow(generate_wordcloud(electronics100))\nax.axis('off')\nax.set_title(\"Electronic Top 100\", fontsize=30)\n\n\n\n\n\nPre-processing: tf-idf\n\n\n\n\n\ntf-idf is the acronym for Term Frequency–inverse Document Frequency. It quantifies the importance of a particular word in relative to the vocabulary of a collection of documents or corpus. The metric depends on two factors:\n\nTerm Frequency: the occurences of a word in a given document (i.e. bag of words)\nInverse Document Frequency: the reciprocal number of times a word occurs in a corpus of documents\nThink about of it this way: If the word is used extensively in all documents, its existence within a specific document will not be able to provide us much specific information about the document itself. So the second term could be seen as a penalty term that penalizes common words such as \"a\", \"the\", \"and\", etc. tf-idf can therefore, be seen as a weighting scheme for words relevancy in a specific document.\n\n\n\n\n\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(min_df=10,\n                             max_features=180000,\n                             tokenizer=tokenize,\n                             ngram_range=(1, 2))\n\n\n\n\n\nall_desc = np.append(train['item_description'].values, test['item_description'].values)\nvz = vectorizer.fit_transform(list(all_desc))\n\n\n\n\n\nvz is a tfidf matrix where:\n\nthe number of rows is the total number of descriptions\nthe number of columns is the total number of unique tokens across the descriptions\n\n\n\n\n\n#  create a dictionary mapping the tokens to their tfidf values\ntfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\ntfidf = pd.DataFrame(columns=['tfidf']).from_dict(\n                    dict(tfidf), orient='index')\ntfidf.columns = ['tfidf']\n\n\n\n\n\nBelow is the 10 tokens with the lowest tfidf score, which is unsurprisingly, very generic words that we could not use to distinguish one description from another.\n\n\n\n\n\n\ntfidf.sort_values(by=['tfidf'], ascending=True).head(10)\n\n\n\n\n\nBelow is the 10 tokens with the highest tfidf score, which includes words that are a lot specific that by looking at them, we could guess the categories that they belong to:\n\n\n\n\n\n\ntfidf.sort_values(by=['tfidf'], ascending=False).head(10)\n\n\n\n\n\nGiven the high dimension of our tfidf matrix, we need to reduce their dimension using the Singular Value Decomposition (SVD) technique. And to visualize our vocabulary, we could next use t-SNE to reduce the dimension from 50 to 2. t-SNE is more suitable for dimensionality reduction to 2 or 3.\n\nt-Distributed Stochastic Neighbor Embedding (t-SNE)\nt-SNE is a technique for dimensionality reduction that is particularly well suited for the visualization of high-dimensional datasets. The goal is to take a set of points in a high-dimensional space and find a representation of those points in a lower-dimensional space, typically the 2D plane. It is based on probability distributions with random walk on neighborhood graphs to find the structure within the data. But since t-SNE complexity is significantly high, usually we'd use other high-dimension reduction techniques before applying t-SNE.\n\nFirst, let's take a sample from the both training and testing item's description since t-SNE can take a very long time to execute. We can then reduce the dimension of each vector from to n_components (50) using SVD.\n\n\n\n\n\n\ntrn = train.copy()\ntst = test.copy()\ntrn['is_train'] = 1\ntst['is_train'] = 0\n​\nsample_sz = 15000\n​\ncombined_df = pd.concat([trn, tst])\ncombined_sample = combined_df.sample(n=sample_sz)\nvz_sample = vectorizer.fit_transform(list(combined_sample['item_description']))\n\n\n\n\n\nfrom sklearn.decomposition import TruncatedSVD\n​\nn_comp=30\nsvd = TruncatedSVD(n_components=n_comp, random_state=42)\nsvd_tfidf = svd.fit_transform(vz_sample)\n\n\n\n\n\nNow we can reduce the dimension from 50 to 2 using t-SNE!\n\n\n\n\n\n\nfrom sklearn.manifold import TSNE\ntsne_model = TSNE(n_components=2, verbose=1, random_state=42, n_iter=500)\n\n\n\n\n\ntsne_tfidf = tsne_model.fit_transform(svd_tfidf)\n\n\n\n\n\nIt's now possible to visualize our data points. Note that the deviation as well as the size of the clusters imply little information in t-SNE.\n\n\n\n\n\n\noutput_notebook()\nplot_tfidf = bp.figure(plot_width=700, plot_height=600,\n                       title=\"tf-idf clustering of the item description\",\n    tools=\"pan,wheel_zoom,box_zoom,reset,hover,previewsave\",\n    x_axis_type=None, y_axis_type=None, min_border=1)\n\n\n\n\n\ncombined_sample.reset_index(inplace=True, drop=True)\n\n\n\n\n\ntfidf_df = pd.DataFrame(tsne_tfidf, columns=['x', 'y'])\ntfidf_df['description'] = combined_sample['item_description']\ntfidf_df['tokens'] = combined_sample['tokens']\ntfidf_df['category'] = combined_sample['general_cat']\n\n\n\n\n\nplot_tfidf.scatter(x='x', y='y', source=tfidf_df, alpha=0.7)\nhover = plot_tfidf.select(dict(type=HoverTool))\nhover.tooltips={\"description\": \"@description\", \"tokens\": \"@tokens\", \"category\":\"@category\"}\nshow(plot_tfidf)\n\n\n\n\n\nK-Means Clustering\nK-means clustering obejctive is to minimize the average squared Euclidean distance of the document / description from their cluster centroids.\n\n\n\n\n\n\nfrom sklearn.cluster import MiniBatchKMeans\n​\nnum_clusters = 30 # need to be selected wisely\nkmeans_model = MiniBatchKMeans(n_clusters=num_clusters,\n                               init='k-means++',\n                               n_init=1,\n                               init_size=1000, batch_size=1000, verbose=0, max_iter=1000)\n\n\n\n\n\nkmeans = kmeans_model.fit(vz)\nkmeans_clusters = kmeans.predict(vz)\nkmeans_distances = kmeans.transform(vz)\n\n\n\n\n\nsorted_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\nterms = vectorizer.get_feature_names()\n​\nfor i in range(num_clusters):\n    print(\"Cluster %d:\" % i)\n    aux = ''\n    for j in sorted_centroids[i, :10]:\n        aux += terms[j] + ' | '\n    print(aux)\n    print() \n\n\n\n\n\nIn order to plot these clusters, first we will need to reduce the dimension of the distances to 2 using tsne:\n\n\n\n\n\n\n# repeat the same steps for the sample\nkmeans = kmeans_model.fit(vz_sample)\nkmeans_clusters = kmeans.predict(vz_sample)\nkmeans_distances = kmeans.transform(vz_sample)\n# reduce dimension to 2 using tsne\ntsne_kmeans = tsne_model.fit_transform(kmeans_distances)\n\n\n\n\n\ncolormap = np.array([\"#6d8dca\", \"#69de53\", \"#723bca\", \"#c3e14c\", \"#c84dc9\", \"#68af4e\", \"#6e6cd5\",\n\"#e3be38\", \"#4e2d7c\", \"#5fdfa8\", \"#d34690\", \"#3f6d31\", \"#d44427\", \"#7fcdd8\", \"#cb4053\", \"#5e9981\",\n\"#803a62\", \"#9b9e39\", \"#c88cca\", \"#e1c37b\", \"#34223b\", \"#bdd8a3\", \"#6e3326\", \"#cfbdce\", \"#d07d3c\",\n\"#52697d\", \"#194196\", \"#d27c88\", \"#36422b\", \"#b68f79\"])\n\n\n\n\n\n#combined_sample.reset_index(drop=True, inplace=True)\nkmeans_df = pd.DataFrame(tsne_kmeans, columns=['x', 'y'])\nkmeans_df['cluster'] = kmeans_clusters\nkmeans_df['description'] = combined_sample['item_description']\nkmeans_df['category'] = combined_sample['general_cat']\n#kmeans_df['cluster']=kmeans_df.cluster.astype(str).astype('category')\n\n\n\n\n\nplot_kmeans = bp.figure(plot_width=700, plot_height=600,\n                        title=\"KMeans clustering of the description\",\n    tools=\"pan,wheel_zoom,box_zoom,reset,hover,previewsave\",\n    x_axis_type=None, y_axis_type=None, min_border=1)\n\n\n\n\n\nsource = ColumnDataSource(data=dict(x=kmeans_df['x'], y=kmeans_df['y'],\n                                    color=colormap[kmeans_clusters],\n                                    description=kmeans_df['description'],\n                                    category=kmeans_df['category'],\n                                    cluster=kmeans_df['cluster']))\n​\nplot_kmeans.scatter(x='x', y='y', color='color', source=source)\nhover = plot_kmeans.select(dict(type=HoverTool))\nhover.tooltips={\"description\": \"@description\", \"category\": \"@category\", \"cluster\":\"@cluster\" }\nshow(plot_kmeans)\n\n\n\n\n\nLatent Dirichlet Allocation\nLatent Dirichlet Allocation (LDA) is an algorithms used to discover the topics that are present in a corpus.\n\nLDA starts from a fixed number of topics. Each topic is represented as a distribution over words, and each document is then represented as a distribution over topics. Although the tokens themselves are meaningless, the probability distributions over words provided by the topics provide a sense of the different ideas contained in the documents.\n\nReference: https://medium.com/intuitionmachine/the-two-paths-from-natural-language-processing-to-artificial-intelligence-d5384ddbfc18\n\nIts input is a bag of words, i.e. each document represented as a row, with each columns containing the count of words in the corpus. We are going to use a powerful tool called pyLDAvis that gives us an interactive visualization for LDA.\n\n\n\n\n\n\ncvectorizer = CountVectorizer(min_df=4,\n                              max_features=180000,\n                              tokenizer=tokenize,\n                              ngram_range=(1,2))\n\n\n\n\n\ncvz = cvectorizer.fit_transform(combined_sample['item_description'])\n\n\n\n\n\nlda_model = LatentDirichletAllocation(n_components=20,\n                                      learning_method='online',\n                                      max_iter=20,\n                                      random_state=42)\n\n\n\n\n\nX_topics = lda_model.fit_transform(cvz)\n\n\n\n\n\nn_top_words = 10\ntopic_summaries = []\n​\ntopic_word = lda_model.components_  # get the topic words\nvocab = cvectorizer.get_feature_names()\n​\nfor i, topic_dist in enumerate(topic_word):\n    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n    topic_summaries.append(' '.join(topic_words))\n    print('Topic {}: {}'.format(i, ' | '.join(topic_words)))\n\n\n\n\n\n# reduce dimension to 2 using tsne\ntsne_lda = tsne_model.fit_transform(X_topics)\n\n\n\n\n\nunnormalized = np.matrix(X_topics)\ndoc_topic = unnormalized/unnormalized.sum(axis=1)\n​\nlda_keys = []\nfor i, tweet in enumerate(combined_sample['item_description']):\n    lda_keys += [doc_topic[i].argmax()]\n​\nlda_df = pd.DataFrame(tsne_lda, columns=['x','y'])\nlda_df['description'] = combined_sample['item_description']\nlda_df['category'] = combined_sample['general_cat']\nlda_df['topic'] = lda_keys\nlda_df['topic'] = lda_df['topic'].map(int)\n\n\n\n\n\nplot_lda = bp.figure(plot_width=700,\n                     plot_height=600,\n                     title=\"LDA topic visualization\",\n    tools=\"pan,wheel_zoom,box_zoom,reset,hover,previewsave\",\n    x_axis_type=None, y_axis_type=None, min_border=1)\n\n\n\n\n\nsource = ColumnDataSource(data=dict(x=lda_df['x'], y=lda_df['y'],\n                                    color=colormap[lda_keys],\n                                    description=lda_df['description'],\n                                    topic=lda_df['topic'],\n                                    category=lda_df['category']))\n​\nplot_lda.scatter(source=source, x='x', y='y', color='color')\nhover = plot_kmeans.select(dict(type=HoverTool))\nhover = plot_lda.select(dict(type=HoverTool))\nhover.tooltips={\"description\":\"@description\",\n                \"topic\":\"@topic\", \"category\":\"@category\"}\nshow(plot_lda)\n\n\n\n\n\ndef prepareLDAData():\n    data = {\n        'vocab': vocab,\n        'doc_topic_dists': doc_topic,\n        'doc_lengths': list(lda_df['len_docs']),\n        'term_frequency':cvectorizer.vocabulary_,\n        'topic_term_dists': lda_model.components_\n    } \n    return data\n\n\n\n\n\nNote: It's a shame that by putting the HTML of the visualization using pyLDAvis, it will distort the layout of the kernel, I won't upload in here. But if you follow the below code, there should be an HTML file generated with very interesting interactive bubble chart that visualizes the space of your topic clusters and the term components within each topic.\n\n\n\n\n\n\n\n\nimport pyLDAvis\n​\nlda_df['len_docs'] = combined_sample['tokens'].map(len)\nldadata = prepareLDAData()\npyLDAvis.enable_notebook()\nprepared_data = pyLDAvis.prepare(**ldadata)\n\n\n\n\n\npyLDAvis\n\n\n\n\n\n\nimport IPython.display\nfrom IPython.core.display import display, HTML, Javascript\n​\n#h = IPython.display.display(HTML(html_string))\n#IPython.display.display_HTML(h)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}