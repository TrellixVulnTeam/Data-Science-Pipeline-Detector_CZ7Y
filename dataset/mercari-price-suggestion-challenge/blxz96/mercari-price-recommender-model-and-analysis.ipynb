{"cells":[{"metadata":{"_uuid":"02cb4ac4da49c9c08fc892950123d276a28be35f"},"cell_type":"markdown","source":"# Simple Neural Network Price Predictor\n***\nTopics covered in this kernel:\n1. Neural Networks\n2. K-Fold Validation\n3. Text Analytics\n\nOther stuff included:\n1. Emebedding\n2. Transfer Learning\n3. Ensembles"},{"metadata":{"_uuid":"8e56603a21e212b1f2e958d265612e78ea4102ed"},"cell_type":"markdown","source":"## Packages\n\nLoading required packages."},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":false},"cell_type":"code","source":"from __future__ import division\nimport pyximport\npyximport.install()\nimport os\nimport random\nimport numpy as np\nimport tensorflow as tf\nos.environ['PYTHONHASHSEED'] = '10000'\nnp.random.seed(10001)\nrandom.seed(10002)\nsession_conf = tf.ConfigProto(intra_op_parallelism_threads=5, inter_op_parallelism_threads=1)\nfrom keras import backend\ntf.set_random_seed(10003)\nbackend.set_session(tf.Session(graph=tf.get_default_graph(), config=session_conf))\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Input, Dropout, Dense, concatenate, GRU, Embedding, Flatten, Activation, BatchNormalization, PReLU\nfrom keras.initializers import he_uniform\nfrom keras.layers import Conv1D\nfrom keras.layers import GlobalMaxPooling1D, GlobalAveragePooling1D\nfrom keras.optimizers import Adam, SGD\nfrom keras.models import Model\n\nimport pandas as pd \nfrom sklearn.preprocessing import LabelEncoder\nfrom collections import defaultdict\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom scipy.sparse import csr_matrix, hstack\nfrom itertools import combinations\nfrom sklearn.linear_model import LinearRegression\nimport re\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import Ridge\nfrom multiprocessing import Pool\nimport gc\nimport nltk\nfrom nltk import ngrams\nfrom nltk.stem import WordNetLemmatizer\nwordnet_lemmatizer = WordNetLemmatizer()\nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words('english'))\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ba6f14e588ebf11790287d4f11b8276ef05141f8"},"cell_type":"markdown","source":"## Global Variables"},{"metadata":{"_kg_hide-input":false,"_uuid":"ab0fa74be8858a2b3197cf2761c85cc6964f5600","trusted":false},"cell_type":"code","source":"# Total length of train is 1482535\n# 0.8 - 0.2 split: 1186028\n# 0.7 - 0.3 split: 1037774\n# No of data to use. -1 to train on all\nsplit = 1037774\n\npath = '../input/'\ncores = 4\nmax_text_length = 60\nmin_df_one = 5\nmin_df_bi = 5\n\nkeep = ['item_condition_id', 'shipping', 'nb_words_item_description',\n        'mean_price_category1', 'mean_price_category2', 'mean_price_category3',\n        'mean_price_category_name', 'mean_price_brand_name']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"28c5b2320f5ef863df3d0b4e4d9175c59bd61ef0"},"cell_type":"markdown","source":"# Preprocessing & Feature Engineering\n***\nSome data would have to be cleaned while other forms of data added to improve accuracy of model"},{"metadata":{"_uuid":"8ea0d180c4aa3a4137c0fa51de90f7f47dfd2b68"},"cell_type":"markdown","source":"## Text Cleaning\n\nSub wrongly encoded alphabets with the right one. Then remove all none alphanumeric characters and unneccesary whitespace. Without this. words like '©ap' and 'Cap' will be treated as different words."},{"metadata":{"_uuid":"b5ab11aa730ac99f3d592a0c4c4f95956ed0d500","trusted":false},"cell_type":"code","source":"def clean_str(text):\n    try:\n        text = ' '.join( [w for w in text.split()[:max_text_length]] )        \n        text = text.lower()\n        text = re.sub(u\"é\", u\"e\", text)\n        text = re.sub(u\"ē\", u\"e\", text)\n        text = re.sub(u\"è\", u\"e\", text)\n        text = re.sub(u\"ê\", u\"e\", text)\n        text = re.sub(u\"à\", u\"a\", text)\n        text = re.sub(u\"â\", u\"a\", text)\n        text = re.sub(u\"ô\", u\"o\", text)\n        text = re.sub(u\"ō\", u\"o\", text)\n        text = re.sub(u\"ü\", u\"u\", text)\n        text = re.sub(u\"ï\", u\"i\", text)\n        text = re.sub(u\"ç\", u\"c\", text)\n        text = re.sub(u\"\\u2019\", u\"'\", text)\n        text = re.sub(u\"\\xed\", u\"i\", text)\n        text = re.sub(u\"w\\/\", u\" with \", text)\n        \n        text = re.sub(u\"[^a-z0-9]\", \" \", text)\n        text = u\" \".join(re.split('(\\d+)',text) )\n        text = re.sub( u\"\\s+\", u\" \", text ).strip()\n        text = ''.join(text)\n    except:\n        text = np.NaN\n    return text\n\ndef clean_str_df(df):\n    return df.apply( lambda s : clean_str(s))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8311bd8455c5931860d2e67eb62a37d6f2dc095b"},"cell_type":"markdown","source":"## Bigrams\n\nUsing an naive approach in text mining like extracting every single word without accounting for phrases will cause the analysis to be less reliable. An example would be the word \"New York', seperately 'New' and 'York' mean very different things from the phrase 'New York'. Therefore, we account for this issue by creating bigrams where common occuring phrases would now be accounted for individually."},{"metadata":{"_uuid":"615dff44feff61f98c27c910874831083e60f8d8","trusted":false},"cell_type":"code","source":"def create_bigrams(text):\n    try:\n        text = np.unique([wordnet_lemmatizer.lemmatize(w) for w in text.split() if w not in stop_words])\n        lst_bi = []\n        for combo in combinations(text, 2):\n            cb1=combo[0]+combo[1]\n            cb2=combo[1]+combo[0]\n            in_dict=False\n            if cb1 in word_count_dict_one:\n                new_word = cb1\n                in_dict=True\n            if cb2 in word_count_dict_one:\n                new_word = cb2\n                in_dict=True\n            if not in_dict:\n                new_word = combo[0]+'___'+combo[1]\n            if len(cb1)>=0:\n                lst_bi.append(new_word)\n        return ' '.join(lst_bi)\n    except:\n        return ' '\n    \ndef create_bigrams_df(df):\n    return df.apply(create_bigrams)\n\ndef remove_low_freq(text, dc):\n    return ' '.join([w for w in text.split() if w in dc])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e9b09d6753e56eac22f9b12d2e4d03625d2f8228"},"cell_type":"markdown","source":"## Count length of description\n\nLength of description have an impact on the final price of the product sold. Thus this will be added in as a new feature."},{"metadata":{"_uuid":"cee76d4b002ef080762e4505100c31044acb2dff","trusted":false},"cell_type":"code","source":"def create_count_features(df_data):\n    def lg(text):\n        text = [x for x in text.split() if x!='']\n        return len(text)\n    df_data['nb_words_item_description'] = df_data['item_description'].apply(lg).astype(np.uint16)\n\ndef word_count(text, dc):\n    text = set(text.split(' ')) \n    for w in text:\n        dc[w]+=1\n        \ndef tokenize(text):\n    return [w for w in text.split()]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a465c9f9b656dbf2e4a69a9b79a4ef306c846e3d"},"cell_type":"markdown","source":"## Prepare Data\n\nSteps taken:\n1. Clean NA values\n2. Category provided is in the form of cat1/cat2/cat3, we will first have to split them into different columns.\n3. Count number of words in item description\n4. Clean Text"},{"metadata":{"_uuid":"9eebbeff5dfbeeb8e6ba37e8f222ff024067b869","trusted":false},"cell_type":"code","source":"def prepare_data(df_data, train=True):\n    def fill_brand_name(x):\n        try:\n            k=[]\n            for n in [4,3,2,1]:\n                temp =  [' '.join(xi) for xi in ngrams(x.split(' '), n) if ' '.join(xi) in brand_names] \n                if len(temp)>0:\n                    k = k+temp\n            if len(k) > 0:\n                return k[0]\n            else:\n                return np.NaN\n        except:\n            return np.NaN\n        \n    def fill_cat(x, i, new=False):\n        try:\n            if new:\n                return x.split('/')[i-1].strip()\n            else:\n                return ' '.join( x.split('/')).strip()\n        except:\n            return ''\n        \n    df_data['name'].fillna('', inplace=True)\n    df_data['item_description'].fillna('', inplace=True)\n    df_data['item_description'] = df_data['item_description'].apply(lambda x : x.replace('No description yet',''))\n    \n    #create 3 categories and remove / from category name and replace nan\n    df_data['category_name'].fillna('//', inplace=True)\n    df_data['category1'] = df_data.category_name.apply(lambda x : x.split('/')[0].strip())\n    df_data['category2'] = df_data.category_name.apply(lambda x : x.split('/')[1].strip())\n    df_data['category3'] = df_data.category_name.apply(lambda x : x.split('/')[2].strip())\n    df_data['category_name'] = df_data['category_name'].apply( lambda x : ' '.join( x.split('/') ).strip())\n\n    create_count_features(df_data)     \n    df_data['nb_words_item_description'] /= max_text_length\n\n    df_data['brand_name'] = parallelize_dataframe(df_data['brand_name'], clean_str_df)  \n    df_data['name'] = parallelize_dataframe(df_data['name'], clean_str_df)  \n    df_data['item_description'] = parallelize_dataframe(df_data['item_description'], clean_str_df)                                                                            \n    \n    df_data.loc[df_data['brand_name'].isnull(), 'brand_name'] = df_data.loc[df_data['brand_name'].isnull(), 'name'].apply(fill_brand_name)\n    df_data['brand_name'].fillna('', inplace=True)\n    \n    if train:        \n        for feat in ['brand_name', 'category_name', 'category1', 'category2', 'category3']:\n            temp = df_data[feat].unique()\n            lb = LabelEncoder()\n            df_data[feat] = lb.fit_transform(df_data[feat]).astype(np.uint16)\n            labels_dict[feat] = (lb, temp)\n    else:   \n        for feat in ['brand_name', 'category1', 'category2', 'category3', 'category_name']:\n            idx = labels_dict[feat][1]\n            df_data.loc[-df_data[feat].isin(idx), feat] = ''\n            df_data[feat] = labels_dict[feat][0].transform(df_data[feat]).astype(np.uint16)\n\n    df_data['name_old'] = df_data['name'].copy()    \n    \n    df_data['brand_cat']  = 'cat1_'+df_data['category1'].astype(str)+' '+\\\n    'cat2_'+df_data['category2'].astype(str)+' '+\\\n    'cat3_'+df_data['category3'].astype(str)+' '+\\\n    'brand_'+df_data['brand_name'].astype(str) \n    \n    df_data['name']  = df_data['brand_cat']  + ' ' + df_data['name']\n    \n    df_data['name_desc']  = df_data['name'] + ' ' +\\\n    df_data['item_description'].apply(lambda x : ' '.join( x.split()[:5]))\n    \n    df_data['item_condition_id'] = df_data['item_condition_id']/5.\n    return df_data\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ed683d5a71c9b700ccbebc7b1d92e7d483a066ae"},"cell_type":"markdown","source":"### Parallelize Training\nJust to increase speed of training. Not relevant."},{"metadata":{"_uuid":"d00db3f782a778158f2b1af8d50ee3c1afad06ae","trusted":false},"cell_type":"code","source":"def parallelize_dataframe(df, func):\n    df_split = np.array_split(df, cores)\n    pool = Pool(cores)\n    df = pd.concat(pool.map(func, df_split))\n    pool.close()\n    pool.join()\n    return df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cdbe2595e31608b72cfbdc8d4bfc75840bfe3a0d"},"cell_type":"markdown","source":"# Dataset Processing"},{"metadata":{"_uuid":"1c1dcfe319fcb3afad308699d8f1e9f8ca55d98f"},"cell_type":"markdown","source":"## Processing Train\n---\n### Load dataset for train"},{"metadata":{"_uuid":"296b84d6b3e302ea8fd246b8fb61f2105c749011","trusted":false},"cell_type":"code","source":"labels_dict = dict()\n\ndf_train = pd.read_csv(path+'train.tsv', sep='\\t', encoding='utf-8')\ndf_train['item_condition_id'].fillna(2, inplace=True)\ndf_train['shipping'].fillna(0, inplace=True)\nif split > 0:\n    df_train = df_train.loc[:split].reset_index(drop=True)\ndf_train = df_train.loc[df_train.price > 0].reset_index(drop=True)\ndf_train['price'] = np.log1p(df_train['price']).astype(np.float32)\ndf_train.drop('train_id', axis=1, inplace=True)\nbrand_names = df_train.groupby('brand_name').size()  \ndf_train = prepare_data(df_train, train=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dcf2954d65cc3e5203efa98ba1688d34a110d043"},"cell_type":"markdown","source":"### Create Dictionary for unigram words\n\nA dictionary of all the unigram words in the train dataset is created."},{"metadata":{"_kg_hide-input":true,"_uuid":"afaa845d44d72b9997ce037ab547ab4010701311","trusted":false},"cell_type":"code","source":"word_count_dict_one = defaultdict(np.uint32)\nfor feat in ['name','item_description']:\n    df_train[feat].apply(lambda x : word_count(x, word_count_dict_one))\nrare_words = [key for key in word_count_dict_one if  word_count_dict_one[key] < min_df_one]\nfor key in rare_words:\n    word_count_dict_one.pop(key, None)\nfor feat in ['name','item_description']:\n    df_train[feat] = df_train[feat].apply(lambda x : remove_low_freq(x, word_count_dict_one))\nword_count_dict_one = dict(word_count_dict_one)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b0a160cdceea33ab37984c8b2bc19c6f1f4d79d6"},"cell_type":"markdown","source":"### Create Dictionary for Bigram words\n\nA dictionary of all the bigram words in the train dataset is created."},{"metadata":{"_uuid":"e814927ae68c1f5ba1a46258acebe626d94b2fcb","trusted":false},"cell_type":"code","source":"word_count_dict_bi = defaultdict(np.uint32)\ndef word_count_bi(text):\n    text = text.split(' ') \n    for w in text:\n        word_count_dict_bi[w]+=1\ndf_train['name_bi'] = parallelize_dataframe(df_train['name_desc'], create_bigrams_df)\ndf_train['name_bi'].apply(word_count_bi)\nrare_words = [key for key in word_count_dict_bi if word_count_dict_bi[key] < min_df_bi]\nfor key in rare_words:\n    word_count_dict_bi.pop(key, None)\ndf_train['name_bi'] = df_train['name_bi'].apply(lambda x : remove_low_freq(x, word_count_dict_bi))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9a193b957e40eaccbd0686daa53ccc0772badb34"},"cell_type":"markdown","source":"### Finalise Dictionary"},{"metadata":{"_uuid":"bc3733c96cdf979db03b34e69e967a1af1c17e21","trusted":false},"cell_type":"code","source":"word_count_dict_bi = dict(word_count_dict_bi)\nvocabulary_one = word_count_dict_one.copy()\nvocabulary_bi = word_count_dict_bi.copy()\nfor dc in [vocabulary_one, vocabulary_bi]:\n    cpt=0\n    for key in dc:\n        dc[key]=cpt\n        cpt+=1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"df1fb3d8a67049f309ee49e645e5effbf9dc4376"},"cell_type":"markdown","source":"### Mean Price\nProvide mean price of category and brand as a new feature"},{"metadata":{"_uuid":"3129383a8ee01f18c7366eb533223e56e3121cb3","trusted":false},"cell_type":"code","source":"mean_dc=dict()\nfor feat in ['category1', 'category2', 'category3', 'category_name', 'brand_name']:\n    mean_dc[feat] = df_train.groupby(feat)['price'].mean().astype(np.float32)\n    mean_dc[feat] /= np.max(mean_dc[feat])\n    df_train['mean_price_'+feat] = df_train[feat].map(mean_dc[feat]).astype(np.float32)\n    df_train['mean_price_'+feat].fillna(mean_dc[feat].mean(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3778669add5bfc1d62c4601576badc101e94e493"},"cell_type":"markdown","source":"### Vectorise\n\nModels are unable to train on text directly. We are unable to pass words to the model, instead we have to encoded them to numbers."},{"metadata":{"_uuid":"8171db5e3dc52c87b29f392405133707c2ffbd15","trusted":false},"cell_type":"code","source":"# Unigram Name\nvect_name_one = CountVectorizer(vocabulary = vocabulary_one, dtype=np.uint8, tokenizer=tokenize, binary=True) \ntrain_name_one  = vect_name_one.fit_transform(df_train['name'])\n\n# Unigram Description\nvect_item_one = CountVectorizer(vocabulary = vocabulary_one, dtype=np.uint8, tokenizer=tokenize, binary = True) \ntrain_item_one  = vect_item_one.fit_transform(df_train['item_description'])\n\n# Bigram Name\nvect_name_bi = CountVectorizer(vocabulary= vocabulary_bi, dtype=np.uint8, tokenizer=tokenize, binary=True) \ntrain_name_bi = vect_name_bi.fit_transform(df_train['name_bi'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"58c29867982a695f802700c77ebb213a52e18659"},"cell_type":"markdown","source":"## Train"},{"metadata":{"_uuid":"2bf02f4b942935423751f30bbbf282f5cd970e01"},"cell_type":"markdown","source":"### Ridge Regression Model\n---\nIt's more or less a regression model but with something called the ridge penalty. This will cause it to not provide the regression with the best MSE but instead a line that generalise better but will increase it's accuracy on 'unseen' data.\n\n#### Declare independent and dependent variables"},{"metadata":{"_uuid":"f3298e5611f78b31d2b05f2081ae9db0efc5a643","trusted":false},"cell_type":"code","source":"dtrain_y = df_train.price.values\ndtrain = hstack((df_train[keep].values, train_name_one, train_item_one, train_name_bi)).tocsr()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"159f5a675a70a44f3f42e09159a6d3b9ef04df13"},"cell_type":"markdown","source":"#### Initialize Model"},{"metadata":{"_uuid":"b5c75f996ab0d0e04f1958fcecc2d2517499fa5b","trusted":false},"cell_type":"code","source":"model_ridge_name = Ridge(alpha=20, copy_X=True, fit_intercept=True, solver='auto',\n                         max_iter=100, normalize=False, random_state=0, tol=0.0025)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ccb18727d7ab1fa42dcdcf40d9f31393223e4bfc"},"cell_type":"markdown","source":"#### Start Training"},{"metadata":{"_uuid":"19030bd2b6277b9e5dbc6933ca0abcf68fc7b3f9","trusted":false},"cell_type":"code","source":"model_ridge_name.fit(dtrain, dtrain_y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4eed5fa17abc8c318e441825bb20cec747ad0222","trusted":false},"cell_type":"code","source":"# Clean up to save memory\n# Not relevant \ndel dtrain, train_name_bi, train_name_one, train_item_one\ndf_train.drop('name_bi', axis=1, inplace=True)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"35fb713dd2f75135732a6fcfaf1875c85525ee4a"},"cell_type":"markdown","source":"### SPARSE NN MODEL\n---\nJust like any other NN just that weights are mostly 0 so the training is faster.\n\n#### Tokenize Words"},{"metadata":{"_uuid":"70f84214701be19ce5266bf0bd3910c895997094","trusted":false},"cell_type":"code","source":"vect_sparse = CountVectorizer(lowercase=False, min_df=5, ngram_range=(1,2), max_features=200000,\n                              dtype=np.uint8, tokenizer=tokenize, strip_accents=None, binary=True)\n\nvect_sparse.fit(df_train['name']+' '+df_train['item_description'])\n\ndef get_keras_sparse(df):\n    X = {'sparse_data': vect_sparse.transform( df['name']+' '+df['item_description']),\n        'item_condition': np.array(df['item_condition_id']),\n        'shipping': np.array(df[\"shipping\"]),\n        'temp': np.array(df[\"mean_price_category2\"]),\n        'temp2': np.array(df[\"nb_words_item_description\"])\n    }\n    return X\n\ntrain_keras = get_keras_sparse(df_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f7f8ab5704044fb2f9e2887b2581c8ad08e9fea0"},"cell_type":"markdown","source":"#### Network Architecture\n\nThis is actually a multi-input or multi-branch NN where it is able to accept multiple inputs of different types. In our case it accepts tokenized text inputs as well as multiple categorical inputs."},{"metadata":{"_uuid":"a975c7a8297723eeb0e78b539eaf875d5b04809e","trusted":false},"cell_type":"code","source":"def sparseNN():                                             \n    sparse_data = Input( shape=[train_keras[\"sparse_data\"].shape[1]], \n        dtype = 'float32', sparse = True, name='sparse_data')  \n\n    item_condition = Input(shape=[1], name=\"item_condition\")\n    shipping = Input(shape=[1], name=\"shipping\")\n    temp = Input(shape=[1], name=\"temp\")\n    temp2 = Input(shape=[1], name=\"temp2\")\n    \n    x = Dense(200, kernel_initializer=he_uniform(seed=0))(sparse_data)    \n    x = PReLU()(x)\n    x = concatenate([x, item_condition, shipping, temp, temp2]) \n    x = Dense(200, kernel_initializer=he_uniform(seed=0))(x)\n    x = PReLU()(x)\n    x = Dense(100, kernel_initializer=he_uniform(seed=0))(x)\n    x = PReLU()(x)\n    x= Dense(1)(x)\n    \n    model = Model([sparse_data, item_condition, shipping, temp, temp2], x)\n    \n    optimizer = Adam(.0011)\n    model.compile(loss=\"mse\", optimizer=optimizer)\n    return model\n\nsparse_nn = sparseNN()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"01b6f5b1449a8e1079b4f1254803b05c322e9364"},"cell_type":"markdown","source":"#### Start Training NN"},{"metadata":{"_uuid":"3c5041e397d4bac60b849eb088827454cf1af294","trusted":false},"cell_type":"code","source":"BATCH_SIZE = 2000\nepochs = 3\n\nmean_price = np.mean(df_train.price.values)\n\nfor ep in range(epochs):\n    BATCH_SIZE = int(BATCH_SIZE*2)\n    sparse_nn.fit(train_keras, (df_train.price.values-mean_price), batch_size=BATCH_SIZE, epochs=1, verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3833de5a7f16eecc0da5311faeb10f0b68075050","trusted":false},"cell_type":"code","source":"# Clean up to save memory\n# Not relevant \ndel train_keras\ngc.collect","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"181bbfd74c089f052b08dfdf423461e03f37d330"},"cell_type":"markdown","source":"### FASTTEXT-CNN DATA PREPARATION\n\nCreating a text embedding for neural network based on fasttext. Embedding can be considered as a form of transfer learning\n\n- [Simple Intro to Transfer Learning](https://towardsdatascience.com/transfer-learning-946518f95666)\n- [Simple Intro to Fasttext](https://towardsdatascience.com/fasttext-under-the-hood-11efc57b2b3)"},{"metadata":{"_uuid":"4e35f02ca450aad4a517c6bd388f7d81b47d1e2e","trusted":false},"cell_type":"code","source":"MAX_NAME_SEQ = 20\nMAX_ITEM_DESC_SEQ = 30\nMAX_TEXT = len(vocabulary_one) + 1\nMAX_CATEGORY = np.max(df_train['category_name'].max()) + 1\nMAX_CATEGORY1 = np.max(df_train['category1'].max()) + 1\nMAX_CATEGORY2 = np.max(df_train['category2'].max()) + 1\nMAX_CATEGORY3 = np.max(df_train['category3'].max()) + 1\nMAX_BRAND = np.max(df_train['brand_name'].max()) + 1\n\ndef preprocess_keras(text):\n    return [vocabulary_one[w] for w in (text.split())[:MAX_ITEM_DESC_SEQ]]\ndef preprocess_keras_df(df):\n    return df.apply(preprocess_keras)\n\ndf_train['seq_name'] = parallelize_dataframe(df_train['name'], preprocess_keras_df)\ndf_train['seq_item_description'] = parallelize_dataframe(df_train['item_description'], preprocess_keras_df)\n\ndef get_keras_fasttext(df):\n    X = {\n        'name': pad_sequences(df['seq_name'], maxlen=MAX_NAME_SEQ),\n        'item_desc': pad_sequences(df['seq_item_description'], maxlen=MAX_ITEM_DESC_SEQ),\n        'brand_name': np.array(df['brand_name']),\n        'category_name': np.array(df['category_name']),\n        'category1': np.array(df['category1']),\n        'category2': np.array(df['category2']),\n        'category3': np.array(df['category3']),\n        'item_condition': np.array(df['item_condition_id']),\n        'shipping': np.array(df[\"shipping\"]),\n        'temp': np.array(df[\"mean_price_category2\"]),\n        'temp2': np.array(df[\"nb_words_item_description\"])\n    }\n    return X\n\ntrain_keras = get_keras_fasttext(df_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7fbbf3330e3968ded278b5f4a1f68257125e8748"},"cell_type":"markdown","source":"### FASTTEXT-CNN MODEL\n\nJust a typical multi-branch CNN model."},{"metadata":{"_uuid":"89990808bc7c4f7fa8fbdad5bda40bac2b5bbfe3","trusted":false},"cell_type":"code","source":"def fasttext_model():\n    name = Input(shape=[train_keras[\"name\"].shape[1]], name=\"name\")\n    item_desc = Input(shape=[train_keras[\"item_desc\"].shape[1]], name=\"item_desc\")\n    brand_name = Input(shape=[1], name=\"brand_name\")\n    category_name = Input(shape=[1], name=\"category_name\")\n    category1 = Input(shape=[1], name=\"category1\")\n    category2 = Input(shape=[1], name=\"category2\")\n    category3 = Input(shape=[1], name=\"category3\")\n    item_condition = Input(shape=[1], name=\"item_condition\")\n    shipping = Input(shape=[1], name=\"shipping\")\n    temp = Input(shape=[1], name=\"temp\")\n    temp2 = Input(shape=[1], name=\"temp2\")\n    \n    shared_embedding = Embedding(MAX_TEXT, 50)    \n    emb_name = shared_embedding (name)\n    emb_item_desc = shared_embedding (item_desc)\n    \n    val=10\n    emb_brand_name = Flatten()(Embedding(MAX_BRAND, val)(brand_name))\n    emb_category1 = Flatten()(Embedding(MAX_CATEGORY, val)(category1))\n    emb_category2 = Flatten()(Embedding(MAX_CATEGORY, val)(category2))\n    emb_category3 = Flatten()(Embedding(MAX_CATEGORY, val)(category3))\n    \n    emb_name = GlobalAveragePooling1D( name='output_name_max')(emb_name)\n    emb_item_desc = GlobalAveragePooling1D(name='output_item_max')(emb_item_desc)\n\n    x = concatenate([item_condition , shipping, emb_name, emb_item_desc, emb_brand_name, emb_category1, emb_category2, emb_category3])\n    x = BatchNormalization()(x)\n    x = Dense(1024)(x)\n    x = Activation('relu')(x)\n\n    x = Dense(1, activation=\"linear\") (x)\n    model = Model([name, item_desc, brand_name , category_name, category1, category2, category3, item_condition, shipping, temp, temp2], x)\n    optimizer = Adam(.002)\n    model.compile(loss=\"mse\", optimizer=optimizer)\n\n    return model\n\nfasttext_model = fasttext_model()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"648cc2147fed23ea943bc94417f1f6792c77a9a9"},"cell_type":"markdown","source":"#### Start Training FastText Model"},{"metadata":{"_uuid":"58e0a389faf14d2d468e7cb55e83f9cf459bfa9b","trusted":false},"cell_type":"code","source":"BATCH_SIZE = 128\nepochs = 4\n\nfor ep in range(epochs):\n    BATCH_SIZE = int(BATCH_SIZE*2)\n    fasttext_model.fit(train_keras, (df_train.price.values-mean_price),\n                       batch_size=BATCH_SIZE, epochs=1, verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"37b72632e51ee00f193fc3d08cf3c134de8f74f2","trusted":false},"cell_type":"code","source":"# Clean up to save memory\n# Not relevant \ndel train_keras\ngc.collect","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b3e1110592593aa9a8692034a4ecbb0a3859624f"},"cell_type":"markdown","source":"## Predict"},{"metadata":{"_uuid":"5eedfd16f4e2a8a4126f9b3752d9ed3dbab6375f","trusted":false},"cell_type":"code","source":"models_predictions = defaultdict(list)\nsubmission_idx = []\nchunk_counter=1\n\nfor df_submission in pd.read_csv(path+'test.tsv', sep='\\t', encoding='utf-8' , chunksize=350000):\n    df_submission['price'] = -99\n    chunk_counter += 1\n    submission_idx += list(df_submission.test_id.values)\n    \n    if split > 0:\n        print ('USING HOLDOUT AS SUBMISSION')\n        df_submission = pd.read_csv(path+'train.tsv', sep='\\t',  encoding='utf-8')[split:]\n        df_submission = df_submission.loc[df_submission.price>0].reset_index(drop=True)\n        df_submission['price'] = np.log1p(df_submission['price']).astype(np.float32)\n        df_submission.drop('train_id', axis=1, inplace=True)\n        sub_price = df_submission.price.values\n        \n    df_submission['item_condition_id'].fillna(2, inplace=True)\n    df_submission['shipping'].fillna(0, inplace=True)   \n\n    print('SUBMISSION CHUNK SIZE : ',df_submission.shape)\n\n    df_submission = prepare_data(df_submission, train=False)\n    \n    for feat in ['name', 'item_description']:\n        df_submission[feat] = df_submission[feat].apply(lambda x : remove_low_freq(x, vocabulary_one))\n        \n    df_submission['name_bi'] = parallelize_dataframe( df_submission['name_desc'], create_bigrams_df)\n    df_submission['name_bi'] = df_submission['name_bi'].apply(lambda x : remove_low_freq(x, word_count_dict_bi))\n    print('Finished NAME BIGRAMS and REMOVING NEW WORDS.')\n\n    for feat in ['category1', 'category2', 'category3', 'category_name', 'brand_name']:\n        df_submission['mean_price_'+feat] = df_submission[feat].map(mean_dc[feat]).astype(np.float32)\n        df_submission['mean_price_'+feat].fillna(mean_dc[feat].mean(), inplace=True)\n    \n    submission_name_one = vect_name_one.transform(df_submission['name'])\n    submission_item_one = vect_item_one.transform( df_submission['item_description'])\n    submission_name_bi = vect_name_bi.transform(df_submission['name_bi'])\n    print('Finished VECTORIZING SUBMISSION.')\n    \n    df_submission['seq_name'] = parallelize_dataframe(df_submission['name'], preprocess_keras_df)\n    df_submission['seq_item_description'] = parallelize_dataframe(df_submission['item_description'], preprocess_keras_df)\n    \n    # RIDGE MODEL\n    dsubmit_y = df_submission.price.values\n    dsubmit  = hstack((df_submission[keep].values, submission_name_one,submission_item_one,  submission_name_bi  )).tocsr()\n    preds = model_ridge_name.predict(dsubmit)\n    models_predictions['RIDGE1'] += list( preds )\n    if split > 0:\n        print('RIDGE SCORE:',np.sqrt(mean_squared_error(dsubmit_y, preds)))\n    del dsubmit, submission_name_bi, submission_name_one, submission_item_one\n    gc.collect()\n\n        \n    #SPARSE NN MODEL\n    submission_keras = get_keras_sparse(df_submission)\n    preds = sparse_nn.predict(submission_keras, batch_size=100000)+mean_price\n    models_predictions['SPARSENN'] += list( preds.reshape((1,-1))[0])\n    if split > 0:\n        print('SPARSE NN: ', np.sqrt(mean_squared_error(df_submission.price.values,preds)) )\n        \n\n    #FASTTEXT NN MODEL\n    submission_keras = get_keras_fasttext(df_submission)\n    preds = fasttext_model.predict(submission_keras, batch_size=100000)+mean_price\n    models_predictions['FASTTEXT'] += list( preds.reshape((1,-1))[0])\n    if split > 0:\n        print('FASTTEXT: ', np.sqrt(mean_squared_error(df_submission.price.values,preds)))\n        \n    del submission_keras, df_submission\n    gc.collect()\n    \n    if split > 0:\n        break\n        \nsubmission_preds_df = pd.DataFrame(models_predictions)\n\nif split > 0:\n    print ('ENSEMBLE MEAN SCORE :',np.sqrt(mean_squared_error( sub_price, submission_preds_df.mean(axis=1))))\n    from sklearn.linear_model import LinearRegression\n    lr = LinearRegression()\n    lr.fit( submission_preds_df.values, sub_price)\n    preds = lr.predict(submission_preds_df.values)\n    print('ENSEMBLE LR SCORE :', np.sqrt(mean_squared_error(sub_price, preds)))\n    print(lr.coef_)\n    \nif split == -1:    \n    mysubmission = pd.DataFrame()\n    mysubmission['test_id']=submission_idx\n    preds = np.expm1(submission_preds_df.mean(axis=1))\n    preds[preds<3] = 3\n    preds[preds>1000] = 1000\n    mysubmission['price'] = preds\n    mysubmission.to_csv('mean.csv', index=False)\n    print(mysubmission.shape)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}