{"cells":[{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"a804979531697f5934ea328ad582c4ae7e412841","_cell_guid":"a5bdf498-50b6-48a3-89d3-fe38e3743024","collapsed":true},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport types\nfrom sklearn.linear_model import LinearRegression\nfrom scipy.sparse import coo_matrix, hstack\nfrom scipy import io\nimport tensorflow as tf\nimport math\nimport sys, csv, h5py\n\nfrom scipy.sparse import coo_matrix, hstack\nfrom scipy import io\nfrom sklearn.model_selection import train_test_split\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"eefffcb1715c66efb6db40c315f3b0bb7d1655fc","_cell_guid":"3726f4d8-5fc0-43b4-9992-6dfb1bfc7898","collapsed":true},"outputs":[],"source":"# pandasのデータフレームを返す\n# train_or_testには'train'か'test'を入れる\ndef load_data(path,train_or_test,brand_threshold = 100,category_threshold = 50,frequent_brands=None,frequent_categories=None):\n    data_pd = pd.read_csv(path, error_bad_lines=False, encoding='utf-8', header=0, delimiter='\\t')\n    #ブランド名がないものを'NO_BRAND'とする\n    data_pd['brand_name'] = data_pd['brand_name'].fillna('NO_BRAND')\n    data_pd=data_pd.fillna(\"\")\n\n    if train_or_test == 'train':\n        frequent_brands = data_pd['brand_name'].value_counts()[data_pd['brand_name'].value_counts()>brand_threshold].index\n        frequent_categories = data_pd['category_name'].value_counts()[data_pd['category_name'].value_counts()>category_threshold].index\n    elif train_or_test != 'test':\n        print('Error : Please input \"train\" or \"test\" in train_or_test')\n        return\n    \n    if type(frequent_brands)==type(None) or type(frequent_categories)==type(None):\n        print('Error : Please load train data first')\n        return\n    else:\n        data_pd.loc[~data_pd['brand_name'].isin(frequent_brands),'brand_name']= 'SOME_BRAND'\n        data_pd.loc[~data_pd['category_name'].isin(frequent_categories),'category_name'] = 'SOME_CATEGORY'\n        \n    return data_pd,frequent_brands,frequent_categories"},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"5f331b3122af5714a3f16d3beaa81ede3f7cb02d","_cell_guid":"cf34e9b2-7eeb-4d50-af73-e08288a0db6b","collapsed":true},"outputs":[],"source":"csv_train_path = u'../input/train.tsv'\ncsv_test_path = u'../input/test.tsv'\ntrain_data_pd, frequent_brands, frequent_categories = load_data(csv_train_path,'train',brand_threshold=100,category_threshold=50)\ntest_data_pd, _, _ = load_data(csv_test_path,'test',frequent_brands=frequent_brands,frequent_categories=frequent_categories)\nprint('loading data completed')"},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"9eb2bad1aa71892ddeeb365cb49ca850da598f78","_cell_guid":"58889cdd-516c-4934-9e28-d3fdaf9c263b","collapsed":true},"outputs":[],"source":"use_cols = ['item_condition_id','brand_name','shipping','category_name']\ntrain_num = len(train_data_pd)\ntest_num = len(test_data_pd)"},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"80b02996d0fe5c960604dee7a4b13a75575a4df2","_cell_guid":"1cc38025-abee-4d85-9d1f-00633178f0e4","collapsed":true},"outputs":[],"source":"prices = np.array(train_data_pd['price'])\nprices_log = np.log(prices+1)"},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"7921ba31c3011204682959675519d54346ae75d2","_cell_guid":"07f0ce87-e0b2-4144-b6a9-0bdf82a689f7","collapsed":true},"outputs":[],"source":"# scipyのsparse matrix(coo_matrix)X_transform と 変数のリストvariables を返す\n# save_pathに何も指定しない場合ファイルを保存しない 指定した場合指定したディレクトリ内に保存する\ndef make_onehot(use_cols,data_pd,train_or_test,save_path=None):\n    variables = []\n    flag = 0\n    for use_col in use_cols:\n        dummy_pd = pd.get_dummies(data_pd[use_col]).astype(np.uint8)\n        if flag==0:\n            X_transform = coo_matrix(dummy_pd.values)\n            flag=1\n        else:\n            X_transform = hstack([X_transform,coo_matrix(dummy_pd.values)])\n        \n        variables.extend( list( dummy_pd.columns ) )\n        \n        if save_path is not None:\n            if train_or_test != 'test' and train_or_test != 'train':\n                print('Error : Please input \"train\" or \"test\" in train_or_test')\n                return\n            save_path_ = '{}/{}_{}.csv'.format(save_path,use_col,train_or_test)\n            dummy_pd.to_csv(save_path_,index=False,encoding=\"utf8\")\n            \n    if save_path is not None:\n        # sparse matrixの保存\n        io.savemat(\"{}/X_transform_{}\".format(save_path,train_or_test), {\"X_transform\":X_transform})\n        print('sparse matrixを保存しました。次回からはsparse matrixを読み込んで学習に利用してください')\n\n    return X_transform,np.array(variables)"},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"2310e8663f5ba6645888afc145c17c4ea6e15967","_cell_guid":"e56bb840-bd5d-4ad7-b114-b5916c49ce24","collapsed":true},"outputs":[],"source":"X_transform_train,variables = make_onehot(use_cols,train_data_pd,'train',save_path=None)\nX_transform_test,variables_ = make_onehot(use_cols,test_data_pd,'test',save_path=None)\nprint('converting data completed')"},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"8992129b8a5bd1ba3bcef1cb4b214da375c15379","_cell_guid":"033a3603-19c5-46e7-a0c8-6a80cbc2b9b6","collapsed":true},"outputs":[],"source":"# NNの学習\nMAX_EPOCH = 5\nBATCH_SIZE = 1000\nUNIT_NOS = [500,50]\nfeatures = X_transform_test.shape[1]\ndata_path = \"working_dir\"\npatience = 2\n\nmax_size = 100000000\nhidden_no = len(UNIT_NOS)\nx = tf.placeholder(tf.float32, [None, features])\nW_list = []\nb_list = []\n\nold_unit_no = features\nz = x\nfor i, unit_no in enumerate(UNIT_NOS):\n    W = tf.Variable(tf.random_normal([old_unit_no, unit_no], mean=0.0, stddev=0.05))\n    b = tf.Variable(tf.constant(0.1, shape=[unit_no]))\n    W_list.append(W)\n    b_list.append(b)\n    z = tf.nn.relu(tf.matmul(z, W) + b)\n    old_unit_no = unit_no\nW_last = tf.Variable(tf.random_normal([UNIT_NOS[-1], 1], mean=0.0, stddev=0.05))\nb_last = tf.Variable(tf.random_normal([1], mean=0.0, stddev=0.05))\ny = tf.matmul(z, W_last) + b_last\n\ny_ = tf.placeholder(tf.float32, [None, 1])\nmse = tf.reduce_mean((y - y_) * (y - y_))\n\ntrain_step = tf.train.AdamOptimizer(1e-2).minimize(mse)\ninit = tf.initialize_all_variables()\n"},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"b54c4ade970e91b5880853055c48eb113c2c0226","_cell_guid":"9fd8b150-a296-465c-beec-7fd484ff60a2","collapsed":true},"outputs":[],"source":"sess = tf.Session()\nsess.run(init)\n\n#学習の保存\nsaver = tf.train.Saver(max_to_keep=1)\nckpt = tf.train.get_checkpoint_state(data_path)\nif ckpt and ckpt.model_checkpoint_path:\n    print(\"Reading model parameters from %s\" % ckpt.model_checkpoint_path)\n    saver.restore(sess, ckpt.model_checkpoint_path)\n"},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"600a0e753fbf8584825fd77df83b017945e83487","_cell_guid":"f36936ec-debe-4f56-a818-5d65a07fa31c","collapsed":true},"outputs":[],"source":"# データ取得\n# # sparse matrixの読み込み\n# X_transform = io.loadmat(\"../../../onehots/X_transform_train\")[\"X_transform\"]\n# y = np.load('../../../onehots/y_log.npy')\n# X_transform_test = io.loadmat(\"../../../onehots/X_transform_test\")[\"X_transform\"]\n\ntrain_X, test_X, train_y, test_y = train_test_split(X_transform_train, prices_log, test_size=0.1, random_state=42)\ntrain_X, valid_X, train_y, valid_y = train_test_split(train_X, train_y, test_size=0.1, random_state=42)\n\nprint('data_length:'+str(train_X.shape[0]))\nbatch_no = int( (train_X.shape[0] - 1) / BATCH_SIZE + 1)\nprint('batch_no:'+str(batch_no))\n"},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"abd573a3935e9748d756bc1292589d9d49b95b53","_cell_guid":"4b1d1354-2af4-42c8-93cd-ccccbcaf5695","collapsed":true},"outputs":[],"source":"count = 0"},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"13e5093b9cb5b45094bcd161e75fb55ed506adfe","_cell_guid":"0f89d0a2-baa9-45d2-8036-0361ae6dc055","collapsed":true},"outputs":[],"source":"min = np.inf\nfor i in range(MAX_EPOCH):\n    print(\"epoch:\"+str(i+1))\n\n    # SGDを実装している\n    for j in range(batch_no):\n        batch_xs = (train_X.toarray())[j * BATCH_SIZE:(j + 1) * BATCH_SIZE]\n        batch_ys = train_y[j * BATCH_SIZE:(j + 1) * BATCH_SIZE].reshape(-1, 1)\n        sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n#         if j%100==1:\n#             print('   batch:'+str(j))\n    train_cost = sess.run(mse, feed_dict={x: (train_X.toarray())[:BATCH_SIZE], y_: train_y[:BATCH_SIZE].reshape(-1, 1)})\n    valid_cost = sess.run(mse, feed_dict={x: valid_X.toarray(), y_: valid_y.reshape(-1, 1)})\n    print (str(i + 1) + \"epoch:train cost(rmse)=\" + str(math.sqrt(train_cost)) + \", rmse=\" + str(math.sqrt(valid_cost)) )\n\n    if valid_cost < min:\n        count = 0\n        # for i, W, b in zip(range(hidden_no), W_list, b_list):\n        #     np.savetxt(path + \"W\" + str(i + 1) + \".csv\", sess.run(W), delimiter=\",\")\n        #     np.savetxt(path + \"b\" + str(i + 1) + \".csv\", sess.run(b), delimiter=\",\")\n        # np.savetxt(path + \"W.csv\", sess.run(W_last), delimiter=\",\")\n        # np.savetxt(path + \"b.csv\", sess.run(b_last), delimiter=\",\")\n        min = valid_cost\n    else:\n        count += 1\n\n\n    # 改善されなかった回数がpatience回以上で学習終了\n    if count >= patience:\n        break\nprint (\"test rmse=\" + str(math.sqrt(sess.run(mse, feed_dict={x: test_X.toarray(), y_: test_y[:].reshape(-1, 1)}))) )\n"},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"8cbbda1f11e7d9ae982501fb64543e7ba72a43a7","_cell_guid":"c76c029e-4a52-4151-ad2e-0d6b83157368","collapsed":true},"outputs":[],"source":"prediction_log = sess.run(y,feed_dict={x:X_transform_test.toarray()}).reshape(-1)\nprediction = np.exp(prediction_log)-1\ntest_id = np.arange(prediction.shape[0])"},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"99ee6a7d4afbb4a145f6bfb194cdf45aff0bcf41","_cell_guid":"cd30300c-7185-4cd0-a2d8-12aafce969c6","collapsed":true},"outputs":[],"source":"submission = pd.DataFrame([])\nsubmission['test_id'] = test_id\nsubmission['price'] = pd.DataFrame(prediction)\nsubmission.to_csv('submission.csv',index=None)\nprint(submission.iloc[:10])"}],"nbformat":4,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","file_extension":".py","mimetype":"text/x-python","pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.3","codemirror_mode":{"name":"ipython","version":3}}},"nbformat_minor":1}