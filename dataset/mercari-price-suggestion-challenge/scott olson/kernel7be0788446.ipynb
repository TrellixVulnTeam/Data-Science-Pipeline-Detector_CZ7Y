{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport pathlib\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import BatchNormalization, Conv2D, Conv2DTranspose, Dense, Dropout, Flatten, LeakyReLU, MaxPooling2D\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.python.client import device_lib\nimport glob\n# import imageio\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport pathlib\nimport os\nfrom PIL import Image\nimport time\nfrom IPython import display\n\nprint('TF version {ver}'.format(ver=tf.__version__))\nprint('Built with CUDA {cudaSupport} and GPU available {gpuAvailable}'.format(cudaSupport=tf.test.is_built_with_cuda(), gpuAvailable=tf.test.is_gpu_available()))\nprint(device_lib.list_local_devices())\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nprint(os.listdir('../input/mercari-price-suggestion-challenge'))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"DATA_BASE = '../input/mercari-price-suggestion-challenge'\nEPOCHS = 10\nBATCH_SIZE = 100000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('../input/mercari-price-suggestion-challenge/train.tsv', sep='\\t')\nprint(df_train.shape)\ndf_train.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = pd.read_csv('../input/mercari-price-suggestion-challenge/test.tsv', sep='\\t')\nprint(df_test.shape)\ndf_train.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.groupby('brand_name').size().to_frame()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del df_train['train_id']\ndel df_train['name']\ndel df_train['item_description']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def prepare_data(df):\n    df = df.replace(np.nan, 0).replace(np.inf, 1e+5).replace(-np.inf, -1e+5)\n    for column in df.columns:\n        if df[column].dtype.name == 'object':\n            df[column] = pd.Categorical(df[column]).codes\n            \n        if column not in ['value']:\n            col_stats = df[column].describe()\n            df[column] = (df[column] - col_stats['mean']) / col_stats['std']\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_na = (df_train.isnull().sum() / len(df_train)) * 100\ndf_train_na = df_train_na.drop(df_train_na[df_train_na == 0].index).sort_values(ascending=False)[:30]\ndf_train_missing_data = pd.DataFrame({'Missing Ratio' :df_train_na})\ndf_train_missing_data.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['brand_name'].fillna('**unknown**')\ndf_train['category_name'].fillna('**unknown**')\ndf_train['item_condition_id'] = df_train['item_condition_id'].astype(str)\ndf_train['shipping'] = df_train['shipping'].astype(str)\ndf_train[\"price\"] = np.log1p(df_train[\"price\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check the new distribution \nsns.distplot(df_train['price'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(df_train['price'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('price distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(df_train['price'], plot=plt)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = prepare_data(df_train)\ndf_train.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Correlation map to see how features are correlated with price\ncorrmat = df_train.corr()\nplt.subplots(figsize=(12,9))\nsns.heatmap(corrmat, vmax=0.9, square=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots()\nax.scatter(x = df_train['brand_name'], y = df_train['price'])\nplt.ylabel('Price', fontsize=13)\nplt.xlabel('BrandName', fontsize=13)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = df_train.price.values\ny_train = y_train.ravel().astype(np.float64)\ndf_train.drop('price', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model():\n  model = tf.keras.Sequential([\n    Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001), input_shape=[len(df_train.keys())]),\n    Dropout(0.3),\n    Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n    Dropout(0.3),\n    Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n    Dropout(0.3),\n    Dense(1)\n  ])\n\n  optimizer = tf.keras.optimizers.RMSprop(1e-4)\n#   optimizer = tf.keras.optimizers.Adam(1e-4)\n\n#   model.compile(loss='mse',\n#                 optimizer=optimizer,\n#                 metrics=['mae', 'mse'])\n\n  model.compile(loss='mse',\n                optimizer=optimizer,\n                metrics=['mae', 'mse'])\n  return model\n\nmodel = build_model()\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_train.dtypes)\ndf_train['item_condition_id'] = df_train['item_condition_id'].astype('float16')\ndf_train['category_name'] = df_train['category_name'].astype('float16')\ndf_train['brand_name'] = df_train['brand_name'].astype('float16')\ndf_train['shipping'] = df_train['shipping'].astype('float16')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# @tf.function\n# def train_step(ds):\n# #   losses = []\n#   with tf.GradientTape() as gen_tape, tf.GradientTape() as tape:\n#     real_output = model(ds[0], training=True)\n\n#     loss = cross_entropy(ds[1], real_output)\n#     losses.append(loss)\n\n#   gradients = tape.gradient(loss, model.trainable_variables)\n#   optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n# #   return losses\n\n# def train(ds, epochs):\n#   for epoch in range(epochs):\n#     start = time.time()\n#     losses = []\n\n# #     for i in range(int(DS_SIZE / BATCH_SIZE)):\n# #       image_batch = next(iter(dataset))\n#     batch_losses = train_step(ds)\n#     losses.extend(batch_losses)\n\n# #     checkpoint.step.assign_add(1)\n\n#     print(f'Time for epoch {epoch} is {time.time()-start:.2f} sec, loss: {np.array(losses).mean():.6f}')\n\n# dataset = tf.data.Dataset.from_tensor_slices((df_train.values, y_train))\n# train(dataset, EPOCHS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MyProgbarLogger(tf.keras.callbacks.Callback):\n  def on_train_begin(self, logs=None):\n    self.seen = 0\n    self.progbar = tf.keras.utils.Progbar(\n        target=EPOCHS,\n        unit_name='epoch')\n\n  def on_epoch_end(self, epoch, logs=None):\n    self.seen += 1\n    self.progbar.update(self.seen)\n    \nclass PrintDot(tf.keras.callbacks.Callback):\n  def on_epoch_end(self, epoch, logs=None):\n    print('.', end='')\n    \n# progbar = keras.callbacks.ProgbarLogger(params={'verbose': False})\n# early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n\nhistory = model.fit(\n  df_train, y_train,\n  epochs=EPOCHS, validation_split = 0.2, verbose=0,\n#   callbacks=[early_stop, MyProgbarLogger()])\n    callbacks=[MyProgbarLogger()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_history(history):\n  hist = pd.DataFrame(history.history)\n  hist['epoch'] = history.epoch\n\n  plt.figure()\n  plt.xlabel('Epoch')\n  plt.ylabel('Mean Abs Error [price]')\n  plt.plot(hist['epoch'], hist['mae'],\n           label='Train Error')\n  plt.plot(hist['epoch'], hist['val_mae'],\n           label = 'Val Error')\n  plt.ylim([0,1])\n  plt.legend()\n\n  plt.figure()\n  plt.xlabel('Epoch')\n  plt.ylabel('Mean Square Error [$price^2$]')\n  plt.plot(hist['epoch'], hist['mse'],\n           label='Train Error')\n  plt.plot(hist['epoch'], hist['val_mse'],\n           label = 'Val Error')\n  plt.ylim([0,1])\n  plt.legend()\n  plt.show()\n\n\nplot_history(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}