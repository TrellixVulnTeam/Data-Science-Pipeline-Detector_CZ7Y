{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Key Challenge :** \n\nSome fields like \"name\" and \"item description\" are having substantial text content to work with. Hence, treating those fields as simple categorical features and converting them into label encoded features might not give satisfactory result to predict the price of a mercari product. We need to apply advanced text processing.\n\n**Key Takeways from this Kernel :**\n\n* Performance of Supervised Predictive Model without/with using Advanced Text Processing \n* Compare and See the Difference! **(for above)**\n* Concept of CountVectorizer, TfidfVectorizer, LabelBinarizer\n* Concept of Sparse Matrices\n* Concept of Topic Modelling and Latent Dirichlet Allocation (Unsupervised method)\n* Using **\"pyLDAvis\"** for visualizing topics in LDA Topic Model","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%javascript\nIPython.OutputArea.prototype._should_scroll = function() {\n    return False;\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!apt-get install p7zip","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!p7zip -d '../input/mercari-price-suggestion-challenge/train.tsv.7z'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us import training and testing data sets.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\ntrain_data = pd.read_csv(\"train.tsv\", sep='\\t') \ntest_data = pd.read_csv(\"../input/mercari-price-suggestion-challenge/test_stg2.tsv.zip\" , sep='\\t')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, let us have a look at the data and data types.","metadata":{}},{"cell_type":"code","source":"train_data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.head()","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We are creating a copy of training data set. We will initially work with the traing data copy. Later we will use the original training data set for applying advanced text pre-processing on it.","metadata":{}},{"cell_type":"code","source":"train_copy = train_data.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us free up some memory to avoid getting \"memory exceeded\" warning.","metadata":{}},{"cell_type":"code","source":"import gc\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Pre-Processing :","metadata":{}},{"cell_type":"markdown","source":"We will split category name into three parts: (1) Main Category, (2) First Sub-Category, (3) Second Sub-Category. Whenever blanks are found, they will be replaced as \"No Label\" for these three. Then we will apply label encoding on them.","metadata":{}},{"cell_type":"code","source":"# Splitting category name\ndef split_cat(text):\n    try: return text.split(\"/\")\n    except: return (\"No Label\", \"No Label\", \"No Label\") ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Splitting category name into: Main Category, SubCategory_1, SubCategory_2\ntrain_copy['main_category'], train_copy['subcat_1'], train_copy['subcat_2'] = zip(*train_copy['category_name'].apply(lambda x: split_cat(x)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Label Encoding\nfrom sklearn import preprocessing\ndef toNumeric(data,to):\n    if train_copy[data].dtype == type(object):\n        le = preprocessing.LabelEncoder()\n        train_copy[to] = le.fit_transform(train_copy[data].astype(str))   \ntoNumeric('name','n_name')\ntoNumeric('category_name','n_category_name')\ntoNumeric('brand_name','n_brand_name')\ntoNumeric('main_category','n_main_category')\ntoNumeric('subcat_1','n_subcat_1')\ntoNumeric('subcat_2','n_subcat_2')\ntrain_copy.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Cleaning :\nWe will apply basic data cleaning like: filling up missing data, dropping NA etc.","metadata":{}},{"cell_type":"code","source":"#Checking for NULL values in the columns\ntrain_copy.isnull().any()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Category_name, brand_name and item_description have null values. So we will fill up missing data for these coulumns.","metadata":{}},{"cell_type":"code","source":"def fill_missing_data(data):\n    data.category_name.fillna(value = \"Other/Other/Other\", inplace = True)\n    data.brand_name.fillna(value = \"Unknown brand\", inplace = True)\n    data.item_description.fillna(value = \"No description\", inplace = True)\n    return data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\ntrain_copy = fill_missing_data(train_copy)\ntrain_copy = train_copy.dropna()\nprint(np.shape(train_copy))\ntrain_copy.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"main_category, subcat_1, subcat_2 are now segregated. Also we have got separate columns for corresponding label encoded values of those parameters.","metadata":{}},{"cell_type":"markdown","source":"### Exploratory Data Analysis :\n\nWe will proceed with doing some EDA now to explore some interesting findings.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(context='notebook')\nsns.set_style(\"whitegrid\", {'axes.grid' : False})\nplt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### [ Sale Price ]","metadata":{}},{"cell_type":"code","source":"print(\"Range of price : \")\nprint(\"Minimum Price: \",'$', train_copy[\"price\"].min())\nprint(\"Maximum Price: \",'$', train_copy[\"price\"].max())\nfig, ax = plt.subplots(3, 1, figsize = (13, 16))\nax[0].hist(train_copy.price, bins = 100, range = [min(train_copy.price), max(train_copy.price)+100], label = \"price\", color='red', alpha=0.7)\nax[0].annotate(' Outliers\\n present\\n till\\n this point', xy=(max(train_copy.price), 100), xytext=(max(train_copy.price), 125000), arrowprops=dict(facecolor='black'), color='black')\nax[0].set_title(\"Histogram of Price Distribution\", fontsize = 13)\nax[0].set_xlabel(\"Price\", fontsize = 10)\nax[0].set_ylabel(\"Frequency \", fontsize = 10)\n\nax[1].set_title(\"Histogram of Price Distribution (Focused Mode)\", fontsize = 13)\nax[1].hist(train_copy.price, bins = 100, range = [0, 200], label = \"price\", color='red', alpha=0.7)\nax[1].set_xlabel(\"Price\", fontsize = 10)\nax[1].set_ylabel(\"Frequency \", fontsize = 10)\n\n\nsns.boxplot(train_copy.price, showfliers = False, ax = ax[2], linewidth=0.7, color='red')\nax[2].set_title(\"Box Plot for Price Distribution\", fontsize = 13)\nax[2].set_xlabel(\"Price\", fontsize = 10)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Price distribution is right-skewed and not quite in the shape of conforming with Normal Distribution. The range of outliers for price is very wide.","metadata":{}},{"cell_type":"markdown","source":"#### [ Brand Name ]","metadata":{}},{"cell_type":"code","source":"brands = train_copy[\"brand_name\"].value_counts()\nprint(\"No. of Unique Brand Names :\", brands.size)\nfig, ax = plt.subplots(1, 2, figsize = (13, 6))\n# we skipped '0' index and started from 1st because 0th index has \"unknown brands\"\nsns.barplot(brands[1:11].values, brands[1:11].index,ax = ax[0], edgecolor='k', linewidth=0.5, palette='rocket') \nax[0].set_title(\"Top 10 Most Frequently Used Brand Names\", fontsize = 13)\nax[0].set_xlabel(\"Counts\", fontsize = 10)\nax[0].set_ylabel(\"Brand Name\", fontsize = 10)\n\nimport pandas as pd\ntop10_brands = train_copy.groupby('brand_name', axis=0).mean()\ndf_expPrice = pd.DataFrame(top10_brands.sort_values('price', ascending = False)['price'][0:10].reset_index())\nax[1].set_title(\"Top 10 Most Costly Brands\", fontsize = 13)\nax[1] = sns.barplot(x=\"brand_name\", y=\"price\", data=df_expPrice, edgecolor='k', linewidth=0.5, palette='PuRd')\nax[1].set_xlabel(\"Brand Name\", fontsize = 10)\nax[1].set_ylabel(\"Sale Price\", fontsize = 10)\nax[1].set_xticklabels(ax[1].get_xticklabels(),rotation=35)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\"Victoria's Secret\" is the most widely used brand whereas \"Demdaco\" is the costliest brand in the lot. ","metadata":{}},{"cell_type":"markdown","source":"#### [ Item Condition Id ]","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 2, figsize = (13,6))\nsns.countplot(train_copy.item_condition_id, ax = ax[0], palette='Blues_r', edgecolor='k', linewidth=0.6)\nrectangles = ax[0].patches\nax[0].set_title(\"Count Distribution of Item Condition Ids\", fontsize = 13)\nlabels = train_copy.item_condition_id.value_counts().values\nfor rect, label in zip(rectangles, labels):\n    height = rect.get_height()\n    ax[0].text(rect.get_x() + rect.get_width()/2, height + 5, label, ha = \"center\", va = \"bottom\")\nax[1].set_title(\"Sale Price Distribution of Item Condition Ids\", fontsize = 13)    \nsns.boxplot(x = train_copy.item_condition_id, y = train_copy.price, showfliers = False, orient = \"v\", ax = ax[1], hue = train_copy.shipping, palette=\"Set1\", linewidth=0.6)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\"1\" is the most widely used item_condition_id. Products having item_condition_ids \"1\" and \"5\" are costly. Interestingly, item_condition_ids with shipping flag \"true\" are having lesser price as compared to those with shipping flag \"no\".","metadata":{}},{"cell_type":"markdown","source":"#### [ Main Category ]","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(2, 1, figsize = (13,18))\nsns.countplot(train_copy.main_category, ax = ax[0], palette='Reds', edgecolor='k', linewidth=0.6)\nax[0].set_xticklabels(ax[0].get_xticklabels(),rotation=15)\nax[0].set_title(\"Count Distribution of Main Categories\", fontsize = 13)\nrectangles = ax[0].patches\nlabels = train_copy.main_category.value_counts().values\nfor rect, label in zip(rectangles, labels):\n    height = rect.get_height()\n    ax[0].text(rect.get_x() + rect.get_width()/2, height + 5, label, ha = \"center\", va = \"bottom\")\nsns.boxplot(x = train_copy.main_category, y = train_copy.price, showfliers = False, orient = \"v\", ax = ax[1], hue = train_copy.shipping, palette=\"Set1\", linewidth=0.6)\nax[1].set_xticklabels(ax[1].get_xticklabels(),rotation=15)\nax[1].set_title(\"Sale Price Distribution of Main Categories\", fontsize = 13)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Main_category belonging to \"Women\", \"Beauty\" and \"Kids\" are the most frequently bought. Products belonging to \"Electronics\" and \"Men\" main categories are pricier than the other ones. Shipping flag \"yes\" are less costlier than shipping flag \"no\" products. ","metadata":{}},{"cell_type":"markdown","source":"#### [ Item Description ]","metadata":{}},{"cell_type":"code","source":"#python -m pip install wordcloud\nfrom wordcloud import WordCloud\nimport os\nwordcloud = WordCloud(width = 2400, height = 1200).generate(\" \".join(train_copy.item_description.astype(str)))\nplt.figure(figsize = (13, 10))\nplt.imshow(wordcloud)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\"Brand new\", \"free shipping\", \"great condition\", \"good condition\", \"never worn\", \"never used\", \n\"Victoria Secret\", \"smoke free\", \"Size large\", \"Size medium\", \"Size small\", \"excellent condition\" \nare some frequently appearing item description texts.\n\n### Log-Transformation of Target Variable :","metadata":{}},{"cell_type":"code","source":"train_copy['price'] = np.log1p(train_copy['price'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(2, 1, figsize = (13, 12))\nax[0].hist(train_copy.price, bins = 100, range = [min(train_copy.price), max(train_copy.price)+100], label = \"price\", color='red', alpha=0.7)\nax[0].annotate(' Outliers\\n present\\n till\\n this point', xy=(max(train_copy.price), 100), xytext=(max(train_copy.price), 125000), arrowprops=dict(facecolor='black'), color='black')\nax[0].set_title(\"Histogram of Log(Price) Distribution\", fontsize = 13)\nax[0].set_xlabel(\"Price\", fontsize = 10)\nax[0].set_ylabel(\"Frequency \", fontsize = 10)\n\nax[1].set_title(\"Histogram of Log(Price) Distribution (Focused Mode)\", fontsize = 13)\nax[1].hist(train_copy.price, bins = 100, range = [0, 8], label = \"price\", color='red', alpha=0.7)\nax[1].set_xlabel(\"Log(Price)\", fontsize = 10)\nax[1].set_ylabel(\"Frequency \", fontsize = 10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now the transformed price distribution has taken the symmetric shape and can be said that it is following Normal Distribution. We can work with this.","metadata":{}},{"cell_type":"markdown","source":"### Correlation Analysis :","metadata":{}},{"cell_type":"code","source":"import numpy as np\nmask = np.zeros_like(train_copy.corr().fillna(0), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nplt.figure(figsize=(10,10))\nsns.heatmap(train_copy.corr(), mask = mask, vmin = -1, annot = True, fmt='.1g', cmap = 'plasma', edgecolor='w', linewidth=0.6)\nplt.suptitle(' Correlations Heat Map for all Attributes', fontsize=13)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"'n_category_name' and 'n_main_category' are very highly correlated. Hence we will choose to keep 'n_main_category' in the predictive model discarding 'n_category_name'.","metadata":{}},{"cell_type":"code","source":"gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Modeling\nWe are using only one simple regression models here. We are not tuning hyperparameters for the boost models. The main goal of this work is to check the power of using advanced text pre-processing. So we will use only simple predictive models here. Later we will re-check the performance of same predictive models (i.e. Ridge, LGBM and XGB regressors) after using advanced text pre-processing and compare how much improvement is evident.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nr_data= train_copy[['item_condition_id','shipping','n_name','n_brand_name','n_main_category','n_subcat_1','n_subcat_2']]\nX_train, x_test, Y_train, y_test = train_test_split(r_data, train_copy['price'], test_size=0.25, random_state=12345)\n\ndef run_model(model, X_train, Y_train, x_test, y_test, verbose = False):\n    Y_train = Y_train[:, np.newaxis].ravel()\n    model.fit(X_train, Y_train)\n    y_predict = model.predict(x_test)\n    mse = mean_squared_error(y_test,y_predict)\n    r_sq = r2_score(y_test,y_predict)\n    print(\"Mean Squared Error Value : \"+\"{:.2f}\".format(mse))\n    print(\"R-Squared Value : \"+\"{:.2f}\".format(r_sq))\n    return model, mse, r_sq","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Model-1: Ridge Regression","metadata":{}},{"cell_type":"code","source":"from sklearn import linear_model\nridge_reg = linear_model.Ridge()\nprint(\"Ridge Regression\")\nprint(\"----------------\")\nmodel_1, mse_1, r_sq_1 = run_model(ridge_reg, X_train, Y_train, x_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Model-2: LGBM Regression","metadata":{}},{"cell_type":"code","source":"import lightgbm\nlgbm_reg = lightgbm.LGBMRegressor()\nprint(\"LGBM Regression\")\nprint(\"---------------\")\nmodel_2, mse_2, r_sq_2 = run_model(lgbm_reg, X_train, Y_train, x_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Model-3: XGB Regression","metadata":{}},{"cell_type":"code","source":"import xgboost\nxgb_params = {'n_estimators':500, 'max_depth':8}\nxgb_reg = xgboost.XGBRegressor(**xgb_params)\nprint(\"XGBoost Regression\")\nprint(\"------------------\")\nmodel_3, mse_3, r_sq_3 = run_model(xgb_reg, X_train, Y_train, x_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations :** \n\n1. very high **MSE** (52%) and too low \\\\( R^2 \\\\) value (only 8%) for Ridge regression;\n2. high **MSE** (35%) and low \\\\( R^2 \\\\) value (39%) for LGBM regression;\n3. medium **MSE** (27%) and medium \\\\( R^2 \\\\) value (52%) for XGB regression.\n\n\n### Improved Approach with Advanced Text Pre-Processing :","metadata":{}},{"cell_type":"code","source":"# Creating a set combining Train & Test data. Applying Count Vectorizer on combined set will help us to get the list of all possible words.\ncombined_data = pd.concat([train_data,test_data])\n\n# Specify size of training set\ntrain_size = len(train_data)\n\n# Submission set containing only the test IDs\nsubmission = test_data[['test_id']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combined_data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Taking a fraction (10%) of combined data set for experimentation. Dropping train/test ids here\ncombined_frac = combined_data.sample(frac=0.1).reset_index(drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combined_frac.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The steps we will apply for ***advanced text pre-processing*** are:\n1. Removing Puncuations\n2. Removing Digits\n3. Removing Stopwords\n4. Changing to Lower-case words\n5. Lemmatization or Stemming","metadata":{}},{"cell_type":"code","source":"from string import punctuation\npunctuation","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a list of punctuation replacements\npunctuation_symbols = []\nfor symbol in punctuation:\n    punctuation_symbols.append((symbol, ''))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove Punctuation\nimport string\ndef remove_punctuation(sentence: str) -> str:\n    return sentence.translate(str.maketrans('', '', string.punctuation))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove Digits\ndef remove_digits(x):\n    x = ''.join([i for i in x if not i.isdigit()])\n    return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove Stopwords\nfrom nltk.corpus import stopwords\n\nstop = stopwords.words('english')\n\ndef remove_stop_words(x):\n    x = ' '.join([i for i in x.lower().split(' ') if i not in stop])\n    return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Change to LowerCase Words\ndef to_lower(x):\n    return x.lower()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Segregating \"category_name\" into \"category_main\", \"subcat_1\", \"subcat_2\" like we did before \ndef transform_category_name(category_name):\n    try:\n        main, sub1, sub2= category_name.split('/')\n        return main, sub1, sub2\n    except:\n        return np.nan, np.nan, np.nan\n\ntrain_data['category_main'], train_data['subcat_1'], train_data['subcat_2'] = zip(*train_data['category_name'].apply(transform_category_name))\ncat_train = train_data[['category_main','subcat_1','subcat_2', 'price']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Item Description Analysis","metadata":{}},{"cell_type":"code","source":"# Remove Digits, Punctuation, Stopwords, Converting to Lower-case and See the Effect\ncombined_data.item_description = combined_data.item_description.astype(str)\ndescr = combined_data[['item_description', 'price']]\ndescr['count'] = descr['item_description'].apply(lambda x : len(str(x)))\ndescr['item_description'] = descr['item_description'].apply(remove_digits)\ndescr['item_description'] = descr['item_description'].apply(remove_punctuation)\ndescr['item_description'] = descr['item_description'].apply(remove_stop_words)\ndescr.head(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.stem.porter import PorterStemmer\nporter = PorterStemmer()\ndescr['item_description'] = descr['item_description'].apply(porter.stem)\ndescr.tail(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Basic data imputation of missing values\ndef handle_missing_values(df):\n    df['category_name'].fillna(value='missing', inplace=True)\n    df['brand_name'].fillna(value='None', inplace=True)\n    df['item_description'].fillna(value='None', inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Converts to Categorical Features \ndef to_categorical(df):\n    df['brand_name'] = df['brand_name'].astype('category')\n    df['category_name'] = df['category_name'].astype('category')\n    df['item_condition_id'] = df['item_condition_id'].astype('category')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"handle_missing_values(combined_frac)\nto_categorical(combined_frac)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"handle_missing_values(combined_data)\nto_categorical(combined_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove Digits, Punctuation, Stopwords, Converting to Lower-case for combined_frac\ncombined_frac.item_description = combined_frac.item_description.astype(str)\ncombined_frac['item_description'] = combined_frac['item_description'].apply(remove_digits)\ncombined_frac['item_description'] = combined_frac['item_description'].apply(remove_punctuation)\ncombined_frac['item_description'] = combined_frac['item_description'].apply(remove_stop_words)\ncombined_frac['item_description'] = combined_frac['item_description'].apply(to_lower)\ncombined_frac['name'] = combined_frac['name'].apply(remove_digits)\ncombined_frac['name'] = combined_frac['name'].apply(remove_punctuation)\ncombined_frac['name'] = combined_frac['name'].apply(remove_stop_words)\ncombined_frac['name'] = combined_frac['name'].apply(to_lower)\ncombined_frac.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove Digits, Punctuation, Stopwords, Converting to Lower-case for combined_data\ncombined_data.item_description = combined_data.item_description.astype(str)\ncombined_data['item_description'] = combined_data['item_description'].apply(remove_digits)\ncombined_data['item_description'] = combined_data['item_description'].apply(remove_punctuation)\ncombined_data['item_description'] = combined_data['item_description'].apply(remove_stop_words)\ncombined_data['item_description'] = combined_data['item_description'].apply(to_lower)\ncombined_data['name'] = combined_data['name'].apply(remove_digits)\ncombined_data['name'] = combined_data['name'].apply(remove_punctuation)\ncombined_data['name'] = combined_data['name'].apply(remove_stop_words)\ncombined_data['name'] = combined_data['name'].apply(to_lower)\ncombined_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Applying CountVectorizer / TfidfVectorizer / LabelBinarizer\n\n* CountVectorizer counts word frequencies. \n* TF-IDF Vectorizer gives more significance (puts more weights) on rare words, and less significance (puts lesser weights) on frequent words. \n* Label Binarizer converts labels into numeric representations for e.g. \"A,B,C\" -> [1,2,3]","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.preprocessing import LabelBinarizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply Count Vectorizer to \"name\", this converts it into a sparse matrix \ncv = CountVectorizer(min_df=10)\nX_name = cv.fit_transform(combined_data['name'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply Count Vectorizer to \"category_name\", this converts it into a sparse matrix\ncv = CountVectorizer()\nX_category = cv.fit_transform(combined_data['category_name'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply TFIDF to \"item_description\", \ntv = TfidfVectorizer(max_features=55000, ngram_range=(1, 2), stop_words='english')\nX_description = tv.fit_transform(combined_data['item_description'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply LabelBinarizer to \"brand_name\"\nlb = LabelBinarizer(sparse_output=True)\nX_brand = lb.fit_transform(combined_data['brand_name'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# vstack - adds rows\n# hstack - adds columns\n# csr_matrix - handles sparse matrix\n\nfrom scipy.sparse import vstack, hstack, csr_matrix\nX_dummies = csr_matrix(pd.get_dummies(combined_data[['item_condition_id', 'shipping']], sparse=True).values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the final sparse matrix combining everything together\nsparse_merge = hstack((X_dummies, X_description, X_brand, X_category, X_name)).tocsr()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_sparse = sparse_merge[:train_size]\nX_test = sparse_merge[train_size:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import KFold\nkf = KFold(n_splits=3, shuffle=True, random_state=12345)\ny = np.log1p(train_data['price'])\ni = 0;\nfor train_indicies, valid_indicies in kf.split(X_train_sparse):\n    X_train, y_train = X_train_sparse[train_indicies], y[train_indicies]\n    X_valid, y_valid = X_train_sparse[valid_indicies], y[valid_indicies]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_model_advText(model, X_train, y_train, X_valid, y_valid, verbose = False):\n    model.fit(X_train, y_train)\n    preds_valid = model.predict(X_valid)\n    mse = mean_squared_error(y_valid,preds_valid)\n    r_sq = r2_score(y_valid,preds_valid)\n    print(\"Mean Squared Error Value : \"+\"{:.2f}\".format(mse))\n    print(\"R-Squared Value : \"+\"{:.2f}\".format(r_sq))\n    return model, mse, r_sq","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ridge_reg = linear_model.Ridge(solver = \"saga\", fit_intercept=False)\nprint(\"Ridge Regression (After advanced Text Pre-processing)\")\nprint(\"-----------------------------------------------------\")\nmodel_11, mse_11, r_sq_11 = run_model_advText(ridge_reg, X_train, y_train, X_valid, y_valid)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgbm_reg = lightgbm.LGBMRegressor()\nprint(\"LGBM Regression (After advanced Text Pre-processing)\")\nprint(\"----------------------------------------------------\")\nmodel_22, mse_22, r_sq_22 = run_model_advText(lgbm_reg, X_train, y_train, X_valid, y_valid)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_params = {'n_estimators':500, 'max_depth':8}\nxgb_reg = xgboost.XGBRegressor(**xgb_params)\nprint(\"XGB Regression (After advanced Text Pre-processing)\")\nprint(\"---------------------------------------------------\")\nmodel_33, mse_33, r_sq_33 = run_model_advText(xgb_reg, X_train, y_train, X_valid, y_valid)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Improvement in Model Performance after using Advanced Text Pre-Processing :\n* **MSE** has now decreased for all models\n* \\\\( R^2 \\\\) value has now increased for all models\n\nHowever, for Ridge model, the improvement seems the highest. Hence, we will choose **Ridge** to apply on the test dataset.","metadata":{}},{"cell_type":"code","source":"mse_before = [mse_1, mse_2, mse_3]\nr_sq_before = [r_sq_1, r_sq_2, r_sq_3]\nmse_after = [mse_11, mse_22, mse_33]\nr_sq_after = [r_sq_11, r_sq_22, r_sq_33]\nmodel_data = {'Model': ['Ridge','LGBM','XGB'],\n              'MSE_without_Advanced_TextProcessing': mse_before,\n              'R_Square_without_Advanced_TextProcessing': r_sq_before,\n              'MSE_with_Advanced_TextProcessing': mse_after,\n              'R_Square_with_Advanced_TextProcessing': r_sq_after}\ndata_compare = pd.DataFrame(model_data)\n\nimport matplotlib.pyplot as plt\n\nfig, (ax[0],ax[1]) = plt.subplots(1,2, figsize=(13,4))\nax[0]=data_compare.plot(kind='line', x='Model', y='MSE_without_Advanced_TextProcessing', color='DarkBlue', linewidth=0.7, marker='o', markersize=6, ax=ax[0])\nax[0]=data_compare.plot(kind='line', x='Model', y='MSE_with_Advanced_TextProcessing', secondary_y=False,color='Red', linewidth=0.7, marker='o', markersize=6, ax=ax[0])\nax[1]=data_compare.plot(kind='line', x='Model', y='R_Square_without_Advanced_TextProcessing', color='DarkBlue', linewidth=0.7, marker='^', markersize=6, ax=ax[1])\nax[1]=data_compare.plot(kind='line', x='Model', y='R_Square_with_Advanced_TextProcessing', secondary_y=False,color='Red', linewidth=0.7, marker='^', markersize=6, ax=ax[1])\n\nax[0].set_title(\"Improvement in MSE after Advanced Text Processing\", fontsize = 13)\nax[0].set_ylabel(\"Mean Square Error\")\nax[1].set_title(\"Improvement in R-Square after Advanced Text Processing\", fontsize = 13)\nax[1].set_ylabel(\"R-Square Value\")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Topic Modelling and LDA :\nA topic model examines a set of documents (or a simple text corpus) and discover the important topics based on the statistics of the words in each. The \"topics\" produced by topic modeling techniques are clusters of similar words ([Wikipedia](http://en.wikipedia.org/wiki/Topic_model)).  \n\nTopic Modelling is an **unsupervised concept**. Topic models are also referred to as \"probabilistic topic models\", which refers to statistical algorithms for discovering the latent semantic structures of an extensive text body. In the current age, the amount of the written material we encounter each day is simply beyond our processing capacity. Topic models can help to organize and offer insights for us to understand large collections of unstructured text bodies ([Wiki](http://en.wikipedia.org/wiki/Topic_model)). \n\nLDA (Latent Dirichlet Allocation) is a generative statistical model. In the initialization stage, each word is assigned to a random topic. Iteratively, the algorithm goes through each word and reassigns the word to a topic taking into consideration what is the probability of a new word belonging to a topic and what is the probability of the document (or text corpus) to be generated by a topic ([NLP for Hackers](https://nlpforhackers.io/topic-modeling/)).","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import LatentDirichletAllocation\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ncvectorizer = CountVectorizer(max_features=20000,stop_words='english',lowercase=True)\n\n# Fit it to dataset fraction\ncvz = cvectorizer.fit_transform(combined_frac['item_description'])\n\n# Initialize LDA Model with 10 Topics\nlda_model = LatentDirichletAllocation(n_components=10,random_state=12345)\n\n# Fit it to CountVectorizer Transformation\nX_topics = lda_model.fit_transform(cvz)\n\n# Define variables\nn_top_words = 10\ntopic_summaries = []\n\n# Get the topic words\ntopic_word = lda_model.components_\n\n# Get the vocabulary from the text features\nvocab = cvectorizer.get_feature_names()\n\n# Display the Topic Models\nfor i, topic_dist in enumerate(topic_word):\n    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n    topic_summaries.append(' '.join(topic_words))\n    print('Topic {}: {}'.format(i, ' | '.join(topic_words)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualizing LDA Topics :\nYou need to put \"1\",\"2\",\"3\",.........,\"10\" in place of \"Selected Topic\" and view top 30 most relevant terms for that particular topic in the right hand side. In the right hand side stacked bar graph, the sky blue bars represent overall term frequency and the red bars represent estimated term frequency within the selected topic.","metadata":{}},{"cell_type":"code","source":"import pyLDAvis.sklearn\npyLDAvis.enable_notebook()\npanel = pyLDAvis.sklearn.prepare(lda_model, cvz, cvectorizer, mds='tsne')\npanel","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Apply Final Ridge Model on Test Dataset and Submission :","metadata":{}},{"cell_type":"code","source":"predictions = ridge_reg.predict(X_test)\nsubmission[\"price\"] = np.expm1(predictions)\nsubmission.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}