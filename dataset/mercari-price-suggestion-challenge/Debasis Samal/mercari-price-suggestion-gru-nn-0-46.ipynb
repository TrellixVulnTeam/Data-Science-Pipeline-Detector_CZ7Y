{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Introduction\n\nIt can be hard to know how much something’s really worth. Small details can mean big differences in pricing. It can be hard to know how much something’s really worth. Small details can mean big differences in pricing. \n\nHere we are using the dataset proided by the Mercari, Japan’s biggest community-powered shopping app, to build a model which can offer pricing suggestions to sellers or even to the buyers to know if they are purchasing the product a correct price.\n\nHere, we will be using user-inputted text descriptions of the products, including details like product category name, brand name, and item condition.\n\n### Data\n\nThe files consist of a list of product listings. These files are tab-delimited.\n\n* train_id or test_id - the id of the listing\n* name - the title of the listing. \n* item_condition_id - the condition of the items provided by the seller\n* category_name - category of the listing\n* brand_name\n* price - the price that the item was sold for. This is the target variable that you will predict. The unit is USD. This column doesn't exist in test.tsv since that is what you will predict.\n* shipping - 1 if shipping fee is paid by seller and 0 by buyer\n* item_description - the full description of the item.","metadata":{}},{"cell_type":"markdown","source":"### Importing Libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport math\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Input, Dropout, Dense, BatchNormalization, Activation, concatenate, GRU, Embedding, Flatten, BatchNormalization\nfrom keras.models import Model\nfrom keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\nfrom keras import backend as K\n\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### File Installation\n\nInstalling file to extract the data from train test .tsv.7z file","metadata":{}},{"cell_type":"code","source":"!apt-get install p7zip\n!p7zip -d -f -k /kaggle/input/mercari-price-suggestion-challenge/train.tsv.7z\n!p7zip -d -f -k /kaggle/input/mercari-price-suggestion-challenge/test.tsv.7z\n!p7zip -d -f -k /kaggle/input/mercari-price-suggestion-challenge/sample_submission.csv.7z","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Unzipping dataset\n\nUnzipping he test andsample submission dataset","metadata":{}},{"cell_type":"code","source":"!unzip /kaggle/input/mercari-price-suggestion-challenge/sample_submission_stg2.csv.zip\n!unzip /kaggle/input/mercari-price-suggestion-challenge/test_stg2.tsv.zip","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Reading the Train and Test file","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv('train.tsv', sep='\\t')\ntrain_copy = train_data.copy()\ntest_data = pd.read_csv('test_stg2.tsv', sep='\\t')\ntest_copy = test_data.copy()\nprint(train_data.shape)\nprint(test_data.shape)\nprint(train_data.columns)\nprint(test_data.columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Exploratory data analysis","metadata":{}},{"cell_type":"code","source":"train_data.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = train_data.dropna()\ntest_data = test_data.dropna()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #HANDLE MISSING VALUES\n# print(\"Handling missing values...\")\n# def handle_missing(dataset):\n#     dataset.category_name.fillna(value=\"missing\", inplace=True)\n#     dataset.brand_name.fillna(value=\"missing\", inplace=True)\n#     dataset.item_description.fillna(value=\"missing\", inplace=True)\n#     return (dataset)\n\n# train_data = handle_missing(train_data)\n# test_data = handle_missing(test_data)\n# print(train_data.shape)\n# print(test_data.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = train_data.drop(['train_id'],axis=1)\ntest_data = test_data.drop(['test_id'],axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.category_name.value_counts()[train_data.category_name.value_counts() > 10000]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.brand_name.value_counts()[train_data.brand_name.value_counts() > 10000]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def split_cat(text):\n    try: return text.split(\"/\")\n    except: return (\"No Label\", \"No Label\", \"No Label\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['general_cat'], train_data['subcat_1'], train_data['subcat_2'] = \\\nzip(*train_data['category_name'].apply(lambda x: split_cat(x)))\ntrain_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data['general_cat'], test_data['subcat_1'], test_data['subcat_2'] = \\\nzip(*test_data['category_name'].apply(lambda x: split_cat(x)))\ntest_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = train_data.drop(['category_name'],axis=1)\ntest_data = test_data.drop(['category_name'],axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"There are %d unique first sub-categories.\" % train_data['subcat_1'].nunique())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"There are %d unique first sub-categories.\" % train_data['subcat_2'].nunique())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### EDA Through Visualization","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(16,5))\nsns.countplot(x = 'general_cat', data = train_data, order = train_data['general_cat'].value_counts().index, palette = 'Spectral_r')\nplt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(16,5))\nsns.countplot(x = 'subcat_1', data = train_data, order = train_data['subcat_1'].value_counts().iloc[:10].index, palette = 'mako')\nplt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(16,5))\nsns.countplot(x = 'subcat_2', data = train_data, order = train_data['subcat_2'].value_counts().iloc[:10].index, palette = 'rocket')\nplt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,5))\ndf = train_data.groupby('brand_name')['price'].mean().reset_index()\nbrands = list(train_data.brand_name.value_counts()[train_data.brand_name.value_counts() > 1000].index)\ndata = {'brand_name':  [df[df['brand_name']==i].values[0][0] for i in brands],\n        'price': [df[df['brand_name']==i].values[0][1] for i in brands]}\ndf = pd.DataFrame(data).sort_values('price',ascending=False)[:10]\nsns.barplot(data=df, x='brand_name', y='price', palette = 'icefire')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,5))\ndf = train_data.groupby('brand_name')['price'].mean().reset_index()\nsns.barplot(data=df, x='brand_name', y='price', order = train_data['brand_name'].value_counts().iloc[:10].index, palette = 'Spectral')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,5))\nlen_desc = [len(i.split()) for i in train_data['item_description']]\nsns.lineplot(data=train_data, x=len_desc, y='price')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,5))\ndf = train_data.groupby('general_cat')['price'].mean().reset_index()\ndf = df.sort_values('price')\nsns.lineplot(data=df, x='general_cat', y='price')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Women, Beauty and kids are the most selling general category items.\n* Athletic appareal, Shoes, Makeup makes up the highest selling items.\n* Louis Vuitton, Gucci, Air Jordan, Tiffany & Co. and Tory Burch are the top 5 expensive brand on an average with more than 1000 products sold. Which makes sense as they are the most branded, luxurious and famous fashion companies.\n* We checked if the lemgth of the description impacts the price of the product and weuldn't find any relation between them and which a good thing.\n* From the genalroduct category, we found that the electronics items are the most expensive on.","metadata":{}},{"cell_type":"markdown","source":"### PROCESS CATEGORICAL DATA","metadata":{}},{"cell_type":"code","source":"print(\"Handling categorical variables...\")\nle = LabelEncoder()\n\nle.fit(np.hstack([train_data.general_cat, test_data.general_cat]))\ntrain_data.general_cat = le.transform(train_data.general_cat)\ntest_data.general_cat = le.transform(test_data.general_cat)\n\nle.fit(np.hstack([train_data.brand_name, test_data.brand_name]))\ntrain_data.brand_name = le.transform(train_data.brand_name)\ntest_data.brand_name = le.transform(test_data.brand_name)\n\nle.fit(np.hstack([train_data.subcat_1, test_data.subcat_1]))\ntrain_data.subcat_1 = le.transform(train_data.subcat_1)\ntest_data.subcat_1 = le.transform(test_data.subcat_1)\n\nle.fit(np.hstack([train_data.subcat_2, test_data.subcat_2]))\ntrain_data.subcat_2 = le.transform(train_data.subcat_2)\ntest_data.subcat_2 = le.transform(test_data.subcat_2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Processing the text","metadata":{}},{"cell_type":"code","source":"raw_text = np.hstack([train_data.item_description.str.lower(), train_data.name.str.lower()])\n\nprint(\"   Fitting tokenizer...\")\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(raw_text)\nprint(\"   Transforming text to seq...\")\n\ntrain_data[\"seq_item_description\"] = tokenizer.texts_to_sequences(train_data.item_description.str.lower())\ntest_data[\"seq_item_description\"] = tokenizer.texts_to_sequences(test_data.item_description.str.lower())\ntrain_data[\"seq_name\"] = tokenizer.texts_to_sequences(train_data.name.str.lower())\ntest_data[\"seq_name\"] = tokenizer.texts_to_sequences(test_data.name.str.lower())\ntrain_data.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Sequences variable analysis","metadata":{}},{"cell_type":"code","source":"max_name_seq = np.max([np.max(train_data.seq_name.apply(lambda x: len(x))), np.max(test_data.seq_name.apply(lambda x: len(x)))])\nmax_item_description_seq = np.max([np.max(train_data.seq_item_description.apply(lambda x: len(x)))\n                                   , np.max(test_data.seq_item_description.apply(lambda x: len(x)))])\nprint(\"max name seq \"+str(max_name_seq))\nprint(\"max item desc seq \"+str(max_item_description_seq))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.seq_name.apply(lambda x: len(x)).hist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.seq_item_description.apply(lambda x: len(x)).hist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Based on the histograms we will select the next length","metadata":{}},{"cell_type":"code","source":"#EMBEDDINGS MAX VALUE\nMAX_NAME_SEQ = 10\nMAX_ITEM_DESC_SEQ = 75\nMAX_TEXT = np.max([np.max(train_data.seq_name.max()), np.max(test_data.seq_name.max())\n                  , np.max(train_data.seq_item_description.max()), np.max(test_data.seq_item_description.max())])+3\nMAX_GENERAL_CAT = np.max([train_data.general_cat.max(),test_data.general_cat.max()])+1\nMAX_SUBCAT_1 = np.max([train_data.subcat_1.max(),test_data.subcat_1.max()])+1\nMAX_SUBCAT_2 = np.max([train_data.subcat_2.max(),test_data.subcat_2.max()])+1\nMAX_BRAND = np.max([train_data.brand_name.max(), test_data.brand_name.max()])+1\nMAX_CONDITION = np.max([train_data.item_condition_id.max(), test_data.item_condition_id.max()])+1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Normalizing target value","metadata":{}},{"cell_type":"code","source":"#SCALE target variable\ntrain_data[\"target\"] = np.log(train_data.price+1)\ntarget_scaler = MinMaxScaler(feature_range=(-1, 1))\ntrain_data[\"target\"] = target_scaler.fit_transform(np.array(train_data.target).reshape(-1,1))\npd.DataFrame(train_data.target).hist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* It is a good practice to normalize the target vairable between -1 and 1, if we are specially dealing with regression data with equential model such as LSTM, GRU or RNN ","metadata":{}},{"cell_type":"markdown","source":"### Splitting of the data","metadata":{}},{"cell_type":"code","source":"#splitting the data\ntrain, valid = train_test_split(train_data, random_state=123, test_size=0.01)\nprint(train.shape)\nprint(valid.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Keras data defination and padding of the text sequences ","metadata":{}},{"cell_type":"code","source":"def get_keras_data(dataset):\n    X = {\n        'name': pad_sequences(dataset.seq_name, maxlen=MAX_NAME_SEQ),\n        'item_desc': pad_sequences(dataset.seq_item_description, maxlen=MAX_ITEM_DESC_SEQ),\n        'brand_name': np.array(dataset.brand_name),\n        'general_cat': np.array(dataset.general_cat),\n        'subcat_1': np.array(dataset.subcat_1),\n        'subcat_2': np.array(dataset.subcat_2),\n        'item_condition': np.array(dataset.item_condition_id),\n        'shipping': np.array(dataset[[\"shipping\"]])\n    }\n    return X\n\nX_train = get_keras_data(train)\nX_valid = get_keras_data(valid)\nX_test = get_keras_data(test_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Keras model Defination","metadata":{}},{"cell_type":"code","source":"def get_callbacks(filepath, patience=2):\n    es = EarlyStopping('val_loss', patience=patience, mode=\"min\")\n    msave = ModelCheckpoint(filepath, save_best_only=True)\n    return [es, msave]\n\ndef rmsle_cust(y_true, y_pred):\n    first_log = K.log(K.clip(y_pred, K.epsilon(), None) + 1.)\n    second_log = K.log(K.clip(y_true, K.epsilon(), None) + 1.)\n    return K.sqrt(K.mean(K.square(first_log - second_log), axis=-1))\n\ndef get_model():\n    #params\n    dr_r = 0.1\n    \n    #Inputs\n    name = Input(shape=[X_train[\"name\"].shape[1]], name=\"name\")\n    item_desc = Input(shape=[X_train[\"item_desc\"].shape[1]], name=\"item_desc\")\n    brand_name = Input(shape=[1], name=\"brand_name\")\n    general_cat = Input(shape=[1], name = 'general_cat')\n    subcat_1 = Input(shape=[1], name = 'subcat_1')\n    subcat_2 = Input(shape=[1], name = 'subcat_2')\n    item_condition = Input(shape=[1], name=\"item_condition\")\n    shipping = Input(shape=[X_train[\"shipping\"].shape[1]], name=\"shipping\")\n    \n    #Embeddings layers\n    emb_name = Embedding(MAX_TEXT, 50)(name)\n    emb_item_desc = Embedding(MAX_TEXT, 50)(item_desc)\n    emb_brand_name = Embedding(MAX_BRAND, 10)(brand_name)\n    emb_general_cat = Embedding(MAX_GENERAL_CAT, 10)(general_cat)\n    emb_subcat_1 = Embedding(MAX_SUBCAT_1, 10)(subcat_1)\n    emb_subcat_2 = Embedding(MAX_SUBCAT_2, 10)(subcat_2)\n    emb_item_condition = Embedding(MAX_CONDITION, 5)(item_condition)\n    \n    #rnn layer\n    rnn_layer1 = GRU(16) (emb_item_desc)\n    rnn_layer2 = GRU(8) (emb_name)\n    \n    #main layer\n    main_l = concatenate([Flatten() (emb_brand_name), \n                          Flatten() (emb_general_cat),\n                          Flatten() (emb_subcat_1),\n                          Flatten() (emb_subcat_2),\n                          Flatten() (emb_item_condition), \n                          rnn_layer1, \n                          rnn_layer2, \n                          shipping])\n    main_l = Dropout(dr_r) (Dense(128) (main_l))\n    main_l = Dropout(dr_r) (Dense(64) (main_l))\n    \n    #output\n    output = Dense(1, activation=\"linear\") (main_l)\n    \n    #model\n    model = Model([name, item_desc, brand_name\n                   , general_cat, subcat_1, subcat_2, item_condition, shipping], output)\n    model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mae\", rmsle_cust])\n    \n    return model\n\n    \nmodel = get_model()\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Fitting the model","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 20000\nepochs = 10\n\nmodel = get_model()\nmodel.fit(X_train, train.target, epochs=epochs, batch_size=BATCH_SIZE\n          , validation_data=(X_valid, valid.target), verbose = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Root Mean Squared Logarithmic Error","metadata":{}},{"cell_type":"code","source":"def rmsle(y, y_pred):\n    assert len(y) == len(y_pred)\n    to_sum = [(math.log(y_pred[i] + 1) - math.log(y[i] + 1)) ** 2.0 for i,pred in enumerate(y_pred)]\n    return (sum(to_sum) * (1.0/len(y))) ** 0.5\n#Source: https://www.kaggle.com/marknagelberg/rmsle-function","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluating the model on the validation dataset \nHow is it doing?","metadata":{}},{"cell_type":"code","source":"val_preds = model.predict(X_valid)\nval_preds = target_scaler.inverse_transform(val_preds)\nval_preds = np.exp(val_preds)+1\n\n#mean_absolute_error, mean_squared_log_error\ny_true = np.array(valid.price.values)\ny_pred = val_preds[:,0]\nv_rmsle = rmsle(y_true, y_pred)\nprint(\" RMSLE error on dev test: \"+str(v_rmsle))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(10):\n    print('')\n    print('')\n    if int((int(np.array(valid.price)[i])-int(val_preds[i]))/int(np.array(valid.price)[i])*100) >= -5 and (int(np.array(valid.price)[i])-int(val_preds[i]))/int(np.array(valid.price)[i])*100 <= 30:\n        print('\\033[92m Fair priced')\n    elif int((int(np.array(valid.price)[i])-int(val_preds[i]))/int(np.array(valid.price)[i])*100) < -5:\n        print('\\033[93m Over priced')\n    else:\n        print('\\033[91m Under priced')\n\n    print('Product name: ',np.array(valid.name)[i])\n    print('Prodct actual price: ', int(np.array(valid.price)[i]))\n    print('Product predicted price: ',int(val_preds[i]))\n    print('Change in price percentage:',int((int(np.array(valid.price)[i])-int(val_preds[i]))/int(np.array(valid.price)[i])*100))\n    print('*****************************************************************************')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}