{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import nltk\nimport string\nimport re\nimport itertools\nimport numpy as np\nimport pandas as pd\nimport pickle\n#import lda\n\nfrom operator import itemgetter\n\nfrom nltk.stem.porter import *\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.corpus import stopwords\n\nfrom sklearn.pipeline import make_pipeline, make_union, Pipeline\nfrom sklearn.preprocessing import FunctionTransformer, StandardScaler\nfrom sklearn.feature_extraction import stop_words\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.ensemble import RandomForestRegressor, BaggingRegressor\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn import tree\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nPATH = \"../input/\"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-output":false,"_kg_hide-input":false},"cell_type":"code","source":"nsamples = None\ntrain_raw = pd.read_csv(f'{PATH}train.tsv', sep='\\t', nrows=nsamples)\ntest_raw = pd.read_csv(f'{PATH}test_stg2.tsv', sep='\\t', nrows=nsamples)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bd1f1389c05e6427f91a84f9e4790f19d450d637"},"cell_type":"code","source":"# sentence preprocessor\nstop = set(stopwords.words('english'))\nregex = re.compile('[' +re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]')\n    \n# ストップワードに入ってたりアルファベット以外で始まったり短かったりするやつはだめ\ndef check_word(w):\n    return w not in stop and re.search('[a-zA-Z]', w) and len(w)>=3\n\n# だめな単語を除去した文章を返す\ndef preprocess_sentence(text: str) -> str:\n    text = regex.sub(\" \", text) # remove punctuation\n    tokens = filter(check_word, (w.lower() for w in text.split()))\n    return \" \".join(tokens)\n\n# カテゴリ分割(使ってない)\ndef split_cat(text):\n    try:\n        return text.split(\"/\")\n    except:\n        return (\"\", \"\", \"\")\n\n# Preprocessor\n# 次の２つを処理してそれぞれpreprocess_sentenceにかけ、\"name\", \"text\"にする\n# - 名前・ブランド名(欠損値を\"\"で埋める)を結合したもの\n# - すべての自然言語でできた情報を結合したもの\n# \"category\"はユニークな番号をふる\ndef preprocess(df: pd.DataFrame) -> pd.DataFrame:\n    df[\"name\"] = (df['name'].fillna('')\n                  + ' ' + df['brand_name'].fillna('')).apply(preprocess_sentence)\n    df[\"text\"] = (df['name'].fillna('')\n                  + ' '+ df['category_name'].fillna('')\n                  + ' '+ df['item_description'].fillna('')).apply(preprocess_sentence)\n    df[\"category\"] = df[\"category_name\"].astype(\"category\").cat.codes\n    return df[['name', 'text', 'shipping', 'item_condition_id', 'category']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c85c577452926348835c616a4d5b68e3932ac6f7"},"cell_type":"code","source":"# DataFrameの特定の列を拾ってきて、第二引数以降で与えられる処理を行うパイプラインを作る関数\ndef on_field(field: str, *vec) -> Pipeline:\n    return make_pipeline(FunctionTransformer(itemgetter(field), validate=False), *vec)\n\n# 引数に与えられた処理をした後、特異値分解で次元を削減する関数\ndef with_svd(*vec) -> Pipeline:\n    return make_pipeline(*vec, TruncatedSVD(n_components=5))\n\ntfidf_max_features = 100000\n\n# 以下の処理をすべて行い、横に結合したベクトルを作るFeatureUnion\n# - \"name\"を拾ってきてTF-IDFにかけ、次元削減\n# - \"text\"を拾ってきてTF-IDFにかけ、次元削減\n# - その他はそのまま値として出す\n# 結果としては1サンプルあたり13次元のベクトルになる\n# ついでに並列処理\nvectorizer = make_union(\n    on_field(\"name\", with_svd(TfidfVectorizer(max_features=tfidf_max_features))),\n    on_field(\"text\", with_svd(TfidfVectorizer(max_features=tfidf_max_features,\n                                              ngram_range=(1, 2)))),\n    on_field([\"shipping\", \"item_condition_id\", \"category\"]),\n    n_jobs=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"de3aea07168a291307447543319112e35c1e9cf4"},"cell_type":"code","source":"train = preprocess(train_raw)\ntest = preprocess(test_raw)\ncombined = pd.concat([train, test])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4b0e44d71044887652f309c4e4128adda9453a18","scrolled":false},"cell_type":"code","source":"vec = vectorizer.fit_transform(combined)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0b13285d9ebb42c5d7a8ad38d2e9398cc18ce229"},"cell_type":"code","source":"train_x, test_x= vec[:len(train.index)], vec[len(train.index):]\ntrain_t = np.log(train_raw[\"price\"] + 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a93f6764bc4f938c010db583855a21a8cefe857c","scrolled":false},"cell_type":"code","source":"# model = RandomForestRegressor(n_jobs=4, min_samples_leaf=5, n_estimators=200)\nmodel = BaggingRegressor(tree.DecisionTreeRegressor(), n_estimators=100, max_samples=0.9)\nmodel.fit(train_x, train_t)\nmodel.score(train_x, train_t)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"baeafc58de2008411f66b8ded20c2aaa1123fe5c"},"cell_type":"code","source":"preds = model.predict(test_x)\npreds = pd.Series(np.exp(preds) - 1)\n\nsubmit = pd.concat([test_raw.test_id, preds], axis=1)\nsubmit.columns = ['test_id', 'price']\nsubmit.to_csv('submit_rf_base.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1642ea87ba0831d75a07bf74a6c2420d42c68d35"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}