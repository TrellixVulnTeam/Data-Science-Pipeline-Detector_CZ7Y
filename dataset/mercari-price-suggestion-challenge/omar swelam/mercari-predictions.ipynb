{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!apt-get install p7zip\n!p7zip -d -f -k /kaggle/input/mercari-price-suggestion-challenge/train.tsv.7z\n!p7zip -d -f -k /kaggle/input/mercari-price-suggestion-challenge/test.tsv.7z\n!p7zip -d -f -k /kaggle/input/mercari-price-suggestion-challenge/sample_submission.csv.7z","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = pd.read_csv('train.tsv', sep='\\t')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = train_data.sample(300000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"our understanding for the problem will makes us not drop the `brand_name` but the null values in the brand name would be an incentive to use additional features along with it."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Checking target values"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.price.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.price.hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(np.log1p(train_data.price))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Due to the large skewness in the data we will use the log for the evaluation of the target."},{"metadata":{},"cell_type":"markdown","source":"# Pipeline"},{"metadata":{},"cell_type":"markdown","source":"Our preprocessing will include the following steps:\n\n1. **imputation**  \n1. **split category**\n1. **minmax scaler**  \n1. **onehot encoder** \n1. **text preprocessing**"},{"metadata":{},"cell_type":"markdown","source":"## brand_name and category_name"},{"metadata":{},"cell_type":"markdown","source":"Capitalization of brandnames and categories"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['category_name'] = train_data['category_name'].str.lower()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['brand_name'] = train_data['brand_name'].str.lower()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['brand_name'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the category names consists of 3 hierarchical levels "},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['category_name'].str.split('/', expand=True).info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['category_name'].str.split('/',n=2, expand=True).info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When it comes to brand_name we can see that there a lot of brands that are not frequent and will cause an explosion when using one-hot encoding so we will select the most frequent ones only."},{"metadata":{"trusted":true},"cell_type":"code","source":"(train_data['brand_name'].value_counts() > 100).sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"frequent_brands = list(train_data['brand_name'].value_counts()[train_data['brand_name'].value_counts() > 100].index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['price'].describe(percentiles=[0.75,0.9,0.95])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data[train_data['brand_name'].isin(frequent_brands)].query('price<50 & price>30').nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So frequent are well represented in ranges of 30 to 50"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data[train_data['brand_name'].isin(frequent_brands)].query('price>250').nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data[~train_data['brand_name'].isin(frequent_brands)].query('price>250').nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"But it seems that some other brands are found in higher prices categories so we will add other prices"},{"metadata":{"trusted":true},"cell_type":"code","source":"frequent_brands = frequent_brands + list(train_data[~train_data['brand_name'].isin(frequent_brands) & ~train_data['brand_name'].isna()].query('price>250').brand_name.unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['brand_name'] = train_data['brand_name'].apply(lambda x: x if x in frequent_brands else 'Others')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(frequent_brands)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When inspecting the categories, we splot them into 3 main levels, but we will fill the unknown values first."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.fillna({'category_name':'Unknown/Unknown/Unknown', 'brand_name':'Unknown', 'name':'Unknown', 'item_description':'Unknown'}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = pd.concat([train_data.drop('category_name', axis=1),train_data['category_name'].str.split('/',n=2, expand=True)], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.rename({0: 'cat_0', 1: 'cat_1', 2: 'cat_2'}, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['cat_2'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['cat_2'].value_counts()[train_data['cat_2'].value_counts() > 100]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data[train_data['cat_2'].isin(train_data['cat_2'].value_counts()[train_data['cat_2'].value_counts() < 100].index)].price.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the prices distribution isn't variant for cat_2 that are not frequent, we will ignore them."},{"metadata":{"trusted":true},"cell_type":"code","source":"frequent_cat2 = list(train_data['cat_2'].value_counts()[train_data['cat_2'].value_counts() > 100].index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"frequent_cat2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['cat_2'] = train_data['cat_2'].apply(lambda x: x if x in frequent_cat2 else 'Others')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['cat_1'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"log_prices = np.log1p(train_data.price)\ntrain_data.drop(['price','train_id'], axis=1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In order to get an accurate representation for the test data, we will use stratified sampling based on the subcategories."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedShuffleSplit\n\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=42)\nfor train_index, test_index in split.split(train_data, train_data['cat_1']):\n    train_idx = train_index\n    test_idx = test_index","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Text manipulation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.preprocessing import MinMaxScaler, LabelBinarizer\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['name'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['name'].str.len().describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['item_description'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['item_description'].str.len().describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize \nfrom nltk.stem import PorterStemmer\nimport re\nimport string","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ps = PorterStemmer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"re_punc = re.compile('[%s]' % re.escape(string.punctuation))\nst = ' '.join(word_tokenize(\"KYLIE(TRUE BROWN K) MATTE LIPSTICK&LINER\"))\nst = re_punc.sub('', st)\nst = ' '.join([ps.stem(w) for w in st.split()])\nvecto = CountVectorizer(lowercase=True,stop_words='english')\ncntrr = vecto.fit_transform([st])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"st","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vecto.get_feature_names()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ps = PorterStemmer()\nre_punc = re.compile('[%s]' % re.escape(string.punctuation))\ndef preprocess_text(X):\n    X = X.lower()\n    X = ' '.join(word_tokenize(X))\n    X = re_punc.sub('', X)\n    X = ' '.join([ps.stem(w) for w in X.split()])\n    return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocess_text(\"KYLIE(TRUE BROWN K) MATTE LIPSTICK&LINER\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['item_description'] = train_data['item_description'].apply(preprocess_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['name'] = train_data['name'].apply(preprocess_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['item_description']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['name']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Final preprocessed data representation"},{"metadata":{},"cell_type":"markdown","source":"Since in the names we seek the type of the product regardless how many times this product was found in the corpus (items list) so we will use CountVectorizer when it comes to the name. However, we will use TF-IDF for the description for emphasis on genuine descriptions for the item."},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf_desc = TfidfVectorizer(max_features=50000, ngram_range=(1, 1), stop_words='english', norm='l2',lowercase=True)\nname_vectorizer = CountVectorizer(stop_words='english')\n\nX_train_descp = tfidf_desc.fit_transform(train_data['item_description'])\nX_train_name = name_vectorizer.fit_transform(train_data['name'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_name.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Avoid using get_dummies because of memory inefficient usage. So we use LabelBinarizer instead.  \n`X_brand_cat = pd.get_dummies(train_data[['brand_name', 'cat_0', 'cat_1', 'cat_2']]).values`"},{"metadata":{"trusted":true},"cell_type":"code","source":"lb_brand_name = LabelBinarizer(sparse_output=True)\nX_train_brand = lb_brand_name.fit_transform(train_data['brand_name'])\n\nlb_shipping = LabelBinarizer(sparse_output=True)\nX_train_shipping = lb_shipping.fit_transform(train_data['shipping'])\n\nlb_cat_0 = LabelBinarizer(sparse_output=True)\nX_train_cat_0 = lb_cat_0.fit_transform(train_data['cat_0'])\n\nlb_cat_1 = LabelBinarizer(sparse_output=True)\nX_train_cat_1 = lb_cat_1.fit_transform(train_data['cat_1'])\n\nlb_cat_2 = LabelBinarizer(sparse_output=True)\nX_train_cat_2 = lb_cat_2.fit_transform(train_data['cat_2'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = MinMaxScaler()\nX_train_item_condition_id = scaler.fit_transform(train_data['item_condition_id'].values.reshape(-1,1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.sparse import hstack\n\nX_train = hstack((X_train_name, X_train_descp, X_train_brand, X_train_item_condition_id, X_train_shipping, X_train_cat_0, X_train_cat_1, X_train_cat_2)).tocsr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train[train_idx]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"log_prices","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = X_train[test_idx]\nlog_test_prices = log_prices.iloc[test_idx]\ntrain_data = X_train[train_idx]\nlog_train_prices = log_prices.iloc[train_idx]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Another preprocessing pipeline approach"},{"metadata":{},"cell_type":"markdown","source":"I tried using preprocessing pipeline, but conversion from dataframes to numpy arrays for such large scale values (due to the abundance of expanded features) caused the failure to due excessive memory usage, thus we only referred to using Sklearn CSR matrices."},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.preprocessing import FunctionTransformer, MinMaxScaler, OneHotEncoder\n# from sklearn.impute import SimpleImputer\n# from sklearn.pipeline import Pipeline, FeatureUnion\n# from sklearn.compose import ColumnTransformer\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def categorical_imputer(X, nlp=False):\n#     if nlp:\n#         return X.fillna({'category_name':'Unknown/Unknown/Unknown','name':'Unknown','item_description':'Unknown', 'brand_name':'Unknown'})\n#     else:\n#         return X.fillna({'category_name':'Unknown/Unknown/Unknown', 'brand_name':'Unknown'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def split_cat_name(X):\n#     split_cat = X.fillna({'category_name':'Unknown/Unknown/Unknown'})['category_name'].str.split('/',n=2, expand=True)\n#     split_cat.rename({0: 'cat_0', 1: 'cat_1', 2: 'cat_2'}, axis=1, inplace=True)\n#     return pd.concat([X.drop('category_name', axis=1),split_cat], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def bin_encoder(X):\n#     ls = list(X.columns)\n#     return pd.get_dummies(X[ls])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def cat_cols(nlp=False):\n#     if nlp:\n#         return ['name', 'category_name', 'brand_name', 'item_description']\n#     else:\n#         return ['category_name', 'brand_name']\n\n# numeric_cols = list(train_data.select_dtypes(include=np.number).columns)\n# cat_columns =  cat_cols(nlp=False)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def select_numeric(X):\n#     return X[numeric_cols]\n# def select_categorical(X):\n#     return X[cat_columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# num_pipeline = Pipeline([\n#         ('selector',FunctionTransformer(select_numeric, validate=False)),\n#         ('imputer', SimpleImputer(strategy=\"most_frequent\")),\n#         ('min_max_scaler', MinMaxScaler()),\n#     ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cat_pipeline = Pipeline([\n#         ('selector',FunctionTransformer(select_categorical, validate=False)),\n#         ('imputer',  FunctionTransformer(categorical_imputer, validate=False,\n#                                          kw_args={\"nlp\": False})),\n#         ('splitter', FunctionTransformer(split_cat_name, validate=False)),\n#         ('cat_encoder', FunctionTransformer(bin_encoder, validate=False)),\n#     ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.model_selection import StratifiedShuffleSplit\n\n# split = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=42)\n# for train_index, test_index in split.split(train_data, train_data['category_name'].str.split('/',n=2, expand=True)[1]):\n#     test_data = train_data.loc[test_index]\n#     log_test_prices = log_prices[test_index]\n#     train_data = train_data.loc[train_index]\n#     log_train_prices = log_prices[train_index]\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_data = train_data[cat_cols(nlp=False)+numeric_cols]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# full_pipeline = FeatureUnion(transformer_list=[\n#         (\"num_pipeline\", num_pipeline),\n#         (\"cat_pipeline\", cat_pipeline),\n#     ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# full_pipeline = ColumnTransformer([\n#         (\"num\", num_pipeline, numeric_cols),\n#         (\"cat\", cat_pipeline, cat_cols(nlp=False)),\n#         ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X = full_pipeline.fit_transform(train_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling"},{"metadata":{},"cell_type":"markdown","source":"In the modelling approach we will try multiple algorithms and perform hyperparameter tuning for tree based regression methods to select the best boosting parameters."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression, SGDRegressor, Lasso, Ridge, ElasticNet\nfrom sklearn.svm import LinearSVR, SVR\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nimport time","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LinearRegression"},{"metadata":{},"cell_type":"markdown","source":"The `LinearRegression` took to much to run and eventually didn't converge (more than an hour) "},{"metadata":{"trusted":true},"cell_type":"code","source":"# lin_reg = LinearRegression()\n# lin_reg.fit(train_data, log_train_prices)\n# lin_reg.score(train_data, log_train_prices)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# mean_absolute_error(lin_reg.predict(train_data),log_train_prices)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# mean_absolute_error(lin_reg.predict(test_data),log_test_prices)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Lasso"},{"metadata":{},"cell_type":"markdown","source":"It took too much time to fit althogh I have set `the max_iter` to 10, so, I used ridge instead"},{"metadata":{"trusted":true},"cell_type":"code","source":"# lasso_lin_reg = Lasso(alpha=0.01 , max_iter=10)\n# lasso_lin_reg.fit(train_data, log_train_prices)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# mean_absolute_error(lasso_lin_reg.predict(train_data),log_train_prices)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# mean_absolute_error(lasso_lin_reg.predict(test_data),log_test_prices)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## SGDRegressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"t = time.time()\nsgd_lin_reg = SGDRegressor(penalty='l2',  alpha=1, max_iter=400, early_stopping=False, learning_rate='invscaling', eta0=0.01, verbose=False)\nsgd_lin_reg.fit(train_data, log_train_prices)\nelapsed = time.time() - t\nprint(\"elapsed training time is \"+ str(elapsed))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Training mean absolute error is \"+ str(mean_absolute_error(sgd_lin_reg.predict(train_data),log_train_prices)))\nprint(\"Testing mean absolute error is \"+ str(mean_absolute_error(sgd_lin_reg.predict(test_data),log_test_prices)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ridge"},{"metadata":{"trusted":true},"cell_type":"code","source":"t = time.time()\nridge_lin_reg = Ridge(alpha=4, max_iter= 500)\nridge_lin_reg.fit(train_data, log_train_prices)\nelapsed = time.time() - t\nprint(\"elapsed training time is \"+ str(elapsed))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Training mean absolute error is \"+ str(mean_absolute_error(ridge_lin_reg.predict(train_data),log_train_prices)))\nprint(\"Testing mean absolute error is \"+ str(mean_absolute_error(ridge_lin_reg.predict(test_data),log_test_prices)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Linear SVR"},{"metadata":{"trusted":true},"cell_type":"code","source":"t = time.time()\nlin_SVR = LinearSVR(max_iter=500, C=0.1)\nlin_SVR.fit(train_data, log_train_prices)\nelapsed = time.time() - t\nprint(\"elapsed training time is \"+ str(elapsed))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Training mean absolute error is \"+ str(mean_absolute_error(lin_SVR.predict(train_data),log_train_prices)))\nprint(\"Testing mean absolute error is \"+ str(mean_absolute_error(lin_SVR.predict(test_data),log_test_prices)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## SVR"},{"metadata":{"trusted":true},"cell_type":"code","source":"t = time.time()\nkernel_SVR = SVR(kernel='rbf', verbose=True, max_iter= 1000)\nkernel_SVR.fit(train_data, log_train_prices)\nelapsed = time.time() - t\nprint(\"elapsed training time is \"+ str(elapsed))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Training mean absolute error is \"+ str(mean_absolute_error(kernel_SVR.predict(train_data),log_train_prices)))\nprint(\"Testing mean absolute error is \"+ str(mean_absolute_error(kernel_SVR.predict(test_data),log_test_prices)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## RandomForest"},{"metadata":{"trusted":true},"cell_type":"code","source":"t = time.time()\nforest_reg = RandomForestRegressor(verbose=True, max_depth=15, n_estimators=30)\nforest_reg.fit(train_data, log_train_prices)\nelapsed = time.time() - t\nprint(\"elapsed training time is \"+ str(elapsed))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Training mean absolute error is \"+ str(mean_absolute_error(forest_reg.predict(train_data),log_train_prices)))\nprint(\"Testing mean absolute error is \"+ str(mean_absolute_error(forest_reg.predict(test_data),log_test_prices)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LGBMRegressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"from lightgbm import LGBMRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lgbm_model = LGBMRegressor(n_estimators=200, learning_rate=0.5, num_leaves=125)\nt = time.time()\nlgbm_model = LGBMRegressor(verbose=0, max_depth=15, n_estimators=30)\nlgbm_model.fit(train_data, log_train_prices, verbose=0)\nelapsed = time.time() - t\nprint(\"elapsed training time is \"+ str(elapsed))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Training mean absolute error is \"+ str(mean_absolute_error(lgbm_model.predict(train_data),log_train_prices)))\nprint(\"Testing mean absolute error is \"+ str(mean_absolute_error(lgbm_model.predict(test_data),log_test_prices)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Hyperparameter tuning"},{"metadata":{},"cell_type":"markdown","source":"Given the excessive time needed to train a simple random forest algorithm with only 30 estimators at max depth of 15 lead to excluding it for our randomized search since LightGBM resulted in similar evaluastion metrics vlue but in much more efficient time.  \nConsequently, we will use LightGBM for randomized search of optimal hyperparamters and use it for detecting feature importance."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import PredefinedSplit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"split_index = [-1 if x in train_idx else 0 for x in range(X_train.shape[0])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\n\nparam_distribs = {\n        'n_estimators': randint(low=5, high=200),\n        'max_depth': randint(low=10, high=100),\n    }\n\nlgbm_srch = LGBMRegressor()\npds = PredefinedSplit(test_fold = split_index)\n\nrnd_search = RandomizedSearchCV(lgbm_srch, param_distributions=param_distribs,\n                                n_iter=20, cv=pds, scoring='neg_mean_squared_error')\n\nrnd_search.fit(X_train, log_prices)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cvres = rnd_search.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-mean_score), params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Testing mean absolute error for best estimator is \"+ str(mean_absolute_error(rnd_search.best_estimator_.predict(test_data),log_test_prices)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importances = rnd_search.best_estimator_.feature_importances_\nfeature_importances","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_of_features_shapes = [X_train_name, X_train_descp, X_train_brand, X_train_item_condition_id, X_train_shipping, X_train_cat_0, X_train_cat_1, X_train_cat_2]\nlist_of_features = [\"name\",\"item_description\",\"brand_name\",\"item_condition_id\",\"shipping\",\"cat_0\",\"cat_1\",\"cat_2\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_range = {}\nfor i in range(len(list_of_features)):\n    if i == 0:\n        cat_range[list_of_features[i]] = (0 , list_of_features_shapes[i].shape[1])\n    elif i == len(list_of_features)-1:\n        cat_range[list_of_features[i]] = (cat_range[list_of_features[i-1]][1]+1 , X_train.shape[1])\n    else:\n        cat_range[list_of_features[i]] = (cat_range[list_of_features[i-1]][1]+1 , cat_range[list_of_features[i-1]][1]+list_of_features_shapes[i].shape[1])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_imp_cat = []\nfor feature_idx in feature_importances.argsort()[-40:]:\n    for key, value in cat_range.items():\n        if value[0] <= feature_idx <= value[1]: \n            feature_imp_cat.append(key)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_imp_cat","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's clear that the categories are the most dominant when it comes to their importance in defining the price of the product."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}