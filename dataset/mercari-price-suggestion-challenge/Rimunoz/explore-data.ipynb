{"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"},"language_info":{"version":"3.6.3","nbconvert_exporter":"python","file_extension":".py","pygments_lexer":"ipython3","mimetype":"text/x-python","name":"python","codemirror_mode":{"version":3,"name":"ipython"}}},"nbformat_minor":1,"cells":[{"cell_type":"code","metadata":{"_uuid":"fd2c925daba048dca542a68c9520cd2642b33775","_cell_guid":"66bbc270-79ff-4156-bdfe-f3e69ce7eefc"},"execution_count":null,"source":"# IDEAS FROM\n# https://www.kaggle.com/valentinw/simple-data-exploration-and-visualization\n# https://www.kaggle.com/iamprateek/submission-to-mercari-price-suggestion-challenge\n# https://www.kaggle.com/huguera/mercari-data-analysis\n# https://www.kaggle.com/rimunoz/titanic/\n\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom wordcloud import WordCloud, STOPWORDS\nimport squarify \nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\ndef transform_category_name(category_name):\n    try:\n        main, sub1, sub2= category_name.split('/')\n        return main, sub1, sub2\n    except:\n        return np.nan, np.nan, np.nan\nprint('OK')\n\n# load dataset\ntrain = pd.read_csv('../input/train.tsv', sep = \"\\t\")\ntest = pd.read_csv('../input/test.tsv', sep = \"\\t\")\n\ntrain = train.sample(frac=0.10, replace=False)\n#test  = test.sample(frac=0.25, replace=False)\n\n# Store our ID for easy access\ntestId = test['test_id']\n\n#train.head(1)\nprint(train.shape)\nprint(test.shape)","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"c6d60d3b096de628b550f34c3f3e62dec7e9939d","_cell_guid":"d58586de-3464-4f0e-916d-17531d5e0905"},"execution_count":null,"source":"from sklearn.feature_extraction.text import CountVectorizer\n\nnrow_train = train.shape[0]\nnrow_test  = test.shape[0]\nprint(str(nrow_train) + \"-\" + str(nrow_test))\n################################# ITEM DESCRIPTION #############\ntext_a = train['item_description'].fillna('NA')\ntext_b =  test['item_description'].fillna('NA')\ntext = text_a.append(text_b)\n\nvect = CountVectorizer(max_features = 20,stop_words='english')\ndtm = vect.fit_transform(text)\nitem_dtm = pd.DataFrame(dtm.toarray(), columns=vect.get_feature_names()) \n\nprint('item')\n################################# NAME #############\ntext_a = train['name'].fillna('NA')\ntext_b =  test['name'].fillna('NA')\ntext = text_a.append(text_b)\n\nvect = CountVectorizer(max_features = 10,stop_words='english')\ndtm = vect.fit_transform(text)\nname_dtm = pd.DataFrame(dtm.toarray(), columns=vect.get_feature_names()) \n\nprint('name')\n################################# CATEGORY NAME #############\ntext_a = train['category_name'].fillna('NA')\ntext_b =  test['category_name'].fillna('NA')\ntext = text_a.append(text_b)\n\nvect = CountVectorizer(max_features = 5,stop_words='english')\ndtm = vect.fit_transform(text)\ncat_dtm = pd.DataFrame(dtm.toarray(), columns=vect.get_feature_names()) \n\nprint('cat')\n################################# BRAND NAME #############\ntext_a = train['brand_name'].fillna('NA')\ntext_b =  test['brand_name'].fillna('NA')\ntext = text_a.append(text_b)\n\nvect = CountVectorizer(max_features = 5,stop_words='english')\ndtm = vect.fit_transform(text)\nbrand_dtm = pd.DataFrame(dtm.toarray(), columns=vect.get_feature_names()) \n\nprint('brand')\n\ntext_dtm = item_dtm\nprint(text_dtm.shape)\ntext_dtm = pd.concat([text_dtm.reset_index(drop=True), name_dtm.reset_index(drop=True)], axis=1)\nprint(text_dtm.shape)\ntext_dtm = pd.concat([text_dtm.reset_index(drop=True), cat_dtm.reset_index(drop=True)], axis=1)\nprint(text_dtm.shape)\ntext_dtm = pd.concat([text_dtm.reset_index(drop=True), brand_dtm.reset_index(drop=True)], axis=1)\nprint(text_dtm.shape)\n\ntrain_text_dtm = text_dtm.iloc[:nrow_train,].reset_index(drop=True)\ntest_text_dtm  = text_dtm.iloc[nrow_train:,].reset_index(drop=True)\n\nnrow_train1 = train_text_dtm.shape[0]\nnrow_test1  = test_text_dtm.shape[0]\nprint(str(nrow_train1) + \"-\" + str(nrow_test1))\nprint(train_text_dtm.info())\nprint(test_text_dtm.info())","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"b7c30aa2caad63d6b72cea123174772ff710afc4","_cell_guid":"49d50f66-f4de-4734-9ff1-0ce1ae3ea321"},"execution_count":null,"source":"np.sum(train_text_dtm)","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"013d86e18c2f7270f8f5d48e2052add907e18fa8","_cell_guid":"4d8f9010-aa1f-4cef-bb23-3ad9a69ceb66"},"execution_count":null,"source":"def if_null(row):\n    if row == row:\n        return 1\n    else:\n        return 0\nprint('OK')","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"62bbf8a90f2507c60156c7127afa3cf32535ad18","_cell_guid":"83b233c4-4283-4f9d-8509-8d383b44bdc7"},"source":"df = train.groupby(['brand_name']).count()['train_id']\n\nimport pandas as pd\ndf=pd.DataFrame(df)\ndf.sort_values('train_id', ascending= False).head(20)"},{"cell_type":"code","metadata":{"_uuid":"6cfd0c4dd1898c7de5cedec2aeebb78429b5a89f","_cell_guid":"b60f6d48-7a02-4489-bc30-122c30e3100c"},"execution_count":null,"source":"full_data = [train, test]\nfor dataset in full_data:\n    dataset['category_main'], dataset['category_sub1'], dataset['category_sub2'] = zip(*dataset['category_name'].apply(transform_category_name))\n    dataset['item_description_len'] = dataset['item_description'].str.len()\n    dataset['name_len'] = dataset['name'].str.len()\n    \n    dataset['has_descrip'] = 1\n    dataset.loc[dataset.item_description=='No description yet', 'has_descrip'] = 0\n    dataset['has_brand'] = dataset.brand_name.apply(lambda row : if_null(row))\n    \n    dataset['contains_brand_new'] = dataset['item_description'].str.contains(\"Brand New\")\n    dataset['contains_brand_new'] = dataset['contains_brand_new'].map( {True: 1, False: 0} ).astype(float)\n    \n    dataset['contains_free_shipping'] = dataset['item_description'].str.contains(\"free shipping\")\n    dataset['contains_free_shipping'] = dataset['contains_free_shipping'].map( {True: 1, False: 0} ).astype(float)\n    \n    dataset['contains_price_firm'] = dataset['item_description'].str.contains(\"Price firm\")\n    dataset['contains_price_firm'] = dataset['contains_price_firm'].map( {True: 1, False: 0} ).astype(float)\n    \n    dataset['contains_rm'] = dataset['item_description'].str.contains(\"[rm]\")\n    dataset['contains_rm'] = dataset['contains_rm'].map( {True: 1, False: 0} ).astype(float)\n    \n    dataset['is_Adidas'] = dataset['brand_name'].str.contains(\"Adidas\")\n    dataset['is_Adidas'] = dataset['is_Adidas'].map( {True: 1, False: 0} ).astype(float)\n    dataset['is_American_Eagle'] = dataset['brand_name'].str.contains(\"American Eagle\")\n    dataset['is_American_Eagle'] = dataset['is_American_Eagle'].map( {True: 1, False: 0} ).astype(float)\n    dataset['is_Apple'] = dataset['brand_name'].str.contains(\"Apple\")\n    dataset['is_Apple'] = dataset['is_Apple'].map( {True: 1, False: 0} ).astype(float)\n    dataset['is_Bath__Body_Works'] = dataset['brand_name'].str.contains(\"Bath & Body Works\")\n    dataset['is_Bath__Body_Works'] = dataset['is_Bath__Body_Works'].map( {True: 1, False: 0} ).astype(float)\n    dataset['is_Coach'] = dataset['brand_name'].str.contains(\"Coach\")\n    dataset['is_Coach'] = dataset['is_Coach'].map( {True: 1, False: 0} ).astype(float)\n    dataset['is_Disney'] = dataset['brand_name'].str.contains(\"Disney\")\n    dataset['is_Disney'] = dataset['is_Disney'].map( {True: 1, False: 0} ).astype(float)\n    dataset['is_FOREVER_21'] = dataset['brand_name'].str.contains(\"FOREVER 21\")\n    dataset['is_FOREVER_21'] = dataset['is_FOREVER_21'].map( {True: 1, False: 0} ).astype(float)\n    dataset['is_Funko'] = dataset['brand_name'].str.contains(\"Funko\")\n    dataset['is_Funko'] = dataset['is_Funko'].map( {True: 1, False: 0} ).astype(float)\n    dataset['is_LuLaRoe'] = dataset['brand_name'].str.contains(\"LuLaRoe\")\n    dataset['is_LuLaRoe'] = dataset['is_LuLaRoe'].map( {True: 1, False: 0} ).astype(float)\n    dataset['is_Lululemon'] = dataset['brand_name'].str.contains(\"Lululemon\")\n    dataset['is_Lululemon'] = dataset['is_Lululemon'].map( {True: 1, False: 0} ).astype(float)\n    dataset['is_Michael_Kors'] = dataset['brand_name'].str.contains(\"Michael Kors\")\n    dataset['is_Michael_Kors'] = dataset['is_Michael_Kors'].map( {True: 1, False: 0} ).astype(float)\n    dataset['is_Nike'] = dataset['brand_name'].str.contains(\"Nike\")\n    dataset['is_Nike'] = dataset['is_Nike'].map( {True: 1, False: 0} ).astype(float)\n    dataset['is_Nintendo'] = dataset['brand_name'].str.contains(\"Nintendo\")\n    dataset['is_Nintendo'] = dataset['is_Nintendo'].map( {True: 1, False: 0} ).astype(float)\n    dataset['is_Old_Navy'] = dataset['brand_name'].str.contains(\"Old Navy\")\n    dataset['is_Old_Navy'] = dataset['is_Old_Navy'].map( {True: 1, False: 0} ).astype(float)\n    dataset['is_PINK'] = dataset['brand_name'].str.contains(\"PINK\")\n    dataset['is_PINK'] = dataset['is_PINK'].map( {True: 1, False: 0} ).astype(float)\n    dataset['is_Rae_Dunn'] = dataset['brand_name'].str.contains(\"Rae Dunn\")\n    dataset['is_Rae_Dunn'] = dataset['is_Rae_Dunn'].map( {True: 1, False: 0} ).astype(float)\n    dataset['is_Sephora'] = dataset['brand_name'].str.contains(\"Sephora\")\n    dataset['is_Sephora'] = dataset['is_Sephora'].map( {True: 1, False: 0} ).astype(float)\n    dataset['is_Sony'] = dataset['brand_name'].str.contains(\"Sony\")\n    dataset['is_Sony'] = dataset['is_Sony'].map( {True: 1, False: 0} ).astype(float)\n    dataset['is_Under_Armour'] = dataset['brand_name'].str.contains(\"Under Armour\")\n    dataset['is_Under_Armour'] = dataset['is_Under_Armour'].map( {True: 1, False: 0} ).astype(float)\n    dataset['is_Victoria_Secret'] = dataset['brand_name'].str.contains(\"Victoria's Secret\")\n    dataset['is_Victoria_Secret'] = dataset['is_Victoria_Secret'].map( {True: 1, False: 0} ).astype(float)\n    \n    dataset['item_description'] = dataset['item_description'].str.lower()\n    \n    \n    ### VAR IDEAS FROM https://www.kaggle.com/lopuhin/eli5-for-mercari\n    dataset['contains_gb'] = dataset['item_description'].str.contains(\"gb \")\n    dataset['contains_gb'] = dataset['contains_gb'].map( {True: 1, False: 0} ).astype(float)\n    \n    dataset['contains_14k'] = dataset['item_description'].str.contains(\"14k \")\n    dataset['contains_14k'] = dataset['contains_14k'].map( {True: 1, False: 0} ).astype(float)\n    \n    dataset['contains_unlocked'] = dataset['item_description'].str.contains(\"unlocked\")\n    dataset['contains_unlocked'] = dataset['contains_unlocked'].map( {True: 1, False: 0} ).astype(float)\n    \n    dataset['contains_carat'] = dataset['item_description'].str.contains(\"carat\")\n    dataset['contains_carat'] = dataset['contains_carat'].map( {True: 1, False: 0} ).astype(float)\n    \n    dataset['is_vitamix'] = dataset['brand_name'].str.contains(\"vitamix\")\n    dataset['is_vitamix'] = dataset['is_vitamix'].map( {True: 1, False: 0} ).astype(float)\n    \n    dataset['is_david_yurman'] = dataset['brand_name'].str.contains(\"david yurman\")\n    dataset['is_david_yurman'] = dataset['is_david_yurman'].map( {True: 1, False: 0} ).astype(float)\n    \n    dataset['is_hatchimal'] = dataset['name'].str.contains(\"hatchimal\")\n    dataset['is_hatchimal'] = dataset['is_hatchimal'].map( {True: 1, False: 0} ).astype(float)\n    \n    dataset['is_dockatot'] = dataset['name'].str.contains(\"dockatot\")\n    dataset['is_dockatot'] = dataset['is_dockatot'].map( {True: 1, False: 0} ).astype(float)   \n    \n\n    #####################################################################\n    dataset['category_main'] = dataset['category_main'].str.replace(' ','_')\n    dataset['category_sub1'] = dataset['category_sub1'].str.replace(' ','_')\n    dataset['category_sub2'] = dataset['category_sub2'].str.replace(' ','_')\n    \n    dataset['category_main'] = dataset['category_main'].str.replace('&','')\n    dataset['category_sub1'] = dataset['category_sub1'].str.replace('&','')\n    dataset['category_sub2'] = dataset['category_sub2'].str.replace('&','')\n    \n   \n \n\ntrain = pd.concat([train, pd.get_dummies(train.category_main, prefix_sep='', prefix='M_')], axis=1)\ntest  = pd.concat([test , pd.get_dummies(test.category_main , prefix_sep='', prefix='M_')], axis=1)\ntrain = pd.concat([train, pd.get_dummies(train.item_condition_id, prefix_sep='', prefix='M_')], axis=1)\ntest  = pd.concat([test , pd.get_dummies(test.item_condition_id , prefix_sep='', prefix='M_')], axis=1)\n\n#dataset = pd.concat([dataset, pd.get_dummies(dataset.category_sub1, prefix_sep='', prefix='S1_')], axis=1)\n#dataset = pd.concat([dataset, pd.get_dummies(dataset.category_sub2, prefix_sep='', prefix='S2_')], axis=1)\n    \ntrain.head(3)","outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"_uuid":"98b535192c002d3c3bc6a9f03667516bc9a81c96","_cell_guid":"bae295fd-b4fb-42f7-8989-19b699d0ce8a"},"source":"wc = WordCloud(background_color=\"white\", max_words=5000, \n               stopwords=STOPWORDS, max_font_size= 50)\n\nwc.generate(\" \".join(str(s) for s in train.item_description.values))\n\nplt.figure(figsize=(20,12))\nplt.axis('off')\nplt.imshow(wc, interpolation='bilinear')"},{"cell_type":"markdown","metadata":{"collapsed":true,"_uuid":"32e70428c11cd7efee48b9741c9342f88cbc030c","_cell_guid":"f38fff75-5d67-46e1-87ca-af6aa5ae2b5a"},"source":"train.groupby(['brand_name'])['price'].mean()"},{"cell_type":"code","metadata":{"_uuid":"de3e786c8829cc7c0a5865ba84d17a8f6a7cbd89","_cell_guid":"faaa3d24-fc56-4300-947e-690f79a36b7f"},"execution_count":null,"source":"# Feature Selection\ndrop_elements = ['train_id', 'name', 'category_name', 'brand_name', 'item_description', 'category_main', 'category_sub1', 'category_sub2', 'item_condition_id']\ntrain = train.drop(drop_elements, axis = 1)\ndrop_elements = ['test_id', 'name', 'category_name', 'brand_name', 'item_description', 'category_main', 'category_sub1', 'category_sub2', 'item_condition_id']\ntest  = test.drop(drop_elements, axis = 1)\n\ntrain.head(3)","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"2febfdeeb69ff5044b8c284f374d9793a653a054","_cell_guid":"b2383d70-4455-48ab-a4dd-35cdf4bce098"},"execution_count":null,"source":"print (train.info())","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"dae544a415091ef2f34863ac2d0e5ab0d6712352","_cell_guid":"301b6dfe-800c-45a9-9d3e-8315645588ea"},"execution_count":null,"source":"full_data = [train, test]\nfor dataset in full_data:   \n    dataset['item_description_len']   = dataset['item_description_len'].fillna(0)\n    dataset['contains_brand_new']     = dataset['contains_brand_new'].fillna(0).astype(int)\n    dataset['contains_free_shipping'] = dataset['contains_free_shipping'].fillna(0).astype(int)\n    dataset['contains_price_firm']    = dataset['contains_price_firm'].fillna(0).astype(int)\n    dataset['contains_rm']    = dataset['contains_rm'].fillna(0).astype(int)\n    dataset['has_brand']    = dataset['has_brand'].fillna(0).astype(int)\n    dataset['is_Adidas'] = dataset['is_Adidas'].fillna(0).astype(int)\n    dataset['is_American_Eagle'] = dataset['is_American_Eagle'].fillna(0).astype(int)\n    dataset['is_Apple'] = dataset['is_Apple'].fillna(0).astype(int)\n    dataset['is_Bath__Body_Works'] = dataset['is_Bath__Body_Works'].fillna(0).astype(int)\n    dataset['is_Coach'] = dataset['is_Coach'].fillna(0).astype(int)\n    dataset['is_Disney'] = dataset['is_Disney'].fillna(0).astype(int)\n    dataset['is_FOREVER_21'] = dataset['is_FOREVER_21'].fillna(0).astype(int)\n    dataset['is_Funko'] = dataset['is_Funko'].fillna(0).astype(int)\n    dataset['is_LuLaRoe'] = dataset['is_LuLaRoe'].fillna(0).astype(int)\n    dataset['is_Lululemon'] = dataset['is_Lululemon'].fillna(0).astype(int)\n    dataset['is_Michael_Kors'] = dataset['is_Michael_Kors'].fillna(0).astype(int)\n    dataset['is_Nike'] = dataset['is_Nike'].fillna(0).astype(int)\n    dataset['is_Nintendo'] = dataset['is_Nintendo'].fillna(0).astype(int)\n    dataset['is_Old_Navy'] = dataset['is_Old_Navy'].fillna(0).astype(int)\n    dataset['is_PINK'] = dataset['is_PINK'].fillna(0).astype(int)\n    dataset['is_Rae_Dunn'] = dataset['is_Rae_Dunn'].fillna(0).astype(int)\n    dataset['is_Sephora'] = dataset['is_Sephora'].fillna(0).astype(int)\n    dataset['is_Sony'] = dataset['is_Sony'].fillna(0).astype(int)\n    dataset['is_Under_Armour'] = dataset['is_Under_Armour'].fillna(0).astype(int)\n    dataset['is_Victoria_Secret'] = dataset['is_Victoria_Secret'].fillna(0).astype(int)\n    \n    dataset['contains_gb'] = dataset['contains_gb'].fillna(0).astype(int)\n    dataset['contains_14k'] = dataset['contains_14k'].fillna(0).astype(int)    \n    dataset['contains_unlocked'] = dataset['contains_unlocked'].fillna(0).astype(int) \n    dataset['contains_carat'] = dataset['contains_carat'].fillna(0).astype(int)\n    dataset['is_vitamix'] = dataset['is_vitamix'].fillna(0).astype(int)\n    dataset['is_david_yurman'] = dataset['is_david_yurman'].fillna(0).astype(int)\n    dataset['is_hatchimal'] = dataset['is_hatchimal'].fillna(0).astype(int)\n    dataset['is_dockatot'] = dataset['is_dockatot'].fillna(0).astype(int)\n\n\nprint (train.info())\n#print(train.describe())\n","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"5af1467a83f445847c6e8c96e2e6b0274d4db890","_cell_guid":"0a6eb3ed-d06b-4be0-9e3a-bdc8a7d60004"},"execution_count":null,"source":"print (test.info())\n#print(test.describe())","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"dc179f3b237d8f5be4e656f1b9c426ff58d92297","_cell_guid":"22e49eb7-2802-4d62-b50f-91a6eb6fb64c"},"execution_count":null,"source":"np.sum(train)","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"0d64d404e8a44c5cb8a2dee5e495a0627b6679c2","_cell_guid":"f52faeeb-4ac2-411a-90bd-1d8ff68b61a0"},"execution_count":null,"source":"# Feature Selection 2\ndrop_elements = ['is_vitamix' ,'is_david_yurman','is_hatchimal','is_dockatot']\ntrain = train.drop(drop_elements, axis = 1)\ntest  = test.drop(drop_elements, axis = 1)\n\ntrain.head(3)","outputs":[]},{"cell_type":"code","metadata":{},"execution_count":null,"source":"full_data = [train_text_dtm, test_text_dtm]\nfor dataset in full_data:  \n    for column in dataset:\n        #print(column)\n        dataset[column]   = dataset[column].fillna(0).astype(int)\nprint(train_text_dtm.info())\nprint(test_text_dtm.info())","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"f499a8b0f69a82d4fe961a05aef9a52b3afd65e3","_cell_guid":"9a9ccb4f-22d8-4717-a1d1-f1df15934f5b"},"execution_count":null,"source":"from sklearn import preprocessing\n\n#yy_train = (np.log(train['price']+1))\nyy_train = train['price']\nx_train = (train.drop('price',axis=1))\nx_train = x_train.iloc[:, 0:].values\n\nx_train = pd.concat([pd.DataFrame(x_train).reset_index(drop=True), train_text_dtm.reset_index(drop=True)], axis=1)\ntest = pd.concat([pd.DataFrame(test).reset_index(drop=True), test_text_dtm.reset_index(drop=True)], axis=1)\n\nprint(x_train.shape)\nprint(test.shape)\nprint(x_train.info())\nprint(test.info())\n\n\n# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\ndata_test = sc.transform(test)\n\nimport numpy as np\n#x_train = np.delete(x_train, 4, axis=1)\n#data_test = np.delete(data_test, 4, axis=1)\n#x_train   = x_train.astype(int)\n#data_test = data_test.astype(int)\n\ndata_train = x_train\n\nprint(data_train.__class__.__name__)\nprint(data_test.__class__.__name__)\n#print(pd.DataFrame(yy_train).describe())\n#print(pd.DataFrame(data_train).describe())\n#print(pd.DataFrame(data_test).describe())\n\n","outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"_uuid":"0ce94ba596ed656fe23b20451a8b4b8344e44095","_cell_guid":"1b1c5c6c-83e2-4777-9670-521ab209a282"},"execution_count":null,"source":"data_test","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"4db52fd8ef882486fcfbee85132a915ab077c5a3","_cell_guid":"2e0800e0-821d-4034-a77c-051544fdc2e5"},"execution_count":null,"source":"import numpy as np\n\ndef rmsle(h, y): \n    \"\"\"\n    Compute the Root Mean Squared Log Error for hypthesis h and targets y\n\n    Args:\n        h - numpy array containing predictions with shape (n_samples, n_targets)\n        y - numpy array containing targets with shape (n_samples, n_targets)\n    \"\"\"\n    return np.sqrt(np.square(np.log(h + 1) - np.log(y + 1)).mean())\nprint('ok')","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"fa6c93c2d424f7eed79f85a0cbc60307fef1ef20","_cell_guid":"cd460aa0-950d-4bfc-bc0a-dd50d80601f3"},"execution_count":null,"source":"# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(data_train, yy_train, test_size = 0.2, random_state = 0)\nprint('ok')","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"be341ec499225ea4c456d9a40c1b7cd8fe4d31e4","_cell_guid":"df7e31a0-5cf1-490f-9ac6-4c7744420b07"},"execution_count":null,"source":"from sklearn.linear_model import LinearRegression, SGDRegressor, RidgeCV, Lasso, LassoCV, ElasticNetCV\nfrom sklearn.linear_model import Ridge\nfrom sklearn.svm import SVR\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF, Matern, RationalQuadratic, ExpSineSquared, DotProduct,ConstantKernel                                            \n                                              \n\nimport tensorflow as tf\nimport tensorflow.contrib.learn as learn\n#Some useful packages\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn   import metrics\nfrom sklearn.model_selection import train_test_split\n\ndef trainModels(model, X_train, y_train, X_test):\n    if(model == 'linear'):\n        cl = LinearRegression()   \n    if(model == 'SGD'):\n        cl= SGDRegressor()\n    if(model == 'Ridge'):\n        cl = Ridge(solver='auto',\n        fit_intercept=True,\n        alpha=0.5,\n        max_iter=100,\n        normalize=False,\n        tol=0.05)\n    if(model == 'RidgeCV'):\n        cl = RidgeCV()        \n    if(model == 'Lasso'):\n        cl = LassoCV()\n    if(model == 'ElasticNet'):\n        cl = ElasticNetCV()\n    if(model == 'SVR'):\n        cl = SVR(kernel='linear', C=1e4) \n    if(model == 'NeuralNet'):\n        cl = MLPRegressor(solver='lbfgs', \n                                       alpha=1e-5, \n                                       hidden_layer_sizes=(80,50,20), \n                                       random_state=1)\n    if(model == 'RandomForest'):\n        cl = RandomForestRegressor(n_estimators=500,oob_score=True, max_features = 5, max_depth = 3)\n    if(model == 'ExtraTrees'):\n        cl = ExtraTreesRegressor(n_estimators=500)\n        \n    if(model == 'GradientBoosting'):\n        cl = GradientBoostingRegressor(n_estimators=1000, \n                                                  learning_rate=0.05, \n                                                  min_samples_leaf=50, \n                                                  min_samples_split=20, \n                                                  loss='huber')\n\n    cl.fit(X_train, y_train)\n    pred = cl.predict(X_test)\n    #score = np.sqrt(metrics.mean_squared_error(y_test,pred))\n    score = rmsle(pred,y_test)\n    return score, cl\n\nprint('OK')","outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"_uuid":"5cace31e427f453c60962dad92ed779fdb56f682","_cell_guid":"fc517a36-8c69-4805-948e-ad2009080064"},"execution_count":null,"source":"a = train.drop('price',axis=1).columns.tolist()\nb = train_text_dtm.columns.tolist()\nlista = a + b\nprint(len(a))\nprint(len(b))\nprint(len(lista))\n","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"450d4c1ea53dd9017a01a344d21dc541441c1c61","_cell_guid":"f268e89d-f79f-43b0-80f8-674bfd262ac6"},"execution_count":null,"source":"clf = ExtraTreesRegressor()\nclf = clf.fit(X_train, y_train)\nclf.feature_importances_  \n#model = SelectFromModel(clf, prefit=True)\nimportances = clf.feature_importances_\nstd = np.std([f.feature_importances_ for f in clf.estimators_], axis=0)\nindices = np.argsort(importances)[::-1]\nfeatures = lista","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"f570a20782d205ecb12d209f215818950dd2d1aa","_cell_guid":"eb592f71-e1ee-48a1-b1ed-4c6089555bb8"},"execution_count":null,"source":"print(len(indices))\nprint(len(features))\nprint(len(importances))","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"a5fcb8231f365f782b664df2973b74f71c8f1aa4","_cell_guid":"f16f2b4d-8118-4398-828e-1909e1c1cfea"},"execution_count":null,"source":"# Print the feature ranking\nprint(\"Feature ranking:\")\n\ncum_imp = 0\nsel_features = list()\nsel_features_idx = list()\nfor f in range(X_train.shape[1]):\n    cum_imp = cum_imp + importances[indices[f]]\n    print('rank '+(str(f)) + \" var: \"+ (str(indices[f])) + \" \"+ features[indices[f]] + \"--\\t\\t\" + str(importances[indices[f]]) + \"--\\t\" + str(cum_imp))\n    if(cum_imp <= 0.8):\n        sel_features.append(features[indices[f]])\n        sel_features_idx.append(indices[f])\n\nplt.figure()\nplt.title(\"Feature importances\")\nplt.bar(range(X_train.shape[1]), importances[indices],\n       color=\"r\", yerr=std[indices], align=\"center\")\nplt.xticks(range(X_train.shape[1]), indices)\nplt.xlim([-1, X_train.shape[1]])\nplt.show()\n\nprint(sel_features)\nprint(sel_features_idx)","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"3a3874d2cdcb9b6c2105e8d9407b8d8c6971b5f4","_cell_guid":"485f37fa-c695-439d-a564-af549465df84"},"execution_count":null,"source":"# Feature Selection 3\nX_train    = X_train[:,sel_features_idx]\nX_test     = X_test[:,sel_features_idx]\ndata_test = data_test[:,sel_features_idx]\n\nX_train","outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"_uuid":"bada13dafe3389629e5cf737da66b66c9b3ca028","_cell_guid":"a4fab774-e852-4b43-8e0d-d93416803680"},"execution_count":null,"source":"s, cl_linear = trainModels('linear', X_train, y_train, X_test)\nprint('linear:\\t' + str(s))\ns, cl_SGD = trainModels('SGD', X_train, y_train, X_test)\nprint('SGD:\\t' + str(s))\ns, cl_Ridge = trainModels('Ridge', X_train, y_train, X_test)\nprint('Ridge:\\t' + str(s))\ns, cl_RidgeCV = trainModels('RidgeCV', X_train, y_train, X_test)\nprint('RidgeCV:\\t' + str(s))\ns, cl_Lasso = trainModels('Lasso', X_train, y_train, X_test)\nprint('Lasso:\\t' + str(s))\ns, cl_ElasticNet = trainModels('ElasticNet', X_train, y_train, X_test)\nprint('ElasticNet:\\t' + str(s))\n#s, cl_SVR = trainModels('SVR', X_train, y_train, X_test)\n#print('SVR:\\t' + str(s))\ns, cl_NeuralNet = trainModels('NeuralNet', X_train, y_train, X_test)\nprint('NeuralNet:\\t' + str(s))\ns, cl_RandomForest = trainModels('RandomForest', X_train, y_train, X_test)\nprint('RandomForest:\\t' + str(s))\ns, cl_ExtraTrees = trainModels('ExtraTrees', X_train, y_train, X_test)\nprint('ExtraTrees:\\t' + str(s))\ns, cl_GradientBoosting = trainModels('GradientBoosting', X_train, y_train, X_test)\nprint('GradientBoosting:\\t' + str(s))\n\n\n","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"8af22ec60b1982e336129f4d66fc070fef89b47a","_cell_guid":"fd6b91e1-f13f-429d-8a57-44a7e1a6146f"},"execution_count":null,"source":"candidate_regressor = cl_GradientBoosting\ncandidate_regressor.fit(X_train, y_train)\npred = candidate_regressor.predict(X_test)\n#y_test = np.exp(y_test)-1\n#pred = np.exp(pred)-1\nscore = rmsle(pred,y_test)\nprint(score)\nresult = candidate_regressor.predict(data_test)\n\n\n# Generate Submission File \nSubmission = pd.DataFrame({ 'test_id': testId, 'price': result })\n\nprint(Submission['price'].mean())\nprint(Submission.head(3))\n","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"8c9fb2e426f70ac05747667f5449002183068ac7","_cell_guid":"872acdec-f5b7-4cf4-be50-7b07be1e3f80"},"execution_count":null,"source":"Submission.to_csv(\"Submission.csv\", index=False)\nprint('OK')","outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"_uuid":"5bfcea7191905398ea304f99d784a8950d3f0539","_cell_guid":"7bb6ed3e-c9ca-4796-b974-6847c130341e"},"execution_count":null,"source":"print(X_train.shape[0])\nprint(X_train.shape[1])","outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"collapsed":true,"_uuid":"5005059c80bf1e1d1e42458816ac6a0435248398","_cell_guid":"40513f28-1643-41b9-8b77-c6c9fbc1f2bc"},"execution_count":null,"source":"from numpy import array\nfrom matplotlib import pyplot\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras import backend\nfrom keras.layers import Dropout\n \ndef rmse_k(y_true, y_pred):\n    return backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))\n\ndef rmsle_k(y_true, y_pred):\n    return backend.sqrt(backend.mean(backend.square(backend.log(y_pred + 1) - backend.log(y_true + 1)), axis=-1))\n\n# create model\ndrop = 0.3\nmodel = Sequential()\nmodel.add(Dense(units = 12, input_dim=12, kernel_initializer = 'uniform', activation='relu'))\nmodel.add(Dropout(drop))\nmodel.add(Dense(units = 10, kernel_initializer = 'uniform', activation='relu'))\nmodel.add(Dropout(drop))\nmodel.add(Dense(units = 8, kernel_initializer = 'uniform', activation = 'relu'))\nmodel.add(Dropout(drop))\nmodel.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\nmodel.add(Dropout(drop))\nmodel.add(Dense(units = 1,  kernel_initializer = 'uniform'))\nmodel.compile(loss='mse', optimizer='adam', metrics=[rmsle_k])\n# train model\nhistory = model.fit(X_train, y_train, epochs=30, batch_size=1000, verbose=0)\n# plot metrics\npyplot.plot(history.history['rmsle_k'])\npyplot.show()","outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"collapsed":true,"_uuid":"b8bf03681b9d2f985a3dd80c340054407524e8fa","_cell_guid":"a1a3d12c-7f3c-4e03-9cf8-893b2dadef1c"},"execution_count":null,"source":"pred = model.predict(X_test)\npred = backend.cast_to_floatx(pred)\nprice_test = backend.cast_to_floatx(y_test)\n#pred = np.exp(pred) -1\n#price_test = np.exp(price_test) -1\n\nscore = rmsle(pred,price_test)\nprint(score)\n\n#result = model.predict(data_test)\n#result = result[:,0]\n\n# Generate Submission File \n#Submission = pd.DataFrame({ 'test_id': testId, 'price': result })\n\n#print(Submission['price'].mean())\n#print(Submission.head(3))","outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"_uuid":"47aada433e90ae14ed0847e348cec482ce3b8bce","_cell_guid":"90fe323c-97d6-4159-a2b3-af8c6968e771"},"execution_count":null,"source":"#GradientBoostingRegressor OPTIMIZATION\nfrom sklearn.ensemble import GradientBoostingRegressor\n#from sklearn.model_selection import cross_validation, metrics   #Additional scklearn functions\nfrom sklearn.model_selection  import GridSearchCV   #Perforing grid search\n\ndef rmsle_gd(y_true, y_pred): return np.sqrt(np.square(np.log(y_pred + 1) - np.log(y_true + 1)).mean()) \n\nparametros = {'n_estimators':      [1000],\n              'min_samples_leaf':  [50],\n              'min_samples_split': [20,50]\n              #'learning_rate':     [0.05, 0.1]\n             }\nprint(parametros)\n\ncustom_score = 'neg_mean_squared_error'\n\ngsearch1 = GridSearchCV(estimator = \n                        GradientBoostingRegressor(loss='huber'), \n                            param_grid = parametros,\n                            n_jobs= 1,\n                            iid=False,\n                            scoring=custom_score,\n                            cv=3)\n#gsearch1.fit(X_train, y_train)\n\n\n","outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"_uuid":"e43d01b9a5b44573bd04e62f7882d8140acdcdb9","_cell_guid":"e58a31ff-c777-4a44-bba0-f07b929db380"},"execution_count":null,"source":"#best_parameters = gsearch1.best_params_\n#best_accuracy = gsearch1.best_score_\n\n#print(best_parameters)\n#print(best_accuracy)\n#gsearch1.grid_scores_","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"6ed57aebd845089926b3c4a65cb6a3408eb43f2a","_cell_guid":"80feae8b-5e8f-4e65-8b07-77b2c030f9fb"},"source":"[mean: -1296.47879, std: 80.36403, params: {'n_estimators': 100},\n mean: -1282.70548, std: 78.57870, params: {'n_estimators': 200},\n mean: -1273.14318, std: 77.92016, params: {'n_estimators': 1000}]\n [mean: -1274.88992, std: 79.04721, params: {'min_samples_leaf': 25, 'n_estimators': 1000},\n mean: -1271.36757, std: 78.19887, params: {'min_samples_leaf': 50, 'n_estimators': 1000}]\n \n "},{"cell_type":"code","metadata":{"collapsed":true,"_uuid":"8fde4395b064a4a051289c71e9de9a3e7d517605","_cell_guid":"21ab0e67-927e-4757-8cac-7c9f3659f8f3"},"execution_count":null,"source":"from numpy import array\nfrom matplotlib import pyplot\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras import backend\nfrom keras.layers import Dropout\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection  import GridSearchCV \n# Importing the Keras libraries and packages\nfrom keras.layers import Conv2D\nfrom keras.layers import MaxPooling2D\nfrom keras.layers import Flatten\nfrom keras.layers import Conv1D, MaxPooling1D\n    \ndef rmse_k(y_true, y_pred):\n    return backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))\n\ndef rmsle_k(y_true, y_pred):\n    return backend.sqrt(backend.mean(backend.square(backend.log(y_pred + 1) - backend.log(y_true + 1)), axis=-1))\n\n# create model\ndef build_classifier(drop, optimizer):\n    model = Sequential()\n    model.add(Dense(units = 40, input_dim=44, kernel_initializer = 'uniform', activation='relu'))\n    model.add(Dropout(drop))\n    model.add(Dense(units = 20, kernel_initializer = 'uniform', activation='relu'))\n    model.add(Dropout(drop))\n    model.add(Dense(units = 15, kernel_initializer = 'uniform', activation = 'relu'))\n    model.add(Dropout(drop))\n    model.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n    model.add(Dropout(drop))\n    model.add(Dense(units = 1,  kernel_initializer = 'uniform'))\n    model.compile(loss='mse', optimizer=optimizer, metrics=[rmsle_k])\n    return model\n\n# 118603\n# 44\nX_train1 = np.expand_dims(X_train, axis=2) # reshape (569, 30) to (569, 30, 1) \nX_test1  = np.expand_dims(X_test, axis=2) # reshape (569, 30) to (569, 30, 1) \ndef build_classifier_CNN(drop, optimizer):\n    classifier = Sequential()\n    classifier.add(Conv1D(44, (5), input_shape = (44,1), activation = 'relu'))\n    classifier.add(Dropout(drop))\n    classifier.add(MaxPooling1D(pool_size = (3)))\n    classifier.add(Conv1D(20, (5), activation = 'relu'))\n    classifier.add(MaxPooling1D(pool_size = (3)))\n    classifier.add(Flatten())\n    classifier.add(Dense(units = 15, activation = 'relu'))\n    classifier.add(Dropout(drop))\n    classifier.add(Dense(units = 10, activation = 'relu'))\n    classifier.add(Dense(units = 1,  kernel_initializer = 'uniform'))\n    classifier.compile(optimizer = optimizer, loss = 'msle', metrics = [rmsle_k])\n    return classifier\n\n\nclassifier = KerasClassifier(build_fn = build_classifier)\nclassifier_CNN = KerasClassifier(build_fn = build_classifier_CNN)\n\nparameters = {'batch_size': [5000], #, 10000],\n              'epochs': [30],#, 40],\n              'optimizer': ['adam'],#, 'rmsprop'],\n              'drop': [0.2]#,0.3]\n             }\n#'rmsprop'\ncustom_score = 'neg_mean_squared_log_error'\n\ngrid_search = GridSearchCV(estimator = classifier_CNN,\n                           param_grid = parameters,\n                           scoring = custom_score,\n                           cv = 3)\n#grid_search = grid_search.fit(X_train1, y_train)\nbest_parameters = grid_search.best_params_\nbest_accuracy = grid_search.best_score_\nbest_parameters\nbest_accuracy","outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"_uuid":"c2374beb7e5ccd145fb7124fec73b5c4bbb4a083","_cell_guid":"1fc6ae02-66e2-4acc-a81a-f2056bb8168e"},"execution_count":null,"source":"best_parameters = grid_search.best_params_\nbest_accuracy = grid_search.best_score_\n\nprint(best_parameters)\nprint(best_accuracy)\ngrid_search.grid_scores_","outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"collapsed":true,"_uuid":"7ae973d3b92eea93a28ebe3350817190805fee8d","_cell_guid":"6a469be2-8cae-44b4-9585-6e131f5b9e23"},"execution_count":null,"source":"build_fn = build_classifier_CNN(drop = 0.2,optimizer = 'adam')\ncl_CNN = KerasClassifier(build_fn)\nhistory = cl_CNN.fit(X_train1, y_train, epochs=30, batch_size=2000, verbose=2,  validation_data = (X_test1, y_test))\npyplot.plot(history.history['rmsle_k'])\npyplot.show()","outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"_uuid":"2de6662c1c1e38e477664c85392d46916baa7d6a","_cell_guid":"959a3d44-7883-412f-bd30-36b9a7a4a126"},"execution_count":null,"source":"pred = cl_CNN.predict(X_train1)\nprint(pd.DataFrame(pred).describe())\npred = backend.cast_to_floatx(pred)\nprice_test = backend.cast_to_floatx(y_train)\nprint(pred)\nscore = rmsle(pred,price_test)\nprint(score)\n\n#data_test1  = np.expand_dims(data_test, axis=2) # reshape (569, 30) to (569, 30, 1) \n#result = classifier_CNN.predict(data_test1)\n#result = result[:,0]\n\n# Generate Submission File \n#Submission = pd.DataFrame({ 'test_id': testId, 'price': result })\n\n#print(Submission['price'].mean())\n#print(Submission.head(3))","outputs":[]}],"nbformat":4}