{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","scrolled":true,"execution":{"iopub.status.busy":"2022-01-05T19:54:46.870993Z","iopub.execute_input":"2022-01-05T19:54:46.871315Z","iopub.status.idle":"2022-01-05T19:54:46.884772Z","shell.execute_reply.started":"2022-01-05T19:54:46.871285Z","shell.execute_reply":"2022-01-05T19:54:46.883441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.autograd import Variable\nfrom torch import optim\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, utils\nimport torch.nn.functional as F\nimport time\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\nfrom sklearn.model_selection import train_test_split\nimport math\nfrom nltk.corpus import stopwords","metadata":{"execution":{"iopub.status.busy":"2022-01-05T19:54:46.888416Z","iopub.execute_input":"2022-01-05T19:54:46.889082Z","iopub.status.idle":"2022-01-05T19:54:46.898516Z","shell.execute_reply.started":"2022-01-05T19:54:46.88904Z","shell.execute_reply":"2022-01-05T19:54:46.897273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Unzip, load dữ liệu train/test\n* Dữ liệu train nằm trong file train.tsv.7z\n* Dữ liệu test nằm trong file test_stg2.tsv.zip","metadata":{}},{"cell_type":"code","source":"!apt-get install p7zip\n!p7zip -d -f -k /kaggle/input/mercari-price-suggestion-challenge/train.tsv.7z\n!unzip -o /kaggle/input/mercari-price-suggestion-challenge/test_stg2.tsv.zip\n# !p7zip -d -f -k /kaggle/input/mercari-price-suggestion-challenge/test.tsv.7z","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-01-05T19:54:46.90069Z","iopub.execute_input":"2022-01-05T19:54:46.90173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read data\ntrain = pd.read_csv(\"train.tsv\", sep='\\t')\ntest = pd.read_csv(\"test_stg2.tsv\", sep='\\t')\n# test = pd.read_csv(\"test.tsv\", sep='\\t')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Phân tích dữ liệu","metadata":{}},{"cell_type":"code","source":"# train shape & train info\nprint(train.shape)\ntrain.info(memory_usage=\"deep\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(test.shape)\ntest.info(memory_usage=\"deep\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# price analysis\ntrain.price.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Item condition analysis\nprint(train['item_condition_id'].value_counts())\nprint('item_condition_id is null:', train['item_condition_id'].isnull().sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Shipping analysis\nprint(train['shipping'].value_counts())\nprint('shipping is null:', train['shipping'].isnull().sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Brand name analysis\nprint(train['brand_name'].value_counts())\nprint('brand_name isn\\'t null:', train['brand_name'].count())\nprint('brand_name is null:', train['brand_name'].isnull().sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Category name analysis\nprint(train['category_name'].value_counts())\nprint('category_name isn\\'t null:', train['category_name'].count())\nprint('category_name is null:', train['category_name'].isnull().sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split item category name into 3 different fields: general_cat, subcat_1, subcat_2\n# eg:(Women/Athletic Apparel/Pants) => (Women), (Athletic Apparel), (Pants) \n\ndef split_cat(text):\n    try: return text.split(\"/\")\n    except: return (\"None\", \"None\", \"None\")\n    \ntrain['general_cat'], train['subcat_1'], train['subcat_2'] = zip(*train['category_name'].apply(lambda x: split_cat(x)))\ntest['general_cat'], test['subcat_1'], test['subcat_2'] = zip(*test['category_name'].apply(lambda x: split_cat(x)))\n\nprint(train['general_cat'].value_counts(),end=\"\\n\\n\")\nprint(train['subcat_1'].value_counts(),end=\"\\n\\n\")\nprint(train['subcat_2'].value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting some histograms of categorical Variables\nplt.figure(figsize=(10,10))\nplt.subplot(3,3,1)\ncount_classes_general_cat = pd.value_counts(train.general_cat, sort = True)\ncount_classes_general_cat.plot(kind = 'bar')\nplt.title(\"General Category histogram\")\nplt.xlabel(\"Class\")\nplt.ylabel(\"Frequency\")\n# subcategory 1\nplt.subplot(3,3,3)\ncount_classes_subcat_1 = pd.value_counts(train.subcat_1, sort = True)[:15]\ncount_classes_subcat_1.plot(kind = 'bar')\nplt.title(\"Sub Category 1 histogram\")\nplt.xlabel(\"Class\")\nplt.ylabel(\"Frequency\")\n# subcategory 2\nplt.subplot(3,3,9)\ncount_classes_subcat_2 = pd.value_counts(train.subcat_2, sort = True)[:15]\ncount_classes_subcat_2.plot(kind = 'bar')\nplt.title(\"Sub Category 2 histogram\")\nplt.xlabel(\"Class\")\nplt.ylabel(\"Frequency\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Description analysis\nprint(train['item_description'].value_counts())\nprint('item_description isn\\'t null:', train['item_description'].count())\nprint('item_description is null:', train['item_description'].isnull().sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Xử lý dữ liệu:\n- Xử lý dữ liệu trống\n- Mã hóa 'brand_name', 'general_cat', 'subcat_1', 'subcat_2', 'category_name'\n- Chuẩn hóa, lowercase, loại bỏ ký tự không hợp lệ, phân đoạn chuỗi kỹ tự của 'name' và 'item_description', mã hóa chúng và ghi vào trường mới đuôi _seq","metadata":{}},{"cell_type":"code","source":"# Handle missing values\ndef handle_missing(dataset):\n    dataset.brand_name.fillna(value=\"None\", inplace=True)\n    dataset.item_description.fillna(value=\"None\", inplace=True)\n    dataset.category_name.fillna(value=\"None\", inplace=True)\n    return (dataset)\n\ntrain = handle_missing(train)\ntest = handle_missing(test)\nprint(train.shape)\nprint(test.shape)\ntrain.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Normalize labels, encoder brand_name & category_name text data\ndef encode_text(column):\n    le = LabelEncoder()\n    le.fit(np.hstack([train[column], test[column]]))\n    train[column+'_index'] = le.transform(train[column])\n    test[column+'_index'] = le.transform(test[column])\n    \nencode_text('brand_name')\nencode_text('general_cat')\nencode_text(\"subcat_1\")\nencode_text('subcat_2')\nencode_text('category_name')\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Category:\n    def __init__(self, name):\n        self.name = name\n        self.word2index = {}\n        self.word2count = {}\n        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n        self.n_words = 2 \n\n    def addSentence(self, sentence):\n        for word in sentence.split(' '):\n            self.addWord(word)\n\n    def addWord(self, word):\n        if word not in self.word2index:\n            self.word2index[word] = self.n_words\n            self.word2count[word] = 1\n            self.index2word[self.n_words] = word\n            self.n_words += 1\n        else:\n            self.word2count[word] += 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\n\n# Lowercase, trim, and remove non-letter characters\ndef normalizeString(s):\n    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n    return s\n\ndef normalizeLine(sentence):\n    return [normalizeString(s) for s in sentence.split('\\t')]\n\n\ndef prepareData(lang1,data):\n    \n    input_cat = Category(lang1)\n    print(\"Counting words:\")\n    print(input_cat.name, end=\" \")\n    for sentence in data:\n        normalize_line = [normalizeString(s) for s in sentence.split('\\t')]\n        input_cat.addSentence(normalize_line[0])\n        \n    print(input_cat.n_words)\n    return input_cat\n\ndef indexesFromSentence(lang, sentence):\n    return [lang.word2index[word] for word in sentence.split(' ')]\n\ndef variableFromSentence(lang, sentence):\n    indexes = indexesFromSentence(lang, sentence)\n    return indexes\n\n\ndef token_fit(column):\n    raw_text = np.hstack([(train[column]).str.lower(), (test[column]).str.lower()])\n    cat1 = prepareData(column,raw_text)\n    train[column + '_seq'] = [variableFromSentence(cat1,normalizeLine(sentence.lower())[0]) for sentence in train[column]]\n    test[column + '_seq'] = [variableFromSentence(cat1,normalizeLine(sentence.lower())[0]) for sentence in test[column]]\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"token_fit('name')\ntoken_fit('item_description')\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# handle price using log and scale, excep test data don't have 'price' so we make test taget =0\ntest[\"target\"] = 0\ntrain[\"target\"] = np.log(train.price+1)\ntarget_scaler = MinMaxScaler(feature_range=(-1, 1))\ntrain[\"target\"] = target_scaler.fit_transform(train.target.values.reshape(-1,1))\ntrain.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(train.target).hist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Chuẩn bị mô hình","metadata":{}},{"cell_type":"code","source":"# Split train/validation data\ndtrain, dvalid = train_test_split(train, random_state=123, train_size=0.99)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#SEQUENCES VARIABLES ANALYSIS\nmax_name_seq = np.max([np.max(train.name_seq.apply(lambda x: len(x))), np.max(test.name_seq.apply(lambda x: len(x)))])\nmax_item_description_seq = np.max([np.max(train.item_description_seq.apply(lambda x: len(x)))\n                                   , np.max(test.item_description_seq.apply(lambda x: len(x)))])\nprint(\"max name seq \"+str(max_name_seq))\nprint(\"max item desc seq \"+str(max_item_description_seq))\n\n#EMBEDDINGS MAX VALUE\nMAX_NAME_SEQ = 10\nMAX_ITEM_DESC_SEQ = 75\nMAX_TEXT_NAME = np.max([np.max(train.name_seq.max()) \n                   , np.max(test.name_seq.max())])+2\nMAX_TEXT_ITEM = np.max([np.max(train.item_description_seq.max()) \n                   , np.max(test.item_description_seq.max())])+2\nMAX_GEN_CATEGORY = np.max([train.general_cat_index.max(), test.general_cat_index.max()])+1\nMAX_SUB_CAT1_CATEGORY = np.max([train.subcat_1_index.max(), test.subcat_1_index.max()])+1\nMAX_SUB_CAT2_CATEGORY = np.max([train.subcat_2_index.max(), test.subcat_2_index.max()])+1\nMAX_BRAND = np.max([train.brand_name_index.max(), test.brand_name_index.max()])+1\nMAX_CONDITION = np.max([train.item_condition_id.max(), test.item_condition_id.max()])+1\nMAX_CATEGORY_NAME = np.max([train.category_name_index.max(), test.category_name_index.max()])+1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pad(tensor, length):\n    if length > tensor.size(0):\n        return torch.cat([tensor, tensor.new(length - tensor.size(0), *tensor.size()[1:]).zero_()])\n    else:\n        return torch.split(tensor, length, dim=0)[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert ndarrays in sample to Tensors\nclass ToTensor(object):\n\n    def __call__(self, sample):\n        name, item_desc,brand_name,cat_name,general_category,subcat1_category,subcat2_category, \\\n        item_condition,shipping,target = sample['name'], sample['item_desc'], sample['brand_name'], \\\n        sample['cat_name'], sample['general_category'], sample['subcat1_category'], sample['subcat2_category'], \\\n        sample['item_condition'], sample['shipping'],sample['target']    \n        return {'name': pad(torch.from_numpy(np.asarray(name)).long().view(-1),MAX_NAME_SEQ),\n                'item_desc': pad(torch.from_numpy(np.asarray(item_desc)).long().view(-1),MAX_ITEM_DESC_SEQ),\n               'brand_name':torch.from_numpy(np.asarray(brand_name)),\n               'cat_name':torch.from_numpy(np.asarray(cat_name)),\n               'general_category':torch.from_numpy(np.asarray(general_category)),\n               'subcat1_category':torch.from_numpy(np.asarray(subcat1_category)),\n               'subcat2_category':torch.from_numpy(np.asarray(subcat2_category)),\n               'item_condition':torch.from_numpy(np.asarray(item_condition)),\n               'shipping':torch.torch.from_numpy(np.asarray(shipping)),\n               'target':torch.from_numpy(np.asarray(target))}\n\n#  Define the Dataset to use in a DataLoader\nclass MercariDataset(Dataset):\n\n    def __init__(self, data_pd, transform=None):\n        self.mercari_frame = data_pd\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.mercari_frame)\n\n    def __getitem__(self, idx):\n        name = [self.mercari_frame.name_seq.iloc[idx]]\n        item_desc = [self.mercari_frame.item_description_seq.iloc[idx]]\n        brand_name = [self.mercari_frame.brand_name_index.iloc[idx]]\n        cat_name = [self.mercari_frame.category_name_index.iloc[idx]]\n        general_category = [self.mercari_frame.general_cat_index.iloc[idx]]\n        subcat1_category = [self.mercari_frame.subcat_1_index.iloc[idx]]\n        subcat2_category = [self.mercari_frame.subcat_2_index.iloc[idx]]\n        item_condition = [self.mercari_frame.item_condition_id.iloc[idx]]\n        shipping = [self.mercari_frame.shipping.iloc[idx]]\n        target = [self.mercari_frame.target.iloc[idx]]\n        sample = {'name': name,\n                'item_desc': item_desc,\n               'brand_name': brand_name,\n               'cat_name': cat_name,   \n               'general_category': general_category,\n               'subcat1_category': subcat1_category,\n               'subcat2_category': subcat2_category,\n               'item_condition': item_condition,\n               'shipping': shipping,\n               'target': target}\n\n        if self.transform:\n            sample = self.transform(sample)\n\n        return sample","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# take data into batch dataloader \n\n### Test data\nmercari_test =  MercariDataset(test,transform=transforms.Compose([ToTensor()]))           \ntest_sizes = len(mercari_test)\ntest_dataloaders = torch.utils.data.DataLoader(mercari_test, batch_size=50, shuffle=False)\n\n### Train data\nmercari_datasets = {'train': MercariDataset(dtrain,transform=transforms.Compose([ToTensor()])), \n                    'val': MercariDataset(dvalid,transform=transforms.Compose([ToTensor()]))}\ndataset_sizes = {x: len(mercari_datasets[x]) for x in ['train', 'val']}\nmercari_dataloaders = {x: torch.utils.data.DataLoader(mercari_datasets[x], batch_size=50, shuffle=True) for x in ['train', 'val']}\n\n\nprint(\"number of data in mercari train: \", dataset_sizes['train'])\nprint(\"number of batch in mercari train: \", dataset_sizes['train']/50)\nprint()\n\nprint(\"number of data in mercari validate: \", dataset_sizes['val'])\nprint(\"number of batch in mercari validate: \", dataset_sizes['val']/50)\nprint()\n\n\nprint(\"number of data in mercari test: \", test_sizes)\nprint(\"number of batch in mercari test: \", test_sizes/50)\nprint()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\n\n# Definition of the Pytorch Model\nclass RegressionNeural(nn.Module):\n    def __init__(self, max_sizes):\n        super(RegressionNeural, self).__init__()\n        self.name_embedding = nn.Embedding(max_sizes['max_text_name'].item()+100000, 50)\n        self.item_embedding = nn.Embedding(max_sizes['max_text_item'].item()+100000, 50)\n        self.brand_embedding = nn.Embedding(max_sizes['max_brand'].item(), 10)\n        self.gencat_embedding = nn.Embedding(max_sizes['max_gen_category'].item(), 10)\n        self.subcat1_embedding = nn.Embedding(max_sizes['max_subcat1_category'].item(), 10)\n        self.subcat2_embedding = nn.Embedding(max_sizes['max_subcat2_category'].item(), 10)\n        self.condition_embedding = nn.Embedding(max_sizes['max_condition'].item(), 5)\n        self.catname_embedding = nn.Embedding(max_sizes['max_cat_name'].item(), 10)\n        \n        self.conv1_name = nn.Conv1d(50, 1, 2, stride=1)\n        self.conv2_name = nn.Conv1d(16, 8, 2, stride=1)\n        self.conv3_name = nn.Conv1d(8, 4, 2, stride=1)\n        \n        self.conv1_item_desc = nn.Conv1d(50, 1, 5, stride=5) \n        self.conv2_item_desc = nn.Conv1d(64, 16, 5, stride=1)\n        self.conv3_item_desc = nn.Conv1d(16, 4, 5, stride=1)\n        \n        self.dropout = nn.Dropout(p=0.2)\n        \n        self.input_fc1_count = 50 \n        self.fc1 = nn.Linear(self.input_fc1_count, 64)\n        self.fc2 = nn.Linear(64,32)\n        self.fc3 = nn.Linear(32,1)\n        \n        self.relu = nn.ReLU()  \n            \n    def forward(self, x, batchsize):\n        embed_name = self.name_embedding(x['name']) \n        embed_name = F.relu(self.conv1_name(embed_name.transpose(1,2)))\n        embed_item = self.item_embedding(x['item_desc'])\n        embed_item = F.relu(self.conv1_item_desc(embed_item.transpose(1,2)))\n        embed_brand = self.brand_embedding(x['brand_name'])\n        embed_gencat = self.gencat_embedding(x['general_category'])\n        embed_subcat1 = self.subcat1_embedding(x['subcat1_category'])\n        embed_subcat2 = self.subcat2_embedding(x['subcat2_category'])\n        embed_condition = self.condition_embedding(x['item_condition'])\n        embed_catname = self.catname_embedding(x['cat_name'])\n        \n        out = torch.cat((embed_brand.view(batchsize,-1), embed_catname.view(batchsize,-1), \\\n                         embed_condition.view(batchsize,-1),embed_name.view(batchsize,-1), \\\n                         embed_item.view(batchsize,-1),x['shipping']),1)\n        \n        out = (self.fc1(out))\n        out = F.relu(self.dropout(out))\n        out = (self.fc2(out))\n        out = (self.dropout(out))\n        out = self.fc3(out)\n        return out\n\nmax_sizes = {'max_text_name':MAX_TEXT_NAME,'max_text_item':MAX_TEXT_ITEM,'max_name_seq':MAX_NAME_SEQ,'max_item_desc_seq':MAX_ITEM_DESC_SEQ, \\\n             'max_brand':MAX_BRAND,'max_cat_name':MAX_CATEGORY_NAME,'max_gen_category':MAX_GEN_CATEGORY,\\\n             'max_subcat1_category':MAX_SUB_CAT1_CATEGORY,'max_subcat2_category':MAX_SUB_CAT2_CATEGORY,\\\n             'max_condition':MAX_CONDITION} \n\ndeep_learn_model = RegressionNeural(max_sizes)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_sizes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(model, criterion, optimizer, num_epochs=1, print_every = 1000, device=\"cpu\"):\n    start = time.time()\n\n    best_acc = 0.0\n    print_loss_total = 0 \n\n    for epoch in range(num_epochs):\n        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n        print('-' * 10)\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train(True)  # Set model to training mode\n            else:\n                model.train(False)  # Set model to evaluate mode\n\n            running_loss = 0.0\n            num_batches = dataset_sizes[phase]/50.\n            print(\"number of batch in\", phase, np.uint64(num_batches))\n            print(\"comming....\")\n            \n            for i_batch, sample_batched in enumerate(mercari_dataloaders[phase]): \n            # get inputs\n                inputs = {'name':Variable(sample_batched['name']).to(device), \n                          'item_desc':Variable(sample_batched['item_desc']).to(device), \\\n                        'brand_name':Variable(sample_batched['brand_name']).to(device), \\\n                        'cat_name':Variable(sample_batched['cat_name']).to(device), \\\n                        'general_category':Variable(sample_batched['general_category']).to(device), \\\n                        'subcat1_category':Variable(sample_batched['subcat1_category']).to(device), \\\n                        'subcat2_category':Variable(sample_batched['subcat2_category']).to(device), \\\n                        'item_condition':Variable(sample_batched['item_condition']).to(device), \\\n                        'shipping':Variable(sample_batched['shipping'].float()).to(device)}\n                \n                # get price\n                prices = Variable(sample_batched['target'].float().to(device))  \n        \n                batch_size = len(sample_batched['shipping'])   \n                optimizer.zero_grad()\n                \n                model.to(device)\n                prices.to(device)\n                \n                outputs = model(inputs, batch_size)\n                loss = criterion(outputs, prices)\n\n                if phase == 'train':\n                    loss.backward()\n                    optimizer.step()\n\n                running_loss += loss.data\n                print_loss_total += loss.data                \n                \n                if (i_batch+1) % print_every == 0:\n                    print_loss_avg = print_loss_total / print_every\n                    print_loss_total = 0\n                    print('(%d %d%%) Loss_avg: %.4f' % (i_batch, i_batch / num_batches*100, print_loss_avg), end=', ')\n                    time_ongoing = time.time() - start\n                    print('Training in {:.0f}m {:.0f}s'.format(time_ongoing // 60, time_ongoing % 60))\n            epoch_loss = running_loss / num_batches\n            print('{} Loss: {:.4f}'.format(phase, epoch_loss), end=\"\\n\\n\")\n            \n        print()\n\n    time_elapsed = time.time() - start\n    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n\n    return model        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion = nn.MSELoss()\noptimizer_ft = optim.SGD(deep_learn_model.parameters(), lr=0.001, momentum=0.9)\n\n\ndevice = torch.device('cuda')\ntrain_model(deep_learn_model,criterion,optimizer_ft,num_epochs=20, device=device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predict the model results against test data\ndef validate(model, dataloader, print_every = 500, device = 'cpu'):\n    start = time.time()\n    num_batches = len(dataloader)\n    print('num batches: ',num_batches)\n    \n    y_pred_full = np.array([])\n    for i_batch, sample_batched in enumerate(dataloader): \n        inputs = {'name':Variable(sample_batched['name']).to(device), \n                  'item_desc':Variable(sample_batched['item_desc']).to(device), \\\n                  'brand_name':Variable(sample_batched['brand_name']).to(device), \\\n                  'cat_name':Variable(sample_batched['cat_name']).to(device), \\\n                  'general_category':Variable(sample_batched['general_category']).to(device), \\\n                  'subcat1_category':Variable(sample_batched['subcat1_category']).to(device), \\\n                  'subcat2_category':Variable(sample_batched['subcat2_category']).to(device), \\\n                  'item_condition':Variable(sample_batched['item_condition']).to(device), \\\n                  'shipping':Variable(sample_batched['shipping'].float()).to(device)}\n        batch_size = len(sample_batched['shipping'])\n        \n        model.to(device)\n        try:\n            outputs = model(inputs,batch_size)\n\n            val_preds = target_scaler.inverse_transform(outputs.cpu().data.numpy())\n            val_preds = np.exp(val_preds)-1\n            y_pred = val_preds[:,0]\n        except:\n            print(i_batch, \"err, make 0 price\")\n            y_pred = np.zeros(batch_size)\n        \n        y_pred_full= np.append(y_pred_full,y_pred)\n        if (i_batch+1) % print_every == 0:\n            print('(%d %d%%)' % (i_batch, i_batch / num_batches*100), end=\",\")\n#             print('inputname shape, input item desc:', inputs['name'].shape, inputs['item_desc'].shap)\n             \n    return y_pred_full","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_test = validate(deep_learn_model,test_dataloaders,device=device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit = pd.DataFrame({'test_id': test['test_id'], \n                    'price': y_pred_test})\nsubmit.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install torchviz\n# from torchviz import make_dot\n\n# for i_batch, sample_batched in enumerate(mercari_dataloaders['train']): \n#     inputs = {'name':Variable(sample_batched['name']), 'item_desc':Variable(sample_batched['item_desc']), \\\n#         'brand_name':Variable(sample_batched['brand_name']), \\\n#         'cat_name':Variable(sample_batched['cat_name']), \\\n#         'general_category':Variable(sample_batched['general_category']), \\\n#         'subcat1_category':Variable(sample_batched['subcat1_category']), \\\n#         'subcat2_category':Variable(sample_batched['subcat2_category']), \\\n#         'item_condition':Variable(sample_batched['item_condition']), \\\n#         'shipping':Variable(sample_batched['shipping'].float())}\n#     prices = Variable(sample_batched['target'].float())   \n#     batch_size = len(sample_batched['shipping'])\n#     if i_batch ==0:\n#         a = inputs\n#         break\n    \n    \n# batch = a\n# deep_learn_model.to('cpu')\n# deep_learn_model.eval()\n# yhat = deep_learn_model(batch, 50)\n# make_dot(yhat, params=dict(list(deep_learn_model.named_parameters()))).render(\"rnn_torchviz\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_log_error\ndef RMSLE(y_true:np.ndarray, y_pred:np.ndarray) -> np.float64:\n    return np.sqrt(mean_squared_log_error(y_true, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_val = validate(deep_learn_model,mercari_dataloaders['val'],device=device)\ny_true_val = dvalid['price']\n\nprint('RMSLE in validate data: ', RMSLE(y_true_val, y_pred_val))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_train = validate(deep_learn_model,mercari_dataloaders['train'],device=device)\ny_true_train = dtrain['price']\n\nprint('RMSLE in validate data: ', RMSLE(y_true_train, y_pred_train))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}