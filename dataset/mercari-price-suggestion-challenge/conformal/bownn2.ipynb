{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport warnings\nwarnings.filterwarnings('ignore')\nimport matplotlib.pyplot as plt \nplt.style.use('dark_background')\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport wordcloud\nimport pandas\nfrom multiprocessing import Pool\nfrom sklearn.feature_extraction.text import TfidfTransformer  \nfrom sklearn.feature_extraction.text import CountVectorizer\nimport re\nimport gc\nfrom nltk.util import ngrams\nfrom sklearn import preprocessing\nfrom itertools import combinations\nfrom scipy.sparse import csr_matrix, hstack\nimport time\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import train_test_split\nfrom keras.layers import Input, Dropout, Dense, concatenate, PReLU, BatchNormalization\nfrom keras.optimizers import Adam\nfrom keras.models import Model\nfrom keras.initializers import he_uniform\nimport keras.backend.tensorflow_backend as KTF\nimport tensorflow as tf\nfrom keras import regularizers\nfrom keras.callbacks import LearningRateScheduler\nimport keras.backend as K\n\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nsess = tf.Session(config=config)\nKTF.set_session(sess)\n\n\ndef mse(y_true, y_pred):\n    return KTF.mean(KTF.square(y_pred - y_true))\n\nclass Clock:\n    \n    def __enter__(self):\n        self.begin_time = time.time()\n        \n    def __exit__(self, exec_type, exec_value, exec_trace):\n        print(\"time:\", time.time() - self.begin_time, \"s\")\n\n\ndef parallelize_dataframe(df, func, cores=4):\n    \"\"\"\n    对一个dataframe并行计算, df是一个dataframe, func是处理一个dataframe的函数, 而不是\n    处理某一列的函数, 如果我们要把处理某一列的函数封装成func, 需要在用一个处理dataframe的\n    函数对这个处理单列的函数进行封装, 最后会返回这个处理完的dataframe\n    \"\"\"\n    # 分割data frame, array_split用来在垂直方向上分割dataframe\n    df_split = np.array_split(df, cores)\n    pool = Pool(cores)\n    \n    # 并行计算, 最终结果是按照df_split的顺序返回的, 并在axis=0, 也就是垂直进行连接\n    df = pd.concat(pool.map(func, df_split), axis=0)\n    pool.close()\n    pool.join()\n    \n    return df\n\n\ndef clean_str(text, max_text_length=60):\n    \"\"\"\n    去掉特殊字符, 只保留数字和字母, 并且把数字和字母分开, 然后用空格分割各个单词\n    处理如果我们的文本超过了max_text_length, 就会被截断成max_text_length.\n    \"\"\"\n    try:\n        # 进行切分后合并, 最多取60个词\n        text = ' '.join([w for w in text.split()[:max_text_length]] )        \n        text = text.lower()\n        # 特殊字符\n        text = re.sub(u\"é\", u\"e\", text)\n        text = re.sub(u\"ē\", u\"e\", text)\n        text = re.sub(u\"è\", u\"e\", text)\n        text = re.sub(u\"ê\", u\"e\", text)\n        text = re.sub(u\"à\", u\"a\", text)\n        text = re.sub(u\"â\", u\"a\", text)\n        text = re.sub(u\"ô\", u\"o\", text)\n        text = re.sub(u\"ō\", u\"o\", text)\n        text = re.sub(u\"ü\", u\"u\", text)\n        text = re.sub(u\"ï\", u\"i\", text)\n        text = re.sub(u\"ç\", u\"c\", text)\n        text = re.sub(u\"\\u2019\", u\"'\", text)\n        text = re.sub(u\"\\xed\", u\"i\", text)\n        # 简写\n        text = re.sub(u\"w\\/\", u\" with \", text)\n        \n        # 去掉非数字以及字母符\n        text = re.sub(u\"[^a-z0-9]\", \" \", text)\n        # 如果pattern中含有括号, 那么括号中匹配的也能被保留下来, 将数字与字母分开\n        text = u\" \".join(re.split('(\\d+)',text))\n        # 去掉任何空白字符, 转换为单个空格\n        text = re.sub( u\"\\s+\", u\" \", text).strip()\n    except:\n        text = np.NaN\n        \n    return text\n\ndef take(generator):\n    try:\n        for elem in generator:\n            yield elem\n    except StopIteration:\n        return\n\n\ndef fill_brandname(name, brand_name_set):\n    try:\n        for size in [4, 3, 2, 1]:\n            for x in take(ngrams(name.split(), size)):\n                if \" \".join(x) in brand_name_set:\n                    return \" \".join(x)\n        return np.NaN\n    except:\n        return np.NaN\n\n    \ndef clean_str_df(x):\n    return x.apply(lambda y: clean_str(y))\n\n\ndef fill_brandname_df(x):\n    # x是dataframe\n    global brand_name_set\n    # 返回的也应该是一个dataframe\n    return x.apply(lambda y: fill_brandname(y, brand_name_set))\n\n\ndef lg(text):\n    text = [x for x in text.split() if x != '']\n    return len(text)\n\n\ndef tokenize(text):\n    return [w for w in text.split()]\n\n\ndef get_sample(train_file, test_file, max_text_length=60):\n    # 读取样本\n    print(\"读取样本\")\n    with Clock():\n        train = pd.read_csv(train_file, sep='\\t', encoding='utf-8')\n        train.drop(columns=[\"train_id\"], inplace=True)\n        train[\"price\"] = np.log1p(train[\"price\"].values)\n        print('train shape:', train.shape, \"columns:\", train.columns)\n        test = pd.read_csv(test_file, sep='\\t', encoding='utf-8')\n        test.drop(columns=[\"test_id\"], inplace=True)\n        print('test shape:', test.shape, \"columns:\", test.columns)\n        sample = pd.concat([train.drop(columns=[\"price\"], axis=0), test], axis=0)\n        gc.collect()\n    \n    with Clock():\n        print(\"处理特征\")\n        # 对item_description进行填充\n        sample[\"item_description\"].fillna(\"\", inplace=True) #na\n        # No description yet\n        sample[\"item_description\"] = sample[\"item_description\"].apply(lambda x : \n                                                                     x.replace('No description yet',''))\n\n        # 清理字符串\n        sample[\"item_description\"] = parallelize_dataframe(sample[\"item_description\"], clean_str_df)\n        sample[\"name\"] = parallelize_dataframe(sample[\"name\"], clean_str_df)\n        sample[\"brand_name\"] = parallelize_dataframe(sample[\"brand_name\"], clean_str_df)\n\n        # 获取brand_name和name大小的对应关系\n        global brand_name_set \n        brand_name_set = sample.groupby(\"brand_name\").size()\n        # 在name中抽取brand name进行对brand name的填充\n        sample.loc[sample[\"brand_name\"].isnull(), \"brand_name\"] = parallelize_dataframe(\n            sample.loc[sample[\"brand_name\"].isnull(), \"name\"], \n            fill_brandname_df)\n        # 填充na\n        sample[\"brand_name\"].fillna(\"\", inplace=True)\n        sample[\"item_condition_id\"].fillna(2, inplace=True)\n\n        # 填充category_name\n        sample[\"category_name\"].fillna(\"//\", inplace=True)\n        sample[\"catery_name_first\"] =  sample[\"category_name\"].apply(lambda x: x.split(\"/\")[0])\n        sample[\"catery_name_second\"] = sample[\"category_name\"].apply(lambda x: x.split(\"/\")[1])\n        sample[\"catery_name_third\"] = sample[\"category_name\"].apply(lambda x: x.split(\"/\")[2])\n        sample['category_name'] = sample['category_name'].apply( lambda x: ' '.join(x.split('/') ).strip())\n\n        sample['nb_words_item_description'] = sample['item_description'].apply(lg).astype(np.uint16)\n        sample['nb_words_item_description'] = 1.0 * sample['nb_words_item_description'] / max_text_length\n\n        for x in ['brand_name', 'category_name', 'catery_name_first', 'catery_name_second', \n                  'catery_name_third']:\n            le = preprocessing.LabelEncoder()\n            sample[x] = le.fit_transform(sample[x])\n            \n        sample[\"item_condition_id\"] = 1.0 * sample[\"item_condition_id\"] / 5\n        sample[\"old_name\"] = sample[\"name\"].copy()\n        sample[\"brand_cat\"] = \"cat1_\" + sample[\"catery_name_first\"].astype(str) + \" \" + \\\n                    \"cat2_\" + sample[\"catery_name_second\"].astype(str) + \" \" + \\\n                    \"cat3_\" +  sample[\"catery_name_third\"].astype(str) + \" \" + \\\n                    \"brand_\" + sample[\"brand_name\"].astype(str)\n        sample[\"name\"] = sample[\"brand_cat\"] + \" \" + sample[\"name\"]\n        # 融合了cate, brand, name, desc\n        sample[\"name_desc\"] = sample[\"name\"] + \" \" + sample[\"item_description\"].apply(lambda x: \" \".join(x.split()[:5]))  \n        gc.collect()\n        \n    print(\"构造文本特征:\")\n    split_num = train.shape[0]\n    with Clock():\n        print(\"brand_cat\")\n        vectorizer_brandcat = CountVectorizer(lowercase=True, tokenizer=tokenize, \n                                     ngram_range=(1, 3), min_df=2, max_features=100000, \n                                     binary=True, dtype=np.int8)\n        vectorizer_brandcat.fit(sample[:split_num]['brand_cat'])\n        sparse_data_brandcat = vectorizer_brandcat.transform(sample['brand_cat'])\n        del vectorizer_brandcat\n        sample.drop(columns=[\"brand_cat\"], inplace=True)\n        gc.collect()\n        \n        print(\"old_name\")\n        vectorizer_name = CountVectorizer(lowercase=True, analyzer=\"char\", \n                                     ngram_range=(2, 4), min_df=5, max_features=100000, \n                                     binary=True, dtype=np.int8)\n        vectorizer_name.fit(sample[:train.shape[0]]['old_name'])\n        sparse_data_name = vectorizer_name.transform(sample['old_name'])\n        del vectorizer_name\n        sample.drop(columns=[\"old_name\"], inplace=True)\n        gc.collect()\n        \n        print(\"item_description\")\n        vectorizer_desc = CountVectorizer(lowercase=True, tokenizer=tokenize, stop_words=\"english\",\n                                     ngram_range=(1, 1), min_df=20, max_features=100000, \n                                     binary=True, dtype=np.int8)\n        vectorizer_desc.fit(sample[:train.shape[0]][\"item_description\"])\n        sparse_data_desc = vectorizer_desc.transform(sample['item_description'])\n        del vectorizer_desc\n        sample.drop(columns=[\"item_description\"], inplace=True)\n        gc.collect()\n        \n        \n    print(\"构建统计特征:\")\n    with Clock():\n        train_new = sample[:train.shape[0]].copy()\n        train_new[\"price\"] = train[\"price\"]\n        for x in ['catery_name_first', 'catery_name_second', 'catery_name_third', 'category_name', 'brand_name'  ]:\n            tmp = train_new.groupby(x)[\"price\"].mean().astype(np.float32)\n            sample['mean_price_' + x] = sample[x].map(tmp).astype(np.float32)\n            sample['mean_price_' + x].fillna(tmp.mean(), inplace=True)\n        gc.collect()\n    \n    print(\"构造样本:\")\n    with Clock():\n        x = {\n           'sparse_data_name': sparse_data_name[:split_num],\n           'sparse_data_brandcat': sparse_data_brandcat[:split_num],\n           'sparse_data_desc': sparse_data_desc[:split_num],\n           'item_condition': np.array(sample['item_condition_id'])[:split_num],\n           'shipping': np.array(sample[\"shipping\"])[:split_num],\n           'mean_price_catery_name_second': np.array(sample[\"mean_price_catery_name_second\"])[:split_num],\n           'nb_words_item_description': np.array(sample[\"nb_words_item_description\"])[:split_num]\n              }\n        test = {\n           'sparse_data_name': sparse_data_name[split_num:],\n           'sparse_data_brandcat':  sparse_data_brandcat[split_num:],\n           'sparse_data_desc': sparse_data_desc[split_num:],\n           'item_condition': np.array(sample['item_condition_id'])[split_num:],\n           'shipping': np.array(sample[\"shipping\"])[split_num:],\n           'mean_price_catery_name_second': np.array(sample[\"mean_price_catery_name_second\"])[split_num:],\n           'nb_words_item_description': np.array(sample[\"nb_words_item_description\"])[split_num:]\n              }\n        y = train.price.values\n    return x, y, test \n\n\ndef model(len_sparse_data_name, len_sparse_data_brandcat, len_sparse_data_desc):\n    sparse_data_name = Input(shape=[len_sparse_data_name], name=\"sparse_data_name\", dtype='float32', \n                             sparse=True)\n    sparse_data_brandcat = Input(shape=[len_sparse_data_brandcat], name=\"sparse_data_brandcat\",\n                                 dtype='float32', sparse=True)\n    sparse_data_desc = Input(shape=[len_sparse_data_desc], name=\"sparse_data_desc\",\n                                 dtype='float32', sparse=True)\n    item_condition = Input(shape=[1], name=\"item_condition\", dtype='float32')\n    shipping = Input(shape=[1], name=\"shipping\", dtype='float32')\n    mean_price_catery_name_second = Input(shape=[1], name=\"mean_price_catery_name_second\", \n                                          dtype='float32')\n    nb_words_item_description = Input(shape=[1], name=\"nb_words_item_description\", \n                                          dtype='float32')\n    \n    x = PReLU()(Dense(50)(sparse_data_name))\n    y = PReLU()(Dense(50)(sparse_data_brandcat))\n    z = PReLU()(Dense(50)(sparse_data_desc))\n    x = concatenate([x, y, z, item_condition, shipping, mean_price_catery_name_second, \n                     nb_words_item_description])\n    x = PReLU()(Dense(50)(x))\n    x = Dense(1)(x)\n    m = Model([sparse_data_name, sparse_data_brandcat, sparse_data_desc, \n              item_condition, shipping, mean_price_catery_name_second, \n                     nb_words_item_description], x)\n\n    optimizer = Adam(0.002)\n    m.compile(loss=\"mse\", optimizer=optimizer)\n    return m","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"gc.collect()\npath = \"../input/\"\nx, y, test = get_sample(path + \"train.tsv\", path + \"test_stg2.tsv\", max_text_length=60)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m = model(x[\"sparse_data_name\"].shape[1], x[\"sparse_data_brandcat\"].shape[1],\n          x[\"sparse_data_desc\"].shape[1])\nm.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m.fit(x, y - np.mean(y), 2048, epochs=3)\npred = m.predict(test, 2048) + np.mean(y)\npred = np.expm1(pred)\npred[pred < 3] = 3\npred[pred > 1000] = 1000\ntest_id= pd.read_csv(path + \"test_stg2.tsv\", sep='\\t', encoding='utf-8')[[\"test_id\"]]\ntest_id[\"price\"] = pred\ntest_id.to_csv(\"ans.csv\", index=False)\ndel x\ndel y\ndel test\ndel pred\ndel test_id\ndel m\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}