{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt \nplt.style.use('dark_background')\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport wordcloud\nimport pandas\nfrom multiprocessing import Pool\nimport nltk\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\nnltk.download('wordnet')\nfrom nltk import WordNetLemmatizer, pos_tag, word_tokenize\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfTransformer  \nfrom sklearn.feature_extraction.text import CountVectorizer\nimport re\nimport gc\nfrom nltk.util import ngrams\nfrom sklearn import preprocessing\nfrom itertools import combinations\nfrom scipy.sparse import csr_matrix, hstack\nimport time\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import train_test_split\n\n\nclass Clock:\n    \n    def __enter__(self):\n        self.begin_time = time.time()\n        \n    def __exit__(self, exec_type, exec_value, exec_trace):\n        print(\"time:\", time.time() - self.begin_time, \"s\")\n\n\ndef parallelize_dataframe(df, func, cores=4):\n    \"\"\"\n    对一个dataframe并行计算, df是一个dataframe, func是处理一个dataframe的函数, 而不是\n    处理某一列的函数, 如果我们要把处理某一列的函数封装成func, 需要在用一个处理dataframe的\n    函数对这个处理单列的函数进行封装, 最后会返回这个处理完的dataframe\n    \"\"\"\n    # 分割data frame, array_split用来在垂直方向上分割dataframe\n    df_split = np.array_split(df, cores)\n    pool = Pool(cores)\n    \n    # 并行计算, 最终结果是按照df_split的顺序返回的, 并在axis=0, 也就是垂直进行连接\n    df = pd.concat(pool.map(func, df_split), axis=0)\n    pool.close()\n    pool.join()\n    \n    return df\n\n\ndef clean_str(text, max_text_length=60):\n    \"\"\"\n    去掉特殊字符, 只保留数字和字母, 并且把数字和字母分开, 然后用空格分割各个单词\n    处理如果我们的文本超过了max_text_length, 就会被截断成max_text_length.\n    \"\"\"\n    try:\n        # 进行切分后合并, 最多取60个词\n        text = ' '.join([w for w in text.split()[:max_text_length]] )        \n        text = text.lower()\n        # 特殊字符\n        text = re.sub(u\"é\", u\"e\", text)\n        text = re.sub(u\"ē\", u\"e\", text)\n        text = re.sub(u\"è\", u\"e\", text)\n        text = re.sub(u\"ê\", u\"e\", text)\n        text = re.sub(u\"à\", u\"a\", text)\n        text = re.sub(u\"â\", u\"a\", text)\n        text = re.sub(u\"ô\", u\"o\", text)\n        text = re.sub(u\"ō\", u\"o\", text)\n        text = re.sub(u\"ü\", u\"u\", text)\n        text = re.sub(u\"ï\", u\"i\", text)\n        text = re.sub(u\"ç\", u\"c\", text)\n        text = re.sub(u\"\\u2019\", u\"'\", text)\n        text = re.sub(u\"\\xed\", u\"i\", text)\n        # 简写\n        text = re.sub(u\"w\\/\", u\" with \", text)\n        \n        # 去掉非数字以及字母符\n        text = re.sub(u\"[^a-z0-9]\", \" \", text)\n        # 如果pattern中含有括号, 那么括号中匹配的也能被保留下来, 将数字与字母分开\n        text = u\" \".join(re.split('(\\d+)',text))\n        # 去掉任何空白字符, 转换为单个空格\n        text = re.sub( u\"\\s+\", u\" \", text).strip()\n    except:\n        text = np.NaN\n        \n    return text\n\n\ndef fill_brandname(name, brand_name_set):\n    try:\n        for size in [4, 3, 2, 1]:\n            for x in ngrams(name.split(), size):\n                if \" \".join(x) in brand_name_set:\n                    return \" \".join(x)\n        return np.NaN\n    except:\n        return np.NaN\n\n    \n    \ndef clean_str_df(x):\n    return x.apply(lambda y: clean_str(y))\n\n\ndef fill_brandname_df(x):\n    # x是dataframe\n    global brand_name_set\n    # 返回的也应该是一个dataframe\n    return x.apply(lambda y: fill_brandname(y, brand_name_set))\n\n\ndef lg(text):\n    text = [x for x in text.split() if x != '']\n    return len(text)\n\n\ndef word_doc_counts(words, word_doc_count_dic):\n    # 文档内去重\n    words = set(words.split())\n    for x in words:\n        if x in word_doc_count_dic:\n            word_doc_count_dic[x] = word_doc_count_dic[x] + 1\n        else:\n            word_doc_count_dic[x] = 1\n\n            \ndef remove_rare_words(words, word_doc_count_dic):\n    stops = set(stopwords.words(\"english\"))\n    return \" \".join([x for x in words.split() if x in word_doc_count_dic and x not in stops])\n\n\ndef lemmatize_all(sentence):\n    ans = []\n    stops = set(stopwords.words(\"english\"))\n    \n    wnl = WordNetLemmatizer()\n    for word, tag in pos_tag(word_tokenize(sentence)):\n        if tag.startswith('NN'):\n            new_word = wnl.lemmatize(word, pos='n')\n        elif tag.startswith('VB'):\n            new_word = wnl.lemmatize(word, pos='v')\n        elif tag.startswith('JJ'):\n            new_word = wnl.lemmatize(word, pos='a')\n        elif tag.startswith('R'):\n            new_word = wnl.lemmatize(word, pos='r')\n        else:\n            new_word = word\n        # 去除停用词\n        if new_word in stops:\n            continue\n        ans.append(new_word)\n    \n    return \" \".join(ans)\n\n\ndef create_bi_gram(text, word_doc_count_dic):\n    ret = []\n        \n    for x in combinations(np.unique(lemmatize_all(text).split()), 2):\n        if (x[0] + x[1]) in word_doc_count_dic:\n            new_word = x[0] + x[1]\n            if len(new_word) > 0:\n                ret.append(new_word)\n                continue\n                \n        if (x[1] + x[0]) in word_doc_count_dic:\n            new_word = x[1] + x[0]\n            if len(new_word) > 0:\n                ret.append(new_word)\n                continue\n        \n        if len(x[0] + x[1]) > 0:\n            ret.append(x[0] + \"___\" + x[1])\n        \n    return \" \".join(ret)     \n\t\n\t\ndef create_bigrams_df(x):\n    global word_doc_count_dic\n    return x.apply(lambda y: create_bi_gram(y, word_doc_count_dic))\n\n\ndef tokenize(text):\n    return [w for w in text.split()]\n\ndef get_sample(train_file, test_file, max_text_length=60):\n    # 读取样本\n    print(\"读取样本\")\n    with Clock():\n        train = pd.read_csv(train_file, sep='\\t', encoding='utf-8')\n        train.drop(columns=[\"train_id\"], inplace=True)\n        train[\"price\"] = np.log1p(train[\"price\"].values)\n        print('train shape:', train.shape, \"columns:\", train.columns)\n        test = pd.read_csv(test_file, sep='\\t', encoding='utf-8')\n        test.drop(columns=[\"test_id\"], inplace=True)\n        print('test shape:', test.shape, \"columns:\", test.columns)\n        sample = pd.concat([train.drop(columns=[\"price\"], axis=0), test], axis=0)\n        gc.collect()\n    \n    with Clock():\n        print(\"处理特征\")\n        # 对item_description进行填充\n        sample[\"item_description\"].fillna(\"\", inplace=True) #na\n        # No description yet\n        sample[\"item_description\"] = sample[\"item_description\"].apply(lambda x : \n                                                                     x.replace('No description yet',''))\n\n        # 清理字符串\n        sample[\"item_description\"] = parallelize_dataframe(sample[\"item_description\"], clean_str_df)\n        sample[\"name\"] = parallelize_dataframe(sample[\"name\"], clean_str_df)\n        sample[\"brand_name\"] = parallelize_dataframe(sample[\"brand_name\"], clean_str_df)\n\n        # 获取brand_name和name大小的对应关系\n        global brand_name_set \n        brand_name_set = sample.groupby(\"brand_name\").size()\n        # 在name中抽取brand name进行对brand name的填充\n        sample.loc[sample[\"brand_name\"].isnull(), \"brand_name\"] = parallelize_dataframe(\n            sample.loc[sample[\"brand_name\"].isnull(), \"name\"], \n            fill_brandname_df)\n        # 填充na\n        sample[\"brand_name\"].fillna(\"\", inplace=True)\n\n        # 填充category_name\n        sample[\"category_name\"].fillna(\"//\", inplace=True)\n        sample[\"catery_name_first\"] =  sample[\"category_name\"].apply(lambda x: x.split(\"/\")[0])\n        sample[\"catery_name_second\"] = sample[\"category_name\"].apply(lambda x: x.split(\"/\")[1])\n        sample[\"catery_name_third\"] = sample[\"category_name\"].apply(lambda x: x.split(\"/\")[2])\n        sample['category_name'] = sample['category_name'].apply( lambda x: ' '.join(x.split('/') ).strip())\n\n        sample['nb_words_item_description'] = sample['item_description'].apply(lg).astype(np.uint16)\n        sample['nb_words_item_description'] = 1.0 * sample['nb_words_item_description'] / max_text_length\n\n        for x in ['brand_name', 'category_name', 'catery_name_first', 'catery_name_second', 'catery_name_third']:\n            le = preprocessing.LabelEncoder()\n            sample[x] = le.fit_transform(sample[x])\n\n        sample[\"old_name\"] = sample[\"name\"].copy()\n        sample[\"brand_cat\"] = \"cat1_\" + sample[\"catery_name_first\"].astype(str) + \" \" + \\\n                    \"cat2_\" + sample[\"catery_name_second\"].astype(str) + \" \" + \\\n                    \"cat3_\" +  sample[\"catery_name_third\"].astype(str) + \" \" + \\\n                    \"brand_\" + sample[\"brand_name\"].astype(str)\n        sample[\"name\"] = sample[\"brand_cat\"] + \" \" + sample[\"name\"]\n        # 融合了cate, brand, name, desc\n        sample[\"name_desc\"] = sample[\"name\"] + \" \" + sample[\"item_description\"].apply(lambda x: \" \".join(x.split()[:5]))  \n        gc.collect()\n        \n    print(\"构造文本特征:\")\n    with Clock():\n#         global word_doc_count_dic\n#         word_doc_count_dic = dict()\n#         for x in ['name','item_description']:\n#             sample[x].apply(lambda y: word_doc_counts(y, word_doc_count_dic))\n#         min_df_one = 5\n#         rare_words = [x for x in word_doc_count_dic if word_doc_count_dic[x] < min_df_one]\n#         for x in rare_words:\n#             del word_doc_count_dic[x]\n#         for x in ['name','item_description']:\n#             sample[x] = sample[x].apply(lambda y: remove_rare_words(y, word_doc_count_dic))\n\n#         sample['name_bi'] = parallelize_dataframe(sample['name_desc'],  create_bigrams_df)\n#         bi_word_doc_count_dic = dict()\n#         sample['name_bi'].apply(lambda y: word_doc_counts(y, bi_word_doc_count_dic))\n#         rare_words = [x for x in bi_word_doc_count_dic if bi_word_doc_count_dic[x] < min_df_one]\n#         for x in rare_words:\n#             del bi_word_doc_count_dic[x]\n#         sample[\"name_bi\"] = sample[\"name_bi\"].apply(lambda y: remove_rare_words(y, bi_word_doc_count_dic))\n\n        # 构建索引表\n#         vocabulary_one = word_doc_count_dic.copy()\n#         vocabulary_bi = bi_word_doc_count_dic.copy()\n#         for dc in [vocabulary_one,  vocabulary_bi]:\n#             cpt = 0\n#             for key in dc:\n#                 dc[key] = cpt\n#                 cpt += 1\n#         count = 0\n#         for key in word_doc_count_dic:\n#             vocabulary_one[key] = count\n#             count += 1\n        print(\"train_name_one\")\n        vect_name_one  = CountVectorizer(stop_words=\"english\",\n                                         min_df=5,\n                                         dtype=np.uint8,\n                                         tokenizer=tokenize,\n                                         binary=True) \n        train_name_one  = vect_name_one.fit_transform(sample['name'])\n\n        print(\"vect_name_one\")\n        vect_item_one = CountVectorizer(stop_words=\"english\",\n                                        ngram_range=(1, 2),\n                                        min_df=5,   \n                                        dtype=np.uint8, \n                                        tokenizer=tokenize, \n                                        binary=True) \n        print(\"train_item_one\")\n        train_item_one = vect_item_one.fit_transform(sample['item_description'])\n#         vect_name_bi = CountVectorizer(vocabulary= vocabulary_bi,   \n#                                    dtype=np.uint8, \n#                                    tokenizer=tokenize, \n#                                    binary=True) \n#         train_name_bi = vect_name_bi.fit_transform(sample['name_bi'])\n        gc.collect()\n        \n    print(\"构建统计特征:\")\n    with Clock():\n        train_new = sample[:train.shape[0]].copy()\n        train_new[\"price\"] = train[\"price\"]\n        for x in ['catery_name_first', 'catery_name_second', 'catery_name_third', 'category_name', 'brand_name'  ]:\n            tmp = train_new.groupby(x)[\"price\"].mean().astype(np.float32)\n            sample['mean_price_' + x] = sample[x].map(tmp).astype(np.float32)\n            sample['mean_price_' + x].fillna(tmp.mean(), inplace=True)\n        gc.collect()\n    \n    print(\"构造样本:\")\n    with Clock():\n        keep = ['item_condition_id', 'shipping', 'nb_words_item_description',\n           'mean_price_catery_name_first', 'mean_price_catery_name_second', 'mean_price_catery_name_third',\n            'mean_price_category_name', 'mean_price_brand_name']\n        y = train.price.values\n        x = hstack([sample[keep].values, train_name_one, train_item_one]).tocsr()\n        test = x[train.shape[0]:]\n        x = x[:train.shape[0]]\n        gc.collect()\n    \n    return x, y, test \n\ndef train(x, y):\n    ridge = Ridge(alpha=20, copy_X=True, \n              fit_intercept=True, solver='auto', \n              max_iter=200, normalize=False, \n              random_state=0,  tol=0.0025)\n    ridge.fit(x, y)\n    return ridge\n\ndef submit(model, submit, test):\n    path = '../input/'\n    pred = model.predict(test)\n    pred = np.expm1(pred)\n    pred[pred < 3] = 3\n    pred[pred > 1000] = 1000\n    test_id= pd.read_csv(path + \"test_stg2.tsv\", sep='\\t', encoding='utf-8')[[\"test_id\"]]\n    test_id[\"price\"] = pred\n    test_id.to_csv(submit, index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = '../input/'\nx, y, test = get_sample(path + \"train.tsv\", path + \"test_stg2.tsv\", max_text_length=60)\nridge = train(x, y)\nsubmit(ridge, \"ans.csv\", test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}