{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport warnings\nwarnings.filterwarnings('ignore')\nimport matplotlib.pyplot as plt \nplt.style.use('dark_background')\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport wordcloud\nimport pandas\nfrom multiprocessing import Pool\nfrom sklearn.feature_extraction.text import TfidfTransformer  \nfrom sklearn.feature_extraction.text import CountVectorizer\nimport re\nimport gc\nfrom nltk.util import ngrams\nfrom sklearn import preprocessing\nfrom itertools import combinations\nfrom scipy.sparse import csr_matrix, hstack\nimport time\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import train_test_split\nfrom keras.layers import Input, Dropout, Dense, concatenate, PReLU, Embedding, Flatten, Activation, BatchNormalization\nfrom keras.layers.pooling import GlobalAveragePooling1D\nfrom keras.optimizers import Adam\nfrom keras.models import Model\nfrom keras.initializers import he_uniform\nfrom keras.preprocessing.sequence import pad_sequences\nimport keras.backend.tensorflow_backend as KTF\nimport tensorflow as tf\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth=True\nsess = tf.Session(config=config)\nKTF.set_session(sess)\n\n\nclass Clock:\n    \n    def __enter__(self):\n        self.begin_time = time.time()\n        \n    def __exit__(self, exec_type, exec_value, exec_trace):\n        print(\"time:\", time.time() - self.begin_time, \"s\")\n\n\ndef parallelize_dataframe(df, func, cores=4):\n    \"\"\"\n    对一个dataframe并行计算, df是一个dataframe, func是处理一个dataframe的函数, 而不是\n    处理某一列的函数, 如果我们要把处理某一列的函数封装成func, 需要在用一个处理dataframe的\n    函数对这个处理单列的函数进行封装, 最后会返回这个处理完的dataframe\n    \"\"\"\n    # 分割data frame, array_split用来在垂直方向上分割dataframe\n    df_split = np.array_split(df, cores)\n    pool = Pool(cores)\n    \n    # 并行计算, 最终结果是按照df_split的顺序返回的, 并在axis=0, 也就是垂直进行连接\n    df = pd.concat(pool.map(func, df_split), axis=0)\n    pool.close()\n    pool.join()\n    \n    return df\n\n\ndef clean_str(text, max_text_length=60):\n    \"\"\"\n    去掉特殊字符, 只保留数字和字母, 并且把数字和字母分开, 然后用空格分割各个单词\n    处理如果我们的文本超过了max_text_length, 就会被截断成max_text_length.\n    \"\"\"\n    try:\n        # 进行切分后合并, 最多取60个词\n        text = ' '.join([w for w in text.split()[:max_text_length]] )        \n        text = text.lower()\n        # 特殊字符\n        text = re.sub(u\"é\", u\"e\", text)\n        text = re.sub(u\"ē\", u\"e\", text)\n        text = re.sub(u\"è\", u\"e\", text)\n        text = re.sub(u\"ê\", u\"e\", text)\n        text = re.sub(u\"à\", u\"a\", text)\n        text = re.sub(u\"â\", u\"a\", text)\n        text = re.sub(u\"ô\", u\"o\", text)\n        text = re.sub(u\"ō\", u\"o\", text)\n        text = re.sub(u\"ü\", u\"u\", text)\n        text = re.sub(u\"ï\", u\"i\", text)\n        text = re.sub(u\"ç\", u\"c\", text)\n        text = re.sub(u\"\\u2019\", u\"'\", text)\n        text = re.sub(u\"\\xed\", u\"i\", text)\n        # 简写\n        text = re.sub(u\"w\\/\", u\" with \", text)\n        \n        # 去掉非数字以及字母符\n        text = re.sub(u\"[^a-z0-9]\", \" \", text)\n        # 如果pattern中含有括号, 那么括号中匹配的也能被保留下来, 将数字与字母分开\n        text = u\" \".join(re.split('(\\d+)',text))\n        # 去掉任何空白字符, 转换为单个空格\n        text = re.sub( u\"\\s+\", u\" \", text).strip()\n    except:\n        text = np.NaN\n        \n    return text\n\ndef take(generator):\n    try:\n        for elem in generator:\n            yield elem\n    except StopIteration:\n        return\n\n\ndef fill_brandname(name, brand_name_set):\n    try:\n        for size in [4, 3, 2, 1]:\n            for x in take(ngrams(name.split(), size)):\n                if \" \".join(x) in brand_name_set:\n                    return \" \".join(x)\n        return np.NaN\n    except:\n        return np.NaN\n\n    \ndef clean_str_df(x):\n    return x.apply(lambda y: clean_str(y))\n\n\ndef fill_brandname_df(x):\n    # x是dataframe\n    global brand_name_set\n    # 返回的也应该是一个dataframe\n    return x.apply(lambda y: fill_brandname(y, brand_name_set))\n\n\ndef lg(text):\n    text = [x for x in text.split() if x != '']\n    return len(text)\n\n\ndef tokenize(text):\n    return [w for w in text.split()]\n\n\ndef word_doc_counts(words, word_doc_count_dic):\n    # 文档内去重\n    words = set(words.split())\n    for x in words:\n        if x in word_doc_count_dic:\n            word_doc_count_dic[x] = word_doc_count_dic[x] + 1\n        else:\n            word_doc_count_dic[x] = 1\n            \n            \ndef remove_rare_words(words, word_doc_count_dic):\n    return \" \".join([x for x in words.split() if x in word_doc_count_dic])\n\ndef name_to_seq(df):\n    global vocabulary_one\n    global MAX_NAME_SEQ\n    return df.apply(lambda x: [vocabulary_one[y] for y in x.split()[:MAX_NAME_SEQ]])\n\n\ndef description_to_seq(df):\n    global vocabulary_one\n    global MAX_ITEM_DESC_SEQ\n    ret = df.apply(lambda x: [vocabulary_one[y] for y in x.split()[:MAX_ITEM_DESC_SEQ]])\n    return ret\n\ndef get_sample(train_file, test_file, max_text_length=60):\n    # 读取样本\n    print(\"读取样本\")\n    with Clock():\n        train = pd.read_csv(train_file, sep='\\t', encoding='utf-8')\n        train.drop(columns=[\"train_id\"], inplace=True)\n        print('train shape:', train.shape, \"columns:\", train.columns)\n        test = pd.read_csv(test_file, sep='\\t', encoding='utf-8')\n        test.drop(columns=[\"test_id\"], inplace=True)\n        print('test shape:', test.shape, \"columns:\", test.columns)\n        train['price'] = np.log1p(train['price'].values)\n        sample = pd.concat([train.drop(columns=[\"price\"], axis=0), test], axis=0)\n        gc.collect()\n    \n    with Clock():\n        print(\"处理特征\")\n        # 对item_description进行填充\n        sample[\"item_description\"].fillna(\"\", inplace=True) #na\n        # No description yet\n        sample[\"item_description\"] = sample[\"item_description\"].apply(lambda x : \n                                                                     x.replace('No description yet',''))\n\n        # 清理字符串\n        sample[\"item_description\"] = parallelize_dataframe(sample[\"item_description\"], clean_str_df)\n        sample[\"name\"] = parallelize_dataframe(sample[\"name\"], clean_str_df)\n        sample[\"brand_name\"] = parallelize_dataframe(sample[\"brand_name\"], clean_str_df)\n\n        # 获取brand_name和name大小的对应关系\n        global brand_name_set \n        brand_name_set = sample.groupby(\"brand_name\").size()\n        # 在name中抽取brand name进行对brand name的填充\n        sample.loc[sample[\"brand_name\"].isnull(), \"brand_name\"] = parallelize_dataframe(\n            sample.loc[sample[\"brand_name\"].isnull(), \"name\"], \n            fill_brandname_df)\n        # 填充na\n        sample[\"brand_name\"].fillna(\"\", inplace=True)\n        sample[\"item_condition_id\"].fillna(2, inplace=True)\n\n        # 填充category_name\n        sample[\"category_name\"].fillna(\"//\", inplace=True)\n        sample[\"catery_name_first\"] =  sample[\"category_name\"].apply(lambda x: x.split(\"/\")[0])\n        sample[\"catery_name_second\"] = sample[\"category_name\"].apply(lambda x: x.split(\"/\")[1])\n        sample[\"catery_name_third\"] = sample[\"category_name\"].apply(lambda x: x.split(\"/\")[2])\n        sample['category_name'] = sample['category_name'].apply( lambda x: ' '.join(x.split('/') ).strip())\n\n        sample['nb_words_item_description'] = sample['item_description'].apply(lg).astype(np.uint16)\n        sample['nb_words_item_description'] = 1.0 * sample['nb_words_item_description'] / max_text_length\n\n        for x in ['brand_name', 'category_name', 'catery_name_first', 'catery_name_second', \n                  'catery_name_third']:\n            le = preprocessing.LabelEncoder()\n            sample[x] = le.fit_transform(sample[x])\n            \n        sample[\"item_condition_id\"] = 1.0 * sample[\"item_condition_id\"] / 5\n        sample[\"old_name\"] = sample[\"name\"].copy()\n        sample[\"brand_cat\"] = \"cat1_\" + sample[\"catery_name_first\"].astype(str) + \" \" + \\\n                    \"cat2_\" + sample[\"catery_name_second\"].astype(str) + \" \" + \\\n                    \"cat3_\" +  sample[\"catery_name_third\"].astype(str) + \" \" + \\\n                    \"brand_\" + sample[\"brand_name\"].astype(str)\n        sample[\"name\"] = sample[\"brand_cat\"] + \" \" + sample[\"name\"]\n        # 融合了cate, brand, name, desc\n        sample[\"name_desc\"] = sample[\"name\"] + \" \" + sample[\"item_description\"].apply(lambda x: \" \".join(x.split()[:5]))  \n        gc.collect()\n        \n        return sample, train\n    \ndef get_statistics_feature(sample, train):\n    print(\"构建统计特征:\")\n    with Clock():\n        train_new = sample[:train.shape[0]].copy()\n        train_new[\"price\"] = train[\"price\"]\n        for x in ['catery_name_first', 'catery_name_second', 'catery_name_third', 'category_name', 'brand_name'  ]:\n            tmp = train_new.groupby(x)[\"price\"].mean().astype(np.float32)\n            sample['mean_price_' + x] = sample[x].map(tmp).astype(np.float32)\n            sample['mean_price_' + x].fillna(tmp.mean(), inplace=True)\n        gc.collect()\n        return sample\n    \n    \ndef get_bownn1_feature(sample, train):\n    print(\"构造文本特征:\")\n    max_features = 200000\n    with Clock():\n        vectorizer = CountVectorizer(lowercase=True, tokenizer=tokenize, stop_words='english', \n                                     ngram_range=(1, 2), min_df=5, max_features=max_features, \n                                     binary=True, dtype=np.int8)\n        vectorizer.fit(sample['name'] + ' ' + sample['item_description'])\n    \n    print(\"构造样本:\")\n    with Clock():\n        split_num = train.shape[0]\n        x = {\n           'sparse_data': vectorizer.transform(sample[:split_num]['name'] + \n                                               ' ' + sample[:split_num]['item_description']),\n           'item_condition': np.array(sample['item_condition_id'])[:split_num],\n           'shipping': np.array(sample[\"shipping\"])[:split_num],\n           'mean_price_catery_name_second': np.array(sample[\"mean_price_catery_name_second\"])[:split_num],\n           'nb_words_item_description': np.array(sample[\"nb_words_item_description\"])[:split_num]\n              }\n        test = {\n           'sparse_data': vectorizer.transform(sample[split_num:]['name'] + \n                                               ' ' + sample[split_num:]['item_description']),\n           'item_condition': np.array(sample['item_condition_id'])[split_num:],\n           'shipping': np.array(sample[\"shipping\"])[split_num:],\n           'mean_price_catery_name_second': np.array(sample[\"mean_price_catery_name_second\"])[split_num:],\n           'nb_words_item_description': np.array(sample[\"nb_words_item_description\"])[split_num:]\n              }\n        y = train.price.values\n    return x, y, test \n\n\ndef bownn1_model(max_features=200000):\n    sparse_data = Input(shape=[max_features], name=\"sparse_data\", dtype='float32', sparse=True)\n    item_condition = Input(shape=[1], name=\"item_condition\", dtype='float32')\n    shipping = Input(shape=[1], name=\"shipping\", dtype='float32')\n    mean_price_catery_name_second = Input(shape=[1], name=\"mean_price_catery_name_second\", \n                                          dtype='float32')\n    nb_words_item_description = Input(shape=[1], name=\"nb_words_item_description\", \n                                          dtype='float32')\n    x = Dense(200, kernel_initializer=he_uniform(seed=0))(sparse_data)\n    x = PReLU()(x)\n    x = concatenate([x, item_condition, shipping, mean_price_catery_name_second,\n                    nb_words_item_description], axis=1)\n    x = Dense(200, kernel_initializer=he_uniform(seed=0))(x)\n    x = PReLU()(x)\n    x = Dense(100, kernel_initializer=he_uniform(seed=0))(x)\n    x = PReLU()(x)\n    x = Dense(1, kernel_initializer=he_uniform(seed=0))(x)\n    m = Model([sparse_data, item_condition, shipping, mean_price_catery_name_second, \n              nb_words_item_description], x)\n    optimizer = Adam(.001)\n    m.compile(loss=\"mse\", optimizer=optimizer)\n    return m\n\n\ndef get_bownn2_feature(sample, train):\n    print(\"构造文本特征:\")\n    split_num = train.shape[0]\n    with Clock():\n        print(\"brand_cat\")\n        vectorizer_brandcat = CountVectorizer(lowercase=True, tokenizer=tokenize, \n                                     ngram_range=(1, 3), min_df=2, max_features=100000, \n                                     binary=True, dtype=np.int8)\n        vectorizer_brandcat.fit(sample[:split_num]['brand_cat'])\n        sparse_data_brandcat = vectorizer_brandcat.transform(sample['brand_cat'])\n        del vectorizer_brandcat\n        sample.drop(columns=[\"brand_cat\"], inplace=True)\n        gc.collect()\n        \n        print(\"old_name\")\n        vectorizer_name = CountVectorizer(lowercase=True, analyzer=\"char\", \n                                     ngram_range=(2, 4), min_df=5, max_features=100000, \n                                     binary=True, dtype=np.int8)\n        vectorizer_name.fit(sample[:train.shape[0]]['old_name'])\n        sparse_data_name = vectorizer_name.transform(sample['old_name'])\n        del vectorizer_name\n        sample.drop(columns=[\"old_name\"], inplace=True)\n        gc.collect()\n        \n        print(\"item_description\")\n        vectorizer_desc = CountVectorizer(lowercase=True, tokenizer=tokenize, stop_words=\"english\",\n                                     ngram_range=(1, 1), min_df=20, max_features=100000, \n                                     binary=True, dtype=np.int8)\n        vectorizer_desc.fit(sample[:train.shape[0]][\"item_description\"])\n        sparse_data_desc = vectorizer_desc.transform(sample['item_description'])\n        del vectorizer_desc\n        sample.drop(columns=[\"item_description\"], inplace=True)\n        gc.collect()\n        \n    print(\"构造样本:\")\n    with Clock():\n        x = {\n           'sparse_data_name': sparse_data_name[:split_num],\n           'sparse_data_brandcat': sparse_data_brandcat[:split_num],\n           'sparse_data_desc': sparse_data_desc[:split_num],\n           'item_condition': np.array(sample['item_condition_id'])[:split_num],\n           'shipping': np.array(sample[\"shipping\"])[:split_num],\n           'mean_price_catery_name_second': np.array(sample[\"mean_price_catery_name_second\"])[:split_num],\n           'nb_words_item_description': np.array(sample[\"nb_words_item_description\"])[:split_num]\n              }\n        test = {\n           'sparse_data_name': sparse_data_name[split_num:],\n           'sparse_data_brandcat':  sparse_data_brandcat[split_num:],\n           'sparse_data_desc': sparse_data_desc[split_num:],\n           'item_condition': np.array(sample['item_condition_id'])[split_num:],\n           'shipping': np.array(sample[\"shipping\"])[split_num:],\n           'mean_price_catery_name_second': np.array(sample[\"mean_price_catery_name_second\"])[split_num:],\n           'nb_words_item_description': np.array(sample[\"nb_words_item_description\"])[split_num:]\n              }\n        y = train.price.values\n    return x, y, test \n\n\ndef bownn2_model(len_sparse_data_name, len_sparse_data_brandcat, len_sparse_data_desc):\n    sparse_data_name = Input(shape=[len_sparse_data_name], name=\"sparse_data_name\", dtype='float32', \n                             sparse=True)\n    sparse_data_brandcat = Input(shape=[len_sparse_data_brandcat], name=\"sparse_data_brandcat\",\n                                 dtype='float32', sparse=True)\n    sparse_data_desc = Input(shape=[len_sparse_data_desc], name=\"sparse_data_desc\",\n                                 dtype='float32', sparse=True)\n    item_condition = Input(shape=[1], name=\"item_condition\", dtype='float32')\n    shipping = Input(shape=[1], name=\"shipping\", dtype='float32')\n    mean_price_catery_name_second = Input(shape=[1], name=\"mean_price_catery_name_second\", \n                                          dtype='float32')\n    nb_words_item_description = Input(shape=[1], name=\"nb_words_item_description\", \n                                          dtype='float32')\n    \n    x = PReLU()(Dense(50)(sparse_data_name))\n    y = PReLU()(Dense(50)(sparse_data_brandcat))\n    z = PReLU()(Dense(50)(sparse_data_desc))\n    x = concatenate([x, y, z, item_condition, shipping, mean_price_catery_name_second, \n                     nb_words_item_description])\n    x = PReLU()(Dense(50)(x))\n    x = Dense(1)(x)\n    m = Model([sparse_data_name, sparse_data_brandcat, sparse_data_desc, \n              item_condition, shipping, mean_price_catery_name_second, \n                     nb_words_item_description], x)\n\n    optimizer = Adam(0.002)\n    m.compile(loss=\"mse\", optimizer=optimizer)\n    return m\n\n\ndef get_embedding_feature(sample, train):\n    print(\"构造文本特征:\")\n    with Clock():\n        word_doc_count_dic = dict()\n        for x in ['name','item_description']:\n            sample[x].apply(lambda y: word_doc_counts(y, word_doc_count_dic))\n        min_df_one = 5\n        rare_words = [x for x in word_doc_count_dic if word_doc_count_dic[x] < min_df_one]\n        for x in rare_words:\n            del word_doc_count_dic[x]\n        for x in ['name','item_description']:\n            sample[x] = sample[x].apply(lambda y: remove_rare_words(y, word_doc_count_dic))\n        global vocabulary_one \n        vocabulary_one = word_doc_count_dic.copy()\n        count = 0\n        for x in word_doc_count_dic:\n            count += 1\n            vocabulary_one[x] = count\n            \n    print(\"构造样本:\")\n    # 定义一些参数\n    global MAX_NAME_SEQ \n    MAX_NAME_SEQ = 20\n    global MAX_ITEM_DESC_SEQ \n    MAX_ITEM_DESC_SEQ = 30\n    # 0是默认值\n    MAX_TEXT = len(vocabulary_one) + 1\n    # 加1是因为从0开始计算, 这些都是要embedding\n    MAX_CATEGORY    = np.max(sample['category_name'].max()) + 1\n    MAX_CATEGORY1   = np.max(sample['catery_name_first'].max()) + 1\n    MAX_CATEGORY2   = np.max(sample['catery_name_second'].max()) + 1\n    MAX_CATEGORY3   = np.max(sample['catery_name_third'].max()) + 1\n    MAX_BRAND       = np.max(sample['brand_name'].max()) + 1\n    split_num = train.shape[0]\n        \n    with Clock():\n        new_train = sample[:split_num]\n        new_train[\"seq_name\"] = parallelize_dataframe(new_train[\"name\"], name_to_seq)\n        new_train[\"seq_item_description\"] = parallelize_dataframe(new_train[\"item_description\"],\n                                                                  description_to_seq)\n        seq_name = pad_sequences(new_train[\"seq_name\"], maxlen=MAX_NAME_SEQ)\n        seq_item_description = pad_sequences(new_train[\"seq_item_description\"], \n                                             maxlen=MAX_ITEM_DESC_SEQ)\n        x = {\n            \"name\": seq_name,\n            \"item_desc\": seq_item_description,\n            \"brand_name\": np.array(new_train[\"brand_name\"]),\n            \"category_name\": np.array(new_train[\"category_name\"]),\n            \"catery_name_first\": np.array(new_train[\"catery_name_first\"]),\n            \"catery_name_second\": np.array(new_train[\"catery_name_second\"]),\n            \"catery_name_third\": np.array(new_train[\"catery_name_third\"]),\n            \"item_condition\": np.array(new_train[\"item_condition_id\"]),\n            \"shipping\": np.array(new_train[\"shipping\"])\n            }\n        y = train.price.values\n        new_test = sample[split_num:]\n        \n        def test_gen():\n            batch = 500000\n            remain = new_test.shape[0]\n            current = 0\n            while remain > 0:\n                if remain < batch:\n                    tmp = new_test[current:]\n                else:\n                    tmp = new_test[current:current + batch]\n\n                remain = remain - batch\n                current = current + batch\n\n                tmp[\"seq_name\"] = parallelize_dataframe(tmp[\"name\"], name_to_seq)\n                tmp[\"seq_item_description\"] = parallelize_dataframe(tmp[\"item_description\"],\n                                                                          description_to_seq)\n                seq_name = pad_sequences(tmp[\"seq_name\"], maxlen=MAX_NAME_SEQ)\n                seq_item_description = pad_sequences(tmp[\"seq_item_description\"], \n                                                     maxlen=MAX_ITEM_DESC_SEQ)\n                x = {\n                    \"name\": seq_name,\n                    \"item_desc\": seq_item_description,\n                    \"brand_name\": np.array(tmp[\"brand_name\"]),\n                    \"category_name\": np.array(tmp[\"category_name\"]),\n                    \"catery_name_first\": np.array(tmp[\"catery_name_first\"]),\n                    \"catery_name_second\": np.array(tmp[\"catery_name_second\"]),\n                    \"catery_name_third\": np.array(tmp[\"catery_name_third\"]),\n                    \"item_condition\": np.array(tmp[\"item_condition_id\"]),\n                    \"shipping\": np.array(tmp[\"shipping\"])\n                  }\n                yield x\n\n        return x, y, test_gen, MAX_CATEGORY, MAX_CATEGORY1, MAX_CATEGORY2, MAX_CATEGORY3, MAX_BRAND, MAX_TEXT, MAX_NAME_SEQ, MAX_ITEM_DESC_SEQ\n    \n    \ndef embeding_model(MAX_CATEGORY, MAX_CATEGORY1, MAX_CATEGORY2, MAX_CATEGORY3, MAX_BRAND, MAX_TEXT,\n         MAX_NAME_SEQ):\n    name = Input(shape=[MAX_NAME_SEQ], name=\"name\")\n    item_desc = Input(shape=[MAX_ITEM_DESC_SEQ], name=\"item_desc\")\n    brand_name = Input(shape=[1], name=\"brand_name\")\n    category_name = Input(shape=[1], name=\"category_name\")\n    catery_name_first = Input(shape=[1], name=\"catery_name_first\")\n    catery_name_second = Input(shape=[1], name=\"catery_name_second\")\n    catery_name_third = Input(shape=[1], name=\"catery_name_third\")\n    item_condition = Input(shape=[1], name=\"item_condition\")\n    shipping = Input(shape=[1], name=\"shipping\")\n    \n    \n    share_embedding = Embedding(MAX_TEXT, 50)\n    # [batch, MAX_NAME_SEQ, 50]\n    name_pool = GlobalAveragePooling1D()(share_embedding(name))\n    # [batch, MAX_ITEM_DESC_SEQ, 50]\n    item_desc_pool = GlobalAveragePooling1D()(share_embedding(item_desc))\n    \n    brand_name_emb = Flatten()(Embedding(MAX_BRAND, 10)(brand_name))\n    category_name_emb = Flatten()(Embedding(MAX_CATEGORY, 10)(category_name))\n    catery_name_first_emb = Flatten()(Embedding(MAX_CATEGORY1, 10)(catery_name_first))\n    catery_name_second_emb = Flatten()(Embedding(MAX_CATEGORY2, 10)(catery_name_second))\n    catery_name_third_emb = Flatten()(Embedding(MAX_CATEGORY3, 10)(catery_name_third))\n    x = concatenate([name_pool, item_desc_pool, brand_name_emb, category_name_emb, \n                     catery_name_first_emb, catery_name_second_emb, catery_name_third_emb,\n                    item_condition, shipping])\n    x = BatchNormalization()(x)\n    x = Dense(1024)(x)\n    x = Activation(\"relu\")(x)\n    x = Dense(1)(x)\n    model = Model([name, item_desc, brand_name, category_name, catery_name_first, \n                  catery_name_second, catery_name_third, item_condition, shipping], x)\n    model.compile(loss=\"mse\", optimizer=Adam(0.001))\n    return model","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"path = \"../input/\"\ntest_id= pd.read_csv(path + \"test_stg2.tsv\", sep='\\t', encoding='utf-8')[[\"test_id\"]]\nsample, train = get_sample(path + \"train.tsv\", path + \"test_stg2.tsv\", max_text_length=60)\nsample = get_statistics_feature(sample, train)\n\nx, y, test = get_bownn1_feature(sample, train)\n\nm = bownn1_model()\nm.summary()\n\nm.fit(x, y - np.mean(y), 2048, epochs=3)\n\nnew_feature = pd.DataFrame(m.predict(x, 2048) + np.mean(y), columns=[\"price1\"])\ntest_id[\"price1\"] = m.predict(test, 2048) + np.mean(y)\ntest_id.to_csv(\"new_test.csv\", index=False)\nnew_feature.to_csv(\"new_train.csv\", index=False)\ndel new_feature\ndel test_id\ndel x\ndel y\ndel test\ndel m\ngc.collect()\n\n\nx, y, test_gen, MAX_CATEGORY, MAX_CATEGORY1, MAX_CATEGORY2, MAX_CATEGORY3, MAX_BRAND, MAX_TEXT, MAX_NAME_SEQ, MAX_ITEM_DESC_SEQ = get_embedding_feature(sample, train)\nm = embeding_model(MAX_CATEGORY, MAX_CATEGORY1,\n              MAX_CATEGORY2, MAX_CATEGORY3, \n              MAX_BRAND, MAX_TEXT, MAX_NAME_SEQ)\nm.summary()\nm.fit(x, y - np.mean(y), 2048, epochs=3)\nnew_feature = pd.read_csv(\"new_train.csv\")\nnew_feature[\"price2\"] = m.predict(x, 2048) + np.mean(y)\nnew_feature.to_csv(\"new_train.csv\", index=False)\ndel new_feature\ndel x\ngc.collect()\n\npred = []\nfor test in test_gen():\n    tmp = m.predict(test, 2048) + np.mean(y)\n    pred.append(tmp)\n    del test\n    gc.collect()\npred = np.concatenate(pred)\ntest_id= pd.read_csv(\"new_test.csv\")\ntest_id[\"price2\"] = pred\ntest_id.to_csv(\"new_test.csv\", index=False)\ndel test_id\ndel y\ndel m\ndel pred\ngc.collect()\n\n\nx, y, test = get_bownn2_feature(sample, train)\n\nm = bownn2_model(x[\"sparse_data_name\"].shape[1], x[\"sparse_data_brandcat\"].shape[1],\n          x[\"sparse_data_desc\"].shape[1])\nm.summary()\n\nm.fit(x, y - np.mean(y), 2048, epochs=3)\nnew_feature = pd.read_csv(\"new_train.csv\")\nnew_feature[\"price3\"] = m.predict(x, 2048) + np.mean(y)\nnew_feature.to_csv(\"new_train.csv\", index=False)\ndel new_feature\ntest_id= pd.read_csv(\"new_test.csv\")\ntest_id[\"price3\"] = m.predict(test, 2048) + np.mean(y)\ntest_id.to_csv(\"new_test.csv\", index=False)\ndel test_id\n\ndel x\ndel test\ndel m\ndel sample\ndel train\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\nnew_feature = pd.read_csv(\"new_train.csv\")\ntest_id= pd.read_csv(\"new_test.csv\")\nmodel = LinearRegression()\nmodel.fit(new_feature, y)\n\npred = model.predict(test_id.drop(columns=[\"test_id\"]))\npred = np.expm1(pred)\npred[pred < 3] = 3\npred[pred > 1000] = 1000\ntest_id.drop(columns=[\"price1\", \"price2\", \"price3\"], inplace=True)\ntest_id[\"price\"] = pred\ntest_id.to_csv(\"ans.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}