{"cells":[{"metadata":{"_uuid":"d9c4e75689c538b41aad45753aba45d601011830","_cell_guid":"049dad6e-bee5-4822-9e0e-ae11b0e7f26a"},"cell_type":"markdown","source":"# **Introduction**\n\nThis is an initial Explanatory Data Analysis for the [Mercari Price Suggestion Challenge](https://www.kaggle.com/c/mercari-price-suggestion-challenge#description) with matplotlib. [bokeh](https://bokeh.pydata.org/en/latest/) and [Plot.ly](https://plot.ly/feed/) - a visualization tool that creates beautiful interactive plots and dashboards.  The competition is hosted by Mercari, the biggest Japanese community-powered shopping app with the main objective to predict an accurate price that Mercari should suggest to its sellers, given the item's information. \n\n***Update***: The abundant amount of food from my family's Thanksgiving dinner has really energized me to continue working on this model. I decided to dive deeper into the NLP analysis and found an amazing tutorial by Ahmed BESBES. The framework below is  based on his [source code](https://ahmedbesbes.com/how-to-mine-newsfeed-data-and-extract-interactive-insights-in-python.html).  It provides guidance on pre-processing documents and  machine learning techniques (K-means and LDA) to clustering topics.  So that this kernel will be divided into 2 parts: \n\n1. Explanatory Data Analysis \n2. Text Processing  \n    2.1. Tokenizing and  tf-idf algorithm  \n    2.2. K-means Clustering  \n    2.3. Latent Dirichlet Allocation (LDA)  / Topic Modelling\n ","execution_count":null},{"metadata":{"_kg_hide-input":true,"_uuid":"af187b17315c15081fcdcdc99942db794844d90d","_cell_guid":"4f211658-d449-4052-87c4-0ab5d10545e8","_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"import nltk\nimport string\nimport re\nimport numpy as np\nimport pandas as pd\nimport pickle\n#import lda\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"white\")\n\nfrom nltk.stem.porter import *\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction import stop_words\n\nfrom collections import Counter\nfrom wordcloud import WordCloud\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n%matplotlib inline\n\nimport bokeh.plotting as bp\nfrom bokeh.models import HoverTool, BoxSelectTool\nfrom bokeh.models import ColumnDataSource\nfrom bokeh.plotting import figure, show, output_notebook\n#from bokeh.transform import factor_cmap\n\nimport warnings\nwarnings.filterwarnings('ignore')\nimport logging\nlogging.getLogger(\"lda\").setLevel(logging.WARNING)\n\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom IPython.display import display\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\npd.set_option('display.float_format', lambda x:'%.5f' % x)\nimport numpy as np\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"d82faf4720f0574d2be682880d2f662b96918ffa","_cell_guid":"5fa6bae8-c5fc-4c1a-8339-acfa14aac199","_kg_hide-output":true},"cell_type":"markdown","source":"# **Exploratory Data Analysis**\nOn the first look at the data, besides the unique identifier (item_id), there are 7 variables in this model. This notebook will sequentially go through each of them with a brief statistical summary. \n\n1. **Numerical/Continuous Features**\n    1. price: the item's final bidding price. This will be our reponse / independent variable that we need to predict in the test set\n    2. shipping cost     \n \n1. **Categorical Features**: \n    1. shipping cost: A binary indicator, 1 if shipping fee is paid by seller and 0 if it's paid by buyer\n    2. item_condition_id: The condition of the items provided by the seller\n    1. name: The item's name\n    2. brand_name: The item's producer brand name\n    2. category_name: The item's single or multiple categories that are separated by \"\\\" \n    3. item_description: A short description on the item that may include removed words, flagged by [rm]","execution_count":null},{"metadata":{"_kg_hide-input":true,"_uuid":"122a209567af15b4422a75b50bfb8bb4c7b5fdfe","_cell_guid":"fc7f8ff2-a9f2-488d-b9b8-0c47c1d64a9d","trusted":true},"cell_type":"code","source":"PATH = \"../input/filestmp/\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ca7edb8850b4b2d5dfbe044e08cc709886348c2e","_cell_guid":"6f1e2926-af11-4801-9df9-943162e72478","trusted":true},"cell_type":"code","source":"train = pd.read_csv(f'{PATH}train.tsv', sep='\\t')\ntest = pd.read_csv(f'{PATH}test.tsv', sep='\\t')\nprint(train.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b247cf361d89fc2431adc5150d0c4533691f71b2","_cell_guid":"f58f905d-8279-42fa-8877-862c1f23aff8","trusted":true},"cell_type":"code","source":"# different data types in the dataset: categorical (strings) and numeric\ntrain.dtypes","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"04c5b96c1de907bf9dc259a0a8d2e4df4f02375a","_cell_guid":"8fd3b4d2-2bc9-47dc-8396-5980aad64a1f","trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def display_all(df):\n    with pd.option_context(\"display.max_rows\", 1000):\n        with pd.option_context(\"display.max_columns\", 1000):\n            display(df)\n \n# trainの基本統計量を表示\ndisplay_all(train.describe(include='all').transpose())\n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# trainのカテゴリ名、商品説明、投稿タイトル、ブランド名のデータタイプを「category」へ変換する\ntrain.category_name = train.category_name.astype('category')\ntrain.item_description = train.item_description.astype('category')\ntrain.name = train.name.astype('category')\ntrain.brand_name = train.brand_name.astype('category')\n# testのカテゴリ名、商品説明、投稿タイトル、ブランド名のデータタイプを「category」へ変換する\ntest.category_name = test.category_name.astype('category')\ntest.item_description = test.item_description.astype('category')\ntest.name = test.name.astype('category')\ntest.brand_name = test.brand_name.astype('category')\n# dtypesで念のためデータ形式を確認しましょう\ntrain.dtypes, test.dtypes\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train,testのサイズとuniquなIDの数は一致している\n# trainの中のユニークな値を確認する\ntrain.apply(lambda x: x.nunique())\n# testの中のユニークな値を確認する\ntest.apply(lambda x: x.nunique())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# trainの欠損データの個数と%を確認\ntrain.isnull().sum(),train.isnull().sum()/train.shape[0]\n# testの欠損データの個数と%を確認\ntest.isnull().sum(),test.isnull().sum()/test.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# trainとtestのidカラム名を変更する\ntrain = train.rename(columns = {'train_id':'id'})\ntest = test.rename(columns = {'test_id':'id'})\n# 両方のセットへ「is_train」のカラムを追加\n# 1 = trainのデータ、0 = testデータ\ntrain['is_train'] = 1\ntest['is_train'] = 0\n# trainのprice(価格）以外のデータをtestと連結\ntrain_test_combine = pd.concat([train.drop(['price'], axis=1),test],axis=0)\n# 念のためデータの中身を表示させましょう\ntrain_test_combine.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_test_combineの文字列のデータタイプを「category」へ変換\ntrain_test_combine.category_name = train_test_combine.category_name.astype('category')\ntrain_test_combine.item_description = train_test_combine.item_description.astype('category')\ntrain_test_combine.name = train_test_combine.name.astype('category')\ntrain_test_combine.brand_name = train_test_combine.brand_name.astype('category')\n# combinedDataの文字列を「.cat.codes」で数値へ変換する\ntrain_test_combine.name = train_test_combine.name.cat.codes\ntrain_test_combine.category_name = train_test_combine.category_name.cat.codes\ntrain_test_combine.brand_name = train_test_combine.brand_name.cat.codes\ntrain_test_combine.item_description = train_test_combine.item_description.cat.codes\n# データの中身とデータ形式を表示して確認しましょう\ntrain_test_combine.head()\ntrain_test_combine.dtypes\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 「is_train」のフラグでcombineからtestとtrainへ切り分ける\ndf_test = train_test_combine.loc[train_test_combine['is_train'] == 0]\ndf_train = train_test_combine.loc[train_test_combine['is_train'] == 1]\n# 「is_train」をtrainとtestのデータフレームから落とす\ndf_test = df_test.drop(['is_train'], axis=1)\ndf_train = df_train.drop(['is_train'], axis=1)\n# サイズの確認をしておきましょう\ndf_test.shape, df_train.shape\n((693359, 7), (1482535, 7))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_trainへprice（価格）を戻す\ndf_train['price'] = train.price\n# price（価格）をlog関数で処理\ndf_train['price'] = df_train['price'].apply(lambda x: np.log(x) if x>0 else x)\n# df_trainを表示して確認\ndf_train.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom IPython.display import display\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\npd.set_option('display.float_format', lambda x:'%.5f' % x)\nimport numpy as np\n# x ＝ price以外の全ての値、y = price（ターゲット）で切り分ける\nx_train, y_train = df_train.drop(['price'], axis=1), df_train.price\n# モデルの作成\nm = RandomForestRegressor(n_jobs=-1, min_samples_leaf=5, n_estimators=200)\nm.fit(x_train, y_train)\n# スコアを表示\nm.score(x_train, y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 作成したランダムフォレストのモデル「m」に「df_test」を入れて予測する\npreds = m.predict(df_test)\n# 予測値 predsをnp.exp()で処理\nnp.exp(preds)\n# Numpy配列からpandasシリーズへ変換\npreds = pd.Series(np.exp(preds))\n# テストデータのIDと予測値を連結\nsubmit = pd.concat([df_test.id, preds], axis=1)\n# カラム名をメルカリの提出指定の名前をつける\nsubmit.columns = ['test_id', 'price']\n# 提出ファイルとしてCSVへ書き出し\nsubmit.to_csv('submission.csv', index=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os; os.environ['OMP_NUM_THREADS'] = '1'\nfrom contextlib import contextmanager\nfrom functools import partial\nfrom operator import itemgetter\nfrom multiprocessing.pool import ThreadPool\nimport time\nfrom typing import List, Dict\n\nimport keras as ks\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.feature_extraction import DictVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer as Tfidf\nfrom sklearn.pipeline import make_pipeline, make_union, Pipeline\nfrom sklearn.preprocessing import FunctionTransformer, StandardScaler\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.model_selection import KFold\n\n@contextmanager\ndef timer(name):\n    t0 = time.time()\n    yield\n    print(f'[{name}] done in {time.time() - t0:.0f} s')\n\ndef preprocess(df: pd.DataFrame) -> pd.DataFrame:\n    df['name'] = df['name'].fillna('') + ' ' + df['brand_name'].fillna('')\n    df['text'] = (df['item_description'].fillna('') + ' ' + df['name'] + ' ' + df['category_name'].fillna(''))\n    return df[['name', 'text', 'shipping', 'item_condition_id']]\n\ndef on_field(f: str, *vec) -> Pipeline:\n    return make_pipeline(FunctionTransformer(itemgetter(f), validate=False), *vec)\n\ndef to_records(df: pd.DataFrame) -> List[Dict]:\n    return df.to_dict(orient='records')\n\ndef fit_predict(xs, y_train) -> np.ndarray:\n    X_train, X_test = xs\n    config = tf.ConfigProto(\n        intra_op_parallelism_threads=1, use_per_session_threads=1, inter_op_parallelism_threads=1)\n    with tf.Session(graph=tf.Graph(), config=config) as sess, timer('fit_predict'):\n        ks.backend.set_session(sess)\n        model_in = ks.Input(shape=(X_train.shape[1],), dtype='float32', sparse=True)#MLPの設計\n        out = ks.layers.Dense(192, activation='relu')(model_in)\n        out = ks.layers.Dense(64, activation='relu')(out)\n        out = ks.layers.Dense(64, activation='relu')(out)\n        out = ks.layers.Dense(1)(out)\n        model = ks.Model(model_in, out)\n        model.compile(loss='mean_squared_error', optimizer=ks.optimizers.Adam(lr=3e-3))\n        for i in range(3):#3エポック\n            with timer(f'epoch {i + 1}'):\n                model.fit(x=X_train, y=y_train, batch_size=2**(11 + i), epochs=1, verbose=0)#バッチサイズは指数関数的に増加させる\n        return model.predict(X_test)[:, 0]#予想を返す\n\ndef main():\n    vectorizer = make_union(\n        on_field('name', Tfidf(max_features=100000, token_pattern='\\w+')),\n        on_field('text', Tfidf(max_features=100000, token_pattern='\\w+', ngram_range=(1, 2))),\n        on_field(['shipping', 'item_condition_id'],\n                 FunctionTransformer(to_records, validate=False), DictVectorizer()),\n        n_jobs=4)\n    y_scaler = StandardScaler()\n    with timer('process train'):\n        train = pd.read_table('../input/filestmp/train.tsv')\n        train = train[train['price'] > 0].reset_index(drop=True)\n        cv = KFold(n_splits=20, shuffle=True, random_state=42)\n        train_ids, valid_ids = next(cv.split(train))\n        train, valid = train.iloc[train_ids], train.iloc[valid_ids]\n        y_train = y_scaler.fit_transform(np.log1p(train['price'].values.reshape(-1, 1)))\n        X_train = vectorizer.fit_transform(preprocess(train)).astype(np.float32)\n        print(f'X_train: {X_train.shape} of {X_train.dtype}')\n        del train\n    with timer('process valid'):\n        X_valid = vectorizer.transform(preprocess(valid)).astype(np.float32)\n    with ThreadPool(processes=4) as pool: #4つのスレッドにする\n        Xb_train, Xb_valid = [x.astype(np.bool).astype(np.float32) for x in [X_train, X_valid]]\n        xs = [[Xb_train, Xb_valid], [X_train, X_valid]] * 2\n        y_pred = np.mean(pool.map(partial(fit_predict, y_train=y_train), xs), axis=0)#4コアで学習したものの平均をとっている\n    y_pred = np.expm1(y_scaler.inverse_transform(y_pred.reshape(-1, 1))[:, 0])#logで変換していたものを価格に戻す\n    print('Valid RMSLE: {:.4f}'.format(np.sqrt(mean_squared_log_error(valid['price'], y_pred))))\n\nif __name__ == '__main__':\n    main()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import os; os.environ['OMP_NUM_THREADS'] = '1'\nfrom contextlib import contextmanager\nfrom functools import partial\nfrom operator import itemgetter\nfrom multiprocessing.pool import ThreadPool\nimport time\nfrom typing import List, Dict\n\nimport keras as ks\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.feature_extraction import DictVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer as Tfidf\nfrom sklearn.pipeline import make_pipeline, make_union, Pipeline\nfrom sklearn.preprocessing import FunctionTransformer, StandardScaler\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.model_selection import KFold\n\ndef on_field(f: str, *vec) -> Pipeline:\n    print(itemgetter(f))\n    return make_pipeline(FunctionTransformer(itemgetter(f), validate=False), *vec)\n\ndef to_records(df: pd.DataFrame) -> List[Dict]:\n    return df.to_dict(orient='records')\n\nvectorizer = make_union(\n        on_field('name', Tfidf(max_features=100000, token_pattern='\\w+')),\n        on_field('text', Tfidf(max_features=100000, token_pattern='\\w+', ngram_range=(1, 2))),\n        on_field(['shipping', 'item_condition_id'],\n                 FunctionTransformer(to_records, validate=False), DictVectorizer()),\n        n_jobs=4)\ny_scaler = StandardScaler()\n\n\nprint(Tfidf(max_features=100000, token_pattern='\\w+'))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from typing import List, Dict\n\ndef to_records(df: pd.DataFrame) -> List[Dict]:\n   \n    return df.to_dict(orient='records')\n\ndef on_field(f: str, *vec) -> Pipeline:\n\n    return make_pipeline(FunctionTransformer(itemgetter(f), validate=False), *vec)\n\non_field(['shipping', 'item_condition_id'],\n                 FunctionTransformer(to_records, validate=False), DictVectorizer())\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"df=pd.DataFrame({\"shipping\":[0,1]})\nvec=DictVectorizer()\nv=train[\"shipping\"].to_dict()\nprint(v)\nX=vec.fit_transform(v)\nprint(vec.get_feature_names())\nX.toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os; os.environ['OMP_NUM_THREADS'] = '1'\nfrom contextlib import contextmanager\nfrom functools import partial\nfrom operator import itemgetter\nfrom multiprocessing.pool import ThreadPool\nimport time\nfrom typing import List, Dict\n\nimport keras as ks\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.feature_extraction import DictVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer as Tfidf\nfrom sklearn.pipeline import make_pipeline, make_union, Pipeline\nfrom sklearn.preprocessing import FunctionTransformer, StandardScaler\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.model_selection import KFold\n\n@contextmanager\ndef timer(name):\n    t0 = time.time()\n    yield\n    print(f'[{name}] done in {time.time() - t0:.0f} s')\n\ndef preprocess(df: pd.DataFrame) -> pd.DataFrame:\n    df['name'] = df['name'].fillna('') + ' ' + df['brand_name'].fillna('')\n    df['text'] = (df['item_description'].fillna('') + ' ' + df['name'] + ' ' + df['category_name'].fillna(''))\n    return df[['name', 'text', 'shipping', 'item_condition_id']]\n\ndef on_field(f: str, *vec) -> Pipeline:\n    return make_pipeline(FunctionTransformer(itemgetter(f), validate=False), *vec)\n\ndef to_records(df: pd.DataFrame) -> List[Dict]:\n    return df.to_dict(orient='records')\n\ndef fit_predict(xs, y_train) -> np.ndarray:\n    X_train, X_test = xs\n    config = tf.ConfigProto(\n        intra_op_parallelism_threads=1, use_per_session_threads=1, inter_op_parallelism_threads=1)\n    with tf.Session(graph=tf.Graph(), config=config) as sess, timer('fit_predict'):\n        ks.backend.set_session(sess)\n        model_in = ks.Input(shape=(X_train.shape[1],), dtype='float32', sparse=True)\n        out = ks.layers.Dense(192, activation='relu')(model_in)\n        out = ks.layers.Dense(64, activation='relu')(out)\n        out = ks.layers.Dense(64, activation='relu')(out)\n        out = ks.layers.Dense(1)(out)\n        model = ks.Model(model_in, out)\n        model.compile(loss='mean_squared_error', optimizer=ks.optimizers.Adam(lr=3e-3))\n        for i in range(3):\n            with timer(f'epoch {i + 1}'):\n                model.fit(x=X_train, y=y_train, batch_size=2**(11 + i), epochs=1, verbose=0)\n        return model.predict(X_test)[:, 0]\n\nvectorizer = make_union(\n    on_field('name', Tfidf(max_features=100000, token_pattern='\\w+')),\n    on_field('text', Tfidf(max_features=100000, token_pattern='\\w+', ngram_range=(1, 2))),\n    on_field(['shipping', 'item_condition_id'],FunctionTransformer(to_records, validate=False), DictVectorizer()),\n    n_jobs=3)\ny_scaler = StandardScaler()\n\nwith timer('process train'):\n    train = pd.read_table('../input/filestmp/train.tsv')\n    train = train[train['price'] > 0].reset_index(drop=True)\n    cv = KFold(n_splits=20, shuffle=True, random_state=42)\n    train_ids, valid_ids = next(cv.split(train))\n    train, valid = train.iloc[train_ids], train.iloc[valid_ids]\n    y_train = y_scaler.fit_transform(np.log1p(train['price'].values.reshape(-1, 1)))\n    X_train = vectorizer.fit_transform(preprocess(train)).astype(np.float32)\n    print(f'X_train: {X_train.shape} of {X_train.dtype}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"InTmp=preprocess(train)\nInTmp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" with timer('process valid'):\n        X_valid = vectorizer.transform(preprocess(valid)).astype(np.float32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(Xb_train.data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"with ThreadPool(processes=4) as pool:\n        Xb_train, Xb_valid = [x.astype(np.bool).astype(np.float32) for x in [X_train, X_valid]]        \n        xs = [[Xb_train, Xb_valid], [X_train, X_valid]] * 2 #4コアに対応するために","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d=[[Xb_train, Xb_valid], [X_train, X_valid]]\nd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(xs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}