{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Mercari \nMercari is an e-commerce company founded in February 2013 and currently operating in Japan and the United States."},{"metadata":{},"cell_type":"markdown","source":"# Products & Services\nMercari's main product is the Mercari marketplace app, which allows users to buy and sell items quickly from their smartphones. In Japan, the app is known for its ease of use and unique shipping system, which allows users to ship items anonymously from local convenience stores through agreements with Yamato Transport and Japan Post. In the United States, Mercari collaborates with USPS and FedEx to let users print shipping labels, etc.\n\nfor more info: 1) https://en.wikipedia.org/wiki/Mercari\n               2) https://www.mercari.com/about/"},{"metadata":{},"cell_type":"markdown","source":"# Mercari Price Suggestion Challenge\n\nThis competition will help them to automatically generate a recommendation price to their users, which will be a strong competitive advantage for them!\n\nMoreover, Mercari profit from taking service fee from each of the transection, which means this automatic price recommendation will be a boost of their revenue!!!"},{"metadata":{},"cell_type":"markdown","source":"# Outline\n\n- Loading Data and libraries\n- Imputing Data\n- EDA\n- Feature Engineering + Modeling\n- Cross Validation"},{"metadata":{},"cell_type":"markdown","source":"# Evaluation Matrix\n\nhttps://www.kaggle.com/c/mercari-price-suggestion-challenge/overview/evaluation\n\n- The evaluation metric for this competition is Root Mean Squared Logarithmic Error.\n\n- The RMSLE is calculated as\n\nepsilon = sqrt{frac{1}{n} sum_{i=1}^n (log(p_i + 1) - log(a_i+1))^2 }\n\n\n- Where:\n\n- Ïµ is the RMSLE value (score)\n- n is the total number of observations in the (public/private) data set,\n- pi is your prediction of price, and\n- ai is the actual sale price for i. \n- log(x) is the natural logarithm of x"},{"metadata":{},"cell_type":"markdown","source":"# Loading Data and libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport re\nfrom wordcloud import WordCloud, STOPWORDS \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.preprocessing import LabelBinarizer\nfrom scipy.sparse import vstack, hstack, csr_matrix\nfrom scipy import sparse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train = pd.read_csv('/kaggle/input/mercari-price-suggestion-challenge/train.tsv', delimiter='\\t')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_test =  pd.read_csv('/kaggle/input/mercari-price-suggestion-challenge/test_stg2.tsv', delimiter='\\t')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_test.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color =blue>- Now from above we can see that brand name has many NAN values so lets first check for the Missing values</font>"},{"metadata":{},"cell_type":"markdown","source":"#### Observation\n - features and its types\n - name -----text\n - item_condition_id -----numerical\n - category_name ----- categorical with multiple level\n - brand_name ----- categorical\n - shiping -----Binary  (0: \"shipping not paid by customer\",1: \"shipping paid by customer\")\n - item_description -----text"},{"metadata":{"trusted":true},"cell_type":"code","source":"#we will use isnull() function to get missing values\nprint(\"Train DataFrame\")\nprint(pd.isnull(data_train).sum())\nprint(\"=\"*50)\nprint(\"Test DataFrame\")\nprint(pd.isnull(data_test).sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print((pd.isnull(data_train['brand_name']).sum())*100/data_train.shape[0])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"{0:.2f}% brand_name has missing value in train data\".format((pd.isnull(data_train['brand_name']).sum())*100/data_train.shape[0]))\n\nprint(\"{0:.2f}% category_name has missing value in train data\".format((pd.isnull(data_train['category_name']).sum())*100/data_train.shape[0]))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"{0:.2f}% brand_name has missing value in test data\".format((pd.isnull(data_test['brand_name']).sum())*100/data_test.shape[0]))\n\nprint(\"{0:.2f}% category_name has missing value in test data\".format((pd.isnull(data_test['category_name']).sum())*100/data_test.shape[0]))\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observation\n- we can see that in train and test data frame category_name column has multiple levels separated by \"/\"\n- there are 2 columns which hase missing values\n- brand_name has almost half values missing both in train and test so we can not remove those rows we have to fill them\n- while category_name column has 0.43-0.44% values missing so we can remove those rows or fill them"},{"metadata":{},"cell_type":"markdown","source":"# Imputing Missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"# filling missing values with brand_name as Unknown\ndata_train['brand_name'] = data_train['brand_name'].fillna('Unknown')\ndata_test['brand_name'] = data_test['brand_name'].fillna('Unknown')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# filling missing values with category_name as NO/NO/NO or we cn simply remove these rows\ndata_train['category_name'] = data_train['category_name'].fillna('No/No/No')\ndata_test['category_name'] = data_test['category_name'].fillna('No/No/No')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking the levels of sub categories\nlevels =[]\n\nfor values in data_train['category_name']:\n    levels.append(values.count(\"/\"))\n\n\nprint(\"MIN no of levels:        {0:.0f} \".format(np.min(levels)+1))\nprint(\"MEDIAN of levels:        {0:.0f} \".format(np.percentile(levels, 50)+1))\nprint(\"90 percentile  of levels:{0:.0f} \".format(np.percentile(levels, 90)+1))\nprint(\"MAX no of levels:        {0:.0f}\".format(np.max(levels)+1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observation\n- category_name has max 5 levels seperated with \"/\"\n- category_name has min 3 levels seperated with \"/\"\n- 90 precentile of category_name has 3 levels seperated with \"/\"\n- so we will use till 3 level only"},{"metadata":{"trusted":true},"cell_type":"code","source":"##### seperate sub categories from main\ndef find_sub_cat(X):\n    try:\n        return(X.split(\"/\"))\n        return(\"jubu\")\n    except:\n        return(\"None\",\"None\",\"None\")\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train['main_category']=''\ndata_train['sub_category_1']=''\ndata_train['sub_category_2']=''\n\ndata_train['main_category'],data_train['sub_category_1'],data_train['sub_category_2']\\\n                                = zip(*data_train['category_name'].apply(lambda x: find_sub_cat(x)))\n\n##Because we have list of list we have to use zip(*) to get final 3 list\n#https://www.youtube.com/watch?v=Rlak6CTcUDI","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_test['main_category']=''\ndata_test['sub_category_1']=''\ndata_test['sub_category_2']=''\n\ndata_test['main_category'],data_test['sub_category_1'],data_test['sub_category_2']\\\n                                = zip(*data_test['category_name'].apply(lambda x: find_sub_cat(x)))\n\n##Because we have list of list we have to use zip(*) to get final 3 list\n#https://www.youtube.com/watch?v=Rlak6CTcUDI","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#data_train['main_category']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#data_train = data_train.drop('category_name',axis=1)\ndata_train.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA"},{"metadata":{},"cell_type":"markdown","source":"### Numerical feature analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"#to get better display format follow below link\n#https://stackoverflow.com/questions/55394854/how-to-change-the-format-of-describe-output\npd.set_option('display.float_format', lambda x: '%.5f' % x)\n\ndata_train.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observation\n- here we can see that our data contains 4 columns with numeric values but only 2 of them 'item_condition_id' and 'shipping' are useful in prediction because price is dependent variable which we have to predict\n- we can see thay min price is 0 means some of items are literally free and price go upto max of 2009 with mean price of 26 dollars and midean(50%) price of 17 dollars\n- most of the items have price of 29 dollar and shipping cost 1\n- rest of the features are either categorical or text format so we will dive deep into it in next few cells"},{"metadata":{},"cell_type":"markdown","source":"## Dependent variable analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nplt.hist(data_train['price'],bins=50, edgecolor='white',range=[0,300])\nplt.ylabel('Frequency')\nplt.xlabel('Price')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observation:\n\n- we can see that the distribution above is skewed distibution if we ignore the price value at 0th with long tail\n- and we also have RMSLE which is nothing but the log of RMS so lets try plotting log of price plot\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nplt.hist(np.log(data_train['price']+1),bins=50, edgecolor='white') #log(0) is undefined\nplt.ylabel('Frequency')\nplt.xlabel('log_Price')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- The distribution of log(price) is less skewed and well distributed around the mean\n- we will use log(price) as feature instead of using price it self"},{"metadata":{},"cell_type":"markdown","source":"## Univariate Analysis\n- item_condition_id\n- brand_name\n- shipping\n- main_category\n- sub_category_1\n- sub_category_2\n- name\n- item_discription"},{"metadata":{},"cell_type":"markdown","source":"#### Item_condition_id"},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.countplot('item_condition_id',data=data_train)\nax.set_title('Count of each Item_condition_id')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- most of the product have condtion_id between 1 and 3\n- very few product have condition_id 4 or 5"},{"metadata":{},"cell_type":"markdown","source":"#### brand_name"},{"metadata":{"trusted":true},"cell_type":"code","source":"#list of top 10 frequent brand name in data\n\n#brands = data_train.groupby(['brand_name']).count().sort_values(ascending=False)[:10]\n#print(brands)\nbrands = data_train['brand_name'].value_counts()\nprint(brands[:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,15))\nplt.subplot(331)\nplt.hist(np.log(data_train['price'][data_train['brand_name']=='PINK']+1),bins=50, edgecolor='white') #log(0) is undefined\nplt.ylabel('Frequency')\nplt.xlabel('log_Price')\nplt.title('PINK')\n\nplt.subplot(332)\nplt.hist(np.log(data_train['price'][data_train['brand_name']=='Nike']+1),bins=50, edgecolor='white') #log(0) is undefined\nplt.ylabel('Frequency')\nplt.xlabel('log_Price')\nplt.title('Nike')\n\nplt.subplot(333)\nplt.hist(np.log(data_train['price'][data_train['brand_name']==\"Victoria's Secret\"]+1),bins=50, edgecolor='white') #log(0) is undefined\nplt.ylabel('Frequency')\nplt.xlabel('log_Price')\nplt.title(\"Victoria's Secret\")\n\nplt.subplot(334)\nplt.hist(np.log(data_train['price'][data_train['brand_name']=='LuLaRoe']+1),bins=50, edgecolor='white') #log(0) is undefined\nplt.ylabel('Frequency')\nplt.xlabel('log_Price')\nplt.title('LuLaRoe')\n\nplt.subplot(335)\nplt.hist(np.log(data_train['price'][data_train['brand_name']=='Apple']+1),bins=50, edgecolor='white') #log(0) is undefined\nplt.ylabel('Frequency')\nplt.xlabel('log_Price')\nplt.title('Apple')\n\nplt.subplot(336)\nplt.hist(np.log(data_train['price'][data_train['brand_name']=='FOREVER 21']+1),bins=50, edgecolor='white') #log(0) is undefined\nplt.ylabel('Frequency')\nplt.xlabel('log_Price')\nplt.title('FOREVER 21')\n\nplt.subplot(337)\nplt.hist(np.log(data_train['price'][data_train['brand_name']=='Nintendo']+1),bins=50, edgecolor='white') #log(0) is undefined\nplt.ylabel('Frequency')\nplt.xlabel('log_Price')\nplt.title('Nintendo')\n\nplt.subplot(338)\nplt.hist(np.log(data_train['price'][data_train['brand_name']=='Lululemon']+1),bins=50, edgecolor='white') #log(0) is undefined\nplt.ylabel('Frequency')\nplt.xlabel('log_Price')\nplt.title('Lululemon')\n\nplt.subplot(339)\nplt.hist(np.log(data_train['price'][data_train['brand_name']=='Michael Kors']+1),bins=50, edgecolor='white') #log(0) is undefined\nplt.ylabel('Frequency')\nplt.xlabel('log_Price')\nplt.title('Michael Kors')\n\nplt.subplots_adjust(top=0.92, bottom=0.08, left=0.10, right=0.95, hspace=0.25,\n                    wspace=0.35)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#list of less 10 frequent brand\nprint(brands[200:210])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,15))\nplt.subplot(331)\nplt.hist(np.log(data_train['price'][data_train['brand_name']=='Spin Master']+1),bins=50, edgecolor='white') #log(0) is undefined\nplt.ylabel('Frequency')\nplt.xlabel('log_Price')\nplt.title('Spin Master')\n\nplt.subplot(332)\nplt.hist(np.log(data_train['price'][data_train['brand_name']=='Yankee Candle']+1),bins=50, edgecolor='white') #log(0) is undefined\nplt.ylabel('Frequency')\nplt.xlabel('log_Price')\nplt.title('Yankee Candle')\n\nplt.subplot(333)\nplt.hist(np.log(data_train['price'][data_train['brand_name']==\"James Avery\"]+1),bins=50, edgecolor='white') #log(0) is undefined\nplt.ylabel('Frequency')\nplt.xlabel('log_Price')\nplt.title(\"James Avery\")\n\nplt.subplot(334)\nplt.hist(np.log(data_train['price'][data_train['brand_name']=='Dr. Martens']+1),bins=50, edgecolor='white') #log(0) is undefined\nplt.ylabel('Frequency')\nplt.xlabel('log_Price')\nplt.title('Dr. Martens')\n\nplt.subplot(335)\nplt.hist(np.log(data_train['price'][data_train['brand_name']=='Keurig']+1),bins=50, edgecolor='white') #log(0) is undefined\nplt.ylabel('Frequency')\nplt.xlabel('log_Price')\nplt.title('Keurig')\n\nplt.subplot(336)\nplt.hist(np.log(data_train['price'][data_train['brand_name']=='WWE']+1),bins=50, edgecolor='white') #log(0) is undefined\nplt.ylabel('Frequency')\nplt.xlabel('log_Price')\nplt.title('WWE')\n\nplt.subplot(337)\nplt.hist(np.log(data_train['price'][data_train['brand_name']=='Bullhead']+1),bins=50, edgecolor='white') #log(0) is undefined\nplt.ylabel('Frequency')\nplt.xlabel('log_Price')\nplt.title('Bullhead')\n\nplt.subplot(338)\nplt.hist(np.log(data_train['price'][data_train['brand_name']=='AmazonBasics']+1),bins=50, edgecolor='white') #log(0) is undefined\nplt.ylabel('Frequency')\nplt.xlabel('log_Price')\nplt.title('AmazonBasics')\n\nplt.subplot(339)\nplt.hist(np.log(data_train['price'][data_train['brand_name']=='NBA']+1),bins=50, edgecolor='white') #log(0) is undefined\nplt.ylabel('Frequency')\nplt.xlabel('log_Price')\nplt.title('NBA')\n\nplt.subplots_adjust(top=0.92, bottom=0.08, left=0.10, right=0.95, hspace=0.25,\n                    wspace=0.35)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Obseravtion : \n- For most frequent brand name price distribution is not much skewed and well distributed\n- while in less frequent brand name distribution is not that well and skewed in some cases"},{"metadata":{},"cell_type":"markdown","source":"#### shipping"},{"metadata":{"trusted":true},"cell_type":"code","source":"(data_train['shipping'].value_counts())*100/data_train.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,8))\nplt.hist(np.log(data_train['price'][data_train['shipping']==0]+1),bins=30, edgecolor='white',color=\"blue\", label='shipping not paid by customer') #log(0) is undefined\nplt.hist(np.log(data_train['price'][data_train['shipping']==1]+1),bins=30, edgecolor='white',color=\"green\",label=\"shipping paid by customer\") #log(0) is undefined\nplt.ylabel('Frequency')\nplt.xlabel('log_Price')\nplt.legend(loc='upper right')\nplt.title('Shiping')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observation\n- Almost half(55%) customers do not have to pay shipping cost\n- We can intuitively say that is the price of product includes shipping cost than it is higger than the price which do not include the shippinc cost(shipping paid by customer)\n- From above histogram we can observe that the avg price of product not including the shipping cost is lowe than the avg price of the product which includes shipping cost"},{"metadata":{},"cell_type":"markdown","source":"#### main_category"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"There are %d unique main_categories.\" % data_train['main_category'].nunique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,8))\nax = sns.countplot('main_category',data=data_train)\nax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\nax.set_title('Count of each main_category')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,15))\nplt.subplot(331)\nplt.hist(np.log(data_train['price'][data_train['main_category']=='Men']+1),bins=50, edgecolor='white') #log(0) is undefined\nplt.ylabel('Frequency')\nplt.xlabel('log_Price')\nplt.title('Men')\n\nplt.subplot(332)\nplt.hist(np.log(data_train['price'][data_train['main_category']=='Electronics']+1),bins=50, edgecolor='white') #log(0) is undefined\nplt.ylabel('Frequency')\nplt.xlabel('log_Price')\nplt.title('Electronics')\n\nplt.subplot(333)\nplt.hist(np.log(data_train['price'][data_train['main_category']==\"Women\"]+1),bins=50, edgecolor='white') #log(0) is undefined\nplt.ylabel('Frequency')\nplt.xlabel('log_Price')\nplt.title(\"Women\")\n\nplt.subplot(334)\nplt.hist(np.log(data_train['price'][data_train['main_category']=='Home']+1),bins=50, edgecolor='white') #log(0) is undefined\nplt.ylabel('Frequency')\nplt.xlabel('log_Price')\nplt.title('Home')\n\nplt.subplot(335)\nplt.hist(np.log(data_train['price'][data_train['main_category']=='Sports & Outdoors']+1),bins=50, edgecolor='white') #log(0) is undefined\nplt.ylabel('Frequency')\nplt.xlabel('log_Price')\nplt.title('Keurig')\n\nplt.subplot(336)\nplt.hist(np.log(data_train['price'][data_train['main_category']=='Beauty']+1),bins=50, edgecolor='white') #log(0) is undefined\nplt.ylabel('Frequency')\nplt.xlabel('log_Price')\nplt.title('Beauty')\n\nplt.subplot(337)\nplt.hist(np.log(data_train['price'][data_train['main_category']=='Kids']+1),bins=50, edgecolor='white') #log(0) is undefined\nplt.ylabel('Frequency')\nplt.xlabel('log_Price')\nplt.title('Kids')\n\nplt.subplot(338)\nplt.hist(np.log(data_train['price'][data_train['main_category']=='Handmade']+1),bins=50, edgecolor='white') #log(0) is undefined\nplt.ylabel('Frequency')\nplt.xlabel('log_Price')\nplt.title('Handmade')\n\nplt.subplot(339)\nplt.hist(np.log(data_train['price'][data_train['main_category']=='Other']+1),bins=50, edgecolor='white') #log(0) is undefined\nplt.ylabel('Frequency')\nplt.xlabel('log_Price')\nplt.title('Other')\n\nplt.subplots_adjust(top=0.92, bottom=0.08, left=0.10, right=0.95, hspace=0.25,\n                    wspace=0.35)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"data_train.groupby('main_category')['price'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### sub_category_1"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"There are %d unique sub_category_1.\" % data_train['sub_category_1'].nunique())","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,8))\nax = sns.countplot('sub_category_1',data=data_train,order=data_train.sub_category_1.value_counts().iloc[:15].index)\nax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\nax.set_title('Count of each sub_category_1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,15))\nplt.subplot(331)\nplt.hist(np.log(data_train['price'][data_train['sub_category_1']=='Athletic Apparel']+1),bins=50, edgecolor='white') #log(0) is undefined\nplt.ylabel('Frequency')\nplt.xlabel('log_Price')\nplt.title('Athletic Apparel')\n\nplt.subplot(332)\nplt.hist(np.log(data_train['price'][data_train['sub_category_1']=='Makeup']+1),bins=50, edgecolor='white') #log(0) is undefined\nplt.ylabel('Frequency')\nplt.xlabel('log_Price')\nplt.title('Makeup')\n\nplt.subplot(333)\nplt.hist(np.log(data_train['price'][data_train['sub_category_1']==\"Tops & Blouses\"]+1),bins=50, edgecolor='white') #log(0) is undefined\nplt.ylabel('Frequency')\nplt.xlabel('log_Price')\nplt.title(\"Tops & Blouses\")\n\nplt.subplot(334)\nplt.hist(np.log(data_train['price'][data_train['sub_category_1']=='Shoes']+1),bins=50, edgecolor='white') #log(0) is undefined\nplt.ylabel('Frequency')\nplt.xlabel('log_Price')\nplt.title('Shoes')\n\nplt.subplot(335)\nplt.hist(np.log(data_train['price'][data_train['sub_category_1']=='Jewelry']+1),bins=50, edgecolor='white') #log(0) is undefined\nplt.ylabel('Frequency')\nplt.xlabel('log_Price')\nplt.title('Jewelry')\n\nplt.subplot(336)\nplt.hist(np.log(data_train['price'][data_train['sub_category_1']=='Toys']+1),bins=50, edgecolor='white') #log(0) is undefined\nplt.ylabel('Frequency')\nplt.xlabel('log_Price')\nplt.title('Toys')\n\nplt.subplot(337)\nplt.hist(np.log(data_train['price'][data_train['sub_category_1']=='Cell Phones & Accessories']+1),bins=50, edgecolor='white') #log(0) is undefined\nplt.ylabel('Frequency')\nplt.xlabel('log_Price')\nplt.title('Cell Phones & Accessories')\n\nplt.subplot(338)\nplt.hist(np.log(data_train['price'][data_train['sub_category_1']==\"Jeans\"]+1),bins=50, edgecolor='white') #log(0) is undefined\nplt.ylabel('Frequency')\nplt.xlabel('log_Price')\nplt.title(\"Jeans\")\n\nplt.subplot(339)\nplt.hist(np.log(data_train['price'][data_train['sub_category_1']=='Dresses']+1),bins=50, edgecolor='white') #log(0) is undefined\nplt.ylabel('Frequency')\nplt.xlabel('log_Price')\nplt.title('Dresses')\n\nplt.subplots_adjust(top=0.92, bottom=0.08, left=0.10, right=0.95, hspace=0.25,\n                    wspace=0.35)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"data_train.groupby('sub_category_1')['price'].describe()[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### sub_category_2"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"There are %d unique sub_category_2.\" % data_train['sub_category_2'].nunique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,8))\nax = sns.countplot('sub_category_2',data=data_train,order=data_train.sub_category_2.value_counts().iloc[:15].index)\nax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\nax.set_title('Count of each sub_category_2')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,15))\nplt.subplot(331)\nplt.hist(np.log(data_train['price'][data_train['sub_category_2']=='Boots']+1),bins=50, edgecolor='white') #log(0) is undefined\nplt.ylabel('Frequency')\nplt.xlabel('log_Price')\nplt.title('Boots')\n\nplt.subplot(332)\nplt.hist(np.log(data_train['price'][data_train['sub_category_2']=='Other']+1),bins=50, edgecolor='white') #log(0) is undefined\nplt.ylabel('Frequency')\nplt.xlabel('log_Price')\nplt.title('Other')\n\nplt.subplot(333)\nplt.hist(np.log(data_train['price'][data_train['sub_category_2']==\"Face\"]+1),bins=50, edgecolor='white') #log(0) is undefined\nplt.ylabel('Frequency')\nplt.xlabel('log_Price')\nplt.title(\"Face\")\n\nplt.subplot(334)\nplt.hist(np.log(data_train['price'][data_train['sub_category_2']=='T-Shirts']+1),bins=50, edgecolor='white') #log(0) is undefined\nplt.ylabel('Frequency')\nplt.xlabel('log_Price')\nplt.title('T-Shirts')\n\nplt.subplot(335)\nplt.hist(np.log(data_train['price'][data_train['sub_category_2']=='Shoes']+1),bins=50, edgecolor='white') #log(0) is undefined\nplt.ylabel('Frequency')\nplt.xlabel('log_Price')\nplt.title('Shoes')\n\nplt.subplot(336)\nplt.hist(np.log(data_train['price'][data_train['sub_category_2']=='Games']+1),bins=50, edgecolor='white') #log(0) is undefined\nplt.ylabel('Frequency')\nplt.xlabel('log_Price')\nplt.title('Games')\n\nplt.subplot(337)\nplt.hist(np.log(data_train['price'][data_train['sub_category_2']=='Athletic']+1),bins=50, edgecolor='white') #log(0) is undefined\nplt.ylabel('Frequency')\nplt.xlabel('log_Price')\nplt.title('Athletic')\n\nplt.subplot(338)\nplt.hist(np.log(data_train['price'][data_train['sub_category_2']==\"Eyes\"]+1),bins=50, edgecolor='white') #log(0) is undefined\nplt.ylabel('Frequency')\nplt.xlabel('log_Price')\nplt.title(\"Eyes\")\n\nplt.subplot(339)\nplt.hist(np.log(data_train['price'][data_train['sub_category_2']=='Shorts']+1),bins=50, edgecolor='white') #log(0) is undefined\nplt.ylabel('Frequency')\nplt.xlabel('log_Price')\nplt.title('Shorts')\n\nplt.subplots_adjust(top=0.92, bottom=0.08, left=0.10, right=0.95, hspace=0.25,\n                    wspace=0.35)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train.groupby('sub_category_2')['price'].describe()[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observation\n- As we can see in above plots that price(log(price)) is  pretty well distributed for the main_category,sub_category_1,sub_category_2 \n- From the main_category product plot as we can see that most of the products are for womens, 2nd and 3rd highest is for beauty and kids respectively "},{"metadata":{},"cell_type":"markdown","source":"#### name"},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://gist.github.com/sebleier/554280\n# we are removing the words from the stop words list: 'no', 'nor', 'not'\nstopwords= ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n            'won', \"won't\", 'wouldn', \"wouldn't\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\ndef decontracted(phrase):\n    # specific\n    try:\n        phrase = re.sub(r\"won't\", \"will not\", phrase)\n        phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n        # general\n        phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n        phrase = re.sub(r\"\\'re\", \" are\", phrase)\n        phrase = re.sub(r\"\\'s\", \" is\", phrase)\n        phrase = re.sub(r\"\\'d\", \" would\", phrase)\n        phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n        phrase = re.sub(r\"\\'t\", \" not\", phrase)\n        phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n        phrase = re.sub(r\"\\'m\", \" am\", phrase)\n        return phrase\n    except:\n        return 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def wordCount_with_cleaning(sentance):\n    try:\n        sent = decontracted(sentance)\n        sent = sent.replace('\\\\r', ' ')\n        sent = sent.replace('\\\\\"', ' ')\n        sent = sent.replace('\\\\n', ' ')\n        sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n        # https://gist.github.com/sebleier/554280\n        sent = ' '.join(e for e in sent.split() if e.lower() not in stopwords)\n        sent = sent.strip()\n        return(len(sent.split()))\n    except:\n        return 0 ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def wordCount_without_cleaning(sentance):\n    try:   \n        # https://gist.github.com/sebleier/554280\n        sent = ' '.join(e for e in sentance.split())\n        sent = sent.strip()\n        return(len(sent.split()))\n    except:\n        return 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def len_str(x):\n    return(len(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# words in name\n\nx = data_train['name'].apply(lambda x: wordCount_without_cleaning(x))\nplt.hist(x,bins = 30,range=[0,10])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#length of name\nx = data_train['name'].apply(lambda x: len_str(x))\nplt.hist(x,bins = 30,range=[0,50],edgecolor='white')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text = data_train['name'].values\nwordcloud = WordCloud(width = 800, height = 800, \n                background_color ='white', \n                stopwords = STOPWORDS, \n                min_font_size = 10).generate(str(text))\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observation\n- Word count for the name columns is well distributed but distribution for lenght of name is not that well\n- Legnth of name is in range 1 to 42"},{"metadata":{},"cell_type":"markdown","source":"#### item_description"},{"metadata":{"trusted":true},"cell_type":"code","source":"# words count without cleaning in description\nx = data_train['item_description'].apply(lambda x: wordCount_without_cleaning(x))\nplt.figure(figsize=(8,4))\nplt.hist(x,bins = 30,range=[0,10])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#length of description\nx = data_train['item_description'].apply(lambda x: len_str(str(x)))\nplt.figure(figsize=(8,4))\nplt.hist(x,bins = 30,range=[0,50],edgecolor='white')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize = (40, 30))\ntext = data_train['item_description'][data_train['main_category']=='Men'].values\n#text = data_train['item_description'].values\nwordcloud = WordCloud(width = 800, height = 800, \n                background_color ='white', \n                stopwords = STOPWORDS, \n                min_font_size = 10, max_words=50,random_state=42).generate(str(text))\nplt.subplot(131)\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.tight_layout(pad=1)\nplt.title(\"Men\",fontsize= 50)\n\n\n\n\ntext = data_train['item_description'][data_train['main_category']=='Electronics'].values\n#text = data_train['item_description'].values\nwordcloud = WordCloud(width = 800, height = 800, \n                background_color ='white', \n                stopwords = STOPWORDS, \n                min_font_size = 10,max_words=50,random_state=42).generate(str(text))\nplt.subplot(132)\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.tight_layout(pad=1)\nplt.title(\"Electronics\",fontsize= 50)\n\n\n\n\ntext = data_train['item_description'][data_train['main_category']=='Kids'].values\n#text = data_train['item_description'].values\nwordcloud = WordCloud(width = 800, height = 800, \n                background_color ='white', \n                stopwords = STOPWORDS, \n                min_font_size = 10,max_words=50,random_state=42).generate(str(text))\nplt.subplot(133)\nplt.title(\"Kids\",fontsize= 50)\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.tight_layout(pad=1)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize = (40, 30))\n#    facecolor = 'k',\n#    edgecolor = 'k')\ntext = data_train['item_description'][data_train['sub_category_1']=='Athletic Apparel'].values\n#text = data_train['item_description'].values\nwordcloud = WordCloud(width = 800, height = 800, \n                background_color ='white', \n                stopwords = STOPWORDS, \n                min_font_size = 10, max_words=50,random_state=42).generate(str(text))\nplt.subplot(131)\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.tight_layout(pad=1)\nplt.title(\"Athletic Apparel\",fontsize= 50)\n\n\n\n\ntext = data_train['item_description'][data_train['sub_category_1']=='Makeup'].values\n#text = data_train['item_description'].values\nwordcloud = WordCloud(width = 800, height = 800, \n                background_color ='white', \n                stopwords = STOPWORDS, \n                min_font_size = 10,max_words=50,random_state=42).generate(str(text))\nplt.subplot(132)\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.tight_layout(pad=1)\nplt.title(\"Makeup\",fontsize= 50)\n\n\n\n\ntext = data_train['item_description'][data_train['sub_category_1']==\"Tops & Blouses\"].values\n#text = data_train['item_description'].values\nwordcloud = WordCloud(width = 800, height = 800, \n                background_color ='white', \n                stopwords = STOPWORDS, \n                min_font_size = 10,max_words=50,random_state=42).generate(str(text))\nplt.subplot(133)\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.tight_layout(pad=1)\nplt.title(\"Tops & Blouses\",fontsize= 50)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize = (40, 30))\ntext = data_train['item_description'][data_train['sub_category_2']=='Games'].values\n#text = data_train['item_description'].values\nwordcloud = WordCloud(width = 800, height = 800, \n                background_color ='white', \n                stopwords = STOPWORDS, \n                min_font_size = 10, max_words=50,random_state=42).generate(str(text))\nplt.subplot(131)\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.tight_layout(pad=1)\nplt.title(\"Games\",fontsize= 50)\n\n\n\n\ntext = data_train['item_description'][data_train['sub_category_2']=='Face'].values\n#text = data_train['item_description'].values\nwordcloud = WordCloud(width = 800, height = 800, \n                background_color ='white', \n                stopwords = STOPWORDS, \n                min_font_size = 10,max_words=50,random_state=42).generate(str(text))\nplt.subplot(132)\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.tight_layout(pad=1)\nplt.title(\"Face\",fontsize= 50)\n\n\n\n\ntext = data_train['item_description'][data_train['sub_category_2']==\"T-Shirts\"].values\n#text = data_train['item_description'].values\nwordcloud = WordCloud(width = 800, height = 800, \n                background_color ='white', \n                stopwords = STOPWORDS, \n                min_font_size = 10,max_words=50,random_state=42).generate(str(text))\nplt.subplot(133)\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.tight_layout(pad=1)\nplt.title(\"T-Shirts\",fontsize= 50)\n\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Obseravtion\n- range for the word count of item_description is 1 to 42\n- Almost 120K products has word count 3\n- Almost equal number of products has word count range 2 to 10\n- Around 100K products have description length equal to 18\n- Desciption length ranges from 1 to 50"},{"metadata":{},"cell_type":"markdown","source":"# Pre-processing"},{"metadata":{},"cell_type":"markdown","source":"- item_description"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\npreprocessed_item_description = []\n# tqdm is for printing the status bar\nfor sentance in tqdm(data_train['item_description'].values):\n    sent = decontracted(str(sentance))\n    sent = sent.replace('\\\\r', ' ')\n    sent = sent.replace('\\\\\"', ' ')\n    sent = sent.replace('\\\\n', ' ')\n    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n    # https://gist.github.com/sebleier/554280\n    sent = ' '.join(e for e in sent.split() if e.lower() not in stopwords)\n    preprocessed_item_description.append(sent.lower().strip())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\npreprocessed_test_item_description = []\n# tqdm is for printing the status bar\nfor sentance in tqdm(data_test['item_description'].values):\n    sent = decontracted(str(sentance))\n    sent = sent.replace('\\\\r', ' ')\n    sent = sent.replace('\\\\\"', ' ')\n    sent = sent.replace('\\\\n', ' ')\n    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n    # https://gist.github.com/sebleier/554280\n    sent = ' '.join(e for e in sent.split() if e.lower() not in stopwords)\n    preprocessed_test_item_description.append(sent.lower().strip())","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# after preprocesing\ndata_train = data_train.drop('item_description',axis=1)\ndata_train['item_description'] = preprocessed_item_description\ndata_train.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# after preprocesing\ndata_test = data_test.drop('item_description',axis=1)\ndata_test['item_description'] = preprocessed_test_item_description\ndata_test.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#data  = data_train.drop(['main_category','sub_category_1','sub_category_2'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Train-Test split"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = np.log10(np.array(data_train['price'])+1)\nX = data_train.drop('price',axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_cv,Y_train,Y_cv = train_test_split(X, y, test_size=0.20, random_state=42)\n#X_test,X_cv,Y_test,Y_cv = train_test_split(X_test, Y_test, test_size=0.50, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Featurization"},{"metadata":{},"cell_type":"markdown","source":"#### Gessing null Brands from name and category..."},{"metadata":{},"cell_type":"markdown","source":"https://www.kaggle.com/gspmoreira/cnn-glove-single-model-private-lb-0-41117-35th"},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating dictionary containing brand_name and category_name\ndef concat_categories(x):\n    return set(x.values)\n\n\n#function returning brand name using brand and category_name\ndef brandfinder(name, category):    \n    for brand in brands_sorted_by_size:\n        if brand in name and category in brand_names_categories[brand]:\n            return brand\n    return 'Unknown'\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"brand_names_categories = dict(X_train[X_train['brand_name'] != 'Unknown'][['brand_name','category_name']].astype('str')\\\n                              .groupby('brand_name').agg(concat_categories).reset_index().values.tolist())\\\n\n#Brands sorted by length (decreasinly), so that longer brand names have precedence in the null brand search\nbrands_sorted_by_size = list(sorted(filter(lambda y: len(y) >= 3, \\\n                                           list(brand_names_categories.keys())), \\\n                                            key = lambda x: -len(x)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_names_unknown_brands_train = X_train[X_train['brand_name'] == 'Unknown'][['name','category_name']].\\\n                            astype('str').values\n\ntrain_names_unknown_brands_cv = X_cv[X_cv['brand_name'] == 'Unknown'][['name','category_name']].\\\n                            astype('str').values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_names_unknown_brands_data_test = data_test[data_test['brand_name'] == 'Unknown'][['name','category_name']].\\\n                            astype('str').values","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"train_estimated_brands_train = []\nfor name, category in tqdm(train_names_unknown_brands_train):\n    brand = brandfinder(name, category) \n    train_estimated_brands_train.append(brand)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_estimated_brands_cv = []\nfor name, category in tqdm(train_names_unknown_brands_cv):\n    brand = brandfinder(name, category) \n    train_estimated_brands_cv.append(brand)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_estimated_brands_data_test = []\nfor name, category in tqdm(train_names_unknown_brands_data_test):\n    brand = brandfinder(name, category) \n    train_estimated_brands_data_test.append(brand)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.loc[X_train['brand_name'] == 'Unknown', 'brand_name'] = train_estimated_brands_train\n\nX_cv.loc[X_cv['brand_name'] == 'Unknown', 'brand_name'] = train_estimated_brands_cv\n\ndata_test.loc[data_test['brand_name'] == 'Unknown', 'brand_name'] = train_estimated_brands_data_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### top brands and categories"},{"metadata":{"trusted":true},"cell_type":"code","source":"max_brand = 2500\nmax_category_name = 1000\nmax_category = 1000\nname_min_df =10\nmax_item_description_features = 50000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cutting brand names\nkeep_brand = X_train['brand_name'].value_counts().loc[lambda x: x.index != 'Unknown'].index[:max_brand]\n\nX_train.loc[~X_train['brand_name'].isin(keep_brand), 'brand_name'] = 'Unknown'\nX_cv.loc[~X_cv['brand_name'].isin(keep_brand), 'brand_name'] = 'Unknown'\ndata_test.loc[~data_test['brand_name'].isin(keep_brand), 'brand_name'] = 'Unknown'\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cutting category\nkeep_category1 = X_train['main_category'].value_counts().loc[lambda x: x.index != 'Unknown'].index[:max_category]\nkeep_category2 = X_train['sub_category_1'].value_counts().loc[lambda x: x.index != 'Unknown'].index[:max_category]\nkeep_category3 = X_train['sub_category_2'].value_counts().loc[lambda x: x.index != 'Unknown'].index[:max_category]\n\nX_train.loc[~X_train['main_category'].isin(keep_category1), 'main_category'] = 'Unknown'\nX_train.loc[~X_train['sub_category_1'].isin(keep_category2), 'sub_category_1'] = 'Unknown'\nX_train.loc[~X_train['sub_category_2'].isin(keep_category3), 'sub_category_1'] = 'Unknown'\n                                                             \nX_cv.loc[~X_cv['main_category'].isin(keep_category1), 'main_category'] = 'Unknown'\nX_cv.loc[~X_cv['sub_category_1'].isin(keep_category2), 'sub_category_1'] = 'Unknown'\nX_cv.loc[~X_cv['sub_category_2'].isin(keep_category3), 'sub_category_1'] = 'Unknown'\n\ndata_test.loc[~data_test['main_category'].isin(keep_category1), 'main_category'] = 'Unknown'\ndata_test.loc[~data_test['sub_category_1'].isin(keep_category2), 'sub_category_1'] = 'Unknown'\ndata_test.loc[~data_test['sub_category_2'].isin(keep_category3), 'sub_category_1'] = 'Unknown'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### vectorization of features...."},{"metadata":{"trusted":true},"cell_type":"code","source":"# item_description...TfidfVectorizer\ntv = TfidfVectorizer(max_features=max_item_description_features,ngram_range=(1, 3),token_pattern=r'(?u)\\b\\w+\\b',stop_words='english')\n#tv = TfidfVectorizer(max_features=50000, ngram_range=(1, 3),token_pattern=r'(?u)\\b\\w+\\b',stop_words='english')\ntv.fit(X_train['item_description'])\nX_train_item_description_tfidf = tv.transform(X_train['item_description'])\nX_cv_item_description_tfidf = tv.transform(X_cv['item_description'])\ndata_test_item_description_tfidf = tv.transform(data_test['item_description'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# name.......CountVectorizer\n#cv = CountVectorizer(min_df=name_min_df,ngram_range=(1, 2),stop_words='english')\n#tv = TfidfVectorizermin_df=name_min_df,ngram_range=(1, 2),stop_words='english')\ntv.fit(X_train['name'])\nX_train_name_tfidf = tv.transform(X_train['name'])\nX_cv_name_tfidf = tv.transform(X_cv['name'])\ndata_test_name_tfidf = tv.transform(data_test['name'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# category_name......CountVectorizer\ncv = CountVectorizer(min_df=name_min_df)\n\n#category_name(used in lgbm)\ncv.fit(X_train['category_name'].astype('category'))\nX_train_category =cv.transform(X_train['category_name'].astype('category'))\nX_cv_category =cv.transform(X_cv['category_name'].astype('category'))\ndata_test_category =cv.transform(data_test['category_name'].astype('category'))\n\n##################################################################################################################\n#main_category\n#cv.fit(X_train['main_category'].astype('category'))\n#X_train_main_category =cv.transform(X_train['main_category'].astype('category'))\n#X_cv_main_category =cv.transform(X_cv['main_category'].astype('category'))\n#data_test_main_category =cv.transform(data_test['main_category'].astype('category'))\n\n#sub_category_1\n#cv.fit(X_train['sub_category_1'].astype('category'))\n#X_train_sub_category_1 =cv.transform(X_train['sub_category_1'].astype('category'))\n#X_cv_sub_category_1 =cv.transform(X_cv['sub_category_1'].astype('category'))\n#data_test_sub_category_1 =cv.transform(data_test['sub_category_1'].astype('category'))\n\n#sub_category_2\n#cv.fit(X_train['sub_category_2'].astype('category'))\n#X_train_sub_category_2 =cv.transform(X_train['sub_category_2'].astype('category'))\n#X_cv_sub_category_2 =cv.transform(X_cv['sub_category_2'].astype('category'))\n#data_test_sub_category_2 =cv.transform(data_test['sub_category_2'].astype('category'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# brand_name.....LabelBinarizer\nlb = LabelBinarizer(sparse_output=True)\nlb.fit(X_train['brand_name'].astype('category'))\nX_train_brand = lb.transform(X_train['brand_name'].astype('category'))\nX_cv_brand = lb.transform(X_cv['brand_name'])\ndata_test_brand = lb.transform(data_test['brand_name'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# shipping...pd.getdummies\n# item_condition id...pd.getdummies\n#X_train['item_condition_id'] = X_train['item_condition_id'].astype('category')\n#X_test['item_condition_id'] = X_test['item_condition_id'].astype('category')\n\nX_train_dummies = csr_matrix(pd.get_dummies(X_train[['item_condition_id', 'shipping']], sparse=True).values)\nX_cv_dummies = csr_matrix(pd.get_dummies(X_cv[['item_condition_id', 'shipping']], sparse=True).values)\ndata_test_dummies = csr_matrix(pd.get_dummies(data_test[['item_condition_id', 'shipping']], sparse=True).values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# word count for item description\nX_train_word_count_item_desc = X_train['item_description'].apply(lambda x: wordCount_with_cleaning(x))\nX_cv_word_count_item_desc = X_cv['item_description'].apply(lambda x: wordCount_with_cleaning(x))\ndata_test_word_count_item_desc = data_test['item_description'].apply(lambda x: wordCount_with_cleaning(x))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# word count for name\nX_train_word_count_name = X_train['name'].apply(lambda x: wordCount_with_cleaning(x))\nX_cv_word_count_name = X_cv['name'].apply(lambda x: wordCount_with_cleaning(x))\ndata_test_word_count_name = data_test['name'].apply(lambda x: wordCount_with_cleaning(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check the shapes\nprint(X_train_item_description_tfidf.shape)\nprint(X_train_name_tfidf.shape)\nprint(X_train_category.shape)\n#print(X_train_main_category.shape)\n#print(X_train_sub_category_1.shape)\n#print(X_train_sub_category_2.shape)\nprint(X_train_brand.shape)\nprint(X_train_dummies.shape)\nprint(X_train_word_count_item_desc.shape)\nprint(X_train_word_count_name.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check the shapes\nprint(X_cv_item_description_tfidf.shape)\nprint(X_cv_name_tfidf.shape)\nprint(X_cv_category.shape)\n#print(X_cv_main_category.shape)\n#print(X_cv_sub_category_1.shape)\n#print(X_cv_sub_category_2.shape)\nprint(X_cv_brand.shape)\nprint(X_cv_dummies.shape)\nprint(X_cv_word_count_item_desc.shape)\nprint(X_cv_word_count_name.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check the shapes\nprint(data_test_item_description_tfidf.shape)\nprint(data_test_name_tfidf.shape)\nprint(data_test_category.shape)\n#print(data_test_main_category.shape)\n#print(data_test_sub_category_1.shape)\n#print(data_test_sub_category_2.shape)\nprint(data_test_brand.shape)\nprint(data_test_dummies.shape)\nprint(data_test_word_count_item_desc.shape)\nprint(data_test_word_count_name.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# stacking features vectors together\nX_train_vectorized = hstack((X_train_item_description_tfidf,\\\n                             X_train_name_tfidf,\\\n                             X_train_category,\\\n                             X_train_brand,\\\n                             X_train_dummies,\\\n                             X_train_word_count_item_desc.values.reshape(-1,1),\\\n                             X_train_word_count_name.values.reshape(-1,1))).tocsr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_vectorized.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# stacking features vectors together\n#X_train_vectorized_with_sub_cat = hstack((X_train_item_description_tfidf,\\\n#                             X_train_name_tfidf,\\\n#                             X_train_main_category,\\\n#                             X_train_sub_category_1,\\\n#                             X_train_sub_category_2,\\\n#                             X_train_brand,\\\n#                             X_train_dummies,\\\n#                             X_train_word_count_item_desc.values.reshape(-1,1),\\\n#                             X_train_word_count_name.values.reshape(-1,1))).tocsr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#X_train_vectorized_with_sub_cat.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# stacking features vectors together\nX_cv_vectorized = hstack((X_cv_item_description_tfidf,\\\n                             X_cv_name_tfidf,\\\n                             X_cv_category,\\\n                             X_cv_brand,\\\n                             X_cv_dummies,\\\n                             X_cv_word_count_item_desc.values.reshape(-1,1),\\\n                             X_cv_word_count_name.values.reshape(-1,1))).tocsr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_cv_vectorized.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# stacking features vectors together\n#X_cv_vectorized_with_sub_cat = hstack((X_cv_item_description_tfidf,\\\n#                             X_cv_name_tfidf,\\\n#                             X_cv_main_category,\\\n#                             X_cv_sub_category_1,\\\n#                             X_cv_sub_category_2,\\\n#                             X_cv_brand,\\\n#                             X_cv_dummies,\\\n#                             X_cv_word_count_item_desc.values.reshape(-1,1),\\\n#                             X_cv_word_count_name.values.reshape(-1,1))).tocsr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#X_cv_vectorized_with_sub_cat.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# stacking features vectors together\ndata_test_vectorized = hstack((data_test_item_description_tfidf,\\\n                            data_test_name_tfidf,\\\n                            data_test_category,\\\n                            data_test_brand,\\\n                            data_test_dummies,\\\n                            data_test_word_count_item_desc.values.reshape(-1,1),\\\n                            data_test_word_count_name.values.reshape(-1,1))).tocsr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_test_vectorized.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# stacking features vectors together\n#data_test_vectorized_with_sub_cat = hstack((data_test_item_description_tfidf,\\\n#                             data_test_name_tfidf,\\\n#                             data_test_main_category,\\\n#                             data_test_sub_category_1,\\\n#                             data_test_sub_category_2,\\\n#                             data_test_brand,\\\n#                             data_test_dummies,\\\n#                             data_test_word_count_item_desc.values.reshape(-1,1),\\\n#                             data_test_word_count_name.values.reshape(-1,1))).tocsr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#data_test_vectorized_with_sub_cat.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\ndel(data_train,preprocessed_item_description,preprocessed_test_item_description,X,y)\ndel(train_estimated_brands_train,train_estimated_brands_cv,train_estimated_brands_data_test,keep_brand)\ndel(data_test_item_description_tfidf,\n    data_test_name_tfidf,\\\n    data_test_category,\\\n    #data_test_main_category,\\\n    #data_test_sub_category_1,\\\n    #data_test_sub_category_2,\\\n    data_test_brand,\\\n    data_test_dummies,\\\n    data_test_word_count_item_desc,\\\n    data_test_word_count_name)\ndel(X_train_item_description_tfidf,\n    X_train_name_tfidf,\\\n    X_train_category,\\\n    #X_train_main_category,\\\n    #X_train_sub_category_1,\\\n    #X_train_sub_category_2,\\\n    X_train_brand,\\\n    X_train_dummies,\\\n    X_train_word_count_item_desc,\\\n    X_train_word_count_name)\ndel(X_cv_item_description_tfidf,\n    X_cv_name_tfidf,\\\n    X_cv_category,\\\n    #X_cv_main_category,\\\n    #X_cv_sub_category_1,\\\n    #X_cv_sub_category_2,\\\n    X_cv_brand,\\\n    X_cv_dummies,\\\n    X_cv_word_count_item_desc,\\\n    X_cv_word_count_name)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Models"},{"metadata":{"trusted":true},"cell_type":"code","source":"def rmsle(y, y0):\n    assert len(y) == len(y0)\n    return np.sqrt(np.mean(np.power(np.log1p(y)-np.log1p(y0), 2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Error_dict  = {}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1 Light BGM\nhttps://medium.com/@pushkarmandot/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc"},{"metadata":{},"cell_type":"markdown","source":"https://stackoverflow.com/questions/46202223/root-mean-log-squared-error-issue-with-scitkit-learn-ensemble-gradientboostingre\n\nhttps://www.kaggle.com/opanichev/lightgbm-regressor"},{"metadata":{},"cell_type":"markdown","source":"### Tuned lgbm_1"},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\n\n\nparams = {\n         'colsample_bytree': 0.42799939792816927,\n          'max_depth': 10,\n          'min_child_samples': 370,\n          'min_child_weight': 0.01,\n          'num_leaves': 49,\n          'reg_lambda': 5,\n          'subsample': 0.6739316550896339,\n          'learning_rate':0.1,\n          'reg_alpha' :0.5,\n          'boosting_type': 'gbdt',\n          'objective' : 'regression',\n          'metric' : 'RMSE',\n          'verbosity': -1\n         }\n\nd_train = lgb.Dataset(X_train_vectorized, label=Y_train)\nd_valid = lgb.Dataset(X_cv_vectorized, label=Y_cv)\nwatchlist = [d_train, d_valid]\n\nmodel_1 = lgb.train(params, train_set=d_train,valid_sets=watchlist,num_boost_round=2000,verbose_eval=200,early_stopping_rounds=100) \n#d_train = lgb.Dataset(X_train_vectorized, label=Y_train)\n#clf = lgb.train(params, d_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm_1_pred_train=model_1.predict(X_train_vectorized)\nlgbm_1_pred_cv=model_1.predict(X_cv_vectorized)\nlgbm_1_pred_data_test=model_1.predict(data_test_vectorized)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Train rmsle: \"+str(rmsle(10 ** Y_train-1, 10 ** lgbm_1_pred_train-1)))\nprint(\"CV rmsle: \"+str(rmsle(10 ** Y_cv-1, 10 ** lgbm_1_pred_cv-1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Error_dict['lgbm_1_Train_rmsle'] = rmsle(10 ** Y_train-1, 10 ** lgbm_1_pred_train-1) \nError_dict['lgbm_1_cv_rmsle'] = rmsle(10 ** Y_cv-1, 10 ** lgbm_1_pred_cv-1) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ridge Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ridge model\nfrom sklearn.linear_model import Ridge\nreg_ridge = Ridge(solver='sag', alpha=5)\nreg_ridge.fit(X_train_vectorized, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg_ridge_pred_train=reg_ridge.predict(X_train_vectorized)\nreg_ridge_pred_cv=reg_ridge.predict(X_cv_vectorized)\nreg_ridge_pred_data_test=reg_ridge.predict(data_test_vectorized)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Train rmsle: \"+str(rmsle(10 ** Y_train-1, 10 ** reg_ridge_pred_train-1)))\nprint(\"CV rmsle: \"+str(rmsle(10 ** Y_cv-1, 10 ** reg_ridge_pred_cv-1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Error_dict['reg_ridge_Train_rmsle'] = rmsle(10 ** Y_train-1, 10 ** reg_ridge_pred_train-1) \nError_dict['reg_ridge_Cv_rmsle'] = rmsle(10 ** Y_cv-1, 10 ** reg_ridge_pred_cv-1 ) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Stacking"},{"metadata":{"trusted":true},"cell_type":"code","source":"stacked_pred_train = pd.DataFrame(index=np.array(range(X_train.shape[0])))\nstacked_pred_cv = pd.DataFrame(index=np.array(range(X_cv.shape[0])))\nstacked_pred_data_test = pd.DataFrame(index=np.array(range(data_test.shape[0])))\n\nstacked_pred_train.insert(0,\"lgbm_model_pred\",lgbm_1_pred_train)\nstacked_pred_train.insert(1,\"Ridge_model_pred\",reg_ridge_pred_train)\n\nstacked_pred_cv.insert(0,\"lgbm_model_pred\",lgbm_1_pred_cv)\nstacked_pred_cv.insert(1,\"Ridge_model_pred\",reg_ridge_pred_cv)\n\nstacked_pred_data_test.insert(0,\"lgbm_model_pred\",lgbm_1_pred_data_test)\nstacked_pred_data_test.insert(1,\"Ridge_model_pred\",reg_ridge_pred_data_test)\n\n\n#stacked_pred_train = (lgbm_1_pred_train + reg_ridge_pred_train)/2\n#stacked_pred_cv = (lgbm_1_pred_cv + reg_ridge_pred_cv)/2\n#stacked_pred_data_test = (lgbm_1_pred_data_test + reg_ridge_pred_data_test)/2\n\n#final_pred_train = (lgbm_1_pred_train + lgbm_2_pred_train)/2\n#final_pred_test = (lgbm_1_pred_test + lgbm_2_pred_test)/2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_stacked = sparse.csr_matrix(hstack([X_train_vectorized, sparse.csr_matrix(stacked_pred_train)]))\nX_cv_stacked = sparse.csr_matrix(hstack([X_cv_vectorized, sparse.csr_matrix(stacked_pred_cv)]))\ndata_test_stacked = sparse.csr_matrix(hstack([data_test_vectorized, sparse.csr_matrix(stacked_pred_data_test)]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\n\n\nparams = {\n         'colsample_bytree': 0.42799939792816927,\n          'max_depth': 10,\n          'min_child_samples': 370,\n          'min_child_weight': 0.01,\n          'num_leaves': 49,\n          'reg_lambda': 5,\n          'subsample': 0.6739316550896339,\n          'learning_rate':0.1,\n          'reg_alpha' :0.5,\n          'boosting_type': 'gbdt',\n          'objective' : 'regression',\n          'metric' : 'RMSE',\n          'verbosity': -1\n         }\n\nd_train = lgb.Dataset(X_train_stacked, label=Y_train)\nd_valid = lgb.Dataset(X_cv_stacked, label=Y_cv)\nwatchlist = [d_train, d_valid]\n\nfinal_lgbm = lgb.train(params, train_set=d_train,valid_sets=watchlist,num_boost_round=2000,verbose_eval=200,early_stopping_rounds=100) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_pred_train=final_lgbm.predict(X_train_stacked)\nfinal_pred_cv=final_lgbm.predict(X_cv_stacked)\nfinal_pred_data_test=final_lgbm.predict(data_test_stacked)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Train rmsle: \"+str(rmsle(10 ** Y_train-1, 10 ** final_pred_train-1)))\nprint(\"CV rmsle: \"+str(rmsle(10 ** Y_cv-1, 10 ** final_pred_cv-1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame()\nsubmission['test_id'] = data_test['test_id']\n\nlightgbm_submission = submission.copy()\nlightgbm_submission['price'] = pd.DataFrame(10 ** final_pred_data_test - 1)\nlightgbm_submission.to_csv('stacked_submission_1.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}