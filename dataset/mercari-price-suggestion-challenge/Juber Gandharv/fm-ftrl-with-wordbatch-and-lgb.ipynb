{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport re\nfrom wordcloud import WordCloud, STOPWORDS \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.preprocessing import LabelBinarizer\nfrom scipy.sparse import vstack, hstack, csr_matrix\nfrom scipy import sparse\nimport gc\nimport psutil\nfrom nltk.corpus import stopwords\nimport string\nfrom sklearn.feature_selection.univariate_selection import SelectKBest, f_regression\nimport sys\nsys.path.insert(0, '../input/wordbatch/wordbatch/')\nimport wordbatch\nfrom wordbatch.extractors import WordBag\nfrom wordbatch.models import FM_FTRL\nfrom sklearn.linear_model import Ridge\nfrom sklearn.naive_bayes import MultinomialNB\nimport lightgbm as lgb\nfrom tqdm import tqdm\nfrom scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train = pd.read_csv('../input/mercari-price-suggestion-challenge/train.tsv', delimiter='\\t')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"####split data train test first\ny = np.log10(np.array(data_train['price'])+1)\nX = data_train.drop('price',axis=1)\n\nX_train,X_cv,Y_train,Y_cv = train_test_split(X, y, test_size=0.20, random_state=42)\n\ndel(X, y ,data_train)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.drop('train_id', axis=1, inplace=True)\nX_cv.drop('train_id', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def rmsle(y, y0):\n    return np.sqrt(np.mean(np.power(np.log1p(y)-np.log1p(y0), 2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://gist.github.com/sebleier/554280\n# we are removing the words from the stop words list: 'no', 'nor', 'not'\nstopwords= ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n            'won', \"won't\", 'wouldn', \"wouldn't\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nfrom nltk.corpus import stopwords\nstopwords = {x: 1 for x in stopwords.words('english')}\nnon_alphanums = re.compile(u'[^A-Za-z0-9]+')\nnon_alphanumpunct = re.compile(u'[^A-Za-z0-9\\.?!,; \\(\\)\\[\\]\\'\\\"\\$]+')\nRE_PUNCTUATION = '|'.join([re.escape(x) for x in string.punctuation])\n\n\ndef concat_categories(x):\n    return set(x.values)\n\ndef brandfinder(name, category):    \n    for brand in brands_sorted_by_size:\n        if brand in name and category in brand_names_categories[brand]:\n            return brand\n    return 'Unknown'\n\n\n# function to count repetition of first name\ndef create_dictionary(col_name,data_frame= X_train):\n    dictionary = dict(zip(data_frame[col_name],data_frame.groupby(col_name)[col_name].transform('count')))\n    return dictionary\n    \ndef transform_col(data_frame, col_name):\n    dictionary = create_dictionary(col_name)\n    transformed_column = []\n    for value in data_frame[col_name].values:\n        transformed_column.append(dictionary.get(value,1))\n    dictionary = None\n    del(dictionary)\n    gc.collect()\n    return transformed_column\n\n# function returns only first name(first_word)\ndef clean_name(x):\n    if len(x):\n        x = non_alphanums.sub(' ', x).split()\n        if len(x):\n            return x[0].lower()\n    return ''\n\ndef to_number(x):\n    try:\n        if not x.isdigit():\n            return 0\n        x = int(x)\n        if x > 100:\n            return 100\n        else:\n            return x\n    except:\n        return 0\n\ndef sum_numbers(desc):\n    if not isinstance(desc, str):\n        return 0\n    try:\n        return sum([to_number(s) for s in desc.split()])\n    except:\n        return 0\n    \ndef normalize_text(text):\n    return u\" \".join(\n        [x for x in [y for y in non_alphanums.sub(' ', text).lower().strip().split(\" \")] \\\n         if len(x) > 1 and x not in stopwords])\n\ndef decontracted(phrase):\n    # specific\n    try:\n        phrase = re.sub(r\"won't\", \"will not\", phrase)\n        phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n        # general\n        phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n        phrase = re.sub(r\"\\'re\", \" are\", phrase)\n        phrase = re.sub(r\"\\'s\", \" is\", phrase)\n        phrase = re.sub(r\"\\'d\", \" would\", phrase)\n        phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n        phrase = re.sub(r\"\\'t\", \" not\", phrase)\n        phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n        phrase = re.sub(r\"\\'m\", \" am\", phrase)\n        return phrase\n    except:\n        return 0\n\ndef cleaning_text(df):\n    from tqdm import tqdm\n    preprocessed_item_description = []\n    # tqdm is for printing the status bar\n    for sentance in tqdm(df['item_description'].values):\n        sent = decontracted(str(sentance))\n        sent = sent.replace('\\\\r', ' ')\n        sent = sent.replace('\\\\\"', ' ')\n        sent = sent.replace('\\\\n', ' ')\n        sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n        # https://gist.github.com/sebleier/554280\n        sent = ' '.join(e for e in sent.split() if e.lower() not in stopwords)\n        preprocessed_item_description.append(sent.lower().strip())\n    df = df.drop('item_description',axis=1)\n    df['item_description'] = preprocessed_item_description\n    preprocessed_item_description = None\n    del(preprocessed_item_description)\n    gc.collect()\n    return(df)\n\ndef fill_brand(df):\n    brand_names_categories = dict(df[df['brand_name'] != 'missing'][['brand_name','category_name']].astype('str')\\\n                              .groupby('brand_name').agg(concat_categories).reset_index().values.tolist())\n    brands_sorted_by_size = list(sorted(filter(lambda y: len(y) >= 3, \\\n                                           list(brand_names_categories.keys())), \\\n                                            key = lambda x: -len(x)))\n    \n    train_names_unknown_brands_train = df[df['brand_name'] == 'missing'][['name','category_name']].\\\n                            astype('str').values\n    train_estimated_brands_train = []\n    \n    \n    for name, category in tqdm(train_names_unknown_brands_train):\n        for brand in brands_sorted_by_size:\n            if brand in name and category in brand_names_categories[brand]:\n                brand_name = brand\n            else:\n                brand_name = 'missing'\n        train_estimated_brands_train.append(brand_name)\n        \n    df.loc[df['brand_name'] == 'missing', 'brand_name'] = train_estimated_brands_train\n    \n    brand_names_categories = None\n    brands_sorted_by_size = None\n    train_names_unknown_brands_train = None\n    train_estimated_brands_train = None\n    brand_name= None\n    del(brand_names_categories,brands_sorted_by_size,train_names_unknown_brands_train,train_estimated_brands_train,brand_name)\n    gc.collect()\n    return(df)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocessing(df):\n    #cleaning text\n    cleaning_text(df)\n    print(\"Text_cleaning Done\")\n    # filling missing values with brand_name as 'missing'\n    df['brand_name'] = df['brand_name'].fillna('missing')\n    df['brand_name'] = df['brand_name'].astype('category')\n    print(\"preprocessing for brand_name Done\")\n    \n    # filling missing values with category_name as missing/missing/missing or we can simply remove these rows\n    df['category_name'] = df['category_name'].fillna('missing/missing/missing')\n    df['category_name'] = df['category_name'].fillna('missing/missing/missing')\n    print(\"preprocessing for category_name Done\")\n    \n    df['item_description'] = df['item_description'].fillna('missing')\n    print(\"preprocessing for item_description Done\")\n    \n    df['category_name']= df['category_name'].str.split('/')\n    df['main_category'] = df['category_name'].str.get(0).replace('', 'missing').astype('category')\n    df['sub_category_1'] = df['category_name'].str.get(1).replace('', 'missing').astype('category')\n    df['sub_category_2'] = df['category_name'].str.get(2).replace('', 'missing').astype('category')\n    print(\"split main category in to 3 Done\")\n        \n    df['item_condition_id'] = df['item_condition_id'].astype('category')\n    df['item_condition_id'] = df['item_condition_id'].cat.add_categories(['missing']).fillna('missing')\n    print(\"preprocessing for item_condition_id Done\")\n    \n    df['shipping'] = df['shipping'].astype('category')\n    df['shipping'] = df['shipping'].cat.add_categories(['missing']).fillna('missing')\n    print(\"preprocessing for shipping Done\")\n    \n    df['item_description'].fillna('missing', inplace=True)\n    print(\"preprocessing for item_description Done\")    \n    fill_brand(df)\n    return(df)\n\ndef adding_new_features(df):\n    df['name_first'] = df['name'].apply(clean_name)\n    df['name_first_count'] = transform_col(data_frame = df, col_name = 'name_first' )\n    df['main_cat_count'] = transform_col(data_frame = df, col_name = 'main_category' )\n    df['sub_cat_1_count'] = transform_col(data_frame = df, col_name = 'sub_category_1' )\n    df['sub_cat_2_count'] = transform_col(data_frame = df, col_name = 'sub_category_2' )\n    df['brand_name_count'] = transform_col(data_frame = df, col_name = 'brand_name' )\n    df['DescriptionLower'] = df.item_description.str.count('[a-z]')\n    df['NameLower'] = df.name.str.count('[a-z]')\n    df['NameUpper'] = df.name.str.count('[A-Z]')\n    df['DescriptionUpper'] = df.item_description.str.count('[A-Z]')\n    df['name_len'] = df['name'].apply(lambda x: len(x))\n    df['des_len'] = df['item_description'].apply(lambda x: len(x))\n    df['name_desc_len_ratio'] = df['name_len']/df['des_len']\n    df['desc_word_count'] = df['item_description'].apply(lambda x: len(x.split()))\n    df['mean_des'] = df['item_description'].apply(lambda x: 0 if len(x) == 0 else float(len(x.split())) / len(x)) * 10\n    df['name_word_count'] = df['name'].apply(lambda x: len(x.split()))\n    df['mean_name'] = df['name'].apply(lambda x: 0 if len(x) == 0 else float(len(x.split())) / len(x))  * 10\n    df['desc_letters_per_word'] = df['des_len'] / df['desc_word_count']\n    df['name_letters_per_word'] = df['name_len'] / df['name_word_count']\n    df['NameLowerRatio'] = df['NameLower'] / df['name_len']\n    df['DescriptionLowerRatio'] = df['DescriptionLower'] / df['des_len']\n    df['NameUpperRatio'] = df['NameUpper'] / df['name_len']\n    df['DescriptionUpperRatio'] = df['DescriptionUpper'] / df['des_len']\n    df['NamePunctCount'] = df.name.str.count(RE_PUNCTUATION)\n    df['DescriptionPunctCount'] = df.item_description.str.count(RE_PUNCTUATION)\n    df['NamePunctCountRatio'] = df['NamePunctCount'] / df['name_word_count']\n    df['DescriptionPunctCountRatio'] = df['DescriptionPunctCount'] / df['desc_word_count']\n    df['NameDigitCount'] = df.name.str.count('[0-9]')\n    df['DescriptionDigitCount'] = df.item_description.str.count('[0-9]')\n    df['NameDigitCountRatio'] = df['NameDigitCount'] / df['name_word_count']\n    df['DescriptionDigitCountRatio'] = df['DescriptionDigitCount']/df['desc_word_count']\n    df['stopword_ratio_desc'] = df['item_description'].apply(lambda x: len([w for w in x.split() if w in stopwords])) / df['desc_word_count']\n    df['num_sum'] = df['item_description'].apply(sum_numbers) \n    df['weird_characters_desc'] = df['item_description'].str.count(non_alphanumpunct)\n    df['weird_characters_name'] = df['name'].str.count(non_alphanumpunct)\n    df['prices_count'] = df['item_description'].str.count('[rm]')\n    df['price_in_name'] = df['item_description'].str.contains('[rm]', regex=False).astype('int')\n    #df.drop('category_name', axis=1, inplace=True)\n    return(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocessing(X_train)\nadding_new_features(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check for the NAN values\nX_train.columns[X_train.isna().any()].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocessing(X_cv)\nadding_new_features(X_cv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_cv.columns[X_cv.isna().any()].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_cols = set(X_train.columns.values)\n\nbasic_cols = {'name', 'item_condition_id', 'brand_name',\n  'shipping', 'item_description', 'main_category',\n  'sub_category_1', 'sub_category_2', 'name_first'}\n\nnumeric_cols = total_cols - basic_cols\n\ncols_to_normalize = numeric_cols - {'price_in_name'}\n\ntext_cols = {'name', 'item_description'}\n\ncategorical_cols = {'item_condition_id','brand_name','shipping','main_category','sub_category_1','sub_category_2','name_first'}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Normalizing numerical columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import Normalizer\n\nnormalizer = Normalizer(copy=False)\nnormalizer.fit(X_train[list(cols_to_normalize)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\npickle.dump(normalizer, open(\"normalizer.pickle\", \"wb\"),protocol=4)\nnormalizer = None\ndel(normalizer)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def normalize_dataframe(df):\n    with open('normalizer.pickle',mode='rb') as model_f:\n        normalizer_load = pickle.load(model_f)\n    df[list(cols_to_normalize)]= normalizer_load.transform(df[list(cols_to_normalize)])\n    df[list({'item_condition_id','shipping'})] = df[list({'item_condition_id','shipping'})].astype('category')\n    normalizer_load = None \n    del(normalizer_load)\n    gc.collect()\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"normalize_dataframe(X_train)\nnormalize_dataframe(X_cv)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# item_desc at train time"},{"metadata":{"trusted":true},"cell_type":"code","source":"wb_desc = wordbatch.WordBatch(normalize_text, extractor=(WordBag, {\"hash_ngrams\": 2,\n                                                              \"hash_ngrams_weights\": [1.0, 1.0],\n                                                              \"hash_size\": 2 ** 28,\n                                                              \"norm\": \"l2\",\n                                                              \"tf\": 1.0,\n                                                              \"idf\": None}), procs=8)\nwb_desc.dictionary_freeze = True\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wb_desc.fit(X_train['item_description'])\nX_description_train_wb = wb_desc.transform(X_train['item_description'])\nX_description_cv_wb = wb_desc.fit_transform(X_cv['item_description'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mask_desc = np.where(X_description_train_wb.getnnz(axis=0) > 3)[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_description_train_wb = X_description_train_wb[:, mask_desc]\nX_description_cv_wb = X_description_cv_wb[:, mask_desc]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_description_train_wb.shape\nX_description_cv_wb.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_desc = Ridge(solver=\"sag\", fit_intercept=True, random_state=205, alpha=5)\nmodel_desc.fit(X_description_train_wb, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_train_1 = model_desc.predict(X_description_train_wb)\npred_cv_1 = model_desc.predict(X_description_cv_wb)\nprint(\"Train rmsle: \"+str(rmsle(10 ** Y_train-1, 10 ** pred_train_1-1)))\nprint(\"CV rmsle: \"+str(rmsle(10 ** Y_cv-1, 10 ** pred_cv_1-1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle \npickle.dump(mask_desc, open(\"mask_desc.pickle\", \"wb\"),protocol=4)\npickle.dump(wb_desc, open(\"wb_desc.pickle\", \"wb\"),protocol=4)\npickle.dump(model_desc, open(\"model_desc.pickle\", \"wb\"),protocol=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mask_desc = None\nwb_desc = None\nmodel_desc = None\ndel(mask_desc,wb_desc,model_desc)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# name at train time"},{"metadata":{"trusted":true},"cell_type":"code","source":"wb_name = wordbatch.WordBatch(normalize_text, extractor=(WordBag, {\"hash_ngrams\": 2,\n                                                              \"hash_ngrams_weights\": [1.5, 1.0],\n                                                              \"hash_size\": 2 ** 29,\n                                                              \"norm\": None,\n                                                              \"tf\": 'binary',\n                                                              \"idf\": None,\n                                                              }), procs=8)\nwb_name.dictionary_freeze = True\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wb_name.fit(X_train['name'])\nX_name_train_wb = wb_name.transform(X_train['name'])\nX_name_cv_wb = wb_name.fit_transform(X_cv['name'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mask_name = np.where(X_name_train_wb.getnnz(axis=0) > 3)[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_name_train_wb = X_name_train_wb[:, mask_name]\nX_name_cv_wb = X_name_cv_wb[:, mask_name]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_name = Ridge(solver=\"sag\", fit_intercept=True, random_state=205, alpha= 5)\nmodel_name.fit(X_name_train_wb, Y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_train_2 = model_name.predict(X_name_train_wb)\npred_cv_2 = model_name.predict(X_name_cv_wb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Train rmsle: \"+str(rmsle(10 ** Y_train-1, 10 ** pred_train_2-1)))\nprint(\"CV rmsle: \"+str(rmsle(10 ** Y_cv-1, 10 ** pred_cv_2-1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\n\npickle.dump(mask_name, open(\"mask_name.pickle\", \"wb\"),protocol=4)\npickle.dump(wb_name, open(\"wb_name.pickle\", \"wb\"),protocol=4)\npickle.dump(model_name, open(\"model_name.pickle\", \"wb\"),protocol=4)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mask_name = None\nwb_name = None\nmodel_name = None\ndel(mask_name,wb_name,model_name)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LB training"},{"metadata":{"trusted":true},"cell_type":"code","source":"lb_brand_name = LabelBinarizer(sparse_output=True)\n\nX_brand_train = lb_brand_name.fit_transform(X_train['brand_name'])\n\nX_brand_cv = lb_brand_name.transform(X_cv['brand_name'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lb_main_category = LabelBinarizer(sparse_output=True)\n\nX_main_cat_train = lb_main_category.fit_transform(X_train['main_category'])\n\nX_main_cat_cv = lb_main_category.transform(X_cv['main_category'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lb_sub_category_1 = LabelBinarizer(sparse_output=True)\n\nX_main_sub_cat_1_train = lb_sub_category_1.fit_transform(X_train['sub_category_1'])\n\nX_main_sub_cat_1_cv = lb_sub_category_1.transform(X_cv['sub_category_1'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lb_sub_category_2 = LabelBinarizer(sparse_output=True)\n\nX_main_sub_cat_2_train = lb_sub_category_2.fit_transform(X_train['sub_category_2'])\n\nX_main_sub_cat_2_cv = lb_sub_category_2.transform(X_cv['sub_category_2'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_dummies_train = csr_matrix(\n    pd.get_dummies(X_train[list(total_cols - (basic_cols))],\n                   sparse=True).values)\n\nX_dummies_train_1 = csr_matrix(\n    pd.get_dummies(X_train[list({'item_condition_id', 'shipping'})],\n                   sparse=True).values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_dummies_cv = csr_matrix(\n    pd.get_dummies(X_cv[list(total_cols - (basic_cols))],\n                   sparse=True).values)\n\nX_dummies_cv_1 = csr_matrix(\n    pd.get_dummies(X_cv[list({'item_condition_id', 'shipping'})],\n                   sparse=True).values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sparse_merge_train = hstack((X_name_train_wb , X_description_train_wb, X_brand_train, X_main_cat_train,\n                             X_main_sub_cat_1_train, X_main_sub_cat_2_train,X_dummies_train,X_dummies_train_1)).tocsr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sparse_merge_cv = hstack(( X_name_cv_wb,X_description_cv_wb,X_brand_cv,X_main_cat_cv,\n                             X_main_sub_cat_1_cv,X_main_sub_cat_2_cv,X_dummies_cv,X_dummies_cv_1)).tocsr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_dummies_train = None\nX_description_train_wb = None\nX_name_train_wb = None\nX_dummies_cv = None\nX_description_cv_wb = None\nX_name_cv_wb = None\ndel(X_dummies_train, X_description_train_wb,X_name_train_wb)\ndel(X_dummies_cv, X_description_cv_wb,X_name_cv_wb)\n\n#del(X_dummies_train, X_description_train, X_brand_train, X_main_cat_train,\\\n#                             X_main_sub_cat_1_train, X_main_sub_cat_2_train, X_name_train)\n#del(X_dummies_cv, X_description_cv, X_brand_cv, X_main_cat_cv,\\\n#                             X_main_sub_cat_1_cv, X_main_sub_cat_2_cv, X_name_cv)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sparse_merge_train.shape)\nprint(sparse_merge_cv.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\n\npickle.dump(lb_brand_name, open(\"lb_brand_name.pickle\", \"wb\"),protocol=4)\npickle.dump(lb_main_category, open(\"lb_main_category.pickle\", \"wb\"),protocol=4)\npickle.dump(lb_sub_category_1, open(\"lb_sub_category_1.pickle\", \"wb\"),protocol=4)\npickle.dump(lb_sub_category_2, open(\"lb_sub_category_2.pickle\", \"wb\"),protocol=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lb_brand_name = None\nlb_main_category = None\nlb_sub_category_1 = None\nlb_sub_category_2 = None\ndel(lb_brand_name,lb_main_category,lb_sub_category_1,lb_sub_category_2)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# FMFTRL training"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_FM_FTRL = FM_FTRL(alpha=0.035, beta=0.001, L1=0.00001, L2=0.15, D=sparse_merge_train.shape[1],\n                alpha_fm=0.05, L2_fm=0.0, init_fm=0.01,\n                D_fm=100, e_noise=0, iters=1, inv_link=\"identity\", threads=4)\nmodel_FM_FTRL.fit(sparse_merge_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_train_3 = model_FM_FTRL.predict(sparse_merge_train)\npred_cv_3 = model_FM_FTRL.predict(sparse_merge_cv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Train rmsle: \"+str(rmsle(10 ** Y_train-1, 10 ** pred_train_3-1)))\nprint(\"CV rmsle: \"+str(rmsle(10 ** Y_cv-1, 10 ** pred_cv_3-1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\n\npickle.dump(model_FM_FTRL, open(\"model_FM_FTRL.pickle\", \"wb\"),protocol=4)\nmodel_FM_FTRL = None\ndel(model_FM_FTRL)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sparse_merge_train.shape)\nprint(sparse_merge_cv.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# KBestSelect Train model"},{"metadata":{"trusted":true},"cell_type":"code","source":"fselect = SelectKBest(f_regression, k=48000)\ntrain_kbest_features = fselect.fit_transform(sparse_merge_train, Y_train)\ncv_kbest_features = fselect.transform(sparse_merge_cv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\npickle.dump(fselect, open(\"fselect.pickle\", \"wb\"),protocol=4)\nfselect = None\ndel(fselect)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_kbest_features.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sparse_merge_train= None\nsparse_merge_cv = None\ndel(sparse_merge_train,sparse_merge_cv)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TFIDF Desc Train"},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf_desc = TfidfVectorizer(max_features=500000,\n                     ngram_range=(1, 3),\n                     stop_words=None)\nX_desc_train_tfidf = tfidf_desc.fit_transform(X_train['item_description'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_desc_cv_tfidf = tfidf_desc.transform(X_cv['item_description'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pickle.dump(tfidf_desc, open(\"tfidf_desc.pickle\", \"wb\"),protocol=4)\ntfidf_desc = None\ndel(tfidf_desc)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TFIDF Name Train"},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf_name = TfidfVectorizer(max_features=250000,\n                     ngram_range=(1, 3),\n                     stop_words=None)\nX_name_train_tfidf = tfidf_name.fit_transform(X_train['name'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_name_cv_tfidf =tfidf_name.transform(X_cv['name'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pickle.dump(tfidf_name, open(\"tfidf_name.pickle\", \"wb\"),protocol=4)\ntfidf_name = None\ndel(tfidf_name)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# hstack features set_2"},{"metadata":{"trusted":true},"cell_type":"code","source":"sparse_merge_train_1 = hstack((X_name_train_tfidf , X_desc_train_tfidf, X_brand_train, X_main_cat_train,\n                             X_main_sub_cat_1_train, X_main_sub_cat_2_train,X_dummies_train_1)).tocsr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_dummies_train_1 = None\nX_brand_train = None\nX_main_cat_train = None\nX_main_sub_cat_1_train = None\nX_main_sub_cat_2_train = None\nX_name_train_tfidf = None\nX_desc_train_tfidf = None\ndel(X_dummies_train_1, X_brand_train, X_main_cat_train,\\\n                            X_main_sub_cat_1_train, X_main_sub_cat_2_train, X_name_train_tfidf,X_desc_train_tfidf)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sparse_merge_cv_1 = hstack((X_name_cv_tfidf , X_desc_cv_tfidf, X_brand_cv, X_main_cat_cv,\n                             X_main_sub_cat_1_cv, X_main_sub_cat_2_cv,X_dummies_cv_1)).tocsr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_dummies_cv_1 = None\nX_brand_cv = None\nX_main_cat_cv = None\nX_main_sub_cat_1_cv = None\nX_main_sub_cat_2_cv = None\nX_name_cv_tfidf = None\nX_desc_cv_tfidf = None\ndel(X_dummies_cv_1, X_brand_cv, X_main_cat_cv,\\\n                            X_main_sub_cat_1_cv, X_main_sub_cat_2_cv, X_name_cv_tfidf,X_desc_cv_tfidf)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ridge on Feature set-2"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_Ridge_set_2 = Ridge(solver=\"sag\", fit_intercept=True, random_state=205, alpha=5)\nmodel_Ridge_set_2.fit(sparse_merge_train_1, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_train_4 = model_Ridge_set_2.predict(sparse_merge_train_1)\npred_cv_4 =  model_Ridge_set_2.predict(sparse_merge_cv_1)\nprint(\"Train rmsle: \"+str(rmsle(10 ** Y_train-1, 10 ** pred_train_4-1)))\nprint(\"CV rmsle: \"+str(rmsle(10 ** Y_cv-1, 10 ** pred_cv_4-1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sparse_merge_train_1.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pickle.dump(model_Ridge_set_2, open(\"model_Ridge_set_2.pickle\", \"wb\"),protocol=4)\nmodel_Ridge_set_2 = None\ndel(model_Ridge_set_2)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# MultinomialNB on Feature set-2"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_MNB_set_2 = MultinomialNB(alpha=1.0, fit_prior=True)\nmodel_MNB_set_2.fit(sparse_merge_train_1, Y_train >= 4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_train_5 = model_MNB_set_2.predict(sparse_merge_train_1)\npred_cv_5 =  model_MNB_set_2.predict(sparse_merge_cv_1)\nprint(\"Train rmsle: \"+str(rmsle(10 ** Y_train-1, 10 ** pred_train_5-1)))\nprint(\"CV rmsle: \"+str(rmsle(10 ** Y_cv-1, 10 ** pred_cv_5-1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pickle.dump(model_MNB_set_2, open(\"model_MNB_set_2.pickle\", \"wb\"),protocol=4)\nmodel_MNB_set_2 = None\nsparse_merge_train_1 = None\nsparse_merge_cv_1 = None\ndel(model_MNB_set_2)\ndel(sparse_merge_train_1,sparse_merge_cv_1)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Adding prediction to data frame"},{"metadata":{},"cell_type":"markdown","source":"# Target Encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"f_cats = ['brand_name', 'main_category', 'sub_category_1', 'sub_category_2', 'name_first']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from category_encoders.target_encoder import TargetEncoder\n\ntargetencoder = TargetEncoder(min_samples_leaf=100, smoothing=10,cols=f_cats,return_df=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"targetencoder.fit(X_train[f_cats],Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_target_encode = targetencoder.transform(X_train[f_cats])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_cv_target_encode = targetencoder.transform(X_cv[f_cats])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_noise(series, noise_level):\n    return series * (1 + noise_level * np.random.randn(len(series)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_target_encode[:,0].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_target_encode[:,0] = add_noise(X_train_target_encode[:,0],noise_level=0.01)\nX_train_target_encode[:,1] = add_noise(X_train_target_encode[:,1],noise_level=0.01)\nX_train_target_encode[:,2] = add_noise(X_train_target_encode[:,2],noise_level=0.01)\nX_train_target_encode[:,3] = add_noise(X_train_target_encode[:,3],noise_level=0.01)\nX_train_target_encode[:,4] = add_noise(X_train_target_encode[:,4],noise_level=0.01)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_target_encode[0,:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pickle.dump(targetencoder, open(\"targetencoder.pickle\", \"wb\"),protocol=4)\ntargetencoder = None\ndel(targetencoder)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Stacking all features "},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = hstack((pred_train_1.reshape(-1,1),\\\n                         pred_train_2.reshape(-1,1),\\\n                         pred_train_3.reshape(-1,1),\\\n                         pred_train_4.reshape(-1,1),\\\n                         pred_train_5.reshape(-1,1),\\\n                         X_train_target_encode,\\\n                         train_kbest_features)).tocsr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_features = hstack((pred_cv_1.reshape(-1,1),\\\n                      pred_cv_2.reshape(-1,1),\\\n                      pred_cv_3.reshape(-1,1),\\\n                      pred_cv_4.reshape(-1,1),\\\n                      pred_cv_5.reshape(-1,1),\\\n                      X_cv_target_encode,\\\n                      cv_kbest_features)).tocsr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pickle.dump(train_features, open(\"train_features.pickle\", \"wb\"),protocol=4)\npickle.dump(cv_features, open(\"cv_features.pickle\", \"wb\"),protocol=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_train_1 = None\npred_train_2 = None\npred_train_3 = None\npred_train_4 = None\npred_train_5 = None\nX_train_target_encode = None\ntrain_kbest_features = None\n\npred_cv_1 = None\npred_cv_2 = None\npred_cv_3 = None\npred_cv_4 = None\npred_cv_5 = None\nX_cv_target_encode = None\ncv_kbest_features = None\n\n\ndel(pred_train_1,\\\n    pred_train_2,\\\n    pred_train_3,\\\n    pred_train_4,\\\n    pred_train_5,\\\n    X_train_target_encode,\\\n    train_kbest_features)\ndel(pred_cv_1,\\\n    pred_cv_2,\\\n    pred_cv_3,\\\n    pred_cv_4,\\\n    pred_cv_5,\\\n    X_cv_target_encode,\\\n    cv_kbest_features)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Final LGB model"},{"metadata":{"trusted":true},"cell_type":"code","source":"d_train = lgb.Dataset(train_features, label=Y_train)\nd_valid = lgb.Dataset(cv_features, label=Y_cv)\nwatchlist = [d_train, d_valid]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n         'colsample_bytree': 0.42799939792816927,\n          'max_depth': 8,\n          'min_child_samples': 370,\n          'min_child_weight': 0.01,\n          'num_leaves': 29,\n          'reg_lambda': 5,\n          'subsample': 0.6739316550896339,\n          'learning_rate':0.1,\n          'reg_alpha' :0.5,\n          'boosting_type': 'gbdt',\n          'objective' : 'regression',\n          'metric' : 'RMSE',\n          'verbosity': -1,\n          'lambda_l1': 10,\n         'lambda_l2': 10\n         }\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_lgb_final = lgb.train(params,\n                  train_set=d_train,\n                  num_boost_round=3000,\n                  valid_sets=watchlist,\n                  verbose_eval=200,early_stopping_rounds=100)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_train_6 = model_lgb_final.predict(train_features)\npred_cv_6 = model_lgb_final.predict(cv_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Train rmsle: \"+str(rmsle(10 ** Y_train-1, 10 ** pred_train_6-1)))\nprint(\"CV rmsle: \"+str(rmsle(10 ** Y_cv-1, 10 ** pred_cv_6-1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pickle.dump(model_lgb_final, open(\"model_lgb_final.pickle\", \"wb\"),protocol=4)\nmodel_lgb_final = None\ndel(model_lgb_final)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features= None\ncv_features = None\nd_train = None\nd_valid = None\nwatchlist = None\ndel(train_features,cv_features,d_train,d_valid,watchlist)\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = None\nX_cv = None\nY_train = None\nY_test = None\ndel(X_train,X_cv,Y_train,Y_cv)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prediction on test_stg2"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_id = []\nprediction = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict_final(df):\n    test_id.extend(list(df['test_id']))\n    df.drop('test_id', axis=1, inplace=True)\n    preprocessing(df)\n    adding_new_features(df)\n    normalize_dataframe(df)\n    \n######### Loading wordbag_desc_model & model_1 pred ###########    \n    with open('mask_desc.pickle',mode='rb') as model_f:\n        mask_desc = pickle.load(model_f)\n    with open('wb_desc.pickle',mode='rb') as model_f:\n        wb_desc= pickle.load(model_f)\n    with open('model_desc.pickle',mode='rb') as model_f:\n        model_desc = pickle.load(model_f)\n    X_description_test_wb = wb_desc.transform(df['item_description'])\n    X_description_test_wb = X_description_test_wb[:, mask_desc]\n    print(X_description_test_wb.shape)\n    \n    pred_test_1 = model_desc.predict(X_description_test_wb)\n    mask_desc = None\n    wb_desc = None\n    model_desc = None\n    del(mask_desc,wb_desc,model_desc)\n    gc.collect()\n\n######### Loading wordbag_name_model & model_2 pred ###########    \n    with open('mask_name.pickle',mode='rb') as model_f:\n        mask_name = pickle.load(model_f)\n    with open('wb_name.pickle',mode='rb') as model_f:\n        wb_name= pickle.load(model_f)\n    with open('model_name.pickle',mode='rb') as model_f:\n        model_name = pickle.load(model_f)\n    X_name_test_wb = wb_name.transform(df['name'])\n    X_name_test_wb = X_name_test_wb[:, mask_name]\n    \n    print(X_name_test_wb.shape)\n    pred_test_2 = model_name.predict(X_name_test_wb)\n    mask_name = None\n    wb_name = None\n    model_name = None\n    del(mask_name,wb_name,model_name)\n    gc.collect()\n\n###################### Lb and Dummies ##########################\n    with open('lb_brand_name.pickle',mode='rb') as model_f:\n        lb_brand_name = pickle.load(model_f)\n    with open('lb_main_category.pickle',mode='rb') as model_f:\n        lb_main_category = pickle.load(model_f)\n    with open('lb_sub_category_1.pickle',mode='rb') as model_f:\n        lb_sub_category_1 = pickle.load(model_f)\n    with open('lb_sub_category_2.pickle',mode='rb') as model_f:\n        lb_sub_category_2 = pickle.load(model_f)\n        \n    X_brand_test = lb_brand_name.transform(df['brand_name'])\n    X_main_cat_test = lb_main_category.transform(df['main_category'])\n    X_main_sub_cat_1_test = lb_sub_category_1.transform(df['sub_category_1'])\n    X_main_sub_cat_2_test = lb_sub_category_2.transform(df['sub_category_2'])\n    \n    X_dummies_test = csr_matrix(\n        pd.get_dummies(df[list(total_cols - (basic_cols))],\n                   sparse=True).values)\n\n    X_dummies_test_1 = csr_matrix(\n        pd.get_dummies(df[list({'item_condition_id', 'shipping'})],\n                   sparse=True).values)\n    \n##################### sparse matrix feature_set_1 #############################\n    sparse_merge_test = hstack((X_name_test_wb , X_description_test_wb, X_brand_test, X_main_cat_test,\n                             X_main_sub_cat_1_test, X_main_sub_cat_2_test,X_dummies_test,X_dummies_test_1)).tocsr()\n    X_dummies_test = None \n    X_description_test_wb = None\n    X_name_test_wb = None\n    del(X_dummies_test, X_description_test_wb,X_name_test_wb)\n    gc.collect()\n    print(sparse_merge_test.shape)\n\n############################### FMFTRL ###################################\n    with open('model_FM_FTRL.pickle',mode='rb') as model_f:\n        model_FM_FTRL = pickle.load(model_f)\n    pred_test_3 = model_FM_FTRL.predict(sparse_merge_test)\n    model_FM_FTRL = None\n    del(model_FM_FTRL)\n    gc.collect()\n######################### Kbest Select ##################################\n    with open('fselect.pickle',mode='rb') as model_f:\n        fselect = pickle.load(model_f)\n    test_kbest_features = fselect.transform(sparse_merge_test)\n    fselect = None\n    print(test_kbest_features.shape)\n    del(fselect)\n    gc.collect()\n    sparse_merge_train= None\n    sparse_merge_cv = None\n    del(sparse_merge_train,sparse_merge_cv)\n    gc.collect()\n    \n########################## TFIDF Desc Train ############################\n    with open('tfidf_desc.pickle',mode='rb') as model_f:\n        tfidf_desc = pickle.load(model_f)\n    X_desc_test_tfidf = tfidf_desc.transform(df['item_description'])\n    tfidf_desc = None\n    del(tfidf_desc)\n    gc.collect()\n    \n########################## TFIDF Name Train ############################\n    with open('tfidf_name.pickle',mode='rb') as model_f:\n        tfidf_name = pickle.load(model_f)\n    X_name_test_tfidf = tfidf_name.transform(df['name'])\n    tfidf_name = None\n    del(tfidf_name)\n    gc.collect()\n\n##################### sparse matrix feature_set_2 #############################\n    sparse_merge_test_1 = hstack((X_name_test_tfidf , X_desc_test_tfidf, X_brand_test, X_main_cat_test,\n                             X_main_sub_cat_1_test, X_main_sub_cat_2_test,X_dummies_test_1)).tocsr()\n    X_dummies_test_1 = None\n    X_brand_test = None\n    X_main_cat_test = None\n    X_main_sub_cat_1_test = None\n    X_main_sub_cat_2_test = None\n    X_name_test_tfidf = None\n    X_desc_test_tfidf = None\n    del(X_dummies_test_1, X_brand_test, X_main_cat_test,\\\n                            X_main_sub_cat_1_test, X_main_sub_cat_2_test, X_name_test_tfidf,X_desc_test_tfidf)\n    gc.collect()\n\n######################### Ridge model on feature_set_2 #############################\n    with open('model_Ridge_set_2.pickle',mode='rb') as model_f:\n        model_Ridge_set_2 = pickle.load(model_f)\n    pred_test_4 = model_Ridge_set_2.predict(sparse_merge_test_1)\n    model_Ridge_set_2 = None\n    del(model_Ridge_set_2)\n    gc.collect()\n\n########################## MNB model on feature_set_2 ############################\n    with open('model_MNB_set_2.pickle',mode='rb') as model_f:\n        model_MNB_set_2 = pickle.load(model_f)\n    pred_test_5 = model_MNB_set_2.predict(sparse_merge_test_1)\n    model_MNB_set_2 = None\n    sparse_merge_train_1 = None\n    sparse_merge_cv_1 = None\n    del(model_MNB_set_2)\n    del(sparse_merge_train_1,sparse_merge_cv_1)\n    gc.collect()\n\n############################ target_encoding ##################################\n    with open('targetencoder.pickle',mode='rb') as model_f:\n        targetencoder = pickle.load(model_f)\n    f_cats = ['brand_name', 'main_category', 'sub_category_1', 'sub_category_2', 'name_first']\n    X_test_target_encode = targetencoder.transform(df[f_cats])\n    targetencoder = None\n    del(targetencoder)\n    gc.collect()\n    \n######################### Final sparse matrix #################################\n\n    test_features = hstack((pred_test_1.reshape(-1,1),\\\n                         pred_test_2.reshape(-1,1),\\\n                         pred_test_3.reshape(-1,1),\\\n                         pred_test_4.reshape(-1,1),\\\n                         pred_test_5.reshape(-1,1),\\\n                         X_test_target_encode,\\\n                         test_kbest_features)).tocsr()\n    \n    pred_test_1 = None\n    pred_test_2 = None\n    pred_test_3 = None\n    pred_test_4 = None\n    pred_test_5 = None\n    X_test_target_encode = None\n    test_kbest_features = None\n\n\n    del(pred_test_1,\\\n        pred_test_2,\\\n        pred_test_3,\\\n        pred_test_4,\\\n        pred_test_5,\\\n        X_test_target_encode,\\\n        test_kbest_features)\n    gc.collect()\n\n########################## Final lgbm prediction ##################\n    with open('model_lgb_final.pickle',mode='rb') as model_f:\n        model_lgb_final = pickle.load(model_f)\n    #pred_test_6 = model_lgb_final.predict(test_features)\n    prediction.extend(list(model_lgb_final.predict(test_features)))\n    #prediction = prediction + list(model_lgb_final.predict(test_features))\n########################## del everything ######################\n    train_features = None\n    cv_features = None \n    d_train = None\n    d_valid = None\n    watchlist = None\n    del(train_features,cv_features,d_train,d_valid,watchlist)\n    gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"chunksize = 10 ** 6\nfor chunk in pd.read_csv('../input/mercari-price-suggestion-challenge/test_stg2.tsv', delimiter='\\t',chunksize=chunksize):\n    predict_final(chunk)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame()\nsubmission['test_id'] = np.asarray(test_id)\nsubmission['price'] = (10 ** np.asarray(prediction) - 1)\nsubmission.to_csv('stacked_submission_1.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}