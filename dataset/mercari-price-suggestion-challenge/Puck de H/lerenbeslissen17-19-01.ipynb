{"nbformat_minor":1,"nbformat":4,"cells":[{"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.model_selection import train_test_split\nfrom scipy.sparse import hstack\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom scipy.sparse import csr_matrix\nimport math\nimport gc\nimport time\n#------------------------------------------------------------------------------------------------#\n\nstart_time = time.time()\n\n# Read in the datafiles\ndf_train = pd.read_csv('../input/train.tsv', sep='\\t') \ndf_test = pd.read_csv('../input/test.tsv', sep='\\t')\n\n#Mercari only lets users sell products with a price higher than 3.0, so we only want the \n#data to contain products with a price of 3.0 or higher.\ndf_train[df_train.price >= 3.0]\n\n# Concatenate the training and test data but save the length of the training data for later use\ntrain_rows = len(df_train)\nall_data = pd.concat([df_train, df_test])\nprint(\"[{}] Data concatenated.\".format(time.time() - start_time))\n\n# Function for filling the NaN values in the dataframe\ndef transform_values(data):\n    data.fillna(value='missing', inplace = True)\n    return data\n\nall_data = transform_values(all_data)\n\n# Split the categories into a maincategory, subcategory 1 and subcategory 2\ndef transform_category_name(category_name):\n    # If there is a category split it\n    try:\n        main, sub1, sub2= category_name.split('/')\n        return main, sub1, sub2\n    # Else return \"missing\" for each (sub-)category\n    except:\n        return 'missing', 'missing', 'missing'\n\nall_data['category_main'], all_data['category_sub1'], all_data['category_sub2'] = zip(*all_data['category_name'].apply(transform_category_name))\n# Delete all the entries without an item description\nall_data[all_data.item_description != 'missing']\nprint(\"[{}] Data transformed.\".format(time.time() - start_time))\n\n# Create a vector for all the names with count vectorizer\nname_cv = CountVectorizer(ngram_range=(1,2), min_df=7, stop_words='english')\nX_name = name_cv.fit_transform(all_data['name'])\nprint(\"[{}] Name vector created.\".format(time.time() - start_time))\n\n# Create seperate vectors for all the (sub-)categories using a CountVectorizer\ncat_cv = CountVectorizer()\nX_catmain = cat_cv.fit_transform(all_data['category_main'],)\nX_catsub1 = cat_cv.fit_transform(all_data['category_sub1'])\nX_catsub2 = cat_cv.fit_transform(all_data['category_sub2'])\nprint(\"[{}] Category vectors created.\".format(time.time() - start_time))\n\n# Function to get length of item descriptions\ndef descrLength(description):\n    try:\n        if description == \"missing\":\n            return 0\n        else:\n            words = [w for w in description.split(\" \")]\n            return len(words)\n    except:\n        return 0\n      \n# Create an extra column in the dataframe that keeps track of the length of the descriptions\nall_data['descr_length'] = all_data['item_description'].apply(lambda x: descrLength(x))\nall_data['descr_length'] = all_data['descr_length'].astype(str)\nX_length = cat_cv.fit_transform(all_data['descr_length'])\nprint(\"[{}] Description length vector created.\".format(time.time() - start_time))\n\n# Create a vector with tf-idf values of the words in the descriptions.\nvectorizer = TfidfVectorizer(ngram_range=(1,2), max_features=90000, strip_accents=\"ascii\",\n                             stop_words='english')\nX_description = vectorizer.fit_transform(all_data['item_description'])\nprint(\"[{}] Description vector created.\".format(time.time() - start_time))\n       \n# Make a vector of the brands using a label binarizer.\nlb = LabelBinarizer(sparse_output=True)\nX_brand = lb.fit_transform(all_data['brand_name'])\nprint(\"[{}] Brand vector created.\".format(time.time() - start_time))\n\n# Use get dummies to create a sparse csr matrix of the shipping and item condition values.\nX_dummies = csr_matrix(pd.get_dummies(all_data[['item_condition_id', 'shipping']],\n                                          sparse=True).values)\nprint(\"[{}] Shipping and condition vector created.\".format(time.time() - start_time))\n\ntrain_X = hstack((X_name[:train_rows], X_catmain[:train_rows], X_catsub1[:train_rows], \n                X_catsub2[:train_rows], X_brand[:train_rows], X_length[:train_rows], X_description[:train_rows],\n                X_dummies[:train_rows]))\nprint('[{}] Training data stacked'.format(time.time() - start_time))\n\ntest_X = hstack((X_name[train_rows:], X_catmain[train_rows:], X_catsub1[train_rows:], \n                X_catsub2[train_rows:], X_brand[train_rows:], X_length[train_rows:], X_description[train_rows:],\n                X_dummies[train_rows:]))\nprint('[{}] Test data stacked.'.format(time.time() - start_time))\n\nY_train = np.log1p(df_train['price']) #log1p because is nice\nprint(\"[{}] Data processing and feature extraction complete.\".format(time.time() - start_time))","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"e191eed6-6285-4c36-a41c-b76cbd0b427a","_uuid":"c6a5f2de3a77de4639000155a6f1e5fcc3d50e0f","collapsed":true,"scrolled":false},"cell_type":"code"},{"source":"from sklearn.linear_model import Ridge\nfrom sklearn.model_selection import train_test_split\n\nridge_time = time.time()\n\n# Split the training data in training and validation sets to use cross validation\ndata_train_X, data_val_X, data_train_Y, data_val_Y = train_test_split(train_X, Y_train)\nprint('[{}] Data splitted.'.format(time.time() - ridge_time))\n\n# Training the Ridge algorithm on the trainingsset\nclf1 = Ridge(alpha=5.3, fit_intercept=True, normalize=False, \n      copy_X=True, max_iter=None, tol=0.01, solver='auto', random_state=None)\nclf1.fit(train_X, Y_train)\nprint(\"[{}] Data 1 fitted.\".format(time.time() - ridge_time))\n\nclf2 = Ridge(alpha=.75, fit_intercept=True, normalize=False, \n      copy_X=True, max_iter=None, tol=0.01, solver='auto', random_state=100)\nclf2.fit(train_X, Y_train)\nprint(\"[{}] Data 2 fitted.\".format(time.time() - ridge_time))\n\n# Use the Ridge model to predict the Y values for the validation set\n#Y_pred1 = clf1.predict(data_val_X)\n#Y_pred2 = clf2.predict(data_val_X)\n\n#A function to calculate Root Mean Squared Logarithmic Error (RMSLE)\n'''def rmsle(y, y0):\n     assert len(y) == len(y0)\n     return np.sqrt(np.mean(np.power(np.log1p(y)-np.log1p(y0), 2)))\n\nprint(\"Error on 1:\", rmsle(np.expm1(Y_pred1), np.expm1(data_val_Y)))\nprint(\"Error on 2:\", rmsle(np.expm1(Y_pred2), np.expm1(data_val_Y)))\n'''","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"5e6e4492-dacd-4ec6-84b5-1df914c32ce9","_uuid":"5a7354828d06f7669a46e5247d06f65c896a1bdb","collapsed":true},"cell_type":"code"},{"source":"5.3 = 0.4512697\n\n0.53 = 0.4590716\n\nwith everything = 0.4504744\n\nwithout brand = 0.45662041 --> slechter\n\nwithout category = 0.4595244 --> slechter\n\nwithout shipping = 0.457 --> slechter\n\nwithout item condition = \n\nprice higher than 3.0 = 0.4488579\n\nmissing ipv no description = 0.4500646/0.4504321/0.4496406 --> geen verbetering\n\ngewoon no description = 0.4507507/0.4493476/0.4503064\n\nwith description length = 0.4503645/0.44881417/0.4481222 --> beetje verbetering\n\nwith normal brands = 0.4499219\n\nwith extra brands from name = 0.4492130/0.450550142/0.4502868\n","metadata":{"_cell_guid":"046339cc-9038-4778-896e-97e3ea8b82e2","_uuid":"a22f9b54e084980de5304648bdd6aa2cc2d83c7c"},"cell_type":"markdown"},{"source":"from sklearn.model_selection import train_test_split\nimport lightgbm as lgb\n\nlgb_time = time.time()\n\n# Split the training data in training and validation sets to use cross validation\ndata_train_X, data_val_X, data_train_Y, data_val_Y = train_test_split(train_X, Y_train)\nprint('[{}] Data splitted.'.format(time.time() - lgb_time))\n\n# Create the datasets to use for LGBM\ndataset_train = lgb.Dataset(data_train_X, label=data_train_Y)\ndataset_val = lgb.Dataset(data_val_X, label=data_val_Y)\n# The list of data that LGBM uses to evaluate its results to avoid overfitting\nwatchlist = [dataset_train, dataset_val]\n\n# Parameters for the LGBM model.\nparams = {\n        'learning_rate': 0.35,\n        'application': 'regression',\n        'max_depth': 6,\n        'num_leaves': 60,\n        'verbosity': -1,\n        'metric': 'RMSE',\n        'data_random_seed': 1,\n        'bagging_fraction': 0.5,\n        'nthread': 4\n    }\n\n# Train the LGBM model on the trainingdata\nlgb_model = lgb.train(params, train_set=dataset_train, num_boost_round=7500, valid_sets=watchlist, \n            early_stopping_rounds=1000, verbose_eval=500) \nprint(\"[{}] Model completed.\".format(time.time() - lgb_time))\n\n# Predict the values of Y for the validation set with the created model\n#lgb_pred =lgb_model.predict(data_val_X)\nprint(\"[{}] Prediction completed.\".format(time.time() - lgb_time))\n\n#print(rmsle(np.expm1(lgb_pred), np.expm1(data_val_Y)))\n\n","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"65877f2c-eed5-4a15-8412-98210e003bf5","_uuid":"aa663c47def33e7aaea85a23e4f7ef3c7a15d4fc","collapsed":true,"scrolled":false},"cell_type":"code"},{"source":"final_time = time.time()\n\nfinal_ridge_pred1 = clf1.predict(test_X)\nprint(\"[{}] First Ridge prediction completed.\".format(time.time() - final_time))\nfinal_ridge_pred2 = clf2.predict(test_X)\nprint(\"[{}] Second Ridge prediction completed.\".format(time.time() - final_time))\nfinal_lgb_pred = lgb_model.predict(test_X)\nprint(\"[{}] Lgb prediction completed.\".format(time.time() - final_time))\n\nfinal_pred = 0.25*final_ridge_pred1 + 0.25*final_ridge_pred2 + 0.5*final_lgb_pred\n\nsubmission = pd.DataFrame(data=df_test[['test_id']])\nsubmission['price'] = np.expm1(final_pred)\nsubmission.to_csv(\"submission_ridge_lgbm.csv\", index=False)","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"a784e465-656f-4943-b5f5-1e1eb16ca663","_uuid":"e3c6316e6aaba7c4f697de040a274b72e9ce4135","collapsed":true},"cell_type":"code"},{"source":"'''\n# Try and find brand names in product names\nall_brands = set(all_data['brand_name'])\ndef brands_in_name(entry):\n    name = entry[0]\n    brand = entry[1]\n    new_brand = ''\n    split_name = name.split(' ')\n    if brand == 'missing':\n        for words in split_name:\n            if words in all_brands:\n                new_brand += words\n        if new_brand == '':\n            return 'missing'\n        else:\n            return new_brand\n    return brand\n\nall_data['brand_name'] = all_data[['name', 'brand_name']].apply(brands_in_name, axis = 1) \nprint('new brands extracted')\n'''","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"d4e750d1-0d46-4c43-a2c9-50003afc3124","_uuid":"20865ff665fabb1d616ee77e34fdf19c91c3ae6d","collapsed":true},"cell_type":"code"},{"source":"* alpha 5 = 0.45512\n* alpha 5.1 = 0.45513\n* alpha 5.2 = 0.45511\n* alpha 5.3 = 0.45510\n* alpha 5.4 = 0.45512\n* alpha 5.5 = 0.45511\n* alpha 0.5 = 0.461\n* alpha 0.6 = 0.460\n* alpha 15 = 0.458\n* alpha 100 = 0.481","metadata":{"_cell_guid":"da1f02c0-1850-42f2-9989-8dc1411f0e1f","_uuid":"9c6e28d9a2af656d12301919dd00c94bf36d9a00","collapsed":true},"cell_type":"markdown"}],"metadata":{"language_info":{"file_extension":".py","name":"python","version":"3.6.4","pygments_lexer":"ipython3","codemirror_mode":{"version":3,"name":"ipython"},"mimetype":"text/x-python","nbconvert_exporter":"python"},"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"}}}