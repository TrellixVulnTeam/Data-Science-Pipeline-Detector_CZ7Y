{"cells":[{"metadata":{"trusted":true,"_uuid":"1b1e93d211f7ce442460efa0274ce6326a3c78ea","_kg_hide-input":false,"_kg_hide-output":false},"cell_type":"markdown","source":"This Kernal is Cloned one of Mercari Interactive EDA + Topic Modelling by ThyKhueLy.<br />\nSo, I'd like to give BIG APPALUSE to him, Thanks.\n> https://www.kaggle.com/thykhuely/mercari-interactive-eda-topic-modelling"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"# Introduction <br />\nThis is an initial Explanatory Data Analysis for the Mercari Price Suggestion Challenge with matplotlib. <br />\n=> 이 커널은 해당 컴피티션의 가장 기초적인 EDA분석입니다 <br />\nbokeh and Plot.ly - a visualization tool that creates beautiful interactive plots and dashboards.<br />\n=> boken 과 Plot.ly는 시각화 툴을 좀더 상호작용을 높여주는 툴입니다. <br />\nThe competition is hosted by Mercari, the biggest Japanese community-powered shopping app with the main objective to predict an accurate price that Mercari should suggest to its sellers, given the item's information. <br />\n=> 해당 컴피티션은 일본에서 가장 큰 쇼핑앱이며 우리의 목표는 회사가 판매자들에게 아이템의 정보를 받았을 때 그 상품의 적절한 가격을 제안해 주는 것입니다. <br />\n\n## Update: The abundant amount of food from my family's Thanksgiving dinner has really energized me to continue working on this model. <br />\nI decided to dive deeper into the NLP analysis and found an amazing tutorial by Ahmed BESBES. <br />\n=> Ahmed BESBES 의 NLP분석을 참고하여 추가작업을 했습니다. <br />\nThe framework below is based on his source code. <br />\n=> 추가된 것들은 그의 코드를 기반으로 작성하였습니다. <br />\nIt provides guidance on pre-processing documents and machine learning techniques (K-means and LDA) to clustering topics. So that this kernel will be divided into 2 parts: <br />\n=> 그의 코드는 문서의 전처리와 clustering topic들에 머신러닝 기술들(K-means 와 LDA)의 가이드가 될 것입니다. 따라서 이 커널은 크게 두가지로 나뉩니다 <br />\n\n### Explanatory Data Analysis <br />\n\n### Text Processing <br />\n#### 2.1. Tokenizing and tf-idf algorithm <br />\n#### 2.2. K-means Clustering <br />\n#### 2.3. Latent Dirichlet Allocation (LDA) / Topic Modelling <br />"},{"metadata":{"trusted":true,"_uuid":"6e1b862ee59cbca4ee2fc7e606fb77a3c5414ac0"},"cell_type":"code","source":"import nltk\nimport string\nimport re\nimport numpy as np\nimport pandas as pd\nimport pickle\n#import lda\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"white\")\n\nfrom nltk.stem.porter import *\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction import stop_words\n\nfrom collections import Counter\nfrom wordcloud import WordCloud\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n%matplotlib inline\n\nimport bokeh.plotting as bp\nfrom bokeh.models import HoverTool, BoxSelectTool\nfrom bokeh.models import ColumnDataSource\nfrom bokeh.plotting import figure, show, output_notebook\n#from bokeh.transform import factor_cmap\n\nimport warnings\nwarnings.filterwarnings('ignore')\nimport logging\nlogging.getLogger(\"lda\").setLevel(logging.WARNING)\n\nPATH = \"../input/\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3a1c9dcb3af3342dbc069a09eee580ef9a7f6705"},"cell_type":"code","source":"len(stopwords.words('english'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"09ef2f970fb6dfcc4a482198558efcd622eca41b"},"cell_type":"code","source":"len(stop_words.ENGLISH_STOP_WORDS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"60cd62e168d2b4c2a2501966e2a21c47cb69abf7"},"cell_type":"code","source":"train = pd.read_csv(f\"{PATH}train.tsv\",sep='\\t')\ntest = pd.read_csv(f\"{PATH}test.tsv\",sep='\\t')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"509d1cd24534d8da831a708c05670d23257a9761"},"cell_type":"markdown","source":"우리가 사용하게 될 데이터셋의 생김새는 아래와 같다."},{"metadata":{"trusted":true,"_uuid":"dcdc865d97ecf97e58631787bfd426f2624af84e"},"cell_type":"code","source":"print(train.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"56f683ea568ed44b8f8e742b7ad4c88dc58b2f01"},"cell_type":"code","source":"frac = 0.1\ntrain = train.sample(frac=frac,random_state=200)\ntest = test.sample(frac=frac,random_state=200)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"84be4a9ec468e14da7dd6c46ea50d86f101d32b1"},"cell_type":"code","source":"print(train.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e0450e0258438d3b3295d7af8f493f42e32ea583"},"cell_type":"markdown","source":"또한 훈련데이터셋에서 데이터프레임 각각의 컬럼들의 타입은 아래와 같다."},{"metadata":{"_uuid":"d0d8e8a138e04d496b05bb01ad522467bbad08e2"},"cell_type":"markdown","source":"### The files consist of a list of product listings. These files are tab-delimited. <br />\n\n* **train_id or test_id - the id of the listing <br />**\n=> 목록의 고유한 id <br />\n* **name - the title of the listing. Note that we have cleaned the data to remove text that look like prices (e.g. $20) to avoid leakage. These removed prices are represented as [rm] <br />**\n=> 목록에 올라온 제목. 가격처럼 생긴 텍스트는 데이터에서 지우려고 하고 있습니다. 이렇게 지워진 가격들은 [rm]이라고 표시되어 있습니다.<br />\n\n* **item_condition_id - the condition of the items provided by the seller <br />**\n=> 판매자에게 제공받은 물품의 상태를 나타냄<br />\n* **category_name - category of the listing <br />**\n=> 목록의 카테고리<br />\n* **brand_name <br />**\n=> 브랜드 이름 <br />\n* **price - the price that the item was sold for. This is the target variable that you will predict. The unit is USD. This column doesn't exist in test.tsv since that is what you will predict. <br />**\n=> 아이템이 팔린 가격. 해당 컬럼이 우리가 예측하고자하는 타겟 컬럼이다. 단위는 미국 달러이며. 해당 컬럼은 test.tsv 에는 따로 기재되어 있지 않다,\n* **shipping - 1 if shipping fee is paid by seller and 0 by buyer <br />**\n=> 1이면 판매자가 운임을 냈으며 0이면 구매자가 처리한다<br />\n* **item_description - the full description of the item. Note that we have cleaned the data to remove text that look like prices (e.g. $20) to avoid leakage. These removed prices are represented as [rm] <br />**\n=> 물품의 상세내용. 이 데이터 또한 name에서 처럼 가격을 지우려고 했으며 지워진 물품에 대해서는 [rm] 표기되어 있습니다. <br />\n\nPlease note that in stage 1, all the test data will be calculated on the public leaderboard. In stage 2, we will swap the test.tsv file to the complete test dataset that includes the private leaderboard data."},{"metadata":{"trusted":true,"_uuid":"a1c9a0fb7284801f892759cf1854f0d05ed05316"},"cell_type":"code","source":"train.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3002f5b156fbba4d4c77ba2dd82d12ff4191046f"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0d17f2d578a2e527b14710d20e1f72e58e93ca07"},"cell_type":"markdown","source":"## Target Variable: Price <br />\nThe next standard check is with our response or target variables, which in this case is the price we are suggesting to the Mercari's marketplace sellers. <br />\n=> 다음으로 확인해봐야 할 것은 Price컬럼입니다. <br />\nThe median price of all the items in the training is about \\$267 but given the existence of some extreme values of over \\$100 and the maximum at \\$2,009, the distribution of the variables is heavily skewed to the left. <br />\n=> 가격의 중앙값은 267달러이지만, 어떤 가격은 100달러를 웃돌거나 최대로는 2009달러 짜리 물품도 있습니다. 해당 가격의 분포는 굉장히 좌측으로 치우쳐 있습니다. <br />\nSo let's make log-transformation on the price (we added +1 to the value before the transformation to avoid zero and negative values). <br />\n=> 따라서 가격에 대해서 로그화를 진행해 봅시다. <br />"},{"metadata":{"trusted":true,"_uuid":"272e98895b49db7b7a258e4fb5e5a651c3bef82b"},"cell_type":"code","source":"train['price'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"92599d174751757d1c08fc93cc08dfe2b18df2db"},"cell_type":"code","source":"fig,ax = plt.subplots(1,2,figsize=[10,5])\nax = plt.subplot(1,2,1)\nsns.distplot(train['price'],hist=False,ax=ax,bins=50)\nax.set(ylabel='Density',title='Distribution of price')\nax = plt.subplot(1,2,2)\nsns.distplot(np.log1p(train['price']),hist=False,ax=ax,bins=50)\nax.set(xlabel='log1p(price)',ylabel='Density',title='Distribution of log1p(price)')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f22284aafd469dfa868694c4ba2b22867f760b95"},"cell_type":"markdown","source":"## Shipping <br />\nThe shipping cost burden is decently splitted between sellers and buyers with more than half of the items' shipping fees are paid by the sellers (55%). <br />\n부담하는 배송비는 판매자와 구매자가 거의 반반으로 나누어져 있습니다. <br />\n\nIn addition, the average price paid by users who have to pay for shipping fees is lower than those that don't require additional shipping cost. <br />\n=> 게다가 배송비로 사용자가 지불해야하는 돈의 평균은 추가적인 배송비를 필요로하지 않는 것보다 낮았습니다. <br />\nThis matches with our perception that the sellers need a lower price to compensate for the additional shipping. <br />\n=> 이에 따르면 판매자들은 낮은 가격을 배송비를 추가함으로써 무마하려는 것 같습니다. <br />"},{"metadata":{"trusted":true,"_uuid":"4c3209b847e60bf910e5cb51562335a960bae865"},"cell_type":"code","source":"train['shipping'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f2e4cfd79a469fa6c0bfc4ac10c110ff0986b613"},"cell_type":"code","source":"shipping_buyer_price = train.loc[train['shipping'] == 0,'price']\nshipping_seller_price = train.loc[train['shipping'] == 1,'price']\n\nfig = plt.figure(figsize=[10,5])\nsns.distplot(np.log1p(shipping_buyer_price),bins=50,kde=False,color='red',label='buyer')\nsns.distplot(np.log1p(shipping_seller_price),bins=50,kde=False,color='blue',label='seller')\nplt.xlabel('log1p(price)')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"40ad15239e030704ab375cb9b84a0d4a640b4b83"},"cell_type":"markdown","source":"## Item Category <br />\nThere are about 1,287 unique categories but among each of them, we will always see a main/general category firstly, followed by two more particular subcategories (e.g. Beauty/Makeup/Face or Lips). <br />\n=> 1287개의 서로다른 카테고리들이 존재한다. 우리는 가장 주된 카테고리를 먼저 볼 수 있으며, 다음으로 두 개의 다른 카테고리를 더 볼 수 있다. <br />\nIn adidition, there are about 6,327 items that do not have a category labels. <br />\n=> 그러나 6327개의 물품들이 카테고리가 존재하지 않음을 알 수 있다. <br />\n\nLet's split the categories into three different columns. <br />\n=> 일단 카테고리들을 3개의 컬럼으로 나누어 보자 <br />\nWe will see later that this information is actually quite important from the seller's point of view and how we handle the missing information in the brand_name column will impact the model's prediction. <br />\n=> 나누어진 컬럼들은 추후에 판매자의 입장에서 굉장히 중요하며, 브랜드 이름 컬럼에서 손실값을 어떻게 다루는지가 모델의 예측을 다르게 할 것입니다."},{"metadata":{"trusted":true,"_uuid":"7bc9b5b092a8d6fa21ac174cf616d550823a773c"},"cell_type":"code","source":"train['category_name'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7250fa9622f30c097be4ec620fac652c500c5615"},"cell_type":"code","source":"train['category_name'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"952ec7bfa76ff5e3ce8d258258e1e67e518ea36b"},"cell_type":"code","source":"category_null = train.loc[train['category_name'].isnull()]\ncategory_notnull = train.loc[train['category_name'].notnull()]\n\n# print(category_notnull.shape)\n# category_notnull.info()\n\nsep_categories = category_notnull['category_name'].apply(lambda x:x.split('/'))\n\ncol_names = ['general_cat','subcat_1','subcat_2']\n\ncats = []\n\nfor cats_ in sep_categories:\n    cat_dict = {col_names[0]:cats_[0],col_names[1]:cats_[1],col_names[2]:cats_[2]}\n    cats.append(cat_dict)\n    \ndf_cats = pd.DataFrame(cats)\n\ncategory_notnull = category_notnull.reset_index(drop=True)\ncategory_notnull = pd.concat([category_notnull,df_cats],axis=1)\n\nfor col in col_names:\n    category_null[col] = 'No Label'\n\n# assert category_notnull.index.max() == category_notnull.train_id.max()\n\ntrain = pd.concat([category_notnull,category_null],axis=0,sort=False).sort_values(by='train_id').reset_index(drop=True)\ntrain.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"00232c2479ce259b14aa764a9915cdb77ed7ef52"},"cell_type":"code","source":"# # reference: BuryBuryZymon at https://www.kaggle.com/maheshdadhich/i-will-sell-everything-for-free-0-55\n# def split_cat(text):\n#     try: return text.split(\"/\")\n#     except: return (\"No Label\", \"No Label\", \"No Label\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"34113b7bccaf0b2696d2755918976f88317d450c"},"cell_type":"code","source":"# train['general_cat'], train['subcat_1'], train['subcat_2'] = \\\n# zip(*train['category_name'].apply(lambda x: split_cat(x)))\n# train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9061c2b9d8a6a50faba7e3dd974ad49495bb36b7"},"cell_type":"code","source":"print(\"There are %d unique general-categories.\" % train['general_cat'].nunique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7d7ec9238e97153bc323eea3c4c1cdd829893623"},"cell_type":"code","source":"print(\"There are %d unique first sub-categories.\" % train['subcat_1'].nunique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9eedec9b196e506f6c8aec40aa3f49c943173642"},"cell_type":"code","source":"print(\"There are %d unique second sub-categories.\" % train['subcat_2'].nunique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"630ea3001436cd9eef5dcd1d9b51a990918814b9"},"cell_type":"code","source":"train['general_cat']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"f883d1426cb91d6ae97acb903339620e1f1a4a2a"},"cell_type":"code","source":"plt.figure(figsize=[8,6])\ntrain['general_cat'].value_counts().plot(kind='bar',linewidth=2)\nplt.title('Number of Items by Main Category')\nplt.ylabel('Count')\nplt.xlabel('Categories')\nplt.xticks(rotation=60)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"44de77b5a0383e877789abd9f2c3ee5b5e2e8292"},"cell_type":"code","source":"train['subcat_1'].value_counts()[:15].plot(kind='bar',linewidth=2, figsize=[12,10])\nplt.title('Number of Items by Subset_1')\nplt.ylabel('Count')\nplt.xlabel('Categories')\nplt.xticks(rotation=60)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"86256028852aed0ad5b6de806aa03da67a5772aa"},"cell_type":"code","source":"fig = plt.figure(figsize=[12,10])\nsns.boxplot(x=train['general_cat'],y=np.log1p(train['price']))\nplt.xticks(rotation=60)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e7c45654aaa46d3503a211c363b3a00df081bb83"},"cell_type":"code","source":"def wordCount(text):\n    # convert to lower case and strip regex\n    try:\n         # convert to lower case and strip regex\n        text = text.lower()\n        #regular expression pattern 생성\n        regex = re.compile('[' +re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]')\n        #패턴을 이용하여 들어오는 텍스트에서 해당 패턴의 기호들을 제거하는 것\n        txt = regex.sub(\" \", text)\n        # tokenize\n        # words = nltk.word_tokenize(clean_txt)\n        # remove words in stop words\n        words = [w for w in txt.split(\" \") \\\n                 if not w in stop_words.ENGLISH_STOP_WORDS and len(w)>3]\n        return len(words)\n    except: \n        return 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a90a40c1eed8d1a3926bd966ed0bdb9f789f845e"},"cell_type":"code","source":"train['item_description'][3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1d3a4300ec340529cfc261b8204ba97a4916daf8"},"cell_type":"code","source":"regex = re.compile('[' +re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"385c2bbe83f0fc9ebb9bcf2ae62f29481682e370"},"cell_type":"code","source":"regex.sub(\" \",train['item_description'][3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"e51e10696e8d0512c7c763937838fc8905c4265f"},"cell_type":"code","source":"list(stop_words.ENGLISH_STOP_WORDS)[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"81f5acf3b2fface757eecb674f50b9a3b5e4d329"},"cell_type":"code","source":"# add a column of word counts to both the training and test set\ntrain['desc_len'] = train['item_description'].apply(lambda x: wordCount(x))\ntest['desc_len'] = test['item_description'].apply(lambda x: wordCount(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"78801d54ea905ca0eb29a27520121ab36c5b0f66"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"757fc13c540b2186296ca170850313030a4f214b"},"cell_type":"code","source":"df = train.groupby('desc_len')['price'].mean().reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d455b8d15535dd6c75825bd65bbb19fafdd47cde"},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c4a32e9033af57db83237a620bcf6f8836e13d82"},"cell_type":"code","source":"plt.figure(figsize=[20,8])\nsns.pointplot(x=df.columns[0],y=df.columns[1],data=df)\nplt.xticks(rotation=60)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"24ea89988615bdeb42f8a833e4c1f8418a2a70dc"},"cell_type":"markdown","source":"We also need to check if there are any missing values in the item description (4 observations don't have a description) and l remove those observations from our training set. <br />\n=> 아이템 설명 란에 다른 손실값이 있는지 찾아봅시다. 4개의 손실값을 찾을 수 있었으며 우리의 훈련셋에서 제외합시다."},{"metadata":{"trusted":true,"_uuid":"32c510199f2482df8640cde3100fea91d8dedf89"},"cell_type":"code","source":"train['item_description'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d3234bd32f32b780eabbc40be63f4a28a69e2f0b"},"cell_type":"code","source":"train = train.loc[train['item_description'].notnull()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c4476bbac7f546ffed972d817985e09bee07f468"},"cell_type":"code","source":"stop = set(stopwords.words('english'))\ndef tokenize(text):\n    \"\"\"\n    sent_tokenize(): segment text into sentences\n    word_tokenize(): break sentences into words\n    \"\"\"\n    try: \n        regex = re.compile('[' +re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]')\n        text = regex.sub(\" \", text) # remove punctuation\n        \n        tokens_ = [word_tokenize(s) for s in sent_tokenize(text)]\n\n        tokens = []\n        for token_by_sent in tokens_:\n            tokens += token_by_sent\n\n        tokens = list(filter(lambda t: t.lower() not in stop, tokens))\n        filtered_tokens = [w for w in tokens if re.search('[a-zA-Z]', w)]\n        filtered_tokens = [w.lower() for w in filtered_tokens if len(w)>=3]\n        \n        return filtered_tokens\n            \n    except TypeError as e: print(text,e)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4dc91429ec3a4b0b8878a73d0aceb0aaf0cfd334"},"cell_type":"code","source":"cat_desc = dict()\nfor cat in train.general_cat.unique(): \n    text = \" \".join(train.loc[train['general_cat']==cat, 'item_description'].values)\n    cat_desc[cat] = tokenize(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e0df9bb078faf01e5a42836931fcae6f2d983fb1"},"cell_type":"code","source":"# flat list of all words combined\nflat_lst = [item for sublist in list(cat_desc.values()) for item in sublist]\nallWordsCount = Counter(flat_lst)\nall_top10 = allWordsCount.most_common(20)\nx = [w[0] for w in all_top10]\ny = [w[1] for w in all_top10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4dc84011d7c504602545ac66e79a96282cbd4019"},"cell_type":"code","source":"len(flat_lst)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"49ebf8e40f6996321f668cd3c88ad835685fc0f2"},"cell_type":"code","source":"len(allWordsCount)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e2b771ea1ecb576c475028c171ce705b3ea23fbe"},"cell_type":"code","source":"all_top10","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7a0c3f4e4e570acf43e7cc6508d0b66fcc0a53d8"},"cell_type":"markdown","source":"If we look at the most common words by category, we could also see that, size, free and shipping is very commonly used by the sellers, probably with the intention to attract customers, which is contradictory to what we have shown previously that there is little correlation between the two variables price and shipping (or shipping fees do not account for a differentiation in prices). <br />\n=> 카테고리에 따라 가장 자주나온 단어를 본다면, 우리는 size,free 그리고 shipping 이라는 단어들이 판매자들이 소비자들의 관심을 얻기위해 주로 사용합니다. 이러한 가격과 배송의 연관관계는 앞에서 살펴보았습니다. (또는 배송비는 가격을 결정하는데 작용하지 않을 수도 있습니다.) <br />\nBrand names also played quite an important role - it's one of the most popular in all four categories. <br />\n=> 브랜드이름은 꽤나 중요한 역할을 합니다 - 네개의 카테고리들 중에 가장 빈번한 것들중 하나입니다."},{"metadata":{"_uuid":"6a91bb97bd50b7c89def96a3be46a63124a6aa4f"},"cell_type":"markdown","source":"Text Processing - Item Description <br />\n=> 물품 상세 컬럼을 위한 문자 전처리 <br />\nThe following section is based on the tutorial at https://ahmedbesbes.com/how-to-mine-newsfeed-data-and-extract-interactive-insights-in-python.html"},{"metadata":{"_uuid":"87c10093363755a77b2534e7732e3a9e83959cc6"},"cell_type":"markdown","source":"## **Pre-processing:  tokenization**\n\nMost of the time, the first steps of an NLP project is to **\"tokenize\"** your documents, which main purpose is to normalize our texts. The three fundamental stages will usually include: </br>\n=> 자연어 처리의 첫 번째 해야할 일은 문서를 tokenize하는 것입니다. (우리의 문자들을 정규화하는 것이죠). 이는 대부분 세 개의 절차로 나누어져 있습니다.\n* break the descriptions into sentences and then break the sentences into tokens <br />\n=> 설명을 문장별로 나누고 그 문장을 토큰화합니다 <br />\n* remove punctuation and stop words <br />\n=> 각종 따옴표등을 없애고 단어들로 만듭니다 <br />\n* lowercase the tokens\n=> 토큰들을 소문자화 합니다 <br />\n* herein, I will also only consider words that have length equal to or greater than 3 characters <br />\n=>여기에서는 우리는 3글자보다 크거나 같은 길이를 가진 단어들만 고려하면 됩니다."},{"metadata":{"trusted":true,"_uuid":"d92eae4df1dd739b7d05f8003e93553ccd22ef2c"},"cell_type":"code","source":"# stop = set(stopwords.words('english'))\n# def tokenize(text):\n#     \"\"\"\n#     sent_tokenize(): segment text into sentences\n#     word_tokenize(): break sentences into words\n#     \"\"\"\n#     try: \n#         regex = re.compile('[' +re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]')\n#         text = regex.sub(\" \", text) # remove punctuation\n        \n# #         print(sent_tokenize(text))\n        \n#         tokens_ = [word_tokenize(s) for s in sent_tokenize(text)]\n# #         print(tokens_)\n#         tokens = []\n#         for token_by_sent in tokens_:\n#             tokens += token_by_sent\n# #         print(tokens)\n#         tokens = list(filter(lambda t: t.lower() not in stop, tokens))\n# #         print(tokens)\n#         filtered_tokens = [w for w in tokens if re.search('[a-zA-Z]', w)]\n# #         print(filtered_tokens)\n#         filtered_tokens = [w.lower() for w in filtered_tokens if len(w)>=3]\n# #         print(filtered_tokens)\n        \n#         return filtered_tokens\n            \n#     except TypeError as e: print(text,e)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e7cab1da39878eb102adcd83c4b23b208672e176"},"cell_type":"code","source":"train['tokens'] = train['item_description'].apply(tokenize)\ntest['tokens'] = train['item_description'].apply(tokenize)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f602afaf9abd638d414443032650eea603c884ec"},"cell_type":"code","source":"for description, tokens in zip(train['item_description'].head(),\n                              train['tokens'].head()):\n    print('description:', description)\n    print('tokens:', tokens)\n    print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e819bd04cffa8b532c90842b80bc403520040bb1"},"cell_type":"code","source":"#어짜피 토큰 만들어놨는데 다시 조인해서 토크나이즈 하는 것은 낭비라고 생각\ncat_desc = dict()\nfor cat in train.general_cat.unique():\n    cat_list = []\n    for token in train.loc[train['general_cat']==cat,'tokens']:\n        for one in token:\n            cat_list.append(one)\n    cat_desc[cat] = cat_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"659e29b7ef9e0f79aa011492604cc524d1a47d02"},"cell_type":"code","source":"women100 = Counter(cat_desc['Women']).most_common(100)\nbeauty100 = Counter(cat_desc['Beauty']).most_common(100)\nkids100 = Counter(cat_desc['Kids']).most_common(100)\nelectronics100 = Counter(cat_desc['Electronics']).most_common(100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"53142ca2b8e25f1622654dd271f92c1279ac7563"},"cell_type":"code","source":"women100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5bd28df150721aaeab01c430b35e3fa9581dd1e3"},"cell_type":"code","source":"def generate_wordcloud(tup):\n    wordcloud = WordCloud(background_color='white',max_words=50,max_font_size=40,random_state=42).generate(str(tup))\n    return wordcloud","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c7605596bf693fa759189d3988f3379a91b4eb10"},"cell_type":"code","source":"fig,ax = plt.subplots(2,2,figsize=[30,15])\n\nax = plt.subplot(2,2,1)\nax.imshow(generate_wordcloud(women100),interpolation=\"bilinear\")\nax.axis('off')\n\nax = plt.subplot(2,2,2)\nax.imshow(generate_wordcloud(beauty100))\nax.axis('off')\n\nax = plt.subplot(2,2,3)\nax.imshow(generate_wordcloud(kids100))\nax.axis('off')\n\nax = plt.subplot(2,2,4)\nax.imshow(generate_wordcloud(electronics100))\nax.axis('off')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8b06af3622aff3667dc37ce71c4f38dabe529a12"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ca2fe1d5248e2627b85053b7734ed858395b6535"},"cell_type":"markdown","source":"## **Pre-processing:  tf-idf**\n\ntf-idf is the acronym for **Term Frequency–inverse Document Frequency**. It quantifies the importance of a particular word in relative to the vocabulary of a collection of documents or corpus. The metric depends on two factors: <br />\n=> tf-idf는 \"Term Frequency-inverse Document Frequency\"의 약어입니다. 이는 문서나 말뭉치의 집합의 단어와 관련된 특정언어의 중요도를 수치화합니다 <br />\n- **Term Frequency**: the occurences of a word in a given document (i.e. bag of words)\n=> 주어진 문서에서 단어의 등장 정도<br />\n- **Inverse Document Frequency**: the reciprocal number of times a word occurs in a corpus of documents\n=>단어의 뭉치에서 단어가 등장한 상호간의 횟수<br />\n\nThink about of it this way: If the word is used extensively in all documents, its existence within a specific document will not be able to provide us much specific information about the document itself. <br />\n=> 만약 단어가 모든 문서에서 많이 사용된다면, 어느 특정한 문서 내의 해당 단어의 존재는 그 문서에 대해 특정한 정보라고 제시할 수 없습니다.\nSo the second term could be seen as a penalty term that penalizes common words such as \"a\", \"the\", \"and\", etc. tf-idf can therefore, be seen as a weighting scheme for words relevancy in a specific document. <br />\n=> 그래서 두번째로 제약을 거는데, 많이 등장하는 언어 a,the,and 등에. 그러므로 tf-idf는 어느 특정한 문서에 단어들의 연관성에 가중치를 두는것과 유사합니다.\n"},{"metadata":{"_uuid":"145ee66f8bdc5f3b3127c3556c5fa1da4e841015"},"cell_type":"markdown","source":"## ** tf-idf: 단어빈도-역문서빈도** [얼마나 의미 있는 특성인지를 계산해서 스케일을 조정하는 방식]\n**말뭉치의 다른 문서보다 특정 문서에 자주 나타나는 단어에 높은 가중치를 주는 방법.** <br />\nex)) 한 단어가 특정 문서에 자주 나타나고 다른 여러 문서에는 그렇지 않다면, 그 문서의 내용을 잘 설명하는 단어라고 볼 수있음.\n\n### Term Frequency: 주어진 다큐먼트(하나의 문자열)에 단어의 등장횟수(CountVectorizer)\n### Inverse Document Frequency: 다큐먼트의 하나의 말뭉치에서 임의의 단어가 상호간에 등장하는 횟수\n=> CountVectorizer를 혼용하는 것 같음\n<br />\n\ne.g) 임의의 단어가 모든 다큐먼트들에서 빈번하기 나온다면, 어느 특정한 다큐먼트에서 그 단어의 존재는 그 다큐먼트에서 해당 단어가 특정한 정보를 제공하지 못한다고 할수있다. <br />\n=> 만약 영화에 대한 평가 텍스트라면 서로 다른 영화에서 나올 수 있는 평가인 \"그리고\" 라는 토큰은 특정 영화가 어느 영화인지 가늠할 수 없지만, \"스릴러\"라는 토큰은 멜로, 음악등의 영화에서는 등장하지 않는 어느 특정영화에만 나오는 토큰이기 때문에 어느 영화인지 가늠이 가능하다.\n\n따라서 위와 같이 **여러 다큐먼트에서 나오는 토큰들의 경우에는 규제를 가함으로써 가중치를 낮춘다.** 이 때문에 가중치가 높은 단어는 어느 특정 다큐먼트와 관련이 있을 가능성이 커지는 것이다."},{"metadata":{"trusted":true,"_uuid":"73d0afd8e34f1ab759992dcf668cc7bed01efea2"},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer(min_df=10,max_features=180000,tokenizer=tokenize,ngram_range=(1,2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d926c4165e2798d74c216809ba42a8e5886bcd5c"},"cell_type":"code","source":"all_desc = np.append(train['item_description'].values,test['item_description'].values)\nvz = vectorizer.fit_transform(list(all_desc))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b54ac0baa166dbf6fbadc239c6988f78d5f7de6b"},"cell_type":"markdown","source":"vz is a tfidf matrix where: <br />\n=> tfidf 행렬은 아래와 같습니다 <br />\n* the number of rows is the total number of descriptions <br />\n=> 행은 전체 설명의 개수 <br />\n* the number of columns is the total number of unique tokens across the descriptions <br />\n=> 열은 설명들에 있어서 유일한 토큰의 개수를 의미한다"},{"metadata":{"trusted":true,"_uuid":"98f8336836a7e9c8ecdf522e8e1fc37544856a4b"},"cell_type":"code","source":"tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\ntfidf = pd.DataFrame(columns=['tfidf']).from_dict(\n                    dict(tfidf), orient='index')\ntfidf.columns = ['tfidf']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"55576bca918b41902de60755938d693488332bec"},"cell_type":"markdown","source":"Below is the 10 tokens with the lowest tfidf score, which is unsurprisingly, very generic words that we could not use to distinguish one description from another. <br />\n=> tfidf 점수가 낮은 10개의 토큰들은 다른 물품들을 구별할만한 설명이 아닌 단어들이었습니다."},{"metadata":{"trusted":true,"_uuid":"c930035e00458699ef58b770a22b12602b4d1600"},"cell_type":"code","source":"tfidf.sort_values(by='tfidf',ascending=True).head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"14503c9a02e72087e782cb4916b03382055e4542"},"cell_type":"markdown","source":"Below is the 10 tokens with the highest tfidf score, which includes words that are a lot specific that by looking at them, we could guess the categories that they belong to:  <br />\n=> tfidf 점수가 높은 10개의 토큰은 물품들의 설명하는데 있어서 특징적인 것들이 많습니다."},{"metadata":{"trusted":true,"_uuid":"4d3836878a1a467465b87f0dcb1b7d048f28c509"},"cell_type":"code","source":"tfidf.sort_values(by='tfidf',ascending=False).head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bd3a194651c32bbb52d356571c334678684448b1"},"cell_type":"markdown","source":"Given the high dimension of our tfidf matrix, we need to reduce their dimension using the Singular Value Decomposition (SVD) technique. <br />\n=> 주어진 고차원의 tfidf 행렬을 가지고, 우리는 SVD를 활용하여 그 차원을 축소할 수 있습니다. <br />\nAnd to visualize our vocabulary, we could next use t-SNE to reduce the dimension from 50 to 2. t-SNE is more suitable for dimensionality reduction to 2 or 3. <br /> \n=> 그리고 우리의 단어들을 시각화하고, t-SNE를 50차원에서 2차원으로 줄이기 위해 사용할 것입니다. t-SNE는 2차원 또는 3차원으로 줄이는데 가장 적절한 방법입니다.\n\n### **t-Distributed Stochastic Neighbor Embedding (t-SNE)**\n\nt-SNE is a technique for dimensionality reduction that is particularly well suited for the visualization of high-dimensional datasets. <br />\n=>t-SNE는 특히 고차원의 데이터셋을 시각화하는데 잘 맞는 차원 축소 기술입니다 <br />\nThe goal is to take a set of points in a high-dimensional space and find a representation of those points in a lower-dimensional space, typically the 2D plane. <br />\n=> 목표는 고차원공간에 있는 점들의 집합을 가지고 특히 2차원 공간에 있는 것과 같은 저차원의 점들의 표현법을 찾아내는 것입니다. <br />\nIt is based on probability distributions with random walk on neighborhood graphs to find the structure within the data. <br />\n=> 이는 neighborhood 그래프에서 random walk를 가지고 데이터의 구조를 찾기 위한 확률 분포를 기초로 합니다. <br />\nBut since t-SNE complexity is significantly high, usually we'd use other high-dimension reduction techniques before applying t-SNE. <br />\n=>그러나 t-SNE의 복잡도가 상당히 높다면, t-SNE를 적용하기전에 다른 고차원 축소 기술들을 사용할 수 있습니다. <br />\n\nFirst, let's take a sample from the both training and testing item's description since t-SNE can take a very long time to execute.<br />\n=> 일단은 훈련과 테스트셋의 아이템 설명에서 샘플을 가져와봅시다. 왜냐하면 t-SNE는 실행에 오래걸리기 때문입니다.<br />\nWe can then reduce the dimension of each vector from to n_components (50) using SVD. <br />\n=> 이후에 SVD를 이용하여 차원을 줄여볼 것입니다.\n"},{"metadata":{"_uuid":"4a2a9142ecd86200410d17592a3bae3f878124c8"},"cell_type":"markdown","source":"## 차원 축소를 하는 이유?\n### **1. 시각화**\n#### 2. 데이터 압축\n#### 3. 추가적 처리\n<br />\n\n차원축소 방법: **주성분분석(PCA), 비음수 행렬 분해(NMF), t-SNE**"},{"metadata":{"trusted":true,"_uuid":"9681a656fbbf627f3ad5a495e967e53c40b31128"},"cell_type":"code","source":"trn = train.copy()\ntst = test.copy()\ntrn['is_train'] = 1\ntst['is_train'] = 0\n\n#t-SNE가 시간이 많이 걸리는 작업이기 때문에 임의의 샘플사이즈로 나누고 이를 차원분할해보자\nsample_sz = 15000\n\ncombined_df = pd.concat([trn,tst])\ncombined_sample = combined_df.sample(n=sample_sz)\nvz_sample = vectorizer.fit_transform(list(combined_sample['item_description']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"35bf2e863bb3016aa2a1c633802036e6ad37dee1"},"cell_type":"code","source":"vz_sample.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1db0250dd4bcfea498c3bc8d79008f54ec5d7afb"},"cell_type":"code","source":"from sklearn.decomposition import TruncatedSVD\n\nn_comp = 30\nsvd = TruncatedSVD(n_components=n_comp, random_state=42)\nsvd_tfidf = svd.fit_transform(vz_sample)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2dae2bbe912b1c9067637a43b8f4d0787999e077"},"cell_type":"code","source":"svd_tfidf.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"08b62bd17aad91a0db61b391187294d9316098b9"},"cell_type":"markdown","source":"Now we can reduce the dimension from 50 to 2 using t-SNE!"},{"metadata":{"trusted":true,"_uuid":"b7633fb2713ca52986aae0e8ddd55338e3d061a1"},"cell_type":"code","source":"from sklearn.manifold import TSNE\ntsne_model = TSNE(n_components=2, verbose=1, random_state=42, n_iter=500)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c508cd7837701dc16d2bc9a7e5aa8bca90ef5820"},"cell_type":"code","source":"tsne_tfidf = tsne_model.fit_transform(svd_tfidf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d2c3316dc121b84a1b9140eef7c35e7ed91a5b53"},"cell_type":"code","source":"tsne_tfidf","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"93f8acac6d58d2d2b7b7d13554352086d924a528"},"cell_type":"markdown","source":"It's now possible to visualize our data points. Note that the deviation as well as the size of the clusters imply little information  in t-SNE."},{"metadata":{"trusted":true,"_uuid":"f63ce9dc5ec3731c1b073c4dda16c7695a6ba59c"},"cell_type":"code","source":"output_notebook()\nplot_tfidf = bp.figure(plot_width=700, plot_height=600,\n                       title=\"tf-idf clustering of the item description\",\n    tools=\"pan,wheel_zoom,box_zoom,reset,hover,previewsave\",\n    x_axis_type=None, y_axis_type=None, min_border=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ea18239a3a402e4c36584382da5d6a713c80779b"},"cell_type":"code","source":"combined_sample = combined_sample.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bfdb2f28cf6c455b5cc39791d904ba5eaf450329"},"cell_type":"code","source":"tfidf_df = pd.DataFrame(tsne_tfidf, columns=['x', 'y'])\ntfidf_df['description'] = combined_sample['item_description']\ntfidf_df['tokens'] = combined_sample['tokens']\ntfidf_df['category'] = combined_sample['general_cat']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ae7905c5cd7ef3386f1be46f4600c452acb4dccc"},"cell_type":"code","source":"tfidf_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"949482225878ee79537ba6ef038baa9a17833fae"},"cell_type":"code","source":"plot_tfidf.scatter(x='x', y='y', source=tfidf_df, alpha=0.7)\nhover = plot_tfidf.select(dict(type=HoverTool))\nhover.tooltips={\"description\": \"@description\", \"tokens\": \"@tokens\", \"category\":\"@category\"}\nshow(plot_tfidf)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"597df67a110092fa0c349d257350c854f4751ec2"},"cell_type":"markdown","source":"## **K-Means Clustering** [데이터 셋을 클러스터라는 그룹으로 나누는 작업]\n\nK-means clustering obejctive is to minimize the average squared Euclidean distance of the document / description from their cluster centroids. <br />\n=> 위의 목표는 문서나 설명의 cluster centroid의 Euclidean 거리 제곱의 평균을 줄이는 것\n"},{"metadata":{"trusted":true,"_uuid":"7cc69ffe5db09bebfcbcd92ac6a8f980ead109a4"},"cell_type":"code","source":"from sklearn.cluster import MiniBatchKMeans\n\nnum_clusters = 30 # need to be selected wisely\nkmeans_model = MiniBatchKMeans(n_clusters=num_clusters,\n                               init='k-means++',\n                               n_init=1,\n                               init_size=1000, batch_size=1000, verbose=0, max_iter=1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4f3fb54499841be56acc08da1ad8ba7192854464"},"cell_type":"code","source":"kmeans = kmeans_model.fit(vz)\nkmeans_clusters = kmeans.predict(vz)\nkmeans_distances = kmeans.transform(vz)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5590796a06dfbd792de0aa3d3d18454f65778d90"},"cell_type":"code","source":"# sorted_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\n# terms = vectorizer.get_feature_names()\n\n# for i in range(num_clusters):\n#     print(\"Cluster %d:\" % i)\n#     aux = ''\n#     for j in sorted_centroids[i, :10]:\n#         aux += terms[j] + ' | '\n#     print(aux)\n#     print() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7127991ffdefb1ec6fcc1e8a9c5af61444d1855d"},"cell_type":"code","source":"# repeat the same steps for the sample\nkmeans = kmeans_model.fit(vz_sample)\nkmeans_clusters = kmeans.predict(vz_sample)\nkmeans_distances = kmeans.transform(vz_sample)\n# reduce dimension to 2 using tsne\ntsne_kmeans = tsne_model.fit_transform(kmeans_distances)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3b97b02981a8501fcc80d29b3b058a6103e0938a"},"cell_type":"code","source":"colormap = np.array([\"#6d8dca\", \"#69de53\", \"#723bca\", \"#c3e14c\", \"#c84dc9\", \"#68af4e\", \"#6e6cd5\",\n\"#e3be38\", \"#4e2d7c\", \"#5fdfa8\", \"#d34690\", \"#3f6d31\", \"#d44427\", \"#7fcdd8\", \"#cb4053\", \"#5e9981\",\n\"#803a62\", \"#9b9e39\", \"#c88cca\", \"#e1c37b\", \"#34223b\", \"#bdd8a3\", \"#6e3326\", \"#cfbdce\", \"#d07d3c\",\n\"#52697d\", \"#194196\", \"#d27c88\", \"#36422b\", \"#b68f79\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c0efa29e6db7757180796d59129c68299f61de0c"},"cell_type":"code","source":"#combined_sample.reset_index(drop=True, inplace=True)\nkmeans_df = pd.DataFrame(tsne_kmeans, columns=['x', 'y'])\nkmeans_df['cluster'] = kmeans_clusters\nkmeans_df['description'] = combined_sample['item_description']\nkmeans_df['category'] = combined_sample['general_cat']\n#kmeans_df['cluster']=kmeans_df.cluster.astype(str).astype('category')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dd685a5f99c61d2228f55e95d99270ffc5fc1015"},"cell_type":"code","source":"plot_kmeans = bp.figure(plot_width=700, plot_height=600,\n                        title=\"KMeans clustering of the description\",\n    tools=\"pan,wheel_zoom,box_zoom,reset,hover,previewsave\",\n    x_axis_type=None, y_axis_type=None, min_border=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bf8e3d1dd300c7063c79ffe5c8489fed3e1d8819"},"cell_type":"code","source":"source = ColumnDataSource(data=dict(x=kmeans_df['x'], y=kmeans_df['y'],\n                                    color=colormap[kmeans_clusters],\n                                    description=kmeans_df['description'],\n                                    category=kmeans_df['category'],\n                                    cluster=kmeans_df['cluster']))\n\nplot_kmeans.scatter(x='x', y='y', color='color', source=source)\nhover = plot_kmeans.select(dict(type=HoverTool))\nhover.tooltips={\"description\": \"@description\", \"category\": \"@category\", \"cluster\":\"@cluster\" }\nshow(plot_kmeans)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2aa05074edbce36f3e35179c2483e59a1207d32a"},"cell_type":"markdown","source":"## **Latent Dirichlet Allocation**\n\nLatent Dirichlet Allocation (LDA) is an algorithms used to discover the topics that are present in a corpus. <br />\n=> LDA는 말뭉치내에 존재하는 토픽들을 찾는데 이용되는 알고리즘입니다.\n\n>  LDA starts from a fixed number of topics. Each topic is represented as a distribution over words, and each document is then represented as a distribution over topics. Although the tokens themselves are meaningless, the probability distributions over words provided by the topics provide a sense of the different ideas contained in the documents. <br />\nLDA는 고정된 토픽의 수를 결정하면서 시작하며, 각각의 토픽은 모든 단어들에 걸쳐서 나뉘고, 각각의 다큐먼트는 모든 토픽들에 걸쳐서 나뉩니다. 비록 토큰들이 의미가 없더라도, 토픽별로 제공되는 모든 단어들에 걸친 확률분포는 다른 종류의 아이디어들을 제공합니다.\n> \n> Reference: https://medium.com/intuitionmachine/the-two-paths-from-natural-language-processing-to-artificial-intelligence-d5384ddbfc18\n\nIts input is a **bag of words**, i.e. each document represented as a row, with each columns containing the count of words in the corpus. We are going to use a powerful tool called pyLDAvis that gives us an interactive visualization for LDA. "},{"metadata":{"trusted":true,"_uuid":"ed08fc10e5fa03d1cae10fb16b8123aed6a7e63a"},"cell_type":"code","source":"cvectorizer = CountVectorizer(min_df=4,\n                              max_features=180000,\n                              tokenizer=tokenize,\n                              ngram_range=(1,2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"18ed94637b8231a21882309fc12eca3db9655968"},"cell_type":"code","source":"cvz = cvectorizer.fit_transform(combined_sample['item_description'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a404893bfcb12cdf29c585539cbfeba83addd4a5"},"cell_type":"code","source":"cvz","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"342fcebaa98c071ebaf2daa2c84d6bc0c6fd50d4"},"cell_type":"code","source":"lda_model = LatentDirichletAllocation(n_components=100,\n                                      learning_method='online',\n                                      max_iter=20,\n                                      random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d1080eec02f0196590b126a82bf7054fa1127aca"},"cell_type":"code","source":"X_topics = lda_model.fit_transform(cvz)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1ef8bccf39806875605b23012f60d464df283f02"},"cell_type":"code","source":"lda_model.components_.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6f8038092762c46744009903d621f594bfb4463d"},"cell_type":"code","source":"sorting = np.argsort(lda_model.components_,axis=1)[::-1]\nfeature_names = np.array(cvectorizer.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aa1680a92ea614277e0301a0227eb0e2bccefb19"},"cell_type":"code","source":"fig, ax = plt.subplots(1,2,figsize=[10,12])\ntopic_names = [\"{:>2} \".format(i)+\" \".join(words) for i,words in enumerate(feature_names[sorting[:,:2]])]\n\nfor col in [0,1]:\n    start = col * 50\n    end = (col+1) * 50\n    ax[col].barh(np.arange(50),np.sum(X_topics,axis=0)[start:end])\n    ax[col].set_yticks(np.arange(50))\n    ax[col].set_yticklabels(topic_names[start:end],ha='left',va='top')\n    ax[col].invert_yaxis()\n    ax[col].set_xlim(0,1000)\n    yax = ax[col].get_yaxis()\n    yax.set_tick_params(pad=130)\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7402cc56ebf0ca6607de6a199291d8e4a2963d8c"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}