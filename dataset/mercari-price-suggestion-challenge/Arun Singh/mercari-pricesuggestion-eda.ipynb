{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Mercari Price Suggestion Challenge\n\n## Overview\n\nPredicting the price of a product is a tough challenge since very similar products having minute differences such as different brand name, additional specifications, quality of the product, demand of the product, etc. can have very different prizes. For example the price of a pair of running shoes by a very common brand (say Puma) might be around INR 2,500 whereas a similar pair made by Asics can cost around INR 10,000.<br>\n\nPrice prediction gets even more difficult when there is a huge range of products, which is common with most of the online shopping platforms. While it might be simpler to predict the price of a particular category of products using some simple criteria, it’s highly challenging to predict the price of almost anything that is listed on online platforms. We may have multiple listings  of the same product by a large number of sellers priced differently.<br>\n\n<a href=\"https://www.mercari.com\">Mercari</a> is Japan’s biggest community-powered shopping app. Mercari’s challenge is to build an algorithm that automatically suggests the right product prices.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Data collection\n\nThe data can be downloaded from <a href=\"https://www.kaggle.com/c/mercari-price-suggestion-challenge/data\">Kaggle</a> competion page.<br>\n\nWe have been provided user-inputted text descriptions of their products, including details like product category name, brand name, and item condition","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## ML Problem\n\nUsing the given data, we have to come up with a model that predicts the price of a product listed on Mercari as accurately as possible.\n\nThis is a standard regression problem.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Performance Metric\n\nThe performance of the model is measured by Root Mean Squared Logarithmic Error(RMSLE). Lesser the RMSLE, better is our prediction model.<br>\n\nThe RMSLE is calculated as\n\\begin{align*}\n\\epsilon = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (\\log(p_i + 1) - \\log(a_i+1))^2 }\n\\end{align*}<br>\n\n- ϵ is the RMSLE value (score)\n- n is the total number of observations in the (public/private) data set,\n- pi is your prediction of price, and\n- ai is the actual sale price for i\n- log(x) is the natural logarithm of x","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nimport shutil\nimport datetime\nimport gc\nfrom tqdm import tqdm\n\nimport pandas as pd\nimport numpy as np\nfrom numpy import median\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style='whitegrid')\n\nfrom sklearn.manifold import TSNE\nfrom sklearn import preprocessing\n\nfrom collections import Counter\n\nimport string\nimport re\nfrom nltk.corpus import stopwords\n\nimport scipy\nfrom scipy import hstack\n\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom sklearn.metrics import mean_squared_error as mse\nfrom math import sqrt\nfrom sklearn.linear_model import Ridge\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import log_loss\n\nfrom sklearn.model_selection import RandomizedSearchCV \nfrom scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform\nfrom sklearn.feature_selection.univariate_selection import SelectKBest, f_regression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tracemalloc\nimport time","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tracemalloc.start()\n\nstart_time = time.time()\nsnapshot1 = tracemalloc.take_snapshot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 0. Loading data","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# https://www.kaggle.com/peterhurford/lgb-and-fm-18th-place-0-40604\ndef split_cat(text):\n    try:\n        return text.split(\"/\")\n    except:\n        return (\"No Label\", \"No Label\", \"No Label\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train = pd.read_csv('train.tsv', sep='\\t', \n                      dtype={'item_condition_id': 'category', 'shipping': 'category'}, \n                      converters={'category_name': split_cat})\ntest = pd.read_csv('test.tsv', sep='\\t', \n                     dtype={'item_condition_id': 'category', 'shipping': 'category'}, \n                     converters={'category_name': split_cat})","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print('Shape of train data: ', train.shape)\nprint('Shape of test data: ', test.shape)\ntrain.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. Data Overview","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Handling missing values","execution_count":null},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"train.isnull().any()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"test.isnull().any()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Columns **brand_name, item_description** have NAs.\n\nNAs in **category_name** have been replaced by empty lists because of the converter we have used while loading the data.\n\nWe will replace NAs, empty lists with *'missing'*.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# Split category_name by '/' into subcategories and replace nulls with 'missing'\ntrain['gencat_name'] = train['category_name'].str.get(0).replace('', 'missing').astype('category')\ntrain['subcat1_name'] = train['category_name'].str.get(1).fillna('missing').astype('category')\ntrain['subcat2_name'] = train['category_name'].str.get(2).fillna('missing').astype('category')\ntrain.drop('category_name', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Split category_name by '/' into subcategories and replace nulls with 'missing'\ntest['gencat_name'] = test['category_name'].str.get(0).replace('', 'missing').astype('category')\ntest['subcat1_name'] = test['category_name'].str.get(1).fillna('missing').astype('category')\ntest['subcat2_name'] = test['category_name'].str.get(2).fillna('missing').astype('category')\ntest.drop('category_name', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train['item_description'].fillna('missing', inplace=True)\ntrain['brand_name'] = train['brand_name'].fillna('missing').astype('category')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test['item_description'].fillna('missing', inplace=True)\ntest['brand_name'] = test['brand_name'].fillna('missing').astype('category')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Check for duplicate rows, NAs","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"train[train.duplicated()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No duplicate rows in train data","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"train.isnull().any()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Check for rows with invalid price","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"print('Removed {} rows' .format(len(train[train.price<=0])))\ntrain = train[train.price > 0].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Column-wise overview of data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### name","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"train.name.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### item_condition_id","execution_count":null},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"train.item_condition_id.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"condition_count = Counter(list(train.item_condition_id))\nx, y = zip(*condition_count.most_common())\nplt.figure(figsize=[8,6])\nplt.bar(x, y, )\nfor i, val in enumerate(y):\n           plt.annotate(val, (x[i], y[i]), color='b')\nplt.xlabel('item condition')\nplt.ylabel('count')\nplt.grid(False, axis='x')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Majority of the items  are in **condition 1**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### brand_name","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"train.brand_name.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"brand_count = Counter(list(train.brand_name.values))\nx, y = zip(*brand_count.most_common(15))\n\nplt.figure(figsize=[6,8])\nplt.barh(x, y)\nfor i, val in enumerate(y):\n           plt.annotate(val, (y[i], x[i]), color='b')\nplt.gca().invert_yaxis()\nplt.ylabel('Brand name')\nplt.xlabel('count')\nplt.grid(False, axis='y')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"brand_missing = train[train.brand_name=='missing'].shape[0]\nprint('Brand name is missing for {} datapoints, i.e. {:.2f} % of train data.' .format(brand_missing, 100.0*brand_missing/train.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### gencat_name","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"train.gencat_name.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"gencat_count = Counter(list(train.gencat_name.values))\nx, y = zip(*gencat_count.most_common(15))\nplt.figure(figsize=[6,8])\nplt.barh(x, y)\nfor i, val in enumerate(y):\n           plt.annotate(val, (y[i], x[i]), color='b')\nplt.gca().invert_yaxis()\nplt.ylabel('General category')\nplt.xlabel('count')\nplt.grid(False, axis='y')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Majority of the items are from the category **women**","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"gencat_missing = train[train.gencat_name=='missing'].shape[0]\nprint('category name is missing for {} datapoints, i.e. {:.2f} % of train data.' .format(gencat_missing, 100.0*gencat_missing/train.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### subcat1_name","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"train.subcat1_name.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"subcat1_count = Counter(list(train.subcat1_name.values))\nx, y = zip(*subcat1_count.most_common(15))\nplt.figure(figsize=[6,10])\nplt.barh(x, y)\nfor i, val in enumerate(y):\n           plt.annotate(val, (y[i], x[i]), color='b')\nplt.gca().invert_yaxis()\nplt.ylabel('Sub-category1')\nplt.xlabel('count')\nplt.grid(False, axis='y')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"subcat1_missing = train[train.subcat1_name=='missing'].shape[0]\nprint('subcategory1 name is missing for {} datapoints, i.e. {:.2f} % of train data.' .format(subcat1_missing, 100.0*subcat1_missing/train.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### subcat2_name","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"train.subcat2_name.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"subcat2_count = Counter(list(train.subcat2_name.values))\nx, y = zip(*subcat2_count.most_common(15))\nplt.figure(figsize=[6,10])\nplt.barh(x, y)\nfor i, val in enumerate(y):\n           plt.annotate(val, (y[i], x[i]), color='b')\nplt.gca().invert_yaxis()\nplt.ylabel('Sub-category2')\nplt.xlabel('count')\nplt.grid(False, axis='y')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"subcat2_missing = train[train.subcat2_name=='missing'].shape[0]\nprint('subcategory2 name is missing for {} datapoints, i.e. {:.2f} % of train data.' .format(subcat2_missing, 100.0*subcat2_missing/train.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### item_description","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"desc_missing = train[train.item_description=='missing'].shape[0]\nprint('item description is missing for {} datapoints, i.e. {:.5f} % of train data.' .format(desc_missing, 100.0*desc_missing/train.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"train[train.item_description=='missing']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### price","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.FacetGrid(train,size=6) \\\n    .map(sns.kdeplot,\"price\") \\\n    .add_legend();\nplt.title('price density distribution')\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.boxplot(y='price', data=train, showfliers=False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"for i in range(0, 100, 10):\n    var =train[\"price\"].values\n    var = np.sort(var,axis = None)\n    print(\"{} percentile value is {}\".format(i,var[int(len(var)*(float(i)/100))]))\nprint(\"100 percentile value is \",var[-1])","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"for i in range(90, 100, 1):\n    var =train[\"price\"].values\n    var = np.sort(var,axis = None)\n    print(\"{} percentile value is {}\".format(i,var[int(len(var)*(float(i)/100))]))\nprint(\"100 percentile value is \",var[-1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- **97% of datapoints have price less than USD 100.**\n- **Very few (only 1%) datapoints have price more than USD 170**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 2. Pre-processing","execution_count":null},{"metadata":{"slideshow":{"slide_type":"skip"},"trusted":false},"cell_type":"code","source":"def preprocess_name(text_col):\n    preprocessed_names = []\n    for sentence in tqdm(text_col.values):\n        sent = sentence.replace('\\\\r', ' ')\n        sent = sent.replace('\\\\\"', ' ')\n        sent = sent.replace('\\\\n', ' ')\n        sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n        preprocessed_names.append(sent.lower().strip())\n    return preprocessed_names\n\nstopwords = stopwords.words('english')\ndef preprocess_desc(text_col):\n    preprocessed_descs = []\n    for sentence in tqdm(text_col.values):\n        sent = sentence.replace('\\\\r', ' ')\n        sent = sent.replace('\\\\\"', ' ')\n        sent = sent.replace('\\\\n', ' ')\n        sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n        sent = ' '.join(e for e in sent.split() if e not in stopwords)\n        preprocessed_descs.append(sent.lower().strip())\n    return preprocessed_descs","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train['preprocessed_name'] = preprocess_name(train['name'])\ntest['preprocessed_name'] = preprocess_name(test['name'])\n\ntrain['preprocessed_description'] = preprocess_desc(train['item_description'])\ntest['preprocessed_description'] = preprocess_desc(test['item_description'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def clean_cat(cat_values):\n    '''takes categorical column values as arguments and returns list of cleaned categories'''\n    \n    catogories = list(cat_values)\n\n    cat_list = []\n    for i in tqdm(catogories):\n        i = re.sub('[^A-Za-z0-9]+', ' ', i)\n        i = i.replace(' ','')\n        i = i.replace('&','_')\n        cat_list.append(i.strip())\n    \n    return cat_list ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train['gencat_name'] = clean_cat(train['gencat_name'].values)\ntest['gencat_name'] = clean_cat(test['gencat_name'].values)\n\ntrain['subcat1_name'] = clean_cat(train['subcat1_name'].values)\ntest['subcat1_name'] = clean_cat(test['subcat1_name'].values)\n\ntrain['subcat2_name'] = clean_cat(train['subcat2_name'].values)\ntest['subcat2_name'] = clean_cat(test['subcat2_name'].values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Basic Exploratory Data Analysis","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.set(style='whitegrid')\nplt.figure(figsize=(12,6))\nsns.boxplot(x='item_condition_id', y='price', data=train, showfliers=False)\nplt.title('item_condition-wise distribution of price')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**There is slight variation of price based on item condition. Median Price decreases as we go from condition 1 to 4. Items in condition 5 seem to be having higher price, which is a bit strange.**","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(15,6))\nsns.boxplot(y='price', x='gencat_name', data=train, showfliers=False)\nplt.xticks(rotation=45)\nplt.title('category-wise distribution of price')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"sns.barplot(y='gencat_name', x='price', data=train)\nplt.title('mean price of various categories')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(10,25))\nsns.barplot(y='subcat1_name', x='price', data=train)\nplt.title('mean price of various subcategories')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(10,25))\nsns.barplot(y='subcat1_name', x='price', data=train, estimator=median)\nplt.title('median price of various subcategories')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Prices of items belonging to various categories and subcategories vary significantly.**<br>\n**This indicates that categories are going to be important features in determining the price of an item.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 4. Featurization","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 4.1. Feature Engineering","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**name_first**: cleaned name first word\n\n**name_first_count**: count of name in data\n\n**gencat_name_count**: count of gencat in data\n\n**subcat1_name_count**: count of subcat1 in data\n\n**subcat2_name_count**: count of subcat2 in data\n\n**brand_name_count**: count of brand_name in data","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"def get_name_first(name):\n    \n    name =  re.sub('[^A-Za-z0-9]+', ' ', name) .split()\n    if len(name):\n            return name[0].lower()\n    return ''\n        \n        \ntrain['name_first'] = train['name'].apply(get_name_first)\ntest['name_first'] = test['name'].apply(get_name_first)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def transform_test(base_col, feat_col):\n    '''\n    Returns feat_col column of test data by mapping from the values already calculated for the same column in train data\n    \n    Parameters:\n    \n    base_col: column based on which a transform(count, mean, median) has been applied\n    \n    feat_col: desired feature column after applying the transform\n    '''\n    #Create dictionary of feature values from train data\n    di = pd.Series(train[feat_col].values, index=train[base_col].values).to_dict()\n    \n    #Map test data using dictionary and fill NAs with 0\n    \n    if base_col == 'item_condition_id':\n        #No chance of NAs\n        return test[base_col].map(di).astype(float)\n        \n    return test[base_col].map(di).fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train['name_first_count'] = train.groupby('name_first')['name_first'].transform('count')\ntest['name_first_count'] = transform_test('name_first', 'name_first_count')\n\ntrain['gencat_name_count'] = train.groupby('gencat_name')['gencat_name'].transform('count')\ntest['gencat_name_count'] = transform_test('gencat_name', 'gencat_name_count')\n\ntrain['subcat1_name_count'] = train.groupby('subcat1_name')['subcat1_name'].transform('count')\ntest['subcat1_name_count'] = transform_test('subcat1_name', 'subcat1_name_count')\n\ntrain['subcat2_name_count'] = train.groupby('subcat2_name')['subcat2_name'].transform('count')\ntest['subcat2_name_count'] = transform_test('subcat2_name', 'subcat2_name_count')\n\ntrain['brand_name_count'] = train.groupby('brand_name')['brand_name'].transform('count')\ntest['brand_name_count'] = transform_test('brand_name', 'brand_name_count')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n**NameLower**: # lowercase letters in name\n\n**DescriptionLower**: # lowercase letters in description\n\n**NameUpper**: # uppercase letters in name\n\n**DescriptionUpper**: # uppercase letters in description\n\n**name_len**: char length of name\n\n**des_len**: char length of desc\n\n**name_desc_len_ratio**: name_len / des_len\n\n**desc_word_count**\n\n**mean_des**: 10 * desc_word_count / des_len\n\n**name_word_count**\n\n**mean_name**: 10 * name_word_count / name_len\n\n**desc_letters_per_word**: des_len / desc_word_count\n\n**name_letters_per_word**: name_len / name_word_count\n\n**NameLowerRatio**: NameLower / name_len\n\n**DescriptionLowerRatio**: DescriptionLower / des_len\n\n**NameUpperRatio**: NameUpper / name_len\n\n**DescriptionUpperRatio**: DescriptionUpper / des_len","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"train['NameLower'] = train.name.str.count('[a-z]')\ntrain['DescriptionLower'] = train.item_description.str.count('[a-z]')\ntrain['NameUpper'] = train.name.str.count('[A-Z]')\ntrain['DescriptionUpper'] = train.item_description.str.count('[A-Z]')\ntrain['name_len'] = train['name'].apply(lambda x: len(x))\ntrain['des_len'] = train['item_description'].apply(lambda x: len(x))\ntrain['name_desc_len_ratio'] = train['name_len']/train['des_len']\ntrain['desc_word_count'] = train['item_description'].apply(lambda x: len(x.split()))\ntrain['mean_des'] = train['item_description'].apply(lambda x: 0 if len(x) == 0 else float(len(x.split())) / len(x)) * 10\ntrain['name_word_count'] = train['name'].apply(lambda x: len(x.split()))\ntrain['mean_name'] = train['name'].apply(lambda x: 0 if len(x) == 0 else float(len(x.split())) / len(x))  * 10\ntrain['desc_letters_per_word'] = train['des_len'] / train['desc_word_count']\ntrain['name_letters_per_word'] = train['name_len'] / train['name_word_count']\ntrain['NameLowerRatio'] = train['NameLower'] / train['name_len']\ntrain['DescriptionLowerRatio'] = train['DescriptionLower'] / train['des_len']\ntrain['NameUpperRatio'] = train['NameUpper'] / train['name_len']\ntrain['DescriptionUpperRatio'] = train['DescriptionUpper'] / train['des_len']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test['NameLower'] = test.name.str.count('[a-z]')\ntest['DescriptionLower'] = test.item_description.str.count('[a-z]')\ntest['NameUpper'] = test.name.str.count('[A-Z]')\ntest['DescriptionUpper'] = test.item_description.str.count('[A-Z]')\ntest['name_len'] = test['name'].apply(lambda x: len(x))\ntest['des_len'] = test['item_description'].apply(lambda x: len(x))\ntest['name_desc_len_ratio'] = test['name_len']/test['des_len']\ntest['desc_word_count'] = test['item_description'].apply(lambda x: len(x.split()))\ntest['mean_des'] = test['item_description'].apply(lambda x: 0 if len(x) == 0 else float(len(x.split())) / len(x)) * 10\ntest['name_word_count'] = test['name'].apply(lambda x: len(x.split()))\ntest['mean_name'] = test['name'].apply(lambda x: 0 if len(x) == 0 else float(len(x.split())) / len(x))  * 10\ntest['desc_letters_per_word'] = test['des_len'] / test['desc_word_count']\ntest['name_letters_per_word'] = test['name_len'] / test['name_word_count']\ntest['NameLowerRatio'] = test['NameLower'] / test['name_len']\ntest['DescriptionLowerRatio'] = test['DescriptionLower'] / test['des_len']\ntest['NameUpperRatio'] = test['NameUpper'] / test['name_len']\ntest['DescriptionUpperRatio'] = test['DescriptionUpper'] / test['des_len']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**NamePunctCount**: # punctuations in name\n\n**DescriptionPunctCount**: # punctuations in desc\n\n**NamePunctCountRatio**: NamePunctCount / name_word_count\n\n**DescriptionPunctCountRatio**: DescriptionPunctCount / desc_word_count\n\n**NameDigitCount**: # digits in name\n\n**DescriptionDigitCount**: # digits in desc\n\n**NameDigitCountRatio**: NameDigitCount / name_word_count\n\n**DescriptionDigitCountRatio**: DescriptionDigitCount / desc_word_count\n\n**stopword_ratio_desc**: # stopwords in desc / desc_word_count\n\n**num_sum**: Sum of numbers in desc\n\n**weird_characters_desc**: # non-alphanumeric, non-punct in desc\n\n**weird_characters_name**: # non-alphanumeric, non-punct in desc\n\n**prices_count**: # of [rm] (removed price) in desc\n\n**price_in_name**: 1 if desc contains [rm]; 0 otherwise","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"from nltk.corpus import stopwords\n\nRE_PUNCTUATION = '|'.join([re.escape(x) for x in string.punctuation])\ns_words = {x: 1 for x in stopwords.words('english')} #converting to dictionary for fast look up\nnon_alphanumpunct = re.compile(u'[^A-Za-z0-9\\.?!,; \\(\\)\\[\\]\\'\\\"\\$]+')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#https://www.kaggle.com/peterhurford/lgb-and-fm-18th-place-0-40604\n\ndef to_number(x):\n    try:\n        if not x.isdigit():\n            return 0\n        x = int(x)\n        if x > 100:\n            return 100\n        else:\n            return x\n    except:\n        return 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train['NamePunctCount'] = train.name.str.count(RE_PUNCTUATION)\ntrain['DescriptionPunctCount'] = train.item_description.str.count(RE_PUNCTUATION)\ntrain['NamePunctCountRatio'] = train['NamePunctCount'] / train['name_word_count']\ntrain['DescriptionPunctCountRatio'] = train['DescriptionPunctCount'] / train['desc_word_count']\ntrain['NameDigitCount'] = train.name.str.count('[0-9]')\ntrain['DescriptionDigitCount'] = train.item_description.str.count('[0-9]')\ntrain['NameDigitCountRatio'] = train['NameDigitCount'] / train['name_word_count']\ntrain['DescriptionDigitCountRatio'] = train['DescriptionDigitCount']/train['desc_word_count']\ntrain['stopword_ratio_desc'] = train['item_description'].apply(lambda x: len([w for w in x.split() if w in s_words])) / train['desc_word_count']\ntrain['num_sum'] = train['item_description'].apply(lambda x: sum([to_number(s) for s in x.split()])) \ntrain['weird_characters_desc'] = train['item_description'].str.count(non_alphanumpunct)\ntrain['weird_characters_name'] = train['name'].str.count(non_alphanumpunct)\ntrain['prices_count'] = train['item_description'].str.count('[rm]')\ntrain['price_in_name'] = train['item_description'].str.contains('[rm]', regex=False).astype('category')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test['NamePunctCount'] = test.name.str.count(RE_PUNCTUATION)\ntest['DescriptionPunctCount'] = test.item_description.str.count(RE_PUNCTUATION)\ntest['NamePunctCountRatio'] = test['NamePunctCount'] / test['name_word_count']\ntest['DescriptionPunctCountRatio'] = test['DescriptionPunctCount'] / test['desc_word_count']\ntest['NameDigitCount'] = test.name.str.count('[0-9]')\ntest['DescriptionDigitCount'] = test.item_description.str.count('[0-9]')\ntest['NameDigitCountRatio'] = test['NameDigitCount'] / test['name_word_count']\ntest['DescriptionDigitCountRatio'] = test['DescriptionDigitCount']/test['desc_word_count']\ntest['stopword_ratio_desc'] = test['item_description'].apply(lambda x: len([w for w in x.split() if w in s_words])) / test['desc_word_count']\ntest['num_sum'] = test['item_description'].apply(lambda x: sum([to_number(s) for s in x.split()])) \ntest['weird_characters_desc'] = test['item_description'].str.count(non_alphanumpunct)\ntest['weird_characters_name'] = test['name'].str.count(non_alphanumpunct)\ntest['prices_count'] = test['item_description'].str.count('[rm]')\ntest['price_in_name'] = test['item_description'].str.contains('[rm]', regex=False).astype('category')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**brand_mean_price**: mean price of items by a brand\n\n**name_mean_price**: mean price of an item by name\n\n**gencat_mean_price**: mean price of items belonging to gencat\n\n**subcat1_mean_price**: mean price of items belonging to subcat1\n\n**subcat2_mean_price**: mean price of items belonging to subcat2\n\n**condition_mean_price**: mean price of items by condition\n\n**brand_median_price**: median price of items by a brand\n\n**name_median_price**: median price of an item by name\n\n**gencat_median_price**: median price of items belonging to gencat\n\n**subcat1_median_price**: median price of items belonging to subcat1\n\n**subcat2_median_price**: median price of items belonging to subcat2\n\n**condition_median_price**: median price of items by condition","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"train['brand_mean_price'] = train.groupby('brand_name')['price'].transform('mean')\ntest['brand_mean_price'] = transform_test('brand_name', 'brand_mean_price')\n\ntrain['name_mean_price'] = train.groupby('name_first')['price'].transform('mean')\ntest['name_mean_price'] = transform_test('name_first', 'name_mean_price')\n\ntrain['gencat_mean_price'] = train.groupby('gencat_name')['price'].transform('mean')\ntest['gencat_mean_price'] = transform_test('gencat_name', 'gencat_mean_price')\n\ntrain['subcat1_mean_price'] = train.groupby('subcat1_name')['price'].transform('mean')\ntest['subcat1_mean_price'] = transform_test('subcat1_name', 'subcat1_mean_price')\n\ntrain['subcat2_mean_price'] = train.groupby('subcat2_name')['price'].transform('mean')\ntest['subcat2_mean_price'] = transform_test('subcat2_name', 'subcat2_mean_price')\n\ntrain['condition_mean_price'] = train.groupby('item_condition_id')['price'].transform('mean')\ntest['condition_mean_price'] = transform_test('item_condition_id', 'condition_mean_price')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train['brand_median_price'] = train.groupby('brand_name')['price'].transform('median')\ntest['brand_median_price'] = transform_test('brand_name', 'brand_median_price')\n\ntrain['name_median_price'] = train.groupby('name_first')['price'].transform('median')\ntest['name_median_price'] = transform_test('name_first', 'name_median_price')\n\ntrain['gencat_median_price'] = train.groupby('gencat_name')['price'].transform('median')\ntest['gencat_median_price'] = transform_test('gencat_name', 'gencat_median_price')\n\ntrain['subcat1_median_price'] = train.groupby('subcat1_name')['price'].transform('median')\ntest['subcat1_median_price'] = transform_test('subcat1_name', 'subcat1_median_price')\n\ntrain['subcat2_median_price'] = train.groupby('subcat2_name')['price'].transform('median')\ntest['subcat2_median_price'] = transform_test('subcat2_name', 'subcat2_median_price')\n\ntrain['condition_median_price'] = train.groupby('item_condition_id')['price'].transform('median')\ntest['condition_median_price'] = transform_test('item_condition_id', 'condition_median_price')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"train.drop(['name', 'item_description'], axis=1, inplace=True)\ntest.drop(['name', 'item_description'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(train.shape, test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Univariate analysis on the above features","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(18,18))\n\nplt.subplot(3,3,1)\nsns.regplot(x='brand_mean_price', y='price', data=train, scatter_kws={'alpha':0.3}, line_kws={'color':'orange'})\nplt.title('brand_mean_price vs price(target)')\n\nplt.subplot(3,3,2)\nsns.regplot(x='gencat_mean_price', y='price', data=train, scatter_kws={'alpha':0.3}, line_kws={'color':'orange'})\nplt.title('category_mean_price vs price(target)')\n\nplt.subplot(3,3,3)\nsns.regplot(x='subcat1_mean_price', y='price', data=train, scatter_kws={'alpha':0.3}, line_kws={'color':'orange'})\nplt.title('subcategory_mean_price vs price(target)')\n\nplt.subplot(3,3,4)\nsns.regplot(x='subcat2_mean_price', y='price', data=train, scatter_kws={'alpha':0.3}, line_kws={'color':'orange'})\nplt.title('subcategory_mean_price vs price(target)')\n\nplt.subplot(3,3,5)\nsns.regplot(x='condition_mean_price', y='price', data=train, scatter_kws={'alpha':0.3}, line_kws={'color':'orange'})\nplt.title('condition_mean_price vs price(target)')\n\nplt.subplot(3,3,6)\nsns.regplot(x='brand_median_price', y='price', data=train, scatter_kws={'alpha':0.3}, line_kws={'color':'orange'})\nplt.title('brand_median_price vs price(target)')\n\nplt.subplot(3,3,7)\nsns.regplot(x='subcat1_median_price', y='price', data=train, scatter_kws={'alpha':0.3}, line_kws={'color':'orange'})\nplt.title('subcategory_median_price vs price(target)')\n\nplt.subplot(3,3,8)\nsns.regplot(x='subcat2_median_price', y='price', data=train, scatter_kws={'alpha':0.3}, line_kws={'color':'orange'})\nplt.title('subcategory_median_price vs price(target)')\n\nplt.subplot(3,3,9)\nsns.regplot(x='name_median_price', y='price', data=train, scatter_kws={'alpha':0.3}, line_kws={'color':'orange'})\nplt.title('name_median_price vs price(target)')\n            \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Features such as *brand_mean_price, brand_median price, subcat2_mean_price, subcat2_median_price* show strong linear trends.**<br>\n\n**Therefore they seem to be useful in determinig price of items.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Dropping rows with blank name and description**","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"n_rows = train.shape[0]\ntrain = train[train.preprocessed_name != ''].reset_index(drop=True)\n\nprint('Dropped {} rows'.format(n_rows - train.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"n_rows = train.shape[0]\ntrain = train[train.preprocessed_description != ''].reset_index(drop=True)\n\nprint('Dropped {} rows'.format(n_rows - train.shape[0]))\n\nprint('Shape of train data: ', train.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.2. Train, Test split for cross validation","execution_count":null},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ny_tr = np.log1p(train['price'])\ntrain.drop(['price'], axis=1, inplace=True)\n\ntrain_df, cv_df , y_train, y_cv = train_test_split(train, y_tr, test_size=0.1, random_state=42)\n\nprint('Train size: {}, CV size: {}, Test size: {}' .format(train_df.shape, cv_df.shape, test.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"del train, y_tr\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.3. Categorical features\nOne-hot encoding of brand_name, gencat_name, subcat1_name, subcat2_name.\n","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"#Cleaning brand name before using count vectorizer\n# Using same preprocessing as used earlier for categories: 'clean_cat()' function\n\ntrain_df['brand_name'] = clean_cat(train_df['brand_name'].values)\ncv_df['brand_name'] = clean_cat(cv_df['brand_name'].values)\ntest['brand_name'] = clean_cat(test['brand_name'].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"vectorizer = CountVectorizer(lowercase=False, binary=True)\ntrain_brand_oneHot = vectorizer.fit_transform(train_df['brand_name'].values)\n\ncv_brand_oneHot = vectorizer.transform(cv_df['brand_name'].values)\ntest_brand_oneHot = vectorizer.transform(test['brand_name'].values)\n\nprint(\"Shape of matrices after one hot encoding\")\nprint(train_brand_oneHot.shape, \"\\n\", cv_brand_oneHot.shape, \"\\n\", test_brand_oneHot.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"vectorizer = CountVectorizer(lowercase=False, binary=True)\ntrain_gencat_oneHot = vectorizer.fit_transform(train_df['gencat_name'].values)\n\ncv_gencat_oneHot = vectorizer.transform(cv_df['gencat_name'].values)\ntest_gencat_oneHot = vectorizer.transform(test['gencat_name'].values)\n\nprint(\"Shape of matrices after one hot encoding\")\nprint(train_gencat_oneHot.shape, \"\\n\", cv_gencat_oneHot.shape, \"\\n\", test_gencat_oneHot.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"vectorizer = CountVectorizer(lowercase=False, binary=True)\ntrain_subcat1_oneHot = vectorizer.fit_transform(train_df['subcat1_name'].values)\n\ncv_subcat1_oneHot = vectorizer.transform(cv_df['subcat1_name'].values)\ntest_subcat1_oneHot = vectorizer.transform(test['subcat1_name'].values)\n\nprint(\"Shape of matrices after one hot encoding\")\nprint(train_subcat1_oneHot.shape, \"\\n\", cv_subcat1_oneHot.shape, \"\\n\", test_subcat1_oneHot.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"vectorizer = CountVectorizer(lowercase=False, binary=True)\ntrain_subcat2_oneHot = vectorizer.fit_transform(train_df['subcat2_name'].values)\n\ncv_subcat2_oneHot = vectorizer.transform(cv_df['subcat2_name'].values)\ntest_subcat2_oneHot = vectorizer.transform(test['subcat2_name'].values)\n\nprint(\"Shape of matrices after one hot encoding\")\nprint(train_subcat2_oneHot.shape, \"\\n\", cv_subcat2_oneHot.shape, \"\\n\", test_subcat2_oneHot.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.4. Tfidf vectorization on text features\n1-3 grams of name<br>\n1-3 grams of item_description","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"vectorizer = TfidfVectorizer(ngram_range=(1, 3), min_df=3, max_features=250000)\n\ntrain_name_tfidf = vectorizer.fit_transform(train_df['preprocessed_name'].values)\n\ncv_name_tfidf = vectorizer.transform(cv_df['preprocessed_name'].values)\ntest_name_tfidf = vectorizer.transform(test['preprocessed_name'].values)\n\nprint(\"Shape of matrices after vectorization\")\nprint(train_name_tfidf.shape, \"\\n\", cv_name_tfidf.shape, \"\\n\", test_name_tfidf.shape)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"vectorizer = TfidfVectorizer(ngram_range=(1, 3), min_df=5, max_features=500000)\n\ntrain_description_tfidf = vectorizer.fit_transform(train_df['preprocessed_description'].values)\n\ncv_description_tfidf = vectorizer.transform(cv_df['preprocessed_description'].values)\ntest_description_tfidf = vectorizer.transform(test['preprocessed_description'].values)\n\nprint(\"Shape of matrices after vectorization\")\nprint(train_description_tfidf.shape, \"\\n\", cv_description_tfidf.shape, \"\\n\", test_description_tfidf.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Data preparation","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"submission_df = pd.DataFrame(test['test_id'])\nprint(submission_df.shape)\nsubmission_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.1. Normalize numerical features","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"cols = set(train_df.columns.values) - {'train_id'}\nskip_cols = {'preprocessed_name', 'item_condition_id', 'brand_name',\n  'shipping', 'preprocessed_description', 'gencat_name',\n  'subcat1_name', 'subcat2_name', 'name_first', 'price_in_name'}\n\ncols_to_normalize = cols - skip_cols\nprint(\"Normalizing following columns: \", cols_to_normalize)\n\ndef normalize(df):\n    result1 = df.copy()\n    for feature_name in df.columns:\n        if (feature_name in cols_to_normalize):\n            max_value = df[feature_name].max()\n            min_value = df[feature_name].min()\n            result1[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)\n    return result1\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_normalized = normalize(train_df)\ncv_normalized = normalize(cv_df)\ntest_normalized = normalize(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"del train_df, cv_df, test\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.2. Remove  non-features from dataframes","execution_count":null},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"#Separating and storing all numerical features\n\nX_tr = train_normalized[list(cols_to_normalize)]\nX_val = cv_normalized[list(cols_to_normalize)]\nX_te = test_normalized[list(cols_to_normalize)]\n\nX_tr.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from scipy.sparse import csr_matrix\n\n# Storing categorical features to sparse matrix\n\nX_tr_cat = csr_matrix(pd.get_dummies(train_normalized[['item_condition_id', 'shipping', 'price_in_name']], sparse=True).values)\n\nX_cv_cat = csr_matrix(pd.get_dummies(cv_normalized[['item_condition_id', 'shipping', 'price_in_name']], sparse=True).values)\n\nX_te_cat = csr_matrix(pd.get_dummies(test_normalized[['item_condition_id', 'shipping', 'price_in_name']], sparse=True).values)\n\nprint(X_tr_cat.shape, X_cv_cat.shape, X_te_cat.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"del train_normalized, cv_normalized, test_normalized\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.3. Consolidate all features to a sparse matrix","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"from scipy.sparse import hstack\n\n# stack all categorical and text sparse matrices\n\ntrain_sparse = hstack((train_brand_oneHot, train_gencat_oneHot, train_subcat1_oneHot, train_subcat2_oneHot, \\\n               train_name_tfidf, train_description_tfidf, X_tr_cat)).tocsr()\n\ncv_sparse = hstack((cv_brand_oneHot, cv_gencat_oneHot, cv_subcat1_oneHot, cv_subcat2_oneHot, \\\n               cv_name_tfidf, cv_description_tfidf, X_cv_cat)).tocsr()\n\ntest_sparse = hstack((test_brand_oneHot, test_gencat_oneHot, test_subcat1_oneHot, test_subcat2_oneHot, \\\n               test_name_tfidf, test_description_tfidf, X_te_cat)).tocsr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(train_sparse.shape, cv_sparse.shape, test_sparse.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# stack dense feature matrix with categorical and text vectors\n\nX_train = hstack((X_tr.values, train_sparse)).tocsr()\n\nX_cv = hstack((X_val.values, cv_sparse)).tocsr()\n\nX_test = hstack((X_te.values, test_sparse)).tocsr()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"print('Train size: {}, CV size: {}, Test size: {}' .format(X_train.shape, X_cv.shape, X_test.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"del vectorizer\ndel train_brand_oneHot, train_gencat_oneHot, train_subcat1_oneHot, train_subcat2_oneHot, \\\n            train_name_tfidf, train_description_tfidf, X_tr_cat\n\ndel cv_brand_oneHot, cv_gencat_oneHot, cv_subcat1_oneHot, cv_subcat2_oneHot, cv_name_tfidf, cv_description_tfidf, X_cv_cat\n\ndel test_brand_oneHot, test_gencat_oneHot, test_subcat1_oneHot, test_subcat2_oneHot, \\\n               test_name_tfidf, test_description_tfidf, X_te_cat\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6. Modeling","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error as mse\nfrom math import sqrt\nfrom sklearn.linear_model import Ridge","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Target variable is ln(1+y), therefore  calculating mean square error on ln(1+y) will effectively give us MSLE on y\n\n- MSLE(y^, y) = MSE(ln(1+y^), ln(1+y))","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 6.1. Ridge Model\nLinear least squares with l2 regularization\n<a href= \"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html\">sklearn.linear_model.Ridge </a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Hyper-parameter tuning","execution_count":null},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"alpha = [1, 2, 3, 3.5, 4, 4.5, 5, 6, 7] \ncv_rmsle_array=[] \nfor i in tqdm(alpha):\n    model = Ridge(solver=\"sag\", random_state=42, alpha=i)\n    model.fit(X_train, y_train)\n    preds_cv = model.predict(X_cv)\n    cv_rmsle_array.append(sqrt(mse(y_cv, preds_cv)))\n\nfor i in range(len(cv_rmsle_array)):\n    print ('RMSLE for alpha = ',alpha[i],'is',cv_rmsle_array[i])\n    \nbest_alpha = np.argmin(cv_rmsle_array)\n\nfig, ax = plt.subplots()\nax.plot(alpha, cv_rmsle_array)\nax.scatter(alpha, cv_rmsle_array)\nfor i, txt in enumerate(np.round(cv_rmsle_array,3)):\n    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],cv_rmsle_array[i]))\n\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha\")\nplt.ylabel(\"Error\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Training using best hyper-parameters and testing","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"print(\"Best alpha: \",  alpha[best_alpha])\nmodel = Ridge(solver=\"sag\", random_state=42, alpha=alpha[best_alpha])\nmodel.fit(X_train, y_train)\nridge_preds_tr = model.predict(X_train)\nridge_preds_cv = model.predict(X_cv)\nridge_preds_te = model.predict(X_test)\n\nprint('Train RMSLE:', sqrt(mse(y_train, ridge_preds_tr)))\n\nridge_rmsle = sqrt(mse(y_cv, ridge_preds_cv))\nprint(\"Cross validation RMSLE: \", ridge_rmsle)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6.2. Multinomial Naive Bayes: Regression using Classification\n<a href= \"https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html\">sklearn.naive_bayes.MultinomialNB </a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"This model has been used  as a correction for the tendency of the Ridge model to underestimate.\nThe predicted values are saved for use as features in a later model.\nThe authors of 18th place solution <a href= \"https://www.kaggle.com/peterhurford/lgb-and-fm-18th-place-0-40604\">kernel</a> improved their RMSLE by 0.003 using this.\n","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\n\nmodel = MultinomialNB(alpha=0.01)\nmodel.fit(X_train, y_train>= 4)\n\nmnb_preds_tr = model.predict_proba(X_train)[:, 1]\nmnb_preds_cv = model.predict_proba(X_cv)[:, 1]\nmnb_preds_te = model.predict_proba(X_test)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SelectKBest:  Selecting  top 48k features from categorical and text features","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# from sklearn.feature_selection.univariate_selection import SelectKBest, f_regression\n\nfselect = SelectKBest(f_regression, k=48000)\ntrain_features = fselect.fit_transform(train_sparse, y_train)\n\ncv_features = fselect.transform(cv_sparse)\ntest_features = fselect.transform(test_sparse)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"print('Shapes after SelectKBest:', train_features.shape, cv_features.shape, test_features.shape)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"# stack feature matrix with Ridge, MNB model predictions, engineered features\nX_train = hstack((X_tr.values, ridge_preds_tr.reshape(-1,1), mnb_preds_tr.reshape(-1,1), train_features)).tocsr()\n\nX_cv = hstack((X_val.values, ridge_preds_cv.reshape(-1,1), mnb_preds_cv.reshape(-1,1), cv_features)).tocsr()\n\nX_test = hstack((X_te.values, ridge_preds_te.reshape(-1,1), mnb_preds_te.reshape(-1,1), test_features)).tocsr()\n\nprint('Train size: {}, CV size: {}, Test size: {}' .format(X_train.shape, X_cv.shape, X_test.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"del train_features, cv_features\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print('Time taken: ', time.time()-start_time)\nsnapshot2 = tracemalloc.take_snapshot()\ntop_stats = snapshot2.compare_to(snapshot1, 'lineno')\n\nprint(\"[ Top 10 ]\")\nfor stat in top_stats[:10]:\n    print(stat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"submission_df['price'] = np.exp(ridge_preds_te) - 1\n\nsubmission_df.to_csv('ridge_submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"scipy.sparse.save_npz(\"cv_final.npz\", X_cv)\nnp.save('y_cv', y_cv)\n\ndel X_cv, y_cv\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"scipy.sparse.save_npz(\"train_final.npz\", X_train)\nnp.save('y_train', y_train)\n\ndel X_train, y_train\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"scipy.sparse.save_npz(\"test_final.npz\", X_test)\n\ndel X_test\ngc.collect()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}