{"cells":[{"metadata":{"_cell_guid":"c0cfc83c-29ee-4673-b2fd-ee77aaadd35c","_uuid":"b29004616894b0edd6bfcad97744e7ea27b608ac"},"source":"# A PyTorch Neural Network for price prediction (Linear Regression) using loss_SGD, loss_Momentum, loss_RMSprop, loss_Adam\n\n\nLearning curve:\n![logo](https://github.com/QuantScientist/Deep-Learning-Boot-Camp/raw/master/Kaggle-PyTorch/mercari/rms.png)\n\n\n## Introduction\n- Work In Progress: will update as I make progress\n- Heavily based on: https://www.kaggle.com/bguberfain/naive-catboost for preprocessing . \n\n## On github\nhttps://github.com/QuantScientist/Deep-Learning-Boot-Camp/tree/master/Kaggle-PyTorch\n\n#### S\n","cell_type":"markdown"},{"execution_count":null,"outputs":[],"cell_type":"code","source":"%reset -f\nfrom __future__ import print_function\nfrom __future__ import division\n\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.\n\nimport torch\nimport sys\nimport torch\nfrom torch.utils.data.dataset import Dataset\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom torch import nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.autograd import Variable\n\nfrom sklearn import cross_validation\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_auc_score, log_loss, roc_auc_score, roc_curve, auc\nfrom sklearn.cross_validation import StratifiedKFold, ShuffleSplit, cross_val_score, train_test_split\n\nprint('__Python VERSION:', sys.version)\nprint('__pyTorch VERSION:', torch.__version__)\n\nimport numpy\nimport numpy as np\n\n\n\n# ! watch -n 0.1 'ps f -o user,pgrp,pid,pcpu,pmem,start,time,command -p `lsof -n -w -t /dev/nvidia*`'\n# sudo apt-get install dstat #install dstat\n# sudo pip install nvidia-ml-py #install Python NVIDIA Management Library\n# wget https://raw.githubusercontent.com/datumbox/dstat/master/plugins/dstat_nvidia_gpu.py\n# sudo mv dstat_nvidia_gpu.py /usr/share/dstat/ #move file to the plugins directory of dstat\n\nimport pandas\nimport pandas as pd\n\nimport logging\nhandler=logging.basicConfig(level=logging.INFO)\nlgr = logging.getLogger(__name__)\n\n\n# !pip install psutil\nimport psutil\nimport os\ndef cpuStats():\n        print(sys.version)\n        print(psutil.cpu_percent())\n        print(psutil.virtual_memory())  # physical memory usage\n        pid = os.getpid()\n        py = psutil.Process(pid)\n        memoryUse = py.memory_info()[0] / 2. ** 30  # memory use in GB...I think\n        print('memory GB:', memoryUse)\n\ncpuStats()\n\n# Data params\nTARGET_VAR= 'target'\nBASE_FOLDER = '../input/'","metadata":{"_cell_guid":"1450a423-d442-4adf-96f2-2878f1d0d33c","_uuid":"8d8ffc194d3352e15cfffecbbcc257525e8c740c","collapsed":true}},{"metadata":{"_cell_guid":"401431a3-e74c-4718-82eb-91b92e1dc726","_uuid":"cdb3b4d60ca404b8161646065625fcff58b8ccd1"},"source":"# CUDA","cell_type":"markdown"},{"execution_count":null,"outputs":[],"cell_type":"code","source":"\nuse_cuda = torch.cuda.is_available()\n# use_cuda = False\n\nFloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\nLongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\nTensor = FloatTensor\n\nprint(\"USE CUDA=\" + str (use_cuda))\n\n#torch.backends.cudnn.benchmark = True","metadata":{"_cell_guid":"49d9abae-5aae-4517-982a-cd9c33f8f354","_uuid":"82af9f893b48d52894a85883c1db722d8ea7a8a1","collapsed":true}},{"metadata":{"_cell_guid":"3ed40f58-da9b-41b2-862e-7465a30313f2","_uuid":"eb9be36bd1a358382664ef40cc73d50e4e172ea5"},"source":"### References\n\n\n## Load the data, split the training data into a training and validation set\n\n","cell_type":"markdown"},{"execution_count":null,"outputs":[],"cell_type":"code","source":"# fix seed\nseed=17*19\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif use_cuda:\n    torch.cuda.manual_seed(seed)\n\n#####\n# Load in the data\n#####\nprint('loading data')\n\ndf_train = pd.read_csv('../input/train.tsv', sep='\\t')\ndf_test = pd.read_csv('../input/test.tsv', sep='\\t')\n\nprint('Train shape:{}\\nTest shape:{}'.format(df_train.shape, df_test.shape))\n\ndf_train.head(5)","metadata":{"_cell_guid":"a4d9e0cc-0b2e-4657-8dcb-7e867f9c9a1d","_uuid":"b28e4558e0db0bf35f090ecf7ef81e38b2e68618","collapsed":true}},{"execution_count":null,"outputs":[],"cell_type":"code","source":"# df_train.plot(kind='scatter', x='item_condition_id', y='price', title='Weight and height in adults')\n","metadata":{"_cell_guid":"7831345d-fa0b-4908-bace-4bcf7d9f2246","_uuid":"09fc5862141836e71edcdcff453651ec6aa02007","collapsed":true}},{"execution_count":null,"outputs":[],"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.pipeline import Pipeline\nfrom collections import defaultdict\nfrom sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\nfrom sklearn import preprocessing\n\nd = defaultdict(LabelEncoder)\nTARGET_VAR='price'\n\ndef split_cat(s):\n    try:\n        return s.split('/')[0],s.split('/')[1],s.split('/')[2],\n    except:\n        return [0,0,0]\n\ndf_train[['cat1','cat2','cat3']] = pd.DataFrame(df_train.category_name.apply(split_cat).tolist(),\n                                   columns = ['cat1','cat2','cat3'])\ndf_test[['cat1','cat2','cat3']] = pd.DataFrame(df_test.category_name.apply(split_cat).tolist(),\n                                   columns = ['cat1','cat2','cat3'])\n\nprint('making the magic...')\ncorpus1 = df_train.name.values.astype('U').tolist() + df_test.name.values.astype('U').tolist()\ncorpus2 = df_train.item_description.values.astype('U').tolist() + df_test.item_description.values.astype('U').tolist()\n\nvectorizer1 = CountVectorizer(min_df=1,stop_words='english')\nvectorizer1.fit(corpus1)\n\nvectorizer2 = CountVectorizer(min_df=1,stop_words='english')\nvectorizer2.fit(corpus2)\n\ntrain_cor1 = vectorizer1.transform(df_train.name.values.astype('U').tolist())\ntrain_cor2 = vectorizer2.transform(df_train.item_description.values.astype('U').tolist())\n\ntest_cor1 = vectorizer1.transform(df_test.name.values.astype('U').tolist())\ntest_cor2 = vectorizer2.transform(df_test.item_description.values.astype('U').tolist())\n\n\ndf_train['cor1'] = np.mean(train_cor1,1)\ndf_train['cor2'] = np.mean(train_cor2,1)\n\ndf_test['cor1'] = np.mean(test_cor1,1)\ndf_test['cor2'] = np.mean(test_cor2,1)\n\n\ndf_train['len1'] = df_train.name.str.len()\ndf_train['len2'] = df_train.item_description.str.len()\n\ndf_test['len1'] = df_test.name.str.len()\ndf_test['len2'] = df_test.item_description.str.len()\n\nprint(\"label encoding...\")\nle = preprocessing.LabelEncoder()\nle.fit(df_train.brand_name.values.tolist() + df_test.brand_name.values.tolist())\ndf_train['brands']= le.transform(df_train.brand_name.values.tolist())\ndf_test['brands']= le.transform(df_test.brand_name.values.tolist())\n\ndf_train = df_train.fillna(999)\ndf_test = df_test.fillna(999)\n\nle = preprocessing.LabelEncoder()\nle.fit(df_train.cat1.values.tolist() + df_test.cat1.values.tolist())\ndf_train['cat1']= le.transform(df_train.cat1.values.tolist())\n\nle = preprocessing.LabelEncoder()\nle.fit(df_train.cat2.values.tolist() + df_test.cat2.values.tolist())\ndf_train['cat2']= le.transform(df_train.cat2.values.tolist())\n\nle = preprocessing.LabelEncoder()\nle.fit(df_train.cat3.values.tolist() + df_test.cat3.values.tolist())\ndf_train['cat3']= le.transform(df_train.cat3.values.tolist())\n\ndf_train = df_train.fillna(999)\ndf_test = df_test.fillna(999)\n\n\nanswers_1_SINGLE = np.abs(df_train[TARGET_VAR])\ndrop_features = ['train_id', 'name', 'category_name', 'brand_name', 'price', 'item_description']\ndf_train = df_train.drop(drop_features, axis=1)\n\ndf_train = df_train.fillna(999)\ndf_test = df_test.fillna(999)\n\ndf_train.head()\n","metadata":{"_cell_guid":"2867469a-d13e-48a2-a1c9-75f84c9f952a","_uuid":"d06e320e331373f111021ac9adffd8f81bccfedf","collapsed":true}},{"execution_count":null,"outputs":[],"cell_type":"code","source":"df_train.to_csv('train_clean.csv', header=False,  index = False)    \ndf_train= pd.read_csv('train_clean.csv', header=None, dtype=np.float32)    \ndf_train = pd.concat([df_train, answers_1_SINGLE], axis=1)\nfeature_cols = list(df_train.columns[:-1])\nprint (feature_cols)\ntarget_col = df_train.columns[-1]\ntrainX, trainY = df_train[feature_cols], df_train[target_col]\ndf_train.head()","metadata":{"_cell_guid":"46646008-4395-4656-a082-588225267a4d","_uuid":"b207986d70a840bfa77481dce6decd423378b957","collapsed":true}},{"execution_count":null,"outputs":[],"cell_type":"code","source":"# Make sure the shape and data are OK\n# Make sure the shape and data are OK\nprint(trainX.shape)\nprint(trainY.shape)\nprint(type(trainY))\nprint(type(trainY))\n\nfrom sklearn.model_selection import train_test_split\n\ndata_train, data_val, labels_train, labels_val = train_test_split(trainX, trainY, \n                                                                    test_size=0.20, random_state=999)\ndata_train=data_train.values\nlabels_train=labels_train.values\nprint(data_train.shape)\nprint(labels_train.shape)\nprint(type(data_train))\nprint(type(labels_train))\n\ndata_val=data_val.values\nlabels_val=labels_val.values\nprint(data_val.shape)\nprint(labels_val.shape)\nprint(type(data_val))\nprint(type(labels_val))\n","metadata":{"_cell_guid":"55a04a87-6efd-4bb9-a298-b1db37378d2e","_uuid":"ec1da30b148afbf4a2cc0bb705257d05b44d1a9f","collapsed":true}},{"metadata":{"_cell_guid":"bb89f815-ce44-479f-b3bc-53929723c62f","_uuid":"39efba1314e5699e15c7a62da2480c1f14c39fa6"},"source":"## Set necessary paramaters/hyperparamaters","cell_type":"markdown"},{"execution_count":null,"outputs":[],"cell_type":"code","source":"print('designing model')\n# Training Parameters\nlearning_rate = 0.005\n# Network Parameters\nN_FEATURES=data_train.shape[1] # # Number of features for the input layer\nnum_classes = 1 # Linear\ndropout = 0.5 # Dropout, probability to keep units\nprint ('Num of features:' + str (N_FEATURES))","metadata":{"_cell_guid":"9e11311b-1904-416b-b481-7ee253fce27a","_uuid":"9dc858a7e9deac088bb1bc0688f48a28acc8187c","collapsed":true}},{"metadata":{"_cell_guid":"af60a383-192d-4e54-a849-a98c77a615a8","_uuid":"0f4f7f4ba9d7cf100c61657aedd14bada23d047c"},"source":"# PyTorch tensors","cell_type":"markdown"},{"execution_count":null,"outputs":[],"cell_type":"code","source":"# Convert the np arrays into the correct dimention and type\n# Note that BCEloss requires Float in X as well as in y\ndef XnumpyToTensor(x_data_np):\n    x_data_np = np.array(x_data_np, dtype=np.float32)        \n    print(x_data_np.shape)\n    print(type(x_data_np))\n\n    if use_cuda:\n        lgr.info (\"Using the GPU\")    \n        X_tensor = Variable(torch.from_numpy(x_data_np).cuda()) # Note the conversion for pytorch    \n    else:\n        lgr.info (\"Using the CPU\")\n        X_tensor = Variable(torch.from_numpy(x_data_np)) # Note the conversion for pytorch\n    \n    print(type(X_tensor.data)) # should be 'torch.cuda.FloatTensor'\n    print(x_data_np.shape)\n    print(type(x_data_np))    \n    return X_tensor\n\n\n# Convert the np arrays into the correct dimention and type\n# Note that BCEloss requires Float in X as well as in y\ndef YnumpyToTensor(y_data_np):    \n    y_data_np=y_data_np.reshape((y_data_np.shape[0],1)) # Must be reshaped for PyTorch!\n    print(y_data_np.shape)\n    print(type(y_data_np))\n\n    if use_cuda:\n        lgr.info (\"Using the GPU\")            \n    #     Y = Variable(torch.from_numpy(y_data_np).type(torch.LongTensor).cuda())\n        Y_tensor = Variable(torch.from_numpy(y_data_np)).type(torch.FloatTensor).cuda()  # BCEloss requires Float        \n    else:\n        lgr.info (\"Using the CPU\")        \n    #     Y = Variable(torch.squeeze (torch.from_numpy(y_data_np).type(torch.LongTensor)))  #         \n        Y_tensor = Variable(torch.from_numpy(y_data_np)).type(torch.FloatTensor)  # BCEloss requires Float        \n\n    print(type(Y_tensor.data)) # should be 'torch.cuda.FloatTensor'\n    print(y_data_np.shape)\n    print(type(y_data_np))    \n    return Y_tensor","metadata":{"_cell_guid":"0cfbb24b-947b-4e00-a1c8-0f3f0e5797cc","_uuid":"9ccbd470ff2fe126a4a5de420faa69eab449fbfb","collapsed":true}},{"metadata":{"_cell_guid":"e99ca51d-52e0-451c-b041-d91a49728948","_uuid":"1334e80f2f35179b386d0e9576324bc1b173263c"},"source":"## Design the  neural network  (very naive so that it can run here)\n","cell_type":"markdown"},{"execution_count":null,"outputs":[],"cell_type":"code","source":"DEBUG_ON=True\ndef debug(msg, x):\n    if DEBUG_ON:\n        print (msg + ', (size():' + str (x.size()))\n\ndropout = torch.nn.Dropout(0.3)\nrelu=torch.nn.LeakyReLU()\nN_HIDDEN=16\n\nnet_overfitting = torch.nn.Sequential(\n    torch.nn.Linear(N_FEATURES, N_HIDDEN),\n    torch.nn.ReLU(),\n    torch.nn.Linear(N_HIDDEN, N_HIDDEN),\n    torch.nn.ReLU(),\n    torch.nn.Linear(N_HIDDEN, 1),\n)\n\nnet_dropped = torch.nn.Sequential(\n    torch.nn.Linear(N_FEATURES, N_HIDDEN),\n    nn.BatchNorm1d(N_HIDDEN),\n    torch.nn.Dropout(0.3),\n    torch.nn.ReLU(),\n    torch.nn.Linear(N_HIDDEN, N_HIDDEN),\n    torch.nn.Dropout(0.3),\n    torch.nn.ReLU(),\n    torch.nn.Linear(N_HIDDEN, 1),\n)\n\nclass LinReg(nn.Module):    \n    def __init__(self, n_input, n_hidden, n_output):\n        super(LinReg, self).__init__()    \n        self.n_input=n_input\n        self.n_hidden=n_hidden\n        self.n_output= n_output \n                            \n        linear1=torch.nn.Linear(n_input,n_hidden)\n        torch.nn.init.xavier_uniform(linear1.weight)        \n        \n        linear2=torch.nn.Linear(n_hidden,1)\n        torch.nn.init.xavier_uniform(linear2.weight)        \n                \n        self.classifier = torch.nn.Sequential(\n#                                             linear1, nn.BatchNorm1d(n_hidden),dropout, relu,\n                                            linear1,dropout, relu,\n                                            linear2,              \n                                  )                                                                 \n    def forward(self, x):        \n#         debug('x',x)           \n        varSize=x.data.shape[0] # must be calculated here in forward() since its is a dynamic size                          \n        x=x.contiguous() \n        x=self.classifier(x)                   \n        return x\n    \nmodel=LinReg(N_FEATURES,N_HIDDEN,1)\n\nprint (model)\nprint(net_overfitting)\nprint(net_dropped)","metadata":{"_cell_guid":"8fd6bd75-c563-4059-8227-88c55e95236f","_uuid":"64a429641983054798ad95f6d51aeee390e677d7","collapsed":true}},{"metadata":{"_cell_guid":"c2767a6d-9134-48f0-8a0a-425dc5ad31ad","_uuid":"0a20d8c1b2feae0212c19e40e9276dd4a78fb6d6"},"source":"## Define the loss function\n\n","cell_type":"markdown"},{"execution_count":null,"outputs":[],"cell_type":"code","source":"criterion = torch.nn.MSELoss(size_average=True)\nprint (criterion)","metadata":{"_cell_guid":"86526176-f4b9-4a71-86f4-5906178e0600","_uuid":"62426df78525ac8f215b643167fbcc967369e77e","collapsed":true}},{"metadata":{"_cell_guid":"19adb2de-8691-4478-8c81-277d45a5a77c","_uuid":"4b98f46e4979267e0be9d3eb39640a4d302390ed"},"source":"## Define the Optimizer\n\nGradient descent is defined as the training method used to minimize the loss function","cell_type":"markdown"},{"execution_count":null,"outputs":[],"cell_type":"code","source":"optimizer = optim.Adam(model.parameters(), lr=learning_rate)\nprint (optimizer)","metadata":{"_cell_guid":"39549353-1452-44ba-8aee-e1e7f5edf95f","_uuid":"945e59241670fbdf8a3415da5505225030ef5afd","collapsed":true}},{"metadata":{"_cell_guid":"e66ea734-7dc1-4925-ae12-b8de831d986f","_uuid":"9528edb12282ac4ab480595e1fd7920fb0ea4111"},"source":"# Prepare the Tensors","cell_type":"markdown"},{"execution_count":null,"outputs":[],"cell_type":"code","source":"LR = 0.005\nBATCH_SIZE=32\nEPOCH = 10 \n\nimport gc\ndf_train=None\n\ngc.collect()\n\n\ndata_train = np.array(data_train, dtype=np.float32)\n\nlabels_train=labels_train.reshape((labels_train.shape[0],1)) # Must be reshaped for PyTorch!\nlabels_train = np.array(labels_train, dtype=np.float)\n\nX_tensor = (torch.from_numpy(data_train)).type(torch.FloatTensor) # Note the conversion for pytorch\nY_tensor = (torch.from_numpy(labels_train)).type(torch.FloatTensor) # Note the conversion for pytorch    \n\n\nimport torch.utils.data as Data\ndataset = Data.TensorDataset(data_tensor = X_tensor, target_tensor = Y_tensor)\nloader = Data.DataLoader(dataset = dataset, batch_size = BATCH_SIZE, shuffle = True, num_workers = 0)\n\nprint (loader)","metadata":{"_cell_guid":"7289bd5f-03b0-4eb0-9879-cefc8f5315a8","_uuid":"0da8af2f0ad74ff286f44ec18b3b51b788af6247","collapsed":true}},{"metadata":{"_cell_guid":"1ecdd6d6-7a02-4bd6-948d-deba69c9591d","_uuid":"a8589435c1cf182d04f8fa2af4d085c93adec52e"},"source":"## Train the model (you need a GPU for this, and increase EPOCHs)","cell_type":"markdown"},{"execution_count":null,"outputs":[],"cell_type":"code","source":"\nLR = 0.005\nBATCH_SIZE=32\nEPOCH = 10 \n\nnet_SGD = LinReg(N_FEATURES,N_HIDDEN,1)\nnet_Momentum = LinReg(N_FEATURES,N_HIDDEN,1)\nnet_RMSprop = LinReg(N_FEATURES,N_HIDDEN,1)\nnet_Adam = LinReg(N_FEATURES,N_HIDDEN,1)\n\n\nopt_SGD = torch.optim.SGD(net_SGD.parameters(), lr = LR)\nopt_Momentum = torch.optim.SGD(net_Momentum.parameters(), lr = LR, momentum = 0.9)\nopt_RMSprop = torch.optim.RMSprop(net_RMSprop.parameters(), lr = LR, alpha = 0.9)\nopt_Adam = torch.optim.Adam(net_Adam.parameters(), lr = LR, betas= (0.9, 0.99))\n\nloss_func = torch.nn.MSELoss()\n\nloss_SGD = []\nloss_Momentum = []\nloss_RMSprop =[]\nloss_Adam = []\n\n# losses = [loss_SGD, loss_Momentum, loss_RMSprop, loss_Adam]\nlosses = [loss_SGD, loss_Adam]\n# nets = [net_SGD, net_Momentum, net_RMSprop, net_Adam]\nnets = [net_SGD, net_Adam]\n# optimizers = [opt_SGD, opt_Momentum, opt_RMSprop, opt_Adam]\noptimizers = [opt_SGD, opt_Adam]\n\nprint (nets[0])\n\nfor epoch in range(0, EPOCH + 1):\n    print('Training Epoch= {}/{} '.format(epoch,EPOCH))                    \n    for step, (batch_x, batch_y) in enumerate(loader):\n        var_x = Variable(batch_x)\n        var_y = Variable(batch_y)\n        for net, optimizer, loss_history in zip(nets, optimizers, losses): \n#             print ('Model:' + type(net).__name__) \n#             print ('Opt:' + type(optimizer).__name__)\n            prediction = net(var_x)            \n            loss = loss_func(prediction, var_y)            \n            optimizer.zero_grad()            \n            loss.backward()            \n            optimizer.step()            \n            loss_history.append(loss.data[0])\n            \n#     if epoch % 5  == 0:        \n    loss_run = loss.data[0]                \n    print(step, loss_run)               \n    print('Training MSELoss=%.4f' % loss_run)                    ","metadata":{"_cell_guid":"6438b4c7-e84c-4f60-906f-a29bb0948975","_uuid":"8ca1fff8cc5aa4b7b56b308db119b1304627a885","collapsed":true}},{"metadata":{"_cell_guid":"2365a984-6550-45d6-8961-996281be384e","_uuid":"af1fd98d20452a58ca1495fe3e91641b1bda41b6"},"source":"# Visualize Loss Graph using Visdom¶\n### Make sure you have Visdom installed and running\n- pip install visdom\n- python -m visdom.server &","cell_type":"markdown"},{"execution_count":null,"outputs":[],"cell_type":"code","source":"%matplotlib inline\nimport matplotlib.pyplot as plt\n\nlabels = ['SGD', 'Adam']\n          \nfor i, loss_history in enumerate(losses):\n    plt.plot(loss_history, label = labels[i])\n          \nplt.legend(loc = 'best')\nplt.xlabel('Steps')\nplt.ylabel('Loss')\nplt.ylim((0, 0.2))\nplt.show()","metadata":{"_cell_guid":"a9799b4d-014a-4429-8a69-a907c0290e03","_uuid":"742cab678c9d01dcdededb8e74d04db5732375ec","collapsed":true}},{"execution_count":null,"outputs":[],"cell_type":"code","source":"%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.plot(loss_history)\nplt.show()\n\n# # # ! pip install visdom\n\n# # from visdom import Visdom\n# # viz = Visdom()\n\n# # num_epoch=int(epochs/div_factor)\n\n# # x = np.reshape([i for i in range(num_epoch)],newshape=[num_epoch,1])\n# # loss_data = np.reshape(loss_arr,newshape=[num_epoch,1])\n\n# # win3=viz.line(\n# #     X = x,\n# #     Y = loss_data,\n# #     opts=dict(\n# #         xtickmin=0,\n# #         xtickmax=num_epoch,\n# #         xtickstep=1,\n# #         ytickmin=0,\n# #         ytickmax=20,\n# #         ytickstep=1,\n# #         markercolor=np.random.randint(0, 255, num_epoch),\n# #     ),\n# # )","metadata":{"_cell_guid":"e801d2b7-668d-4a7c-ae5f-47f6641f4709","_uuid":"3256a95e9378c2b02c0d0ff410326c1d74bb846e","collapsed":true}},{"metadata":{"_cell_guid":"685ee3e5-5f3c-4d40-919a-cbed96b99987","_uuid":"fbf7c99c8441d49a6bf2f1475b9926a1ccf04728"},"source":"## Prepare the test data\n\n","cell_type":"markdown"},{"execution_count":null,"outputs":[],"cell_type":"code","source":"# df_test.to_csv('test_clean.csv', header=False,  index = False)    \n# df_test= pd.read_csv('test_clean.csv', header=None, dtype=np.float32)    \n# feature_cols = list(df_train.columns[:-1])\n# print (feature_cols)\n# trainX = df_test[feature_cols]]","metadata":{"_cell_guid":"76093cf8-ef5a-4065-b6b9-9441192838cb","_uuid":"0b2c63dd7b23e0964651ec584af5f9acc0d6c926","collapsed":true}},{"metadata":{"_cell_guid":"1c0c8b85-4345-4e93-8bc8-452e7f2c91c3","_uuid":"48d9669fd271c245b96945f4b5eaae608978e6b1"},"source":"## Make predictions\n","cell_type":"markdown"},{"execution_count":null,"outputs":[],"cell_type":"code","source":"\nprint('making predictions\\n')\n","metadata":{"_cell_guid":"77a84110-23dd-4ec5-afdf-3838099a608f","_uuid":"671e1d099596d27cb8e4f73dde05a2da551b1425","collapsed":true}},{"metadata":{"_cell_guid":"ceec4e8b-b660-48cc-ada7-6738f73a634b","_uuid":"8cee5a4aa9ad034a1e4b96cf461ced1572491b06"},"source":"## Write output to file\n\nLastly we take the predictions and construct a dataframe which we output to a .csv and can then submit for evalutation!","cell_type":"markdown"},{"execution_count":null,"outputs":[],"cell_type":"code","source":"import pandas as pd\nx=pd.read_csv ('../input/0609034-0608800-submission/0.609034_0.608800_submission.csv')\nx.to_csv('sample_submission.csv', index=False)","metadata":{"_cell_guid":"0e0ceb34-138b-4e7b-a5a1-5b82d398f328","_uuid":"d2808efad7d4b3eef78c657e7d06ed6ae003eb76","collapsed":true}},{"metadata":{"_cell_guid":"9d8d2289-7cc7-41c7-9273-37d2900f8d62","_uuid":"55de4dbef7c3f1ed61bcd9f9a15601248c7888d2"},"source":"","cell_type":"markdown"}],"nbformat":4,"nbformat_minor":1,"metadata":{"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","nbconvert_exporter":"python","mimetype":"text/x-python","version":"3.6.3","pygments_lexer":"ipython3","name":"python"}}}