{"cells":[{"metadata":{"_uuid":"97d6f1fd821fbd87ce7f6c03ef5f9a64e4d7e6e5","trusted":true},"cell_type":"code","source":"#importing various packages that I will use\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import GradientBoostingRegressor\nimport re\nfrom sklearn.model_selection import KFold\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e272cb03525e7c4cc66c84d1c6402be5b32806f8"},"cell_type":"markdown","source":"# Basic EDA"},{"metadata":{"_uuid":"59414109f217a867f209ebe4a9229f7f5fc874f1","trusted":true},"cell_type":"code","source":"#Loading the Datasets I will be doing analysis on\ntrain = pd.read_table('../input/train.tsv')\ntest = pd.read_table('../input/test_stg2.tsv')\nprint('Done With Section 1')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"594bf17a2173d90a587b983449b862e6fba96191","trusted":true},"cell_type":"code","source":"train['is_train'] = 1\ntest['is_train'] = 0\nmerged = pd.concat([train.drop(['train_id','price'], axis=1), test.drop('test_id', axis=1)], axis=0)\nprint('Done With Section 2')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4869b2af92e70cb707a72cc21e446b19401cfa34","trusted":true},"cell_type":"code","source":"#getting information on the data and dropping outlier values after looking at the percentage of data I am losing\nprint(train.dtypes)\n#print(train[train['price'] == 0.0].shape[0]/train.shape[0])\ntrain = train[train['price']>0.0]\ntrain.shape\nprint('Done With Section 3')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6cc2be77dce16e2a51be1111d794e8a6a47650a2","trusted":true},"cell_type":"code","source":"#Transformed the data to a log transformation to make the target variable normalized\ntrain['LogPrice'] = np.log2(train['price'])\nplt.hist(train['LogPrice'], bins=20, edgecolor='black')\nplt.title('Log Price Distribution of Mercari Items')\nplt.show()\nprint('Done With Section 4')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c9e95528a556cda5780dd9966f6f56b87ba3ef1d","trusted":true},"cell_type":"code","source":"# Breaking up the Category Section from 'X/X/X' format to a three categories and handling the missing information on category\ntrain['category_name']=train['category_name'].fillna('Missing/Missing/Missing')\ncategories = train['category_name']\ntargets = categories.str.split('/',2)\nmain = []\nsub1 = []\nsub2 = []\nfor i in targets:\n    a,b,c = i\n    main.append(a)\n    sub1.append(b)\n    sub2.append(c)\ntrain['Main'], train['Sub1'], train['Sub2'] = main, sub1, sub2\nprint('Done With Section 5')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"675091fb6eca81ab5ef199040cff3c8f74964c93","trusted":true},"cell_type":"code","source":"# dropped category_name due to redundant data\ntrain = train.drop('category_name', axis=1)\nprint('Done With Section 6')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a5e11ddf2a243397f796ce40a875901f788a3dce"},"cell_type":"markdown","source":"# Modeling And Extra Feature Engineering"},{"metadata":{"trusted":true,"_uuid":"181e1d495eccd89c059dd4f16b1950916d22bbb1"},"cell_type":"code","source":"train['MainSub1'] = (train['Main'] + '|' + train['Sub1']).values\ntrain['MainSub1Sub2'] = (train['Main'] + '|' + train['Sub1'] +'|'+ train['Sub2']).values\nprint('Done With Section 7')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b31488844ad10859bd008da3b924383ef69fe0b1","trusted":true},"cell_type":"code","source":"#Looking at the median prices of each Main category and creating a dictionary based on it\ntbl1 = train.groupby(['Main'])['LogPrice'].median().reset_index()\ntbl1dict = dict(zip(tbl1['Main'], tbl1['LogPrice']))\nprint('Done With Section 8')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7f560cec25a416dc879532c4424ebdec11cab605","trusted":true},"cell_type":"code","source":"#Looking at the median prices of each Sub1 category and creating a dictionary based on it\n\ntbl2 = train.groupby(['Main','Sub1'])['LogPrice'].median().reset_index()\ncomb = []\nfor i, j in tbl2.iterrows():\n    both = j['Main'] + '|' + j['Sub1']\n    comb.append(both)\ntbl2['comb'] = comb\ntbl2dict = dict(zip(tbl2['comb'], tbl2['LogPrice']))\ntbl2dict\nprint('Done With Section 9')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"38ad7c9468249a5ec11c6271b4a422fed6bb4d07","trusted":true},"cell_type":"code","source":"#Looking at the median prices of each Sub 2 category and creating a dictionary based on it\n\ntbl3 = train.groupby(['Main','Sub1','Sub2'])['LogPrice'].median().reset_index()\ncomb = []\nfor i, j in tbl3.iterrows():\n    both = j['Main'] + '|' + j['Sub1'] + '|' + j['Sub2']\n    comb.append(both)\ntbl3['comb'] = comb\ntbl3dict = dict(zip(tbl3['comb'], tbl3['LogPrice']))\ntbl3dict\nprint('Done With Section 10')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e8ba423353440c8d781faf6148d81a3a72f5d1b5","trusted":true},"cell_type":"code","source":"#resets the index of the train set as some of the rows have been removed so I can iterate through the dataframe\ntrain = train.reset_index().drop('index',axis=1)\nprint('Done With Section 11')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0a867db9bb8d8433b4d935ade381157822aa102b"},"cell_type":"markdown","source":"# Median Encoding\n\n## Assigning a encoded value for the Main, Sub1, Sub2 categories so I can use these values to do modeling analysis on the items to predict the prices from the test set items "},{"metadata":{"trusted":true,"_uuid":"c14c216805525da9f7ffae4ba82d2a482c5d53ec"},"cell_type":"code","source":"train['MainMed']= train['Main'].map(tbl1dict)\ntrain['Sub1Med']= train['MainSub1'].map(tbl2dict)\ntrain['Sub2Med']= train['MainSub1Sub2'].map(tbl3dict)\nprint('Done With Section 12')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"df1633b9375e60b411fb527d2b797e83f69fc324","trusted":true},"cell_type":"code","source":"#adding the median encoded variables and category names for each item in the Test dataset\n\ntest['category_name']=test['category_name'].fillna('Missing/Missing/Missing')\ncategories = test['category_name']\ntargets = categories.str.split('/',2)\nmain = []\nsub1 = []\nsub2 = []\nfor i in targets:\n    a,b,c = i\n    main.append(a)\n    sub1.append(b)\n    sub2.append(c)\ntest['Main'], test['Sub1'], test['Sub2'] = main, sub1, sub2\nprint('Done With Section 13')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"02852af393bdd98bd9478c98975c380b1ba51a07"},"cell_type":"code","source":"test['MainSub1'] = (test['Main'] + '|' + test['Sub1']).values\ntest['MainSub1Sub2'] = (test['Main'] + '|' + test['Sub1']+ '|' + test['Sub2']).values\nprint('Done With Section 14')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b735dd0726b80471837cbbea9d5bace7ac829938"},"cell_type":"code","source":"test['MainMed']= test['Main'].map(tbl1dict)\ntest['Sub1Med']= test['MainSub1'].map(tbl2dict)\ntest['Sub2Med']= test['MainSub1Sub2'].map(tbl3dict)\ntest['Sub2Med'] = test['Sub2Med'].fillna(test['Sub1Med'])\nprint('Done With Section 15')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5b2c74778be210cc18d8a237b2cc3dd714025947"},"cell_type":"markdown","source":"# Method 1: Median Value of the Sub 2 Category"},{"metadata":{"_uuid":"5132ff2ad963fe19543d55490f40b85284b4a57f"},"cell_type":"markdown","source":"# Method 2: Creating Decision Tree Regressor"},{"metadata":{"_uuid":"574e3711b1dfac654e5fcafc5500265fe7c5c907"},"cell_type":"markdown","source":"# Method 3: Random Forest Feature Importance"},{"metadata":{"_uuid":"67b59d5061ed497573c67bb6ff6e3cba3a75d310"},"cell_type":"markdown","source":"# Method 4: Boosting"},{"metadata":{"_uuid":"d8a5151e28bacb217f6fd358a89af08c89972079"},"cell_type":"markdown","source":"# Brand Data"},{"metadata":{"_uuid":"7216d68d7c0b416730109093fb3dd1f82931d388","trusted":true},"cell_type":"code","source":"train['brand_name'] = train['brand_name'].fillna('missing')\ntest['brand_name'] = test['brand_name'].fillna('missing')\ntrain.groupby('brand_name')['LogPrice'].median().reset_index()\nprint('Done With Section 16')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"691ef9f006682057eb4ed2744b5a698a507e860a","trusted":true},"cell_type":"code","source":"#train['brand_name']=train['brand_name'].fillna('missing')\nbrandprice = train.groupby('brand_name')['LogPrice'].median().reset_index()\n#brandcounts = train.groupby('brand_name')['train_id'].count().reset_index()\nbrandprice['LogPrice'] = brandprice['LogPrice'].fillna(0.0)#brandprice['LogPrice']\nplt.hist(brandprice['LogPrice'], bins=30, edgecolor='black')\n#plt.boxplot(brandcounts['train_id'])\nplt.title('Log Price Distribution of Mercari Items')\nplt.show()\nprint('Done With Section 17')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ee839b0fb4bbc8928c55b979fd3771d98001bc02","trusted":true},"cell_type":"code","source":"brandprice['Category'] = pd.cut(brandprice['LogPrice'], bins=5, labels=[1,2,3,4,5])\nbrandprice\nprint('Done With Section 18')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5bff2a7a3d237c9b865121429e73b12a2f12f530","trusted":true},"cell_type":"code","source":"branddict = dict(zip(brandprice['brand_name'], brandprice['Category']))\ntrain['brand_category'] = train['brand_name'].map(branddict)\ntest['brand_category'] = test['brand_name'].map(branddict)\ntest['brand_category'] = test['brand_category'].fillna(3)\nprint('Done With Section 19')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"563f0149ab0b0aa0080b70f9d4aeea553481709f","trusted":true},"cell_type":"code","source":"merged['brand_name'] = merged['brand_name'].fillna('missing')\nbrandcounts = merged.groupby('brand_name')['name'].count().reset_index()\nbrandcounts.describe\nbins = [0, 2, 7, 41, 2000000000000]\nfreqbin = pd.cut(brandcounts['name'], bins, right=False, labels=[1,2,3,4])\nbrandcounts['brand_frequency'] = freqbin\nbrandfreqdict = dict(zip(brandcounts['brand_name'], brandcounts['brand_frequency']))\nbrandfreqdict\nprint('Done With Section 20')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2a5ebbece19c40b3a06fdf6e0227269391419632"},"cell_type":"code","source":"train['brand_freq'] = train['brand_name'].map(brandfreqdict)\ntest['brand_freq'] = test['brand_name'].map(brandfreqdict)\ntest['brand_freq'] = test['brand_freq'].fillna(1)\nprint('Done With Section 21')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"32af567af301d46b5d2c645837dac09131279b83"},"cell_type":"markdown","source":"# Item Description"},{"metadata":{"_uuid":"f5519dcce453c340daf5f37b43ca619cd0397c05","trusted":true},"cell_type":"code","source":"a = train['item_description'].fillna('').apply(lambda x: str(re.sub('[^ a-zA-Z0-9]','',x)))\nb = test['item_description'].fillna('').apply(lambda x: str(re.sub('[^ a-zA-Z0-9]','',x)))\nprint('Done With Section 22')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"afba28029d4239325cc7c00925ed196bf7740d71"},"cell_type":"code","source":"tfidf = TfidfVectorizer(max_features=100, stop_words='english')\nf = tfidf.fit_transform(a.values)\nprint('Done With Section 23')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"974d8c6ea3cde98cc83ae13d0269a8e90fe3f2e5"},"cell_type":"code","source":"ftest = tfidf.transform(b.values)\nprint('Done With Section 24')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a37bc0efe9c9542f4fc22269c0578030538e63a8"},"cell_type":"code","source":"ftest2 = ftest.toarray()\nprint('Done With Section 25')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"079e623ced949140171ea9b4a2de15eb578bb909"},"cell_type":"code","source":"#tfidf.get_feature_names()\nf2 = f.toarray()\nprint('Done With Section 26')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fbfdc451431d481ea41f5cf4d7525ae12def2fd3"},"cell_type":"code","source":"scores = np.array([sum(x) for x in f2])\nprint('Done With Section 27')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5f3ffdbd9211db96dd7e0bbeb6afc74fdd2266bc"},"cell_type":"code","source":"train['idf'] = scores\nprint('Done With Section 28')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b4b4e4633b9c893f1879be38296c792a3d17f8cd"},"cell_type":"code","source":"test['idf'] = np.array([sum(x) for x in ftest2])\nprint('Done With Section 29')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"72d12428a40fa5b4870d7bb8faed1eec3dc153c2"},"cell_type":"code","source":"rfcols = train[['item_condition_id','LogPrice','shipping', 'MainMed', 'Sub1Med', 'Sub2Med', 'brand_category', 'brand_freq', 'idf']]\ny2=rfcols['LogPrice']\nX2 = rfcols.drop('LogPrice', axis=1)\nXTest = test[['item_condition_id','shipping', 'MainMed', 'Sub1Med', 'Sub2Med', 'brand_category', 'brand_freq', 'idf']]\nrf2 = RandomForestRegressor()\nrf2.fit(X2,y2)\npred = rf2.predict(XTest)\nprint('Done With Section 30')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b57626adc28d0f9303c0da4b2594df437906fa55"},"cell_type":"code","source":"test['price'] = 2**pred\ntest_sub = test[['test_id', 'price']]\ntest_sub.to_csv('./testsub.csv', index=False)\nprint('Done')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7002eba637f6ab5295ed56aaff8a57e2c1e1db1c"},"cell_type":"markdown","source":"# In Progress\n\n- Implementing Method 3 and seeing how much it improves the RMSLE score\n- Doing more Natural Language Processing to gain more insights on Brand and Item Description Information for Feature Engineering\n    - Understanding different NLP techniques such as CountVectorizer, tfidfVectorizer, Topic Modelling, N-Gram to implement\n- Repeating first 3 methods to see if extra features improve RMSLE score\n- Continue fixing models and using different methods to create models\n- Understand test dataset better\n- look at competitor websites to get better ideas on new features"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}