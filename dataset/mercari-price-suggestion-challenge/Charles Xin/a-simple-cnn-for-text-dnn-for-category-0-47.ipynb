{"cells":[{"outputs":[],"source":"import numpy as np \nimport pandas as pd \nfrom keras import backend as K\nimport gc\nimport time\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import LabelBinarizer\nfrom keras.preprocessing.text import Tokenizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.cross_validation import train_test_split\nfrom keras.preprocessing.sequence import pad_sequences\n\ndef rmsle_cust(y_true, y_pred):\n    first_log = K.log(K.clip(y_pred, K.epsilon(), None) + 1.)\n    second_log = K.log(K.clip(y_true, K.epsilon(), None) + 1.)\n    return K.sqrt(K.mean(K.square(first_log - second_log), axis=-1))\n\ndef split_cat(text):\n    try: return text.split(\"/\")\n    except: return (\"No Label\", \"No Label\", \"No Label\")\n \n\ndef handle_missing_inplace(dataset):\n    dataset['general_cat'].fillna(value='No Label', inplace=True)\n    dataset['subcat_1'].fillna(value='No Label', inplace=True)\n    dataset['subcat_2'].fillna(value='No Label', inplace=True)\n    dataset['brand_name'].fillna(value='missing', inplace=True)\n    dataset['item_description'].fillna(value='No description yet', inplace=True)\n\n\ndef cutting(dataset):\n    pop_brand = dataset['brand_name'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_BRANDS]\n    dataset.loc[~dataset['brand_name'].isin(pop_brand), 'brand_name'] = 'missing'\n    pop_category1 = dataset['general_cat'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_CATEGORIES]\n    pop_category2 = dataset['subcat_1'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_CATEGORIES]\n    pop_category3 = dataset['subcat_2'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_CATEGORIES]\n    dataset.loc[~dataset['general_cat'].isin(pop_category1), 'general_cat'] = 'missing'\n    dataset.loc[~dataset['subcat_1'].isin(pop_category2), 'subcat_1'] = 'missing'\n    dataset.loc[~dataset['subcat_2'].isin(pop_category3), 'subcat_2'] = 'missing'\n\n\ndef to_categorical(dataset):\n    dataset['general_cat'] = dataset['general_cat'].astype('category')\n    dataset['subcat_1'] = dataset['subcat_1'].astype('category')\n    dataset['subcat_2'] = dataset['subcat_2'].astype('category')\n\n\ndef raw_text(dataset):   \n    raw_text = np.hstack([dataset.item_description.str.lower(), dataset.name.str.lower()])\n    tok_raw = Tokenizer(num_words=20000,\n                    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n                    lower=True,\n                    split=\" \",\n                    char_level=False)\n    tok_raw.fit_on_texts(raw_text)\n    dataset[\"seq_item_description\"] = tok_raw.texts_to_sequences(dataset.item_description.str.lower())\n    dataset[\"seq_name\"] = tok_raw.texts_to_sequences(dataset.name.str.lower())\n    dataset[\"Raw Text Combined\"] = dataset.seq_name + dataset.seq_item_description\n\n\ndef get_keras_data(dataset):\n    X = {\n        'name': pad_sequences(dataset.seq_name, maxlen=10)\n        ,'item_desc': pad_sequences(dataset.seq_item_description, maxlen=75)\n        ,'brand_name': np.array(dataset.brand_name)\n        ,'general_cat': np.array(dataset.general_cat)\n        ,'subcat_1': np.array(dataset.subcat_1)\n        ,'subcat_2': np.array(dataset.subcat_2)\n        ,'item_condition': np.array(dataset.item_condition_id)\n        ,'num_vars': np.array(dataset.shipping)\n    }\n    return X\n\nNUM_BRANDS = 4000\nNUM_CATEGORIES = 1000\n","execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"9aca301e-eaa5-41ef-a98e-0d31feb4b07a","_uuid":"5accf954b770969974dc8ebbe0b7e77bd0bdf6f2"}},{"outputs":[],"source":"#LOAD DATA\ntrain = pd.read_table(\"../input/train.tsv\")\ntest = pd.read_table(\"../input/test.tsv\")\n\nprint(train.shape)\nprint(test.shape)\ntrain.head(3)","execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"4be498b6-6a98-4682-85c6-e98399365aee","scrolled":true,"_uuid":"4822739212490e805076eaee586585f857b025a2"}},{"outputs":[],"source":"start_time = time.time()\n\nnrow_train = train.shape[0]\nmerge: pd.DataFrame = pd.concat([train, test])\nsubmission: pd.DataFrame = test[['test_id']]\n\ndel train\ndel test\ngc.collect()\n\nmerge['general_cat'], merge['subcat_1'], merge['subcat_2'] = \\\nzip(*merge['category_name'].apply(lambda x: split_cat(x)))\nmerge.drop('category_name', axis=1, inplace=True)\nprint('[{}] Split categories completed.'.format(time.time() - start_time))\n\nhandle_missing_inplace(merge)\nprint('[{}] Handle missing completed.'.format(time.time() - start_time))\n\ncutting(merge)\nprint('[{}] Cut completed.'.format(time.time() - start_time))\n\nto_categorical(merge)\nprint('[{}] Convert categorical completed'.format(time.time() - start_time))\n\nraw_text(merge)\nprint('[{}] Raw text completed'.format(time.time() - start_time))\n\nle = LabelEncoder()\nmerge.brand_name = le.fit_transform(merge.brand_name)\nmerge.general_cat = le.fit_transform(merge.general_cat)\nmerge.subcat_1 = le.fit_transform(merge.subcat_1)\nmerge.subcat_2 = le.fit_transform(merge.subcat_2)\nprint('[{}] category variable labelled completed'.format(time.time() - start_time))\n\nmerge.head(3)","execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"c4998727-9ada-4602-87a3-fbb6844ec478","scrolled":true,"_uuid":"a436779f7373b73a57b7f271c37ba53c6e0ad69c"}},{"outputs":[],"source":"#EXTRACT DEVELOPTMENT TEST\ndtest = merge.iloc[nrow_train:, ]\ndtrain, dvalid = train_test_split(merge.iloc[:nrow_train, ], random_state=123, train_size=0.7)\nprint(dtrain.shape)\nprint(dvalid.shape)\n\n\nX_train = get_keras_data(dtrain)\nX_valid = get_keras_data(dvalid)\nX_test = get_keras_data(dtest)\n\nY_train =  np.log1p(np.array(dtrain.price))\nY_valid =  np.log1p(np.array(dvalid.price))","execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"351c015d-e47c-4034-a057-0fa509106bd5","_uuid":"536582c413553f5a7a1d26a840a2ee671222e27d"}},{"source":"**Model Part **","cell_type":"markdown","metadata":{"_cell_guid":"9a7e6575-3373-49a8-819f-278667e41b14","_uuid":"17275e93fdb0e5fd4749b09edf10e2b4d776c2f5"}},{"outputs":[],"source":"MAX_TEXT = np.max([np.max(merge.seq_name.max()), np.max(merge.seq_item_description.max())])+2\nMAX_general_cat = np.max([merge.general_cat.max()])+1\nMAX_subcat_1 = np.max([merge.subcat_1.max()])+1\nMAX_subcat_2 = np.max([merge.subcat_2.max()])+1\nMAX_BRAND = np.max([merge.brand_name.max()])+1\n\nprint(MAX_TEXT)\nprint(MAX_general_cat)\nprint(MAX_subcat_1)\nprint(MAX_subcat_2)\nprint(MAX_BRAND)","execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"e8a01894-1bb3-4e92-a143-b6d3ee1b51e3","_uuid":"a0d9e074a287ba583375ba29e6320eb789b58537"}},{"outputs":[],"source":"#KERAS MODEL DEFINITION\nfrom keras.layers import Input, Dropout, Dense, BatchNormalization, Activation, concatenate, Conv1D, GlobalMaxPooling1D, Embedding, Flatten, BatchNormalization\nfrom keras.models import Model\nfrom keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\nfrom keras import backend as K\n\ndef get_callbacks(filepath, patience=2):\n    es = EarlyStopping('val_loss', patience=patience, mode=\"min\")\n    msave = ModelCheckpoint(filepath, save_best_only=True)\n    return [es, msave]\n\ndef rmsle_cust(y_true, y_pred):\n    first_log = K.log(K.clip(y_pred, K.epsilon(), None) + 1.)\n    second_log = K.log(K.clip(y_true, K.epsilon(), None) + 1.)\n    return K.sqrt(K.mean(K.square(first_log - second_log), axis=-1))\n\ndef get_model():\n    #params\n    dr_r = 0.5\n    \n    #Inputs\n    name = Input(shape=[X_train[\"name\"].shape[1]], name=\"name\")\n    item_desc = Input(shape=[X_train[\"item_desc\"].shape[1]], name=\"item_desc\")\n    brand_name = Input(shape=[1], name=\"brand_name\")\n    general_cat = Input(shape=[1], name=\"general_cat\")\n    subcat_1 = Input(shape=[1], name=\"subcat_1\")\n    subcat_2 = Input(shape=[1], name=\"subcat_2\")\n    item_condition = Input(shape=[1], name=\"item_condition\")\n    num_vars = Input(shape=[1], name=\"num_vars\")\n    \n    #Embeddings layers\n    emb_name = Embedding(MAX_TEXT, 10)(name)\n    emb_item_desc = Embedding(MAX_TEXT, 10)(item_desc)\n    emb_brand_name = Embedding(MAX_BRAND, 50)(brand_name)\n    emb_general_cat = Embedding(MAX_general_cat, 10)(general_cat)\n    emb_subcat_1 = Embedding(MAX_subcat_1, 20)(subcat_1)\n    emb_subcat_2 = Embedding(MAX_subcat_2, 30)(subcat_2)\n \n    #rnn layer\n    cnn_layer1 = Conv1D(filters=16, kernel_size=3, activation='relu') (emb_item_desc)\n    cnn_layer2 = Conv1D(filters=8, kernel_size=3, activation='relu')(emb_name)\n    \n    cnn_layer1 = GlobalMaxPooling1D()(cnn_layer1)\n    cnn_layer2 = GlobalMaxPooling1D()(cnn_layer2)\n    \n    #main layer\n    main_l = concatenate([\n        Flatten() (emb_brand_name)\n        , Flatten() (emb_general_cat)\n        , Flatten() (emb_subcat_1)\n        , Flatten() (emb_subcat_2)\n        , cnn_layer1\n        , cnn_layer2\n        , num_vars\n        , item_condition\n    ])\n    \n    main_l = Dropout(dr_r) (Dense(256, activation=\"relu\") (main_l))\n    main_l = Dropout(dr_r) (Dense(128, activation=\"relu\") (main_l))\n    main_l = Dropout(dr_r) (Dense(64, activation=\"relu\") (main_l))\n    \n    \n    #output\n    output = Dense(1, activation=\"linear\") (main_l)\n    \n    #model\n    model = Model([name, item_desc, brand_name, general_cat, subcat_1, subcat_2, item_condition, num_vars], output)\n    \n    model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mae\", rmsle_cust])\n    \n    return model\n\n    \nmodel = get_model()\nmodel.summary()","execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"84395024-9994-48cd-ba88-664f2972c141","_uuid":"456bf6d0b1aebb88ffe0e6b015c89e7a8242a67e"}},{"outputs":[],"source":"#FITTING THE MODEL\nBATCH_SIZE = 20000\nepochs = 25\n\nmodel = get_model()\nmodel.fit(X_train, Y_train, epochs=epochs, batch_size=BATCH_SIZE\n          , validation_data=(X_valid, Y_valid)\n          , verbose=1)","execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"2e8cd21e-7f46-40e2-8ff6-cfc2b672fea6","_uuid":"cc95cb40827349dfa143e8ea3380f13a00cbccbb"}},{"outputs":[],"source":"#CREATE PREDICTIONS\npreds = model.predict(X_test, batch_size=BATCH_SIZE)\nsubmission[\"price\"] = np.expm1(preds)\nsubmission.to_csv(\"./myNNsubmission.csv\", index=False)","execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"61faf29f-0476-4fda-83cd-7de7e432a1e7","_uuid":"94f886da086d2bb8fd1200436ed9f50a61eef595"}},{"outputs":[],"source":"","execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"f36c110b-52d8-4e09-99dc-0519e4deb55e","_uuid":"fd62c91274546039b1227d70b5da3ee6ab96c66e"}}],"nbformat_minor":1,"nbformat":4,"metadata":{"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"},"language_info":{"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.3","pygments_lexer":"ipython3"}}}