{"cells":[{"metadata":{},"cell_type":"markdown","source":"In this blogpost, I'll try to explain pandas and seaborn for exploring mercari price prediction data.\nAfter going through numerous kaggle kernels, video lectures and blogs this is what I have learnt about the most primary use cases of the two libraries. Pandas and seaborn!"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n!pip install git+https://github.com/LIAAD/yake\n# For example, here's several helpful packages to load in \nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction import stop_words\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport multiprocessing as mp\n\nimport string\nimport spacy \nimport en_core_web_sm\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.base import TransformerMixin, BaseEstimator\nfrom collections import Counter\nfrom wordcloud import WordCloud\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\n\nimport string\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re\nimport plotly.graph_objs as go\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.\n\npkmn_type_colors = ['#78C850',  # Grass\n                    '#F08030',  # Fire\n                    '#6890F0',  # Water\n                    '#A8B820',  # Bug\n                    '#A8A878',  # Normal\n                    '#A040A0',  # Poison\n                    '#F8D030',  # Electric\n                    '#E0C068',  # Ground\n                    '#EE99AC',  # Fairy\n                    '#C03028',  # Fighting\n                    '#F85888',  # Psychic\n                    '#B8A038',  # Rock\n                    '#705898',  # Ghost\n                    '#98D8D8',  # Ice\n                    '#7038F8',  # Dragon\n                   ]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Applying pandas methods for EDA"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_data = pd.read_table(\"../input/train.tsv\")\ntest_data = pd.read_table(\"../input/test_stg2.tsv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#As the size of the dataset could be too large to be able to run this notebook quickly,\n#we have selected first 20K points for some of the experiments \ntrain_data_partial = train_data.loc[0:20000,:].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"In any dataframe the very primary checks or tasks which we can perform with pandas are enlisted below!\n"},{"metadata":{},"cell_type":"markdown","source":"Lets divide analysis into parts\n0. overall analysis\n1. Categorical variables : (i) shipping (ii) condition_id (iii) brand_name\n2. nlp data : (i) item_description (ii) name (iii) category_name\n3. float : price\n4. joint relations\n\n# 0. Overall analysis\n1. Check the shape of the data\n2. dtypes\n3. head\n4. missing values\n5. Describe\n6. Duplicate values"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"train data shape = \",train_data.shape)\nprint(\"test data shape = \", test_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.isnull().sum()/train_data.shape[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can infer than brand name is null in almost 43% of the datapoints. So if we are to reduce the dataset size to half due to computational constraints, these can be omitted instead of random sampling "},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"42% of the datapoints do not have a brand name"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.describe(include = 'all')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusions 0.5:\n1. Most of the names are unique\n2. fewer items with condition 4,5\n3. Women's clothing is the topmost category\n4. 4809 unique brands. (I had previously checked if the brand name needs preprocessing for consistent wordings, ie brand pink and PInk are same. But the dataset has reliable brand name values. 849853 null values\n5. skewed distribution of price\n6. Shipping, almost balanced distribution\n7. No desription yet is the most common value. thus 5% of the datapoints don't have a description. Instead of no description, we might add missing for better performance\n\n### relevant questions we might wanna ask\n1. What is the distribution of price, for items without description?\n2. Does it make sense to predict brand name for missing values prior to the final modeling\n3. does category and brand name share very strong mapping/ corelation? (highly probable)\n4. Shipping and brand name can also be corelated, ie some brands always ship their products \n5. Would log transformation of price to the model improve results?"},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"## 0.6 Duplicates\nHow do we exactly define duplicates. \n(name, description, brand, shipping and price) could be one of the choices.\nif the condition of the items is different, for same 4 of above, the data will be inconsistent. In such cases, one of the duplicated rows could be omitted. \nCategory of same items could be mistakenly different. Thus it doesn't make sense to add it in the check of uniqueness \n\nSome level of processing on nlp features is required before choosing if they match! \n1. lowercase\n2. punctuation , tags removal, removing numbers could be risky. 24 carrot gold and 18 carrot gold. or the quantity of an item could be specified in some the descriptions\nAs we might need word count for further analysis, lets store it in a separate variable.\n\nSome of the words are removed from the stop_words as the removal of negative words, might be misleading for the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.duplicated(subset = ['name', 'item_description', 'brand_name', 'shipping','category_name', 'price']).sum()  #name, description, brand, shipping and price","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.duplicated(subset = ['item_description', 'shipping', 'price']).sum()  #name, description, brand, shipping and price","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.duplicated(subset = ['name', 'brand_name', 'shipping', 'price']).sum()  #name, description, brand, shipping and price","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data[train_data.duplicated(subset = ['name', 'brand_name', 'category_name', 'shipping', 'price'])].head()  #name, description, brand, shipping and price","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data[(train_data.name == \"Giffin 25 rdta full tank kit\") & (train_data.price == 25.0) ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_data.loc[1199,'item_description'])\nprint(train_data.loc[1630,'item_description'])\n#mostly same items!","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets try couple of more\ntrain_data[(train_data.name == \"Too Faced Better Than Sex Mascara\") & (train_data.price == 18.0) ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data[(train_data.name == \"PINK by Victoria's Secret lace bandeau\") & (train_data.price == 7.0) ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_data.loc[53,'item_description'])\nprint(train_data.loc[3329,'item_description'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.1 Target Variable : Price"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.price.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Price 0 is not possible, thus we will remove those points\ntrain_data = train_data[train_data.price!=0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(20, 10))\nplt.subplot(1, 2, 1)\nsns.distplot(train_data['price'], bins = 50)\nplt.xlabel('price+', fontsize=17)\nplt.ylabel('frequency', fontsize=17)\nplt.tick_params(labelsize=15)\nplt.title('Price Distribution - Training Set', fontsize=17)\n\nplt.subplot(1, 2, 2)\nsns.distplot(np.log(train_data['price'] + 1), bins = 50)\nplt.xlabel('log(price+1)', fontsize=17)\nplt.ylabel('frequency', fontsize=17)\nplt.tick_params(labelsize=15)\nplt.title('Log(Price) Distribution - Training Set', fontsize=17)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10, 10))\n#How many values have price above 500?\nprint((train_data.price>500).sum())\n\n# Very small proportion. This could be better explored with a box plot.\nsns.boxplot(y = 'price', data = train_data)\nplt.ylim(0,300)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For almost 95% the of the datapoints, the price is less than 75\n# 99% data is covered by price less than 170\ntrain_data.price.describe(percentiles=[0.8,0.9,0.95,0.99])\n\n#These prices could be outliers or very expensive gadgets/gold and other comodities ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.2 Shipping \n*shipping cost: A binary indicator, 1 if shipping fee is paid by seller and 0 if it's paid by buyer*\n\n1. What is the proportion of items shipped by the seller or otherwise\n2. Corelation between price and shipping\n3. The average factor alpha by which shipping increases the cost of the item\n4. Any brands which always follow a given shipping value"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.shipping.value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.groupby('shipping')['price'].describe(percentiles = [0.9,0.95,0.99])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(20, 10))\nsns.distplot(np.log(train_data.loc[train_data['shipping']==1,\"price\"])+1 , color=\"red\", label=\"shipping 1\",hist=False, rug=True)\nsns.distplot(np.log(train_data.loc[train_data['shipping']==0,\"price\"])+1 , color=\"skyblue\", label=\"Shipping 0\",hist=False, rug = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.3 item condition id"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_data.item_condition_id.value_counts())\ntrain_data.item_condition_id.value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.groupby('item_condition_id')['price'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# the distribution of condition vs price in box plot could be helpful\nfig, ax = plt.subplots(figsize=(20, 10))\nsns.violinplot(x='item_condition_id', y='price', data=train_data)\nplt.ylim(0,200)\n\ntrain_data.groupby('item_condition_id').price.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Its observed that, items with condition 5, are also sold at comparable prices as that of condition 1, though their distributions differ. Most of the products with condition 4,have price 23 approximately\n\nLet's investigate this"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(20, 10))\nsns.distplot(train_data.loc[train_data['item_condition_id']==5,'price'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lmplot(x = 'item_condition_id',y = 'price',data = train_data,x_estimator = np.median,col='shipping',fit_reg=False)\nsns.lmplot(x = 'item_condition_id',y = 'price',data = train_data,x_estimator = np.mean,col='shipping',fit_reg=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.FacetGrid(train_data,col='item_condition_id',row='shipping')\ng.map(plt.hist,\"price\")\nplt.xlim(0,500)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Are there any specific brands, for which old items are sold at reasonable prices?"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.loc[(train_data['item_condition_id']==5) & (train_data['price']>100)].groupby('brand_name').describe()\n# Apple products with bad condition still have high price!","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_colwidth',1000)\ntrain_data.loc[(train_data['item_condition_id']==5) & (train_data['price']>100) & (train_data['brand_name']=='American Girl ®')]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"after going through these we can conclude, its not inconsistent/erred data but rather, some of the products from high\nbrands with condition id 5 are sold at good price!\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(train_data.item_condition_id,train_data.shipping,normalize=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thus we can conclude that new items are usually shipped by the seller, but does it affect price"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nsns.lmplot(x='item_condition_id', y='price', col='shipping',data=train_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.4 Brand name\n\nAlso this code is the modified version of code from one of the kaggle kernels, link :https://www.kaggle.com/valkling/mercari-rnn-2ridge-models-with-notes-0-42755,\n\n\nBriefly,\nThe brand name data is sparse, missing over 600,000 values. This gets some of those values back by checking their names. However, It does not seem to help the models either way at this point. An exact name match against all_brand names will find about 3000 of these. We can be pretty confident in these. At the other extreme, we can search for any matches throughout all words in name. This finds over 200,000 but a lot of these are incorrect. Can land somewhere in the middle by either keeping cases or trimming out some of the 5000 brand names.\n\nFor example, PINK is a brand by victoria secret. If we remove case, then almost all pink items are labeled as PINK brand. The other issue is that some of the \"brand names\" are not brands but really categories like \"Boots\" or \"Keys\".\n\nCurrently, checking every word in name of a case-sensitive match does best. This gets around 137,000 finds while avoiding the problems with brands like PINK.\n\nIn this code brands having two words, might not be found, (the name variable is split in indivial words, thus it checks if victoria present in brand_names? but victoria secret might be present.) thus I have made changes accordingly.\nApart from that, names containing letters such as 'M' which is also a brand name, could be mistakenly assigned everytime there is letter m in the name. Thus brand_name which we assign has to be of length min 3"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Brand name is absent in \", train_data.brand_name.isnull().sum(), \"number of samples\")\nprint(\"Equivalnt to \", (train_data['brand_name'].isnull().sum()/train_data.shape[0]*100), \"%\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Many a times brand names contribute a lot to the pricing of the products. Let's verify this hypothesis\nprint(\"Number of unique brands = \",train_data.brand_name.nunique())\ntrain_data.brand_name.value_counts().head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# brand_name \n# What is the average price of the most frequent brands?\ntrain_data.groupby('brand_name').price.agg({\"mean_price\": np.mean,\"count\":'count'}).sort_values(\n    \"count\", ascending=False).head(10).reset_index()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most frequent brands, are not very costly( as we have items costing over 1500 as well)\n\nThen what are some of the most expensive brands?"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.groupby('brand_name').price.agg({'mean' : np.mean, 'count':'count'}).sort_values(by = 'mean', ascending=False).head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" %%time\n# attempt to find missing brand names\n\ntrain_data.brand_name.fillna(value=\"missing\", inplace=True)\ntest_data.brand_name.fillna(value=\"missing\", inplace=True)\nunique_brand_names = set(train_data.brand_name.unique()).union(set(test_data.brand_name.unique()))\n\n# get to finding!\npremissing = len(train_data.loc[train_data['brand_name'] == 'missing'])\ndef brandfinder(line):\n    brand = line[0]\n    name = line[1]\n    if brand == 'missing':\n        for brand_name in unique_brand_names:\n            if brand_name in name and len(brand_name)>2:\n                return brand_name\n    return brand","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntrain_data['brand_name'] = train_data[['brand_name','name']].apply(brandfinder, axis = 1)\ntest_data['brand_name'] = test_data[['brand_name','name']].apply(brandfinder, axis = 1)\nfound = premissing-len(train_data.loc[train_data['brand_name'] == 'missing'])\nprint(found)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Going through the values of brand name, the filled values seem to be sensible\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's go through the relationship between brand popularity(frequency of occurance) and price! \nPopular brands are expensive, but these should be compared within the same category.\n\nThus this is explored in the next section."},{"metadata":{},"cell_type":"markdown","source":"## 2.5 Category_name"},{"metadata":{"trusted":true},"cell_type":"code","source":"top_10_categories = train_data.category_name.value_counts()[0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_10_categories","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def create_churn_trace(col, visible=False):\n#     return go.Histogram(\n#         x=churn[col],\n#         name='churn',\n#         marker = dict(color = colors[1]),\n#         visible=visible\n#     )\n\n# def create_no_churn_trace(col, visible=False):\n#     return go.Histogram(\n#         x=no_churn[col],\n#         name='no churn',\n#         marker = dict(color = colors[0]),\n#         visible = visible,\n#     )\n\n# features_not_for_hist = [\"state\", \"phone_number\", \"churn\"]\n# features_for_hist = [x for x in pre_df.columns if x not in features_not_for_hist]\n# active_idx = 0\n# traces_churn = [(create_churn_trace(col) if i != active_idx else create_churn_trace(col, visible=True)) for i, col in enumerate(features_for_hist)]\n# traces_no_churn = [(create_no_churn_trace(col) if i != active_idx else create_no_churn_trace(col, visible=True)) for i, col in enumerate(features_for_hist)]\n# data = traces_churn + traces_no_churn\n\n# n_features = len(features_for_hist)\n# steps = []\n# for i in range(n_features):\n#     step = dict(\n#         method = 'restyle',  \n#         args = ['visible', [False] * len(data)],\n#         label = features_for_hist[i],\n#     )\n#     step['args'][1][i] = True # Toggle i'th trace to \"visible\"\n#     step['args'][1][i + n_features] = True # Toggle i'th trace to \"visible\"\n#     steps.append(step)\n\n# sliders = [dict(\n#     active = active_idx,\n#     currentvalue = dict(\n#         prefix = \"Feature: \", \n#         xanchor= 'center',\n#     ),\n#     pad = {\"t\": 50},\n#     steps = steps,\n# )]\n\n# layout = dict(\n#     sliders=sliders,\n#     yaxis=dict(\n#         title='#samples',\n#         automargin=True,\n#     ),\n# )\n\n# fig = dict(data=data, layout=layout)\n\n# iplot(fig, filename='histogram_slider')\n# 0\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def drawBarGraph(data_x,data_y, hover_text, graph_title,x_title,y_title,colors):\n    trace1 = go.Bar(y=data_y, x=data_x, text=hover_text,marker=dict(\n                color = data_y,colorscale=colors,showscale=True,\n                reversescale = False\n                ))\n    layout = dict(title= graph_title,\n                yaxis = y_title,\n                xaxis = x_title)\n    fig=dict(data=[trace1], layout=layout)\n    py.iplot(fig)\n    \nhover_text = [(\"%.2f\"%(v*100/len(train_data)))+\"%\"for v in (top_10_categories.values)]\ndata_x = top_10_categories.index\ndata_y = top_10_categories.values\ntitle = 'Number of items by category'\nx_title = dict(title='Category')\ny_title = dict(title='Count')\ndrawBarGraph(data_x, data_y,hover_text, title,x_title,y_title,colors = 'Picnic')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reference: BuryBuryZymon at https://www.kaggle.com/maheshdadhich/i-will-sell-everything-for-free-0-55\ndef split_cat(text):\n    try: return text.split(\"/\")\n    except: return (\"No Label\", \"No Label\", \"No Label\")\ntrain_data['first_category'], train_data['second_category'], train_data['third_category'] = \\\nzip(*train_data['category_name'].apply(lambda x: split_cat(x)))\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"first_category_value_counts = train_data.first_category.value_counts()[0:15]\nhover_text = [(\"%.2f\"%(v*100/len(train_data)))+\"%\"for v in (first_category_value_counts.values)]\ndata_x = first_category_value_counts.index\ndata_y = first_category_value_counts.values\ntitle = 'Number of items by primary category'\nx_title = dict(title='Primary Category')\ny_title = dict(title='Count')\ndrawBarGraph(data_x, data_y,hover_text, title,x_title,y_title,colors = 'Viridis')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the primary category, as products of women almost contribute to 44% of samples, we can explore it further by considering specific brands which could be costlier than others!"},{"metadata":{"trusted":true},"cell_type":"code","source":"women_products = train_data.loc[train_data.first_category=='Women',['brand_name','price']]\nfirst_N_brands = 20\ntop_women_brands = women_products.brand_name.value_counts()[:first_N_brands] \nc = ['hsl('+str(h)+',50%'+',50%)' for h in np.linspace(0, 360, first_N_brands)]\ni=0\ndata = [{\n    'y': women_products.loc[women_products['brand_name']==brand_name, 'price'], \n    'type':'box',\n    'name':brand_name,\n    'marker':{'color': c[freq%first_N_brands]}\n    } for brand_name, freq in top_women_brands.iteritems()]\n    \nlayout = {'xaxis': {'title':'brand name','showgrid':True,'zeroline':False, 'tickangle':60,'showticklabels':False},\n          'yaxis': {'title':\"price\",'zeroline':False,'gridcolor':'white'},\n          'paper_bgcolor': 'rgb(233,233,233)',\n          'plot_bgcolor': 'rgb(233,233,233)',\n          }\npy.iplot(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"second_category_value_counts = train_data.second_category.value_counts()[0:15]\nhover_text = [(\"%.2f\"%(v*100/len(train_data)))+\"%\"for v in (second_category_value_counts.values)]\ndata_x = second_category_value_counts.index\ndata_y = second_category_value_counts.values\ntitle = 'Number of items by secondary category'\nx_title = dict(title='Secondary Category')\ny_title = dict(title='Count')\ndrawBarGraph(data_x, data_y,hover_text, title,x_title,y_title,colors = 'Viridis')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"third_category_value_counts = train_data.third_category.value_counts()[0:15]\nhover_text = [(\"%.2f\"%(v*100/len(train_data)))+\"%\"for v in (third_category_value_counts.values)]\ndata_x = third_category_value_counts.index\ndata_y = third_category_value_counts.values\ntitle = 'Number of items by third category'\nx_title = dict(title='Third Category')\ny_title = dict(title='Count')\ndrawBarGraph(data_x, data_y,hover_text, title,x_title,y_title,colors = 'Viridis')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_data['first_category'].nunique())\nprint(train_data['second_category'].nunique())\nprint(train_data['third_category'].nunique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = []\ncolors = ['red','pink','blue','green','orange']\ni=0\nfor category,count in second_category_value_counts[0:5].iteritems():\n    grouped_brand_data = train_data.loc[train_data['second_category']==category].groupby('brand_name') \\\n                              .price \\\n                              .agg({'mean':np.mean,'count':'count'}) \\\n                              .sort_values(by = 'count', ascending=False)[0:20]    \n    trace = { \"x\": grouped_brand_data.loc[:,'mean'], \n              \"y\": grouped_brand_data.index, \n              \"marker\": {\"color\": colors[i], \"size\": 12}, \n              \"mode\": \"markers\", \n              \"name\": category, \n              \"type\": \"scatter\", \n        }\n    data.append(trace)\n    i=i+1\n                              \nlayout = go.Layout(\n    title=\"Price distribution of top 20 brands in top 5 secondary categories \",\n    xaxis=dict(\n        title='Price',\n        showgrid=True,\n        showline=True,\n        linecolor='rgb(102, 102, 102)',\n        titlefont=dict(\n            color='rgb(204, 204, 204)'\n        ),\n        tickfont=dict(\n            color='rgb(102, 102, 102)',\n        ),\n        showticklabels=True,\n        dtick=10,\n        ticks='outside',\n        tickcolor='rgb(102, 102, 102)',\n    ),\n    yaxis = dict(title='brand names'),\n    margin=dict(\n        l=140,\n        r=40,\n        b=50,\n        t=80\n    ),\n    legend=dict(\n        font=dict(\n            size=10,\n        ),\n        yanchor='bottom',\n        xanchor='right',\n    ),\n    width=800,\n    height=600,\n    paper_bgcolor='rgb(254, 247, 234)',\n    plot_bgcolor='rgb(254, 247, 234)',\n    hovermode='closest',\n)\n\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig)\n                                                                     ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Jewelry items of David Yurman cost on an average 250, and we can see the trend in how prices are affected by brands."},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"#Before handling the brand_name lets preprocess it\n\ndef wordCount(text):\n    # convert to lower case and strip regex\n    try:\n         # convert to lower case and strip regex\n        text = text.lower()\n        regex = re.compile('[' +re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]')\n        txt = regex.sub(\" \", text)\n        # tokenize\n        # words = nltk.word_tokenize(clean_txt)\n        # remove words in stop words\n        words = [w for w in txt.split(\" \") \\\n                 if not w in stop_words.ENGLISH_STOP_WORDS]\n        return len(words), ' '.join(words)\n    except: \n        return 0, \"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.5 Name! :P"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.name.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Proportion of unique product names is = \",train_data['name'].str.lower().str.strip().nunique()/train_data.shape[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thus 76% of our product names are unique! 24% of those are duplicated. Could it mean that they are same products or different?"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['name'].value_counts().head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that these names are too generic, such as dress, Nike, Coach purse and need not share same prices. \n\nNonetheless lets pre-process the text data before proceeding! "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Pipeline for text preprocessing\n#This is very generic list of contractions and most of the words may not even appear in an item description\ncontractions = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\",\n                \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\",\n                \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\n                \"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\",\n                \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\",\n                \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\",\n                \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \n                \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\n                \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\n                \"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\",\n                \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n                \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\",\n                \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\",\n                \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \n                \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \n                \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \n                \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \n                \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\",\n                \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \n                \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \n                \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \n                \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \n                \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\",\n                \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \n                \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \n                \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \n                \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\",\n                \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n                \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n                \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \n                \"you're\": \"you are\", \"you've\": \"you have\" }\n\n\nclass TextPreprocessor(BaseEstimator, TransformerMixin):\n    def __init__(self,\n                 contractions={},\n                 stop_words={},\n                 spellings={},\n                 user_abbrevs={},\n                 n_jobs=1):\n        \"\"\"\n        Text preprocessing transformer includes steps:\n            1. Text normalization\n            2. contractions\n            3. Punctuation removal\n            4. Stop words removal - words like not are excluded from stop words\n        \"\"\"\n       \n        self.user_abbrevs = user_abbrevs\n        self.n_jobs = n_jobs\n        self.contractions = contractions\n        self.stop_words = stop_words\n        self.spellings = spellings\n        \n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, *_):\n        X_copy = X.copy()\n\n        partitions = 1\n        cores = mp.cpu_count()\n        if self.n_jobs <= -1:\n            partitions = cores\n        elif self.n_jobs <= 0:\n            return X_copy.apply(self._preprocess_text)\n        else:\n            partitions = min(self.n_jobs, cores)\n\n        data_split = np.array_split(X_copy, partitions)   # split data for parallel processing\n        pool = mp.Pool(cores)                           # create pools\n        data = pd.concat(pool.map(self._preprocess_part, data_split))   # concatenate results\n        pool.close()                                  \n        pool.join()\n\n        return data\n\n    def _preprocess_part(self, part):\n        return part.apply(self._preprocess_text)\n\n    def _preprocess_text(self, text):\n        lowercase_text = self._lowercase(text)\n        expanded_contractions = self._expand_contactions(lowercase_text)\n        removed_punct = self._remove_punct(expanded_contractions)\n        removed_stop_words = self._remove_stop_words(removed_punct)\n        return (removed_stop_words)\n   \n    def _lowercase(self, text):\n        return text.lower()\n    \n    def _normalize(self, text):\n        # some issues in normalise package\n        try:\n            return ' '.join(normalise(text, user_abbrevs=self.user_abbrevs, verbose=True))\n        except:\n            return text\n\n    def _expand_contactions(self, doc):\n        new_text = \"\"\n        for t in doc.split():\n            if t in contractions:\n                new_text = new_text + \" \" + (contractions[t])\n            else: \n                new_text = new_text + \" \" + t\n        return new_text\n \n    def _remove_punct(self, doc):\n        return ' '.join([t for t in doc.split() if t not in string.punctuation])\n\n    def _remove_stop_words(self, doc):\n        return ' '.join([t for t in doc.split() if t not in self.stop_words])    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\n# A list of contractions from http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\nfrom sklearn.feature_extraction import stop_words\n\n\nrefined_stop_words = stop_words.ENGLISH_STOP_WORDS - {\"not\", \"none\", \"nothing\", \"nowhere\", \"never\", \"cannot\",\n                                \"cant\", \"couldnt\", \"except\", \"hasnt\", \"neither\", \"no\", \n                                 \"nobody\", \"nor\", \"without\" }\n\n%time\ntextPreprocessor = TextPreprocessor(n_jobs=-1, contractions=contractions,\n                 stop_words=refined_stop_words)\n    \ntrain_data['name'] = textPreprocessor.transform(train_data['name'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntrain_data['name_length'] = train_data['name'].apply(lambda x: len(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#Could their be corelation between the length of name and the price?\n# so first let's get the average price of the products having same length names.\n\nname_length_price_df = train_data.groupby('name_length').price.agg(['mean','median'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"name_length_price_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trace1 = go.Scatter(\n    x = name_length_price_df.index,\n    y = name_length_price_df['mean'],\n    #mode = 'lines mean',\n    name = 'price mean'\n) \n\ntrace2 = go.Scatter(\n    x = name_length_price_df.index,\n    y = name_length_price_df['median'],\n    #mode = 'lines median',\n    name = 'price median'\n)\nlayout = dict(title= 'Average (Price) by Name Length',\n              yaxis = dict(title='(Price)'),\n              xaxis = dict(title='Name Length'))\nfig=dict(data=[trace1, trace2], layout=layout)\npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The gap between mean and median initially could suggest some extreme valued products or outliers,\n\nthere is some corelation between the length of the name and the price"},{"metadata":{},"cell_type":"markdown","source":"Using topic modeling we can find what are the most expensive categories!"},{"metadata":{},"cell_type":"markdown","source":"# Item description"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Proportion of unique product names is = \",train_data['item_description'].str.lower().str.strip().nunique()/train_data.shape[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some of the descriptions are repeated as well. Do they have same prices?"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['item_description'].value_counts().head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def handle_missing(text):\n    if (text == \"No description yet\") or (text == np.NAN) or text==\"\" or text == None:\n        return \"missing\"\n    return text\ntrain_data['item_description'].fillna(\"missing\",inplace=True)\ntrain_data['item_description'] = train_data['item_description'].apply(lambda x: handle_missing(x))\ntest_data['item_description'].fillna(\"missing\",inplace=True)\ntest_data['item_description'] = test_data['item_description'].apply(lambda x: handle_missing(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntrain_data['item_description'] = textPreprocessor.transform(train_data['item_description'])\ntest_data['item_description'] = textPreprocessor.transform(test_data['item_description'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntrain_data['item_description_length'] = train_data['item_description'].apply(lambda x: len(x))\ntest_data['item_description_length'] = test_data['item_description'].apply(lambda x: len(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"item_description_price_df = train_data.groupby('item_description_length').price.agg(['mean','median'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"item_description_price_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trace1 = go.Scatter(\n    x = item_description_price_df.index,\n    y = item_description_price_df['mean'],\n    #mode = 'lines mean',\n    name = 'price mean'\n) \n\ntrace2 = go.Scatter(\n    x = item_description_price_df.index,\n    y = item_description_price_df['median'],\n    #mode = 'lines median',\n    name = 'price median'\n)\nlayout = dict(title= 'Average (Price) by Description Length',\n              yaxis = dict(title='(Price)'),\n              xaxis = dict(title='Name Length'))\nfig=dict(data=[trace1, trace2], layout=layout)\npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Name and descriptions"},{"metadata":{},"cell_type":"markdown","source":"Word clouds could help us visualize the text data based on the frequency of words. Thus lets try to plot word cloud for name and description"},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_wordcloud(tup):\n    wordcloud = WordCloud(background_color='white',\n                          max_words=50, max_font_size=40,\n                          random_state=42\n                         ).generate(str(tup))\n    return wordcloud","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,axes = plt.subplots(1, 2, figsize=(30, 15))\n\nax = axes[0]\nax.imshow(generate_wordcloud(train_data['name']), interpolation=\"bilinear\")\n\nax = axes[1]\nax.imshow(generate_wordcloud(train_data['item_description']), interpolation=\"bilinear\")\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Using YAAAKE!\n# import yake\n# train_data['keywords'] = \"\"\n# # assuming default parameters\n# simple_kwextractor = yake.KeywordExtractor(lan=\"en\", n=3, dedupLim=0.9, dedupFunc='seqm', windowsSize=1, top=20, features=None)\n# train_data.loc[0:10000,'keywords'] = train_data.loc[0:10000,'item_description'].apply(lambda x:simple_kwextractor.extract_keywords(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}