{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport time\nimport os\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly_express as px\nimport plotly.graph_objects as go\n\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train data\ntrain = pd.read_csv('/kaggle/input/mercari-price-prediction-train-and-test-data/train.tsv', sep='\\t')\ntest = pd.read_csv('/kaggle/input/mercari-price-prediction-train-and-test-data/test.tsv', sep='\\t')\n\ndisplay(train.head())\nprint('Train data:', train.shape)\n#display(train.shape)\n\nprint('Test data shape: ', test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Check for missing values ###"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum()[train.isnull().sum().values > 0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Target variable - 'price'"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.price.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Target variable - 'price' - distribution\n* Log transformation to change to normal distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 5))\n\nax1.hist(train['price'], range=[0,250], bins=50, edgecolor='w')\nax1.set_xlabel('price')\nax1.set_ylabel('distribution')\nax1.set_title('histogram of price')\n\nax2.hist(np.log1p(train['price']), range=[0,10], bins=30, edgecolor='w')\nax2.set_xlabel('Log of price')\nax2.set_ylabel('distribution')\nax2.set_title('histogram of Log price')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['logprice'] = np.log1p(train['price'])\ntrain.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### * Shipping variable *\n- 'shipping':\n    - 1 if shipping fee is paid by seller\n    - 0 if shipping fee is paid by buyer\n    \n- 'Shipping fee' is quite evenly split  between the buyer and seller\n    - 55% of shipping fee is paid by the buyer"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['shipping'].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Check for any relationship between shipping fee and price\n- the price of good is slightly lesser when the seller pays for the shipping to make the price competitive"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\ntrain.groupby('shipping')['price'].plot(kind='hist', range=[0,100], bins=25, alpha=0.7)\nplt.legend(labels=['buyer', 'seller'])\nplt.xlabel('price')\nplt.ylabel('distribution')\nplt.title('variations across buyer and seller')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### * Category types *"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Number of unique categories is: {}\\n'.format(len(train['category_name'].value_counts())))\nprint('Top 10 categories: \\n{}'.format(train['category_name'].value_counts()[:10]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split the categories\n\n# text = 'Women/Athletic Apparel/Pants, Tights, Leggings'\n# text.split('/')\n\ndef split_categories(text):\n    try:\n        return text.split('/')\n    # if no category name\n    except: \n        return ['no label', 'no label', 'no label']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['general_cat'] = train['category_name'].apply(lambda x: split_categories(x)[0])\ntrain['sub_cat1'] = train['category_name'].apply(lambda x: split_categories(x)[1])\ntrain['sub_cat2'] = train['category_name'].apply(lambda x: split_categories(x)[2])\n\ntrain.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nplt.bar(x = train['general_cat'].value_counts().index.values, height = train['general_cat'].value_counts().values)\nplt.xlabel('General categories')\nplt.ylabel('Count')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# show in plotly\n\ntrace = go.Bar(x = train['general_cat'].value_counts().index.values, \n               y = train['general_cat'].value_counts().values,\n                text = round(train['general_cat'].value_counts(normalize=True)*100,2)\n              )\nlayout = go.Layout(dict(\n                        title = 'Number of items by general category'),\n                        xaxis= dict(title = 'general categories'),\n                        yaxis= dict(title = 'Count')\n                  )\nfig  = go.Figure(data =trace, layout = layout)\npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Sub category 1 - Top 20"},{"metadata":{"trusted":true},"cell_type":"code","source":"# show in plotly\n\ntrace = go.Bar(x = train['sub_cat1'].value_counts().index.values[:20], \n               y = train['sub_cat1'].value_counts().values[:20],\n                text = round(train['sub_cat1'].value_counts(normalize=True)*100,2)\n              )\nlayout = go.Layout(dict(\n                        title = 'Number of items by sub_category 1'),\n                        xaxis= dict(title = 'sub_category 1'),\n                        yaxis= dict(title = 'Count')\n                  )\nfig  = go.Figure(data =trace, layout = layout)\npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# show in plotly\n\ntrace = go.Bar(x = train['sub_cat2'].value_counts().index.values[:20], \n               y = train['sub_cat2'].value_counts().values[:20],\n                text = round(train['sub_cat2'].value_counts(normalize=True)*100,2),\n                             )\nlayout = go.Layout(dict(\n                        title = 'Number of items by sub_category 2'),\n                        xaxis= dict(title = 'sub_category 2'),\n                        yaxis= dict(title = 'Count'),\n                        )\nfig  = go.Figure(data =trace, layout = layout)\npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gen_cat = train['general_cat'].unique()\nx = [train.loc[train['general_cat'] == cat, 'price'] for cat in gen_cat]\n\ntrace = [go.Box(x = np.log1p(x[i]), name = gen_cat[i]) for i in range(len(gen_cat))]\nlayout = dict(\n            title = 'Price distrbution across general category',\n            xaxis = dict(title='distribution'),\n            yaxis = dict(title = 'category')\n            )\nfig = go.Figure(data = trace, layout=layout)\npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum()[train.isnull().sum().values > 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fill the columns that have missing values\n\ndef handle_missing_values(df):\n    df['category_name'].fillna(value = 'missing', inplace=True)\n    df['brand_name'].fillna(value = 'missing', inplace=True)\n    df['item_description'].replace('No description yet', 'missing', inplace=True)\n    df['item_description'].fillna(value = 'missing', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Brand names**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Number of unique brands is : ', len(train['brand_name'].value_counts()))\nprint('Number of unique brands with count > 1 : ', len(train['brand_name'].value_counts()[train['brand_name'].value_counts() > 1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Item description\n- the data is very unstructured\n- has punctuations, numbers etc which need to be removed\n- remove stop words\n- also remove any words with len < 3"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['item_description']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text = train.iloc[1482534]['item_description']\ntext","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nstop = stopwords.words('english')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\ndef tokenize_text(text):\n    regex = re.compile(r'[a-zA-Z]{3,}')\n    txt = regex.findall(str(text).lower())\n    #tokens = [t for t in txt if t not in stop and len(t)>3]\n    tokens = [t for t in txt if t not in stop and len(t)>3]\n    return tokens","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### get count of tokens"},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.feature_extraction.text import CountVectorizer\n# #cv = CountVectorizer(stop_words='english', token_pattern= r'\\b[^\\d\\W]+\\b')\n# cv = CountVectorizer(stop_words='english', token_pattern= r'[a-zA-Z]{3,}')\n# #cv = CountVectorizer()\n\n# def word_count(text):\n#     try:\n#         cv.fit([text]) ## if this doesnt work, try 'text' instead of '[text]'\n#         return len(cv.get_feature_names())\n#     except:\n#         return 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### get tokens"},{"metadata":{"trusted":true},"cell_type":"code","source":"#cv = CountVectorizer(stop_words='english', token_pattern= r'\\b[^\\d\\W]+\\b')\n# cv = CountVectorizer(stop_words='english', token_pattern= r'[a-zA-Z]{3,}')\n# #cv = CountVectorizer(stop_words='english')\n\n# def word_token(text):\n#     try:\n#         cv.fit([text]) ## if this doesnt work, try 'text' instead of '[text]'\n#         tokens = cv.get_feature_names()\n#         return tokens\n#     except:\n#         return 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Looking at a sample of the full dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# trace = go.Scatter(x = df['desc_length'], y = df['price'],\n#                   mode = 'lines+markers')\n# layout = go.Layout(dict(title='price vz description'))\n\n# fig = go.Figure(data=[trace], layout=layout)\n# fig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Taking the complete train dataset"},{"metadata":{},"cell_type":"markdown","source":"### test a sample"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sample = train.sample(frac=0.001, random_state=123)\ntrain_sample['tokens'] = train_sample['item_description'].apply(lambda x: tokenize_text(x))\ntrain_sample['desc_length'] = train_sample['tokens'].apply(lambda x: len(x))\n\ntrain_sample.head(10)\n\nfor description, tokens, length in zip(train_sample['item_description'], train_sample['tokens'], train_sample['desc_length']):\n    print('Item description: ', description)\n    print('Word tokens', tokens)\n    print('Description length: ', length)\n    print('\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Taking the complete train dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# train['tokens'] = train['item_description'].apply(lambda x: tokenize_text(x))\n# train['desc_length'] = train['tokens'].apply(lambda x: len(x))\n\n# train.head(10)\n\n# for description, tokens, length in zip(train_sample['item_description'], train_sample['tokens'], train_sample['desc_length']):\n#     print('Item description: ', description)\n#     print('Word tokens', tokens)\n#     print('Description length: ', length)\n#     print('\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_sample = train.sample(frac=0.001, random_state=10)\n# train_sample['tokens'] = train_sample['item_description'].apply(lambda x: tokenize_text([x]))\n# train_sample['desc_length'] = train_sample['item_description'].apply(lambda x: word_count([x]))\n# train_sample[['tokens','desc_length']].head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import datetime\nimport time\n\nstart = time.time()\nprint('Start time:', datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\"))\ntrain['tokens'] = train['item_description'].apply(lambda x: tokenize_text(x))\ntrain['desc_length'] = train['tokens'].apply(lambda x: len(x))\n\nprint('time taken for train:', time.time() - start)\nprint('End time:', datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\"))\n\n# for test\nstart = time.time()\nprint('Start time:', datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\"))\ntest['tokens'] = test['item_description'].apply(lambda x: tokenize_text(x))\ntest['desc_length'] = test['tokens'].apply(lambda x: len(x))\nprint('time taken for test:', time.time() - start)\n# print('End time:', datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\"))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Saving the file into a new csv file"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.to_csv('train_desc.csv',index=False)\ntest.to_csv('test_desc.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train = pd.read_csv('/kaggle/input/mercari-price-suggestion-with-desc/train_desc.csv')\n# test = pd.read_csv('/kaggle/input/mercari-price-suggestion-with-desc/test_desc.csv')\n# train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### * Mean price for description length *"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = train.groupby('desc_length')['price'].mean().reset_index()\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trace = go.Scatter(\n                    x = df['desc_length'],\n                    y = df['price'],\n                    mode = 'lines+markers',\n                    )\n\nlayout = go.Layout(dict(title = 'Average price per description length',\n                       xaxis = dict(title = 'Description length'),\n                       yaxis = dict(title = 'price')\n                       )\n                  )\nfig = go.Figure(data = [trace], layout=layout)\npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(train.isnull().sum())\ndisplay(train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# removing the entries which do not have a 'item description'\n\ntrain_new = train[pd.notnull(train['item_description'])]\ndisplay(train_new.isnull().sum())\ndisplay(train_new.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### General categories"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['general_cat'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create a 'WordCloud' for the most common words in each 'general' category"},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\nfrom collections import Counter\n\ncat_desc = dict()\nfor cat in train_sample['general_cat'].value_counts().index.tolist():\n    text = \" \".join(train_sample.loc[train_sample['general_cat'] == cat, 'item_description'].values)\n    cat_desc[cat]= tokenize_text(text)\n\nwomens100 = Counter(cat_desc['Women']).most_common(100)\nbeauty100 = Counter(cat_desc['Beauty']).most_common(100)\nkids100 = Counter(cat_desc['Kids']).most_common(100)\nelectronics100 = Counter(cat_desc['Electronics']).most_common(100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"keys = [k for (k,v) in womens100]\ntext = \" \".join(keys)\ntext","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\n#optional - to remove the word 'shipping' from the wordcloud\nstopwords = set(STOPWORDS)\n#print(stopwords)\nstopwords.update(['shipping', 'Shipping', \"shipping'\"])\n#optional\n\ndef create_wordcloud(text):\n    wordcloud = WordCloud(background_color='white', stopwords=stopwords,\n                          max_font_size=30).generate(text)\n    return wordcloud","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(2,2, figsize=(15, 10))\n\n# wordcloud requires input text to be a string\n\nax[0,0].imshow(create_wordcloud(str(womens100)), interpolation='bilinear')\nax[0,0].set_title('women', fontsize=25)\n\nax[0,1].imshow(create_wordcloud(str(beauty100)), interpolation='bilinear')\nax[0,1].set_title('beauty', fontsize=25)\n\nax[1,0].imshow(create_wordcloud(str(kids100)), interpolation='bilinear')\nax[1,0].set_title('kids', fontsize=25)\n\nax[1,1].imshow(create_wordcloud(str(electronics100)), interpolation='bilinear')\nax[1,1].set_title('Electronics', fontsize=25)\n\nplt.tight_layout(pad =1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Some random tests\n\nimport operator\ntemp = Counter(cat_desc['Women'])\n#sorted(temp.items(), key=operator.itemgetter(1), reverse=True)\n\nlofwords=[]\nfor k, v  in temp.items():\n    if v > 10:\n        lofwords.append(k)\n\nlen(lofwords)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### * Using Tf-Idf vectorizer *\n- It depends on two factors:\n    * Terms frequency\n    * Inverse Document frequency"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(min_df=10,\\\n                             max_features= 180000, \\\n                             tokenizer = tokenize_text, \\\n                             ngram_range=(1,2)\n                            )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_new.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply to train and test together\n# use train_new, which has no item description missing\n\nstart = time.time()\nall_desc = np.append(train_new['item_description'].values, test['item_description'].values)\nvz = vectorizer.fit_transform(all_desc)\nprint('Time taken:', time.time() - start)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# saving the model file\nimport pickle\npickle.dump(vz, open('tfidf_vectorizer_combined_whole.pkl', 'wb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# extract features\n# zip the features and the correspondinf IDF scores together\ntfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n\n#create dataframe\ntfidf = pd.DataFrame(tfidf.items(), columns=['features', 'idf_score'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### *Top 10 features with the highest tfidf scores *"},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf.sort_values('idf_score', ascending=False).head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Each document is represented as a row vector of shape 180,000"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Shape of the vectorized implementation of item description: ',vz.shape)\nprint('Dimension of the Tf-Idf dataframe is :', tfidf.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualize the data using t-SNE (t-Distributed Stochastic Neighbor Embedding)\n* combine train and test data\n* sample a fraction from the combined"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_copy = train_new.copy()\ntest_copy = test.copy()\n\ntrain_copy['is_train'] = 1\ntest_copy['is_train'] = 0\n\n\ncombined_df = pd.concat([train_copy, test_copy], sort=False)\n\nsample_size= 15000\ncombined_sample = combined_df.sample(n=sample_size, random_state=1)\n\n# applying tfidf vectorizer\n#vz_sample = vectorizer.fit_transform(combined_sample['item_description'].values)\nvz_sample = vectorizer.transform(combined_sample['item_description'].values)\n\nprint('Shape of the sample tfidf matrix is: ', vz_sample.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Using Truncated SVD to reduce dimensions"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import TruncatedSVD\n\nn_comp= 30\nsvd = TruncatedSVD(n_components=n_comp, random_state=1)\nsvd_tfidf = svd.fit_transform(vz_sample)\nprint('Shape of svd_tfidf matrix is:', svd_tfidf.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Use TSNE"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.manifold import TSNE\ntsne_model = TSNE(n_components=2, verbose=1, random_state=42, n_iter=500)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tsne_tfidf = tsne_model.fit_transform(svd_tfidf)\nprint('Shape of tsne_tfidf matrix is:', tsne_tfidf.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# saving the tsne model\npickle.dump(tsne_tfidf, open('tsne_svd_tfidf_combined_sample.pkl', 'wb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_sample.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_sample.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_sample.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# After SVD and T-SNE on a sample of the combined train and test datasets\n\ntfidf_df = pd.DataFrame(tsne_tfidf, columns=['pc1', 'pc2'])\ntfidf_df['item_description'] = combined_sample['item_description']\ntfidf_df['tokens'] = combined_sample['tokens']\ntfidf_df['category'] = combined_sample['general_cat']\n\ntfidf_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import bokeh.plotting as bp\nfrom bokeh.models import HoverTool, BoxSelectTool\nfrom bokeh.models import ColumnDataSource\nfrom bokeh.plotting import figure, show, output_notebook","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output_notebook()\nplot_tfidf = bp.figure(plot_width=700, plot_height=600,\n                       title=\"tf-idf clustering of the item description\",\n    tools=\"pan,wheel_zoom,box_zoom,reset,hover, save\",\n    x_axis_type=None, y_axis_type=None, min_border=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_tfidf.scatter(x='pc1', y='pc2', source=tfidf_df, alpha=0.7)\nhover = plot_tfidf.select(dict(type=HoverTool))\nhover.tooltips={\"description\": \"@item_description\", \"tokens\": \"@tokens\", \"category\":\"@category\"}\nshow(plot_tfidf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plt.scatter(tfidf_df['pc1'], tfidf_df['pc2'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### K-Means clustering"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import MiniBatchKMeans\n\nnum_clusters=30\n\nkmeans_model = MiniBatchKMeans(n_clusters = num_clusters, \n                               init='k-means++',\n                               n_init=1,\n                               init_size=1000, \n                               batch_size=1000, \n                               max_iter=1000,\n                               verbose=1\n                              )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using the original tfidf vectorizer which will be fit on the combined \"whole\" train and test\nvectorizer = TfidfVectorizer(min_df=10,\\\n                             max_features= 180000, \\\n                             tokenizer = tokenize_text, \\\n                             ngram_range=(1,2)\n                            )\n\nall_desc = np.append(train_new['item_description'].values, test['item_description'].values)\nvz = vectorizer.fit_transform(all_desc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Number of features or tokens ', len(vectorizer.get_feature_names()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applying the kmeans clustering to the tfidf matrix, to reduce dimensions\nkmeans = kmeans_model.fit(vz)\nkmeans_clusters = kmeans_model.predict(vz)\nkmeans_distances = kmeans_model.transform(vz)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# saving the model\npickle.dump(kmeans_model, open('kmeans_tfidf_combined_whole.pkl', 'wb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.unique(kmeans_clusters, return_counts=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(kmeans.cluster_centers_.shape)\nterms = vectorizer.get_feature_names()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(terms)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\nsorted_centroids","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(num_clusters):\n    print('Cluster centroid: %d' %i)\n    aux = ''\n    #finding the top 10 words for every cluster\n    for j in sorted_centroids[i, :10]:\n        aux += terms[j] + ' | '\n        \n    print(aux)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Number of clusters:', np.unique(kmeans_clusters))\n# This is the same as\nprint(np.unique(kmeans.labels_))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### For visualization of the K-Means Clusters, we will take a sample"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Shape of the vz_sample tfidf matrix', vz_sample.shape)\n\n# Apply the Batched K-Means on the sample\nkmeans = kmeans_model.fit(vz_sample)\n# get the cluster centroids and mapping of features to the clusters\nkmeans_clusters = kmeans.predict(vz_sample)\nkmeans_distances = kmeans.transform(vz_sample)\n\n# apply the T-SNE model to reduce dimension to 2, so it becomes easier to visualize\ntsne_kmeans = tsne_model.fit_transform(kmeans_distances)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# saving the model\npickle.dump(kmeans_model, open('tsne_kmeans_combined_sample.pkl', 'wb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dimension has been shrunk to 2\ntsne_kmeans.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creata dataframe for the sample\nkmeans_df = pd.DataFrame(tsne_kmeans, columns=['pc1', 'pc2'] )\nkmeans_df['cluster']= kmeans_clusters\nkmeans_df['description']= combined_sample['item_description']\nkmeans_df['category']= combined_sample['general_cat']\n\nkmeans_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_kmeans = bp.figure(plot_width=700, plot_height=600,\n                        title=\"KMeans clustering of the description\",\n    tools=\"pan,wheel_zoom,box_zoom,reset,hover,save\",\n    x_axis_type=None, y_axis_type=None, min_border=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"colormap = np.array([\"#6d8dca\", \"#69de53\", \"#723bca\", \"#c3e14c\", \"#c84dc9\", \"#68af4e\", \"#6e6cd5\",\n\"#e3be38\", \"#4e2d7c\", \"#5fdfa8\", \"#d34690\", \"#3f6d31\", \"#d44427\", \"#7fcdd8\", \"#cb4053\", \"#5e9981\",\n\"#803a62\", \"#9b9e39\", \"#c88cca\", \"#e1c37b\", \"#34223b\", \"#bdd8a3\", \"#6e3326\", \"#cfbdce\", \"#d07d3c\",\n\"#52697d\", \"#194196\", \"#d27c88\", \"#36422b\", \"#b68f79\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"source = ColumnDataSource(data=dict(x=kmeans_df['pc1'], y=kmeans_df['pc2'],\n                                    color=colormap[kmeans_clusters],\n                                    description=kmeans_df['description'],\n                                    category=kmeans_df['category'],\n                                    cluster=kmeans_df['cluster']))\n\nplot_kmeans.scatter(x='x', y='y', color='color', source=source)\nhover = plot_kmeans.select(dict(type=HoverTool))\nhover.tooltips={\"description\": \"@description\", \"category\": \"@category\", \"cluster\":\"@cluster\" }\nshow(plot_kmeans)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# using seaborn\nplt.figure(figsize=(20, 10))\nsns.scatterplot(x=kmeans_df['pc1'], y=kmeans_df['pc2'], hue=kmeans_df['cluster'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Topic Modeling - using LDA(Latent Dirichlet Allocation)\n- the input to the topic model is a bag of words, for which we will use a CountVectorizer\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\ncvectorizer = CountVectorizer(min_df=5, \n                             max_features=180000,\n                             tokenizer = tokenize_text,\n                             ngram_range = (1,2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cvz = cvectorizer.fit_transform(combined_sample['item_description'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# saving the model\npickle.dump(cvz, open('countVectorizer_for_lda.pkl', 'wb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import logging\nlogging.getLogger(\"lda\").setLevel(logging.WARNING)\n\nfrom sklearn.decomposition import LatentDirichletAllocation\n\nnum_topics=20\nlda_model = LatentDirichletAllocation(n_components = num_topics, \n                                     max_iter=20, \n                                     random_state= 42, \n                                     learning_method='online'\n                                     )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# input is a bag of words\nX_topics = lda_model.fit_transform(cvz)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# saving the model\npickle.dump(lda_model, open('lda_model_countVectorizer.pkl', 'wb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Shape of the X_topics is:', X_topics.shape)\nprint(X_topics)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# getting the topic components\ntopic_word = lda_model.components_\nprint('Shape of topic_word is: ', topic_word.shape)\nprint(topic_word)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.argsort(topic_word[0]))\nprint(np.argsort(topic_word[0])[:-10:-1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.array(vocabulary)[np.argsort(topic_word[0])[:-10:-1]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocabulary[1345]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topic_word = lda_model.components_ # these are the 20 topics words\nvocabulary = cvectorizer.get_feature_names() # bag of words\n\ntopic_summaries=[]\n\nn_top_words =10 # 10 most relevant words related to the topic\n\nfor i, topic_dist in enumerate(topic_word):\n    indexes = np.argsort(topic_dist)[:-(n_top_words+1): -1] # getting the indexes of the top 10 words, in descending order\n    topic_words = np.array(vocabulary)[indexes]\n    print('topic %d' %i)\n    print(' | '.join(topic_words))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### X_topics:\n    * each row indicates a document with a probabilistic distribution across the 20 different topics"},{"metadata":{"trusted":true},"cell_type":"code","source":"# reduce dimensions using T-SNE\nprint('Dimension of X_topics is :', X_topics.shape)\ntsne_lda = tsne_model.fit_transform(X_topics)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# saving the model\npickle.dump(tsne_lda, open('tsne_lda_model_combined_sample.pkl', 'wb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert to matrix to normalize the values across columns, such that sum across rows =1\nunnormalized = np.matrix(X_topics)\n\ndoc_top_normalized = unnormalized / unnormalized.sum(axis=1)\ndoc_top_normalized","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check if normalized across rows\ndoc_top_normalized[0].sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# finding the most relevant topic for each item description\nlda_keys=[]\nfor i, description in enumerate(combined_sample['item_description']):\n    lda_keys += [doc_top_normalized[i].argmax()] # get the indexes of the topic with the highest probablisitic values\n    \nprint(lda_keys[:10])\n\n#create a Dataframe\nlda_df = pd.DataFrame(tsne_lda, columns=['pc1', 'pc2'])\nlda_df['description'] = combined_sample['item_description']\nlda_df['category'] = combined_sample['general_cat']\nlda_df['topic'] = lda_keys\nlda_df['len_docs'] = combined_sample['tokens'].map(len)\n\n\nlda_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_lda = bp.figure(plot_width=700,\n                     plot_height=600,\n                     title=\"LDA topic visualization\",\n    tools=\"pan,wheel_zoom,box_zoom,reset,hover,save\",\n    x_axis_type=None, y_axis_type=None, min_border=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"source = ColumnDataSource(data=dict(x=lda_df['pc1'], y=lda_df['pc2'],\n                                    color=colormap[lda_keys],\n                                    description=lda_df['description'],\n                                    topic=lda_df['topic'],\n                                    category=lda_df['category']))\n\nplot_lda.scatter(source=source, x='x', y='y', color='color')\n\nhover = plot_kmeans.select(dict(type=HoverTool))\nhover = plot_lda.select(dict(type=HoverTool))\nhover.tooltips={\"description\":\"@description\",\n                \"topic\":\"@topic\", \"category\":\"@category\"}\nshow(plot_lda)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def prepareLDAData():\n    data = {\n        'vocab': vocabulary,   # vocabulary = cvectorizer.get_feature_names()\n        'doc_topic_dists': doc_top_normalized,\n        'doc_lengths': list(lda_df['len_docs']),\n        'term_frequency':cvectorizer.vocabulary_,\n        'topic_term_dists': lda_model.components_\n    } \n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('lda_model.components_ shape:', lda_model.components_.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get the count frequency of words in the entire corpus of item description in 'combined_sample'\ncvectorizer.vocabulary_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### For interactive visualization for LDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pyLDAvis\n\nldadata = prepareLDAData()\npyLDAvis.enable_notebook(sort=True)\nprepared_data = pyLDAvis.prepare(**ldadata)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# saving the model\nimport pickle\nwith open('pyldavis_model.pkl', 'wb') as f:\n    pickle.dump(prepared_data, f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pyLDAvis.save_html(prepared_data, 'prepared_data_lda.html')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}