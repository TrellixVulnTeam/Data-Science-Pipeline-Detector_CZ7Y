{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"markdown","source":"**<font size=9>Mercari Price Suggestion Challenge</font>**\nTING-LE,LIN    ,SCU FEAM\n\n**<font size=6>前言</font>**\n\n不知道大家有沒有曾經在拍賣平台中販售物品的經驗，販售物品時最困難的決策莫過於商品售價，**若賣得太貴，會使得商品乏人問津，若賣太便宜，又會不小心和財富擦肩而過**。大部分的人在定價時會通常做的事情是，參考同品項的價格、商品原價、新舊程度等等，**但這些步驟顯得相當繁瑣沒有效率，也有可能花了大量時間到最後卻還是無法訂出合適價格。**\n\n今天我就要跟大家談談這個問題的實現，這是由Mercari在Kaggle所舉辦的一場比賽，比賽目的在於，**為拍賣平台中的使用者提供參考售價**，在比賽中，我們會得到的資訊有**商品名稱、運費歸屬、商品分類、商品品牌、商品狀況、商品描述**，我們被要求以這些資訊來預測商品的售價，這些資料全都是由**文字**組成，因此在預測時相對較複雜。評分方式以**RMSLE**計算，較低者勝。\n        \n**<font size=6>解決方案</font>**\n\n**<font size=3>1. 資料載入及前置處理</font>**\n\n訓練資料有1482353筆，測試資料有3460725筆(stage 2)。為了後面cross-validation評分方便，先將訓練資料的售價做log(price+1)處理。\n\n這個部分除了處理資料缺失值外，由於商品品牌數量過多，在程式運行時會佔據過多內存，導致程式無法運行，**因此只保留前2500大**，而2500個商品品牌已經包含了70%以上的商品數量。\n        \n        \n        "},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"748e60dfbd74ebbcfc641d5f14fad0f3a4e08585","_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"#-----------------載入套件-------------------------\nimport pandas as pd\nimport numpy as np\nimport scipy\n\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.preprocessing import LabelBinarizer\n\nfrom sklearn.linear_model import Ridge\nimport xgboost as xgb\nimport lightgbm as lgb\n\nfrom sklearn.model_selection import KFold\n\nfrom sklearn.metrics import mean_squared_error\n\nfrom scipy.optimize import minimize\n\nimport matplotlib.pyplot as plt\n\nimport gc\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"539da57fe375b9f63e9a83080579cd7b75b3c5c5","collapsed":true},"cell_type":"code","source":"#-------------------資料準備-----------------------\nprint(\"Reading in Data\")\ndf_train = pd.read_csv('../input/train.tsv', sep='\\t')\ndf_test = pd.read_csv('../input/test_stg2.tsv', sep='\\t')\n\nnrow_train=df_train.shape[0]\nnrow_test=df_test.shape[0]\nY_train=np.log1p(df_train[\"price\"])\n\nprint('Size of train set',nrow_train)\nprint('Size of test set',nrow_test)\n\ndf=pd.concat([df_train,df_test],axis=0,sort=True)\ndf_test=pd.DataFrame(df_test[\"test_id\"])\n\n\n#----------------處理缺失值--------------------\nNUM_BRAND=2500\ndf[\"category_name\"]=df[\"category_name\"].fillna(\"Other\").astype(\"category\")\n\ndf[\"brand_name\"]=df[\"brand_name\"].fillna(\"Unknown\")\npop_brands=df[\"brand_name\"].value_counts().index[1:NUM_BRAND+1]\ndf.loc[~df[\"brand_name\"].isin(pop_brands),\"brand_name\"]=\"Other\"\ndf[\"brand_name\"]=df[\"brand_name\"].astype(\"category\")\n\ndf[\"item_description\"]=df[\"item_description\"].fillna(\"None\")\n\ndf[\"item_condition_id\"]=df[\"item_condition_id\"].astype(\"category\")\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"trusted":true,"_uuid":"e2c1b6cac5955f10fa0eee2e86831d01b94b69da","collapsed":true},"cell_type":"code","source":"del df_train\ngc.collect","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ce834a2f328869ca29aefcdf42da7a05abc60f27"},"cell_type":"markdown","source":"**<font size=3>2. 特徵處理</font>**\n\n試想在網路商販售物品時，填寫哪個商品特徵欄位會讓你下最多著墨呢? 我想大部分的人都會回答商品描述吧，本次特徵的處理就是以這個想法出發，透過對**大部分的項目用CountVectorizer來做特徵提取**，**運費歸屬及商品狀況使用LabelBinarizer加註標籤**，最後，本次最重要的項目處理方式為，**TF-IDF對商品描述的加權特徵提取**(TfidfVectorizer)，實踐對這次特徵想法，最後再將特徵結合並轉換成稀疏矩正。"},{"metadata":{"trusted":true,"_uuid":"2d687312912d6947a0275661bd90e5ffd359061f","collapsed":true},"cell_type":"code","source":"#--------------------encode------------------------\nNAME_MIN_DF=10\nMAX_FEAT_DESCP=50000\nprint(\"Encodings\")\n\nprint(\"Condition Encoders\")\nvect_condition=LabelBinarizer(sparse_output=True)\nX_Condition=vect_condition.fit_transform(df[\"item_condition_id\"])\n\nprint(\"Shipping Encoders\")\nvect_shipping=LabelBinarizer(sparse_output=True)\nX_Shipping=vect_shipping.fit_transform(df[\"shipping\"])\n\nprint(\"Name Encoders\")\ncount_name=CountVectorizer(min_df=NAME_MIN_DF)\nX_name=count_name.fit_transform(df[\"name\"])\n\nprint(\"Category Encoders\")\ncount_category=CountVectorizer()\nX_category=count_category.fit_transform(df[\"category_name\"])\n\nprint(\"Brand Encoders\")\ncount_brand=CountVectorizer()\nX_brand=count_brand.fit_transform(df[\"brand_name\"])\n\n\nprint(\"Descp Encoders\")\ncount_descp=TfidfVectorizer(max_features=MAX_FEAT_DESCP,\n                            ngram_range=(1,3),\n                            stop_words=\"english\")\nX_descp=count_descp.fit_transform(df[\"item_description\"])\n\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"trusted":true,"collapsed":true,"_uuid":"e97c21f4c6c482408f49171659fb7c053b8eda8e"},"cell_type":"code","source":"del df\ngc.collect\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"collapsed":true,"_uuid":"05421a11f221864e0dfb15ae05bb7641064d8256"},"cell_type":"code","source":"X=scipy.sparse.hstack([X_Shipping,X_Condition,X_brand,\n                       X_category,X_descp,X_name]).tocsr()\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true,"collapsed":true,"_uuid":"39c7fc1583130547b49775915d332ffd235fe5b6"},"cell_type":"code","source":"del X_descp\ndel X_brand\ndel X_category\ndel X_name\ndel X_Shipping\ndel X_Condition\ngc.collect\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c840c222d94c368d463bc356e9c5b4c49332e9bd"},"cell_type":"code","source":"X_train=X[:nrow_train]\nX_test=X[nrow_train:]\ndtest=xgb.DMatrix(X_test)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true,"collapsed":true,"_uuid":"092bee6cde3e7df8cf85d346b4c1358f5743410b"},"cell_type":"code","source":"del X\ngc.collect","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9a7b7949231a52c6da81eaee5c6b2db27e843b67"},"cell_type":"markdown","source":"**<font size=3>3. 交叉驗證及預測</font>**\n\n在時間許可下，我使用**KFold來做Cross-Validation**，將訓練資料分成三個Fold，製作出三個模型，**將三次Validation的數值儲存，結合成完整預測資料**，並將其RMSLE分數計算出來，作為下個步驟的材料，同時分別**使用三個模型對測試資料做預測並取平均**。在上述的過程中，我們會以三個不同的演算法，**Ridge Regression、LightGBM、XGBoost**來執行，至於原因則會在下個步驟做解釋。\n\n![](https://i.imgur.com/8svMOQl.jpg)"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"37c06603573bc203ef7ffd65c1c0ff1427e45938"},"cell_type":"code","source":"#--------------------Cross-Validation------------------------\nprint(\"Cross-Validation\")\nxgb_pred_val_index = np.zeros(X_train.shape[0])\nridge_pred_val_index = np.zeros(X_train.shape[0])\nlgb_pred_val_index = np.zeros(X_train.shape[0])\nxgb_pred_all_sum=[]\nridge_pred_all_sum=[]\nlgb_pred_all_sum=[]\nxgb_cv_RMSLE_sum=0\nridge_cv_RMSLE_sum=0\nlgb_cv_RMSLE_sum=0\n\n\nfolds=3\nkf = KFold(n_splits=folds, random_state=1001)\nfor i, (train_index, val_index) in enumerate(kf.split(X_train, Y_train)):\n    x_train, x_val = X_train[train_index], X_train[val_index]\n    y_train, y_val = Y_train[train_index], Y_train[val_index]\n\n    dtrain=xgb.DMatrix(x_train,y_train)\n    dval=xgb.DMatrix(x_val)\n    deval=xgb.DMatrix(x_val,y_val)\n    \n    \n    params = {\n    'booster': 'gblinear',\n    'objective': 'reg:linear', \n    'gamma': 0,                \n    'max_depth': 10,           \n    'lambda': 0,                   \n    'subsample': 0.85,             \n    'colsample_bytree': 0.9,      \n    'min_child_weight': 17,\n    'silent': 1,                  \n    'eta': 0.4,                 \n    'seed': 1001,\n    'nthread': 4,                 \n    'eval_metric':'rmse'\n    }\n    \n    plst = params.items()\n    evallist = [(deval, 'eval'), (dtrain, 'train')]\n    num_round=300\n    \n    model=xgb.train(plst,dtrain,num_round,evallist, verbose_eval=100,early_stopping_rounds=100)\n    xgb_pred_val=model.predict(dval)\n    xgb_RMSLE=np.sqrt(mean_squared_error(xgb_pred_val,y_val))\n    print('\\n Fold %02d XGBoost RMSLE: %.6f' % ((i + 1), xgb_RMSLE))\n    xgb_pred_all=model.predict(dtest)\n    \n    del dtrain\n    del dval\n    del deval\n    gc.collect()\n    \n    params = {\n        'boosting': 'gbdt',\n        'max_depth': 7,\n        'min_data_in_leaf': 80,\n        'num_leaves': 30,\n        'learning_rate': 0.4,\n        'objective': 'regression',\n        'metric': 'rmse',\n        'nthread': 4,\n        'bagging_freq': 1,\n        'subsample': 0.9,\n        'colsample_bytree': 0.7,\n        'min_child_weight': 17,\n        'is_unbalance': False,\n        'verbose': -1,\n        'seed': 1001,\n        'max_bin':511,\n        'num_threads':4\n    }\n    \n    dtrain = lgb.Dataset(x_train, label=y_train)\n    deval = lgb.Dataset(x_val, label=y_val)\n    watchlist = [dtrain, deval]\n    watchlist_names = ['train', 'val']\n\n    model = lgb.train(params,\n    train_set=dtrain,\n    num_boost_round=3000,\n    valid_sets=watchlist,\n    valid_names=watchlist_names,\n    early_stopping_rounds=100,\n    verbose_eval=300)\n    lgb_pred_val = model.predict(x_val)\n    lgb_RMSLE = np.sqrt(mean_squared_error(lgb_pred_val,y_val))\n    print(' Fold %02d LightGBM RMSLE: %.6f' % ((i + 1), lgb_RMSLE))\n    lgb_pred_all = model.predict(X_test)\n    \n    del dtrain\n    del deval\n    gc.collect()\n    \n    \n    \n    model=Ridge(solver='sag',alpha=4.75)\n    model.fit(x_train,y_train)\n    ridge_pred_val=model.predict(x_val)\n    ridge_RMSLE=np.sqrt(mean_squared_error(ridge_pred_val,y_val))\n    print('\\n Fold %02d Ridge RMSLE: %.6f' % ((i + 1), ridge_RMSLE))\n    ridge_pred_all=model.predict(X_test)\n    \n    del x_train\n    del y_train\n    del x_val\n    del y_val\n    gc.collect()\n    \n    xgb_pred_val_index[val_index] = xgb_pred_val\n    ridge_pred_val_index[val_index] = ridge_pred_val\n    lgb_pred_val_index[val_index] = lgb_pred_val\n    \n    if i > 0:\n        xgb_pred_all_sum = xgb_pred_all_sum + xgb_pred_all\n        ridge_pred_all_sum = ridge_pred_all_sum + ridge_pred_all\n        lgb_pred_all_sum = lgb_pred_all_sum + lgb_pred_all\n    else:\n        xgb_pred_all_sum = xgb_pred_all\n        ridge_pred_all_sum = ridge_pred_all\n        lgb_pred_all_sum = lgb_pred_all\n    \n    xgb_cv_RMSLE_sum = xgb_cv_RMSLE_sum + xgb_RMSLE\n    ridge_cv_RMSLE_sum = ridge_cv_RMSLE_sum + ridge_RMSLE\n    lgb_cv_RMSLE_sum = lgb_cv_RMSLE_sum + lgb_RMSLE\n\nxgb_cv_avg_score=xgb_cv_RMSLE_sum/folds\nridge_cv_avg_score=ridge_cv_RMSLE_sum/folds\nlgb_cv_avg_score=lgb_cv_RMSLE_sum/folds\n\nxgb_val_real_RMSLE=np.sqrt(mean_squared_error(xgb_pred_val_index,Y_train))\nridge_val_real_RMSLE=np.sqrt(mean_squared_error(ridge_pred_val_index,Y_train))\nlgb_val_real_RMSLE=np.sqrt(mean_squared_error(lgb_pred_val_index,Y_train))\n\nprint('\\n Average XGBoost RMSLE(cv):\\t%.6f' % xgb_cv_avg_score)\nprint(' Out-of-fold XGBoost RMSLE:\\t%.6f' % xgb_val_real_RMSLE)\nprint('\\n Average LightGBM RMSLE(cv):\\t%.6f' % lgb_cv_avg_score)\nprint(' Out-of-fold LightGBM RMSLE:\\t%.6f' % lgb_val_real_RMSLE)\nprint('\\n Average Ridge RMSLE(cv):\\t%.6f' % ridge_cv_avg_score)\nprint(' Out-of-fold Ridge RMSLE:\\t%.6f' % ridge_val_real_RMSLE)\n\nxgb_pred_all_avg=xgb_pred_all_sum/folds\nridge_pred_all_avg=ridge_pred_all_sum/folds\nlgb_pred_all_avg=lgb_pred_all_sum/folds\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0d578dc355114cbcee432d171e679b26b257d4e1"},"cell_type":"markdown","source":"Cross-Validation中可以發現，LightGBM的成績是最優秀的，其次是Ridge Regression，最後才是XGBoost。LightGBM建模仍有更優化的可能，但由於時間關係疊代次數設為3000次，XGBoost建模過程雖然訓練誤差持續下降，但驗證誤差卻不斷上升，因此不得已捨棄更高的疊代次數。"},{"metadata":{"_uuid":"75570da2053e50c8402c8c45105bdbd10015abb6"},"cell_type":"markdown","source":"**<font size=3>4. 模型混合與最終預測</font>**\n\n完成步驟三之後，會有Ridge、XGBoost、LightGBM對訓練資料做出的預測，用**線性組合將三組資料的預測做混合，算出其錯誤率，找出最佳的模型比例，最後依比例做出測試資料的最終答案**。\n\n![](https://i.imgur.com/HzuS1XH.jpg)"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2b3eb7a0595e788c6a7facaf218a828d26b36d88"},"cell_type":"code","source":"#------------blend-------------------\ndef rmse_min_func(weights):\n    final_prediction=0\n    for weight,prediction in zip(weights,blend_train):\n        final_prediction+=weight*prediction\n    return np.sqrt(mean_squared_error(Y_train,final_prediction))\n\nblend_train = []\nblend_test = []\n\nblend_train.append(xgb_pred_val_index) \nblend_train.append(lgb_pred_val_index)\nblend_train.append(ridge_pred_val_index)\nblend_train=np.array(blend_train)\n\nblend_test.append(xgb_pred_all_avg)\nblend_test.append(lgb_pred_all_avg)\nblend_test.append(ridge_pred_all_avg)\nblend_test=np.array(blend_test)\n\nprint('\\n Finding Blending Weights ...')\n\nres_list=[]\nweight_list=[]\n\nfor k in range(20):\n    starting_value=np.random.uniform(-1,1,len(blend_train))\n    bounds=[(-1,1)]*len(blend_train)\n    \n    res=minimize(\n        rmse_min_func,\n        starting_value,\n        method='L-BFGS-B',\n        bounds=bounds,\n        options={'disp':False,\n        'maxiter':100000})\n    \n    res_list.append(res['fun'])\n    weight_list.append(res['x'])\n    print('{iter}\\tScore: {score}\\tWeights: {weights}'.format(\n        iter=(k+1),\n        score=round(res['fun'],6),\n        weights='\\t'.join([str(round(item,10)) for item in res['x']])))\n\nbestSC=np.min(res_list)\nbestweight=weight_list[np.argmin(res_list)]\n\nprint('\\n Ensemble Score:{best_score}'.format(best_score=bestSC))\nprint('\\n Best Weights:{weight}'.format(weight=bestweight))\n\n\ntest_price=np.zeros(len(blend_test[0]))\ntrain_price =np.zeros(len(blend_train[0]))\n\nprint('\\n Your final model:')\nfor k in range(len(blend_test)):\n    print('%.6f * model-%d'%(bestweight[k],(k+1)))\n    test_price+=blend_test[k]*bestweight[k]\n    train_price+= blend_train[k] * bestweight[k]\n\ndf_test[\"price\"]=np.expm1(test_price)\nprint(\"Generatig File\")\ndf_test[[\"test_id\",\"price\"]].to_csv(\"submission.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bc937395cecb3a91d2ddf07d3c8d977f2396ccea"},"cell_type":"markdown","source":"最終模型約為，**XGBoost 30%、LightGBM 60%、 Ridge Regression 10%**，與Cross-Validation中的驗證誤差並沒有直接相關，但LightGBM模型確實占了最大的比例。"},{"metadata":{"_uuid":"b2635ec199294141747aa63971c4740ba9f5a2a7"},"cell_type":"markdown","source":"**<font size=3>5. 模型檢視</font>**\n\n從圖形來看，**LightGBM與Ridge Regression對商品價格高估的情況較多，而XGBoost則是高估的比例較高**，由此可以解釋最後的模型比例，由**LightGBM跟XGBoost來做互補**，而準確率較高LightGBM占最多比例。最終的模型上在低估狀況改善許多。"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"93a152f2b4c1bd3d0a1494ab85863db09a2b4bee"},"cell_type":"code","source":"#------------------模型檢視---------------------------\nprint('\\n Making scatter plots of actual vs. predicted prices ...')\nx_true = np.expm1(Y_train)\nx_pred = np.expm1(xgb_pred_val_index)\ncm = plt.cm.get_cmap('RdYlBu')\n# Normalized prediction error clipped so the color-coding covers -75% to 75% range\nx_diff = np.clip(100 * ((x_pred - x_true) / x_true), -75, 75)\nplt.figure(1, figsize=(12, 10))\nplt.title('Actual vs. Predicted Prices - XGBoost')\nplt.scatter(x_true, x_pred, c=x_diff, s=10, cmap=cm)\nplt.colorbar()\nplt.plot([x_true.min() - 50, x_true.max() + 50],\n         [x_true.min() - 50, x_true.max() + 50],\n         'k--',lw=1)\nplt.xlabel('Prices')\nplt.ylabel('Predicted Prices')\nplt.xlim(-50, 2050)\nplt.ylim(-50, 2050)\nplt.tight_layout()\nplt.show()\n\n\nx_pred = np.expm1(lgb_pred_val_index)\n# Normalized prediction error clipped so the color-coding covers -75% to 75% range\nx_diff = np.clip(100 * ((x_pred - x_true) / x_true), -75, 75)\nplt.figure(1, figsize=(12, 10))\nplt.title('Actual vs. Predicted Prices - LightGBM')\nplt.scatter(x_true, x_pred, c=x_diff, s=10, cmap=cm)\nplt.colorbar()\nplt.plot([x_true.min() - 50, x_true.max() + 50],\n         [x_true.min() - 50, x_true.max() + 50],\n         'k--',lw=1)\nplt.xlabel('Prices')\nplt.ylabel('Predicted Prices')\nplt.xlim(-50, 2050)\nplt.ylim(-50, 2050)\nplt.tight_layout()\nplt.show()\n\n\nx_pred = np.expm1(ridge_pred_val_index)\n# Normalized prediction error clipped so the color-coding covers -75% to 75% range\nx_diff = np.clip(100 * ((x_pred - x_true) / x_true), -75, 75)\nplt.figure(1, figsize=(12, 10))\nplt.title('Actual vs. Predicted Prices - Ridge Regression')\nplt.scatter(x_true, x_pred, c=x_diff, s=10, cmap=cm)\nplt.colorbar()\nplt.plot([x_true.min() - 50, x_true.max() + 50],\n         [x_true.min() - 50, x_true.max() + 50],\n         'k--',lw=1)\nplt.xlabel('Prices')\nplt.ylabel('Predicted Prices')\nplt.xlim(-50, 2050)\nplt.ylim(-50, 2050)\nplt.tight_layout()\nplt.show()\n\n\nx_pred = np.expm1(train_price)\n# Normalized prediction error clipped so the color-coding covers -75% to 75% range\nx_diff = np.clip(100 * ((x_pred - x_true) / x_true), -75, 75)\nplt.figure(4, figsize=(12, 10))\nplt.title('Actual vs. Predicted Prices - Blend')\nplt.scatter(x_true, x_pred, c=x_diff, s=10, cmap=cm)\nplt.colorbar()\nplt.plot([x_true.min() - 50, x_true.max() + 50],\n         [x_true.min() - 50, x_true.max() + 50],\n         'k--',lw=1)\nplt.xlabel('Prices')\nplt.ylabel('Predicted Prices')\nplt.xlim(-50, 2050)\nplt.ylim(-50, 2050)\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"edf877d15f40d08e2f1f6046ce51aa905560fa14"},"cell_type":"markdown","source":"**<font size=6>後記</font>**\n\n在特徵處理方面，我還試過將商品描述做**詞幹提取(Stemming)**，更精準的提取描述重點，但是這麼做使得內存無法負荷且時間過長，因此就放棄了這個做法。\n\n這次比賽的做法仍有很多可修正的方式，例如**研究不同演算法之間的配適程度、以更複雜的方式混合模型**，找出更好的演算法組合，但對於我目前的能力還沒辦法完成，所以就當作給大家優化的參考，**本次解答最後得到0.44327分，排名約為288名**(比賽結束後完成，無明確名次)，第一名分數為0.37758，參加隊伍為2384組。\n![](https://i.imgur.com/s2qtX6w.jpg)\n\n<font size=1>比賽網址:https://www.kaggle.com/c/mercari-price-suggestion-challenge</font>"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}