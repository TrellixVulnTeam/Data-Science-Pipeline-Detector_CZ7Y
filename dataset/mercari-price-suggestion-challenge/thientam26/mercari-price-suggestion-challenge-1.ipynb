{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport gc\nimport glob\nimport os\nimport json\n\nimport pprint\n\nfrom joblib import Parallel, delayed\nfrom tqdm import tqdm\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.applications.densenet import preprocess_input, DenseNet121\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Input, Lambda, AveragePooling1D, \\\n    MaxPooling1D, Dense, BatchNormalization, Dropout, Embedding, Reshape, Concatenate\nfrom tensorflow.keras import losses\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras import regularizers\nimport tensorflow.keras.backend as K\n\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD, NMF\nfrom sklearn.metrics import cohen_kappa_score, mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\n\nimport scipy as sp\n\nfrom collections import Counter\nfrom functools import partial\nfrom math import sqrt\n\nfrom sklearn.metrics import cohen_kappa_score, mean_squared_error\nfrom sklearn.metrics import confusion_matrix as sk_cmatrix\nimport lightgbm as lgb\n\nfrom gensim.models import KeyedVectors\n\nimport nltk\nimport string\nimport re\nimport pickle\n#import lda\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"white\")\n\nfrom nltk.stem.porter import *\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction import stop_words\n\nfrom collections import Counter\nfrom wordcloud import WordCloud\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n%matplotlib inline\n\nimport bokeh.plotting as bp\nfrom bokeh.models import HoverTool, BoxSelectTool\nfrom bokeh.models import ColumnDataSource\nfrom bokeh.plotting import figure, show, output_notebook\n#from bokeh.transform import factor_cmap\n\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\nfrom sklearn.model_selection import train_test_split\nimport math\nfrom subprocess import check_output\nfrom tensorflow.keras.preprocessing.text import Tokenizer\n\nimport warnings\nwarnings.filterwarnings('ignore')\nimport logging\nlogging.getLogger(\"lda\").setLevel(logging.WARNING)\n\nimport zipfile\nfrom subprocess import check_output\nfrom keras.preprocessing.sequence import pad_sequences\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Exploratory Data Analysis**\nOn the first look at the data, besides the unique identifier (item_id), there are 7 variables in this model. This notebook will sequentially go through each of them with a brief statistical summary. \n\n1. **Numerical/Continuous Features**\n    1. price: the item's final bidding price. This will be our reponse / independent variable that we need to predict in the test set\n    2. shipping cost     \n \n1. **Categorical Features**: \n    1. shipping cost: A binary indicator, 1 if shipping fee is paid by seller and 0 if it's paid by buyer\n    2. item_condition_id: The condition of the items provided by the seller\n    1. name: The item's name\n    2. brand_name: The item's producer brand name\n    2. category_name: The item's single or multiple categories that are separated by \"\\\" \n    3. item_description: A short description on the item that may include removed words, flagged by [rm]"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/mercari/train.tsv', sep='\\t')\ntest = pd.read_csv('/kaggle/input/mercari/test.tsv', sep='\\t')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Missing values of data train\\n',train.isnull().sum())\nprint('----------------------')\nprint('Missing values of data test\\n',test.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_mis_values(dataset):\n    dataset.category_name.fillna(value = 'missing', inplace = True)\n    dataset.brand_name .fillna(value = 'missing', inplace = True)\n    dataset.item_description .fillna(value = 'missing', inplace = True)\n    return dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = process_mis_values(train)\ntest = process_mis_values(test)\nprint(train.isnull().sum())\nprint(test.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check types of variable\n# Numeric variable\nnumber = [f for f in train.columns if train.dtypes[f] != 'object']\n# Object variable\nobjects = [f for f in train.columns if train.dtypes[f] == 'object']\nprint(number)\nprint(objects)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  Process categorical variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"le = LabelEncoder()\nle.fit(np.hstack([train.category_name,test.category_name]))\ntrain.category_name = le.transform(train.category_name)\ntest.category_name = le.transform(test.category_name)\n\nle.fit(np.hstack([train.brand_name,test.brand_name]))\ntrain.brand_name = le.transform(train.brand_name)\ntest.brand_name = le.transform(test.brand_name)\n\ndel le","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Process text"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Text to sequence processing\ntoken = Tokenizer()\nraw_text = np.hstack([train.item_description.str.lower(), test.item_description.str.lower()])\ntoken.fit_on_texts(raw_text)\n\ntrain['seq_item_descri'] = token.texts_to_sequences(train.item_description.str.lower())\ntest['seq_item_descri'] = token.texts_to_sequences(test.item_description.str.lower())\ntrain['seq_name'] = token.texts_to_sequences(train.name.str.lower())\ntest['seq_name'] = token.texts_to_sequences(test.name.str.lower())\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(len(train.seq_item_descri.max()))\n#print(type(train.seq_name[2]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(token.word_index)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Sequence Analyst"},{"metadata":{"trusted":true},"cell_type":"code","source":"max_name_seq = np.max([np.max(train.seq_name.apply(lambda x: len(x))),np.max(test.seq_name.apply(lambda x:len(x)))])\nmax_seq_item_descri = np.max([np.max(train.seq_item_descri.apply(lambda x: len(x))),np.max(test.seq_item_descri.apply(lambda x:len(x)))])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('max name seq', max_name_seq)\nprint('max seq item descri',max_seq_item_descri )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.seq_name.apply(lambda x: len(x)).hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.seq_item_descri.apply(lambda x: len(x)).hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Base on the histograms, we select the next lengths\nmax_name_seq = 10\nmax_descri = 100\nmax_text = np.max([np.max(train.seq_name.max()),\n                np.max(test.seq_name.max()),\n                np.max(train.seq_item_descri.max()),\n                np.max(test.seq_item_descri.max())])+2\nmax_categoty = np.max([np.max(train.category_name),np.max(test.category_name)])+1\nmax_brand = np.max([np.max(train.brand_name), np.max(train.brand_name)])+1\nmax_condition = np.max([train.item_condition_id.max(), test.item_condition_id.max()])+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(max_text)\nprint(max_categoty)\nprint(max_brand)\nprint(max_condition)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#SCALE target variable\ntrain[\"target\"] = np.log(train.price+1)\ntarget_scaler = MinMaxScaler(feature_range=(-1, 1))\ntrain[\"target\"] = target_scaler.fit_transform(train.target.values.reshape(-1,1))\npd.DataFrame(train.target).hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#EXTRACT DEVELOPTMENT TEST\n#dtrain, dvalid = train_test_split(train[['train_id','brand_name','category_name','item_condition_id','price','shipping','seq_item_descri','seq_name']], random_state=123, train_size=0.99)\n#print(dtrain.shape)\n#print(dvalid.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_keras_data(dataset):\n    df_name = pd.DataFrame(data=pad_sequences(dataset.seq_name, maxlen=max_name_seq),index = train['train_id'], columns=['name_factor' + '_' + str(k) for k in range(max_name_seq)])\n    df_name = df_name.reset_index()\n    df_item = pd.DataFrame(data=pad_sequences(dataset.seq_item_descri, maxlen=max_descri),index = train['train_id'], columns=['item_factor' + '_' + str(k) for k in range(max_descri)])\n    df_item = df_item.reset_index()\n    X = dataset[['train_id','item_condition_id','brand_name','category_name','shipping','target']]\n    X_1 = pd.merge(df_name,df_item, on = 'train_id')\n    X_final = pd.merge(X,X_1, on = 'train_id')\n    X_final = X_final.drop('train_id',axis = 1)\n    return X_final\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = get_keras_data(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_keras_data_stg2(dataset):\n    df_name = pd.DataFrame(data=pad_sequences(dataset.seq_name, maxlen= max_name_seq),index = test['test_id'], columns=['name_factor' + '_' + str(k) for k in range(max_name_seq)])\n    df_name = df_name.reset_index()\n    df_item = pd.DataFrame(data=pad_sequences(dataset.seq_item_descri, maxlen= max_descri),index = test['test_id'], columns=['item_factor' + '_' + str(k) for k in range(max_descri)])\n    df_item = df_item.reset_index()\n    X = dataset[['test_id','item_condition_id','brand_name','category_name','shipping']]\n    X_1 = pd.merge(df_name,df_item, on = 'test_id')\n    X_final = pd.merge(X,X_1, on = 'test_id')\n    X_final = X_final.drop('test_id',axis = 1)\n    return X_final","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_stg2 = get_keras_data_stg2(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_stg2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model base\n# LightGBM\nimport lightgbm as lgb\n\nparams = {'application': 'regression',\n          'boosting': 'gbdt',\n          'metric': 'rmse',\n          'num_leaves': 70,\n          'max_depth': 9,\n          'learning_rate': 0.5,\n          'bagging_fraction': 0.85,\n          'feature_fraction': 0.8,\n          'min_split_gain': 0.02,\n          'min_child_samples': 150,\n          'min_child_weight': 0.02,\n          'lambda_l2': 0.0475,\n          'verbosity': -1,\n          'data_random_seed': 17,\n          'tree learner':'feature'\n          }\n\n# Additional parameters:\nearly_stop = 1000\nverbose_eval = 50\nnum_rounds = 5000\nn_splits = 5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def LGBM_train(X_train, X_test):\n    \n    kfold = KFold(n_splits, random_state = 1337 )\n    oof_train = np.zeros([X_train.shape[0]])\n    oof_test = np.zeros([X_test.shape[0], n_splits])\n    i = 0\n    for train_index, valid_index in kfold.split(X_train, X_train['target'].values):\n    \n        X_tr = X_train.iloc[train_index, :]\n        X_val = X_train.iloc[valid_index, :]\n\n        y_tr = X_tr['target'].values\n        X_tr = X_tr.drop(['target'], axis=1)\n\n        y_val = X_val['target'].values\n        X_val = X_val.drop(['target'], axis=1)\n    \n#         print('\\ny_tr distribution: {}'.format(Counter(y_tr)))\n\n        d_train = lgb.Dataset(X_tr, label=y_tr)\n        d_valid = lgb.Dataset(X_val, label=y_val)\n        watchlist = [d_train, d_valid]\n\n        print('training LGB:')\n        model = lgb.train(params,\n                          train_set=d_train,\n                          num_boost_round=num_rounds,\n                          valid_sets=watchlist,\n                          verbose_eval=verbose_eval,\n                          early_stopping_rounds=early_stop)\n\n        val_pred = model.predict(X_val, num_iteration = model.best_iteration)\n        test_pred = model.predict(X_test, num_iteration = model.best_iteration)\n\n        oof_train[valid_index] = val_pred\n        oof_test[:,i] = test_pred\n        i +=1\n        \n    return oof_train, oof_test, model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oof_train, oof_test, model = LGBM_train(X,X_stg2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_preds = target_scaler.inverse_transform(oof_test)\nval_preds = np.expm1(val_preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"A = pd.DataFrame(data = val_preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"A","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"A['final_price'] = A.mean(axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"A","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({'test_id': test['test_id'], 'price': A['final_price']})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}