{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install py7zr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport scipy\nimport sqlite3\nimport pandas as pd\nimport numpy as np\nimport nltk\nimport string\nimport datetime\nimport time\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport xgboost as xgb\nimport gc\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport shutil\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import mean_squared_error\nfrom py7zr import unpack_7zarchive\nimport math\nimport re\n# Tutorial about Python regular expressions: https://pymotw.com/2/re/\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom scipy.sparse import csr_matrix\nfrom scipy.sparse import hstack\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.sparse import coo_matrix, hstack\nfrom gensim.models import Word2Vec\nfrom gensim.models import KeyedVectors\nfrom prettytable import PrettyTable\nfrom sklearn.linear_model import RidgeCV\nimport pickle\nimport zipfile\nfrom tqdm import tqdm\nimport os\nfrom sklearn.linear_model import Ridge\nfrom os import path\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom stop_words import get_stop_words\nfrom collections import Counter\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import TensorBoard,EarlyStopping,ModelCheckpoint\nfrom tensorflow.keras.layers import Input, Embedding, GRU, Dense,Flatten\nfrom tensorflow.keras.models import Model,load_model\nfrom numpy import zeros\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.utils import plot_model\nfrom contextlib import contextmanager\ntf.keras.backend.clear_session()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_from_archive():\n    #https://stackoverflow.com/questions/50745486/how-to-use-pyunpack-to-unpack-7z-file\n    if not os.path.exists('/kaggle/working/train/'):\n        os.makedirs('/kaggle/working/train/')\n    shutil.register_unpack_format('7zip', ['.7z'], unpack_7zarchive)\n    shutil.unpack_archive('/kaggle/input/mercari-price-suggestion-challenge/train.tsv.7z', '/kaggle/working/train/')\n    shutil.unpack_archive('/kaggle/input/mercari-price-suggestion-challenge/test_stg2.tsv.zip', '/kaggle/working/test/')\n\nextract_from_archive()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#<h2>**2 Utility Functions:**</h2>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def split_categories(category):\n    '''\n    function that splits the category column in the dataset and creates 3 new columns:\n    'main_category','sub_cat_1','sub_cat_2'\n    '''\n    try:\n        sub_cat_1,sub_cat_2,sub_cat_3 = category.split(\"/\")\n        return sub_cat_1,sub_cat_2,sub_cat_3\n    except:\n        return (\"No label\",\"No label\",\"No label\")\n\ndef create_split_categories(data):\n    '''\n    function that creates 3 new columns using split_categories function\n    : 'main_category','sub_cat_1','sub_cat_2'\n    '''\n    data['main_category'],data['sub_cat_1'],data['sub_cat_2']=zip(*data['category_name'].\\\n                                                                  apply(lambda x: split_categories(x)))\n\ndef log_price(price):\n    return np.log1p(price)#changes\n\n#https://www.kaggle.com/valkling/mercari-rnn-2ridge-models-with-notes-0-42755\ndef countwords(text):\n    try:\n        if text == 'No description yet':\n            return 0\n        else:\n            text = text.lower()\n            words = [w for w in text.split(\" \")]\n            return len(words)\n    except: \n        return 0\n\ndef fill_nan(dataset):\n    '''\n    Function to fill the NaN values in various columns\n    '''\n    dataset[\"item_description\"].fillna(\"No description yet\",inplace=True)\n    dataset[\"brand_name\"].fillna(\"missing\",inplace=True)\n    dataset[\"category_name\"].fillna(\"missing\",inplace=True)\n\ndef get_dummies_item_id_shipping(df):\n    df['item_condition_id'] = df[\"item_condition_id\"].astype(\"category\")\n    df['shipping'] = df[\"shipping\"].astype(\"category\")\n    item_id_shipping = csr_matrix(pd.get_dummies(df[['item_condition_id', 'shipping']],\\\n                                                      sparse=True).values)\n    return item_id_shipping\n\ndef one_hot_encode(train,test):\n    '''\n    Function to one hot encode the categorical columns\n    '''\n    vectorizer = CountVectorizer(token_pattern='.+')\n    \n    vectorizer = vectorizer.fit(train['category_name'].values) # fit has to happen only on train data\n    column_cat = vectorizer.transform(test['category_name'].values)\n\n    #vectorizing the main_category column\n    vectorizer = vectorizer.fit(train['main_category'].values) # fit has to happen only on train data\n    column_mc = vectorizer.transform(test['main_category'].values)\n    \n    #vectorizing sub_cat_1 column\n    vectorizer = vectorizer.fit(train['sub_cat_1'].values) # fit has to happen only on train data\n    column_sb1 = vectorizer.transform(test['sub_cat_1'].values)\n    \n    #vectorizing sub_cat_2 column\n    vectorizer = vectorizer.fit(train['sub_cat_2'].values) # fit has to happen only on train data\n    column_sb2 = vectorizer.transform(test['sub_cat_2'].values)\n\n    #vectorizing brand column\n    vectorizer = vectorizer.fit(train['brand_name'].astype(str)) # fit has to happen only on train data\n    brand_encodes = vectorizer.transform(test['brand_name'].astype(str))\n\n    #https://github.com/numpy/numpy/issues/11666\n   # vectorizer = CountVectorizer(vocabulary= list(train['shipping'].unique()),binary = True)\n    #shipping\n    #vectorizer = vectorizer.fit(train['shipping'].astype(str)) # fit has to happen only on train data\n    #column_shipping = vectorizer.transform(test['shipping'].astype(str))\n    \n    #vectorizer = CountVectorizer(vocabulary= list(train['item_condition_id'].unique()),binary = True)\n    #item_condition_id\n    #vectorizer = vectorizer.fit(train['item_condition_id'].astype(str)) # fit has to happen only on train data\n    #column_item_id = vectorizer.transform(test['item_condition_id'].astype(str))\n    \n    print(\"created OHE columns for main_category,sub_cat_1,sub_cat_2\\n\")\n    print(column_cat.shape)\n    print(column_mc.shape)\n    print(column_sb1.shape)\n    print(column_sb2.shape)\n    print(brand_encodes.shape)\n    #print(column_shipping.shape)\n    #print(column_item_id.shape)\n    print(\"=\"*100)\n    return column_cat,column_mc,column_sb1,column_sb2,brand_encodes\n\ndef rank_category(dataset,column_name):\n    '''This function takes a column name which is categorical and returns the categories with rank'''\n    counter = dataset[column_name].value_counts().index.values\n    total = list(dataset[column_name])\n    ranked_cat = {}\n    for i in range(1,len(counter)+1):\n        ranked_cat.update({counter[i-1] : i})\n    return ranked_cat,len(counter)\n\ndef encode_ranked_category(train,test,column):\n    '''\n    This function calls the rank_category function and returns the encoded category column    '''\n    train[column] = train[column].astype('category')\n    test[column] = test[column].astype('category')\n    \n    cat_list = list(train[column].unique())\n    ranked_cat_tr,count = rank_category(train,column)\n\n    encoded_col_tr = []\n    encoded_col_te = []\n\n    for category in train[column]:\n        encoded_col_tr.append(ranked_cat_tr[category])\n\n    for category in test[column]:\n        if category in cat_list:\n            encoded_col_te.append(ranked_cat_tr[category])\n        else:\n            encoded_col_te.append(0)\n    \n    encoded_col_tr = np.asarray(encoded_col_tr)\n    encoded_col_te = np.asarray(encoded_col_te)\n    return encoded_col_tr,encoded_col_te,count\n\ndef tokenize_text(train,test,column):\n    global t\n    t = Tokenizer()\n    t.fit_on_texts(train[column].str.lower())\n    vocab_size = len(t.word_index) + 1\n    # integer encode the documents\n    encoded_text_tr = t.texts_to_sequences(train[column].str.lower())\n    encoded_text_te = t.texts_to_sequences(test[column].str.lower())\n    return encoded_text_tr,encoded_text_te,vocab_size\n\ndef data_gru(train,test):\n    \n    global max_length,desc_size,name_size\n    encoded_brand_tr,encoded_brand_te,brand_len = encode_ranked_category(train,test,'brand_name')\n    #encoded_cat_tr,encoded_cat_te,cat_len = encode_ranked_category(train,test,'category_name')\n    encoded_main_cat_tr,encoded_main_cat_te,main_cat_len = encode_ranked_category(train,test,'main_category')\n    encoded_sub_cat_1_tr,encoded_sub_cat_1_te,sub_cat1_len = encode_ranked_category(train,test,'sub_cat_1')\n    encoded_sub_cat_2_tr,encoded_sub_cat_2_te,sub_cat2_len = encode_ranked_category(train,test,'sub_cat_2')\n    \n    tokenized_desc_tr,tokenized_desc_te,desc_size = tokenize_text(train,test,'item_description')\n     \n    tokenized_name_tr,tokenized_name_te,name_size = tokenize_text(train,test,'name')\n      \n    max_length = 160\n    desc_tr_padded = pad_sequences(tokenized_desc_tr, maxlen=max_length, padding='post')\n    desc_te_padded = pad_sequences(tokenized_desc_te, maxlen=max_length, padding='post')\n    del tokenized_desc_tr,tokenized_desc_te\n\n    name_tr_padded = pad_sequences(tokenized_name_tr, maxlen=10, padding='post')\n    name_te_padded = pad_sequences(tokenized_name_te, maxlen=10, padding='post')\n    del tokenized_name_tr,tokenized_name_te\n\n    gc.collect()\n\n    train_inputs = [name_tr_padded,desc_tr_padded,encoded_brand_tr.reshape(-1,1),\\\n                    encoded_main_cat_tr.reshape(-1,1),encoded_sub_cat_1_tr.reshape(-1,1),\\\n                    encoded_sub_cat_2_tr.reshape(-1,1),train['shipping'],\\\n                    train['item_condition_id'],train['wc_desc'],\\\n                    train['wc_name']]\n    test_inputs = [name_te_padded,desc_te_padded,encoded_brand_te.reshape(-1,1),\\\n                    encoded_main_cat_te.reshape(-1,1),encoded_sub_cat_1_te.reshape(-1,1),\\\n                    encoded_sub_cat_2_te.reshape(-1,1),test['shipping'],\\\n                    test['item_condition_id'],test['wc_desc'],\\\n                    test['wc_name']]\n    \n    item_condition_counter = train['item_condition_id'].value_counts().index.values\n\n    list_var = [brand_len,main_cat_len,sub_cat1_len,sub_cat2_len,len(item_condition_counter)]\n    \n    return train_inputs,test_inputs,list_var\n\ndef construct_GRU(train,var_list,drop_out_list):\n    #GRU input layer for name\n    input_name =  tf.keras.layers.Input(shape=(10,), name='name')\n    embedding_name = tf.keras.layers.Embedding(name_size, 20)(input_name)\n    gru_name = tf.keras.layers.GRU(8)(embedding_name)\n    #flatten1 = tf.keras.layers.Flatten()(lstm_out)\n\n    #GRU input layer for description\n    input_desc =  tf.keras.layers.Input(shape=(max_length,), name='desc')\n    embedding_desc = tf.keras.layers.Embedding(desc_size, 60)(input_desc)\n    gru_desc = tf.keras.layers.GRU(16)(embedding_desc)\n\n    #input layer for brand_name\n    input_brand =  tf.keras.layers.Input(shape=(1,), name='brand')\n    embedding_brand = tf.keras.layers.Embedding(var_list[0] + 1, 10)(input_brand)\n    flatten1 = tf.keras.layers.Flatten()(embedding_brand)\n\n    #categorical input layer main_category\n    input_cat = tf.keras.layers.Input(shape=(1,), name='main_cat')\n    Embed_cat = tf.keras.layers.Embedding(var_list[1] + 1, \\\n                                          10,input_length=1)(input_cat)\n    flatten2 = tf.keras.layers.Flatten()(Embed_cat)\n\n    #categorical input layer sub_cat_1\n    input_subcat1 = tf.keras.layers.Input(shape=(1,), name='subcat1')\n    Embed_subcat1 = tf.keras.layers.Embedding(var_list[2] + 1, \\\n                                              10,input_length=1)(input_subcat1)\n    flatten3 = tf.keras.layers.Flatten()(Embed_subcat1)\n\n    #categorical input layer sub_cat_2\n    input_subcat2 = tf.keras.layers.Input(shape=(1,), name='subcat2')\n    Embed_subcat2 = tf.keras.layers.Embedding(var_list[3] + 1, \\\n                                              10,input_length=1)(input_subcat2)\n    flatten4 = tf.keras.layers.Flatten()(Embed_subcat2)\n\n    #categorical input layer shipping\n    input_shipping = tf.keras.layers.Input(shape=(1,), name='shipping')\n    # Embed_shipping = tf.keras.layers.Embedding(var_list[3] + 1, \\\n    #                                            7,input_length=1)(input_shipping)\n    # flatten5 = tf.keras.layers.Flatten()(Embed_shipping)\n\n    #categorical input layer item_condition_id\n    input_item = tf.keras.layers.Input(shape=(1,), name='item_condition_id')\n    Embed_item = tf.keras.layers.Embedding(var_list[4] + 1, \\\n                                           5,input_length=1)(input_item)\n    flatten5 = tf.keras.layers.Flatten()(Embed_item)\n\n    #numerical input layer\n    desc_len_input = tf.keras.layers.Input(shape=(1,), name='description_length')\n    desc_len_embd = tf.keras.layers.Embedding(DESC_LEN,5)(desc_len_input)\n    flatten6 = tf.keras.layers.Flatten()(desc_len_embd)\n\n    #name_len input layer\n    name_len_input = tf.keras.layers.Input(shape=(1,), name='name_length')\n    name_len_embd = tf.keras.layers.Embedding(NAME_LEN,5)(name_len_input)\n    flatten7 = tf.keras.layers.Flatten()(name_len_embd)\n\n    # concatenating the outputs\n    concat_layer = tf.keras.layers.concatenate(inputs=[gru_name,gru_desc,flatten1,flatten2,flatten3,flatten4,input_shipping,flatten5,\\\n                                                       flatten6,flatten7],name=\"concatenate\")\n    #dense layers\n    Dense_layer1 = tf.keras.layers.Dense(units=512,activation='relu',kernel_initializer='he_normal',\\\n                                         name=\"Dense_l\")(concat_layer)\n    dropout_1 = tf.keras.layers.Dropout(drop_out_list[0],name='dropout_1')(Dense_layer1)\n    batch_n1 = tf.keras.layers.BatchNormalization()(dropout_1)\n    \n    Dense_layer2 = tf.keras.layers.Dense(units=256,activation='relu',kernel_initializer='he_normal',\\\n                                         name=\"Dense_2\")(batch_n1)\n    dropout_2 = tf.keras.layers.Dropout(drop_out_list[1],name='dropout_2')(Dense_layer2)\n    batch_n2 = tf.keras.layers.BatchNormalization()(dropout_2)\n\n    Dense_layer3 = tf.keras.layers.Dense(units=128,activation='relu',kernel_initializer='he_normal',\\\n                                         name=\"Dense_3\")(batch_n2)\n    dropout_3 = tf.keras.layers.Dropout(drop_out_list[2],name='dropout_3')(Dense_layer3)\n\n    Dense_layer4 = tf.keras.layers.Dense(units=64,activation='relu',kernel_initializer='he_normal',\\\n                                         name=\"Dense_4\")(dropout_3)\n    dropout_4 = tf.keras.layers.Dropout(drop_out_list[3],name='dropout_4')(Dense_layer4)\n    \n    #output_layer\n    final_output = tf.keras.layers.Dense(units=1,activation='linear',name='output_layer')(dropout_4)\n\n    model = tf.keras.Model(inputs=[input_name,input_desc,input_brand,input_cat,input_subcat1,input_subcat2,\\\n                                   input_shipping,input_item,desc_len_input,name_len_input],\n                           outputs=[final_output])\n    # we specified the model input and output\n    print(model.summary())\n#    img_path = \"GRU_model_2_lr.png\"\n#    plot_model(model, to_file=img_path, show_shapes=True, show_layer_names=True) \n    return model\n\n#https://www.tensorflow.org/guide/keras/train_and_evaluate\ndef compile_predict_GRU(train_input,test_input,y_train,variable_list,drop_list):\n    filepath=\"GRU_lr-{epoch:03d}-{val_loss:.3f}.hdf5\"\n    model_checkpoint = ModelCheckpoint(filepath, monitor='val_loss', mode='min', save_best_only=True)\n    #tensorboard = tf.keras.callbacks.TensorBoard(log_dir='/kaggle/working/',\\\n     #                                        write_graph=True)\n\n    #call_backs = [model_checkpoint,tensorboard]\n    #https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LearningRateScheduler\n    def scheduler(epoch):\n        if epoch < 2:\n            return 0.005\n        else:\n            return 0.001\n\n    lr_schedule = tf.keras.callbacks.LearningRateScheduler(scheduler)\n    call_backs = [model_checkpoint,lr_schedule]\n    #call_backs = [model_checkpoint]\n    #https://machinelearningmastery.com/use-different-batch-sizes-training-predicting-python-keras/\n    x_train = train_input\n    model= construct_GRU(x_train,variable_list,drop_list)\n    model.compile(optimizer='adam', loss='mean_squared_error')\n    # for i in range(3):\n    #     with timer(f'epoch {i + 1}'):\n    #         model_history= model.fit(x_train, y_train,\\\n    #                                            batch_size=2**(8 + i),\\\n    #                                            epochs=1, verbose=1,\\\n    #                                            callbacks=call_backs,\\\n    #                                            validation_split = 0.1)\n    \n    model.fit(x_train, y_train, epochs=3,batch_size=2**10, callbacks=call_backs,validation_split=0.1)\n\n    #pred_on_train = model.predict(x_train,batch_size = 2**10,verbose = 1)\n    pred_on_test = model.predict(test_input,batch_size = 2**10,verbose = 1)\n    return pred_on_test\n\ndef get_tensorboard_ready():\n    ! rm -rf ./logs/ \n    logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n    #logdir = \"\"\n    print(logdir)\n    return logdir\n\ndef hyperparameter_tuning_random(x,y,model_estimator,param_dict,cv_no):\n    start = time.time()\n    hyper_tuned = GridSearchCV(estimator = model_estimator, param_grid = param_dict,\\\n                                    return_train_score=True, scoring = 'neg_mean_squared_error',\\\n                                    cv = cv_no, \\\n                                    verbose=2, n_jobs = -1)\n    hyper_tuned.fit(x,y)\n    print(\"\\n######################################################################\\n\")\n    print ('Time taken for hyperparameter tuning is {} sec\\n'.format(time.time()-start))\n    print('The best parameters_: {}'.format(hyper_tuned.best_params_))\n    return hyper_tuned.best_params_\n\ndef rmsle_compute(y_true, y_pred):\n    assert len(y_true) == len(y_pred)\n    score = np.sqrt(np.mean(np.power(np.log1p(y_pred) - np.log1p(y_true), 2)))\n    return score\n\ndef scale_back(x):\n    '''\n    Function to inverse transform the scaled values\n    '''\n    x= np.expm1(y_scalar.inverse_transform(x.reshape(-1,1))[:,0])#changes\n    return x\n\n@contextmanager\ndef timer(name):\n    t0 = time.time()\n    yield\n    print(f'[{name}] done in {time.time() - t0:.0f} s')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def function_1(x):\n    ############################\n    #step1: load the train file\n    ############################\n    gc.collect()\n    train = pd.read_csv('/kaggle/working/train/train.tsv',sep='\\t')\n    test = x\n    print(\"Finished loading the files....\\n\")\n    print(\"train: {0}\\ntest: {1}\\n\".format(train.shape,test.shape))\n    ##########################################################\n    #step2: Data cleaning and preprocessing of train and test\n    ##########################################################\n    #https://www.kaggle.com/valkling/mercari-rnn-2ridge-models-with-notes-0-42755\n    train = train.drop(train[(train.price < 3.0)].index)\n    fill_nan(train)\n    fill_nan(test)\n    print(\"filled nan\\n\")\n\n    train['wc_desc'] = train['item_description'].apply(lambda x: countwords(x))\n    test['wc_desc'] = test['item_description'].apply(lambda x: countwords(x))\n    train['wc_name'] = train['name'].apply(lambda x: countwords(x))\n    test['wc_name'] = test['name'].apply(lambda x: countwords(x))\n    create_split_categories(train)\n    create_split_categories(test)\n    print(\"Completed data cleaning and preprocessing\\n\")\n\n    # train test split\n    #X_train, X_test= train_test_split(train, train_size=0.99, random_state=123)\n    print(\"shape of train: {}\".format(train.shape))\n    print(\"shape of test: {}\".format(test.shape))\n\n    global y_scalar,DESC_LEN,NAME_LEN\n    y_scalar = StandardScaler()#changes\n    y_train = y_scalar.fit_transform(log_price(train['price']).values.reshape(-1, 1))#changes\n    #y_test = y_scalar.transform(log_price(X_test['price']).values.reshape(-1, 1))#\n\n    DESC_LEN = train.wc_desc.max() + 1\n    NAME_LEN = train.wc_name.max() + 1\n    #################################\n    #step:3 Featurizing\n    #################################\n    train_inputs,test_inputs,list_var = data_gru(train,test)\n\n    #################################\n    #step:4 compiling and predicting\n    #################################\n    dropout_list = [0.10,0.10,0.20,0.20]\n    te_preds_m1 = compile_predict_GRU(train_inputs,test_inputs,y_train,list_var,dropout_list)\n\n    dropout_list = [0.10,0.20,0.30,0.40]\n    te_preds_m2 = compile_predict_GRU(train_inputs,test_inputs,y_train,list_var,dropout_list)\n\n    del train_inputs,test_inputs,list_var\n    gc.collect()\n    \n    #https://machinelearningmastery.com/model-averaging-ensemble-for-deep-learning-neural-networks/\n\n    y_hats = np.array([te_preds_m1,te_preds_m2]) #making an array out of all the predictions\n    # mean across ensembles\n    mean_preds = np.mean(y_hats, axis=0)\n    return scale_back(mean_preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#call for function_1\ntest = pd.read_csv('/kaggle/working/test/test_stg2.tsv',sep='\\t')\n\ntest[\"price\"] = function_1(test)\nsub = test[[\"test_id\", \"price\"]]\nsub.to_csv(\"submission.csv\", index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}