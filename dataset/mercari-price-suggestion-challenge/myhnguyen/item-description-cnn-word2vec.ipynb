{"cells":[{"outputs":[],"cell_type":"code","metadata":{"collapsed":true},"execution_count":null,"source":"import pandas as pd\nimport numpy as np\nfrom IPython.display import display"},{"outputs":[],"cell_type":"code","metadata":{"collapsed":true},"execution_count":null,"source":"# read in data\n# train_raw_path = '\\\\\\\\SEAGATE-D4/Documents/My Hoang Nguyen/ML-SDrive/Kaggle/Mercari/train.tsv'\ntrain_raw_path = '../input/mercari-price-suggestion-challenge/train.tsv'\ntrain_raw = pd.read_table(train_raw_path)\n\n# test_raw_path = '\\\\\\\\SEAGATE-D4/Documents/My Hoang Nguyen/ML-SDrive/Kaggle/Mercari/test.tsv'\ntest_raw_path = '../input/mercari-price-suggestion-challenge/test.tsv'\ntest_raw = pd.read_table(test_raw_path)"},{"outputs":[],"cell_type":"code","metadata":{},"execution_count":null,"source":"print('train_raw\\n', train_raw.shape)\nprint(train_raw.dtypes)\ndisplay(train_raw.head())\n\nprint('test_raw\\n', test_raw.shape)\nprint(test_raw.dtypes)\ndisplay(test_raw.head())"},{"cell_type":"markdown","metadata":{},"source":"# extract features from [item_description] by training a cnn"},{"outputs":[],"cell_type":"code","metadata":{},"execution_count":null,"source":"# load Google's pre-trained word2vec\nimport gensim\n# pretrained_word2vec_path = '\\\\\\\\SEAGATE-D4/Documents/My Hoang Nguyen/ML-SDrive/Sentiment Analysis/data/GoogleNews-vectors-negative300.bin'\npretrained_word2vec_path = '../input/word2vecnegative300/GoogleNews-vectors-negative300.bin'\nword2vec = gensim.models.KeyedVectors.load_word2vec_format(pretrained_word2vec_path, binary=True)"},{"outputs":[],"cell_type":"code","metadata":{},"execution_count":null,"source":"# tokenize text\nfrom keras.preprocessing.text import Tokenizer\n\nn_words = 20 # top most common words\ntext = train_raw['item_description'].astype(str).tolist()\ntokenizer = Tokenizer(num_words=n_words)\ntokenizer.fit_on_texts(text)\n\n# pad_sequences so they are all of the same length\nfrom keras.preprocessing.sequence import pad_sequences\n\nsequences = tokenizer.texts_to_sequences(text) # list, same length as data. represent word as rank/index\npadded_seq = pad_sequences(sequences)\nprint('padded_seq.shape', padded_seq.shape)"},{"outputs":[],"cell_type":"code","metadata":{},"execution_count":null,"source":"# kfold cv\nfrom sklearn.model_selection import KFold\n\nx = padded_seq\ny = np.asarray(train_raw['price'])\n\nn_splits = 2\n\nkf = KFold(n_splits=n_splits)\nkf.get_n_splits(x)"},{"outputs":[],"cell_type":"code","metadata":{"collapsed":true},"execution_count":null,"source":"# create embedding_matrix to feed in as weights for embedding_layer\nword_index = tokenizer.word_index\nvocab_size = len(word_index)\nEMBEDDING_DIM = 300 # this is from the pretrained vectors\n\nembedding_matrix = np.zeros((vocab_size + 1, EMBEDDING_DIM))\nfor word, i in word_index.items():\n    if word in word2vec:\n        embedding_vector = word2vec[word]\n    else:\n        embedding_vector = None\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector"},{"outputs":[],"cell_type":"code","metadata":{"collapsed":true},"execution_count":null,"source":"# custom loss function\nimport keras.backend as K\n\ndef rmsle(y_true, y_pred):\n    first_log = K.log(K.clip(y_pred, K.epsilon(), None) + 1.)\n    second_log = K.log(K.clip(y_true, K.epsilon(), None) + 1.)\n    \n    return K.sqrt(K.mean(K.square(first_log - second_log), axis=-1))\n\nimport keras\ndef truncated_normal(seed):\n    return keras.initializers.TruncatedNormal(mean=0.0, stddev=0.05, seed=seed)"},{"outputs":[],"cell_type":"code","metadata":{"collapsed":true},"execution_count":null,"source":"# create cnn model\nfrom keras.layers import Embedding, Dense, Input, Flatten\nfrom keras.layers import Conv1D, GlobalMaxPooling1D, Concatenate, Dropout\nfrom keras.models import Model\n\n# parameters\ninput_length = padded_seq.shape[1] # len (num words) of longest description\nseed = 0\nfilter_sizes = [2,3]\nn_filters = 2\ndropout_prob = 0.5\n\ndef create_cnn(include_top=True, weights=None):    \n\n    # input\n    sequence_input = Input(shape=(input_length,), dtype='int32', name='input')\n    # embedding_layer\n    embedding_layer = Embedding(vocab_size + 1, EMBEDDING_DIM, weights=[embedding_matrix], input_length=input_length\n                                , name='embedding', trainable=False)(sequence_input)\n    # conv layer\n    features = []\n    i = 0\n    for filter_size in filter_sizes:\n        i += 1\n        # conv layer\n        conv = Conv1D(n_filters, filter_size, activation='relu', kernel_initializer=truncated_normal(seed)\n                      , name='conv'+str(i))(embedding_layer)\n        # global max pooling\n        conv = GlobalMaxPooling1D(name='pool'+str(i))(conv)\n        # add features together\n        features.append(conv)\n    # penultimate layer\n    nn = Concatenate(name='features')(features)\n    if include_top:\n        # dropout\n        nn = Dropout(dropout_prob, seed=seed, name='dropout')(nn)\n        # fully connected layer\n        preds = Dense(1, kernel_initializer=truncated_normal(seed), name='output')(nn)\n\n        model = Model(sequence_input, preds)\n    else:\n        model = Model(sequence_input, nn)\n    \n    \n    if weights is not None:\n        model.set_weights(weights)\n        \n    return model"},{"outputs":[],"cell_type":"code","metadata":{},"execution_count":null,"source":"model = create_cnn()\nmodel.summary()"},{"outputs":[],"cell_type":"code","metadata":{},"execution_count":null,"source":"# train cnn\nmodel.compile(loss='msle', optimizer='adadelta', metrics=[rmsle])\n\nbatch_size = 128\nepochs = 1\nfor train_index, val_index in kf.split(x):\n    x_train, x_val = x[train_index], x[val_index]\n    y_train, y_val = y[train_index], y[val_index]\n    model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=epochs, batch_size=batch_size)"},{"outputs":[],"cell_type":"code","metadata":{},"execution_count":null,"source":"# predict\n# format test data\ndata = test_raw['item_description'].astype(str).tolist()\nsequences = tokenizer.texts_to_sequences(data) # list, same length as data. represent word as rank/index\nx_test = pad_sequences(sequences, maxlen=input_length)\nprint('x_test.shape', x_test.shape)\n\n# predict\ny_test = model.predict(x_test, batch_size=batch_size)\nprint('y_test.shape', y_test.shape)"},{"outputs":[],"cell_type":"code","metadata":{},"execution_count":null,"source":"# submission\nsubmission = pd.concat((test_raw, pd.DataFrame(np.reshape(y_test,(-1,1)), columns=['price'])), axis=1)[['test_id', 'price']]\ndisplay(submission.head())\n\n# export submission\nsubmission.to_csv('first_submission.csv', index=False)"},{"outputs":[],"cell_type":"code","metadata":{"_kg_hide-output":true,"collapsed":true},"execution_count":null,"source":"# export submission\nsubmission.to_csv('first_submission.csv', index=False)"}],"metadata":{"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"},"language_info":{"version":"3.6.2","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3"}},"nbformat_minor":1,"nbformat":4}