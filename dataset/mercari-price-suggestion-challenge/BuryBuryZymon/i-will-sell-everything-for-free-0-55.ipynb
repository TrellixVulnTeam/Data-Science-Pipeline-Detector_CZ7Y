{"metadata":{"language_info":{"version":"3.6.3","nbconvert_exporter":"python","name":"python","file_extension":".py","pygments_lexer":"ipython3","mimetype":"text/x-python","codemirror_mode":{"version":3,"name":"ipython"}},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat_minor":1,"cells":[{"metadata":{"_uuid":"dad0470bb8ccfc5a5decdcc2ebe0d62c7d4a8599","_cell_guid":"753da39b-9e05-4376-91fc-3f34a73f9f66"},"source":"# Mercari Price Suggestion Challenge\n\nThis is first of its kind and kernel only competition, and stage2 files will not be downloaded and will be available only in kernels. In this notebook, I am doing basic data exploration and reporting my findings. I will be making a simple model at the end to just as to make it complete. Val loss will be reported at the end of this notebook.\n\n**NOTE - If you find this kernel useful, please upvote, and if you have any suggestion or if anything is not clear please comment, I will try to explain my work**\n\nIn this notebook we will extract many features and make a simple model which runs faster than **Bojan** :P I will be extracting following features - \n- **Yes/No features ** - if description present, if brand name present\n- ** Category and brand encoding** - category has three levels and we will be seperating each level and then provide encoding for all three levels.\n- ** SVD comp on if-idf calculated over item description** - *self explanatory*\n- **SVD comp on if-itf calculated over product name** - *self explanatory*\n- **item_condition_id** - use it without processing\n- **shipping** - use it without processing\n\n*Only very few features in first round of feature extraction*\n\n*** We will train a XGB model to check how the extracted features are performing***\n\n**PS_1:  These features are working very well, please change n_comp in SVD and change the number of iterations in XGB from 80 to 1000, you will get very high accuracies.**\n\n**PS_2: I am learning markov models for predicting probability of author given sentence ( with history of last 3 words), I have taken a part of code from internet and I have written a probability function, but that function is faulty and I am having difficulty fixing it. If you know markov models, please contact or have a look at [notebook](https://www.kaggle.com/maheshdadhich/creative-feature-engineering-lb-0-35) **\n\n\n\n","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"da160c4f44b40c1b53367c1ade79a099be2c4c59","collapsed":true,"_cell_guid":"c8a8af20-85a1-4125-9f31-d754144a8ff4"},"cell_type":"code","source":"import pandas as pd  #pandas for using dataframe and reading csv \nimport numpy as np   #numpy for vector operations and basic maths \nimport urllib        #for url stuff\nimport re            #for processing regular expressions\nimport datetime      #for datetime operations\nimport calendar      #for calendar for datetime operations\nimport time          #to get the system time\nimport scipy         #for other dependancies\nfrom sklearn.cluster import KMeans # for doing K-means clustering\nfrom haversine import haversine # for calculating haversine distance\nimport math          #for basic maths operations\nimport seaborn as sns #for making plots\nimport matplotlib.pyplot as plt # for plotting\nimport os                # for os commands\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nimport xgboost as xgb\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn import ensemble, metrics, model_selection, naive_bayes","outputs":[]},{"metadata":{"_uuid":"53a88100dc06c9598a02caf9b30d85d20bb1a6a4","_cell_guid":"64fc549e-a524-4ef2-908d-e1e3162710dc"},"source":"## Reading in the files..","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"87397808fd552929c4374b3d0d92a655ebb4a270","collapsed":true,"_cell_guid":"0aecedd0-f93f-49a3-a0fe-3333cc940b92"},"cell_type":"code","source":"train_df = pd.read_csv('../input/train.tsv', sep='\\t')\ntrain_df.head(10)","outputs":[]},{"execution_count":null,"metadata":{"_uuid":"3a28e1e7740d9fb861aaca95b3f8a1c6daf6e1e5","collapsed":true,"_cell_guid":"82a9b10f-ff03-4d5b-a16c-5c2f62fe627d"},"cell_type":"code","source":"# checking test file.. \ntest_df = pd.read_csv('../input/test.tsv', sep='\\t')\ntest_df.head()\n# its clear that we are supposed to predict the price, given other variables.","outputs":[]},{"execution_count":null,"metadata":{"_uuid":"d90afaeb27a15e6dd4a20e7337fdbe4acb2103bb","collapsed":true,"_cell_guid":"b9e5bf9b-90c6-4ecf-abbe-5a50519e1b75"},"cell_type":"code","source":"# Lets check the basic price histogram and see if what is the range of prices \n%matplotlib inline\nstart = time.time()\nsns.set(style=\"white\", palette=\"muted\", color_codes=True)\nf, axes = plt.subplots(1, 1, figsize=(11, 7), sharex=True)\nsns.despine(left=True)\nsns.distplot(np.log(train_df['price'].values+1), axlabel = 'Log(price)', label = 'log(trip_duration)', bins = 50, color=\"y\")\nplt.setp(axes, yticks=[])\nplt.tight_layout()\nend = time.time()\nprint(\"Time taken by above cell is {}.\".format((end-start)))\nplt.show()","outputs":[]},{"metadata":{"_uuid":"08b79842337f30af429711e6deaa8f6eb433cc6f","_cell_guid":"e3416693-0d33-43e3-89b5-6f5d8762ccce"},"source":"** Findings ** - This is the plot of log(price), I expected that the prices variation will be there a lot, so I checked the log(price) histogram instead of normal histogram. Its clear from above plot that most of the products have price greater than 1 dollars , and highest number of products have prices between \\$ 100  to \\$ 1000. ","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"a84f4bede685c54dab62d1c0a5bdd0abf259ac96","collapsed":true,"_cell_guid":"dabeb9e7-12f8-4d31-bf11-543c125713bf"},"cell_type":"code","source":"# Lets check if shipping has any impact on prices \nstart = time.time()\nfig, ax = plt.subplots(figsize=(11, 7), nrows=2, sharex=True, sharey=True)\nsns.distplot(np.log(train_df.loc[train_df['shipping']==1]['price'].values+1), ax=ax[0], color='blue', label='shipping')\nsns.distplot(np.log(train_df.loc[train_df['shipping']==0]['price'].values+1), ax=ax[1], color='green', label='No shipping')\nax[0].legend(loc=0)\nax[1].legend(loc=0)\nplt.show()\nend = time.time()\nprint(\"Time taken by above cell is {}.\".format((end-start)))","outputs":[]},{"metadata":{"_uuid":"d8687ebfbd2faa945322114a9a32458687af9c6a","_cell_guid":"6ea475a3-b1f9-41c6-971e-d53364f0b0a8"},"source":"**findings** - Its clear that the No shipping has slightly narrow distribution of prices and starting from 100 dollors while shipping products are starting from 10 dollars ","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"63554a098de7c69dbbcb4dd3258e7238784ccea6","collapsed":true,"_cell_guid":"c3df1fbe-03fa-408a-a9b3-18683ce4533f"},"cell_type":"code","source":"# Lets check the basic price histogram and see if what is the range of prices \n%matplotlib inline\nstart = time.time()\nsns.set(style=\"white\", palette=\"muted\", color_codes=True)\nf, axes = plt.subplots(1, 1, figsize=(11, 7), sharex=True)\nsns.despine(left=True)\nsns.distplot(train_df['item_condition_id'], axlabel = 'item_condition_id', label = 'item_condition_id', bins = 12, color=\"g\", kde = False)\nplt.setp(axes, yticks=[])\nplt.tight_layout()\nend = time.time()\nprint(\"Time taken by above cell is {}.\".format((end-start)))\nplt.show()","outputs":[]},{"metadata":{"_uuid":"ba130dea526c388120396efc289d82556d8bdaf8","_cell_guid":"92affe4c-a29b-42cd-8d3c-b1d089bd830b"},"source":"# Basic feature engineering \nLets start with features that we have mentioned in introduction in FE round 1, we will add other features but first check how these features are performing in for prediction. For finding category label, we need to first devide item category into three categories, primary, secondary and tirtiary ( I have named them _1/_2/_3 here). After that we will make a dictionary and give labels to different category and make cat labels features.\n\n- **1. Category label features - **","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"8a7b38c534021b0dbc72d99ccac55a24da0c8158","collapsed":true,"_cell_guid":"58a75630-33c5-4069-b5d9-6667f5ee1818"},"cell_type":"code","source":"# 1. Extract 3 category related features \ndef cat_split(row):\n    try:\n        text = row\n        txt1, txt2, txt3 = text.split('/')\n        return txt1, txt2, txt3\n    except:\n        return np.nan, np.nan, np.nan\n\n\ntrain_df[\"cat_1\"], train_df[\"cat_2\"], train_df[\"cat_3\"] = zip(*train_df.category_name.apply(lambda val: cat_split(val)))\ntest_df[\"cat_1\"], test_df[\"cat_2\"], test_df[\"cat_3\"] = zip(*test_df.category_name.apply(lambda val: cat_split(val)))\ntrain_df.head()","outputs":[]},{"execution_count":null,"metadata":{"_uuid":"f3bbbfb7e50ca3b2e5ab4ef778df060093c884ed","collapsed":true,"_cell_guid":"df423686-f8cf-4ddf-914e-b687beaf10de"},"cell_type":"code","source":"test_df.head()","outputs":[]},{"execution_count":null,"metadata":{"_uuid":"6b4370c9bf7d1bc0ade465e1864e6c02b7beeab4","collapsed":true,"_cell_guid":"9ed01ad3-2afb-45ac-a1c5-fa92bab92207"},"cell_type":"code","source":"# making dictionaries for different categories \nkeys = train_df.cat_1.unique().tolist() + test_df.cat_1.unique().tolist()\nkeys = list(set(keys))\nvalues = list(range(keys.__len__()))\ncat1_dict = dict(zip(keys, values))\n\nkeys2 = train_df.cat_2.unique().tolist() + test_df.cat_2.unique().tolist()\nkeys2 = list(set(keys2))\nvalues2 = list(range(keys2.__len__()))\ncat2_dict = dict(zip(keys2, values2))\n\nkeys3 = train_df.cat_3.unique().tolist() + test_df.cat_3.unique().tolist()\nkeys3 = list(set(keys3))\nvalues3 = list(range(keys3.__len__()))\ncat3_dict = dict(zip(keys3, values3))","outputs":[]},{"execution_count":null,"metadata":{"_uuid":"7993cd545e11714d629855f196c3d813a6f01bc7","collapsed":true,"_cell_guid":"afa17996-b43b-4fd7-9bed-ae312b8a566f"},"cell_type":"code","source":"# function to assign category label\ndef cat_lab(row,cat1_dict = cat1_dict, cat2_dict = cat2_dict, cat3_dict = cat3_dict):\n    \"\"\"function to give cat label for cat1/2/3\"\"\"\n    txt1 = row['cat_1']\n    txt2 = row['cat_2']\n    txt3 = row['cat_3']\n    return cat1_dict[txt1], cat2_dict[txt2], cat3_dict[txt3]\n\ntrain_df[\"cat_1_label\"], train_df[\"cat_2_label\"], train_df[\"cat_3_lable\"] = zip(*train_df.apply(lambda val: cat_lab(val), axis =1))\ntest_df[\"cat_1_label\"], test_df[\"cat_2_label\"], test_df[\"cat_3_lable\"] = zip(*test_df.apply(lambda val: cat_lab(val), axis =1))\ntrain_df.head(10)","outputs":[]},{"metadata":{"_uuid":"c1b9f9d84756923ced7993fac8ff016db63d91ce","_cell_guid":"9dd88f5f-befc-4624-8256-3a66547c3363"},"source":"** 2. if category present -yes/no features -**","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"32dcca60547064a15500e050d41e066160ea2e93","collapsed":true,"_cell_guid":"ad384503-7afd-4b28-adf6-abe9c8321052"},"cell_type":"code","source":"def if_catname(row):\n    \"\"\"function to give if brand name is there or not\"\"\"\n    if row == row:\n        return 1\n    else:\n        return 0\n    \ntrain_df['if_cat'] = train_df.category_name.apply(lambda row : if_catname(row))\ntest_df['if_cat'] = test_df.category_name.apply(lambda row : if_catname(row))\ntrain_df.head()","outputs":[]},{"metadata":{"_uuid":"160b78aec5df11f86065455d4b1ddcc3328a1a8e","_cell_guid":"47cf7a99-c43d-4877-acb0-5973a4a502ce"},"source":"**3. If brand name is present - yes/no features -**","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"e34487e65d53ad40a481007e698137c9acbd7a66","collapsed":true,"_cell_guid":"80ea98e4-40cb-400d-b2f3-b92491cdae02"},"cell_type":"code","source":"# brand name related features \ndef if_brand(row):\n    \"\"\"function to give if brand name is there or not\"\"\"\n    if row == row:\n        return 1\n    else:\n        return 0\n    \ntrain_df['if_brand'] = train_df.brand_name.apply(lambda row : if_brand(row))\ntest_df['if_brand'] = test_df.brand_name.apply(lambda row : if_brand(row))\ntrain_df.head()","outputs":[]},{"metadata":{"_uuid":"3924b1f3b1e000887495c0b61255dd60f566439f","_cell_guid":"c68bdc68-d82b-4330-bfdf-2ab1e12a86fa"},"source":"** 4. Brand name label features -** ","cell_type":"markdown"},{"execution_count":null,"metadata":{"scrolled":false,"_uuid":"2f907a49e83b99029086d82cf211446747072ec4","collapsed":true,"_cell_guid":"d26ab951-5a6f-4f45-8402-563c96f90bf0"},"cell_type":"code","source":"# makinfg brand name dict features \nkeys = train_df.brand_name.dropna().unique()\nvalues = list(range(keys.__len__()))\nbrand_dict = dict(zip(keys, values))\n\ndef brand_label(row):\n    \"\"\"function to assign brand label\"\"\"\n    try:\n        return brand_dict[row]\n    except:\n        return np.nan\n\ntrain_df['brand_label'] = train_df.brand_name.apply(lambda row: brand_label(row))\ntest_df['brand_label'] = test_df.brand_name.apply(lambda row: brand_label(row))\ntrain_df.head()","outputs":[]},{"metadata":{"_uuid":"af5c87557497e6ffcf6e6086c9bacf739dda73eb","_cell_guid":"5c030896-8548-41dc-92dc-2d9ac20d2889"},"source":"** 5. if item_description present - yes/no feature -**","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"0ae71db1ca6c8258b896d7a2527f37ec69f42b08","collapsed":true,"_cell_guid":"9c142e07-77ce-437e-8cdc-f5212707d09f"},"cell_type":"code","source":"# item description related features \nprint(\"Description of item is not present in {}\".format(train_df.loc[train_df.item_description == 'No description yet'].shape[0]))\nprint(\"while the shape of train_df is {}\".format(train_df.shape[0]))\n\ndef if_description(row):\n    \"\"\"function to say if description is present or not\"\"\"\n    if row == 'No description yet':\n        a = 0\n    else:\n        a = 1\n    return a\n\ntrain_df['is_description'] = train_df.item_description.apply(lambda row : if_description(row))\ntest_df['is_description'] = test_df.item_description.apply(lambda row : if_description(row))\ntrain_df.head()","outputs":[]},{"execution_count":null,"metadata":{"_uuid":"e3ab94ef41cbaef36d708035cb2cf2ee790ecb13","collapsed":true,"_cell_guid":"3971d5a0-79e9-4542-91a5-459b5f0eab3f"},"cell_type":"code","source":"# Nulls in item description in train or test as tf-idf is not defined on nan\nprint(train_df.item_description.isnull().sum())\nprint(test_df.item_description.isnull().sum())\n# lets drop these 4 items \nprint(train_df.shape[0])\ntrain_df = train_df.loc[train_df.item_description == train_df.item_description]\ntest_df = test_df.loc[test_df.item_description == test_df.item_description]\ntrain_df = train_df.loc[train_df.name == train_df.name]\ntest_df = test_df.loc[test_df.name == test_df.name]\nprint(train_df.shape[0])\nprint(\"Dropped records where item description was nan\")","outputs":[]},{"metadata":{"_uuid":"e43ee543b9d35f940ad78e0f087da4b347b228b9","_cell_guid":"839b2989-fe97-49f3-9899-b55042ca2d3a"},"source":"** 6. SVD on tf-idf on unigrams for iten_description -**","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"e1efe2b37b924b1eb0877881d45c5f2121045e3d","collapsed":true,"_cell_guid":"24ea4561-fce6-406b-8efb-c61128af8930"},"cell_type":"code","source":"# description related tf-idf features \n# I guess \"No dscription present won't affact these features ... So, I am not removing them.\nimport time\nstart = time.time()\ntfidf_vec = TfidfVectorizer(stop_words='english', ngram_range=(1,1))\nfull_tfidf = tfidf_vec.fit_transform(train_df['item_description'].values.tolist() + test_df['item_description'].values.tolist())\ntrain_tfidf = tfidf_vec.transform(train_df['item_description'].values.tolist())\ntest_tfidf = tfidf_vec.transform(test_df['item_description'].values.tolist())\n\nn_comp = 40\nsvd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\nsvd_obj.fit(full_tfidf)\ntrain_svd = pd.DataFrame(svd_obj.transform(train_tfidf))\ntest_svd = pd.DataFrame(svd_obj.transform(test_tfidf))\n    \ntrain_svd.columns = ['svd_item_'+str(i) for i in range(n_comp)]\ntest_svd.columns = ['svd_item_'+str(i) for i in range(n_comp)]\ntrain_df = pd.concat([train_df, train_svd], axis=1)\ntest_df = pd.concat([test_df, test_svd], axis=1)\nend = time.time()\nprint(\"time taken {}\".format(end - start))","outputs":[]},{"execution_count":null,"metadata":{"_uuid":"646cc1cb8b7c59e86fcb1f73a7998b5787484db7","collapsed":true,"_cell_guid":"90114c07-f8c5-473f-8aae-94e0ffc0335b"},"cell_type":"code","source":"print(train_df.shape[0])\ntrain_df = train_df.loc[train_df.item_description == train_df.item_description]\ntest_df = test_df.loc[test_df.item_description == test_df.item_description]\ntrain_df = train_df.loc[train_df.name == train_df.name]\ntest_df = test_df.loc[test_df.name == test_df.name]\nprint(train_df.shape[0])\nprint(\"Dropped records where item description was nan\")","outputs":[]},{"metadata":{"_uuid":"c67ffcf9132231b662dd94944528c15bca5cbbf8","_cell_guid":"70935515-7e8a-4b99-8f8e-c2c624345c38"},"source":"**7. SVD on tf-idf of unigram of product name features -**\n","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"7deec63d077fa5de57371d25526ce71eccaa1987","collapsed":true,"_cell_guid":"31e18035-94e5-431b-a951-c1819d930764"},"cell_type":"code","source":"print(train_df.shape[0])\ntrain_df = train_df.loc[train_df.item_description == train_df.item_description]\ntest_df = test_df.loc[test_df.item_description == test_df.item_description]\ntrain_df = train_df.loc[train_df.name == train_df.name]\ntest_df = test_df.loc[test_df.name == test_df.name]\nprint(train_df.shape[0])\nprint(\"Dropped records where item description was nan\")","outputs":[]},{"execution_count":null,"metadata":{"_uuid":"8a20967aea2103ef15147b08231bfe75587724d9","collapsed":true,"_cell_guid":"401a5b53-0f07-470a-9ba7-015e52ffa5af"},"cell_type":"code","source":"# product name related features \n\ntfidf_vec = TfidfVectorizer(stop_words='english', ngram_range=(1,1))\nfull_tfidf = tfidf_vec.fit_transform(train_df['name'].values.tolist() + test_df['name'].values.tolist())\ntrain_tfidf = tfidf_vec.transform(train_df['name'].values.tolist())\ntest_tfidf = tfidf_vec.transform(test_df['name'].values.tolist())\n\nn_comp = 40\nsvd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\nsvd_obj.fit(full_tfidf)\ntrain_svd = pd.DataFrame(svd_obj.transform(train_tfidf))\ntest_svd = pd.DataFrame(svd_obj.transform(test_tfidf))\n    \ntrain_svd.columns = ['svd_name_'+str(i) for i in range(n_comp)]\ntest_svd.columns = ['svd_name_'+str(i) for i in range(n_comp)]\ntrain_df = pd.concat([train_df, train_svd], axis=1)\ntest_df = pd.concat([test_df, test_svd], axis=1)","outputs":[]},{"execution_count":null,"metadata":{"_uuid":"25176f1c1b7678581b023201bbe03506f8f5281f","collapsed":true,"_cell_guid":"5c27d2b9-482a-42b9-b5c2-a7fd64f9f1e2"},"cell_type":"code","source":"# test check for dimensions before model \nprint(\"Train should have one columns more than test\")\nprint(train_df.shape[1])\nprint(test_df.shape[1])\nprint(\"perfect The data is fine\")","outputs":[]},{"metadata":{"_uuid":"bf443d84f40e7205db6fa268ec9e592fce3db858","_cell_guid":"f2bad224-b443-4a31-ad9d-21656f4cd164"},"source":" # XGboost regressor ...\n Now we have 49 features which could be used in price prediction and let's use them and see how they are performing ","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"b1bfc471d0473492439c3d4d52015f5318162db2","collapsed":true,"_cell_guid":"0c643e0f-d739-4d77-997a-6a6a019708a8"},"cell_type":"code","source":"# XGboost regressor ...\n# replace all nan with -1 \nprint(train_df.isnull().sum())\ntrain_df.fillna(0, inplace=True)\ntest_df.fillna(0, inplace=True)\nprint(train_df.isnull().sum())","outputs":[]},{"execution_count":null,"metadata":{"_uuid":"cac1060857de2bdbe0a59a1a8597c0e08dfab7b2","collapsed":true,"_cell_guid":"24f0db11-6fca-442d-a980-772865130a81"},"cell_type":"code","source":"train = train_df.copy()\ntest = test_df.copy()\nprint(\"Difference of features in train and test are {}\".format(np.setdiff1d(train.columns, test.columns)))\nprint(\"\")\ndo_not_use_for_training = ['cat_1','test_id','cat_2','cat_3','train_id','name', 'category_name', 'brand_name', 'price', 'item_description']\nfeature_names = [f for f in train.columns if f not in do_not_use_for_training]\nprint(\"We will be using following features for training {}.\".format(feature_names))\nprint(\"\")\nprint(\"Total number of features are {}.\".format(len(feature_names)))","outputs":[]},{"execution_count":null,"metadata":{"_uuid":"da3a1859086d6b27d26ecf42a70fcf4733799f15","collapsed":true,"_cell_guid":"5a2dcc29-5e3b-4845-a794-7a30ff35d57e"},"cell_type":"code","source":"y = np.log(train['price'].values + 1)","outputs":[]},{"execution_count":null,"metadata":{"_uuid":"f189b15e98cec8b5a992fc2464e97ff92017d19d","collapsed":true,"_cell_guid":"9b8b0529-da9b-44bf-a10d-bfead2177d53"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nXtr, Xv, ytr, yv = train_test_split(train[feature_names].values, y, test_size=0.2, random_state=1987)\ndtrain = xgb.DMatrix(Xtr, label=ytr)\ndvalid = xgb.DMatrix(Xv, label=yv)\ndtest = xgb.DMatrix(test[feature_names].values)\nwatchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n\nstart = time.time()\nxgb_par = {'min_child_weight': 20, 'eta': 0.05, 'colsample_bytree': 0.5, 'max_depth': 15,\n            'subsample': 0.9, 'lambda': 2.0, 'nthread': -1, 'booster' : 'gbtree', 'silent': 1,\n            'eval_metric': 'rmse', 'objective': 'reg:linear'}\n\nmodel_1 = xgb.train(xgb_par, dtrain, 80, watchlist, early_stopping_rounds=20, maximize=False, verbose_eval=20)\nprint('Modeling RMSLE %.5f' % model_1.best_score)\nend = time.time()\nprint(\"Time taken in training is {}.\".format(end - start))","outputs":[]},{"execution_count":null,"metadata":{"_uuid":"0618cb1da8e2fc9147eca2da014336ed91ec248a","collapsed":true,"_cell_guid":"b3357346-b3f0-4a82-9b25-b25613588915"},"cell_type":"code","source":"start = time.time()\nyvalid = model_1.predict(dvalid)\nytest = model_1.predict(dtest)\nend = time.time()\nprint(\"Time taken in prediction is {}.\".format(end - start))","outputs":[]},{"execution_count":null,"metadata":{"_uuid":"6f221071d4a7a8105338fa0cd729b7f0b2f9a16a","collapsed":true,"_cell_guid":"3b39abb0-2daa-464e-8132-83d559b676ce"},"cell_type":"code","source":"# Lets check how the distribution of test and vaidation set looks like ...\nstart = time.time()\nfig, ax = plt.subplots(nrows=2, sharex=True, sharey=True)\nsns.distplot(yvalid, ax=ax[0], color='blue', label='Validation')\nsns.distplot(ytest, ax=ax[1], color='green', label='Test')\nax[0].legend(loc=0)\nax[1].legend(loc=0)\nplt.show()\nend = time.time()\nprint(\"Time taken by above cell is {}.\".format((end-start)))","outputs":[]},{"execution_count":null,"metadata":{"_uuid":"b2f9df4bf1f1449ff13bf7d13344c812b5eeb476","collapsed":true,"_cell_guid":"c778ec8c-1e97-479b-b7fd-baacfb4bc056"},"cell_type":"code","source":"start = time.time()\nif test.shape[0] == ytest.shape[0]:\n    print('Test shape OK.') \ntest['price'] = np.exp(ytest) - 1\ntest[['test_id', 'price']].to_csv('mahesh_xgb_submission_mercari.csv', index=False)\nend = time.time()\nprint(\"Time taken in training is {}.\".format(end - start))","outputs":[]},{"metadata":{"_uuid":"ccd107e53e98506473c406f2630e4287914f2b7a","_cell_guid":"11521c61-628d-455f-8ef9-b782ec98d88d"},"source":"**Sleep time zzzzz......**\n# Upvote if you find this analysis useful... Thanks (y)","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"bcbda71aa05534aa93a415163ba22a82d0e3e9c8","collapsed":true,"_cell_guid":"7744d081-3d72-43e5-92ff-37cb3a60e139"},"cell_type":"code","source":"","outputs":[]}],"nbformat":4}