{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"https://www.kaggle.com/heatherqiu/ctr-features-gbdt-lr\n\nhttps://www.kaggle.com/akishen74/ctr-practice/","metadata":{}},{"cell_type":"code","source":"%matplotlib inline\n%config InlineBackend.figure_format = 'svg'\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport gc, itertools, time, math\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport category_encoders as ce\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, log_loss, roc_auc_score\nfrom sklearn.preprocessing import OneHotEncoder, KBinsDiscretizer, LabelEncoder, RobustScaler\nfrom catboost import CatBoostClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import plot_metric\nfrom sklearn.model_selection import train_test_split","metadata":{"papermill":{"duration":1.76253,"end_time":"2021-03-15T18:34:51.411839","exception":false,"start_time":"2021-03-15T18:34:49.649309","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-22T05:21:14.226519Z","iopub.execute_input":"2021-07-22T05:21:14.2271Z","iopub.status.idle":"2021-07-22T05:21:18.76061Z","shell.execute_reply.started":"2021-07-22T05:21:14.226879Z","shell.execute_reply":"2021-07-22T05:21:18.759495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"这里注意尽量均匀地从训练集中读取数据，仅仅采样数据集中的前n行是不够的。这样处理之后成绩提升了0.005左右","metadata":{}},{"cell_type":"code","source":"# 连续字段和分类型字段\ncontinue_var = ['I' + str(i) for i in range(1, 14)]\ncat_features = ['C' + str(i) for i in range(1,27)]","metadata":{"execution":{"iopub.status.busy":"2021-07-22T18:03:01.063464Z","iopub.execute_input":"2021-07-22T18:03:01.06382Z","iopub.status.idle":"2021-07-22T18:03:01.068058Z","shell.execute_reply.started":"2021-07-22T18:03:01.063783Z","shell.execute_reply":"2021-07-22T18:03:01.067231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 需要注意的是，测试集样本量是训练集的6倍多\ncol_names_train = ['Label'] + continue_var + cat_features\ncol_names_test = col_names_train[1:]\n\nreader = pd.read_csv('/kaggle/input/criteo-dataset/dac/train.txt', sep='\\t', \n                     names=col_names_train, chunksize=100000, iterator=True)\n\ntrain = pd.DataFrame()\nstart = time.time()  \nfor i, chunk in enumerate(reader): \n    if train.shape[0] > 1000000:\n        break\n    train = pd.concat([train, chunk.sample(frac=.05, replace=False, random_state=911)], axis=0)  \n    # print(train.shape[0])\n    if i % 20 == 0:\n        print('Processing Chunk No. {}'.format(i)) \nprint('Reading data costs %.2f seconds'%(time.time() - start))\n\n#train = pd.read_csv('/kaggle/input/criteo-dataset/dac/train.txt', \n#                       sep='\\t', names=col_names_train,\n#                       chunksize=100000) # ten chunks: first 1,000,000\n\ntest = pd.read_csv('/kaggle/input/criteo-dataset/dac/test.txt',sep='\\t', names=col_names_test)\n#train = train.get_chunk(1000000)\nprint('train has {} rows and {} columns'.format(train.shape[0], train.shape[1]))\nprint('test has {} rows and {} columns'.format(test.shape[0], test.shape[1]))\n#train = train.convert_dtypes()\n#test = test.convert_dtypes()","metadata":{"execution":{"iopub.status.busy":"2021-07-22T05:21:18.776943Z","iopub.execute_input":"2021-07-22T05:21:18.777337Z","iopub.status.idle":"2021-07-22T05:26:45.60898Z","shell.execute_reply.started":"2021-07-22T05:21:18.777276Z","shell.execute_reply":"2021-07-22T05:26:45.60765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.rcParams['figure.facecolor'] = 'white'\n#train['C1'].value_counts().head(50).plot.bar(color='blue',alpha=0.7)\n#test['C1'].value_counts().head(50).plot.bar(color='red',alpha=0.7)","metadata":{"execution":{"iopub.status.busy":"2021-07-22T05:26:45.61798Z","iopub.execute_input":"2021-07-22T05:26:45.618434Z","iopub.status.idle":"2021-07-22T05:26:45.628461Z","shell.execute_reply.started":"2021-07-22T05:26:45.618388Z","shell.execute_reply":"2021-07-22T05:26:45.625837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2021-07-22T05:26:45.630327Z","iopub.execute_input":"2021-07-22T05:26:45.631225Z","iopub.status.idle":"2021-07-22T05:26:48.715352Z","shell.execute_reply.started":"2021-07-22T05:26:45.631131Z","shell.execute_reply":"2021-07-22T05:26:48.714106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"训练集和验证集存在分布不一致的问题。很多id类类别字段的枚举值在测试集中出现，却未在训练集中出现。这也导致了提交后的分数和local cv分数极为不一致。","metadata":{}},{"cell_type":"code","source":"ls = list(train.columns)\nls.remove('Label')\n\nunbalance = []\nshort = []\nprint('输出原始数据枚举值对照表')\nprint('-'*50)\nfor i in ls:\n    train_len = len(train[i].astype(str).value_counts())\n    test_len = len(test[i].astype(str).value_counts())\n    if (test_len > train_len) and test_len > 100 and train[i].dtype == 'object':\n        unbalance.append(i)\n    elif (test_len > train_len) and test_len <= 100 and train[i].dtype == 'object':\n        short.append(i)\n    else:\n        pass\n    print(i, ' | train: {} | test：{}'.format(train_len, test_len))","metadata":{"execution":{"iopub.status.busy":"2021-07-22T05:26:48.717089Z","iopub.execute_input":"2021-07-22T05:26:48.717913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# unbalance\nprint('合并长尾枚举值，使分类特征在训练集与测试集分布一致')\nprint('-'*50)\nDISTRI = 0.7 # 数字越大，则枚举值分得越多越详细\nstart = time.time()\nfor col in short:\n    print('processing: {}'.format(col))\n    d1 = train[col].astype(str).value_counts()  # 训练集频数表\n    envalue = d1[: int(len(d1) * DISTRI)].index   # 取按照频数排序后前n%项目的枚举值\n    train[col] = np.where(train[col].isin(envalue), train[col], 'longtail')\n    test[col] = np.where(test[col].isin(envalue) , test[col], 'longtail')\nprint(\"the program costs {:.2f} seconds\".format(time.time() - start))\n\nDISTRI = 0.5 # 数字越大，则枚举值分得越多越详细\nstart = time.time()\nfor col in unbalance:\n    print('processing: {}'.format(col))\n    d1 = train[col].astype(str).value_counts()  # 训练集频数表\n    envalue = d1[: int(len(d1) * DISTRI)].index   # 取按照频数排序后前n%项目的枚举值\n    train[col] = np.where((train[col].isin(envalue)) | (train[col].isna()) , train[col], 'longtail')\n    test[col] = np.where((test[col].isin(envalue)) | (test[col].isna()), test[col], 'longtail')\nprint(\"the program costs {:.2f} seconds\".format(time.time() - start))\nprint('\\n')\nprint('输出经过处理后的枚举值对照表')\nprint('-'*50)\nfor i in ls:\n    train_len = len(train[i].astype(str).value_counts())\n    test_len = len(test[i].astype(str).value_counts())\n    print(i, ' | train: {} | test：{}'.format(train_len, test_len))","metadata":{"execution":{"iopub.status.idle":"2021-07-22T05:34:27.396698Z","shell.execute_reply.started":"2021-07-22T05:30:17.732133Z","shell.execute_reply":"2021-07-22T05:34:27.395318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2021-07-22T05:34:27.400847Z","iopub.execute_input":"2021-07-22T05:34:27.40117Z","iopub.status.idle":"2021-07-22T05:34:30.458345Z","shell.execute_reply.started":"2021-07-22T05:34:27.401139Z","shell.execute_reply":"2021-07-22T05:34:30.456141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 连续型字段用分组均值填充\nfill_mean = lambda x: x.fillna(x.mean())\nstart = time.time()\nfor col in continue_var:\n    print('filling NA value of {} ...'.format(i))\n    train[col] = train[col].groupby(train['C7']).apply(fill_mean)\n    test[col] = test[col].groupby(test['C7']).apply(fill_mean)\n    train[col] = train[col].fillna(test[col].mean())\n    test[col] = test[col].fillna(test[col].mean())\n    train[col] = train[col].astype('float64')\n    test[col] = test[col].astype('float64')\nprint(\"filling NA costs {:.2f} seconds\".format(time.time() - start))\n\ntrain = train.fillna('unknown')\ntest = test.fillna('unknown')","metadata":{"execution":{"iopub.status.busy":"2021-07-22T05:34:30.461186Z","iopub.execute_input":"2021-07-22T05:34:30.461672Z","iopub.status.idle":"2021-07-22T05:34:52.819946Z","shell.execute_reply.started":"2021-07-22T05:34:30.461625Z","shell.execute_reply":"2021-07-22T05:34:52.818883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def robust_transfer(df):\n    '''transfer multiple columns using robustscaler\n       对连续型字段使用robustscaler\n    '''\n    start = time.time()\n    Scaler = RobustScaler().fit(df) \n    df1 = Scaler.transform(df)\n    print(\"encoding is over! the program costs {:.2f} seconds\".format(time.time() - start))\n    return df1\n\n#train0 = robust_transfer(train[features])\n#test0 = robust_transfer(test[features])\n\ndef cut_bins(df, con_cols, qnumbers):\n    '''cut columns into bins\n       对连续型字段进行分桶\n    '''\n    start = time.time()\n    for col in con_cols:\n        df[col] = pd.qcut(df[col], qnumbers, duplicates='drop')\n        print('processing: {}'.format(col))\n    print(\"cutting is over! the program costs {:.2f} seconds\".format(time.time() - start))\n    return df\n\n#train = cut_bins(train, continue_var, 6)\n#test = cut_bins(test, continue_var, 6)\n#train[continue_var] = robust_transfer(train[continue_var])\n#test[continue_var] = robust_transfer(test[continue_var])","metadata":{"execution":{"iopub.status.busy":"2021-07-22T05:34:52.821509Z","iopub.execute_input":"2021-07-22T05:34:52.821925Z","iopub.status.idle":"2021-07-22T05:34:52.83336Z","shell.execute_reply.started":"2021-07-22T05:34:52.82188Z","shell.execute_reply":"2021-07-22T05:34:52.829552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## def recode_variable(train, test, col, n):\n##     '''recode longtail categorical columns\n##     '''\n##     d1 = train[col].astype(str).value_counts() # 训练集频数表\n##     envalue = d1[: int(len(d1) * n)].index     # 取按照频数排序后前n%项目的枚举值\n##     train[col] = np.where(train[col].isin(envalue), train[col], 'longtail')\n##     test[col] = np.where(test[col].isin(envalue), test[col], 'longtail')\n##     return train, test\n##     \n## train, test = recode_variable(train, test, 'C19', 0.15)","metadata":{"execution":{"iopub.status.busy":"2021-07-22T05:34:52.834908Z","iopub.execute_input":"2021-07-22T05:34:52.835439Z","iopub.status.idle":"2021-07-22T05:34:52.846857Z","shell.execute_reply.started":"2021-07-22T05:34:52.835395Z","shell.execute_reply":"2021-07-22T05:34:52.845627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"https://contrib.scikit-learn.org/category_encoders/targetencoder.html","metadata":{}},{"cell_type":"code","source":"\n\nTRAIN_LEN = len(train)\n# dt_all = pd.concat([train, test])\n# del train, test\n# gc.collect()\n# handle missing values\n# col_names = dt_all.columns\n# na_dict = dict.fromkeys(col_names[1:14], -10) \n# na_dict.update(dict.fromkeys(col_names[14:], 'NA'))\n# print(na_dict)\n# dt_all = dt_all.fillna(na_dict)\n\ndef transfrom_target_encoder(dt_all, cat_features):\n    start = time.time()\n    for col in cat_features:    \n        dt_all[col] = ce.TargetEncoder().fit_transform(dt_all[col], dt_all['Label'])\n    print(\"encoding is over! the program costs {:.2f} seconds\".format(time.time() - start))\n    return dt_all\n\n\ndef transfrom_target_encoder0(train, test, cat_features):\n    '''fit on train data, transfrom test data\n    '''\n    start = time.time()  \n    ce_target_encoder = ce.TargetEncoder(cols = cat_features).fit(train, train['Label'])\n    train = ce_target_encoder.transform(train)\n    test['Label'] = np.nan\n    test = ce_target_encoder.transform(test)\n    print(\"encoding is over! the program costs {:.2f} seconds\".format(time.time() - start))\n    return train, test\n\ndef transfrom_count_encoder(dt_all, cat_features):\n    start = time.time()\n    for col in cat_features:    \n        dt_all[col] = ce.CountEncoder().fit_transform(dt_all[col])\n    print(\"encoding is over! the program costs {:.2f} seconds\".format(time.time() - start))\n    return dt_all\n\ndef transfrom_label_encoder(dt_all, cat_features):\n    '''do label encoding\n    '''\n    start = time.time()\n    for col in cat_features:\n        dt_all[col] = LabelEncoder().fit_transform(dt_all[col]).astype(str)\n        print('processing: {}'.format(col))\n    print(\"encoding is over! the program costs {:.2f} seconds\".format(time.time() - start))\n    return dt_all\n\ndef cross_features(df, features, outputFeaturelist = False):\n    '''generate synthetic features\n    '''\n    start = time.time()\n    for col in features:\n        for col2 in features:\n            feature_name = col + 'X' + col2\n            features.append(feature_name)\n            df[feature_name] = df[col] * df[col2]\n        print('processing: {}'.format(col))\n    print(\"encoding is over! the program costs {:.2f} seconds\".format(time.time() - start))\n    if outputFeaturelist:\n        return df, features\n    else:\n        return df\n\n#train, continue_var = cross_features(train, continue_var, outputFeaturelist = True)\n#test = cross_features(test)\n#continue_var\n# train, test = transfrom_target_encoder0(train, test)\n# dt_all = transfrom_label_encoder(dt_all)\n#train = transfrom_label_encoder(train, continue_var)\n#test = transfrom_label_encoder(test, continue_var)","metadata":{"execution":{"iopub.status.busy":"2021-07-22T05:34:52.848659Z","iopub.execute_input":"2021-07-22T05:34:52.849213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#x_train = dt_all[:TRAIN_LEN]\n#test = dt_all[TRAIN_LEN:]\n# test = test.drop(['C9','I12','I10','C20'], axis=1)\n#cat = ['C'+str(i) for i in range(1,27)]\n# cat.remove(['C9','C20'])\n#x_train = x_train[continue_var]\n#test = test[continue_var]\n'''\nlabel_encoder = LabelEncoder()\nfor col in cat_features:\n    x_train[col]= label_encoder.fit_transform(x_train[col]) \n    test[col]= label_encoder.fit_transform(test[col]) \n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['I13_I6'] = train['I13'] * train['I6']\ntest['I13_I6'] = test['I13'] * test['I6']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = train[['Label']]\nx_train = train.drop(['Label'], axis=1)\nx_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.20, stratify=y_train, random_state=256)\n#cat_features = [i for i in range(0, 35)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#test.iloc[:,11:35]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_catboost(y_label):\n    '''plot catboost learning curve\n    '''\n    learn_error = pd.read_csv('./catboost_info/learn_error.tsv', sep='\\t')\n    test_error = pd.read_csv('./catboost_info/test_error.tsv', sep='\\t')\n    metric = pd.concat([learn_error, test_error.iloc[:,1]], axis=1)\n    metric.columns = ['iterations','train','test']\n    plt.rcParams['figure.facecolor'] = 'white'\n    metric.plot(x='iterations',y=['train','test'])\n    plt.ylabel(y_label)\n    plt.grid()\n    plt.show()\n    \ndef plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n    '''This function prints and plots the confusion matrix.\n       Normalization can be applied by setting `normalize=True`.\n    '''\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n    plt.rcParams['figure.facecolor'] = 'white'\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Automatic Feature Selection Using Catboost Feature Importance\n\n\n首先对训练集建立一个Catboost算法，输出每个特征的重要性，再根据重要性排序，丢弃最不重要的后n%的特征","metadata":{}},{"cell_type":"code","source":"model = CatBoostClassifier(\n    iterations=100,\n    learning_rate=0.4,\n    task_type='GPU',\n    loss_function='Logloss',\n    depth=8,\n)\n\nfit_model = model.fit(\n    x_train, y_train,\n    eval_set=(x_val, y_val),\n    cat_features = cat_features,\n    verbose=10\n)\n\nfeature_im = fit_model.feature_importances_\n\n# plot_catboost('logloss')\n\ndef feature_selection(features, feature_importance, DISTRI=0.7, selection=False, plot=True):\n    '''do feature selection\n    '''\n    feimp = pd.DataFrame({'feature': features,\n                          'importance': feature_importance}).sort_values(by=['importance'], \n                                                           ascending=False)\n    if selection:\n        feimp = feimp.iloc[: int(feimp.shape[0] * DISTRI),]\n    else:\n        pass\n    \n    if plot:\n        plt.figure(figsize=(7, 10))\n        sns.barplot(data = feimp, x='importance', y='feature')\n        plt.title('Catboost Feature Importances')\n    else:\n        pass\n    return feimp\n    \nfeimp = feature_selection(x_train.columns, feature_im, 0.7)\nfeimp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feimp = feature_selection(x_train.columns, feature_im, 0.7, selection=True, plot=False)\nfeimp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Catboost Fitting for Prediction","metadata":{}},{"cell_type":"code","source":"x_train = x_train[feimp['feature']]\nx_val = x_val[feimp['feature']]\ntest = test[feimp['feature']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_features0 = [i for i in feimp['feature'] if 'C' in i]\n# cat_features0","metadata":{"execution":{"iopub.status.busy":"2021-07-22T18:03:24.16035Z","iopub.execute_input":"2021-07-22T18:03:24.160714Z","iopub.status.idle":"2021-07-22T18:03:24.165124Z","shell.execute_reply.started":"2021-07-22T18:03:24.160664Z","shell.execute_reply":"2021-07-22T18:03:24.164034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = CatBoostClassifier(\n    iterations=100,\n    learning_rate=0.4,\n    task_type='GPU',\n    loss_function='Logloss',\n     #gpu_ram_part=0.9,\n     #boosting_type='Plain',\n     #max_ctr_complexity=2,\n     depth=8,\n     #gpu_cat_features_storage='CpuPinnedMemory'\n)\n\nfit_model = model.fit(\n    x_train, y_train,\n    eval_set=(x_val, y_val),\n    cat_features=cat_features0,\n    verbose=10\n)\n\nfeature_im = fit_model.feature_importances_\n\nplot_catboost('logloss')\n\n\ny_test = model.predict(test, \n                       prediction_type='Probability',\n                       ntree_end=model.get_best_iteration(), \n                       thread_count=-1,\n                       verbose=None)\n\ny_val_pred = model.predict(x_val, \n                       prediction_type='Probability',\n                       ntree_start=0,\n                       ntree_end=model.get_best_iteration(), \n                       thread_count=-1,\n                       verbose=None)\n\ny_test_pred = y_test[:,1]\ny_val_pred = y_val_pred[:,1]\ny_val_class = np.where(y_val_pred > 0.5,1,0)\nprint('Out of folds logloss is {:.4f}'.format(log_loss(y_val, y_val_pred)))\n\nsubmission = pd.read_csv('../input/criteo-display-ad-challenge/random_submission.zip', compression='zip')\nsubmission = pd.DataFrame({'Id': submission['Id'], 'Predicted': y_test_pred })\nsubmission.to_csv('submission.csv',index = False)\n\nsubmission = pd.DataFrame({'Id': submission['Id'], 'Predicted': y_test_pred * 0.98})\nsubmission.to_csv('submission2.csv',index = False)\n\nsubmission = pd.DataFrame({'Id': submission['Id'], 'Predicted': y_test_pred * 1.02})\nsubmission.to_csv('submission3.csv',index = False)\nsubmission","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feimp = pd.DataFrame({'feature': x_train.columns,\n              'importance': feature_im }).sort_values(by=['importance'], \n                                                           ascending=False)\nplt.figure(figsize=(7, 10))\nsns.barplot(data = feimp, x='importance', y='feature')\nplt.title('Catboost Feature Importances')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_names = ['0','1']\nplot_confusion_matrix(confusion_matrix(y_val, y_val_class),\n                      classes=class_names, \n                      normalize=True, \n                      title='Confusion Matrix')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Xgboost","metadata":{}},{"cell_type":"code","source":"'''\nimport xgboost as xgb\nmodel = XGBClassifier(\n    objective='binary:logistic',\n    tree_method = 'gpu_hist',\n    n_jobs=-1,\n    n_estimators=1000,\n    max_depth=16,\n    colsample_bytree=0.8, \n    subsample=0.8, \n    learning_rate=0.2,\n    min_child_weight=6# 叶子上的最小样本数\n)\n\n\nmodel.fit(\n    x_train, \n    y_train, \n    eval_metric='logloss', \n    eval_set=[(x_train, y_train), (x_val, y_val)], \n    verbose=True, \n    early_stopping_rounds = 10\n)\n\nplt.rcParams['figure.facecolor'] = 'white'\nevals_result = model.evals_result()\nax = plot_metric(evals_result, metric = 'logloss')\nplt.title('Xgboost Learning Curve')\nplt.show()\n\n\nfig,ax = plt.subplots(figsize=(7,10))\nxgb.plot_importance(model,\n                ax=ax,\n                height=0.5).set(xlabel='feature importance',\n                                         title='',\n                                         ylabel='feature')\n\ny_test_pred = model.predict_proba(test)[:,1]\ny_val_pred = model.predict_proba(x_val)[:,1]\ny_val_class = np.where(y_val_pred > 0.5,1,0)\nprint('Out of folds logloss is {:.4f}'.format(log_loss(y_val, y_val_pred)))\n\nsubmission = pd.read_csv('../input/criteo-display-ad-challenge/random_submission.zip', compression='zip')\nsubmission = pd.DataFrame({'Id': submission['Id'], 'Predicted': y_test_pred})\nsubmission.to_csv('submission_xgb.csv',index = False)\nsubmission\n\nclass_names = ['0','1']\nplot_confusion_matrix(confusion_matrix(y_val, y_val_class),\n                      classes=class_names, \n                      normalize=True, \n                      title='Normalized Confusion Matrix: Xgboost')\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}