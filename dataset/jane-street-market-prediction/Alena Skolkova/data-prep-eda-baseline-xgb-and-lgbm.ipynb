{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport datatable as dt\nimport scipy.stats\nfrom sklearn.metrics import mean_absolute_error\n\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nimport xgboost as xgb\n\nfrom tqdm import tqdm\nimport os","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 0. Create Environment"},{"metadata":{"trusted":true},"cell_type":"code","source":"import janestreet\nenv = janestreet.make_env() # initialize the environment\niter_test = env.iter_test() # an iterator which loops over the test set","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Data Preparation"},{"metadata":{},"cell_type":"markdown","source":"## Read the data"},{"metadata":{},"cell_type":"markdown","source":"Use datatable to read the data faster:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = dt.fread('../input/jane-street-market-prediction/train.csv').to_pandas()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train[train['weight'] != 0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Manage missing values"},{"metadata":{},"cell_type":"markdown","source":"First, check the shape of the dataset. We have 2 390 491 rows and 138 columns.\nSecond, check if there are any missings:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.shape)\nmissing_val_count_by_column = (train.isnull().sum())\nprint(missing_val_count_by_column[missing_val_count_by_column > 0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All missings are for features, never for a target variable (resp) that is somewhere among first 10 columns. Good.\n\nNotably, some features have a lot of missings, let's fill them:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.fillna(train.mean(),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define key variables/matrices"},{"metadata":{},"cell_type":"markdown","source":"Define an action variable and an exact list of features:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['action'] = (train['resp'] > 0).astype('int')\nfeatures = [c for c in train.columns if 'feature' in c]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, we will use all features (X) to predict resp (y) and make an action:"},{"metadata":{"trusted":true},"cell_type":"code","source":"#X = train[features[1:]]  # all except for feature_0 that indicates buying/selling\nX = train[features]\ny = train.loc[:, 'action']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Other potentially usefull stuff:"},{"metadata":{"trusted":true},"cell_type":"code","source":"resp_cols = ['resp_1', 'resp_2', 'resp_3', 'resp_4']\n\nweights = train['weight'].values\nresp = train['resp'].values\nresps_rest = train[resp_cols].values\ndates = train['date'].values\n\nf0 = train['feature_0'].values\n\nweights_resp = weights * abs(resp)\nweights_resp_sq = weights * (abs(resp) ** 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Data exploration"},{"metadata":{},"cell_type":"markdown","source":"## Return's visualization\n\nI will have a look at different returns: resp, and other reps?"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15, 7))\nplt.plot(resp)\nplt.title('Returns [resp]')\nplt.grid(True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not very informative, let's look into a subset:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15, 7))\nplt.plot(resp[0:500])\nplt.title('Returns [resp]')\nplt.grid(True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ok, now we an observe some variance clusterization.\n\nLet's look into other returns (resps), the same subset from the beginning of the sample:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15, 7))\nplt.plot(resps_rest[0:500, 0])\nplt.plot(resps_rest[0:500, 1])\nplt.plot(resps_rest[0:500, 2])\nplt.plot(resps_rest[0:500, 3])\nplt.legend(['resp_1','resp_2','resp_3','resp_4'])\nplt.title('Returns [resp_1, resp_2, resp_3, resp_4]')\nplt.grid(True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is not so much difference visible this way. However, resp_4 looks more volatile than resp_1. \n\nFinally, let's compare resp with resp_1 and resp_4 over even smaller period for the sake of clarity:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15, 7))\nplt.plot(resp[0:300])\nplt.plot(resps_rest[0:300, 0])\nplt.plot(resps_rest[0:300, 3])\nplt.legend(['resp','resp_1','resp_4'])\nplt.title('Returns [resp, resp_1, resp_4]')\nplt.grid(True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that resp is often somewhere in between of resp_1 and resp_4 in terms of volatility. Probably, resp was generated as an average of resp_1-resp_4.\n"},{"metadata":{},"cell_type":"markdown","source":"Let's look at the distribution of returns, it looks like normal, but with excess mass near 0 and probably heavy tails:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15, 7))\nplt.hist(resp, 400)\nplt.title('Returns [resp]')\nplt.grid(True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look at the estimate of the kurtosis:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(scipy.stats.kurtosis(resp))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Indeed, it is very different from the kurtosis of the normal distribution.\n\n## ACF and PACF plots\n\nI will use again data subsample (o/w computations take too much time) to plot autocorrelation and partial autocorrelation function for returns and squared returns: "},{"metadata":{"trusted":true},"cell_type":"code","source":"resp_sq = resp ** 2;\n\nfig, (ax1, ax2) = plt.subplots(1, 2,figsize=(14,5), dpi= 80)\nsm.graphics.tsa.plot_acf(resp[0:100000], ax=ax1, lags=10, alpha=0.01)\nsm.graphics.tsa.plot_acf(resp_sq[0:100000], ax=ax2, lags=10, alpha=0.01)\n\n\n# Decorate\n# lighten the borders\nax1.spines[\"top\"].set_alpha(.3); ax2.spines[\"top\"].set_alpha(.3)\nax1.spines[\"bottom\"].set_alpha(.3); ax2.spines[\"bottom\"].set_alpha(.3)\nax1.spines[\"right\"].set_alpha(.3); ax2.spines[\"right\"].set_alpha(.3)\nax1.spines[\"left\"].set_alpha(.3); ax2.spines[\"left\"].set_alpha(.3)\n\n# font size of tick labels\nax1.tick_params(axis='both', labelsize=12)\nax2.tick_params(axis='both', labelsize=12)\nplt.show()\n\nfig, (ax1, ax2) = plt.subplots(1, 2,figsize=(14,5), dpi= 80)\nsm.graphics.tsa.plot_pacf(resp[0:100000], ax=ax1, lags=10, alpha=0.01)\nsm.graphics.tsa.plot_pacf(resp_sq[0:100000], ax=ax2, lags=10, alpha=0.01)\n\n\n# Decorate\n# lighten the borders\nax1.spines[\"top\"].set_alpha(.3); ax2.spines[\"top\"].set_alpha(.3)\nax1.spines[\"bottom\"].set_alpha(.3); ax2.spines[\"bottom\"].set_alpha(.3)\nax1.spines[\"right\"].set_alpha(.3); ax2.spines[\"right\"].set_alpha(.3)\nax1.spines[\"left\"].set_alpha(.3); ax2.spines[\"left\"].set_alpha(.3)\n\n# font size of tick labels\nax1.tick_params(axis='both', labelsize=12)\nax2.tick_params(axis='both', labelsize=12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Overall, we see that for the returns there is about no serial correlation, while for the squared returns there is certainly some serial correlation. So, there is serial (nonlinear) dependence of returns that can be exploited. \n"},{"metadata":{},"cell_type":"markdown","source":"# 3. Prepare for Validation\nI would like to split the sample into training and validation portions. Unfortunately, with time series the standard random sampling into them is not appropriate. There exists a TimeSeriesSplit function\nhttps://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html, but its application will take more time for training than I am ready to allocate for that right now.\n\nThus, for now let's just split manually into two parts , keeping the order of observations, 70%/30%:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(1981287*0.7)  # obs 0-1386900 -> training, 1386901-1981287 -> validation\nt_split = 1386900;","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print([round(np.mean(y[0:t_split].values), 3), round(np.var(y[0:t_split].values), 3)]);\nprint([round(np.mean(y[t_split:1981287].values), 3), round(np.var(y[t_split:1981287].values), 3)]);\nprint(scipy.stats.ttest_ind(y[0:t_split].values, y[t_split:1981287].values))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The means are statistically different, I have to come up with a better splitting scheme later."},{"metadata":{},"cell_type":"markdown","source":"# 4. Baseline Model"},{"metadata":{},"cell_type":"markdown","source":"### Sample split"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = X[0:t_split] # to be changed later\ny_train = y[0:t_split]\n\nX_val = X[t_split+1:]\ny_val = y[t_split+1:]\n\nweights_resp_train =  weights_resp[0:t_split]\nweights_resp_sq_train = weights_resp_sq[0:t_split]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_boost_tree = xgb.XGBClassifier(\n    n_estimators=500,\n    max_depth=10,\n    gamma=0.5,\n    learning_rate=0.05,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    missing=-999,\n    random_state=7,\n    tree_method='gpu_hist'  \n)\n# --> results in 0.4838","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_boost_tree.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluate Prediction Quality"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = base_boost_tree.predict(X_val)\nbase_boost_tree_mae = mean_absolute_error(y_pred, y_val)\nprint(base_boost_tree_mae) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Baseline+-"},{"metadata":{},"cell_type":"markdown","source":"1) Decrease the number of estimators from 500 to 50:"},{"metadata":{"trusted":true},"cell_type":"code","source":"#base_boost_tree_1 = xgb.XGBClassifier(\n#    n_estimators=50,\n#    max_depth=10,\n#    gamma=0.5,\n#    learning_rate=0.05,\n#    subsample=0.8,\n#    colsample_bytree=0.8,\n#    missing=-999,\n#    random_state=7,\n#    tree_method='gpu_hist'  \n#)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#base_boost_tree_1.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#y_pred = base_boost_tree_1.predict(X_val)\n#base_boost_tree_1_mae = mean_absolute_error(y_pred, y_val)\n#print(base_boost_tree_1_mae) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This change results in minor decrease of MAE but takes much less time to train."},{"metadata":{},"cell_type":"markdown","source":"2) Increase gamma from 0.5 to 0.7:"},{"metadata":{"trusted":true},"cell_type":"code","source":"base_boost_tree_2 = xgb.XGBClassifier(\n    n_estimators=50,\n    max_depth=10,\n    gamma=0.7,\n    learning_rate=0.05,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    missing=-999,\n    random_state=7,\n    tree_method='gpu_hist'  \n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#base_boost_tree_2.fit(X_train, y_train)\n#y_pred = base_boost_tree_2.predict(X_val)\n#base_boost_tree_2_mae = mean_absolute_error(y_pred, y_val)\n#print(base_boost_tree_2_mae) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3) Change subsample from 0.8 to 0.7:"},{"metadata":{"trusted":true},"cell_type":"code","source":"base_boost_tree_3 = xgb.XGBClassifier(\n    n_estimators=50,\n    max_depth=10,\n    gamma=0.7,\n    learning_rate=0.05,\n    subsample=0.7,\n    colsample_bytree=0.8,\n    missing=-999,\n    random_state=7,\n    tree_method='gpu_hist'  \n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#base_boost_tree_3.fit(X_train, y_train)\n#y_pred = base_boost_tree_3.predict(X_val)\n#base_boost_tree_3_mae = mean_absolute_error(y_pred, y_val)\n#print(base_boost_tree_3_mae) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6. Baseline XGBClassifier with sample weights\n(python reports that the weights are unused, skip it)"},{"metadata":{"trusted":true},"cell_type":"code","source":"weighted_XGB = xgb.XGBClassifier(\n    n_estimators=500,\n    max_depth=11,\n    gamma=0.5,\n    learning_rate=0.05,\n    subsample=0.9,\n    colsample_bytree=0.8,\n    missing=-999,\n    random_state=7,\n    tree_method='gpu_hist'  \n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#weighted_XGB.fit(X_train, y_train, sample_weight = weights_resp_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#y_pred = weighted_XGB.predict(X_val)\n#weighted_XGB_mae = mean_absolute_error(y_pred, y_val)\n#print(weighted_XGB_mae) # -> 0.4845","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" # 7. LGBMClassifier with sample weights"},{"metadata":{"trusted":true},"cell_type":"code","source":"from lightgbm import LGBMClassifier\nweighted_LGBM = LGBMClassifier(\n    n_estimators=500,\n    max_depth=10,\n    gamma=0.5,\n    learning_rate=0.05,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    missing=-999,\n    random_state=7,\n    tree_method='gpu_hist'  \n)\n# add num_leaves \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weighted_LGBM.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = weighted_LGBM.predict(X_val)\nweighted_LGBM_mae = mean_absolute_error(y_pred, y_val)\nprint(weighted_LGBM_mae) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#for s_weights in [weights_resp_train, weights_resp_sq_train]:\n#    weighted_LGBM.fit(X_train, y_train, sample_weight=s_weights)\n#    y_pred = weighted_LGBM.predict(X_val)\n#    weighted_LGBM_mae = mean_absolute_error(y_pred, y_val)\n#    print(weighted_LGBM_mae)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Making Predictions"},{"metadata":{},"cell_type":"markdown","source":"### Whole Sample Model Fitting"},{"metadata":{"trusted":true},"cell_type":"code","source":"#final_model = base_boost_tree_3\nfinal_model = weighted_LGBM","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_model.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for (test_df, sample_prediction_df) in tqdm(iter_test):\n    X_test = test_df.loc[:, test_df.columns.str.contains('feature')]\n    y_preds = final_model.predict(X_test)\n    sample_prediction_df.action = y_preds\n    env.predict(sample_prediction_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}