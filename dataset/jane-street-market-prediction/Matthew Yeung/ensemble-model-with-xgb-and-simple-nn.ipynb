{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Ensembling Model with XGB and Simple Neural Network:\n---\nIn this Notebook, I applied an ensemble model with XGB and a simple neural network. Using a ensemble model can theoretically reduce overfitting. In this competition, the inference time is of upmost importance. For this reason, the following ways can reduce the inference time:\n1. import data with 'float32' instead of 'float64'\n2. using treelite module for XGB model\n3. Use small neural network instead of deep one\n\nAlso, the 'resp_1','resp_2' ,'resp_3' ,'resp_4' was told to be using as the regularization purpose for the competitors. We use all the 5 labels as target."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport os, gc\n# import cudf\nimport pandas as pd\nimport numpy as np\n# import cupy as cp\nimport janestreet\nimport xgboost as xgb\nfrom hyperopt import hp, fmin, tpe, Trials\nfrom hyperopt.pyll.base import scope\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.model_selection import GroupKFold\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nfrom joblib import dump, load\n\nimport tensorflow as tf\ntf.random.set_seed(42)\nimport tensorflow.keras.backend as K\nimport tensorflow.keras.layers as layers\nfrom tensorflow.keras.callbacks import Callback, ReduceLROnPlateau, ModelCheckpoint, EarlyStopping \n\n# treelite\nimport treelite\nimport treelite_runtime","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"tf.random.set_seed(42)\nfeatures_columns = [\"feature_%d\" % i for i in range(130)]\ncolumns_dtypes = {}\nfor column in features_columns:\n    columns_dtypes[column] = \"float32\"\ncolumns_dtypes[\"resp_1\"] = \"float32\"\ncolumns_dtypes[\"resp_2\"] = \"float32\"\ncolumns_dtypes[\"resp_3\"] = \"float32\"\ncolumns_dtypes[\"resp_4\"] = \"float32\"\ncolumns_dtypes[\"resp\"] = \"float32\"\ntrain = pd.read_csv('/kaggle/input/jane-street-market-prediction/train.csv', dtype=columns_dtypes)\nfeatures = [c for c in train.columns if \"feature\" in c]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f_mean = train[features].median()\ntrain = train.query('date > 85').reset_index(drop = True)\ntrain = train.query('weight > 0').reset_index(drop = True)\ntrain[features] = train[features].fillna(f_mean)\n\n# We use all the 5 labels as the target:\ntrain['action'] = (  (train['resp_1'] > 0 ) & (train['resp_2'] > 0 ) & (train['resp_3'] > 0 ) & (train['resp_4'] > 0 ) &  (train['resp'] > 0 )   ).astype('int')\nresp_cols = ['resp_1', 'resp_2', 'resp_3', 'resp', 'resp_4']\n\nnp.save('f_mean.npy', f_mean)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train[features].values\ny = np.stack([(train[c] > 0).astype('int') for c in resp_cols]).T #Multitarget","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XGB"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Hyperparameters for XGB\nparams = {\n    'colsample_bytree': 0.4,                 \n    'learning_rate': 0.001,\n    'max_depth': 12,\n    'subsample': 0.8,\n    'seed': 42,\n    'tree_method': 'gpu_hist'            # Let's use GPU for a faster experiment\n}\nparams[\"objective\"] = 'binary:logistic'\nparams[\"eval_metric\"] = 'logloss'        # target","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit xgb\ny_tr = train['action'].values\nxgb_path = 'xgb_best.model'\ndtrain = xgb.DMatrix(X, label=y_tr)\nbst = xgb.train(params, dtrain, 100, [(dtrain, 'train')])\nbst.save_model(xgb_path)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### NN Loop"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_mlp(num_columns, num_labels, hidden_units, dropout_rates, label_smoothing, learning_rate):\n    \n    inp = tf.keras.layers.Input(shape = (num_columns, ))\n    x = tf.keras.layers.BatchNormalization()(inp)\n    x = tf.keras.layers.Dropout(dropout_rates[0])(x)\n    for i in range(len(hidden_units)): \n        x = tf.keras.layers.Dense(hidden_units[i])(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n        x = tf.keras.layers.Dropout(dropout_rates[i+1])(x)    \n        \n    x = tf.keras.layers.Dense(num_labels)(x)\n    out = tf.keras.layers.Activation('sigmoid')(x)\n    \n    model = tf.keras.models.Model(inputs = inp, outputs = out)\n    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate),\n                  loss = tf.keras.losses.BinaryCrossentropy(label_smoothing = label_smoothing), \n                  metrics = tf.keras.metrics.AUC(name = 'AUC'), \n                 )\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 5000\nhidden_units = [160, 160, 160]\ndropout_rates = [\n    0.25,\n    0.25,\n    0.25,\n    0.25\n]\nlabel_smoothing = 1e-2\nlearning_rate = 1e-3\n\nnum_models = 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = create_mlp(X.shape[1], y.shape[1], hidden_units, dropout_rates, label_smoothing, learning_rate)\nckp_path = 'best_val_AUC.hdf5'\nmodel.fit(X, y, epochs = 200, batch_size = batch_size, verbose = 0)\nmodel.save_weights('best_val_AUC.hdf5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Retrieve the Models"},{"metadata":{"trusted":true},"cell_type":"code","source":"# XGB retreive model\nxgb_models = []\n#bst_model = xgb.Booster()\n#bst_model.load_model('xgb_best.model')\nxgb_model = treelite.Model.from_xgboost(bst)\nxgb_model.export_lib(toolchain='gcc', libpath='./xgb.so', params={'parallel_comp': 32}, verbose=True)\nprint('finish')\npredictor = treelite_runtime.Predictor('./xgb.so', verbose=True)\nxgb_models.append(predictor)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# NN retreive model\nmodels = []\nmodels.append(model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"env = janestreet.make_env()\nenv_iter = env.iter_test()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"opt_th = 0.5\nf = np.median\nf_mean = np.load('./f_mean.npy')\nfor (test_df, pred_df) in tqdm(env_iter):\n    if test_df['weight'].item() > 0:\n        x_tt = test_df.loc[:, features].values\n        if np.isnan(x_tt[:,:].sum()):\n            x_tt[:, :] = np.nan_to_num(x_tt[:, :]) + np.isnan(x_tt[:, :]) * f_mean\n        pred = []\n        for xgb_ in xgb_models:\n            batch = treelite_runtime.Batch.from_npy2d(x_tt, rbegin=0, rend=1)\n            pred.append(np.array(xgb_.predict(batch)).reshape(1,1))\n        for clf in models:\n            predict = np.mean(clf(x_tt, training = False).numpy(), axis = 0)\n            pred.append(f(predict))\n        pred_df.action = np.where(np.mean(pred) >= opt_th, 1, 0).astype(int)\n    else:\n        pred_df.action = 0\n    env.predict(pred_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}