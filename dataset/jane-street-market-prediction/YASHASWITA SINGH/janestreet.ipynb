{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Installing packages","metadata":{}},{"cell_type":"code","source":"!pip install plotly","metadata":{"execution":{"iopub.status.busy":"2021-05-20T14:22:27.263445Z","iopub.execute_input":"2021-05-20T14:22:27.263834Z","iopub.status.idle":"2021-05-20T14:22:33.330936Z","shell.execute_reply.started":"2021-05-20T14:22:27.263801Z","shell.execute_reply":"2021-05-20T14:22:33.330029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Importing all the necessary packages","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom pandas.plotting import scatter_matrix\nimport seaborn as sns\nimport sklearn\nimport janestreet\nimport os, sys\nimport gc\nimport math\nimport random\nimport pathlib\nfrom sklearn.preprocessing import MinMaxScaler\nfrom catboost import CatBoostClassifier\nimport cv2\nimport plotly.express as px\nimport plotly.figure_factory as ff\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2021-05-20T14:22:33.334522Z","iopub.execute_input":"2021-05-20T14:22:33.334898Z","iopub.status.idle":"2021-05-20T14:22:33.350386Z","shell.execute_reply.started":"2021-05-20T14:22:33.334857Z","shell.execute_reply":"2021-05-20T14:22:33.349642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading the dataset","metadata":{}},{"cell_type":"code","source":"train_df=pd.read_csv('../input/jane-street-market-prediction/train.csv')","metadata":{"execution":{"iopub.status.busy":"2021-05-20T14:22:33.351988Z","iopub.execute_input":"2021-05-20T14:22:33.352341Z","iopub.status.idle":"2021-05-20T14:24:37.784979Z","shell.execute_reply.started":"2021-05-20T14:22:33.352306Z","shell.execute_reply":"2021-05-20T14:24:37.783934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Total number of entries in the train dataset are:', len(train_df))\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T14:24:37.786717Z","iopub.execute_input":"2021-05-20T14:24:37.787289Z","iopub.status.idle":"2021-05-20T14:24:37.844062Z","shell.execute_reply.started":"2021-05-20T14:24:37.787245Z","shell.execute_reply":"2021-05-20T14:24:37.842891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_df = pd.read_csv('/kaggle/input/jane-street-market-prediction/features.csv', index_col = 0)\nprint('Total number of features are:', len(features_df))\nfeatures_df.head()\n","metadata":{"execution":{"iopub.status.busy":"2021-05-20T14:24:37.848296Z","iopub.execute_input":"2021-05-20T14:24:37.848674Z","iopub.status.idle":"2021-05-20T14:24:37.894979Z","shell.execute_reply.started":"2021-05-20T14:24:37.848637Z","shell.execute_reply":"2021-05-20T14:24:37.89409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"print(\"Properties\",train_df.info())\nprint(\"Shape:\",train_df.shape)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T14:24:37.89796Z","iopub.execute_input":"2021-05-20T14:24:37.898598Z","iopub.status.idle":"2021-05-20T14:24:37.921343Z","shell.execute_reply.started":"2021-05-20T14:24:37.898551Z","shell.execute_reply":"2021-05-20T14:24:37.920332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.describe()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T14:24:37.922754Z","iopub.execute_input":"2021-05-20T14:24:37.923363Z","iopub.status.idle":"2021-05-20T14:24:48.804499Z","shell.execute_reply.started":"2021-05-20T14:24:37.923325Z","shell.execute_reply":"2021-05-20T14:24:48.80353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Properties\",features_df.info())\nprint(\"Shape:\",features_df.shape)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T14:24:48.809819Z","iopub.execute_input":"2021-05-20T14:24:48.810195Z","iopub.status.idle":"2021-05-20T14:24:48.834549Z","shell.execute_reply.started":"2021-05-20T14:24:48.810155Z","shell.execute_reply":"2021-05-20T14:24:48.833707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_df.describe()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T14:24:48.835872Z","iopub.execute_input":"2021-05-20T14:24:48.836459Z","iopub.status.idle":"2021-05-20T14:24:48.944683Z","shell.execute_reply.started":"2021-05-20T14:24:48.836419Z","shell.execute_reply":"2021-05-20T14:24:48.94319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training set","metadata":{}},{"cell_type":"markdown","source":"### Missing Data","metadata":{}},{"cell_type":"code","source":"train_df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T14:24:48.949195Z","iopub.execute_input":"2021-05-20T14:24:48.949555Z","iopub.status.idle":"2021-05-20T14:24:49.789735Z","shell.execute_reply.started":"2021-05-20T14:24:48.94952Z","shell.execute_reply":"2021-05-20T14:24:49.788985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.isna().head()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T14:24:49.791009Z","iopub.execute_input":"2021-05-20T14:24:49.791385Z","iopub.status.idle":"2021-05-20T14:24:50.096305Z","shell.execute_reply.started":"2021-05-20T14:24:49.791351Z","shell.execute_reply":"2021-05-20T14:24:50.095186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_df.isna().head()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T14:24:50.097724Z","iopub.execute_input":"2021-05-20T14:24:50.098074Z","iopub.status.idle":"2021-05-20T14:24:50.124001Z","shell.execute_reply.started":"2021-05-20T14:24:50.098035Z","shell.execute_reply":"2021-05-20T14:24:50.122969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig=px.bar(x = train_df.isnull().sum().index,y = train_df.isnull().sum().values, labels = dict(x = \"Attributes\", y = \"Number of Missing Values\"), title= 'Missing Data')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T14:24:50.125657Z","iopub.execute_input":"2021-05-20T14:24:50.126165Z","iopub.status.idle":"2021-05-20T14:24:51.774209Z","shell.execute_reply.started":"2021-05-20T14:24:50.126098Z","shell.execute_reply":"2021-05-20T14:24:51.77335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tags","metadata":{}},{"cell_type":"code","source":"tags = features_df.sum(axis = 1)\ntags_dict = {'Features' : tags.index.values, 'Tag Count' : tags.values}\ntags_df = pd.DataFrame(tags_dict)\nplt.figure(figsize = (130, 25))\nplt.xlabel('Features', fontsize = 100)\nplt.ylabel('Tag Count', fontsize = 100)\nplt.title('Tag Counts of Features', fontsize = 120)\nplt.xticks(rotation ='vertical', fontsize = 50)\nplt.yticks(fontsize = 50)\nsns.barplot(x = 'Features', y = 'Tag Count', data = tags_df,palette=\"viridis\")\nplt.show()\ndel tags_dict,tags_df","metadata":{"execution":{"iopub.status.busy":"2021-05-20T14:24:51.775633Z","iopub.execute_input":"2021-05-20T14:24:51.776141Z","iopub.status.idle":"2021-05-20T14:24:54.007554Z","shell.execute_reply.started":"2021-05-20T14:24:51.776091Z","shell.execute_reply":"2021-05-20T14:24:54.006433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**ANALYSIS**\n* feature_0 has no tags\n* feature 79 to 119 all has 4 tags\n* feature 7 to 36 have 3 and 4 tags periodically\n* Similar trend between 2 to 7, 37 to 40, 120 to 129\n","metadata":{}},{"cell_type":"markdown","source":"**Feature_0 Analysis**","metadata":{}},{"cell_type":"code","source":"train_df['feature_0'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T14:24:54.009183Z","iopub.execute_input":"2021-05-20T14:24:54.009703Z","iopub.status.idle":"2021-05-20T14:24:54.034024Z","shell.execute_reply.started":"2021-05-20T14:24:54.00966Z","shell.execute_reply":"2021-05-20T14:24:54.03338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_0_is_plus_one  = train_df.query('feature_0 ==  1').reset_index(drop = True)\nfeature_0_is_minus_one = train_df.query('feature_0 == -1').reset_index(drop = True)\n# the plot\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 4))\nax1.plot((pd.Series(feature_0_is_plus_one['resp']*feature_0_is_plus_one['weight']).cumsum()), lw=3, label='return', color=\"purple\")\nax2.plot((pd.Series(feature_0_is_minus_one['resp']*feature_0_is_minus_one['weight']).cumsum()), lw=3, label='return', color=\"violet\")\nax1.set_title (\"feature_0 = 1\", fontsize=18)\nax2.set_title (\"feature_0 = -1\", fontsize=18)\nax1.legend(loc=\"lower left\")\nax2.legend(loc=\"lower right\");\n\ndel feature_0_is_plus_one\ndel feature_0_is_minus_one\ngc.collect();","metadata":{"execution":{"iopub.status.busy":"2021-05-20T14:24:54.035366Z","iopub.execute_input":"2021-05-20T14:24:54.035774Z","iopub.status.idle":"2021-05-20T14:24:57.051597Z","shell.execute_reply.started":"2021-05-20T14:24:54.035747Z","shell.execute_reply":"2021-05-20T14:24:57.050673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**ANALYSIS:** when feature_0 is 1, plot shows negative slope while in contrast, when feature_0 is -1, plot shows positive slope. My guess is that feature_0 corresponds to Buy(1) and Sell(-1) or vice versa. So if we set action to 1 with feature_0 = 1 then we are selling and when we set action to 0 with feature_0 = -1, then we are buying. This makes sense since whether we are buying or selling we can still lose or gain profit.","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"### Weights","metadata":{}},{"cell_type":"code","source":"\nnull_weights = (train_df['weight'] == 0).sum()\ntotal_weights = len(train_df['weight'])\nnull_weights_per = null_weights / total_weights * 100\nplt.figure(figsize = (15, 6))\nplt.pie(((train_df.weight==0).mean(),(1-(train_df.weight==0).mean())), explode = (0, 0.1),shadow=True, labels=(f'Null Weights\\n{round((train_df.weight==0).mean()*100,3)}%',f'Considerable Weights\\n{round((1-(train_df.weight==0).mean())*100,3)}%'.format()), colors = ['pink', 'green'])\nplt.legend(title='Weights')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T14:24:57.053238Z","iopub.execute_input":"2021-05-20T14:24:57.053624Z","iopub.status.idle":"2021-05-20T14:24:57.159221Z","shell.execute_reply.started":"2021-05-20T14:24:57.053576Z","shell.execute_reply":"2021-05-20T14:24:57.15793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**ANALYSIS:** dataset has too many NULL weights that can be removed for memory efficiency","metadata":{}},{"cell_type":"code","source":"date_weight_df = pd.DataFrame({'Date' : np.unique(train_df['date'].values), 'NULL_Weights' : train_df[train_df['weight'] == 0.0].groupby(['date']).size().values, 'NOT_NULL_Weights' : train_df[train_df['weight'] != 0.0].groupby(['date']).size().values})\ndate_weight_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T14:24:57.16089Z","iopub.execute_input":"2021-05-20T14:24:57.161307Z","iopub.status.idle":"2021-05-20T14:24:58.483426Z","shell.execute_reply.started":"2021-05-20T14:24:57.161268Z","shell.execute_reply":"2021-05-20T14:24:58.482357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(505, 100))\n\nplt.xticks(rotation ='vertical', fontsize = 60)\nplt.yticks(fontsize = 200)\n\nax = fig.add_subplot(111) \nax2 = ax.twinx() \n\ndate_weight_df.NOT_NULL_Weights.plot(kind='bar',color='green',ax=ax, position = 0)\ndate_weight_df.NULL_Weights.plot(kind='bar',color='pink', ax=ax2, position = 1)\n\nax.grid(None)\nax2.grid(None)\n\nax.set_ylabel('NOT NULL Weights', fontsize = 300)\nax2.set_ylabel('NULL Weights', fontsize = 300)\nax.set_xlabel('Time (in Days)',fontsize = 300)\nfig.suptitle('NULL Weights Vs NOT NULL Weights per Day', fontsize = 450)\n\nax.set_xlim(-1, 505)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T14:24:58.48491Z","iopub.execute_input":"2021-05-20T14:24:58.485482Z","iopub.status.idle":"2021-05-20T14:25:14.204126Z","shell.execute_reply.started":"2021-05-20T14:24:58.485441Z","shell.execute_reply":"2021-05-20T14:25:14.203258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (12,5))\nax = sns.distplot(train_df['weight'], \n             bins=1000, \n             kde_kws={\"clip\":(0.001,1)}, \n             hist_kws={\"range\":(0.001,1)},\n             color='purple', \n             kde=False);\nvalues = np.array([rec.get_height() for rec in ax.patches])\nnorm = plt.Normalize(values.min(), values.max())\nplt.xlabel(\"Histogram of non-zero weights\", size=10)\nplt.show();\ndel values\ngc.collect();","metadata":{"execution":{"iopub.status.busy":"2021-05-20T14:25:14.205587Z","iopub.execute_input":"2021-05-20T14:25:14.206071Z","iopub.status.idle":"2021-05-20T14:25:16.156002Z","shell.execute_reply.started":"2021-05-20T14:25:14.206031Z","shell.execute_reply":"2021-05-20T14:25:16.155239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**ANALYSIS** We can see that most weights are around 0.2 and we can see two 'peaks' which is around 0.2 and 0.3. Note that maximum weight was 167.29 represented by 1.0 on x-axis. ","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(15, 5))\nbalance= pd.Series(train_df['resp']).cumsum()\nresp_1= pd.Series(train_df['resp_1']).cumsum()\nresp_2= pd.Series(train_df['resp_2']).cumsum()\nresp_3= pd.Series(train_df['resp_3']).cumsum()\nresp_4= pd.Series(train_df['resp_4']).cumsum()\nax.set_xlabel (\"Trade\", fontsize=18)\nax.set_title (\"Cumulative resp and time horizons 1, 2, 3, and 4 (500 days)\", fontsize=18)\nbalance.plot(lw=3)\nresp_1.plot(lw=3)\nresp_2.plot(lw=3)\nresp_3.plot(lw=3)\nresp_4.plot(lw=3)\nplt.legend(loc=\"upper left\")\ndel resp_1\ndel resp_2\ndel resp_3\ndel resp_4\ngc.collect();","metadata":{"execution":{"iopub.status.busy":"2021-05-20T14:25:16.157283Z","iopub.execute_input":"2021-05-20T14:25:16.157651Z","iopub.status.idle":"2021-05-20T14:25:17.966907Z","shell.execute_reply.started":"2021-05-20T14:25:16.157616Z","shell.execute_reply":"2021-05-20T14:25:17.965995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**ANALYSIS:** \nWe can see that resp is closely related to resp_4 (blue and purple). Resp_1 and resp_2 also seem to be closely related but much much linear. Resp_3 seem to be in the middle, where the shape is closer to upper group but position is slightly closer to green and orange.\n","metadata":{}},{"cell_type":"markdown","source":"**POINT OF IMPORTANCE:** Weight and resp multiplied together represents a return on the trade.","metadata":{}},{"cell_type":"code","source":"train_df['weight_resp']   = train_df['weight']* train_df['resp']\ntrain_df['weight_resp_1'] = train_df['weight']* train_df['resp_1']\ntrain_df['weight_resp_2'] = train_df['weight']* train_df['resp_2']\ntrain_df['weight_resp_3'] = train_df['weight']* train_df['resp_3']\ntrain_df['weight_resp_4'] = train_df['weight']* train_df['resp_4']\n\nfig, ax = plt.subplots(figsize=(15, 5))\nresp    = pd.Series(1+( train_df.groupby('date')['weight_resp'].mean())).cumprod()\nresp_1  = pd.Series(1+( train_df.groupby('date')['weight_resp_1'].mean())).cumprod()\nresp_2  = pd.Series(1+( train_df.groupby('date')['weight_resp_2'].mean())).cumprod()\nresp_3  = pd.Series(1+( train_df.groupby('date')['weight_resp_3'].mean())).cumprod()\nresp_4  = pd.Series(1+( train_df.groupby('date')['weight_resp_4'].mean())).cumprod()\nax.set_xlabel (\"Day\", fontsize=18)\nax.set_title (\"Cumulative daily return(500 days)\", fontsize=18)\nresp.plot(lw=3, label='resp x weight')\nresp_1.plot(lw=3, label='resp_1 x weight')\nresp_2.plot(lw=3, label='resp_2 x weight')\nresp_3.plot(lw=3, label='resp_3 x weight')\nresp_4.plot(lw=3, label='resp_4 x weight')\nplt.legend(loc=\"lower left\")\ndel resp_1\ndel resp_2\ndel resp_3\ndel resp_4\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T14:25:17.968373Z","iopub.execute_input":"2021-05-20T14:25:17.968924Z","iopub.status.idle":"2021-05-20T14:25:18.634067Z","shell.execute_reply.started":"2021-05-20T14:25:17.968883Z","shell.execute_reply":"2021-05-20T14:25:18.632941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**ANALYSIS:** we can see that there were 'bigger' gains in the beginning and as time approach 500, the gain becomes smaller. In conclusion, the earlier trades are much bigger but we don't know what it's going to be like in our competition test set.","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"## Features","metadata":{}},{"cell_type":"markdown","source":"### Visualization","metadata":{}},{"cell_type":"markdown","source":"**Correlation Between Features**","metadata":{}},{"cell_type":"code","source":"corr = train_df.iloc[:, 7 : 137].corr()\npx.imshow(corr, labels = dict(x = \"Features\", y = \"Features\"), width = 1000, height = 1000, title = \"Correlation between Features\",color_continuous_scale='algae')","metadata":{"execution":{"iopub.status.busy":"2021-05-20T14:25:18.635589Z","iopub.execute_input":"2021-05-20T14:25:18.635945Z","iopub.status.idle":"2021-05-20T14:26:35.418808Z","shell.execute_reply.started":"2021-05-20T14:25:18.63591Z","shell.execute_reply":"2021-05-20T14:26:35.417474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**ANALYSIS:** Features seem to be forming clusters in the above correlation matrix. Features 17 to 26, 27 to 36, and 120 to 129 are some of the many examples shown. These are positively inclined to eachother. In a cluster, the intra cluster distance is lower than the inter cluster distance. Similarly, certain features are clearly negatively related to other features. Amongst the neutral grid, the postive and negative associations stand out!","metadata":{}},{"cell_type":"markdown","source":"**Correlation Between features and resps**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (20, 5))\nfig = sns.heatmap(train_df.corr().iloc[2 : 7, 7 : -6], cmap = 'Paired')\nfig.set(xlabel = 'Resps', ylabel = 'Features')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T14:26:35.420458Z","iopub.execute_input":"2021-05-20T14:26:35.420823Z","iopub.status.idle":"2021-05-20T14:28:10.119989Z","shell.execute_reply.started":"2021-05-20T14:26:35.42078Z","shell.execute_reply":"2021-05-20T14:28:10.119022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**ANALYSIS:** Features are either postively, negatively, or neutrally correlated to Resps. A pattern can be observed in the above heatmap which allows us to explore and dig deeper into their distributions. All the greens indicate negative association, blues indicate very negative association, reds indicate no association, oranges indicate positive association, purples, yellow, and brown indicate increasingly postive association in order.","metadata":{}},{"cell_type":"markdown","source":"**Correlation matrix is really large and confusing but there are clearly some patterns. I will cut it in parts for easier understanding and compare it to features dataframe.**","metadata":{}},{"cell_type":"code","source":"def show_corr_heatmap(df, method=\"pearson\", width=10, calc_corr=False, annot=True):\n    \n    if calc_corr == True:\n        if method == \"MI\":\n            corr = MI_correlations(df)\n        else:\n            corr = df.corr(method)\n    else:\n        corr = df\n        \n    # Generate a mask for the upper triangle\n    mask = np.triu(np.ones_like(corr, dtype=bool))\n\n    # Set up the matplotlib figure\n    f, ax = plt.subplots(figsize=(width, width))\n\n    # Generate a custom diverging colormap\n    cmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n    # Draw the heatmap with the mask and correct aspect ratio\n    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=1, center=0,\n                square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=annot, fmt=\".2f\")\n    \n    if calc_corr == True:\n        return corr\n\n\ndef MI_correlations(df):\n    corrs = {}\n    for col_init in df.columns:\n        corrs[col_init] = {}\n        for col_corr in df.columns:\n            if col_init != col_corr:\n                corrs[col_init][col_corr] = calc_MI(df[col_init], df[col_corr])\n\n    return pd.DataFrame(corrs)\n\ndef calc_MI(col_init, col_corr):\n    \n    if col_init.dtype == np.object:\n        col_init = col_init.astype('category').cat.codes\n    elif col_init.dtype.name == \"category\":\n        col_init = col_init.cat.codes\n        \n    if col_corr.dtype == np.object:\n        col_corr = col_corr.astype('category').cat.codes\n    elif col_corr.dtype.name == \"category\":\n        col_corr = col_corr.cat.codes\n\n    mi = mutual_info_score(col_init, col_corr)\n\n    return mi","metadata":{"execution":{"iopub.status.busy":"2021-05-20T14:28:10.12149Z","iopub.execute_input":"2021-05-20T14:28:10.122041Z","iopub.status.idle":"2021-05-20T14:28:10.138247Z","shell.execute_reply.started":"2021-05-20T14:28:10.121994Z","shell.execute_reply":"2021-05-20T14:28:10.13736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Features 0-50**","metadata":{}},{"cell_type":"code","source":"unnamed_features = [x for x in train_df.columns if \"feature\" in x]\ncorr_matrix = show_corr_heatmap(train_df[unnamed_features].iloc[0:50, 0:50], method=\"spearman\", width=30, calc_corr=True, annot=True)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T14:28:10.139764Z","iopub.execute_input":"2021-05-20T14:28:10.140156Z","iopub.status.idle":"2021-05-20T14:28:15.052067Z","shell.execute_reply.started":"2021-05-20T14:28:10.140107Z","shell.execute_reply":"2021-05-20T14:28:15.051243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Features 51-100**","metadata":{}},{"cell_type":"code","source":"corr_matrix = show_corr_heatmap(train_df[unnamed_features].iloc[51:100, 51:100], method=\"spearman\", width=30, calc_corr=True, annot=True)\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T14:28:15.0536Z","iopub.execute_input":"2021-05-20T14:28:15.054194Z","iopub.status.idle":"2021-05-20T14:28:19.281703Z","shell.execute_reply.started":"2021-05-20T14:28:15.054153Z","shell.execute_reply":"2021-05-20T14:28:19.280518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**ANALYSIS:** Features 73-95 are highly correlated. Features 85-95 are closely related that they may show linear relationship.","metadata":{}},{"cell_type":"markdown","source":"**Features 110-120**","metadata":{}},{"cell_type":"code","source":"sns.pairplot(corr.iloc[110 : 120, 110 : 120])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T14:28:19.283566Z","iopub.execute_input":"2021-05-20T14:28:19.28393Z","iopub.status.idle":"2021-05-20T14:28:43.330655Z","shell.execute_reply.started":"2021-05-20T14:28:19.283894Z","shell.execute_reply":"2021-05-20T14:28:43.329719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Features 120-130**","metadata":{}},{"cell_type":"code","source":"sns.pairplot(corr.iloc[120 : 130, 120 : 130])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T14:28:43.332369Z","iopub.execute_input":"2021-05-20T14:28:43.33277Z","iopub.status.idle":"2021-05-20T14:29:07.521166Z","shell.execute_reply.started":"2021-05-20T14:28:43.33273Z","shell.execute_reply":"2021-05-20T14:29:07.520149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Features with resps**","metadata":{}},{"cell_type":"code","source":"for i in range(120, 122):\n\n    fig, axes = plt.subplots(2, 2, figsize=(12,12))\n\n    sns.scatterplot(data = train_df, x = f\"feature_{str(i)}\", y = \"resp\", ax = axes[0, 0], color = 'red')\n    axes[0,0].set_title(f\"Feature {str(i)} and Resp\", fontsize = 12)\n    axes[0,0].legend(labels=[f'Feature {str(i)}'])\n\n    sns.scatterplot(data = train_df, x = f\"feature_{str(i)}\", y = \"resp_1\", ax = axes[0, 1], color = 'blue')\n    axes[0,1].set_title(f\"Feature {str(i)} and Resp 1\", fontsize = 12)\n    axes[0,1].legend(labels=[f'Feature {str(i)}'])\n\n    sns.scatterplot(data = train_df, x = f\"feature_{str(i)}\", y = \"resp_2\", ax = axes[1, 0], color = 'green')\n    axes[1,0].set_title(f\"Feature {str(i)} and Resp 2\", fontsize = 12)\n    axes[1,0].legend(labels=[f'Feature {str(i)}'])\n\n    sns.scatterplot(data = train_df, x = f\"feature_{str(i)}\", y = \"resp_3\", ax = axes[1, 1], color = 'yellow')\n    axes[1,1].set_title(f\"Feature {str(i)} and Resp 3\", fontsize = 12)\n    axes[1,1].legend(labels=[f'Feature {str(i)}'])\n    \n    plt.show()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T14:40:08.777484Z","iopub.execute_input":"2021-05-20T14:40:08.777815Z","iopub.status.idle":"2021-05-20T14:40:13.581918Z","shell.execute_reply.started":"2021-05-20T14:40:08.777786Z","shell.execute_reply":"2021-05-20T14:40:13.580624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ts_ids","metadata":{}},{"cell_type":"code","source":"trades_per_day = train_df.groupby(['date'])['ts_id'].count()\nfig, ax = plt.subplots(figsize=(15, 5))\nplt.plot(trades_per_day, color=\"purple\")\nax.set_xlabel (\"Day\", fontsize=18)\nax.set_title (\"Total number of ts_id for each day\", fontsize=18)\nax.set_xlim(xmin=0)\nax.set_xlim(xmax=200)\nplt.show()\ndel trades_per_day\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T14:38:07.623847Z","iopub.status.idle":"2021-05-20T14:38:07.624615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"code","source":"features = [col for col in list(train_df.columns) if 'feature' in col]\n\ntrain_df = train_df[train_df['weight'] != 0]","metadata":{"execution":{"iopub.status.busy":"2021-05-20T14:40:15.653592Z","iopub.execute_input":"2021-05-20T14:40:15.653922Z","iopub.status.idle":"2021-05-20T14:40:16.779958Z","shell.execute_reply.started":"2021-05-20T14:40:15.653891Z","shell.execute_reply":"2021-05-20T14:40:16.779107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Cleaning the dataset","metadata":{}},{"cell_type":"code","source":"train_df['action'] = (train_df['resp'].values > 0).astype(int)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T14:40:19.611783Z","iopub.execute_input":"2021-05-20T14:40:19.612108Z","iopub.status.idle":"2021-05-20T14:40:19.629233Z","shell.execute_reply.started":"2021-05-20T14:40:19.612076Z","shell.execute_reply":"2021-05-20T14:40:19.62838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Imputing NAN values with mean","metadata":{}},{"cell_type":"code","source":"NAN_VALUE = -999\n\nf_mean = train_df.mean()\ntrain_df.fillna(f_mean)\n\nX_train = train_df.loc[:, features]\ny_train = train_df.loc[:, 'action']\n\ndel train_df\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T14:40:22.777319Z","iopub.execute_input":"2021-05-20T14:40:22.777758Z","iopub.status.idle":"2021-05-20T14:40:26.640033Z","shell.execute_reply.started":"2021-05-20T14:40:22.777724Z","shell.execute_reply":"2021-05-20T14:40:26.639288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Using Catboost Classifier","metadata":{}},{"cell_type":"code","source":"\nprint('Creating classifier...', end='')\nclf = CatBoostClassifier(loss_function = 'Logloss',\n                         task_type=\"GPU\",\n                         learning_rate = 0.1)\n\n\nclf.fit(X_train, y_train)\n\nprint('Finished.')\n\ndel X_train, y_train\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T14:40:29.685678Z","iopub.execute_input":"2021-05-20T14:40:29.686015Z","iopub.status.idle":"2021-05-20T14:41:05.33126Z","shell.execute_reply.started":"2021-05-20T14:40:29.685983Z","shell.execute_reply":"2021-05-20T14:41:05.33045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nenv = janestreet.make_env()\niter_test = env.iter_test()\nfor (test_df, sample_prediction_df) in iter_test:    \n    test_weight = test_df.iloc[0].weight\n    if test_weight > 0:\n        sample_prediction_df.action = clf.predict(test_df.loc[:, features].fillna(NAN_VALUE))[0]\n    else:\n        sample_prediction_df.action = 0\n    env.predict(sample_prediction_df)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T14:41:47.740243Z","iopub.execute_input":"2021-05-20T14:41:47.740629Z","iopub.status.idle":"2021-05-20T14:41:47.776081Z","shell.execute_reply.started":"2021-05-20T14:41:47.740594Z","shell.execute_reply":"2021-05-20T14:41:47.77461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}