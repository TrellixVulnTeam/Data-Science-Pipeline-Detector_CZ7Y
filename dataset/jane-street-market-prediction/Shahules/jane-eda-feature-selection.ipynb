{"cells":[{"metadata":{},"cell_type":"markdown","source":"<html>\n    <body>\n        <p><font size=\"6\" color=\"blue\">Contents</font></p>\n    </body>\n    \n- [Overview](#1)\n- [Basic Idea](#2)\n- [Target distribution](#3)\n- [Exploratory data analysis](#4)\n- [Random Forest Feature Selection](#5)\n- [Explainability](#6)    "},{"metadata":{},"cell_type":"markdown","source":"###  <p><font size=\"5\" color=\"blue\">Overview</font></p><a id=\"1\" ></a>\n![](https://media.giphy.com/media/S4178TW2Rm1LW/giphy.gif)\n\n##### <p><font size='3' color='blue'>In this challenge, our task is to build a quantitative trading model to maximize returns using market data from a major global stock exchange. Each row in the dataset represents a trading opportunity, for which you will be predicting an action value: 1 to make the trade and 0 to pass on it.</font></p>"},{"metadata":{},"cell_type":"markdown","source":"### <p><font size ='4' color='blue'> Import libraries</font></p>"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.ensemble import RandomForestClassifier\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### <font size ='4' color='blue'><a> Load data files </a></font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"%time\ntrain = pd.read_csv(\"../input/jane-street-market-prediction/train.csv\",nrows=1e5)\ntest = pd.read_csv(\"../input/jane-street-market-prediction/example_test.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <font size ='5' color='blue'><a> Basic Idea </a></font><a id=\"2\" ></a>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print(f\"Train data contains {train.shape[0]} rows and {train.shape[1]} features\")\nprint(f\"Example test data contains {test.shape[0]} rows and {test.shape[1]} features\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### <font size ='4' color='blue'><a> Missing Values </a></font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = pd.DataFrame(train.isna().sum().sort_values(ascending=False)*100/train.shape[0],columns=['missing %']).head(20)\ntemp.style.background_gradient(cmap='Purples')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=train[train['weight']!=0]\ntrain['action']=(train['resp']>0)*1\ntrain.action.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### <font size ='4' color='blue'><a> Target distribution </a></font><a id=\"3\" ></a>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,2,figsize=(20,5))\nsns.countplot(train.action.values,ax=ax[0],palette='husl')\nsns.violinplot(x=train.action.values, y=train.index.values, ax=ax[1], palette=\"husl\")\nsns.stripplot(x=train.action.values, y=train.index.values,\n              jitter=True, ax=ax[1], color=\"black\", size=0.5, alpha=0.5)\nax[1].set_xlabel(\"Target\")\nax[1].set_ylabel(\"Index\");\nax[0].set_xlabel(\"Target\")\nax[0].set_ylabel(\"Counts\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- The class distribution seems to be almost the same.\n- There is no relation between the target and index value."},{"metadata":{},"cell_type":"markdown","source":"## <font size ='5' color='blue'><a> Exploratory Data analysis </a></font><a id=\"4\" ></a>\n#### <font size ='4' color='blue'><a> Some Feature distribution </a></font><a id=\"4\" ></a>\nLet's check the distribution of some features for each target"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def plot_features(df1,target='action',features=[]):\n    \n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(5,5,figsize=(14,14))\n    \n    \n    for feature in features:\n        i += 1\n        plt.subplot(5,5,i)\n        sns.distplot(df1[df1[target]==1][feature].values,label='1')\n        sns.distplot(df1[df1[target]==0][feature].values,label='0')\n        plt.xlabel(feature, fontsize=9)\n        plt.legend()\n    \n    plt.show();\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_features(train,features=[f'feature_{i}' for i in range(25)])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- <p><font size='3' color='red'> Unhide output to see feature distribution</font></p>"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"plot_features(train,features=[f'feature_{i}' for i in range(25,50)])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- <p><font size='3' color='red'> Unhide output to see feature distribution</font></p>"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"plot_features(train,features=[f'feature_{i}' for i in range(50,75)])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### <font size ='4' color='blue'><a> Weight </a></font>\nLet's check the distribution of values of weight feature"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"\nfig,ax = plt.subplots(1,2,figsize=(12,6))\nplt.subplot(1,2,1)\nplt.title(\"Distribution of weight\")\nsns.distplot(train['weight'],color='blue',kde=True,bins=100)\n\nt0 = train[train['action']==0]\nt1 =  train[train['action']==1]\nplt.subplot(1,2,2)\nsns.distplot(train['weight'],color='blue',kde=True,bins=100)\nsns.distplot(t0['weight'],color='blue',kde=True,bins=100,label='action = 0')\nsns.distplot(t1['weight'],color='red',kde=True,bins=100,label='action = 1')\nplt.legend()\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The distribution of weight is highly left skewed,which indicates that there are many samples with 0 weight which we will have to remove."},{"metadata":{},"cell_type":"markdown","source":"#### <font size ='4' color='blue'><a>How response variables changes with weight factor? </a></font>\n"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots(2,2,figsize=(12,10))\nfor i,col in enumerate([f'resp_{i}' for i in range(1,5)]):\n    plt.subplot(2,2,i+1)\n    plt.scatter(train[train.weight!=0].weight,train[train.weight!=0][col])\n    plt.ylabel(col)\n    plt.xlabel('weight')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Most of weights are in range of 0 to 20 and resp variables are in range of -0.05 to 0.05\n- It seems that all of the resp variables follows almost same pattern.\n- The outlier remains almost the same in all cases."},{"metadata":{},"cell_type":"markdown","source":"#### <font size ='4' color='blue'><a> Resp </a></font>\nLet's compare values of resp_1,resp_2,resp_3,resp_4 with resp. There are changes in different time zones."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def plot_resp():\n    fig,ax = plt.subplots(2,2,figsize=(12,10))\n    i=1\n    for col in ([f'resp_{i}' for i in range(1,5)]):\n        \n        plt.subplot(2,2,i)\n        plt.plot(train.ts_id.values,train.resp.values,label='resp',color='blue')\n        plt.plot(train.ts_id.values,train[f'resp_{i}'].values,label=f'resp_{i}',color='red')\n        plt.xlabel('ts_id')\n        plt.legend()\n        \n        i+=1\n    plt.show()\n    \nplot_resp()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nplt.scatter(train.resp.values,train.resp_1.values,color='red',label='resp_1')\nplt.scatter(train.resp.values,train.resp_2.values,color='blue',label='resp_2')\nplt.scatter(train.resp.values,train.resp_3.values,color='orange',label='resp_3')\nplt.scatter(train.resp.values,train.resp_4.values,color='green',label='resp_4')\nplt.xlabel(\"resp\")\nplt.ylabel('other resp variables')\nplt.legend()\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Most of the values have linear relationship with resp variable.\n"},{"metadata":{},"cell_type":"markdown","source":"#### <font size ='4' color='blue'><a> Cumilative sum of response variable in diff time horizons</a></font>\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,6))\nfor col in [f'resp_{i}' for i in range(1,5)]:\n    plt.plot(train[col].cumsum().values,label=col)   \nplt.legend()\nplt.title(\"resp in different time horizons\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### <font size ='4' color='blue'><a> feature_0</a></font>\nfeature_0 is a catergorical variable,let's check it's distribution.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsns.countplot(train.feature_0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <font size ='5' color='blue'><a>Feature Stats</a></font>\n#### <font size ='4' color='blue'><a> Distribution of mean values per row in the train set </a></font>\n"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"features = [col for col in train.columns if 'feature' in col]\nt0 = train.loc[train['action'] == 0]\nt1 = train.loc[train['action'] == 1]\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of mean values per row in the train set\")\nsns.distplot(t0[features].mean(axis=1),color=\"red\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].mean(axis=1),color=\"blue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <font size ='4' color='blue'><a> Distribution of mean values per column in the train set </a></font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of mean values per column in the train set\")\nsns.distplot(t0[features].mean(axis=0),color=\"green\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].mean(axis=0),color=\"darkblue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <font size ='4' color='blue'><a>Distribution of standard deviation values per row in the train set</a></font>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"features = [col for col in train.columns if 'feature' in col]\nt0 = train.loc[train['action'] == 0]\nt1 = train.loc[train['action'] == 1]\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of standard deviation values per row in the train set\")\nsns.distplot(t0[features].std(axis=1),color=\"red\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].std(axis=1),color=\"blue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <font size ='4' color='blue'><a> Distribution of std values per column in the train set </a></font>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of standard deviation values per column in the train set\")\nsns.distplot(t0[features].std(axis=0),color=\"green\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].std(axis=0),color=\"darkblue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <font size ='4' color='blue'><a> Distribution of min values per row in the train set</a></font>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"t0 = train.loc[train['action'] == 0]\nt1 = train.loc[train['action'] == 1]\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of min values per row in the train set\")\nsns.distplot(t0[features].min(axis=1),color=\"orange\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].min(axis=1),color=\"darkblue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <font size ='4' color='blue'><a> Distribution of min values per column in the train set</a></font>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of min values per column in the train set\")\nsns.distplot(t0[features].min(axis=0),color=\"red\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].min(axis=0),color=\"blue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <font size ='5' color='blue'><a>Are there correlations between features?</a></font>\nLet's check the of there are highly correlated features in our data."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"train_corr = train[features].corr().values.flatten()\ntrain_corr = train_corr[train_corr!=1]\ntest_corr = test[features].corr().values.flatten()\ntest_corr = test_corr[test_corr!=1]\n\n\nplt.figure(figsize=(20,5))\nsns.distplot(train_corr, color=\"Red\", label=\"train\")\nsns.distplot(test_corr, color=\"Green\", label=\"test\")\nplt.xlabel(\"Correlation values found in train (except 1)\")\nplt.ylabel(\"Density\")\nplt.title(\"Are there correlations between features?\"); \nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are some highly correlated features in our data,we should probably remove them in the future."},{"metadata":{},"cell_type":"markdown","source":"### <font size ='5' color='blue'><a>PCA components of feature varibles</a></font>\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,5))\npca = PCA().fit(train[features].iloc[:,1:].fillna(train.fillna(train.mean())))\nplt.plot(np.cumsum(pca.explained_variance_ratio_),linewidth=4)\nplt.axhline(y=0.9, color='r', linestyle='-')\nplt.xlabel(\"number of components\")\nplt.ylabel(\"sum of explained variance ratio\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We only need less than 20 PCA components to explain 90% of varience of features.\n\nNow,let's check if there exists any clusters"},{"metadata":{"trusted":true},"cell_type":"code","source":"rb = RobustScaler()\ndata = rb.fit_transform(train[features].iloc[:,1:].fillna(train[features].fillna(train[features].mean())))\ndata = PCA(n_components=2).fit_transform(data)\nplt.figure(figsize=(7,7))\nsns.scatterplot(data[:,0],data[:,1],hue=train['action'])\nplt.xlabel('pca comp 1')\nplt.ylabel('pca comp 2')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <font size ='5' color='blue'><a>KMeans clustering </a></font><a id=\"5\" ></a>\nLet's first chose number of clusters K by using elbow method"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\nX_std = train[[f'feature_{i}' for i in range(1,130)]].fillna(train.mean()).values\nsse = []\nlist_k = list(range(1, 10))\n\nfor k in list_k:\n    km = KMeans(n_clusters=k)\n    km.fit(data)\n    sse.append(km.inertia_)\n\n# Plot sse against k\nplt.figure(figsize=(6, 6))\nplt.plot(list_k, sse, '-o')\nplt.xlabel(r'Number of clusters *k*')\nplt.ylabel('Sum of squared distance');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now,let's cluster and see.."},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KMeans(n_clusters=2)\nlabels=knn.fit_predict(data)\nsns.scatterplot(data[:,0],data[:,1],hue=labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <font size ='5' color='blue'><a> Random Forest feature importances </a></font><a id=\"5\" ></a>\nLet's build a quick tree based model and see which all are the most important features."},{"metadata":{"trusted":true},"cell_type":"code","source":"target='action'\ncols_drop = list(np.setdiff1d(train.columns,test.columns))+['ts_id','date']\n\nclf = RandomForestClassifier()\nclf.fit(train.drop(cols_drop,axis=1).fillna(-999),train['action'])","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"top=20\ntop_features = np.argsort(clf.feature_importances_)[::-1][:top]\nfeature_names = train.drop(cols_drop,axis=1).iloc[:,top_features].columns","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,7))\nsns.barplot(clf.feature_importances_[top_features],feature_names,color='blue')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <font size ='4' color='blue'><a> Distribution of top features </a></font>\nLet's check the feature value distribution for each target for the topn 8 features."},{"metadata":{"trusted":true},"cell_type":"code","source":"top=8\ntop_features = np.argsort(clf.feature_importances_)[::-1][:top]\ntop_features = train.drop(cols_drop,axis=1).iloc[:,top_features].columns\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def plot_features(df1,target='action',features=[]):\n    \n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(4,2,figsize=(14,14))\n    \n    \n    for feature in features:\n        i += 1\n        plt.subplot(4,2,i)\n        sns.distplot(df1[df1[target]==1][feature].values,label='1')\n        sns.distplot(df1[df1[target]==0][feature].values,label='0')\n        plt.xlabel(feature, fontsize=9)\n        plt.legend()\n    \n    plt.show();\n    \nplot_features(train,features=top_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <font size ='4' color='blue'><a>Top feature interactions </a></font><a id=\"6\" ></a>\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(train[list(feature_names[:10])+['action']],hue='action')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <font size ='4' color='blue'><a>SHAP Exaplainability </a></font><a id=\"6\" ></a>\nLet's take a look at SHAP feature importance values\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import shap\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"explainer = shap.TreeExplainer(clf)\nX = train.drop(cols_drop,axis=1).fillna(-999).sample(1000)\nshap_values = explainer.shap_values(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.summary_plot(shap_values, X, plot_type=\"bar\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <font size ='4' color='blue'><a>SHAP Dependence Plots</a></font>\nSHAP dependence plots show the effect of a single feature across the whole dataset. They plot a feature's value vs. the SHAP value of that feature across many samples. Let's take a look at `feature_35`"},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.dependence_plot('feature_35', shap_values[1], X, display_features=X.sample(1000))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <p><font size ='4' color=\"red\">Please do an upvote if you liked it :)</font></p>\n### <font size ='3' color='blue'><a> References. </a></font>\n- https://www.kaggle.com/gpreda/santander-eda-and-prediction\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}