{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Jane Street: Super Fast Utility Score Function\n\nIn this notebook, I compare the time-consumption of different utility score function implementations from discussion forum [Super Fast Utility Score Function Implementation\n][1].\n\n[1]: https://www.kaggle.com/c/jane-street-market-prediction/discussion/201257"},{"metadata":{"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport torch \ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nprint(device)\n\nimport os, gc, random\nif device == 'cuda':\n    import cudf\n    import cupy as cp\nimport datatable as dtable\nimport pandas as pd\nimport numpy as np\nimport janestreet\nfrom numba import njit\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nfrom joblib import dump, load\n\nimport tensorflow as tf\ntf.random.set_seed(42)\nimport tensorflow.keras.backend as K\nimport tensorflow.keras.layers as layers\nfrom tensorflow.keras.callbacks import Callback, ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n\nQUICK_TEST = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed_value):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        \nseed_everything(42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing"},{"metadata":{"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"print('Loading...')\nif QUICK_TEST:\n    train = pd.read_csv('/kaggle/input/jane-street-market-prediction/train.csv', nrows = 10000)\nelse:\n    train = dtable.fread('../input/jane-street-market-prediction/train.csv').to_pandas()\nfeatures = [c for c in train.columns if 'feature' in c]\n\nprint('Forward-Filling...')\ntrain = train.query('weight > 0').reset_index(drop = True)\ntrain[features] = train[features].fillna(method = 'ffill').fillna(0)\ntrain['action'] = (train['resp'] > 0).astype('int')\n\nprint('Finish.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Utility Score Functions"},{"metadata":{},"cell_type":"markdown","source":"For-loop version is very slow. We would better replace it with a magic numpy function called `numpy.bincount()`."},{"metadata":{"trusted":true},"cell_type":"code","source":"def utility_score_loop(date, weight, resp, action):\n    count_i = len(np.unique(date))\n    Pi = np.zeros(count_i)\n    for i, day in enumerate(np.unique(date)):\n        Pi[i] = np.sum(weight[date == day] * resp[date == day] * action[date == day])\n    t = np.sum(Pi) / np.sqrt(np.sum(Pi ** 2)) * np.sqrt(250 / count_i)\n    u = np.clip(t, 0, 6) * np.sum(Pi)\n    return u\n\ndef utility_score_bincount(date, weight, resp, action):\n    count_i = len(np.unique(date))\n    Pi = np.bincount(date, weight * resp * action)\n    t = np.sum(Pi) / np.sqrt(np.sum(Pi ** 2)) * np.sqrt(250 / count_i)\n    u = np.clip(t, 0, 6) * np.sum(Pi)\n    return u","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Further improvement by changing `numpy.unique()` to `pandas.unique()` because it does not sort the values. However, if your date values are consecutive and chronological, using `date.max() + 1` or `date[-1] + 1` is the optimal solution."},{"metadata":{"trusted":true},"cell_type":"code","source":"def utility_score_pd(date, weight, resp, action):\n    count_i = len(pd.unique(date))\n    Pi = np.bincount(date, weight * resp * action)\n    t = np.sum(Pi) / np.sqrt(np.sum(Pi ** 2)) * np.sqrt(250 / count_i)\n    u = np.clip(t, 0, 6) * np.sum(Pi)\n    return u\n\ndef utility_score_max(date, weight, resp, action):\n    count_i = date.max() + 1\n    Pi = np.bincount(date, weight * resp * action)\n    t = np.sum(Pi) / np.sqrt(np.sum(Pi ** 2)) * np.sqrt(250 / count_i)\n    u = np.clip(t, 0, 6) * np.sum(Pi)\n    return u\n\ndef utility_score_last(date, weight, resp, action):\n    count_i = date[-1] + 1\n    Pi = np.bincount(date, weight * resp * action)\n    t = np.sum(Pi) / np.sqrt(np.sum(Pi ** 2)) * np.sqrt(250 / count_i)\n    u = np.clip(t, 0, 6) * np.sum(Pi)\n    return u","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Calibrator has suggested using `len(Pi)` and `@njit(fastmath = True)` for acceleration. Let's check how it performs!"},{"metadata":{"trusted":true},"cell_type":"code","source":"@njit(fastmath = True)\ndef utility_score_numba(date, weight, resp, action):\n    Pi = np.bincount(date, weight * resp * action)\n    t = np.sum(Pi) / np.sqrt(np.sum(Pi ** 2)) * np.sqrt(250 / len(Pi))\n    u = min(max(t, 0), 6) * np.sum(Pi)\n    return u","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We also compare some of the functions in pandas from the discussion forum."},{"metadata":{"trusted":true},"cell_type":"code","source":"# LDMTWO's\ndef utility_score_LDMTWO(df, labels='action,.r0,.weight,.date'.split(',')):\n    \"\"\"Calculate utility score of a dataframe according to formulas defined at\n    https://www.kaggle.com/c/jane-street-market-prediction/overview/evaluation\n    \"\"\"\n    action,resp,weight,date = labels\n    df = df.set_index(date)\n    p = df[weight]  * df[resp] * df[action]\n    p_i = p.groupby(date).sum()\n    t = (p_i.sum() / np.sqrt((p_i**2).sum())) * (np.sqrt(250 / p_i.index.size))\n    return np.clip(t,0,6) * p_i.sum()\n\n# Jorijn Jacko Smit's\ndef utility_score_Jorijn(df):\n    \"\"\"Calculate utility score of a dataframe according to formulas defined at\n    https://www.kaggle.com/c/jane-street-market-prediction/overview/evaluation\n    \"\"\"\n\n    df['p'] = df['weight']  * df['resp'] * df['action']\n    p_i = df.set_index('date')['p'].groupby('date').sum()\n    t = (p_i.sum() / np.sqrt((p_i**2).sum())) * (np.sqrt(250 / p_i.index.size))\n    return min(max(t, 0), 6) * p_i.sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Time-Consumption Comparison"},{"metadata":{"trusted":true},"cell_type":"code","source":"date = train['date'].values\nweight = train['weight'].values\nresp = train['resp'].values\naction = train['action'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('numpy for-loop:')\n%timeit utility_score_loop(date, weight, resp, action)\nprint('-' * 70)\nprint('numpy.bincount():')\n%timeit utility_score_bincount(date, weight, resp, action)\nprint('-' * 70)\nprint('numpy.bincount() + pandas.unique():')\n%timeit utility_score_pd(date, weight, resp, action)\nprint('-' * 70)\nprint('numpy.bincount() + date.max() + 1:')\n%timeit utility_score_max(date, weight, resp, action)\nprint('-' * 70)\nprint('numpy.bincount() + date[-1] + 1:')\n%timeit utility_score_last(date, weight, resp, action)\nprint('-' * 70)\nprint('numba:')\n%timeit utility_score_numba(date, weight, resp, action)\nprint('-' * 70)\nprint('LDMTWO\\'s:')\n%timeit utility_score_LDMTWO(train, labels = 'action,resp,weight,date'.split(','))\nprint('-' * 70)\nprint('Jorijn\\'s:')\n%timeit utility_score_Jorijn(train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Perfect Prediction on Train\n\nWow, we can get a utility score of **224162** if we perfectly predict every action in the train set."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(utility_score_numba(date, weight, resp, action))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Optimisation Based On Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_tr = train.loc[(train['date'] > 80) & (train['date'] <= 370), features]\ny_tr = train.loc[(train['date'] > 80) & (train['date'] <= 370), 'action']\n\nX_tr2 = train.loc[(train['date'] > 370) & (train['date'] <= 400), features]\ny_tr2 = train.loc[(train['date'] > 370) & (train['date'] <= 400), 'action']\n\nX_val = train.loc[train['date'] > 400, features]\ny_val = train.loc[train['date'] > 400, 'action']\n\ndate = train.loc[train['date'] > 400, 'date'].values\nweight = train.loc[train['date'] > 400, 'weight'].values\nresp = train.loc[train['date'] > 400, 'resp'].values\n\nrubbish = gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_mlp(num_columns, num_labels, hidden_units, dropout_rates, learning_rate):\n    \n    inp = tf.keras.layers.Input(shape = (num_columns, ))\n    x = tf.keras.layers.BatchNormalization()(inp)\n    x = tf.keras.layers.Dropout(dropout_rates[0])(x)\n    for i in range(len(hidden_units)):\n        x = tf.keras.layers.Dense(hidden_units[i])(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n        x = tf.keras.layers.Dropout(dropout_rates[i+1])(x)    \n        \n    x = tf.keras.layers.Dense(num_labels)(x)\n    out = tf.keras.layers.Activation('sigmoid')(x)\n    \n    model = tf.keras.models.Model(inputs = inp, outputs = out)\n    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate),\n                  loss = tf.keras.losses.BinaryCrossentropy(), \n                  metrics = tf.keras.metrics.AUC(name = 'AUC'), \n                 )\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hidden_units = [384, 896, 384]\ndropout_rates = [0.10143786981358652, 0.19720339053599725, 0.2703017847244654, 0.2357768967777311]\n\nckp_path = 'JSModel.hdf5'\nmodel = create_mlp(X_tr.shape[1], 1, hidden_units, dropout_rates, 1e-3)\nrlr = ReduceLROnPlateau(monitor = 'val_AUC', factor = 0.1, patience = 3, verbose = 1, \n                        min_delta = 1e-4, mode = 'max')\nckp = ModelCheckpoint(ckp_path, monitor = 'val_AUC', verbose = 0, \n                      save_best_only = True, save_weights_only = True, mode = 'max')\nes = EarlyStopping(monitor = 'val_AUC', min_delta = 1e-4, patience = 5, mode = 'max', \n                   baseline = None, restore_best_weights = True, verbose = 0)\nhistory = model.fit(X_tr.values, y_tr.values, validation_data = (X_val.values, y_val.values), epochs = 100, \n                    batch_size = 4096, callbacks = [rlr, ckp, es], verbose = 1)\nhist = pd.DataFrame(history.history)\nprint(hist['val_AUC'].max())\n\ndel model\nK.clear_session()\nrubbish = gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@njit(fastmath = True)\ndef decision_threshold_optimisation(preds, date, weight, resp, low = 0, high = 1, bins = 100, eps = 1):\n    opt_threshold = low\n    gap = (high - low) / bins\n    action = np.where(preds >= opt_threshold, 1, 0)\n    opt_utility = utility_score_numba(date, weight, resp, action)\n    for threshold in np.arange(low, high, gap):\n        action = np.where(preds >= threshold, 1, 0)\n        utility = utility_score_numba(date, weight, resp, action)\n        if utility - opt_utility > eps:\n            opt_threshold = threshold\n            opt_utility = utility\n    print('Optimal Decision Threshold:', opt_threshold)\n    print('Optimal Utility Score:', opt_utility)\n    return opt_threshold, opt_utility","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The optimised threshold is very high, it seems the market trend in the last 100 days is decreasing dramatically so the model needs to take fewer actions. We may use cross-validation score instead of one leave-out validation score for a better threshold optimisation result."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Optimise Decision Threshold on the Validation Set\nmodel = create_mlp(X_tr.shape[1], 1, hidden_units, dropout_rates, 1e-5)\nmodel.load_weights(ckp_path)\npreds = model.predict(X_val, batch_size = 4096, verbose = 1).ravel()\nopt_threshold, opt_utility = decision_threshold_optimisation(preds, date, weight, resp, preds.min(), preds.max(), 1000, 1)\n\nrubbish = gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Finetune 3 epochs\n# model.fit(np.concatenate((X_tr2.values, X_val.values)), np.concatenate((y_tr2.values, y_val.values)), \n#           epochs = 3, batch_size = 4096, verbose = 1)\n# model.save_weights(ckp_path)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Try training on the entire train set."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_tr = train.loc[train['date'] > 80, features]\ny_tr = train.loc[train['date'] > 80, 'action']\nmodel = create_mlp(X_tr.shape[1], 1, hidden_units, dropout_rates, 1e-4)\nmodel.fit(X_tr.values, y_tr.values, batch_size = 4096, epochs = 10, verbose = 1)\nmodel.save_weights(ckp_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"example_test = pd.read_csv('../input/jane-street-market-prediction/example_test.csv')\nexample_test = example_test.query('weight > 0').reset_index(drop = True)\nexample_test[features] = example_test[features].fillna(method = 'ffill').fillna(0)\ntest_preds = model.predict(example_test[features].values, batch_size = 4096, verbose = 1).ravel()\nprint(test_preds.min())\nprint(test_preds.max())\nprint(test_preds.mean())\nprint(test_preds.std())\nplt.hist(test_preds, bins = 100)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@njit\ndef fast_fillna(array, values):\n    if np.isnan(array.sum()):\n        array = np.where(np.isnan(array), values, array)\n    return array","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env = janestreet.make_env()\nenv_iter = env.iter_test()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Try 0.5 threshold\nopt_threshold = 0.5\ntmp = np.zeros(len(features))\nfor (test_df, pred_df) in tqdm(env_iter):\n    if test_df['weight'].item() > 0:\n        x_tt = test_df.loc[:, features].values\n        x_tt[0, :] = fast_fillna(x_tt[0, :], tmp)\n        tmp = x_tt[0, :]\n        pred = model(x_tt, training = False).numpy().item()\n        pred_df.action = np.where(pred >= opt_threshold, 1, 0).astype(int)\n    else:\n        pred_df.action = 0\n    env.predict(pred_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\n\nSo far, the fastest version is the numba version! Tribute to [Calibrator][1]!\n\n[1]: https://www.kaggle.com/calibrator"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom numba import njit\n\n@njit(fastmath = True)\ndef utility_score_numba(date, weight, resp, action):\n    Pi = np.bincount(date, weight * resp * action)\n    t = np.sum(Pi) / np.sqrt(np.sum(Pi ** 2)) * np.sqrt(250 / len(Pi))\n    u = min(max(t, 0), 6) * np.sum(Pi)\n    return u","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}