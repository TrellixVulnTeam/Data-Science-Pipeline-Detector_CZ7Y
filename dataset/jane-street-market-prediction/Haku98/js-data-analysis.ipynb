{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nd = '/kaggle/input/jane-street-market-prediction/'\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import norm, laplace\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv(d + \"train.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First let's gather some basic information about the data and save it so we don't have to recompute it every time:"},{"metadata":{"trusted":true},"cell_type":"code","source":"gauss = {}\nfor col in train:\n    if 'feature' in col:\n        gauss[col] = (train[col].min(), train[col].max(), train[col].mean(), train[col].std())\n\nstart = end = int(train.iloc[0].date)\nend = int(train.iloc[-1].date)\ndays = [i for i in range(start, end + 2)]\nbatch_idx = train.date.values.searchsorted(days)\n\nlaplacians = {}\nfor col in train:\n    if 'resp' in col:\n        laplacians[col] = (train[col].median(), (train[col] - train[col].median()).abs().mean())\n\ngaussians_df = pd.DataFrame.from_dict(gauss).rename(index={0: 'min', 1: 'max', 2: 'mean', 3: 'std'})\nlaplacians_df = pd.DataFrame.from_dict(laplacians).rename(index={0: 'median', 1: 'mad'})\nbatch_idx_df = pd.DataFrame(batch_idx)\nlaplacians_df.to_csv('laplacians.csv')\ngaussians_df.to_csv('gaussians.csv')\nbatch_idx_df.to_csv('batch_idx.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First thing I'd like to do is to find out what's the distribution of the 'resp' data. If X is our data and F is the CDF of our guess then we can verify whether it's a good guess by plotting a histogram of F(X). If it's close to uniform then our guess was good. We can verify this method by applying it to some samples from standard normal:"},{"metadata":{"trusted":true},"cell_type":"code","source":"rand_normal = np.random.normal(size=1000000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_ = plt.hist(norm.cdf(rand_normal), bins=100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see these are approximately uniformly distributed so that means. Now let's look at the resp data"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(1, 2)\naxs1, axs2 = axes.flatten()\naxs1.hist(train.resp, bins=100)\n_ = axs2.hist(train.resp, bins=100, log=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above are the plots of distribution of 'resp' values. The second one is scaled by logarithm for better visibility. At this point my guess is that the data has a log-laplace distribution but there are also other possibilities. For now let's plot the CDF of 'resp':"},{"metadata":{"trusted":true},"cell_type":"code","source":"_ = plt.plot(train['resp'].sort_values().values, np.array([i for i in range(len(train))]) / len(train))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Aside from the graph being very steep around 0 (approximately the mean of 'resp') I don't see any characteristics that would point towards a particular distribution. Let's implement some known CDFs for now:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def laplace_cdf(x, mu, b):\n    p = x < mu\n    n = x > mu\n    p = p * np.exp((x - mu) / b) / 2\n    n = n * (1 - np.exp(- (x - mu) / b) / 2)\n    return n + p\n\ndef log_laplace_cdf(y, mu, b):\n    s = (np.log(y) < mu) * 2 - 1\n    return (1 + s * (1 - np.exp(-np.abs(np.log(y) - mu)/b))) / 2\ndef cauchy_cdf(x, x0, gamma):\n    return np.arctan((x - x0) / gamma) / np.pi + 1/2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_ = cauchy_cdf(train['resp'], 5.662163451492418e-05, 0.00718).hist(bins=20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So cauchy is not the worst guess, but we should be able to do better."},{"metadata":{"trusted":true},"cell_type":"code","source":"t1 = laplace_cdf(train.resp,  laplacians_df.loc['median', 'resp'],  laplacians_df.loc['mad', 'resp'])\n_ = t1.hist(bins=20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That's a bit weird looking graph. Maybe we could flatten it out more with some other parameters?"},{"metadata":{"trusted":true},"cell_type":"code","source":"t2 = laplace_cdf(train.resp,  laplacians_df.loc['median', 'resp'],  laplacians_df.loc['mad', 'resp'] / 2)\n_ = t2.hist(bins=20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That looks somewhat better, but we still have these heavy tails near 0 and 1. Let's try log-laplace now"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(laplacians_df.loc['median', 'resp'] + 1,  laplacians_df.loc['mad', 'resp'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_ = plt.hist(np.log(train.resp + 1), bins=100, log=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stats.laplace.fit(np.log(train.resp + 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}