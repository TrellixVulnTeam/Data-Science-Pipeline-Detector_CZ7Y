{"cells":[{"metadata":{},"cell_type":"markdown","source":"## 本版本重要更改 \n描述：提前映射成0-1 转化为0-1classification\n1. missing=none\n2. 完成cv\n3. 完成一组参数的寻优 metric  \n## 本版本目标 8000"},{"metadata":{},"cell_type":"markdown","source":"## 当前情况更新（20210216）  \n1. 关于auc 与 utility score的关系  auc基本维持不变，us会随着迭代次数的增加增长，但在LB上的表现不见好转  \n2. 关于多阶段模型"},{"metadata":{},"cell_type":"markdown","source":"## prepare the librarias used below"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import datatable as dt\nfrom tqdm import tqdm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip --quiet install ../input/treelite/treelite-0.93-py3-none-manylinux2010_x86_64.whl","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip --quiet install ../input/treelite/treelite_runtime-0.93-py3-none-manylinux2010_x86_64.whl","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# treelite\nimport treelite\nimport treelite_runtime ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# load data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_datatable = dt.fread('../input/jane-street-market-prediction/train.csv')\ntrain_data = train_data_datatable.to_pandas()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=train_data.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_data['date']\n# df[['weight','resp']].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Generally weights larger than 1 do correspond to leverage"},{"metadata":{"trusted":true},"cell_type":"code","source":"# df['weight'].value_counts()/len(df)\n# 0.17 20%左右\n#  为什么weight=0也会放上来\n# 虽然weight=0 但是resp仍是准确的吧 室友有信息可用\n# df[df['weight']==0]['resp'].plot(kind='hist')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# len(df[(df['weight']==0) & (df['resp']>0)])/len(df[df['weight']==0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# len(df[df['resp']>0])/len(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# len(df[(df['weight']!=0) & (df['resp']>0)])/len(df[df['weight']!=0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # 差异显著性检验\n# deltap=0.5044130406145096-0.5024071123449428\n# p=0.504069666022587\n# z=(deltap)/np.sqrt(p*(1-p)*(1/len(df[df['weight']==0])+1/len(df[df['weight']!=0])))\n# z","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 可见weight下确实有一些甄别作用\n# import scipy.stats as st\n# st.norm.cdf(z)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df['resp'].plot(kind='hist')\n# 限定weight==0前后的resp分布基本一致 所以信息基本是有用的\n# weight是什么？\n# weight里面也是含有预判和信息的 所以可以理论上可以加入特征的行列","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 内在逻辑是 对于weight>0的股票 我进行了交易，然后体现出来的resp是市面上的收益情况？-- 因为weight=0时同样有resp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# len(df[(df['weight']>=0.5) & (df['resp']>0)])/len(df[df['weight']>=0.5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# len(df[(df['weight']>=1) & (df['resp']>0)])/len(df[df['weight']>=1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 继续看下weight与resp的关联","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 回归问题","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 多目标怎样利用","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# xgb与NN的集成","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Simple pre_process for xgb"},{"metadata":{"trusted":true},"cell_type":"code","source":"## 缺失值暂不处理","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# np.percentile(df['weight'],[0,5,25,50,75,95,100])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## weight=0 but resp's info is ready to use"},{"metadata":{"trusted":true},"cell_type":"code","source":"## 暂时只取weight>0的部分\n# df=df[df['weight']>0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## generate ground truth action"},{"metadata":{"trusted":true},"cell_type":"code","source":"## 目前先直接根绝resp正负性转化为0-1问题\ndf['action']=df['resp'].apply(lambda x: 1 if x>0 else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df['action'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1,0 还是比较均衡的，所以不存在样本不均衡的问题"},{"metadata":{"trusted":true},"cell_type":"code","source":"import copy\ndef add_features(df, features):\n    new_features = copy.deepcopy(features)\n    \n    # todo\n    df[\"cross_1_2\"] = df[\"feature_1\"] / (df[\"feature_2\"] + 1e-5)\n    df[\"cross_41_42_43\"] = df[\"feature_41\"] + df[\"feature_42\"] + df[\"feature_43\"]\n    new_features.extend([\"cross_1_2\", \"cross_41_42_43\"])\n\n    return df, new_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## 使用原始全部feature\nfeatures = [c for c in df.columns if 'feature' in c]\ntarget = ['action']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df,new_features=add_features(df,features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# splitbydate:输入四个date节点拆分原始数据集为train and test"},{"metadata":{"trusted":true},"cell_type":"code","source":"# original dataframe;feature list;target list;and time point used to split dataset\n# def splitbydate(oridf,feature,target,test_end,test_start,train_end,train_start=0): \n#     if 'date' not in oridf.columns:\n#         print('Error!')\n#         return\n#     else:\n#         feature_=feature.copy()\n#         target_=target.copy()\n#         feature_.append('date')\n#         target_.append('date')\n#         #print(target_)\n#         x_df=oridf[feature_]\n#         y_df=oridf[target_]\n#         X_train,X_test=x_df[(x_df['date']>=train_start) & (x_df['date']<train_end)],x_df[(x_df['date']>=test_start) & (x_df['date']<test_end)]\n#         y_train,y_test=y_df[(y_df['date']>=train_start) & (y_df['date']<train_end)],y_df[(y_df['date']>=test_start) & (y_df['date']<test_end)]\n#         vali=oridf[[\"date\",\"weight\",\"resp\",\"action\"]][(oridf['date']>=test_start) & (oridf['date']<test_end)]\n#         return X_train[feature],X_test[feature],y_train[target],y_test[target],vali","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# original dataframe;feature list;target list;and time point used to split dataset\ndef splitbydate_more(oridf,feature,target,test_end,test_start,train_end,train_start=0): \n    if 'date' not in oridf.columns:\n        print('Error!')\n        return\n    else:\n        feature_=feature.copy()\n        target_=target.copy()\n        feature_.append('date')\n        target_.append('date')\n        #print(target_)\n        x_df=oridf[feature_]\n        y_df=oridf[target_]\n        X_train,X_test=x_df[(x_df['date']>=train_start) & (x_df['date']<train_end)],x_df[(x_df['date']>=test_start) & (x_df['date']<test_end)]\n        y_train,y_test=y_df[(y_df['date']>=train_start) & (y_df['date']<train_end)],y_df[(y_df['date']>=test_start) & (y_df['date']<test_end)]\n        vali=oridf[[\"date\",\"weight\",\"resp\",\"action\"]][(oridf['date']>=test_start) & (oridf['date']<test_end)]\n        trai=oridf[[\"date\",\"weight\",\"resp\",\"action\"]][(oridf['date']>=train_start) & (oridf['date']<train_end)]\n        return X_train[feature],X_test[feature],y_train[target],y_test[target],trai,vali","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_train,X_test,y_train,y_test,trai,vali=splitbydate_more(df,features,target,500,450,400,85)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# xgboost参数\nhttps://www.kaggle.com/code1110/janestreet-faster-inference-by-xgb-with-treelite"},{"metadata":{"trusted":true},"cell_type":"code","source":"def utility_scoring(df):\n    \"\"\"\n    To get the utility score used in the challenge.\n    \"\"\"\n    from math import sqrt\n    u = 0\n    Pi = []\n    Pis = []\n    count_i = len(df['date'].unique())\n    for i in list(df['date'].unique()):\n        #print(\"date value= \", i)\n        #print(10*\"=\")\n        tmp = df[df['date'] == i][[\"date\",\"weight\",\"resp\",\"action\"]]\n        tmp[\"mult\"] = tmp['weight'] * tmp['resp'] * tmp['action']\n        Pi.append(tmp[\"mult\"].sum())\n        Pis.append((tmp[\"mult\"].sum())**2)\n    \n    t =  sum(Pi)/sqrt(sum(Pis)) * sqrt(250/count_i)\n    u = min(max(t,0),6)*sum(Pi)\n    return u","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"thres=0.5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we need/have\n# 并不涉及到与原始真实值的比较 只是自顾自得输出\n# 先弄一个绝对评分的函数- 对比得分与auc表现的一致性\n# 可以弄一个指标 例如预测得分/真实得分\n\n# trai vai\ndef utility_score(predt,dtrain):\n    \"\"\"\n    To get the utility score used in the challenge.\n    \"\"\"\n    y = dtrain.get_label()\n    tarlen=len(y)\n    if tarlen==len(trai):\n        u=utility_scoring(trai)\n    elif tarlen==len(vali):\n        u=utility_scoring(vali)\n#     vali['actionv'] = (predt> 0.5)*1\n    \n    return 'utilityscore',float(u)\n\n# trai vai\ndef relative_utility_score(predt,dtrain):\n    \"\"\"\n    To get the utility score used in the challenge.\n    \"\"\"\n    \n    y = dtrain.get_label()\n    tarlen=len(y)\n    if tarlen==len(trai):\n        trai['action']=(predt> thres)*1\n        u2=utility_scoring(trai)\n        u=u2/trai_ori\n    elif tarlen==len(vali):\n        vali['action']=(predt> thres)*1\n        u2=utility_scoring(vali)\n        u=u2/vali_ori\n#     vali['actionv'] = (predt> 0.5)*1\n    \n    return 'r_utilityscore',float(u)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# params['learning_rate']=0.05\n# params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dtrain = xgb.DMatrix(X_train.values, label=y_train.values)\n# dvalid = xgb.DMatrix(X_test.values, label=y_test.values)\n# bst = xgb.train(params,\n#                 dtrain,\n#                 feval=utility_score,\n#                 num_boost_round=500,\n#                 evals=[(dtrain, 'train'), (dvalid, 'eval')],\n#                 early_stopping_rounds=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# trai_ori=utility_scoring(trai)\n# vali_ori=utility_scoring(vali)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\ngc.enable()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# early_stop = xgb.callback.EarlyStopping(rounds=10,\n#                                     metric_name='r_utilityscore',\n#                                     data_name='eval')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED = 21\nparams = {\n    #general parameter\n    \n    'booster':'gbtree',\n    \n    #tree parameter\n    \n    # shrink the weight to make the process more conservative\n    'learning_rate': 0.05,\n    # minimum loss reduce needed to make a new partition\n    'gamma':0,\n    # max depth of the tree\n    'max_depth': 8,\n    'min_child_weight':10,\n    'tree_method': 'gpu_hist', # Let's use GPU for a faster experiment\n    # 随机采样\n    'colsample_bytree': 0.72,                 \n    'subsample': 0.8,\n    \n    'seed': SEED,\n    \n    #print every 10 times\n#     'verbose_eval':100,\n   \n    #learning task parameter\n    # 可考虑更换目标函数为auc\n    'objective':'binary:logistic',\n    'disable_default_eval_metric': 1\n#     'eval_metric':['auc']\n#     'n_boost_round': 600,\n#     'early_stopping_rounds':20\n    \n\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# results={}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dtrain = xgb.DMatrix(X_train.values, label=y_train.values)\n# dvalid = xgb.DMatrix(X_test.values, label=y_test.values)\n\n# bst = xgb.train(params,\n#                 dtrain,\n#                 evals=[(dtrain, 'train'), (dvalid, 'eval')],\n#                 feval=relative_utility_score,\n#                 num_boost_round=50,\n#                 callbacks=[early_stop]\n# #                 evals_result=results\n# #                 early_stopping_rounds=10\n#                )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plt.figure(figsize=(15,10))\n# plt.plot(np.arange(1000),results['train']['r_utilityscore'])\n# plt.plot(np.arange(1000),results['eval']['r_utilityscore'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import matplotlib.pyplot as plt\n# plt.figure(figsize=(20,15)) \n# xgb.plot_importance(bst,ax=plt.gca())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fscore=pd.DataFrame.from_dict(bst.get_fscore(),orient='index')\n# fscore.columns=['fs']\n# fscore=fscore.sort_values(by=['fs'],ascending=False)\n# # fscore","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plt.figure(figsize=(15,10))\n# plt.bar(fscore.index,fscore['fs'].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fscore[fscore['fs']>1000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(\"# original features: {}\".format(len(features)))\n# print(\"# features included in xgb model: {}\".format(len(fscore)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import re\n# def cutnum(x):\n#     res=re.match(r'([a-z]*)([0-9]*)',x).group(2)\n#     return int(res)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# one feature has none improvement on prediction\n# select top 60% feature to service for final training\n# fscore['num_f']=fscore.index.map(cutnum)\n# fscore_se=fscore[:90]\n# fscore_se","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# f_feature=[]\n# for i in fscore_se['num_f']:\n# #     print(i)\n#     f_feature.append(features[i])\n# f_feature","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# new_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# del X_train,X_test,y_train,y_test,trai,vali,trai_ori,vali_ori,dtrain,dvalid","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,y_train,y_test,trai,vali=splitbydate_more(df,new_features,target,500,500,85+415,85)\ndtrain = xgb.DMatrix(X_train.values, label=y_train.values)\ntrai_ori=utility_scoring(trai)\n# vali_ori=utility_scoring(vali)\n# dvalid = xgb.DMatrix(X_test.values, label=y_test.values)\nbst = xgb.train(params,\n        dtrain,\n        num_boost_round=600)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# subtree_depth_params = [\n#     (max_depth, min_child_weight)\n#     for max_depth in range(8,13)\n#     for min_child_weight in range(7,10)\n# ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# subtree_depth_params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cv_times=5\n# start_list=[90,110,130,150,170]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import gc\n# gc.enable()\n\n# del train_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# bestscore=0\n# bestparams=params\n# cv_score=[]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def tune_parameters(params,params_valuelist):\n#     for max_depth, min_child_weight in params_valuelist:\n#         print(\"CV with max_depth={}, min_child_weight={}\".format(\n#                                  max_depth,\n#                                  min_child_weight))\n#         # Update our parameters\n#         params['max_depth'] = max_depth\n#         params['min_child_weight'] = min_child_weight\n#         score=[]\n#         for i in range(cv_times):\n#             # train length 250\n#             # test legth 50\n#             # 500 450 420 170\n#             # 480 430 400 150\n#             # 460 410 380 130\n#             # 110\n#             # 90\n#             X_train,X_test,y_train,y_test,vali=splitbydate(df,features,target,start_list[i]+330,start_list[i]+280,start_list[i]+250,start_list[i])\n#             dtrain = xgb.DMatrix(X_train.values, label=y_train.values)\n#             dvalid = xgb.DMatrix(X_test.values, label=y_test.values)\n#             bst = xgb.train(params,\n#                     dtrain,\n#                     num_boost_round=500,\n#                     evals=[(dtrain, 'train'), (dvalid, 'eval')],\n#                     early_stopping_rounds=20,\n#                     verbose_eval=100)\n#             score.append(bst.best_score)\n#             del X_train,X_test,y_train,y_test,vali\n\n#         cv_score.append(np.mean(score))\n#     #     print(\"CV with max_depth={}, min_child_weight={}\".format(\n#     #                              max_depth,\n#     #                              min_child_weight))\n#         if np.mean(score)>bestscore:\n#     #         bestparams=params\n#             bestscore=np.mean(score)\n#             best_max_depth,best_min_child_weight=max_depth,min_child_weight\n\n#     bestparams['max_depth'] = max_depth\n#     bestparams['min_child_weight'] = min_child_weight\n#     bestparams","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for max_depth, min_child_weight in subtree_depth_params:\n#     print(\"CV with max_depth={}, min_child_weight={}\".format(\n#                              max_depth,\n#                              min_child_weight))\n#     # Update our parameters\n#     params['max_depth'] = max_depth\n#     params['min_child_weight'] = min_child_weight\n#     score=[]\n#     for i in range(cv_times):\n#         # train length 250\n#         # test legth 50\n#         # 500 450 420 170\n#         # 480 430 400 150\n#         # 460 410 380 130\n#         # 110\n#         # 90\n#         X_train,X_test,y_train,y_test,vali=splitbydate(df,features,target,start_list[i]+330,start_list[i]+280,start_list[i]+250,start_list[i])\n#         dtrain = xgb.DMatrix(X_train.values, label=y_train.values)\n#         dvalid = xgb.DMatrix(X_test.values, label=y_test.values)\n#         bst = xgb.train(params,\n#                 dtrain,\n#                 num_boost_round=500,\n#                 evals=[(dtrain, 'train'), (dvalid, 'eval')],\n#                 early_stopping_rounds=20,\n#                 verbose_eval=100)\n#         score.append(bst.best_score)\n#         del X_train,X_test,y_train,y_test,vali\n    \n#     cv_score.append(np.mean(score))\n# #     print(\"CV with max_depth={}, min_child_weight={}\".format(\n# #                              max_depth,\n# #                              min_child_weight))\n#     if np.mean(score)>bestscore:\n# #         bestparams=params\n#         bestscore=np.mean(score)\n#         best_max_depth,best_min_child_weight=max_depth,min_child_weight\n        \n# bestparams['max_depth'] = max_depth\n# bestparams['min_child_weight'] = min_child_weight\n# bestparams","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plt.plot([str(x) for x in subtree_depth_params],cv_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# bestparams","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# bestscore","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# subtree_depth_params = [\n#     (max_depth, min_child_weight)\n#     for max_depth in range(6,10)\n#     for min_child_weight in range(8,12)\n# ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# del X_train,X_test,y_train,y_test,trai,vali,trai_ori,vali_ori,dtrain,dvalid","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\ngc.enable()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# bestscore=0\n# bestparams=params\n# cv_score=[]\n# for lr in [.3, .2, .1, .05, .01, .005]:\n#     print(\"learning rate={}\".format(\n#                              lr))\n#     # Update our parameters\n#     params['learning_rate'] = lr\n#     score=[]\n#     for i in range(cv_times):\n#         X_train,X_test,y_train,y_test,trai,vali=splitbydate_more(df,f_feature,target,start_list[i]+330,start_list[i]+280,start_list[i]+250,start_list[i])\n#         trai_ori=utility_scoring(trai)\n#         vali_ori=utility_scoring(vali)\n#         dtrain = xgb.DMatrix(X_train.values, label=y_train.values)\n#         dvalid = xgb.DMatrix(X_test.values, label=y_test.values)\n#         bst = xgb.train(params,\n#                 dtrain,\n#                 num_boost_round=500,\n#                 feval=relative_utility_score,\n#                 evals=[(dtrain, 'train'), (dvalid, 'eval')],\n#                 early_stopping_rounds=20,\n#                 verbose_eval=100)\n#         score.append(bst.best_score)\n#         del X_train,X_test,y_train,y_test,trai,vali,trai_ori,vali_ori,dtrain,dvalid\n    \n#     cv_score.append(np.mean(score))\n# #     print(\"CV with max_depth={}, min_child_weight={}\".format(\n# #                              max_depth,\n# #                              min_child_weight))\n#     if np.mean(score)>bestscore:\n#         bestparams=params\n#         bestscore=np.mean(score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cv_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plt.plot([str(x) for x in subtree_depth_params],cv_score)\n# plt.xticks(rotation=60)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# bestparams['max_depth'] = 6\n# bestparams['min_child_weight'] = 10\n# bestparams","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# del X_train,X_test,y_train,y_test,vali","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_train,X_test,y_train,y_test,vali=splitbydate(df,features,target,500,470,440,85)\n# dtrain = xgb.DMatrix(X_train.values, label=y_train.values)\n# dvalid = xgb.DMatrix(X_test.values, label=y_test.values)\n# bst = xgb.train(bestparams,\n#                 dtrain,\n#                 num_boost_round=500,\n#                 evals=[(dtrain, 'train'), (dvalid, 'eval')],\n#                 early_stopping_rounds=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 基于treelite进行编译\nmodel = treelite.Model.from_xgboost(bst)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# generate shared library\ntoolchain = 'gcc'\nmodel.export_lib(toolchain=toolchain, libpath='./mymodel_score_600rounds_fullfeature_1.so',\n                 params={'parallel_comp': 32}, verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predictor from treelite\n# predictor = treelite_runtime.Predictor('./mymodel_score.so', verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dtest=xgb.DMatrix(X_test.values, label=y_test.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def utility_scoring(df):\n#     \"\"\"\n#     To get the utility score used in the challenge.\n#     \"\"\"\n#     from math import sqrt\n#     u = 0\n#     Pi = []\n#     Pis = []\n#     count_i = len(df['date'].unique())\n#     for i in list(df['date'].unique()):\n#         #print(\"date value= \", i)\n#         #print(10*\"=\")\n#         tmp = df[df['date'] == i][[\"date\",\"weight\",\"resp\",\"actionv\"]]\n#         tmp[\"mult\"] = tmp['weight'] * tmp['resp'] * tmp['actionv']\n#         Pi.append(tmp[\"mult\"].sum())\n#         Pis.append((tmp[\"mult\"].sum())**2)\n    \n#     t =  sum(Pi)/sqrt(sum(Pis)) * sqrt(250/count_i)\n#     u = min(max(t,0),6)*sum(Pi)\n#     return u","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# treelite\n# batch = treelite_runtime.Batch.from_npy2d(X_test.values)\n# predicted_treelite = predictor.predict(batch)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predicted_treelite","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.metrics import log_loss\n# score = log_loss(y_test.values, predicted_treelite)\n# print(score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# best_i = 0\n# best_u = 0\n# for i in [0.41, 0.45,0.49,0.5, 0.51,0.52, 0.55, 0.6,0.63, 0.65]:\n    \n#     vali['action'] = (predictor.predict(batch) > i)*1\n#     u = utility_scoring(vali)\n#     print(u)\n#     if u > best_u:\n#         best_u = u\n#         best_i = i","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import janestreet\n# env = janestreet.make_env() # initialize the environment\n# iter_test = env.iter_test() # an iterator which loops over the test set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# threshold=best_i","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# best_i","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for (test_df, pred_df) in tqdm(iter_test):\n#     if test_df['weight'].item() > 0:\n#         # inference with treelite\n#         batch = treelite_runtime.Batch.from_npy2d(test_df.loc[:, features].values)\n#         pred_df.action = (predictor.predict(batch) > threshold).astype('int')\n#     else:\n#         pred_df.action = 0\n#     env.predict(pred_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}