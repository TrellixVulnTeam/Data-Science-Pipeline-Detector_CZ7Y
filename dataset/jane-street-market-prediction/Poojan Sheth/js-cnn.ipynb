{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\ntraining=False\ntuning=False\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"if tuning or training:\n    train_data =  pd.read_csv('/kaggle/input/jane-street-market-prediction/train.csv')\n    #train_data = train_data.apply(lambda x: np.where(x.isnull(), x.dropna().sample(len(x), replace=True), x))\n    train_data.fillna(train_data.mean(),inplace=True)\n    \n    metadata = pd.read_csv('/kaggle/input/jane-street-market-prediction/features.csv')\n    metadata.drop(['feature'],axis=1,inplace=True)\n\n    def replace_bool(tf):\n        if tf:\n            return 1\n        else:\n            return 0\n\n    metadata = metadata.applymap(replace_bool)\n\n    metadata_norm = metadata/metadata.sum()\n    metadata_norm = metadata_norm.applymap(np.sqrt)\n    features_transform = metadata_norm.values    \n    \n    \n    start_date=86\n    \n    feature_columns = [col for col in train_data.columns.values if 'feature' in col]\n    X_m = np.matmul(train_data[train_data['date']>=start_date][feature_columns].values,features_transform)\n    \n    \n    \n    \n    corr=abs(train_data[feature_columns].corr())\n    \n    ordered_feature_columns=[feature_columns.pop(0)]\n    \n    for col in feature_columns:\n        corr_max = corr[col][ordered_feature_columns].idxmax()\n        corr_max_idx = ordered_feature_columns.index(corr_max)\n        ordered_feature_columns.insert(corr_max_idx+1,col)\n    \n    f_mean = train_data[ordered_feature_columns].mean()\n    f_mean.to_csv('f_mean.csv',index=False)\n    import pickle\n    with open(\"ordered_columns.txt\", 'wb') as f:\n        pickle.dump((ordered_feature_columns), f)\n        \n    resp_cols = ['resp_1', 'resp_2', 'resp_3', 'resp', 'resp_4']\n    \n    train_data = train_data[train_data['date']>=start_date]\n    X = np.concatenate((train_data[ordered_feature_columns].values,X_m),axis=1)\n    y = np.stack([(train_data[c] > 0).astype('int') for c in resp_cols]).T \n    \n    \n    \n    max_date = train_data['date'].max()\n    split_date = int(0.7*(max_date-start_date))\n    train_test_split = train_data['date'][train_data['date']==split_date].index[0]\n    del train_data\n    \n    X_train=X[:train_test_split,:]\n    X_test=X[train_test_split:,:]\n    y_train=y[:train_test_split,:]\n    y_test=y[train_test_split:,:]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# corr_new=train_data[ordered_feature_columns].corr()\n# import matplotlib.pyplot as plt\n# f = plt.figure(figsize=(19, 15))\n# plt.matshow(corr_new, fignum=f.number)\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\ntf.random.set_seed(42)\nimport tensorflow.keras.layers as layers\nfrom tensorflow.keras import losses\nfrom tensorflow.keras.callbacks import Callback, ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.optimizers import Adam\nimport kerastuner as kt\nSEED=42\n\n\ndef create_model(hp, num_columns, num_labels):\n\n    inp = tf.keras.layers.Input(shape = (num_columns, 1))\n    #x = encoder(inp)\n    #x = tf.keras.layers.Concatenate()([x,inp])\n    x = tf.keras.layers.BatchNormalization()(inp)\n    x = tf.keras.layers.Conv1D(filters=8,\n                      kernel_size=hp.Int('kernel_size',5,10,step=5),\n                      strides=1,\n                      activation='relu')(x)\n    x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)\n    x = tf.keras.layers.Flatten()(x)\n    for i in range(hp.Int('num_layers',12,16)): \n        x = tf.keras.layers.Dense(hp.Int(f'num_units_{i}',32,64))(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n        x = tf.keras.layers.Dropout(hp.Float(f'dropout_{i}',0,0.5))(x)    \n\n    x = tf.keras.layers.Dense(num_labels)(x)\n    out = tf.keras.layers.Activation('sigmoid')(x)\n\n    model = tf.keras.models.Model(inputs = inp, outputs = out)\n    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = hp.Float('lr',0.00001,0.1,default=0.001)),\n                  loss = tf.keras.losses.BinaryCrossentropy(label_smoothing = 0.01), \n                  metrics = tf.keras.metrics.AUC(name = 'AUC'), \n                 )\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if tuning:   \n    model_fn = lambda hp: create_model(hp,X_train.shape[-1],y_train.shape[-1])\n\n    tuner = kt.tuners.bayesian.BayesianOptimization(\n            hypermodel=model_fn,\n            objective= kt.Objective('val_AUC', direction='max'),\n            num_initial_points=4,\n            max_trials=20)\n\n    tuner.search(X_train,y_train,batch_size=4096,epochs=20, validation_data = (X_test, y_test), callbacks=[EarlyStopping('val_AUC', mode='max',patience=5)])\n    hp  = tuner.get_best_hyperparameters(1)[0]\n    pd.to_pickle(hp,'best_hp_cnn_day_86_metadata_deep.pkl')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if training:\n    hp = pd.read_pickle('best_hp_cnn_day_86_metadata_deep.pkl')\n    model_fn = lambda hp: create_model(hp,X_train.shape[-1],y_train.shape[-1])\n    model = model_fn(hp)\n    model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=100,batch_size=4096,\n              callbacks=[EarlyStopping('val_AUC',mode='max',patience=10,restore_best_weights=True)])\n    model.save_weights('JS_CNN_day_86_metadata_deep.hdf5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if not training or tuning:\n    \n    model_fn = lambda hp: create_model(hp,130,5)\n\n    hp = pd.read_pickle('/kaggle/input/jscnn/best_hp_cnn_day_86.pkl')\n    model = model_fn(hp)\n    model.load_weights('/kaggle/input/jscnn/JS_CNN_day_86.hdf5') \n        \n    samples_mean = pd.read_csv('/kaggle/input/jscnn/f_mean.csv')\n    features_transform = np.load('/kaggle/input/jscnn/features_transform _130.npy')\n      \n    #import time\n    import pickle\n    import janestreet\n    env = janestreet.make_env() # initialize the environment\n    iter_test = env.iter_test() # an iterator which loops over the test set\n#     time_of_day =-1\n#     prev_date=0\n\n    for (test_df, sample_prediction_df) in iter_test:\n        weight = test_df.weight.iloc[0]\n        with open(\"/kaggle/input/jscnn/ordered_columns.txt\", 'rb') as f:\n            ordered_cols = pickle.load(f)\n        test_df = test_df[ordered_cols]\n        X_test = test_df.values[0] \n        \n        \n        if weight==0:\n            sample_prediction_df.action  = 0\n        else:\n            #X_test = X_test[1:-1]\n\n            for index, x in np.ndenumerate(X_test):\n                idx=index[0]\n                if np.isnan(x):\n                    X_test[idx] = samples_mean.iloc[idx]\n\n            X_test=X_test.reshape((1,-1))\n            X_metadata = np.matmul(X_test,features_transform)\n            X = np.concatenate((X_test,X_metadata),axis=1)\n\n            prediction = np.median(model(X, training=False).numpy()[0])\n            #end=time.time()\n\n            sample_prediction_df.action  = int(round(prediction))        \n        env.predict(sample_prediction_df)\n\n        #print(end-start)\n        #break","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}