{"cells":[{"metadata":{},"cell_type":"markdown","source":"In many notebooks, models predict actions corresponding to ['resp_1', 'resp_2', 'resp_3', 'resp_4', 'resp'] and take mean/median of them. In order to improve the utility score, we consider linear combination instead of taking mean/median of them, and optimize the weights using the CMA-ES.\n\nThe Covariance Matrix Adaptation Evolution Strategy (CMA-ES) is a stochastic method for black-box continuous optimization, where the gradient of the objective function cannot be accessed such as the utility score in this competition. The CMA-ES defines a Gaussian distribution on the search space and iteratively updates the parameters of the distribution (the mean vector and the covariance matrix) to improve the objective function value of the samples generated from the distribution. \n\nâ†“ Illustration of CMA-ES optimizing a simple two-dimensional function."},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://upload.wikimedia.org/wikipedia/commons/d/d8/Concept_of_directional_optimization_in_CMA-ES_algorithm.png\" width=\"500px\">\n\nhttps://en.wikipedia.org/wiki/CMA-ES"},{"metadata":{"trusted":true},"cell_type":"code","source":"from typing import List, Tuple\nimport numpy as np\nimport pandas as pd\nimport gc\nimport os\nimport warnings\nfrom dataclasses import dataclass\nfrom tqdm.notebook import tqdm\nfrom random import choices\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport random\nimport pickle\nimport itertools\nfrom cmaes import CMA\nfrom tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Concatenate, Lambda, Activation\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.layers.experimental.preprocessing import Normalization\nimport tensorflow_addons as tfa\nimport tensorflow as tf\nfrom tensorflow import keras","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def seed_everything(seed: int):\n    np.random.seed(seed)\n    random.seed(seed)\n    tf.random.set_seed(seed)\n    \n\n@dataclass\nclass Config:\n    n_fold: int\n    dropouts: List[float]\n    n_units: List[int]\n    lr: float\n    ls: float\n    wd: float\n    patience: int\n    batch_size: int\n    epochs: int\n    seed: int\n    train: bool","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Configuration\nSwith 'train=True' when you train models."},{"metadata":{"trusted":true},"cell_type":"code","source":"config = Config(\n    n_fold=4,\n    dropouts=[0.1, 0.5, 0.5],\n    n_units=[256, 256],\n    lr=1e-3,\n    ls=1e-2,\n    wd=1e-5,\n    patience=10,\n    batch_size=4096,\n    epochs=300,\n    seed=36,\n    train=False\n)\nwarnings.filterwarnings('ignore')\nsns.set()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"class DateKFold:\n    __i: int = -1\n        \n    def __init__(self, data: pd.DataFrame, n_fold: int):\n        self.data = data\n        self.n_fold = n_fold\n        self.uni_date = np.unique(data['date'].values)\n        self.date_block = np.array_split(self.uni_date, n_fold)\n        self.block_idxs = [(np.delete(np.arange(n_fold), i), i) for i in range(n_fold)]\n    \n    def __len__(self):\n        return self.n_fold\n    \n    def __iter__(self):\n        self.__i = -1\n        return self\n    \n    def __next__(self):\n        self.__i += 1\n        if self.__i < 0 or self.n_fold <= self.__i:\n            raise StopIteration()\n        return self.split()\n    \n    def __split_date(self) -> Tuple[List[int], List[int]]:\n        return np.hstack([self.date_block[j] for j in self.block_idxs[self.__i][0]]).tolist(), self.date_block[self.block_idxs[self.__i][1]].tolist()\n    \n    def split(self):\n        tr_date, va_date = self.__split_date()\n        return self.data.query(f'date in {tr_date}'), self.data.query(f'date in {va_date}')\n    \n    def plot(self):\n        fig, ax = plt.subplots(1, 1, figsize=(15, 4))\n        for i in range(self.n_fold):\n            self.__i = i\n            tr_date, va_date = self.__split_date()\n            if i == 0:\n                ax.scatter(tr_date, [i] * len(tr_date), label='train', color='blue')\n                ax.scatter(va_date, [i] * len(va_date), label='valid', color='red')\n            else:\n                ax.scatter(tr_date, [i] * len(tr_date), color='blue')\n                ax.scatter(va_date, [i] * len(va_date), color='red')\n        ax.set_title('K-fold by date')\n        ax.set_ylabel('fold')\n        ax.set_xlabel('date')\n        ax.set_yticks(list(range(self.n_fold)))\n        ax.legend()\n        plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"class UtilityScoreCallback(keras.callbacks.Callback):\n    def __init__(self, df: pd.DataFrame, features: List[str]):\n        super().__init__()\n        self.__weight = df['weight'].values\n        self.__resp = df['resp'].values\n        self.__date = df['date'].values\n        self.__X = df[features].values\n        \n    def on_train_begin(self, logs={}):\n        self.scores = list()\n        self.__best = -np.inf\n    \n    def on_epoch_end(self, epoch, logs={}):\n        preds = self.model(self.__X, training=False).numpy()\n        us = self.bincount(preds=preds.mean(axis=1))\n        self.scores.append(us)\n        print(f\"\\nEpoch {epoch + 1}: utility score = {us}\")\n    \n    def bincount(self, preds: np.ndarray) -> float:\n        \"\"\"\n        preds: np.ndarray predictions for 'action' (resp > 0)\n        \"\"\"\n        action = np.where(preds > 0.5, 1, 0)\n        count_i = len(np.unique(self.__date))\n        Pi = np.bincount(self.__date, self.__weight * self.__resp * action)\n        t = np.sum(Pi) / np.sqrt(np.sum(Pi ** 2)) * np.sqrt(250 / count_i)\n        u = np.clip(t, 0, 6) * np.sum(Pi)\n        u = 0 if np.isnan(u) else u\n        return u\n    \n    def get_scores(self) -> List[float]:\n        return self.scores\n    \n\ndef create_model(input_dim: int, output_dim: int, n_units: List[int], dropouts: List[float], lr: float, ls: float, wd: float):\n    i = Input(input_dim)\n    x = BatchNormalization()(i)\n    x = Dropout(dropouts[0])(x)\n    \n    for unit, dropout in zip(n_units, dropouts[1:]):\n        x = Dense(unit)(x)\n        x = BatchNormalization()(x)\n        x = Activation(tf.keras.activations.swish)(x)\n        x = Dropout(dropout)(x)\n    \n    x = Dense(output_dim, activation='sigmoid')(x)\n    \n    model = Model(inputs=i, outputs=x)\n    \n    model.compile(\n        optimizer=tfa.optimizers.RectifiedAdam(learning_rate=lr, weight_decay=wd),\n        loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=ls),\n        metrics=tf.keras.metrics.AUC(name=\"AUC\"),\n    )\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ntrain = pd.read_feather('../input/janestreetmarketprediction/train.feather')\ntrain = train.query('date > 86 & weight != 0').reset_index(drop=True)\ntrain.fillna(train.mean(), inplace=True)\ntrain['rtn'] = train['weight'] * train['resp']\n\nfeatures = [c for c in train.columns if 'feature' in c]\nresp_cols = [c for c in train.columns if 'resp' in c]\n\nfor i, c in enumerate(resp_cols):\n    train[f'action_{i}'] = (train[c] > 0).astype('int8')\n\naction_cols = [c for c in train.columns if 'action' in c]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cross Validation\nIn this notebook, I used a simple K-Fold (K=4)."},{"metadata":{"trusted":true},"cell_type":"code","source":"dkf = DateKFold(data=train, n_fold=config.n_fold)\ndkf.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"if config.train:\n    for fold, (tr_df, va_df) in enumerate(dkf):\n        print(f'fold {fold}')\n\n        utility_score = UtilityScoreCallback(df=va_df, features=features)\n        model = create_model(\n            input_dim=len(features), \n            output_dim=len(action_cols), \n            n_units=config.n_units,\n            dropouts=config.dropouts,\n            lr=config.lr,\n            ls=config.ls,\n            wd=config.wd,\n        )\n\n        model.fit(\n            tr_df[features].values,\n            tr_df[action_cols].values,\n            validation_data=(va_df[features].values, va_df[action_cols].values),\n            epochs=config.epochs,\n            batch_size=config.batch_size,\n            verbose=1,\n            callbacks=[\n                utility_score,\n                EarlyStopping(monitor='val_loss', min_delta=0, patience=config.patience, verbose=1, mode='min'),\n                ReduceLROnPlateau(monitor='val_loss', foctor=0.2, patience=int(config.patience * 0.5), min_lr=1e-5, verbose=1),\n            ]\n        )\n\n        model.save(f'./model-f{fold}.h5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Load Models\nThe utility scores of each validation data is like this."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def utility_score(action: np.ndarray, df: pd.DataFrame) -> float:\n    date, rtn = df['date'].values, df['rtn'].values\n    count_i = len(np.unique(date))\n    Pi = np.bincount(date, rtn * action)\n    t = np.sum(Pi) / np.sqrt(np.sum(Pi ** 2)) * np.sqrt(250 / count_i)\n    u = np.clip(t, 0, 6) * np.sum(Pi)\n    u = 0 if np.isnan(u) else u\n    return u","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"models = [keras.models.load_model(f'../input/janestreetmarketprediction/model-f{fold}.h5') for fold in range(config.n_fold)]\n\npreds_list = list()\nfor fold, (_, va_df) in enumerate(dkf):\n    preds = models[fold](va_df[features].values, training=False).numpy()\n    preds_list.append(preds)\n    print(f'fold {fold + 1}/{config.n_fold}: utility score = {utility_score(action=(np.mean(preds, axis=1) > 0.5).astype(int), df=va_df)}')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"class LinearCombination:\n    def __init__(self, w_init: np.ndarray):\n        self.w = w_init\n\n    def __call__(self, X: np.ndarray, step: bool = True) -> np.ndarray:\n        \"\"\"\n        :param X: np.ndarray shape=(n_data, n_dim)\n        :return np.ndarray shape=(n_data, )\n        \"\"\"\n        y = np.dot(X, self.w)\n        if step:\n            y = np.where(y > 0.5, 1, 0)\n        return y\n\n\ndef split_tr_val_fold(preds_list: List[np.ndarray], fold: int, dkf: DateKFold) -> Tuple[pd.DataFrame, np.ndarray]:\n    tr_set = list()\n    va_set = None\n    for i, ((_, va_df), preds) in enumerate(zip(dkf, preds_list)):\n        if fold == i:\n            va_set = (va_df, preds)\n        else:\n            tr_set.append((va_df, preds))\n    return tr_set, va_set\n\n\ndef train_fold_i(dkf: DateKFold, preds_list: List[np.ndarray], fold: int, n_iters: int = 50, sigma=1.) -> LinearCombination:\n    tr_set, va_set = split_tr_val_fold(preds_list=preds_list, fold=fold, dkf=dkf)\n\n    ordinary = utility_score(action=(np.mean(va_set[1], axis=1) > 0.5).astype(int), df=va_set[0])\n\n    lc = LinearCombination(w_init=np.array([0.2] * 5))\n    lc = optimize_utility_score(\n        train_set=tr_set, \n        valid_set=va_set,\n        lc=lc, \n        step=True, \n        n_iters=n_iters, \n        sigma=sigma\n    )\n    opt = utility_score(action=lc(X=va_set[1], step=True), df=va_set[0])\n    \n    print('### valid')\n    print(f'{ordinary} â‡¨ {opt}')\n    return lc\n\n\ndef optimize_utility_score(train_set: List[Tuple[pd.DataFrame, np.ndarray]], valid_set: Tuple[pd.DataFrame, np.ndarray],\n                           lc: LinearCombination, step: bool, patience: int = 5, n_iters: int = 50, sigma: float = 1.):\n    \n    optimizer = CMA(mean=lc.w, sigma=sigma, seed=2020)\n    init_value = np.sum([utility_score(action=lc(X=X, step=step), df=df) for df, X in train_set])\n\n    best = np.inf\n    val_best = -np.inf\n    wait = 0\n    best_x = None\n    __stop = False\n    for generation in tqdm(range(n_iters)):\n        solutions = list()\n        for _ in range(optimizer.population_size):\n\n            x = optimizer.ask()\n            lc.w = x\n            value = -np.sum([utility_score(action=lc(X=X, step=step), df=df) for df, X in train_set])\n            solutions.append((x, value))\n\n            if best > value:\n                best = value\n                val_values = utility_score(action=lc(X=valid_set[1], step=step), df=valid_set[0]) # warning: using the original utility score\n                print(f'train best={-best}, valid value={val_values}')\n                if val_best < val_values:\n                    best_x = x\n                    val_best = val_values\n                    wait = 0\n                else:\n                    wait += 1\n                    if wait > patience:\n                        print(f'Early Stopping at {generation}: score={val_best}')\n                        __stop = True\n        \n        optimizer.tell(solutions)\n        if __stop:\n              break\n    \n    print(\"\\n### train\")\n    print(f'f0={init_value} â‡¨ fbest={-best}')\n    print(f'best_x={best_x}')\n    \n    lc.w = best_x\n    return lc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Optimize the weights using the CMA-ES\nInitial values of the mean vector is set to [0.2, 0.2, 0.2, 0.2, 0.2], which is equivalent to taking mean of predictions.\nThe Scale of distribution ('sigma') and the number of iterations is also set to 0.1 and 50.\n\nThe algorithm is terminated when it reaches 50 iterations or the utility score for the validation data does not improve for 5 iterations."},{"metadata":{"trusted":true},"cell_type":"code","source":"lcs = [train_fold_i(dkf=dkf, preds_list=preds_list, fold=i, n_iters=50, sigma=0.1) for i in range(config.n_fold)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### the weights"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(10, 6))\nfor i, c in enumerate(resp_cols):\n    ax.plot([lc.w[i] for lc in lcs], label=f'action ({c})')\nax.set_xticks(list(range(config.n_fold)))\nax.set_xlabel('fold')\nax.set_ylabel('weight')\nax.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We can see the utility scores improve on every fold!!!"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"for fold, (_, va_df) in enumerate(dkf):\n    action_mean = utility_score(action=(np.mean(preds_list[fold], axis=1) > 0.5).astype(int), df=va_df)\n    action_lc = utility_score(action=lcs[fold](X=preds_list[fold], step=True), df=va_df)\n    print(f'fold {fold + 1}/{config.n_fold}: utility score = {action_mean} â‡¨ {action_lc}')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}