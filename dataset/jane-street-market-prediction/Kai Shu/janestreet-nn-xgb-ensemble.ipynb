{"cells":[{"metadata":{},"cell_type":"markdown","source":"Thought ensembling is not possible in this competiton due to the limited inference time?\n\nYes, it is! (But at most 2x shallow NN + 1x GBDT, in my experiment)\n\nHere I demonstrate it by simply ensembling the following public notebooks of mine:\n\nGBDT:\n\n- [[JaneStreet] Faster Inference by XGB with Treelite](https://www.kaggle.com/code1110/janestreet-faster-inference-by-xgb-with-treelite)\n\nNN (one of the following):\n\n- [[janestreet] 1DCNN for Feature Extraction (infer)](https://www.kaggle.com/code1110/janestreet-1dcnn-for-feature-extraction-infer)\n- [[janestreet] ResNet with AutoEncoder (infer)](https://www.kaggle.com/code1110/janestreet-resnet-with-autoencoder-infer)\n- [Jane Street with Keras NN overfit](https://www.kaggle.com/code1110/jane-street-with-keras-nn-overfit)\n\n\nThe key is to use [Treelite](https://treelite.readthedocs.io/en/latest/) for a GBDT model to accelerate the inference speed."},{"metadata":{},"cell_type":"markdown","source":"# Treelite"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip --quiet install ../input/treelite/treelite-0.93-py3-none-manylinux2010_x86_64.whl","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip --quiet install ../input/treelite/treelite_runtime-0.93-py3-none-manylinux2010_x86_64.whl","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n# treelite\nimport treelite\nimport treelite_runtime \n\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nimport pathlib\nfrom tqdm import tqdm\nfrom random import choices\n\nimport operator\nimport xgboost as xgb\n        \n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tf setup\nprint(\"Tensorflow version \" + tf.__version__)\nAUTO = tf.data.experimental.AUTOTUNE\n\nMIXED_PRECISION = False\nXLA_ACCELERATE = True\n\nif MIXED_PRECISION:\n    from tensorflow.keras.mixed_precision import experimental as mixed_precision\n    if tpu: policy = tf.keras.mixed_precision.experimental.Policy('mixed_bfloat16')\n    else: policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n    mixed_precision.set_policy(policy)\n    print('Mixed precision enabled')\n\nif XLA_ACCELERATE:\n    tf.config.optimizer.set_jit(True)\n    print('Accelerated Linear Algebra enabled')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED = 2021\nSTART_DATE = 86\nFOLDS = 5\nNN_NAME = 'mlp' # 1dcnn, resnet, mlp","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading the training data"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n# train = pd.read_csv('../input/jane-street-market-prediction/train.csv')\ntrain = pd.read_feather('../input/janestreet-save-as-feather/train.feather')\ntrain = train.query(f'date >= {START_DATE}').reset_index(drop = True) \ntrain = train.astype({c: np.float32 for c in train.select_dtypes(include='float64').columns}) #limit memory use\ntrain.fillna(train.mean(),inplace=True)\ntrain = train.query('weight > 0').reset_index(drop = True)\n#train['action'] = (train['resp'] > 0).astype('int')\ntrain['action'] =  (  (train['resp_1'] > 0 ) & (train['resp_2'] > 0 ) & (train['resp_3'] > 0 ) & (train['resp_4'] > 0 ) &  (train['resp'] > 0 )   ).astype('int')\nfeatures = [c for c in train.columns if 'feature' in c]\n\nresp_cols = ['resp_1', 'resp_2', 'resp_3', 'resp', 'resp_4']\n\nX = train[features].values\ny = np.stack([(train[c] > 0).astype('int') for c in resp_cols]).T #Multitarget\n\nf_mean = np.mean(train[features[1:]].values,axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.shape[-1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load XGB with Treelite"},{"metadata":{"trusted":true},"cell_type":"code","source":"predictor = treelite_runtime.Predictor('../input/janestreet-faster-inference-by-xgb-with-treelite/mymodel.so', verbose=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load NN"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_autoencoder(input_dim,output_dim,noise=0.05):\n    i = tf.keras.layers.Input(input_dim)\n    encoded = tf.keras.layers.BatchNormalization()(i)\n    encoded = tf.keras.layers.GaussianNoise(noise)(encoded)\n    encoded = tf.keras.layers.Dense(64,activation='relu')(encoded)\n    decoded = tf.keras.layers.Dropout(0.2)(encoded)\n    decoded = tf.keras.layers.Dense(input_dim,name='decoded')(decoded)\n    x = tf.keras.layers.Dense(32,activation='relu')(decoded)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout(0.2)(x)\n    x = tf.keras.layers.Dense(output_dim,activation='sigmoid',name='label_output')(x)\n    \n    encoder = tf.keras.models.Model(inputs=i,outputs=encoded)\n    autoencoder = tf.keras.models.Model(inputs=i,outputs=[decoded,x])\n    \n    autoencoder.compile(optimizer=tf.keras.optimizers.Adam(0.001), \n                        loss={'decoded':'mse','label_output':'binary_crossentropy'})\n    return autoencoder, encoder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"autoencoder, encoder = create_autoencoder(X.shape[-1],y.shape[-1],noise=0.1)\nencoder.load_weights('../input/janestreet-1dcnn-for-feature-extraction-train/encoder.hdf5') \nencoder.trainable = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_resnet(n_features, n_labels, encoder, label_smoothing = 0.0005):    \n    input_1 = tf.keras.layers.Input(shape = (n_features,))\n    input_2 = encoder(input_1)\n\n    head_1 = tf.keras.Sequential([\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(512, activation=\"elu\"), \n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.4),\n        tf.keras.layers.Dense(256, activation = \"elu\")\n        ],name='Head1') \n\n    input_3 = head_1(input_1)\n    input_3_concat = tf.keras.layers.Concatenate()([input_2, input_3])\n\n    head_2 = tf.keras.Sequential([\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(512, \"relu\"),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.3),\n        tf.keras.layers.Dense(512, \"elu\"),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.3),\n        tf.keras.layers.Dense(256, \"relu\"),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.3),\n        tf.keras.layers.Dense(256, \"elu\")\n        ],name='Head2')\n\n    input_4 = head_2(input_3_concat)\n    input_4_avg = tf.keras.layers.Average()([input_3, input_4]) \n\n    head_3 = tf.keras.Sequential([\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(256, kernel_initializer='lecun_normal', activation='selu'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.2),\n        tf.keras.layers.Dense(128, kernel_initializer='lecun_normal', activation='selu'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.2),\n        tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1), name='l2_norm'),\n        tf.keras.layers.Dense(n_labels, activation=\"sigmoid\")\n        ],name='Head3')\n\n    output = head_3(input_4_avg)\n\n    model = tf.keras.models.Model(inputs = [input_1, ], outputs = output)\n    opt = tfa.optimizers.RectifiedAdam(learning_rate=1e-03)\n    opt = tfa.optimizers.SWA(opt)\n    model.compile(optimizer=opt, \n                  loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=label_smoothing), \n                  metrics=['AUC'])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nif NN_NAME == 'resnet':\n    models = []\n\n    for fold in range(FOLDS):\n        tf.keras.backend.clear_session()\n        model = create_resnet(X.shape[-1], y.shape[-1], encoder)\n        model.load_weights(pathlib.Path(f'../input/janestreet-resnet-with-autoencoder-train/model_{SEED}_{fold}.hdf5'))\n        models.append(model)\n        \n    models = [models[-1]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nif NN_NAME == 'mlp':\n    model = tf.keras.models.load_model('../input/jane-street-with-keras-nn-overfit/model.h5')\n    models = [model]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"f = np.median\nth = 0.500\n\nimport janestreet\nenv = janestreet.make_env()\nfor (test_df, pred_df) in tqdm(env.iter_test()):\n    if test_df['weight'].item() > 0:\n        x_tt = test_df.loc[:, features].values\n        \n        # GBDT inference with treelite\n        batch = treelite_runtime.Batch.from_npy2d(x_tt)\n        xgb_pred = predictor.predict(batch)\n    \n        # NN inference\n        if np.isnan(x_tt[:, 1:].sum()):\n            x_tt[:, 1:] = np.nan_to_num(x_tt[:, 1:]) + np.isnan(x_tt[:, 1:]) * f_mean\n        \n        pred = np.mean([model(x_tt, training = False).numpy() for model in models],axis=0)\n        pred = f(pred)\n        \n        # ensemble\n        pred_df.action = np.where(0.85*pred + 0.15*xgb_pred >= th, 1, 0).astype(int)\n    else:\n        pred_df.action = 0\n    env.predict(pred_df)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}