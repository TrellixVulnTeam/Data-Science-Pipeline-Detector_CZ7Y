{"cells":[{"metadata":{},"cell_type":"markdown","source":"**If you found this notebook helpful please upvote and comment, it really helps me out!**"},{"metadata":{},"cell_type":"markdown","source":"# **Importing Libraries**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\nfrom random import choices\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Activation\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.metrics import AUC\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nfrom sklearn.metrics import accuracy_score\n\nRS = 69420\ntf.random.set_seed(RS)\nnp.random.seed(RS)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Loading Data**"},{"metadata":{},"cell_type":"markdown","source":"**You can see we only use values after day 85, this is due to all the excellent exploration notebooks which have shown JaneSt to recalibrate models post day 85**\n\n**I am using a HDF5 version of the training data because it loads nearly 10x faster than csv's, let me know if you would like access**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_hdf(\"../input/janesthdf/train.hdf5\")\ntrain = train.query('date > 85').reset_index(drop = True) \ntrain = train[train['weight'] != 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['action'] = ((train['resp'].values) > 0).astype(int)\nfeatures = [c for c in train.columns if \"feature\" in c]\nf_mean = np.mean(train[features[1:]].values,axis=0)\nresp_cols = ['resp_1', 'resp_2', 'resp_3', 'resp', 'resp_4']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train.loc[:, train.columns.str.contains('feature')]\ny = train['action']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Train-Test-Validation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n# 60, 20, 20\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RS)\n\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=RS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.impute import SimpleImputer\nimpute = SimpleImputer(missing_values = np.nan)\n\nX_train = impute.fit_transform(X_train)\nX_test = impute.transform(X_test)\nX_val = impute.transform(X_val)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Building the   *W I D E*   boy**"},{"metadata":{},"cell_type":"markdown","source":"**Use callbacks because we hate overfitting**"},{"metadata":{"trusted":true},"cell_type":"code","source":"callback = EarlyStopping(monitor='val_loss', patience=3, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_mlp(num_columns, label_smoothing):\n    \n    model = Sequential()\n    model.add(Input(shape=(num_columns,)))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.3))\n    \n    model.add(Dense(1280))\n    model.add(BatchNormalization())\n    model.add(Activation(tf.keras.activations.swish))\n    model.add(Dropout(0.3))\n    \n    model.add(Dense(1280))\n    model.add(BatchNormalization())\n    model.add(Activation(tf.keras.activations.swish))\n    model.add(Dropout(0.3))\n    \n    model.add(Dense(1280))\n    model.add(BatchNormalization())\n    model.add(Activation(tf.keras.activations.swish))\n    model.add(Dropout(0.3))\n\n    model.add(Dense(1))\n    model.add(Activation(\"sigmoid\"))\n    \n    model.compile(\n                    optimizer = 'adam',\n                    loss = BinaryCrossentropy(label_smoothing = label_smoothing),\n                    metrics= [AUC(), 'acc']\n                 )\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_smoothing = 1e-2\n\n# clf = create_mlp(len(features), label_smoothing)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# clf.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Train dat bih**"},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 2560","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*More Robust application of this model as it utilizes early stopping upon the validation set we split out*"},{"metadata":{"trusted":true},"cell_type":"code","source":"# history = clf.fit(X_train, y_train, validation_data = (X_val, y_val), callbacks = callback, epochs=1000, batch_size = batch_size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* *Tbh I dont really know why I chose to train a mega-wide network, largely its for the memes and so I can say   W I D E B O I*  \n* I would appreciate any academic rigour all you wonderful kagglers could provide!"},{"metadata":{"trusted":true},"cell_type":"code","source":"# history = pd.DataFrame(clf.history.history)\n# history[['val_loss', 'loss']].plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Results**"},{"metadata":{},"cell_type":"markdown","source":"Remember that ***Accuracy is not everything*** this is a markets related competition, the markets are so dynamic there is no need for extreme accuracy, rather we want to focus on the gerneralisability of the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# y_pred = clf.predict_proba(X_test)\n# y_valpred = clf.predict(X_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# If you have any recommendations to tidy this code up please comment below\n\n# th = 0.5\n# y_preds = []\n# y_valpreds = []\n\n# for i in y_pred:\n#     if i < 0.5:\n#         y_preds.append(0)\n#     else:\n#         y_preds.append(1)\n    \n# for i in y_valpred:\n#     if i < 0.5:\n#         y_valpreds.append(0)\n#     else:\n#         y_valpreds.append(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# mean_acc = np.mean((accuracy_score(y_test, y_preds),\n#                     accuracy_score(y_val, y_valpreds)))\n\n# print(\"The Testing Accuracy is: {}\".format(accuracy_score(y_test, y_preds)))\n# print(\"The Validation Accuracy is: {}\".format(accuracy_score(y_val, y_valpreds)))\n# print(\"The Mean Accuracy is: {}\".format(mean_acc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# clf.save('wide_boi.hdf5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Submission**"},{"metadata":{},"cell_type":"markdown","source":"**To prepare this notebook for submission first actually run and train the model (all lines above) then save and download the model. Comment out all the code above when ready to submit and then after this you will be able to re-upload the model to kaggle and load it in again below for submission**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\n\nclf = tf.keras.models.load_model(\"../input/wide-boi-3/wide_boi.hdf5\")\n\nf_mean = 0.2674099\nresp_cols = ['resp_1', 'resp_2', 'resp_3', 'resp', 'resp_4']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Submitting Predictions**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Submission Code Thanks to: https://www.kaggle.com/tarlannazarov/own-jane-street-with-keras-nn\n\nmodels = []\n\nmodels.append(clf)\n\nth = 0.5000\n\n\nf = np.median\nmodels = models[-3:]\nimport janestreet\nenv = janestreet.make_env()\n\nfor (test_df, pred_df) in tqdm(env.iter_test()):\n    if test_df['weight'].item() > 0:\n        x_tt = test_df.loc[:, features].values\n        if np.isnan(x_tt[:, 1:].sum()):\n            x_tt[:, 1:] = np.nan_to_num(x_tt[:, 1:]) + np.isnan(x_tt[:, 1:]) * f_mean\n        pred = np.mean([model(x_tt, training = False).numpy() for model in models],axis=0)\n        pred = f(pred)\n        pred_df.action = np.where(pred >= th, 1, 0).astype(int)\n    else:\n        pred_df.action = 0\n    env.predict(pred_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**If you found this notebook helpful please upvote and comment, it really helps me out!**"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}