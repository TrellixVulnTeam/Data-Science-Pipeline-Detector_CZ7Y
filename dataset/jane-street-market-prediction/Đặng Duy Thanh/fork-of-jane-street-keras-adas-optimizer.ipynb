{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport datatable as dt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load Data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = dt.fread('/kaggle/input/jane-street-market-prediction/train.csv').to_pandas()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.query('date > 85').reset_index(drop = True) \ntrain = train[train['weight'] != 0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"resp_cols = ['resp_1', 'resp_2', 'resp_3', 'resp', 'resp_4']\ny = np.stack([(train[c] > 0).astype('int') for c in resp_cols]).T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.fillna(X.mean(),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.isnull().values.any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_memory_usage(df):   \n    start_memory = df.memory_usage().sum() / 1024**2\n    print(f\"Memory usage of dataframe is {start_memory} MB\")\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != 'object':\n            c_min = df[col].min()\n            c_max = df[col].max()\n            \n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n                    \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    pass\n        else:\n            df[col] = df[col].astype('category')\n    \n    end_memory = df.memory_usage().sum() / 1024**2\n    print(f\"Memory usage of dataframe after reduction {end_memory} MB\")\n    print(f\"Reduced by {100 * (start_memory - end_memory) / start_memory} % \")\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Reducing Memory\")\n# train = reduce_memory_usage(train)\nX = reduce_memory_usage(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_features = [c for c in X.columns if 'feature' in c]\nX = X[selected_features]\nf_mean = np.mean(X[selected_features[1:]].values,axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_shape0 = X.shape[0]\nX_shape1 = X.shape[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_val, Y_train, Y_val = train_test_split(X, y, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Concatenate, Lambda, GaussianNoise, Activation\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.layers.experimental.preprocessing import Normalization\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://github.com/YanaiEliyahu/AdasOptimizer/blob/master/adasopt.py\n# MIT License\n#\n# Copyright (c) 2020 YanaiEliyahu\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n\"\"\"Implementation of Adas.\"\"\"\n\nfrom tensorflow.keras.optimizers import Optimizer\nfrom tensorflow.keras import backend as K\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import state_ops\nfrom tensorflow.python.training import moving_averages\n\nimport tensorflow as tf\n\nclass AdasOptimizer(Optimizer):\n    \"\"\"\n    Introduction:\n        For the mathematical part see https://github.com/YanaiEliyahu/AdasOptimizer,\n        the `Theory` section contains the major innovation,\n        and then `How ADAS works` contains more low level details that are still somewhat related to the theory.\n    Compatibility:\n        Tested under tensorflow versions 1.5.4 and 2.3.1.\n    Arguments:\n        lr: float > 0. Initial learning rate that is per feature/input (e.g. dense layer with N inputs and M outputs, will have N learning rates).\n        lr2: float >= 0.  lr's Initial learning rate. (just ~1-2 per layer, additonal one because of bias)\n        lr3: float >= 0. lr2's fixed learning rate. (global)\n        beta_1: 0 < float < 1. Preferably close to 1. Second moments decay factor to update lr and lr2 weights.\n        beta_2: 0 < float < 1. Preferably close to 1. 1/(1 - beta_2) steps back in time that `lr`s will be optimized for, larger dataset might require more nines.\n        beta_3: 0 < float < 1. Preferably close to 1. Same as beta_2, but for `lr2`s.\n        epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n    \"\"\"\n    def __init__(self,\n            lr = 0.001, lr2 = .005, lr3 = .0005,\n            beta_1 = 0.999, beta_2 = 0.999, beta_3 = 0.9999,\n            epsilon = None, **kwargs):\n        super(AdasOptimizer, self).__init__('Adas',**kwargs)\n        # TODO: use self._set_hyper and self._get_hyper\n        self._lr = lr\n        self._lr2 = lr2\n        self._lr3 = lr3\n        self._beta_1 = beta_1\n        self._beta_2 = beta_2\n        self._beta_3 = beta_3\n        if epsilon is None:\n            epsilon = K.epsilon()\n        self._epsilon = epsilon\n        self._tf1 = tf.__version__.split(\".\")[0] == '1'\n        if not self._tf1:\n            self._varn = None\n            self._is_create_slots = None\n            self._curr_var = None\n\n    def _assign(self,x,y):\n        if self._tf1:\n            result = K.update(x,y)\n        else:\n            result = state_ops.assign(x,y,use_locking=self._use_locking)\n        self._updates.append(result)\n        return result\n\n    def _add(self,x,y):\n        if self._tf1:\n            result = K.update_add(x,y)\n        else:\n            result = state_ops.assign_add(x,y,use_locking=self._use_locking)\n        self._updates.append(result)\n        return result\n\n    def _moving_average(self,var,value,momentum):\n        if self._tf1:\n            return self._assign(var,var * momentum + value * (1 - momentum))\n        result = K.moving_average_update(var,value,momentum)\n        self._updates.append(result)\n        return result\n\n    # TODO: fix variables' names being too convoluted in _derivatives_normalizer and _get_updates_universal_impl\n    def _derivatives_normalizer(self,derivative,beta):\n        if self._tf1:\n            self._iterations = self._make_variable()\n            self._add(self._iterations,1)\n        t = K.cast(self._iterations if self._tf1 else self.iterations, K.floatx()) + 1\n        lr_t = K.sqrt(1. - K.pow(self._beta_1, t))\n        m = self._make_variable(0,K.int_shape(derivative),K.dtype(derivative))\n        old_moments = self._make_variable(0,K.int_shape(derivative),K.dtype(derivative))\n        old_moments2 = self._make_variable(0,K.int_shape(derivative),K.dtype(derivative))\n        res = self._assign(old_moments2,old_moments)\n        v_t = self._moving_average(m,K.square(derivative),self._beta_1)\n        with tf.control_dependencies([v_t]):\n            np_t = derivative * lr_t / (K.sqrt(v_t) + self._epsilon)\n        with tf.control_dependencies([np_t]):\n            m_t = self._moving_average(old_moments,np_t,beta)\n        return (res,np_t)\n\n    def _make_variable(self,value = 0,shape = (),dtype = K.floatx()):\n        if self._tf1:\n            return K.variable(K.constant(value,shape=shape, dtype=dtype))\n        self._varn += 1\n        name = 'unnamed_variable' + str(self._varn)\n        if self._is_create_slots:\n            return self.add_slot(self._curr_var,name,initializer = K.constant(value,shape=shape, dtype=dtype))\n        else:\n            return self.get_slot(self._curr_var,name)\n\n    def _get_updates_universal_impl(self, grad, param):\n        self._updates = []\n        lr = self._make_variable(value = self._lr,shape=K.int_shape(param)[:-1], dtype=K.dtype(param))\n        moment, deriv = self._derivatives_normalizer(grad,self._beta_3)\n        param_t = self._add(param, - K.expand_dims(lr,len(K.int_shape(param)) - 1) * deriv)\n        with tf.control_dependencies([moment]):\n            lr_deriv = math_ops.reduce_sum(moment * grad,len(K.int_shape(param)) - 1)\n        master_lr = self._make_variable(self._lr2)\n        m2,d2 = self._derivatives_normalizer(lr_deriv,self._beta_2)\n        lr_t = self._add(lr,master_lr * lr * d2)\n        with tf.control_dependencies([m2]):\n            master_lr_deriv2 = math_ops.reduce_sum(m2 * lr_deriv)\n        m3,d3 = self._derivatives_normalizer(master_lr_deriv2,0.)\n        with tf.control_dependencies([m3]):\n            self._add(master_lr,self._lr3 * master_lr * d3)\n        return self._updates\n\n    def _get_updates_universal(self, param, grad = None, is_create_slots = False):\n        self._curr_var = param\n        self._is_create_slots = is_create_slots\n        self._varn = 0\n        return self._get_updates_universal_impl(grad if grad is not None else K.constant(0,shape=param.shape,dtype=K.dtype(param)),param)\n\n    def get_updates(self, loss, params):\n        return sum([self._get_updates_universal_impl(grad,var) for (grad,var) in zip(self.get_gradients(loss, params),params)],[])\n\n    def _create_slots(self, var_list):\n        for var in var_list:\n            self._get_updates_universal(var,is_create_slots = True)\n\n    def _resource_apply_dense(self, grad, var):\n        return control_flow_ops.group(*self._get_updates_universal(var,grad))\n\n    def get_config(self):\n        config = {\n            'lr': float(self._lr),\n            'lr2': float(self._lr2),\n            'lr3': float(self._lr3),\n            'beta_1': float(K.get_value(self._beta_1)),\n            'beta_2': float(K.get_value(self._beta_2)),\n            'beta_3': float(K.get_value(self._beta_3)),\n            'epsilon': self._epsilon\n        }\n        base_config = super(AdasOptimizer, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_mlp(\n    num_columns, num_labels, hidden_units, dropout_rates, label_smoothing, learning_rate\n):\n\n    inp = tf.keras.layers.Input(shape=(num_columns,))\n    x = tf.keras.layers.BatchNormalization()(inp)\n    x = tf.keras.layers.Dropout(dropout_rates[0])(x)\n    for i in range(len(hidden_units)):\n        x = tf.keras.layers.Dense(hidden_units[i])(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n        x = tf.keras.layers.Dropout(dropout_rates[i + 1])(x)\n\n    x = tf.keras.layers.Dense(num_labels)(x)\n    out = tf.keras.layers.Activation(\"sigmoid\")(x)\n\n    model = tf.keras.models.Model(inputs=inp, outputs=out)\n    model.compile(\n        optimizer=AdasOptimizer(lr=learning_rate),\n        loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=label_smoothing),\n        metrics=tf.keras.metrics.AUC(name=\"AUC\"),\n    )\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 5000\nepochs = 200\nhidden_units = [150, 150, 150]\ndropout_rates = [0.2, 0.2, 0.2, 0.2]\nlabel_smoothing = 1e-2\nlearning_rate = 1e-3\n\nmodel = create_mlp(\n    X_shape1, 5, hidden_units, dropout_rates, label_smoothing, learning_rate\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"es = EarlyStopping(monitor='val_accuracy', mode='auto', patience=10, verbose=1)\nmc = ModelCheckpoint('/kaggle/working/best_weight.pt', monitor='val_accuracy', mode='auto', verbose=1, save_weights_only=True)\nmodel.fit(\n    x=X_train,\n    y=Y_train,\n    epochs=epochs,\n    steps_per_epoch = X_shape0/batch_size,\n    validation_data = (X_val, Y_val),\n    validation_steps = X_shape0/batch_size*2,\n    callbacks=[es, mc]\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = create_mlp(\n    X_shape1, 5, hidden_units, dropout_rates, label_smoothing, learning_rate\n)\nmodel.load_weights('/kaggle/working/best_weight.pt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!rm -f '/kaggle/working/best_weight*'\n!rm -f '/kaggle/working/checkpoint'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = []\nmodels.append(model)\n\nth = 0.5000\nf = np.median\nmodels = models[-3:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\nimport janestreet\nenv = janestreet.make_env() # initialize the environment","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for (test_df, pred_df) in tqdm(env.iter_test()):\n    if test_df['weight'].item() > 0:\n        x_tt = test_df.loc[:, selected_features].values\n        if np.isnan(x_tt[:, 1:].sum()):\n            x_tt[:, 1:] = np.nan_to_num(x_tt[:, 1:]) + np.isnan(x_tt[:, 1:]) * f_mean\n        pred = np.mean([model(x_tt, training = False).numpy() for model in models],axis=0)\n        pred = f(pred)\n        pred_df.action = np.where(pred >= th, 1, 0).astype(int)\n    else:\n        pred_df.action = 0\n    env.predict(pred_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# References\n1 - https://www.kaggle.com/tarlannazarov/own-jane-street-with-keras-nn\n\n2 - https://github.com/YanaiEliyahu/AdasOptimizer/blob/master/adasopt.py"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}