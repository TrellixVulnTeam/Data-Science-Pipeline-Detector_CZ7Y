{"cells":[{"metadata":{},"cell_type":"markdown","source":"## TODO\n1. add features\n2. change targets\n3. model\n4. loss_fn\n5. tune the parameters"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## packages and settings"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nimport random\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nimport pandas as pd\nimport os\nimport copy\nimport sys\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nfrom torch.nn import CrossEntropyLoss, MSELoss\nfrom torch.nn.modules.loss import _WeightedLoss\nimport torch.nn.functional as F\nfrom sklearn.metrics import log_loss, roc_auc_score\n\nfrom sklearn.model_selection import GroupKFold\nfrom tqdm import tqdm\n\npd.set_option('display.max_columns', 100)\npd.set_option('display.max_rows', 100)\n\nseed = 123\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\n\nif not os.path.exists(\"results\"):\n    os.mkdir(\"results\")\n\nTRAINING = True\nread_path = '/kaggle/input/jane-street-market-prediction/train.csv'\nmodel_path = \"/kaggle/input/skeleton-with-pytorch/results/123/best_model\"\nsave_path = os.path.join(\"results\", str(seed))\n\ndevice = torch.device(\"cuda:0\")\nif not os.path.exists(save_path):\n    os.mkdir(save_path)\n    \n# train = pd.read_csv(read_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAINING = True","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## preprocess the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(read_path)\ntrain = train.query('date > 85').reset_index(drop = True) \nfeatures = [c for c in train.columns if 'feature' in c]\n\n# preprocess the features\nf_mean = train[features].mean()\ntrain = train.loc[train.weight > 0].reset_index(drop = True)\ntrain[features] = train[features].fillna(f_mean)\n\ntrain = train.astype(\"float32\")\ntrain['action'] = (train['resp'] > 0).astype('int')\ntrain['action1'] = (train['resp_1'] > 0).astype('int')\ntrain['action2'] = (train['resp_2'] > 0).astype('int')\ntrain['action3'] = (train['resp_3'] > 0).astype('int')\ntrain['action4'] = (train['resp_4'] > 0).astype('int')\n\n# targets = ['resp']\n# targets = ['action']\ntargets = ['action', 'action_1', 'action_2', 'action_3', 'action_4']\n# train[targets] = (train[targets]>0).astype('int')\n\n\ndef add_features(df, features):\n    new_features = copy.deepcopy(features)\n    \n    # todo\n    df[\"cross_1_2\"] = df[\"feature_1\"] / (df[\"feature_2\"] + 1e-5)\n    df[\"cross_41_42_43\"] = df[\"feature_41\"] + df[\"feature_42\"] + df[\"feature_43\"]\n    new_features.extend([\"cross_1_2\", \"cross_41_42_43\"])\n\n    return df, new_features\n\ntrain, train_features = add_features(train, features)\n\n\n# to do: update the mean online\n# f_mean = f_mean.values\n# np.save(os.path.join(save_path, 'f_mean.npy'), f_mean)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(read_path)\nvalid = train.loc[(train.date >= 450) & (train.date < 500)].reset_index(drop=True)\nvalid.fillna(train.mean(), inplace=True)\ndel train\nvalid['action'] = (valid['resp'] > 0).astype('int')\nvalid['action_1'] = (valid['resp_1'] > 0).astype('int')\nvalid['action_2'] = (valid['resp_2'] > 0).astype('int')\nvalid['action_3'] = (valid['resp_3'] > 0).astype('int')\nvalid['action_4'] = (valid['resp_4'] > 0).astype('int')\nvalid['cross_1_2'] = valid['feature_1'] / (valid['feature_2'] + 1e-5)\nvalid['cross_41_42_43'] = valid['feature_41'] + valid['feature_42'] + valid['feature_43']\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 8192\nclass MarketDataset:\n    def __init__(self, df):\n        self.features = df[train_features].values\n\n        self.label = df[targets].values.reshape(-1, len(targets))\n\n    def __len__(self):\n        return len(self.label)\n\n    def __getitem__(self, idx):\n        return {\n            'features': torch.tensor(self.features[idx], dtype=torch.float),\n            'label': torch.tensor(self.label[idx], dtype=torch.float)\n        }\n\nvalid_set = MarketDataset(valid)\nvalid_loader = DataLoader(valid_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n\ndef inference_fn1(model, dataloader, device):\n    model.eval()\n    preds = []\n\n    for data in dataloader:\n        features = data['features'].to(device)\n\n        with torch.no_grad():\n            outputs = model(features)\n\n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n\n    preds = np.concatenate(preds).reshape(-1, len(targets))\n\n    return preds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## the dataset and model(resnet)"},{"metadata":{"trusted":true},"cell_type":"code","source":"class MyDataset:\n    def __init__(self, df, features, targets):\n        self.features = df[features].values\n        \n        # preprocess the labels\n        # self.labels = (df[targets] > 0).astype('int').values\n        self.labels = df[targets].values\n        self.weights = df['weight'].values\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        feat_ = torch.tensor(self.features[idx], dtype=torch.float)\n        label_ = torch.tensor(self.labels[idx], dtype=torch.float)\n        weight_ = torch.tensor(self.weights[idx], dtype=torch.float)\n        \n        return feat_, label_, weight_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model1(nn.Module):\n    def __init__(self, features, targets):\n        super(Model, self).__init__()\n        self.batch_norm0 = nn.BatchNorm1d(len(features))\n        self.dropout0 = nn.Dropout(0.10143786981358652)\n\n        hidden_size = 256\n        self.dense1 = nn.Linear(len(features), 384)\n        self.batch_norm1 = nn.BatchNorm1d(384)\n        self.dropout1 = nn.Dropout(0.19720339053599725)\n\n        self.dense2 = nn.Linear(384, 896)\n        self.batch_norm2 = nn.BatchNorm1d(896)\n        self.dropout2 = nn.Dropout(0.2703017847244654)\n\n        self.dense3 = nn.Linear(896, 896)\n        self.batch_norm3 = nn.BatchNorm1d(896)\n        self.dropout3 = nn.Dropout(0.23148340929571917)\n\n        self.dense4 = nn.Linear(896, 394)\n        self.batch_norm4 = nn.BatchNorm1d(394)\n        self.dropout4 = nn.Dropout(0.2357768967777311)\n\n        self.dense5 = nn.Linear(394, len(targets))\n\n        self.Relu = nn.ReLU(inplace=True)\n        self.PReLU = nn.PReLU()\n        self.LeakyReLU = nn.LeakyReLU(negative_slope=0.01, inplace=True)\n        # self.GeLU = nn.GELU()\n        self.RReLU = nn.RReLU()\n\n    def forward(self, x):\n        x = self.batch_norm0(x)\n        x = self.dropout0(x)\n\n        x = self.dense1(x)\n        x = self.batch_norm1(x)\n        x = x * torch.sigmoid(x)\n        x = self.dropout1(x)\n\n        x = self.dense2(x)\n        x = self.batch_norm2(x)\n        x = x * torch.sigmoid(x)\n        x = self.dropout2(x)\n        \n        x = self.dense3(x)\n        x = self.batch_norm3(x)\n        x = x * torch.sigmoid(x)\n        x = self.dropout3(x)\n        \n        x = self.dense4(x)\n        x = self.batch_norm4(x)\n        x = x * torch.sigmoid(x)\n        x = self.dropout4(x)\n\n        x = self.dense5(x)\n\n        return x\n    \n    \n    \nclass SmoothBCEwLogits(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n        assert 0 <= smoothing < 1\n        with torch.no_grad():\n            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n        return targets\n\n    def forward(self, inputs, targets, weights=None):\n        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n            self.smoothing)\n        loss = F.binary_cross_entropy_with_logits(inputs, targets, weights)\n\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss\n\n    \nclass EarlyStopping:\n    def __init__(self, patience=7, mode=\"max\", delta=0.):\n        self.patience = patience\n        self.counter = 0\n        self.mode = mode\n        self.best_score = None\n        self.early_stop = False\n        self.delta = delta\n        if self.mode == \"min\":\n            self.val_score = np.Inf\n        else:\n            self.val_score = -np.Inf\n\n    def __call__(self, epoch_score, model, model_path):\n\n        if self.mode == \"min\":\n            score = -1.0 * epoch_score\n        else:\n            score = np.copy(epoch_score)\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(epoch_score, model, model_path)\n        elif score < self.best_score: #  + self.delta\n            self.counter += 1\n            print('EarlyStopping counter: {} out of {}'.format(self.counter, self.patience))\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            # ema.apply_shadow()\n            self.save_checkpoint(epoch_score, model, model_path)\n            # ema.restore()\n            self.counter = 0\n\n    def save_checkpoint(self, epoch_score, model, model_path):\n        if epoch_score not in [-np.inf, np.inf, -np.nan, np.nan]:\n            print('Validation score improved. Saving model!')\n            torch.save(model.state_dict(), model_path)\n        self.val_score = epoch_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.batch_norm0 = nn.BatchNorm1d(len(train_features))\n        self.dropout0 = nn.Dropout(0.2)\n\n        dropout_rate = 0.2\n        hidden_size = 256\n        self.dense1 = nn.Linear(len(train_features), hidden_size)\n        self.batch_norm1 = nn.BatchNorm1d(hidden_size)\n        self.dropout1 = nn.Dropout(dropout_rate)\n\n        self.dense2 = nn.Linear(hidden_size+len(train_features), hidden_size)\n        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n        self.dropout2 = nn.Dropout(dropout_rate)\n\n        self.dense3 = nn.Linear(hidden_size+hidden_size, hidden_size)\n        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n        self.dropout3 = nn.Dropout(dropout_rate)\n\n        self.dense4 = nn.Linear(hidden_size+hidden_size, hidden_size)\n        self.batch_norm4 = nn.BatchNorm1d(hidden_size)\n        self.dropout4 = nn.Dropout(dropout_rate)\n\n        self.dense5 = nn.Linear(hidden_size+hidden_size, len(targets))\n\n        self.Relu = nn.ReLU(inplace=True)\n        self.PReLU = nn.PReLU()\n        self.LeakyReLU = nn.LeakyReLU(negative_slope=0.01, inplace=True)\n        # self.GeLU = nn.GELU()\n        self.RReLU = nn.RReLU()\n\n    def forward(self, x):\n        x = self.batch_norm0(x)\n        x = self.dropout0(x)\n\n        x1 = self.dense1(x)\n        x1 = self.batch_norm1(x1)\n        # x = F.relu(x)\n        # x = self.PReLU(x)\n        x1 = self.LeakyReLU(x1)\n        x1 = self.dropout1(x1)\n\n        x = torch.cat([x, x1], 1)\n\n        x2 = self.dense2(x)\n        x2 = self.batch_norm2(x2)\n        # x = F.relu(x)\n        # x = self.PReLU(x)\n        x2 = self.LeakyReLU(x2)\n        x2 = self.dropout2(x2)\n\n        x = torch.cat([x1, x2], 1)\n\n        x3 = self.dense3(x)\n        x3 = self.batch_norm3(x3)\n        # x = F.relu(x)\n        # x = self.PReLU(x)\n        x3 = self.LeakyReLU(x3)\n        x3 = self.dropout3(x3)\n\n        x = torch.cat([x2, x3], 1)\n\n        x4 = self.dense4(x)\n        x4 = self.batch_norm4(x4)\n        # x = F.relu(x)\n        # x = self.PReLU(x)\n        x4 = self.LeakyReLU(x4)\n        x4 = self.dropout4(x4)\n\n        x = torch.cat([x3, x4], 1)\n\n        x = self.dense5(x)\n\n        return x","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train"},{"metadata":{},"cell_type":"markdown","source":"### utility"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n    model.train()\n    final_loss = 0\n\n    for feature, label, weight in dataloader:\n        feature = feature.to(device)\n        label = label.to(device)\n        weight = weight.to(device)\n        optimizer.zero_grad()\n        outputs = model(feature)\n        \n        \n        loss = loss_fn(outputs.reshape(-1, 1), label.reshape(-1, 1)) #  weight.reshape(-1,1)\n        \n        loss.backward()\n        optimizer.step()\n        if scheduler:\n            scheduler.step()\n\n        final_loss += loss.item()\n\n    final_loss /= len(dataloader)\n    return final_loss\n        \n    \ndef inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n    labels = []\n\n    for feature, label, weight in dataloader:\n        feature = feature.to(device)\n        # label = label.to(device)\n        with torch.no_grad():\n            outputs = model(feature)\n            preds.append(outputs.cpu().numpy())\n            labels.append(label.cpu().numpy())\n    \n    preds = np.concatenate(preds, axis=0)\n    labels = np.concatenate(labels, axis=0)\n\n    return preds, labels\n    \n    \ndef utility_score(date, weight, resp, action):    \n    values = weight * resp * action\n    to_bincount = {}\n\n    for d, v in zip(date, values):\n        to_bincount.setdefault(d, []).append(v)\n\n    Pi = []\n    for val in to_bincount.values():\n        Pi.append(np.sum(val))\n    Pi = np.array(Pi)\n    count_i = len(np.unique(date))\n    t = np.sum(Pi) / np.sqrt(np.sum(Pi ** 2)) * np.sqrt(250 / count_i)\n    u = np.clip(t, 0, 6) * np.sum(Pi)\n    \n    return u\n\n    \ndef loss_mse(preds, targets):\n    \n    return ((preds-targets)**2).mean()\n\n\ndef loss_ce(preds, targets, weight=None):\n    \n    return F.binary_cross_entropy_with_logits(preds, targets, weight)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test_preds, true_test_labels = inference_fn(model, test_loader, device)\n# test_preds = np.where(test_preds >= 0.5, 1, 0).astype(int)\n# u_score = utility_score(date=test.date.values.reshape(-1),\n#                         weight=test.weight.values.reshape(-1),\n#                         resp=test.resp.values.reshape(-1),\n#                         action=action1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## valid"},{"metadata":{"trusted":true},"cell_type":"code","source":"NFOLDS = 5\nmodels = []\n#tmp = np.zeros(len(feat_cols))\nfor _fold in range(NFOLDS):\n    torch.cuda.empty_cache()\n    model = Model()\n    model.to(device)\n    model_weights = f\"/kaggle/input/mlp012003weights/online_model{_fold}.pth\"\n    model.load_state_dict(torch.load(model_weights))\n    model.eval()\n    models.append(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_pred = np.zeros((len(valid), len(targets)))\nfor model in models:\n    valid_pred += inference_fn1(model, valid_loader, device) / len(models)\n\nvalid_pred = np.median(valid_pred, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_threshold, best_u_score = 0.5, 0\n\nthres = float(5000) / 10000\nslice_valid_pred = valid_pred.copy()\nslice_valid_pred = np.where(slice_valid_pred >= thres, 1, 0).astype(int)\nvalid_u_score = utility_score(date=valid.date.values, weight=valid.weight.values,\n                                       resp=valid.resp.values, action=slice_valid_pred)\nprint(f'thresold={thres:.4f}, valid_u_score={valid_u_score:.4f}')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f_mean = np.load(f'/kaggle/input/mlp012003weights/f_mean_online.npy')\n\nif True:\n    import janestreet\n    env = janestreet.make_env()\n    env_iter = env.iter_test()\n\n    for (test_df, pred_df) in tqdm(env_iter):\n        if test_df['weight'].item() > 0:\n            x_tt = test_df.loc[:, features].values\n            if np.isnan(x_tt.sum()):\n                x_tt = np.nan_to_num(x_tt) + np.isnan(x_tt) * f_mean\n                \n            cross_1_2 = x_tt[:, 1] / (x_tt[:, 2] + 1e-5)\n            cross_41_42_43 = x_tt[:, 41] + x_tt[:, 42] + x_tt[:, 43]\n            feature_inp = np.concatenate((\n                x_tt,\n                np.array(cross_41_42_43).reshape(x_tt.shape[0], 1),\n                np.array(cross_1_2).reshape(x_tt.shape[0], 1),\n            ), axis=1)\n\n            # torch_pred\n            torch_pred = np.zeros((1, len(targets)))\n            for model in models:\n                torch_pred += model(torch.tensor(feature_inp, dtype=torch.float).to(device)).sigmoid().detach().cpu().numpy() / NFOLDS\n            pred = np.median(torch_pred)\n            \n            \n            pred_df.action = np.where(pred >= 0.5, 1, 0).astype(int)\n        else:\n            pred_df.action = 0\n        env.predict(pred_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}