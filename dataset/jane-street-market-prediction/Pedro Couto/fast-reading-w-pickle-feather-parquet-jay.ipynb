{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Train.csv transforming!\n\nAs we can see, the fille train.csv of the competition has 138 features and 2390490 instances.  \nJust to read it with pandas.read_csv() it takes about 1min 43s and read it everytime we start the notebook would take too much time in the long run.  \n\n**As [Vopani](https://www.kaggle.com/rohanrao) did in the RIIID competition and helped me a lot, I'm convertin this data to formats that are faster to read and therefore better to read!**\n\n#### You may find the train data in all this formats in this dataset: https://www.kaggle.com/pedrocouto39/jane-street-market-train-data-best-formats"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# datatable installation with internet\n!pip install datatable==0.11.0 > /dev/null\n\nimport numpy as np \nimport pandas as pd \nimport datatable as dt\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntrain = pd.read_csv(\"../input/jane-street-market-prediction/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Converting to multiple formatas\nThe formats that will be created are:\n1. *Pickle* - great for object serialization and though it has a slower performance when comparing with other formats, it may work for our porpuse.\n2. *Feather* - is a fast, lightweight, and easy-to-use binary file format for storing data frames.\n3. *Parquet* - compared to a traditional approach where data is stored in row-oriented approach, parquet is more efficient in terms of storage and performance.\n4. *Jay* - also a binary format, that means it is fast, lightweight, and easy-to-use binary file format for storing data frames."},{"metadata":{"trusted":true},"cell_type":"code","source":"# writing dataset as pickle\ntrain.to_pickle(\"jane_street_train.pkl.gzip\")\n\n# writing dataset as feather\ntrain.to_feather(\"jane_street_train.feather\")\n\n# writing dataset as parquet\ntrain.to_parquet(\"jane_street_train.parquet\")\n\n# writing dataset as jay\ndt.Frame(train).to_jay(\"jane_street_train.jay\")\n\n# writing dataset as hdf5\ntrain.to_hdf(\"jane_street_train.h5\", \"jane_street_train\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Reading and timing\nNow let's read each file and time them to see how long it will take for each one."},{"metadata":{},"cell_type":"markdown","source":"### 2.1 Pickle"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntrain_pickle = pd.read_pickle(\"./jane_street_train.pkl.gzip\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Pickle 4.45s and CSV 1min 43s."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_pickle.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.2 Feather"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntrain_feather = pd.read_feather(\"./jane_street_train.feather\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Feather 4.35s and CSV 1min 43s."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_feather.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.3 Parquet"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntrain_parquet = pd.read_parquet(\"./jane_street_train.parquet\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"hdf5 8.75s and CSV 1min 35s."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_parquet.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.4 Jay"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntrain_jay = dt.fread(\"./jane_street_train.jay\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"hdf5 641Âµs and CSV 1min 35s."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_jay.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Conclusion\nThough the original train.csv didn't take an extreme long time be read, we can improve it a lot by using other formats as above and that is why I will share this data in dataset to make it availible to anyone in the competition. \nLet's save time focus on where it is more import: data exploration and building models!\n\nJust in order to compare the reading time:\n- CSV: 1min 42s\n- Pickle: 4.45s\n- Feather: 4.35s\n- Parquet: 8.31s\n- Jay: 8.12ms or 0.0812s (blazing fast!)"},{"metadata":{},"cell_type":"markdown","source":"Happy Competition!!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}