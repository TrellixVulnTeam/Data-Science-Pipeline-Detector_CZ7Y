{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":false,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# DEFAULT IMPORTS\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# CUSTOM IMPORTS\n\n# Faster load packages\n\n!pip install ../input/python-datatable/datatable-0.11.0-cp37-cp37m-manylinux2010_x86_64.whl > /dev/null 2>&1\nimport datatable as dt\n\n# Garbage collection\n\nimport gc\n\n# For creating deep copies\n\nimport copy\n\n# Charts\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n!pip install seaborn --upgrade\nimport seaborn as sns\n\n# Statistics\n\nfrom scipy.stats import pearsonr, spearmanr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":true},"cell_type":"code","source":"# CONTANTS AND VARIABLES\n\n# Seaborn settings\n\nsns.set(style=\"darkgrid\")\n\n# Colors for graphs\n\ncolor_resp = [\"#000080\", \"#0000EE\", \"#5190ED\", \"#88ACE0\", \"#B0C4DE\"]\n\n# Correlations p-values\n\ndef pearsonr_pval(x,y):\n    return pearsonr(x,y)[1]\n\ndef spearmanr_pval(x,y):\n    return spearmanr(x,y)[1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load Data\n\nIn order to make the process faster and use less memory I read the data with the datatable and convert it with the datatable inspired by [Jane Street: EDA of day 0 and feature importance](https://www.kaggle.com/carlmcbrideellis/jane-street-eda-of-day-0-and-feature-importance#train_csv) and [Tutorial on reading large datasets](https://www.kaggle.com/rohanrao/tutorial-on-reading-large-datasets#Method:-Datatable). Datatable object is temporary. Therefore I delete it as soon as it will not be useful anymore to save the memory.\n\n"},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":true},"cell_type":"code","source":"# train.csv\n\ndt_full_train = dt.fread('../input/jane-street-market-prediction/train.csv')\ndf_full_train = dt_full_train.to_pandas()\ndel dt_full_train\n\n# features.csv\n\ndt_full_features = dt.fread('../input/jane-street-market-prediction/features.csv')\ndf_full_features = dt_full_features.to_pandas()\ndel dt_full_features\n\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`df_full_train` - DataFrame with all the data from train.csv.\n\n`df_full_features` - DataFrame with all the data from features.csv"},{"metadata":{},"cell_type":"markdown","source":"To save even more memory while not losing the precision of data significantly, it is fine to convert float64 datatypes to float32 in DataFrame from df_full_train. It is inspired by [One-liner to Halve Your Memory Usage](https://www.kaggle.com/jorijnsmit/one-liner-to-halve-your-memory-usage)."},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":true},"cell_type":"code","source":"df_full_train = df_full_train.astype({c: np.float32 for c in df_full_train.select_dtypes(include='float64').columns})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# General dataset overview"},{"metadata":{},"cell_type":"markdown","source":"## Meaning of the values in the dataset"},{"metadata":{},"cell_type":"markdown","source":"This is what we know for now about the data:\n\n* `date` - the day of the trade.\n* `feature_{0...129}` - real stock market data features. The examples of such features could be volume in various time horizons, volatility in various time horizons, indicators like RSI. The features are anonymyzed.\n* `resp` - represents a return.\n* `resp_1`, `resp_2`, `resp_3`, `resp_4` - it is a supplemental data of returns over specific but unspecified time horizons.\n* `tag_{0...28}` - components/concepts used in future derivation.\n* `ts_id` - time ordering.\n* `weight` - the importance of the trade assigned by Jane Street which probably is some kind of ratio of transaction cost or in other words the capital invested in the trade.\n\n`train.csv` consists of numerical data of resps, weights and features for specific trading opportunities. `features.csv` consists of information which tags are connected with the features (e.g. if feature_4 is the volatility of stock A in last 30 days and if tag_8 is volatility, tag_12 is 30 days, tag_16 is 5 days, then the intersection of the records mentioned above will be respectively True, True, False).\n\nMore information on the meaning of the values:\n* [Data card in the competition page](https://www.kaggle.com/c/jane-street-market-prediction/data)\n* [Discussion with janestreet-jjia](https://www.kaggle.com/c/jane-street-market-prediction/discussion/198965)\n* [Metric de-anonymization by miguel perez](https://www.kaggle.com/c/jane-street-market-prediction/discussion/199107)\n* [Question about resps answered by Will Cukierski](https://www.kaggle.com/c/jane-street-market-prediction/discussion/199478)"},{"metadata":{},"cell_type":"markdown","source":"## train.csv"},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":true},"cell_type":"code","source":"df_full_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":true},"cell_type":"code","source":"df_full_train.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":true},"cell_type":"code","source":"df_full_train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":true},"cell_type":"code","source":"df_full_train.columns.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":true},"cell_type":"code","source":"df_full_train.groupby(['date']).count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## features.csv"},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":true},"cell_type":"code","source":"df_full_features.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":true},"cell_type":"code","source":"df_full_features[\"feature\"].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":true},"cell_type":"code","source":"df_full_features.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":true},"cell_type":"code","source":"df_full_features.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Date"},{"metadata":{},"cell_type":"markdown","source":"From outputs below you can see that date values range goes from 0 to 499. There are 500 dates in 2390491 rows. The dates do not occur the same number of times. Dates are montonically increasing. There are no NaN values in date column. Each value represents the day of trade."},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":true},"cell_type":"code","source":"df_full_train.groupby(['date']).size().values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":true},"cell_type":"code","source":"df_full_train[\"date\"].describe().apply(\"{0:.0f}\".format)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"df_full_train[\"date\"].is_monotonic","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":true},"cell_type":"code","source":"df_full_train[\"date\"].isna().values.any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":true},"cell_type":"code","source":"fig = plt.figure(constrained_layout=True, figsize=(12, 6))\nfig.add_subplot()\nfig.suptitle('Days and opportunities')\nsns.lineplot(data=df_full_train[\"date\"], ci=None);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"scrolled":false},"cell_type":"code","source":"del fig\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ts_id"},{"metadata":{},"cell_type":"markdown","source":"`ts_id` is an identifier for a time. It turns out that it is the same as indices. It means that the dataset is sorted by the time of the opportunitity."},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":true},"cell_type":"code","source":"df_full_train[\"ts_id\"].head(100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":true},"cell_type":"code","source":"df_full_train[\"ts_id\"].tail(100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":true},"cell_type":"code","source":"df_full_train[\"ts_id\"].describe().apply(\"{0:.1f}\".format)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":true},"cell_type":"code","source":"df_full_train[\"ts_id\"].is_monotonic","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":true},"cell_type":"code","source":"df_full_train[\"ts_id\"].isna().values.any()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# resp"},{"metadata":{},"cell_type":"markdown","source":"`Resp`s are returns. Their values seems to be in percentages, where for example (for the first `resp`) 0.006270 is 0,627%."},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":true},"cell_type":"code","source":"df_train_resps = df_full_train[[\"resp\", \"resp_1\", \"resp_2\", \"resp_3\", \"resp_4\"]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":true},"cell_type":"code","source":"df_train_resps.head(100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":true},"cell_type":"code","source":"df_train_resps.tail(100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What we see that medians and means for all the `resp`s is above 0. Minimum values in absolute terms are higher than maximum values."},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":true},"cell_type":"code","source":"df_train_resps.describe().applymap(\"{0:.5f}\".format)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Time series for all of the `resp`s look random, but they do not need to be random. Charts confirm the values of standard deviations from the table."},{"metadata":{"trusted":true,"_kg_hide-input":true,"scrolled":false},"cell_type":"code","source":"fig = plt.figure(constrained_layout=True, figsize=(12, 15))\nspec = fig.add_gridspec(4, 2)\nax1 = fig.add_subplot(spec[:2, 0:])\nax1.set_title('resp')\nax1.set(ylim=(-1, 1))\nax2 = fig.add_subplot(spec[2, 0])\nax2.set_title('resp_1')\nax2.set(ylim=(-1, 1))\nax3 = fig.add_subplot(spec[2, 1])\nax3.set_title('resp_2')\nax3.set(ylim=(-1, 1))\nax4 = fig.add_subplot(spec[3, 0])\nax4.set_title('resp_3')\nax4.set(ylim=(-1, 1))\nax5 = fig.add_subplot(spec[3, 1])\nax5.set_title('resp_4')\nax5.set(ylim=(-1, 1))\nfig.suptitle('resp values')\nsns.lineplot(data=df_train_resps[\"resp\"], color=color_resp[0], ax=ax1, ci=None);\nsns.lineplot(data=df_train_resps[\"resp_1\"], color=color_resp[1], ax=ax2, ci=None);\nsns.lineplot(data=df_train_resps[\"resp_2\"], color=color_resp[2], ax=ax3, ci=None);\nsns.lineplot(data=df_train_resps[\"resp_3\"], color=color_resp[3], ax=ax4, ci=None);\nsns.lineplot(data=df_train_resps[\"resp_4\"], color=color_resp[4], ax=ax5, ci=None);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"scrolled":false},"cell_type":"code","source":"del fig, spec, ax1, ax2, ax3, ax4, ax5\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is clearly visible that no `resp` follows normal distribution. They are steeper and have long tails."},{"metadata":{"trusted":true,"_kg_hide-input":true,"scrolled":false},"cell_type":"code","source":"def mark_points(ax):\n    data_x1, data_y1 = ax.get_lines()[0].get_data()\n    for i, val in enumerate([0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95]):\n        yi = val \n        xi = np.interp(yi, data_y1, data_x1)\n        x_margin = 0.01 if (i % 2 == 0) else -0.0075\n        y_margin = 0\n        annotate_alignment = \"left\" if (i % 2 == 0) else \"right\"\n        ax.plot([xi], [yi], 'o', color='r')\n        ax.annotate(\"x={0:.2f},y={1}\".format(xi, yi), (xi + x_margin, yi + y_margin), fontsize=8, fontweight=\"bold\", color='r', horizontalalignment=annotate_alignment)\n\ndef draw_dist(figure, specific, name, quantity):\n    for i in range(quantity):\n        resp = name\n        if (i != 0):\n            resp = resp + \"_{}\".format(i)\n        axi = figure.add_subplot(specific[i, 0:])\n        axi.set_title(resp)\n        axi.set(ylim=(0, 0.012), xlim=(-0.6, 0.6))\n        axj = axi.twinx()\n        axj.set(ylim=(0, 1.2), xlim=(-0.6, 0.6))\n        axi.set_zorder(1)\n        axj.set_zorder(1)\n        axj.yaxis.grid(False)\n        sns.histplot(data=df_train_resps[resp], color=color_resp[i], stat=\"probability\", ax=axi)\n        ax2 = sns.ecdfplot(data=df_train_resps[resp], color=\"#FF9999\", ax=axj, linewidth=3)\n        mark_points(ax2)\n\nfig1 = plt.figure(constrained_layout=True, figsize=(12, 20))\nspec1 = fig1.add_gridspec(5, 1)\ndraw_dist(fig1, spec1, \"resp\", 5)\nfig1.suptitle('resp values')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"scrolled":false},"cell_type":"code","source":"del mark_points, draw_dist, fig1, spec1\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The boxplots below confirm long tails by showing a large number of outliers. IQR (interquartile range, Q3 - Q1) is 1.5 which means that outliers lie below Q1 (the 1st quartile) and above Q3 (the 3rd quartile) by 1.5 * IQR."},{"metadata":{"trusted":true,"scrolled":true,"_kg_hide-input":true},"cell_type":"code","source":"fig2 = plt.figure(constrained_layout=True, figsize=(12, 20));\nax11 = sns.boxplot(data=df_train_resps);\nfig2.add_axes(ax11);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"scrolled":false},"cell_type":"code","source":"del fig2, ax11\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since `resp`s are returns in percentage a proper way to cumulate them is by multiplying the sum of value and 1 (products). In case of `resp`s there are so many records that the result passes the limit for `float64` (the result is inf). The value fits within numpy's `float128`. The chart below does not show any significant results apart from the chaos and very high values received. The table below the plots show that cumulative products are close to or above $10^{100}$."},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":true},"cell_type":"code","source":"ls_df_train_resps_cumprod = [df_train_resps[\"resp\"].map(lambda x: x + 1).cumprod(), df_train_resps[\"resp_1\"].map(lambda x: x + 1).cumprod(), df_train_resps[\"resp_2\"].map(lambda x: x + 1).cumprod(), df_train_resps[\"resp_3\"].map(lambda x: x + 1).cumprod(), df_train_resps[\"resp_4\"].map(lambda x: x + 1).cumprod()]\n\nfig3 = plt.figure(constrained_layout=True, figsize=(12, 21))\nspec3 = fig3.add_gridspec(6, 2)\nax21 = fig3.add_subplot(spec3[:2, 0:])\nax21.set_title('all')\nax22 = fig3.add_subplot(spec3[2:4, 0:])\nax22.set_title('resp')\nax23 = fig3.add_subplot(spec3[4, 0])\nax23.set_title('resp_1')\nax24 = fig3.add_subplot(spec3[4, 1])\nax24.set_title('resp_2')\nax25 = fig3.add_subplot(spec3[5, 0])\nax25.set_title('resp_3')\nax26 = fig3.add_subplot(spec3[5, 1])\nax26.set_title('resp_4')\nfig3.suptitle('resp cumulative product values')\nsns.lineplot(data=ls_df_train_resps_cumprod, palette=color_resp, ax=ax21, ci=None);\nsns.lineplot(data=ls_df_train_resps_cumprod[0], color=color_resp[0], ax=ax22, ci=None);\nsns.lineplot(data=ls_df_train_resps_cumprod[1], color=color_resp[1], ax=ax23, ci=None);\nsns.lineplot(data=ls_df_train_resps_cumprod[2], color=color_resp[2], ax=ax24, ci=None);\nsns.lineplot(data=ls_df_train_resps_cumprod[3], color=color_resp[3], ax=ax25, ci=None);\nsns.lineplot(data=ls_df_train_resps_cumprod[4], color=color_resp[4], ax=ax26, ci=None);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":true},"cell_type":"code","source":"pd.concat(ls_df_train_resps_cumprod, axis=1).describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":true},"cell_type":"code","source":"del ls_df_train_resps_cumprod, fig3, spec3, ax21, ax22, ax23, ax24, ax25, ax26\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The purpose of charts below is to show the magnitude and importance of long tails for `resp`s. Each charts presents the ratios between the products of two ranges. Every barplot compares the product of values in a tail and the rest of the records for below or above the median for different resps. The values of these ranges were obtained arbitrarily by trials and errors. 1 is the higher value in the pair and the second value is the ratio of the first one. They were calculated only for visualisation purposes. The green bars are for positive `resp` values and red bars for negative `resp` values.\n\nBecause of very high values passing `float64`, the calculations were made using natural logarithms.\n\n$ratio = \\frac{productrange_{1}}{productrange_{2}} \\Leftrightarrow \\ln ratio = \\ln productrange_{1} - \\ln productrange_{2} \\Leftrightarrow e^{\\ln ratio} = e^{\\ln productrange_{1} - \\ln productrange_{2}} \\Leftrightarrow e^{\\ln ratio} = \\frac{e^{\\ln productrange_{1}}}{e^{\\ln productrange_{2}}} \\Leftrightarrow ratio = \\frac{productrange_{1}}{productrange_{2}}$\n"},{"metadata":{"trusted":true,"_kg_hide-input":true,"scrolled":false},"cell_type":"code","source":"def ln_quant_values(series, quantiles_values):\n    # Getting sum of natural logarithms from certain ranges of pandas series\n    ln_values_list = []\n    for i, v in enumerate(quantiles_values):\n        if i < len(quantiles_values) - 1:\n            v1 = quantiles_values[i+1]\n            quant1 = series.quantile(q=v) if i != 0 else series.quantile(q=v) - 0.001 # For the first value include the minimum\n            quant2 = series.quantile(q=v1)\n            ln_sum = series.loc[(series > quant1) & (series <= quant2)].apply(lambda x: np.log(x + 1)).sum()\n            ln_values_list.append(ln_sum)\n        else:\n            break\n    return ln_values_list\n\ndef ln_normalized(ln_values_list):\n    # Normalizing ln values so that the one with the highest absolute value is/has exponent equal to 0 (and argument equal to 1) and\n    # the rest of logarithms has the argument which is a share of abs max argument\n    ln_normalized_list = []\n    for i, v in enumerate(ln_values_list):\n        ln_values_list[i] = abs(ln_values_list[i])\n    max_ln_value = max(ln_values_list)\n    ln_element = max_ln_value\n    for i in ln_values_list:\n        if i != max_ln_value:\n            ln_part_1 = i - ln_element\n            ln_normalized_list.append(ln_part_1)\n        else:\n            ln_normalized_list.append(np.log(1))\n    return ln_normalized_list\n\ndef is_positive(ln_values_list):\n    # Returns the list of whether values are positive (true) or negative (false)\n    pos_neg_bool_list = []\n    for i in ln_values_list:\n        if i >= 0:\n            pos_neg_bool_list.append(True)\n        else:\n            pos_neg_bool_list.append(False)\n    return pos_neg_bool_list\n\ndef conversion_to_values(ln_normalized_list):\n    # Getting (normalized) natural logarithm arguments by solving the exponential equations where the base is euler's number and exponent\n    # is natural logarithm\n    values_list = []\n    for i in ln_normalized_list:\n        values_list.append(np.exp(i))\n    return values_list\n\ndef get_resp_prod_normalized(series, quantiles_values):\n    # The function above combined returning values list and info which values were positive (true) and which negative (false)\n    ln_values_list = ln_quant_values(series, quantiles_values)\n    pos_neg_bool_list = is_positive(ln_values_list)\n    ln_normalized_list = ln_normalized(ln_values_list)\n    values_list = conversion_to_values(ln_normalized_list)\n    return {\"values_list\": values_list, \"pos_neg_bool_list\": pos_neg_bool_list}\n\ndef fig5_color_generator(pos_neg_bool_list):\n    # Make the list of colors for positive (green) and negatives (red)\n    fg5_color_pos = \"#228B22\"\n    fg5_color_neg = \"#DC143C\"\n    return [fg5_color_pos if i == True else fg5_color_neg for i in pos_neg_bool_list]\n\ndef single_tail_chart_generator(figure, specific_array, s, rng, resp_string, tail_string):\n    # Generate a single bar chart for products of certain ranges (specificially 2)\n    ax_n = figure.add_subplot(specific_array)\n    ax_n.set_title(\"{}: Ratio of quantiles products - {} tail\".format(resp_string, tail_string))\n    dict_n = get_resp_prod_normalized(s, rng)\n    df_n = pd.DataFrame(data={\"Ranges\": [\"<{} ; {})\".format(str(rng[0]), str(rng[1])), \"<{} ; {}>\".format(str(rng[1]), str(rng[2]))], \"Fraction of max\": dict_n[\"values_list\"]})\n    df_n.set_index(\"Ranges\", inplace=True)\n    color_list_n = fig5_color_generator(dict_n[\"pos_neg_bool_list\"])\n    sns.barplot(x=df_n.index, y=\"Fraction of max\", data=df_n, palette=color_list_n, ax=ax_n, ci=None)\n\ndef tails_comparison_chart(figure, specific, data):\n    # Builds entire figure with bar charts\n    for i, v in enumerate(data):\n        resp_name = \"resp\" + v[0]\n        single_tail_chart_generator(figure, specific[i, 0], df_train_resps[resp_name], v[1], resp_name, \"left\")\n        single_tail_chart_generator(figure, specific[i, 1], df_train_resps[resp_name], v[2], resp_name, \"right\")\n\n# Found values (not optimal, they are just for an outline)\ndata_fig5 = [\n    [\n        \"\",\n        [0, 0.0583175, 0.5],\n        [0.5, 0.937985, 1]\n    ],\n    [\n        \"_1\",\n        [0, 0.04987, 0.5],\n        [0.5, 0.948195, 1]\n    ],\n    [\n        \"_2\",\n        [0, 0.051, 0.5],\n        [0.5, 0.946225, 1]\n    ],\n    [\n        \"_3\",\n        [0, 0.0576575, 0.5],\n        [0.5, 0.937035, 1]\n    ],\n    [\n        \"_4\",\n        [0, 0.059975, 0.5],\n        [0.5, 0.9343625, 1]\n    ]\n]\n\nfig5 = plt.figure(constrained_layout=True, figsize=(12, 21))\nspec5 = fig5.add_gridspec(6, 2)\nfig5.suptitle('Tails products compared to tails of other ranges for resps')\ntails_comparison_chart(fig5, spec5, data_fig5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"scrolled":false},"cell_type":"code","source":"del tails_comparison_chart, single_tail_chart_generator, fig5_color_generator, get_resp_prod_normalized, conversion_to_values, is_positive, ln_normalized, ln_quant_values, data_fig5, fig5, spec5\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the chart below we can see `resp`s for cumulative sum which is just an aggregation of pure, unchanged `resp`s values. It shows that the values does not seem to have constant variation or in other words be equally distributed throughout the time for all the `resp`s. E.g. for `resp` the plot looks exponentially."},{"metadata":{"trusted":true,"_kg_hide-input":true,"scrolled":false},"cell_type":"code","source":"ls_df_train_resps_cumsum = [df_train_resps[\"resp\"].cumsum(), df_train_resps[\"resp_1\"].cumsum(), df_train_resps[\"resp_2\"].cumsum(), df_train_resps[\"resp_3\"].cumsum(), df_train_resps[\"resp_4\"].cumsum()]\n\nfig4 = plt.figure(constrained_layout=True, figsize=(12, 21))\nspec4 = fig4.add_gridspec(6, 2)\nax31 = fig4.add_subplot(spec4[:2, 0:])\nax31.set_title('all')\nax31.set(ylim=(-20, 1100))\nax32 = fig4.add_subplot(spec4[2:4, 0:])\nax32.set_title('resp')\nax32.set(ylim=(-20, 1100))\nax33 = fig4.add_subplot(spec4[4, 0])\nax33.set_title('resp_1')\nax33.set(ylim=(-20, 1100))\nax34 = fig4.add_subplot(spec4[4, 1])\nax34.set_title('resp_2')\nax34.set(ylim=(-20, 1100))\nax35 = fig4.add_subplot(spec4[5, 0])\nax35.set_title('resp_3')\nax35.set(ylim=(-20, 1100))\nax36 = fig4.add_subplot(spec4[5, 1])\nax36.set_title('resp_4')\nax36.set(ylim=(-20, 1100))\nfig4.suptitle('resp cumulative values')\nsns.lineplot(data=ls_df_train_resps_cumsum, palette=color_resp, ax=ax31, ci=None);\nsns.lineplot(data=ls_df_train_resps_cumsum[0], color=color_resp[0], ax=ax32, ci=None);\nsns.lineplot(data=ls_df_train_resps_cumsum[1], color=color_resp[1], ax=ax33, ci=None);\nsns.lineplot(data=ls_df_train_resps_cumsum[2], color=color_resp[2], ax=ax34, ci=None);\nsns.lineplot(data=ls_df_train_resps_cumsum[3], color=color_resp[3], ax=ax35, ci=None);\nsns.lineplot(data=ls_df_train_resps_cumsum[4], color=color_resp[4], ax=ax36, ci=None);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"scrolled":false},"cell_type":"code","source":"del ls_df_train_resps_cumsum, fig4, spec4, ax31, ax32, ax33, ax34, ax35, ax36\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"scrolled":false},"cell_type":"code","source":"del df_train_resps\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# features and tags"},{"metadata":{"_kg_hide-input":true,"trusted":true,"scrolled":false},"cell_type":"code","source":"df_features = df_full_features.set_index(\"feature\")","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"scrolled":false},"cell_type":"code","source":"pd.set_option('display.max_rows', 130)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the first table below there are the lists of the tags received by features and their count. In the second one we see which features occured in specific tags."},{"metadata":{"_kg_hide-input":true,"trusted":true,"scrolled":true},"cell_type":"code","source":"def get_feature_tags(df):\n    dict_features = {\"index\": [],\n                     \"count\": [],\n                    \"tags\": []}\n    for index, (index_name, row) in enumerate(df.iterrows()):\n        dict_features[\"index\"].append(row.name)\n        dict_features[\"count\"].append(0)\n        dict_features[\"tags\"].append(\"\")\n        for index1, value in row.items():\n            if value == True:\n                space_between_tags = \", \" if dict_features[\"tags\"][index] != \"\" else \"\"\n                dict_features[\"count\"][index] += 1\n                dict_features[\"tags\"][index] += (space_between_tags + index1)\n    return dict_features\n\ndict_feature_tags = get_feature_tags(df_features)\ndf_feature_tags = pd.DataFrame(dict_feature_tags)\ndf_feature_tags = df_feature_tags.set_index(\"index\")\ndf_feature_tags","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"df_feature_tags[\"tags_lists\"] = df_feature_tags[\"tags\"].apply(lambda x: x.split(\", \"))\ndf_feature_tags","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"pd.set_option('display.max_colwidth', None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"def create_feature_names_dict_count(df):\n    dict_features_list = dict.fromkeys(list(df.index), 0)\n    return dict_features_list\n\ndef create_feature_dict_tree(df):\n    dict_features_tree = {}\n    for i in list(df.index):\n        dict_feature_element = copy.deepcopy(create_feature_names_dict_count(df))\n        dict_features_tree[i] = dict_feature_element\n    return dict_features_tree\n\ndef get_features_frequency(df):\n    dict_features = create_feature_dict_tree(df)\n    for index, (index_name, row) in enumerate(df.iterrows()):\n        if len(row[\"tags_lists\"]) != 0 and row[\"tags_lists\"] != ['']:\n            feature_tag_list = row[\"tags_lists\"]\n            feature_denominator = len(feature_tag_list)\n            for index1, (index_name1, row1) in enumerate(df.iterrows()):\n                if len(row1[\"tags_lists\"]) != 0 and row1[\"tags_lists\"] != ['']:\n                    n_feature_common_tags = 0\n                    for i in feature_tag_list:\n                        if i in row1[\"tags_lists\"]:\n                            n_feature_common_tags += 1\n                    dict_features[index_name][index_name1] = n_feature_common_tags / feature_denominator\n\n    return dict_features\n\ndf_features_frequency = pd.DataFrame(get_features_frequency(df_feature_tags)).applymap(float)\ndf_features_frequency.applymap(\"{:.2f}\".format)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style=\"white\")\nfig9 = plt.figure(constrained_layout=True, figsize=(80, 80))\nax9 = fig9.add_subplot()\nmask9 = np.triu(np.ones_like(df_features_frequency, dtype=bool))\nsns.heatmap(data=df_features_frequency, mask=mask9, ax=ax9, annot=True, fmt=\".2f\", cmap=\"vlag\", center=0, cbar=False);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del fig9, ax9, df_features_frequency","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"scrolled":false},"cell_type":"code","source":"pd.set_option('display.max_colwidth', None)\nsns.set(style=\"darkgrid\")","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"scrolled":false},"cell_type":"code","source":"def get_tag_features(df):\n    dict_tags = {\"index\": [],\n                    \"count\": [],\n                    \"features\": []}\n    for index, (index_name, column) in enumerate(df.iteritems()):\n        dict_tags[\"index\"].append(index_name)\n        dict_tags[\"count\"].append(0)\n        dict_tags[\"features\"].append(\"\")\n        for index_name1, value in column.iteritems():\n            if value == True:\n                space_between_features = \", \" if dict_tags[\"features\"][index] != \"\" else \"\"\n                dict_tags[\"count\"][index] += 1\n                dict_tags[\"features\"][index] += (space_between_features + index_name1)\n    return dict_tags\n        \ndict_tag_features = get_tag_features(df_features)\ndf_tag_features = pd.DataFrame(dict_tag_features)\ndf_tag_features.set_index(\"index\", inplace=True)\ndf_tag_features","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"scrolled":false},"cell_type":"code","source":"def create_tag_names_dict_count(df):\n    dict_tags_list = dict.fromkeys(list(df.columns), 0)\n    return dict_tags_list\n\ndef create_tag_dict_tree(df):\n    dict_tags_tree = {}\n    for i in list(df.columns):\n        dict_tag_element = copy.deepcopy(create_tag_names_dict_count(df))\n        dict_tags_tree[i] = dict_tag_element\n    return dict_tags_tree\n\ndef get_tags_common(df):\n    dict_tags = create_tag_dict_tree(df_features)\n    for index, (index_name, column) in enumerate(df.iteritems()):\n        for index1, (index_name1, value) in enumerate(column.iteritems()):\n            if(value == True):\n                for index2, (index_name2, value2) in enumerate(df.iloc[index1].iteritems()):\n                    if(value2 == True):\n                        dict_tags[index_name][index_name2] += 1\n    return dict_tags\n\ndf_tags_frequency = pd.DataFrame(get_tags_common(df_features))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"True values for tags appear together with other tags. Some of them can be interconnected. The first table below presents the count of coappearance of the tags while the second one shows the frequency in ratios (divided by the number of total tag occurances)."},{"metadata":{"_kg_hide-input":true,"trusted":true,"scrolled":false},"cell_type":"code","source":"df_tags_frequency","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"scrolled":false},"cell_type":"code","source":"df_tags_frequency = df_tags_frequency.apply(lambda x: x.map(lambda y: y / x.max())).round(2)\ndf_tags_frequency","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The charts below show the ratio tables in barplots. If the color of the bar is red, it means that tag from x-axis is True in all the cases of True values for specific tag. The orange value happens in at least 75% of these cases, the yellow one in at least 50% of cases and the blue one when it is between 0% and 50%. Gray values are the values for analysed tags themselves and always amount of 100%."},{"metadata":{"_kg_hide-input":true,"trusted":true,"scrolled":true},"cell_type":"code","source":"def color_generator_fig8(s):\n    color_list = []\n    name = s.name\n    for index, value in s.iteritems():\n        if index == s.name:\n            color_list.append(\"#9B9898\")\n        else:\n            if value == 1:\n                color_list.append(\"#FF0000\")\n            elif value >= 0.75:\n                color_list.append(\"#FF7400\")\n            elif value >= 0.5:\n                color_list.append(\"#FFC100\")\n            else:\n                color_list.append(\"#0080FF\")\n    return color_list\n\ndef barchart_generator_fig8(df, figure, specific):\n    for i, v in enumerate(df):\n        ax8 = figure.add_subplot(specific[i, 0:])\n        ax8.set_title(v)\n        barplot8 = sns.barplot(x=df_tags_frequency.index, y=df_tags_frequency[v], palette=color_generator_fig8(df_tags_frequency[v]), ax=ax8, ci=None)\n        ax8.set_ylabel(\"\")\n        barplot8.set_xticklabels(barplot8.get_xticklabels(), rotation=90);\n\nfig8 = plt.figure(constrained_layout=True, figsize=(12, 80))\nspec8 = fig8.add_gridspec(29, 2)\nbarchart_generator_fig8(df_tags_frequency, fig8, spec8)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true,"scrolled":false},"cell_type":"code","source":"del get_feature_tags, get_tag_features, create_tag_names_dict_count, create_tag_dict_tree, get_tags_common, df_features, color_generator_fig8, barchart_generator_fig8, df_feature_tags, df_tag_features, df_tags_frequency, dict_feature_tags, dict_tag_features, fig8, spec8\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"scrolled":false},"cell_type":"code","source":"df_full_train_features = df_full_train.iloc[:, 7:137]\ndf_full_train_features","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"scrolled":false},"cell_type":"code","source":"pd.set_option('display.max_columns', 140)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"scrolled":false},"cell_type":"code","source":"df_full_train_features.describe().applymap(\"{0:.8f}\".format)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_full_train_features.isna().sum().T.to_frame().transpose()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_full_train_features.isna().any(axis=1).sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect();","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"scrolled":false},"cell_type":"code","source":"def draw_feature_charts(s, figure, specific, i):\n    ax1 = figure.add_subplot(specific[i*2, 0:])\n    ax1.set_title(s.name)\n    sns.lineplot(data=s, ax=ax1, ci=None);\n    ax1.set_ylabel(\"\")\n    ax2 = figure.add_subplot(specific[(i*2)+1, 0])\n    sns.scatterplot(data=s, ax=ax2, ci=None);\n    ax2.set_ylabel(\"\")\n    ax3 = figure.add_subplot(specific[(i*2)+1, 1])\n    sns.histplot(data=s, stat=\"probability\",ax=ax3);\n    ax3.set_ylabel(\"\")\n    ax3.set_xlabel(\"\")\n    ax4 = figure.add_subplot(specific[(i*2)+1, 2])\n    sns.boxplot(data=s, ax=ax4);\n\ndef draw_feature_figure(df):\n    n_columns = len(df.columns)\n    fig7 = plt.figure(constrained_layout=True, figsize=(12, 5*n_columns))\n    spec7 = fig7.add_gridspec(2*n_columns, 3)\n    for index, (index_name, column) in enumerate(df.iteritems()):\n        draw_feature_charts(column, fig7, spec7, index)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"scrolled":false},"cell_type":"code","source":"draw_feature_figure(df_full_train_features.iloc[:, :10])\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"scrolled":false},"cell_type":"code","source":"draw_feature_figure(df_full_train_features.iloc[:, 10:20])\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"scrolled":false},"cell_type":"code","source":"draw_feature_figure(df_full_train_features.iloc[:, 20:30])\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"scrolled":false},"cell_type":"code","source":"draw_feature_figure(df_full_train_features.iloc[:, 30:40])\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"scrolled":false},"cell_type":"code","source":"draw_feature_figure(df_full_train_features.iloc[:, 40:50])\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"scrolled":false},"cell_type":"code","source":"draw_feature_figure(df_full_train_features.iloc[:, 50:60])\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"scrolled":false},"cell_type":"code","source":"draw_feature_figure(df_full_train_features.iloc[:, 60:70])\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"scrolled":false},"cell_type":"code","source":"draw_feature_figure(df_full_train_features.iloc[:, 70:80])\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"scrolled":false},"cell_type":"code","source":"draw_feature_figure(df_full_train_features.iloc[:, 80:90])\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"scrolled":false},"cell_type":"code","source":"draw_feature_figure(df_full_train_features.iloc[:, 90:100])\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"scrolled":false},"cell_type":"code","source":"draw_feature_figure(df_full_train_features.iloc[:, 100:110])\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"scrolled":false},"cell_type":"code","source":"draw_feature_figure(df_full_train_features.iloc[:, 110:120])\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"scrolled":false},"cell_type":"code","source":"draw_feature_figure(df_full_train_features.iloc[:, 120:130])\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true,"scrolled":false},"cell_type":"code","source":"del draw_feature_figure, draw_feature_charts\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style=\"white\")","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df_full_features_corr = df_full_train_features.corr(method=\"pearson\")\ndf_full_features_corr","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig10 = plt.figure(constrained_layout=True, figsize=(80, 80))\nfig10.suptitle('Pearsons correlation coefficients', fontsize=100)\nax10 = fig10.add_subplot()\nmask10 = np.triu(np.ones_like(df_full_features_corr, dtype=bool))\nsns.heatmap(data=df_full_features_corr, mask=mask10, ax=ax10, annot=True, fmt=\".2f\", cmap=\"vlag\", center=0, cbar=False);","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"del df_full_features_corr, fig10, ax10, mask10\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df_full_features_corr_p = df_full_train_features.corr(method=pearsonr_pval)\ndf_full_features_corr_p","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig11 = plt.figure(constrained_layout=True, figsize=(80, 80))\nfig11.suptitle('Pearsons correlation p-value', fontsize=100)\nax11 = fig11.add_subplot()\nmask11 = np.triu(np.ones_like(df_full_features_corr_p, dtype=bool))\ncmap11 = sns.diverging_palette(145, 300, s=60, as_cmap=True)\nsns.heatmap(data=df_full_features_corr_p, mask=mask11, ax=ax11, annot=True, fmt=\".2f\", cmap=cmap11, center=0.05, cbar=False);","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"del df_full_train_features, df_full_features_corr_p, fig11, ax11, mask11, cmap11\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# weight"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_full_train[\"weight\"].isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig12 = plt.figure(constrained_layout=True, figsize=(12, 12))\nspec12 = fig12.add_gridspec(2, 1)\nfig12.suptitle('Weight')\nax12 = fig12.add_subplot(spec12[0,0])\nsns.lineplot(data=df_full_train[\"weight\"], ax=ax12, ci=None)\nax13 = fig12.add_subplot(spec12[1,0])\nsns.scatterplot(data=df_full_train[\"weight\"], ax=ax13, ci=None);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del fig12, ax12, ax13\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_weights_0 = df_full_train[df_full_train[\"weight\"] == 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(df_weights_0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_weights_0[[\"resp\", \"resp_1\", \"resp_2\", \"resp_3\", \"resp_4\"]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_weights_0[[\"resp\", \"resp_1\", \"resp_2\", \"resp_3\", \"resp_4\"]].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig14 = plt.figure(constrained_layout=True, figsize=(12, 6))\nfig14.suptitle('Resp for weights values 0')\nax14 = fig14.add_subplot()\nsns.scatterplot(data=df_weights_0[\"resp\"], ax=ax14, ci=None);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del df_weights_0, fig14, ax14\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# resps and other values"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_columns', 140)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df_full_train_bool = copy.deepcopy(df_full_train)\ndf_full_train_bool[\"resp\"] = df_full_train_bool[\"resp\"].map(lambda x: True if x >= 0 else False)\ndf_full_train_bool ","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def histplot_resp_true_false(df, column_name, figure, specific, i):\n    ax = figure.add_subplot(specific[i,0])\n    sns.histplot(data=df.loc[df[\"resp\"] == True, [column_name]], palette=[\"#228B22\"], ax=ax, stat=\"probability\")\n    label_string = column_name + \" for resp>=0\"\n    ax.legend(labels=[label_string])\n    ax.set_ylabel(\"\")\n    ax1 = figure.add_subplot(specific[i,1])\n    sns.histplot(data=df.loc[df[\"resp\"] == False, [column_name]], palette=[\"#DC143C\"], ax=ax1, stat=\"probability\");\n    label_string1 = column_name + \" for resp<0\"\n    ax1.legend(labels=[label_string1])\n    ax1.set_ylabel(\"\")\n    # Bottom\n    if ax.get_ylim()[0] < ax1.get_ylim()[0]:\n        ax1.set_ylim(bottom=ax.get_ylim()[0])\n    else:\n        ax.set_ylim(bottom=ax1.get_ylim()[0])\n    # Top\n    if ax.get_ylim()[1] > ax1.get_ylim()[1]:\n        ax1.set_ylim(top=ax.get_ylim()[1])\n    else:\n        ax.set_ylim(top=ax1.get_ylim()[1])\n    # Right\n    if ax.get_xlim()[1] < ax1.get_xlim()[1]:\n        ax1.set_xlim(right=ax.get_xlim()[1])\n    else:\n        ax.set_xlim(right=ax1.get_xlim()[1])\n    # Left\n    if ax.get_xlim()[0] > ax1.get_xlim()[0]:\n        ax1.set_xlim(left=ax.get_xlim()[0])\n    else:\n        ax.set_xlim(left=ax1.get_xlim()[0])\n        \ndef draw_bool_resp(df):\n    # resp in the first column\n    n_columns = len(df.columns) - 1\n    fig13 = plt.figure(constrained_layout=True, figsize=(12, 2*n_columns))\n    spec13 = fig13.add_gridspec(n_columns, 2)\n    for index, (index_name, column) in enumerate(df.iteritems()):\n        if column.name != \"resp\":\n            histplot_resp_true_false(df, column.name, fig13, spec13, index - 1)\n\nfeatures_columns_names = list(df_full_train_bool.columns)[7:137]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"draw_bool_resp(df_full_train_bool[[\"resp\"] + features_columns_names[:10]])\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"draw_bool_resp(df_full_train_bool[[\"resp\"] + features_columns_names[10:20]])\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"draw_bool_resp(df_full_train_bool[[\"resp\"] + features_columns_names[20:30]])\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"draw_bool_resp(df_full_train_bool[[\"resp\"] + features_columns_names[30:40]])\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"draw_bool_resp(df_full_train_bool[[\"resp\"] + features_columns_names[40:50]])\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"draw_bool_resp(df_full_train_bool[[\"resp\"] + features_columns_names[50:60]])\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"draw_bool_resp(df_full_train_bool[[\"resp\"] + features_columns_names[60:70]])\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"draw_bool_resp(df_full_train_bool[[\"resp\"] + features_columns_names[70:80]])\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"draw_bool_resp(df_full_train_bool[[\"resp\"] + features_columns_names[80:90]])\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"draw_bool_resp(df_full_train_bool[[\"resp\"] + features_columns_names[90:100]])\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"draw_bool_resp(df_full_train_bool[[\"resp\"] + features_columns_names[100:110]])\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"draw_bool_resp(df_full_train_bool[[\"resp\"] + features_columns_names[110:120]])\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"draw_bool_resp(df_full_train_bool[[\"resp\"] + features_columns_names[120:130]])\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"del features_columns_names\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"draw_bool_resp(df_full_train_bool[[\"resp\", \"resp_1\", \"resp_2\", \"resp_3\", \"resp_4\", \"weight\"]])\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"del draw_bool_resp, histplot_resp_true_false, df_full_train_bool\ngc.collect();","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}