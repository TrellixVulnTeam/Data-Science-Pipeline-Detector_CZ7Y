{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\nTrading for profit has always been a difficult problem to solve, even more so in todayâ€™s fast-moving and complex financial markets. Electronic trading allows for thousands of transactions to occur within a fraction of a second, resulting in nearly unlimited opportunities to potentially find and take advantage of price differences in real time.\n\nYou will build your own quantitative trading model to maximize returns using market data from a major global stock exchange. \n\nThe challenge will be to use the historical data, mathematical tools, and technological tools at your disposal to create a model that gets as close to certainty as possible. You will be presented with a number of potential trading opportunities, which your model must choose whether to accept or reject.\n\n# Data Description\n\nThis dataset contains an anonymized set of features, **feature_{0...129}**, representing real stock market data. Each row in the dataset represents a trading opportunity, for which you will be predicting an **action** value: 1 to make the trade and 0 to pass on it. Each trade has an associated **weight** and **resp**, which together represents a return on the trade. The **date** column is an integer which represents the day of the trade, while **ts_id** represents a time ordering. In addition to anonymized feature values, you are provided with metadata about the features in features.csv.\n\nIn the training set, train.csv, you are provided a **resp** value, as well as several other **resp_{1,2,3,4}** values that represent returns over different time horizons. These variables are not included in the test set. Trades with **weight = 0** were intentionally included in the dataset for completeness, although such trades will not contribute towards the scoring evaluation."},{"metadata":{},"cell_type":"markdown","source":"### Library\n\nLibraries necessary for the execution of this notebook."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport datatable as dt\nfrom scipy import stats\n\n# Plot\nimport matplotlib.pyplot as plt\nfrom matplotlib import pyplot\nimport seaborn as sns\nimport plotly.express as px\n\n# Preparing features\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\n# PCA\nfrom sklearn.decomposition import PCA\n\n# Training and test data\nfrom sklearn.model_selection import train_test_split\n\n# Base estimators\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Ensemble methods\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\n\n# Warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading Data\n\n**features.csv** - metadata pertaining to the anonymized features\n\n**train.csv** - the training set, contains historical data and returns"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"features = dt.fread('../input/jane-street-market-prediction/features.csv')\nfeatures = features.to_pandas()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"%%time\n\ntrain = dt.fread('../input/jane-street-market-prediction/train.csv')\ntrain = train.to_pandas()\n\nprint(\"train size:\", train.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis\n\nLet's take a look at the summary table of features and training data. Showing data type, missing, unique values, their first three values end entropy value. However, as there are many variables in the training data, let's take a look at the first 25."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def resumetable(df):\n    print(f\"Dataset Shape: {df.shape}\")\n    summary = pd.DataFrame(df.dtypes,columns=['dtypes'])\n    summary = summary.reset_index()\n    summary['Name'] = summary['index']\n    summary = summary[['Name','dtypes']]\n    summary['Missing'] = df.isnull().sum().values    \n    summary['Uniques'] = df.nunique().values\n    summary['First Value'] = df.loc[0].values\n    summary['Second Value'] = df.loc[1].values\n    summary['Third Value'] = df.loc[2].values\n\n    return summary","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"cm = sns.light_palette(\"blue\", as_cmap=True)\n\nresumetable(features).style.background_gradient(cmap=cm)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"cm = sns.light_palette(\"blue\", as_cmap=True)\n\nresumetable(train)[:25].style.background_gradient(subset=['Missing', 'First Value', \n                                                          'Second Value', 'Third Value'], cmap=cm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Features overview"},{"metadata":{},"cell_type":"markdown","source":"Let's take a look at the density curves for **resp** and **weight**."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,12))\nplt.subplot(221)\ng = sns.distplot(train['resp'])\ng.set_title(\"Resp\", fontsize=18)\ng.set_xlabel(\"\")\ng.set_ylabel(\"Probability\", fontsize=15)\n\nplt.subplot(222)\ng = sns.distplot(train['resp_1'])\ng.set_title(\"Resp 1\", fontsize=18)\ng.set_xlabel(\"\")\ng.set_ylabel(\"Probability\", fontsize=15)\n\nplt.figure(figsize=(16,12))\n\nplt.subplot(221)\ng = sns.distplot(train['resp_2'])\ng.set_title(\"Resp 2\", fontsize=18)\ng.set_xlabel(\"\")\ng.set_ylabel(\"Probability\", fontsize=15)\n\nplt.subplot(222)\ng = sns.distplot(train['resp_3'])\ng.set_title(\"Resp 3\", fontsize=18)\ng.set_xlabel(\"\")\ng.set_ylabel(\"Probability\", fontsize=15)\n\nplt.figure(figsize=(16,12))\n\nplt.subplot(221)\ng = sns.distplot(train['resp_4'])\ng.set_title(\"Resp 4\", fontsize=18)\ng.set_xlabel(\"\")\ng.set_ylabel(\"Probability\", fontsize=15)\n\nplt.subplot(222)\ng = sns.distplot(train['weight'])\ng.set_title(\"weight\", fontsize=18)\ng.set_xlabel(\"\")\ng.set_ylabel(\"Probability\", fontsize=15)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, the response values are highly concentrated at 0, thus turning the other values into outliers."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(13,8))\n\nsns.boxplot(data=train.iloc[:,2:7])\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can perceive both visually and by the quantiles that the values of resp are around 0. However, another 50% of the values of the variable Weight are below 1, showing a high variability in this variable."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print(\"Quantiles for resp and weight:\")\nprint(train[['resp','resp_1','resp_2','resp_3','resp_4','weight']].quantile([.01, .025, .1, .25, .5, .75, .9, .975, .99]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's see what the density curve looks like for the first 6 features."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,12))\nplt.subplot(221)\ng = sns.distplot(train['feature_1'])\ng.set_title(\"Feature 1\", fontsize=18)\ng.set_xlabel(\"\")\ng.set_ylabel(\"Probability\", fontsize=15)\n\nplt.subplot(222)\ng = sns.distplot(train['feature_2'])\ng.set_title(\"Feature 2\", fontsize=18)\ng.set_xlabel(\"\")\ng.set_ylabel(\"Probability\", fontsize=15)\n\nplt.figure(figsize=(16,12))\n\nplt.subplot(221)\ng = sns.distplot(train['feature_3'])\ng.set_title(\"Feature 3\", fontsize=18)\ng.set_xlabel(\"\")\ng.set_ylabel(\"Probability\", fontsize=15)\n\nplt.subplot(222)\ng = sns.distplot(train['feature_4'])\ng.set_title(\"Feature 4\", fontsize=18)\ng.set_xlabel(\"\")\ng.set_ylabel(\"Probability\", fontsize=15)\n\nplt.figure(figsize=(16,12))\n\nplt.subplot(221)\ng = sns.distplot(train['feature_5'])\ng.set_title(\"Feature 5\", fontsize=18)\ng.set_xlabel(\"\")\ng.set_ylabel(\"Probability\", fontsize=15)\n\nplt.subplot(222)\ng = sns.distplot(train['feature_6'])\ng.set_title(\"Feature 6\", fontsize=18)\ng.set_xlabel(\"\")\ng.set_ylabel(\"Probability\", fontsize=15)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see how is the correlation between all the answers in the database. We can see a high positive correlation between some resp variables, however there is no correlation between the weight variable and the other resp. For now we will not look at the correlations of the features, after all I will use the PCA to decrease the dimension of the data."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(13, 10))\n\nmatrix = np.triu(train.iloc[:, 1:7].corr())\n\nc = sns.heatmap(train.iloc[:, 1:7].corr(), annot = True, vmin = -1, vmax = 1, center = 0, cmap = 'coolwarm', mask = matrix)\nc.set_title(\"Resp and Weight Correlation\", fontsize=18)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preparing features\n\nBefore applying any model for data prediction, I will give a light organized in the data.\n\nLet's select all the features."},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# Select all features\nfeatures = train.columns[7:137]\nfeatures","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the weight variable has a very large amount of 0 and this can interfere in the model's forecast, I will remove it from the analysis (who knows, I may leave the values and use it in a model for comparison purposes)."},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# filtering the values 0\ntrain = train[train['weight'] != 0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As the competition submission example shows, the name of the output (target) variable is **action** and your answer will be 0 or 1."},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"train['action'] = (train['resp'].values > 0).astype(int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The distribution of the categories of the action variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 5))\n\nfreq = len(train)\n\ng = sns.countplot(train['action'])\ng.set_xlabel(\"Action\", fontsize = 13)\ng.set_ylabel(\"Count\", fontsize = 13)\n\nfor p in g.patches:\n    height = p.get_height()\n    g.text(p.get_x() + p.get_width() / 2., height + 3,\n          '{:1.2f}%'.format(height / freq * 100),\n          ha = \"center\", fontsize = 15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With the filter of the weight variable, we will see if there are still missing values.\nWell, we have a lot of variables with missing values. It is common in these cases to fill in the missing values with the average of their variables, but taking into account that there may be variables with outlier values, I will choose to fill in the median (for comparison criteria, then I will change to the average and compare the models.)"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"[col for col in list(train.columns) if train[col].isnull().any()]\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"train_median = train.median()\ntrain = train.fillna(train_median)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, no variables with missing values."},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"[col for col in train.columns if train[col].isnull().any()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Principal Componente Analysis (PCA)\n\nPrincipal Component Analysis (PCA) is a method for extracting important variables (in the form of components) from a large set of variables, available in a data set. This technique allows you to extract a small number of dimensional sets from a highly dimensional dataset. With fewer variables the visualization also becomes much more significant."},{"metadata":{},"cell_type":"markdown","source":"Before going straight to the PCA, it is important to ensure that the input variables are on the same scale, so that the PCA performs better."},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler().fit(train.loc[:, features].values)\nrescaledX = scaler.transform(train.loc[:, features].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca_mod = PCA()\ncomp = pca_mod.fit_transform(rescaledX)\n\nexp_var_cumul = np.cumsum(pca_mod.explained_variance_ratio_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is possible to notice that, from the graph with the principal components, much of the variance explained for the data set is contained in the first fifteen principal components (80%). So, instead of using the 129 database variables, we can only use 15 principal components in which they will explain 80% of the variability of the original data."},{"metadata":{"trusted":true},"cell_type":"code","source":"px.area(x = range(1, exp_var_cumul.shape[0] + 1),\n    y = exp_var_cumul,\n    labels = {\"x\": \"Principal Component\", \"y\": \"Explained Variance\"}\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"comp.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# transforming to dataframe and adding column names\nfeat_cols = ['PC'+str(i) for i in range(comp.shape[1])]\ncomp_feat = pd.DataFrame(comp,columns = feat_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"comp_feat","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Machine Learning\n\n## Ensemble Methods\n\nIn statistics and machine learning, ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.\n\nI'll use some ensemble methods and then check which one got the best performance."},{"metadata":{},"cell_type":"markdown","source":"But first, I will divide the data into training and testing."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Division into training and test data\nX_train, X_test, Y_train, Y_test = train_test_split(comp_feat.iloc[:,0:50], \n                                                    train['action'], \n                                                    test_size = 0.30, \n                                                    random_state = 101)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"#%%time\n\n# Create the classifier\n#RF = RandomForestClassifier(n_estimators  = 10)\n\n# Training the model\n#RF.fit(X_train, Y_train)\n\n#result = RF.score(X_test, Y_test)\n#print('Accuracy in test data: %.3f%%' % (result * 100.0))\n\n#-----------------------------------------------------------\n\n#CPU times: user 3min 6s, sys: 125 ms, total: 3min 6s\n#Wall time: 3min 6s\n#RandomForestClassifier(n_estimators=10)\n\n#Accuracy in test data: 51.502%","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Bagging\n\nFor the Bagging estimator, the Logistic Regression model will be used as the base estimator. Not that it is the best model, because it was not my first choice, but it was the model that managed to generate a faster response. I even used the SVM and KNN models as base estimators, but it took a long time to complete. However, compared to Random Forest we have a small loss in accuracy, but we gain in preprocessing time."},{"metadata":{"trusted":true},"cell_type":"code","source":"#%%time\n\n# Base estimator\n#base = LogisticRegression()\n\n# Create the classifier\n#BAG = BaggingClassifier(base_estimator = base, max_samples = 0.5, max_features = 0.5)\n\n# Training the model\n#BAG.fit(X_train, Y_train)\n\n#result = BAG.score(X_test, Y_test)\n#print('Accuracy in test data: %.3f%%' % (result * 100.0))\n\n#-----------------------------------------------------------\n\n#CPU times: user 36.2 s, sys: 1.24 s, total: 37.5 s\n#Wall time: 18.9 s\n#BaggingClassifier(base_estimator=LogisticRegression(), max_features=0.5,\n#                  max_samples=0.5)\n\n#Accuracy in test data: 51.408%","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Adaboost\n\nUsing the Decision Tree Classifier algorithm as a base estimator. In this model I had a slightly higher gain in accuracy, but a lot more processing soon."},{"metadata":{"trusted":true},"cell_type":"code","source":"#%%time\n\n# Base estimator\n#base = DecisionTreeClassifier(max_depth = 1, min_samples_leaf = 1)\n\n# Create the classifier\n#ADA = AdaBoostClassifier(base_estimator = base,\n#                         learning_rate = 0.1, \n#                         n_estimators = 100, \n#                         algorithm = \"SAMME.R\")\n\n# Training the model\n#ADA.fit(X_train, Y_train)\n\n#result = ADA.score(X_test, Y_test)\n#print('Accuracy in test data: %.3f%%' % (result * 100.0))\n\n#-----------------------------------------------------------\n\n#CPU times: user 9min 28s, sys: 7.37 s, total: 9min 36s\n#Wall time: 9min 36s\n\n#Accuracy in test data: 51.618%","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Gradient Boosting"},{"metadata":{"trusted":true},"cell_type":"code","source":"#%%time\n\n# Create the classifier\n#GB = GradientBoostingClassifier()\n\n# Training the model\n#GB.fit(X_train, Y_train)\n\n#result = GB.score(X_test, Y_test)\n#print('Accuracy in test data: %.3f%%' % (result * 100.0))\n\n#-----------------------------------------------------------\n\n#CPU times: user 23min 59s, sys: 941 ms, total: 23min 59s\n#Wall time: 24min 1s\n\n#Accuracy in test data: 52.167%","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XGBoosting\n\nXGBoost showed faster and more accurate processing than Adaboost and Gradient Boosting methods."},{"metadata":{"trusted":true},"cell_type":"code","source":"#%%time\n\n# Create the classifier\n#XGB = XGBClassifier()\n\n# Training the model\n#XGB.fit(X_train, Y_train)\n\n#result = XGB.score(X_test, Y_test)\n#print('Accuracy in test data: %.3f%%' % (result * 100.0))\n\n#-----------------------------------------------------------\n\n#CPU times: user 5min 25s, sys: 1.08 s, total: 5min 26s\n#Wall time: 5min 27s\n\n#Accuracy in test data: 52.203%","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LightGBM\n\nAmong all the ensamble methods presented here, LightGBM was the most accurate and fast. With the same speed as Bagging, but with a better fit to the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nfrom lightgbm import LGBMClassifier\n\n# Create the classifier\nLGBM = LGBMClassifier()\n\n# Training the model\nLGBM.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = LGBM.score(X_test, Y_test)\nprint('Accuracy in test data: %.3f%%' % (result * 100.0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission File"},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components = 50).fit(train.loc[:, features])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fillna_npwhere(array, values):\n    if np.isnan(array.sum()):\n        array = np.where(np.isnan(array), values, array)\n    return array","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import janestreet\nenv = janestreet.make_env() # initialize the environment\niter_test = env.iter_test() # an iterator which loops over the test set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for (test_df, sample_prediction_df) in iter_test:\n    wt = test_df.iloc[0].weight\n    if(wt == 0):\n        sample_prediction_df.action = 0 \n    else:\n        sample_prediction_df.action = LGBM.predict(pca.transform\n                                                          (scaler.transform(\n                                                              fillna_npwhere(test_df[features].values,\n                                                                             train_median[features].values))))\n    env.predict(sample_prediction_df)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}