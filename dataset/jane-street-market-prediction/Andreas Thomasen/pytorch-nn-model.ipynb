{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Simple Predictor for Jane-Street Market competition\n\nHere I show how to build a very simple neural network model in Pytorch and train it on a GPU.\n\nPre-processing of data is done as described in [this notebook](https://www.kaggle.com/andreasthomasen/preprocessing-and-feature-selection). The main difference is that we only do PCA here and retain a lot of features. The reason is that we do not use RNNs, but instead only rely on instantaneous feature values. So this model can be trained with quite a lot of features included.\n\nIf you read this notebook from start to finish, you will learn how to\n* Load data into pandas\n* Do feature reduction using PCA\n* Define a neural network model in pytorch\n* Train the model and save it using pickle\n\nThanks for reading, if you like it, feel free to copy it. Nothing revolutionary in this notebook. It would also be helpful if you upvoted :)\n\nUPDATE: Including the training step, it took too long to run this notebook for submission. So instead it now saves the model at the end. You can run it later in a private submission.\nEnjoy!"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nif torch.cuda.is_available():\n    dev = torch.device(\"cuda\")\nelse:\n    dev = torch.device(\"cpu\")\n\nimport pickle\n    \nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load data and reduce dimensions"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/jane-street-market-prediction/train.csv')\nbatch_size = len(train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The tensors below will be used later. The wrtensor is used in training. We store feature_0 in a separate tensor since it is the only integer valued feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"wrtensor = torch.tensor(train.loc[:,['weight','resp']].to_numpy(),dtype=torch.float)\nwrtensor = torch.mul(wrtensor[:,0],wrtensor[:,1]).to(dev)\nitensor = torch.tensor(((train.loc[:,'feature_0']+1)//2).to_numpy(),dtype=torch.long,device=dev)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We make a separate tensor that contains all other features"},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_names = ['feature_'+str(i) for i in range(1,130)]\ntrain = train[feature_names]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's remove outliers first"},{"metadata":{"trusted":true},"cell_type":"code","source":"maxindex = np.zeros((129,3))\nfor i in range(129):\n    counts = train[feature_names[i]].value_counts()\n    mean = train[feature_names[i]].mean()\n    std = train[feature_names[i]].std()\n    sigmas = np.abs(counts.index[0]-mean)/std\n    maxindex[i] = [counts.index[0], counts.iloc[0], sigmas]\n    \nfor i in range(129):\n    if maxindex[i,1] > 100 and maxindex[i,2] > 1:\n        train.replace({feature_names[i]: maxindex[i,0]},np.nan)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we need to deal with NaN. We impute those missing values with the mean of each column."},{"metadata":{"trusted":true},"cell_type":"code","source":"fill_val=train.mean()\ntrain = train.fillna(fill_val)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We compute the principal components and reduce the feature space using sklearn"},{"metadata":{"trusted":true},"cell_type":"code","source":"pca_components = 60\nsc = StandardScaler().fit(train.to_numpy())\ntrain = sc.transform(train.to_numpy())\npca = PCA(n_components = pca_components).fit(train)\ntrain=pca.transform(train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally we have a tensor with the last features we will use"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = torch.tensor(train,dtype=torch.float,device=dev)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model"},{"metadata":{},"cell_type":"markdown","source":"We will make a very simple model at first using pytorch. The idea is to have fully connected layers deal with all of the floating point features, while feature_0 is used in an embedding layer."},{"metadata":{"trusted":true},"cell_type":"code","source":"e_size = 64\nfc_input = pca_components\nh_dims = [512,512,256,128]\ndropout_rate = 0.5\nepochs = 200\nminibatch_size = 100000\n\nclass MarketPredictor(nn.Module):\n    def __init__(self):\n        super(MarketPredictor, self).__init__()\n        \n        self.e = nn.Embedding(2,e_size)\n        self.deep = nn.Sequential(\n            nn.Linear(fc_input,h_dims[0]),\n            nn.BatchNorm1d(h_dims[0]),\n            nn.LeakyReLU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(h_dims[0],h_dims[1]),\n            nn.BatchNorm1d(h_dims[1]),\n            nn.LeakyReLU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(h_dims[1],h_dims[2]),\n            nn.BatchNorm1d(h_dims[2]),\n            nn.LeakyReLU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(h_dims[2],h_dims[3]),\n            nn.BatchNorm1d(h_dims[3]),\n            nn.LeakyReLU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(h_dims[3],e_size),\n            nn.BatchNorm1d(e_size),\n            nn.LeakyReLU(),\n            nn.Dropout(dropout_rate)\n            )\n        self.reduce = nn.utils.weight_norm(nn.Linear(e_size,1))\n        self.sig = nn.Sigmoid()\n        \n    def forward(self,xi,xf):\n        e_out = self.e(xi)\n        f_out = self.deep(xf)\n        ef_out = self.reduce(e_out+f_out)\n        sig_out = self.sig(ef_out)\n        \n        return sig_out\n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we train it. Let's define the loss function first. In the competition we're told that the return on day $i$ is\n\\begin{equation}\np_i = \\sum_j (\\mathit{weight}_{ij}*\\mathit{resp}_{ij}*\\mathit{action}_{ij})\n\\end{equation}\nThe way we've made the network it gives a sigmoidal output $s_{ij} \\in[0;1]$. Let's make the cost-function\n\\begin{equation}\nC = \\sum_i c_i = -\\sum_{i,j} (\\mathit{weight}_{ij}*\\mathit{resp}_{ij}*s_{ij}).\n\\end{equation}\nThis has the same minimum as $p_i$, but the advantage is that it's got finite gradients with respect to the model parameters, and so should work better with SGD.\n\nWe will also use minibatches to prevent overfitting."},{"metadata":{"trusted":true},"cell_type":"code","source":"def loss(s,wr):\n    return - torch.dot(s,wr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's make some torch tensors which hold the training data and apply our model to it"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = MarketPredictor().to(dev)\nopt = optim.Adam(model.parameters())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"minibatches = batch_size//minibatch_size\n\nfor i in range(epochs):\n    permutation = torch.randperm(batch_size)\n    print('Epoch is',i,'/',epochs)\n    for j in range(minibatches):\n        opt.zero_grad()\n        s = model(itensor[permutation[j*minibatch_size:(j+1)*minibatch_size]],train[permutation[j*minibatch_size:(j+1)*minibatch_size]])\n        c = loss(s.squeeze(),wrtensor[permutation[j*minibatch_size:(j+1)*minibatch_size]])\n        c.backward()\n        opt.step()\n    print('Loss is',c.item())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Saving the model\nIt's pretty easy to save a pytorch model. We will use pickle and save the state dict of the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"path = 'marketpredictor_state_dict_'+str(epochs)+'epochs.pt'\ntorch.save(model.state_dict(),path)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will also need the standard scaler and pca objects, as well as the maxindex and fill_val for when we run things for submission later"},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('feature_processing.pkl','wb') as f:\n    pickle.dump([sc,pca,maxindex,fill_val],f)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}