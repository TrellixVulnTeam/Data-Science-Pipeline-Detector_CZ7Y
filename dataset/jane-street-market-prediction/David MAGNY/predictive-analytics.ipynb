{"cells":[{"metadata":{},"cell_type":"markdown","source":"The purpose is to perform predictive analytics on the data provided for the Jane Street Market Prediction "},{"metadata":{},"cell_type":"markdown","source":"### Import the relevant packages an librairies"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport time\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import make_scorer, accuracy_score\n# CatBoost model\nfrom catboost import CatBoostClassifier, Pool\nfrom sklearn.model_selection import train_test_split, KFold, GridSearchCV, StratifiedKFold\n# import lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### DataSets Loading "},{"metadata":{"trusted":true},"cell_type":"code","source":"# df.fread().to_pandas() is faster than pd.read_csv()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"folder_path = '../input/jane-street-market-prediction/'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"%%time\ntrain_df = pd.read_csv(folder_path +'train.csv' , nrows=1800000)\nfeatures_df = pd.read_csv(folder_path + 'features.csv')\nsample_df = pd.read_csv(folder_path + 'example_sample_submission.csv')\ntest_data_df = pd.read_csv(folder_path + 'example_test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"    train_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = [c for c in train_df.columns if 'feature' in c]\nresps = [c for c in train_df.columns if 'resp' in c]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_df[train_df['weight'] != 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['action'] = train_df['resp'].apply(lambda x:x>0).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df_median = train_df[features].median()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train_df[features].fillna(train_df_median)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train_df['action']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data reduction"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Before we perform PCA, we need to normalise the features so that they have zero mean and unit variance\nscaler = StandardScaler()\nscaler.fit(X)\nx_norm = scaler.transform(X)\n\npca = PCA()\ncomp = pca.fit(x_norm)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We plot a graph to show how the explained variation in the 129 features varies with the number of principal components\nplt.plot(np.cumsum(comp.explained_variance_ratio_))\nplt.grid()\nplt.xlabel('Number of Principal Components')\nplt.ylabel('Explained Variance')\nsns.despine();\n\n# The first 15 principal components explains about 80% of the variation\n# The first 40 principal components explains about 95% of the variation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=50).fit(x_norm)\nx_transform = pca.transform(x_norm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_norm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_transform","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Machine Learning"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = CatBoostClassifier()\ncv = StratifiedKFold(shuffle=True, n_splits=5, random_state=108)\nparams = {'iterations': [100],\n          #'depth': [6, 8, 10, 15],\n          'depth': [10, 15],\n          'loss_function': ['Logloss'],\n          #'learning_rate':[0.01, 0.02, 0.03, 0.04,0.05],\n          'learning_rate':[0.03,0.05],\n          #'l2_leaf_reg': np.logspace(-20, -19, 3),\n          #'leaf_estimation_iterations': [10],\n          'eval_metric': ['Accuracy'],\n          #'use_best_model': ['True'],\n          'task_type':['GPU'],\n#          'logging_level':['Silent'],\n          'random_seed': [42]\n         }\nscorer = make_scorer(accuracy_score)\nclf = GridSearchCV(estimator=model, param_grid=params, scoring=scorer, cv=cv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model = CatBoostClassifier(iterations=10000, task_type=\"GPU\", learning_rate=0.05, l2_leaf_reg=3.5, depth=11, loss_function= 'Logloss', eval_metric='AUC',use_best_model=True,random_seed=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make the x for train and test (also called validat ion data)\nxtrain,xval, ytrain, yval = train_test_split(x_transform, y,train_size=0.8,random_state=42)\n# sanity check to ensure all features are categories. In our case, yes.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf.fit(xtrain, ytrain, use_best_model=True, eval_set=[(xval, yval)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_param = clf.best_params_\nbest_param","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_estimator = clf.best_estimator_\nbest_estimator","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_score = clf.best_score_\nbest_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_feature_importance(importance,names,model_type):\n    \n    #Create arrays from feature importance and feature names\n    feature_importance = np.array(importance)\n    feature_names = np.array(names)\n    \n    #Create a DataFrame using a Dictionary\n    data={'feature_names':feature_names,'feature_importance':feature_importance}\n    fi_df = pd.DataFrame(data)\n    \n    #Sort the DataFrame in order decreasing feature importance\n    fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True)\n    \n    #Define size of bar plot\n    plt.figure(figsize=(10,8))\n    #Plot Searborn bar chart\n    sns.barplot(x=fi_df['feature_importance'], y=fi_df['feature_names'])\n    #Add chart labels\n    plt.title(model_type + 'FEATURE IMPORTANCE')\n    plt.xlabel('FEATURE IMPORTANCE')\n    plt.ylabel('FEATURE NAMES')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot the catboost result\n#plot_feature_importance(clf.get_feature_importance(),train.columns,'CATBOOST')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# use_best_model params to prevent model overfitting\nbesthyperparameters = CatBoostClassifier(iterations=1000,\n                           loss_function=best_param['loss_function'],\n                           depth=best_param['depth'],\n                            learning_rate=best_param['learning_rate'],\n                          # l2_leaf_reg=best_param['l2_leaf_reg'],\n                           eval_metric='Accuracy',\n                           #leaf_estimation_iterations=10,\n                           use_best_model=True,\n                           # logging_level='Silent',\n                        task_type=\"GPU\",\n                           random_seed=42\n                          )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make the x for train and test (also called validat ion data)\n#xtrain,xval, ytrain, yval = train_test_split(x_transform, y,train_size=0.8,random_state=42)\n# sanity check to ensure all features are categories. In our case, yes.\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"besthyperparameters.fit(xtrain, ytrain, eval_set=(xval, yval))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import scikitplot as skplt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = besthyperparameters.predict(x_transform)\nskplt.metrics.plot_confusion_matrix(y, y_pred, normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"import janestreet\nenv = janestreet.make_env() # initialize the environment\niter_test = env.iter_test() # an iterator which loops over the test set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_test_transform\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fillna_npwhere(array, values):\n    if np.isnan(array.sum()):\n        array = np.where(np.isnan(array), values, array)\n    return array","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfor (test_df, sample_prediction_df) in iter_test:\n    wt = test_df.iloc[0].weight\n    if(wt == 0):\n        sample_prediction_df.action = 0 \n    else:\n        sample_prediction_df.action = besthyperparameters.predict(pca.transform(scaler.transform(fillna_npwhere(test_df[features].values,train_df_median[features].values))))\n    env.predict(sample_prediction_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%%time\n#for (test_df, #sample_prediction_df) in iter_test:\n    #sample_prediction_df[\"action\"] = clf.predict(X_test_transform[features]).astype(int)\n #   sample_prediction_df[\"action\"] = clf.predict(np.transpose(X_test_transform)).astype(int)\n    #env.predict(sample_prediction_df)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}