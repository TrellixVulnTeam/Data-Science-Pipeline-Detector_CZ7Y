{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Goals: \n\n## 1) Reduce the memory usage in processing Jane Street Data.\n\n## 2) Look at overall market trends by date using mean and sum.\n\n## 3) Use conditional mean by date to impute missing values."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport scipy\nfrom scipy import stats\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import StratifiedKFold\n%matplotlib inline\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"####Import Temp Time Series Data\ntrain_temp = pd.read_csv(\"../input/jane-street-market-prediction/train.csv\", nrows=5)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reduce Memory Use Technique by Down Casting\nSpecial thanks to https://www.kaggle.com/akosciansky/how-to-import-large-csv-files-and-save-efficiently. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get information on the datatypes\ntrain_temp.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_cols=list(train_temp.columns)\nlen(train_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find out the smallest data type possible for each numeric feature\nfloat_cols = train_temp.select_dtypes(include=['float'])\nint_cols = train_temp.select_dtypes(include=['int'])\n\nfor cols in float_cols.columns:\n    train_temp[cols] = pd.to_numeric(train_temp[cols], downcast='float')\n    \nfor cols in int_cols.columns:\n    train_temp[cols] = pd.to_numeric(train_temp[cols], downcast='integer')\n\nprint(train_temp.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_cols_dict = { i : 'float32' for i in train_cols }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"int_cols_names=list(int_cols.columns)\nint_cols_names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_cols_dict[ 'feature_0']  =  'int8'\ntrain_cols_dict['ts_id']  =  'int32'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_cols_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/jane-street-market-prediction/train.csv\", usecols=train_cols,dtype=train_cols_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ID=train.iloc[::, -1:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ID","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"date_weight=train.iloc[::, 0:2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_resp=train.iloc[::, 6:-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"updated_train = pd.merge(date_weight, features_resp, left_index=True, right_index=True, how='inner')\nupdated_train2 = pd.merge(ID, updated_train, left_index=True, right_index=True, how='inner')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"updated_train2[:5588]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"updated_train2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntrend=updated_train2[['date','resp','feature_0']]\ndf1=trend.groupby(['date']).resp.mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plotting overall market trends by date using mean and sum"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot with subplots\ndf1.plot(subplots=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df1[0:100].plot(subplots=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df1[100:200].plot(subplots=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df1[200:300].plot(subplots=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df1[300:400].plot(subplots=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df1[400:500].plot(subplots=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2=trend.groupby(['date']).resp.sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Plot with subplots\ndf2.plot(subplots=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df2[0:100].plot(subplots=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df2[100:200].plot(subplots=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df2[200:300].plot(subplots=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df2[300:400].plot(subplots=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df2[400:500].plot(subplots=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df2.sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Time Series Analysis EDA \n\nTrend of gains over time. \n"},{"metadata":{},"cell_type":"markdown","source":"# Aggregated Returns Over Time Trends and Seasonality"},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.seasonal import seasonal_decompose\ndf3=pd.DataFrame(df2.copy())\ndf3['resp'] = df3['resp'].cumsum()\n\nresult = seasonal_decompose(df3, model=\"add\", freq = 88)\nfig = result.plot()\n\n#result = seasonal_decompose(df1, freq = 88)\n#df1[\"trend\"]=result.trend\n#df1[\"seasonal\"]=result.seasonal","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Additive Seasonal Effect (add) means the peaks and valleys are someone similar over time. \n#Multiplicative Seasonal Effect (mul) means the peaks and valleys increase over time. \nresult = seasonal_decompose(df3, model=\"add\", freq = 88)\ndf3[\"trend\"]=result.trend\ndf3[\"seasonal\"]=result.seasonal","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df3.plot(figsize = (14,6), grid = True);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Returns Trends and Seasonality"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf4=pd.DataFrame(df2.copy())\n\n\nresult = seasonal_decompose(df4, model=\"add\", freq = 88)\nfig = result.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = seasonal_decompose(df4, model=\"add\", freq = 88)\ndf4[\"trend\"]=result.trend\ndf4[\"seasonal\"]=result.seasonal\ndf4.plot(figsize = (14,6), grid = True);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Find NaN Values"},{"metadata":{"trusted":false},"cell_type":"code","source":"###Explore NaN Values###\n\nnans=pd.DataFrame(updated_train2.isnull().sum(axis = 0))\nnans.reset_index(drop=False, inplace=True)\nnans.columns = ['column','nans_num']\nhigh_nans=nans[nans['nans_num']>100000]\nnans=nans[nans['nans_num']>0]\nprint(high_nans.sort_values(by=['nans_num'], ascending=False))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Replace NaN Values with date Column Means (Hashed Out)"},{"metadata":{"trusted":false},"cell_type":"code","source":"nans_list=nans[['column']]\narr=np.ravel(np.array(nans_list.astype(str)))\nnans_list2 = arr.tolist()\nnans_list2 ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Replace the NaNs in column by the mean of values\n# in column nans_list2 respectively\n#updated_train2[nans_list2] = updated_train2[nans_list2].fillna(value=updated_train2[nans_list2].mean())\n#print(updated_train2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"updated_train2.isnull().sum(axis = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Replace NaN's with Conditional Means based on Column and Date"},{"metadata":{"trusted":false},"cell_type":"code","source":"means = updated_train2.groupby(['date'])[nans_list2].mean()\nupdated_train2 = updated_train2.set_index(['date'])\nupdated_train2[nans_list2] = updated_train2[nans_list2].fillna(means)\nupdated_train2 = updated_train2.reset_index()\nprint(updated_train2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"updated_train2.isnull().sum(axis = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"means","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"### Audit Conditional Means to Confirm they Remain the Same\nupdated_train2[updated_train2['date']==3].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Splitting Data into 5 Folds Based on Date to Get More Variation in Conditional Means:\nThis is not complete yet. But, the idea is to hash out conditional mean imputation above and to spit data into 5 folds based on date before calculating conditional mean to get more variation since so many values are missing. "},{"metadata":{"trusted":false},"cell_type":"code","source":"updated_train2.date=pd.to_numeric(updated_train2.date, downcast='integer')\nupdated_train2.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y=updated_train2.date\nX=updated_train2.date\n# Use stratified k-fold to create multiple datasets with date structure in place\nkfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n# enumerate the splits and summarize the distributions\ntest_list=[]\nfor train_ix, test_ix in kfold.split(X, y):\n    test_list.append(test_ix)\n\n\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_df_list=[]\nfor i in range(5):\n    df=updated_train2.iloc[list(test_list[i])]\n    train_df_list.append(df)\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_df_list[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}