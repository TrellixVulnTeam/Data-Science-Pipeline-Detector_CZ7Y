{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport gc, pickle, os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport janestreet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dfの各列の型を設定しメモリ軽減\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/jane-street-market-prediction/train.csv')\n# 欠損値の補完(前の値で補完する)\ntrain.fillna(method = 'ffill', inplace=True) \ntrain.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"resp_params = (train['resp'].mean(), train['resp'].std())\nresp_standardized = ((train['resp'] - resp_params[0])/resp_params[1]).values\nresp_info = (resp_params, resp_standardized)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 列を取得\ncolumns = train.columns.drop(['date', 'resp_1', 'resp_2', 'resp_3', 'resp_4', 'resp', 'ts_id'])\n\n# 基準化\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nsc.fit(train[columns])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Z = sc.transform(train[columns])\ntrain = pd.DataFrame(Z, columns=columns)\n# メモリ対策\ntrain = reduce_mem_usage(train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 主成分分析"},{"metadata":{"trusted":true},"cell_type":"code","source":"import sklearn\nfrom sklearn.decomposition import PCA\n\n# 主成分分析\npca = PCA()\npca.fit(train.drop(['weight'],axis=1).values)\n\n# データを主成分空間に写像\nscore = pca.transform(train.drop(['weight'],axis=1).values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# respと主成分スコアを1つのdfにまとめる\ntarget = pd.DataFrame(np.concatenate([resp_info[1][:, np.newaxis], score[:, :16]], axis=1))\ntarget.columns = pd.Index(['resp'] + ['PC{}'.format(i+1) for i in range(16)])\n\n# 'weight'を追加\ntarget = pd.concat([target, train['weight']], axis=1).copy()\n\n# メモリ対策\ndel score\ndel train\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# K-Means"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans # K-means\nkmeans_model = KMeans(n_clusters=5, random_state=0).fit(target.iloc[:, 1:]) # resp以外でクラスタリング\n\n# 結果をdfにまとめる\nkm_result = pd.concat([target, pd.DataFrame(kmeans_model.labels_, columns=['cluster'])],axis=1)\nkm_result['resp_pn'] = km_result['resp'].apply(lambda x:'p' if x>0 else 'n')\nkm_result.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#modelを保存しておく\n#with open('./kmeans_model.pickle', 'wb') as f: pickle.dump(kmeans_model,f)\n\n#結果を保存しておく\n#km_result['resp_pn'] = km_result['resp'].apply(lambda x:'p' if x>0 else 'n') km_result.to_pickle('./km_result.pickle')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# モデル構築"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 学習デートと検証データに分ける(時系列データのため、直近2割を検証用)\nfrom sklearn.model_selection import train_test_split\ntrain_data, valid_data = train_test_split(km_result, shuffle=False, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\nlgb_train = lgb.Dataset(train_data.drop(['resp', 'resp_pn'], axis=1), train_data['resp'])\nlgb_eval = lgb.Dataset(valid_data.drop(['resp', 'resp_pn'], axis=1), valid_data['resp'])\n\n# LightGBM parameters\nparams = {\n        'task': 'train',\n        'boosting_type': 'gbdt',\n        'objective': 'regression', # 目的 : 回帰  \n        'metric': {'rmse'}, # 評価指標 : rsme(平均二乗誤差の平方根)\n        'num_iteration': 10000, #10000回学習\n        'verbose': 0\n}\n\n# モデルの学習\nmodel = lgb.train(params, # パラメータ\n            train_set=lgb_train, # トレーニングデータの指定\n            valid_sets=lgb_eval # 検証データの指定\n            #early_stopping_rounds=100 # 100回ごとに検証精度の改善を検討　→ 精度が改善しないなら学習を終了(過学習に陥るのを防ぐ)\n            )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#modelを保存しておく\n#with open('./lgb_model.pickle', 'wb') as f: pickle.dump(model,f)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 検証データについて"},{"metadata":{"trusted":true},"cell_type":"code","source":"mu, sigma = resp_info[0]\nvalid_predict = model.predict(valid_data.drop(['resp', 'resp_pn'], axis=1)) * sigma + mu","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ヒストグラムを書いてみる\nplt.figure()\nplt.subplot(1, 2, 1)\nplt.hist(valid_predict, bins=50, label='valid_predict', color='blue')\nplt.title('predict')\n\nplt.subplot(1, 2, 2)\nvalid_data['resp'].hist(bins=50, histtype='step', label='valid_resp', color='red')\nplt.title('true resp')\n\n#plt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"形状は似通っているように見える"},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_data['predict'] = valid_predict * sigma + mu\n\n# actionを決める為、thresholdを設定する(予測が正だったものから1%刻みのパーセンタイルとする)\nscore_data = {}\nfor i in range(100):\n    threshold = np.percentile(valid_data.loc[valid_data['predict'] > 0]['predict'], i)\n    score = valid_data['resp'].loc[valid_data['predict'] > threshold].sum()\n    # save\n    score_data[i] = [threshold, score]\n\n# total scoreのthreshold毎の推移\nplt.plot([score for _, score in score_data.values()], label='total score')\nplt.title('total score in each thresholds')\nplt.xlabel('threshold')\nplt.ylabel('total score')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_score_point = np.argmax([score for _, score in score_data.values()])\nbest_score = score_data[best_score_point][1]\nbest_score_threshold = score_data[best_score_point][0]\nprint('best score: {}'.format(best_score))\nprint('best score point: {}'.format(best_score_point))\nprint('best threshold: {}'.format(best_score_threshold))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# テストデータの推定"},{"metadata":{"trusted":true},"cell_type":"code","source":"env = janestreet.make_env()\niter_test = env.iter_test()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"first_step = True\nsecond_step = False\n\nfor (test_df, sample_prediction_df) in iter_test:\n    null_pos = test_df.isnull().values #　欠損値の位置(True or Flaseの配列)\n    with_null = null_pos.any() # 欠損値の判定\n    # 最初の欠損値の処理:actionをしないでスキップ\n    if first_step:\n        if with_null:\n            sample_prediction_df[\"action\"] = 0\n            env.predict(sample_prediction_df)\n        else:\n            first_step = False\n            second_step = True  \n\n    # 欠損値が無いデータ以降の処理(※途中、欠損値を含む)\n    if second_step:\n        if with_null:\n            # 欠損値を前のレコードの値で埋める\n            null_columns = np.where(null_pos)[1]\n            test_df.iloc[:, null_columns] = test_df_prv.iloc[:, null_columns].values\n        \n        # 前レコードを保存\n        test_df_prv = test_df.copy()\n        \n        if test_df['weight'].items() == 0:\n            sample_prediction_df[\"action\"] = 0\n            env.predict(sample_prediction_df)\n            continue\n        \n        # 正規化\n        Z = sc.transform(test_df[columns])\n        \n        # 主成分分析:データを主成分空間に写像\n        score_test = pca.transform(Z[:, 1:])\n\n        # weightを追加する\n        score_test = np.append(score_test[:, :16], Z[:, 0].item())\n\n        # クラスター番号(予測値)を追加する\n        cluster_num = kmeans_model.predict(score_test[np.newaxis, :])\n\n        # respの推定\n        y_pred = np.dot(model.predict(np.append(score_test, cluster_num)[np.newaxis, :]), sigma) + mu\n\n        # action{0, 1}に変換\n        action = 1 if y_pred > best_score_threshold else 0\n\n        # 結果を格納\n        sample_prediction_df[\"action\"] = action\n        env.predict(sample_prediction_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}