{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport pickle\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dfの各列の型を設定しメモリ軽減\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## データの標準化"},{"metadata":{"trusted":true},"cell_type":"code","source":"# read data\ntrain = pd.read_csv('/kaggle/input/jane-street-market-prediction/train.csv')\ntest = pd.read_csv('/kaggle/input/jane-street-market-prediction/example_test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 欠損値の補完(前の値で補完する)\ntrain.fillna(method = 'ffill', inplace=True) \ntrain.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# respについて\nresp_params = (train['resp'].mean(), train['resp'].std())\nresp_standardized = ((train['resp'] - resp_params[0])/resp_params[1]).values\nresp_info = (resp_params, resp_standardized)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 基準化\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nsc.fit(train[test.columns])\nZ = sc.transform(train[test.columns])\ntrain = pd.DataFrame(Z, columns=test.columns) # dfの整形\ntrain = reduce_mem_usage(train) # メモリ対策","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 保存\nimport pickle\ntrain.to_pickle('/kaggle/working/train_standardized_without_null.pickle')\nwith open('/kaggle/working/SC.pickle', 'wb') as f:\n    pickle.dump(sc, f)\nwith open('/kaggle/working/resp_info.pickle', 'wb') as f:\n    pickle.dump(resp_info, f)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 主成分分析(PCA)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA \nimport matplotlib.pyplot as plt\n\n# 特徴量のみにする\nX = train.drop(['weight', 'date', 'ts_id'],axis=1).values\n\n# 主成分分析\npca = PCA()\npca.fit(X)\n# データを主成分空間に写像(主成分スコア)\nscore = pca.transform(X)\n\n# 累積寄与率を図示する\nplt.plot([0] + list( np.cumsum(pca.explained_variance_ratio_)), \"-o\")\nplt.xlabel(\"Number of principal components\")\nplt.ylabel(\"Cumulative contribution rate\")\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> 上図から概ね第30主成分程度で特徴量全体の90%を表現できていることがわかる。調べてみたら第16主成分で80%、第20主成分で85%、第28主成分目で90%を上回っていた。"},{"metadata":{},"cell_type":"markdown","source":"## リターンと各主成分の関係"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\n# respと主成分スコアの関係\n# df作成\ntarget = pd.DataFrame(np.concatenate([resp_info[1][:, np.newaxis], score[:, :16]], axis=1))\ntarget.columns = pd.Index(['resp'] + ['PC{}'.format(i+1) for i in range(16)])\n# ヒートマップ作成\nsns.heatmap(target.corr())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"各主成分やrespの間に相関は低いことがわかる。"},{"metadata":{},"cell_type":"markdown","source":"## 主成分回帰モデル(PCRモデル)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.api as sm\n\n# 'weight'を追加\ntarget = pd.concat([target, train['weight']], axis=1).copy()\ntarget.head()\n\n# PCR\nx = target.drop(['resp'], axis=1)\nx = sm.add_constant(x)\nmodel = sm.OLS(target['resp'], x)\nresult = model.fit()\nprint(result.summary())\n\n# グラフを書く\nplt.plot(target['resp'], label='resp', linestyle='--')\nresult.fittedvalues.plot(label='fitted', style=':')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"全期間で線形回帰を行った結果、決定係数は0.002と酷い。全期間のデータを使うのは賢明ではなさそう。周期性などを考慮したほうが良いのかもしれない。次に、最初の100個を対象に最適化を行った結果は以下の通りである。"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# PCR\nx = target.drop(['resp'], axis=1)\nx = sm.add_constant(x)\nmodel = sm.OLS(target['resp'][:100], x[:100])\nresult = model.fit()\nprint(result.summary())\n\n# グラフを書く\nplt.plot(target['resp'][:100], label='resp', linestyle='--')\nresult.fittedvalues.plot(label='fitted', style=':')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"そこそこフィットしている様子が見られる。データを少なくすればフィットするようである。期間を複数に分け、各期間ごとで最適なモデルを採用するようなものにすると良いのかもしれない。分析に入る前に、リターンの特徴(周期性など)を最初に見るべきだった。"},{"metadata":{},"cell_type":"markdown","source":"## クラスタリング(K-means)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans # K-means\nkmeans_model = KMeans(n_clusters=5, random_state=0).fit(target.iloc[:, 1:]) # resp以外\nkm_result = pd.concat([target, pd.DataFrame(kmeans_model.labels_, columns=['cluster'])],axis=1)\nkm_result.groupby('cluster').describe()['resp']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#respを正負に変換し、クラスタごとにカウントしてみる。\nkm_result['resp_pn'] = km_result['resp'].apply(lambda x:'p' if x>0 else 'n')\nkm_result.groupby(['cluster', 'resp_pn']).count()['resp'].plot.bar(color=['blue', 'red'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"カウントベースでみると、クラスター1と3ではマイナスの方が数が多いことがわかる。クラスター1については、平均もマイナスであり、マイナスの特性を持ちそうだ。"},{"metadata":{"trusted":true},"cell_type":"code","source":"# クラスター数:3\nkmeans_model = KMeans(n_clusters=3, random_state=0).fit(target.iloc[:, 1:]) # resp以外\nkm_result = pd.concat([target, pd.DataFrame(kmeans_model.labels_, columns=['cluster'])],axis=1)\nkm_result.groupby('cluster').describe()['resp']\nkm_result['resp_pn'] = km_result['resp'].apply(lambda x:'p' if x>0 else 'n')\nkm_result.groupby(['cluster', 'resp_pn']).count()['resp'].plot.bar(color=['blue', 'red'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# クラスター数:10\nkmeans_model = KMeans(n_clusters=3, random_state=0).fit(target.iloc[:, 1:]) # resp以外\nkm_result = pd.concat([target, pd.DataFrame(kmeans_model.labels_, columns=['cluster'])],axis=1)\nkm_result.groupby('cluster').describe()['resp']\nkm_result['resp_pn'] = km_result['resp'].apply(lambda x:'p' if x>0 else 'n')\nkm_result.groupby(['cluster', 'resp_pn']).count()['resp'].plot.bar(color=['blue', 'red'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"クラスターで分けることでマイナスの特徴を捉えることができている可能性がある。また、クラスター数は、３程度でも十分と思われる。ひとまず、今回の結果を踏まえると、モデル開発においてクラスター番号を入れてみる価値はありそうだ。ただ、ポジティブの特徴について見ると、クラスター数5の(2,p)が捉えることができている可能性があるのを踏まえると、クラスター数は5としてみようと思う。"},{"metadata":{},"cell_type":"markdown","source":"## 主成分分析＋クラスタリング"},{"metadata":{"trusted":true},"cell_type":"code","source":"# PCR + Clustering\nx = km_result.drop(['resp', 'resp_pn'], axis=1)\nx = sm.add_constant(x)\nmodel = sm.OLS(km_result['resp'][:100], x[:100])\nresult = model.fit()\nprint(result.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"あまり改善はされていない。変数が一つ増えたことで、補正Ｒ２は低下している。\nそもそも主成分分析を行い、次元数を縮小させることに価値があるのか確認のため、元の特徴量(feature_X)で全期間に対して線形回帰を行ってみた。"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}