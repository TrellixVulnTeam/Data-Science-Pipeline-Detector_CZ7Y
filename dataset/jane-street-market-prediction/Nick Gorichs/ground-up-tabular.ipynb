{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport random\nimport math","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\n    \ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n#     print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n#     print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n#     print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_embeddings(cats, df):\n    cat_info = {}\n    for cat in cats:\n        cat_info[cat] = df[cat].nunique()-1\n\n    emb_dims = {}\n    for key in cat_info.keys():\n        cardin = df[key].nunique()\n        layer_tensor = torch.tensor([cardin,50 if cardin>50 else cardin//2]) # \"The rule of thumb for determining the embedding size is the cardinality size divided by 2, but no bigger than 50.\" https://forums.fast.ai/t/size-of-embedding-for-categorical-variables/42608/4\n        emb_dims[key] = layer_tensor\n\n    emb_layers = nn.ModuleList(nn.Embedding(x,y) for x,y in emb_dims.values())\n    summ = sum(y for x,y in emb_dims.values())\n    return emb_layers,summ\n\n\ndef cat_transform(cats,df):\n    for cat in cats:\n        for idx,label in zip(range(df[cat].nunique()),df[cat].unique()):\n            df.loc[df[cat]==label,cat] = idx\n        df[cat] = df[cat].astype('int')\n    df = reduce_mem_usage(df)\n    return df\n\n\ndef create_dataloaders(df, target, bs, valid_idx):\n    train = df.loc[~df.index.isin(valid_idx),:]\n    x_train = train.loc[:,train.columns!=target]\n    y_train = train.loc[:,target]\n    \n    valid = df.loc[df.index.isin(valid_idx),:]\n    x_valid = valid.loc[:,valid.columns!=target]\n    y_valid = valid.loc[:,target]\n    \n    x_train, y_train = torch.from_numpy(x_train.values), torch.from_numpy(y_train.values)\n    x_valid, y_valid = torch.from_numpy(x_valid.values), torch.from_numpy(y_valid.values)\n    \n    train_ds,valid_ds = Dataset(x_train,y_train),Dataset(x_valid,y_valid)\n    train_dl,valid_dl = Dataloader(train_ds,bs),Dataloader(valid_ds,bs)\n    return train_dl,valid_dl\n    \n    \nclass Dataset():\n    def __init__(self,x,y):\n        self.x = x\n        self.y = y\n    def __len__(self):\n        return len(self.x)\n    def __getitem__(self,i):\n        return self.x[i],self.y[i]\n    \n    \nclass Dataloader():\n    def __init__(self,ds,bs):\n        self.ds = ds\n        self.bs = bs\n    def __iter__(self):\n        for batch in range(0,len(self.ds),self.bs):\n            yield self.ds[batch:batch+self.bs]\n    def __len__(self):\n        len_dl = len(self.ds)//self.bs\n        len_dl += 1 if len(self.ds)%self.bs != 0 else 0\n        return len_dl\n    \n\n\n# def accuracy(preds,targs): return (preds.round().int()==targs).float().mean()\ndef accuracy(preds,targs): return (preds.argmax(dim=-1)==targs).float().mean()\n\ndef fit(epochs, loss_func, acc_metric):\n    for epoch in range(epochs):\n        for x_batch,y_batch in train_dl:\n            preds = model(x_batch.float())\n            loss = loss_func(preds, y_batch.long())\n            loss.backward()\n            opt.step()\n            opt.zero_grad()\n        with torch.no_grad():\n            tot_loss, tot_acc = 0,0\n            for x_b,y_b in valid_dl:\n                predicts = model(x_b.float())\n                tot_loss += loss_func(predicts,y_b.long())\n                tot_acc += acc_metric(predicts,y_b)\n            print(f'Epoch {epoch} -> loss: {tot_loss/len(valid_dl):.13f}  accuracy: {tot_acc/len(valid_dl):.13f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Config():\n    def __init__(self, cols):\n        self.lr = 0.1\n        inp_layers = cols\n        hid_layers = 2\n        out_layers = 2\n        bs = 64\n        targ = 'Survived'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class tabular_model(nn.Module):\n    def __init__(self, df, cats, targs, config):\n        self.config = config\n        self.embeds,self.summ = create_embeddings(cats, df)\n        self.model = model(self.embeds, config)\n        self.opt = opt(self.model.parameters(), lr=lr)\n        self.cats_len = cats\n        self.conts_len = len(df.columns) - len(self.cats) - 1 # - 1 for target column\n        \n    def forward(self, cats, conts):\n        if self.cats != 0:\n            x = [emb(cats[:,idx]) for idx,emb in enumerate(self.embeds)]\n            x = torch.cat(x, 1)\n        if self.conts != 0:\n            x = torch.cat([x,conts], 1) if self.num_embeds != 0 else conts\n        return self.layers(x)\n    \n    def model(embs, inp,nh,outp,lr):\n        return nn.Sequential(embs, nn.Linear(inp,nh),nn.ReLU(),nn.Linear(nh,outp))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = reduce_mem_usage(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/titanic/train.csv').drop(['Name','Ticket'],axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/titanic/train.csv').drop(['Name','Ticket'],axis=1)\ndf.loc[df['Age'].isna(),'Age'] = round(df['Age'].mean()); df\n\nlen_feats = df.columns.size\nconfig = Config(feats)\n\nvalid_idx = random.sample(list(df.index), int(0.33*len(df)))\ncategories = ['PassengerId','Survived','Sex','Pclass','Cabin','Embarked']\ndf = cat_transform(categories,df)\nemb_layers,summ = create_embeddings(categories,df)\ntrain_dl, valid_dl = create_dataloaders(df, target, batch_size, valid_idx)\nrows,cols = df.shape\n\nloss = F.cross_entropy\nmodel,opt = a_model(emb_layers,inp=cols,nh=2,outp=2,lr=0.1)\nfit(10, loss, accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nn.Sequential(emb_layers,nn.Linear(10,2),nn.ReLU(),nn.Linear(2,2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emb_layers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# FastAI version","metadata":{}},{"cell_type":"code","source":"from fastai.tabular.all import *\n\ndef accuracy(preds,targs):\n    return (preds.round().int()==targs).float().mean()\n\nvalid_idx = list(df.iloc[round((len(df)-len(df)*0.2)):,:].index)\ncat_names = ['Survived','Name','Sex','Age','SibSp','Parch','Ticket','Fare','Cabin','Embarked']\ncont_names = list(df.loc[:,[item not in cat_names for item in list(df.columns)]].columns)\nprocs = [Categorify, FillMissing, Normalize]\ndls = TabularDataLoaders.from_df(df, procs=procs, cat_names=cat_names, cont_names=cont_names, \n                                 y_names='Survived', valid_idx=valid_idx, bs=32)\nlearn = tabular_learner(dls, metrics=[accuracy])\nlearn.lr_find()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.fit_one_cycle(10,lr_max=0.006)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from fastai.tabular.all import *\n\nvalid_idx = list(df.iloc[round((len(df)-len(df)*0.2)):,:].index)\ncat_names = ['Class']\ncont_names = list(df.iloc[:,:-1].columns)\nprocs = [Categorify, FillMissing, Normalize]\ndls = TabularDataLoaders.from_df(df, procs=procs, cat_names=cat_names, cont_names=cont_names, \n                                 y_names=\"Class\", valid_idx=valid_idx, bs=64)\nlearn = tabular_learner(dls, y_range=torch.tensor([1,2]), metrics=[accuracy])\nlearn.lr_find()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.show_training_loop()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds,targs = learn.get_preds()\npreds,targs","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/creditcardfraud/creditcard.csv')\n\ntrain_idx = random.sample(list(df.index),round(len(df)*.8))\ntrain_idx.sort()\nx_train = df.iloc[train_idx,:-1]\ny_train = df.iloc[train_idx,-1]\nx_valid = df.iloc[~df.index.isin(train_idx),:-1]\ny_valid = df.iloc[~df.index.isin(train_idx),-1]\nx_row,x_col = x_train.shape\n\npos = df[df['Class']==1]['Class'].count()\n\nprint(f'Number of positives: {pos}')\nprint(f'Percentage of training set: {round(pos/len(y_train)*100,5)}%')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}