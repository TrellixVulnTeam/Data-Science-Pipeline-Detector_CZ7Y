{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"code","source":"import torch\nimport numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndef make_layer(in_feature, out_feature):\n    layer = nn.Sequential(*[\n            nn.Linear(in_feature, out_feature),\n            nn.BatchNorm1d(out_feature),\n            nn.ReLU()\n        ])\n    return layer\n\nclass Hardswish(nn.Module):\n    def __init__(self, inplace=False):\n        super(Hardswish, self).__init__()\n\n    def forward(self, x):\n        return (x > 3).to(dtype=torch.float) * x + (x > -3).to(dtype=torch.float) * (x < 3).to(dtype=torch.float) * (x*(x+3)/6)\n\nclass BottleneckEncoder(nn.Module):\n    def __init__(self, in_feature, dev):\n        super(BottleneckEncoder, self).__init__()\n        self.dev = dev\n        self.in_bn = nn.BatchNorm1d(in_feature)\n        self.fc1 = nn.Linear(in_feature, 640)\n        self.act1 = nn.ReLU(inplace=True)\n        # self.bn1 = nn.BatchNorm1d(128)\n        self.dp1 = nn.Dropout(p=0.2)\n\n        self.out_fc = nn.Linear(640, in_feature)\n        # self.fc2 = nn.Linear(128, 64)\n        # self.act2 = nn.ReLU(inplace=True)\n        # self.bn2 = nn.BatchNorm1d(64)\n        # self.dp2 = nn.Dropout(p=0.2)\n\n        # self.fc3 = nn.Linear(64, 128)\n        # self.act3 = nn.ReLU(inplace=True)\n        # self.bn3 = nn.BatchNorm1d(128)\n        # self.dp3 = nn.Dropout(p=0.2)\n\n        # self.out_fc = nn.Linear(128, in_feature)\n\n    def forward(self, x):\n        x = self.in_bn(x)\n        x = x + self.dev * torch.randn(*x.size()).to(x.device)\n        x = self.dp1(self.act1(self.fc1(x)))\n\n        return self.out_fc(x), x\n\nclass AEClassifier(nn.Module):\n    def __init__(self, in_feature, out_feature):\n        super(AEClassifier, self).__init__()\n        self.fc1 = nn.Linear(in_feature, 320)\n        self.act1 = nn.ReLU()\n        self.bn1 = nn.BatchNorm1d(320)\n        self.dp1 = nn.Dropout(p=0.2)\n\n        self.out_fc = nn.Linear(320, out_feature)\n\n    def forward(self, x):\n        x = self.dp1(self.bn1(self.act1(self.fc1(x))))\n\n        return torch.sigmoid(self.out_fc(x))\n\n\n\nclass MLPClassifier(nn.Module):\n    def __init__(self, in_feature, out_feature):\n        super(MLPClassifier, self).__init__()\n        self.in_bn = nn.BatchNorm1d(in_feature)\n        self.in_dp = nn.Dropout(p=0.2)\n\n        self.fc1 = nn.Linear(in_feature, 256)\n        self.bn1 = nn.BatchNorm1d(256)\n        self.act1 = Hardswish(inplace=True)\n        self.dp1 = nn.Dropout(p=0.2)\n\n        self.fc2 = nn.Linear(256, 128)\n        self.bn2 = nn.BatchNorm1d(128)\n        self.act2 = Hardswish(inplace=True)\n        self.dp2 = nn.Dropout(p=0.2)\n\n        self.fc3 = nn.Linear(128, 64)\n        self.bn3 = nn.BatchNorm1d(64)\n        self.act3 = Hardswish(inplace=True)\n        self.dp3 = nn.Dropout(p=0.2)\n\n        self.out_fc = nn.Linear(64, out_feature)\n\n    def forward(self, x):\n        x = self.in_dp(self.in_bn(x))\n        x = self.dp1(self.act1(self.bn1(self.fc1(x))))\n\n        x = self.dp2(self.act2(self.bn2(self.fc2(x))))\n\n        x = self.dp3(self.act3(self.bn3(self.fc3(x))))\n        return torch.sigmoid(self.out_fc(x))\nclass MLP(nn.Module):\n    '''\n    layer_setting[list]: [130,64,2]\n    '''\n    def __init__(self, layer_setting=[130,64,2]):\n        super(MLP, self).__init__()\n        self.layers = nn.Sequential(*[make_layer(layer_setting[i], layer_setting[i+1]) for i in range(len(layer_setting)-2)])\n        self.out_layer = nn.Linear(layer_setting[-2], layer_setting[-1])\n\n    def forward(self, x):\n        \n        return torch.sigmoid(self.out_layer(self.layers(x)))\n\nclass AutoEncoder(nn.Module):\n    def __init__(self, layer_setting):\n        super(AutoEncoder, self).__init__()\n        self.layers = nn.ModuleList([make_layer(layer_setting[i], layer_setting[i+1]) for i in range(len(layer_setting)-2)])\n        self.out_layer = nn.Linear(layer_setting[-2], layer_setting[-1])\n\n    def forward(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        return self.out_layer(x)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAINING = False\ngpu_device = torch.device('cuda:0')\nfeat_tags = ['feature_{}'.format(i) for i in range(130)]\nresp_tags = ['resp'] + ['resp_{}'.format(i) for i in range(1,5)]\nepsilons = [0] * len(resp_tags)\nnum_epoch = 1\nlr = 1e-3\nbatch_size = 4096\nloader_num_workers = 8\nprint_every = 100\nsave_every = 1000\n\n\nFOLDS = 5\n\ntrain = pd.read_csv('../input/jane-street-market-prediction/train.csv')\ntrain = train.query('date > 85').reset_index(drop = True) \ntrain = train.astype({c: np.float32 for c in train.select_dtypes(include='float64').columns}) #limit memory use\ntrain.fillna(train.mean(),inplace=True)\ntrain = train.query('weight > 0').reset_index(drop = True)\n#train['action'] = (train['resp'] > 0).astype('int')\ntrain['action'] =  (  (train['resp_1'] > 0.00001 ) & (train['resp_2'] > 0.00001 ) & (train['resp_3'] > 0.00001 ) & (train['resp_4'] > 0.00001 ) &  (train['resp'] > 0.00001 )   ).astype('int')\nf_mean = np.mean(train[feat_tags[1:]].values,axis=0)\nprint(train.columns)\nprint('Whole Shape {}'.format(train.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nin_feat = len(feat_tags)\nout_feat = len(resp_tags)\ndev = 0.1\nFOLDS = 5\nae = BottleneckEncoder(in_feat, dev) \n# Final Classifier\ncls_models = [MLPClassifier(640+in_feat, out_feat) for i in range(FOLDS)]\n\nrestore_path = '../input/js-bottleneck/5_7_half_ae_ckpt.pth.tar'\nif os.path.exists(restore_path):\n    # restore_path = self.args.restore_checkpoint_path\n    print('Restoring from checkpoint:{}'.format(restore_path))\n    checkpoint = torch.load(restore_path, map_location=gpu_device)\n    #     if 'net_classifier_state' in checkpoint and checkpoint['net_classifier_state'] is not None:\n    #         classifier.load_state_dict(checkpoint['net_classifier_state'])\n    #         print('Loading Classifier.')\n    if 'net_ae_state' in checkpoint and checkpoint['net_ae_state'] is not None:\n        ae.load_state_dict(checkpoint['net_ae_state'])\n        print('Loading AutoEncoder.')\n        \nmodels_restore_path = '../input/js-bottleneck/5_7_half_models_ckpt.pth.tar'\nif os.path.exists(models_restore_path):\n    checkpoint = torch.load(models_restore_path, map_location=gpu_device)\n    for i, cls_model in enumerate(cls_models):\n        cls_model.load_state_dict(checkpoint[i])\n        cls_model.eval()\n        print('Loading Classifiers.')\nae.eval()\n\n\ncls_models = [cls_models[i] for i in [0,2,3,4]]\nimport janestreet\n# janestreet.competition.make_env.__called__ = False\nenv = janestreet.make_env()\nth = 0.5\n#w = np.asarray([0.1,0.1,0.1,0.5,0.2])\nfor (test_df, pred_df) in env.iter_test():\n    if test_df['weight'].item() > 0:\n        x_tt = test_df.loc[:, feat_tags].values\n        if np.isnan(x_tt[:, 1:].sum()):\n            x_tt[:, 1:] = np.nan_to_num(x_tt[:, 1:]) + np.isnan(x_tt[:, 1:]) * f_mean\n        x_tt = torch.from_numpy(x_tt).to(dtype=torch.float)\n        x_recon, x_enc = ae(x_tt)\n        x_con = torch.cat([x_tt, x_enc], dim=1)\n        pred = torch.cat([cls_model(x_con) for cls_model in cls_models], dim=1) #classifier(x_con)\n        pred_df.action = int((pred.mean(dim=1) > th).to(dtype=torch.float).item())\n    else:\n        pred_df.action = 0\n    env.predict(pred_df)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}