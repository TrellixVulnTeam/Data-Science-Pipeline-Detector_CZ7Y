{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport time\nimport janestreet\n\nimport os, gc\n#import cudf\nimport pandas as pd\nimport numpy as np\n#import cupy as cp\nimport janestreet\nimport xgboost as xgb\nfrom hyperopt import hp, fmin, tpe, Trials\nfrom hyperopt.pyll.base import scope\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.model_selection import GroupKFold\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nfrom joblib import dump, load\n\nimport tensorflow as tf\ntf.random.set_seed(42)\nimport tensorflow.keras.backend as K\nimport tensorflow.keras.layers as layers\nfrom tensorflow.keras.callbacks import Callback, ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n\ntempo = time.time()\n#dados = cudf.read_csv('/kaggle/input/jane-street-market-prediction/train.csv')\ndados = pd.read_csv('/kaggle/input/jane-street-market-prediction/train.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def verifica_nulos(dataset):\n    rows = []\n    for column in dataset.columns:\n        row = {'coluna': column, 'nans': dataset[column].isnull().sum(), 'frac_nans': dataset[column].isnull().sum() / dataset.shape[0]}\n        #print(row)\n        rows.append(row)  \n\n    res = pd.DataFrame(rows)\n    res[res.nans>0].sort_values('nans', ascending=False)\n    return res\n    \n\n# cria uma lista com as 'features'\nfeatures = [c for c in dados.columns if 'feature' in c]\n\n'''\nprint(dados.shape)\n\n# pega a média de cada 'feature'\n# o 'feature_0' parece ser compra/venda\nf_mean = dados[features[1:]].mean()\n\n#print(dados['weight'].value_counts())\n\n# pega somente as linhas com \"weight\" maior que 0\n# Nota: quem sabe implementar o threshold??\ndados = dados.query('weight > 0').reset_index(drop = True)\n\nprint(dados.shape)\n\n# completa os dados faltantes com a média\ndados[features[1:]] = dados[features[1:]].fillna(f_mean)\n\ndados['action'] = (dados['resp'] > 0).astype('int')\n\n\nprint('Converting...')\ndados = dados.to_pandas()\nf_mean = f_mean.values.get()\nprint(f_mean)\nnp.save('f_mean.npy', f_mean)\n\nprint('Finish.')\n!ls\n\n'''\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_mlp(num_columns, num_labels, hidden_units, dropout_rates, label_smoothing, learning_rate):\n    \n    inp = tf.keras.layers.Input(shape = (num_columns, ))\n    x = tf.keras.layers.BatchNormalization()(inp)\n    x = tf.keras.layers.Dropout(dropout_rates[0])(x)\n    for i in range(len(hidden_units)): \n        x = tf.keras.layers.Dense(hidden_units[i])(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n        x = tf.keras.layers.Dropout(dropout_rates[i+1])(x)    \n        \n    x = tf.keras.layers.Dense(num_labels)(x)\n    out = tf.keras.layers.Activation('sigmoid')(x)\n    \n    model = tf.keras.models.Model(inputs = inp, outputs = out)\n    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate),\n                  loss = tf.keras.losses.BinaryCrossentropy(label_smoothing = label_smoothing), \n                  metrics = tf.keras.metrics.AUC(name = 'AUC'), \n                 )\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 4096\nhidden_units = [384, 896, 896, 394]\ndropout_rates = [0.10143786981358652, 0.19720339053599725, 0.2703017847244654, 0.23148340929571917, 0.2357768967777311]\nlabel_smoothing = 1e-2\nlearning_rate = 1e-3\n\n'''\noof = np.zeros(len(dados['action']))\ngkf = GroupKFold(n_splits = 5)\nfor fold, (tr, te) in enumerate(gkf.split(dados['action'].values, dados['action'].values, dados['date'].values)):\n    \n    X_tr, X_val = dados.loc[tr, features].values, dados.loc[te, features].values\n    y_tr, y_val = dados.loc[tr, 'action'].values, dados.loc[te, 'action'].values\n    \n    ckp_path = f'JSModel_{fold}.hdf5'\n    model = create_mlp(X_tr.shape[1], 1, hidden_units, dropout_rates, label_smoothing, learning_rate)\n    rlr = ReduceLROnPlateau(monitor = 'val_AUC', factor = 0.1, patience = 3, verbose = 0, \n                            min_delta = 1e-4, mode = 'max')\n    ckp = ModelCheckpoint(ckp_path, monitor = 'val_AUC', verbose = 0, \n                          save_best_only = True, save_weights_only = True, mode = 'max')\n    es = EarlyStopping(monitor = 'val_AUC', min_delta = 1e-4, patience = 7, mode = 'max', \n                       baseline = None, restore_best_weights = True, verbose = 0)\n    model.fit(X_tr, y_tr, validation_data = (X_val, y_val), epochs = 1000, \n              batch_size = batch_size, callbacks = [rlr, ckp, es], verbose = 0)\n                \n    oof[te] += model.predict(X_val, batch_size = batch_size * 4).ravel()\n    score = roc_auc_score(y_val, oof[te])\n    print(f'Fold {fold} ROC AUC:\\t', score)\n    \n    # Finetune 3 epochs on validation set with small learning rate\n    model = create_mlp(X_tr.shape[1], 1, hidden_units, dropout_rates, label_smoothing, learning_rate / 100)\n    model.load_weights(ckp_path)\n    model.fit(X_val, y_val, epochs = 3, batch_size = batch_size, verbose = 0)\n    model.save_weights(ckp_path)\n    \n    K.clear_session()\n    del model\n    rubbish = gc.collect()\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#score_oof = roc_auc_score(dados['action'].values, oof)\n#print(score_oof)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_models = 1\n\nmodels = []\nfor i in range(num_models):\n    clf = create_mlp(len(features), 1, hidden_units, dropout_rates, label_smoothing, learning_rate)\n    clf.load_weights(f'../input/testeupload/JSModel_{i}.hdf5')\n#     clf.load_weights(f'./JSModel_{i}.hdf5')\n    models.append(clf)\n    \nf_mean = np.load('../input/testeupload/f_mean.npy')\n# f_mean = np.load('./f_mean.npy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env = janestreet.make_env()\nenv_iter = env.iter_test()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nopt_th = 0.5\nfor (test_df, pred_df) in tqdm(env_iter):\n    if test_df['weight'].item() > 0:\n        x_tt = test_df.loc[:, features].values\n        if np.isnan(x_tt[:, 1:].sum()):\n            x_tt[:, 1:] = np.nan_to_num(x_tt[:, 1:]) + np.isnan(x_tt[:, 1:]) * f_mean\n        pred = 0.\n        for clf in models:\n            pred += clf(x_tt, training = False).numpy().item() / num_models\n#         pred = models[0](x_tt, training = False).numpy().item()\n        pred_df.action = np.where(pred >= opt_th, 1, 0).astype(int)\n    else:\n        pred_df.action = 0\n    env.predict(pred_df)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final = time.time() - tempo\nprint(\"%s secs\" % str(final))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}