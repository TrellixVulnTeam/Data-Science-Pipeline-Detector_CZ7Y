{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Import Dependencies"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nimport hyperopt\nfrom hyperopt import fmin, tpe, hp, STATUS_OK, Trials\nfrom hyperopt import tpe\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport janestreet\nimport xgboost as xgb\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Initialize the environment"},{"metadata":{"trusted":true},"cell_type":"code","source":"env = janestreet.make_env() \niter_test = env.iter_test()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Loading training data...')\ntrain = pd.read_csv('../input/jane-street-market-prediction/train.csv')\nprint('Finished.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()\ntrain.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing\n\nThis competition is evaluated on a utility score. Each row in the test set represents a trading opportunity for which you will be predicting an action value, 1 to make the trade and 0 to pass on it. Each trade j has an associated weight and resp, which represents a return."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Preprocessing...')\ntrain = train[train['weight'] != 0] #do not train data with 0 weight\ntrain['action'] = (train['resp'].values > 0).astype('int') \n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_values_count = train.isnull().sum()\nmissing_values_count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train.loc[:, train.columns.str.contains('feature')]\nf_mean = X.mean()\nX.fillna(f_mean) #fill na values with feature mean\n\ny = train.loc[:, 'action']\nprint('Finished.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Split into test and train"},{"metadata":{"trusted":true},"cell_type":"code","source":"#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create Model\n\n![](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2016/07/XGBoost-Plot-of-Single-Decision-Tree-Left-To-Right.png)\n\nModel used [1][2]"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Creating classifier...')\nclf = xgb.XGBClassifier(\n    \n    n_estimators=400,\n    max_depth=7,\n    eta=0.5, \n    missing=None,\n    random_state=42,\n    tree_method='gpu_hist',\n    subsample=0.8,\n    colsample_bytree=1,\n    verbosity=2  \n)\nprint('Finished.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Training classifier...')\nclf.fit(X, y)\n#clf.fit(X_train, y_train)\nprint('Finished.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Score Classifier "},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print('Scoring model...')\n#y_pred = clf.predict(X_test)\n#evaluate predictions\n#accuracy = accuracy_score(y_test, y_pred)\n#print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prepare Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"for (test_df, sample_prediction_df) in iter_test:\n    X_test = test_df.loc[:, test_df.columns.str.contains('feature')]\n    y_preds = clf.predict(X_test)\n    sample_prediction_df.action = y_preds\n    env.predict(sample_prediction_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Hyperopt for Hyperparameter Tuning\n\n\nhp.choice(label, options) — Returns one of the options, which should be a list or tuple.\n\nhp.randint(label, upper) — Returns a random integer between the range [0, upper).\n\nhp.uniform(label, low, high) — Returns a value uniformly between low and high.\n\nhp.quniform(label, low, high, q) — Returns a value round(uniform(low, high) / q) * q, i.e it rounds the decimal values and returns an integer\n\nhp.normal(label, mean, std) — Returns a real value that’s normally-distributed with mean and standard deviation sigma.\n1. [3][4]"},{"metadata":{"trusted":true},"cell_type":"code","source":"#def hyperparameter_tuning(space):\n    #print('Building Model...')\n    #model = xgb.XGBClassifier(\n        #n_estimators=space['n_estimators'],\n        #max_depth=space['max_depth'],\n        #min_child_weight=space['min_child_weight'],\n        #random_state=42,\n        #subsample=space['subsample'],\n        #learning_rate=space['learning_rate'],\n        #gamma=space['gamma'],\n        #colsample_bytree=space['colsample_bytree'],\n        #tree_method='gpu_hist'\n        #)\n\n    #evaluation = [(X_train, y_train), (X_test, y_test)]\n\n    #model.fit(X_train, y_train,\n              #eval_set=evaluation, eval_metric=\"rmse\",\n              #early_stopping_rounds=10, verbose=False)\n    #print('Finished.')\n\n    #pred = model.predict(X_test)\n    #accuracy = accuracy_score(y_test, pred > 0.5)\n    #print(\"SCORE:\", accuracy)\n    # change the metric if you like\n    #return {'loss': -accuracy, 'status': STATUS_OK, 'model': model}\n\n\n#space = {\n        #'max_depth': hp.choice('max_depth', np.arange(10, 20, dtype=int)),\n        #'min_child_weight': hp.quniform('min_child', 1, 30, 1),\n        #'subsample': hp.uniform('subsample', 0.8, 1),\n        #'n_estimators': hp.choice('n_estimators', np.arange(100, 10000, 100, dtype=int)),\n        #'learning_rate': hp.quniform('learning_rate', 0.025, 0.5, 0.025),\n        #'gamma': hp.quniform('gamma', 0.5, 1, 0.05),\n        #'colsample_bytree': hp.quniform('colsample_bytree', 0.5, 1, 0.05)\n    #}\n\n#trials = Trials()\n\n#best = fmin(fn=hyperparameter_tuning,\n                #space=space,\n                #algo=tpe.suggest,\n                #max_evals=10,\n                #trials=trials)\n\n#print(best)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Refrences "},{"metadata":{},"cell_type":"markdown","source":"[1]https://www.kaggle.com/hamditarek/market-prediction-xgboost-with-gpu-fit-in-1min\n\n[2]https://machinelearningmastery.com/evaluate-gradient-boosting-models-xgboost-python/\n\n[3]https://medium.com/analytics-vidhya/hyperparameter-tuning-hyperopt-bayesian-optimization-for-xgboost-and-neural-network-8aedf278a1c9\n\n[4]https://www.kaggle.com/henrylidgley/xgboost-with-hyperopt-tuning\n********"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}