{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Market Prediction Using XGBoost "},{"metadata":{},"cell_type":"markdown","source":"**Market Basics:** Financial market is a dynamic world where investors, speculators, traders, hedgers understand the market by different strategies and use the opportunities to make profit. They may use fundamental, technical analysis, sentimental analysis,etc. to place their bet. As data is growing, many professionals use data to understand and analyse previous trends and predict the future prices to book profit.\n\n**Competition Description:** The dataset provided contains set of features, **feature_{0...129}**,representing real stock market data. \nEach row in the dataset represents a trading opportunity, for which we will be predicting an action value: 1 to make the trade and 0 to pass on it. \nEach trade has an associated weight and resp, which together represents a return on the trade. \nIn the training set, **train.csv**, you are provided a **resp** value, as well as several other **resp_{1,2,3,4}** values that represent returns over different time horizons.\n\nIn **Test set** we don't have **resp** value, and other **resp_{1,2,3,4}** data, so we have to use only **feature_{0...129}** to make prediction.\n\nTrades with **weight = 0** were intentionally included in the dataset for completeness, although such trades **will not** contribute towards the scoring evaluation. So we will ignore it.\n\n**XGBoost Classification** is used here with hyperparamter tuning. Please go through the notebook, I have tried to explain every step. If you find this notebook helpful please **UPVOTE** it!ðŸ˜Š\n\nComments, suggestions, and queries are appreciated. Happy Learning!ðŸŽ¯"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Import Libraries ðŸ“‚"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport xgboost as xgb\nimport optuna   \nimport cudf\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Importing Data ðŸ“š\nCudf is faster than pandas for reading csv file, so we will use that.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using cudf \ntrain_cudf = cudf.read_csv('../input/jane-street-market-prediction/train.csv')\ntrain = train_cudf.to_pandas()\ndel train_cudf\ntrain = train.astype({c: np.float32 for c in train.select_dtypes(include='float64').columns}) #limit memory use","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Cleaning Data ðŸª“\nDataFrame.query is faster than slicing method, so we will use that."},{"metadata":{"trusted":true},"cell_type":"code","source":"#We don't want weight=0 datas so we are ignoring it.\ntrain = train.query('weight > 0').reset_index(drop = True)\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Understanding Features ðŸ“Š"},{"metadata":{"trusted":true},"cell_type":"code","source":"TRADING_THRESHOLD = 0.500\ntrain.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking Missing Values in the features\nn_features = 45\nnan_val = train.isna().sum()[train.isna().sum() > 0].sort_values(ascending=False)\nprint(nan_val)\n\n\nfig, axs = plt.subplots(figsize=(10, 10))\n\nsns.barplot(y = nan_val.index[0:n_features], \n            x = nan_val.values[0:n_features], \n            alpha = 0.8\n           )\n\nplt.title(f'NaN values of train dataset (Top {n_features})')\nplt.xlabel('NaN values')\nfig.savefig(f'nan_values_top_{n_features}_features.png')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Filling the missing values with median value \nf_median = train.median()\nx_train = train.fillna(f_median)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating Train and Test DataFrame "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generating 0 or 1 values on the basis of resp features and storing it to 'action' column\n# It will serve as our test data \ntrain['action'] = (train['resp'] > 0 ).astype('int')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train.loc[:, train.columns.str.contains('feature')]\ny = train.loc[:, 'action']\n\n# Splitting X,y into train and validation data \nx_train, x_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can use this to fill nan values in unseen test but here I am not using this\ntest_median = X.median()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis ðŸ“ˆ"},{"metadata":{"trusted":true},"cell_type":"code","source":"# We will check if the target class is balanced or unbalanced in the training data\nsns.set_palette(\"hls\")\nax = sns.barplot(y_train.value_counts().index, y_train.value_counts()/len(y_train))\nax.set_title(\"Proportion of trades with action=0 and action=1\")\nax.set_ylabel(\"Percentage\")\nax.set_xlabel(\"Action\")\nsns.despine();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Resp Analysis\n#Last subplot doesn't mean anything\nresp_df = ['resp', 'resp_1', 'resp_2', 'resp_3', 'resp_4']\nfig, axes = plt.subplots(nrows=2\n                         , ncols=3,figsize=(20,10))\nfor i, column in enumerate(resp_df):\n    sns.distplot(train[column],ax=axes[i//3,i%3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cumulative return analysis\nfig, ax = plt.subplots(figsize=(16, 8))\n\nresp = train['resp'].cumsum()\nresp_1 = train['resp_1'].cumsum()\nresp_2 = train['resp_2'].cumsum()\nresp_3 = train['resp_3'].cumsum()\nresp_4 = train['resp_4'].cumsum()\n\nresp.plot(linewidth=2)\nresp_1.plot(linewidth=2)\nresp_2.plot(linewidth=2)\nresp_3.plot(linewidth=2)\nresp_4.plot(linewidth=2)\n\nax.set_xlabel (\"Trade\", fontsize=12)\nax.set_title (\"Cumulative Trade Returns\", fontsize=18)\n\nplt.legend(loc=\"upper left\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"resp and resp_4 variable are closely related so we can use this to set our 'action' variable.\n"},{"metadata":{},"cell_type":"markdown","source":"# Training XGBClassifier | Using Optuna for Hyperparameter Tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Created the Xgboost specific DMatrix data format from the numpy array to optimise memory consumption\ndtrain = xgb.DMatrix(x_train, label=y_train)\ndvalid = xgb.DMatrix(x_valid, label=y_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def objective(trial):\n    \n# params specifies the XGBoost hyperparameters to be tuned\n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 400, 600),\n        'max_depth': trial.suggest_int('max_depth', 10, 20),\n        'learning_rate': trial.suggest_uniform('learning_rate', 0.01, .1),\n        'subsample': trial.suggest_uniform('subsample', 0.50, 1),\n        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.50, 1),\n        'gamma': trial.suggest_int('gamma', 0, 10),\n        'tree_method': 'gpu_hist',  \n        'objective': 'binary:logistic'\n    }\n    \n    bst = xgb.train(params, dtrain)\n    preds = bst.predict(dvalid)\n    pred_labels = np.rint(preds)\n# trials will be evaluated based on their accuracy on the test set\n    accuracy = sklearn.metrics.accuracy_score(y_valid, pred_labels)\n    return accuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"study = optuna.create_study()\nstudy.optimize(objective,n_trials=5) \n#You can increase n_trials parameter","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Best trial: score {}, params {}'.format(study.best_trial.value, study.best_trial.params))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Set tree_method to gpu_hist to utilize gpu power and it will add some magic!!"},{"metadata":{"trusted":true},"cell_type":"code","source":"best_params = study.best_trial.params\nbest_params['tree_method'] = 'gpu_hist'      #gpu_hist is really fast\nbest_params['objective'] = 'binary:logistic'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del x_train, x_valid, y_train, y_valid, dtrain, dvalid  #free some space","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit the XGBoost classifier with optimal hyperparameters\nclf = xgb.XGBClassifier(**best_params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time clf.fit(X, y)  #Used the whole training data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fitting classifier on test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\nimport janestreet\nenv = janestreet.make_env() # initialize the environment\niter_test = env.iter_test() # an iterator which loops over the test set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for (test_df, pred_df) in tqdm(iter_test):\n    if test_df['weight'].item() > 0:\n        X_test = test_df.loc[:, test_df.columns.str.contains('feature')]\n        y_preds = clf.predict(X_test)\n        pred_df.action = y_preds\n    else:\n        pred_df.action = 0\n    env.predict(pred_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# If you find this notebook helpful then let me know through comments. It motivates me to put more such work!ðŸ˜Š"},{"metadata":{},"cell_type":"markdown","source":"# Queries/Suggestions are appreciated. Happy Learing!âœŒ"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}