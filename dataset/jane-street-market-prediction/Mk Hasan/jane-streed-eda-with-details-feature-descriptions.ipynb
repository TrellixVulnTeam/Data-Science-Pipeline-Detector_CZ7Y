{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Content\n1. [Intoduction](#1)\n2. [Dataset Description](#2)\n3. [Feature Descrtions](#3)\n4. [Exploratory Data Analysis](#5)\n    * [Features feature_{0...129}](#61) \n"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"1\"></a>\n\n<a id=\"3\"></a>\n<a id=\"4\"></a>\n<a id=\"5\"></a>\n<a id=\"61\"></a>\n# 1-Introduction"},{"metadata":{},"cell_type":"markdown","source":"## “Buy low, sell high.” It sounds so easy….\n\nIn reality, trading for profit has always been a difficult problem to solve, even more so in today’s fast-moving and complex financial markets. Electronic trading allows for thousands of transactions to occur within a fraction of a second, resulting in nearly unlimited opportunities to potentially find and take advantage of price differences in real time."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"2\"></a>\n# 2-Dataset Descriptions"},{"metadata":{},"cell_type":"markdown","source":"* train.csv - the training set, contains historical data and returns. \n\n* example_test.csv - a mock test set which represents the structure of the unseen test set. You will not be directly using the test set or sample submission in this competition, as the time-series API will get/set the test set and predictions.\n* example_sample_submission.csv - a mock sample submission file in the correct format\n\n* \n\n### features.csv :\n\n this are metadata pertaining to the anonymized features. The purpose of feature.csv is to show the relationship between the anonymized features, tag0 ~ tag28 are anonymized shared components/concepts used in feature derivation. For example, if the value for (feature_i, tag_j) is True, then it means the tag_j is used to derive feature_i.\n\nLet's assume we only have the following three features, and their definitions are\n\nfeature_0: volatility of this stock in past 30 days\nfeature_1: volume of this stock in past 30 days\nfeature_2: volume of this stock in past 10 days\n\nBut after the anonymization, the info about the connection between the features is lost, so in order to show this info to some extent, we create two tags here with the following definition\n\ntag0: some metric on the past 30 days\ntag1: volume of this stock\n\nThen the feature.csv should be something like\n\nfeature0, True, False\nfeature1, True, True\nfeature2, False, True"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"3\"></a>\n# Explore the features of data"},{"metadata":{},"cell_type":"markdown","source":"### **feature_{0...129}**\n\n -- representing real stock market data. Each row in the dataset represents a trading opportunity, for which you will be predicting an action value: 1 to make the trade and 0 to pass on it.\n \n \n ### weight & resp\n \n-- Each trade has an associated weight and resp, which together represents a return on the trade as well as several other resp_{1,2,3,4} values that represent returns over different time horizons. resp is how much we would gain from the trade (and it can be negative). All trades also have a weight associated with them that resp gets multiplied by. Resp1 - Resp4 only exists in the train.csv, they are correlated to Resp but not exactly the same (see the data description). They are provided just in case some people want some alternative objective metrics to regularize their model training.\n\n\n ### date\n\n -- date column is an integer which represents the day of the trade, \n \n ### ts_id\n --ts_id represents a time ordering. In addition to anonymized feature values, you are provided with metadata about the features in features.csv.\n \n \n ### Action: \n \n \n We're not trying to predict action, but rather deciding whether to perform the trade (action = 1) or not (action = 0).\n\nThe simplest way to do that is probably to predict the return of the trade (resp) and perform the trade if it is positive (action= resp > 0) (you could take the weight into account as well by multiplying the return with weight).\n\nAlso We're not predicting what trades will take place; we're picking the trades we believe will give the highest score (it's not just total returns, we also have to take volatility into account)!\n\nBut as you've noted, we do not have resp (the returns) in the test set. So, we have to find some way of predicting which trades are \"good trades\" — either directly, or by predicting resp (and perhaps even its probability distribution), and then somehow optimise for the utility score.\n"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4\"></a>\n# Exploratory Data Analysis"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Import the data "},{"metadata":{},"cell_type":"markdown","source":"* To speed up to load the data here i have used the dataable library and then converted into pandas dataframe. \n* It can can read large datasets fairly quickly and is often faster than pandas. It is specifically meant for data processing of tabular datasets with emphasis on speed and support for large sized data.\n"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!pip install ../input/python-datatable/datatable-0.11.0-cp37-cp37m-manylinux2010_x86_64.whl\n\nimport datatable as dt\n\ntrain = dt.fread(\"../input/jane-street-market-prediction/train.csv\").to_pandas() #, max_nrows=30000000\nfeatures = pd.read_csv(\"../input/jane-street-market-prediction/features.csv\")\nexample_test = pd.read_csv(\"../input/jane-street-market-prediction/example_test.csv\", nrows=10**3 )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'train shape:{train.shape}')\nprint(f'featues shape:{features.shape}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.sample(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features.sample(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"example_test.sample(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The columns and number of null vaues in train set"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"No of columsn containing null values\")\nprint(len(train.columns[train.isna().any()]))\n\n\nprint(\"No. of columns not containing null values\")\nprint(len(train.columns[train.notna().all()]))\n\nprint(\"the columns containing null values\")\nprint(train.columns[train.isna().any()])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The columns and number of null vaues in feature set"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"No. of columns containing null values\")\nprint(len(features.columns[features.isna().any()]))\n\nprint(\"No. of columns not containing null values\")\nprint(len(features.columns[features.notna().all()]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Correlation coefficients\nCorrelation coefficients are used in statistics to measure how strong a relationship is between two variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(18,18))\ndata_corr = train.corr()\nsns.heatmap(data_corr, cmap='coolwarm')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_corr.style.background_gradient(cmap='coolwarm', axis=None).set_precision(2)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Features feature_{0...129}\n"},{"metadata":{},"cell_type":"markdown","source":"# Distribution of features\nThere are 500 days of data in train.csv . Let us take a look at date=0 (the first day)"},{"metadata":{"trusted":true},"cell_type":"code","source":"date = 0\nn_features = 130\n\ncols = [f'feature_{i}' for i in range(1, n_features)]\nhist = px.histogram(\n    train[train[\"date\"] == date], \n    x=cols, \n    animation_frame='variable', \n    range_y=[0, 700], \n    range_x=[-8, 8]\n)\n\nhist.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"date = 0\nn_features = 130\ncols = [f'feature_{i}' for i in range(1, n_features)]\nhist = px.scatter(\n    train[train[\"date\"] == date], \n    x=cols, \n    animation_frame='variable', \n    range_y=[-500, 6500], \n    range_x=[-10, 10]\n)\nhist.layout.showlegend = False\nhist.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* There are a total of 500 days of data in train.csv (i.e. two years of trading data). Let us take a look at the cumulative value of feature 0 over time"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(15, 5))\nfeature_0 = pd.Series(train['feature_0']).cumsum()\nax.set_xlabel (\"Trade\", fontsize=18)\nax.set_ylabel (\"feature_0 (cumulative)\", fontsize=18);\nfeature_0.plot(lw=3);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Now look at the return or rsp over the time. There are total 500 days of data. "},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(15, 5))\nbalance= pd.Series(train['resp']).cumsum()\nax.set_xlabel (\"Trade\", fontsize=18)\nax.set_ylabel (\"Cumulative return\", fontsize=18);\nbalance.plot(lw=3);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Now, lets see all the cummalitive return for diffenrent resp with different time horizon."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(15, 5))\nbalance= pd.Series(train['resp']).cumsum()\nresp_1= pd.Series(train['resp_1']).cumsum()\nresp_2= pd.Series(train['resp_2']).cumsum()\nresp_3= pd.Series(train['resp_3']).cumsum()\nresp_4= pd.Series(train['resp_4']).cumsum()\nax.set_xlabel (\"Trade\", fontsize=18)\nax.set_title (\"Cumulative return of resp and time horizons 1, 2, 3, and 4 (500 days)\", fontsize=18)\nbalance.plot(lw=3)\nresp_1.plot(lw=3)\nresp_2.plot(lw=3)\nresp_3.plot(lw=3)\nresp_4.plot(lw=3)\nplt.legend(loc=\"upper left\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Insight from the plot: \n\"The longer the Time Horizon, the more aggressive, or riskier portfolio, an investor can build. The shorter the Time Horizon, the more conservative, or less risky, the investor may want to adopt.\"\n\n#### What is an Investment Time Horizon?\nAn Investment Time Horizon, or just Time Horizon, is the period of time one expects to hold an investment until they need the money back. Time horizons are largely dictated by investment goals and strategies. For example, saving for a down payment on a house, maybe 2 years, would be considered a short-term time horizon, while saving for college a medium-term time horizon, and investing for retirement a long-term time horizon."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}