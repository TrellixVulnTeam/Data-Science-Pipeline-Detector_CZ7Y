{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<center><h1 style=\"color:blue\">Jane Street Market Prediction</h1></center>\n<center><h1 style=\"color:red\">Test Your model against future real market data</h1></center>","metadata":{}},{"cell_type":"markdown","source":"<center><img src=\"https://storage.googleapis.com/kaggle-competitions/kaggle/23304/logos/header.png?t=2020-11-16-17-41-04\"></center>","metadata":{}},{"cell_type":"markdown","source":"Thought of adding the most relatable meme from the most famous series: Scam 1992 ;)\n<center><img src=\"https://www.beyoung.in/beyoungistan/wp-content/uploads/2020/10/1-1024x853.jpg\" style=\"height:300px;width:300px\"></center>","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-08-01T07:33:41.915048Z","iopub.execute_input":"2021-08-01T07:33:41.915629Z","iopub.status.idle":"2021-08-01T07:33:41.940276Z","shell.execute_reply.started":"2021-08-01T07:33:41.915573Z","shell.execute_reply":"2021-08-01T07:33:41.939434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Brief Description","metadata":{}},{"cell_type":"markdown","source":"<center><h2 style=\"color:orange;font-size=200px\">“Buy low, sell high.”</h2></center>\n\n# 1.1 About\nDeveloping trading strategies to identify and take advantage of inefficiencies is challenging. Even if a strategy is profitable now, it may not be in the future, and market volatility makes it impossible to predict the profitability of any given trade with certainty. As a result, it can be hard to distinguish good luck from having made a good trading decision.\n\nIn the first three months of this challenge, you will build your own quantitative trading model to maximize returns using market data from a major global stock exchange. Next, you’ll test the predictiveness of your models against future market returns and receive feedback on the leaderboard.\n\n# 1.2 Challenge Brief\nYour challenge will be to use the historical data, mathematical tools, and technological tools at your disposal to create a model that gets as close to certainty as possible. You will be presented with a number of potential trading opportunities, which your model must choose whether to accept or reject.\n\nIn general, if one is able to generate a highly predictive model which selects the right trades to execute, they would also be playing an important role in sending the market signals that push prices closer to “fair” values. That is, a better model will mean the market will be more efficient going forward. However, developing good models will be challenging for many reasons, including a very low signal-to-noise ratio, potential redundancy, strong feature correlation, and difficulty of coming up with a proper mathematical formulation.\n\n# 1.3 Organizers\nJane Street has spent decades developing their own trading models and machine learning solutions to identify profitable opportunities and quickly decide whether to execute trades. These models help Jane Street trade thousands of financial products each day across 200 trading venues around the world.\nAdmittedly, this challenge far oversimplifies the depth of the quantitative problems Jane Streeters work on daily, and Jane Street is happy with the performance of its existing trading model for this particular question. However, there’s nothing like a good puzzle, and this challenge will hopefully serve as a fun introduction to a type of data science problem that a Jane Streeter might tackle on a daily basis. Jane Street looks forward to seeing the new and creative approaches the Kaggle community will take to solve this trading challenge.","metadata":{}},{"cell_type":"markdown","source":"# 1.-1 TL;DR\n\n- Based on the historical data, we need to develop a model to predict a return value for a given trade.\n- The return value can be 1(traded) or 0(passed) and depends on the weights and resp.\n- The evaluation will take place via a time-series API, which will rerun the submitted model on a data stream, iteratively on each data point.\n- Submissions will be evaluated by the utility score, explained below and implemented [here](https://www.kaggle.com/c/jane-street-market-prediction/discussion/201257).\n\nKeep on reading the notebook :)","metadata":{}},{"cell_type":"markdown","source":"# 2. Evaluation and Submission","metadata":{}},{"cell_type":"markdown","source":"# 2.1 Metric","metadata":{}},{"cell_type":"markdown","source":"## Utility Score\n\nEach row in the test set represents a trading opportunity for which we will be predicting an action value, **1 to make the trade and 0 to pass on it**. Each trade j has an associated weight and resp, which represents a return.\n\nFor each date i, we define:","metadata":{}},{"cell_type":"markdown","source":"$$ p_i = \\sum_j(weight_{ij} * resp_{ij} * action_{ij}), $$\n$$ t = \\frac{\\sum p_i }{\\sqrt{\\sum p_i^2}} * \\sqrt{\\frac{250}{|i|}}, $$","metadata":{}},{"cell_type":"markdown","source":"where $|i|$ is the number of unique dates in the test set. The utility is then defined as:","metadata":{}},{"cell_type":"markdown","source":"$$u = min(max(t,0), 6)  \\sum p_i.$$","metadata":{}},{"cell_type":"markdown","source":"[Here](https://www.kaggle.com/c/jane-street-market-prediction/discussion/199089) I am posting a discussion thread that explains about the evaluation metrics as well as utility score. Do checkout the thread and the implementation.","metadata":{}},{"cell_type":"markdown","source":"# 2.2 Submission","metadata":{}},{"cell_type":"markdown","source":"Time series API again, after RIIID Challenge ;)  \nSo the submission will look like the following: ","metadata":{}},{"cell_type":"markdown","source":"```\nimport janestreet\nenv = janestreet.make_env() # initialize the environment\niter_test = env.iter_test() # an iterator which loops over the test set\n\nfor (test_df, sample_prediction_df) in iter_test:\n    sample_prediction_df.action = 0 #make your 0/1 prediction here\n    env.predict(sample_prediction_df)\n```","metadata":{}},{"cell_type":"markdown","source":"# 2.3 Code Requirements\n\nYes, this is a code competition, hence it has certain requirements for submission. Submissions to this competition must be made through Notebooks. For this competition, training is not required in Notebooks. In order for to be eligible for submission, the following conditions must be met:\n\n## Training Phase\n- Your notebook must use the time-series module to make predictions\n- CPU Notebook <= 4 hours run-time\n- GPU Notebook <= 4 hours run-time\n- Freely & publicly available external data is allowed, including pre-trained models\n\n## Forecasting Phase\nBecause the size of the test set will change during the live forecasting phase, the time limits will be adjusted in proportion to the test set size, with a **10% added time allowance**. As a hypothetical example, if there are <span style=\"color:red\">1,000,000 test rows</span> and a <span style=\"color:red\">4 hour runtime</span> limit during the **training phase** and the **forecasting phase** has <span style=\"color:red\">2,000,000 rows</span>, your notebook will be allowed **8 hours + 10% = 8.8 hours during the forecasting phase**.","metadata":{}},{"cell_type":"markdown","source":"# Data Overview","metadata":{}},{"cell_type":"markdown","source":"This dataset contains an anonymized set of features, <span style=\"color:red\">feature_{0...129}</span>, representing real stock market data. \n1. Each row in the dataset represents a trading opportunity, for which you will be predicting an action value: \n    - 1 to make the trade and \n    - 0 to pass on it. \n\n2. Each trade has an associated <span style=\"color:red\">weight and resp</span>, which together represents a return on the trade. \n3. The `date column` is an **integer** which represents the day of the trade.\n4. The `ts_id` represents a time ordering. \n5. In addition to anonymized feature values, you are provided with metadata about the features in `features.csv`.\n\n# 3.1 Train.csv\n\n- In the training set, train.csv, you are provided a resp value, as well as several other resp_{1,2,3,4} values that represent returns over different time horizons. These variables are not included in the test set. \n- **Non-Scored:** Trades with weight = 0 were intentionally included in the dataset for completeness, although such trades will not contribute towards the scoring evaluation.\n\n# 3.2 Test Set\n- During the model training phase of the competition, this unseen test set is comprised of historical data.\n- During the live forecasting phase, the test set will use periodically updated live market data.\n- Note that during the second (forecasting) phase of the competition, the notebook time limits will scale with the number of trades presented in the test set.\n\n    1. **example_test.csv** - a mock test set which represents the structure of the unseen test set. You will not be directly using the test set or sample submission in this competition, as the time-series API will get/set the test set and predictions.\n    2. **example_sample_submission.csv** - a mock sample submission file in the correct format\n","metadata":{}},{"cell_type":"markdown","source":"<h1 style=\"color:red\">Work in Progress</h1>\n<h2 style=\"color:red\">If you like the notebook, consider upvoting it :) and shoot your thoughts in the comment section.</h2>","metadata":{}},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"!pip3 install ../input/python-datatable/datatable-0.11.1-cp37-cp37m-manylinux2010_x86_64.whl","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-08-01T07:33:41.94345Z","iopub.execute_input":"2021-08-01T07:33:41.943848Z","iopub.status.idle":"2021-08-01T07:33:49.153326Z","shell.execute_reply.started":"2021-08-01T07:33:41.943813Z","shell.execute_reply":"2021-08-01T07:33:49.151826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\nimport random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport datatable as dt\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\n\nimport xgboost as xgb\n\nimport tensorflow as tf\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport janestreet\n\n%matplotlib inline\n\npd.set_option('display.max_columns', 200)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T07:33:49.155676Z","iopub.execute_input":"2021-08-01T07:33:49.156199Z","iopub.status.idle":"2021-08-01T07:33:56.156269Z","shell.execute_reply.started":"2021-08-01T07:33:49.156143Z","shell.execute_reply":"2021-08-01T07:33:56.155218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seedAll(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    pass\n\nseedAll(2020)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T07:33:56.159048Z","iopub.execute_input":"2021-08-01T07:33:56.159453Z","iopub.status.idle":"2021-08-01T07:33:56.170703Z","shell.execute_reply.started":"2021-08-01T07:33:56.159421Z","shell.execute_reply":"2021-08-01T07:33:56.169903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4.1 Read the Data","metadata":{}},{"cell_type":"code","source":"%%time\n\n# using datatable for faster loading\ntrain_dt = dt.fread(\"../input/jane-street-market-prediction/train.csv\")\ntrain = train_dt.to_pandas()\nfeatures = pd.read_csv(\"../input/jane-street-market-prediction/features.csv\")\nexamples_test = pd.read_csv(\"../input/jane-street-market-prediction/example_test.csv\")\nexample_sample_submission = pd.read_csv(\"../input/jane-street-market-prediction/example_sample_submission.csv\")\n\n# free up space\ndel train_dt\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-08-01T07:33:56.173792Z","iopub.execute_input":"2021-08-01T07:33:56.174274Z","iopub.status.idle":"2021-08-01T07:34:41.949113Z","shell.execute_reply.started":"2021-08-01T07:33:56.174241Z","shell.execute_reply":"2021-08-01T07:34:41.948269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4.2 train","metadata":{}},{"cell_type":"code","source":"train","metadata":{"execution":{"iopub.status.busy":"2021-08-01T07:34:41.951618Z","iopub.execute_input":"2021-08-01T07:34:41.952073Z","iopub.status.idle":"2021-08-01T07:34:42.114499Z","shell.execute_reply.started":"2021-08-01T07:34:41.952042Z","shell.execute_reply":"2021-08-01T07:34:42.113611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"============= Memory Usage =============\")\ntrain.memory_usage()","metadata":{"execution":{"iopub.status.busy":"2021-08-01T07:34:42.11615Z","iopub.execute_input":"2021-08-01T07:34:42.11646Z","iopub.status.idle":"2021-08-01T07:34:42.133676Z","shell.execute_reply.started":"2021-08-01T07:34:42.116429Z","shell.execute_reply":"2021-08-01T07:34:42.132505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4.2.1 Descriptive Statistics of Train","metadata":{}},{"cell_type":"code","source":"train.describe().style.background_gradient(cmap=\"gnuplot\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-01T07:34:42.135294Z","iopub.execute_input":"2021-08-01T07:34:42.135601Z","iopub.status.idle":"2021-08-01T07:34:59.051894Z","shell.execute_reply.started":"2021-08-01T07:34:42.135573Z","shell.execute_reply":"2021-08-01T07:34:59.05089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4.2.2 Presence of NaN over Columns","metadata":{}},{"cell_type":"code","source":"temp = train.isna().sum().sort_values(ascending=False).to_frame().reset_index()\ntemp.columns = [\"Cols\", \"NaN-counts\"]\n\ndef plot_bar(x, y, df):\n    plt.figure(figsize=(20, 30))\n    sns.barplot(x=x, y=y, data=df, orient=\"h\")\n    plt.title(f\"Top {len(df)} {x} {y}\")\n    plt.show()\n    pass","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-01T07:34:59.053685Z","iopub.execute_input":"2021-08-01T07:34:59.054014Z","iopub.status.idle":"2021-08-01T07:35:00.212848Z","shell.execute_reply.started":"2021-08-01T07:34:59.053982Z","shell.execute_reply":"2021-08-01T07:35:00.211611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_bar(x='NaN-counts', y='Cols', df=temp.head(50))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-01T07:35:00.214606Z","iopub.execute_input":"2021-08-01T07:35:00.214934Z","iopub.status.idle":"2021-08-01T07:35:00.988994Z","shell.execute_reply.started":"2021-08-01T07:35:00.214904Z","shell.execute_reply":"2021-08-01T07:35:00.98776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for k, v in temp.groupby(\"NaN-counts\").groups.items():\n    print(f\"\\n{k}: {temp.iloc[v, :]}\")","metadata":{"execution":{"iopub.status.busy":"2021-08-01T07:35:00.990628Z","iopub.execute_input":"2021-08-01T07:35:00.990999Z","iopub.status.idle":"2021-08-01T07:35:01.046002Z","shell.execute_reply.started":"2021-08-01T07:35:00.990967Z","shell.execute_reply":"2021-08-01T07:35:01.044523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"These grouping will help later in analysing the phishing features. Let's save here as a table.","metadata":{}},{"cell_type":"markdown","source":"||NaN Counts|col|\n|---|---|---|\n||395535|feature_28, feature_27, feature_18, feature_17 |\n||393135|fearure_7, feature_8|\n||351426|feature_96, feature_108, feature_114, feature_102, feature_84, feature_72, feature_90, feature_78|\n||81444|feature_21, feature_22, feature_32, feature_31|\n||80015|feature_11, feature_12|\n||69854|feature_120, feature_121|\n||68409|feature_55|\n||64088|feature_74, feature_86, feature_92, feature_98, feature_80, feature_104, feature_110, feature_116|\n||16083|feature_124, feature_125|\n||15353|feature_117, feature_111, feature_75, feature_81, feature_23, feature_87, 'feature_24, feature_34, feature_14, feature_93, feature_33, feature_105, feature_13, feature_99|\n||8853|feature_126, feature_127|\n||6683|feature_15, feature_76, feature_112, feature_118, feature_16, feature_82, feature_25, feature_26, feature_88, feature_36, feature_35, feature_106, feature_94, feature_100|\n||1921|feature_128, feature_129|\n||788|feature_29, feature_20, feature_19, feature_103, feature_115, feature_10, feature_97, feature_9, feature_91, feature_109, feature_85, feature_30, feature_79, feature_73|\n||719|feature_56|\n||448|feature_3, feature_44, feature_45, feature_4|\n||223|feature_123, feature_122|\n||48|feature_59|\n||1|feature_58|","metadata":{}},{"cell_type":"code","source":"del temp\ngc.collect()","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-08-01T07:35:01.047638Z","iopub.execute_input":"2021-08-01T07:35:01.048014Z","iopub.status.idle":"2021-08-01T07:35:01.241849Z","shell.execute_reply.started":"2021-08-01T07:35:01.047982Z","shell.execute_reply":"2021-08-01T07:35:01.239824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's look at the weights distribution. ","metadata":{}},{"cell_type":"markdown","source":"# 4.2.3 Weights Analysis","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(20, 8))\nsns.distplot(train[\"weight\"], ax=axs[0])\nsns.distplot(train.query('weight != 0')[\"weight\"], ax=axs[0])\naxs[0].set_title(\"Weights Distribution\")\n\nsns.boxplot(x=\"weight\", data=train[train[\"weight\"]!=0], ax=axs[1])\nsns.boxplot(x=\"weight\", data=train[train[\"weight\"]==0], ax=axs[1])\naxs[1].set_title(\"Weights Box plot\")\n\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-01T07:35:01.244281Z","iopub.execute_input":"2021-08-01T07:35:01.244587Z","iopub.status.idle":"2021-08-01T07:35:07.626403Z","shell.execute_reply.started":"2021-08-01T07:35:01.244557Z","shell.execute_reply":"2021-08-01T07:35:07.624936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Okay, it's more of a gamma distribution. There are quite a lot outliers as we can see, and yeah, most of the values being under 10-15 range. Yet, the max value jumps upto 175 range. I am no domain expert, but I would surely look for the application and methodology for these weights calculation for trading.  \nKeeping this aside for a moment, we can see ZERO weights present over there, and as mentioned in the data description, these are not-scored. Let's see how many do they contribute.","metadata":{}},{"cell_type":"code","source":"percent_zeros = (100/train.shape[0])*((train.weight.values == 0).sum())\nprint('Percentage of zero weights is: %i' % percent_zeros +\"%\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-01T07:35:07.628044Z","iopub.execute_input":"2021-08-01T07:35:07.628481Z","iopub.status.idle":"2021-08-01T07:35:07.641157Z","shell.execute_reply.started":"2021-08-01T07:35:07.628435Z","shell.execute_reply":"2021-08-01T07:35:07.639982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Well, we have 17% non-scored data I can say. So I don't think having a pretrained model on this data will help that much, but yeah, we can't certainly ignore the possibility of the option.","metadata":{}},{"cell_type":"markdown","source":"# 4.2.4 Resp Analysis","metadata":{}},{"cell_type":"code","source":"NSW_COLS = [c for c in train.columns if 'resp' in c]\nfor f in NSW_COLS:\n    fig, axs = plt.subplots(1, 4, figsize=(15, 4))\n    sns.distplot(train[f], ax=axs[0])\n    sns.distplot(train.query('weight != 0')[f], ax=axs[1])\n    try:\n        sns.distplot(train.query('weight > 0 and resp > 0')[f].dropna().apply(np.log1p), ax=axs[2])\n        sns.distplot(train.query('weight > 0 and resp < 0')[f].dropna().apply(np.log1p), ax=axs[2])\n    except:\n        pass\n    train.sample(5000).plot(kind='scatter', x=f, y='resp', ax=axs[3])\n    fig.suptitle(f, fontsize=15, y=1.1)\n    \n    axs[0].set_title('feature distribution')\n    axs[1].set_title('Non Zero Weights')\n    axs[2].set_title('log transform')\n    axs[3].set_title('feature vs. response')\n    \n    plt.tight_layout()\n    plt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-01T07:35:07.642781Z","iopub.execute_input":"2021-08-01T07:35:07.643137Z","iopub.status.idle":"2021-08-01T07:35:45.286714Z","shell.execute_reply.started":"2021-08-01T07:35:07.643098Z","shell.execute_reply":"2021-08-01T07:35:45.285762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- `resp` col is the key col to generate a return and `resp_{1, 4}` also represents the same entity but from different timezones. \n- Just from the above graphs, we can see that, all the resp cols are linear(almost, but with some noise added) with each other. We will dive deep into it, but let's see the `weight and resp` **together** effect on each time zone.","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(1, 5, figsize=(20, 6))\nfor i, f in enumerate(NSW_COLS):\n\n    train.sample(5000).plot(kind='scatter', x=f, y='weight', ax=axs[i])\n    axs[i].set_title(f'{f} vs. weight')\n\nplt.tight_layout()\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-01T07:35:45.288473Z","iopub.execute_input":"2021-08-01T07:35:45.288822Z","iopub.status.idle":"2021-08-01T07:35:46.837658Z","shell.execute_reply.started":"2021-08-01T07:35:45.288785Z","shell.execute_reply":"2021-08-01T07:35:46.836858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, weight and resp are independent(almost) of each other. I guess if we can clear the noise, we can surely conclude this. Why don't we look into the correlation plot? Well, let's create a separate chunk for this target data instead.","metadata":{}},{"cell_type":"markdown","source":"resp somewhat represents the return of a trade, but we have values from different time zones. Let's have a look at the cumulative values.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(20, 6))\nbalance= pd.Series(train['resp']).cumsum()\nresp_1= pd.Series(train['resp_1']).cumsum()\nresp_2= pd.Series(train['resp_2']).cumsum()\nresp_3= pd.Series(train['resp_3']).cumsum()\nresp_4= pd.Series(train['resp_4']).cumsum()\nax.set_xlabel (\"Trade\", fontsize=18)\nax.set_title (\"Cumulative return of resp and time horizons 1, 2, 3, and 4\", fontsize=18)\nbalance.plot(lw=3)\nresp_1.plot(lw=3)\nresp_2.plot(lw=3)\nresp_3.plot(lw=3)\nresp_4.plot(lw=3)\nplt.legend(loc=\"upper left\");","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-01T07:35:46.839001Z","iopub.execute_input":"2021-08-01T07:35:46.839463Z","iopub.status.idle":"2021-08-01T07:35:49.102709Z","shell.execute_reply.started":"2021-08-01T07:35:46.839423Z","shell.execute_reply":"2021-08-01T07:35:49.101928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4.2.5 Correlation ","metadata":{}},{"cell_type":"markdown","source":"As we have seen in the earlier plots, yes, resp_4 and resp do have a strong correlation with each other and the other resp values also seems a bit correlated, so why don't plot the correlation plot and observe?","metadata":{}},{"cell_type":"code","source":"corr_temp = train[[\"resp\", \"resp_1\", \"resp_2\", \"resp_3\", \"resp_4\"]].corr()\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr_temp, dtype=bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr_temp, mask=mask, annot=True)\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-01T07:35:49.104033Z","iopub.execute_input":"2021-08-01T07:35:49.104505Z","iopub.status.idle":"2021-08-01T07:35:49.626319Z","shell.execute_reply.started":"2021-08-01T07:35:49.104456Z","shell.execute_reply":"2021-08-01T07:35:49.62519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The numbers aren't shocking as it was quite evident from the earlier analysis. Will continue after a break :)","metadata":{}},{"cell_type":"code","source":"targets_feats = train[[\"date\", \"weight\", \"resp\", \"resp_1\", \"resp_2\", \"resp_3\", \"resp_4\", \"ts_id\"]]\ntargets_feats","metadata":{"execution":{"iopub.status.busy":"2021-08-01T07:35:49.628035Z","iopub.execute_input":"2021-08-01T07:35:49.62853Z","iopub.status.idle":"2021-08-01T07:35:49.706243Z","shell.execute_reply.started":"2021-08-01T07:35:49.628493Z","shell.execute_reply":"2021-08-01T07:35:49.705384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"weights having 0 values won't be included in the evaluation, so why not use this extra chunk of information to train a model and then use the weights to train our main model? We will see, and yeah, I won't be surpised this approach being applied later in this competition.","metadata":{}},{"cell_type":"code","source":"scored_weights = targets_feats[targets_feats[\"weight\"] != 0].copy().reset_index(drop=True)\nNonscored_weights = targets_feats[targets_feats[\"weight\"] == 0].copy().reset_index(drop=True)\n\ndel targets_feats\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-08-01T07:35:49.707426Z","iopub.execute_input":"2021-08-01T07:35:49.707898Z","iopub.status.idle":"2021-08-01T07:35:50.234187Z","shell.execute_reply.started":"2021-08-01T07:35:49.707862Z","shell.execute_reply":"2021-08-01T07:35:50.23308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(scored_weights.head())\ndisplay(Nonscored_weights.head())","metadata":{"execution":{"iopub.status.busy":"2021-08-01T07:35:50.235545Z","iopub.execute_input":"2021-08-01T07:35:50.235857Z","iopub.status.idle":"2021-08-01T07:35:50.26432Z","shell.execute_reply.started":"2021-08-01T07:35:50.235829Z","shell.execute_reply":"2021-08-01T07:35:50.262967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- A date represents a day of trading, I mean a working day. As per this nice [discussion thread](https://www.kaggle.com/c/jane-street-market-prediction/discussion/199107), we have 250 trading days in a year, and we have 500 unique dates or days I should say in our dataset, so we have the data of **2 years.** \n- And `ts_id` represents an ordering (?) So shall we consider or assume that, the dates are in chronological order? We will see.","metadata":{}},{"cell_type":"code","source":"# Just making sure, if we are missing anything or not\nprint(\"Number of trading days in NonScored points: \", Nonscored_weights[\"date\"].nunique())\nprint(\"Number of trading days in Scored points: \", scored_weights[\"date\"].nunique())","metadata":{"execution":{"iopub.status.busy":"2021-08-01T07:35:50.265989Z","iopub.execute_input":"2021-08-01T07:35:50.266322Z","iopub.status.idle":"2021-08-01T07:35:50.292197Z","shell.execute_reply.started":"2021-08-01T07:35:50.266288Z","shell.execute_reply":"2021-08-01T07:35:50.290991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# What we Know till now:\n\nNow we will focus our analysis on the target features. Well, don't get confused with actual target features when I am saying above columns as target features. \n- As the description stated, the `action` is decided from `weight` and `resp`. \n- And in the earlier plots, we have seen that, these two columns are almost independet of each other and the noise is ofcourse we can say added by the organizers. We will try to clean the noise and see if we can find something.\n- `resp_1`, `resp_2`, `resp_3` and `resp_4` represnts return over different time horizons, and yeah, `resp` also represnts the same and we can clearly see the correlation between the resp columns. So, what could be the possible approaches towards calculating the `action` for each trade? \n- **Zero weights** won't be considered for evaluation, yet they are added [***intentionally***](https://www.kaggle.com/c/jane-street-market-prediction/data) by organizers.\n- Total **500** trading days data is available and we have non-scored weight in each day. Wanna know why and what are the characteristics of such data points? Let me know in the comments till I find an explanation to that :(\n- And there are **17%** trades having zero weights.\n\nWhat more? We will see.","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(20, 16))\nsns.scatterplot(\"ts_id\", \"weight\", data=scored_weights[scored_weights[\"date\"]==0], ax=axs[0])\nsns.scatterplot(\"ts_id\", \"weight\", data=scored_weights[scored_weights[\"date\"]==1], ax=axs[0])\naxs[0].set_xlabel(\"Sequence\")\naxs[0].set_ylabel(\"Weight\")\naxs[0].set_title(\"Weights Sequence Plot\")\n\naxs[1].plot(\"ts_id\", \"weight\", data=scored_weights[scored_weights[\"date\"]==0])\naxs[1].plot(\"ts_id\", \"weight\", data=scored_weights[scored_weights[\"date\"]==1])\naxs[1].set_xlabel(\"Sequence\")\naxs[1].set_ylabel(\"Weight\")\naxs[1].set_title(\"Weights Sequence Plot\")\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-01T07:35:50.293899Z","iopub.execute_input":"2021-08-01T07:35:50.29423Z","iopub.status.idle":"2021-08-01T07:35:50.790421Z","shell.execute_reply.started":"2021-08-01T07:35:50.294196Z","shell.execute_reply":"2021-08-01T07:35:50.789189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we observe the spikes in the data. Well, that's pretty expected from the distribution, yet we need to explore more on this behaviour.","metadata":{}},{"cell_type":"code","source":"FEAT_COLS = [c for c in train.columns if 'feature' in c]\nfor f in FEAT_COLS:\n    fig, axs = plt.subplots(1, 4, figsize=(15, 4))\n    sns.distplot(train[f], ax=axs[0])\n    sns.distplot(train.query('weight > 0')[f], ax=axs[1])\n    try:\n        sns.distplot(train.query('weight > 0 and resp > 0')[f].dropna().apply(np.log1p), ax=axs[2])\n        sns.distplot(train.query('weight > 0 and resp < 0')[f].dropna().apply(np.log1p), ax=axs[2])\n    except:\n        pass\n    train.sample(5000).plot(kind='scatter', x=f, y='resp', ax=axs[3])\n    fig.suptitle(f, fontsize=15, y=1.1)\n    \n    axs[0].set_title('feature distribution')\n    axs[1].set_title('only weight > 0')\n    axs[2].set_title('log transform')\n    axs[3].set_title('feature vs. response')\n    \n    plt.tight_layout()\n    plt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-01T07:35:50.792289Z","iopub.execute_input":"2021-08-01T07:35:50.792603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I am still wondering on the causality of below equation i.e taking action as the binaized value of multiplication of weight and resp. Well, I will explore each 130 features individually in another notebooks, to see if I will be able to find out something.","metadata":{}},{"cell_type":"code","source":"train['action'] = ((train['weight'].values * train['resp'].values) > 0).astype('int')\n\nplt.figure(figsize=(12, 8))\nsns.countplot(train[\"action\"])\nplt.title(\"Action Distribution\")\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4.3 features","metadata":{}},{"cell_type":"code","source":"features = pd.read_csv(\"../input/jane-street-market-prediction/features.csv\", index_col=0)\nfeatures = features * 1\nfeatures.T.style.background_gradient(cmap=\"cividis\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp = features.sum(axis=1).sort_values(ascending=False).to_frame().reset_index()\ntemp.columns = [\"features\", \"True-counts\"]\ntemp[\"features\"] = temp[\"features\"].apply(lambda x: \"features-\"+str(x))\n\nplt.figure(figsize=(20, 30))\nsns.barplot(x='True-counts', y='features', data=temp, orient=\"h\")\nplt.title(\"Top 50 True counts Cols\")\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Any conclusion from the above counts? I am still wondering.","metadata":{}},{"cell_type":"code","source":"del temp\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4.4 example_test","metadata":{}},{"cell_type":"code","source":"examples_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4.5 example sample submission","metadata":{}},{"cell_type":"code","source":"example_sample_submission","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"color:red\">The submission trainig and inference has been shifted to another notebook, and that will be made public soon :)</span>","metadata":{}},{"cell_type":"markdown","source":"Thanks for reading through the notebook, I will keep updating as the competition progresses.","metadata":{}}]}