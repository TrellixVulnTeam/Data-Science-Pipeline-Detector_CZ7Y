{"cells":[{"metadata":{},"cell_type":"markdown","source":"Earlier, in a separate notebook, which you can find [here](https://www.kaggle.com/ariannsz/dirty-but-easy-pca-with-dask), we did PCA and save the results in a csv file.\nWe are now going to use that file. However, we had forgotten to include the weight and resp in the csv file, so we need to do a little bit of dirty work first.  \n  \nAfter that, we are going to create a NN, using TensorFlow and Keras, feed it with the 40 principle components along weights and resp, and try to do a good regression or classification! \n\nSo, hang on and let's see what we can do! \n"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport dask.dataframe as dd\nimport dask.array as da\nimport gc\n\nimport tensorflow as tf\nimport tensorflow.keras as keras\nimport tensorflow.keras.layers as layers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df = dd.read_csv('../input/janestreet-pca/pca.csv')\ndf = df.drop(columns=['Unnamed: 0'])\ndf = df.to_dask_array()\ndf = df.compute()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from dask_ml.preprocessing import StandardScaler as ss\n\ntemp = dd.read_csv('../input/jane-street-market-prediction/train.csv')\nweights = temp['weight']\nweights = weights.to_dask_array()\n\nweights = weights.compute()\nweights = weights.reshape(-1,1)\n\nscaler = ss()\nweights= scaler.fit_transform(weights)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"temp = dd.read_csv('../input/jane-street-market-prediction/train.csv')\ntarget = temp['resp']\ntarget = target.to_dask_array()\ntarget = target.compute()\ntarget = target.reshape(-1,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target = np.divide(target, np.abs(target)).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target += 1\ntarget = np.divide(target, 2).astype(int)\nprint(target[:5,0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"del(temp)\ndata = [weights, df]\ndata = da.concatenate(data, axis=1)\ndata = data.compute()\ndel(weights)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data.shape)\nprint(target.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Classification First\nLets make it easier and see this problem as a binary classification in the first step.\nTo do so, we are going to map the resp values to {1, -1} based on their sign. "},{"metadata":{},"cell_type":"markdown","source":"# A couple of notes\n* First, I had target values as +1, -1, and I was getting val_AUC = 0!  \nAfter diggin a little, I found the following statement from keras documentation ([link](https://keras.io/api/losses/probabilistic_losses/#binary_crossentropy-function)):\n\n> *Use this cross-entropy loss when there are only two label classes (assumed to be 0 and 1). For each example, there should be a single floating-point value per prediction.*  >\n\n\nSo, I changed the target values to 0 and 1 and the problem was solved!"},{"metadata":{},"cell_type":"markdown","source":"* The other note-worthy feature is the Dropout; it really helps with overfitting problem.  \nI have not tried many different architectures, but among the ones that I tried, Dropout has proved to be helpful"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_valid, X_train = data[:5000,:] , data[5000:, :]\ny_valid, y_train = target[:5000], target[5000:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = keras.models.Sequential([\n    layers.Input(shape=(data.shape[1]), ),\n    layers.Dropout(0.35),\n    layers.Dense(84, activation='relu'),\n    layers.Dropout(0.25),\n    layers.Dense(42, activation='relu'),\n    layers.Dropout(0.15),\n    layers.Dense(21, activation='relu'),\n    layers.Dropout(0.15),\n    layers.Dense(8, activation='relu'),\n    layers.Dense(1, activation='sigmoid')\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(\n    keras.optimizers.Adam(learning_rate=1e-3),\n    loss = tf.keras.losses.BinaryCrossentropy(),\n    metrics = tf.keras.metrics.AUC(name = 'AUC'), \n    )\n\nes = keras.callbacks.EarlyStopping(monitor = 'val_AUC', min_delta = 1e-4, patience = 5, mode = 'max', \n                       baseline = None, restore_best_weights = True, verbose = 0)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X_train, y_train, validation_data = (X_valid, y_valid), epochs = 2000, batch_size = 8197, callbacks = [es], verbose = 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\npd.DataFrame(history.history).plot(figsize=(8,5))\nplt.grid(True)\nplt.gca().set_ylim(0.4, 0.8) # set the vertical range to [0-1]\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}