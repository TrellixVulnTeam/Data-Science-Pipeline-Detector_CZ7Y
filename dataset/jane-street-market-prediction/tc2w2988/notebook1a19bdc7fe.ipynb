{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport xgboost as xgb\nimport optuna\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#把數據庫放進train裡面\ntrain = pd.read_csv('/kaggle/input/jane-street-market-prediction/train.csv', nrows=2000000)\ntrain.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#把weight=0的部分刪掉\ntrain = train[train['weight']!=0]\n\n# 創建action\n# 因為resp是用來當分類器所以把resp定義成action\n# 為了把resp的效用最大化所以要最大化pi，pi=∑j(weightij∗respij∗actionij)\n# resp會增加pi\ntrain['action'] = train['resp'].apply(lambda x:x>0).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = [col for col in list(train.columns) if 'feature' in col]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train[features]\ny = train['action']\n\n# 保留部分train的數據當作保留驗證集\ntrain_x, valid_x, train_y, valid_y = train_test_split(X, y, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 先檢查目標數據在訓練數據內有沒有平衡\nsns.set_palette(\"colorblind\")\nax = sns.barplot(train_y.value_counts().index, train_y.value_counts()/len(train_y))\nax.set_title(\"Proportion of trades with action=0 and action=1\")\nax.set_ylabel(\"Percentage\")\nax.set_xlabel(\"Action\")\nsns.despine();\n# 目標數據相當均衡，幾乎每個action的對應交易都占50％","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 繪製對角線相關圖，用來查看各個特徵的關聯性\ncorr = train_x.corr()\n\n\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n\nf, ax = plt.subplots(figsize=(12, 10))\n\n\ncmap = sns.diverging_palette(20, 230, as_cmap=True)\n\n\nsns.heatmap(corr, mask=mask, cmap=cmap, vmin=-1, vmax=1, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n# 有幾個feature有強烈的關聯性","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 調查feature的缺失值且進行估算\nmissing_values = pd.DataFrame()\nmissing_values['feature'] = features\nmissing_values['num_missing'] = [train_x[i].isna().sum() for i in features]\nmissing_values.T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_median = train_x.median()\n# 在訓練集和保留驗證集中估算中位數\ntrain_x = train_x.fillna(train_median)\nvalid_x = valid_x.fillna(train_median)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 要進行PCA前要先把所有feature內的資料標準化\nscaler = StandardScaler()\nscaler.fit(train_x)\ntrain_x_norm = scaler.transform(train_x)\n\npca = PCA()\ncomp = pca.fit(train_x_norm)\n\n# 繪製圖表顯示129個feature的變化如何隨feature的數量而變化\n# 前15個feature包括了大約80％的變化\n# 前40個feature包括大約95％的變化\n\n\nplt.plot(np.cumsum(comp.explained_variance_ratio_))\nplt.grid()\nplt.xlabel('Number of Principal Components')\nplt.ylabel('Explained Variance')\nsns.despine();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 只使用前50個feature來使用PCA而不使用所有feature來提高速度\npca = PCA(n_components=50).fit(train_x_norm)\ntrain_x_transform = pca.transform(train_x_norm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#轉換驗證集\nvalid_x_transform = pca.transform(scaler.transform(valid_x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dtrain = xgb.DMatrix(train_x_transform, label=train_y)\ndvalid = xgb.DMatrix(valid_x_transform, label=valid_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def objective(trial):\n    \n# 設定xgboost的參數\n# params設定xgboost要調整的參數範圍\n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 200, 600),\n        'max_depth': trial.suggest_int('max_depth', 10, 25),\n        'learning_rate': trial.suggest_uniform('learning_rate', 0.01, 0.1),\n        'subsample': trial.suggest_uniform('subsample', 0.50, 1),\n        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.50, 1),\n        'gamma': trial.suggest_int('gamma', 0, 10),\n        'tree_method': 'gpu_hist',  \n        'objective': 'binary:logistic'\n    }\n    \n    bst = xgb.train(params, dtrain)\n    preds = bst.predict(dvalid)\n    pred_labels = np.rint(preds)\n# 根據測試集的準確性評估並測試\n    accuracy = sklearn.metrics.accuracy_score(valid_y, pred_labels)\n    return accuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if __name__ == \"__main__\":\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=25, timeout=600)\n\n    print(\"Number of finished trials: \", len(study.trials))\n    print(\"Best trial:\")\n    trial = study.best_trial\n\n    print(\"  Value: {}\".format(trial.value))\n    print(\"  Params: \")\n    for key, value in trial.params.items():\n        print(\"    {}: {}\".format(key, value))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 讓xgboost的分類器使用最佳的參數\nbest_params = trial.params\nbest_params['tree_method'] = 'gpu_hist' \nbest_params['objective'] = 'binary:logistic'\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimal_clf = xgb.XGBClassifier(**best_params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimal_clf.fit(train_x_transform, train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 繪製最佳準確率如何隨train的次數增加\nfig = optuna.visualization.plot_optimization_history(study)\nfig.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 繪製參數改變的相對重要性\nfig = optuna.visualization.plot_param_importances(study)\nfig.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 利用中位數估算缺失值\ndef fillna_npwhere(array, values):\n    if np.isnan(array.sum()):\n        array = np.where(np.isnan(array), values, array)\n    return array","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import janestreet\nenv = janestreet.make_env() # initialize the environment\niter_test = env.iter_test() # an iterator which loops over the test set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for (test_df, sample_prediction_df) in iter_test:\n    wt = test_df.iloc[0].weight\n    if(wt == 0):\n        sample_prediction_df.action = 0 \n    else:\n        sample_prediction_df.action = optimal_clf.predict(pca.transform(scaler.transform(fillna_npwhere(test_df[features].values,train_median[features].values))))\n    env.predict(sample_prediction_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}