{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import janestreet\nenv = janestreet.make_env() # initialize the environment\niter_test = env.iter_test() # an iterator which loops over the test set\n\nfor (test_df, sample_prediction_df) in iter_test:\n    sample_prediction_df.action = 0 #make your 0/1 prediction here\n    env.predict(sample_prediction_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"import pandas as pd\n\ntrain = pd.read_csv('../input/jane-street-market-prediction/train.csv')\n\nprint(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"import numpy as np\n\nexample_test = pd.read_csv('../input/jane-street-market-prediction/example_test.csv')\n\nprint(example_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"import pandas as pd\n\ntrain = pd.read_csv('../input/jane-street-market-prediction/train.csv')\n\ntrain.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"import pandas as pd\n\nexample_test = pd.read_csv('../input/jane-street-market-prediction/example_test.csv')\n\nexample_test.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"import pandas as pd\n\nfeatures = pd.read_csv('../input/jane-street-market-prediction/features.csv')\n\nfeatures.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n\ntrain = pd.read_csv('../input/jane-street-market-prediction/train.csv')\nexample_test = pd.read_csv(\"../input/jane-street-market-prediction/example_test.csv\")\n\ntrain_x = train.drop(['resp', 'resp1', 'resp2', 'resp3', 'resp4'], axis=1)\ntrain_y = train['resp', 'resp1', 'resp2', 'resp3', 'resp4']\n\ntest_x = test.copy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfeatures = pd.read_csv('../input/jane-street-market-prediction/features.csv')\n\nfs = features.iloc[:,1:30]\n\nfs_i = fs.replace([True, False], [1,0])\n\nfs_i","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport numpy as np\nimport pandas as pd\nfeatures = pd.read_csv('../input/jane-street-market-prediction/features.csv')\n\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nX = dataset[:,1:30]\nY = dataset[:,0]\nseed = 1\n\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=seed)\n\nmodel = XGBClassifier()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nimport numpy as np\nimport pandas as pd\nfeatures = pd.read_csv('../input/jane-street-market-prediction/features.csv')\n\nfs = features.iloc[:,1:30]\n\nfs_i = fs.replace([True, False], [1,0])\n\n\nfrom sklearn.preprocessing import LabelEncoder\n\nfs_i = features()\nX = fs_i.data\ny = fs_i.target\nX_cols = fs_i.features_names\n\nimport xgboost as xgb\nfrom sklearn.model_selection import cross_validate,cross_val_predict, StratifiedKFold\n \nsplits = 5\nskf = StratifiedKFold(n_splits=splits, shuffle=True, random_state=42)\nscore_funcs = [\"accuracy\",\"precision_macro\",\"recall_macro\",\"f1_macro\"]\n \nclf = xgb.XGBClassifier(objective=\"binary:logistic\")\n \nscore = cross_validate(clf, X, y, cv=skf, scoring=score_funcs,return_estimator=True)\n \nprint(score[\"test_accuracy\"].mean())\nprint(score[\"test_precision_macro\"].mean())\nprint(score[\"test_recall_macro\"].mean())\nprint(score[\"test_f1_macro\"].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\ntrain = pd.read_csv('../input/jane-street-market-prediction/train.csv')\nexample_test = pd.read_csv(\"../input/jane-street-market-prediction/example_test.csv\")\n\ntrain_x = train.drop(['resp'], axis=1)\ntrain_y = train['resp']\n\ntest_x = example_test.copy\n\nxmean = train_x.mean()\nxstd = np.std(train_x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import skew\n%matplotlib inline\n\nresps = pd.DataFrame({'resp':train['resp'],\n                      'log(resp + 1)':np.log1p(train['resp'])})\n\nprint(resps, '\\n')\n\nprint('resp skew :', skew(resps['resp']))\nprint('log(resp + 1) skew :', skew(resps['log(resp + 1)']))\n\nplt.rcParams['figure.figsize'] = (12.0, 6.0)\nresps.hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import skew\n%matplotlib inline\n\nresps = pd.DataFrame({'resp_1':train['resp_1'],\n                      'log(resp_1 + 1)':np.log1p(train['resp_1'])})\n\nprint(resps, '\\n')\n\nprint('resp_1 skew :', skew(resps['resp_1']))\nprint('log(resp_1 + 1) skew :', skew(resps['log(resp_1 + 1)']))\n\nplt.rcParams['figure.figsize'] = (12.0, 6.0)\nresps.hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import skew\n%matplotlib inline\n\nresps = pd.DataFrame({'resp_2':train['resp_2'],\n                      'log(resp_2 + 1)':np.log1p(train['resp_2'])})\n\nprint(resps, '\\n')\n\nprint('resp_2 skew :', skew(resps['resp_2']))\nprint('log(resp_2 + 1) skew :', skew(resps['log(resp_2 + 1)']))\n\nplt.rcParams['figure.figsize'] = (12.0, 6.0)\nresps.hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import skew\n%matplotlib inline\n\nresps = pd.DataFrame({'resp_3':train['resp_3'],\n                      'log(resp_3 + 1)':np.log1p(train['resp_3'])})\n\nprint(resps, '\\n')\n\nprint('resp_3 skew :', skew(resps['resp_3']))\nprint('log(resp_3 + 1) skew :', skew(resps['log(resp_3 + 1)']))\n\nplt.rcParams['figure.figsize'] = (12.0, 6.0)\nresps.hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import skew\n%matplotlib inline\n\nresps = pd.DataFrame({'resp_4':train['resp_4'],\n                      'log(resp_4 + 1)':np.log1p(train['resp_4'])})\n\nprint(resps, '\\n')\n\nprint('resp_4 skew :', skew(resps['resp_4']))\nprint('log(resp_4 + 1) skew :', skew(resps['log(resp_4 + 1)']))\n\nplt.rcParams['figure.figsize'] = (12.0, 6.0)\nresps.hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPClassifier\ndf = pd.read_csv('../input/jane-street-market-prediction/train.csv')\narr = df.values\nX = arr[:,2:6]\ny = arr[:,0:1 & 7:]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\nclf = MLPClassifier()\nclf.fit(X_train, y_train)\nprint (clf.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\n\nmodel= GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)\nmodel.fit(X, y)\npredicted= model.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom sklearn import datasets\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model, Sequential\n\nfrom tensorflow.keras.layers import Dense, SimpleRNN\nfrom tensorflow.keras.optimizers import RMSprop\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n\ntrain = pd.read_csv('../input/jane-street-market-prediction/train.csv')\nexample_test = pd.read_csv(\"../input/jane-street-market-prediction/example_test.csv\")\n\ntrain = train.astype({c: np.float32 for c in train.select_dtypes(include='float64').columns})\n\ntrain = train.query('weight > 0').reset_index(drop = True)\n#train['action'] = (train['resp'] > 0).astype('int')\ntrain['action'] =  (  (train['resp_1'] > 0 ) & (train['resp_2'] > 0 ) & (train['resp_3'] > 0 ) & (train['resp_4'] > 0 ) &  (train['resp'] > 0 )   ).astype('int')\nfeatures = [c for c in train.columns if 'feature' in c]\n\nresp_cols = ['resp_1', 'resp_2', 'resp_3', 'resp', 'resp_4']\n\nX = train[features].values\ny = np.stack([(train[c] > 0).astype('int') for c in resp_cols]).T #Multitarget\n\nf_mean = np.mean(train[features[1:]].values,axis=0)\n\n\nx_train = train.drop(['resp', 'resp1', 'resp2', 'resp3', 'resp4'], axis=1)\ny_train = train['resp', 'resp1', 'resp2', 'resp3', 'resp4']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class RNN(Model):\n    def __init__(self, hidden_dim, output_dim):\n        super().__init__()\n        self.l1 = Dense(hidden_dim, kernel_initializer='he_normal')\n        self.b1 = BatchNormalization()\n        self.a1 = Activation('relu')\n        self.d1 = Dropout(0.5)\n        self.l2 = Dense(hidden_dim, kernel_initializer='he_normal')\n        self.b2 = BatchNormalization()\n        self.a2 = Activation('relu')\n        self.d2 = Dropout(0.5)\n        self.l3 = Dense(hidden_dim, kernel_initializer='he_normal')\n        self.b3 = BatchNormalization()\n        self.a3 = Activation('relu')\n        self.d3 = Dropout(0.5)\n        self.l4 = Dense(output_dim, kernel_initializer='he_normal'\n                        activation='sigmoid')\n        \n        self.ls = [self.l1, self.b1, self.a1, self.d1\n                   self.l2, self.b2, self.a2, self.d2\n                   self.l3, self.b3, self.a3, self.d3\n                   self.l4]\n        \n    def call(self, x)\n        for layer in self.ls:\n            x = layer(x)\n            \n        return x\n    \n    \n    \n    model = MLP(3, 1)\n    \n    \n    \n    criterion = losses.SparseCategoricalCrossentropy()\n    optimizer = optimizers.RMSprop(learning_rate=0.001, rho=0.99)\n    train_loss = metrics.Mean()\n    train_acc = metrics.SparseCategoricalAccuracy()\n    val_loss = metrics.Mean()\n    val_acc = metrics.SparseCategoricalAccuracy()\n    \n    def compute_loss(t,y):\n        return criterion(t,y)\n    \n    def train_step(x, t):\n        with tf.GradientTape() as tape:\n            preds = model(x)\n            loss = compute_loss(t, preds)\n        grads = tape.gradient(loss, model.trainable_variebles)\n        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n        train_loss(loss)\n        train_acc(t, preds)\n        return loss\n    \n    def val_step(x, t):\n        preds = model(x)\n        loss = compute_loss(t, preds)\n        val_loss(loss)\n        val_acc(t, preds)\n    \n    epochs = 1000\n    batch_size = 100\n    n_batches_train = x_train.shape[0] // batch_size\n    n_batches_val = x_val.shape[0] // batch_size\n    hist = {'val_loss': [], 'val_accuracy': [],\n           'val_loss': [], 'val_accuracy': []}\n    es = EarlyStopping(patience=5, verbose=1)\n    \n    \n    for epoch in range(epochs):\n        x_, t_ = shuffle(x_train, t_train)\n        \n        for batch in range(n_batches_train):\n            start = batch * batch_size\n            end = start + batch_size\n            train_step(x_[start:end], t_[start:end])\n            \n        for batch in range(n_batches_val):\n            start = batch * batch_size\n            end = start + batch_size\n            val_step(x_val[start:end], t_val[start:end])\n            \n        hist['val_loss'].append(val_loss.result())\n        hist['val_accuracy'].append(val_acc.result())\n        hist['val_loss'].append(val_loss.result())\n        hist['val_accuracy'].append(val_acc.result())\n            \n            \n        print('epoch: {}, loss: {:.3}, acc: {:.3f}'\n              ', val_loss: {:.3}, val_acc: {:.3f}'.format(\n            epoch+1,\n            train_loss.result(),\n            train_acc.result(),\n            val_loss.result(),\n            val_acc.result()\n        ))\n    \n    if es(val_loss.result()):\n        break\n\n    \n    val_loss = hist['val_loss']\n    \n    fig = plt.figure()\n    plt.rc('font', family='selif')\n    plt.plot(range(len(val_loss)), val_loss,\n            color='black', linewidth=1)\n    plt.xlabel('epochs')\n    plt.ylabel('loss')\n    # plt.savefig('output.jpg'): \n    plt.show()\n    \n    \n    \n    test_loss = metrics.Mean()\n    test_acc = metrics.SparseCategoricalAccuracy()\n    \n    def test_step(x, t):\n        preds = model(x)\n        loss = compute_loss(t, preds)\n        test_loss(loss)\n        test_acc(t, preds)\n        return loss\n    \n    test_step(x_test, t_test)\n    \n    print('test_loss: {:.3f}, test_acc: {:.3f}'.format(\n        test_loss.result(),\n        test_acc.result()\n    ))\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class EarlyStopping:\n    def __init__(self, patience=0, verbos=0):\n        self._step = 0\n        self._loss = float('inf')\n        self.patience = patience\n        self.verbose = verbose\n        \n    def __call__(self, loss):\n        if self._loss < loss:\n            self._step += 1\n            if self._step > self.patience:\n                if self.verbose:\n                    print('early stopping')\n                    return True\n                \n        else:\n            self._step = 0\n            self._loss = loss\n            \n        return False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}