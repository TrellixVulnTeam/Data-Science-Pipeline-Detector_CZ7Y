{"cells":[{"metadata":{},"cell_type":"markdown","source":"# SeriesNet\nI'm quite fond of using dilated convolutional neural networks (especially [SeriesNet](https://github.com/kristpapadopoulos/seriesnet)) when it comes to forecasting tasks, so this notebook is dedicated to doing just that. The advantage of using dilated convolutions is that it allows for a very large receptive field (i.e. look far back into the past). This is a work in progress, and I expect it'll be heavily modified. The notebook may of course also be abandoned if it turns out this approach is stupid for the task at hand :)\n\nThe architecture for SeriesNet looks as follows:\n\n<img src=\"https://i.postimg.cc/4xmbR8cH/Screenshot-from-2020-11-24-09-20-56.png\" width=\"50%\" />\n\n[Taken from official GitHub](https://github.com/kristpapadopoulos/seriesnet/blob/master/seriesnet-Krist-Papadopoulos-v1.pdf)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import gc\nimport math \nfrom tqdm import tqdm\n\nimport numpy as np\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Prep\nWithout having investigated the data too much at this point, I'll just treat it as a simple multivariate timeseries with a matrix `[timesteps X features]`, and the `resp` as the target."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Load the data\ndf = pd.read_csv('/kaggle/input/jane-street-market-prediction/train.csv').fillna(0)\n\n# Get a list of the feature columns\nFEATURES = [c for c in df.columns if 'feature' in c]\nTARGET   = 'resp'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scale the features in the data\nmeans = np.nanmean(df[FEATURES], axis=0)\nstds = np.nanstd(df[FEATURES], axis=0)\ndf[FEATURES] = (df[FEATURES] - means) / stds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define Model\nSeriesNet is basically just a convolutional neural network with dilations, allowing for a big receptive field. And since it's a convolutional net, we can put in multiple features and their previous values as well. Adapted directly from [SeriesNet github](https://github.com/kristpapadopoulos/seriesnet/blob/master/seriesnet.py)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.layers import Conv1D, Input, Add, Activation, Dropout\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.initializers import TruncatedNormal\nfrom tensorflow.keras.layers import LeakyReLU, ELU\nfrom tensorflow.keras.utils import Sequence\nfrom tensorflow.keras import optimizers\n\n\ndef DC_CNN_Block(nb_filter, filter_length, dilation, l2_layer_reg):\n    def f(input_):        \n        residual =    input_        \n        layer_out =   Conv1D(filters=nb_filter, kernel_size=filter_length, \n                      dilation_rate=dilation, \n                      activation='linear', padding='causal', use_bias=False,\n                      kernel_initializer=TruncatedNormal(mean=0.0, stddev=0.05, \n                      seed=42), kernel_regularizer=l2(l2_layer_reg))(input_)                    \n        layer_out =   Activation('selu')(layer_out)        \n        skip_out =    Conv1D(1,1, activation='linear', use_bias=False, \n                      kernel_initializer=TruncatedNormal(mean=0.0, stddev=0.05, \n                      seed=42), kernel_regularizer=l2(l2_layer_reg))(layer_out)        \n        network_in =  Conv1D(1,1, activation='linear', use_bias=False, \n                      kernel_initializer=TruncatedNormal(mean=0.0, stddev=0.05, \n                      seed=42), kernel_regularizer=l2(l2_layer_reg))(layer_out)                      \n        network_out = Add()([residual, network_in])        \n        return network_out, skip_out    \n    return f\n\n\ndef DC_CNN_Model(length, features):    \n    input = Input(shape=(length,features))    \n    l1a, l1b = DC_CNN_Block(32,2,1,0.001)(input)    \n    l2a, l2b = DC_CNN_Block(32,2,2,0.001)(l1a) \n    l3a, l3b = DC_CNN_Block(16,2,4,0.001)(l2a)\n    l4a, l4b = DC_CNN_Block(16,2,8,0.001)(l3a)\n    l5a, l5b = DC_CNN_Block(16,2,16,0.001)(l4a)\n    l6a, l6b = DC_CNN_Block(16,2,32,0.001)(l5a)\n    l6b = Dropout(0.8)(l6b) #dropout used to limit influence of earlier data\n    l7a, l7b = DC_CNN_Block(16,2,64,0.001)(l6a)\n    l7b = Dropout(0.8)(l7b) #dropout used to limit influence of earlier data\n    \n    l8 =   Add()([l1b, l2b, l3b, l4b, l5b, l6b, l7b])    \n    l9 =   Activation('relu')(l8)           \n    l21 =  Conv1D(1,1, activation='linear', use_bias=False, \n           kernel_initializer=TruncatedNormal(mean=0.0, stddev=0.05, seed=42),\n           kernel_regularizer=l2(0.001))(l9)\n    model = Model(inputs=input, outputs=l21)    \n    adam = optimizers.Adam(lr=0.00075, beta_1=0.9, beta_2=0.999, epsilon=None, \n                           decay=0.0, amsgrad=False)\n    model.compile(loss='mae', optimizer=adam, metrics=['mse'])    \n    return model\n\nclass DataIterator(Sequence):\n\n    def __init__(self, df, batch_size):        \n        length = len(df)\n        self.x = df[FEATURES].values.reshape(1, length, len(FEATURES))\n        self.y = df[TARGET].values.reshape(1, length, 1)    \n        self.batch_size = batch_size\n\n    def __len__(self):\n        return math.floor(len(self.x[0]) / self.batch_size)\n\n    def __getitem__(self, idx):\n        batch_x = self.x[:, idx * self.batch_size:(idx + 1) * self.batch_size]\n        batch_y = self.y[:, idx * self.batch_size:(idx + 1) * self.batch_size]\n        gc.collect()\n        return batch_x, batch_y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We cant fit all the data, so we chop it up in pieces\nBATCH_SIZE = 1000\n\n# Instantiate the model\nmodel = DC_CNN_Model(BATCH_SIZE, len(FEATURES))\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data generator. Can't fit it all in memory, so we chop it up\ngenerator = DataIterator(df=df, batch_size=BATCH_SIZE)\n\n# Train the model\nmodel.fit_generator(generator, epochs=2, workers=2, use_multiprocessing=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating Submission\nFinally time for doing the predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def shiftArray(arr, fill_value=np.nan):\n    result = np.empty_like(arr)\n    result[:1] = fill_value\n    result[1:] = arr[:-1]\n    return result\n\nimport janestreet\nenv = janestreet.make_env() # initialize the environment\niter_test = env.iter_test() # an iterator which loops over the test set\n\n# Use last data for initial timeseries data\neval_data = df.iloc[-BATCH_SIZE:][FEATURES].values\n\n# Delete data\ndel df\ngc.collect()\n\n# Loop through the environment\nfor (test_df, sample_prediction_df) in tqdm(iter_test):\n    \n    # Update evaluation data\n    eval_data = shiftArray(eval_data, fill_value=(test_df[FEATURES].values - means) / stds)\n    \n    # Prediction\n    pred = model(eval_data.reshape(1, BATCH_SIZE, len(FEATURES)), training=False)[0, -1, 0]\n    \n    # Submit prediction\n    sample_prediction_df.action = 1 if pred > 0 else 0\n    env.predict(sample_prediction_df)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}