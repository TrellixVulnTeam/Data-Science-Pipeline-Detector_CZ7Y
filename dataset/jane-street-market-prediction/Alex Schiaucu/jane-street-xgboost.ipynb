{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This notebook is based on [this](https://www.kaggle.com/wilddave/xgb-starter) starter kit. Thanks for sharing it!"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\n\nimport pandas as pd\npd.set_option('display.max_columns', 500)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport xgboost as xgb\nprint(\"XGBoost version:\", xgb.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Reading datasets')\n\ntrain = pd.read_csv('/kaggle/input/jane-street-market-prediction/train.csv')\n\nfeatures_meta = pd.read_csv('/kaggle/input/jane-street-market-prediction/features.csv')\n\nprint('Finished reading')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Shape of train dataset {} and of features {}'.format(train.shape, features_meta.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Eliminate 0-weights and create a binary action column, to be 1 if resp is positive and 0 otherwise."},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.query('date > 85')\ntrain_weights = train[train['weight'] != 0]\ndel train\n\ntrain_weights.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_weights.head(n=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_weights['resp_all'] = train_weights['resp'] + train_weights['resp_1'] + \\\ntrain_weights['resp_2'] + train_weights['resp_3'] + train_weights['resp_4']\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"See how different the signs of the resp and resp_all columns are"},{"metadata":{"trusted":true},"cell_type":"code","source":"resp_different = ( (((train_weights['resp_all'].values) > 0) & ((train_weights['resp'].values) < 0)) \\\n                  | (((train_weights['resp_all'].values) < 0) & ((train_weights['resp'].values) > 0)) )\nresp_different_count = ((resp_different > 0).astype(int)).sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('There are {0} differences out of {1} trades if we take in account resp_1,2,3,4 '.format(resp_different_count, train_weights.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Binarize the targets according to resp_all"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_weights['action'] = (train_weights['resp_all'].values > 0).astype(int)\ny_full = train_weights.loc[:, 'action']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of missing values in each column of training data\nmissing_val_count_by_column = (train_weights.isnull().sum())\nmissing_values_count = missing_val_count_by_column[missing_val_count_by_column > 0]\nprint('number of columns with missing values ', len(missing_values_count))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(missing_values_count.values, bins=50)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A lot of NaNs in the features, let's address that via feature selection with xgb classifier and sci-kit learn's selection model."},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_cols = [col_name for col_name in train_weights.columns if 'feature' in col_name]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Number of features ', len(feature_cols))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Make a dictionary with number of selections per feature, which will be used when splitting the train df into folds"},{"metadata":{"trusted":true},"cell_type":"code","source":"features_selection = {feature:0 for feature in feature_cols}\nprint('Length of feature selection dictionary ', len(features_selection))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_not_imputed = train_weights[feature_cols]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Handle missing values with simple imputer, to be able to select best features"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.impute import SimpleImputer\nsimple_imputer = SimpleImputer()\n\nX_imputed = pd.DataFrame(simple_imputer.fit_transform(X_not_imputed))\nX_imputed.columns = X_not_imputed.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some cleanup is mandatory, but keep X not imputed for later imputer fitting, when features are selected"},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_weights","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_imputed.head(n=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Define a method to return a XGBoost classifier with same configuration for feature selection and later generating the model used for prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_classifier(verbosity=0):\n    return xgb.XGBClassifier(\\\n                             n_estimators=1000,\\\n                             max_depth=7,\\\n                             learning_rate=0.05,\\\n                             missing=None,\\\n                             random_state=42,\\\n                             tree_method='gpu_hist',\\\n                             subsample=0.8,\\\n                             colsample_bytree=1,\\\n                             eval_metric='auc',\\\n                             objective='binary:logistic',\\\n                             verbosity=verbosity)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Define the method responsible for selecting a subset of features.\\\nWe'll use sci-kit learn's train test split with a train ratio of 80% and valid ratio of 20%"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectFromModel\nfrom sklearn.model_selection import train_test_split\n\ndef select_features_xgb(X, y):\n    \"\"\"Return selected features using xgb classifier \"\"\"\n    xgb_model_for_selection = make_classifier()\n    \n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                          random_state=42)\n    print('Fit xgb classifier for feature selection with shape of X ', X.shape)\n    xgb_model_for_selection.fit(X_train, y_train,\\\n                                eval_set=[(X_valid, y_valid)],\\\n                                early_stopping_rounds=3)\n\n    model_for_selection = SelectFromModel(xgb_model_for_selection, prefit=True)\n    print('Transform selection model')\n    X_selected = model_for_selection.transform(X)\n    print('Create df with selected features having non-zero variance')\n    selected_features = pd.DataFrame(model_for_selection.inverse_transform(X_selected), index = X.index, columns=X.columns)\n    del X_selected\n    print('Selecting columns wih best features')\n    selected_columns = selected_features.columns[selected_features.var() != 0]\n    del selected_features\n\n    selected_columns_list = list(selected_columns)\n    print('Number of selected columns ', len(selected_columns_list))\n    for feature_col in selected_columns_list:\n        features_selection[feature_col] += 1\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's choose the column features based on partitions in K folds and a dictionary counting occurrences of selected features per each fold."},{"metadata":{"trusted":true},"cell_type":"code","source":"folds = 4\nprint('Starting feature selection with {0} folds and rows of dataframe {1}'.format(folds, X_imputed.shape[0]))\nfor k in range(folds):\n    nb_elem_fold = int(X_imputed.shape[0] / folds)\n    left_margin = k * nb_elem_fold\n    right_margin = (k + 1) * nb_elem_fold if k < folds - 1 else X_imputed.shape[0]\n    print('Start for fold ', k)\n    selected_features_per_fold = select_features_xgb(X_imputed[left_margin:right_margin], y_full[left_margin:right_margin])\n    print('End fold ', k)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No more need for X_imputed"},{"metadata":{"trusted":true},"cell_type":"code","source":"del X_imputed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Dictionary of feature selection')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(features_selection.values(), bins=5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_at_least_1 = [feat_col for (feat_col, nb_times) in features_selection.items() if nb_times >= 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(features_at_least_1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We shall select only features that were selected at least once during the folding selection algorithm above"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_selected_features = features_at_least_1.copy()\nprint('Number of selected features is ', len(all_selected_features))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Recreate the imputer and fit it to the df with features that will be selected"},{"metadata":{"trusted":true},"cell_type":"code","source":"simple_imputer = SimpleImputer()\n\nX_ni_feat_sel = X_not_imputed.loc[:, all_selected_features]\ndel X_not_imputed\n\nX_full = pd.DataFrame(simple_imputer.fit_transform(X_ni_feat_sel))\nX_full.columns = X_ni_feat_sel.columns\ndel X_ni_feat_sel\n\nprint('Shape of X_full is ', X_full.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(X_full, y_full, train_size=0.8, test_size=0.2,\n                                                      random_state=42)\ndel X_full","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creating XGBoost classifier to be used for predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Creating classifier...', end='')\nmodel = make_classifier(1)\nprint('Finished.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit the model\nprint('Training classifier...', end='')\n%time model.fit(X_train, y_train,\\\n                eval_set=[(X_valid, y_valid)],\\\n                early_stopping_rounds=3)\nprint('Finished.')\nprint('Done')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And predict with the provided environment in the submission file"},{"metadata":{"trusted":true},"cell_type":"code","source":"import janestreet\nenv = janestreet.make_env() \n\nprint('Creating submissions file...', end='')\nrcount = 0\nfor (test_df, prediction_df) in env.iter_test():\n    test_df_selected_features = test_df.loc[:, all_selected_features]\n    X_test = pd.DataFrame(simple_imputer.transform(test_df_selected_features))\n    X_test.columns = test_df_selected_features.columns\n    \n    y_preds = model.predict(X_test)\n    prediction_df['action'] = y_preds\n    env.predict(prediction_df)\n    rcount += len(test_df.index)\nprint(f'Finished processing {rcount} rows.')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}