{"cells":[{"metadata":{},"cell_type":"markdown","source":"### This notebook has the following Notebooks as reference on detailed analysis and reasonable way of handling the missing values, and feature generation and selection. Apart from that there are few other good notebooks from where this notebook got some value addition pieces of ideas! I wish to thank my fellow kagglers who compel me to learn and grow!\n\n### [Kaggle Notebook] [Jane TF Keras LSTM](https://www.kaggle.com/rajkumarl/jane-tf-keras-lstm) (to fill missing values)\n### [Kaggle Notebook] [Jane Day 242 Feature Generation and Selection](https://www.kaggle.com/rajkumarl/jane-day-242-feature-generation-and-selection) (to generate and select features)\n"},{"metadata":{},"cell_type":"markdown","source":"# 1. IMPORT LIBRARIES"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom tensorflow.keras import models, layers, regularizers\nfrom sklearn.model_selection import KFold\nimport datatable\nimport gc\nimport warnings\nsns.set_style('darkgrid')\nwarnings.filterwarnings('ignore')\nSEED = 2222\ntf.random.set_seed(SEED)\nnp.random.seed(SEED)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. LOAD DATA AND OPTIMIZE MEMORY"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_path = '../input/jane-street-market-prediction/train.csv'\n\n# use datatable to load big data file\ntrain_file = datatable.fread(train_path).to_pandas()\ntrain_file.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# It is found from info() that there are only two datatypes - float64 and int32\nfor c in train_file.columns:\n    min_val, max_val = train_file[c].min(), train_file[c].max()\n    if train_file[c].dtype == 'float64':\n        if min_val>np.finfo(np.float16).min and max_val<np.finfo(np.float16).max:\n            train_file[c] = train_file[c].astype(np.float16)\n        elif min_val>np.finfo(np.float32).min and max_val<np.finfo(np.float32).max:\n            train_file[c] = train_file[c].astype(np.float32)\n    elif train_file[c].dtype == 'int32':\n        if min_val>np.iinfo(np.int8).min and max_val<np.iinfo(np.int8).max:\n            train_file[c] = train_file[c].astype(np.int8)\n        elif min_val>np.iinfo(np.int16).min and max_val<np.iinfo(np.int16).max:\n            train_file[c] = train_file[c].astype(np.int16)\ntrain_file.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### That's a great reduction in memory usage (around 74% reduction)! It will help us go further efficiently!"},{"metadata":{},"cell_type":"markdown","source":"# 3. HANDLING MISSING VALUES"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('There are %s NAN values in the train data'%train_file.isnull().sum().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = train_file.columns[train_file.columns.str.contains('feature')]\n\nval_range = train_file[features].max()-train_file[features].min()\nfiller = pd.Series(train_file[features].min()-0.01*val_range, index=features)\n# This filler value will be used as a constant replacement of missing values \n\n\n\"\"\"\nA function to fill all missing values with negative outliers as discussed in the referred notebook\nhttps://www.kaggle.com/rajkumarl/jane-tf-keras-lstm\n\"\"\"\ndef fill_missing(df):\n    df[features] = df[features].fillna(filler)\n    return df  \n\ntrain = fill_missing(train_file)\ntrain = train.loc[train.weight > 0]\ntrain.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Now we have %d missing values in our data\" %train.isnull().sum().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. FEATURE GENERATION AND SELECTION"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nfrom notebook\nhttps://www.kaggle.com/rajkumarl/jane-day-242-feature-generation-and-selection/comments\nbased on selected features\n\"\"\"\ndef feature_transforms(df):\n    # Generate Features using Linear shifting, Natural Logarithm and Square Root\n    for f in [f'feature_{i}' for i in [1,2,6,7,9,10,20,25,35,37,38,39,40,42,50,51,52,53,54,56,69,70,71,83,97,109,112,122,123,124,126,128,129]]: \n        # linear shifting to value above 1.0\n        df['pos_'+str(f)] = (df[f]+abs(train[f].min())+1).astype(np.float16)\n    for f in [f'feature_{i}' for i in [1,2,6,7,20,25,35,37,38,39,40,42,50,51,52,53,54,69,70,71,97,109,112,122,123,126,128,129]]: \n        # Natural log of all the values\n        df['log_'+str(f)] = np.log(df['pos_'+str(f)]).astype(np.float16)\n    for f in [f'feature_{i}' for i in [1,2,6,9,10,37,38,39,40,50,51,52,53,54,56,69,70,71,83,109,112,122,123,124,126,128,129]]: \n        # Square root of all the values\n        df['sqrt_'+str(f)] = np.sqrt(df['pos_'+str(f)]).astype(np.float16)\n    \n    # Linearly shifted values are used for log and sqrt transformations\n    # However they are useless since we have our original values which are 100% correlated\n    # Let's drop them from our data\n    df.drop([f'pos_feature_{i}' for i in [1,2,6,7,9,10,20,25,35,37,38,39,40,42,50,51,52,53,54,56,69,70,71,83,97,109,112,122,123,124,126,128,129]], inplace=True, axis=1)\n    \n    # From the Shap Dependence plots, the following features seem to have cubic relationship with target\n    cubic = [37, 39]\n    for i in cubic:\n        f = f'feature_{i}'\n        threes = np.full((len(df[f])), 3)\n        df['cub_'+f] =np.power(df[f], threes) \n        \n    # From the Shap Dependence plots, the following features seem to have quadratic relationship with target\n    quad = [53, 64, 67, 68]\n    for i in quad:\n        f = f'feature_{i}'\n        df['quad_'+f] =np.square(df[f]) \n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nfrom notebook\nhttps://www.kaggle.com/rajkumarl/jane-day-242-feature-generation-and-selection/comments\nbased on selected features\n\"\"\"\ndef manipulate_pairs(df):\n    # features that can be added together or subtracted\n    sub_pairs = [(3,6),(30,37)]\n    for i,j in sub_pairs:\n        df[f'sub_{i}_{j}'] = df[f'feature_{i}']-df[f'feature_{j}']\n\n    add_pairs = [(35,39)]\n    for i,j in add_pairs:\n        df[f'add_{i}_{j}'] = df[f'feature_{i}']+df[f'feature_{j}']\n\n    sub_log_pairs = [(9,20), (29,25), (109,7),(112,97)]\n    for i,j in sub_log_pairs:\n        df[f'sub_{i}_log{j}'] = df[f'feature_{i}']-df[f'log_feature_{j}']\n    \n    add_log_pairs = [(9,20), (29,25), (109,7), (112,97)]\n    for i,j in add_log_pairs:\n        df[f'add_{i}_log{j}'] = df[f'feature_{i}']+df[f'log_feature_{j}']\n        \n    # features that can be multiplied together\n    mul_pairs = [(39,95), (122,35)]\n    for i,j in mul_pairs:\n        df[f'mul_{i}_{j}'] = df[f'feature_{i}']*df[f'feature_{j}']\n\n    mul_log_pairs = [(6,42),(122,35)]\n    for i,j in mul_log_pairs:\n        df[f'mul_{i}_log{j}'] = df[f'feature_{i}']*df[f'log_feature_{j}']\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Perform feature generation\ntrain = feature_transforms(train)\ntrain = manipulate_pairs(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nfrom notebook\nhttps://www.kaggle.com/rajkumarl/jane-day-242-feature-generation-and-selection/comments\n\"\"\"\nselected_features = ['weight', 'feature_1', 'feature_2', 'feature_6', 'feature_9',\n       'feature_10', 'feature_16', 'feature_20', 'feature_29', 'feature_37',\n       'feature_38', 'feature_39', 'feature_40', 'feature_51', 'feature_52',\n       'feature_53', 'feature_54', 'feature_69', 'feature_70', 'feature_71',\n       'feature_83', 'feature_100', 'feature_109', 'feature_112',\n       'feature_122', 'feature_123', 'feature_124', 'feature_126',\n       'feature_128', 'feature_129', 'log_feature_1', 'log_feature_2',\n       'log_feature_6', 'log_feature_37', 'log_feature_38', 'log_feature_39',\n       'log_feature_40', 'log_feature_50', 'log_feature_51', 'log_feature_52',\n       'log_feature_53', 'log_feature_54', 'log_feature_69', 'log_feature_70',\n       'log_feature_71', 'log_feature_109', 'log_feature_112',\n       'log_feature_122', 'log_feature_123', 'log_feature_126',\n       'log_feature_128', 'log_feature_129', 'sqrt_feature_1',\n       'sqrt_feature_2', 'sqrt_feature_6', 'sqrt_feature_9', \n       'sqrt_feature_10', 'sqrt_feature_37', 'sqrt_feature_38', 'sqrt_feature_39',\n       'sqrt_feature_40', 'sqrt_feature_50', 'sqrt_feature_51',\n       'sqrt_feature_52', 'sqrt_feature_53', 'sqrt_feature_54',\n       'sqrt_feature_56', 'sqrt_feature_69', 'sqrt_feature_70',\n       'sqrt_feature_71', 'sqrt_feature_83', 'sqrt_feature_109',\n       'sqrt_feature_112', 'sqrt_feature_122', 'sqrt_feature_123',\n       'sqrt_feature_124', 'sqrt_feature_126', 'sqrt_feature_128',\n       'sqrt_feature_129', 'cub_feature_37', 'cub_feature_39',\n       'quad_feature_53', 'quad_feature_64', 'quad_feature_67',\n       'quad_feature_68', 'sub_3_6', 'sub_30_37', 'add_35_39', 'add_9_log20',\n       'sub_9_log20', 'add_29_log25', 'sub_29_log25', 'add_109_log7',\n       'sub_109_log7', 'add_112_log97', 'sub_112_log97', 'mul_39_95',\n       'mul_122_35', 'mul_6_log42', 'mul_122_log35']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Split data into examples (X) with `selected_features` and target (y) "},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train[selected_features]\ny = train[[c for c in train.columns if 'resp' in c]]\ny = (y>0)*1\ny = (y.mean(axis=1)>0.5).astype(np.int64)\nX.shape, y.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. PREPARE DATA FOR RESIDUAL NEURAL NET"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Expand dimension of dataset to adopt in the neural network\nX = np.expand_dims(X,-2)\ny = np.expand_dims(y,-1)\nX.shape, y.shape","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# save some memory\ndel train_file, train\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's incorporate 5-fold Cross Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_data = []\nfor train, val in KFold(3).split(X,y):\n    train_data = tf.data.Dataset.from_tensor_slices((X[train],y[train]))\n    val_data = tf.data.Dataset.from_tensor_slices((X[val],y[val]))\n    cv_data.append((train_data, val_data))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# save some memory\ndel X, y\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6. RESIDUAL NETWORK MODELING"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Residual(tf.keras.Model):  \n    \"\"\"The Residual layer of ResNet\"\"\"\n    def __init__(self, units):\n        super().__init__()\n        self.d1 = layers.Dense(units, kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4))\n        self.d2 = layers.Dense(units, kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4))\n        self.d3 = layers.Dense(units, kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4))\n        self.bn1 = layers.BatchNormalization()\n        self.bn2 = layers.BatchNormalization()\n\n    def call(self, X):\n        Y = tf.keras.activations.relu(self.bn1(self.d1(X)))\n        Y = layers.Dropout(0.3)(self.bn2(self.d2(Y)))\n        X = self.d3(X)\n        Y += X\n        return layers.Dropout(0.3)(tf.keras.activations.relu(Y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ResnetBlock(layers.Layer):\n    def __init__(self, num_units, num_residuals):\n        super(ResnetBlock, self).__init__()\n        self.residual_layers = []\n        for i in range(num_residuals):\n            self.residual_layers.append(Residual(num_units))\n\n    def call(self, X):\n        for layer in self.residual_layers.layers:\n            X = layer(X)\n        return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model():\n    model= tf.keras.Sequential([\n        layers.Input(shape=(100,)),\n        layers.GaussianNoise(0.2),\n        layers.Dense(64, kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4)),\n        layers.BatchNormalization(),\n        layers.Activation('relu'),\n        layers.Dropout(0.5),\n        ResnetBlock(64, 2),\n        ResnetBlock(128, 2),\n        ResnetBlock(256, 2),\n        ResnetBlock(512, 2),\n        layers.Dense(64, activation='relu'),\n        layers.Dense(1, activation='sigmoid')])\n    \n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n                  loss=tf.keras.losses.BinaryCrossentropy(), \n                  metrics=['accuracy'])\n    return model\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filler","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 7. TRAINING"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"train_accu = []\ntrain_loss = []\nval_accu = []\nval_loss = []\nmodels = []\nfold = 1\nfor train, val in cv_data:\n    model = create_model()\n    hist = model.fit(train_data.repeat(), \n                 epochs=40, \n                 steps_per_epoch=200, \n                 batch_size=5120, \n                 validation_data=val_data, \n                 validation_steps=100, \n                 verbose=0)\n    models.append(model)\n    train_accu.append(hist.history['accuracy'])\n    train_loss.append(hist.history['loss'])\n    val_accu.append(hist.history['val_accuracy'])\n    val_loss.append(hist.history['val_loss'])\n    print('\\n Fold completed: ',fold, '\\n')\n    fold += 1    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's visualize the model performance"},{"metadata":{"trusted":true},"cell_type":"code","source":"# plt.figure(figsize=(12,4))\nfor i in range(3):\n    plt.figure(figsize=(12,4))\n    plt.plot(train_loss[i], label=f'training_{i+1}')\n    plt.plot(val_loss[i], label=f'validation_{i+1}')\n    plt.ylabel('Binary CrossEntropy Loss', size=16)\n    plt.xlabel('Number of Epochs', size=16)\n    plt.title('Training vs Validation Loss', color='r', size=20)\n    plt.ylim([0.68,0.71])\n    plt.legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plt.figure(figsize=(12,4))\nfor i in range(3):\n    plt.figure(figsize=(12,4))\n    plt.plot(train_accu[i], label=f'training_{i+1}')\n    plt.plot(val_accu[i], label=f'validation_{i+1}')\n    plt.ylabel('Accuracy', size=16)\n    plt.xlabel('Number of Epochs', size=16)\n    plt.title('Training vs Validation Loss', color='r', size=20)\n    plt.legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model performance seems quite same for all three folds of data"},{"metadata":{},"cell_type":"markdown","source":"## Save model for inference"},{"metadata":{"trusted":true},"cell_type":"code","source":"for m in range(3):\n    model = models[m]\n    model.save_weights(f'resnet_select_feature_{m+1}.h5', save_format='h5')\n    print(f'model_{m+1} weights saved successfully.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Thank you for your time!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}