{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pickle\nimport janestreet\nimport pdb\n\ndef test_model(model, cols_to_drop):\n    env = janestreet.make_env() # initialize the environment\n    iter_test = env.iter_test() # an iterator which loops over the test set\n    for (test_df, sample_prediction_df) in iter_test:\n        test_df = test_df.drop(columns = ['date'])\n        test_df = test_df.drop(columns = cols_to_drop)\n        sample_prediction_df[\"action\"] = model.predict(test_df).astype(int)\n        env.predict(sample_prediction_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport pdb\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.model_selection import cross_val_score\nfrom xgboost.sklearn import XGBClassifier\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n\n\n#from sklearn import cross_validation, metrics   #Additional scklearn functions\n#from sklearn.grid_search import GridSearchCV   #Perforing grid search\nimport matplotlib.pylab as plt\n\nfrom matplotlib.pylab import rcParams\nrcParams['figure.figsize'] = 12, 4\n\ndef linear_reg(X_train, y_train):\n\tmodel = LinearRegression()\n\tmodel.fit(X_train, y_train)\n\treturn model\n\ndef logistic_regression(X_train, y_train):\n\tmodel = LogisticRegression(random_state = 0)\n\tmodel.fit(X_train, y_train)\n\treturn model\n\ndef knn(X_train, y_train):\n\tmodel = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p =2)\n\tmodel.fit(X_train, y_train)\n\treturn model\n\ndef svm(X_train, y_train, kernel):\n\tmodel = SVC(kernel = kernel, random_state = 0)\n\tmodel.fit(X_train, y_train)\n\treturn model\n\ndef naive_bayes(X_train, y_train):\n\tmodel = GaussianNB()\n\tmodel.fit(X_train, y_train)\n\treturn model\n\ndef random_forest(X_train, y_train):\n\tmodel = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n\tmodel.fit(X_train, y_train)\n\treturn model\n\n\ndef random_forest_time(X_train,y_train, X_test, y_test):\n\tclassifier = TimeSeriesForest()\n\tclassifier.fit(X_train, y_train)\n\treturn classifier\n\n\n#Best params for LGBM after hyper parameter tuning with Optuna\ndef light_gbm(X_train, y_train):\n\n    model = lgb.LGBMClassifier(boosting_type= 'gbdt',\n    lambda_l1= 0.9656,\n    lambda_l2= 3.164298687541461e-06,\n    num_leaves= 58,\n    feature_fraction= 0.5677801334905795,\n    bagging_fraction= 0.5522201790927705,\n    bagging_freq= 2,\n    min_child_samples= 83,\n    min_child_weight= 100.0,\n    scale_pos_weight= 1)\n    \n    model.fit(X_train, y_train)\n    return model\n    \n#Best params for XGB after hyper parameter tuning with Optuna\ndef xgb(X_train, y_train):\n    model = XGBClassifier(n_estimators= 453,\n\tmax_depth= 9, \n\tlearning_rate= 0.02509726938336729, \n\tsubsample=0.5231595958438151, \n\tcolsample_bytree= 0.5572267372335459,\n\tgamma= 19, \n\tscale_pos_weight= 0.9992243147907729,\n\tmissing= -999,\n\tnthread= 4,\n\tverbosity= 0,\n\tobjective= 'binary:logistic', \n\tuse_label_encoder= False)\n    \n    model.fit(X_train, y_train)\n    return model\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport datatable as dt\nimport time\nimport pdb\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom scipy import stats\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\n#This function creates a pickle file for faster reading. Only needs to be called once.\ndef read_data():\n    '''\n    Read data as datatable\n    Convert to dataframe and store as pkl file\n    '''\n    train_data_datatable = dt.fread('data/train.csv')\n    train_data = train_data_datatable.to_pandas()\n    train_data.to_pickle('data/train_pickle.pkl')\n    return train_data\n\n#This function transforms all resp values to 1 when positive and 0 when negative\ndef discretization_resp(data):\n    '''\n    Create action column based on resp values\n    Drop columns: resp, resp_1, resp_2, resp_3, resp_4\n    '''\n    data['action'] = np.where(data[\"resp\"] <= 0, 0, 1)\n    drop_features = ['date','resp','resp_1', 'resp_2', 'resp_3', 'resp_4'] \n    data = data.drop(columns = drop_features)\n    #data = data.drop(index = data[data[\"weight\"] == 0].index)\n    return data\n\n#This function replaces all missing values with mean of that column/feature\ndef missing_values(X):\n    imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n    X = imputer.fit_transform(X)\n    X = pd.DataFrame(X, columns = col_names)\n    return X\n    \n#This function reduces the dimensions using PCA\ndef dimensionality_reduction(X, pca_obj = None):\n    '''\n    If a PCA object is passed in arguements, transform data with this object - Used to fit test data with previous PCA object\n    Else, fit the data on new object and return data and PCA Object - used for train data\n    '''\n    col_names = X.columns\n    if pca_obj != None:\n        pca = pca_obj\n        X = pca.transform(X)\n    else:\n        pca = PCA(n_components = 0.9)\n        X = pca.fit_transform(X)\n    n_pcs= pca.n_components_ \n    most_important = [np.abs(pca.components_[i]).argmax() for i in range(n_pcs)]\n    most_important_names = [col_names[most_important[i]] for i in range(n_pcs)]\n    \n    return pd.DataFrame(X), pca\n\n#This function extracts information such as mean and variances of attributes\ndef featrue_engineering(df):\n    means = []\n    variances = []\n    for column in df.columns:\n        means.append(np.mean(df[column]))\n        variances.append(np.var(df[column]))\n    return df\n\n#This function scales features within the range of [0,1]\ndef feature_scaling(X):\n    col_names = X.columns\n    std_scaler = StandardScaler()\n    X = std_scaler.fit_transform(X)\n    return pd.DataFrame(X, columns = col_names)\n\n#This function reduces the dimensions by computing feature correlations \ndef feature_correlations(X):\n    '''\n    Extract related features info from features.csv\n    Compute feature correlations and remove features which have high correlations (> 0.9)\n    '''\n    columns_to_drop = []\n    features = pd.read_csv(\"/kaggle/input/jane-street-market-prediction/features.csv\")\n    for column in features.columns:\n        correlations = abs(X[features[features[column] == True][\"feature\"] ].corr())\n        for col in correlations.columns:\n            if col in columns_to_drop:\n                continue\n            else:\n                remove_col = correlations[correlations[col] > 0.9].index\n                if col in remove_col:\n                    remove_col = remove_col.drop(col)\n                for element in remove_col:\n                    if element in columns_to_drop:\n                        pass\n                    else:\n                        columns_to_drop.append(element)\n    return columns_to_drop\n\n#This function computes the outliers in the data\ndef remove_outliers(data, threshold = 4):\n    '''\n    Compute the z-score of all rows in the data\n    Remove rows that have z score  > 4 \n    '''\n    z = np.abs(stats.zscore(data, nan_policy='omit'))\n    row_index = np.where(z > threshold)\n    data = data[(z < threshold).all(axis=1)].reset_index(drop=True)\n    return data\n\n\ndef main():\n    data = pd.read_pickle('/kaggle/input/reduce-train-data/red_train_pickle.pkl')\n    data = data.sort_values(by = ['date','ts_id'])\n    data = data.set_index('ts_id')\n\n    data = discretization_resp(data)\n\n    data = remove_outliers(data)\n    #test_data = remove_outliers(test_data)\n    X_train = data.loc[:, data.columns != 'action']\n    y_train = data['action']\n\n    '''\n    X_test = test_data.loc[:, test_data.columns != 'action']\n    y_test = test_data['action']\n    '''\n    \n    cols_to_remove = feature_correlations(X_train)\n    X_train = X_train.drop(columns = cols_to_remove)\n    \n    #X_train = feature_scaling(X_train)\n    #X_train,pca_obj = dimensionality_reduction(X_train)\n\n    #X_test = feature_scaling(X_test)\n    #X_test, pca_obj = dimensionality_reduction(X_test, pca_obj)\n    \n    models_list = ['lin','lgbm','xgb']\n    for model_name in models_list:\n        print(\"Model: \",model_name)\n        if model_name == 'lr':\n            model = logistic_regression(X_train, y_train)\n        elif model_name == 'lin':\n            model = linear_reg(X_train, y_train)\n        elif model_name == 'knn':\n            model = knn(X_train, y_train)\n        elif model_name == 'svm':\n            model = svm(X_train, y_train, kernel = 'linear')\n        elif model_name == 'rbf_svm':\n            model = svm(X_train, y_train, kernel = 'rbf')\n        elif model_name == 'nb':\n            model = naive_bayes(X_train, y_train)\n        elif model_name == 'rf':\n            model = random_forest(X_train, y_train)\n        elif model_name == 'xgb':\n            model = xgb(X_train, y_train)\n        elif model_name == 'lgbm':\n            model = light_gbm(X_train, y_train)\n        else:\n            pass\n        #pdb.set_trace()\n        test_model(model, cols_to_remove)\n    \nif __name__ == \"__main__\":\n    main()\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}