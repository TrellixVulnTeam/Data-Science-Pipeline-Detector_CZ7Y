{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\npd.plotting.register_matplotlib_converters()\nplt.rc(\"figure\", figsize=(12,5))\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Chargement des jeux de données"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/jane-street-market-prediction/train.csv\")#data\n\nsample = pd.read_csv(\"/kaggle/input/jane-street-market-prediction/example_sample_submission.csv\")#test\n\ntest =  pd.read_csv(\"/kaggle/input/jane-street-market-prediction/example_test.csv\")\n\nfeature = pd.read_csv(\"/kaggle/input/jane-street-market-prediction/features.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ajout de la colonne Action \nPour ajouter cette colonne, il faut recuperer les informations des colonnes ['resp_1','resp_2','resp_3', 'resp_4', 'resp']. Si ces cinq valeurs sont positifs alors la valeur de la nouvelle colonne ['action'] est fixé a 1."},{"metadata":{"trusted":true},"cell_type":"code","source":"train['action'] =  ((train['resp_1'] > 0 ) & (train['resp_2'] > 0 ) & (train['resp_3'] > 0 ) & (train['resp_4'] > 0 ) & (train['resp'] > 0 )).astype('int')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"size = int(len(train)*0.1)\ntrain_reduct = train.iloc[:size]\nnp.shape(train_reduct)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualisation de la répartition des classes \nLa colonne ['action'] est la variable à predire. Il est interessant d'observer la répartition des classes de la variable à prédire.\nSi la proportion des classes est déséquilibrée alors le résultat de la prédiction peut être biaisé."},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"action\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_pct = []\nfor i in range(0,2):\n    value = len(train_reduct[train_reduct[\"action\"]==i])\n    value_pct = value/len(train_reduct)*100\n    list_pct.append(value_pct)\nplt.bar(['0','1'], [list_pct[0],list_pct[1]])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Remplacement des valeurs manquantes\nCette étape est importante car la valeur choisie pour le remplacement peut modifier le résultat de prédiction.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_r = train_reduct.replace(np.nan, 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Etude des liens entre les variables\nCette partie permet de visualiser les liens linéaires entre les variables explicatives et la variable cible"},{"metadata":{"trusted":true},"cell_type":"code","source":"col_features = [c for c in train.columns if 'feature' in c]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"var_exp = train_r[col_features]\nvar_target = train_r[\"action\"]\ncorr = var_exp.corrwith(var_target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr.plot.bar(figsize = (30, 15),title = \"Correlation\" , fontsize = 20,\n        rot = 90, grid = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Importance des variables\nL'analyse en composante principale permet d'observer le nombre de variable apportant de l'information"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nx_exp = var_exp.values\nx_aexp = var_target.values\nmean = np.mean(x_exp, axis=0)\ncov_matrix = (x_exp - mean).T.dot((x_exp - mean)) / (x_exp.shape[0]-1)\n\nnbr_exp = np.shape(x_exp)[1]\n\n# normalisation des donnees\nfrom sklearn.preprocessing import StandardScaler\n\nX_std = StandardScaler().fit_transform(x_exp)\n# extraction des valeurs propres et vecteurs propres de la matrice de covariance\neig_vals, eig_vecs = np.linalg.eig(cov_matrix)\n# creation de paire de valeurs et vecteurs propres\neig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\neig_pairs.sort(key=lambda x: x[0], reverse=True)\ntot = sum(eig_vals)\n# calcul de la variance expliquee\nvar_exp = [(i / tot)*100 for i in sorted(eig_vals, reverse=True)]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15, 5))\n\nplt.bar(range(nbr_exp), var_exp, alpha=0.5, align='center',\n    label='Variance expliquée')\nplt.ylabel('Variance expliquée ratio')\nplt.xlabel('Composantes principales')\nplt.legend(loc='best')\nplt.tight_layout()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Préparation à la prédiction\nIl faut dans un premier temps séparer le jeu de données en deux parties: jeu d'entrainement et jeu de test\nEnsuite il faut normaliser le jeu d'entrainement pour ensuite normaliser le jeu de test."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\npourc_train = 0.7\ncol_target = \"action\"\ncol_feature = [c for c in train_reduct.columns if 'feature' in c]\n\n\n# X : matrice de variables explicatives \nX = train_r[col_feature]\n# Y : vecteur (cas univarié) ou matrice (cas multivarié) de variable a expliquer\nY = np.array(train_r[col_target]).reshape(-1,1)\n\nsize = int(len(train_r)*pourc_train)\nX_train, Y_train = X[:size], Y[:size]\nX_test, Y_test = X[size:], Y[size:]\n\n# Normalisation\nscaler = MinMaxScaler()\nscaler = MinMaxScaler()\nX_train_scale = scaler.fit_transform(X_train)\nY_train_scale = scaler.fit_transform(Y_train)\n\nX_test_scale = scaler.transform(X_test)\nY_test_scale = scaler.transform(Y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modèle de prédiction\nQuatre modèles vont être challenger pour la classification: Random Forest, Regression Logistique, SVM et un modèle à base de réseau de neurone récurrent"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import svm\nfrom sklearn.metrics import mean_squared_error\n\nmodel_rfc = RandomForestClassifier(max_depth=2, random_state=0)\nmodel_rfc.fit(X_train, Y_train)\npred_rfc = model_rfc.predict(X_test) \n\nmodel_rlog = LogisticRegression(random_state=0, solver='lbfgs', multi_class='ovr').fit(X_train, Y_train)\nmodel_rlog.fit(X_train, Y_train)\npred_rlog = model_rlog.predict(X_test) \n\nmodel_svm = svm.LinearSVC()\nmodel_svm.fit(X_train, Y_train)\npred_svm = model_svm.predict(X_test) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluation de la performance"},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install termtables\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\ny_pred = pred_rfc\ny_true = Y_test_scale.reshape(1,-1)[0]\nprint(\"Matrice de confusion:\")\ntn, fp, fn, tp = confusion_matrix(y_true,y_pred).ravel()\nval_0_b = np.round(tn/len(y_true)*100,2)\nval_0_m = np.round(fn/len(y_true)*100,2)\nval_1_b = np.round(tp/len(y_true)*100,2)\nval_1_m = np.round(fp/len(y_true)*100,2)\nimport termtables\nheader = [\" \", \"Classe 0\", \"Classe 1\"]\ndata = [\n    [\"Bien classé (%)\", val_0_b, val_1_b],\n    [\"Mal classé (%)\", val_0_m , val_1_m]]\n\ntable = termtables.to_string(data, header=header)\nprint(table)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.api as sm\nsm.stats.acorr_ljungbox(train_reduct[\"feature_3\"], lags=[10], return_df=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Verification de la distribution des variables explicatives"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_reduct[[\"feature_31\",\"feature_32\"]].boxplot()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}