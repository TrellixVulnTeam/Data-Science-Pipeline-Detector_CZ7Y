{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Jane Street Market Prediction\n\nReference:  https://www.kaggle.com/c/jane-street-market-prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport pandas as pd\nimport numpy as np\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sb\n\nimport tensorflow as tf\n\nimport sys\n\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n%matplotlib inline\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Functions To Get Trainning Data\n\nDue to the size of trainning data, supporting functions are available to create and read a sample of trainning data."},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN_FILE = '/kaggle/input/jane-street-market-prediction/train.csv'\nSAMPLE_TRAINNING_FILE = './train-sample.csv'\n\n\n# Get all trainning data\ndef get_trainning_data():\n    print(\"Reading training data...\")\n    df = pd.read_csv(TRAIN_FILE)\n    return df\n\n# Create sample of trainning data\ndef create_trainning_sample(frac = 0.2):\n    df_train = get_trainning_data();\n    print(\"Creating sample file...\")\n    df_sample = df_train.sample(frac=frac)\n    df_sample.to_csv(SAMPLE_TRAINNING_FILE, index=False, header=True)\n    \ndef get_sample_trainning_data():\n    print(\"Reading sample training data...\")\n    # Read training data\n    return pd.read_csv(SAMPLE_TRAINNING_FILE)    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create Trainning Data Sample\n\nRun this block to create or recreate the sample trainning data file."},{"metadata":{"trusted":true},"cell_type":"code","source":"#create_trainning_sample(frac = 0.02);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Start Exploration"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read only sample data created in previous step above\n#df_train = get_sample_trainning_data()\n\n# Read all the data\ndf_train = get_trainning_data()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Generate action column...\")\ndf_train['action'] = 0\ndf_train.loc[(df_train['resp'] > 0), 'action'] = 1\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for balance\nsb.distplot(df_train['action']);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Sort By Time"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.sort_values(by=['date', 'ts_id'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Delete rows"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = df_train[df_train['weight'] != 0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Drop unecessary columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"columns_to_delete = ['date', 'weight', 'resp_1', 'resp_2', 'resp_3', 'resp_4', 'resp', 'ts_id']\nfor column in columns_to_delete:\n    df_train.drop(column, axis=1, inplace=True)\n    \nfeature_count = len(df_train.columns) - 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Handle Missing Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.fillna(df_train.mean(),inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reset Index"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = df_train.reset_index(drop=True)\ndf_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 20 / 80 split\ndf_validation, df_train = np.split(df_train, [int(.2*len(df_train))])\n\ndf_train = df_train.reset_index(drop=True)\ndf_validation = df_validation.reset_index(drop=True)\n\ndf_train_labels = df_train['action'].copy()\ndf_train.drop('action', inplace=True, axis=1)\n\ndf_validation_labels = df_validation['action'].copy()\ndf_validation.drop('action', inplace=True, axis=1)\n\nprint(\"Shapes: (train={}, train_labels={}, validation={}, validation_labels={})\".format(df_train.shape, df_train_labels.shape, df_validation.shape, df_validation_labels.shape))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ninput_length = 10\nbatch_size = 100\n\ngenerator_train = tf.keras.preprocessing.sequence.TimeseriesGenerator(\n    df_train, \n    df_train_labels,\n    length=input_length,\n    batch_size=batch_size)\n\ngenerator_validation = tf.keras.preprocessing.sequence.TimeseriesGenerator(\n    df_validation, \n    df_validation_labels,\n    length=input_length,\n    batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# some values that may or may not get used as I experiment\nBATCH_SIZE=50\nEPOCHS = 200\nSTEPS = len(df_train) / BATCH_SIZE\nLEARNING_RATE = 0.001","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"\nmodel = tf.keras.models.Sequential()\n\nmodel.add(tf.keras.layers.LSTM(\n    feature_count, \n    activation='relu', \n    input_shape=(input_length, feature_count), \n    return_sequences=True))\n\nmodel.add(tf.keras.layers.Dropout(0.2))\n\nmodel.add(tf.keras.layers.LSTM(\n    feature_count, \n    activation='relu', \n    input_shape=(input_length, feature_count), \n    return_sequences=True))\n\nmodel.add(tf.keras.layers.Dropout(0.2))\n\nmodel.add(tf.keras.layers.LSTM(\n    feature_count, \n    activation='relu', \n    input_shape=(input_length, feature_count), \n    return_sequences=True))\n\nmodel.add(tf.keras.layers.Dropout(0.2))\n\nmodel.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n\nmodel.compile(loss=tf.keras.losses.Hinge(), #tf.keras.losses.BinaryCrossentropy(), \n                optimizer=tf.optimizers.Adam(learning_rate=LEARNING_RATE),\n                metrics=[\"accuracy\"])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"es = EarlyStopping(monitor='val_loss', verbose=1, patience=5)\n\nmodel.fit(generator_train,\n          validation_data = generator_validation,\n          epochs = EPOCHS,\n          verbose = 1,\n          callbacks = [es]\n    )  \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_history = model.history.history\n\nplt.figure(1)\nplt.subplot(211)\nplt.ylim(top=5)\nplt.plot(train_history['loss'])\nplt.ylabel('Average Loss Per Epoch')\nplt.xlabel('Epoch')\nplt.title('Average Loss Per Epoch vs Epoch')\n\nplt.subplot(212)\nplt.ylim(top=15)\nplt.plot(train_history['val_loss'])\nplt.ylabel('Val Loss per Epoch')\nplt.xlabel('epoch')\nplt.title('Val Loss per Epoch vs Epoch')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import janestreet\nenv = janestreet.make_env() # initialize the environment\niter_test = env.iter_test() # an iterator which loops over the test set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Starting predictions...\")\n\nfor (test_df, sample_prediction_df) in iter_test:\n    # drop columns\n    weight = test_df['weight'].item()\n    columns_to_delete = ['date', 'weight']\n    for column in columns_to_delete:\n        test_df.drop(column, axis=1, inplace=True)\n        \n    if weight > 0:  \n        test_df.fillna(test_df.mean(),inplace=True)\n        prediction = model.predict(np.array(test_df.values).reshape(1,1,130));\n        if prediction[0][0][0] > 0.79:\n            action = 1\n        else:\n            action = 0\n        sample_prediction_df.action = action\n    else:\n        sample_prediction_df.action = 0\n      \n    print(\"\\rPrediciton: (index={}, action={})\".format(sample_prediction_df.index.values[0], sample_prediction_df.action.values[0]), end=\"\");\n    env.predict(sample_prediction_df)   \n\nprint(\"\\nFinished predictions\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}