{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!mkdir cache","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-24T19:09:08.480558Z","iopub.execute_input":"2021-08-24T19:09:08.481029Z","iopub.status.idle":"2021-08-24T19:09:09.149179Z","shell.execute_reply.started":"2021-08-24T19:09:08.48093Z","shell.execute_reply":"2021-08-24T19:09:09.148188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\nfrom sklearn.utils.validation import _deprecate_positional_args\n\n\n# modified code for group gaps; source\n# https://github.com/getgaurav2/scikit-learn/blob/d4a3af5cc9da3a76f0266932644b884c99724c57/sklearn/model_selection/_split.py#L2243\nclass PurgedGroupTimeSeriesSplit(_BaseKFold):\n    \"\"\"Time Series cross-validator variant with non-overlapping groups.\n    Allows for a gap in groups to avoid potentially leaking info from\n    train into test if the model has windowed or lag features.\n    Provides train/test indices to split time series data samples\n    that are observed at fixed time intervals according to a\n    third-party provided group.\n    In each split, test indices must be higher than before, and thus shuffling\n    in cross validator is inappropriate.\n    This cross-validation object is a variation of :class:`KFold`.\n    In the kth split, it returns first k folds as train set and the\n    (k+1)th fold as test set.\n    The same group will not appear in two different folds (the number of\n    distinct groups has to be at least equal to the number of folds).\n    Note that unlike standard cross-validation methods, successive\n    training sets are supersets of those that come before them.\n    Read more in the :ref:`User Guide <cross_validation>`.\n    Parameters\n    ----------\n    n_splits : int, default=5\n        Number of splits. Must be at least 2.\n    max_train_group_size : int, default=Inf\n        Maximum group size for a single training set.\n    group_gap : int, default=None\n        Gap between train and test\n    max_test_group_size : int, default=Inf\n        We discard this number of groups from the end of each train split\n    \"\"\"\n\n    @_deprecate_positional_args\n    def __init__(self,\n                 n_splits=5,\n                 *,\n                 max_train_group_size=np.inf,\n                 max_test_group_size=np.inf,\n                 group_gap=None,\n                 verbose=False\n                 ):\n        super().__init__(n_splits, shuffle=False, random_state=None)\n        self.max_train_group_size = max_train_group_size\n        self.group_gap = group_gap\n        self.max_test_group_size = max_test_group_size\n        self.verbose = verbose\n\n    def split(self, X, y=None, groups=None):\n        \"\"\"Generate indices to split data into training and test set.\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n        y : array-like of shape (n_samples,)\n            Always ignored, exists for compatibility.\n        groups : array-like of shape (n_samples,)\n            Group labels for the samples used while splitting the dataset into\n            train/test set.\n        Yields\n        ------\n        train : ndarray\n            The training set indices for that split.\n        test : ndarray\n            The testing set indices for that split.\n        \"\"\"\n        if groups is None:\n            raise ValueError(\n                \"The 'groups' parameter should not be None\")\n        X, y, groups = indexable(X, y, groups)\n        n_samples = _num_samples(X)\n        n_splits = self.n_splits\n        group_gap = self.group_gap\n        max_test_group_size = self.max_test_group_size\n        max_train_group_size = self.max_train_group_size\n        n_folds = n_splits + 1\n        group_dict = {}\n        u, ind = np.unique(groups, return_index=True)\n        unique_groups = u[np.argsort(ind)]\n        n_samples = _num_samples(X)\n        n_groups = _num_samples(unique_groups)\n        for idx in np.arange(n_samples):\n            if (groups[idx] in group_dict):\n                group_dict[groups[idx]].append(idx)\n            else:\n                group_dict[groups[idx]] = [idx]\n        if n_folds > n_groups:\n            raise ValueError(\n                (\"Cannot have number of folds={0} greater than\"\n                 \" the number of groups={1}\").format(n_folds,\n                                                     n_groups))\n\n        group_test_size = min(n_groups // n_folds, max_test_group_size)\n        group_test_starts = range(n_groups - n_splits * group_test_size,\n                                  n_groups, group_test_size)\n        for group_test_start in group_test_starts:\n            train_array = []\n            test_array = []\n\n            group_st = max(0, group_test_start - group_gap - max_train_group_size)\n            for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\n                train_array_tmp = group_dict[train_group_idx]\n\n                train_array = np.sort(np.unique(\n                    np.concatenate((train_array,\n                                    train_array_tmp)),\n                    axis=None), axis=None)\n\n            train_end = train_array.size\n\n            for test_group_idx in unique_groups[group_test_start:\n            group_test_start +\n            group_test_size]:\n                test_array_tmp = group_dict[test_group_idx]\n                test_array = np.sort(np.unique(\n                    np.concatenate((test_array,\n                                    test_array_tmp)),\n                    axis=None), axis=None)\n\n            test_array = test_array[group_gap:]\n\n            if self.verbose > 0:\n                pass\n\n            yield [int(i) for i in train_array], [int(i) for i in test_array]\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-24T19:09:09.152652Z","iopub.execute_input":"2021-08-24T19:09:09.152916Z","iopub.status.idle":"2021-08-24T19:09:09.926055Z","shell.execute_reply.started":"2021-08-24T19:09:09.152889Z","shell.execute_reply":"2021-08-24T19:09:09.925157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import copy\nimport os\n\nimport numpy as np\nimport pandas as pd\nimport datatable as dt\nimport pytorch_lightning as pl\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, Subset, BatchSampler, SequentialSampler, DataLoader\nfrom numba import njit\nfrom pytorch_lightning import Callback\nfrom pytorch_lightning import loggers as pl_loggers\nfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping\nfrom pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\nfrom sklearn.metrics import roc_auc_score\nfrom tqdm import tqdm\n\nimport janestreet\n\n\nclass FinData(Dataset):\n    def __init__(self, data, target, date, mode='train', transform=None, cache_dir=None, multi=False):\n        self.data = data\n        self.target = target\n        self.mode = mode\n        self.transform = transform\n        self.cache_dir = cache_dir\n        self.date = date\n        self.multi = multi\n\n    def __getitem__(self, index):\n        if torch.is_tensor(index):\n            index.to_list()\n        if self.transform:\n            return self.transform(self.data.iloc[index].values)\n        else:\n            if type(index) is list:\n                if self.multi == False:\n                    sample = {\n                        'target': torch.Tensor(self.target.iloc[index].values),\n                        'data': torch.FloatTensor(self.data[index]),\n                        'date': torch.Tensor(self.date.iloc[index].values)\n                    }\n                elif self.multi == True:\n                    sample = {\n                        'target': torch.Tensor(self.target[index]),\n                        'data': torch.FloatTensor(self.data[index]),\n                        'date': torch.Tensor(self.date.iloc[index].values)\n                    }\n\n            else:\n                if self.multi == False:\n                    sample = {\n                        'target': torch.Tensor(self.target.iloc[index]),\n                        'data': torch.FloatTensor(self.data[index]),\n                        'date': torch.Tensor(self.date.iloc[index])\n                    }\n                elif self.multi == True:\n                    sample = {\n                        'target': torch.Tensor(self.target[index]),\n                        'data': torch.FloatTensor(self.data[index]),\n                        'date': torch.Tensor(self.date.iloc[index])\n                    }\n        return sample\n\n    def __len__(self):\n        return len(self.data)\n\ndef load_data(root_dir, mode, overide=None):\n    if overide:\n        data = dt.fread(overide).to_pandas()\n    elif mode == 'train':\n        data = dt.fread(root_dir + 'train.csv').to_pandas()\n    elif mode == 'test':\n        data = dt.fread(root_dir + 'example_test.csv').to_pandas()\n    elif mode == 'sub':\n        data = dt.fread(root_dir + 'example_sample_submission.csv').to_pandas()\n    return data\n\n\n\ndef preprocess_data(data: pd.DataFrame, scale: bool = False, nn: bool = False,\n                    action: str = 'weight'):\n    \"\"\"\n    Preprocess the data.\n\n    Parameters\n    ----------\n    data\n        Pandas DataFrame\n    scale\n        scale data with unit std and 0 mean\n    nn\n        return data as np.array\n    missing\n        options to replace missing data with - mean, median, 0\n    action\n        options to create action value  - weight = (weight * resp) > 0\n                                        - combined = (resp_cols) > 0\n                                        - multi = each resp cols >0\n\n    Returns\n    -------\n    data [pd.DataFrame or np.array]\n    target [pd.Series]\n    feature [list]\n    date [pd.Series]\n    \"\"\"\n\n    data = data.query('weight > 0').reset_index(drop=True)\n    data = data.query('date > 85').reset_index(drop=True)\n    if action == 'weight':\n        data['action'] = (\n                (data['weight'].values * data['resp'].values) > 0).astype('float32')\n    if action == 'combined':\n        data['action'] = (\n                (data['resp'].values > 0) and (data['resp_1'].values > 0) and (data['resp_2'].values > 0) and (\n                data['resp_3'].values > 0) and (data['resp_4'].values > 0)).astype('float32')\n    if action == 'multi':\n        resp_cols = ['resp', 'resp_1', 'resp_2', 'resp_3', 'resp_4']\n        for i in range(len(resp_cols)):\n            data['action_' + str(i)] = (data['weight'] * data[resp_cols[i]] > 0).astype('int')\n    \"\"\"\n    missing_col_sums = data.isna().sum()\n    missing_cols_10per = data.loc[:,\n                         missing_col_sums > len(data) * 0.1].columns\n    data = data.drop(missing_cols_10per, axis=1)\n    \"\"\"\n    features = [col for col in data.columns if 'feature' in col] + ['weight']\n    date = data['date']\n\n    if action == 'multi':\n        target = np.array([data['action_' + str(i)]\n                           for i in range(len(resp_cols))]).T\n\n    else:\n        target = data['action']\n    data = data[features]\n    if scale:\n        scaler = StandardScaler()\n        data = scaler.fit_transform(data)\n    if not scale and nn:\n        data = data.values\n    return data, target, features, date\n\n\ndef calc_data_mean(array, cache_dir=None, fold=None, train=True, mode='mean'):\n    if train:\n        if mode == 'mean':\n            f_mean = np.nanmean(array, axis=0)\n            if cache_dir and fold:\n                np.save(f'{cache_dir}/f_{fold}_mean.npy', f_mean)\n            elif cache_dir:\n                np.save(f'{cache_dir}/f_mean.npy', f_mean)\n            array = np.nan_to_num(array) + np.isnan(array) * f_mean\n        if mode == 'median':\n            f_med = np.nanmedian(array, axis=0)\n            if cache_dir and fold:\n                np.save(f'{cache_dir}/f_{fold}_median.npy', f_med)\n            elif cache_dir:\n                np.save(f'{cache_dir}/f_median.npy', f_med)\n            array = np.nan_to_num(array) + np.isnan(array) * f_med\n        if mode == 'zero':\n            array = np.nan_to_num(array) + np.isnan(array) * 0\n    if not train:\n        if mode == 'mean':\n            f_mean = np.load(f'{cache_dir}/f_mean.npy')\n            array = np.nan_to_num(array) + np.isnan(array) * f_mean\n        if mode == 'median':\n            f_med = np.load(f'{cache_dir}/f_med.npy')\n            array = np.nan_to_num(array) + np.isnan(array) * f_med\n        if mode == 'zero':\n            array = np.nan_to_num(array) + np.isnan(array) * 0\n    return array\n\ndef weighted_mean(scores, sizes):\n    largest = np.max(sizes)\n    weights = [size / largest for size in sizes]\n    return np.average(scores, weights=weights)\n\n\ndef create_dataloaders(dataset: Dataset, indexes: dict, batch_size):\n    train_idx = indexes.get('train', None)\n    val_idx = indexes.get('val', None)\n    test_idx = indexes.get('test', None)\n    dataloaders = {}\n    if train_idx:\n        train_set = Subset(\n            dataset, train_idx)\n        train_sampler = BatchSampler(SequentialSampler(\n            train_set), batch_size=batch_size, drop_last=False)\n        dataloaders['train'] = DataLoader(\n            dataset, sampler=train_sampler, num_workers=10, pin_memory=True)\n    if val_idx:\n        val_set = Subset(dataset, val_idx)\n        val_sampler = BatchSampler(SequentialSampler(\n            val_set), batch_size=batch_size, drop_last=False)\n        dataloaders['val'] = DataLoader(\n            dataset, sampler=val_sampler, num_workers=10, pin_memory=True)\n    if test_idx:\n        test_set = Subset(dataset, test_idx)\n        test_sampler = BatchSampler(SequentialSampler(\n            test_set), batch_size=batch_size, drop_last=False)\n        dataloaders['test'] = DataLoader(\n            dataset, sampler=test_sampler, num_workers=10, pin_memory=True)\n    return dataloaders\n\n\ndef seed_everything(seed):\n    random.seed(seed)\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\ndef load_model(path, input_size, output_size, p, pl_lightning):\n    if os.path.isdir(path):\n        models = []\n        for file in os.listdir(path):\n            if pl_lightning:\n                model = Regressor.load_from_checkpoint(checkpoint_path=file, input_size=input_size,\n                                                        output_size=output_size, params=p)\n            else:\n                model = Regressor(input_size, output_size, params=p)\n                model.load_state_dict(torch.load(f'{path}/{file}'))\n            models.append(model)\n        return models\n    elif os.path.isfile(path):\n        if pl_lightning:\n            return Regressor.load_from_checkpoint(checkpoint_path=path, input_size=input_size,\n                                                   output_size=output_size, params=p)\n        else:\n            model = Regressor(input_size, output_size, params=p)\n            model.load_state_dict(torch.load(path))\n            return model\n","metadata":{"execution":{"iopub.status.busy":"2021-08-24T19:09:09.928391Z","iopub.execute_input":"2021-08-24T19:09:09.928642Z","iopub.status.idle":"2021-08-24T19:09:13.15762Z","shell.execute_reply.started":"2021-08-24T19:09:09.928618Z","shell.execute_reply":"2021-08-24T19:09:13.155404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nclass Regressor(pl.LightningModule):\n    def __init__(self, input_size, output_size, params, model_path='models/'):\n        super(Regressor, self).__init__()\n        dim_1 = params['dim_1']\n        dim_2 = params['dim_2']\n        dim_3 = params['dim_3']\n        dim_4 = params['dim_4']\n        self.dropout_prob = params['dropout']\n        self.lr = params['lr']\n        self.activation = params['activation']\n        self.input_size = input_size\n        self.output_size = output_size\n        self.loss = nn.BCEWithLogitsLoss()\n        self.weight_decay = params['weight_decay']\n        self.amsgrad = params['amsgrad']\n        self.label_smoothing = params['label_smoothing']\n        self.model_path = model_path\n        self.encoder = nn.Sequential(\n            nn.BatchNorm1d(input_size),\n            nn.Linear(input_size, dim_1, bias=False),\n            nn.BatchNorm1d(dim_1),\n            self.activation(),\n            nn.Dropout(p=self.dropout_prob),\n            nn.Linear(dim_1, dim_2, bias=False),\n            nn.BatchNorm1d(dim_2),\n            self.activation(),\n            nn.Dropout(p=self.dropout_prob),\n            nn.Linear(dim_2, dim_3, bias=False),\n            nn.BatchNorm1d(dim_3),\n            self.activation(),\n            nn.Dropout(p=self.dropout_prob),\n            nn.Linear(dim_3, dim_4, bias=False),\n            nn.BatchNorm1d(dim_4),\n            self.activation(),\n            nn.Dropout(p=self.dropout_prob),\n            nn.Linear(dim_4, self.output_size, bias=False)\n        )\n\n    def forward(self, x):\n        out = self.encoder(x)\n        return out\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch['data'], batch['target']\n        x = x.view(x.size(1), -1)\n        y = y.view(y.size(1), -1)\n        logits = self(x)\n        loss = self.loss(input=logits, target=y)\n        logits = torch.sigmoid(logits)\n        auc_metric = roc_auc_score(y_true=y.cpu().numpy(),\n                                   y_score=logits.cpu().detach().numpy())\n        self.log('train_auc', auc_metric, on_step=False,\n                 on_epoch=True, prog_bar=True)\n        self.log('train_loss', loss, prog_bar=True)\n        return {'loss': loss}\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch['data'], batch['target']\n        x = x.view(x.size(1), -1)\n        y = y.view(y.size(1), -1)\n        logits = self(x)\n        loss = self.loss(input=logits,\n                         target=y)\n        logits = torch.sigmoid(logits)\n        auc = roc_auc_score(y_true=y.cpu().numpy(),\n                            y_score=logits.cpu().detach().numpy())\n        return {'loss': loss, 'y': y, 'logits': logits, 'auc': auc}\n\n    def validation_epoch_end(self, val_step_outputs):\n        epoch_loss = torch.tensor([x['loss'] for x in val_step_outputs]).mean()\n        epoch_auc = torch.tensor([x['auc'] for x in val_step_outputs]).mean()\n        self.log('val_loss', epoch_loss, prog_bar=True)\n        self.log('val_auc', epoch_auc, prog_bar=True)\n\n    def test_step(self, batch, batch_idx):\n        return self.validation_step(batch, batch_idx)\n\n    def test_epoch_end(self, outputs):\n        epoch_loss = torch.tensor([x['loss'] for x in outputs]).mean()\n        epoch_auc = torch.tensor([x['auc'] for x in outputs]).mean()\n        self.log('test_loss', epoch_loss)\n        self.log('test_auc', epoch_auc)\n\n    def configure_optimizers(self):\n        # weight_decay = self.weight_decay,\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr,\n                                     amsgrad=self.amsgrad)\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n            optimizer, patience=5, factor=0.1, min_lr=1e-7, eps=1e-08\n        )\n        return {'optimizer': optimizer, 'lr_scheduler': scheduler, 'monitor': 'val_auc'}\n\ndef init_weights(m, func):\n    if type(m) == nn.Linear:\n        nn.init.xavier_normal_(m.weight, nn.init.calculate_gain(func))\n        # m.bias.data.fill_(1)\n\n\n\ndef cross_val(p) -> object:\n    data_ = load_data(root_dir='./data/', mode='train')\n    data_, target_, features, date = preprocess_data(\n        data_, nn=True, action='multi')\n    input_size = data_.shape[-1]\n    output_size = target_.shape[-1]\n    gts = PurgedGroupTimeSeriesSplit(n_splits=5, group_gap=5)\n    models = []\n    tb_logger = pl_loggers.TensorBoardLogger('logs/multiclass_')\n    for i, (train_idx, val_idx) in enumerate(gts.split(data_, groups=date)):\n        idx = np.concatenate([train_idx, val_idx])\n        data = copy.deepcopy(data_[idx])\n        target = copy.deepcopy(target_[idx])\n        checkpoint_callback = pl.callbacks.ModelCheckpoint(\n            os.path.join('models/', 'multi_class_fold_{}'.format(i)), monitor='val_auc', save_top_k=1, period=10,\n            mode='max'\n        )\n        model = Regressor(input_size, output_size, p)\n        if p['activation'] == nn.ReLU:\n            model.apply(lambda m: init_weights(m, 'relu'))\n        elif p['activation'] == nn.LeakyReLU:\n            model.apply(lambda m: init_weights(m, 'leaky_relu'))\n        train_idx = [i for i in range(0, max(train_idx) + 1)]\n        val_idx = [i for i in range(len(train_idx), len(idx))]\n        data[train_idx] = calc_data_mean(\n            data[train_idx], './cache', train=True, mode='mean')\n        data[val_idx] = calc_data_mean(\n            data[val_idx], './cache', train=False, mode='mean')\n        dataset = FinData(data=data, target=target, date=date, multi=True)\n        dataloaders = create_dataloaders(\n            dataset, indexes={'train': train_idx, 'val': val_idx}, batch_size=p['batch_size'])\n        es = EarlyStopping(monitor='val_auc', patience=10,\n                           min_delta=0.0005, mode='max')\n        trainer = pl.Trainer(logger=tb_logger,\n                             max_epochs=1,\n                             gpus=1,\n                             callbacks=[checkpoint_callback, es],\n                             precision=16)\n        trainer.fit(\n            model, train_dataloader=dataloaders['train'], val_dataloaders=dataloaders['val'])\n        torch.save(model.state_dict(), f'models/fold_{i}_state_dict.pth')\n        models.append(model)\n    return models, features","metadata":{"execution":{"iopub.status.busy":"2021-08-24T19:09:13.159268Z","iopub.execute_input":"2021-08-24T19:09:13.159821Z","iopub.status.idle":"2021-08-24T19:09:13.216578Z","shell.execute_reply.started":"2021-08-24T19:09:13.159779Z","shell.execute_reply":"2021-08-24T19:09:13.215651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def final_train(p, load=False):\n    data_ = load_data(root_dir='./data/', mode='train',overide='/kaggle/input/jane-street-market-prediction/train.csv')\n    data, target, features, date = preprocess_data(data_, nn=True)\n    dataset = FinData(data=data, target=target, date=date)\n    input_size = data.shape[-1]\n    output_size = 1\n    train_idx, val_idx = date[date <= 498].index.values.tolist(), date[date > 498].index.values.tolist()\n    data[train_idx] = calc_data_mean(data[train_idx], './cache', train=True)\n    data[val_idx] = calc_data_mean(data[val_idx], './cache', train=False)\n    checkpoint_callback = pl.callbacks.ModelCheckpoint(filepath='./cache/full_train',\n                                                       monitor=\"val_auc\", mode='max', save_top_k=1, period=10)\n    model = Regressor(input_size=input_size,\n                       output_size=output_size, params=p)\n    if p['activation'] == nn.ReLU:\n        model.apply(lambda m: init_weights(m, 'relu'))\n    elif p['activation'] == nn.LeakyReLU:\n        model.apply(lambda m: init_weights(m, 'leaky_relu'))\n    dataset = FinData(data, target, date)\n    dataloaders = create_dataloaders(dataset, indexes={'train': train_idx, 'val': val_idx}, batch_size=p['batch_size'])\n    es = EarlyStopping(monitor='val_auc', patience=10,\n                       min_delta=0.0005, mode='max')\n    trainer = pl.Trainer(max_epochs=25,\n                         gpus=1,\n                         callbacks=[checkpoint_callback, es],\n                         precision=16)\n    trainer.fit(model, train_dataloader=dataloaders['train'], val_dataloaders=dataloaders['val'])\n    torch.save(model.state_dict(), './cache/final_train.pth')\n    return model, features","metadata":{"execution":{"iopub.status.busy":"2021-08-24T19:09:13.220475Z","iopub.execute_input":"2021-08-24T19:09:13.220838Z","iopub.status.idle":"2021-08-24T19:09:13.242199Z","shell.execute_reply.started":"2021-08-24T19:09:13.220802Z","shell.execute_reply":"2021-08-24T19:09:13.236239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fillna_npwhere(array, values):\n    if np.isnan(array.sum()):\n        array = np.nan_to_num(array) + np.isnan(array) * values\n    return array\n\n\n\ndef test_model(models, features, cache_dir='cache'):\n    env = janestreet.make_env()\n    iter_test = env.iter_test()\n    if type(models) == list:\n        models = [model.eval() for model in models]\n    else:\n        models.eval()\n    f_mean = np.load(f'{cache_dir}/f_mean.npy')\n    for (test_df, sample_prediction_df) in tqdm(iter_test):\n        if test_df['weight'].item() > 0:\n            vals = torch.FloatTensor(\n                fillna_npwhere(test_df[features].values, f_mean))\n            if type(models) == list:\n                # calc mean of each models prediction of each response rather than mean of all predicted responses by each model\n                preds = [torch.sigmoid(model.forward(vals.view(1, -1))).detach().numpy()\n                         for model in models]\n                pred = np.mean(np.mean(preds, axis=1))\n            else:\n                pred = torch.sigmoid(models.forward(vals.view(1, -1))).item()\n            sample_prediction_df.action = np.where(\n                pred > 0.502, 1, 0).astype(int).item()\n        else:\n            sample_prediction_df.action = 0\n        env.predict(sample_prediction_df)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T19:09:13.245436Z","iopub.execute_input":"2021-08-24T19:09:13.246373Z","iopub.status.idle":"2021-08-24T19:09:13.266495Z","shell.execute_reply.started":"2021-08-24T19:09:13.246327Z","shell.execute_reply":"2021-08-24T19:09:13.26529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def main(train=True):\n    \"\"\"\n    Hyperparameters achieved through cross-val hyperparameter optimisation using Optuna\n\n    \"\"\"\n    p = {'dim_1': 167,\n         'dim_2': 454,\n         'dim_3': 371,\n         'dim_4': 369,\n         'dim_5': 155,\n         'activation': nn.LeakyReLU,\n         'dropout': 0.21062362698532755,\n         'lr': 0.0022252024054478523,\n         'label_smoothing': 0.05564974140461841,\n         'weight_decay': 0.04106097088288333,\n         'amsgrad': True,\n         'batch_size': 10072}\n    \n    if train:\n        # models, features = train_cross_val(p)\n        models, features = final_train(p, load=False)\n    else:\n        data_ = load_data(root_dir='./data/', mode='train',overide='/kaggle/input/jane-street-market-prediction/train.csv')\n        data_, target_, features, date = preprocess_data(data_, nn=True)\n        model_path = '/kaggle/input/model-states'\n        f_mean = calc_data_mean(data_, 'cache')\n        models = load_model(model_path, data_.shape[-1], 5, p, False)\n    # model, checkpoint = final_train(p)\n    # best_model_path = checkpoint.best_model_path\n        # model, features = final_train(load=best_model_path)\n    test_model(models, features)\n    return models\n","metadata":{"execution":{"iopub.status.busy":"2021-08-24T19:09:13.271693Z","iopub.execute_input":"2021-08-24T19:09:13.272038Z","iopub.status.idle":"2021-08-24T19:09:13.286513Z","shell.execute_reply.started":"2021-08-24T19:09:13.271982Z","shell.execute_reply":"2021-08-24T19:09:13.285382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"main(train=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T19:09:13.291511Z","iopub.execute_input":"2021-08-24T19:09:13.293718Z","iopub.status.idle":"2021-08-24T19:14:44.267064Z","shell.execute_reply.started":"2021-08-24T19:09:13.293681Z","shell.execute_reply":"2021-08-24T19:14:44.266112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}