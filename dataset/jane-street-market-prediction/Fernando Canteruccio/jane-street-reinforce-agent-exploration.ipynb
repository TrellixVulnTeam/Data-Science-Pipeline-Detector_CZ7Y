{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nnp.random.seed(1)\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import precision_score\nimport janestreet\nimport tqdm\n\n# from sklearn.linear_model import LogisticRegression\n# from sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# train_samples = 4000000\n# test_samples = 400000\n# data_df = pd.read_csv('../input/jane-street-market-prediction/train.csv', nrows=train_samples + test_samples)\ndata_df = pd.read_csv('../input/jane-street-market-prediction/train.csv')\nfeatures_df = pd.read_csv('../input/jane-street-market-prediction/features.csv')\nexample_test = pd.read_csv('../input/jane-street-market-prediction/example_test.csv')\ntrain_samples = int(data_df.shape[0] * 0.9)\n\n\nprint('Train dataset shape: {}'.format(data_df.shape))\nprint('Features dataset shape: {}'.format(features_df.shape))\nprint('Example test dataset shape: {}'.format(example_test.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = data_df.iloc[:train_samples]\ntest_df = data_df.iloc[train_samples:]\n\n# X = train_df.loc[:, train_df.columns.str.contains('feature')]\n# y = (train_df['resp'] > 0) * 1\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .25, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imp = SimpleImputer(missing_values = np.nan , strategy = 'constant', fill_value = 0)\n# ss = StandardScaler()\n\n# X_train = imp.fit_transform(X_train)\n# X_test = imp.transform(X_test)\n# X_train_scaled = ss.fit_transform(X_train)\n# X_test_scaled = ss.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# baseline\n\n# logreg = LogisticRegression(solver = 'saga', penalty='elasticnet', \n#                             l1_ratio = .5, max_iter = 5000)\n# logreg.fit(X_train_scaled, y_train)\n# print('Log Reg Score: {:.3f}'.format(logreg.score(X_test_scaled, y_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# base agents\n\nclass Agent(object):\n    # Our policy that maps state to action parameterized by w\n    def policy(self, state):     \n        raise NotImplementedError('You need to overwrite the policy method.')\n        \n    def predict(self, *state):\n        return self.policy(state)\n    \n    def train(self, *state):\n        return self.policy(state)\n    \n    def store_reward(self, reward):\n        pass\n\n    def update(self):\n        pass\n\n    # Vectorized softmax Jacobian\n    @staticmethod\n    def softmax_grad(softmax):\n        s = softmax.reshape(-1,1)\n        return np.diagflat(s) - np.dot(s, s.T)\n\n\nclass RandomAgent(Agent):\n    def __init__(self, n_actions):\n        self.n_actions = n_actions\n    \n    def policy(self, state):\n        return np.random.binomial(1, 1 / self.n_actions)\n        \n        \nclass AlwaysTradeAgent(Agent):\n    def policy(self, state):\n        return 1\n    \n    \nclass REINFORCE(Agent):\n    '''\n    REINFORCE Policy Gradients agent with linear shallow model\n    https://homes.cs.washington.edu/~todorov/courses/amath579/reading/PolicyGradient.pdf\n    https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ\n    https://medium.com/samkirkiles/reinforce-policy-gradients-from-scratch-in-numpy-6a09ae0dfe12\n    '''\n    def __init__(self, state_dim, n_actions, learning_rate, gamma, train):\n        # Init weight\n        self.w = np.random.rand(state_dim, n_actions) * 0.1\n        self.n_actions = n_actions\n        self.lr = learning_rate\n        self.g = gamma\n        self.grads = []\n        self.rewards = []\n        self._train = train\n                   \n    @staticmethod\n    def preprocess_state(state):\n        return imp.fit_transform(np.array([state]).reshape((1, -1)))\n        \n    # Our policy that maps state to action parameterized by w\n    def policy(self, state):\n        exp = np.exp(state.dot(self.w))\n        probs = exp / np.sum(exp)\n        action = np.random.choice(self.n_actions, p=probs[0])\n        return action, probs\n\n    def train(self, state):\n        state = self.preprocess_state(state)\n        action, probs = self.policy(state)\n        dsoftmax = self.softmax_grad(probs)[action,:]\n        dlog = dsoftmax / probs[0, action]\n        grad = state.T.dot(dlog[None,:])\n        self.grads.append(grad)\n        return action\n    \n    def predict(self, state):\n        if self._train:\n            return self.train(state)\n        else:\n            state = self.preprocess_state(state)\n            return np.argmax(self.policy(state)[1][0])\n        \n    def store_reward(self, reward):\n        # Compute gradient and save with reward in memory for our weight update\n        self.rewards.append(reward)\n\n    def update(self):\n        for i in range(len(self.grads)):\n            # Loop through everything that happend in the episode and update towards the log policy gradient times **FUTURE** reward\n            self.w += self.lr * self.grads[i] * sum([r * (self.g ** r) for t, r in enumerate(self.rewards[i:])])\n        self.grads = []\n        self.rewards = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# simulation environment\n\nclass SimulationEnv(object):\n    def __init__(self, df):\n        self.predictions = []\n        self.rewards = []\n        self.utility = 0\n        self.ps = []\n\n        self.df = df\n        \n\n    def p(self, step_df, agent):\n        result = 0\n        X = step_df.loc[:, step_df.columns.str.contains('feature')].values\n        Y = step_df.eval('weight * resp')\n        for i in range(X.shape[0]):\n            pred = agent.predict(X[i])\n            self.predictions.append(pred)\n            reward = pred * Y.iloc[i]\n            self.rewards.append(reward)\n            agent.store_reward(reward)\n            result += reward\n            \n            agent.update()\n\n        return result\n\n    def simulate(self, agent):\n        ps = []\n        for i in tqdm.tqdm(range(self.df.date.min(), self.df.date.max() + 1)):\n            ps.append(self.p(self.df[self.df.date == i], agent))\n\n        t = np.multiply(np.sum (ps) / np.sqrt(np.sum(np.power(ps, 2))), np.sqrt(250/len(ps)))\n\n        utility = np.multiply(np.min([np.max([t, 0]), 6]), np.sum(ps))\n        self.ps = ps\n        \n        self.utility = utility\n        return utility, ps\n    \n    def reset(self):\n        self.predictions = []\n        self.rewards = []\n        self.utility = 0\n        \n    def print_results(self):\n\n        pred = self.predictions\n        y = (self.df['resp'] > 0) * 1\n        \n        print(f'utility {self.utility}')\n        print(f'precision {precision_score(y, pred)}')\n        plt.plot(self.ps);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env = SimulationEnv(train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env.reset()\nrd_agent = RandomAgent(2)\nenv.simulate(rd_agent)\n\nenv.print_results()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env.reset()\nat_agent = AlwaysTradeAgent()\nenv.simulate(at_agent)\n\nenv.print_results()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env.reset()\nre_agent = REINFORCE(130, 2, 0.001, 0.999, True)\nenv.simulate(re_agent)\n\nenv.print_results()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env = SimulationEnv(test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env.reset()\nenv.simulate(rd_agent)\n\nenv.print_results()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env.reset()\nenv.simulate(at_agent)\n\nenv.print_results()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"re_agent._train = False\n\nenv.reset()\nenv.simulate(re_agent)\n\nenv.print_results()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env = janestreet.make_env() # initialize the environment\niter_test = env.iter_test() # an iterator which loops over the test set\n\nfor (test_df, sample_prediction_df) in tqdm.tqdm(iter_test):\n    X_test_1 = test_df.loc[:, test_df.columns.str.contains('feature')].values\n    sample_prediction_df.action = re_agent.predict(X_test_1)\n    env.predict(sample_prediction_df)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}