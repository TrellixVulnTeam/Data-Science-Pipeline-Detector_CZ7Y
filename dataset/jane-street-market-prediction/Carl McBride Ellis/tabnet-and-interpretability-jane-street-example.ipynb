{"cells":[{"metadata":{},"cell_type":"markdown","source":"# TabNet and Interpretability\n\nFrom the abstract of [TabNet](https://arxiv.org/pdf/1908.07442.pdf):\n\n> \"*TabNet uses [sequential attention](https://arxiv.org/pdf/1706.03762.pdf) to choose which features to reason from at each decision step, enabling interpretability*\"\n\nHere we shall look at the interpretability of TabNet in the context of the kaggle [Jane Street Market Prediction competition](https://www.kaggle.com/c/jane-street-market-prediction). For this notebook we shall be using the recently released version 3.0.0 of [pyTorch TabNet](https://github.com/dreamquark-ai/tabnet)."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"import numpy  as np\nimport pandas as pd\n\n# plotting\nimport matplotlib.pyplot as plt\nimport plotly.express as px\n\n!pip install -q pytorch-tabnet\nfrom pytorch_tabnet.tab_model import TabNetRegressor\n\n!pip install -q datatable \nimport datatable as dt\n\nimport torch","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Load in the data and create `X_train` and `y_train`. Here we shall be using regression, with `resp` as the target. A well known advantage of deep neural network is there is little need for *a priori* feature engineering, which is currently a key aspect in tree-based tabular data learning methods. In view of this we shall provide TabNet with access to all 130 features of this dataset.\n\nFor the training data we shall use the first 495 days of the `train.csv` file, and for the validation data we shall use the remaining 5 days."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# read in the train dataset\ntrain_data = dt.fread('../input/jane-street-market-prediction/train.csv').to_pandas()\n\n# filter out the zero weights\ntrain_data = train_data.query('weight > 0').reset_index(drop = True)\n\n# split for train and validation\nvalidation_data = train_data.query('date   > 494').reset_index(drop = True) \ntrain_data      = train_data.query('date <=  494').reset_index(drop = True) \n\n# training data\nX_train = train_data.loc[:, train_data.columns.str.contains('feature')]\nX_train = X_train.fillna(X_train.mean()).to_numpy()\ny_train = train_data.loc[:, 'resp'].to_numpy().reshape(-1, 1)\n\n# validation data\nX_valid = validation_data.loc[:, validation_data.columns.str.contains('feature')]\nX_valid = X_valid.fillna(X_valid.mean()).to_numpy()\ny_valid = validation_data.loc[:, 'resp'].to_numpy().reshape(-1, 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We shall now perform the regression.\npyTorch TabNet has two options for the masking function, `softmax` [[1]](https://arxiv.org/pdf/1602.02068.pdf) and \n`entmax`[[2](https://papers.nips.cc/paper/2004/file/96f2b50b5d3613adf9c27049b2a888c7-Paper.pdf)], where `gamma` is the coefficient for feature reusage in the masks. Here we shall use `entmax`."},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"%%time\n\n# define the batch size, here 2^13\nBS = 8192\n\n# Training for more epochs might improve the model performance at the cost of longer training time\nMAX_EPOCH = 20\n\nregressor = TabNetRegressor(n_d=64, n_a=64, \n                            n_steps         =5, \n                            gamma           =1.2,\n                            n_independent   =2, \n                            n_shared        =2,\n                            lambda_sparse   =0., \n                            seed            =0,\n                            clip_value      =1,\n                            mask_type       ='entmax',\n                            device_name     ='auto',\n                            optimizer_fn=torch.optim.Adam,\n                            optimizer_params=dict(lr=2e-3),\n                            scheduler_params=dict(max_lr=0.05,\n                                                  steps_per_epoch=int(X_train.shape[0] / BS),\n                                                  epochs=MAX_EPOCH,\n                                                  is_batch_level=True),\n                            scheduler_fn=torch.optim.lr_scheduler.OneCycleLR,\n                            verbose=1)\n\nregressor.fit(X_train=X_train, y_train=y_train,\n          eval_set=[(X_train, y_train), (X_valid, y_valid)],\n          eval_name=[\"train\", \"valid\"],\n          eval_metric=[\"mae\"],\n          batch_size=BS,\n          virtual_batch_size=256,\n          max_epochs=MAX_EPOCH,\n          drop_last=True,\n          pin_memory=True)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"explainability_matrix , masks = regressor.explain(X_valid)\n\n# Normalize the importance by sample\nnormalized_explain_mat = np.divide(explainability_matrix, explainability_matrix.sum(axis=1).reshape(-1, 1)+1e-8)\n\n# Add prediction to better understand correlation between features and predictions\nval_preds = regressor.predict(X_valid)\n\nexplain_and_preds = np.hstack([normalized_explain_mat, val_preds.reshape(-1, 1)])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature importance\nAs with the majority of estimators, TabNet provides access to a ranking of features in terms of their overall importance:"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"feat_importances = regressor.feature_importances_\nindices = np.argsort(feat_importances)\n# plot\nfig, ax = plt.subplots(figsize=(10, 6))\nplt.title(\"Top 25 feature importances\")\nplt.barh(range(len(feat_importances)), feat_importances[indices],color=\"b\", align=\"center\")\nfeatures = ['feature_{}'.format(i) for i in range(0, 130)]\nplt.yticks(range(len(feat_importances)), [features[idx] for idx in indices])\n# all features\n# plt.ylim([-1, len(feat_importances)])\n# Top 25 features\nplt.ylim([len(feat_importances)-25, len(feat_importances)])\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Local interpretability\nHowever, the beauty of TabNet is that it allows us to not only to obtain the overall feature importances, but also inspect the importance of each of the features for each of the individual rows, here for the validation data:"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"px.imshow(explain_and_preds[:,:],\n          labels=dict(x=\"Features\", y=\"Samples\", color=\"Importance\"),\n          x=features+[\"prediction\"],\n          title=\"Sample wise feature importance\",\n          color_continuous_scale='Jet',\n          height=1000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is particularly interesting to now see the daily variation in the Tag 22 features `feature_64`, `feature_65` and `feature_66`, as well as the daily importance of `feature_37` when the Tag 22 features are seemingly *'inactive'*. The most important message we can see here is that a simple overall ranking of features is not telling the whole story of what is really going on when predictions are being made.\n\nWe can also produce a correlation matrix for the importance of the features with respect to each other"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"correlation_importance = np.corrcoef(explain_and_preds.T)\npx.imshow(correlation_importance,\n          labels=dict(x=\"Features\", y=\"Features\", color=\"Correlation\"),\n          x=features+[\"prediction\"], y=features+[\"prediction\"],\n          title=\"Correlation between attention mechanism for each feature and predictions\",\n          color_continuous_scale='Jet')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"although as we can see, in this example the matrix seems somewhat uninformative.\n# Related reading\n* [Sercan O. Arik and Tomas Pfister \"*TabNet: Attentive Interpretable Tabular Learning*\", arXiv:1908.07442 (2019)](https://arxiv.org/pdf/1908.07442.pdf)\n* [TabNet on AI Platform: High-performance, Explainable Tabular Learning](https://cloud.google.com/blog/products/ai-machine-learning/ml-model-tabnet-is-easy-to-use-on-cloud-ai-platform) (Google Cloud)\n* [pytorch-tabnet](https://github.com/dreamquark-ai/tabnet) (GitHub)\n* [Christoph Molnar, Giuseppe Casalicchio, and Bernd Bischl \"*Interpretable Machine Learning -- A Brief History, State-of-the-Art and Challenges*\", arXiv:2010.09337 (2020)](https://arxiv.org/pdf/2010.09337.pdf)\n\n***See also***:\n\n* [Jane Street: TabNet 3.0.0 starter notebook](https://www.kaggle.com/carlmcbrideellis/jane-street-tabnet-3-0-0-starter-notebook) - a simple notebook using TabNet classification for the Jane Street competition."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}