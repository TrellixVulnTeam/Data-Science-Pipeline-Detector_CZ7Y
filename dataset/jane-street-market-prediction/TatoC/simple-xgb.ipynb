{"cells":[{"metadata":{},"cell_type":"markdown","source":"# import Package"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import TimeSeriesSplit, train_test_split,StratifiedKFold,cross_val_score,GridSearchCV\nfrom sklearn.metrics import accuracy_score,balanced_accuracy_score,f1_score,log_loss,make_scorer,roc_auc_score\nfrom sklearn.preprocessing import OneHotEncoder\n\nimport datatable as dt \nimport pandas as pd\npd.set_option('display.max_columns', 500)\n\nimport xgboost as xgb\nprint(\"XGBoost version:\", xgb.__version__)\n\nimport warnings \nwarnings.filterwarnings(\"ignore\")\n\nfrom hyperopt import fmin, tpe, hp, Trials, STATUS_OK, STATUS_FAIL\nfrom functools import partial\n\nfrom tqdm.auto import tqdm\n\ntry:\n    print('choose accelerate...',end='')\n    import cudf\n    import cupy as cp\n    if_gpu=True\n    print('gpu ready.')\nexcept:\n    if_gpu=False\n    print('cpu ready.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# load data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create the environment\nimport janestreet\nprint('Creating competition environment...', end='')\nenv = janestreet.make_env()\niter_test = env.iter_test()\nprint('Finished.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nif if_gpu:\n    train_data_datatable=cudf.read_csv('../input/jane-street-market-prediction/train.csv')\nelse:\n    train_data_datatable = dt.fread('../input/jane-street-market-prediction/train.csv')\ntrain_data = train_data_datatable.to_pandas()\ndel train_data_datatable\nfeatures_with_tag = pd.read_csv('../input/jane-street-market-prediction/features.csv')\nexample_test = pd.read_csv('../input/jane-street-market-prediction/example_test.csv')\nsample_prediction_df = pd.read_csv('../input/jane-street-market-prediction/example_sample_submission.csv')\nprint (\"Data is loaded!\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"filter data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = train_data[train_data['weight'] != 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"example_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# create features"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_feature_features(data):\n    feature_features=data.loc[:, data.columns.str.contains('feature')]\n    def _fill_missing(feature_features):\n        feature_features.fillna(feature_features.mean(axis=0),inplace=True)\n        return feature_features\n    feature_features=_fill_missing(feature_features)\n    return feature_features\n\ndef create_lag_features(data):\n    pass\n\ndef other_features(data):\n    pass\n\ndef create_all_features(data):\n    feature_features=create_feature_features(data)\n    features=pd.concat([feature_features],axis=1)\n    return features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features= create_all_features(train_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# create target"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_action(data,onehot=False):\n#     action=(data['resp'].values > 0).astype('int')\n    one_hot = OneHotEncoder()\n    action=data['resp'].apply(lambda x: int(x>0))\n    if onehot:\n        action = pd.get_dummies(action)\n    return action","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target=create_action(train_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# split_train_val"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(features,\n                                                        target,\n                                                        test_size=0.4,\n                                                        shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XGB"},{"metadata":{},"cell_type":"markdown","source":"HyperOpt搜参"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_xgb(params, X_train, y_train):\n    # run XGBoost algorithm with hyperparameters optimization\n    # this model outperforms the linear regression\n    \"\"\"\n Train XGBoost regressor using the parameters given as input. The model\n is validated using standard cross validation technique adapted for time series\n data. This function returns a friendly output for the hyperopt parameter optimization\n module.\n\n Parameters\n ----------\n params: dict with the parameters of the XGBoost regressor. For complete list see:\n         https://xgboost.readthedocs.io/en/latest/parameter.html\n X_train: pd.DataFrame with the training set features\n y_train: pd.Series with the training set targets\n\n Returns\n -------\n dict with keys 'model' for the trained model, 'status' containing the hyperopt\n status string and 'loss' with the RMSE obtained from cross-validation\n \"\"\"\n\n    if if_gpu:\n        params['tree_method']='gpu_hist'\n#     try:\n    model = xgb.XGBClassifier(seed =123,\n                              **params\n                              )\n\n    result = model.fit(X_train,\n                       y_train.values.ravel(),\n                       eval_set=[(X_train, y_train.values.ravel())],\n                       early_stopping_rounds=50,\n                       verbose=False)\n\n    # cross validate using the right iterator for time series\n    cv_space = TimeSeriesSplit(n_splits=5)\n    cv_score = cross_val_score(model,\n                               X_train, y_train.values.ravel(),\n                               cv=cv_space,\n                               scoring='roc_auc')\n\n    mean_of_cv_score = np.abs(np.mean(np.array(cv_score)))\n    return {\n        \"loss\": -mean_of_cv_score,\n        \"status\": STATUS_OK,\n        \"model\": model\n    }\n\n#     except ValueError as ex:\n#         return {\n#             \"error\": ex,\n#             \"status\": STATUS_FAIL\n#         }\n\ndef optimize_xgb(X_train, y_train, max_evals=10):\n    \"\"\"\n Run Bayesan optimization to find the optimal XGBoost algorithm\n hyperparameters.\n\n Parameters\n ----------\n X_train: pd.DataFrame with the training set features\n y_train: pd.Series with the training set targets\n max_evals: the maximum number of iterations in the Bayesian optimization method\n\n Returns\n -------\n best: dict with the best parameters obtained\n trials: a list of hyperopt Trials objects with the history of the optimization\n \"\"\"\n\n    space = {\n        \"n_estimators\": hp.randint(\"n_estimators\", 200, 600),\n        \"max_depth\": hp.randint(\"max_depth\", 2, 8),\n        \"learning_rate\": hp.loguniform(\"learning_rate\", -9, -1),\n        \"subsample\": hp.uniform(\"subsample\", 0.8, 1),\n        'gamma': hp.uniform('gamma', 0, 10),\n        'min_child_weight': hp.uniform('min_child_weight', 0, 10),\n        'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1), \n    }\n\n    objective_fn = partial(train_xgb,\n                           X_train=X_train,\n                           y_train=y_train)\n\n    trials = Trials()\n    best_params = fmin(fn=objective_fn,\n                space=space,\n                algo=tpe.suggest,\n                max_evals=max_evals,\n                trials=trials)\n    if if_gpu:\n        best_params['tree_method']='gpu_hist'\n    print('Best parameters:',best_params)\n\n    return best_params, trials","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_evals=100\n%time params, trials = optimize_xgb(X_train.iloc[:,:], y_train.iloc[:], max_evals=n_evals)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"模型拟合"},{"metadata":{"trusted":true},"cell_type":"code","source":"#fix params\n# params={'colsample_bytree': 0.5896884284128014, 'gamma': 1.5524549835224408, 'learning_rate': 0.8417086469365996, 'max_depth': 5, 'min_child_weight': 0.1680835208952257, 'n_estimators': 401, 'subsample': 0.8477610855386742, 'tree_method': 'gpu_hist'}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model=xgb.XGBClassifier(**params)\n%time model.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 预测与评估"},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate(y, yhat,ifpri=True):\n    summary = {}\n    summary['accuracy'] = accuracy_score(y, yhat)\n    summary['balanced_accuracy'] = balanced_accuracy_score(y, yhat)\n    summary['f1'] = f1_score(y, yhat)\n    summary['log_loss']=log_loss(y,yhat)\n    summary['roc_auc_score']=roc_auc_score(y,yhat)\n    if ifpri:\n        print(summary)\n    return summary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_val_predict=model.predict(X_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summary=evaluate(y_val,y_val_predict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Finally forecast (not yet, for the final model)"},{"metadata":{"trusted":true},"cell_type":"code","source":"for (test_df, sample_prediction_df) in tqdm(iter_test):\n    X_test = create_all_features(test_df)\n    y_preds = model.predict(X_test)\n    sample_prediction_df.action = y_preds\n    env.predict(sample_prediction_df)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}