{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1><center> Jane Street Market Prediction </h1>\n"},{"metadata":{},"cell_type":"markdown","source":"![](https://storage.googleapis.com/kaggle-organizations/3761/thumbnail.png?r=38)"},{"metadata":{},"cell_type":"markdown","source":"<h2> Table of content </h2>\n<ul> <li> <a href=\"#preparation\"> Preparation </a> </li>\n    <li> <a href=\"#load_data\"> Load The Datas </a> </li>\n    <li> <a href=\"#first_look\"> First Look at The Data </a> </li>\n    <li> <a href=\"#eda\"> Explorations Data analysis </a> </li>\n    <li> <a href=\"missing_values\"> Missing Values </a> </li> \n    <li> <a href=\"feature_engineering\"> Feature engineering </a> </li>\n    <li> <a href=\"feat_exploration\"> Features dataset explorations </a> </li>\n    <li> <a href=\"modeling\"> Modeling </a> </li>\n    <li> <a href=\"submission\"> Submission </a> </li>\n</ul>\n<hr>"},{"metadata":{},"cell_type":"markdown","source":"<h2 id=preparation> Preparation </h2>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns \nimport matplotlib.pyplot as plt \nimport plotly.express as px \nimport matplotlib.gridspec as gridspec\nfrom  collections import defaultdict\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.manifold import TSNE as tsne\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nimport os\nimport gc\nimport pickle\nimport lightgbm as lgbm\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nfrom sklearn.metrics import roc_curve,auc,roc_auc_score\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nfrom sklearn.model_selection import KFold\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def save_pickle(dict_param,nom_fich):\n    with open(nom_fich,\"wb\") as f :\n        pickle.dump(dict_param,f)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2 id=load_data> Load The Datas"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# The root path\npath = \"/kaggle/input/jane-street-market-prediction/\"\n\n# Load the training datas.\ntrain = pd.read_csv(path + \"train.csv\")\n\n# Load the metadata pertaining to the anonymized features.\nfeatures = pd.read_csv(path + \"features.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2 id=first_look> First Look at the Data </h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Take a look at the training data\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# info about training datas.\ntrain.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# some statistics on the training datas.\ntrain.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Display missing values per column\n\nmissing_table = pd.DataFrame({c:(train[c].isna().sum()/len(train))*100 for c in train.columns},index=[\"% missing values\"])\n\nmissing_table","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"===> It's obvious , that we should treat effeciently the little number of missing values in the column from the feature_120 to the feature_129."},{"metadata":{},"cell_type":"markdown","source":"<h2 id=eda> Explorations Data analysis </h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Display the histogram \nfig,axes = plt.subplots(nrows=45,ncols=3,figsize=(25,250))\n\nfor i in range(2,137):\n    sns.distplot(train.iloc[:,i],ax=axes[(i-2)//3,(i-2)%3])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- The most signal features seems have a normal gaussian distribution, and they are zero centered.\n\n- The return features, also are zero centred gaussian distributed. Which it can been seen, that the chance to make profit or to lose, is the same at any time of the day . \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute the correlation between pair features.\n\ncorrelation_table = train[[train.columns[i] for i in range(2,137)]].corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Display the correlation table of pair features.\ncorrelation_table","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def detect_correlated_features(df,threshold=0.5):\n    \"\"\"This function will try detect features who have correlation grower than the introduced \n     threshold value.\n     \n     @param df(DataFrame): The dataframe who resume the correlation values between features.\n     @param threshold(int) : the threshold that the function, will use as reference to detect\n                             correlated features.\n     @return list(List): list of tuple, who resume features that have correlation grower than\n                           the introduced threshold.\n     \"\"\"\n    correlated= defaultdict(list)\n    for col in df.columns:\n        dex = list(df.columns).index(col)\n        for ind in df.index[dex+1:] :\n            if df.loc[col,ind] > threshold:\n               correlated[col].append (ind)\n                \n    return correlated","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Detect the highly correlated features.Which they had coefficient correlation grower than 0.9.\ncorrelated_features = detect_correlated_features(correlation_table,threshold=0.9)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Display a table showing the high correlated features.\nax_features = correlated_features.keys()\nay_features = []\nfor f in correlated_features.values():\n    ay_features.extend(f)\nay_features = np.unique(ay_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set up the matplotlib figure\nf , ax = plt.subplots(figsize=(70,50))\nsns.heatmap(correlation_table.loc[ax_features,ay_features],cmap='BrBG',annot=True,square=True,vmin=-1,vmax=1,\\\n            linewidths=0.5,cbar_kws={\"shrink\": .5})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see that we can save only one feature from each pair highly correlated features."},{"metadata":{"trusted":true},"cell_type":"code","source":"# List of features to drop because of they are highly correlated \n# with others features in the train dataset.\nfeatures_to_drop = [f for f in ay_features if f not in ax_features]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train datas after removing features assigned to drop list of columns.\ntrain_df = train[[ f for f in list(train.columns) if ((f not in features_to_drop) or (f ==\"resp\"))]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's now, study the correlation of each feature with the feature named resp, which should be the feature that make deciders to move on for the trading or not."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute the correlation betwen the features named resp, and the reste of features\nlabel_correlation = pd.DataFrame({c:train_df[\"resp\"].corr(train_df[c]) for\\\n                                  c in train_df.columns if c!=\"resp\" and c!=\"feature_0\"},index=[\"action\"])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualize the correlation in a table named label_correlation.\nl = len(list(label_correlation.columns)) # compute the number  of features for label_correlation dataset\ncol = list(label_correlation.columns) # list of columns names of label_correlation dataset\n\n# Because of , the high number of features in our dataset, we will try divide them into 5,\n# in order to get more clear chart.\nfig ,axes = plt.subplots(nrows=5,ncols=1,figsize=(60,30))\nlevel = [0,int(l/5),int(2*l/5),int(3*l/5),int(4*l/5),l]\nfor i in range(len(level)-1):\n    sns.heatmap(label_correlation.loc[:,col[level[i]:level[i+1]]],annot=True,cmap='BrBG',\\\n                linewidths=0.5,vmin=-1,vmax=1,cbar_kws={\"shrink\": .5},square=True,ax=axes[i])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Except resp_{1,2,3,4} values that represent returns over different time horizons, there is no evident linear relation between retained features and the feature of return."},{"metadata":{"trusted":true},"cell_type":"code","source":"# correlation between the binary feature named feature_0 and the target values.\ntrain_df.loc[:,\"target\"] = list(((train_df[\"resp\"] > 0) & (train_df[\"resp_1\"] >0) & (train_df[\"resp_2\"]>0)\\\n                     & (train_df[\"resp_3\"]>0) & (train_df[\"resp_4\"]>0)).astype(\"int\"))\ntrain_df.loc[:,\"vl\"] = list(train_df[\"target\"].values)\npvt_table=train_df[[\"feature_0\",\"target\"]].pivot_table(index=[\"feature_0\"],columns=[\"target\"],aggfunc=len)\n#nb_1 = len(train_df.loc[train_df[\"target\"]==1,:])\n#nb_0 = len(train_df.loc[train_df[\"target\"]==0,:])\n#pvt_table[1] = pvt_table[1]/nb_1\n#pvt_table[0] = pvt_table[0]/nb_0\ntx= train_df[\"feature_0\"].value_counts()\nty = train_df[\"target\"].value_counts() \ntx = pd.DataFrame(tx)\nty = pd.DataFrame(ty)\nn = len(train_df)\ntx.columns = [\"values\"]\nty.columns = [\"values\"]\n\ncnt = tx.dot(ty.T)/n\nind = cnt.index\npvt_table = pvt_table.loc[ind,:]\nmesure = (cnt - pvt_table)**2/cnt\nxin = mesure.sum().sum()\n\ndel(train_df[\"target\"])\ndel(train_df[\"vl\"])\n#del(nb_1)\n#del(nb_0)\nfig = plt.figure(figsize=(12,8))\nsns.heatmap(mesure,annot=True,linewidths=0.5,cmap=\"BrBG\",vmin=0,vmax=1)\nplt.title(\"Correlation table between feature_0 and the target value \",size=15,color=\"red\")\nprint(\"The total correlation between feature_0 and the target equal to {}\".format(xin))\n#del(pvt_table)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color=redblue> We can conclude that, there is no correlation between feature_0 and the target values."},{"metadata":{},"cell_type":"markdown","source":"<h2 id=missing_values> Missing Values:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Display train_df missing values before imputations.\nmissing_b_imputation = pd.DataFrame({c:(train_df[c].isna().sum()) for c in train_df.columns},index=[\"% missing values\"])\nprint(\"The number of missing values in the train_df dataframe before imputation processing :{}\".format(\\\n                                                                                                      missing_b_imputation.sum().sum()))\nmissing_b_imputation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# identify , which column has missing values in the new dataset named train_df\n\nfeatures_with_missing_values = [] # list of features , has missing values.\n    \nfor f in list(train_df.columns):\n    if missing_table.loc[\"% missing values\",f] > 0 :\n        features_with_missing_values.append(f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train a linear model regression for each feature, had missing values with his one of correlated\n# feature\nfor f in features_with_missing_values :\n    model = LinearRegression()\n    if  len(correlated_features[f]) > 0 :\n        correlated = correlated_features[f][0]\n        if correlated in train.columns :\n           model.fit(train.loc[(train[correlated].notna()) & (train[f].notna()),correlated].values.reshape(-1,1),\\\n              train.loc[(train[correlated].notna()) & (train[f].notna()),f])\n           values_to_impute = train_df.loc[(train[f].isna()) & (train[correlated].notna()),f]\n           imputer = train.loc[(train[f].isna())&(train[correlated].notna()),correlated].values\n           if (len(values_to_impute) > 0) & (len(imputer) > 0) :\n              train_df.loc[(train[f].isna()) & (train[correlated].notna()),f] = model.predict(train.loc[(train[f].isna())&(train[correlated].notna()),correlated].values.\\\n                                                      reshape(-1,1))\n    \n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Display train_df missing values after imputation with linear regression\nmissing_a_imputation = pd.DataFrame({c:(train_df[c].isna().sum()) for c in train_df.columns},index=[\"Number missing values\"])\nprint(\"The number of missing values in the train_df dataframe after imputation processing :{}\".format(\\\n                                                                                                      missing_a_imputation.sum().sum()))\nmissing_a_imputation","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"==> Although this approach to remplace missing values , is very efficient .It was able to impute only about 6% of the prior number of missing values."},{"metadata":{"trusted":true},"cell_type":"code","source":"# for the rest of the missing values, we will use the mean value as imputer for each features.\nfor f in features_with_missing_values:\n    train_df.fillna(train_df[f].mean(),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2 id=feature_engineering> Feature Engineering </h2>"},{"metadata":{},"cell_type":"markdown","source":"The feature that we want to predict through this project , is the feature which can give signal to the investor to move on with the trading or not. The decision of the investor is based on the return of the trading ,so they should predict which one can make profit. Therefore we will create new feature named \"action\" wich give 1 when there is positive return and 0 if it is not."},{"metadata":{"trusted":true},"cell_type":"code","source":"#resp_cols = ['resp_1', 'resp_2', 'resp_3', 'resp', 'resp_4']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In order to enhance the predictive quality of our model, we will check the correlation between the global return and the differents returns in differents time horizons."},{"metadata":{"trusted":true},"cell_type":"code","source":"#\nreturns = [\"resp_1\",\"resp_2\",\"resp_3\",\"resp_4\",\"resp\"]\n                                                             \ndatas = pd.DataFrame({c:(train_df[c]>0).astype(\"int\") for c in returns})\ndatas[\"val\"] = datas.loc[:,\"resp\"].values\nfig , ax = plt.subplots(2,2,figsize= (15,12))\nfor i in range(len(returns)-1):\n    k = i // 2\n    l = i % 2\n    piv_resp_1_resp = pd.pivot_table(datas,index= returns[i],columns=\"resp\",values=\"val\",aggfunc=\"count\")\n    ty = datas[\"resp\"].value_counts()\n    tx = datas[returns[i]].value_counts()\n    tx = pd.DataFrame(tx)\n    ty = pd.DataFrame(ty)\n    ind = piv_resp_1_resp.index\n    col = piv_resp_1_resp.columns\n    tx.columns = [\"values\"]\n    ty.columns = [\"values\"]\n    n = len(datas)\n    cnt = tx.dot(ty.T)/n\n    cnt = cnt.loc[ind,col]\n    mesure = (cnt - piv_resp_1_resp) ** 2 /cnt\n    xid = mesure.sum().sum()\n    mesure = mesure \n    sns.heatmap(mesure,annot=True,linewidths=0.5,cmap=\"BrBG\",ax=ax[k,l],vmin=0,vmax=1)\n    ax[k,l].set_title(\"The Correlation Table Between the feature resp and {} \".\\\n                      format(returns[i]),size=12,color=\"red\")\n    print(\"The total correlation between the feature {} and the resp feature equal to {}\".\\\n          format(returns[i],xid))\n   \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color=redblue> We can conclude that there is high correlation between the return in the different horizons time and the return feature, which confirm that there features are highly correlated with the return feature. For this reason we will choice our feature action , that will be our target, as the combinaison of all theses features."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define new feature named action , which can help investor to make decidion.\n\ntrain_df[\"action\"] = ((train_df[\"resp\"] > 0) & (train_df[\"resp_1\"] >0) & (train_df[\"resp_2\"]>0)\\\n                     & (train_df[\"resp_3\"]>0) & (train_df[\"resp_4\"]>0)).astype(\"int\")\n#Y = np.stack((train_df[c]>0).astype(\"int\") for c in resp_cols).T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2 id=feat_exploration> Features dataset explorations :"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the features data.\nfeatures = pd.read_csv(path + \"features.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's take a look at the features data \nfeatures.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# some statistics on the datas.\nfeatures.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We can notice , that fortunatelly the datas don't have any missing values.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Change the type of features dataframe to int.\nfeatures.set_index('feature',inplace=True)\nfeatures = features.astype(\"int8\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#T_sne implementation\nt_sne = tsne(n_components=2,random_state=42).fit_transform(features.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the embedding of features datas in two dimension using the technique of TSNE.\nfig,ax = plt.subplots(1,1,figsize=(10,6))\nplt.scatter(t_sne[:,0],t_sne[:,1],cmap=\"coolwarm\")\nplt.grid(True)\nplt.title(\"t_SNE\")\nplt.suptitle(\"Dimmensionality Reduction using TSNE technique\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"==> The chart of Dimmensionality Reduction above , shows that the datas can be divided into some number of clusters. Indeed, we notice obviously, many accumulation of points, which are distant and they can be considered as a separated clusters."},{"metadata":{"trusted":true},"cell_type":"code","source":"# We will choice the best number of cluster, who can give best performance using \n# silhoute coefficient.\n\nclusters_number = [4,8,12,16,20,24,28] # number of clusters to test in order to choice the best one.\nsilhouette_performances = {} # Dictionnary of performance.\n\nfor cl_n in clusters_number :\n    kmeans = KMeans(n_clusters=cl_n)\n    kmeans.fit(features.values)\n    sc=silhouette_score(features.values,kmeans.labels_)\n    silhouette_performances[sc] = cl_n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot the the cluster performances in function of the number of clusters.\nfig,ax = plt.subplots(1,1,figsize=(15,8))\nplt.plot(list(silhouette_performances.values()),list(silhouette_performances.keys()))\nplt.title(\"Silhouette score \")\nplt.xlabel(\"Clusters number\",fontsize=10)\nplt.ylabel(\"Silhouette score \",fontsize=10)\nplt.ylim([0.15,0.28])\nbest_performace = np.max(list(silhouette_performances.keys()))\nab = silhouette_performances[best_performace]\ntext = \"best performance\"\nplt.annotate(text,xy=(ab,best_performace),arrowprops=dict(facecolor='black', shrink=0.05),\\\n            xytext=(ab+2,best_performace + 0.005))\nplt.tick_params(axis=\"x\",labelsize=15)\nplt.tick_params(axis=\"y\",labelsize=15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's now visualize theses defined clusters in two dimension, using T_SNE"},{"metadata":{"trusted":true},"cell_type":"code","source":"best_model = KMeans(n_clusters=20)\nbest_model.fit(features.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots(1,1,figsize=(15,8))\nplt.scatter(t_sne[:,0],t_sne[:,1],c=best_model.labels_)\nplt.suptitle(\"Visualize clusters using T_SNE\")\nplt.title(\"T_SNE\")\nplt.grid(True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2 id=modeling> Modeling :</h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_df.query('date > 85').reset_index(drop = True) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = list(train_df.columns) # list of retaind features \nfeatures.remove(\"weight\")\nfeatures.remove(\"resp_1\")\nfeatures.remove(\"resp_2\")\nfeatures.remove(\"resp_3\")\nfeatures.remove(\"resp_4\")\nfeatures.remove(\"resp\")\nfeatures.remove(\"action\")\nfeatures.remove(\"ts_id\")\nfeatures.remove(\"date\")\nfeatures.remove(\"feature_0\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"save_pickle(features,\"features_names\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig=px.pie(train_df.loc[train_df[\"weight\"] > 0,:],names=\"action\",title=\"Class imballance\")\nfig.update_layout(title={\"x\":0.475,\"y\":0.9,\"xanchor\":\"center\",\"yanchor\":\"top\"})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"==> The chart above , show that there is a class imballance , that we should correctly tackled in order to not biased the performance of our model."},{"metadata":{},"cell_type":"markdown","source":"In order to tackle correctly the class imbalnce problem, the easiest way to succefully generalise is to use more datas.The problem is that out-of-the-box classifiers like logistic regression or random forest tend to generalize by discarding the rare class. One easy best practice is building n models that use all the samples of the rare class and n-differing samples of the abundant class. Given that you want to ensemble 10 models, you would keep e.g. the 1.000 cases of the rare class and randomly sample 10.000 cases of the abundant class. Then you just split the 10.000 cases in 10 chunks and train 10 different models.</font>\n\n![](https://www.kdnuggets.com/wp-content/uploads/imbalanced-data-2.png)\n\nIn our case, the size of the abundant class is three time as bigger as the rarely class. So we need to train three models , by splitting the datas of the abundant class to three sample , and use the the datas of the rare class for each model training."},{"metadata":{"trusted":true},"cell_type":"code","source":"abundant_class = train_df.loc[(train_df[\"weight\"] > 0) & (train_df[\"action\"]==0),:] # extract datas which concern abundant datas\nrare_class = train_df.loc[(train_df[\"weight\"] > 0)&(train_df[\"action\"]==1),:]   # extract datas which concern rares datas.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The mean values of each feature to use in order to impute missing values in real production.\nimputer = np.mean(train_df[features].values,axis=0)\nsave_pickle(imputer,\"features_imputation\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"abundant_class = abundant_class.sample(frac=1)\nrare_class = rare_class.sample(frac=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"l = len(rare_class) # the size of data which concern rare class.\ndf1 = abundant_class.iloc[:l,:].append(rare_class) # 1st chunk of datas to train first model\ndf2 = abundant_class.iloc[l:2*l,:].append(rare_class) # 2nd chunk of datas to train second model\ndf3 = abundant_class.iloc[2*l:,:].append(rare_class) # 3nd chunk of datas to train the third model.\n\ndf1 = df1.sample(frac=1) # shuffle the datas\ndf2 = df2.sample(frac=1) # shuffle the datas\ndf3 = df3.sample(frac=1) # shuffle the datas","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(df1),len(df2),len(df3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reduce the memory charge\ndel(train_df)\ndel(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# retained features and label to train differents models on differents chunk of datas.\ntraining= [df1[features],df2[features],df3[features]] \ntargets = [df1[\"action\"],df2[\"action\"],df3[\"action\"]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"datas = []\nfor i in range(3):\n    xtr,xval,ytr,yval = train_test_split(training[i].values,targets[i].values,test_size=0.1,\\\n                                        stratify=targets[i].values)\n    datas.append(((xtr,ytr),(xval,yval)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# refresh memory\ndel(df1)\ndel(df2)\ndel(df3)\ndel(abundant_class)\ndel(rare_class)\ndel(features_with_missing_values)\ndel(correlation_table)\ndel(missing_table)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# modeling step \nparams={\"num_leaves\":300,\n       \"max_bin\":450,\n       \"feature_fraction\":0.52,\n       \"bagging_fraction\":0.52,\n       \"objective\":\"binary\",\n       \"learning_rate\":0.05,\n       \"boosting_type\":\"gbdt\",\n       \"metric\":\"auc\"\n       }\n#kf = KFold(n_splits=3,shuffle=True,random_state=111)\nmodels = [] # list of model , we will train \nfor i in range(3):\n    xtr = datas[i][0][0]\n    ytr = datas[i][0][1]\n    xval = datas[i][1][0]\n    yval = datas[i][1][1]\n    #xval = val_datas[j].loc[:,features]\n    #yval = val_datas[j].loc[:,\"action\"]\n    d_train = lgbm.Dataset(xtr,label=ytr)\n    d_eval = lgbm.Dataset(xval,label=yval,reference=d_train)\n    clf = lgbm.train(params,d_train,valid_sets=[d_train,d_eval],num_boost_round=1500,\\\n                    early_stopping_rounds=50,verbose_eval=50)\n    clf.save_model(\"weights_{}\".format(i))\n    models.append(clf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"import tensorflow as tf \nfrom tensorflow.keras.layers import Input,Dense, Dropout,BatchNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.activations import swish\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.optimizers import Adam\nSEED = 1111\n\ndef create_model(n_features,dr_rate,hidden_units,n_labels,label_smoothing,lr):\n    inp = Input(shape=(n_features,))\n    x = BatchNormalization()(inp)\n    x = Dropout(dr_rate[0])(x)\n    for i in range(len(hidden_units)):\n        x = Dense(hidden_units[0])(x)\n        x = BatchNormalization()(x)\n        x = swish(x)\n        x = Dropout(dr_rate[i+1])(x)\n    out = Dense(n_labels,activation=\"sigmoid\")(x)\n    model = Model(inputs=inp,outputs=out)\n    \n    model.compile(loss = BinaryCrossentropy(label_smoothing=label_smoothing),\\\n                  optimizer = Adam(learning_rate=lr),metrics= tf.keras.metrics.AUC(name=\"AUC\"))\n    return model"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"tf.random.set_seed(SEED)\nnp.random.seed(SEED)\nmodels = []\nn_features = len(features)\ndr_rate = [0.2,0.2,0.2,0.2]\nhidden_units = [150,150,150]\nlabel_smoothing = 1e-2\nlr = 1e-3\nbatch_size = 5000\nfor i in range(len(datas)):\n    hist = []\n    j = (i+1)% 2\n    clf = create_model(n_features,dr_rate,hidden_units,1,label_smoothing,lr)\n    clf.fit(datas[i].values,target[i].values,validation_data = (datas[j].values,\\\n                                                                target[j].values),batch_size=batch_size,epochs=4)\n    \n    hist.append(clf)\n    models.append(hist[-1])"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots(1,3,figsize=(10,20))\nfor i in range(3):\n    lgbm.plot_importance(models[i],ax=ax[i])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2 id=submission> Submission </h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"import janestreet\nenv = janestreet.make_env()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"th = 0.5000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for (test_df, pred_df) in tqdm(env.iter_test()):\n    if test_df[\"weight\"].item() > 0 :\n        x_tt = test_df.loc[:, features].values\n        if np.isnan(x_tt.sum()):\n           x_tt = np.nan_to_num(x_tt) + np.isnan(x_tt) * imputer\n        pred = np.mean(np.stack([model.predict(x_tt) for model in models]),axis=0).T\n        pred_df.action = np.where(pred >= th, 1, 0).astype(int)\n    else :\n        pred_df.action = 0\n    \n    env.predict(pred_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color=red> <b> PLease leave your comments to enhance the work , or upvote if you like it !</font>"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}