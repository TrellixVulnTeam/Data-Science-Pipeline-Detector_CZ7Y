{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Data Description\n\nThis dataset contains an anonymized set of features, feature_{0...129}, representing real stock market data. Each row in the dataset represents a trading opportunity, for which you will be predicting an action value: 1 to make the trade and 0 to pass on it. Each trade has an associated weight and resp, which together represents a return on the trade. The date column is an integer which represents the day of the trade, while ts_id represents a time ordering. In addition to anonymized feature values, you are provided with metadata about the features in features.csv.\n\nIn the training set, train.csv, you are provided a resp value, as well as several other resp_{1,2,3,4} values that represent returns over different time horizons. These variables are not included in the test set. Trades with weight = 0 were intentionally included in the dataset for completeness, although such trades will not contribute towards the scoring evaluation."},{"metadata":{},"cell_type":"markdown","source":"\nFor extensive data analysis for the jane street market dataset go to following link [EDA-Quantitative-Researcher-prespective](https://www.kaggle.com/huzzefakhan/eda-quantitative-researcher-prespective) this notebook will go through the train and features csv's for an extensive exploratory data analysis, Also some data cleaning and preprocessing will be done along the way. In this note book I use Principal Components Analysis (PCA) to identify features to be used for supervised learning. This is first version of modeling i am also tring to add some finding from my EDA in this modeling exercise.\n"},{"metadata":{},"cell_type":"markdown","source":"## About me\n\nWorking as Data Scientist in IT firm in Pakistan. I was Recently Enguaged with Radix Trading LLC which is a firm just like Jane Street which also work in High frequency algorithmic trading. Where i worked as Quantitative Researcher (Quant) to Capture Price movement in High frequency Algorithmic trading through Alphas. Designed many successful Alpha/strategies which is trade-able in real Stock market. For more details kindly visit my linkedin profile.\n\nPlease upvote if you find this notebook helpful! ðŸ˜Š Thank you!.\n"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np  \nimport pandas as pd  \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport xgboost as xgb\nimport optuna","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import dataset as train\ntrain = pd.read_csv('/kaggle/input/jane-street-market-prediction/train.csv')#, nrows=2000000\ntrain.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop rows with 'weight'=0 \n# such trades will not contribute towards the scoring evaluation\ntrain = train[train['weight']!=0]\n\n# Create 'action' column (dependent variable)\ntrain['action'] = train['resp'].apply(lambda x:x>0).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = [col for col in list(train.columns) if 'feature' in col]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train[features]\ny = train['action']\n\n# Next, we hold out part of the training data to form the hold-out validation set\ntrain_x, valid_x, train_y, valid_y = train_test_split(X, y, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# First, we want to check if the target class is balanced or unbalanced in the training data\nsns.set_palette(\"colorblind\")\nax = sns.barplot(train_y.value_counts().index, train_y.value_counts()/len(train_y))\nax.set_title(\"Proportion of trades with action=0 and action=1\")\nax.set_ylabel(\"Percentage\")\nax.set_xlabel(\"Action\")\nsns.despine();\n# Target class is fairly balanced with almost 50% of trades corresponding to each action","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finally, we investigate if there are missing values and we impute them\nn_features = 60\nnan_val = train.isna().sum()[train.isna().sum() > 0].sort_values(ascending=False)\nprint(nan_val)\nfig, axs = plt.subplots(figsize=(10, 10))\nsns.barplot(y = nan_val.index[0:n_features], \n            x = nan_val.values[0:n_features], \n            alpha = 0.8\n           )\nplt.title('Missing values of train dataset')\nplt.xlabel('# of Missing values')\nplt.show()\n# There are quite a lot of missing values across the features\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_median = train_x.median()\n# Impute medians in both training set and the hold-out validation set\ntrain_x = train_x.fillna(train_median)\nvalid_x = valid_x.fillna(train_median)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Before we perform PCA, we need to normalise the features so that they have zero mean and unit variance\nscaler = StandardScaler()\nscaler.fit(train_x)\ntrain_x_norm = scaler.transform(train_x)\n\npca = PCA()\ncomp = pca.fit(train_x_norm)\n\n# We plot a graph to show how the explained variation in the 129 features varies with the number of principal components\nplt.plot(np.cumsum(comp.explained_variance_ratio_))\nplt.grid()\nplt.xlabel('Number of Principal Components')\nplt.ylabel('Explained Variance')\nsns.despine();\n\n# The first 15 principal components explains about 80% of the variation\n# The first 40 principal components explains about 95% of the variation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using the first 40 principal components, we apply the PCA mapping\n# From here on, we work with only 40 features instead of the full set of 129 features\npca = PCA(n_components=40).fit(train_x_norm)\ntrain_x_transform = pca.transform(train_x_norm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_x_transform = pca.transform(scaler.transform(valid_x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We create the XGboost-specific DMatrix data format from the numpy array. \n# This data structure is optimised for memory efficiency and training speed\ndtrain = xgb.DMatrix(train_x_transform, label=train_y)\ndvalid = xgb.DMatrix(valid_x_transform, label=valid_y)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def objective(trial):\n    \n# params specifies the XGBoost hyperparameters to be tuned\n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 200, 600),\n        'max_depth': trial.suggest_int('max_depth', 10, 25),\n        'learning_rate': trial.suggest_uniform('learning_rate', 0.01, 0.1),\n        'subsample': trial.suggest_uniform('subsample', 0.50, 1),\n        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.50, 1),\n        'gamma': trial.suggest_int('gamma', 0, 10),\n        'tree_method': 'gpu_hist',  \n        'objective': 'binary:logistic'\n    }\n    \n    bst = xgb.train(params, dtrain)\n    preds = bst.predict(dvalid)\n    pred_labels = np.rint(preds)\n    accuracy = sklearn.metrics.accuracy_score(valid_y, pred_labels)\n    return accuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"study = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=25, timeout=600)\n\nprint(\"Number of finished trials: \", len(study.trials))\nprint(\"Best trial:\")\ntrial = study.best_trial\n\nprint(\"  Value: {}\".format(trial.value))\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_params = trial.params\nbest_params['tree_method'] = 'gpu_hist' \nbest_params['objective'] = 'binary:logistic'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit the XGBoost classifier with optimal hyperparameters\nclf = xgb.XGBClassifier(**best_params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf.fit(train_x_transform, train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot how the best accuracy evolves with number of trials\nfig = optuna.visualization.plot_optimization_history(study)\nfig.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can also plot the relative importance of different hyperparameter settings\nfig = optuna.visualization.plot_param_importances(study)\nfig.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We impute the missing values with the medians\ndef fillna_npwhere(array, values):\n    if np.isnan(array.sum()):\n        array = np.where(np.isnan(array), values, array)\n    return array","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import janestreet\nenv = janestreet.make_env() # initialize the environment\niter_test = env.iter_test() # an iterator which loops over the test set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_prediction_df = pd.read_csv('../input/jane-street-market-prediction/example_sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for (test_df, sample_prediction_df) in iter_test:\n    wt = test_df.iloc[0].weight\n    if(wt == 0):\n        sample_prediction_df.action = 0 \n    else:\n        sample_prediction_df.action = clf.predict(pca.transform(scaler.transform(fillna_npwhere(test_df[features].values,train_median[features].values))))\n    env.predict(sample_prediction_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Referrences\n\n* https://www.kaggle.com/marketneutral/jane-street-eda-regime-tags-clusters\n* https://www.kaggle.com/wongguoxuan/eda-pca-xgboost-classifier-for-beginners#3 \n* https://www.kaggle.com/huzzefakhan/eda-quantitative-researcher-prespective\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}