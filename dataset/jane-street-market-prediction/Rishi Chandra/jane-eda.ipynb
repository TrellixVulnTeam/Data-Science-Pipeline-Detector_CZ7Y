{"cells":[{"metadata":{"execution":{"iopub.execute_input":"2021-01-22T17:55:45.967945Z","iopub.status.busy":"2021-01-22T17:55:45.967126Z","iopub.status.idle":"2021-01-22T17:55:46.016606Z","shell.execute_reply":"2021-01-22T17:55:46.015938Z"},"papermill":{"duration":0.080791,"end_time":"2021-01-22T17:55:46.016735","exception":false,"start_time":"2021-01-22T17:55:45.935944","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-01-22T17:55:46.078797Z","iopub.status.busy":"2021-01-22T17:55:46.077856Z","iopub.status.idle":"2021-01-22T17:55:58.435764Z","shell.execute_reply":"2021-01-22T17:55:58.434712Z"},"papermill":{"duration":12.397325,"end_time":"2021-01-22T17:55:58.43594","exception":false,"start_time":"2021-01-22T17:55:46.038615","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"import os\nimport gc\nimport pandas as pd\nimport numpy as np\n\nimport fastai\nfrom   fastai.callback import *\nfrom   fastai.callback.all import *\nfrom   fastai.callback.training import GradientClip\nfrom   fastai.callback.all import SaveModelCallback, EarlyStoppingCallback, ReduceLROnPlateau \nfrom   fastai.tabular import *\nfrom   fastai.tabular.data import *\nfrom   fastai.tabular.all import *\nfrom   fastai.tabular.all import TabularPandas, RandomSplitter, CategoryBlock, MultiCategoryBlock, range_of, accuracy, tabular_learner, TabularDataLoaders\nfrom   fastai.learner import Learner\nfrom   fastai.metrics import RocAucMulti\n\nfrom   sklearn.pipeline import Pipeline, FeatureUnion\nfrom   sklearn.impute import SimpleImputer\nfrom   sklearn.preprocessing import PolynomialFeatures, StandardScaler\nfrom   sklearn.compose import ColumnTransformer\nfrom   sklearn.ensemble import RandomForestClassifier\nfrom   sklearn.base import BaseEstimator, TransformerMixin\n\nimport torch.nn as nn\nfrom   torch.nn import CrossEntropyLoss, MSELoss\nfrom   torch.nn.modules.loss import _WeightedLoss\n\nimport pickle\nfrom   functools import partial\nimport warnings\nwarnings.filterwarnings (\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Global Vars\nTP   = None\nDF   = None\nDLs  = None\nPIPE = None\nBS   = 10000\nN_FEATURES  = 0","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-01-22T17:55:58.538074Z","iopub.status.busy":"2021-01-22T17:55:58.537413Z","iopub.status.idle":"2021-01-22T17:55:58.552729Z","shell.execute_reply":"2021-01-22T17:55:58.553283Z"},"papermill":{"duration":0.040645,"end_time":"2021-01-22T17:55:58.553426","exception":false,"start_time":"2021-01-22T17:55:58.512781","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def preprocess_data (filename='../input/jane-street-market-prediction/train.csv', df=None, isTrainData=True):\n    \n    global PIPE, N_FEATURES\n    dtype = None\n    if isTrainData:\n        \n        dtype = {\n            'date'      : 'int64', \n            'weight'    : 'float32',\n            'resp'      : 'float32',\n            'ts_id'     : 'int64',  \n            'feature_0' : 'float32'\n        }\n    else:\n        \n        dtype = {\n            'date'      : 'int64', \n            'weight'    : 'float32',\n            'feature_0' : 'float32'\n        }\n    for i in range (1, 130):\n        k = 'feature_' + str (i)\n        dtype[k] = 'float32'\n    \n    to   = None\n    if isTrainData:\n        df         = pd.read_csv (filename, dtype=dtype)\n        df         = df.query ('date > 85')\n        # df       = df[df['weight'] != 0].reset_index (drop = True)\n        df         = df.reset_index (drop = True)\n        \n        resp_cols  = ['resp_1', 'resp_2', 'resp_3','resp_4', 'resp']    \n        # df[:5000].to_csv (filename+'.dummy', index=False) \n        y          = np.median (np.stack ([df[c] for c in resp_cols]).T, axis=1)\n        df.drop (columns=['weight', 'date', 'ts_id']+resp_cols, inplace=True)\n        f_columns  = [c for c in df.columns if \"feature\" in c]\n        PIPE       = Pipeline ([\n                         (\"imputer\", SimpleImputer (missing_values=np.nan, strategy='mean')),\n                         (\"stndard\", StandardScaler ()),\n        ])\n        X          = PIPE.fit_transform (df)                                   #;print('X.shape =', X.shape)\n        f_columns  = [f\"feature_{i}\" for i in range (X.shape[1])]              #;print ('columns =', columns)\n        df         = pd.DataFrame (np.hstack ((X, y.reshape ((-1,1)))))\n        df.columns = f_columns + ['Y']\n        N_FEATURES = len (f_columns)\n        del X\n    else:\n        \n        df         = df.drop (columns=['weight', 'date']).reset_index (drop = True)\n        X          = PIPE.transform (df)\n        df         = pd.DataFrame (X)\n        df.columns = [f\"feature_{i}\" for i in range (X.shape[1])] \n        del X\n    return df","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-01-22T17:55:58.601049Z","iopub.status.busy":"2021-01-22T17:55:58.600425Z","iopub.status.idle":"2021-01-22T17:58:44.347053Z","shell.execute_reply":"2021-01-22T17:58:44.347603Z"},"papermill":{"duration":165.771921,"end_time":"2021-01-22T17:58:44.347759","exception":false,"start_time":"2021-01-22T17:55:58.575838","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"DF = preprocess_data ()\nwith open (\"PIPE.bin\", \"ab\") as f:\n    pickle.dump (PIPE, f)\n# DF = DF.sample (DF.shape[0]//10)\nY = DF['Y']\nDF.drop (columns=['Y'], inplace=True)\ngc.collect ()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DF.shape, Y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import pearsonr\nUNARY_TRACKER = dict ()\n\ndef replaceFeature (unaryFuncs, df, y):\n    \"\"\"\n    unaryFuncs: list of functions\n    \"\"\"\n    col_corrs = []\n    for colname in df.columns:\n        \n        data = df[colname].values\n        max_corr = abs (pearsonr (data, y)[0])                  #;print (colname, 'max_corr =', max_corr)\n        unaryFunc_i = -1\n        for i in range (len (unaryFuncs)):\n            \n            unaryFunc = unaryFuncs[i]\n            transformed_data = unaryFunc (data)                 #;print ('data =', data, 'transformed_data =', transformed_data)\n            corr = abs (pearsonr (transformed_data, y)[0])      #;print (colname, 'transformed_data corr =', corr)\n            if np.isnan (corr):\n                corr = 0.0\n            if corr > max_corr:\n                \n                df[colname] = transformed_data                  # TODO: uncomment this\n                max_corr = corr                                 #;print (f'replacing by unaryFuncs[{i}]')\n                unaryFunc_i = i\n        if unaryFunc_i > -1:\n            UNARY_TRACKER[colname] = unaryFunc_i\n        col_corrs.append (max_corr)\n    return col_corrs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"UNARY_FUNCS = [lambda x: np.sign(x)*x**2,    lambda x: x**3,    lambda x: np.sign(x)*np.sqrt(np.abs(x)),\n               lambda x: np.sign(x)*np.log(np.abs(x) + (np.abs(x)<1e-4)*1e-4),     lambda x: 1/(x + (np.abs(x)<1e-4)*1e-4) ]\ncol_corrs   = replaceFeature (UNARY_FUNCS, DF, Y)\nmean_feat_corr = np.mean (col_corrs)\nwith open ('UNARY_TRACKER.bin', 'ab') as pfile:\n    pickle.dump (UNARY_TRACKER, pfile)\nmean_feat_corr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_corrs = sorted (col_corrs, reverse=True)\ncol_corrs[:20]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# CORR_THRESH = np.mean (col_corrs)\nCORR_THRESH = np.quantile (col_corrs, 0.97)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Find Correlation among features and remove highly correlated features\nDF['Y'] = Y\ncorr = DF.corr(method='pearson').abs().unstack().sort_values(kind='quicksort', ascending=False).reset_index()\nDF.drop (columns=['Y'], inplace=True)\ncorr.rename(columns={'level_0':'feature_A', 'level_1':'feature_B', 0:'Corr_Coeff'}, inplace=True)\ncorr = corr[corr['Corr_Coeff']<=0.8]\ncorr.dropna(inplace=True)\ncorr.head ()"},{"metadata":{},"cell_type":"markdown","source":"# Which features correlate more with the target?\ncorr[corr['feature_A']=='Y'].head(10)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_interactions (binFuncs, df, y):\n    \n    bin_cols = []\n    for colname1 in df.columns:\n        for colname2 in df.columns:\n            if colname1 == colname2:\n                break                \n            for i in range (len (binFuncs)):\n                \n                binFunc  = binFuncs[i]\n                new_data = binFunc (df[colname1].values, df[colname2].values)\n                corr  = abs (pearsonr (new_data, y)[0])\n                # corr1 = abs (pearsonr (df[colname1].values, y)[0])\n                # corr2 = abs (pearsonr (df[colname2].values, y)[0])\n                if np.isnan (corr):\n                    corr = 0.0\n                # if corr > corr1 and corr > corr2 and corr > CORR_THRESH:\n                if corr > CORR_THRESH:    \n                    # insert the new colname but don't add now in the df else infinite loop\n                    new_col_name = colname1+\"#\"+str(i)+\"#\"+colname2\n                    bin_cols.append ((corr, new_col_name))\n    \n    # now add the new found cols to the df\n    # for t in BIN_COLS:        \n    #     colname1 = t[1].split('#')[0]\n    #     colname2 = t[1].split('#')[-1]\n    #     binFunc  = binFuncs[int (t[1].split('#')[1])]        \n    #     df[t[1]]    = binFunc (df[colname1].values, df[colname2].values)  \n    \n    bin_cols.sort (reverse=True)\n    bin_cols = [t[1] for t in bin_cols]\n    return bin_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BIN_FUNCS = [lambda x,y: x+y,    lambda x,y: x-y,    lambda x,y: x*y,\n             lambda x,y: x/(y + (np.abs(y)<1e-4)*1e-4),  lambda x,y: y/(x + (np.abs(x)<1e-4)*1e-4) ]\nBIN_COLS  = add_interactions (BIN_FUNCS, DF, Y)\n\nwith open('BIN_COLS.bin', 'ab') as pfile:\n    pickle.dump (BIN_COLS, pfile)\nBIN_COLS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len (BIN_COLS)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Inference time usage"},{"metadata":{"trusted":true},"cell_type":"code","source":"pickle_path_dir = \".\"\n\nUNARY_FUNCS = [lambda x: np.sign(x)*x**2,    lambda x: x**3,    lambda x: np.sign(x)*np.sqrt(np.abs(x)),\n               lambda x: np.sign(x)*np.log(np.abs(x) + (np.abs(x)<1e-4)*1e-4),     lambda x: 1/(x + (np.abs(x)<1e-4)*1e-4) ]\nBIN_FUNCS = [lambda x,y: x+y,    lambda x,y: x-y,    lambda x,y: x*y,\n             lambda x,y: x/(y + (np.abs(y)<1e-4)*1e-4),  lambda x,y: y/(x + (np.abs(x)<1e-4)*1e-4) ]\nwith open (pickle_path_dir+\"/PIPE.bin\", \"rb\") as f:\n    PIPE = pickle.load (f)\nwith open (pickle_path_dir+\"/BIN_COLS.bin\", \"rb\") as f:\n    BIN_COLS = pickle.load (f)\nwith open (pickle_path_dir+\"/UNARY_TRACKER.bin\", \"rb\") as f:\n    UNARY_TRACKER = pickle.load (f)\n\n    \nclass EDATransformer (BaseEstimator, TransformerMixin):\n    \n    def __init__(self, unary_tracker, bin_cols, unary_funcs, bin_funcs, n_new_feat=500):\n        \n        super (EDATransformer, self).__init__()\n        self.unary_tracker = unary_tracker\n        self.bin_cols      = bin_cols\n        self.unary_funcs   = unary_funcs\n        self.bin_funcs     = bin_funcs\n        self.n_new_feat    = min (n_new_feat, len (bin_cols))\n        return\n        \n    def fit (self, X, y=None):        \n        return self\n    \n    def transform (self, X, y=None):\n        \n        for colname in self.unary_tracker:\n            col_idx  = int (colname.split (\"_\")[1])\n            uf_idx   = self.unary_tracker[colname]\n            X[:, col_idx] = self.unary_funcs[uf_idx] (X[:, col_idx])\n        \n        for i in range (self.n_new_feat):\n            colname  = self.bin_cols[i]\n            col_idx1 = int (colname.split ('#')[0].split (\"_\")[1])           #;print ('col_idx1 =', col_idx1)\n            col_idx2 = int (colname.split ('#')[-1].split (\"_\")[1])\n            binFunc  = self.bin_funcs[int (colname.split ('#')[1])]        \n            X        = np.hstack ((X, binFunc (X[:, col_idx1], X[:, col_idx2]).reshape ((-1, 1))))\n        return X\n    \n    def set_params (self, **parameters):\n        for parameter, value in parameters.items ():\n            setattr (self, parameter, value)\n        return self\n\n    def get_params (self, deep=True):\n        params = {}\n        return params\n\nedat = EDATransformer (UNARY_TRACKER, BIN_COLS, UNARY_FUNCS, BIN_FUNCS)\nPIPE = Pipeline (PIPE.steps + [('eda', edat)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PIPE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch = DF[:100]\nbatch = PIPE.transform (batch)\nbatch.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_data (filename='../input/jane-street-market-prediction/train.csv', df=None, isTrainData=True):\n    \n    dtype = None\n    if isTrainData:\n        \n        dtype = {\n            'date'      : 'int64', \n            'weight'    : 'float32',\n            'resp'      : 'float32',\n            'ts_id'     : 'int64',  \n            'feature_0' : 'float32'\n        }\n    else:\n        \n        dtype = {\n            'date'      : 'int64', \n            'weight'    : 'float32',\n            'feature_0' : 'float32'\n        }\n    for i in range (1, 130):\n        k = 'feature_' + str (i)\n        dtype[k] = 'float32'    \n    to = None\n    df = None\n    if isTrainData:\n        \n        df         = pd.read_csv (filename, dtype=dtype)\n        df         = df.query ('date > 85')\n        df         = df[df['weight'] != 0].reset_index (drop = True)\n        # df       = df.reset_index (drop = True)        \n        resp_cols  = ['resp_1', 'resp_2', 'resp_3','resp_4', 'resp']    \n        # df[:5000].to_csv (filename+'.dummy', index=False) \n        y          = np.stack ([(df[c] > 0).astype ('int') for c in resp_cols]).T\n        df.drop (columns=['weight', 'date', 'ts_id']+resp_cols, inplace=True)\n        f_columns  = [c for c in df.columns if \"feature\" in c]                \n        df[resp_cols] = y\n        df.columns = f_columns + resp_cols\n        del y\n\n        splits     = RandomSplitter (valid_pct=0.05) (range_of (df))\n        to         = TabularPandas (df, cont_names=f_columns, cat_names=None, y_names=resp_cols, y_block=MultiCategoryBlock(encoded=True, vocab=resp_cols), splits=splits)\n    else:\n        \n        df         = df.drop (columns=['weight', 'date']).reset_index (drop = True)\n        # X          = PIPE.transform (df)        \n        # df.columns = [f\"feature_{i}\" for i in range (X.shape[1])] \n    return to, df","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}