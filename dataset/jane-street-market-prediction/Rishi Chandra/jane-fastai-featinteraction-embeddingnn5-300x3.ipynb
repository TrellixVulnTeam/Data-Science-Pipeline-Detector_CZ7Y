{"cells":[{"metadata":{"execution":{"iopub.execute_input":"2021-01-22T17:55:45.967945Z","iopub.status.busy":"2021-01-22T17:55:45.967126Z","iopub.status.idle":"2021-01-22T17:55:46.016606Z","shell.execute_reply":"2021-01-22T17:55:46.015938Z"},"papermill":{"duration":0.080791,"end_time":"2021-01-22T17:55:46.016735","exception":false,"start_time":"2021-01-22T17:55:45.935944","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"HIDDEN_LAYERS = [300, 300, 300]","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-01-22T17:55:46.078797Z","iopub.status.busy":"2021-01-22T17:55:46.077856Z","iopub.status.idle":"2021-01-22T17:55:58.435764Z","shell.execute_reply":"2021-01-22T17:55:58.434712Z"},"papermill":{"duration":12.397325,"end_time":"2021-01-22T17:55:58.43594","exception":false,"start_time":"2021-01-22T17:55:46.038615","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"import os\nimport gc\nimport pandas as pd\nimport numpy as np\n\nimport fastai\nfrom   fastai.callback import *\nfrom   fastai.callback.all import *\nfrom   fastai.callback.training import GradientClip\nfrom   fastai.callback.all import SaveModelCallback, EarlyStoppingCallback, ReduceLROnPlateau \nfrom   fastai.tabular import *\nfrom   fastai.tabular.data import *\nfrom   fastai.tabular.all import *\nfrom   fastai.tabular.all import TabularPandas, RandomSplitter, CategoryBlock, MultiCategoryBlock, range_of, accuracy, tabular_learner, TabularDataLoaders\n# from fastai import datasets\n# from fastai.dataset import ModelData,ArraysIndexDataset\n# from fastai.dataloader import DataLoader\nfrom   fastai.learner import Learner\nfrom   fastai.metrics import RocAucMulti\n\nfrom   sklearn.pipeline import Pipeline, FeatureUnion\nfrom   sklearn.impute import SimpleImputer\nfrom   sklearn.preprocessing import PolynomialFeatures\nfrom   sklearn.compose import ColumnTransformer\nfrom   sklearn.base import BaseEstimator, TransformerMixin\n\nimport torch.nn as nn\nfrom   torch.nn import CrossEntropyLoss, MSELoss\nfrom   torch.nn.modules.loss import _WeightedLoss\n\nfrom   functools import partial\nimport warnings\nwarnings.filterwarnings (\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Global Vars\nTP   = None\nDF   = None\nDLs  = None\nPIPE = None\nBS   = 10000\nN_FEATURES  = 0\nN_FEAT_TAGS = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class IdentityTransformer (BaseEstimator, TransformerMixin):\n      \n    def fit (self, X, y=None):\n        print ('IdentityTransformer: type(X), X.shape =', type (X), X.shape)\n        return self\n    \n    def transform (self, X, y=None):\n        print ('IdentityTransformer: type(X), X.shape =', type (X), X.shape)\n        return X\n    \n    def set_params (self, **parameters):\n        for parameter, value in parameters.items ():\n            setattr (self, parameter, value)\n        return self\n\n    def get_params (self, deep=True):\n        params = {}\n        return params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dtype = {\n    'feature'  : 'str', \n    'tag_0'    : 'int8'\n}\nfor i in range (1, 29):\n    k = 'tag_' + str (i)\n    dtype[k] = 'int8'\n    \nfeatures_df = pd.read_csv ('../input/jane-street-market-prediction/features.csv', usecols=range (1,30), dtype=dtype)\nN_FEATURES  = features_df.shape[0]  # the features.csv has 130 features (1st row) = no of features in train.csv (feature_0 to feature_129)\nN_FEAT_TAGS = features_df.shape[1]  # the features.csv has 29 tags\n\n# features_df.head ()\ndel features_df\ngc.collect ()\nN_FEATURES, N_FEAT_TAGS","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-01-22T17:55:58.538074Z","iopub.status.busy":"2021-01-22T17:55:58.537413Z","iopub.status.idle":"2021-01-22T17:55:58.552729Z","shell.execute_reply":"2021-01-22T17:55:58.553283Z"},"papermill":{"duration":0.040645,"end_time":"2021-01-22T17:55:58.553426","exception":false,"start_time":"2021-01-22T17:55:58.512781","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def preprocess_data (filename='../input/jane-street-market-prediction/train.csv', df=None, isTrainData=True):\n    \n    global PIPE, N_FEATURES\n    dtype = None\n    if isTrainData:\n        \n        dtype = {\n            'date'      : 'int64', \n            'weight'    : 'float32',\n            'resp'      : 'float32',\n            'ts_id'     : 'int64',  \n            'feature_0' : 'float32'\n        }\n    else:\n        \n        dtype = {\n            'date'      : 'int64', \n            'weight'    : 'float32',\n            'feature_0' : 'float32'\n        }\n    for i in range (1, 130):\n        k = 'feature_' + str (i)\n        dtype[k] = 'float32'\n    \n    to   = None\n    if isTrainData:\n        df         = pd.read_csv (filename, dtype=dtype)\n        df         = df.query ('date > 85')\n        # df       = df[df['weight'] != 0].reset_index (drop = True)\n        df         = df.reset_index (drop = True)\n        \n        resp_cols  = ['resp_1', 'resp_2', 'resp_3','resp_4', 'resp']    \n        # df[:5000].to_csv (filename+'.dummy', index=False) \n        y          = np.stack ([(df[c] > 0).astype ('int') for c in resp_cols]).T\n        df.drop (columns=['weight', 'date', 'ts_id']+resp_cols, inplace=True)\n        f_columns  = [c for c in df.columns if \"feature\" in c]\n        PIPE       = Pipeline ([\n                         (\"imputer\", SimpleImputer (missing_values=np.nan, strategy='mean')),\n                         ('x_x2',  FeatureUnion ([\n                             ('x', IdentityTransformer()),\n                             (\"x2_interactions\", ColumnTransformer ([\n                                 # interactions for [44,45,41,42,43,62,5,60,63,6]\n                                ('top10_interactions', PolynomialFeatures (2, interaction_only=True, include_bias=False), [44,45,41,42,43,62,5,60,63,6]),\n                             ]))\n                         ]))\n        ])\n        X          = PIPE.fit_transform (df)                                   #;print('X.shape =', X.shape)\n        f_columns  = [f\"feature_{i}\" for i in range (X.shape[1])]              #;print ('columns =', columns)\n        df         = pd.DataFrame (np.hstack ((X, y)))\n        df.columns = f_columns + resp_cols\n        N_FEATURES = len (f_columns)\n        del X, y\n\n        splits    = RandomSplitter (valid_pct=0.05) (range_of (df))\n        to        = TabularPandas (df, cont_names=f_columns, cat_names=None, y_names=resp_cols, y_block=MultiCategoryBlock(encoded=True, vocab=resp_cols), splits=splits)\n    else:\n        \n        df         = df.drop (columns=['weight', 'date']).reset_index (drop = True)\n        X          = PIPE.transform (df)\n        df         = pd.DataFrame (X)\n        df.columns = [f\"feature_{i}\" for i in range (X.shape[1])] \n        del X\n    return to, df","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-01-22T17:55:58.601049Z","iopub.status.busy":"2021-01-22T17:55:58.600425Z","iopub.status.idle":"2021-01-22T17:58:44.347053Z","shell.execute_reply":"2021-01-22T17:58:44.347603Z"},"papermill":{"duration":165.771921,"end_time":"2021-01-22T17:58:44.347759","exception":false,"start_time":"2021-01-22T17:55:58.575838","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"TP, DF = preprocess_data ()\nTP.xs.iloc[:2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TP.ys.iloc[:2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TP.xs.shape, TP.ys.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DLs = TP.dataloaders (bs=BS)\nDLs.show_batch ()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DLs.one_batch ()[2].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_cat, x_cont, y = DLs.train.one_batch ()\nx_cat.shape, x_cont.shape, y.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Custom Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"class SmoothBCEwLogits(_WeightedLoss):\n    \n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n        assert 0 <= smoothing < 1\n        with torch.no_grad():\n            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n        return targets\n\n    def forward(self, inputs, targets):\n        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n            self.smoothing)\n        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class FFN (nn.Module):\n    \n    def __init__(self, inputCount=N_FEATURES, outputCount=5, hiddenLayerCounts=[150, 150, 150], \n                 drop_prob=0.2, nonlin=nn.SiLU (), isOpAct=False):\n        \n        super(FFN, self).__init__()\n        \n        self.nonlin     = nonlin\n        self.dropout    = nn.Dropout (drop_prob)\n        self.batchnorm0 = nn.BatchNorm1d (inputCount)\n        self.dense1     = nn.Linear (inputCount, hiddenLayerCounts[0])\n        self.batchnorm1 = nn.BatchNorm1d (hiddenLayerCounts[0])\n        self.dense2     = nn.Linear(hiddenLayerCounts[0], hiddenLayerCounts[1])\n        self.batchnorm2 = nn.BatchNorm1d (hiddenLayerCounts[1])\n        self.dense3     = nn.Linear(hiddenLayerCounts[1], hiddenLayerCounts[2])\n        self.batchnorm3 = nn.BatchNorm1d (hiddenLayerCounts[2])\n        \n        self.outDense   = None\n        if outputCount > 0:\n            self.outDense   = nn.Linear (hiddenLayerCounts[-1], outputCount)\n        self.outActivtn = None\n        if isOpAct:\n            if outputCount == 1 or outputCount == 2:\n                self.outActivtn = nn.Sigmoid ()\n            elif outputCount > 0:\n                self.outActivtn = nn.Softmax (dim=-1)\n        return\n\n    def forward (self, X):\n        \n        # X = self.dropout (self.batchnorm0 (X))\n        X = self.batchnorm0 (X)\n        X = self.dropout (self.nonlin (self.batchnorm1 (self.dense1 (X))))\n        X = self.dropout (self.nonlin (self.batchnorm2 (self.dense2 (X))))\n        X = self.dropout (self.nonlin (self.batchnorm3 (self.dense3 (X))))\n        if self.outDense:\n            X = self.outDense (X)\n        if self.outActivtn:\n            X = self.outActivtn (X)\n        return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Emb_NN_Model (nn.Module):\n    \n    def __init__(self, hidden_layers=HIDDEN_LAYERS, embed_dim=(N_FEAT_TAGS), drop_prob=0.4, csv_file='../input/jane-street-market-prediction/features.csv'):\n        \n        super (Emb_NN_Model, self).__init__()\n        global N_FEAT_TAGS\n        N_FEAT_TAGS = 29\n        \n        # store the features to tags mapping as a datframe tdf, feature_i mapping is in tdf[i, :]\n        dtype = {'tag_0' : 'int8'}\n        for i in range (1, 29):\n            k = 'tag_' + str (i)\n            dtype[k] = 'int8'\n        t_df = pd.read_csv (csv_file, usecols=range (1,N_FEAT_TAGS+1), dtype=dtype)\n        t_df['tag_29'] = np.array ([1] + ([0] * (t_df.shape[0]-1)) ).astype ('int8')\n        self.features_tag_matrix = torch.tensor (t_df.to_numpy ())\n        N_FEAT_TAGS += 1\n        \n        # print ('self.features_tag_matrix =', self.features_tag_matrix)\n        \n        # embeddings for the tags. Each feature is taken a an embedding which is an avg. of its' tag embeddings\n        self.embed_dim     = embed_dim\n        self.tag_embedding = nn.Embedding (N_FEAT_TAGS+1, embed_dim) # create a special tag if not known tag for any feature\n        self.tag_weights   = nn.Linear (N_FEAT_TAGS, 1)\n        \n        self.ffn           = FFN (inputCount=(N_FEATURES+embed_dim), outputCount=0, hiddenLayerCounts=[(hidden_layers[0]+embed_dim), (hidden_layers[1]+embed_dim), (hidden_layers[2]+embed_dim)], drop_prob=drop_prob)\n        self.outDense      = nn.Linear (hidden_layers[2]+embed_dim, 5)\n        return\n    \n    def features2emb (self):\n        \"\"\"\n        idx : int feature index 0 to N_FEATURES-1 (129)\n        \"\"\"\n        \n        all_tag_idxs = torch.LongTensor (np.arange (N_FEAT_TAGS)) #.to (DEVICE)              # (29,)\n        tag_bools    = self.features_tag_matrix                                # (130, 29)\n        # print ('tag_bools.shape =', tag_bools.size())\n        f_emb        = self.tag_embedding (all_tag_idxs).repeat (130, 1, 1)    #;print ('1. f_emb =', f_emb) # (29, 7) * (130, 1, 1) = (130, 29, 7)\n        # print ('f_emb.shape =', f_emb.size())\n        f_emb        = f_emb * tag_bools[:, :, None]                           #;print ('2. f_emb =', f_emb) # (130, 29, 7) * (130, 29, 1) = (130, 29, 7)\n        # print ('f_emb.shape =', f_emb.size())\n        \n        # Take avg. of all the present tag's embeddings to get the embedding for a feature\n        s = torch.sum (tag_bools, dim=1)                                       # (130,)\n        # print ('s =', s)              \n        f_emb = torch.sum (f_emb, dim=-2) / s[:, None]                         # (130, 7)\n        return f_emb\n    \n    def forward (self, cat_featrs, features):\n        \"\"\"\n        when you call `model (x ,y, z, ...)` then this method is invoked\n        \"\"\"\n        \n        cat_featrs = None\n        features   = features.view (-1, N_FEATURES)\n        f_emb      = self.features2emb ()                                #;print ('5. f_emb =', f_emb); print ('6. features =', features) # (130, 7)\n        # print ('features.shape =', features.shape, 'f_emb.shape =', f_emb.shape)\n        features_2 = torch.matmul (features[:, :130], f_emb)                      #;print ('7. features =', features) # (1, 130) * (130, 7) = (1, 7)\n        # print ('features.shape =', features.shape)\n        \n        # Concatenate the two features (features + their embeddings)\n        features   = torch.hstack ((features, features_2))        \n        \n        x          = self.ffn (features)                               #;print ('8. x.shape = ', x.shape, 'x =', x)   # (1, 7) -> (1, 7)\n        # x        = self.layer_normal (x + features)                  #;print ('9. x.shape = ', x.shape, 'x =', x)   # (1, 7) -> (1, 2)\n        out_logits = self.outDense (x)                                 #;print ('10. out_logits.shape = ', out_logits.shape, 'out_logits =', out_logits)        \n        # return sigmoid probs\n        # out_probs = F.sigmoid (out_logits)\n        return out_logits","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@delegates (torch.optim.AdamW.__init__)\ndef pytorch_AdamW (param_groups, **kwargs):\n    return OptimWrapper (torch.optim.AdamW ([{'params': ps, **kwargs} for ps in param_groups]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for vanilla NN use this\npath  = \"../input/jane-fastai-embedding-smoothnn5-300x3/Jane_EmbNN5_featInterac_300_300_300\"\nlearn = TabularLearner (DLs, model=Emb_NN_Model (), model_dir='/kaggle/working/',\n                        loss_func=SmoothBCEwLogits (smoothing=0.01), metrics=RocAucMulti (),\n                        opt_func=partial (pytorch_AdamW, lr=0.007, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01)\n                       )\nlearn = learn.load (path)\nlearn.save (learn.model_dir)\nlearn.summary ()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logits = learn.model (x_cat, x_cont)\nlogits","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_cat, x_cont, y = learn.dls.one_batch ()\ninit_loss = learn.loss_func (learn.model (x_cat, x_cont), y)\ninit_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lr_min, lr_steep = learn.lr_find (start_lr=1e-3, end_lr=1e-2, num_it=100)\n# lr_min, lr_steep","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modelfile = 'Jane_EmbNN5_featInterac_'+str (HIDDEN_LAYERS).replace (' ', '_').replace (',', '').replace ('[', '').replace (']', '')\ncallbacks = [\n    EarlyStoppingCallback (monitor='roc_auc_score', min_delta=0.0001, patience=12),\n    SaveModelCallback     (monitor='roc_auc_score', fname=modelfile),\n    ReduceLROnPlateau     (monitor='roc_auc_score', min_delta=0.0001, factor=2.0, min_lr=1e-7, patience=1),\n    GradientClip (1)\n]\n\nepochs  = 80\n# lr      = lr_min\n# learn.fit_one_cycle (epochs, lr, wd=1e-2, cbs=callbacks)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from fastai.imports import *\nfrom fastai.torch_core import *\nfrom fastai.learner import *\n    \n@patch\n@delegates(subplots)\ndef plot_metrics(self: Recorder, nrows=None, ncols=None, figsize=None, **kwargs):\n    metrics = np.stack(self.values)\n    names = self.metric_names[1:-1]\n    n = len(names) - 1\n    if nrows is None and ncols is None:\n        nrows = int(math.sqrt(n))\n        ncols = int(np.ceil(n / nrows))\n    elif nrows is None: nrows = int(np.ceil(n / ncols))\n    elif ncols is None: ncols = int(np.ceil(n / nrows))\n    figsize = figsize or (ncols * 6, nrows * 4)\n    fig, axs = subplots(nrows, ncols, figsize=figsize, **kwargs)\n    axs = [ax if i < n else ax.set_axis_off() for i, ax in enumerate(axs.flatten())][:n]\n    for i, (name, ax) in enumerate(zip(names, [axs[0]] + axs)):\n        ax.plot(metrics[:, i], color='#1f77b4' if i == 0 else '#ff7f0e', label='valid' if i > 0 else 'train')\n        ax.set_title(name if i > 1 else 'losses')\n        ax.legend(loc='best')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# learn.recorder.plot_loss (skip_start=0, with_valid=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# learn.recorder.plot_metrics ()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# learn.recorder.plot_lr ()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# _, logits, _ = learn.predict (DF.iloc[0])\n# logits","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"MODEL = learn.model.eval ()\nMODEL","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.nn.utils.rnn as rnn_utils\nfrom   torch.autograd import Variable\nfrom   torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n\nDEVICE = torch.device (\"cuda:0\") if torch.cuda.is_available () else torch.device (\"cpu\")","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.026076,"end_time":"2021-01-22T17:58:55.522497","exception":false,"start_time":"2021-01-22T17:58:55.496421","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# For direct submission, without using Fastai since its too slow\nUse Fastai for training models only, not for prediction."},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict_torch (test_df):\n    \n    test_df.drop (columns=['weight', 'date'], inplace=True)\n    test_df.reset_index (drop=True, inplace=True)\n    test_df = PIPE.transform (test_df)        \n    test_df = torch.tensor (test_df).float ().view (-1, N_FEATURES)\n    predictions = []\n    for i in range (test_df.shape[0]):\n        \n        pred_p = torch.sigmoid (MODEL (None, test_df)).detach ().cpu ().numpy ().reshape ((-1, 5))\n        predictions.append (pred_p)\n\n    predictions = np.vstack (predictions)                     #;print ('predictions.shape =', predictions.shape)\n    predictions = np.median (predictions, axis=1)\n    return (predictions >= 0.5).astype (int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# For prediction using Fastai\nDon't use thism its too slow and times out. Use Pytorch for prediction.\nUse Fastai for training the Pytorch models only."},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn.functional as F\n\ndef predict (df, threshold=0.50):\n    \n    dl     = learn.dls.test_dl (df)\n    logits = learn.get_preds (dl=dl)[0]\n    probs  = F.sigmoid (logits).detach ().numpy ()\n    pred   = (np.median (probs, axis=1) >= threshold).astype (int)\n    return pred","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"test_df = DF.copy()\nresp_cols  = ['resp_1', 'resp_2', 'resp_3','resp_4', 'resp']\ntest_df.drop (columns=resp_cols, inplace=True)\n\ntest_df  = preprocess_data (df=test_df, isTrainData=False)\npredict (test_df)"},{"metadata":{"papermill":{"duration":0.02371,"end_time":"2021-01-22T17:58:55.388369","exception":false,"start_time":"2021-01-22T17:58:55.364659","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# Test"},{"metadata":{"execution":{"iopub.execute_input":"2021-01-22T17:58:55.446325Z","iopub.status.busy":"2021-01-22T17:58:55.445226Z","iopub.status.idle":"2021-01-22T17:58:55.471692Z","shell.execute_reply":"2021-01-22T17:58:55.472274Z"},"papermill":{"duration":0.058761,"end_time":"2021-01-22T17:58:55.472431","exception":false,"start_time":"2021-01-22T17:58:55.41367","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"import janestreet\nenv      = janestreet.make_env ()  # initialize the environment\nenv_iter = env.iter_test ()        # an iterator which loops over the test set","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-01-22T17:58:55.57927Z","iopub.status.busy":"2021-01-22T17:58:55.577702Z","iopub.status.idle":"2021-01-22T18:03:14.623339Z","shell.execute_reply":"2021-01-22T18:03:14.62262Z"},"papermill":{"duration":259.074066,"end_time":"2021-01-22T18:03:14.623458","exception":false,"start_time":"2021-01-22T17:58:55.549392","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"for test_df, pred_df in env_iter:\n    if test_df[\"weight\"].item () > 0:\n        \n        pred_df.action = predict_torch (test_df)\n    else:\n        pred_df.action = 0\n        \n    # print (pred_df)\n    # print (\"--------------\")\n    env.predict (pred_df)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-01-22T18:03:14.677426Z","iopub.status.busy":"2021-01-22T18:03:14.676598Z","iopub.status.idle":"2021-01-22T18:03:14.679698Z","shell.execute_reply":"2021-01-22T18:03:14.680217Z"},"papermill":{"duration":0.032627,"end_time":"2021-01-22T18:03:14.680369","exception":false,"start_time":"2021-01-22T18:03:14.647742","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"print ('Done !')","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.026572,"end_time":"2021-01-22T18:03:14.733653","exception":false,"start_time":"2021-01-22T18:03:14.707081","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}