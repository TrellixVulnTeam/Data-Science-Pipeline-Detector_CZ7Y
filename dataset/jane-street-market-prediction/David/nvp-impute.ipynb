{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import regularizers\nfrom sklearn.datasets import make_moons\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow_probability as tfp\n\nfrom tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Concatenate, Lambda, GaussianNoise, Activation, GaussianDropout\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.losses import BinaryCrossentropy, Huber\nfrom tensorflow.keras.optimizers import Adam, RMSprop\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers.experimental.preprocessing import Normalization\n#import tensorflow as tf\n#import numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom random import choices\n\nfrom tensorflow.keras.constraints import max_norm, min_max_norm\nfrom keras.callbacks import ReduceLROnPlateau\nimport os\n\n#from distutils.dir_util import copy_tree\nimport shutil\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/jane-street-market-prediction/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED = 1111\n\ntf.random.set_seed(SEED)\nnp.random.seed(SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_part = train\n#### CHANGE This back to impute strategy...\n#train_part = train_part.fillna(0) # y4es you need this\ntrain_part_mean = train_part.mean()\ntrain_part_std = train_part.std()\ntrain_part_var = train_part.var()\ntrain_part.fillna(train_part_mean,inplace=True)\ntrain_part['action'] = ((train_part['resp'].values) > 0).astype(int)\n\nfeatures = [c for c in train_part.columns if \"feature\" in c]\nfeatures = features + ['resp_1'] + ['resp_2'] + ['resp_3'] + ['resp_4'] + ['resp'] + ['weight']  + ['action'] + ['date']\n\ntrain_part = train_part[features]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_part.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_part_mean\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_part_std","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_part_var","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_part = tf.convert_to_tensor(\n    train_part, dtype=\"float32\", dtype_hint=None, name=None\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"normalized_data_test=train_part # notactually normalized, but lets see if this makes life easier.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"#norm = layers.experimental.preprocessing.Normalization(mean=train_part_mean, var=train_part_var)\n#layers.experimental.preprocessing.Normalization(axis=-1, dtype=None, mean=None, variance=None)\n\n#tf.keras.layers.experimental.preprocessing.Normalization(\n#    mean=3., variance=2.\n#)\n\n#adapt_data = np.array([[1.], [2.], [3.], [4.], [5.]], dtype=np.float32)\n#input_data = np.array([[1.], [2.], [3.]], np.float32)\n#layer = Normalization(mean=3., variance=2.)\n#layer.adapt(adapt_data)\n#layer(input_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"\n\n\n\n#norm.adapt(train_part)\n#normalized_data_test = norm(train_part)\n#normalized_data_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### CHANGE THIS BELOW for it to work based on dims of input....\n#dim_shape=4\ndim_shape = 138 # this works \n#dim_shape = 136 # this works \n#dim_shape = 2 # this works \n\n# Creating a custom layer with keras API.\n#output_dim = 256\noutput_dim = 138 #not sure which one to do\nreg = 0.01\n\n\ndef Coupling(input_shape):\n    input = keras.layers.Input(shape=input_shape)\n\n    t_layer_1 = keras.layers.Dense(\n        output_dim, activation=\"relu\", kernel_regularizer=regularizers.l2(reg)\n    )(input)\n    t_layer_2 = keras.layers.Dense(\n        output_dim, activation=\"relu\", kernel_regularizer=regularizers.l2(reg)\n    )(t_layer_1)\n    t_layer_3 = keras.layers.Dense(\n        output_dim, activation=\"relu\", kernel_regularizer=regularizers.l2(reg)\n    )(t_layer_2)\n    t_layer_4 = keras.layers.Dense(\n        output_dim, activation=\"relu\", kernel_regularizer=regularizers.l2(reg)\n    )(t_layer_3)\n    t_layer_5 = keras.layers.Dense(\n        input_shape, activation=\"linear\", kernel_regularizer=regularizers.l2(reg)\n    )(t_layer_4)\n\n    s_layer_1 = keras.layers.Dense(\n        output_dim, activation=\"relu\", kernel_regularizer=regularizers.l2(reg)\n    )(input)\n    s_layer_2 = keras.layers.Dense(\n        output_dim, activation=\"relu\", kernel_regularizer=regularizers.l2(reg)\n    )(s_layer_1)\n    s_layer_3 = keras.layers.Dense(\n        output_dim, activation=\"relu\", kernel_regularizer=regularizers.l2(reg)\n    )(s_layer_2)\n    s_layer_4 = keras.layers.Dense(\n        output_dim, activation=\"relu\", kernel_regularizer=regularizers.l2(reg)\n    )(s_layer_3)\n    s_layer_5 = keras.layers.Dense(\n        input_shape, activation=\"tanh\", kernel_regularizer=regularizers.l2(reg)\n    )(s_layer_4)\n\n    return keras.Model(inputs=input, outputs=[s_layer_5, t_layer_5])\n\nclass RealNVP(keras.Model):\n    def __init__(self, num_coupling_layers):\n        super(RealNVP, self).__init__()\n\n        self.num_coupling_layers = num_coupling_layers\n\n        # Distribution of the latent space.\n#        self.distribution = tfp.distributions.MultivariateNormalDiag(\n#            loc=[0.0, 0.0], scale_diag=[1.0, 1.0]\n#        )\n        self.distribution = tfp.distributions.MultivariateNormalDiag(\n            loc=np.zeros(dim_shape).tolist(), scale_diag=np.ones(dim_shape).tolist()\n#            loc=np.zeros(dim_shape).tolist(), scale_diag=np.ones(3).tolist()\n#            loc=np.zeros(3).tolist(), scale_diag=np.ones(3).tolist()\n        )\n\n#        self.masks = np.array(\n#            [[0, 1], [1, 0]] * (num_coupling_layers // 2), dtype=\"float32\"\n#        )\n        # need mto make this dynamic e.g. identiy(3) will be euiqvalen to size ofrectangle\n    # also need to check this against original moons data with size of 2 to see if it works..\n        self.masks = np.array(\n            np.identity(dim_shape).tolist() * (num_coupling_layers // 2), dtype=\"float32\"\n#            np.identity(3).tolist() * (num_coupling_layers // 2), dtype=\"float32\"\n        )\n        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n        self.layers_list = [Coupling(dim_shape) for i in range(num_coupling_layers)]\n#        self.layers_list = [Coupling(3) for i in range(num_coupling_layers)]\n\n    @property\n    def metrics(self):\n        \"\"\"List of the model's metrics.\n        We make sure the loss tracker is listed as part of `model.metrics`\n        so that `fit()` and `evaluate()` are able to `reset()` the loss tracker\n        at the start of each epoch and at the start of an `evaluate()` call.\n        \"\"\"\n        return [self.loss_tracker]\n\n    def call(self, x, training=True):\n        log_det_inv = 0\n        direction = 1\n        if training:\n            direction = -1\n        for i in range(self.num_coupling_layers)[::direction]:\n#            import pdb;pdb.set_trace()\n            x_masked = x * self.masks[i]\n#            x_masked = x * self.masks[i]\n            reversed_mask = 1 - self.masks[i]\n#            import pdb;pdb.set_trace()\n            s, t = self.layers_list[i](x_masked)\n            s *= reversed_mask\n            t *= reversed_mask\n            gate = (direction - 1) / 2\n            x = (\n                reversed_mask\n                * (x * tf.exp(direction * s) + direction * t * tf.exp(gate * s))\n                + x_masked\n            )\n            log_det_inv += gate * tf.reduce_sum(s, [1])\n\n        return x, log_det_inv\n\n    # Log likelihood of the normal distribution plus the log determinant of the jacobian.\n\n    def log_loss(self, x):\n        y, logdet = self(x)\n#        import pdb;pdb.set_trace()\n        log_likelihood = self.distribution.log_prob(y) + logdet\n        return -tf.reduce_mean(log_likelihood)\n\n    def train_step(self, data):\n        with tf.GradientTape() as tape:\n\n            loss = self.log_loss(data)\n\n        g = tape.gradient(loss, self.trainable_variables)\n        self.optimizer.apply_gradients(zip(g, self.trainable_variables))\n        self.loss_tracker.update_state(loss)\n\n        return {\"loss\": self.loss_tracker.result()}\n\n    def test_step(self, data):\n        loss = self.log_loss(data)\n        self.loss_tracker.update_state(loss)\n\n        return {\"loss\": self.loss_tracker.result()}\n\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#!cp -r ../input/keras-normalizing-flow-backup/training_PRODUCTION/ ./training_PRODUCTION/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#!mkdir training_PRODUCTION/\n#!cp /kaggle/input/keras-normalizing-flow-backup/training_PRODUCTION/cp_PRODUCTION.ckpt.index training_PRODUCTION/cp_PRODUCTION.ckpt.index\n#!cp /kaggle/input/keras-normalizing-flow-backup/training_PRODUCTION/cp_PRODUCTION.ckpt.data-00000-of-00001 training_PRODUCTION/cp_PRODUCTION.ckpt.data-00000-of-00001\n#!cp /kaggle/input/keras-normalizing-flow-backup/training_PRODUCTION/cp_PRODUCTION.ckpt.index training_PRODUCTION/cp_PRODUCTION.ckpt.index\n#!dir training_PRODUCTION","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    !mkdir training_PRODUCTION/\nexcept:\n    print('couldnt make directory')\n\ntry:\n# issues with recursion previously, so going to just copy what is necessary...\n#    !cp -r /kaggle/input/keras-normalizing-flow-backup/training_PRODUCTION/ training_PRODUCTION/\n    !cp /kaggle/input/nvp-impute/training_PRODUCTION/cp_PRODUCTION.ckpt.index training_PRODUCTION/cp_PRODUCTION.ckpt.index\n    !cp /kaggle/input/nvp-impute/training_PRODUCTION/cp_PRODUCTION.ckpt.data-00000-of-00001 training_PRODUCTION/cp_PRODUCTION.ckpt.data-00000-of-00001\n    !cp /kaggle/input/nvp-impute/training_PRODUCTION/cp_PRODUCTION.ckpt.index training_PRODUCTION/cp_PRODUCTION.ckpt.index\n\nexcept:\n    print('error in copying over.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nif(len(normalized_data_test)==2390491):\n    checkpoint_path = \"training_PRODUCTION/cp_PRODUCTION.ckpt\"\n    checkpoint_path_read = \"/kaggle/input/nvp-impute/training_PRODUCTION/cp_PRODUCTION.ckpt\"\n    checkpoint_path_read_fulldir = \"/kaggle/input/nvp-impute/training_PRODUCTION/\"\n    checkpoint_dir = os.path.dirname(checkpoint_path)\n\n    # Create a callback that saves the model's weights\n    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n                                                     save_weights_only=True,\n                                                     verbose=1)\nelse:\n    checkpoint_path = \"training_QA/cp_QA.ckpt\"\n    checkpoint_dir = os.path.dirname(checkpoint_path)\n\n    # Create a callback that saves the model's weights\n    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n                                                     save_weights_only=True,\n                                                     verbose=1)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# just keep running this, and every say so many times create a backup, like nightly backup\nmodel_chk = RealNVP(num_coupling_layers=6)\nmodel_chk.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001))\n# 3 minutes to evaluate\nmodel_chk.evaluate(normalized_data_test)\ntry:\n    model_chk.load_weights(checkpoint_path_read)\n    # 3 minutes to evaluate\n    model_chk.evaluate(normalized_data_test)\n    model_chk.fit(\n    normalized_data_test, batch_size=256, epochs=30, verbose=2, validation_split=0.2, callbacks = [cp_callback]\n    )\nexcept:\n    print('failed going to refit the model, careful as you can lose all progress')\n#    print('going to just fail so we dont mess something up for now...things should have been copied over already')\n#except:\n    model_chk.fit(\n    normalized_data_test, batch_size=256, epochs=1, verbose=2, validation_split=0.2, callbacks = [cp_callback]\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### save out the normalized data as well so you don't have to keep lodaing it in, maybe it will load faster...\n# Epoch 00015: saving model to training_PRODUCTION/cp_PRODUCTION.ckpt\n#7471/7471 - 297s - loss: 128.7271 - val_loss: 120.2163\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#!ls -l \"/kaggle/input/keras-normalizing-flow-backup/training_PRODUCTION/\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#!ls -l \"training_PRODUCTION/\"#\n#!cat \"training_PRODUCTION/checkpoint\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#normalized_data_part = normalized_data_test[0:5]\n\n\n#print(normalized_data_part)\n# From data to latent space.\n#z, _ = model(normalized_data_part)\n#print(z)\n\n#From latent space to data.\n#samples = model.distribution.sample(3000)\n#x, _ = model.predict(samples)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# need to write code to autoarchive the model weights so have a backup ever say 50 or so runs...\n# e.g. backup_training_production_1/*\n#backup_training_production_2/\n#!mkdir training_prod_backup_20200223\n\n#!cp -r training_PRODUCTION \n#!dir training_PRODUCTION\n#!dir training_prod_backup_20200223","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#!dir training_prod_backup_20200223/training_PRODUCTION","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}