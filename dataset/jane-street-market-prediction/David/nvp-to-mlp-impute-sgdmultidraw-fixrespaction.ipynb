{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import regularizers\nfrom sklearn.datasets import make_moons\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow_probability as tfp\n\nfrom tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Concatenate, Lambda, GaussianNoise, Activation, GaussianDropout\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.losses import BinaryCrossentropy, Huber\nfrom tensorflow.keras.optimizers import Adam, RMSprop, SGD\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers.experimental.preprocessing import Normalization\n#import tensorflow as tf\n#import numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom random import choices\n\nfrom tensorflow.keras.constraints import max_norm, min_max_norm\nfrom keras.callbacks import ReduceLROnPlateau\n\nSEED = 1111\n\ntf.random.set_seed(SEED)\nnp.random.seed(SEED)\n\n#train = pd.read_csv('../input/jane-street-market-prediction/train.csv', nrows=1000)\ntrain = pd.read_csv('../input/jane-street-market-prediction/train.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_part = train\ntrain_part_mean = train_part.mean()\ntrain_part_std = train_part.std()\ntrain_part_var = train_part.var()\ntrain_part.fillna(train_part_mean,inplace=True)\n\nfeatures = ['weight', 'feature_0', 'feature_1', 'feature_2', 'feature_3','feature_4', \n'feature_5', 'feature_6', 'feature_7', 'feature_8', 'feature_9', 'feature_10', \n'feature_11', 'feature_12', 'feature_13',\n'feature_14', 'feature_15', 'feature_16', 'feature_17', 'feature_18',\n'feature_19', 'feature_20', 'feature_21', 'feature_22', 'feature_23',\n'feature_24', 'feature_25', 'feature_26', 'feature_27', 'feature_28',\n'feature_29', 'feature_30', 'feature_31', 'feature_32', 'feature_33',\n'feature_34', 'feature_35', 'feature_36', 'feature_37', 'feature_38',\n'feature_39', 'feature_40', 'feature_41', 'feature_42', 'feature_43',\n'feature_44', 'feature_45', 'feature_46', 'feature_47', 'feature_48',\n'feature_49', 'feature_50', 'feature_51', 'feature_52', 'feature_53',\n'feature_54', 'feature_55', 'feature_56', 'feature_57', 'feature_58',\n'feature_59', 'feature_60', 'feature_61', 'feature_62', 'feature_63',\n'feature_64', 'feature_65', 'feature_66', 'feature_67', 'feature_68',\n'feature_69', 'feature_70', 'feature_71', 'feature_72', 'feature_73',\n'feature_74', 'feature_75', 'feature_76', 'feature_77', 'feature_78',\n'feature_79', 'feature_80', 'feature_81', 'feature_82', 'feature_83',\n'feature_84', 'feature_85', 'feature_86', 'feature_87', 'feature_88',\n'feature_89', 'feature_90', 'feature_91', 'feature_92', 'feature_93',\n'feature_94', 'feature_95', 'feature_96', 'feature_97', 'feature_98',\n'feature_99', 'feature_100', 'feature_101', 'feature_102',\n'feature_103', 'feature_104', 'feature_105', 'feature_106',\n'feature_107', 'feature_108', 'feature_109', 'feature_110',\n'feature_111', 'feature_112', 'feature_113', 'feature_114',\n'feature_115', 'feature_116', 'feature_117', 'feature_118',\n'feature_119', 'feature_120', 'feature_121', 'feature_122',\n'feature_123', 'feature_124', 'feature_125', 'feature_126',\n'feature_127', 'feature_128', 'feature_129', 'date','ts_id',\n'resp_1','resp_2','resp_3','resp_4','resp']\n\ntrain_part = train_part[features]\ndel(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_part = tf.convert_to_tensor(\n    train_part, dtype=\"float32\", dtype_hint=None, name=None\n)\nnormalized_data_test=train_part # notactually normalized, but lets see if this makes life easier.\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_part","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### CHANGE THIS BELOW for it to work based on dims of input....\ndim_shape = 138 # this works \n\n# Creating a custom layer with keras API.\noutput_dim = 138\n#output_dim = 138 #not sure which one to do\nreg = 0.01\n\ndef Coupling(input_shape):\n    input = keras.layers.Input(shape=input_shape)\n\n    t_layer_1 = keras.layers.Dense(\n        output_dim, activation=\"relu\", kernel_regularizer=regularizers.l2(reg)\n    )(input)\n    t_layer_2 = keras.layers.Dense(\n        output_dim, activation=\"relu\", kernel_regularizer=regularizers.l2(reg)\n    )(t_layer_1)\n    t_layer_3 = keras.layers.Dense(\n        output_dim, activation=\"relu\", kernel_regularizer=regularizers.l2(reg)\n    )(t_layer_2)\n    t_layer_4 = keras.layers.Dense(\n        output_dim, activation=\"relu\", kernel_regularizer=regularizers.l2(reg)\n    )(t_layer_3)\n    t_layer_5 = keras.layers.Dense(\n        input_shape, activation=\"linear\", kernel_regularizer=regularizers.l2(reg)\n    )(t_layer_4)\n\n    s_layer_1 = keras.layers.Dense(\n        output_dim, activation=\"relu\", kernel_regularizer=regularizers.l2(reg)\n    )(input)\n    s_layer_2 = keras.layers.Dense(\n        output_dim, activation=\"relu\", kernel_regularizer=regularizers.l2(reg)\n    )(s_layer_1)\n    s_layer_3 = keras.layers.Dense(\n        output_dim, activation=\"relu\", kernel_regularizer=regularizers.l2(reg)\n    )(s_layer_2)\n    s_layer_4 = keras.layers.Dense(\n        output_dim, activation=\"relu\", kernel_regularizer=regularizers.l2(reg)\n    )(s_layer_3)\n    s_layer_5 = keras.layers.Dense(\n        input_shape, activation=\"tanh\", kernel_regularizer=regularizers.l2(reg)\n    )(s_layer_4)\n\n    return keras.Model(inputs=input, outputs=[s_layer_5, t_layer_5])\n\nclass RealNVP(keras.Model):\n    def __init__(self, num_coupling_layers):\n        super(RealNVP, self).__init__()\n\n        self.num_coupling_layers = num_coupling_layers\n\n        # Distribution of the latent space.\n#        self.distribution = tfp.distributions.MultivariateNormalDiag(\n#            loc=[0.0, 0.0], scale_diag=[1.0, 1.0]\n#        )\n        self.distribution = tfp.distributions.MultivariateNormalDiag(\n            loc=np.zeros(dim_shape).tolist(), scale_diag=np.ones(dim_shape).tolist()\n#            loc=np.zeros(dim_shape).tolist(), scale_diag=np.ones(3).tolist()\n#            loc=np.zeros(3).tolist(), scale_diag=np.ones(3).tolist()\n        )\n\n#        self.masks = np.array(\n#            [[0, 1], [1, 0]] * (num_coupling_layers // 2), dtype=\"float32\"\n#        )\n        # need mto make this dynamic e.g. identiy(3) will be euiqvalen to size ofrectangle\n    # also need to check this against original moons data with size of 2 to see if it works..\n        self.masks = np.array(\n            np.identity(dim_shape).tolist() * (num_coupling_layers // 2), dtype=\"float32\"\n#            np.identity(3).tolist() * (num_coupling_layers // 2), dtype=\"float32\"\n        )\n        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n        self.layers_list = [Coupling(dim_shape) for i in range(num_coupling_layers)]\n#        self.layers_list = [Coupling(3) for i in range(num_coupling_layers)]\n\n    @property\n    def metrics(self):\n        \"\"\"List of the model's metrics.\n        We make sure the loss tracker is listed as part of `model.metrics`\n        so that `fit()` and `evaluate()` are able to `reset()` the loss tracker\n        at the start of each epoch and at the start of an `evaluate()` call.\n        \"\"\"\n        return [self.loss_tracker]\n\n    def call(self, x, training=True):\n        log_det_inv = 0\n        direction = 1\n        if training:\n            direction = -1\n        for i in range(self.num_coupling_layers)[::direction]:\n#            import pdb;pdb.set_trace()\n            x_masked = x * self.masks[i]\n            x_masked = x * self.masks[i]\n            reversed_mask = 1 - self.masks[i]\n#            import pdb;pdb.set_trace()\n            s, t = self.layers_list[i](x_masked)\n            s *= reversed_mask\n            t *= reversed_mask\n            gate = (direction - 1) / 2\n            x = (\n                reversed_mask\n                * (x * tf.exp(direction * s) + direction * t * tf.exp(gate * s))\n                + x_masked\n            )\n            log_det_inv += gate * tf.reduce_sum(s, [1])\n\n        return x, log_det_inv\n\n    # Log likelihood of the normal distribution plus the log determinant of the jacobian.\n\n    def log_loss(self, x):\n        y, logdet = self(x)\n#        import pdb;pdb.set_trace()\n        log_likelihood = self.distribution.log_prob(y) + logdet\n        return -tf.reduce_mean(log_likelihood)\n\n    def train_step(self, data):\n        with tf.GradientTape() as tape:\n\n            loss = self.log_loss(data)\n\n        g = tape.gradient(loss, self.trainable_variables)\n        self.optimizer.apply_gradients(zip(g, self.trainable_variables))\n        self.loss_tracker.update_state(loss)\n\n        return {\"loss\": self.loss_tracker.result()}\n\n    def test_step(self, data):\n        loss = self.log_loss(data)\n        self.loss_tracker.update_state(loss)\n\n        return {\"loss\": self.loss_tracker.result()}\n\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\ncheckpoint_path = \"training_PRODUCTION/cp_PRODUCTION.ckpt\"\ncheckpoint_path_read = \"training_PRODUCTION/cp_PRODUCTION.ckpt\"\ncheckpoint_dir = os.path.dirname(checkpoint_path)\n\n# Create a callback that saves the model's weights\ncp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n                                                 save_weights_only=True,\n                                                 verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    !mkdir training_PRODUCTION/\nexcept:\n    print('couldnt make directory')\n\ntry:\n    !cp /kaggle/input/nvp-to-mlp-impute-sgdmultidraw-fixrespaction/training_PRODUCTION/cp_PRODUCTION.ckpt.index training_PRODUCTION/cp_PRODUCTION.ckpt.index\n    !cp /kaggle/input/nvp-to-mlp-impute-sgdmultidraw-fixrespaction/training_PRODUCTION/cp_PRODUCTION.ckpt.data-00000-of-00001 training_PRODUCTION/cp_PRODUCTION.ckpt.data-00000-of-00001\n    !cp /kaggle/input/nvp-to-mlp-impute-sgdmultidraw-fixrespaction/training_PRODUCTION/cp_PRODUCTION.ckpt.index training_PRODUCTION/cp_PRODUCTION.ckpt.index\n\nexcept:\n    print('error in copying over.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmodel_chk_latent = RealNVP(num_coupling_layers=6)\nmodel_chk_latent.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001))\n# 3 minutes to evaluate\nmodel_chk_latent.evaluate(normalized_data_test)\ntry:\n    # you had to add the data source first from the notebook, not you can read in this cross-notebook data..\n    model_chk_latent.load_weights(checkpoint_path_read)\n\n    # 3 minutes to evaluate\n    model_chk_latent.evaluate(normalized_data_test)\n#    model_chk_latent = model_chk\n\n    model_chk_latent.fit(\n    normalized_data_test, batch_size=256, epochs=15, verbose=2, validation_split=0.5, callbacks = [cp_callback]\n    )\nexcept:\n    print('fitting for the first time...')\n\n    model_chk_latent.fit(\n    normalized_data_test, batch_size=256, epochs=1, verbose=2, validation_split=0.5, callbacks = [cp_callback]\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#normalized_data_part = normalized_data_test[1500000:2390491]\n#normalized_data_part = .sample(1)\n\n## if you want to plot uncomment the below. \n## but if just generating data. You don't need to do the first piece.\n#print(normalized_data_part)\n# From data to latent space.\n#start=1200000\n#end=2390491\n#z, _ = model_chk(normalized_data_test[start:end])\n#print(z)\n\n#From latent space to data.\n#samples_1 = model_chk.distribution.sample(500000)\n#samples_2 = model_chk.distribution.sample(500000)\n#samples_3 = model_chk.distribution.sample(500000)\n#samples_4 = model_chk.distribution.sample(500000)\n#samples_5 = model_chk.distribution.sample(500000)\n#samples = model_chk.distribution.sample(3000)\n#x, _ = model_chk.predict(samples)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Concatenate, Lambda, GaussianNoise, Activation, GaussianDropout\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.losses import BinaryCrossentropy, Huber\nfrom tensorflow.keras.optimizers import Adam, RMSprop, SGD\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers.experimental.preprocessing import Normalization\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom random import choices\n\nfrom tensorflow.keras.constraints import max_norm, min_max_norm\nfrom keras.callbacks import ReduceLROnPlateau","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.utils.generic_utils import get_custom_objects\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Activation, LeakyReLU\n\n# Add the GELU function to Keras\n#https://mlfromscratch.com/activation-functions-explained/#/\n    \ndef gelu(x):\n    return 0.5 * x * (1 + tf.tanh(x * 0.7978845608 * (1 + 0.044715 * x * x)))\n    #https://github.com/hendrycks/GELUs\n    #exact version is better not sure if below if exact and above is approximation\n    \n# not sure if below is considered exact\n#    return 0.5 * x * (1 + tf.tanh(tf.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3))))\nget_custom_objects().update({'gelu': Activation(gelu)})\n\n# Add leaky-relu so we can use it as a string\nget_custom_objects().update({'leaky-relu': Activation(LeakyReLU(alpha=0.2))})\n\nact_func = ['sigmoid', 'relu', 'elu', 'leaky-relu', 'selu', 'gelu']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_mlp(num_columns, num_labels, hidden_units, dropout_rates, learning_rate):\n    inp = tf.keras.layers.Input(shape=(num_columns,))\n    x = tf.keras.layers.BatchNormalization()(inp)\n    x = tf.keras.layers.Dropout(dropout_rates[0])(x)\n    x = tf.keras.layers.GaussianDropout(dropout_rates[0])(x)\n    for i in range(len(hidden_units)):\n        # tf.keras.initializers.he_normal\n        # tf.keras.initializers.he_uniform (both of these are said to be good for a relu family of activations)\n#        x = tf.keras.layers.Dense(hidden_units[i],kernel_initializer=tf.keras.initializers.he_normal(seed=1111))(x)\n#        x = tf.keras.layers.Dense(hidden_units[i],kernel_initializer=tf.keras.initializers.he_uniform(seed=1111))(x)\n#        x = tf.keras.layers.Dense(hidden_units[i],kernel_initializer=tf.keras.initializers.Orthogonal(seed=1111))(x)\n#        x = tf.keras.layers.Dense(hidden_units[i],kernel_initializer=tf.keras.initializers.VarianceScaling(seed=1111))(x)\n#        x = tf.keras.layers.Dense(hidden_units[i],kernel_initializer=tf.keras.initializers.TruncatedNormal(seed=1111))(x)\n#        x = tf.keras.layers.Dense(hidden_units[i], kernel_constraint=min_max_norm(min_value=-0.5, max_value=0.5, rate=1.0, axis=0))(x)\n#        x = tf.keras.layers.Dense(hidden_units[i],\n#                                  kernel_initializer=tf.keras.initializers.TruncatedNormal(seed=1111),\n#                                  kernel_constraint=min_max_norm(min_value=-0.5, max_value=0.5, rate=1.0, axis=0))(x)\n        x = tf.keras.layers.Dense(hidden_units[i])(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n#        x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n        x = tf.keras.layers.Activation('gelu')(x)\n#        x = tf.keras.layers.Activation('relu')(x)\n# introduce randomness e.g. flip a coin and choose different dropouts\n#        x = tf.keras.layers.Dropout(dropout_rates[i + 1])(x)\n        x = tf.keras.layers.GaussianDropout(dropout_rates[i + 1])(x)\n\n    x = tf.keras.layers.Dense(num_labels)(x)\n    # i would like to try something else then just sigmoid \n    # try output of linear layer, then input into softmax instead....\n    # or then input into sigmoid as from logits\n    out = tf.keras.layers.Activation(\"sigmoid\")(x)\n\n    model = tf.keras.models.Model(inputs=inp, outputs=out)\n    model.compile(\n#        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n#        optimizer=tf.keras.optimizers.RMSprop(learning_rate=learning_rate),\n        optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate),\n# introduce randomness e.g. flip a coin and choose difference losses\n#        loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=label_smoothing),\n        loss=tf.keras.losses.Huber(),\n#        metrics=tf.keras.metrics.AUC(name=\"AUC\")\n        metrics=[tf.keras.metrics.AUC(name=\"AUC\"),\n                 tf.keras.metrics.TrueNegatives(name=\"TN\"),\n                 tf.keras.metrics.TruePositives(name=\"TP\")]\n#                 tf.keras.metrics.SpecificityAtSensitivity(name=\"SpeciSensi\")]\n        # many other metrics you could try...\n    )\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    !mkdir training_MLP/\nexcept:\n    print('couldnt make directory')\n\ntry:\n# issues with recursion previously, so going to just copy what is necessary...\n#    !cp -r /kaggle/input/keras-normalizing-flow-backup/training_PRODUCTION/ training_PRODUCTION/\n    !cp /kaggle/input/nvp-to-mlp-impute-sgdmultidraw-fixrespaction/training_MLP/cp_PRODUCTION.ckpt.index training_MLP/cp_PRODUCTION.ckpt.index\n    !cp /kaggle/input/nvp-to-mlp-impute-sgdmultidraw-fixrespaction/training_MLP/cp_PRODUCTION.ckpt.data-00000-of-00001 training_MLP/cp_PRODUCTION.ckpt.data-00000-of-00001\n    !cp /kaggle/input/nvp-to-mlp-impute-sgdmultidraw-fixrespaction/training_MLP/cp_PRODUCTION.ckpt.index training_MLP/cp_PRODUCTION.ckpt.index\n\nexcept:\n    print('error in copying over.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint_path = \"training_MLP/cp_PRODUCTION.ckpt\"\ncheckpoint_path_read = \"training_MLP/cp_PRODUCTION.ckpt\"\n#checkpoint_path_read = \"/kaggle/input/nvp-to-mlp-impute-sgdmultidraw/training_MLP/cp_PRODUCTION.ckpt\"\n#checkpoint_path_read_fulldir = \"/kaggle/input/nvp-to-mlp-impute-sgdmultidraw/training_MLP/\"\ncheckpoint_dir = os.path.dirname(checkpoint_path)\n\n# Create a callback that saves the model's weights\ncp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n                                                 save_weights_only=True,\n                                                 verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED = 1111\n\ntf.random.set_seed(SEED)\nnp.random.seed(SEED)\n\niter_over_sgd = 2\nfor i in range(iter_over_sgd):\n    print(i)\n    samples = model_chk_latent.distribution.sample(500000)\n    samples_np = samples.numpy() # shouldn't do it this way, but lets first start by making sure no issues in data syncing between two models\n    print(samples_np.mean())\n    del(samples)\n    train = pd.DataFrame(samples_np,columns=features)\n    \n    x_features=train.columns[0:133]\n    x_features_min=train.columns[0:132]\n    X_train=train.loc[:,x_features]\n\n    y_features = train.columns[133:]\n    y_train = np.stack([(train[c]) for c in y_features]).T\n\n    batch_size = 5000\n    hidden_units = [260, 260, 260, 260, 260, 260]\n    #dropout_rates = [0.2, 0.2, 0.2, 0.2, 0.5, 0.2, 0.2]\n    dropout_rates = [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2]\n    #label_smoothing = 1e-2\n    learning_rate = 1e-2\n#    learning_rate = 1e-3\n    #learning_rate = 1e-4\n\n    model_chk = create_mlp(len(x_features), 5, hidden_units, dropout_rates, learning_rate)\n    # 3 minutes to evaluate\n    #model_chk.evaluate(normalized_data_test)\n    model_chk.evaluate(X_train, y_train)\n    try:\n        model_chk.load_weights(checkpoint_path_read)\n        # 3 minutes to evaluate\n        model_chk.evaluate(X_train, y_train)\n        model_chk.fit(X_train, y_train, epochs=25, batch_size=5000, callbacks=[cp_callback])\n    except:\n        print('failed going to refit the model, careful as you can lose all progress')\n        model_chk.fit(X_train, y_train, epochs=1, batch_size=5000, callbacks=[cp_callback])\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#submission = True\nsubmission = False\n\nif(submission==False):\n    import janestreet\n    janestreet.make_env.__called__ = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom numba import njit\n\n#@njit\n#def fillna_npwhere_njit(array, values):\n#    if np.isnan(array.sum()):\n#        array = np.where(np.isnan(array), values, array)\n#    return array\n\nf = np.median\nimport janestreet\nenv = janestreet.make_env()\n# if calculating a utility score during training, should have way to store this\nif(submission==False):\n    store_data_1 = []\n    store_data_2 = []\n    store_data_3 = []\n    test_df_arr = []\n    pred_df_arr = []\nfor (test_df, pred_df) in tqdm(env.iter_test()):\n    if test_df['weight'].item() > 0:\n#    if test_df['weight'].item() > 3.089004755: # mean with action > mean in sample\n#    if test_df['weight'].item() > 2.52845072: # mean of sample\n#    if test_df['weight'].item() > 2.42904520: # mean with action > mean 0.52 in sample\n#    if test_df['weight'].item() > 1.75373077: # median with action > mean 0.52 in sample\n#        import pdb;pdb.set_trace()\n        ts_id = 0 # make this keep track later # e.g. if date > previous date, then ts_id = 0 else ts_id +=1\n#        x_tt = np.append(test_df.loc[:, x_features_min].values,np.array([ts_id,0,0,0,0,0]))\n        x_tt = np.append(test_df.loc[:, x_features_min].values,np.array([ts_id,999,999,999,999,999]))\n#        x_tt = np.append(test_df.loc[:, x_features_min].values,np.array([ts_id,-999,-999,-999,-999,-999]))\n        if np.isnan(x_tt.sum()):\n            x_tt = np.where(np.isnan(x_tt), 0, x_tt)\n        x_tt_latent = model_chk_latent([x_tt], training=False)[0].numpy()[0] \n        ## what happens if i just predict from latent?\n        pred = np.where(np.mean(x_tt_latent[133:]) >= 0.50, 1, 0).astype(int)\n        pred_df.action = pred\n#        tensor_x_tt_latent_reorg = tf.convert_to_tensor([x_tt_latent[0:133]], dtype=\"float32\", dtype_hint=None, name=None)\n#        pred = np.mean(model_chk(tensor_x_tt_latent_reorg, training = False).numpy())\n#        pred_df.action = np.where(pred >= 0.50, 1, 0).astype(int)\n        \n    else:\n        pred_df.action = 0\n        pred=0.0\n    env.predict(pred_df)\n    if(submission==False):\n        store_data_1.append(pred_df)\n        store_data_2.append(test_df)\n        store_data_3.append(pred)\n        test_df_arr.append(test_df)\n        pred_df_arr.append(pred_df)\n        \n# last speed was 15219it [13:25, 18.88it/s]\n# 15219it [13:12, 19.21it/s]\n# 15219it [11:57, 21.21it/s]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#if this looks good...\n# maybe i should just figure out how to serve the model faster\n# eg via tfxlite or quantization pieces???","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%timeit model_chk_latent([fillna_npwhere_njit(np.append(test_df.loc[:, x_features_min].values,np.array([ts_id,0,0,0,0,0])), 0)], training=False)[0].numpy()[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%timeit np.mean(model_chk(tensor_x_tt_latent_reorg, training = False).numpy())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%timeit model_chk_latent([x_tt], training=False)[0].numpy()[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%timeit fillna_npwhere_njit(x_tt, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%timeit tf.convert_to_tensor([x_tt_latent[0:133]], dtype=\"float32\", dtype_hint=None, name=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%timeit np.append(test_df.loc[:, x_features_min].values,np.array([ts_id,0,0,0,0,0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%timeit np.where(pred >= 0.50, 1, 0).astype(int)#","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if(submission==False):\n    df_data_1 = pd.concat(store_data_1)\n    df_data_2 = pd.concat(store_data_2)\n    df_data_2['action'] = df_data_1['action']\n    df_data_2['resp'] = store_data_3 #careful as some values are just repeat of last value sicne skipped, but we are filterin gthese out\n    train_del = df_data_2\n    print(df_data_2.weight.mean())\n    \n    features_del = [c for c in train_del.columns if 'feature' in c]\n    print('Forward-Filling...')\n    train_del = train_del.query('weight > 0').reset_index(drop = True)\n    train_del[features_del] = train_del[features_del].fillna(method = 'ffill').fillna(0)\n\n    date = train_del['date'].values #* 1.0\n    weight = train_del['weight'].values #* 1.0\n    resp = train_del['resp'].values #* 1.0 # just doing this for now to debug... not sure if my 1 and 0 will also cause issues\n    action = train_del['action'].values #* 1.0\n\n\n    date = date.astype(np.int64)\n    weight = weight.astype(np.float64)\n    resp = resp.astype(np.float64)\n    action = action.astype(np.int64)\n    \n    from numba import njit\n    @njit(fastmath = True)\n    def utility_score_numba(date, weight, resp, action):\n    #    import pdb;pdb.set_trace()\n        Pi = np.bincount(date, weight * resp * action)\n        t = np.sum(Pi) / np.sqrt(np.sum(Pi ** 2)) * np.sqrt(250 / len(Pi))\n        u = min(max(t, 0), 6) * np.sum(Pi)\n        return u\n    \n    somevalue = utility_score_numba(date, weight, resp, action)\n    print(somevalue)\n    assumed_number = 50\n    print(somevalue/assumed_number)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#2.5032816647355256\n#Forward-Filling...\n#46239.871442783515\n#924.7974288556703\n\n#2.5032816647355256\n#Forward-Filling...\n#46239.871442783515\n#924.7974288556703","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#samples = model_chk_latent.distribution.sample(500000)\n#samples_np = samples.numpy() # shouldn't do it this way, but lets first start by making sure no issues in data syncing between two models\n#print(samples_np.mean())\n#x, _ = model_chk.predict(samples)\n#del(samples)\n#train = pd.DataFrame(samples_np,columns=features)\n#train_nonlatent = pd.DataFrame(x,columns=features)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# detect and init the TPU\n#import tensorflow as tf\n#tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n#tf.config.experimental_connect_to_cluster(tpu)\n#tf.tpu.experimental.initialize_tpu_system(tpu)\n\n# instantiate a distribution strategy\n#tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n#batch_size = 5000\n#hidden_units = [130, 130, 130, 130, 130, 130]\n#dropout_rates = [0.2, 0.2, 0.2, 0.2, 0.5, 0.2, 0.2]\n#dropout_rates = [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2]\n#label_smoothing = 1e-2\n#learning_rate = 1e-2\n#learning_rate = 1e-3\n#learning_rate = 1e-4\n\n# instantiating the model in the strategy scope creates the model on the TPU\n#with tpu_strategy.scope():\n#    model = tf.keras.Sequential( … ) # define your model normally\n#    model.compile( … )\n#    clf = create_mlp(\n#        len(features), 5, hidden_units, dropout_rates, learning_rate\n#        )\n\n\n# train model normally\n#model.fit(training_dataset, epochs=EPOCHS, steps_per_epoch=…)\n\n#reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,patience=5, min_lr=0.001)\n#reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,patience=10, min_lr=0.001)\n#clf.fit(X_train, y_train, epochs=5, batch_size=5000, callbacks=[reduce_lr])\n#clf.fit(X_train, y_train, epochs=800, batch_size=5000, callbacks=[reduce_lr])\n#clf.fit(X_train, y_train, epochs=800, batch_size=5000) # try reduce_lr next\n\n\n#clf = create_mlp(len(features), 5, hidden_units, dropout_rates, learning_rate)\n\n#clf.fit(X_train, y_train, epochs=5, batch_size=5000)\n#clf.fit(X_train, y_train, epochs=800, batch_size=5000)\n#clf.fit(X_train, y_train, epochs=300, batch_size=5000)\n#clf.fit(X_train, y_train, epochs=300, batch_size=5000)\n#clf.fit(X_train, y_train, epochs=200, batch_size=5000)\n\n\n#models = []\n\n#models.append(clf)\n\n#th = 0.5000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### maybe need to investigate quantization vs normalization...\n#### mabe quantization would hav been better option come prediction time, to have down also during generative modelling...\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}