{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Import**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport time\nimport pickle\nimport random\nfrom tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Concatenate, Lambda, GaussianNoise, Activation\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers.experimental.preprocessing import Normalization\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nfrom torch.nn import CrossEntropyLoss, MSELoss\nfrom torch.nn.modules.loss import _WeightedLoss\nimport torch.nn.functional as F\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_columns', 100)\npd.set_option('display.max_rows', 100)\n\n# from sklearn.metrics import log_loss, roc_auc_score this is way to track down if the model is overfitting","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Data Save/Load**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#DATA_PATH = '../input/jane-street-market-prediction/' no need to import the data because we're going to only load pytorch and tensorflow models.\nCACHE_PATH = '../input/mlp012003weights'\n\ndef save_pickle(dic, save_path):\n    with open(save_path, 'wb') as f:\n    # with gzip.open(save_path, 'wb') as f:\n        pickle.dump(dic, f)\n\ndef load_pickle(load_path):\n    with open(load_path, 'rb') as f:\n    # with gzip.open(load_path, 'rb') as f:\n        message_dict = pickle.load(f)\n    return message_dict\n\nf_mean = np.load(f'{CACHE_PATH}/f_mean_online.npy')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Feature Engineering**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# list of the features\nfeat_cols = [f'feature_{i}' for i in range(130)]\n\n# list of all the features\nall_feat_cols = [col for col in feat_cols]\n\n# add two more features to the feature list\nall_feat_cols.extend(['cross_41_42_43', 'cross_1_2'])\n\n# resp 1,2,3,4\ntarget_cols = ['action', 'action_1', 'action_2', 'action_3', 'action_4']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Pytorch**"},{"metadata":{},"cell_type":"markdown","source":"model"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model(nn.Module):\n    \n    def __init__(self):\n        \n        super(Model, self).__init__()\n        self.batch_norm0 = nn.BatchNorm1d(len(all_feat_cols))\n        self.dropout0 = nn.Dropout(0.9) # 0.2\n\n        dropout_rate = 0.9 # 0.2\n        hidden_size = 256\n        \n        self.dense1 = nn.Linear(len(all_feat_cols), hidden_size)\n        self.batch_norm1 = nn.BatchNorm1d(hidden_size)\n        self.dropout1 = nn.Dropout(dropout_rate)\n\n        self.dense2 = nn.Linear(hidden_size+len(all_feat_cols), hidden_size)\n        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n        self.dropout2 = nn.Dropout(dropout_rate)\n\n        self.dense3 = nn.Linear(hidden_size+hidden_size, hidden_size)\n        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n        self.dropout3 = nn.Dropout(dropout_rate)\n\n        self.dense4 = nn.Linear(hidden_size+hidden_size, hidden_size)\n        self.batch_norm4 = nn.BatchNorm1d(hidden_size)\n        self.dropout4 = nn.Dropout(dropout_rate)\n\n        self.dense5 = nn.Linear(hidden_size+hidden_size, len(target_cols))\n\n        self.Relu = nn.ReLU(inplace=True)\n        self.PReLU = nn.PReLU()\n        self.LeakyReLU = nn.LeakyReLU(negative_slope=0.01, inplace=True)\n        # self.GeLU = nn.GELU()\n        self.RReLU = nn.RReLU()\n\n    def forward(self, x):\n        \n        x = self.batch_norm0(x)\n        x = self.dropout0(x)\n\n        x1 = self.dense1(x)\n        x1 = self.batch_norm1(x1)\n        # x = F.relu(x)\n        # x = self.PReLU(x)\n        x1 = self.LeakyReLU(x1)\n        x1 = self.dropout1(x1)\n\n        x = torch.cat([x, x1], 1)\n\n        x2 = self.dense2(x)\n        x2 = self.batch_norm2(x2)\n        # x = F.relu(x)\n        # x = self.PReLU(x)\n        x2 = self.LeakyReLU(x2)\n        x2 = self.dropout2(x2)\n\n        x = torch.cat([x1, x2], 1)\n\n        x3 = self.dense3(x)\n        x3 = self.batch_norm3(x3)\n        # x = F.relu(x)\n        # x = self.PReLU(x)\n        x3 = self.LeakyReLU(x3)\n        x3 = self.dropout3(x3)\n\n        x = torch.cat([x2, x3], 1)\n\n        x4 = self.dense4(x)\n        x4 = self.batch_norm4(x4)\n        # x = F.relu(x)\n        # x = self.PReLU(x)\n        x4 = self.LeakyReLU(x4)\n        x4 = self.dropout4(x4)\n\n        x = torch.cat([x3, x4], 1)\n\n        x = self.dense5(x)\n\n        return x","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **GPU/CPU**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Since we're going to onyl load the model, then use it for inference, then it's better to use cpu\n# Otherwise, you want to retrain the model, then enable the GPU for faster calculations\n\nif torch.cuda.is_available():\n    print('using device: cuda')\n    torch.device(\"cuda:0\")\nelse:\n    print('using device: cpu')\n    device = torch.device('cpu')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"PyTorch Load Model weights"},{"metadata":{"trusted":true},"cell_type":"code","source":"NFOLDS = 5\n\nmodel_list = []\ntmp = np.zeros(len(feat_cols))\nfor _fold in range(NFOLDS):\n    torch.cuda.empty_cache()\n    model = Model()\n    model.to(device)\n    model_weights = f\"{CACHE_PATH}/online_model{_fold}.pth\"\n    #model.load_state_dict(torch.load(model_weights))\n    model.load_state_dict(torch.load(model_weights, map_location=device))\n    model.eval()\n    model_list.append(model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Tensorflow**"},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED = 1111\n\nnp.random.seed(SEED)\n\n# fit\ndef create_mlp(\n    num_columns, num_labels, hidden_units, dropout_rates, label_smoothing, learning_rate\n):\n\n    inp = tf.keras.layers.Input(shape=(num_columns,))\n    x = tf.keras.layers.BatchNormalization()(inp)\n    x = tf.keras.layers.Dropout(dropout_rates[0])(x)\n    for i in range(len(hidden_units)):\n        x = tf.keras.layers.Dense(hidden_units[i])(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n        x = tf.keras.layers.Dropout(dropout_rates[i + 1])(x)\n    \n    x = tf.keras.layers.Dense(num_labels)(x)\n    out = tf.keras.layers.Activation(\"sigmoid\")(x)\n\n    model = tf.keras.models.Model(inputs=inp, outputs=out)\n    model.compile(\n        optimizer=tfa.optimizers.RectifiedAdam(learning_rate=learning_rate),\n        loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=label_smoothing),\n        metrics=tf.keras.metrics.AUC(name=\"AUC\"),\n    )\n\n    return model\n\nepochs = 200\nbatch_size = 4096\nhidden_units = [160, 160, 160]\ndropout_rates = [0.8, 0.6, 0.4, 0.2] # all 0.2\nlabel_smoothing = 1e-2\nlearning_rate = 1e-3\n\ntf.keras.backend.clear_session()\ntf.random.set_seed(SEED)\nclf = create_mlp(len(feat_cols), 5, hidden_units, dropout_rates, label_smoothing, learning_rate)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"TensorFlow Load model weights"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit the model and save it with \n#clf.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=2)\n#clf.save(f'model.h5')\n\n# Load the Fitted model\n# !ls ../input/jane-street-with-keras-nn-overfit/\nclf.load_weights('../input/jane-street-with-keras-nn-overfit/model.h5')\n\n# If you have several models, the you can store into a list\n#tf_models = [clf]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Inference(关键部分)**"},{"metadata":{},"cell_type":"markdown","source":"被坑了好久，本次题目提交需要使用在线的操作，而不能使用本地的submission，我一开始没有理解这个意思，后面才明白，需要导入它所给定的janestreet包，其余操作看代码部分。"},{"metadata":{"trusted":true},"cell_type":"code","source":"th = 0.503 # 0.5 0.501 0.0502 0.053 did not have an effect\nimport janestreet\njanestreet.competition.make_env.__called__ = False\n\nenv = janestreet.make_env()\nenv_iter = env.iter_test()\n\n#test_df has one single row of data with all the feautures\n# pred_dfd has 1 or 0 which is an action that comes from test_df\n\nfor (test_df, pred_df) in tqdm(env_iter):\n\n    # Data Processing\n    if test_df['weight'].item() > 0:\n        x_tt = test_df.loc[:, feat_cols].values\n        \n        # if there is only one value eqauls nan, then x_tt.sum will return nan, then np.isnan will be true, and enters the loop\n        if np.isnan(x_tt.sum()):\n            # Replace NaN with zero and infinity with large finite numbers (default behaviour)\n            x_tt = np.nan_to_num(x_tt) + np.isnan(x_tt) * f_mean # the score goew down if we take the if statement\n\n    \n        cross_41_42_43 = x_tt[:, 41] + x_tt[:, 42] + x_tt[:, 43]\n        cross_1_2 = x_tt[:, 1] / (x_tt[:, 2] + 1e-5)\n        feature_inp = np.concatenate((x_tt, np.array(cross_41_42_43).reshape(x_tt.shape[0], 1), np.array(cross_1_2).reshape(x_tt.shape[0], 1),), axis=1)\n\n        \n        # PytTorch prediction \n        torch_pred = np.zeros((1, len(target_cols)))\n        for model in model_list:\n            torch_pred += model(torch.tensor(feature_inp, dtype=torch.float).to(device)).sigmoid().detach().cpu().numpy() / NFOLDS\n        torch_pred = np.median(torch_pred)\n\n        \n        # TensofFlow prediction\n        # I foyu have several TF models then use\n        #tf_pred = np.median(np.mean([model(x_tt, training = False).numpy() for model in tf_models],axis=0))\n        # If you have only one model then use\n        tf_pred = np.median(clf(x_tt))\n\n        \n        # PyTorch and TensorFlow Average prediction\n        pred = torch_pred * 0.44 + tf_pred * 0.56\n        pred_df.action = np.where(pred >= th, 1, 0).astype(int)\n        \n    else:\n        pred_df.action = 0\n        \n    env.predict(pred_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}