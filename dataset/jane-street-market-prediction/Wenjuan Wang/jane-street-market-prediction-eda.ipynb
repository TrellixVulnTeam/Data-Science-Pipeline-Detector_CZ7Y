{"cells":[{"metadata":{},"cell_type":"markdown","source":"## This notebook is for exploring the dataset, including data understanding, distributions, correlations, missing values etc"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# import packages\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_columns', 100) # 100 column limit\npd.set_option('display.max_rows', 100) # 100 row limits","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fetch data"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport dask.dataframe as dd\nfrom dask.distributed import Client, progress\n\n# reading the paths of all the files present in the dataset\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time\ntrain = dd.read_csv(\"../input/jane-street-market-prediction/train.csv\")\n#features = dd.read_csv(\"../input/jane-street-market-prediction/features.csv\")\n#example_test = dd.read_csv(\"../input/jane-street-market-prediction/example_test.csv\")\n# transform train from dask format to pandas dataframe\ntrain = train.compute()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(train.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### There are 500 days in total in train dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"#train['date'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The number of transactions/trades each day is different."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x=\"date\", kind=\"count\", data=train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plot resp_1, resp_2, resp_3, resp_4, resp and weight along time for day 0,1,2\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"date_0 = train.loc[train['date'] == 0]\ndate = date_0.copy()\nx = range(date.shape[0])\n\nfig, axs = plt.subplots(6)\nfig.suptitle('Resp_1, Resp_2, Resp_3, Resp_3, Resp_4, Resp and Weight for Day 0')\naxs[0].plot(x,date['resp_1'],'r')\naxs[1].plot(x,date['resp_2'],'b')\naxs[2].plot(x,date['resp_3'],'g')\naxs[3].plot(x,date['resp_4'],'y')\naxs[4].plot(x,date['resp'],'y')\naxs[5].plot(x,date['weight'],'y')\n\n\nplt.show()\n\ndel date","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"date_1 = train.loc[train['date'] == 1]\ndate = date_1.copy()\nx = range(date.shape[0])\n\nfig, axs = plt.subplots(6)\nfig.suptitle('Resp_1, Resp_2, Resp_3, Resp_3, Resp_4, Resp and Weight for Day 1')\naxs[0].plot(x,date['resp_1'],'r')\naxs[1].plot(x,date['resp_2'],'b')\naxs[2].plot(x,date['resp_3'],'g')\naxs[3].plot(x,date['resp_4'],'y')\naxs[4].plot(x,date['resp'],'y')\naxs[5].plot(x,date['weight'],'y')\n\nplt.show()\ndel date","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"date_2 = train.loc[train['date'] == 2]\ndate = date_2.copy()\nx = range(date.shape[0])\n\nfig, axs = plt.subplots(6)\nfig.suptitle('Resp_1, Resp_2, Resp_3, Resp_3, Resp_4, Resp and Weight for Day 2')\naxs[0].plot(x,date['resp_1'],'r')\naxs[1].plot(x,date['resp_2'],'b')\naxs[2].plot(x,date['resp_3'],'g')\naxs[3].plot(x,date['resp_4'],'y')\naxs[4].plot(x,date['resp'],'y')\naxs[5].plot(x,date['weight'],'y')\nplt.show()\n\ndel date","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### What is the resp like when weight = 0"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_weight_positive = train.loc[train['weight'] != 0]\n\ntrain_weight_0 = train.loc[train['weight'] == 0]\nsns.distplot(train_weight_0['resp'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### What is the percentage of weight = 0"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Can we find out why these weight = 0 from the features?"},{"metadata":{},"cell_type":"markdown","source":"## Feature_0"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['feature_0'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plot Feature_0 vs Resp for Day 0"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x=\"feature_0\",y=\"resp\", kind=\"box\", data=date_0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Contigency table of Feature_0 with Resp >0. \n\nIt shows that feature_0 does not have a clear association with positive return"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(train['feature_0'], train['resp'] > 0,  margins = False) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The distribution of feature_0 for weight = 0 or not"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(train['feature_0'], train['weight'] > 0,  margins = False) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Number of missing values for each feature"},{"metadata":{},"cell_type":"markdown","source":"For the whole train dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_missing_values = pd.DataFrame(train.isna().sum().sort_values(ascending=False),columns=['number of missings'])\nfeature_missing_values.T\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For train dataset when the weight is positive"},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_missing_values = pd.DataFrame(train_weight_positive.isna().sum().sort_values(ascending=False),columns=['number of missings'])\nfeature_missing_values.T\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Correlations of features"},{"metadata":{},"cell_type":"markdown","source":"Feature correlations for the whole train dataset.\n\nSome features are pecfectly correlated. Some correlation are even 1."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.corr().style.background_gradient(cmap='coolwarm', axis=None).set_precision(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plot some features that have high correlation from a time serious view? e.g. feature 60 vs feature 61"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(train['ts_id'],train['feature_60'],'r',train['ts_id'],train['feature_61'],'green')\nplt.title(\"Feature 60 and 61 for whole train dataset\", fontsize=16, fontweight='bold')\nplt.xlabel(\"transactions/trades along time\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The Pearson correlation coefficient for Feature 60 and 61 is 0.997 which means a total positive linear correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"diff = train['feature_60'] - train['feature_61']\nprint(f'The difference between feature 60 and 61 is normally distributed. \\n The range is \\n{diff.describe()}')\n\ncovariance = np.cov(train['feature_60'], train['feature_61'])\nprint(f'The covariance matrix is \\n {covariance}')\n\nfrom scipy.stats import pearsonr\ncorr, _ = pearsonr(train['feature_60'], train['feature_61'])\nprint('Pearsons correlation: %.3f' % corr)\n\nplt.hist(diff, bins = 1000)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"### The distribution of all features and The distribution of all features for positive weight"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Taken from this notebook: https://www.kaggle.com/blurredmachine/jane-street-market-eda-viz-prediction\n\nimport plotly.express as px\n\ndate = 0\nn_features = 130\n\ncols = [f'feature_{i}' for i in range(1, n_features)]\nhist = px.histogram(\n    train[train[\"date\"] == date], \n    x=cols, \n    animation_frame='variable', \n    range_y=[0, 600], \n    range_x=[-7, 7]\n)\n\nhist.show()\n\n\n\ndate = 0\nn_features = 130\n\ncols = [f'feature_{i}' for i in range(1, n_features)]\nhist = px.histogram(\n    train_weight_positive[train_weight_positive[\"date\"] == date], \n    x=cols, \n    animation_frame='variable', \n    range_y=[0, 600], \n    range_x=[-7, 7]\n)\n\nhist.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"### Using PCA to deal with multicollinearity"},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove the returns and ts_id for PCA analysis\ncols = list(train.columns)\nfor removeCol in ['date', 'weight', 'resp_1', 'resp_2', 'resp_3', 'resp_4', 'resp', 'ts_id']:\n    cols.remove(removeCol)\n\ntrain_features = train[cols]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\n#In general it is a good idea to scale the data\nscaler = StandardScaler()\nscaler.fit(train_features)\ntrain_features_scaled = scaler.transform(train_features)\n\n\n# initiate PCA \npca = PCA(n_components=50)\n\n# transform \ntrain_features_scaled_transformed = pca.fit_transform(train_features_scaled[~np.isnan(train_features_scaled).any(axis=1)])\n\n\n# when testing,transform new data using already fitted pca \n# (don't re-fit the pca)\n# newdata_transformed = pca.transform(newdata)\n\n#The amount of variance that each PC explains\nvar= pca.explained_variance_ratio_\nprint(var)\n\n#Cumulative Variance explains\nvar1=np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)\n\nprint(var1)\n\nplt.plot(var1)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loadings = pca.components_.T\ndf_loadings = pd.DataFrame(loadings)\ndf_loadings","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}