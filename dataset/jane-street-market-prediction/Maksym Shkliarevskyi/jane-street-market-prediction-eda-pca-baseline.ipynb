{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Jane Street Market Prediction\n![janestreet](https://www.janestreet.com/assets/logo_horizontal.png)\n\n### “Buy low, sell high.” It sounds so easy….\n\nIn reality, trading for profit has always been a difficult problem to solve, even more so in today’s fast-moving and complex financial markets. Electronic trading allows for thousands of transactions to occur within a fraction of a second, resulting in nearly unlimited opportunities to potentially find and take advantage of price differences in real time.\n\n## See also the second part of this notebook:\n\n## [Jane Street Market Prediction: Baseline (Part 2)](https://www.kaggle.com/maksymshkliarevskyi/jane-street-market-prediction-baseline-part-2)"},{"metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"!pip install bioinfokit","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"!pip install seaborn --upgrade","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# basic packages\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom random import sample\nimport gc\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\n\n# for PCA\nfrom bioinfokit.visuz import cluster\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\n# ignoring warnings\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\nimport janestreet","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"sns.__version__","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading and a first look at the data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/jane-street-market-prediction/train.csv')\nfeatures_df = pd.read_csv('../input/jane-street-market-prediction/features.csv')\nexample_test = pd.read_csv('../input/jane-street-market-prediction/example_test.csv')\nsample_prediction_df = pd.read_csv('../input/jane-street-market-prediction/example_sample_submission.csv')\n\nprint('Train dataset shape: {}'.format(train_df.shape))\nprint('Features dataset shape: {}'.format(features_df.shape))\nprint('Example test dataset shape: {}'.format(example_test.shape))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print('Head of the train data:')\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print('Train dataset dtypes: \\n{}'.format(train_df.dtypes.value_counts()))\nprint('-'*20)\nprint('Features dataset dtypes: \\n{}'.format(features_df.dtypes.value_counts()))\nprint('-'*20)\nprint('Example test dtypes: \\n{}'.format(example_test.dtypes.value_counts()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Almost all columns in train and test datasets are numeric. Features dataset all have bool dtype."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print('Columns with NaN (Train): %d' %train_df.isna().any().sum())\nprint('Columns with NaN (Features): %d' %features_df.isna().any().sum())\nprint('Columns with NaN (Example test): %d' %example_test.isna().any().sum())","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"NaN_train = pd.Series(train_df.isna().sum()[train_df.isna().sum() > 0].\n                      sort_values(ascending = False) / len(train_df) * 100)\n\nsns.set_style(\"whitegrid\")\nplt.figure(figsize=(10, 10))\n\nsns.barplot(y = NaN_train.index[:30], x = NaN_train[:30], \n            edgecolor = 'black', alpha = 0.8,\n            palette = sns.color_palette(\"viridis\", len(NaN_train[:30])))\nplt.title('NaN values of train dataset (30 columns)', size = 13)\nplt.xlabel('NaN values (%)')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"NaN_test = pd.Series(example_test.isna().sum()[example_test.isna().sum() > 0].\n                      sort_values(ascending = False) / len(example_test) * 100)\n\nsns.set_style(\"whitegrid\")\nplt.figure(figsize=(10, 10))\n\nsns.barplot(y = NaN_test.index[:30], x = NaN_test[:30], \n            edgecolor = 'black', alpha = 0.8,\n            palette = sns.color_palette(\"viridis\", len(NaN_test[:30])))\nplt.title('NaN values of test dataset (30 columns)', size = 13)\nplt.xlabel('NaN values (%)')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look at the distribution of 'date' column."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 5))\nplt.title('Date', size = 15)\n\nsns.histplot(data = train_df, x = 'date',\n             edgecolor = 'black',\n             palette = \"viridis\")\nplt.xlabel('')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And also 'resp' and 'weight' columns, which together represents a return on the trade."},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"def my_plot(feat, ax = None):\n    if ax != None:\n        sns.histplot(data = train_df, x = feat,\n                     palette = \"viridis\", ax = ax)\n        ax.set_xlabel('')\n        ax.set_title(f'{feat}')\n    else:\n        sns.histplot(data = train_df, x = feat,\n                     palette = \"viridis\")\n        plt.xlabel('')\n        plt.title(f'{feat}')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 10))\nsns.set_style(\"whitegrid\")\nplt.suptitle('resp 1-4 columns', size = 15)\n\nmy_plot('resp_1', ax1)\nmy_plot('resp_2', ax2)\nmy_plot('resp_3', ax3)\nmy_plot('resp_4', ax4)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8, 5))\n\nmy_plot('resp')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 5))\nplt.title('Weight', size = 15)\n\nsns.histplot(data = train_df, x = 'weight',\n             edgecolor = 'black',\n             palette = \"viridis\", binwidth = 1)\nplt.xlabel('')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print('Rows with weight==0: \\t %d' %len(train_df[train_df.weight == 0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking \"ts_id\" for unique values. The result must be 'True'."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.ts_id.nunique() == len(train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"sample_df = sample(list(train_df.columns[7:]), 4)\n\nfig, ax = plt.subplots(2, 2, figsize=(16, 8))\nsns.set_style(\"whitegrid\")\nplt.suptitle('Random feature columns', size = 15)\n\nmy_plot(sample_df[0], ax[0, 0])\nmy_plot(sample_df[1], ax[0, 1])\nmy_plot(sample_df[2], ax[1, 0])\nmy_plot(sample_df[3], ax[1, 1])\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We should look at the 'features' dataset too."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"features_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dataset represents a set of bool values. Let's check some of the most frequent features."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"features_tags = features_df.apply(lambda x: x[x == True].count(), axis = 1) \\\n    .sort_values(ascending = False).astype('str')\n\nsns.set_style(\"whitegrid\")\nplt.figure(figsize=(10, 5))\n\nsns.histplot(x = features_tags,\n             edgecolor = 'black',\n             palette = \"viridis\", binwidth = 1)\nplt.xlabel('Tags count')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# PCA"},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA()\npca_out = pca.fit(StandardScaler().fit_transform(train_df.iloc[:, 7:-1]\n                                                 .dropna()))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"comp = pca_out.components_\nnum_pc = pca_out.n_features_\npc_list = [\"PC\" + str(i) for i in list(range(1, num_pc + 1))]\ncomp_df = pd.DataFrame.from_dict(dict(zip(pc_list, comp)))\ncomp_df['variable'] = train_df.iloc[:, 7:-1].columns.values\ncomp_df = comp_df.set_index('variable')\n\ncomp_df.head(10).style.background_gradient(cmap = 'viridis')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Positive and negative values in component loadings reflect the positive and negative correlation of the variables with then PCs."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 8))\nplt.title('Corelation matrix of 10 first feature columns (Train dataset)', size = 15)\n\nsns.heatmap(comp_df.iloc[:10, :10], annot = True, cmap = 'Spectral')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We should keep only the PCs which explain the most variance. The eigenvalues for PCs can help to retain the number of PCs. It will be useful for future predictions."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"cluster.screeplot(obj = [pc_list[:20], pca_out.explained_variance_ratio_[:20]], \n                  show = True, dim = (16, 5), axlabelfontsize = 13)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# PCA loadings plots\n# 2D\ncluster.pcaplot(x = comp[0], y = comp[1], \n                labels = range(0, 129, 1), \n                var1 = round(pca_out.explained_variance_ratio_[0]*100, 2),\n                var2 = round(pca_out.explained_variance_ratio_[1]*100, 2),\n                show = True, dim = (10, 8), axlabelfontsize = 13)\n\n# 3D\ncluster.pcaplot(x = comp[0], y = comp[1], z = comp[2],  \n                labels = range(0, 129, 1), \n                var1 = round(pca_out.explained_variance_ratio_[0]*100, 2), \n                var2 = round(pca_out.explained_variance_ratio_[1]*100, 2), \n                var3 = round(pca_out.explained_variance_ratio_[2]*100, 2),\n                show = True, dim = (14, 10), axlabelfontsize = 13)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Also, let's look at the test data example."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pca = PCA()\ntest_pca_out = test_pca.fit(StandardScaler()\n                            .fit_transform(example_test.iloc[:, 2:-1]\n                                           .dropna()))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"comp_test = test_pca_out.components_\ntest_num_pc = test_pca_out.n_features_\ntest_pc_list = [\"PC\" + str(i) for i in list(range(1, test_num_pc + 1))]\ncomp_test_df = pd.DataFrame.from_dict(dict(zip(test_pc_list, comp_test)))\ncomp_test_df['variable'] = example_test.iloc[:, 2:-1].columns.values\ncomp_test_df = comp_test_df.set_index('variable')\n\ncomp_test_df.head(10).style.background_gradient(cmap = 'viridis')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 8))\nplt.title('Corelation matrix of 10 first feature columns (Test dataset)', size = 15)\n\nsns.heatmap(comp_test_df.iloc[:10, :10], annot = True, cmap = 'Spectral')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"cluster.screeplot(obj = [test_pc_list[:20], \n                         test_pca_out.explained_variance_ratio_[:20]], \n                  show = True, dim = (16, 5), axlabelfontsize = 13)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# PCA loadings plots\n# 2D\ncluster.pcaplot(x = comp_test[0], y = comp_test[1], \n                labels = range(0, 129, 1), \n                var1 = round(test_pca_out.explained_variance_ratio_[0]*100, 2),\n                var2 = round(test_pca_out.explained_variance_ratio_[1]*100, 2),\n                show = True, dim = (10, 8), axlabelfontsize = 13)\n\n# 3D\ncluster.pcaplot(x = comp_test[0], y = comp_test[1], z = comp_test[2],  \n                labels = range(0, 129, 1), \n                var1 = round(test_pca_out.explained_variance_ratio_[0]*100, 2), \n                var2 = round(test_pca_out.explained_variance_ratio_[1]*100, 2), \n                var3 = round(test_pca_out.explained_variance_ratio_[2]*100, 2),\n                show = True, dim = (14, 10), axlabelfontsize = 13)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is no significant difference between 'train' and 'example_test' datasets. Both have three PCs with importance over 10% and some number of less importance (around 5%). For future prediction, I'll use 10 PCs firstly."},{"metadata":{},"cell_type":"markdown","source":"# Baseline prediction\n\nNow, we'll make a simple prediction, without complicated data preprocessing and feature engineering. We'll use XGBClassifier as a terrific simple but strong algorithm.\n\nFor the training process, we need feature columns with not zero weight values. As a prediction target ('action') we'll use a feature that contains 'weight' and 'resp' columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading prediction work space\nenv = janestreet.make_env()\niter_test = env.iter_test()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preparing the data\ntrain_df = train_df[train_df['weight'] != 0]\ntrain_df['action'] = ((train_df['weight'].values * train_df['resp']\n                       .values) > 0).astype('int')\n\nX_train = train_df.loc[:, train_df.columns.str.contains('feature')]\ny_train = train_df.loc[:, 'action']\n\nX_train = X_train.fillna(-999)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"sns.set_style(\"whitegrid\")\nplt.figure(figsize=(10, 5))\n\nsns.histplot(x = y_train.astype('str'),\n             edgecolor = 'black',\n             palette = \"viridis\", binwidth = 1)\nplt.xlabel('Action')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have balanced targets."},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_df\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_tr, X_valid, y_tr, y_valid = train_test_split(X_train, y_train, \n#                                                 train_size = 0.85, \n#                                                 random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# params = {'n_estimators': 1000,\n#           'max_depth': 12,\n#           'subsample': 0.9,\n#           'learning_rate': 0.05,\n#           'missing': -999,\n#           'random_state': 0,\n#           'tree_method': 'gpu_hist'}\n\n# model = XGBClassifier(**params)\n\n# model.fit(X_tr, y_tr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Model evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# print('ROC AUC score: %.3f' \n#       %roc_auc_score(y_valid, model.predict(X_valid)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The second part of notebook: \n## [Jane Street Market Prediction: Baseline (Part 2)](https://www.kaggle.com/maksymshkliarevskyi/jane-street-market-prediction-baseline-part-2)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# for (test_df, sample_prediction_df) in iter_test:\n#     X_test = test_df.loc[:, test_df.columns.str.contains('feature')]\n#     X_test.fillna(-999)\n#     preds = model.predict(X_test)\n#     sample_prediction_df.action = preds\n#     env.predict(sample_prediction_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Work in progress..."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}