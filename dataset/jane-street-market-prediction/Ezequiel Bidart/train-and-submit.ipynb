{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import time\nSTART_TIME = time.time()\n\nfrom tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Concatenate, Average, Add, Flatten, Lambda, GaussianNoise, Activation\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.losses import BinaryCrossentropy, KLDivergence\nfrom tensorflow.keras.optimizers import Adam, Nadam, SGD\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.layers.experimental.preprocessing import Normalization\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\n\nfrom sklearn.preprocessing import MinMaxScaler #StandardScaler, RobustScaler, Normalizer, power_transform, PowerTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import log_loss, accuracy_score, roc_auc_score, f1_score\n\nfrom scikitplot.metrics import plot_confusion_matrix\n\n\nfrom tqdm import tqdm\nfrom random import choices\n\nimport kerastuner as kt\n\nimport matplotlib.pyplot as plt\n\nfrom glob import glob","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/jane-street-market-prediction/train.csv')\n# I want to use all the data\n#train = train.query('date > 85').reset_index(drop = True) \ntrain = train.astype({c: np.float32 for c in train.select_dtypes(include='float64').columns})\ntrain.fillna(train.median(), inplace=True)\ntrain = train.query('weight > 0').reset_index(drop = True)\ntrain['action'] = (train['resp'] > 0).astype('int')\nfeatures = [c for c in train.columns if 'feature' in c]\n\nresp_cols = ['resp_1', 'resp_2', 'resp_3', 'resp', 'resp_4']\n\nX = train[features].values\ny = np.stack([(train[c] > 0).astype('int') for c in resp_cols]).T\n\nf_median = np.median(train[features[1:]].values, axis=0)\n#pd.to_pickle(f_median, 'f_median.pkl')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_resps = train[resp_cols].values\ny_actions = np.stack([(train[c] > 0).astype('int') for c in resp_cols]).T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_history(h, epochs_ignored=0.2, title=None):\n\n    vals = [k for k in h.history.keys() if k.startswith('val_')]\n    \n    epochs = len(h.history[vals[0]])\n    if epochs > 2:\n        last_epochs = int(epochs * (1-epochs_ignored))\n        epochs_x = list(range(epochs - last_epochs, epochs))\n    else:\n        last_epochs = epochs\n        epochs_x = list(range(epochs))\n    \n    fig, axes = plt.subplots(len(vals), 3, figsize=(20, len(vals)*4))\n    axes = axes.flatten()\n    for i, val in enumerate(vals):\n        loss = val.replace('val_', '')\n        ax = axes[i*3]\n        ax.plot(epochs_x, h.history[loss][-last_epochs:])\n        ax.set_title(loss)\n        ax = axes[i*3 + 1]\n        ax.plot(epochs_x, h.history[val][-last_epochs:])\n        ax.set_title(val)\n        ax = axes[i*3 + 2]\n        ax.plot(epochs_x, h.history[loss][-last_epochs:])\n        ax.plot(epochs_x, h.history[val][-last_epochs:])\n        ax.legend([loss, val])\n        ax.set_title(f'{loss} vs {val}')\n        \n    if title != None:\n        fig.suptitle(title)\n    fig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# target = [action_resp, resp, action_resps, resps]\n# dims:         1          1         5         5\n\ndef create_model(input_dim, output_dims, add_models=0):\n    \n    input_layer_0 = Input(input_dim)\n    bn_0 = BatchNormalization()(input_layer_0)\n    \n    # n_models to predict responses\n    outputs_layer_0 = []\n    for m in range(2+add_models):\n        x = Dropout(0.2)(bn_0)\n        for i in range(m+1):\n            x = Dense(64)(x)    \n            x = BatchNormalization()(x)\n            x = Lambda(tf.keras.activations.swish)(x)\n            x = Dropout(0.1)(x)\n        output = Dense(output_dims[3], activation='linear', name=f'level_0_output_{m}')(x)\n        outputs_layer_0.append(output)\n    \n    output_layer_0_average = Average(name='output_layer_0_average')(outputs_layer_0)\n    bn_1 = BatchNormalization()(output_layer_0_average)\n\n    #output_layer_0_concatenated = Concatenate(name='output_layer_0_concatenated')(outputs_layer_0)\n    \n    input_layer_1 = Concatenate()([bn_0] + [bn_1])\n    \n    # n_models to predict actions\n    outputs_layer_1 = []\n    for m in range(2+add_models):\n        x = Dropout(0.2)(input_layer_1)\n        for i in range(m+1):\n            x = Dense(64)(x)    \n            x = BatchNormalization()(x)\n            x = Lambda(tf.keras.activations.swish)(x)\n            x = Dropout(0.1)(x)\n        output = Dense(output_dims[2], activation='sigmoid', name=f'level_1_output_{m}')(x)\n        outputs_layer_1.append(output)\n    \n    output_layer_1_average = Average(name='output_layer_1_average')(outputs_layer_1)\n    bn_2 = BatchNormalization()(output_layer_1_average)\n\n    #output_layer_1_concatenated = Concatenate(name='output_layer_1_concatenated')(outputs_layer_1)\n    \n    input_layer_2 = Concatenate()([bn_1] + [bn_2])\n    \n    # n_models to predict resp\n    outputs_layer_2 = []\n    for m in range(2+add_models):\n        x = Dropout(0.2)(input_layer_2)\n        for i in range(m+1):\n            x = Dense(64)(x)    \n            x = BatchNormalization()(x)\n            x = Lambda(tf.keras.activations.swish)(x)\n            x = Dropout(0.1)(x)\n        output = Dense(output_dims[1], activation='linear', name=f'level_2_output_{m}')(x)\n        outputs_layer_2.append(output)\n    \n    output_layer_2_average = Average(name='output_layer_2_average')(outputs_layer_2)\n    bn_3 = BatchNormalization()(output_layer_2_average)\n    \n    input_layer_3 = Concatenate()([bn_2] + [bn_3])\n    \n    # n_models to predict action\n    outputs_layer_3 = []\n    for m in range(2+add_models):\n        x = Dropout(0.2)(input_layer_3)\n        for i in range(m+1):\n            x = Dense(64)(x)    \n            x = BatchNormalization()(x)\n            x = Lambda(tf.keras.activations.swish)(x)\n            x = Dropout(0.1)(x)\n        output = Dense(output_dims[1], activation='sigmoid', name=f'level_3_output_{m}')(x)\n        outputs_layer_3.append(output)\n    \n    output_layer_3_average = Average(name='output_layer_3_average')(outputs_layer_3)\n    \n    model = Model(inputs=input_layer_0, outputs=[output_layer_3_average, output_layer_2_average, output_layer_1_average, output_layer_0_average])\n    \n    loss = {}\n    loss['output_layer_3_average'] = BinaryCrossentropy(label_smoothing = 0.1) # 1 action for resp\n    loss['output_layer_2_average'] = 'mse'                                     # 1 resp\n    loss['output_layer_1_average'] = BinaryCrossentropy(label_smoothing = 0.1) # 5 actions for resps\n    loss['output_layer_0_average'] = 'mse'                                     # 5 resps\n    \n    loss_weights={}\n    loss_weights['output_layer_3_average'] = .25\n    loss_weights['output_layer_2_average'] = .25\n    loss_weights['output_layer_1_average'] = .25\n    loss_weights['output_layer_0_average'] = .25\n    \n    metrics = {}\n    metrics['output_layer_3_average'] = tf.keras.metrics.AUC(name = 'auc')\n    metrics['output_layer_2_average'] = 'mse'    \n    metrics['output_layer_1_average'] = tf.keras.metrics.AUC(name = 'auc')\n    metrics['output_layer_0_average'] = 'mse'\n    \n    model.compile(optimizer = Adam(), loss = loss, metrics = metrics, loss_weights=loss_weights)\n    \n    return model\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 50\nbatch_size = 1024 * 4\nverbose = True\n\nobjective = 'val_output_layer_3_average_auc'\nobjective = 'output_layer_3_average_auc' # overfit\ndirection = 'max'\n\ntr = (0, 400)\nte = (420, 500)\n\ntrain_indices = train[(train.date >= tr[0]) & (train.date < tr[1])].index\ntest_indices = train[(train.date >= te[0]) & (train.date < te[1])].index\n\nmodel = create_model(input_dim=130, output_dims=(1,1,5,5), add_models=3)\n\nX_train, X_test = X[train_indices], X[test_indices]\ny_train = (y_actions[train_indices][:,3], y_resps[train_indices][:,3], y_actions[train_indices], y_resps[train_indices])\ny_test = (y_actions[test_indices][:,3], y_resps[test_indices][:,3], y_actions[test_indices], y_resps[test_indices])\n\nrlr = ReduceLROnPlateau(monitor = objective, factor = 0.5, patience = 4, verbose = 1, min_delta = 1e-4, mode = direction)\nes = EarlyStopping(objective, patience=21, restore_best_weights=True, mode=direction)\n\n#h = model.fit(X_train, y_train, validation_data = (X_test, y_test), epochs=epochs, batch_size=batch_size, verbose=verbose, callbacks = [es, rlr])\nh = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=verbose, callbacks = [es, rlr])\n\nmodel.save_weights('./model.hdf5')\n\nmetrics = model.evaluate(X_test, y_test, batch_size=batch_size)\nprint(metrics)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# action resp actions resps\n# same weight for all:     .5349, .5506\n# weights .40 .40 .10 .10: .5341  .5477\n# weights .45 .45 .05 .05: .5335  .5437  \n# weights .40 .10 .40 .10: .5347  .5518 # decrease importance of mse\n# weights .60 .10 .25 .05: .5353  .5484\n# weights .75 .10 .13 .02: .5342  .5470","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_history(h)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.keras.utils.plot_model(model, to_file=f'model.png', show_shapes=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.call = tf.function(model.call, experimental_relax_shapes=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = model.predict(X_test, batch_size=batch_size, verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred[0].flatten().shape, y_test[0].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred[2][:, 1].shape, y_test[2][:, 3].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import janestreet\nenv = janestreet.make_env()\niter_test = env.iter_test()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_models = [model]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.time()    \nth = 0.5\nj = 0\n\nfor (test_df, pred_df) in tqdm(iter_test):\n\n    if test_df['weight'].item() > 0:\n        \n        x_tt = test_df.loc[:, features].values\n        if np.isnan(x_tt[:, 1:].sum()):\n            x_tt[:, 1:] = np.nan_to_num(x_tt[:, 1:]) + np.isnan(x_tt[:, 1:]) * f_median\n\n        try:\n            pred = model(x_tt, training=False)[2].numpy().flatten()\n            pred = np.median(pred)\n            pred_df.action = np.where(pred >= th, 1, 0).astype(int)\n        except:\n            pred_df.action = 0\n\n    else:\n        pred_df.action = 0\n\n    env.predict(pred_df)\n    j +=1\n\ntotal = time.time() - start\nprint(f'Expected time for 1M: { total * 1000000 / (j*60*60+1):.2} hours')\nprint(f'Iters per second: {j/total:.1f} iter/s')\nprint(f'Global time: {(time.time() - START_TIME) / 60:.1f} minutes')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}