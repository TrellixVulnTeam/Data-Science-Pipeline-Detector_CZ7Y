{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Jane Street Market Prediction\n\nReference:  https://www.kaggle.com/c/jane-street-market-prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport pandas as pd\nimport numpy as np\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sb\n\nimport tensorflow as tf\n\nimport sys\n\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n%matplotlib inline\n\nSEED = 1111\ntf.random.set_seed(SEED)\nnp.random.seed(SEED)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Functions To Get Trainning Data\n\nDue to the size of trainning data, supporting functions are available to create and read a sample of trainning data."},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN_FILE = '/kaggle/input/jane-street-market-prediction/train.csv'\nSAMPLE_TRAINNING_FILE = './train-sample.csv'\n\n\n# Get all trainning data\ndef get_trainning_data():\n    print(\"Reading training data...\")\n    df = pd.read_csv(TRAIN_FILE)\n    return df\n\n# Create sample of trainning data\ndef create_trainning_sample(frac = 0.2):\n    df_train = get_trainning_data();\n    print(\"Creating sample file...\")\n    df_sample = df_train.sample(frac=frac)\n    df_sample.to_csv(SAMPLE_TRAINNING_FILE, index=False, header=True)\n    \ndef get_sample_trainning_data():\n    print(\"Reading sample training data...\")\n    # Read training data\n    return pd.read_csv(SAMPLE_TRAINNING_FILE)    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create Trainning Data Sample\n\nRun this block to create or recreate the sample trainning data file."},{"metadata":{"trusted":true},"cell_type":"code","source":"#create_trainning_sample(frac = 0.02);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Start Exploration"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read only sample data created in previous step above\n#df_train = get_sample_trainning_data()\n\n# Read all the data\ndf_train = get_trainning_data()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Generate action column...\")\ndf_train['action'] = 0\ndf_train.loc[(df_train['resp'] > 0), 'action'] = 1\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for balance\nsb.distplot(df_train['action']);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Sort By Time"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.sort_values(by=['date', 'ts_id'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Delete rows"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = df_train[df_train['weight'] != 0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Drop unecessary columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"columns_to_delete = ['date', 'weight', 'resp_1', 'resp_2', 'resp_3', 'resp_4', 'resp', 'ts_id']\nfor column in columns_to_delete:\n    df_train.drop(column, axis=1, inplace=True)\n    \nfeature_count = len(df_train.columns) - 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Handle Missing Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.fillna(df_train.mean(),inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reset Index"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = df_train.reset_index(drop=True)\ndf_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train"},{"metadata":{"trusted":true},"cell_type":"code","source":"NORMALIZE_NONE = 0\nNORMALIZE_MIN_MAX = 1\nNORMALIZE_MEAN = 2\n\nNORMALIZE_TYPE = NORMALIZE_NONE\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 20 / 80 split\ndf_validation, df_train = np.split(df_train, [int(.2*len(df_train))])\n\ndf_train = df_train.reset_index(drop=True)\ndf_validation = df_validation.reset_index(drop=True)\n\ndf_train_labels = df_train['action'].copy()\ndf_train.drop('action', inplace=True, axis=1)\n\ndf_validation_labels = df_validation['action'].copy()\ndf_validation.drop('action', inplace=True, axis=1)\n\nprint(\"Shapes: (train={}, train_labels={}, validation={}, validation_labels={})\".format(df_train.shape, df_train_labels.shape, df_validation.shape, df_validation_labels.shape))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Normalize Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"def normalize_data(df):\n    if NORMALIZE_TYPE == NORMALIZE_MIN_MAX:\n        return (df-df.min())/(df.max()-df.min())\n    elif NORMALIZE_TYPE == NORMALIZE_MEAN:\n        return (df-df.mean())/df.std()\n    else:\n        return df;\n\ndf_train = normalize_data(df_train)\ndf_validation = normalize_data(df_validation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ninput_length = 7\nbatch_size = 70\n\ngenerator_train = tf.keras.preprocessing.sequence.TimeseriesGenerator(\n    df_train, \n    df_train_labels,\n    length=input_length,\n    batch_size=batch_size)\n\ngenerator_validation = tf.keras.preprocessing.sequence.TimeseriesGenerator(\n    df_validation, \n    df_validation_labels,\n    length=input_length,\n    batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# some values that may or may not get used as I experiment\nEPOCHS = 100\nSTEPS = len(df_train) / batch_size\nLEARNING_RATE = 0.0005\n","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"\nmodel = tf.keras.models.Sequential()\n\nmodel.add(tf.keras.layers.LSTM(\n    feature_count, \n    activation='relu', \n    input_shape=(input_length, feature_count), \n    return_sequences=True))\n\nmodel.add(tf.keras.layers.Dropout(0.02))\n\nmodel.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n\nmodel.compile(loss=tf.keras.losses.BinaryCrossentropy(), \n                optimizer=tf.optimizers.Adam(learning_rate=LEARNING_RATE),\n                metrics=[\"accuracy\"])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"\nmodel.fit(generator_train,\n          validation_data = generator_validation,\n          epochs = EPOCHS,\n          steps_per_epoch = STEPS,\n          validation_steps=STEPS/10,\n          verbose = 1,\n          callbacks = [\n                EarlyStopping(monitor='loss', verbose=1, patience=10),\n                EarlyStopping(monitor='val_loss', verbose=1, patience=10)\n          ]\n    )  \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_history = model.history.history\n\nplt.figure(1)\nplt.subplot(211)\nplt.ylim(top=5)\nplt.plot(train_history['loss'])\nplt.ylabel('Average Loss Per Epoch')\nplt.xlabel('Epoch')\nplt.title('Average Loss Per Epoch vs Epoch')\n\nplt.subplot(212)\nplt.ylim(top=15)\nplt.plot(train_history['val_loss'])\nplt.ylabel('Val Loss per Epoch')\nplt.xlabel('epoch')\nplt.title('Val Loss per Epoch vs Epoch')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"# During prediction phase, what is the threshold to set action = 1\nPREDICTION_THRESHOLD = 0.5\nPREDICTION_IGNORE_WEIGHT = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import janestreet\nenv = janestreet.make_env() # initialize the environment\niter_test = env.iter_test() # an iterator which loops over the test set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Starting predictions...\")\ndf_submission = pd.DataFrame(columns = ['ts_id', 'action'])\n\nzero_weight_count = 0\nprediction_count = 0\nprediction_within_threshold_count = 0\nprediction_value = 0;\n\n# TODO:\n#   1) Figure out how to add a rolling cache of previous test_df. per Gena (https://www.kaggle.com/gdonchyts)\n#\n\nfor (test_df, sample_prediction_df) in iter_test:\n    \n    # drop columns\n    weight = test_df['weight'].item()\n    columns_to_delete = ['date', 'weight']\n    for column in columns_to_delete:\n        test_df.drop(column, axis=1, inplace=True)\n        \n    ts_id = sample_prediction_df.index.values[0]\n    action = 0\n\n    if weight != 0 and PREDICTION_IGNORE_WEIGHT == False:  \n        # handle missing values\n        test_df.fillna(test_df.mean(),inplace=True)\n        # normalize\n        test_df = normalize_data(test_df)\n        # predict\n        prediction = model.predict(np.array(test_df.values).reshape(1,1,130));\n        prediction_value = prediction[0][0][0]\n        if prediction_value > PREDICTION_THRESHOLD:\n            action = 1\n            prediction_within_threshold_count = prediction_within_threshold_count + 1\n        prediction_count = prediction_count + 1\n    else:\n        zero_weight_count = zero_weight_count + 1\n        \n    sample_prediction_df.action = action\n    print(\"\\rPrediciton: (index={}, action={}, prediction={}) Metrics: (submission_size={}, prediction_count={}, prediction_within_threshold_count={}, zero_weight_count={})\".format(\n        # Prediction\n        ts_id, \n        action, \n        prediction_value,\n        # Notes\n        len(df_submission),\n        prediction_count, \n        prediction_within_threshold_count,\n        zero_weight_count\n    ), end=\"\");\n    \n    df_submission = df_submission.append( {'ts_id': ts_id, 'action': action}, ignore_index=True )\n    env.predict(sample_prediction_df)   \n\nprint(\"\\nFinished predictions\")\ndf_submission.to_csv(\"./submission.csv\", index=False, header=True)\nprint(\"Submission file created\")\nprint(\"Metrics: (submission_size={}, prediction_count={}, prediction_within_threshold_count={}, zero_weight_count={})\".format(\n    len(df_submission),\n    prediction_count, \n    prediction_within_threshold_count,\n    zero_weight_count\n))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}