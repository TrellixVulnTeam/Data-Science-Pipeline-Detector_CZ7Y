{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score, roc_curve, roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import accuracy_score\nimport xgboost as xgb\nimport optuna\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport tensorflow.keras.layers as layers\nfrom tensorflow.keras.callbacks import Callback, ReduceLROnPlateau, ModelCheckpoint, EarlyStopping","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Credit:: https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\n\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df\n\n\ndef import_data(file):\n    \"\"\"create a dataframe and optimize its memory usage\"\"\"\n    df = pd.read_csv(file, parse_dates=True, keep_date_col=True)\n    df = reduce_mem_usage(df)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('-' * 80)\nprint('df_train')\ndf_train = import_data(\"../input/jane-street-market-prediction/train.csv\")\nprint('-' * 80)\nprint('df_test')\ndf_test = import_data(\"../input/jane-street-market-prediction/example_test.csv\")\ndf_feature = pd.read_csv(\"../input/jane-street-market-prediction/features.csv\")\ndf_sub = pd.read_csv(\"../input/jane-street-market-prediction/example_sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.shape, df_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_columns', None) ## To display all the columns\npd.set_option('display.max_rows', None) ## To display all the columns\npd.set_option('display.float_format', lambda x: '%.3f' % x) ## No scientific notation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Calculating the missing values\n# null_values = df_train.columns[df_train.isnull().sum() > 0].sort_values(ascending = False)\n# print(null_values)\n\npercent_missing = (df_train.isnull().sum()/df_train.shape[0])*100\nmissing_value_df = pd.DataFrame({'column_name': df_train.columns, 'percent_missing': percent_missing})\n# missing_value_df = missing_value_df.sort_values('percent_missing', inplace = True)\nmissing_value_df.loc[missing_value_df['percent_missing'] != 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ## Converting the datatypes to save the memory\n# df_train.feature_0 = df_train.feature_0.astype(np.int8)\n# df_train.date = df_train.date.astype(np.int16)\n# df_train.ts_id = df_train.ts_id.astype(np.int32)\n\n# for i in df_train:\n#     if df_train[i].dtype == np.float64:\n#         if (((df_train[i] < 0.0001) & (df_train[i] > -0.0001)).mean()) < 0.001:\n#             df_train[i] = df_train[i].astype(np.float32)\n            \n# df_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Removing almost 5% data\nparam = ['feature_129', 'feature_127', 'feature_125', 'feature_123', 'feature_121', 'feature_118', 'feature_117',\n         'feature_110', 'feature_93', 'feature_59', 'feature_58', 'feature_56', 'feature_55', 'feature_45', 'feature_31',\n         'feature_21', 'feature_3']\nfor i in param:\n    df_train = df_train.dropna(axis = 0, subset = [i])\n    \ndf_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_train.isnull().sum()[df_train.isnull().sum() > 0].sort_values(ascending = False))\n\n## We observe that fields like feature_7, feature_8, feature_17, feature_18, feature_27, feature_28, feature_72, feature_78,\n## feature_84, feature_90, feature_96, feature_102, feature_108, feature_114 needs imputation\n\n## Exploring the relationship between features which still have null values\nnulls = df_train.isnull().sum()\nnulls_lst = list(nulls[(nulls > 0)].index)\n\ndf_train[nulls_lst].corr().style.background_gradient(cmap = 'viridis')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will be dropping multiple columns from above list since they are highly correlated (75%)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = df_train.drop(['feature_8', 'feature_18', 'feature_27', 'feature_72', 'feature_84', 'feature_96', 'feature_102', 'feature_108', 'feature_114'], axis = 1)\ndf_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = df_train.copy()\ntemp.isnull().sum()[temp.isnull().sum() > 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = temp.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Updating the column with mean values instead of null values\nfeatures = [c for c in df_train.columns if 'feature' in c]\n# f_mean = df_train[features[1:]].mean()\n# df_train[features[1:]] = df_train[features[1:]].fillna(f_mean)\nf_median = df_train.median()\ndf_train = df_train.fillna(f_median)\n\n## Removing rows where weights = 0\ndf_train = df_train.query('weight > 0').reset_index(drop = True)\n\n## Adding target column in train dataset\ndf_train['action'] = np.where(df_train['resp'] > 0, 1, 0)\n\ndf_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.isnull().sum()[df_train.isnull().sum() > 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Splitting the dataset into train & test\nX = df_train[features]\nY = df_train['action']\n\nx_train, x_val, y_train, y_val = train_test_split(X, Y, test_size = 0.15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scalar = StandardScaler()\nscalar.fit(x_train)\nx_train_norm = scalar.transform(x_train)\n\npca = PCA(n_components = 50).fit(x_train_norm)\nx_train_transform = pca.transform(x_train_norm)\n\nx_val_transform = pca.transform(scalar.transform(x_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_final = xgb.DMatrix(x_train_transform, label = y_train)\nval_final = xgb.DMatrix(x_val_transform, label = y_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def objective(trial):\n    n_estimators = trial.suggest_int('n_estimators', 100, 500)\n    max_depth = trial.suggest_int('max_depth', 5, 10)\n    lr = trial.suggest_loguniform('lr', 1e-5, 1e-0)\n    gamma = trial.suggest_int('gamma', 0, 10)\n    tree_method = trial.suggest_categorical('tree_method', ['auto', 'exact','approx', 'hist', 'gpu_hist'])\n    \n    params = {\n        'n_estimators': n_estimators,\n        'max_depth': max_depth,\n        'lr': lr,\n        'gamma': gamma,\n        'tree_method': tree_method,\n        'objective': 'binary:logistic'\n    }\n    \n    bst = xgb.train(params, train_final)\n    pred = bst.predict(val_final)\n    y_pred = np.rint(pred)\n    \n    accuracy = accuracy_score(y_val, y_pred)\n    return accuracy\n\nstudy = optuna.create_study(direction = 'maximize')\nstudy.optimize(objective, n_trials = 25, timeout = 600)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trial = study.best_trial\nbest_params = trial.params\nbest_params['tree_method'] = 'gpu_hist'\nbest_params['objective'] = 'binary:logistic'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_classifier = xgb.XGBClassifier(**best_params)\nxgb_classifier.fit(x_train_transform, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot how the best accuracy evolves with number of trials\nfig = optuna.visualization.plot_optimization_history(study)\nfig.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can also plot the relative importance of different hyperparameter settings\nfig = optuna.visualization.plot_param_importances(study)\nfig.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We impute the missing values with the medians\ndef fillna_npwhere(array, values):\n    if np.isnan(array.sum()):\n        array = np.where(np.isnan(array), values, array)\n    return array","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import janestreet\nenv = janestreet.make_env()\niter_test = env.iter_test()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for (df_test, sample_prediction_df) in iter_test:\n    wt = df_test.iloc[0].weight\n    if(wt == 0):\n        sample_prediction_df.action = 0 \n    else:\n        sample_prediction_df.action = xgb_classifier.predict(pca.transform(scalar.transform(fillna_npwhere(df_test[features].values,f_median[features].values))))\n    env.predict(sample_prediction_df)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}