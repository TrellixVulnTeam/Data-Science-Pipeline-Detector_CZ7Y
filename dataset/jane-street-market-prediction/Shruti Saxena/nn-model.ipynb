{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport gc\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import StratifiedKFold\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/jane-street-market-prediction/train.csv')\n#train = reduce_mem_usage(train)\nfeatures = [c for c in train.columns if 'feature' in c]\n\nNAN_VALUE = -999","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.astype({c: np.float32 for c in train.select_dtypes(include='float16').columns}) \ntrain = train.fillna(train.mean())\nf_mean = np.mean(train[features[1:]].values,axis=0)\ntrain = train.query('date > 85').reset_index(drop = True)\ntrain = train[train.weight != 0]\nn_folds = 5\nseed = 2020\nskf = StratifiedKFold(n_splits=n_folds, shuffle=False)\nresp_cols = ['resp', 'resp_1', 'resp_2', 'resp_3', 'resp_4']\n\nX_train = train[train['date'] < 400][features]\nX_test = train[train['date'] >= 400][features]\n\ny_train = np.stack([(train[train['date'] < 400][c] > 0).astype('int') for c in resp_cols]).T\ny_test = np.stack([(train[train['date'] >= 400][c] > 0).astype('int') for c in resp_cols]).T\n\n\nX_train = train[features]\ny_train = np.stack([(train[c] > 0).astype('int') for c in resp_cols]).T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TUNNING = False\ndef create_model(hp,input_dim,output_dim):\n    inputs = tf.keras.layers.Input(input_dim)\n    x = tf.keras.layers.BatchNormalization()(inputs)\n    x = tf.keras.layers.GaussianNoise(hp.Choice('noise',[0.0,0.03,0.05]))(x)\n    x = tf.keras.layers.Dropout(hp.Choice('init_dropout',[0.0,0.3,0.5]))(x)    \n    x = tf.keras.layers.Dense(hp.Int('num_units_1', 128, 2048, 64), activation=hp.Choice('activation_1', ['tanh','relu','swish']))(x)\n    x = tf.keras.layers.Dropout(hp.Choice(f'dropout_1',[0.0,0.3,0.5]))(x)\n    x = tf.keras.layers.Dense(hp.Int('num_units_2', 128, 1024, 32), activation=hp.Choice('activation_2', ['tanh','relu','swish']))(x)\n    x = tf.keras.layers.Dropout(hp.Choice(f'dropout_2',[0.0,0.3,0.5]))(x)\n    x = tf.keras.layers.Dense(output_dim, activation='sigmoid')(x)\n    model = tf.keras.models.Model(inputs=inputs,outputs=x)\n    model.compile(optimizer=tf.keras.optimizers.Adam(hp.Choice('lr',[1e-2, 1e-3, 1e-5])),loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=hp.Choice('label_smoothing',[0.0, 0.01, 0.1])),metrics=[tf.keras.metrics.AUC(name = 'auc')])\n    return model\n\nmodel = tf.keras.Sequential([\n    tf.keras.Input(shape = len(features)),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.GaussianNoise(0.05),\n    tf.keras.layers.Dropout(0.3),        \n    tf.keras.layers.Dense(256, activation='tanh'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.3),\n    tf.keras.layers.Dense(64, activation='tanh'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.3),\n    tf.keras.layers.Dense(256, activation='tanh'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.3),   \n    tf.keras.layers.Dense(5, activation = 'sigmoid')\n  ])\n\nEPOCHS = 50\nBATCH_SIZE = 4096\n\nif TUNNING:\n    import kerastuner as kt\n    EPOCHS = 50\n    MAX_TRIAL = 20\n    model_fn = lambda hp: create_model(hp, X_train.shape[-1], y_train.shape[-1])\n    tuner = kt.tuners.BayesianOptimization(model_fn, kt.Objective('val_auc', direction='max'), MAX_TRIAL, seed = 2020)\n    tuner.search(X_train, y_train, epochs=EPOCHS, validation_data=(X_test, y_test),callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = 10, restore_best_weights=True)])\n    model = tuner.get_best_models()[0]\nelse:\n    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n    #optimizer = tf.keras.optimizers.RMSprop()\n    loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=1e-2)\n    model.compile(loss = loss, optimizer=optimizer, metrics=[tf.keras.metrics.AUC()])\n    #history = model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks=[callback], validation_data=(X_test, y_test)) \n    history = model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS)\n#0.63","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import janestreet\nfrom tqdm.notebook import tqdm\n#janestreet.competition.make_env.__called__ = False\nenv = janestreet.make_env()\niter_test = env.iter_test()\nfor (test_df, sample_prediction_df) in tqdm(iter_test):\n    if test_df['weight'].item() > 0:\n        x_tt = test_df.loc[:, features].values\n        if np.isnan(x_tt[:, 1:].sum()):\n            x_tt[:, 1:] = np.nan_to_num(x_tt[:, 1:]) + np.isnan(x_tt[:, 1:]) * f_mean\n            \n        action = np.mean(model(x_tt, training = False).numpy()[0])\n       \n        if (action > 0.5):\n            sample_prediction_df.action = 1\n        else:\n            sample_prediction_df.action = 0 \n    else:\n        sample_prediction_df.action = 0 \n    env.predict(sample_prediction_df)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}