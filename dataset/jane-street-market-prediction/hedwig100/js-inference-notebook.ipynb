{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Summary"},{"metadata":{},"cell_type":"markdown","source":"### First Submission \n- 3*2ensemble model \n- SimpleNN,DenseNet,with Neutralization and CNN.\n- Fillna with 0 => FeatureNeutralization => NN.\n- Fillna with 0 => FeatureNeutralization => DenseNet. \n- Fillna with mean => FeatureNeutralization => CNN.\n- For each of these 3 models, I used model trained in last fold, and model trained with all data(except for weight = 0 and first 85days)\n\n### Second Submission \n- 4*2ensemble model \n- FirstSubmission + pytorch model \n- pytorch model's architecture is the same as [This Notebook](https://www.kaggle.com/a763337092/pytorch-resnet-starter-training), but trained with my CV Strategy. \n\n### Scores \nI forgot PublicLB for these submittions...\n\n|        | FirstSub | SecondSub |\n| :---:  |   :---:  |   :---:   |\n|PublicLB| 8000~9000| 8000~9000 |\n|FirstRun| 4717.338 | 5100.367  |\n|SecondRun|4950.418|5417.607|\n\n\n### Training \ntraining code is [here](). Only densenet was trained in that notebook, but other models are trained almost same way. "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os,gc,pickle,requests,json\nimport numpy as np\nfrom numba import njit \nimport pandas as pd \nimport matplotlib.pyplot as plt \n\nfrom hyperopt import hp,fmin,tpe,Trials \nfrom hyperopt.pyll.base import scope \n\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import roc_auc_score,roc_curve \nfrom sklearn.decomposition import PCA \nfrom sklearn.preprocessing import StandardScaler,QuantileTransformer \n\nfrom matplotlib.colors import ListedColormap \nfrom tqdm.notebook import tqdm \nfrom joblib import dump,load\n\nimport tensorflow_addons as tfa\nimport tensorflow as tf \nimport tensorflow.keras.layers as L \nimport tensorflow.keras.backend as B  \nimport tensorflow.keras.optimizers as O\nimport tensorflow.keras.activations as A\nimport tensorflow.keras.metrics as M \nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import Callback,ReduceLROnPlateau,ModelCheckpoint,EarlyStopping\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nfrom torch.nn import CrossEntropyLoss, MSELoss\nfrom torch.nn.modules.loss import _WeightedLoss\nimport torch.nn.functional as F\n\npd.set_option('display.max_columns', 100)\npd.set_option('display.max_rows', 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"FEATURES = [f\"feature_{i}\" for i in range(130)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# For Preprocess"},{"metadata":{"trusted":true},"cell_type":"code","source":"class NeutralizeTransform:\n    def __init__(self,proportion=1.0):\n        self.proportion = proportion\n    \n    def fit(self,X,y):\n        self.lms = []\n        self.mean_exposure = np.mean(y,axis=0)\n        self.y_shape = y.shape[-1]\n        for x in X.T:\n            scores = x.reshape((-1,1))\n            exposures = y\n            exposures = np.hstack((exposures, np.array([np.mean(scores)] * len(exposures)).reshape(-1, 1)))\n            \n            transform = np.linalg.lstsq(exposures, scores, rcond=None)[0]\n            self.lms.append(transform)\n            \n    def transform(self,X,y=None):\n        out = []\n        for i,transform in enumerate(self.lms):\n            x = X[:,i]\n            scores = x.reshape((-1,1))\n            exposures = np.repeat(self.mean_exposure,len(x),axis=0).reshape((-1,self.y_shape))\n            exposures = np.concatenate([exposures,np.array([np.mean(scores)] * len(exposures)).reshape((-1,1))],axis=1)\n            correction = self.proportion * exposures.dot(transform)\n            out.append(x - correction.ravel())\n        return np.asarray(out).T\n    \n    def fit_transform(self,X,y):\n        self.fit(X,y)\n        return self.transform(X,y)\n\ndef create_autoencoder(input_dim,output_dim,hidden_units,dropout_rate,learning_rate,noise_ratio = 0.05):\n    i = tf.keras.layers.Input(input_dim)\n    encoded = tf.keras.layers.BatchNormalization()(i) \n    encoded = tf.keras.layers.GaussianNoise(noise_ratio)(encoded)\n    encoded = tf.keras.layers.Dense(hidden_units[0],activation = \"relu\")(encoded)\n    decoded = tf.keras.layers.Dropout(dropout_rate[0])(encoded) \n    decoded = tf.keras.layers.Dense(input_dim,name = \"decoded\")(decoded)\n    x = tf.keras.layers.Dense(hidden_units[1],activation=\"relu\")(decoded)\n    x = tf.keras.layers.BatchNormalization()(x) \n    x = tf.keras.layers.Dropout(dropout_rate[1])(x) \n    x = tf.keras.layers.Dense(output_dim,activation = \"sigmoid\",name = \"label_output\")(x) \n\n    encoder = Model(inputs = i,outputs = decoded)\n    autoencoder = Model(inputs = i,outputs = [decoded,x])\n\n    autoencoder.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate),\n                        loss = {\"decoded\" : \"mse\",\"label_output\":\"binary_crossentropy\"})\n    return autoencoder,encoder","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Models"},{"metadata":{},"cell_type":"markdown","source":"## Simple NN"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model(input_dim,output_dim,n_layers,hidden_units,dropout_rate,learning_rate,label_smoothing,encoder):\n    inputs = tf.keras.layers.Input(input_dim)\n    \n    x = encoder(inputs)\n    x = tf.keras.layers.Concatenate()([x,inputs])\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout(dropout_rate[0])(x)\n    \n    for i in range(n_layers):\n        x = tf.keras.layers.Dense(hidden_units[i])(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Lambda(tf.keras.activations.relu)(x)\n        x = tf.keras.layers.Dropout(dropout_rate[i+1])(x)\n    x = tf.keras.layers.Dense(output_dim,activation='sigmoid')(x)\n    model = Model(inputs=inputs,outputs=x)\n    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate),\n                  loss = tf.keras.losses.BinaryCrossentropy(label_smoothing = label_smoothing),\n                  metrics = [tf.keras.metrics.AUC(name = 'auc')])\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## DenseNet"},{"metadata":{"trusted":true},"cell_type":"code","source":"def mish(x):\n    return tf.keras.layers.Lambda(lambda x: x*B.tanh(B.softplus(x)))(x)\ntf.keras.utils.get_custom_objects().update({'mish': tf.keras.layers.Activation(mish)})\n\ndef create_densenet(input_dim,output_dim,n_layers,hidden_units,dropout_rate,learning_rate,label_smoothing,encoder):\n    inp = tf.keras.layers.Input(input_dim)\n    tmp = encoder(inp)\n    tmp = tf.keras.layers.Concatenate()([inp,tmp]) \n    tmp = tf.keras.layers.BatchNormalization()(tmp)\n    xs = [tmp]\n    for i in range(n_layers):\n        if len(xs) > 1:\n            tmp = tf.keras.layers.Concatenate(axis=-1)(xs)\n        else:\n            tmp = xs[0]\n        tmp = tf.keras.layers.Dense(hidden_units[i],activation='mish')(tmp)\n        tmp = tf.keras.layers.BatchNormalization()(tmp)\n        tmp = tf.keras.layers.Dropout(dropout_rate[i])(tmp)\n        xs.append(tmp)\n    \n    output = tf.keras.layers.Dense(output_dim,activation='sigmoid')(tf.keras.layers.Concatenate()(xs))\n    model = tf.keras.models.Model(inp,output)\n    optimizer = tfa.optimizers.RectifiedAdam(learning_rate = learning_rate) \n    model.compile(optimizer, loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=label_smoothing),\n                    metrics=[tf.keras.metrics.AUC(name='auc')])\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## CNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_1dcnn(input_dim, output_dim,n_layers,hidden_units,dropout_rate,learning_rate,label_smoothing,encoder):\n    inputs = tf.keras.layers.Input(input_dim)\n    \n    x = encoder(inputs)\n    x = tf.keras.layers.Concatenate()([x,inputs]) \n    x = tf.keras.layers.BatchNormalization()(x) \n    x = tf.keras.layers.Dense(1024,activation = \"relu\")(x) \n    x = tf.keras.layers.Reshape((32,32))(x) \n    x = tf.keras.layers.Conv1D(filters=16,\n                               kernel_size=7,\n                               strides=1,\n                               activation='relu')(x)\n    x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)\n    x = tf.keras.layers.Flatten()(x)\n    \n    for i in range(n_layers-1):\n        x = tf.keras.layers.Dense(hidden_units[i], activation='relu')(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Dropout(dropout_rate[i])(x)\n    x = tf.keras.layers.Dense(output_dim, activation='sigmoid')(x)\n    model = tf.keras.models.Model(inputs=inputs,outputs=x)\n    \n    # compile\n    opt = tfa.optimizers.RectifiedAdam(learning_rate=learning_rate)\n    opt = tfa.optimizers.SWA(opt)\n    loss = tf.keras.losses.BinaryCrossentropy(label_smoothing=label_smoothing)\n    model.compile(optimizer=opt, \n                  loss=loss, \n                  metrics=[tf.keras.metrics.AUC(name = 'auc')])\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Torch Model "},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"all_feat_cols = [f\"feature_{i}\" for i in range(130)] + [\"cross_41_42_43\",\"cross_1_2\"]\ntarget_cols = ['action', 'action_1', 'action_2', 'action_3', 'action_4']\n\nclass torch_Model(nn.Module):\n    def __init__(self):\n        super(torch_Model, self).__init__()\n        self.batch_norm0 = nn.BatchNorm1d(len(all_feat_cols))\n        self.dropout0 = nn.Dropout(0.2)\n\n        dropout_rate = 0.2\n        hidden_size = 256\n        self.dense1 = nn.Linear(len(all_feat_cols), hidden_size)\n        self.batch_norm1 = nn.BatchNorm1d(hidden_size)\n        self.dropout1 = nn.Dropout(dropout_rate)\n\n        self.dense2 = nn.Linear(hidden_size+len(all_feat_cols), hidden_size)\n        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n        self.dropout2 = nn.Dropout(dropout_rate)\n\n        self.dense3 = nn.Linear(hidden_size+hidden_size, hidden_size)\n        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n        self.dropout3 = nn.Dropout(dropout_rate)\n\n        self.dense4 = nn.Linear(hidden_size+hidden_size, hidden_size)\n        self.batch_norm4 = nn.BatchNorm1d(hidden_size)\n        self.dropout4 = nn.Dropout(dropout_rate)\n\n        self.dense5 = nn.Linear(hidden_size+hidden_size, len(target_cols))\n\n        self.Relu = nn.ReLU(inplace=True)\n        self.PReLU = nn.PReLU()\n        self.LeakyReLU = nn.LeakyReLU(negative_slope=0.01, inplace=True)\n        # self.GeLU = nn.GELU()\n        \n        self.RReLU = nn.RReLU()\n    \n    def forward(self, x):\n        x = self.batch_norm0(x)\n        x = self.dropout0(x)\n\n        x1 = self.dense1(x)\n        x1 = self.batch_norm1(x1)\n        # x = F.relu(x)\n        # x = self.PReLU(x)\n        x1 = self.LeakyReLU(x1)\n        x1 = self.dropout1(x1)\n\n        x = torch.cat([x, x1], 1)\n\n        x2 = self.dense2(x)\n        x2 = self.batch_norm2(x2)\n        # x = F.relu(x)\n        # x = self.PReLU(x)\n        x2 = self.LeakyReLU(x2)\n        x2 = self.dropout2(x2)\n\n        x = torch.cat([x1, x2], 1)\n\n        x3 = self.dense3(x)\n        x3 = self.batch_norm3(x3)\n        # x = F.relu(x)\n        # x = self.PReLU(x)\n        x3 = self.LeakyReLU(x3)\n        x3 = self.dropout3(x3)\n        \n        x = torch.cat([x2, x3], 1)\n\n        x4 = self.dense4(x)\n        x4 = self.batch_norm4(x4)\n        # x = F.relu(x)\n        # x = self.PReLU(x)\n        x4 = self.LeakyReLU(x4)\n        x4 = self.dropout4(x4)\n\n        x = torch.cat([x3, x4], 1)\n\n        x = self.dense5(x)\n\n        return x","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# For Load weights"},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_encoder(params,path):\n    input_dim = 130 \n    output_dim = 5 \n    hidden_units = [params[f\"hidden_units{i}\"] for i in range(2)]  \n    dropout_rate = [params[f\"dropout_rate{i}\"] for i in range(2)]\n    learning_rate = params[\"learning_rate\"] \n\n    autoencoder,encoder = create_autoencoder(input_dim,output_dim,hidden_units,dropout_rate,learning_rate,noise_ratio=params[\"noise_ratio\"]) \n    encoder.load_weights(path)\n    return encoder \n\ndef load_nn_weights(params,encoder,path):\n    input_dim = 130 \n    output_dim = 5 \n    n_layers = params[\"n_layers\"] \n    hidden_units = [params[f\"hidden_units{i}\"] for i in range(n_layers)]\n    dropout_rate = [params[f\"dropout_rate{i}\"] for i in range(n_layers + 1)]\n    learning_rate = params[\"learning_rate\"]\n    label_smoothing = params[\"label_smoothing\"]\n    \n    model = create_model(input_dim,output_dim,n_layers,hidden_units,dropout_rate,learning_rate,label_smoothing,encoder)\n    model.load_weights(path) \n    return model \n\ndef load_densenet_weights(params,encoder,path):\n    input_dim = 130 \n    output_dim = 5 \n    n_layers = params[\"n_layers\"] \n    hidden_units = [params[f\"hidden_units{i}\"] for i in range(n_layers)]\n    dropout_rate = [params[f\"dropout_rate{i}\"] for i in range(n_layers + 1)]\n    learning_rate = params[\"learning_rate\"]\n    label_smoothing = params[\"label_smoothing\"]\n    \n    model = create_densenet(input_dim,output_dim,n_layers,hidden_units,dropout_rate,learning_rate,label_smoothing,encoder)\n    model.load_weights(path) \n    return model\n\ndef load_cnn_weights(params,encoder,path):\n    input_dim = 130 \n    output_dim = 5\n    n_layers = params[\"n_layers\"] \n    hidden_units = [params[f\"hidden_units{i}\"] for i in range(n_layers)]\n    dropout_rate = [params[f\"dropout_rate{i}\"] for i in range(n_layers + 1)]\n    learning_rate = params[\"learning_rate\"]\n    label_smoothing = params[\"label_smoothing\"]\n    \n    model = create_1dcnn(input_dim,output_dim,n_layers,hidden_units,dropout_rate,learning_rate,label_smoothing,encoder)\n    model.load_weights(path) \n    return model ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Params"},{"metadata":{"trusted":true},"cell_type":"code","source":"# params for encoder \nparam_encode = {\"output_dim\": 5,\n                \"seed\" :42,\n                \"noise_ratio\" : 0.05,  \n                \"epochs\" : 200, \n                \"batch_size\" : 4096, \n                \"hidden_units0\" : 64,\n                \"hidden_units1\" : 32,\n                \"learning_rate\" : 0.001}            \nfor i in range(2):\n    param_encode[f\"dropout_rate{i}\"] = 0.2\n\n# params for simple nn\nparams_predict_nn = {\"output_dim\" : 5,\n                      \"n_layers\" : 5,\n                      \"learning_rate\" : 0.001,\n                      \"label_smoothing\" : 0.056961346402280545,\n                      \"seed\" : 1983,\n                      \"epochs\" : 500,\n                      \"batch_size\" : 16382,\n                      \"tune_epochs\" : 3,\n                      \"nfold\" : 5}    \n\nhidden = [256] + [64]*4\ndrop = [0.03228315981516344,0.34046464288383715,0.2476448175592981] + [0.2]*3\nfor i in range(params_predict_nn[\"n_layers\"]):\n    params_predict_nn[f\"hidden_units{i}\"] = hidden[i] \nfor i in range(params_predict_nn[\"n_layers\"]+1):\n    params_predict_nn[f\"dropout_rate{i}\"] = drop[i]\n    \n# params for densenet \nparams_predict_best = {\"output_dim\" : 5,\n                      \"n_layers\" : 10,\n                      \"learning_rate\" : 0.001,\n                      \"label_smoothing\" : 0.056961346402280545,\n                      \"seed\" : 20,\n                      \"epochs\" : 1000,\n                      \"batch_size\" : 4096,\n                      \"tune_epochs\" : 3,\n                      \"nfold\" : 5,}    \n\nhidden = [64]*20  \ndrop = [0.1] + [0.2]*20\nfor i in range(params_predict_best[\"n_layers\"]):\n    params_predict_best[f\"hidden_units{i}\"] = hidden[i] \nfor i in range(params_predict_best[\"n_layers\"]+1):\n    params_predict_best[f\"dropout_rate{i}\"] = drop[i]\n\n# params for cnn \nparams_predict_cnn = {\"output_dim\" : 5,\n                      \"n_layers\" : 5,\n                      \"learning_rate\" : 0.001,\n                      \"label_smoothing\" : 0.056961346402280545,\n                      \"seed\" : 2091,\n                      \"epochs\" : 500,\n                      \"batch_size\" : 4096,\n                      \"tune_epochs\" : 3,\n                      \"nfold\" : 5}    \n\nhidden = [64]*5\ndrop = [0.2]*6\nfor i in range(params_predict_cnn[\"n_layers\"]):\n    params_predict_cnn[f\"hidden_units{i}\"] = hidden[i] \nfor i in range(params_predict_cnn[\"n_layers\"]+1):\n    params_predict_cnn[f\"dropout_rate{i}\"] = drop[i]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prepare Submition"},{"metadata":{},"cell_type":"markdown","source":"## Preprocess"},{"metadata":{"trusted":true},"cell_type":"code","source":"NTs = pickle.load(open(\"../input/janestreetdata/NeutralizeTransform_025.pkl\",\"rb\"))\nf_mean = np.load(\"../input/js-nn-models/f_mean.npy\")\nf_mean = np.concatenate(([0.5],f_mean))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_f(x_tt):\n    x_tt = np.nan_to_num(x_tt)\n    x_tt[:,1:] = np.hstack(tuple(nt.transform(x_tt[:,i+1].reshape(-1,1)).ravel() for i,nt in enumerate(NTs)))\n    return x_tt  \n \n@njit\ndef fast_fillna(array, values):\n    if np.isnan(array.sum()):\n        array = np.where(np.isnan(array), values, array)\n    return array","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Models"},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder5 = load_encoder(param_encode,\"../input/nnmodels/encoder_weight5.hdf5\")\nencoder5.trainable = False\nencoder6 = load_encoder(param_encode,\"../input/trial1/encoder6.hdf5\")\nencoder6.trainable = False\nencoder3 = load_encoder(param_encode,\"../input/nnmodels/encoder_weight3.hdf5\")\nencoder3.trainable = False\nencoder2 = load_encoder(param_encode,\"../input/nnmodels/encoder_weight2.hdf5\")\nencoder2.trainable = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1_1 = load_densenet_weights(params_predict_best,encoder3,\"../input/nnmodels/nn_trainall2.hdf5\") # 8118.784\nmodel1_2 = load_nn_weights(params_predict_nn,encoder2,\"../input/nnmodels/nn_trainall4.hdf5\") # 5759.227\nmodel2_3 = load_cnn_weights(params_predict_cnn,encoder5,\"../input/nnmodels/nn_trainall6.hdf5\") # 6424.009 \n\nmodel1_1_b = load_densenet_weights(params_predict_best,encoder3,\"../input/nnmodels/nn36ver2_4.hdf5\") # 6834.318\nmodel1_2_b = load_nn_weights(params_predict_nn,encoder2,\"../input/nnmodels/nn50ver2_4.hdf5\") # 6620.780\nmodel2_3_b = load_cnn_weights(params_predict_cnn,encoder5,\"../input/nnmodels/nn52_4.hdf5\") # 7131.743","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.config.optimizer.set_jit(True)\nmodel_sub1_1 = tf.function(model1_1.call,experimental_relax_shapes = True)\nmodel_sub1_2 = tf.function(model1_2.call,experimental_relax_shapes = True)\nmodel_sub2_3 = tf.function(model2_3.call,experimental_relax_shapes = True)\n\nmodel_sub1_1_b = tf.function(model1_1_b.call,experimental_relax_shapes = True)\nmodel_sub1_2_b = tf.function(model1_2_b.call,experimental_relax_shapes = True)\nmodel_sub2_3_b = tf.function(model2_3_b.call,experimental_relax_shapes = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda:0\")\ntorch.cuda.empty_cache()\nmodel_torch = torch_Model()\nmodel_torch.to(device)\nmodel_weights = \"../input/pytorch-model/pytorch_trainall_42.pth\" #6847.578\nmodel_torch.load_state_dict(torch.load(model_weights))\nmodel_torch.eval()\n\ntorch.cuda.empty_cache()\nmodel_torch_b = torch_Model()\nmodel_torch_b.to(device)\nmodel_weights = \"../input/pytorch-model/online_model4.pth\" #6549.154\nmodel_torch_b.load_state_dict(torch.load(model_weights))\nmodel_torch_b.eval()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{},"cell_type":"markdown","source":"This submission is second submission version. "},{"metadata":{"trusted":true},"cell_type":"code","source":"import janestreet\nenv = janestreet.make_env() \nth = 0.505\ntest_df_columns = ['weight'] + [f'feature_{i}' for i in range(130)] + ['date']\nindex_features = [n for n,col in enumerate(test_df_columns) if col in FEATURES]\nf = np.median \ng = np.dot \nweight = np.array([0.10,0.05,0.17,0.13,0.20,0.15,0.13,0.07]) # 0.15 0.30 0.35 0.20 \n# weight = np.array([0.25,0.15,0.13,0.07,0.25,0.15]) first submission weight \n\nfor (test_df,pred_df) in tqdm(env.iter_test()):\n    if test_df['weight'].values[0] > 0:\n        x_tt = test_df.values[0][index_features].reshape(1,-1)\n        x_tt_f = preprocess_f(x_tt)\n        pred = [0]*8 \n        pred[0] = f(model_sub1_1(x_tt_f)) \n        pred[1] = f(model_sub1_1_b(x_tt_f))\n        pred[2] = f(model_sub1_2(x_tt_f)) \n        pred[3] = f(model_sub1_2_b(x_tt_f)) \n        x_tt[0,:] = fast_fillna(x_tt[0,:],f_mean) \n        pred[4] = f(model_sub2_3(x_tt))\n        pred[5] = f(model_sub2_3_b(x_tt))\n        \n        #torch \n        cross_41_42_43 = x_tt[:, 41] + x_tt[:, 42] + x_tt[:, 43]\n        cross_1_2 = x_tt[:, 1] / (x_tt[:, 2] + 1e-5)\n        feature_inp = np.concatenate((\n            x_tt,\n            np.array(cross_41_42_43).reshape(x_tt.shape[0], 1),\n            np.array(cross_1_2).reshape(x_tt.shape[0], 1),\n        ), axis=1)\n        pred[6] = f(model_torch(torch.tensor(feature_inp, dtype=torch.float).to(device)).sigmoid().detach().cpu().numpy())\n        pred[7] = f(model_torch_b(torch.tensor(feature_inp, dtype=torch.float).to(device)).sigmoid().detach().cpu().numpy())\n        pred = g(pred,weight)\n        pred_df.action = int(pred >= th) \n    else:\n        pred_df[\"action\"].values[0] = 0\n    env.predict(pred_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}