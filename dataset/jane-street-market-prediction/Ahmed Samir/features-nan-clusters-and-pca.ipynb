{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install ../input/python-datatable/datatable-0.11.0-cp37-cp37m-manylinux2010_x86_64.whl > /dev/null 2>&1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport gc\nimport sys\nimport time\nimport tqdm\nimport random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport datatable as dt\nimport matplotlib.pyplot as plt\nplt.style.use('tableau-colorblind10')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"folder_path = '../input/jane-street-market-prediction/'\ntrain_data = dt.fread(folder_path + 'train.csv').to_pandas()\nfeatures = dt.fread(folder_path + 'features.csv').to_pandas()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# first I need the features with highest nan count\ntop_nan_features_vals = train_data.isna().sum().sort_values(ascending=False)\ntop_nan_features = top_nan_features_vals.index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"date_feature_nan = train_data.groupby('date').apply(lambda x: x.isna().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"date_feature_nan[top_nan_features[:10]].plot(figsize=(12, 6), subplots=True);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Why do features miss values together?"},{"metadata":{"trusted":true},"cell_type":"code","source":"features_and_date = list(top_nan_features[:10]) + ['date']\ntrain_data[features_and_date].query('date == 0').plot(subplots=True, figsize=(12, 8));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data[features_and_date].query('date == 1').plot(subplots=True, figsize=(12, 8));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data[features_and_date].query('date == 2').plot(subplots=True, figsize=(12, 8));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data[features_and_date].query('date == 3').plot(subplots=True, figsize=(12, 8));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data[features_and_date].query('date == 4').plot(subplots=True, figsize=(12, 8));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data[features_and_date].query('date == 5').plot(subplots=True, figsize=(12, 8));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see by visualizaing the features with top 10 NaN value counts in the first 5 days they show a consistent pattern of missing values which is always present in the beginning of the day, and then another one in the beginning of the last third.\n\nIf we keep visualizing different dates we would find the same pattern happening. But why does that happen? Is it just noise or is this some meaningful aspect of the data?\n\nLet's take a full of all the features missing data pattern over the first day to get a clearer picture."},{"metadata":{"trusted":true},"cell_type":"code","source":"top_nan_features_vals[:60]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If we just look into the number of missing values we can see that features cluster with each other. And If we visualize them with this order we can see the clusters clearly."},{"metadata":{"trusted":true},"cell_type":"code","source":"features_and_date = list(top_nan_features[:60]) + ['date']\ntrain_data[features_and_date].query('date == 0').plot(subplots=True, figsize=(12, 8), legend=False);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data[features_and_date].query('date == 1').plot(subplots=True, figsize=(12, 8), legend=False);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data[features_and_date].query('date == 136').plot(subplots=True, figsize=(12, 8), legend=False);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Testing PCA of feature clusters\n\nI'll define the clusters based on similar patterns in NaN values, and I'll first test the idea with the top 14 features."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.impute import SimpleImputer\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n#define first cluster features and fill na with strategy (ffill)\ncluster_1_features = top_nan_features[:14]\ntrain_data.loc[:, cluster_1_features] = train_data.loc[:, cluster_1_features].fillna(method='ffill').fillna(0)\n\n# define pipeline\nsteps = [('scaler', MinMaxScaler()), ('pca', PCA(random_state=42))]\npipeline = Pipeline(steps=steps)\n\npipeline.fit(train_data.loc[:, cluster_1_features])\n\nplt.figure(figsize=(10, 6))\nplt.plot(list(range(1, len(cluster_1_features)+1)), pipeline['pca'].explained_variance_ratio_.cumsum());\nplt.xlabel('No. of Features')\nplt.ylabel('Explained Variance')\nplt.axvline(5, color='r', linestyle='--');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can use 5 features to explain more than 90% of the variance of the original cluster."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\n# second cluster\ncluster_2_features = top_nan_features[14:31]\n\ntrain_data.loc[:, cluster_2_features] = train_data.loc[:, cluster_2_features].fillna(method='ffill').fillna(0)\n\n# define pipeline\nsteps = [('scaler', MinMaxScaler()), ('pca', PCA(random_state=42))]\npipeline = Pipeline(steps=steps)\n\npipeline.fit(train_data.loc[:, cluster_2_features])\n\nplt.figure(figsize=(10, 6))\nplt.plot(list(range(1, len(cluster_2_features)+1)), pipeline['pca'].explained_variance_ratio_.cumsum());\nplt.xlabel('No. of Features')\nplt.ylabel('Explained Variance')\nplt.axvline(7, color='r', linestyle='--');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"7 components in the second cluster is suitable for explaining 95% of the variance."},{"metadata":{"trusted":true},"cell_type":"code","source":"# third cluster\ncluster_3_features = top_nan_features[31:47]\n\ntrain_data.loc[:, cluster_3_features] = train_data.loc[:, cluster_3_features].fillna(method='ffill').fillna(0)\n\n# define pipeline\nsteps = [('scaler', MinMaxScaler()), ('pca', PCA(random_state=42))]\npipeline = Pipeline(steps=steps)\n\npipeline.fit(train_data.loc[:, cluster_3_features])\n\nplt.figure(figsize=(10, 6))\nplt.plot(list(range(1, len(cluster_3_features)+1)), pipeline['pca'].explained_variance_ratio_.cumsum());\nplt.xlabel('No. of Features')\nplt.ylabel('Explained Variance')\nplt.axvline(7, color='r', linestyle='--');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"7 components in the third cluster is suitable for explaining 95% of the variance."},{"metadata":{"trusted":true},"cell_type":"code","source":"# fourth cluster\ncluster_4_features = top_nan_features[47:63]\n\ntrain_data.loc[:, cluster_4_features] = train_data.loc[:, cluster_4_features].fillna(method='ffill').fillna(0)\n\n# define pipeline\nsteps = [('scaler', MinMaxScaler()), ('pca', PCA(random_state=42))]\npipeline = Pipeline(steps=steps)\n\npipeline.fit(train_data.loc[:, cluster_4_features])\n\nplt.figure(figsize=(10, 6))\nplt.plot(list(range(1, len(cluster_4_features)+1)), pipeline['pca'].explained_variance_ratio_.cumsum());\nplt.xlabel('No. of Features')\nplt.ylabel('Explained Variance')\nplt.axvline(6, color='r', linestyle='--');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"6 components in the fourth cluster is almost suitable for explaining around 94% of the variance."},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking out the last features with NaNs\nfeatures_and_date = list(top_nan_features[63:87]) + ['date', 'ts_id']\ntrain_data[features_and_date].query('date == 0 and ts_id > 3000 and ts_id < 4000').plot(subplots=True, figsize=(12, 8), legend=False);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that among these features, some are nearly identical, which could merit the use of PCA."},{"metadata":{"trusted":true},"cell_type":"code","source":"# fifth cluster\ncluster_5_features = top_nan_features[63:87]\n\ntrain_data.loc[:, cluster_5_features] = train_data.loc[:, cluster_5_features].fillna(method='ffill').fillna(0)\n\n# define pipeline\nsteps = [('scaler', MinMaxScaler()), ('pca', PCA(random_state=42))]\npipeline = Pipeline(steps=steps)\n\npipeline.fit(train_data.loc[:, cluster_5_features])\n\nplt.figure(figsize=(10, 6))\nplt.plot(list(range(1, len(cluster_5_features)+1)), pipeline['pca'].explained_variance_ratio_.cumsum());\nplt.xlabel('No. of Features')\nplt.ylabel('Explained Variance')\nplt.axvline(7, color='r', linestyle='--');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"7 components in the fourth cluster is almost suitable for explaining around 94% of the variance."},{"metadata":{},"cell_type":"markdown","source":"**Now maybe these extracted features could be used along all features or along the non NaN features.**"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}