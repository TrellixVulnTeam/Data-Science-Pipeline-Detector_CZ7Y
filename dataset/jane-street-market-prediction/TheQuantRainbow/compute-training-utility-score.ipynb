{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Introduction\nBesides using loss or AUC to optmize the model,\nthis notebook provide another way to fine tune.\n1. Using Pytorch Dataloader to read data.\n2. Save model weight every n epochs.\n3. Compute utility score with respect to each weight.  "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Concatenate, Lambda, GaussianNoise, Activation\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers.experimental.preprocessing import Normalization\nfrom keras.callbacks import ModelCheckpoint\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom random import choices\nimport dask.dataframe as dd\nfrom glob import glob\nimport os","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`create_mlp` is copied from <a link='https://www.kaggle.com/tarlannazarov/own-jane-street-with-keras-nn'>\nOWN Jane Street with Keras NN<a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_mlp(\n    num_columns, num_labels, hidden_units, dropout_rates, label_smoothing, learning_rate\n):\n\n    inp = tf.keras.layers.Input(shape=(num_columns,))\n    x = tf.keras.layers.BatchNormalization()(inp)\n    x = tf.keras.layers.Dropout(dropout_rates[0])(x)\n    for i in range(len(hidden_units)):\n        x = tf.keras.layers.Dense(hidden_units[i])(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n        x = tf.keras.layers.Dropout(dropout_rates[i + 1])(x)\n\n    x = tf.keras.layers.Dense(num_labels)(x)\n    out = tf.keras.layers.Activation(\"sigmoid\")(x)\n    model = tf.keras.models.Model(inputs=inp, outputs=out)\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n        loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=label_smoothing),\n        metrics=tf.keras.metrics.AUC(name=\"AUC\"),\n    )\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Convert csv file to Parquet File  \nTo save time, we just provide the code here and use ```add data``` to import results.\n```python\ndata = pd.read_csv ('../input/jane-street-market-prediction/train.csv')\nfeatures_columns = ['feature_{}'.format(i) for i in range(1,130)]\nresp_columns = ['resp_{}'.format(i) for i in range(1,5)] + ['resp']\nfor i in tqdm(data['date'].unique()):\n    data[data['date']==i].to_parquet('./date_{}.parquet'.format(i))\n```\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import dask.dataframe as dd\n\ndata = dd.read_parquet('../input/janestreetparquetdata/date*.parquet')\n\nfeatures = ['feature_{}'.format(i) for i in range(130)]\nresp_cols = ['resp_1', 'resp_2', 'resp_3', 'resp', 'resp_4']\n\ntrain = data.compute()\ntrain = train.query('date > 85').reset_index(drop = True) \ntrain = train[train['weight'] != 0]\nf_mean = train[features].mean().values\ntrain = train.dropna()\n\nX_train = train[features].values\ny_train = (train[resp_cols]> 0).astype(int).values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Tensorflow ModelCheckpoint\n\nAdd ModelCheckpoint to record weights."},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.callbacks import ModelCheckpoint\n\nSEED = 1111\ntf.random.set_seed(SEED)\nnp.random.seed(SEED)\n\nhidden_units = [150, 150, 150]\ndropout_rates = [0.2, 0.2, 0.2, 0.2]\nlabel_smoothing = 1e-2\nlearning_rate = 1e-3\n\nepochs = 250\nbatch_size = 5000\nsave_every_n_epochs = 10\nsave_freq = (len(X_train)//batch_size)*save_every_n_epochs\n\nclf = create_mlp(\n    len(features), len(resp_cols), hidden_units, dropout_rates, label_smoothing, learning_rate\n    )\ncheckpoint_path = \"./cp-{epoch:04d}.ckpt\"\ncheckpoint = ModelCheckpoint(checkpoint_path,save_weights_only=True,save_freq=save_freq )\nclf.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,callbacks=[checkpoint])\nclf.save_weights('./SimpleMLP.ckpt')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Pytorch Dataloader\nThe following code creates Pytorch Dataloader.  \nreading the parquet files that has been preprocessed.  \nIf you want to change the output, just modify `__getitem__` function."},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nclass TrainData(Dataset):    \n    def __init__(self,file_name,root_dir,predict=False):\n        self.file_name = file_name\n        self.root_dir = root_dir\n        self.feature = ['feature_{}'.format(i) for i in range(130)]\n        self.resp = ['resp_{}'.format(i) for i in range(1,5)]+['resp']\n        self.prediction = predict\n    def __len__(self):\n        return len(glob(os.path.join(self.root_dir,'*.parquet')))\n    def __getitem__(self, idx):\n        data = pd.read_parquet(os.path.join(self.root_dir,self.file_name+'_{}.parquet'.format(idx)))\n        X = data[self.feature].values\n        W = data['weight'].values\n        R = data['resp'].values\n        return X,W,R","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = TrainData('date','../input/janestreetparquetdata/',predict = True)\nweight_path = glob('./*.ckpt.index')\nweight_path = [os.path.basename(each).split('.')[0] for each in weight_path]\nweight_path.sort()\nfor path in weight_path:\n    clf.load_weights('./{}.ckpt'.format(path))\n    p = []\n    for i in range(len(dataset)):\n        net_input,weight,resp= dataset[i]\n        net_input = np.nan_to_num(net_input)+f_mean*(np.isnan(net_input).astype(int))\n        net_prediction = clf.predict(net_input)\n        pre = (np.median(net_prediction,axis=1)>0.5).astype(int)\n        p.append((weight*resp*pre).sum())\n    result = pd.DataFrame(data={'p':p})\n    result ['p2'] = result['p']**2\n    t = (result['p'].sum()/np.sqrt(result['p2'].sum()))*(np.sqrt(250/len(dataset)))\n    print(path,min(max(t,0),6)*result['p'].sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selection = 'cp-0200'\nclf.load_weights('./{}.ckpt'.format(selection))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import janestreet\nenv = janestreet.make_env()\nth = 0.5\nfor (test_df, pred_df) in tqdm(env.iter_test()):\n    if test_df['weight'].item() > 0:\n        x_tt = test_df.loc[:, features].values\n        x_tt = np.nan_to_num(x_tt)+f_mean*(np.isnan(x_tt).astype(int))\n        pred = np.median(clf(x_tt, training=False))\n        pred_df.action = np.where(pred >= th, 1, 0).astype(int)\n    else:\n        pred_df.action = 0\n    env.predict(pred_df)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}