{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"## For Japanese rookie by Japanese rookie\n# **Jane Street: EDA of day 0 and feature importance**\n\n This notebook is begginers version of \n[Jane Street: EDA of day 0 and feature importance](https://www.kaggle.com/carlmcbrideellis/jane-street-eda-of-day-0-and-feature-importance) made by [Mr.Carl McBride Ellis](https://www.kaggle.com/carlmcbrideellis).\n\nI respect him, and I'm impressed by this notebook!  \nBut I'm a beginner,and too new to understand this instantly.  \nSo, I add supplements for ***beginners*** and Japanese translation.  \nI made this just for my own growth.  \nIf you are a beginner who is confused about understanding his notebook(Same trouble as me), I may be able to help you (especially Japanese).   \n### <span style=\"color: red; \">*Note:If there are copyright issues ,offensive authors, or anything else,this notebook will be deleted immediately.*</span>\nPlease forgive me though it is a childish sentence and English.  \n\n\n\n### ●Description\n「低く買って高く売る」は簡単に見えて、難しい  \n電子取引によりリアルタイムで価格差を見つけ利用できる可能性が無限大  \n全員が合理的とすると、価格はいつも公平な価格になるが、現実はそうではない  \n非効率性を利用する取引戦略を作るのは難しい（良い意思決定orただの幸運）  \n　①膨大な取引データからリターンを最大化する定量的なモデルを作る  \n　②将来の市場returnに対してモデルによる予測をする　　\n   \nhistorical dataを、モデルを作るために数学的手法と技術的手法を用いる  　　  \n　potential trading opportunitiesの数が与えられる　　  \n良いモデルができ正しく取引を実行するとより公正な価格になるが、現実的には厳しい　　  \nJaneStreetはtrading modelsとmachine learning solutionsを作った　　  \n　…収益性の高いopportunitiesを特定し、取引の実行を迅速に決定する　　  \nJSの量的問題をはるかに単純化してるが、この課題の解決の新しいアプローチを求めている  　　\n　　\n### ●Evaluation：a utility score　　　  \ntest setのrow：action value をpredictするa trading opportunityを表す　　  \nsubmission file：python time-series APIを用いる　　  \n　modelが時間通りにpeek forwardしないようになる（？）　　  \n  \n### ●Data Description　　    \nfeature_{0...129}：株市場のデータ　　\n\nrow(列):取引機会\naction value：1なら取引、0ならスルー。これを予測する　　  \nresp:収益率＝投資元本の増加分/投資元本。  \nresp1~4はtime horizon(時間の区切ろい)を変えただけ。モデルの精度を上げるために使う。  \ntest setにはrespのみ。\nweight:取引量。weight=0はscoreの評価に寄与しないが、空白を作らないために意図的に入れてある。高いweightは低いrespのみに関係。  \n \ncolumn(行)  \ndate:取引した日にち  \nts_id:時間の順序  \n"},{"metadata":{},"cell_type":"markdown","source":"0. import：numpy,pandas,plottings,dabl,dabl,misc,system\n\n1. The train.csv file is big：訓練用データサイズが大きい\n\n2. resp(once)：時間の経過に伴う応答の累積値culmative valuesをみる\n\n3. weight：tradeにはweightとrespが重要\n\n4. Cumulative return：累積daily return＝weight * calue of resp\n\n5. Time：各日の、ts_idの数をplots ※垂直破線：85日目付近のモデルの修正？\n\n6. The features\n\n7. The features.csv file：匿名化されたfeaturesに関連するメタデータ、tagは29個\n\n8. Action(1:取引する(resp=+), 0:取引しない(resp=-)、のbinary(2進)column)\n\n9. The first day (\"day 0\")：missing dataにはpattternがある\n\n10. Are there any missing values?\n\n11. Is there any missing data: Days 2 and 294\n\n12. DABL plots (targets: action and resp)\n\n13. Permutation(順序) Importance using the Random Forest\n\n14. Is there any correlation between day 100 and day 200?\n\n15. The test data\n\n16. Evaluation：utility score"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"## 0,import：numpy,pandas,plottings,dabl,misc,system\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#numpy\nimport numpy as np\n\n#pandas stuff\nimport pandas as pd\n#pandasの設定値を変更（第1因数に正規表現パターンの文字列、第2因数に設定する値)\n#display.max_rowsの値をNoneにする\npd.set_option('display.max_rows', None)\n#display.max_columnsの値をNoneにする\npd.set_option('display.max_columns', None)\n\n#plottings stuff\nfrom pandas.plotting import lag_plot\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\n#sns(seaborn)の定性的なカラーパレット\n#as_camp=Trueならば、matplotlib.colors.Colormapを返す\ncolorMap = sns.light_palette(\"blue\", as_cmap=True)\n\n\n#install dabl（→chapter12）\n#pipはPythonパッケージのインストールなどを行うユーティリティ（標準装備）\n#> : 欲しいfileに標準出力でリダイレクトする\n#　→dev/nullにリダイレクトする\n#　→一種のvirtual fileとなり、そこに置いたものが削除される\n#　→ディスク領域を使用しない（守るためのコマンド）\n#　ただし、dev/nullはLinux用なので、nullのみでよい\n\n#!pip install dabl > /dev/null\n!pip install dabl > null\nimport dabl\n#install datatable\n!pip install datatable > /dev/null\nimport datatable as dt\n\n#misc\n#欠損値を効果的に可視化する\nimport missingno as msno\n\n#system\n#警告メッセージは、警告した方が良いときにされるが、別にプログラムを終了させるほどではない\n#（バージョンが古い、など）\n#warnings：警告を管理\nimport warnings\n#実行時の紛らわしい警告を非表示(なぜかpipのwarningは消えない)\nwarnings.filterwarnings('ignore')\n\n#for the image import\nimport os\n#ipython.display：音声や動画などをNotebook内に埋め込む\n#Image：画像を表示\nfrom IPython.display import Image\n\n#garbage collector to keep RAM in check\n#RAMを確認するために不要領域を自動的に解法する機能\nimport gc\n#この後のgc.collect()は、\n#使い終わって必要なくなった（回収可能な）オブジェクト（メモリ領域）を自動的に削減する\n#多分上級者向け\n#コードで使用されていたメモリーがOSに返されるという保証はない\n#あくまで、使用済みメモリーが将来別のオブジェクトで使用するために開放される、ことのみを保証\n#どうしても、「開放されるはずのメモリが開放されない」時に使う\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1,The train.csv file is big：訓練用データサイズが大きい\n→サイズ大で時間がかかるので、datatableを使い、pandas dataframeに変換\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#wc -1 (ファイルパス)：データ数（行数）をカウントするコマンド\n!wc -l ../input/jane-street-market-prediction/train.csv\n#サイズが大きいことがわかる","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# 「%time」：一行の実行時間\n# 「%%time」：セル一つの実行時間 　※このコマンドはセルの一番上に書かないとエラーに\n\n#freadは読み込みが非常に速いため、data.tableとして読み込んで、その後にdataframeに変換\ntrain_data_datatable = dt.fread('../input/jane-street-market-prediction/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n#pandas datatableに変換\n\ntrain_data = train_data_datatable.to_pandas()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2,resp(once)：時間の経過に伴う応答の累積値culmative valuesをみる\nTHが長い→投資家は積極的でリスク高い　TH短い→保守的でリスク低い  \n※TH；TimeHorizon：（株や証券などを保有している）時間の範囲  \n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#train.data：500days(2years of trading data)\n#culmative values of resp over time\n\n#plt : matplotlib.pyplot\n#図のサイズを決める(横:縦=15:5)\n#plt.figure():Figureインスタンスを作成(描画できる領域を確保)\n#((横,縦),ドット数,...)\n#fig.add_subplot():Figure上にグラフを描画\n#plt.subplot():現在の描画領域(fig)に追加するメソッド\n\n#plt.subplots(subplotの行数(分割数),列数,...figsize):\n#fig:figure(1つのFigureクラスのインスタンス)\n#ax:axes(1つのAxesSubplotクラスのインスタンス)\nfig, ax = plt.subplots(figsize=(15,5))\n\n#2次元のデータフレームに対して、Seriesは一次元配列\n#cumsum:cumulative sum（累積値）\nbalance = pd.Series(train_data['resp']).cumsum()\n##なぜbalanceという名前？\n\n#x,yのラベル\n#ax:AxesSubplotクラスのインスタンス\nax.set_xlabel(\"Trade\", fontsize=18)\nax.set_ylabel(\"Cumulative resp\", fontsize=18);\n\n#balanceをplot\n#lw:line width(線幅)\nbalance.plot(lw=3);\n\n#balanceの変数を消してメモリを確保\ndel balance\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#同様に、4time horioznsも行う\n\n#train.csvのカラム内容\n#date\n#weight\n#resp_1...4,resp\n#feature_0...129\n#ts_id\n\n#図のサイズを決める(横:縦=15:5)\nfig, ax = plt.subplots(figsize=(15,5))\n#各respをのデータを変数に格納\n#（train_data内の[]の線形データの累積値）\nbalance = pd.Series(train_data['resp']).cumsum()\nresp_1 = pd.Series(train_data['resp_1']).cumsum()\nresp_2 = pd.Series(train_data['resp_2']).cumsum()\nresp_3 = pd.Series(train_data['resp_3']).cumsum()\nresp_4 = pd.Series(train_data['resp_4']).cumsum()\n\n#xのラベル\nax.set_xlabel(\"Trade\", fontsize=18)\n#title\nax.set_title(\"Cumulative resp and time horizons 1,2,3, and 4(500days)\", fontsize=18);\n#balance,resp_1,2,3,4をplot\nbalance.plot(lw=3)\nresp_1.plot(lw=3)\nresp_2.plot(lw=3)\nresp_3.plot(lw=3)\nresp_4.plot(lw=3)\n\n#凡例\n#凡例枠は左上(upper left)\nplt.legend(loc=\"upper left\");\n\n#各変数を消してメモリ削減\ndel balance\ndel resp_1\ndel resp_2\ndel resp_3\ndel resp_4\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"resp（青）はresp4(紫)に近い。\n\n\"Jane Street: time horizons and volatilities\"  \n→最尤推定より、time horizon(Tj)にたいし、resp_1をT1(5trading data)とすると、\n* Tj(resp_2) = 1.4T1\n* Tj(resp_3) = 3.9T1\n* Tj(resp_4) = 11.1T1\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#resp値-0.05~0.05をplot：裾野が広い→全体の範囲は-0.55～0.45  \n#歪度skew    ：0.10  ;分布が正規分布からどれだけ歪んでいるか（どれだけ非対称か）を表す\n#尖度kurtosis：17.36 ;分布が正規分布からどれだけ尖っているか（どれだけ平均周辺に周通するか）を表す   \n\n#図のサイズを決める(横:縦=12:5)\nplt.figure(figsize=(12,5))\n#sns.distplot:ヒストグラム\n#(x, bins;階級幅, hist=True, \n# kde;密度近似関数の描画=True,(デフォルト)\n# rug;実数地の描画=False, fit;正規分布の描画=None,\n# hist_kws=None, kde_kws=None, rug_kws=None, fit_kws=None, color=None, \n# vertical=False, norm_hist=False, axlabel=None, label=None, ax=None)\n\n##_kws：何の略？ 辞書型で、グラフに対して色やラベルを指定\n\nax = sns.distplot(train_data['resp'],\n                bins=3000,\n                kde_kws={\"clip\":(-0.05,0.05)},\n                hist_kws={\"range\":(-0.05,0.05)},\n                color='darkcyan', #ダークシアン(青緑色)\n                kde=False);\n\n#valuesを設定\n#np.array:n次元配列\n#matplotlib.patches：円、楕円、長方形など様々な図形を描画するクラスが用意されている\n#上のヒストグラムの各データにおける高さを取得しvaluesに格納\nvalues = np.array([rec.get_height() for rec in ax.patches])\n\n#matplotlibのcolorbarは、0～1の間の値に対応して色が定義されている\n#今回はvalues.min～values.maxの間の値に対応して色を定義したい\n#なので、valuesのminとmaxを用いて標準化\nnorm = plt.Normalize(values.min(), values.max())\n#plt.cm:グラデーション的に変える(.jet:可視化方法の一つ、hsv,spectralなどもある)\ncolors = plt.cm.jet(norm(values))\n\n#zip:複数のobjectの要素を同時に取得\n#つまり、ax.pathcesとcolorsの値を対応させたうえで、recとcolに格納する\n##recvfrom:戻り値、column:カラム?\nfor rec, col in zip(ax.patches, colors):\n    #rec(ヒストグラムデータ)に色を付ける\n    rec.set_color(col)\n\n#xラベルを加えて表示\nplt.xlabel(\"Histogram of the resp values\", size=14)\nplt.show();\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#respの,min,max,skew,kurtosisを計算\nmin_resp = train_data[\"resp\"].min()\nmax_resp = train_data[\"resp\"].max()\n\n#.formatの.5fは、小数点以下第5位まで表示\n\n#print('The minimum value for resp is: %.5f' % min_resp)\nprint('The minimum value for resp is: {:.5f}' .format(min_resp))\n\n#print('The maximum value for resp is: %.5f' % max_resp)\nprint('The maximum value for resp is: {:.5f}'.format(max_resp))\n\n#print(\"Skew of resp is: %.2f\" % train_data['resp'].skew())\n#print(\"Kurtosis of resp is: %.2f\" %train_data['resp'].kurtosis())\nprint(\"Skew of resp is: {:.2f}\" .format(train_data['resp'].skew()))\nprint(\"Kurtosis of resp is: {:.2f}\" .format(train_data['resp'].kurtosis()))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Cauchy distribution；コーシー分布\n#連続確率分布の一種で、左右対称で、期待値は存在しない（外れ値をとる確率が高い（裾が重い））\n#大数の法則が成り立たず、最頻値＝中央値は定義される\n\n#scipy>optimize>curve_fit(カーブフィッチング;曲線近似)\nfrom scipy.optimize import curve_fit\n\n#the values\n#values:ヒストグラムの各データにおける高さを格納したもの\n#それの要素数をリストに格納;x=[0,1,...]\nx = list(range(len(values)))\n#xの値 = (x-1500)/30000\nx = [((i)-1500)/30000 for i in x]\ny = values\n\n#ローレンツ関数を定義(コーシー分布＝ローレンツ分布)\ndef Lorentzian(x, x0, gamma, A):\n    return A * gamma**2 / (gamma**2+(x-x0)**2)\n\n#seed guess\n#種を予測する\n#初めの値を設定\ninitial_guess = (0,0.001, 3000)\n\n#the fit\n#ローレンツ関数に曲線近似させ、パラメータと共分散を求める\nparameters,covariance = curve_fit(Lorentzian, x, y, initial_guess)\n#共分散の体格成分を抽出し、平方根をとる(=sigma)\nsigma = np.sqrt(np.diag(covariance))\n\n#and plot\nplt.figure(figsize = (12,5))\n#distplot:ヒストグラム\nax = sns.distplot(train_data['resp'],\n                 bins = 3000,\n                 kde_kws = {\"clip\":(-0.05,0.05)},\n                 hist_kws = {\"range\":(-0.05,0.05)},\n                 color = 'darkcyan',\n                 kde = False);\n\nvalues = np.array([rec.get_height() for rec in ax.patches])\n#上で同じことやったので省略\n#norm = plt.Normalize(values.min(), values.max())\n#colors = plt.cm.jet(norm(values))\n#for rec, col in zip(ax.patches, colors):\n#    rec.set_color(col)\nplt.xlabel(\"Histogram of the resp values\", size=14)\n#引数に1つの*をつけると複数の引数をタプルとして受け取る\n#引数に2つの*をつけると複数のキーワード引数を辞書として受け取る\nplt.plot(x, Lorentzian(x,*parameters), '--', color='black', lw=3)\nplt.show();\ndel values\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cauthy distribution は2つの独立した平均０の標準分布に従うランダムの値からなる\nThe paper by David E. Harris \"The Distribution of Returns\"   \nより詳しいCauthy distibutionの説明"},{"metadata":{},"cell_type":"markdown","source":"\n## 3,weight：tradeにはweightとrespが重要\n意図的にweight=0も含む\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#weight_0の全体における割合の計算\n#.shape:次元ごとの大きさを出力 \n#train_data.shape\n#(2390491, 138)\n\n#(100；確率を%にするため) / (train；) * (traindataのweightが0)\n#.shape[0]:一次元目(行)の合計数\n#train_data > weight > valuesが0 をTrueとしたときの、Trueの合計数\npercent_zeros = (100/train_data.shape[0]) * ((train_data.weight.values == 0).sum())\n\n#print('percentage of zero weights is: %i' % percent_zeros + \"%\")\nprint('percentage of zero weights is: {:.0f}'.format(percent_zeros) + \"%\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#negative weightがあるかどうかをチェック（negative weightは意味ないから）\n#weightのmin\nmin_weight = train_data['weight'].min()\n#print('The minimum weight is: %.2f' % min_weight)\nprint('The minimum weight is: {:.2f}'.format(min_weight))\n\n\n#weightのmin\nmax_weight = train_data['weight'].max()\n#print('The maximum weight is: %.2f' % max_weight)\nprint('The maximum weight is: {:.2f}'.format(max_weight))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#[]の中に条件を書く(weightがmax)\ntrain_data[train_data['weight']==train_data['weight'].max()]\n#446日目に起こる","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#weightをplot\n#まずは単色でplot\nplt.figure(figsize = (12,5))\nax = sns.distplot(train_data['weight'],\n                 bins=1400, \n                 kde_kws={\"clip\":(0.001, 1.4)},\n                 hist_kws={\"range\":(0.001,1.4)},\n                 color='darkcyan',\n                 kde=False);\n\n#次に、わかりやすくするため、値の大きさに合わせて色を付ける\n#色を付けるときの一連の流れは同じ、values→Normalize→colors→rec,col→show\n#values\nvalues = np.array([rec.get_height() for rec in ax.patches])\n#Normalize\nnorm = plt.Normalize(values.min(), values.max())\n#colors\ncolors = plt.cm.jet(norm(values))\n#rec,col\nfor rec,col in zip(ax.patches, colors):\n    rec.set_color(col)\n#show\nplt.xlabel(\"Histogram of non-zero weights\", size=14)\nplt.show();\ndel values\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2つのピークがある  \n①weight=0.17：高く狭い  \n②weigh=0.34 ：低く広い\n0以外のヒストグラム：peekは2つ(0.17:高く狭い,0.34:低く広い)\n→2つの分布が重なっている（販売と購入？）\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#weightsの対数分布\n#weightが0でないデータを格納\n#query：指定した条件に合うものを取り出す\n#reset_index:indexを連番に振り直す\n#デフォルトでは、「元のindex」の列が残るが、drop=Trueとすると「元のindex」は消える\ntrain_data_nonZero = train_data.query('weight > 0').reset_index(drop = True)\n\n#plot\nplt.figure(figsize = (10,4))\n#ここで対数化する\nax = sns.distplot(np.log(train_data_nonZero['weight']),\n                 bins=1000,\n                 kde_kws={\"clip\":(-4,5)},\n                 hist_kws={\"range\":(-4,5)},\n                 color='darkcyan',\n                 kde=False);\n\n#色を付けるときの一連の流れは同じ、values→Normalize→colors→rec,col→show\nvalues = np.array([rec.get_height() for rec in ax.patches])\nnorm = plt.Normalize(values.min(),values.max())\ncolors = plt.cm.jet(norm(values))\nfor rec, col in zip(ax.patches, colors):\n    rec.set_color(col)\nplt.xlabel(\"Histogram of the logarithm of the non-zero weights\", size=14)\nplt.show();\ngc.collect();\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ガウス分布\n\n#再び曲線近似\nfrom scipy.optimize import curve_fit\n\n#the values\n#x=0,1,2,...\nx = list(range(len(values)))\n#x = (x/110)-4\nx = [(i/110)-4 for i in x]\ny = values\n\n#define a Gaussian function\n#ガウス関数の定義\ndef Gaussian(x, mu,sigma, A):\n    return A*np.exp(-0.5 * ((x-mu)/sigma)**2)\n\n#二峰［双峰］性関数の定義\ndef bimodal(x, mu_1, sigma_1, A_1, mu_2, sigma_2, A_2):\n    return Gaussian(x, mu_1, sigma_1, A_1) + Gaussian(x, mu_2, sigma_2, A_2)\n\n#seed guess\ninitial_guess = (1, 1, 1, 1, 1, 1)\n\n#the fit\n#双峰性関数で曲線近似\nparameters, covariance = curve_fit(bimodal, x, y, initial_guess)\nsigma = np.sqrt(np.diag(covariance))\n\n#the plot\nplt.figure(figsize=(10,4))\n#対数をとる\nax = sns.distplot(np.log(train_data_nonZero['weight']),\n                 bins=1000,\n                 kde_kws={\"clip\":(-4,5)},\n                 hist_kws={\"range\":(-4,5)},\n                 color='darkcyan',\n                 kde=False)\n\n#色を付けるいつものやつ\nvalues = np.array([rec.get_height() for rec in ax.patches])\nnorm = plt.Normalize(values.min(), values.max())\ncolors = plt.cm.jet(norm(values))\nfor rec, col in zip(ax.patches, colors):\n    rec.set_color(col)\nplt.xlabel(\"Histogram of the logarithm of the non-zero weights\", size=14)\n\n#plot gaussian #1:bimodalのmu_1, sigma_1, A_1を用いる(・・)\nplt.plot(x, Gaussian(x, parameters[0], parameters[1], parameters[2]), ':', color='black', lw=2, label='Gaussina #1', alpha=0.8)\n\n#plot gaussian #2:bimodalのmu_2, sigma_2, A_2を用いる(---)\nplt.plot(x, Gaussian(x, parameters[3], parameters[4], parameters[5]), '--', color='black', lw=2, label='Gaussina #2', alpha=0.8)\n\n#plot the two gaussians toghther:上二つを合わせたもの(実線)\nplt.plot(x, bimodal(x,*parameters), color='black', lw=2, alpha=0.7)\n\n#凡例\nplt.legend(loc=\"upper left\");\n\nplt.show();\ndel values\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"対数分布、ガウス分布でみると、狭い左のpeekは違う分布  \nμ平均  \n小さいガウス：-1.32　大きいガウス：0.4\n## 4,Cumulative return：累積daily return＝weight * calue of resp\nresp_123→保守的conservative\nCumulative return のヒストグラム"},{"metadata":{"trusted":true},"cell_type":"code","source":"#各respについて、新しくweight*respのcolumnをつくる\ntrain_data['weight_resp'] = train_data['weight']*train_data['resp']\ntrain_data['weight_resp_1'] = train_data['weight']*train_data['resp_1']\ntrain_data['weight_resp_2'] = train_data['weight']*train_data['resp_2']\ntrain_data['weight_resp_3'] = train_data['weight']*train_data['resp_3']\ntrain_data['weight_resp_4'] = train_data['weight']*train_data['resp_4']\n\n#figureとAxesSubplotの準備\nfig, ax = plt.subplots(figsize=(15,5))\n#一次元化\n#groupbyは、同じ値を持つデータを集め、それぞれの塊に対し、同じ操作を行う\n#'date'で分け(同じ日付ごとのデータの塊が合計日数分できる)、\n#それぞれの塊におけるweight_respの平均値（つまり一日の平均値)を計算し、\n#それらに1を足して累積積を格納\nresp = pd.Series(1+(train_data.groupby('date')['weight_resp'].mean())).cumprod()\n#resp_1,2,3,4に対しても同様に行う\nresp_1 = pd.Series(1+(train_data.groupby('date')['weight_resp_1'].mean())).cumprod()\nresp_2 = pd.Series(1+(train_data.groupby('date')['weight_resp_2'].mean())).cumprod()\nresp_3 = pd.Series(1+(train_data.groupby('date')['weight_resp_3'].mean())).cumprod()\nresp_4 = pd.Series(1+(train_data.groupby('date')['weight_resp_4'].mean())).cumprod()\n\n##このAxesSubplotのxラベルをつける(下の図でついて無くね？)\nax.set_xlabel(\"Day\", fontsize=18)\nax.set_title(\"Cumulative daily return for resp and time horizons1,2,3, and 4(500days)\", fontsize=18)\nresp.plot(lw=3, label='resp x weight')\nresp_1.plot(lw=3, label='resp_1 x weight')\nresp_2.plot(lw=3, label='resp_2 x weight')\nresp_3.plot(lw=3, label='resp_3 x weight')\nresp_4.plot(lw=3, label='resp_4 x weight')\n\n#day 85 marker\n#垂直線axvline(alpha:透明度)\nax.axvline(x=85, linestyle='--', alpha=0.3, c='red', lw=1)\n#一定区間に背景色を付ける\n#sns.xkcd_rgbで、954色を自由に取り出せる\nax.axvspan(0, 85, color=sns.xkcd_rgb['grey'], alpha=0.1)\n#凡例\nplt.legend(loc=\"lower left\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"最短のtime horizonsはresp_1,2,3で保守的な戦略はリターンが低いことを表している。"},{"metadata":{"trusted":true},"cell_type":"code","source":"#weightのヒストグラムをplotする\n#さっきと同じように、weightが0でないデータを格納(query,reset：条件に合うものを取り出し連番に)\ntrain_data_no_0 = train_data.query('weight > 0').reset_index(drop = True)\n#新しいcolumnを作る、wAbsResp=weight*resp\ntrain_data_no_0['wAbsResp'] = train_data_no_0['weight'] * (train_data_no_0['resp'])\n\n#plot(histgram)\nplt.figure(figsize = (12,5))\nax = sns.distplot(train_data_no_0['wAbsResp'],\n                 bins=1500,\n                 kde_kws={\"clip\":(-0.02,0.02)},\n                 hist_kws={\"range\":(-0.02, 0.02)},\n                 color='darkcyan',\n                 kde=False);\n#COLOR：value,norm,colors,rec/col\nvalues = np.array([rec.get_height() for rec in ax.patches])\nnorm = plt.Normalize(values.min(), values.max())\ncolors = plt.cm.jet(norm(values))\n#\nfor rec, col in zip(ax.patches, colors):\n    rec.set_color(col)\nplt.xlabel(\"Histogram of the weights * resp\", size=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5,Time：各日の、ts_idの数をplot\n\n垂直破線：85日目付近でモデルを修正している可能性があるため  "},{"metadata":{"trusted":true},"cell_type":"code","source":"#各日のtradeの回数の変数をつくる（日付でgroup分けし、ts_idの数を数える）\ntrades_per_day = train_data.groupby(['date'])['ts_id'].count()\n\n#plot(折れ線グラフ)\nfig, ax = plt.subplots(figsize=(15,5))\nplt.plot(trades_per_day)\nax.set_xlabel(\"Day\", fontsize=18)\nax.set_title(\"Total number of ts_id for each day\", fontsize=18)\n\n#day 85 marker\n#x=85の点線\nax.axvline(x=85, linestyle='--', alpha=0.3, c='red', lw=1)\n#その範囲をグレーにする\nax.axvspan(0,85, color=sns.xkcd_rgb['grey'], alpha=0.1)\n#軸の範囲を設定\n#xlim:軸の範囲の制限\n#ax.set_:axのパラメータを後から変更できる\nax.set_xlim(xmin=0)\nax.set_xlim(xmax=500)\n#notebook系では現在の図を勝手にplotしてくれるので、plt.show()は必要なく感じる\n#複数のFigureを最後にplotしたり、途中でplotしたりするときに使える\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#trading day(一日の取引時間)は6.5hours(23400seconds)とする\n\n#Average time between trades for each day\nfig, ax = plt.subplots(figsize=(15,5))\n#平均取引時間 = 時間/取引回数\nplt.plot(23400/trades_per_day)\nax.set_xlabel(\"Day\", fontsize=18)\nax.set_ylabel(\"Av. time between trades(s)\", fontsize=18)\nax.set_title(\"Average time between trades for each day\", fontsize=18)\nax.axvline(x=85, linestyle='--', alpha=0.3, c='red', lw=1)\nax.axvspan(0, 85, color=sns.xkcd_rgb['grey'], alpha=0.1)\nax.set_xlim(xmin=0)\nax.set_xlim(xmax=500)\nax.set_ylim(ymin=0)\nax.set_ylim(ymax=12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#histogram of the number of trades per day\nplt.figure(figsize=(12,4))\n\n#the minimum has been set to 1000 so as not to draw the partial days like day 2 and day 294\n#the maximum number of trade per day is 18884\n#I have used 125 bins for the 500 days\n#→(1000,20000)\n#histgram\nax = sns.distplot(trades_per_day,\n                 #階級幅\n                 bins=125,\n                 kde_kws={\"clip\":(1000,20000)},\n                 hist_kws={\"range\":(1000,20000)},\n                 color='darkcyan',\n                 # kde;密度近似関数の描画=True\n                 kde=True);\nvalues = np.array([rec.get_height() for rec in ax.patches])\nnorm = plt.Normalize(values.min(), values.max())\ncolors = plt.cm.jet(norm(values))\nfor rec, col in zip(ax.patches, colors):\n    rec.set_color(col)\nplt.xlabel(\"Number of trades per day\", size=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#上記より、maxで9000回取引があった日が複数あるのでみてみる\n#volitile=volatile：不安定な；外れ値\nvolitile_days = pd.DataFrame(trades_per_day[trades_per_day > 9000])\nvolitile_days.T #転置\n\n#不安定な日：9000回以上の取引があることもあり、ほとんどは85日目より前  \n#feature_64もtimeに関連（※後述）","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6,The features\n### feature_0"},{"metadata":{"trusted":true},"cell_type":"code","source":"#feature_0\n#唯一 +1 or -1であり、他と違いそうなのでまず見てみる\n\ntrain_data['feature_0'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#feature_0は、features.csv fileの中で唯一Trueを持たない\n#features.csv：feature × Tagで、TFのみをとる\n#→feature_0は、どのTagにも属さない\n\n#折れ線plot\nfig, ax = plt.subplots(figsize=(15,4))\n#train_dataのfeature_0の一次元データの累積値を格納\nfeature_0 = pd.Series(train_data['feature_0']).cumsum()\nax.set_xlabel(\"Trade\", fontsize=18)\nax.set_ylabel(\"feature_0(cumulative)\", fontsize=18);\nfeature_0.plot(lw=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#+1と-1の累積値で、最終的には,1207005-1183486=23519\n#安定的に増加\n\n#また、feature_0=1、feature_0=-1それぞれにおいて、culmative respとreturnは関連あり\n## \"An observation about feature_0\" by therocket290\n#さっきと同じように、feature_0が1(-1)のデータを格納(query,reset：条件に合うものを取り出し連番に)\nfeature_0_is_plus_one = train_data.query('feature_0 == 1').reset_index(drop = True)\nfeature_0_is_minus_one = train_data.query('feature_0 == -1').reset_index(drop = True)\n\n#the plot\n#2(1*2)つの図を用意(feature0 = 1or-1を用意)\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,4))\n#respをplot\nax1.plot((pd.Series(feature_0_is_plus_one['resp']).cumsum()), lw=3, label='resp')\nax2.plot((pd.Series(feature_0_is_minus_one['resp']).cumsum()), lw=3, label='resp')\n#return(resp*weight)をplot\nax1.plot((pd.Series(feature_0_is_plus_one['resp']*feature_0_is_plus_one['weight']).cumsum()), lw=3, label='return')\nax2.plot((pd.Series(feature_0_is_minus_one['resp']*feature_0_is_minus_one['weight']).cumsum()), lw=3, label='return')\n#title,legend\n#ちなみに、legend=伝説、読まれるもの、昔話、集める(lect)、話す、説教、説明、凡例という語源\nax1.set_title(\"feature 0 = 1\", fontsize=18)\nax2.set_title(\"feature 0 = -1\", fontsize=18)\nax1.legend(loc=\"lower left\")\nax2.legend(loc=\"lower left\");\n\ndel feature_0_is_plus_one\ndel feature_0_is_minus_one\ngc.collect();\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"+1 と-1　は全然違うreturnの動きをする。  \n\"Feature 0, beyond feature 0\" written by NanoMathias a uniform manifold     approximation and projection (UMAP)   \n→feature0は他のfeaturesを2つのdistributionsに分ける \n \nfeature_0の値の違い  \n→featureの2つの分布；売買(bid/ask, long/short, call/put)  \n\n\"What is \"feature_0\" ?\"   \n\nLee and Ready 'Tick' model  \n個々の取引を、日中の取引データと見積もりデータを用いて  \nmarket buy orderかmarket sell orderかに分類  \nbuy始まりの取引を1とし、sell始まりの取引を-1とする\n\n(\"Advances in Financial Machine Learning\" by Marcos Lopez de Prado)   \nbuy始まりの取引を1とし、sell始まりの取引を-1とする\n\npt：時間t=1,...Tでのprice b0は任意  \n  \nΔpt > 0 ならbt = 1 (market buy order:buy-initiated trade)  \nΔpt < 0 ならbt = -1(market sell order:sell-initiated trade)  \nΔpt = 0 ならbt-1 = 0（価格pが同じときは一期前と同じ）  \n（日中取引intraday trade ＆ 見積りquoteデータ）  \n  \ncorrelation matrix\n→feature_0とTag12,(24)に正の相関、Tag13,(25,27)に強い負の相関  \n→feature_37,38,39,40以外はすべてresp(特にresp4)と相関強い  \n \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#85日目でデータを分けるために、85日目の一番最初の取引データを取得\nday85 = train_data[train_data.date == 85]\nday85[day85.ts_id == day85.ts_id.min()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### feature_1...129"},{"metadata":{"trusted":true},"cell_type":"code","source":"#feature_1,...129：特徴は4つに分けられそう  \n\n#2*2の4つのplot\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20,10))\n\n#①Linear:feature_1\nax1.plot((pd.Series(train_data['feature_1']).cumsum()), lw=3, color='red')\nax1.set_title(\"Linear\", fontsize=22);\nax1.axvline(x=514052, linestyle='--', alpha=0.3, c='green', lw=2)\nax1.axvspan(0, 514052, color=sns.xkcd_rgb['grey'], alpha=0.1)\n#端っこを0にする\nax1.set_xlim(xmin=0)\n#ax1.set_xlabel(\"Trade\", fontsize=18)\nax1.set_ylabel(\"feature_1\", fontsize=18);\n\n#②Noisy:feature_3\nax2.plot((pd.Series(train_data['feature_3']).cumsum()), lw=3, color='green')\nax2.set_title(\"Noisy\", fontsize=22);\nax2.axvline(x=514052, linestyle='--', alpha=0.3, c='red', lw=2)\nax2.axvspan(0, 514052, color=sns.xkcd_rgb['grey'], alpha=0.1)\nax2.set_xlim(xmin=0)\n#ax2.set_xlabel(\"Trade\", fontsize=18)\nax2.set_ylabel(\"feature_3\", fontsize=18);\n\n#③Hybryd(Tag21):feature_55\nax3.plot((pd.Series(train_data['feature_55']).cumsum()), lw=3, color='darkorange')\nax3.set_title(\"Hybryd(Tag 21)\", fontsize=22);\nax3.axvline(x=514052, linestyle='--', alpha=0.3, c='red', lw=2)\nax3.axvspan(0, 514052, color=sns.xkcd_rgb['grey'], alpha=0.1)\nax3.set_xlim(xmin=0)\nax3.set_xlabel(\"Trade\", fontsize=18)\nax3.set_ylabel(\"feature_3\", fontsize=18);\n\n#④Negative：feature_73\nax4.plot((pd.Series(train_data['feature_73']).cumsum()), lw=3, color='blue')\nax4.set_title(\"Negative\", fontsize=22);\n#ax4.axvline(x=514052, linestyle='--', alpha=0.3, c='red', lw=2)\n#ax4.axvspan(0, 514052, color=sns.xkcd_rgb['grey'], alpha=0.1)\n#ax4.set_xlim(xmin=0)\nax4.set_xlabel(\"Trade\", fontsize=18)\nax4.set_ylabel(\"feature_3\", fontsize=18);\ngc.collect();\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ①Linear：Tag14,18,22 \n* 1\n* 7,9,11,13,15\n* 17,19,21,23,25\n* 18,20,22,24,26\n* 27,29,21,33,35\n* 28,30,32,34,36\n* 84,85,86,87,88\n* 90,91,92,93,94\n* 96,97,98,99,100\n* 102(変化の勾配大),103,104,105,106\n\n* 41,46,47,48,489,50,51,53,54,69,89,95(変化の勾配大),101,107(変化の勾配大),108.110,111,113,114,115,116,117,118,119(変化の勾配大),120,122,124も同様\n\n### Features 41,42,43(Tag14)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tag14はstratified（層化）している\n# →discreate values（離散値）のみ採用（security；証券）\n# day0,1,3のfeature_41,42,43をplot(day2を省く理由は後述)\n\n#day0,1,3のcolumnをつくる　\n#.loc:要素抽出：ラベル名可、絶対座標不可、複数要素指定可\n#.at         ：ラベル名可、絶対座標不可、複数要素指定不可\n#.iloc(iat)　 :ラベル名不可、絶対座標可\nday_0 = train_data.loc[train_data['date'] == 0]\nday_1 = train_data.loc[train_data['date'] == 1]\nday_3 = train_data.loc[train_data['date'] == 3]\n\n#3日間のデータを結合\nthree_days = pd.concat([day_0, day_1, day_3])\n#散布図\n#s:size(default=20)\nthree_days.plot.scatter(x='ts_id', y='feature_41', s=0.5, figsize=(15,3));\nthree_days.plot.scatter(x='ts_id', y='feature_42', s=0.5, figsize=(15,3));\nthree_days.plot.scatter(x='ts_id', y='feature_43', s=0.5, figsize=(15,3));\n\n#del day_0\ndel day_1\ndel day_3\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# feature_41,42,43はlagのplotも面白い\n# ts_id(n)とts_id(n+1)を比べる\n\nfig, ax = plt.subplots(1, 3, figsize=(17,4))\n#pandas.plotting(plt).lag_plot：時系列でlagをplotできる\n#(series=, lag=ずれ(defaultは1), ax;plotするaxis object)\n#sは多分size\nlag_plot(day_0['feature_41'], lag=1, s=0.5, ax=ax[0])\nlag_plot(day_0['feature_42'], lag=1, s=0.5, ax=ax[1])\nlag_plot(day_0['feature_43'], lag=1, s=0.5, ax=ax[2])\n\n#feature41\nax[0].title.set_text('feature_41')\nax[0].set_xlabel(\"ts_id(n)\")\nax[0].set_ylabel(\"ts_id(n+1)\")\n\n#feature42\nax[1].title.set_text('feature_42')\nax[1].set_xlabel(\"ts_id(n)\")\nax[1].set_ylabel(\"ts_id(n+1)\")\n\n#feature43\nax[2].title.set_text('feature_43')\nax[2].set_xlabel(\"ts_id(n)\")\nax[2].set_ylabel(\"ts_id(n+1)\")\n\n#(0.0)に赤い点\nax[0].plot(0, 0, 'r.', markersize=15.0)\nax[1].plot(0, 0, 'r.', markersize=15.0)\nax[2].plot(0, 0, 'r.', markersize=15.0)\ngc.collect();\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Features_44(tag15),45(tag17)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# これらのfeaturesは上に見られるtag14に似ているが、より0を中心としている\n\n#ts_idとfeature_44,45を同様にplot\nthree_days.plot.scatter(x='ts_id', y='feature_44', s=0.5, figsize=(15,3));\nthree_days.plot.scatter(x='ts_id', y='feature_45', s=0.5, figsize=(15,3));\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#同様にlagをplot\nfig, ax = plt.subplots(1, 2, figsize=(15,4))\nlag_plot(day_0['feature_44'], lag=1, s=0.5, ax=ax[0])\nlag_plot(day_0['feature_45'], lag=1, s=0.5, ax=ax[1])\n\n\n#feature41\nax[0].title.set_text('feature_44')\nax[0].set_xlabel(\"ts_id(n)\")\nax[0].set_ylabel(\"ts_id(n+1)\")\n\n#feature42\nax[1].title.set_text('feature_45')\nax[1].set_xlabel(\"ts_id(n)\")\nax[1].set_ylabel(\"ts_id(n+1)\")\n\n#(0.0)に赤い点\nax[0].plot(0, 0, 'r.', markersize=15.0)\nax[1].plot(0, 0, 'r.', markersize=15.0)\ngc.collect();\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Features 60 to 68(Tag22)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 60,61,62,63,64,65,66,67,68\n\n#これらのfeaturesの累積グラフ\n\nfig, ax = plt.subplots(figsize=(15,5))\nfeature_60 = pd.Series(train_data['feature_60']).cumsum()\nfeature_61 = pd.Series(train_data['feature_61']).cumsum()\nfeature_62 = pd.Series(train_data['feature_62']).cumsum()\nfeature_63 = pd.Series(train_data['feature_63']).cumsum()\nfeature_64 = pd.Series(train_data['feature_64']).cumsum()\nfeature_65 = pd.Series(train_data['feature_65']).cumsum()\nfeature_66 = pd.Series(train_data['feature_66']).cumsum()\nfeature_67 = pd.Series(train_data['feature_67']).cumsum()\nfeature_68 = pd.Series(train_data['feature_68']).cumsum()\n#feature_69 = pd.Series(train_data['feature_69']).cumsum()\n\nax.set_xlabel(\"Trade\", fontsize=18)\nax.set_title(\"Cumulative plot for feature_60,...,68(Tag22).\", fontsize=18)\n\nfeature_60.plot(lw=3)\nfeature_61.plot(lw=3)\nfeature_62.plot(lw=3)\nfeature_63.plot(lw=3)\nfeature_64.plot(lw=3)\nfeature_65.plot(lw=3)\nfeature_66.plot(lw=3)\nfeature_67.plot(lw=3)\nfeature_68.plot(lw=3)\n#feature_69.plot(lw=3)\n\nplt.legend(loc=\"upper left\");\ndel feature_60, feature_61, feature_62, feature_63, feature_64, feature_65, feature_66, feature_67, feature_68\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#上のグラフにおいて、feature_60と61,62と63,65と66,67と68は似ているので、\n#それらのdistribution(分布)をplot\n\n#defaultでデフォルトのカラーパレットは、\n#6つのテーマ(deep, muted, pastel, bright, dark, colorblind)がある\nsns.set_palette(\"bright\")\n\nfig, axes = plt.subplots(2,2,figsize=(8,8))\n\n#feature60と61\n#ヒストグラム(階級幅が今までと比べ一桁小さい)\nsns.distplot(train_data[['feature_60']], hist=True, bins=200, ax=axes[0,0])\nsns.distplot(train_data[['feature_61']], hist=True, bins=200, ax=axes[0,0])\naxes[0,0].set_title(\"feature 60 and 61\", fontsize=18)\naxes[0,0].legend(labels=['60', '61'])\n\n#feature62と63\nsns.distplot(train_data[['feature_62']], hist=True, bins=200, ax=axes[0,1])\nsns.distplot(train_data[['feature_63']], hist=True, bins=200, ax=axes[0,1])\naxes[0,1].set_title(\"feature 62 and 63\", fontsize=18)\naxes[0,1].legend(labels=['62', '63'])\n\n#feature65と66\nsns.distplot(train_data[['feature_65']], hist=True, bins=200, ax=axes[1,0])\nsns.distplot(train_data[['feature_66']], hist=True, bins=200, ax=axes[1,0])\naxes[1,0].set_title(\"feature 65 and 66\", fontsize=18)\naxes[1,0].legend(labels=['65', '66'])\n\n#feature67と68\nsns.distplot(train_data[['feature_67']], hist=True, bins=200, ax=axes[1,1])\nsns.distplot(train_data[['feature_68']], hist=True, bins=200, ax=axes[1,1])\naxes[1,1].set_title(\"feature 67 and 68\", fontsize=18)\naxes[1,1].legend(labels=['67', '68'])\n\nplt.show();\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#それらの間にあるfeature_64を見てみる\nplt.figure(figsize = (12,5))\nax = sns.distplot(train_data['feature_64'],\n                  bins=1200,\n                  kde_kws={\"clip\":(-6,6)},\n                  hist_kws={\"range\":(-6,6)},\n                  color='darkcyan',\n                  kde=False);\nvalues = np.array([rec.get_height() for rec in ax.patches])\nnorm = plt.Normalize(values.min(), values.max())\ncolors = plt.cm.jet(norm(values))\nfor rec, col in zip(ax.patches, colors):\n    rec.set_color(col)\nplt.xlabel(\"Hisogram of feature_64\", size=14)\nplt.show();\ndel values\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"上の図において、0.7から1.38の間に大きなgapがある  \nちなみにlog2=0.693、log4=1.386だが、これに意味があるかは不明"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Tag22のfeatures(60-68)はdaily patternも興味深い\n#例えば、feature64の、3日間のscatter(分散)とcumulative(累積)をplot\n\nday_0 = train_data.loc[train_data['date'] == 0]\nday_1 = train_data.loc[train_data['date'] == 1]\nday_3 = train_data.loc[train_data['date'] == 3]\nthree_days = pd.concat([day_0, day_1, day_3])\n\n#plot\n##sharex:複数グラフ間の軸を共有(x軸)\nfig, ax = plt.subplots(2, 1, figsize=(15,6), sharex=True)\n\nax[0].scatter(three_days.ts_id, three_days.feature_64, s=0.5, color='b')\nax[0].set_xlabel('') #下の図と共有\nax[0].set_ylabel('value')\nax[0].set_title('feature_64(days 0, 1 and 3)')\n\nax[1].scatter(three_days.ts_id, pd.Series(three_days['feature_64']).cumsum(), s=0.5, color='r')\nax[1].set_xlabel('ts_id')\nax[1].set_ylabel('cumulative sum')\nax[1].set_title('') #上の図と共有\n\nplt.show();\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#上図においてfeature_64のthe global minimum valueは-6.4\n#feature_64のthe global maximum valueは8\nprint(train_data['feature_64'].max())\nprint(train_data['feature_64'].min())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#NewYorkのStock Exchangeの一日の取引時間は9:30-16:00であることを考えてみる\n\n#feature_64の単位が、およそ30minutesで、feature_64=0が12時に一致していたとすると、\n#arcsin functionをplotでき、y軸を、一日の時間とすることができる\n\n#np.arange:等差数列(start,stop,step) step:間隔\nx = np.arange(-1, 1, 0.01)\n#アークサイン\ny = 2 * np.arcsin(x) + 1\n\n#plot\nfig, ax = plt.subplots(1, 1, figsize=(7,4))\nax.plot(x, y, lw=3)\n\n#目盛ラベル(x軸;メモリ的な意味でのtime(つまりnp.arangeで作った-1～1) は無しにしておく)\nax.set(xticklabels=[])\nax.set(yticklabels=['9:00', '10:00', '11:00', '12:00', '13:00', '14:00', '15:00', '16:00'])\n\n#$\\it{}$ :斜体\n#$\\mathsf{}$：立体\n#\"$\\<書体>{}$\":上付き、下付き文字\n\nax.set_title(\"2$\\it{arcsin}$(t) +1\", fontsize=18)\nax.set_xlabel(\"'tick' time\", fontsize=18)\nax.set_ylabel(\"Clock time\", fontsize=18)\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#上図より、trade(その日におけるts_idの数)は、一日の中間よりも始まりから終わりの方がより頻繁\n#仮想のtick frecuencyをplotする\n\n#図をplot(t≠1,-1なので範囲はそれをふまえる(-0.98がなぜダメかは不明))\n#dash:微分した、という意味\nx_dash = np.arange(-0.98, 0.99, 0.01)\n\n#一個前のセルのやつの微分\n#d/dt(2arcsin(t)+1) = 2/√(1-t^2)\ny_dash = 2 / np.sqrt(1-(x_dash**2))\n\nfig, ax = plt.subplots(1, 1, figsize=(7,4))\nax.plot(x_dash, y_dash, lw=3)\n\n#yのlabelをなくす\nax.set(yticklabels=[])\n\n#x軸を変更：時間区分を8つにするため、-1～1の2区間を、2/7=2.85より2.8ずつに分解\nax.xaxis.set_ticks(np.arange(-1, 1, 0.28))\n#xのlabelを時間に変更\nax.set(xticklabels=[\"9:00\", '10:00', '11:00', '12:00', '13:00', '14:00', '15:00', '16:00'])\nax.set_title(\"d/dt (2$\\it{arcsin}$(t) +1)\", fontsize=18)\nax.set_xlabel(\"Clock time\", fontsize=18)\nax.set_ylabel(\"'tick' frequency\", fontsize=18)\nplt.show();\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#上のことを踏まえると、複数のfeatureにおいて\n#一日の始まりに見られる欠損値のperiodは、\n#一日の中間に見られる欠損値のperiodと同じなのか？？\n\n#またおそらく、一日の始まりと終わりにおける高いthick frequencyは、\n#夜中に重要なpositionがないようにするための\n#open直後の大量の購入と、close直前の大量の売却によるものではないか？？\n\n##marketneutral TOkyo Stock Exchange\n#→日本株に関するものではないか\n#日本のtrading hours は9:00-11:30,breakを挟んで12:30-15:00まで\n##これによりTag22のfeaturesの中央の不連続性（gap）が説明できそう\n\n#feature_65をplot\nthree_days.plot.scatter(x='ts_id', y='feature_65', s=0.5, figsize=(15,4));\n\n## \"Important and Hidden Temporal Data\" written by Lachlan Suter.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ②Noisy：Tag19  \n* 3,4,5,6\n* 8,10,12,14,16\n* 37,38,39,40\n* 72,73,74,75,76\n* 78,79,80,81,82\n* 83"},{"metadata":{"trusted":true},"cell_type":"code","source":"#これらのfeaturesの一部のcumulative plots\n\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16,8))\n\n#以下の「+Tag9」というのはTag9がFalseの時とTrueの時を比べて、ということ\n#例えば、feature3,5,37,39はFalseでfeature4,6,38,40はTrue\n\n#feature3,4\nax1.set_title(\"features 3 and 4 (+Tag 9)\", fontsize=18);\nax1.plot((pd.Series(train_data['feature_3']).cumsum()), lw=2, color='blue')\nax1.plot((pd.Series(train_data['feature_4']).cumsum()), lw=2, color='red')\n\n#feature5,6\nax2.set_title(\"features 5 and 6 (+Tag 9)\", fontsize=18);\nax2.plot((pd.Series(train_data['feature_5']).cumsum()), lw=2, color='blue')\nax2.plot((pd.Series(train_data['feature_6']).cumsum()), lw=2, color='red')\n\n#feature37,38\nax3.set_title(\"features 37 and 38 (+Tag 9)\", fontsize=18);\nax3.plot((pd.Series(train_data['feature_37']).cumsum()), lw=2, color='blue')\nax3.plot((pd.Series(train_data['feature_38']).cumsum()), lw=2, color='red')\n\n#feature39,40\nax4.set_title(\"features 39 and 40 (+Tag 9)\", fontsize=18);\nax4.plot((pd.Series(train_data['feature_39']).cumsum()), lw=2, color='blue')\nax4.plot((pd.Series(train_data['feature_40']).cumsum()), lw=2, color='red')\nax4.axvline(x=514052, linestyle='--', alpha=0.3, c='green', lw=2)\nax4.axvspan(0, 514052, color=sns.xkcd_rgb['grey'], alpha=0.1)\nax4.set_xlabel(\"Trade\", fontsize=18)\n\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"これら(features？)はoffer pricesとTag９のbid pricesを表すことができるか？  　\n* offer price:提示価格＝売値\n* bid price　 :入札価格＝買値\n\n85日の後のfeature_40の値は実際にfeature_39の値よりも大きくなっている　  \n(offer > bid は常 ：差＝スプレッドspread(会社によって違い、変動することもある))\n\n### feature_51(Tag19)\nWeight and feature_51 de-anonymized\" by marketneutral   \n→weightはtradeで予想されるtransaction cost(取引コスト)  \n→weightがrespの分散に反比例  \n→weightは簡単な取引コストの評価基準\n\n→feature51は株式の一日の平均の出来高（の対数）と言われている\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#non-zero weightsに関して、feature_51を再構築してplot\n# ※w.r.t.:with reference to(～に関して)\n\n#分散\nfig, ax = plt.subplots(figsize=(15,4))\n#color:blue\nax.scatter(train_data_nonZero.weight, train_data_nonZero.feature_51, s=0.1, color='b')\nax.set_xlabel('weight')\nax.set_ylabel('feature_51')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### feature_52(tag19)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#折れ線グラフ\nfig, ax = plt.subplots(figsize=(15,3))\nfeature_0 = pd.Series(train_data['feature_52']).cumsum()\nax.set_xlabel(\"ts_id, fontsize=18\")\nax.set_ylabel(\"feature_52(cumulative)\",fontsize=12);\nfeature_0.plot(lw=3)\n\n#lag plot\nfig, ax = plt.subplots(1,1,figsize=(4,4))\nlag_plot(day_0['feature_52'], s=0.5, ax=ax)\nax.title.set_text('feature_52')\nax.set_xlabel(\"ts_id(n)\")\nax.set_ylabel(\"ts_id(n+1)\")\n#r.:ピリオド必須\n##よくわからないけどredでは反応しない\nax.plot(0,0,'r.',markersize=15.0);\n\n#respとも関係している\nfig, ax = plt.subplots(figsize=(15,4))\nax.scatter(train_data_nonZero.feature_52, train_data_nonZero.resp, s=0.1, color='b')\nax.set_xlabel('feature_52')\nax.set_ylabel('resp')\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ③Hybryd：Tag21  \n55,56,57,58,59  "},{"metadata":{"trusted":true},"cell_type":"code","source":"#これらはnoisyから始まる\n##0.2M、0.5M、0.8M(month?)のtrade marks(取引のマーク)付近に顕著にほぼ不連続なステップがある\n#そこから先はlinear(線形)\n#これらの５つのfeaturesはTag21のsetを形成する\n\nfig, ax = plt.subplots(figsize=(15,5))\n\nfeature_55 = pd.Series(train_data['feature_55']).cumsum()\nfeature_56 = pd.Series(train_data['feature_56']).cumsum()\nfeature_57 = pd.Series(train_data['feature_57']).cumsum()\nfeature_58 = pd.Series(train_data['feature_58']).cumsum()\nfeature_59 = pd.Series(train_data['feature_59']).cumsum()\n\nax.set_xlabel(\"Trade\", fontsize=18)\nax.set_title(\"Cumulative plot for the 'Tag21' feature(55-59)\", fontsize=18)\n\nax.axvline(x=514052, linestyle='--', alpha=0.3, c='black', lw=1)\nax.axvspan(0, 514052, color=sns.xkcd_rgb['grey'], alpha=0.1)\n\nfeature_55.plot(lw=3)\nfeature_56.plot(lw=3)\nfeature_57.plot(lw=3)\nfeature_58.plot(lw=3)\nfeature_59.plot(lw=3)\nplt.legend(loc=\"upper left\");\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"これらが5つのrespの値に関係しているとすると？    \n* feature_55はresp_1に関連\n* feature_56はresp_4に関連\n* feature_57はresp_2に関連\n* feature_58はresp_3に関連\n* feature_59はrespに関連\n\nその場合は、  \n\n* Tag0はresp_4のfeaturesを表す\n* Tag1はrespのfeaturesを表す\n* Tag2はresp_3のfeaturesを表す\n* Tag3はresp_2のfeaturesを表す\n* Tag4はresp_1のfeaturesを表す\n\nつまり、  \n\n* resp_1：7,8,17,18,27,28,55,72,78,84,90,96,102,108,114,120,121  \n(すべてのmissing dataの79.6%がこのsetの中にある)\n* resp_2：11,12,21,22,31,32,57,74,80,86,92,98,104,110,116,124,125  \n(すべてのmissing dataの15.2%がこのsetの中にある)\n* resp_3：13,14,23,24,33,34,58,75,81,87,93,99,105,111,117,,126,127\n* resp_4：9,10,19,20,29,30,56,73,79,85,91,97,103,1099,115,122,123\n* resp：15,16,25,26,35,36,59,76,82,88,94,100,106,112,118,128,129"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 各respの17featuresのそれぞれをplot\n# ※これはイメージで、正確には右クリックで拡大せよ\n\n#Note:別のKaggleのnotebookからimport\n#なぜなら、この図を作るのにほぼすべてのnotebookのmemoryを使うため。\n#Image(filename=\"/kaggle/input/jane-17-plots/17_plots.png\", width= \"95%\")\n#どうやって開くのかよくわからない","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"縦の破線：day85  \n  \nresp_1,2,3,4：モデルトレーニングを正規化するための代替的な客観的metricsを必要とするときのためのもの  \n→それを意図しないならば、おそらく4×17=68のfeatures全てを落として、\nfeaturesに関係するrespを保持することになる"},{"metadata":{},"cell_type":"markdown","source":"### ④Negative：Tag23\nfeature_73,75,76,77(noisy),79,81(noisy),82  \n\n  \n### t-SNE plots；t-distributed stohchastic neighbor embedding plots  \n\nt分布の確率的隣接埋めこみプロット  \n\n\"Jane Street: t-SNE using RAPIDS cuML\"  \n計算に時間がかかるため、別のnotebookに作った"},{"metadata":{},"cell_type":"markdown","source":"## 7,The features.csv file：匿名化されたfeaturesに関連するメタデータ\n\"metadata pretaining to the anonymized features\"を含むfile  \n1:True 0:False  \nfeatureに関連づいたtag29個をもつ\n\n\nfeature→tag1～4個(feature_0:tag0個)"},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_tags = pd.read_csv(\"../input/jane-street-market-prediction/features.csv\", index_col=0)\n#binary（二値）に変換;元々True or Falseなので0or1にする\nfeature_tags = feature_tags*1\n#transoirted（転置された）dataframeをplot\n#元のデータは行：feature、列：tag→行と列を入れ替え\n\n#style:DataFrameに様々なカスタマイズ\n#style.background_gradient:各セルの値に応じてカラーマップを適用する\nfeature_tags.T.style.background_gradient(cmap='Oranges')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#上のデータを'8-bit' modeでre-plot\n#全く判読不能だが、もしかしたら全体の視覚の助けとしての役割を果たすかもしれない\nplt.figure(figsize=(32,14))\n#ヒートマップ\n#cbar:カラーバーを設定するか\nsns.heatmap(feature_tags.T,\n           cbar=False,\n           xticklabels=False,\n           yticklabels=False,\n           cmap=\"Oranges\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#それぞれのfeatureのtagの数を合計する\n#DataFrameは、行がfeature、列がTag→転置しているので逆\n#axis=0:行を順に見て行く\n#列ごとに行の和を出す(featureごとにTagの和を出す)\n#pd.DataFrame(feature_tags.sum(axis=1), columns=['Number of tags'])と同じ\n\ntag_sum = pd.DataFrame(feature_tags.T.sum(axis=0), columns=['Number of tags'])\ntag_sum.T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ほぼすべてのfeaturesは少なくとも１つのtagをもち、4つのtagを持つものもある  \n唯一feature_0のみ全くtagをもたない  \nそれぞれ異なる、5(ish)regionsに分けられる   \nish:およそ  \n\n  \n| Region | features | Tags       | missing value? | observations        |\n| ------ | -------- | ---------- | -------------- | ------------------- |\n| 0      | 0        | none       | none           | -1or+1              |\n| 1      | 1...6    | Tag6       |                |                     |\n| 2      | 7...36   | Tag6       |                |                     |\n| 3      | 37...72  | various    |                |                     |\n| 4      | 72...119 | Tag23      |                |                     |\n| 5      | 120...129| Tag28      |                |                     |\n| ------ | -------- | ---------- | -------------- | ------------------- |\n| 2      | 7...36   | Tag6       |                |                     |\n| 2a     | 7...16   | +11        | 7,8 & 11,12    |                     |\n| 2b     | 17...26  | +12        | 17,18 & 21,22  |                     |\n| 2c     | 27...36  | +13        | 27,28 & 31,32  |                     |\n| 3      | 37...72  | various    |                |                     |\n| 3a     | 55...59  | Tag21      |                | All hybrid          |\n| 3b     | 60...68  | Tag22      |                | Cloc+time features? |\n| 4      | 72...119 | Tag23      |                |                     |\n| 4a     | 72...77  | +15&27     | 72 & 74        |                     |\n| 4b     | 78...83  | +17&27     | 78 & 80        |                     |\n| 4c     | 84...89  | +15&25     | 84 & 86        |                     |\n| 4d     | 90...95  | +17&25     | 90 & 92        |                     |\n| 4e     | 96...101 | +15&24     | 96 & 98        |                     |\n| 4f     | 102...107| +17&24     | 102 & 104      |                     |\n| 4g     | 108...113| +15&26     | 108 & 110      |                     |\n| 4h     | 114...119| +17&26     | 114 & 116      |                     |\n| 5      | 120...129| Tag28      |                |                     |\n| 5a     | 120      | +4         | missingdata    |                     |\n| 5b     | 121      | +4&1       | missingdata    |                     |\n| 5c     | 122      | +0         |                |                     |\n| 5d     | 123      | +0&16      |                |                     |\n| 5e     | 124      | +3         |                |                     |\n| 5f     | 125      | +3&16      |                |                     |\n| 5g     | 126      | +2         |                |                     |\n| 5h     | 127      | +2&16      |                |                     |\n| 5i     | 128      | +1         |                |                     |\n| 5j     | 129      | +1&16      |                |                     |"},{"metadata":{},"cell_type":"markdown","source":"上図には、どこよりも、'継続される...'という古典的な価値がある  \n\n従来の表の形式で何が起こっているのかを明らかにすることはとても難しい  \n🌐EDA: Tag Network Analysis (networkx + gephi)🌐\"   \n→tagsとfeaturesの間の関係を示すグラフ分析が実行されている  \n  \nGreg Calvez   \n→tag_6：prices、tag_23：volume、tag_20：spreadに関連  \n→tag_12：minima、tag_13：maxima、tag_22：timeに関連  "},{"metadata":{},"cell_type":"markdown","source":"## 8,Action\nこのコンペの目的：action    \n1:取引する、 0:取引しない、のbinary(2進)column  "},{"metadata":{"trusted":true},"cell_type":"code","source":"#1:取引する(resp=+), 0:取引しない(resp=-)\n# 最後の*1によって、TorF→1or0に変換\ntrain_data['action'] = ((train_data['resp'])>0)*1\n\n#actionとinaction全体を比較\ntrain_data['action'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#上の式より、全体においてわずかにactiveが多い(0.4%)\n#日ごとに見てみる\n\n#actionのratio = 日ごとのaction(0or1)のsum / 日ごとのactionのcount（日数合計）\ndaily_action_sum   = train_data['action'].groupby(train_data['date']).sum()\ndaily_action_count = train_data['action'].groupby(train_data['date']).count()\ndaily_ratio        = daily_action_sum / daily_action_count\n\n\n#plotする\nfig, ax = plt.subplots(figsize=(15,5))\nplt.plot(daily_ratio)\nax.set_xlabel(\"Day\", fontsize=18)\nax.set_ylabel(\"ratio\", fontsize=18)\nax.set_title(\"Daily ration of action to inaction\", fontsize=18)\n\n#y=0.5に直線の理由：action(1)とinaction(0)が1:1ならy=0.5のはず\n#だから、実際にはわずかに上寄り\nplt.axhline(0.5, linestyle='--', alpha=0.85, c='r');\nax.set_xlim(xmin=0)\nax.set_xlim(xmax=500)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#日ごとのactionはかなり一貫性があり、\n#明らかな週・月・季節等の変化はない\n\n#平均\ndaily_ratio_mean = daily_ratio.mean()\n#print('The mean daily ratio is %.3f' % daily_ratio_mean)\nprint('The mean daily ratio is {:3f}'.format(daily_ratio_mean))\n\n#最大\ndaily_ratio_max = daily_ratio.max()\n#print('The maximum daily ratio is %.3f' % daily_ratio_max)\nprint('The maximum daily ratio is {:3f}'.format(daily_ratio_max))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"最大となるのはday294(後述)  \n上記は明らかに複雑なデータセットにおけるとても単純化したtargetである\n\"Target Engineering; CV; ⚡ Multi-Target\" written by marketneutral"},{"metadata":{},"cell_type":"markdown","source":"\n## 9,The first day (\"day 0\")：missing dataにはpattternがある"},{"metadata":{"trusted":true},"cell_type":"code","source":"#day_0というdataframeをつくる\nday_0 = train_data.loc[train_data['date'] == 0]\n\n#そしてplot\nfig, ax = plt.subplots(figsize=(15,5))\nbalance = pd.Series(day_0['resp']).cumsum()\nresp_1 = pd.Series(day_0['resp_1']).cumsum()\nresp_2 = pd.Series(day_0['resp_2']).cumsum()\nresp_3 = pd.Series(day_0['resp_3']).cumsum()\nresp_4 = pd.Series(day_0['resp_4']).cumsum()\n\nax.set_xlabel(\"Trade\", fontsize=18)\nax.set_title(\"Cumulative values for resp and time horizons 1,2,3&4 for day0\", fontsize=18)\nbalance.plot(lw=3)\nresp_1.plot(lw=3)\nresp_2.plot(lw=3)\nresp_3.plot(lw=3)\nresp_4.plot(lw=3)\nplt.legend(loc=\"upper left\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Descriptive statistics of the 'train.csv' file for day 0(記述統計)\n\nday_0.describe().style.background_gradient(cmap=colorMap)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n## 10,Are there any missing values?\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#day_0からスタート\n\n#misc\n#欠損値を効果的に可視化する\n#import missingno as msno\n#msno.matrixでおおまかな欠損値の出現パターンがわかる\n#sns.heatmapの情報に加えて、右側にバーが表示される(非欠損値の個数の線グラフ)\n\n#day0における、行はts-id（全取引）、列はfeaturesなどのcolumn?(138じゃないの？)\n#全ての行が68-144個の非欠損値をもつ\n#color=(r,g,b)\nmsno.matrix(day_0, color=(0.35, 0.35, 0.75));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#グラフ的に見ると、いくつかのcolumnsにmissing dataのchunks（塊）がある\n#パターンがありそうなので、\n#feature_7(resp1の最初のfeature)とfeature_11(resp2の最初のfeature)を見てみる\n#.loc:要素抽出：ラベル名可、絶対座標不可、複数要素指定可\n#.at         ：ラベル名可、絶対座標不可、複数要素指定不可\n#.iloc(iat)　 :ラベル名不可、絶対座標可\n#ちなみにat,iatの方が処理速度ははやい\n\n#絶対座標の複数の要素の値を取得\n#行の指定を「:(全体のスライス)」にすると列を参照できる(インデックス参照ではできない)\n#列番号を指定\nfeats_7_11 = day_0.iloc[:, [14,18]]\nmsno.matrix(feats_7_11, color=(0.35, 0.35, 0.75), width_ratios=(1,3));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ランダムには見えない\n#→2big chunks(各columnのはじめと途中)\n#→trading day spans：9:30-16:00\n#→missing data\n#f_7:9:30-10:03,13:17-13:33(16m)\n#f_11:9:30-9:35, 13:17-12:22(5.5m)\n\n#train.csv file全体のそれぞれのcolumnのmissing dataの数の合計を見てみる\n#数を集計\n#.isna():欠損値ならTrue　→それのSUMより、欠損値の合計数になる\n#.sort_values(ascending=False)：要素でソートする\n#デフォルトは昇順、ascending=Falseで降順（欠損値数が多い順）\nmissing_data = pd.DataFrame(train_data.isna().sum().sort_values(ascending=False),columns=['Total missing'])\nmissing_data.T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#視覚化\n#欠損値の合計データを格納\ngone = train_data.isnull().sum()\n\n#棒グラフ、\n#.values:辞書(key:values)のvalues(つまり欠損値の数)を取得\n#値に応じてcolorをつける\npx.bar(gone, color=gone.values, title=\"Total number of missing values for each columns\").show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"missing data  \n* Tag4(resp1)に79.6%\n* Tag3(resp2)に15.2%、合わせて95%\n\nresp1\n* f_7,8：393135\n* f_17,18,27,28：395535\n* f_72,78,84,90,96,102,108,114：351426\n\nresp2\n* f_21,22,31,32：81444(f_11,12と近い)\n\nよりmisssing valuesが少ないfeaturesがもっと存在する  \n→しかし大事なのはmissing valuesの量よりも、   \n似たmeasures/matrics（特徴）を示すfeaturesが教えてくれることである"},{"metadata":{},"cell_type":"markdown","source":"## 11,Is there any missing data: Days 2 and 294\nfeature_64の分散をplotすると、どの日も同じ抜本的なパターンがある  \nしかしday2は23のts_id、しかも一日の最後の方にある\n\nday0以外にもmissing dayはあるが、patternはない  \n取引数に応じて欠損値数をplot→day2と294を除いて平均的"},{"metadata":{"trusted":true},"cell_type":"code","source":"#day0が特別なのか、毎日missing dataはあるのか？\n#列番号7:137のデータをiloc\n#to_frame:すぐにデータフレーム化\nmissing_features = train_data.iloc[:, 7:137].isnull().sum(axis=1).groupby(train_data['date']).sum().to_frame()\n\n#plot\nfig, ax = plt.subplots(figsize=(15,5))\nplt.plot(missing_features)\nax.set_xlabel(\"Day\", fontsize=18)\nax.set_title(\"Total number of missing values in all features for each day\", fontsize=18)\nax.axvline(x=85, linestyle='--', alpha=0.3,c='red', lw=2)\nax.axvspan(0, 85, color=sns.xkcd_rgb['grey'], alpha=0.1)\nax.set_xlim(xmin=0)\nax.set_xlim(xmax=500)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#missing data はほぼ毎日見られ、discernible（識別可能な）pattern(weekly,monthly...)はない\n#day2,day294は例外\n\n# \"Jane Street EDA Market Regime\"\n#日ごとのtrade数をplot(大体3000～!)\n#驚くほど上のplotと似ている\n#毎日のtrade数に関してmissing valuesの数をplot\n\n#日ごとのtrade数のcolumn\ncount_weights = train_data[['date', 'weight']].groupby('date').agg(['count'])\n#日ごとのmissing data数のcolumnと結合\nresult = pd.merge(count_weights, missing_features, on = \"date\", how=\"inner\")\n#結合した後、column名を付け直す\nresult.columns = ['weights', 'missing']\n#取引数に対するmissingの割合をratioとする\nresult['ratio'] = result['missing'] / result['weights']\n#平均\nmissing_per_trade = result['ratio'].mean()\n\n#plot\nfig, ax = plt.subplots(figsize=(15,5))\nplt.plot(result['ratio'])\nplt.axhline(missing_per_trade, linestyle='--', alpha=0.85, c='r');\nax.set_xlabel(\"Day\", fontsize=18)\nax.set_title(\"Average number of missing feature values per trade,for each day\", fontsize=18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"一日に、取引ごとに平均で3つのmissing featureがある(day2,294を除く)  \nday2,294には全くmissing valueがない  \ndya14がmissing valuesが最も多い  \n\n→目に見えないtest dataのmissing dataをどうすればよいか問題  \n何をするにしても、コンペでは時間が重要なので早くやる必要あり  \n\"Optimise Speed of Filling-NaN Function\"(さらに詳しく)"},{"metadata":{},"cell_type":"markdown","source":"## 11,Is there any missing data: Days 2 and 294"},{"metadata":{"trusted":true},"cell_type":"code","source":"#2日目にマークをするために、2日目の一番最初の取引データを取得\nday2 = train_data[train_data.date == 2]\nmin2 = day2.ts_id.min()\nmax2 = day2.ts_id.max()\nmean2 = day2.ts_id.mean()\n#(min+max)_2と同じ\n\nprint(min2, max2, mean2)\n#なぜ次で15150を持ってきたかは不明","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#feature_64の散布図を作ると、それぞれの日に同じsweeping(抜本的)　patternがある \n#しかしdayには231のts_idしかなく、それは一日の本当に最後から始まっている\n\n#day1,2,3(1,3:blue 2:red)のplot\nday_1 = train_data.loc[train_data['date'] ==1]\nday_2 = train_data.loc[train_data['date'] ==2]\nday_3 = train_data.loc[train_data['date'] ==3]\nthree_days = pd.concat([day_1, day_2, day_3])\n\ntd = three_days.plot.scatter(x='ts_id', y='feature_64', s=0.5, figsize=(15,4), color='blue')\nday_2.plot.scatter(x='ts_id', y='feature_64', s=0.5, figsize=(15,4), color='red', ax=td);\n\n#さらにわかりやすく\nfig,ax = plt.subplots(figsize=(15,3))\nax.scatter(three_days.ts_id, three_days.feature_64, s=0.5, color='b')\nax.scatter(day_2.ts_id, day_2.feature_64, s=0.5, color='r')\n#(15150,5.2のところに、size=1800,facecolor(円の中の色)=noneの黒の点線の円をかく)\nax.scatter(15150, 5.2, s=1800, facecolors='none', edgecolors='black', linestyle='--', lw=2)\nax.set_xlabel('feature_64')\nax.set_ylabel('ts_id')\nax.set_title('feature_64 for days 1, 2 and 3')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"days294も同様になり、ts_idは29個しかない  \nこれも、なぜ他の日の朝食や昼食の時間に見られるmissing valuesが存在しないのかを説明している  \n→外れ値としてその2日をdropすることが価値のある処理となる可能性がある\n\n\n## 12,DABL plots (targets: action and resp)\n：automatic plots for classification&regression\nactionの**0,1はwell balanced"},{"metadata":{"trusted":true},"cell_type":"code","source":"#dabl:データ分析のベースライン的なライブラリ\n#dablを用いてday0を実行\n#まずはactionをplot\n#col=column\ndabl.plot(day_0, target_col=\"action\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#actionの0と1のクラスはほどほどに(reasonably)良いバランスである\n#次にrespを見てみる\ndabl.plot(day_0, target_col=\"resp\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#day0において、time(ts_is)に関してresp valuesをplot\n#trendline;近似曲線 ols:Ordinary Least Squares(最小二乗回帰)\n#marginal:周辺的、欄外のplot(histgramなどもできる)\n#viloin:データの分布密度を視覚化\nfig_1 = px.scatter(day_0, x=day_0['ts_id'], y=day_0['resp'],\n                  trendline=\"ols\", marginal_y=\"violin\",\n                  title=(\"Scatter plot of resp with respect tp ts_id for day 0\"))\nfig_1.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n## 13,Permutation(順序) Importance using the Random Forest\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Very quick Permutation Importance using the Random Forest\n#Random Forestを用いたとてもはやい順序の重要性\n\n## permutation importance \n#どのfeaturesが重要か、はとても大事。それををみる基本的な方法\n#この方法は早く、広く使われ理解されやすく、\n#feature　importanceの尺度に必要なproperty(性質?)と一致しているため\n#modelがfitされた後に計算される\n#respをtargetとしてregression(回帰)を実行\n\n#.contains:特定の文字列を含む行(:→列)を抽出\nX_train = day_0.loc[:, day_0.columns.str.contains('feature')]\n#fillna:欠損値を平均で穴埋め\nX_train = X_train.fillna(X_train.mean())\n\n#targetはaction\ny_train = day_0['resp']\n\nfrom sklearn.ensemble import RandomForestRegressor\n#RandomForestでfit\nregressor = RandomForestRegressor(max_features='auto')\nregressor.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"どのfeatureが重要か→day0：feature_39,43,37,5,42  \n\nNote:  \n悪いモデルにおいて重要性が低いとみなされる(cross-validation scoreが低い)features  \n→良いモデルにとってとても重要となる可能性がある  \n\n重要度を計算する前に、held-out(保持された)setを用いて  \n(もしくはcross-validationを持ちいるとより良く)モデルの予測力を評価することが大切    \n  \nPremmutation importance はそれ自体は内在的なfeatureの予測力を反映しないが、  \nあるモデルにおいてそのfeatureがどれほど重要かを反映している  \nfeatureの重要性の本格的な研究が（また、たくさんのCPUを用いるのが）重要なのは言うまでもない  \n\"Feature selection using the Boruta-SHAP package\".\nしかし、全体のfeatureの重要性のランキングは全体の物語をつづるわけではない   \n\"TabNet and interpretability: Jane Street example\"   \nそれぞれ全ての計算においてどのfeaturesが重要かを示し、  \n静的な全体のランキングが示すよりも過程がダイナミックである"},{"metadata":{},"cell_type":"markdown","source":"\n## 14,Is there any correlation between day 100 and day 200?\n各日がindependent(独立)かどうか??  \n\n→actionとrespの単純な相関は0.54(※単一でrespへの強い相関を示すfeatureはない)  \nTag28:feature120～128  \n→相関が大きいfeatureを減らせる(60代のfeatureから始めればよい)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#day100＆day200の比較\n#（時間的に離れていて、一時的な漏れ(leakage)を減らせる）  \n##Pearson pairwise correlation matrix(ピアソンのペアワイズ相関行列)\n#すごい大きな\n\n##diliverging colormapを用いる\n#赤はプラスの線形の相関を示し、青は線形の非相関を示す\n\n#day_100と200のcolumnを作り結合\nday_100 = train_data.loc[train_data['date'] == 100]\nday_200 = train_data.loc[train_data['date'] == 200]\nday_100_and_200 = pd.concat([day_100, day_200])\n#corr:相関係数を計算(pearsonの方法で)\n#.style.background_gradient:ヒートマップっぽくする\n#set_precision:表示桁数制限(小数点以下第2位まで)\nday_100_and_200.corr(method='pearson').style.background_gradient(cmap='coolwarm', axis=None).set_precision(2)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* actionの単純な定義とrespの値の間にはたった0.54の相関しか見られない  \n* また、一つでrespに強い相関を持つようなfeatureは存在しない　　\n\nSpearman's rank correlation coefficient  \nSpearmanの順位相関係数  \n→financial dataにおいてはより**apropo**（適切？）である  \nそれは単純に、そのような相関を見るために、pandas.DataFrame.corrの中の、method='pearson'をmethod='spearman'に変更することでできる  \n  \n### Tag28 section\nそこの周りをnavigateすることは、よりいくつかのかなりcurious regionsがあるように見える    \n例えば120から129の間のfeaturesをplotすると以下のようになる"},{"metadata":{"trusted":true},"cell_type":"code","source":"subset = day_100_and_200[[\"feature_120\", \"feature_121\", \"feature_122\", \"feature_123\", \"feature_124\", \"feature_125\", \"feature_126\", \"feature_127\", \"feature_128\", \"feature_129\"]]\nsubset.corr(method='pearson').style.background_gradient(cmap='coolwarm', low=1, high=0, axis=None).set_precision(2)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tag 28 features\n* feature_120&121：Tag4(resp_1)\n* feature_122&123：Tag0(resp_4)\n* feature_124&125：Tag3(resp_2)\n* feature_126&127：Tag2(resp_3)\n* feature_128&129：Tag1(resp)\nつまり、\n* resp & resp_4の線形相関：0.98\n* resp & resp_4の線形相関：0.97\n* resp & resp_4の線形相関：0.94\n* resp & resp_4の線形相関：0.89\n\n### High correlations\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#相関が|0.992|以上のfeaturesのpairをplot\n\nfeatures_day_100 = day_100.iloc[:, 7:137]\nfeatures_day_200 = day_100.iloc[:, 7:137]\nfeatures_100_and_200 = pd.concat([features_day_100, features_day_200])\n\n# code from: https://izziswift.com/list-highest-correlation-pairs-from-a-large-correlation-matrix-in-pandas/\n\n#一定の値以上の相関係数をピックアップ\ndef corrFilter(x: pd.DataFrame, bound:float):\n    xCorr = x.corr()\n    xFiltered = xCorr[((xCorr >= bound) | (xCorr <= -bound)) & (xCorr !=1.000)]\n    xFlattened = xFiltered.unstack().sort_values().drop_duplicates()\n    return xFlattened\ncorrFilter(features_100_and_200, .992).to_frame()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"我々のモデルにおいて、最終的なfeaturesの数は確実に削減できるようだ  \n始めるのに良い場所は、60台のfeatures(Tag22)を見ること  \n(それらの間にはたくさんの相関があったたため)   \n\n\n## 15,The test data\n総経過時間wall time≒3.5h　→　提出前のスクリプトのテストが重要(いつもそうだけど)  \nより小さいtest data file(36MB);example_test.csvが与えられている  \nこれは15000越えのrowsを持ち、3日間のデータを示す  \nこれはtrain.csvの中の130のfeaturesと各取引のweightからなる  \nちなみにexample_test.csvはrespデータは含まない  \n\n\n3daysというのはday0,1,2を表し、day2は一日の終わりの方のデータのみしかない\n(train.csv fileと同様)　→扱いに注意\n"},{"metadata":{},"cell_type":"markdown","source":"\n## 16,Evaluation：utility score\n\nこのコンペはutility scoreによって評価される  \nこのスコアのより詳細な説明と計算は下のnotebookを参照  \n\"Understanding the Utility Score Function\" written by Renata Ghisloti Duarte de Souza  \n\"Utility Function and Patterns in Missing Values\" written by Leonie  \n\"Jane Street: Super Fast Utility Score Function\" written by Yirun Zhang  \n\n'off-line' evaluation  \nFound the Holy Grail: GroupTimeSeriesSplit\" written by Jorijn Jacko Smit\n\"Purged Rolling Time Series CV Split\" written by marketneutral\n\n"},{"metadata":{},"cell_type":"markdown","source":"### PostScript(追記)  \n自分自身でML戦略を開発したいならば、oddsはあなたに対して積み重ねられる?  \n1つの真の投資戦略を作ることは、100を作成するくらい大変  \n複雑さは圧倒的  \nMarcos Lopez de Prado in \"Advances in Financial Machine Learning\")\n\nそれは、実際よりも数学的・規則的に見える  \nその正確さは明らかだが、不正確さは隠されている  \nその荒野は待ち構えている  "},{"metadata":{},"cell_type":"markdown","source":"If you have any problems or improvements,please comment.  \nThank you for reading.  \n\nmade by Japanese rookie momijiro"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}