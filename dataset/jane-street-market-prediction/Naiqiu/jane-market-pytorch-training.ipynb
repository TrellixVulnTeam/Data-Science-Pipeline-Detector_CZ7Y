{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install ../input/sklearn24/scikit_learn-0.24.0-cp37-cp37m-manylinux2010_x86_64.whl\n!pip freeze | grep scikit-learn","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Neural Network Starter Pytorch Version\n"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"%config Completer.use_jedi = False\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os, gc, random, time\nimport datatable as dt\nimport pickle\n# import cudf\nimport pandas as pd\nimport numpy as np\n\n# import cupy as cp\nimport janestreet\nimport xgboost as xgb\nfrom hyperopt import hp, fmin, tpe, Trials\nfrom hyperopt.pyll.base import scope\nfrom sklearn.metrics import roc_auc_score, roc_curve, log_loss\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.model_selection import TimeSeriesSplit\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nfrom joblib import dump, load\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nfrom torch.nn import CrossEntropyLoss, MSELoss\nfrom torch.nn.modules.loss import _WeightedLoss\nimport torch.nn.functional as F\n\n#!pip uninstall scikit-learn\n#!pip install scikit-learn==0.24.1\nprint('OK')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed=2021):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n#seed_everything(seed=2020)\nNeutralize = False","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading"},{"metadata":{"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"%%time\nprint('Loading...')\ntrain_datatable = dt.fread('/kaggle/input/jane-street-market-prediction/train.csv')# 用datatable会快一点\ntrain = train_datatable.to_pandas()\ndel train_datatable\n\ntrain.drop(train[(train['date']<85)].index,inplace=True)\ntrain.drop(train[(train['date']==294)].index,inplace=True)\n\nprint(\"Len: \",len(train))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"features_fill = [c for c in train.columns if 'feature' in c]\n\nprint('Filling...')\nf_mean = train[features_fill[1:]].mean()\ntrain = train.loc[train.weight > 0].reset_index(drop = True)\ntrain[features_fill[1:]] = train[features_fill[1:]].fillna(f_mean)\n\ntrain['cross_41_42_43'] = train['feature_41'] * train['feature_42'] * train['feature_43']\ntrain['cross_1_2'] = train['feature_1'] / (train['feature_2'] + 1e-5 )\n\ntrain['weight_x_resp'] =  train['weight']*train['resp']\ntrain['weight_x_resp1'] = train['weight']*train['resp_1']\ntrain['weight_x_resp2'] = train['weight']*train['resp_2']\ntrain['weight_x_resp3'] = train['weight']*train['resp_3']\ntrain['weight_x_resp4'] = train['weight']*train['resp_4']\ntrain['bias'] = 1\nbins = 100\ntrain['daily_wr_bin'] = (\n    train.groupby('date')['weight_x_resp']\n              .apply(pd.qcut, q=bins, labels=False, duplicates='drop')\n)\n\ntrain['daily_wr3_bin'] = (\n    train.groupby('date')['weight_x_resp3']\n              .apply(pd.qcut, q=bins, labels=False, duplicates='drop')\n)\ntrain['daily_wr4_bin'] = (\n    train.groupby('date')['weight_x_resp4']\n              .apply(pd.qcut, q=bins, labels=False, duplicates='drop')\n)\n\ntrain['daily_wr1_bin'] = (\n    train.groupby('date')['weight_x_resp1']\n              .apply(pd.qcut, q=bins, labels=False, duplicates='drop')\n)\ntrain['daily_wr2_bin'] = (\n    train.groupby('date')['weight_x_resp2']\n              .apply(pd.qcut, q=bins, labels=False, duplicates='drop')\n)\n\n\ntrain['action'] =  (train['daily_wr_bin'] > bins//2).astype(int)\ntrain['action1'] = (train['daily_wr1_bin'] > bins//2).astype(int)\ntrain['action2'] = (train['daily_wr2_bin'] > bins//2).astype(int)\ntrain['action3'] = (train['daily_wr3_bin'] > bins//2).astype(int)\ntrain['action4'] = (train['daily_wr4_bin'] > bins//2).astype(int)\n\n#train['action'] = (train['resp'] > 0).astype('int')\n#train['action_1'] = (train['resp_1'] > 0).astype('int')\n#train['action_2'] = (train['resp_2'] > 0).astype('int')\n#train['action_3'] = (train['resp_3'] > 0).astype('int')\n#train['action_4'] = (train['resp_4'] > 0).astype('int')\n\ntarget_cols = ['action', 'action4']#'action_1', 'action_2']\nprint('Finish.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reduce memory"},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_memory_usage(df):   \n    start_memory = df.memory_usage().sum() / 1024**2\n    print(f\"Memory usage of dataframe is {start_memory} MB\")\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != 'object':\n            c_min = df[col].min()\n            c_max = df[col].max()\n            \n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n                    \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    pass\n        else:\n            df[col] = df[col].astype('category')\n    \n    end_memory = df.memory_usage().sum() / 1024**2\n    print(f\"Memory usage of dataframe after reduction {end_memory} MB\")\n    print(f\"Reduced by {100 * (start_memory - end_memory) / start_memory} % \")\n    return df\nprint(\"Reducing Memory\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build_neutralizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_neutralizer(train, features, proportion, return_neut=False):\n    \"\"\"\n    Builds neutralzied features, then trains a linear model to predict neutralized features from original\n    features and return the coeffs of that model.\n    \"\"\"\n    neutralizer = {}\n    neutralized_features = np.zeros((train.shape[0], len(features)))\n    target = train[['resp', 'bias']].values\n    for i, f in enumerate(features):\n        # obtain corrected feature\n        feature = train[f].values.reshape(-1, 1)\n        coeffs = np.linalg.lstsq(target, feature)[0]\n        neutralized_features[:, i] = (feature - (proportion * target.dot(coeffs))).squeeze()\n        \n    # train model to predict corrected features\n    neutralizer = np.linalg.lstsq(train[features+['bias']].values, neutralized_features)[0]\n    \n    if return_neut:\n        return neutralized_features, neutralizer\n    else:\n        return neutralizer\n\ndef neutralize_array(array, neutralizer):\n    neutralized_array = array.dot(neutralizer)\n    return neutralized_array\n\n\ndef test_neutralization():\n    dummy_train = train.loc[:100000, :]\n    proportion = 1.0\n    neutralized_features, neutralizer = build_neutralizer(dummy_train, features, proportion, True)\n    dummy_neut_train = neutralize_array(dummy_train[features+['bias']].values, neutralizer)\n    \n#     assert np.array_equal(neutralized_features, dummy_neut_train)\n    print(neutralized_features[0, :10], dummy_neut_train[0, :10])\n\nif Neutralize:\n    proportion = 1.0\n    neutralizer = build_neutralizer(train, features, proportion)\n    train[features] = neutralize_array(train[features+['bias']].values, neutralizer)\ntrain = reduce_memory_usage(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features=['feature_89',\n 'feature_67',\n 'feature_115',\n 'feature_72',\n 'feature_44',\n 'feature_91',\n 'feature_79',\n 'feature_78',\n 'feature_80',\n 'feature_3',\n 'feature_37',\n 'feature_86',\n 'feature_77',\n 'feature_19',\n 'feature_60',\n 'feature_9',\n 'feature_50',\n 'feature_0',\n 'feature_51',\n 'feature_29',\n 'feature_73',\n 'feature_90',\n 'feature_70',\n 'feature_17',\n 'feature_31',\n 'feature_109',\n 'feature_85',\n 'feature_121',\n 'feature_62',\n 'feature_39',\n 'feature_95',\n 'feature_46',\n 'feature_56',\n 'feature_7',\n 'feature_87',\n 'feature_42',\n 'feature_92',\n 'feature_103',\n 'feature_53',\n 'feature_21',\n 'feature_43',\n 'feature_104',\n 'feature_75',\n 'feature_81',\n 'feature_52',\n 'feature_55',\n 'feature_64',\n 'feature_27',\n 'feature_13',\n 'feature_1',\n 'feature_98',\n 'feature_84',\n 'feature_65',\n 'feature_54',\n 'feature_5',\n 'feature_45',\n 'feature_41',\n 'feature_11',\n 'feature_74',\n  'cross_41_42_43',\n  'cross_1_2']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"dropout_rates = [0.10143786981358652, 0.19720339053599725, 0.2703017847244654, 0.23148340929571917, 0.2357768967777311]\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.batch_norm0 = nn.BatchNorm1d(len(features))\n        self.dropout0 = nn.Dropout(dropout_rates[0])\n\n        dropout_rate = 0.2\n        hidden_size = 256\n        self.dense1 = nn.Linear(len(features), hidden_size)\n        self.batch_norm1 = nn.BatchNorm1d(hidden_size)\n        self.dropout1 = nn.Dropout(dropout_rates[1])\n\n        self.dense2 = nn.Linear(hidden_size+len(features), hidden_size)\n        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n        self.dropout2 = nn.Dropout(dropout_rates[2])\n\n        self.dense3 = nn.Linear(hidden_size+hidden_size, hidden_size)\n        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n        self.dropout3 = nn.Dropout(dropout_rates[3])\n\n        self.dense4 = nn.Linear(hidden_size+hidden_size, hidden_size)\n        self.batch_norm4 = nn.BatchNorm1d(hidden_size)\n        self.dropout4 = nn.Dropout(dropout_rates[4])\n\n        self.dense5 = nn.Linear(hidden_size+hidden_size, len(target_cols))\n\n        self.Relu = nn.ReLU(inplace=True)\n        self.PReLU = nn.PReLU()\n        self.LeakyReLU = nn.LeakyReLU(negative_slope=0.01, inplace=True)\n        # self.GeLU = nn.GELU()\n        self.RReLU = nn.RReLU()\n\n    def forward(self, x):\n        x = self.batch_norm0(x)\n        x = self.dropout0(x)\n\n        x1 = self.dense1(x)\n        x1 = self.batch_norm1(x1)\n        x1 = self.LeakyReLU(x1)\n        x1 = self.dropout1(x1)\n\n        x = torch.cat([x, x1], 1)\n\n        x2 = self.dense2(x)\n        x2 = self.batch_norm2(x2)\n        # x = F.relu(x)\n        # x = self.PReLU(x)\n        x2 = self.LeakyReLU(x2)\n        x2 = self.dropout2(x2)\n\n        x = torch.cat([x1, x2], 1)\n\n        x3 = self.dense3(x)\n        x3 = self.batch_norm3(x3)\n        # x = F.relu(x)\n        # x = self.PReLU(x)\n        x3 = self.LeakyReLU(x3)\n        x3 = self.dropout3(x3)\n\n        x = torch.cat([x2, x3], 1)\n\n        x4 = self.dense4(x)\n        x4 = self.batch_norm4(x4)\n        # x = F.relu(x)\n        # x = self.PReLU(x)\n        x4 = self.LeakyReLU(x4)\n        x4 = self.dropout4(x4)\n\n        x = torch.cat([x3, x4], 1)\n\n        x = self.dense5(x)\n\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n    model.train()\n    final_loss = 0\n\n    for data in dataloader:\n        optimizer.zero_grad()\n        features = data['features'].to(device)\n        label = data['label'].to(device)\n        outputs = model(features)\n        #print('OUTPUTS shape: ',outputs.shape,type(outputs))\n        #print('Label shape: ',label.shape,type(label))\n        loss = loss_fn(outputs, label)\n        loss.backward()\n        optimizer.step()\n        if scheduler:\n            scheduler.step()\n\n        final_loss += loss.item()\n\n    final_loss /= len(dataloader)\n\n    return final_loss\n\ndef inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n\n    for data in dataloader:\n        features = data['features'].to(device)\n        with torch.no_grad():\n            outputs = model(features)\n\n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n\n    preds = np.concatenate(preds)\n\n    return torch.tensor(preds,dtype=torch.float64)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"class MarketDataset:\n    def __init__(self, df,features,labels):\n        self.features = df[features].values\n        self.label = df[labels].values\n\n    def __len__(self):\n        return len(self.label)\n\n    def __getitem__(self, idx):\n        return {\n            'features': torch.tensor(self.features[idx].astype(np.float64), dtype=torch.float),\n            'label': torch.tensor(self.label[idx].astype(np.float64), dtype=torch.float)\n        }\n\nclass SmoothBCEwLogits(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n        assert 0 <= smoothing < 1\n        with torch.no_grad():\n            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n        return targets\n\n    def forward(self, inputs, targets):\n        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n            self.smoothing)\n        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss\n    \n# Ealrly stop stratagem    \nclass EarlyStopping:\n    def __init__(self, patience=7, mode=\"max\", delta=0.):\n        self.patience = patience\n        self.counter = 0\n        self.mode = mode\n        self.best_score = None\n        self.early_stop = False\n        self.delta = delta\n        if self.mode == \"min\":\n            self.val_score = np.Inf\n        else:\n            self.val_score = -np.Inf\n\n    def __call__(self, epoch_score, model, model_path):\n\n        if self.mode == \"min\":\n            score = -1.0 * epoch_score\n        else:\n            score = np.copy(epoch_score)\n \n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(epoch_score, model, model_path)\n        elif score < self.best_score: #  + self.delta\n            self.counter += 1\n            print('EarlyStopping counter: {} out of {}'.format(self.counter, self.patience))\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            # ema.apply_shadow()\n            self.save_checkpoint(epoch_score, model, model_path)\n            # ema.restore()\n            self.counter = 0\n\n    def save_checkpoint(self, epoch_score, model, model_path):\n        if epoch_score not in [-np.inf, np.inf, -np.nan, np.nan]:\n            print('Validation score improved ({} --> {}). Saving model!'.format(self.val_score, epoch_score))\n            torch.save(model.state_dict(), model_path)\n        self.val_score = epoch_score\n        \n        \ndef utility_score_bincount(date, weight, resp, action):\n    count_i = len(np.unique(date))\n    Pi = np.bincount(date, weight * resp * action)\n    t = np.sum(Pi) / np.sqrt(np.sum(Pi ** 2)) * np.sqrt(250 / count_i)\n    u = np.clip(t, 0, 6) * np.sum(Pi)\n    return u","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nfrom sklearn.model_selection import TimeSeriesSplit\ntscv = TimeSeriesSplit(n_splits=5,max_train_size=1241711)\nprint(tscv)\nprint(len(train))\n#gkf = PurgedGroupTimeSeriesSplit(n_splits = FOLDS, group_gap=20)\n#splits = list(gkf.split(y, groups=train['date'].values))\nfor fold, (train_indices, test_indices) in enumerate(tscv.split(train)):\n    print('Fold: ',fold)\n    print('train_indices:{} Len:{}'.format(train_indices,len(train_indices)))\n    print('test_indices:{}  Len:{}'.format(test_indices,len(test_indices)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 4096\nlabel_smoothing = 1e-3\nlearning_rate = 1e-3\nepochs = 50\nseed_everything(seed=2020)\n#feat_cols = [f'feature_{i}' for i in range(130)]\n\nfrom sklearn.model_selection import TimeSeriesSplit\ntscv = TimeSeriesSplit(n_splits=5,max_train_size=1241711)\nstart_time = time.time()\noof = np.zeros(len(train['action']))\n\nfor fold, (tr, te) in enumerate(tscv.split(train)):\n    train_set = MarketDataset(train.loc[tr],features,target_cols)\n    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=4)\n    valid_set = MarketDataset(train.loc[te],features,target_cols)\n    valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=False, num_workers=4)\n    \n    torch.cuda.empty_cache()\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    model = Model()\n    model.to(device)\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n    loss_fn = SmoothBCEwLogits(smoothing=label_smoothing)\n    \n    ckp_path = f'JSModel_{fold}.pth'\n    es = EarlyStopping(patience=epochs/5, mode=\"max\")\n    for epoch in range(epochs):\n        train_loss = train_fn(model, optimizer, None, loss_fn, train_loader, device)\n        valid_pred = inference_fn(model, valid_loader, device)\n        auc_score = 0\n        for i in range(len(target_cols)):\n            auc_score+=roc_auc_score((train.loc[te][target_cols[i]] > 0).astype('int').values.reshape(-1, 1), valid_pred[:,i])/len(target_cols)\n        #logloss_score = log_loss((train.loc[te]['resp'] > 0).astype('int').values.reshape(-1, 1), valid_pred)\n        logloss_score = loss_fn(valid_pred,torch.tensor(np.array(train.loc[te][target_cols].values),dtype=torch.float64))\n        #valid_pred = np.where(valid_pred >= 0.5, 1, 0).astype(int)\n        u_score = utility_score_bincount(date=train.loc[te].date.values, weight=train.loc[te].weight.values, \n                                         resp=train.loc[te].resp.values, action=np.where(valid_pred.mean(axis=1) >= 0.5, 1, 0).astype(int))\n\n        print(f\"FOLD{fold} EPOCH:{epoch:3}, train_loss:{train_loss:.5f},auc:{auc_score:.5f},u_score:{u_score:.5f}, \"\n              f\"time: {(time.time() - start_time) / 60:.2f}min\")\n        \n        es(u_score, model, model_path=ckp_path)\n        if es.early_stop:\n            print(\"Early stopping\")\n            break\n#     break # only train 1 model for fast, you can remove it to train 5 folds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tensorflow Part"},{"metadata":{"trusted":true},"cell_type":"code","source":"class EarlyStopping_Tensorflow:\n    def __init__(self, patience=7, mode=\"max\", delta=0.):\n        self.patience = patience\n        self.counter = 0\n        self.mode = mode\n        self.best_score = None\n        self.early_stop = False\n        self.delta = delta\n        if self.mode == \"min\":\n            self.val_score = np.Inf\n        else:\n            self.val_score = -np.Inf\n\n    def __call__(self, epoch_score, model, model_path):\n\n        if self.mode == \"min\":\n            score = -1.0 * epoch_score\n        else:\n            score = np.copy(epoch_score)\n \n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(epoch_score, model, model_path)\n        elif score < self.best_score: #  + self.delta\n            self.counter += 1\n            #print('EarlyStopping counter: {} out of {}'.format(self.counter, self.patience))\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            # ema.apply_shadow()\n            self.save_checkpoint(epoch_score, model, model_path)\n            # ema.restore()\n            self.counter = 0\n\n    def save_checkpoint(self, epoch_score, model, model_path):\n        if epoch_score not in [-np.inf, np.inf, -np.nan, np.nan]:\n            print('Validation score improved ({} --> {}). Saving model!'.format(self.val_score, epoch_score))\n            clf.save('{}'.format(model_path))\n        self.val_score = epoch_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Concatenate, Lambda, GaussianNoise, Activation\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers.experimental.preprocessing import Normalization\nimport tensorflow as tf\nimport tensorflow_addons as tfa\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom random import choices\n\n\nSEED = 2020\n\nnp.random.seed(SEED)\n\nX_train = train[features]\n#y_train = (train.loc[:, 'action'])\n\ny_train = train[target_cols]\n\n# fit\ndef create_mlp(\n    num_columns, num_labels, hidden_units, dropout_rates, label_smoothing, learning_rate\n):\n\n    inp = tf.keras.layers.Input(shape=(num_columns,))\n    x = tf.keras.layers.BatchNormalization()(inp)\n    x = tf.keras.layers.Dropout(dropout_rates[0])(x)\n    for i in range(len(hidden_units)):\n        x = tf.keras.layers.Dense(hidden_units[i])(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n        x = tf.keras.layers.Dropout(dropout_rates[i + 1])(x)\n    \n    x = tf.keras.layers.Dense(num_labels)(x)\n    out = tf.keras.layers.Activation(\"sigmoid\")(x)\n\n    model = tf.keras.models.Model(inputs=inp, outputs=out)\n    model.compile(\n        optimizer=tfa.optimizers.RectifiedAdam(learning_rate=learning_rate),\n        loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=label_smoothing),\n        metrics=tf.keras.metrics.AUC(name=\"AUC\"),\n    )\n\n    return model\n\nepochs = 200\nbatch_size = 4096\nhidden_units = [160, 160, 160]\ndropout_rates = [0.2, 0.2, 0.2, 0.2]\nlabel_smoothing = 1e-2\nlearning_rate = 1e-3\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import TimeSeriesSplit\ntscv = TimeSeriesSplit(n_splits=5,max_train_size=1241711)\nstart_time = time.time()\nfor fold, (tr, te) in enumerate(tscv.split(train)):\n    \n    tf.keras.backend.clear_session()\n    tf.random.set_seed(SEED)\n    clf = create_mlp(\n    len(features), len(target_cols), hidden_units, dropout_rates, label_smoothing, learning_rate\n    )\n    \n    X_train = train[features].loc[tr]\n    X_valid = train[features].loc[te]\n#y_train = (train.loc[:, 'action'])\n    y_train = train[target_cols].loc[tr]\n    y_valid = train[target_cols].loc[te]\n    \n    ckp_path = f'TensorModel_{fold}.h5'\n    es = EarlyStopping_Tensorflow(patience=5, mode=\"max\")\n\n    for i in range(50):\n        clf.fit(X_train, y_train, epochs=3, batch_size=batch_size, verbose=2)\n        y_pred = clf.predict(X_valid)\n        \n        u_score = utility_score_bincount(date=train.loc[te].date.values, weight=train.loc[te].weight.values, \n                                         resp=train.loc[te].resp.values, action=np.where(y_pred.mean(axis=1) >= 0.5, 1, 0).astype(int))\n        es(u_score, clf, model_path=ckp_path)\n        if es.early_stop:\n            print(\"Early stopping\")\n            break","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Just upvoting if it helps!!!"},{"metadata":{"trusted":true},"cell_type":"code","source":"for _fold in range(5):\n        torch.cuda.empty_cache()\n        model = Model()\n        model.to(device)\n        model_weights = f\"{CACHE_PATH}/online_model{_fold}.pth\"\n        model.load_state_dict(torch.load(model_weights))\n        model.eval()\n        model_list.append(model)\nmodel.load_state_dict(\n)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}