{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis Jane Street\n\n<img src=\"https://www.europol.europa.eu/sites/default/files/images/finance_budget.jpg\">\n\n\n## Hi all! I have decided to try my best at exploring the data. I'm eager to share with you what I've found and to receive your feedback even more! "},{"metadata":{},"cell_type":"markdown","source":"# Imports and Data Loading\n\nHere we just install necessary packages (datatable), import them, define functions for later usage and load train.csv data. \n\n**Train float64 columns will be downcasted to float32 to ease RAM burden**\n"},{"metadata":{"trusted":true,"collapsed":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"!pip install datatable\n!pip install MLXtend","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tqdm\nimport itertools\nfrom mlxtend.preprocessing import TransactionEncoder\nfrom mlxtend.frequent_patterns import apriori\nimport matplotlib.pyplot as plt\nplt.rcParams.update({'figure.max_open_warning': 0})\nplt.style.use('fivethirtyeight')\nimport seaborn as sns\npd.options.display.max_columns = 200\nimport os\nimport gc\nimport re\nimport datatable as dt\ndef chunks(l, n):\n    \"\"\" Yield n successive chunks from l.\n    \"\"\"\n    newn = int(len(l) / n)\n    for i in range(0, n-1):\n        yield l[i*newn:i*newn+newn]\n    yield l[n*newn-newn:]\n    \ninput_path = '/kaggle/input/'\nroot_path = os.path.join(input_path, 'jane-street-market-prediction')\ntrain = dt.fread(os.path.join(root_path, \"train.csv\")).to_pandas()\nfloat64_cols = train.select_dtypes('float64').columns\ntrain[float64_cols] = train[float64_cols].astype('float32')\nresp_cols = [i for i in train.columns if 'resp' in i]\n\nfeatures_names = list(set(train.columns) - set(resp_cols) - set(['weight', 'ts_id', 'date']))\nfeatures_index = list(map(lambda x: int(re.sub(\"feature_\", \"\", x)), features_names))\nfeatures = sorted(list(zip(features_names, features_index)), key = lambda x: x[1])\nfeatures = [i[0] for i in features] + resp_cols","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Basic Train Exploration\n\nHere I start looking at how train.csv looks like: \n\n- dtypes and memory usage\n- nan values distribution\n- unique values distribution\n\n- feature density/boxplot distribution\n- feature distribution through time\n\n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"display(train.info(), train.dtypes.value_counts().to_frame().rename(columns = {0: 'dtype'}))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#count\nnan_values_train = (train\n .apply(lambda x: x.isna().sum(axis = 0)/len(train))\n .to_frame()\n .rename(columns = {0: 'percentage_nan_values'})\n.sort_values('percentage_nan_values', ascending = False)\n)\n\ndisplay((train\n .apply(lambda x: x.isna().sum(axis = 0))\n .to_frame()\n .rename(columns = {0: 'count_nan_values'})\n.sort_values('count_nan_values', ascending = False)\n.transpose()), nan_values_train.transpose())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (20, 12))\n\nsns.set_palette(\"RdBu\", 10)\n#RdBu, YlGn\nax = sns.barplot(x='percentage_nan_values', \n            y='feature', \n            palette = 'GnBu_r',\n            data=nan_values_train.reset_index().rename(columns = {'index': 'feature'}).head(40))\n\nfor p in ax.patches:\n    width = p.get_width() \n    if width < 0.01:# get bar length\n        ax.text(width,       # set the text at 1 unit right of the bar\n            p.get_y() + p.get_height() / 2, # get Y coordinate + X coordinate / 2\n            '{:1.4f}'.format(width), # set variable to display, 2 decimals\n            ha = 'left',   # horizontal alignment\n            va = 'center')  # vertical alignment\n    else:\n        if width < 0.03:\n            color_text = 'black'\n        else:\n            color_text = 'white'\n        ax.text(width /2, \n                # set the text at 1 unit right of the bar\n            p.get_y() + p.get_height() / 2, # get Y coordinate + X coordinate / 2\n            '{:1.4f}'.format(width), # set variable to display, 2 decimals\n            ha = 'left',   # horizontal alignment\n            va = 'center',\n            color = 'white',\n            fontsize = 11)  # vertical alignment\n\nax.set_title('Top 40 Features for percentage of NaN Values')\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://images.emojiterra.com/google/android-10/512px/1f914.png\" width=\"50\" height=\"50\" style=\"top:03%; left:80%\"> \n\nSome features have almost the same number of nan values. Let's plot their distribution over time.\n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"top_nan_features = (nan_values_train.head(30).index.tolist())\ndel nan_values_train\nfig, axes = plt.subplots(10, 3, figsize = (40, 30))\nax = axes.ravel()\n\nts_id = train.ts_id\nmini_df = pd.concat([(train[top_nan_features].isna().astype(int)),train[['date']]], 1)\nmini_df = mini_df.groupby('date').sum().reset_index()\n\nfor i in range(len(top_nan_features)):\n    \n    feature_name = top_nan_features[i]\n    \n    mini_df[[feature_name, \"date\"]].plot(y = feature_name , kind = 'line',\n                                         xlabel = 'date', \n                                         ylabel = feature_name+ \"_nans\", linewidth=0.3,\n                                         legend = False,\n                                         ax = ax[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['daily_ts_id'] = (train.groupby('date').cumcount())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig, axes = plt.subplots(10, 3, figsize = (40, 30))\nax = axes.ravel()\n\nts_id = train.ts_id\nmini_df = pd.concat([(train[top_nan_features].isna().astype(int)),train[['ts_id']]], 1).iloc[:50000, :]\nnew_day = (train.iloc[:50000, :].query(\"daily_ts_id == 0\").ts_id.tolist())\n\nfor i in range(len(top_nan_features)):\n    \n    feature_name = top_nan_features[i]\n    \n    mini_df[[feature_name, \"ts_id\"]].plot(y = feature_name , kind = 'line',\n                                         xlabel = 'ts_id', \n                                         ylabel = feature_name+ \"_nans\", linewidth=0.3,\n                                         legend = False,\n                                         ax = ax[i])\n    for m in range(len(new_day)):\n        ax[i].axvline(new_day[m], alpha = 0.5, ymin = 0, ymax = 1, linestyle = \":\", color = 'blue')\n        if i == 0:\n            if m == 2:\n                ax[i].text(new_day[m]-1500, 1.1, \"day {}\".format(m), size = 7, alpha = 0.8)\n            else:\n                ax[i].text(new_day[m]+200, 1.1, \"day {}\".format(m), size = 7, alpha = 0.8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://images.emojiterra.com/google/android-10/512px/1f914.png\" width=\"50\" height=\"50\" style=\"top:03%; left:80%\"> \n\nIt seems like for most of the features nan happen in the first part of the day (each blue line represents the beginning of a new day).\n"},{"metadata":{},"cell_type":"markdown","source":"Unique values per column in Train"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#percentage\ntrain_unique_values = (train\n .apply(lambda x: x.nunique())\n .to_frame()\n .rename(columns = {0: 'number_unique_values'})\n.sort_values('number_unique_values', ascending = True)\n)\n\ndisplay(train_unique_values.transpose())\ndel train_unique_values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's plot all features and then deepdive on the relationship between them"},{"metadata":{},"cell_type":"markdown","source":"<h4> Features Distributions </h4>"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"feature_chunks = list(chunks(range(len(features)), 15))\n\nfor j in feature_chunks:\n\n    fig, axes = plt.subplots(3, 3, figsize = (20, 12))\n    ax = axes.ravel()\n\n    for i in j:\n\n        feature = train[features[i]]\n    \n        sns.distplot(feature, hist=True, kde=True, \n         #bins=35, \n         color = 'blue', \n         hist_kws={'edgecolor':'black'},\n         kde_kws={'linewidth': 2}, ax = ax[i%9])\n        ax[i%9].grid(True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Boxplots to further understand each feature distribution"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"for j in feature_chunks:\n\n    fig, axes = plt.subplots(3, 3, figsize = (20, 12))\n    ax = axes.ravel()\n\n    for i in j:\n\n        feature = train[features[i]]\n    \n        sns.boxplot(feature, \n         color = 'blue', ax = ax[i%9])\n        ax[i%9].grid(True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://images.emojiterra.com/google/android-10/512px/1f914.png\" width=\"20\" height=\"20\" style=\"left\"> \n\n*Most of the features have long-tailed distributions, some of them in both directions, while others just on the positive axis. \nThere are also some bimodal distributions, it would be nice to inspect their relationship with time*\n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"for j in feature_chunks:\n\n    fig, axes = plt.subplots(3, 3, figsize = (20, 12))\n    ax = axes.ravel()\n\n    for i in j:\n\n        mini_df = (train.groupby('date').agg({features[i]: ['mean', 'std']}))\n        mini_df.iloc[:, 0].plot(kind = 'line',xlabel = 'date', \n                                ylabel = features[i], linewidth=1,\n                               ax = ax[i%9])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://images.emojiterra.com/google/android-10/512px/1f914.png\" width=\"20\" height=\"20\" style=\"left\"> \n\n*Most of the features have a different behaviour before and after date 200, being more spiky before and almost stationary after.*\n"},{"metadata":{},"cell_type":"markdown","source":"# Features relationships\n\nHere I try to go a little bit deeper in inspecting the relationships between features and between feature and time. \n\n- Features Correlations\n- Features AutoCorrelation\n- Features CrossCorrelation"},{"metadata":{},"cell_type":"markdown","source":"Let's create a new columns which gives the daily_tsid, starting each date from 0 and squashing it between 0-1 "},{"metadata":{"trusted":true},"cell_type":"code","source":"train['daily_ts_id'] = (train.groupby('date').cumcount())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"gc.collect() #Let's free some RAM up\ncreated_cols = [] \ncorrelation_matrix = train.drop(created_cols, axis = 1, errors = 'ignore').corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize = (30, 12))\n\nax = sns.heatmap(\n    correlation_matrix, \n    vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(220, 20, n=200),\n    square=True\n)\nax.set_xticklabels(\n    ax.get_xticklabels(),\n    {'fontsize': 5},\n    rotation=90,\n    horizontalalignment='right'\n);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://images.emojiterra.com/google/android-10/512px/1f914.png\" width=\"20\" height=\"20\" style=\"top:03%; left:80%\"> \n\n*It seems like there are group of features very correlated, most of them are also near each other as we can see around the diagonal.\nDate and ts_id are very much correlated, but I don't see any other feature particularly correlated with them.*"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"corrMatrix=correlation_matrix.loc[features, features].copy()\n\ncorrMatrix.loc[:,:] =  np.tril(corrMatrix, k=-1) # borrowed from Karl D's answer\nCORRELATION_THRESHOLD = 0.65\nalready_in = set()\npositive_correlated_features = []\nfor col in corrMatrix:\n    perfect_corr = corrMatrix[col][corrMatrix[col] > CORRELATION_THRESHOLD].index.tolist()\n    if perfect_corr and col not in already_in:\n        already_in.update(set(perfect_corr))\n        perfect_corr.append(col)\n        positive_correlated_features.append(perfect_corr)\n\n\nte = TransactionEncoder()\n\nte_ary = te.fit(positive_correlated_features).transform(positive_correlated_features)\n\ndf = pd.DataFrame(te_ary, columns=te.columns_)\n\nfrequent_itemsets = (apriori(df, min_support=0.03, use_colnames=True))\nfrequent_itemsets = frequent_itemsets.loc[frequent_itemsets.itemsets.apply(lambda x: len(x) > 2)].reset_index(drop = True)\npd.options.display.max_colwidth = 300","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (30, 10))\n#all_but_time = list(set(train.columns) - set(['date', 'ts_id', 'daily_ts_id']))\nax = sns.heatmap(\n    correlation_matrix.loc[['date', 'ts_id', 'daily_ts_id'], features], \n    vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(220, 20, n=200),\n    square=False\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://images.emojiterra.com/google/android-10/512px/1f914.png\" width=\"20\" height=\"20\" style=\"top:03%; left:80%\"> \n\n*There are definitely some features very correlated with daily_ts_id. Let's plot a few of them*"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"abs_daily = (correlation_matrix.loc[features, 'daily_ts_id'].to_frame()\n.rename(columns = {'daily_ts_id' : 'correlation'}).assign(abs_corr=lambda x: abs(x)))\nfeatures_daily = sorted(abs_daily.sort_values('abs_corr', ascending = False).head(5).index.tolist())\n\nfig, axes = plt.subplots(5, 1, figsize = (15, 10), sharex = True)\nnew_day = (train.iloc[:50000, :].query(\"daily_ts_id == 0\").ts_id.tolist())\nax = axes.ravel()\nfor j in range(len(features_daily)):\n    feature_name = features_daily[j]\n    feature = train[feature_name]\n    feature[:50000].plot(linewidth = 0.5, ax = ax[j], xlabel='ts_id', ylabel = feature_name)\n    for m in range(len(new_day)):\n        ax[j].axvline(new_day[m], alpha = 0.5, ymin = -5, ymax = 5, linestyle = \":\", color = 'blue')\n        if j == 0:\n            if m == 2:\n                ax[j].text(new_day[m]-1500, 5.1, \"day {}\".format(m), size = 7, alpha = 0.8)\n            else:\n                ax[j].text(new_day[m]+200, 5.1, \"day {}\".format(m), size = 7, alpha = 0.8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://images.emojiterra.com/google/android-10/512px/1f914.png\" width=\"20\" height=\"20\" style=\"top:03%; left:80%\"> \n\n*Vertical blue lines indicate a new day. These features are supersimilar and definitely seem to have a positive correlation with daily_ts_id. I don't have a clue about their actual meaning*"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def get_redundant_pairs(df):\n    '''Get diagonal and lower triangular pairs of correlation matrix'''\n    pairs_to_drop = set()\n    cols = df.columns\n    for i in range(0, df.shape[1]):\n        for j in range(0, i+1):\n            pairs_to_drop.add((cols[i], cols[j]))\n    return pairs_to_drop\n\ndef get_top_correlations(df, percentage=0.1):\n    au_corr = df.corr().unstack()\n    labels_to_drop = get_redundant_pairs(df)\n    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n    n = int(len(au_corr)*percentage)\n    return au_corr[0:n]\n\nfeatures_without_resp = list(set(features) - set(resp_cols))\ndf_correlation_plot = correlation_matrix.loc[features_without_resp, features_without_resp]\n\ndf_correlation_plot = (get_top_correlations(df_correlation_plot, percentage = 1)\n                      .reset_index(drop = False)\n                      .rename(columns = {0: 'correlation'}))\n\ndf_correlation_plot['index_1'] = df_correlation_plot.level_0.str.replace(\"feature_\", \"\").astype(int)\ndf_correlation_plot['index_2'] = df_correlation_plot.level_1.str.replace(\"feature_\", \"\").astype(int)\ndf_correlation_plot['absolute_correlation'] = df_correlation_plot['correlation'].abs()\ndf_correlation_plot['sign_correlation'] = (df_correlation_plot['correlation'] > 0).astype(int).replace({1: 'positive', 0:'negative'})\n\ndf_correlation_plot['pair_of_features'] = df_correlation_plot['index_1'].astype(str) + \"-\" + df_correlation_plot['index_2'].astype(str) \ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig, axes = plt.subplots(1, 2, figsize = (20, 12))\nax = axes.ravel()\nsns.set_palette(\"RdBu\", 10)\n#RdBu, YlGn\npositive_dict = {0: 'positive', 1: 'negative'}\nfor j in range(2):\n    if j == 0:\n        sns.barplot(x='correlation', \n            y='pair_of_features',\n            ax = ax[j],\n            data=df_correlation_plot.head(40))\n    else:\n        sns.barplot(x='correlation', \n            y='pair_of_features',\n            ax = ax[j],\n            data=df_correlation_plot.tail(40))\n\n    for p in ax[j].patches:\n        width = p.get_width() \n        ax[j].text(width /2, \n                # set the text at 1 unit right of the bar\n            p.get_y() + p.get_height() / 2, # get Y coordinate + X coordinate / 2\n            '{:1.4f}'.format(width), # set variable to display, 2 decimals\n            ha = 'left',   # horizontal alignment\n            va = 'center',\n            color = 'black',\n            fontsize = 11)  # vertical alignment\n\n    ax[j].set_title('Top 40 pair of features for {} correlation'.format(positive_dict[j]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://images.emojiterra.com/google/android-10/512px/1f914.png\" width=\"20\" height=\"20\" style=\"top:03%; left:80%\"> \n\n*So many features are highly correlated, let's see the correlation distribution*"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize = (12, 8))\nquantiles = np.quantile(df_correlation_plot.correlation, [0.05, 0.25, 0.5, 0.75, 0.95])\nsns.distplot(df_correlation_plot.correlation, hist=False, kde=True, \n             color = 'blue', hist_kws={'edgecolor':'black'},\n             kde_kws={'linewidth': 1}, ax = ax)\nfor j in range(len(quantiles)):\n    ax.axvline(quantiles[j], alpha = 0.3, ymax = 2, linestyle = \":\")\nax.grid(True)\nax.text(quantiles[0]-.1, 0.37, \"5th\", size = 10, alpha = 0.8)\nax.text(quantiles[1]-.1, 0.37, \"25th\", size = 10, alpha = 0.85)\nax.text(quantiles[2]-.1, 0.37, \"50th\", size = 10, alpha = 1)\nax.text(quantiles[3]-.1, 0.37, \"75th\", size = 10, alpha = 0.85)\nax.text(quantiles[4]-.1, 0.37, \"95th\", size = 10, alpha =.8)\nax.set_title('Correlation distribution')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Distribution seems pretty symmetric. "},{"metadata":{},"cell_type":"markdown","source":"Autocorrelation for different lags"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"gc.collect()\nautocorr_dataframe = (pd.DataFrame(train.apply(lambda x: x.autocorr(), 0))\n                     .reset_index().rename(columns = {'index': 'feature', 0: 'autocorrelation'})\n                     .sort_values('autocorrelation', ascending = False))\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (20, 12))\n\nsns.set_palette(\"RdBu\", 10)\n#RdBu, YlGn\nax = sns.barplot(x='autocorrelation', \n            y='feature', \n           # palette = 'GnBu_r',\n            data=(autocorr_dataframe\n                  .head(40)))\n\nfor p in ax.patches:\n    width = p.get_width() \n    if width < 0.01:# get bar length\n        ax.text(width,       # set the text at 1 unit right of the bar\n            p.get_y() + p.get_height() / 2, # get Y coordinate + X coordinate / 2\n            '{:1.4f}'.format(width), # set variable to display, 2 decimals\n            ha = 'left',   # horizontal alignment\n            va = 'center')  # vertical alignment\n    else:\n        ax.text(width /2, \n                # set the text at 1 unit right of the bar\n            p.get_y() + p.get_height() / 2, # get Y coordinate + X coordinate / 2\n            '{:1.4f}'.format(width), # set variable to display, 2 decimals\n            ha = 'left',   # horizontal alignment\n            va = 'center',\n            color = 'black',\n            fontsize = 11)  # vertical alignment\n\nax.set_title('Top 40 Features for autocorrelation')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize = (12, 8))\nquantiles = np.quantile(autocorr_dataframe.autocorrelation, [0.05, 0.25, 0.5, 0.75, 0.95])\nsns.distplot(autocorr_dataframe.autocorrelation, hist=False, kde=True, \n             color = 'blue', hist_kws={'edgecolor':'black'},\n             kde_kws={'linewidth': 1}, ax = ax)\nfor j in range(len(quantiles)):\n    ax.axvline(quantiles[j], alpha = 0.3, ymax = 2, linestyle = \":\")\nax.grid(True)\nax.text(quantiles[0]-.05, 0.27, \"5th\", size = 10, alpha = 0.8)\nax.text(quantiles[1]-.05, 0.37, \"25th\", size = 10, alpha = 0.85)\nax.text(quantiles[2]-.05, 0.37, \"50th\", size = 10, alpha = 1)\nax.text(quantiles[3]-.05, 0.37, \"75th\", size = 10, alpha = 0.85)\nax.text(quantiles[4]-.05, 0.37, \"95th\", size = 10, alpha =.8)\nax.set_title('Autocorrelation Lag 1 distribution')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cross Correlation"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def crosscorr(datax, datay, lag=0):\n    \"\"\" Lag-N cross correlation. \n    Parameters\n    ----------\n    lag : int, default 0\n    datax, datay : pandas.Series objects of equal length\n\n    Returns\n    ----------\n    crosscorr : float\n    \"\"\"\n    return datax.corr(datay.shift(lag))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"RECALCULATE = False\nif RECALCULATE:\n\n    total_lags = range(1, 3)\n\n    cross_corr = {}\n\n    combinations = list(itertools.product(features, features))\n\n    for j in total_lags:\n        cross_corr[j] = []\n        for k in tqdm.tqdm(combinations):\n            cross_corr[j].append(crosscorr(train[k[0]], train[k[1]], lag = j))\n        #cross_corr[j] = list(map(lambda x: crosscorr(train[x[0]], train[x[1]], j), combinations))\n\n    cross_correlations = (pd.DataFrame(combinations)\n                          .rename(columns = {0: 'first_feature', 1: 'second_feature'}))\n\n    cross_correlations_melt = (pd.melt(cross_correlations, id_vars=['first_feature', 'second_feature'], \n                               value_vars=['cross_correlation_lag_1', 'cross_correlation_lag_2'],\n                               var_name = 'lag',\n                               value_name = 'cross_correlation')\n                              .assign(lag=lambda x: x.lag.str.replace('cross_correlation_lag_', \"\")))\n\n    cross_correlations_melt['pair_of_features'] = (cross_correlations_melt['first_feature'].str.replace(\"feature_\", \"\") + \n                                              \"_\"  + cross_correlations_melt['second_feature'].str.replace(\"feature_\", \"\") + \"_lag\" +\n                                                   cross_correlations_melt['lag']\n                                                  ).astype(str)\n\nelse:\n    cross_correlations_melt = pd.read_pickle(os.path.join(input_path, 'crosscorrelation/lag1and2crosscorrelations_melted.pickle'))\n    cross_correlations_melt['pair_of_features'] = (cross_correlations_melt['first_feature'].str.replace(\"feature_\", \"\") + \n                                              \"_\"  + cross_correlations_melt['second_feature'].str.replace(\"feature_\", \"\") + \"_lag\" +\n                                                   cross_correlations_melt['lag']\n                                                  ).astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig, axes = plt.subplots(1, 2, figsize = (20, 12))\nax = axes.ravel()\nsns.set_palette(\"RdBu\", 10)\n#RdBu, YlGn\npositive_dict = {0: 'negative', 1: 'positive'}\nfor j in range(2):\n    if j == 0:\n        sns.barplot(x='cross_correlation', \n                y='pair_of_features', \n               # palette = 'GnBu_r',\n                data=(cross_correlations_melt\n                      .sort_values('cross_correlation')\n                      .head(40)),\n                   ax = ax[j])\n    else:\n        sns.barplot(x='cross_correlation', \n                y='pair_of_features', \n               # palette = 'GnBu_r',\n                data=(cross_correlations_melt\n                      .sort_values('cross_correlation', ascending = False)\n                      .head(40)),\n                   ax = ax[j])\n\n    for p in ax[j].patches:\n        width = p.get_width() \n        ax[j].text(width /2, \n                # set the text at 1 unit right of the bar\n            p.get_y() + p.get_height() / 2, # get Y coordinate + X coordinate / 2\n            '{:1.4f}'.format(width), # set variable to display, 2 decimals\n            ha = 'left',   # horizontal alignment\n            va = 'center',\n            color = 'black',\n            fontsize = 11)  # vertical alignment\n\n    ax[j].set_title('Top 40 Features for {} crosscorrelation'.format(positive_dict[j]))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}