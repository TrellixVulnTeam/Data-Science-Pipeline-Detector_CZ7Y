{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\n\nimport optuna\nimport numpy as np\nimport pandas as pd\nimport xgboost as xgb\nfrom kaggle_secrets import UserSecretsClient\nfrom sklearn.model_selection import cross_val_score","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"os.environ['KAGGLE_USERNAME'] = UserSecretsClient().get_secret('KAGGLE_USERNAME')\nos.environ['KAGGLE_KEY'] = UserSecretsClient().get_secret('KAGGLE_KEY')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load Data\nTo minimise the amount of moving parts in this experiment, no filtering of samples or features of any kind is performed.\n\nAction is defined as `resp > 0`."},{"metadata":{"trusted":true},"cell_type":"code","source":"# load pre-loaded parquet file from private dataset, otherwise build it from raw csv\ntry:\n    dtrain = pd.read_parquet('../input/dtrain-parquet/dtrain.parquet')\nexcept:\n    dtrain = pd.read_csv('../input/jane-street-market-prediction/train.csv', index_col='ts_id')\n    dtrain = dtrain.astype({c: np.float32 for c, t in dtrain.dtypes.items() if t == np.float64})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# separate target(s) and various labels from independent variables\ndlabels = dtrain[['date', 'weight', 'resp_1', 'resp_2', 'resp_3', 'resp_4', 'resp']]\ndtrain = dtrain.drop(dlabels.columns, axis=1)\ndlabels['action'] = (dlabels['resp'] > 0).astype('int')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Baseline\n\nAgain, the less variables in this experiment the better, so XGBoost only gets altered along three of its influencial parameters. When one parameter is changed, the other ones stay constant. Leaderboard scores were retrieved using another notebook: https://www.kaggle.com/jorijnsmit/benchmarking-the-public-leaderboard\n\nBasically this is the training set we are going to the tune the cv 'model' on:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# results from submissions to public leaderboard\nlogbook = pd.DataFrame({\n    'seed': [13, 24, 35, 13, 24, 35, 13, 24, 35, 13, 24, 35, 13, 24, 35, 13, 24, 35, 13, 24, 35, 13, 24, 35],\n    'colsample_bytree': [1., 1., 1., .5, .5, .5, 1., 1., 1., .5, .5, .5, 1., 1., 1., .5, .5, .5, 1., 1., 1., .5, .5, .5],\n    'max_depth': [6, 6, 6, 6, 6, 6, 8, 8, 8, 8, 8, 8, 6, 6, 6, 6, 6, 6, 8, 8 ,8, 8, 8, 8],\n    'learning_rate': [.3, .3, .3, .3, .3, .3, .3, .3, .3, .3, .3, .3, .1, .1, .1, .1, .1, .1, .1, .1, .1, .1, .1, .1],\n    'leaderboard_pub': [4356.358, 4356.358, 4356.358, 3360.083, 3370.690, 3672.200, 2926.589, 2926.589, 2926.589, 4723.146, 3384.003, 2889.649, 3430.554, 3430.554, 3430.554, 3425.223, 3626.656, 3941.692, 3543.335, 3543.335, 3543.335, 3626.274, 3917.678, 3557.301],\n})\nlogbook","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tune CV\nGoal now is to test various values for the parameters of `PurgedGroupTimeSeriesSplit` and compare the cross-validation scores to the observed leaderboard scores.\n\nImport of the `class PurgedTimeSeriesSplit` hidden below:"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\nfrom sklearn.utils.validation import _deprecate_positional_args\n\n# https://www.kaggle.com/marketneutral/purged-rolling-time-series-cv-split\n# https://github.com/getgaurav2/scikit-learn/blob/d4a3af5cc9da3a76f0266932644b884c99724c57/sklearn/model_selection/_split.py#L2243\nclass PurgedGroupTimeSeriesSplit(_BaseKFold):\n    \"\"\"Time Series cross-validator variant with non-overlapping groups.\n    Allows for a gap in groups to avoid potentially leaking info from\n    train into test if the model has windowed or lag features.\n    Provides train/test indices to split time series data samples\n    that are observed at fixed time intervals according to a\n    third-party provided group.\n    In each split, test indices must be higher than before, and thus shuffling\n    in cross validator is inappropriate.\n    This cross-validation object is a variation of :class:`KFold`.\n    In the kth split, it returns first k folds as train set and the\n    (k+1)th fold as test set.\n    The same group will not appear in two different folds (the number of\n    distinct groups has to be at least equal to the number of folds).\n    Note that unlike standard cross-validation methods, successive\n    training sets are supersets of those that come before them.\n    Read more in the :ref:`User Guide <cross_validation>`.\n    Parameters\n    ----------\n    n_splits : int, default=5\n        Number of splits. Must be at least 2.\n    max_train_group_size : int, default=Inf\n        Maximum group size for a single training set.\n    group_gap : int, default=None\n        Gap between train and test\n    max_test_group_size : int, default=Inf\n        We discard this number of groups from the end of each train split\n    \"\"\"\n\n    @_deprecate_positional_args\n    def __init__(self,\n                 n_splits=5,\n                 *,\n                 max_train_group_size=np.inf,\n                 max_test_group_size=np.inf,\n                 group_gap=None,\n                 ):\n        super().__init__(n_splits, shuffle=False, random_state=None)\n        self.max_train_group_size = max_train_group_size\n        self.group_gap = group_gap\n        self.max_test_group_size = max_test_group_size\n\n    def split(self, X, y=None, groups=None):\n        \"\"\"Generate indices to split data into training and test set.\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n        y : array-like of shape (n_samples,)\n            Always ignored, exists for compatibility.\n        groups : array-like of shape (n_samples,)\n            Group labels for the samples used while splitting the dataset into\n            train/test set.\n        Yields\n        ------\n        train : ndarray\n            The training set indices for that split.\n        test : ndarray\n            The testing set indices for that split.\n        \"\"\"\n        if groups is None:\n            raise ValueError(\n                \"The 'groups' parameter should not be None\")\n        X, y, groups = indexable(X, y, groups)\n        n_samples = _num_samples(X)\n        n_splits = self.n_splits\n        group_gap = self.group_gap\n        max_test_group_size = self.max_test_group_size\n        max_train_group_size = self.max_train_group_size\n        n_folds = n_splits + 1\n        group_dict = {}\n        u, ind = np.unique(groups, return_index=True)\n        unique_groups = u[np.argsort(ind)]\n        n_samples = _num_samples(X)\n        n_groups = _num_samples(unique_groups)\n        for idx in np.arange(n_samples):\n            if (groups[idx] in group_dict):\n                group_dict[groups[idx]].append(idx)\n            else:\n                group_dict[groups[idx]] = [idx]\n        if n_folds > n_groups:\n            raise ValueError(\n                f'Cannot have number of folds={n_folds} greater than the number of groups={n_groups}'\n            )\n\n        group_test_size = min(n_groups // n_folds, max_test_group_size)\n        group_test_starts = range(n_groups - n_splits * group_test_size,\n                                  n_groups, group_test_size)\n        for group_test_start in group_test_starts:\n            train_array = []\n            test_array = []\n\n            group_st = max(0, group_test_start - group_gap - max_train_group_size)\n            for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\n                train_array_tmp = group_dict[train_group_idx]\n                \n                train_array = np.sort(np.unique(\n                                      np.concatenate((train_array,\n                                                      train_array_tmp)),\n                                      axis=None), axis=None)\n                \n            train_end = np.array(train_array).size\n \n            for test_group_idx in unique_groups[group_test_start:\n                                                group_test_start +\n                                                group_test_size]:\n                test_array_tmp = group_dict[test_group_idx]\n                test_array = np.sort(np.unique(\n                                              np.concatenate((test_array,\n                                                              test_array_tmp)),\n                                     axis=None), axis=None)\n\n            test_array = test_array[group_gap:]\n                    \n            yield [int(i) for i in train_array], [int(i) for i in test_array]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is a big bug in some of the shared implementations of the utility score: `np.bincount` is great, but it also produces zero counts. Therefore, `p_i.size` or `len(p_i)` produce faulty results. For cross-validation folds `date[-1]` is not usable either. In the implementation below those issues are fixed with the use of `np.count_nonzero`."},{"metadata":{"trusted":true},"cell_type":"code","source":"def utility(estimator, X, y):\n    date = dlabels.loc[y.index, 'date'].values\n    weight = dlabels.loc[y.index, 'weight'].values\n    resp = dlabels.loc[y.index, 'resp'].values\n    action = estimator.predict(X)\n\n    p_i = np.bincount(date, weight * resp * action)\n    t = p_i.sum() / np.sqrt((p_i ** 2).sum()) * np.sqrt(250 / np.count_nonzero(p_i))\n    u = np.clip(t, 0, 6) * p_i.sum()\n\n    return u","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def scaled_utility(estimator, X, y):\n    # scale of the public test set (~1mm rows) w.r.t. the given test set\n    scale = 1000000 / y.index.size\n\n    date = dlabels.loc[y.index, 'date'].values\n    weight = dlabels.loc[y.index, 'weight'].values\n    resp = dlabels.loc[y.index, 'resp'].values\n    action = estimator.predict(X)\n\n    p_i = np.bincount(date, weight * resp * action)\n    t = p_i.sum() / np.sqrt((p_i ** 2).sum()) * np.sqrt(250 / np.count_nonzero(p_i))\n    u = np.clip(t, 0, 6) * p_i.sum()\n\n    return u * scale","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is common to take the mean of all cross-validation scores. However, since we are working with an ordinal dataset, an argument could be made for weighing cv scores from folds that occur later heavier. Donate et al. suggests that \"in the forecasting domain, recent patterns should have a higher importance when compared with older ones\". [Donate et al., p. 6](https://doi.org/10.1016/j.neucom.2012.02.053) He goes on to define a weighted cross-validation function based on $w_i = 1 / 2^{n + 1 - i}$ and is able to conclude that it \"improves the accuracy of the forecasts, outperforming both the no weight n−fold ensemble and the simpler holdout validation (0-fold)\" (p. 13).\n\nInstead of using his fixed weights method we implement [`pandas`' exponentially weighted mean](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.window.ewm.ExponentialMovingWindow.mean.html), where $(1 - \\alpha)^i$ is the weight $w_i$:\n\n$$\ny_{t} = \\frac\n{x_{t} + (1 - \\alpha)x_{t-1} + (1 - \\alpha)^2x_{t-2} + ... + (1 - \\alpha)^tx_0}\n{1 + (1 - \\alpha) + (1 - \\alpha)^2 + ... + (1 - \\alpha)^t}\n$$\n\nThis allows for a gradual adjustment of the decay of the weights, giving a better ability to tune its use. Notice that the formula above results in a so-called moving window ($y_t$ is still a vector) so we only take the last value of it. On top of that we force support for `alpha=0`; a linear mean:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def smooth_mean(array, alpha=0):\n    if alpha == 0:\n        return array.mean()\n    else:\n        return pd.Series(array).ewm(alpha=alpha).mean().iat[-1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Build the experiment. Extract the model's parameters from the leaderboard dataframe and pass it to the `XGBClassifier` model. Crucial point here is to calculate squared error `(leaderboard_pub - cv_score)**2` and return that as the outcome.\n\nWe square root it later to compare root mean squared error (RMSE)."},{"metadata":{"trusted":true},"cell_type":"code","source":"def objective(trial):\n    \"\"\"Return the squared error of the cv score and leaderboard score.\"\"\"\n\n    errors = []\n\n    for idx in logbook.index:\n        booster = xgb.XGBClassifier(\n            max_depth=logbook.at[idx, 'max_depth'],\n            learning_rate=logbook.at[idx, 'learning_rate'],\n            colsample_bytree=logbook.at[idx, 'colsample_bytree'],\n            tree_method='gpu_hist',\n            n_jobs=-1,\n            seed=0,\n        )\n\n        if trial.suggest_int('scale_utility_score', 0, 1) == 1:\n            scorer = scaled_utility\n        else:\n            scorer = utility\n \n        cv_params = {\n            'n_splits': trial.suggest_int('n_splits', 3, 20),\n            'group_gap': trial.suggest_int('group_gap', 0, 498),\n            'max_train_group_size': trial.suggest_int('max_train_group_size', 1, 499),\n            'max_test_group_size': trial.suggest_int('max_test_group_size', 1, 499),\n        }\n            \n        cv_scores = cross_val_score(\n            estimator=booster,\n            X=dtrain,\n            y=dlabels['action'],\n            groups=dlabels['date'],\n            scoring=scorer,\n            cv=PurgedGroupTimeSeriesSplit(**cv_params),\n        )\n\n        cv_score = smooth_mean(cv_scores, alpha=trial.suggest_float('smooth_mean_alpha', 0, 1))\n\n        errors.append((logbook.at[idx, 'leaderboard_pub'] - cv_score)**2)\n\n    return np.array(errors).mean()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# just pipeline stuff where trial results are saved and loaded\nif not os.path.exists('./optunadb/optuna.db'):\n    !cp -r ../input/optunadb/ ./optunadb/\n\nstudy = optuna.create_study(\n    storage='sqlite:///optunadb/optuna.db',\n    study_name='cv_purged_full_logbook',\n    direction='minimize',\n    load_if_exists=True,\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"study.optimize(\n    objective,\n    #n_trials=3,\n    timeout=60*60*1, # 8 hours to stay under kaggle's notebook runtime limit of 9 hours\n    catch=(AttributeError, ValueError), # ignore some bugs in PurgedGroupTimeSeriesSplit that sometimes occurs\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# more pipeline stuff\nbest = study.best_value\nname = study.study_name\nn_trials = study.trials_dataframe().index.size\n\n!pip install kaggle==1.5.4\n!kaggle datasets metadata -p optunadb jorijnsmit/optunadb\n!kaggle datasets version -p optunadb -m 'updated {name} up to trial {n_trials} with best of {best}'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"experiments = study.trials_dataframe().sort_values('value').set_index('number')\nexperiments['value'] = np.sqrt(experiments['value'])\nexperiments = experiments.rename(columns={'value': 'params_RMSE'})\nexperiments.drop([c for c in experiments.columns if 'params' not in c], axis=1).rename(columns={'params_RMSE': 'RMSE'}).head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"study.best_params","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}