{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dtrain = pd.read_parquet('../input/dtrain-parquet/dtrain.parquet')\ndtrain = dtrain[[c for c in dtrain.columns if 'feature' in c]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Missing Values\n\nNote that `sklearn.decomposition.PCA` cannot handle missing values. I drop the incomplete observations here, but there are implementations of PPCA that are able to handle missing values. See https://stackoverflow.com/a/56576569/1838257 for pointers."},{"metadata":{"trusted":true},"cell_type":"code","source":"dtrain.isna().any(axis=1).sum() / dtrain.index.size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dtrain = dtrain.dropna()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Standardisation\nIn order to perform PCA we must first standardise the data; i.e. scale all features to the same dimension. `StandardScaler` does exactly this, scaling the standard deviation of each column to exactly 1 and the mean to exactly 0. It does so with:\n$z = (x_i - \\mu) / \\sigma$. Binary data (`feature_0`) cannot be handled like this, instead it has to be scaled by $2\\sigma$. See [Gelman and Hill, 2006, p. 57](https://doi.org/10.1017/CBO9780511790942)."},{"metadata":{"trusted":true},"cell_type":"code","source":"scaled = StandardScaler().fit_transform(dtrain)\nscaled[:, 0] = ((dtrain['feature_0'] - dtrain['feature_0'].mean()) / 2 * dtrain['feature_0'].std()).to_numpy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# PCA"},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=.95).fit(scaled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(pca.explained_variance_ratio_.size)\n\npd.DataFrame(pca.explained_variance_ratio_.cumsum()).plot(style='.', legend=False)\nplt.xlabel('principal component #')\nplt.ylabel('explained variance')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A total of 38 principal components can already explain 95% of the variance in data. About 29% of the original amount of features!"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame({\n    'eigenvalues': pca.explained_variance_, \n    'explained variance': pca.explained_variance_ratio_,\n    'cumulative expl. var.': pca.explained_variance_ratio_.cumsum()\n})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(PCA(n_components=.99).fit(scaled).n_components_)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}