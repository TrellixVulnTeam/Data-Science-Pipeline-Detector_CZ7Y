{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Jane Street Market Prediction - A Multi-layer Perceptron\n*This notebook is a response to the problem posed by the \"Jane Street Market Prediction\" Kaggle Competition (Nov 2020 - Feb 2021).*\n\nThe applications of Deep Learning in financial markets has always been one of the hot topics of the field. The Jane Street Market Prediction competition challenges us to create a quantitative trading model, one that utilizes real-time market data to help make trading decisions and maximise returns.\n\n### Framing the Problem\n\nThe goal of the model is to **predict whether it is better to make a trade or pass on it** at a certain point in time, given an anonymized set of features representing stock market data at that point.\n\nThis is a **Multi-layer Perceptron (MLP)** model. With 131 features in the dataset, a basic MLP should have reasonable performance despite its simplicity and inability to take time into account. After the poor performance of the LSTM model, I decided it will be best to avoid looking back through the data and returning to the basics.\n\nBelow, I go through the preparation of data, model creation and finally prediction."},{"metadata":{},"cell_type":"markdown","source":"## 1. Cleaning the Dataset\n\nWe first have to import the dataset from Kaggle."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport datatable\n\n# datatable reads large csv files faster than pandas\ntrain_df = datatable.fread('/kaggle/input/jane-street-market-prediction/train.csv').to_pandas()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's inspect the data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_df.info())\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The `date` is the day on which the trading opportunity occurs. This goes from Day 0-499.\n\nThe `weight` and `resp` together represent the value of each trade. `resp_1` to `resp_4` are 'resp' values over different time horizons. **The five 'resp' values will be the dependent variables, and hence the targets of prediction.**\n\n`feature_0` to `feature_129` represent stock market data.\n\nThe `ts_id` is the index of each row. It is the number representing the time of the trading opportunity."},{"metadata":{},"cell_type":"markdown","source":"### Dealing with NaN entries\n\nRight away we see that we will have to deal with numerous NaN entries, as seen in feature_121. Let's dig a little deeper:"},{"metadata":{"trusted":true},"cell_type":"code","source":"isna_df = train_df.isnull().sum()\nisna_df[isna_df > 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Max:\", isna_df.max())\nprint(isna_df[isna_df == isna_df.max()])\nisna_df.max()/train_df.size","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that there are 88 columns with NaN entries, with the a maximum of 395535 NaN entries in a single column. However, this is 0.1% of the whole dataset, so it should be okay to fill in the NaN entries.\n\nAn analysis by Tom Warrens strongly suggests that most NaN values occur at the start of the day and during midday, which corresponds to the market opening and lunch breaks. With this information, it makes sense to fill in the NaN values with the last valid observation.\n\nHowever, this only holds true if data is at least generally continuous. Carl McBride's Day 0 Exploratory Data Analysis workbook shows that this is not always the case. `feature_41` to `feature_45` comprise of discrete value. For these features, it makes more sense to fill in NaN values with the mean.\n\n*Tom Warrens' analysis can be found here: https://www.kaggle.com/tomwarrens/nan-values-depending-on-time-of-day*\n\n*Carl McBride's Day 0 EDA can be found here: https://www.kaggle.com/carlmcbrideellis/jane-street-eda-of-day-0-and-feature-importance*"},{"metadata":{"trusted":true},"cell_type":"code","source":"discrete_features = ['feature_41', 'feature_42', 'feature_43', 'feature_44', 'feature_45']\n\nisna_df = train_df[discrete_features].isnull().sum()\nisna_df[isna_df > 0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since there are discrete features with NaN entries, we need to take two different approaches to filling in the data: forward-filling the continuous data and filling with mean for the discrete data.\n\nWe deal with the discrete data first."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[discrete_features] = train_df[discrete_features].fillna(value=train_df[discrete_features].mean())\n\nisna_df = train_df[discrete_features].isnull().sum()\nisna_df[isna_df > 0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, we can use forward-filling to fill the rest of the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.fillna(method=\"ffill\", inplace=True)\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"isna_df = train_df.isnull().sum()\nisna_df[isna_df > 0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the number of NaN entries has been drastically reduced, but there are still many entries with NaN values. This is likely because many NaN values start at index 0 (as can be seen from feature_121 above) and hence do not have a last valid observation to fill from.\n\nAlthough this is not ideal since in actual use we will not have future data on hand, for training purposes we can fill in the last few NaN entries with the next valid observation instead."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.fillna(method=\"bfill\", inplace=True)\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"isna_df = train_df.isnull().sum()\nisna_df[isna_df > 0].size","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Reducing Memory Usage\n\nBefore we continue, we should return to the memory usage of the dataset, as seen above. At 2.4GB, the training dataset takes up quite a lot of memory. Let's try to reduce the memory usage by optimizing the data types.\n\n(Note: if done before we fill the NaN entries, the pandas.fillna method will not work)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_memory_usage(df):\n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != 'object':\n            cmin = df[col].min()\n            cmax = df[col].max()\n            \n            if str(col_type)[:3] == 'int':\n                if cmin > np.iinfo(np.int8).min and cmax < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif cmin > np.iinfo(np.int16).min and cmax < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif cmin > np.iinfo(np.int32).min and cmax < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif cmin > np.iinfo(np.int64).min and cmax < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if cmin > np.finfo(np.float16).min and cmax < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif cmin > np.finfo(np.float32).min and cmax < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                    \n        else:\n            df[col] = df[col].astype('category')\n            \n    return df\n\ntrain_df = reduce_memory_usage(train_df)\ntrain_df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Re-indexing the Data\n\nLastly, we should set the index of train_df to \"ts_id\"."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.set_index(\"ts_id\", drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Transforming the Dataset\n\nNow that the data is clean, we can start to prepare the data for the model. We first separate the features and our dependent variables, which are \"resp\" and the other \"resp\" over the various time frames."},{"metadata":{"trusted":true},"cell_type":"code","source":"Y = (train_df[[\"resp\", \"resp_1\", \"resp_2\", \"resp_3\", \"resp_4\"]] > 0).astype(int)\nX = train_df.drop([\"resp\", \"resp_1\", \"resp_2\", \"resp_3\", \"resp_4\", \"date\", \"ts_id\"], axis=1)\n\nprint(X.head())\nprint(Y.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We split the data into training and validation data (10% of the data will be taken as validation)."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nvalid_ratio = 0.1 # 90% training data 10% validation data\n\ntrain_X, valid_X, train_Y, valid_Y = train_test_split(X, Y, test_size=valid_ratio, random_state=42)\n\nprint(len(train_X.index))\nprint(len(valid_X.index))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Building and Training the Model\n\nWe will then start building the model. I use Keras to build a LSTM model, using Adam as the optimizer, Binary-Crossentropy as the loss, and AUC-ROC and accuracy as the metrics."},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\ndef build_mlp(num_columns, num_labels, dense_units, dropout_rate, learning_rate, label_smoothing):\n    inp = layers.Input(shape=(num_columns, ))\n    x = layers.BatchNormalization()(inp)\n    x = layers.Dropout(dropout_rate)(x)\n        \n    for j in range(len(dense_units)):\n        x = layers.Dense(dense_units[j])(x)\n        x = layers.BatchNormalization()(x)\n        x = layers.Activation(tf.keras.activations.swish)(x)\n        x = layers.Dropout(dropout_rate)(x)\n        \n    x = layers.Dense(num_labels)(x)\n    out = layers.Activation(\"sigmoid\")(x)\n    \n    model = keras.Model(inp, out)\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n                  loss=keras.losses.BinaryCrossentropy(label_smoothing=label_smoothing),\n                  metrics=['AUC', 'accuracy'])\n    print(model.summary())\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tuning attempt 3\nnum_epochs = 30\n\nnum_columns = len(train_X.columns)\nnum_labels = len(train_Y.columns)\ndense_units = [128, 256, 128]\ndropout_rate = 0.2\nlearning_rate = 0.001\nlabel_smoothing = 0.01\n\n# early stopping\ncallback = keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=5, verbose=1)\n\nmlp_model = build_mlp(num_columns, num_labels, dense_units, dropout_rate, learning_rate, label_smoothing)\nmlp_model.fit(train_X, train_Y, validation_data=(valid_X, valid_Y), epochs=num_epochs, callbacks=callback)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Submission\n\nUsing the Jane Street Time-series API, we set up our notebook for submission to the competition."},{"metadata":{"trusted":true},"cell_type":"code","source":"import janestreet\nenv = janestreet.make_env() # initialize the environment\niter_test = env.iter_test() # an iterator which loops over the test set\n\nthreshold = 0.500\n\nfor (test_df, sample_prediction_df) in iter_test:\n    test_df.drop('date', axis=1, inplace=True)\n    \n    if test_df['weight'].values > 0:\n        prediction = mlp_model.predict(test_df)\n        avg = np.sum(prediction) / prediction.size\n        sample_prediction_df.action = 1 if avg > threshold else 0\n    else:\n        sample_prediction_df.action = 0\n    env.predict(sample_prediction_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Notes and Observations\n\nCompared to the previous LSTM model, this MLP model had a much better performance. While the accuracy of the model is comparable at 15-30%, the AUC-ROC is consistently more than 0.56, indicating that the model has significantly more distinguishing power than the LSTM model. Despite the inability to look back into past data, it seems that the 131 features provide enough data to produce a good prediction of returns.\n\nSometimes the basic approach is best."},{"metadata":{},"cell_type":"markdown","source":"### References:\n\nhttps://www.kaggle.com/tomwarrens/nan-values-depending-on-time-of-day\n\nhttps://www.kaggle.com/manavtrivedi/lstm-rnn-classifier/output\n\nhttps://www.kaggle.com/rajkumarl/jane-tf-keras-lstm\n\nhttps://www.kaggle.com/tarlannazarov/own-jane-street-with-keras-nn"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}