{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 1) Import important libraries and packages"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport xgboost as xgb\nimport optuna\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2) Load and clean dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import dataset as train\ntrain = pd.read_csv('/kaggle/input/jane-street-market-prediction/train.csv',skiprows=range(1,1000000),nrows=1000000)\ntrain.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop rows with 'weight'=0 \n# Trades with weight = 0 were intentionally included in the dataset for completeness, \n# although such trades will not contribute towards the scoring evaluation\ntrain = train[train['weight']!=0]\nfeatures = [col for col in list(train.columns) if 'feature' in col]\n\n# Create 'action' column (dependent variable)\n# The 'action' column is defined as such because of the evaluation metric used for this project.\n# We want to maximise the utility function and hence pi where pi=∑j(weightij∗respij∗actionij)\n# Positive values of resp will increase pi\n\ntrain['return'] = np.log(1+np.abs(train['resp']))\ntrain['sign'] = np.where(train['resp']>0,1,0)\ntrain_p = train[train['sign']==1]\ntrain_n = train[train['sign']==0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_p[['return']].hist(bins=100);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_n[['return']].hist(bins=100);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train[features]\ny = train['sign']\n# Next, we hold out part of the training data to form the hold-out validation set\ntrain_x, valid_x, train_y, valid_y = train_test_split(X, y, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_median = train_x.median()\n# Impute medians in both training set and the hold-out validation set\ntrain_x = train_x.fillna(train_median)\nvalid_x = valid_x.fillna(train_median)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Before we perform PCA, we need to normalise the features so that they have zero mean and unit variance\nscaler = StandardScaler()\nscaler.fit(train_x)\ntrain_x_norm = scaler.transform(train_x)\n\n# pca = PCA()\n# comp = pca.fit(train_x_norm)\n# # We plot a graph to show how the explained variation in the 129 features varies with the number of principal components\n# plt.plot(np.cumsum(comp.explained_variance_ratio_))\n# plt.grid()\n# plt.xlabel('Number of Principal Components')\n# plt.ylabel('Explained Variance')\n# sns.despine();\n# # The first 15 principal components explains about 80% of the variation\n# # The first 40 principal components explains about 95% of the variation\n# Using the first 60 principal components, we apply the PCA mapping on both the training and test set\n\npca = PCA(n_components=60).fit(train_x_norm)\ntrain_x_transform = pca.transform(train_x_norm)\nvalid_x_transform = pca.transform(scaler.transform(valid_x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We create the XGboost-specific DMatrix data format from the numpy array. \n# This data structure is optimised for memory efficiency and training speed\ndtrain = xgb.DMatrix(train_x_transform, label=train_y)\ndvalid = xgb.DMatrix(valid_x_transform, label=valid_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The objective function is passed an Optuna specific argument of trial\ndef objective(trial):\n    \n# params specifies the XGBoost hyperparameters to be tuned\n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 200, 600),\n        'max_depth': trial.suggest_int('max_depth', 10, 25),\n        'learning_rate': trial.suggest_uniform('learning_rate', 0.01, 0.1),\n        'subsample': trial.suggest_uniform('subsample', 0.50, 1),\n        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.50, 1),\n        'gamma': trial.suggest_int('gamma', 0, 10),\n        'tree_method': 'gpu_hist',  \n        'objective': 'binary:logistic'\n    }\n    \n    bst = xgb.train(params, dtrain)\n    preds = bst.predict(dvalid)\n    pred_labels = np.rint(preds)\n# trials will be evaluated based on their accuracy on the test set\n    accuracy = sklearn.metrics.accuracy_score(valid_y, pred_labels)\n    return accuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if __name__ == \"__main__\":\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=25, timeout=600)\n\n#     print(\"Number of finished trials: \", len(study.trials))\n#     print(\"Best trial:\")\n    trial = study.best_trial\n\n#     print(\"  Value: {}\".format(trial.value))\n#     print(\"  Params: \")\n#     for key, value in trial.params.items():\n#         print(\"    {}: {}\".format(key, value))\n\nbest_params = trial.params\nbest_params['tree_method'] = 'gpu_hist' \nbest_params['objective'] = 'binary:logistic'\noptimal_clf = xgb.XGBClassifier(**best_params)\noptimal_clf.fit(train_x_transform, train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # # Plot how the best accuracy evolves with number of trials\n# fig = optuna.visualization.plot_optimization_history(study)\n# fig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xp, Xn = train_p[features],train_n[features]\nyp, yn = train_p['return'],train_n['return']\n# Next, we hold out part of the training data to form the hold-out validation set\ntrain_xp, valid_xp, train_yp, valid_yp = train_test_split(Xp, yp, test_size=0.2)\ntrain_xn, valid_xn, train_yn, valid_yn = train_test_split(Xn, yn, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_median_p, train_median_n = train_xp.median(),train_xn.median()\n# Impute medians in both training set and the hold-out validation set\ntrain_xp, valid_xp = train_xp.fillna(train_median_p),valid_xp.fillna(train_median_p)\ntrain_xn, valid_xn = train_xn.fillna(train_median_n),valid_xn.fillna(train_median_n) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler_p,scaler_n = StandardScaler(),StandardScaler()\nscaler_p.fit(train_xp)\nscaler_n.fit(train_xn)\n\ntrain_xp_norm = scaler_p.transform(train_xp)\ntrain_xn_norm = scaler_n.transform(train_xn)\n# pca_p,pca_n = PCA(),PCA()\n# comp_p,comp_n = pca_p.fit(train_xp_norm), pca_n.fit(train_xn_norm)\n# # We plot a graph to show how the explained variation in the 129 features varies with the number of principal components\n# plt.plot(np.cumsum(comp.explained_variance_ratio_))\n# plt.grid()\n# plt.xlabel('Number of Principal Components')\n# plt.ylabel('Explained Variance')\n# sns.despine();\n# # The first 15 principal components explains about 80% of the variation\n# # The first 40 principal components explains about 95% of the variation\n# Using the first 60 principal components, we apply the PCA mapping on both the training and test set\npca_p = PCA(n_components=60).fit(train_xp_norm)\npca_n = PCA(n_components=60).fit(train_xn_norm)\ntrain_xp_transform = pca_p.transform(train_xp_norm)\nvalid_xp_transform = pca_p.transform(scaler_p.transform(valid_xp))\ntrain_xn_transform = pca_n.transform(train_xn_norm)\nvalid_xn_transform = pca_n.transform(scaler_n.transform(valid_xn))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We create the XGboost-specific DMatrix data format from the numpy array. \n# This data structure is optimised for memory efficiency and training speed\ndtrain_p = xgb.DMatrix(train_xp_transform, label=train_yp)\ndvalid_p = xgb.DMatrix(valid_xp_transform, label=valid_yp)\ndtrain_n = xgb.DMatrix(train_xn_transform, label=train_yn)\ndvalid_n = xgb.DMatrix(valid_xn_transform, label=valid_yn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The objective function is passed an Optuna specific argument of trial\ndef objective_p(trial):\n    \n# params specifies the XGBoost hyperparameters to be tuned\n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 200, 600),\n        'max_depth': trial.suggest_int('max_depth', 10, 25),\n        'learning_rate': trial.suggest_uniform('learning_rate', 0.01, 0.1),\n        'subsample': trial.suggest_uniform('subsample', 0.50, 1),\n        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.50, 1),\n        'gamma': trial.suggest_int('gamma', 0, 10),\n        'tree_method': 'gpu_hist',  \n        \"objective\": \"reg:squarederror\",\n    }\n    \n    bst = xgb.train(params, dtrain_p)\n    preds = bst.predict(dvalid_p)\n    rmse = np.sqrt(mean_squared_error(valid_yp, preds))\n    return rmse\n\n# The objective function is passed an Optuna specific argument of trial\ndef objective_n(trial):\n    \n# params specifies the XGBoost hyperparameters to be tuned\n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 200, 600),\n        'max_depth': trial.suggest_int('max_depth', 10, 25),\n        'learning_rate': trial.suggest_uniform('learning_rate', 0.01, 0.1),\n        'subsample': trial.suggest_uniform('subsample', 0.50, 1),\n        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.50, 1),\n        'gamma': trial.suggest_int('gamma', 0, 10),\n        'tree_method': 'gpu_hist',  \n        \"objective\": \"reg:squarederror\",\n    }\n    \n    bst = xgb.train(params, dtrain_n)\n    preds = bst.predict(dvalid_n)\n    rmse = np.sqrt(mean_squared_error(valid_yn, preds))\n    return rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if __name__ == \"__main__\":\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective_p, n_trials=25, timeout=600)\n    trial = study.best_trial\n    \np_params = trial.params\np_params['tree_method'] = 'gpu_hist' \np_params['objective'] = \"reg:squarederror\"\np_reg = xgb.XGBRegressor(**p_params)\np_reg.fit(train_xp_transform, train_yp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if __name__ == \"__main__\":\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective_n, n_trials=25, timeout=600)\n    trial = study.best_trial\n    \nn_params = trial.params\nn_params['tree_method'] = 'gpu_hist' \nn_params['objective'] = \"reg:squarederror\"\nn_reg = xgb.XGBRegressor(**n_params)\nn_reg.fit(train_xn_transform, train_yn)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6) Fit classifier on test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"# We impute the missing values with the medians\ndef fillna_npwhere(array, values):\n    if np.isnan(array.sum()):\n        array = np.where(np.isnan(array), values, array)\n    return array","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import janestreet\n\nenv = janestreet.make_env()  # initialize the environment\niter_test = env.iter_test()  # an iterator which loops over the test set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for (test_df, sample_prediction_df) in iter_test:\n    wt = test_df.iloc[0].weight\n    if wt == 0:\n        sample_prediction_df.action = 0\n    else:\n        sign = optimal_clf.predict_proba(pca.transform(scaler.transform(fillna_npwhere(test_df[features].values,train_median[features].values))))\n        predict_p = p_reg.predict(pca_p.transform(scaler_p.transform(fillna_npwhere(test_df[features].values,train_median_p[features].values))))\n        predict_n = n_reg.predict(pca_n.transform(scaler_n.transform(fillna_npwhere(test_df[features].values,train_median_n[features].values))))\n        sample_prediction_df.action = np.where((-sign[:, 0]*predict_n+sign[:, 1]*predict_p)>0,1,0)\n    env.predict(sample_prediction_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Acknowledgements\nhttps://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60\n\nhttps://www.kaggle.com/saurabhshahane/voting-classifier-beginners\n\nhttps://www.kaggle.com/harshitt21/jane-street-basic-eda-xgb-baseline\n\nhttps://www.kaggle.com/eudmar/jane-street-eda-pca-ensemble-methods\n\nhttps://www.kaggle.com/gogo827jz/optimise-speed-of-filling-nan-function?scriptVersionId=48926407\n\nhttps://github.com/datacamp/Machine-Learning-With-XGboost-live-training/blob/master/notebooks/Machine-Learning-with-XGBoost-solution.ipynb"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}