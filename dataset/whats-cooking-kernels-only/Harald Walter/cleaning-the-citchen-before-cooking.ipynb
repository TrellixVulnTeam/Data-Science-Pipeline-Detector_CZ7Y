{"cells":[{"metadata":{"_uuid":"200f98c87583b41f10fdddcd91481ca5507d6fc4"},"cell_type":"markdown","source":"# Cleaning the citchen before cooking"},{"metadata":{"trusted":true,"_uuid":"b73d87c6d119a81c2924cfea526a710be4a0aec3"},"cell_type":"code","source":"%logstart -o -t log.txt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ecfd6f8b1312e926e5ae30b7de11ad33072b2acc"},"cell_type":"code","source":"#Debugging: Use only 100 samples for training and testing\nDEBUG_SAMPLES=0","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e00d5fa8b197bddf5ee36a2e7445795ad11be0f6"},"cell_type":"markdown","source":"## EDA"},{"metadata":{"trusted":true,"_uuid":"5617c80c4dffde05f8b56989b64bf80777bf93ed"},"cell_type":"code","source":"import json\nimport numpy as np\nimport pandas as pd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sb\nsb.set()\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"20f5a01f8f5bbb6d722186035baf235daa6459de"},"cell_type":"code","source":"with open(\"../input/train.json\", \"r\") as file_train:\n    dat = json.load(file_train)\n    dat = pd.DataFrame(dat)\n    dat.set_index(\"id\", inplace=True, verify_integrity=True)\ndat.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a7bc73f8ea165d53e5fad7cc30db9a1ca4b1b3ba"},"cell_type":"code","source":"dat.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"72cda2d826c6209a7256423474e8c4aab0661a6a"},"cell_type":"code","source":"#debugging\nif DEBUG_SAMPLES:\n    dat=dat.sample(DEBUG_SAMPLES)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f4213261002cb43a3c1346082df5569232949e61"},"cell_type":"markdown","source":"### Missing data?"},{"metadata":{"trusted":true,"_uuid":"7b003dbd0704afe1d66f386db3a251fd2bf60a49"},"cell_type":"code","source":"dat.isna().any()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f1c462092a6f8140a818b9ecd05bed1c15e988c"},"cell_type":"markdown","source":"There are no missing values..."},{"metadata":{"_uuid":"0c37466f4f710da6d2c650230832586ffb35a2e1"},"cell_type":"markdown","source":"### Recipes by country"},{"metadata":{"trusted":true,"_uuid":"7172f9a9235b2478c6881ff72d27ee0a1aebebfa"},"cell_type":"code","source":"recipes_by_country = dat.groupby(\"cuisine\").count().sort_values(\"ingredients\", ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":false,"trusted":true,"_uuid":"59d54aa562429545134af8155d3d31715078df6c"},"cell_type":"code","source":"recipes_by_country[\"ingredients\"].plot(kind=\"bar\", figsize=(10,5), fontsize=14)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8e924c3d1340e79a90185fe82a7c279899aac14b"},"cell_type":"markdown","source":"### Average number of ingredients by country (complexity of recipes)"},{"metadata":{"trusted":true,"_uuid":"9f2d3a687fd79369a00057ef15f9e16ca40bc5a9"},"cell_type":"code","source":"dat[\"num_ingredients\"] = dat.apply(lambda x: len(x[\"ingredients\"]), axis=1)\ndat.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8f83c85e4e437b99b9f25394b7db9f179ff9cb77"},"cell_type":"code","source":"dat.groupby(\"cuisine\").describe().sort_values([(\"num_ingredients\",\"mean\")], ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f90a2bf8c83153c840cafc8df136d376321e91c9"},"cell_type":"markdown","source":"### Main ingredients by country"},{"metadata":{"_kg_hide-output":false,"trusted":true,"_uuid":"76038292956a1ca1189ca5a88bc1b7255382dcc9"},"cell_type":"code","source":"from collections import Counter\n\ning_counts_by_cuisine = {}\n\ndef increment_counter(y):\n    #y is a Series object with the column names as index\n    cuisine = y[\"cuisine\"]\n    ing_counts_by_cuisine[cuisine].update(y[\"ingredients\"])\n\ndef count_ing(x):\n    #x is a DataFrame containing all recipes for one cuisine\n    cuisine = x[\"cuisine\"].iloc[0]\n    ing_counts_by_cuisine[cuisine] = Counter()\n    x.apply(increment_counter, axis=1)\n\ndat_grouped_by_cuisine = dat.groupby(\"cuisine\").apply(count_ing)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ae5ee764efa50801146b0b6edf478a1282eb376"},"cell_type":"code","source":"most_common = 10\n\n# Lets draw an HTML table showing the most common ingredients per cuisine\n# Make a Dataframe out of it\nings = pd.DataFrame(\n        [[items[0] for items in ing_counts_by_cuisine[cuisine].most_common(most_common)] for cuisine in ing_counts_by_cuisine],\n        columns=[\"Ingredient \"+str(idx+1) for idx in range(most_common)],\n        index=[cuisine for cuisine, items in ing_counts_by_cuisine.items()])\nings\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eaf086778651f780094a315645ae467854599480"},"cell_type":"raw","source":"Interesting... the upper end of the tail is often salt (non asia countries) or soy/fish sauce (asia countries).\nLets look at the lower end of the tail. This time we do not group by cusine. Are there ingredients that are only used one time over all recipes?"},{"metadata":{"_uuid":"899e54276a7977c9d470ca401eaecf3858cecc47"},"cell_type":"markdown","source":"### Lower end of the tail"},{"metadata":{"trusted":true,"_uuid":"8bb9a8bd4c57297abf62d1a2d272edb63d4653fe"},"cell_type":"code","source":"# This time I am using the CountVectorizer of scikit to do the actual counting of words\nfrom sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"59127d6dd344460ab32dd4c42ff8e93d7e597fb7"},"cell_type":"code","source":"# lets first create a huge list concatenating all ingredient lists of all training data we have\n# Iterating over a two nested iterables. This is not performant. Is there a better way?\nhuge_ing_list = pd.Series([ing for idx, row in dat.iterrows() for ing in row[\"ingredients\"]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bc3b86cdf31e34329bcced17f59a4db36c29a256"},"cell_type":"code","source":"huge_ing_list.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ca07de795824a45bf39e4a9196f38a9580e74ab3"},"cell_type":"code","source":"cv.fit_transform(huge_ing_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9ce492971cab4a290ab6f2e7378743b6b587a227"},"cell_type":"code","source":"pd.Series(cv.vocabulary_).sample(50)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0e6d5c7d20e22ad125a57edd376a467422293db2"},"cell_type":"markdown","source":"Woops... This is not what I wanted to have. Now I am counting every word. But wait there are also some adjectives like \"chopped\". Do we want to use those for classification later on?\nLets hold that back and decide about later....\n\nBack what I wanted to to do actually. Counting ingredients whereas an ingredient is something atomic. I do not want to count words anymore. "},{"metadata":{"_uuid":"882c77ec336267efc27572691903d3fb74c2680d"},"cell_type":"markdown","source":"I know that there are quite a lot kernels out there that do word or n-gram counting on the cuisine dataset. They perform quite well but I don't focus on them since already done quite often. I wanted to focus on data cleaning and feature selection."},{"metadata":{"_uuid":"c1c5891586965c4b8c30980edbe396e0fe51ba4b"},"cell_type":"markdown","source":"## Data cleaning"},{"metadata":{"_uuid":"76ce2f544012693dc8abd0057400ef1dbe49eb33"},"cell_type":"markdown","source":"### Wite space and lower-case transformations"},{"metadata":{"_uuid":"aa3745c63ebe5b39f01d0adb7ab98ec46eb4c64f"},"cell_type":"markdown","source":"Lets remove white spaces and bring everything to lower-cases"},{"metadata":{"trusted":true,"_uuid":"f1019ac9de092c33bc6ebe31be1b360af2816758"},"cell_type":"code","source":"def to_lowercase_wo_whitespace(x):\n    #x is a series\n    x[\"ingredients\"] = [ing.lower() for ing in x[\"ingredients\"]]\n    x[\"ingredients\"] = [ing.strip() for ing in x[\"ingredients\"]]\n    \n    return x\n\ndat = dat.apply(to_lowercase_wo_whitespace, axis=1);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8f1e8f180f59d9abb9edb8d4271efc43764ec666"},"cell_type":"code","source":"def count_ingredients():\n    ing_counts = Counter()\n\n    def count_ing_separately(x):\n        #x is Series with one recipe\n        ing_counts.update(x[\"ingredients\"])\n\n        return x\n\n    dat.apply(count_ing_separately, axis=1)\n    ing_counts = pd.DataFrame(ing_counts.most_common(), columns=['ingredient', 'occurance'])\n\n    #Lets sort them\n    ing_counts.sort_values('occurance',ascending=False)\n    return ing_counts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"df19a987a47f956180f7d59c411f04c9e8a8b4da"},"cell_type":"code","source":"ing_counts = count_ingredients()\ning_counts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"82239fc7d42dbe70d0a62ea16a5a16e1d068b66c"},"cell_type":"code","source":"ing_counts.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"22620c8f1e0d4356403426b8671887d98f7e3f8d"},"cell_type":"markdown","source":"That looks quite messy. There are ingredients that have\n1. the amount encoded\n1. Derived words like \"chopped\" or plurals of words\n1. some typos like \"yuca\" and \"yucca\" \n1. Lots of ingredients that are only seen once\n\n\n.. Lets clean this up."},{"metadata":{"_uuid":"1241b13478507ccf50145627749864f12c30b705"},"cell_type":"markdown","source":"### Removing symbols and numbers"},{"metadata":{"trusted":true,"_uuid":"9a13cbc842e77aa0ef435bd19457ba44c9bb23d5"},"cell_type":"code","source":"import re","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"467e69f0f5a7a633b1ea290edf4ffe68b33204fa"},"cell_type":"code","source":"def remove_numbers_symbols(x):\n    ings = x[\"ingredients\"]\n    for idx, ing in enumerate(ings):\n        ings[idx] = re.sub(r\"\\W+\", \" \", ing)\n        ings[idx] = re.sub(r\"oz[. ]\", \" \", ing)\n    x[\"ingredients\"] = ings\n    return x\n\ndat = dat.apply(remove_numbers_symbols, axis=1)\ndat.sample(20)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"699a94c711303d8225258b1bb7dc33ad288e9028"},"cell_type":"markdown","source":"### Stemming"},{"metadata":{"trusted":true,"_uuid":"a37fc9ad66d92ecab9ce909f271d2f89866595d7"},"cell_type":"code","source":"from nltk.stem import PorterStemmer\n\nps = PorterStemmer()\ndef stem_ingredients(x):\n    ings = x[\"ingredients\"]\n    for idx, ing in enumerate(ings):\n        ings[idx] = ps.stem(ing)\n    x[\"ingredients\"] = ings\n    return x\n\ndat = dat.apply(stem_ingredients, axis=1)\ndat.sample(20)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fff8fec999eb80be3e7da6a0a9d3f6ef0bd5a351"},"cell_type":"markdown","source":"### Removing typos"},{"metadata":{"_uuid":"21aca8b438fe83d6182e13c4de1e720683ed8467"},"cell_type":"markdown","source":"Lets use fuzzy matching to merge similar written ingredients.\nThis is again a bit tricky due to the nested lists in our data frame. We want to do the fuzzy matching along \nall ingredients and not only the ingredients within one recipe..."},{"metadata":{"trusted":true,"_uuid":"5b2acbd30d86fcf14717530e01008f470445d34f"},"cell_type":"code","source":"import fuzzywuzzy\nfrom fuzzywuzzy import process","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"15daf87550f9969ae713c0a4288f7ff8efccb45e"},"cell_type":"code","source":"#Count the preprocessed ingredients again\ning_counts = count_ingredients()\n\n#Make the ingredient to be the new index and get a unique Series after the last preprocessing steps\ning_counts.set_index('ingredient', inplace=True)\ning_counts.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8ca76d297644d5e3b8a32408d652ec13be076512"},"cell_type":"code","source":"ing_counts.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a2ab57a62f5de2599bfff058a13c223ed2acb719"},"cell_type":"markdown","source":"Cleaning up the ingredients via regex and stemming reduced the amount of ingredients a bit."},{"metadata":{"trusted":true,"_uuid":"cf3da2dc4d1286c47c31e9ec54c680a56c54a418"},"cell_type":"code","source":"from mlxtend.preprocessing import minmax_scaling","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5256349bffd4bf568b8252d657645a924592377d"},"cell_type":"code","source":"\n# We want to save our fuzzy match resolution to avoid doing the matching for one ingredient several times\ning_counts[\"fuzzy_match\"] = None\n\ndef fuzzy_match(x, min_fuzzy_ratio = 90):\n    recipe_ingredients = x[\"ingredients\"]\n    for idx, ing in enumerate(recipe_ingredients):\n        cur_fuzzy_match = ing_counts.loc[ing, [\"fuzzy_match\"]].iloc[0]\n        if (cur_fuzzy_match == None):\n            #Lets use my ingredient counters dataframe to get the whole list of ingredients\n            matches = fuzzywuzzy.process.extract(ing, ing_counts.index, limit=5, scorer=fuzzywuzzy.fuzz.token_sort_ratio)\n            #Minimimum fuzzy matching ratio\n            matches = [match for match in matches if match[1] >= min_fuzzy_ratio]\n            #Interleave with the count values of the words\n            matches_df=pd.DataFrame(matches, columns=[\"ingredient\", \"match_score\"]) \n            matches_df = matches_df.merge(ing_counts, how='inner', on=\"ingredient\")\n            #Normalize\n            matches_df['match_score_norm']= matches_df[\"match_score\"] / 100\n            if (len(matches_df) > 2):\n                matches_df['occurance_norm']= minmax_scaling(matches_df[\"occurance\"], columns=[0])\n            else:\n                matches_df['occurance_norm'] = 1.0\n            #Create an overall scoring\n            matches_df['overall_score'] = matches_df['match_score_norm'] * matches_df['occurance_norm']\n            # Get the ingredient with the maximum scoring\n            idx_max = matches_df['overall_score'].idxmax()\n            #Replace the ingredient by the ingredient with the maximum overall scoring\n            if (recipe_ingredients[idx] != matches_df.loc[idx_max, \"ingredient\"]):\n                print (\"Ingredient {} replaced by {}\".format(recipe_ingredients[idx], matches_df.loc[idx_max, \"ingredient\"]))\n                recipe_ingredients[idx] = matches_df.loc[idx_max, \"ingredient\"]\n\n            #Save the choice to avoid further fuzzy matching when the ingredient shows up again\n            if (ing in ing_counts.index.values):\n                ing_counts.loc[ing, [\"fuzzy_match\"]] = matches_df.loc[idx_max, \"ingredient\"]\n        else:\n            recipe_ingredients[idx] = cur_fuzzy_match\n    x[\"ingredients\"] = recipe_ingredients\n    \n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8d5388202f78e7816b4909f0d7345e3077387d59"},"cell_type":"code","source":"dat = dat.apply(fuzzy_match, axis=1);\n#degugging\n#dat.sample(1).apply(fuzzy_match, axis=1);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0a1ca03cd46593fb7a359d5d635baa4073a63652"},"cell_type":"code","source":"ing_counts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8ec26d44c1356a87ee3b9549effccc2ecb3b9555"},"cell_type":"code","source":"#This is our final ingredients list\ningredients_list = ing_counts[\"fuzzy_match\"].unique()\n\ningredients_list.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5405bfd4c2f32ac79e7caecec90aee9a41ac5303"},"cell_type":"markdown","source":"And again we reduced the total amount of ingredients..."},{"metadata":{"_uuid":"62123dd633dc21006061232b92ac8232c2a4e6c1"},"cell_type":"markdown","source":"### Natural langauge processing"},{"metadata":{"_uuid":"dab767f405d944b75cca30789b7c91ca3a6d2845"},"cell_type":"markdown","source":"#### Tagging words and remove words that are not nouns"},{"metadata":{"trusted":true,"_uuid":"e64fd51b5a9972279fd0b30b7ed18b48115030a6"},"cell_type":"code","source":"import nltk","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a1c35100393c3eaa52c4acebfbbebbefbb98887e"},"cell_type":"code","source":"tagged_ings = nltk.pos_tag(huge_ing_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"885936dcb67a4a560c3c43445924f3af63769e3c"},"cell_type":"code","source":"type(tagged_ings)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0153cbea4cdfcfa319dd83ac1e3e35ecef965dfe"},"cell_type":"markdown","source":"That doesn't look that well. E.g. there are quite a lot ingredients that are detected to be adjectives. This is because our ingredients do not form sentences. They are just a bunch of words. NTLK usually works with the context the words appear in.\n\n**Outstanding improvement: Can I somehow use wordnet to do reasonable word tagging?**"},{"metadata":{"_uuid":"20f092bd6147e6c8ce17b8f572f343e4906f844a"},"cell_type":"markdown","source":"### Ingredients that only occur once"},{"metadata":{"trusted":true,"_uuid":"4c898b2653a8a6f413c968e8b86561dcbc682bca"},"cell_type":"code","source":"ing_counts[ing_counts[\"occurance\"] == 1].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ecaeb2d8612088813c762dfb9554a4f223beb189"},"cell_type":"code","source":"ing_counts[ing_counts[\"occurance\"] == 1].shape[0] / ing_counts.shape[0] * 100","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"35a706c153b3811523941df93efffb2e57510796"},"cell_type":"markdown","source":"A quarter of all ingredients only occur once. What to do with those? On the one hand those unique ingredients can lead to overfitting of the entire model. On the other hand they might be very good to detect very specific recipes.\n\nWe could remove them now. This is a simplistic way of feature selection. However, I commented out this part since there is more advanced feature selection done below."},{"metadata":{"trusted":true,"_uuid":"3c86fd8e8157bafc724d2bee887cd963694194a8"},"cell_type":"code","source":"ing_counts.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"63601b5a28900cf6c84b3a67383f0eced3fe947e"},"cell_type":"code","source":"matched_ingredients = ing_counts[\"fuzzy_match\"].unique()\ning_counts_reduced = pd.DataFrame([ ing_counts.loc[ing, :] for ing in matched_ingredients if ing_counts.loc[ing, \"occurance\"] > 10 ])\ning_counts_reduced.drop(\"fuzzy_match\", axis=1);\ning_counts_reduced.index.name = \"ingredient\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2e7bc779d4f8daa5d43d284e38a698468a19ac38"},"cell_type":"code","source":"ing_counts_reduced.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"54f8fd5ed482dbdc1825b7cefb26ca4567917c97"},"cell_type":"code","source":"def remove_unique_ings(x):\n    recipe_ingredients = []\n    for ing in x[\"ingredients\"]:\n        if ing in ing_counts_reduced.index.values.tolist():\n            recipe_ingredients.append(ing)\n    x[\"ingredients\"] = recipe_ingredients\n    \n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a0c95587afc61e03fc70aee2bdca37663b9a6a26"},"cell_type":"code","source":"# Commenting out as discussed above...\n#dat = dat.apply(remove_unique_ings, axis=1);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d25f5b220c417b16c68d086bfd62194edd5a1e71"},"cell_type":"markdown","source":"### Introducing an own transformator for scikit"},{"metadata":{"_uuid":"cb9533b79bf920457f92905295f998a4c323a968"},"cell_type":"markdown","source":"Yes this is going to be lots of code duplication. But it is just something I wanted to try out. Later-on I could use the transformer as part of a pipeline that I can input to GridSearch.\nI am using a script in parallel to this kernel to do all the work on the test set and the submission. The CuisineTransformer class is actually maintained in there and also used for the work on the train set. "},{"metadata":{"trusted":true,"_uuid":"9da8caaf0f046b98eb899335a96beae05849e410"},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"72f4e42fa27fb99c557215d5cd3f704bab3d06c3"},"cell_type":"code","source":"class CuisineTransformer(BaseEstimator, TransformerMixin):  \n    \"\"\"Data cleaning for the cuisine classification\"\"\"\n\n    def __init__(self, min_fuzzy_ratio=90, min_occurance=10, predefined_ingredients = [], enable_stemming=True, debug_print=False):\n    \n        \"\"\"\n        Called when initializing the classifier\n        \"\"\"\n        self.min_fuzzy_ratio = min_fuzzy_ratio\n        self.min_occurance = min_occurance\n        self.enable_stemming = enable_stemming\n        self.debug_print = debug_print\n        \n        self.ps = PorterStemmer()\n        self.ing_counts = Counter()\n        self.ing_list = pd.DataFrame()\n        self.ing_list_reduced = pd.DataFrame()\n        self.predefined_ingredients = pd.DataFrame(predefined_ingredients)\n        \n        self.processed_ingredients_ = pd.DataFrame()\n        self.transformed_lower_wo_whitespace_ = pd.DataFrame()\n        self.transformed_removed_numbers_ = pd.DataFrame()\n        self.transformed_stemmed_ingredients_ = pd.DataFrame()\n        self.transformed_fuzzy_matched_ingredients_ = pd.DataFrame()\n        self.transformed_removed_unique_ingredients_ = pd.DataFrame()\n        self.transformed_ = pd.DataFrame()\n\n    def fit(self, X, y=None):\n        \"\"\"\n        The main work is done in the transform method\n        \"\"\"\n        return self\n\n    def transform(self, X, y=None):\n        self.transformed_ = X\n        self.transformed_lower_wo_whitespace_ = self.transformed_.apply(self._to_lowercase_wo_whitespace, axis=1)\n        self.transformed_removed_numbers_ = self.transformed_lower_wo_whitespace_.apply(self._remove_numbers_symbols, axis=1)\n        \n        if (self.enable_stemming == True):\n            self.transformed_stemmed_ingredients_ = self.transformed_removed_numbers_.apply(self._stem_ingredients, axis=1)\n        \n        if len(self.predefined_ingredients) == 0:\n            self._count_ingredients(self.transformed_stemmed_ingredients_)\n            self.ing_list.set_index('ingredient', inplace=True)\n        else:\n            self.ing_list = self.predefined_ingredients\n        \n        self.ing_list[\"fuzzy_match\"] = None\n        self.transformed_fuzzy_matched_ingredients_ = self.transformed_stemmed_ingredients_.apply(self._fuzzy_match, axis=1);\n        matched_ingredients = self.ing_list[\"fuzzy_match\"].unique()\n        \n        if (self.min_occurance > 1):\n            self.ing_list_reduced = pd.DataFrame([ self.ing_list.loc[ing, :] for ing in matched_ingredients if self.ing_list.loc[ing, \"occurance\"] > 10 ])\n            self.ing_list_reduced.drop(\"fuzzy_match\", axis=1);\n            self.ing_list_reduced.index.name = self.ing_list.index.name\n            self.transformed_removed_unique_ingredients_ = self.transformed_fuzzy_matched_ingredients_.apply(self._remove_unique_ings, axis=1)\n            \n            self.processed_ingredients_ = self.ing_list_reduced.index\n            self.transformed_ = self.transformed_removed_unique_ingredients_\n        else:\n            self.processed_ingredients_ = matched_ingredients\n            self.transformed_ = self.transformed_fuzzy_matched_ingredients_\n            \n        return self\n    \n    def _to_lowercase_wo_whitespace(self, x):\n        #x is a series\n        x[\"ingredients\"] = [ing.lower() for ing in x[\"ingredients\"]]\n        x[\"ingredients\"] = [ing.strip() for ing in x[\"ingredients\"]]\n        return x\n    \n    def _remove_numbers_symbols(self, x):\n        ings = x[\"ingredients\"]\n        for idx, ing in enumerate(ings):\n            ings[idx] = re.sub(r\"\\W+\", \" \", ing)\n            ings[idx] = re.sub(r\"oz[. ]\", \" \", ing)\n        x[\"ingredients\"] = ings\n        return x\n    \n    def _stem_ingredients(self, x):\n        ings = x[\"ingredients\"]\n        for idx, ing in enumerate(ings):\n            ings[idx] = self.ps.stem(ing)\n        x[\"ingredients\"] = ings\n        return x\n\n    def _count_ingredients(self, x):\n        x.apply(self._count_ing_separately, axis=1)\n        self.ing_list = pd.DataFrame(self.ing_counts.most_common(), columns=['ingredient', 'occurance'])\n\n        #Lets sort them\n        self.ing_list.sort_values('occurance',ascending=False)\n    \n    def _count_ing_separately(self, x):\n        self.ing_counts.update(x[\"ingredients\"])\n        return x\n    \n    def _fuzzy_match(self, x):\n        recipe_ingredients = x[\"ingredients\"]\n        for idx, ing in enumerate(recipe_ingredients):\n            # Check if the ingredient is already in our ingredient list\n            # If the ingredient list was predefined it might not be in there\n            if ing in self.ing_list.index:\n                cur_fuzzy_match = self.ing_list.loc[ing, [\"fuzzy_match\"]].iloc[0]\n            else:\n                cur_fuzzy_match = None\n                \n            if (cur_fuzzy_match == None):\n                #Lets use my ingredient counters dataframe to get the whole list of ingredients\n                matches_raw = fuzzywuzzy.process.extract(ing, self.ing_list.index, limit=5, scorer=fuzzywuzzy.fuzz.token_sort_ratio)\n                #Minimimum fuzzy matching ratio\n                matches = [match for match in matches_raw if match[1] >= self.min_fuzzy_ratio]\n                #Interleave with the count values of the words\n                matches_df=pd.DataFrame(matches, columns=[\"ingredient\", \"match_score\"]) \n                matches_df = matches_df.merge(self.ing_list, how='inner', on=\"ingredient\")\n                if (len(matches_df) > 0):\n                    #Normalize\n                    matches_df['match_score_norm']= matches_df[\"match_score\"] / 100\n                    if (len(matches_df) > 2):\n                        matches_df['occurance_norm']= minmax_scaling(matches_df[\"occurance\"], columns=[0])\n                    else:\n                        matches_df['occurance_norm'] = 1.0\n                    #Create an overall scoring\n                    matches_df['overall_score'] = matches_df['match_score_norm'] * matches_df['occurance_norm']\n                    # Get the ingredient with the maximum scoring\n                    idx_max = matches_df['overall_score'].idxmax()\n                    #Replace the ingredient by the ingredient with the maximum overall scoring\n                    if (recipe_ingredients[idx] != matches_df.loc[idx_max, \"ingredient\"]):\n                        if (self.debug_print == True):\n                            print (\"Ingredient {} replaced by {}\".format(recipe_ingredients[idx], matches_df.loc[idx_max, \"ingredient\"]))\n                        recipe_ingredients[idx] = matches_df.loc[idx_max, \"ingredient\"]\n\n                    #Save the choice to avoid further fuzzy matching when the ingredient shows up again\n                    if (ing in self.ing_list.index.values):\n                        self.ing_list.loc[ing, [\"fuzzy_match\"]] = matches_df.loc[idx_max, \"ingredient\"]\n                else:\n                    if (self.debug_print == True):\n                        print (\"          No fuzzy match for ingredient {}. Next match is {} with score {}. Use original\".format(ing, matches_raw[0][0], matches_raw[0][1]))\n            else:\n                recipe_ingredients[idx] = cur_fuzzy_match\n        x[\"ingredients\"] = recipe_ingredients\n\n        return x\n\n    def _remove_unique_ings(self, x):\n        recipe_ingredients = []\n        for ing in x[\"ingredients\"]:\n            if ing in self.ing_list_reduced.index.values.tolist():\n                recipe_ingredients.append(ing)\n        x[\"ingredients\"] = recipe_ingredients\n\n        return x","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"622ccf3c4bf711588b727b43c23529929dac23bb"},"cell_type":"markdown","source":"## Feature Selection"},{"metadata":{"_uuid":"4e56650a366aec48911d2bec177cf318d7366d33"},"cell_type":"markdown","source":"### Filter: Ingredients with low and high rank "},{"metadata":{"_uuid":"dc558a2636c79e634cd50ffb91d1242377388ead"},"cell_type":"markdown","source":"By discarding ingredients that only occur seldom I already applied a ranking method. The method basically uses the correlation of an individual feature to the outcome variable. Ingredients that occur in only one or a few recipes do not correlate a lot with the  outcome since they are completely useless in the prediction of recipes that do not have that ingredient included.\nHence, ranking based on occurance is a very easy method of ranking."},{"metadata":{"_uuid":"05c4e772798a64f7fbf3b1ed52caca13edf80949"},"cell_type":"markdown","source":"But now, lets look into more advanced feature selection methods."},{"metadata":{"_uuid":"5fff5881f9d50c6365c2380fdb1e38f43a65a0af"},"cell_type":"markdown","source":"###  Univariate feature selection: Ingredients that correlate to the output variable (cuisine)"},{"metadata":{"trusted":true,"_uuid":"800912513e42be8ded492f0c2c6147fcf54db348"},"cell_type":"code","source":"# Lets use Sklearn to transform the ingredient lists to binarized feature matrix first\nfrom sklearn.preprocessing import MultiLabelBinarizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1fc3c0d89b566d09206db2f8c59bfc8284c35710"},"cell_type":"code","source":"y = dat[\"cuisine\"]\n#Pass the ingredients_list as a preordering to MLB. We need this to restore the labels later-on\nmlb = MultiLabelBinarizer(ingredients_list)\nX = mlb.fit_transform(dat[\"ingredients\"])\nX.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d198a8506ea3e6de2f227276a1ac287bcffa0de0"},"cell_type":"code","source":"feature_labels = mlb.classes_\nfeature_labels.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"88122dec81ed720c4cf1fe3f1d1c6f83fd14597a"},"cell_type":"markdown","source":"* Lets start with linear regression and the R^2 score to filter the ingredients. We have that much features and with linear regression we can easily access them individually "},{"metadata":{"trusted":true,"_uuid":"f9408d5259753552c4813c6e59d64de1f3dbc063"},"cell_type":"code","source":"from sklearn.feature_selection import chi2\nfrom sklearn.feature_selection import SelectPercentile","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ba9f0cf4db45cd6c2e1d4e79ee1fc39c53106c21"},"cell_type":"code","source":"kbest_model = SelectPercentile(score_func=chi2, percentile=50)\n%time X_chi2 = kbest_model.fit_transform(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"67a8f22f479887a04da03521710b26eb7a8dfcec"},"cell_type":"code","source":"# lets look at the score\nX_chi2.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0f90b169b508cd697dec87a52fa1769667b727e6"},"cell_type":"code","source":"chi2_features_by_importance = pd.DataFrame({ \"Feature\": feature_labels, \"Score\": kbest_model.scores_})\nchi2_features_by_importance = chi2_features_by_importance.iloc[kbest_model.get_support(), :]\nchi2_features_by_importance.sort_values(by=\"Score\", ascending=False, inplace=True)\nchi2_features_by_importance.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"89a02ab87a3dc5955bd6ed10bdad7aed182be9ca"},"cell_type":"code","source":"chi2_features_by_importance.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7b57d02d61b5150c1592ce286556fa6204d69c49"},"cell_type":"markdown","source":"### Wrapper: Stepwise regression for feature selection"},{"metadata":{"trusted":true,"_uuid":"86f6f7a45ed84861e51eeec86ff69584b66dad2a"},"cell_type":"code","source":"# Lets use the recursive feature eleminiation (RFE) from sklearn\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4a3a176289cf2fdb9e0f124e4e5afe8304f80c1d"},"cell_type":"code","source":"rfe_model = RFE( \\\n    estimator=LogisticRegression(solver=\"newton-cg\",        #newton-cg work well on multinomial datasets\n                                 multi_class=\"multinomial\", #lets use softmax instead of OVR\n                                 n_jobs=-1,                 #use all CPUs\n                                 max_iter=5),               #stop quickly        \n    step=100)                                        #increased step size to make things faster\n%time X_rfe = rfe_model.fit_transform(X, y)\nX_rfe.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b51daaa3df0fc31eea7e5967b1042a9c45fe102d"},"cell_type":"code","source":"rfe_features_by_importance = pd.DataFrame({ \"Feature\": feature_labels, \"Ranking\": rfe_model.ranking_})\nrfe_features_by_importance = rfe_features_by_importance.iloc[rfe_model.support_,:]\nrfe_features_by_importance.head(20)\n#The ranking is always 1 if the feature got selected, hence always 1 in this DataFrame","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"208e4cf38a662602db5e9c29174386e59bfe00b8"},"cell_type":"code","source":"rfe_features_by_importance.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"27e58ec66389cb9c7c2c94daeb009c2acf5aa9f9"},"cell_type":"markdown","source":"> ### Random forest: Rank features based on model"},{"metadata":{"trusted":true,"_uuid":"cfa7f7f2b1ba6adba2d96d3feac11cb940231df2"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import SelectFromModel","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0ed5b1fd4b2f9eb197fee0f4ee2cbe810567f7c7"},"cell_type":"code","source":"rfc = RandomForestClassifier(n_estimators=50, n_jobs=-1, max_depth=8)\n%time rfc.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"059aa3aafde88c2bddfeb73ed65813df74c716d0"},"cell_type":"code","source":"rfc_model = SelectFromModel(rfc,\n                            prefit=True,           #Fit was already called\n                            threshold=\"0.1*mean\")  #More features to have a similar amount of features\n                                                   #like chosen by the other feature selection algo's\n%time X_rfc = rfc_model.transform(X)\nX_rfc.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f54ea310fb7130d704eb9a32835a3eef3e421d72"},"cell_type":"code","source":"rfc_features_by_importance = pd.DataFrame({ \"Feature\": feature_labels, \"Importance\": rfc.feature_importances_})\nrfc_features_by_importance = rfc_features_by_importance.iloc[rfc_model.get_support(), :]\nrfc_features_by_importance.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8b72727aaaf577ba905fac92acfbb764308d69e3"},"cell_type":"code","source":"rfc_features_by_importance.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2325af09731db3915dc0460e8ae74a6c4835d37c"},"cell_type":"markdown","source":"### A further ensemble with AdaBoost"},{"metadata":{"trusted":true,"_uuid":"0938055e11d454b254160cc7c5b5ea1d8bc22ee5"},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.svm import LinearSVC","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"726ee8e2fbdd79f7a8c50d9f06be94e031121c37"},"cell_type":"code","source":"# This setting seems to work quite well. Parameters come from tuning, done below\nsvc = LinearSVC(C=10)\nsvc_ada = AdaBoostClassifier(base_estimator=svc, algorithm=\"SAMME\")\n#Prediction comes below","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"447090920cd20b9ffc81d9807f2e08af1063e587"},"cell_type":"code","source":"from sklearn.naive_bayes import BernoulliNB","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"726ee8e2fbdd79f7a8c50d9f06be94e031121c37"},"cell_type":"code","source":"bnb = BernoulliNB()\nbnb_ada = AdaBoostClassifier(base_estimator=bnb, algorithm=\"SAMME\")\n#Prediction comes below","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"831aac9a64cb14595f3e7b89c8a04d5aa4be8f95"},"cell_type":"markdown","source":"## Models"},{"metadata":{"_uuid":"7d716a51414944541e35d293f832b7d074ddca26"},"cell_type":"markdown","source":"### Naive Bayes"},{"metadata":{"_uuid":"f32f97ea612c585055b7af2f1df03be22e102ba2"},"cell_type":"markdown","source":"Due to the high dimensionality of our feature matrix we use Naive Bayes as a first shot since it can handle the data. Lets use Bernoulli since it is categorical data."},{"metadata":{"trusted":true,"_uuid":"447090920cd20b9ffc81d9807f2e08af1063e587"},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2f4df479bb0f2a7daf5fab6b59a17d973b019f4a","_kg_hide-input":false},"cell_type":"code","source":"# Naive Bayes on original but cleaned up dataset X\nbnb = BernoulliNB()\n%time cross_bnb = cross_val_score(bnb, X, y, cv=10)\ncross_bnb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a81d3d0f5b397c04fa8a3c70807362088113b7d4"},"cell_type":"code","source":"np.mean(cross_bnb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"002c925053a3b8f5e91d6172c64caa874a54e365"},"cell_type":"code","source":"# Naive Bayes on dataset that was scope of univariate feature selection\n%time cross_chi2 = cross_val_score(bnb, X_chi2, y, cv=10)\ncross_chi2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ee889e258b551d822056b7c58cef6a580621f035"},"cell_type":"code","source":"np.mean(cross_chi2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ae8e91db247fcd1d9d541a9149b65b3fed2c10ae"},"cell_type":"code","source":"# Naive Bayes on dataset that was scope of recursive feature elimination\n%time cross_rfe = cross_val_score(bnb, X_rfe, y, cv=10)\ncross_rfe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5d4da897607ffc3124b86851aa34e605a946de2e"},"cell_type":"code","source":"np.mean(cross_rfe)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ae8e91db247fcd1d9d541a9149b65b3fed2c10ae"},"cell_type":"code","source":"# Naive Bayes on dataset that was scope of model based feature selection (Random forest)\n%time cross_rfc = cross_val_score(bnb, X_rfc, y, cv=10)\ncross_rfc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1d3363b53ddcf8e5cf0d61c9f58e0ff8f83974aa"},"cell_type":"code","source":"np.mean(cross_rfc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ae8e91db247fcd1d9d541a9149b65b3fed2c10ae"},"cell_type":"code","source":"# Adaboost performs bad and takes incredibly much time... commenting out\n# otherwise the kernel exceeds time limit...\n\n# An ensemble of Naive Bayes created by Ada Boost\n#%time cross_ada = cross_val_score(bnb_ada, X, y, cv=10)\n#cross_ada","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9ba119d483aadaedd655b762525f4d1f4f953d25"},"cell_type":"code","source":"#np.mean(cross_ada)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7cab12e50ff4972effc694c39641951cafbc2521"},"cell_type":"markdown","source":"### SVC"},{"metadata":{"trusted":true,"_uuid":"6478589ecf76cd7e73276fec03d9a200e7c4c30b"},"cell_type":"code","source":"from sklearn.multiclass import OneVsRestClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a74c8a17375633174af681857cf94098f3e59e07"},"cell_type":"code","source":"svc = LinearSVC(C=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"13ca0219428575cd9d6c7cf1bcca54ee43c719d6"},"cell_type":"code","source":"# SVC on original but cleaned up dataset X\n%time cross_svc = cross_val_score(svc, X, y, cv=10)\ncross_svc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e0f70cc24a1e327acfcdeac3f1d80cfea78f42a7"},"cell_type":"code","source":"np.mean(cross_svc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bb59deef2e36ad72972b057082dccdde63d03017"},"cell_type":"code","source":"# SVC on dataset that was scope of univariate feature selection\n%time cross_chi2_svc = cross_val_score(svc, X_chi2, y, cv=10)\ncross_chi2_svc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8542f49fbd2b9f8fca8a3639c4011affc69c64cb"},"cell_type":"code","source":"np.mean(cross_chi2_svc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5704e019ca4a9718cbd14d8b737359a4ff00847c"},"cell_type":"code","source":"# SVC on dataset that was scope of recursive feature elimination\n%time cross_rfe_svc = cross_val_score(svc, X_rfe, y, cv=10)\ncross_rfe_svc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1ae7175ff3717128314980997c4bcc8f14d919dd"},"cell_type":"code","source":"np.mean(cross_rfe_svc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d0f4f498a748db9de6301412622a75fe61ca25ca"},"cell_type":"code","source":"# SVC on dataset that was scope of model based feature selection (Random forest)\n%time cross_rfc_svc = cross_val_score(svc, X_rfc, y, cv=10)\ncross_rfc_svc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f0ebdaaa296b444ca9b28fc013393d8edbea8a72"},"cell_type":"code","source":"np.mean(cross_rfc_svc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ae8e91db247fcd1d9d541a9149b65b3fed2c10ae"},"cell_type":"code","source":"# Adaboost performs bad and takes incredibly much time... commenting out\n# otherwise the kernel exceeds time limit...\n\n# An ensemble of SVC's created by Ada Boost\n#%time cross_ada = cross_val_score(svc_ada, X, y, cv=10)\n#cross_ada","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"94adc166e561d07298d1b4103f49264d6806ef18"},"cell_type":"code","source":"#np.mean(cross_ada)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8e7eadffe1c210561f0f8d0bc435b90719abf8d0"},"cell_type":"markdown","source":"## Model selection, tuning and metrics"},{"metadata":{"_uuid":"87f65aa59f496b564b10c6d373161ef600b13f3a"},"cell_type":"markdown","source":"### Gridsearch"},{"metadata":{"trusted":true,"_uuid":"0009efc129d91a5cd8a71761cf51d7ecf459e3a0"},"cell_type":"code","source":"#Train the selected model with all training data\nmodel_selection = svc\nX_selection = X_rfc\nfeature_selection = rfc_features_by_importance[\"Feature\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"658ab915d3add34bb366283003e413d12c508665"},"cell_type":"code","source":"param_grid = [\n  {\n   'C': [1, 0.1, 0.01, 10]\n  }\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8ae07a8aeaf67d3af9f672fefacdda2f8dfdd527"},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b6076609ab5f4eb208d0eaaa2e8d0a8398af08c1"},"cell_type":"code","source":"gsc = GridSearchCV(model_selection, param_grid, n_jobs=-1, cv=10, iid=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"54e11f9d89f8733f7e13b2a5aa06d1b9661ad144"},"cell_type":"code","source":"%time gsc.fit(X_selection, y)\nmeans = gsc.cv_results_['mean_test_score']\nstds = gsc.cv_results_['std_test_score']\nfor mean, std, params in zip(means, stds, gsc.cv_results_['params']):\n    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4da741421a0a3596e066bf126cc493d1c5b8bb96"},"cell_type":"code","source":"gsc.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1d838990a11b18ddb0414d4d363dab889128b9e5"},"cell_type":"code","source":"from sklearn.metrics import classification_report","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"67664bd1a4975262e469fccc5ca0b91124149d1f"},"cell_type":"markdown","source":"### Heatmap"},{"metadata":{"trusted":true,"_uuid":"256d675c5177d441a59821600a91385fe27ffa93"},"cell_type":"code","source":"from sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"06d0b77d48fb3abe3768596f27ac7208684c0bc6"},"cell_type":"code","source":"%time y_pred = cross_val_predict(model_selection, X_selection, y, cv=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"60762112fbc12c02f265ece8a884cc3b4758736f"},"cell_type":"code","source":"confusion = confusion_matrix(y, y_pred)\ny_classes = np.unique(y)\ny_classes.sort()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"339af213880fe84607c43dc7e9bffc6965797667"},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10,10))  \nsb.heatmap(confusion, xticklabels=y_classes, yticklabels=y_classes, cbar=False, annot=True, ax=ax)\nplt.xlabel('predicted value')\nplt.ylabel('true value');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7b6c642d9eff321d16ae2717695f2f2430391b72"},"cell_type":"markdown","source":"### ROC curve"},{"metadata":{"_uuid":"e0d7c0ab912b4078befe91dc214cb3e2bb56fb22"},"cell_type":"markdown","source":"## Summary and submission"},{"metadata":{"_uuid":"1c986dac61f0d6f4f9286b380d0c74beeec928c0"},"cell_type":"markdown","source":"**Used parameters:**"},{"metadata":{"trusted":true,"_uuid":"0009efc129d91a5cd8a71761cf51d7ecf459e3a0"},"cell_type":"code","source":"#Train the selected model with all training data\nC=gsc.best_params_['C']\n#For RBF kernels ... they take too long we use linear instead...\n#gamma=gsc.best_params_['gamma']\n#kernel=gsc.best_params_['kernel']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cbf348d0077b57b8cec4fd14e2c4ca26521d21f2"},"cell_type":"code","source":"model_selection = LinearSVC(C=C)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dc7caae6c56572c7dfc55260253431efdb0b91c3"},"cell_type":"code","source":"#Train model with entire training data\nmodel_selection.fit(X_selection,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"4eb867c44550a4e6373ec0f1205a0091b8804be1"},"cell_type":"code","source":"with open(\"../input/test.json\", \"r\") as file_test:\n    test_set = json.load(file_test)\n    test_set = pd.DataFrame(test_set)\n    test_set.set_index(\"id\", inplace=True, verify_integrity=True)\ntest_set.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5a0fbf70ec9c6fe1961115422cc4108ab819956d"},"cell_type":"code","source":"#Debugging\nif DEBUG_SAMPLES:\n    test_set = test_set.sample(DEBUG_SAMPLES)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"972668990adc1165fe52be8ad9ce0b676ace5be6"},"cell_type":"code","source":"test_set.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"267540ba1ab909b7c4b70aa81c77e22782aaf756"},"cell_type":"code","source":"#Process the test_set\n#This time we want to fuzzy match against a lot more. If things are still somehow similar its good\n#Furthermore we dont' want to throw away any ingredient\nct = CuisineTransformer(min_fuzzy_ratio=90, min_occurance=1, predefined_ingredients=ing_counts_reduced)\nct.fit_transform(test_set)\nct.transformed_\n#If we would have used the CuisineTransformer for the train set we could just take out\n#the processed_ingredients from there and put it in the transformer for the test set ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"378989a50e66d7b6fb20d1781828ca933d912061"},"cell_type":"code","source":"ct.transformed_.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2da9cb627ae5b1aebadade95c517c5fde0d0ad43"},"cell_type":"code","source":"ct.transformed_.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d9ac7dc2050cd727c230cbd7ca3c33a9cb175587"},"cell_type":"code","source":"mlb_test = MultiLabelBinarizer(feature_selection)\nX_test = mlb_test.fit_transform(ct.transformed_[\"ingredients\"]);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f39db010bdf1bb20d4cfcb0603e14bce40de2686"},"cell_type":"code","source":"print (\"Known classes from train set: {}.\".format(ing_counts_reduced.index.shape[0]))\nprint (\"Total classes from test set: {}.\".format(ct.processed_ingredients_.shape[0]))\nunknown_ings = [ing_test for ing_test in ct.processed_ingredients_ if ing_test not in ing_counts_reduced.index.values]\nunknown_ings = pd.Series(unknown_ings)\nprint (\"Total unknown classes: {}.\".format(unknown_ings.shape[0]))\nprint (\"{:.2f}% unknown\".format(unknown_ings.shape[0]/ct.processed_ingredients_.shape[0]*100))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"15469a333c6fd37e33f7a3de5da1a5060e648fd8"},"cell_type":"markdown","source":"Wow... there are quite a lot unknowns. How could we reduce the list of unknowns furthermore? Lets look at some of the unknowns:"},{"metadata":{"trusted":true,"_uuid":"fa7b2eb11bcba40a83b91fcce899070c5565abef"},"cell_type":"code","source":"unknown_ings.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"60d186408c5ced4c60bf71dd11d2d9a24b83342c"},"cell_type":"markdown","source":"**Outstanding improvement:**\nThere are lots of ingredients with adjectives included, that are not from interest. However, I already tried to use NLTK to tag those adjectives. Since we miss a meaningful context, that does not work. "},{"metadata":{"trusted":true,"_uuid":"f1afffae32f713b3a86b15437ff55adacd458924"},"cell_type":"code","source":"y_test = model_selection.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"464cf86195a406d569c905dd47035b642d99dec8"},"cell_type":"code","source":"test_ids = ct.transformed_.index\n#test_ids = test_set.index\nsub = pd.DataFrame({'id': test_ids, 'cuisine': y_test}, columns=['id', 'cuisine'])\nsub.to_csv('output.csv', index=False)\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7f8392ef85c049a292d11bffb2712794376814ef"},"cell_type":"code","source":"# Do some consistency checking\n# join by index and compare if the ingredients lists are similar\nct.transformed_.join(test_set, how=\"inner\", lsuffix=\"_transformed\", rsuffix=\"_sub\").sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bdadbc49af2cbfb03f97d95777da235060f1d421"},"cell_type":"code","source":"# This one looks good. Now check if the index complies with the index in the submission test set\nsub_with_index = sub.set_index(\"id\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"618730f915915b6aa2cea33bbd5c03f84d7be469"},"cell_type":"code","source":"sub_with_index.join(ct.transformed_, how=\"inner\", lsuffix=\"_transformed\", rsuffix=\"_sub\").sample(100)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}