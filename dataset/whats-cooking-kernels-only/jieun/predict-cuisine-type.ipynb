{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport json\nimport csv\nfrom tqdm import tqdm_notebook\nimport pickle\nimport time\nimport nltk\nimport re\nimport string\nimport unicodedata\nfrom timeit import default_timer as timer\n\nimport keras\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\nfrom sklearn.naive_bayes import ComplementNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.decomposition import PCA\nfrom xgboost import XGBClassifier\n\nfrom sklearn import metrics\nfrom sklearn import model_selection\nfrom sklearn import preprocessing\n\nfrom hyperopt import STATUS_OK, hp, tpe, fmin, Trials\nfrom hyperopt.pyll.stochastic import sample\nfrom hyperopt.pyll.base import scope\n\nimport matplotlib.pyplot as plt\nsns.set_context('notebook')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Yummly Dataset\n\nObjective: predict the cuisine based on the list of ingredients.\n\nLearning Goal: apply the techniques learned from the previous Titanic exercise with the addition of working with text data, apply dimensionality reduction techniques."},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_0 = pd.read_json('../input/train.json')\ndf_test_0 = pd.read_json('../input/test.json')\n\ndisplay(df_train_0.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.shape(df_train_0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check for any nulls."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_train_0.isnull().sum())\nprint(df_test_0.isnull().sum())\n\ndf_train = df_train_0.copy()\ndf_test = df_test_0.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"How many ingredients are there for each recipe?"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['num_ing'] = df_train['ingredients'].apply(lambda x: len(x))\ndf_test['num_ing'] = df_test['ingredients'].apply(lambda x: len(x))\nplt.figure(1, figsize=(10,3))\ns = sns.countplot(x='num_ing', data=df_train)\ns.set_yscale('log')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's weird that there are single ingredient \"recipes\" but they still contain useful information about each cuisine."},{"metadata":{"trusted":true},"cell_type":"code","source":"display(df_train.loc[df_train.num_ing < 2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What's the recipe with >60 ingredients?!"},{"metadata":{"trusted":true},"cell_type":"code","source":"display(df_train.loc[df_train.num_ing > 60])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gtr_60_ind = df_train[df_train['num_ing'] > 60].index.values\nprint(df_train.ingredients[gtr_60_ind].tolist())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This seems like a totally weird recipe. It calls for 'boneless skinless chicken breast halves', 'cooked chicken', 'chicken cutlets', and 'boneless skinless chicken breasts.' Here's another recipe with suspiciously high number of ingredients, and an even more suspicious ingredient list."},{"metadata":{"trusted":true},"cell_type":"code","source":"display(df_train.loc[df_train.num_ing > 40])\nprint(df_train.ingredients[26103])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on the distribution of number of ingredients, let's make a cut on rows with number of ingredients > 31. For every number of ingredients exceeding 31, there are fewer than 10 recipes each, so we're throwing out fewer than ~100 rows out of close to 40000 rows. We want to be conservative with these cut because we don't want to bias against cuisines that use a lot of ingredients."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = df_train[df_train['num_ing'] < 32]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.reset_index(inplace=True, drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, moving on to the cuisines themselves: italian and Mexican cuisines are the top two popular categories. There's moderate class imbalance."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(1, figsize=(10,5))\ns = sns.countplot(x='cuisine', data=df_train)\ndummy = s.set_xticklabels(s.get_xticklabels(), rotation=60)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's preprocess the ingredient names. We will first pass the ingredients through a lemmatizer to return the lemma, or the base of the word."},{"metadata":{"trusted":true},"cell_type":"code","source":"lemmatizer = nltk.WordNetLemmatizer()\n#tomatoes would be modified to tomato\nprint(lemmatizer.lemmatize('tomatoes'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def ingredients(df, i):\n    ing_list = df['ingredients'].iloc[i].copy()\n\n    ing_concat = []\n    for ing in ing_list:\n        #replace accent characters, copied directly from (https://stackoverflow.com/questions/517923/what-is-the-best-way-to-remove-accents-in-a-python-unicode-string)\n        ing = u\"\".join([c for c in unicodedata.normalize('NFKD', ing) if not unicodedata.combining(c)])\n        #remove alphabet characters, e.g., numbers, special characters\n        ing = re.sub(r'[^a-zA-Z]',' ', ing)\n\n        #this was included later. RF features analysis shows this as an important feature.\n        #turn this into one word so it wont be separated into \"fish\" and \"sauce.\"\n        #ignore prefixes such as \"thai\" and \"asian.\"\n        if 'fish sauce' in ing:\n            ing = 'fishsauce'\n        #similarly for corn starch and sour cream\n        if 'corn starch' in ing:\n            ing = 'cornstarch'\n        if 'sour cream' in ing:\n            ing = 'sourcream'\n\n        word_list = ing.split()\n        for word in word_list:\n            word = word.strip()\n            lem = lemmatizer.lemmatize(word)\n            ing_concat.append(lem)\n    ing_concat = list(np.unique(ing_concat))\n    df.at[i, 'ingredients_concat'] = ' '.join(ing_concat) \n","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"for i in range(df_train.shape[0]):\n    ingredients(df_train, i) \nfor i in range(df_test.shape[0]):\n    ingredients(df_test, i) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Get the simple counts of words from the ingredients."},{"metadata":{"trusted":true},"cell_type":"code","source":"count_vect = CountVectorizer()\nX_train_counts = count_vect.fit_transform(df_train.ingredients_concat)\nX_test_counts = count_vect.transform(df_test.ingredients_concat)\nprint('A total of', X_train_counts.shape[0], 'recipes')\nprint('A total of', X_train_counts.shape[1], 'unique ingredient words')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some of the most common ingredients include things like salt, onions, oil, and water."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"ingredient_names = np.array(count_vect.get_feature_names())\ningredient_counts = dict(zip(ingredient_names, X_train_counts.sum(axis=0).tolist()[0]))\ningredient_counts = pd.DataFrame(list(ingredient_counts.items()), columns=['ingredients', 'counts'])\ningredient_counts['counts_frac'] = ingredient_counts['counts'].apply(lambda x: x/X_train_counts.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ingredient_counts.sort_values(by='counts', ascending=False).head(n=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now apply TF-IDF. Instead of simple counts of ingredients in each recipe, this approach takes into account that some ingredients appear very frequently (e.g., salt) and therefore contain less discriminating power. "},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf = TfidfTransformer()\nX_train_tfidf = tfidf.fit_transform(X_train_counts)\nX_test_tfidf = tfidf.transform(X_test_counts)\nprint(X_train_tfidf.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Encode the cuisine names."},{"metadata":{"trusted":true},"cell_type":"code","source":"mlb = preprocessing.MultiLabelBinarizer()\ny_train_mlb = mlb.fit_transform(df_train.cuisine.values.reshape(-1,1))\n\nle = preprocessing.LabelEncoder()\ny_train_le = le.fit_transform(df_train.cuisine)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dimensionality Reduction\n\nThere are close to 2800 features, which is a fairly large number. We can try applying some dimensionality reduction techniques to select only features with some predictive power and decrease the computation time. Techniques we will explore are:\n\n* low variance filter\n* random forest regressor feature importance\n* principle component analysis\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_tfidf_pd = pd.DataFrame(X_train_tfidf.A, columns=ingredient_names)\nX_test_tfidf_pd = pd.DataFrame(X_test_tfidf.A, columns=np.array(count_vect.get_feature_names()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Low variance filter\nOne method is to drop variables with low variance across the samples in the data set."},{"metadata":{"trusted":true},"cell_type":"code","source":"var = X_train_tfidf_pd.var()\nvar.sort_values(ascending=False).head(n=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"var.sort_values(ascending=True).head(n=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We could cut the number of features by about 60% if we throw out features with $\\log{\\sigma} \\lesssim -4$. But this choice of threshold is somewhat arbitrary and we'd have to test several options."},{"metadata":{"trusted":true},"cell_type":"code","source":"s = sns.distplot(np.log10(var), hist_kws={'cumulative': True}, kde_kws={'cumulative': True})\ns.set_ylabel('cdf')\ns.set_xlabel('log(variance)')\ns.set_xlim(-6.4,-2.4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### RF\n\nRun random forest regressor and obtain the gini importance or mean decrease impurity, which is the total reduction in impurity (or error) in nodes averaged over all trees in the ensemble. Higher feature importance means it's more important for classifying the label correctly.\n\nIn general, we have to be careful with interpreting the feature importance because this is biased toward variables with more categories (not an issue here since the features are binary-coded) and correlated features can artificially boost or decrease the importance of other correlated variables (summarized from this [source](https://blog.datadive.net/selecting-good-features-part-iii-random-forests/)).\n\nSimilar to the previous example, we have to make a choice on the total number of features we want to keep or on the minimum threshold value of feature importance. This is also pretty slow."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"rfr = RandomForestRegressor(max_depth=5, n_estimators=10)\nrfr.fit(X_train_tfidf, y_train_le)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_imp = rfr.feature_importances_\nfeat_imp_ind = np.argsort(feat_imp)[-20:]\ns = sns.barplot(x=feat_imp[feat_imp_ind], y=ingredient_names[feat_imp_ind])\ndummy = s.set_xlabel('feature importance')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### PCA\n\nWe will use PCA to create new features that are linear combinations of the existing features. It's fast, so it works well with large number of features. In our case, we only need 1000 out of ~2800 original features to explain 95% of the variations. Let's proceed with these 1000 components."},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=1000)\nX_train_pca = pca.fit_transform(X_train_tfidf_pd)\nX_test_pca = pca.transform(X_test_tfidf_pd)\nX_train_pca_pd = pd.DataFrame(X_train_pca)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s = sns.barplot(x=np.arange(20)+1, y=pca.explained_variance_ratio_[:20]*100)\ns.set_ylabel('explained variance [%]')\ns.set_xlabel('PCA component')\n\nprint(np.cumsum(pca.explained_variance_ratio_)[-1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predicting the Cuisine"},{"metadata":{},"cell_type":"markdown","source":"Let's test the following algorithms:\n\n* **Complement Naive Bayes**, a slightly modified version of the Naive Bayes algorithm for multinomially distributed data (multiple classes). According to the documentation, this works better than the standard multinomial Naive Bayes algorithm when we have imbalanced data sets. One relevant hyperparameter is $\\alpha$, a smoothing term that appears in the estimation of distribution parameters. NOTE: we cannot use NB with the PCA-transformed data because it requires non-negative data. Running this with the original data yields about 77% accuracy, a good benchmark value to keep in the back of our head.\n* **Logistic Regression**\n* **Support Vector Machine**\n* **Gradient Boosting** is an example of boosting, which is an ensemble method that incrementally improves an initially weak model (e.g., 1 level-deep decision tree, a.k.a. \"decision stump\") by modeling the residual in each step. We're using sklearn.GBC here but another popular option is XGBoost.\n* **Neural Networks**"},{"metadata":{},"cell_type":"markdown","source":"First we want to test the baseline performance of the algorithms. These models take a long time to train! Although SVC and XGB may achieve better accuracy in the long run with careful hyperparameter tuning, linear regression may be our best bet out of these options given computational feasibility."},{"metadata":{"trusted":true},"cell_type":"code","source":"#algorithms = {'LR':LogisticRegression(solver='saga', multi_class='multinomial'), 'SVC':SVC(gamma='scale'), 'XGB': XGBClassifier(n_estimators=100)}\n#\n#name_list = algorithms.keys()\n#for i_n, n in enumerate(name_list):\n#    X_train_0, X_test_0, y_train_0, y_test_0 = model_selection.train_test_split(X_train_pca, y_train_le, test_size=0.25)\n#    clf = algorithms[n]\n#    tic = time.time()\n#    clf = clf.fit(X_train_0, y_train_0)\n#    y_pred_0 = clf.predict(X_test_0)\n#    toc = time.time()\n#    print(n, \"Accuracy: {:.2f}\".format(metrics.accuracy_score(y_test_0, y_pred_0)))\n#    print('Time [s]: {:5.2}'.format(toc-tic))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Optimize the hyperparameters for the algorithm with the best baseline performance.\nWe will turn again to hyperopt to automatically fine-tune the hyperparameters and see if we can improve the performance of the classifier. Originally we wanted to try tuning SVC, following Andrew Ng's rule of thumb and using the Gaussian kernel given the size of n and m (n=1000, m~40000). However, the training time was infeasible. Let's see if we can tune LR instead to eke out a better performance."},{"metadata":{"trusted":true},"cell_type":"code","source":"#domain = {\n#            \"penalty\": hp.choice(\"penalty\", ['l1', 'l2']),\n#            \"C\": hp.loguniform(\"C\",-2,2),\n#        }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sample(domain)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Save the result as it updates."},{"metadata":{"trusted":true},"cell_type":"code","source":"#outfile = 'lr_saga_trials.csv'\n#with open(outfile, 'w') as f:\n#    writer = csv.writer(f)\n#    writer.writerow(['loss', 'acc', 'params', 'train_time'])","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"#def objective(hparams):\n#    \n#    tic = timer()\n#    \n#    clf = LogisticRegression(**hparams, solver='saga', multi_class='multinomial', max_iter=300)\n#    cv_score = model_selection.cross_validate(clf, X_train_pca_pd, y_train_le, cv=3, scoring=['neg_log_loss', 'accuracy'], return_train_score=False)\n#    \n#    #you want to minimize log loss \n#    #sklearn returns negative because its convention is to maximize\n#    #but hyperopt minimizes so undo the negative\n#    loss = -np.mean(cv_score['test_neg_log_loss'])\n#    acc = np.mean(cv_score['test_accuracy'])\n#\n#    toc = timer()\n#    \n#    #write out the results in real time\n#    with open(outfile, 'w') as f:\n#        writer = csv.writer(f)\n#        writer.writerow([loss, acc, hparams, toc-tic])\n#    \n#    #so we can monitor the progress!\n#    pbar.update()\n#\n#    return {'loss': loss, 'acc': acc, 'hparams': hparams, 'status': STATUS_OK}    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#keep the training information\n#bayes_trials = Trials()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#MAX_EVALS = 50\n#pbar = tqdm_notebook(total=MAX_EVALS, desc='Hyperopt')\n#\n#best = fmin(fn=objective, space=domain, algo=tpe.suggest, max_evals=MAX_EVALS, trials=bayes_trials)\n#pbar.close()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With only 50 trials, the hyperparameter tuning process may not be fully converged. But let's check our accuracy score."},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(best)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plt.figure(1, figsize=(9,8))\n#for i_k, k in enumerate(bayes_trials.vals.keys()):\n#    ax = plt.subplot('33'+str(i_k+1))\n#    ax.scatter(range(len(bayes_trials.vals[k])), bayes_trials.vals[k], label=k, s=2)\n#    ax.set_ylabel(' '.join(k.split('_')))\n#    ax.set_xlabel('iteration')\n#    plt.tight_layout()\n#\n#plt.figure(2)\n#ax = plt.subplot('111')\n#ax.scatter(range(len(bayes_trials.losses())), bayes_trials.losses(), s=2)\n#ax.set_xlabel('iteration')\n#ax.set_ylabel('loss')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(\"Accuracy: {:.2f}\".format(max(bayes_trials.vals['acc'])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## SVC after all?\n\nInterestingly, even with hyperparameter optimization for LR, we still can't seem to beat SVC. Let's try a very basic grid search with SVC instead. Again, we'll follow Andrew Ng's rule of thumb and use the Gaussian kernel given the size of n and m (n=1000, m~40000)."},{"metadata":{"trusted":true},"cell_type":"code","source":"#clf = SVC(kernel='rbf')\n#gscv = model_selection.GridSearchCV(clf, {'C': [0.1, 1, 3, 5, 10], 'gamma': ['scale', 0.1, 0.01, 0.001]},\\\n#                                scoring='accuracy', cv=3, refit=True, return_train_score=False, n_jobs=-1, verbose=100)\n#gscv.fit(X_train_pca_pd, y_train_le)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(gscv.best_score_)\n#print(gscv.best_params_)\nbest_params = {'C': 5, 'gamma': 'scale'}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prediction Output for Submission File"},{"metadata":{"trusted":false},"cell_type":"code","source":"SVC_tuned_clf = SVC(kernel='rbf', **best_params)\nSVC_tuned_clf.fit(X_train_pca, y_train_le)\ny_true_pred_le = SVC_tuned_clf.predict(X_test_pca).astype('int')\ny_true_pred = le.inverse_transform(y_true_pred_le)\n\nX_true_test_withpred = pd.DataFrame({'id': df_test_0.id, 'cuisine': y_true_pred})\nX_true_test_withpred.to_csv('svc_gscv_prediction.csv', mode='w', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Extra: Neural Network?\n\nAnother option is to try a simple shallow neural network. For this, let's try using the entire feature set, rather than the PCA outputs, with some dropout."},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train_0, X_test_0, y_train_0, y_test_0 = model_selection.train_test_split(X_train_tfidf, y_train_mlb, test_size=0.2)\n\nnum_cuisines = y_train_mlb.shape[1]\nnum_feats = X_train_tfidf.shape[1]","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"inputs = keras.Input(shape=(num_feats,))\n\nX = keras.layers.Dense(2000, activation=None)(inputs)\nX = keras.layers.BatchNormalization()(X)\nX = keras.layers.Activation('relu')(X)\nX = keras.layers.Dropout(0.8)(X)\n\nX = keras.layers.Dense(2000, activation=None)(inputs)\nX = keras.layers.BatchNormalization()(X)\nX = keras.layers.Activation('relu')(X)\nX = keras.layers.Dropout(0.8)(X)\n\nX = keras.layers.Dense(num_cuisines, activation=None)(X)\nX = keras.layers.BatchNormalization()(X)\nY = keras.layers.Activation('softmax')(X)\n\nmodel = keras.models.Model(inputs=inputs, outputs=Y)\nmodel.compile(optimizer=keras.optimizers.Adam(lr=0.001), loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\ncb = keras.callbacks.ModelCheckpoint('cooking.weights.best.hdf5', monitor='val_acc', verbose=1, save_best_only=True, mode='max')\nes = keras.callbacks.EarlyStopping(monitor='val_loss', patience=4)\n\nmodel.fit(X_train_0, y_train_0, epochs=20, batch_size=64, validation_data=(X_test_0, y_test_0), callbacks=[cb, es])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Future work / Closing thoughts\n\nThe hyperparameter tuning was really slow. Beyond hyperparameter tuning/LR itself, other algorithms could have performed better. For example, SVM is a powerful method but it is highly sensitive to the chosen parameters. It would have been interesting to have optimized SVM as well (it was attempted) but it was way too slow on this personal laptop.\n\nThe distribution of cuisines shows that the classes are moderately imbalanced (e.g., many times more italian cuisine than irish cuisine). Would something like oversampling help here?"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":false,"sideBar":true,"skip_h1_title":true,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false}},"nbformat":4,"nbformat_minor":1}