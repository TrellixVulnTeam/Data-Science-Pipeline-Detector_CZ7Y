{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Capstone Project - Cuisine Categorization by the recipe of ingredients"},{"metadata":{"trusted":true},"cell_type":"code","source":"# for Python 2: use print only as a function\nfrom __future__ import print_function","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Exploration and Adding features\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd ","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_json('../input/train.json')\ntest = pd.read_json('../input/test.json')","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"       cuisine                        ...                                                                ingredients\n0        greek                        ...                          [romaine lettuce, black olives, grape tomatoes...\n1  southern_us                        ...                          [plain flour, ground pepper, salt, tomatoes, g...\n2     filipino                        ...                          [eggs, pepper, salt, mayonaise, cooking oil, g...\n3       indian                        ...                                        [water, vegetable oil, wheat, salt]\n4       indian                        ...                          [black pepper, shallots, cornflour, cayenne pe...\n\n[5 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cuisine</th>\n      <th>id</th>\n      <th>ingredients</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>greek</td>\n      <td>10259</td>\n      <td>[romaine lettuce, black olives, grape tomatoes...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>southern_us</td>\n      <td>25693</td>\n      <td>[plain flour, ground pepper, salt, tomatoes, g...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>filipino</td>\n      <td>20130</td>\n      <td>[eggs, pepper, salt, mayonaise, cooking oil, g...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>indian</td>\n      <td>22213</td>\n      <td>[water, vegetable oil, wheat, salt]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>indian</td>\n      <td>13162</td>\n      <td>[black pepper, shallots, cornflour, cayenne pe...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":false},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"%matplotlib inline\nimport matplotlib.pyplot as plt\n\ntrain['cuisine'].value_counts().plot(kind='barh')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##Exploratory Visualization"},{"metadata":{},"cell_type":"markdown","source":"Next, let me explore on the data exploration. especially on\n\n* how many ingredients there are? Out them, find the unique ingredients.\n* how these ingredients made up of individual words?"},{"metadata":{"trusted":false},"cell_type":"code","source":"import re\n\ntotal_ingredients = []Exploratory Visualization\nfor lst_ingredients in train.ingredients:\n    total_ingredients += [ingredient.lower() for ingredient in lst_ingredients]\n\nno_of_ingredients = len(total_ingredients)\nuniq_ingredients = len(set(total_ingredients))\n\nprint(no_of_ingredients)\nprint(uniq_ingredients)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"word_split = re.compile('[,. ]+')\ntotal_ingredient_words = []\n\nfor ingredients in total_ingredients:\n    total_ingredient_words += re.split(word_split, ingredients)\n    \nno_of_ingredients_words = len(total_ingredient_words)\nuniq_ingredients_words = len(set(total_ingredient_words))\n\nprint(no_of_ingredients_words)\nprint(uniq_ingredients_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from collections import Counter\nimport seaborn as sns\nimport numpy as np\n\ncntr = {}\nfor c in train['cuisine'].unique():\n    cntr[c] = Counter()\n    idx = (train['cuisine'] == c)\n    for ingredients in train[idx]['ingredients']:\n        cntr[c].update(ingredients)\n\ncommon_ingredients = pd.DataFrame([[items[0] for items in cntr[c].most_common(5)] for c in cntr],\n            index=[c for c in cntr],\n            columns=['Most_Common_{}'.format(i) for i in range(1, 6)])\n\ncommon_ingredients\n\ncounter1 = Counter(common_ingredients['Most_Common_1'])\n\ning_name1 = counter1.keys()\ning_count1 = counter1.values()\n\ncounter2 = Counter(common_ingredients['Most_Common_2'])\ning_name2 = counter2.keys()\ning_count2 = counter2.values()\n\ncounter3 = Counter(common_ingredients['Most_Common_3'])\ning_name3 = counter3.keys()\ning_count3 = counter3.values()\n\ncounter4 = Counter(common_ingredients['Most_Common_4'])\ning_name4 = counter4.keys()\ning_count4 = counter4.values()\n\nfig = plt.figure(figsize=(5, 5))\n#fig, ax = plt.subplots(4, 4, figsize=(40, 40))\n\n# Plot histogram using matplotlib bar().\nindexes = np.arange(len(ing_name1))\nwidth = 0.2\nplt.bar(indexes, ing_count1, width)\nplt.xticks(indexes + width * 0.5, ing_name1,rotation='vertical')\nplt.title('Most Common Ingredients Rank 1')\nplt.show()\nfig.savefig('rank1.png')\n\nfig = plt.figure(figsize=(5, 5))\nindexes = np.arange(len(ing_name2))\nwidth = 0.2\nplt.bar(indexes, ing_count2, width)\nplt.xticks(indexes + width * 0.5, ing_name2,rotation='vertical')\nplt.title('Most Common Ingredients Rank 2')\nplt.show()\nfig.savefig('rank2.png')\n\nfig = plt.figure(figsize=(5, 5))\nindexes = np.arange(len(ing_name3))\nwidth = 0.2\nplt.bar(indexes, ing_count3, width)\nplt.xticks(indexes + width * 0.5, ing_name3,rotation='vertical')\nplt.title('Most Common Ingredients Rank 3')\nplt.show()\nfig.savefig('rank3.png')\n\nfig = plt.figure(figsize=(5, 5))\nindexes = np.arange(len(ing_name4))\nwidth = 0.2\nplt.bar(indexes, ing_count4, width)\nplt.xticks(indexes + width * 0.5, ing_name4,rotation='vertical')\nplt.title('Most Common Ingredients Rank 4')\nplt.show()\nfig.savefig('rank4.png')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"list_ingredients = np.unique(common_ingredients.values.ravel())\n\ntrain['total_ingredients'] = train['ingredients'].map(\":\".join)\n\nfig, ax = plt.subplots(5, 5, figsize=(40, 40))\nfor ingredient, ax_idx in zip(list_ingredients, range(25)):\n    indexes = train['total_ingredients'].str.contains(ingredient)\n    ingredient_occur = (train[indexes]['cuisine'].value_counts() / train['cuisine'].value_counts())\n    ingredient_occur.plot(kind='bar', ax=ax.ravel()[ax_idx], fontsize=10, title=ingredient)\n\nfig.savefig('ingredient_occur.plot.png')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Benchmark Model"},{"metadata":{"trusted":false},"cell_type":"code","source":"from text_unidecode import unidecode\n\ndef xform_string(str_list):\n    return \", \".join([\n        unidecode(str).lower()\n        for str in str_list\n    ])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Import train_test_split\nfrom sklearn.cross_validation import train_test_split\n\ntrain_features= train.drop('cuisine', axis = 1)\ntrain_cuisine = pd.DataFrame(train['cuisine'])\n\n# Split the 'features' and 'Yummly' data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(train_features, \n                                                    train_cuisine , \n                                                    test_size = 0.2, \n                                                    random_state = 0)\n\n# Show the results of the split\nprint (\"Training set has {} samples.\".format(X_train.shape[0]))\nprint (\"Testing set has {} samples.\".format(X_test.shape[0]))\nprint (y_train.shape[0])\nprint (y_test.shape[0])\nX_train.shape\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_test.shape\n#X_test.head","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_train.shape\ny_train.head","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\n# Preprocessing\nvector = CountVectorizer(\n    preprocessor = xform_string,\n    analyzer = \"word\",\n    token_pattern = r\"(?u)\\b[a-z]{2,40}\\b\",\n    max_features = 4500\n)\n\nvector.fit(np.concatenate([X_train.ingredients, X_test.ingredients]))\n\nprint (\"Total No. of features:\", len(vector.get_feature_names()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Train\nclf_A = RandomForestClassifier(\n    n_estimators = 50,\n    oob_score = True,\n    verbose = 10,\n    n_jobs = 5\n)\n\n\n# Train\nclf_B =  DecisionTreeClassifier(random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n#RandomForest\nbenchmark_model_A = Pipeline([\n    (\"vector\", vector),\n    (\"scl\", StandardScaler(with_mean=False)),\n    (\"clf_A\", clf_A)\n])\n\nbenchmark_model_A.fit(X_train.ingredients,y_train.cuisine)\n\nprint (\"#\")\nprint (\"# Best score:\", benchmark_model_A.named_steps[\"clf_A\"].oob_score_)\nprint (\"#\")\n\n#DecisionTree\nbenchmark_model_B = Pipeline([\n    (\"vector\", vector),\n    (\"scl\", StandardScaler(with_mean=False)),\n    (\"clf_B\", clf_B)\n])\n\nbenchmark_model_B.fit(X_train.ingredients,y_train.cuisine)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Metrics\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import fbeta_score\nfrom sklearn.metrics import precision_score\n\npred_results_A = benchmark_model_A.predict(X_test.ingredients)\nprint(pred_results_A)\n\n# Train and Test Accuracy for Random Forest\nprint (\"Train Accuracy for Random Forest :: \", accuracy_score(y_train, benchmark_model_A.predict(X_train.ingredients)))\nprint (\"Test Accuracy for Random Forest  :: \", accuracy_score(y_test, pred_results_A))\n\nprint (\"F-Score on Test for Random Forest :: \",fbeta_score(y_test, pred_results_A,average=None, beta = 0.5))\n  \npred_results_B = benchmark_model_B.predict(X_test.ingredients)\nprint(pred_results_B)\n\n# Train and Test Accuracy for  Decision Tree\nprint (\"Train Accuracy for Decision Tree :: \", accuracy_score(y_train, benchmark_model_B.predict(X_train.ingredients)))\nprint (\"Test Accuracy for Decision Tree  :: \", accuracy_score(y_test, pred_results_B))\n\nprint (\"F-Score on Test for Decision Tree :: \",fbeta_score(y_test, pred_results_B,average=None, beta = 0.5))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Metrics\n\nfrom sklearn.metrics import log_loss\n\nclf_probs_A =  benchmark_model_A.predict_proba(X_test.ingredients)\nclf_probs_B =  benchmark_model_B.predict_proba(X_test.ingredients)\n\n#print(clf_probs_A)\n\nscore_A = log_loss(y_test, clf_probs_A)\nprint(\"Log Loss for Random Forest :: \",score_A)\n\nscore_B = log_loss(y_test, clf_probs_B)\nprint(\"Log Loss for Decision Tree :: \",score_B)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Kaggle submission files\n\npred_results_A = benchmark_model_A.predict(test.ingredients)\ntest['cuisine'] = pred_results_A\nprint(pred_results_A)\n\nout_file= \"../input/bench_mark_random_forest.csv\"\nop = pd.DataFrame(data={\n        \"id\": test.id,\n        \"cuisine\": test.cuisine\n        })\nop.sort_values(by=\"id\", inplace=True)\nop.to_csv(out_file, columns=[\"id\", \"cuisine\"], index=False, quoting=3)\nprint (\"Submission for bench_mark Random Forest written to\", out_file)\n\n\npred_results_B = benchmark_model_B.predict(test.ingredients)\ntest['cuisine'] = pred_results_B\nprint(pred_results_B)\n\nout_file= \"../input/bench_mark_decision_tree.csv\"\nop = pd.DataFrame(data={\n        \"id\": test.id,\n        \"cuisine\": test.cuisine\n        })\nop.sort_values(by=\"id\", inplace=True)\nop.to_csv(out_file, columns=[\"id\", \"cuisine\"], index=False, quoting=3)\nprint (\"Submission for bench_mark Decision Tree written to\", out_file)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# a function that adds new features and the dataframe\n\ndef add_features(df): \n    \n    # no of ingredients\n    df['no_ingredients'] = df.ingredients.apply(len)\n    \n    # average length of ingredient names\n    df['ingredient_len'] = df.ingredients.apply(lambda x: np.mean([len(item) for item in x]))\n    \n    # make strings of the ingredients list\n    df['ingredients_string'] = df.ingredients.astype(str)\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# create the same features in the training data and the new data\ntrain = add_features(pd.read_json('../input/train.json'))\nnew = add_features(pd.read_json('../input/test.json'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"new.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"new.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Optimize Cross Validation using Pipeline"},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# assign X and y\nX = train.ingredients_string\ny = train.cuisine","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# X is just an ingredient series\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# define the regex pattern for teh purpose of tokenization\nfrom sklearn.feature_extraction.text import CountVectorizer\nvector = CountVectorizer(token_pattern=r\"'([a-z ]+)'\")","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# import and declare the Multinomial Naive Bayes along with the default parameters\nfrom sklearn.naive_bayes import MultinomialNB\nmnb = MultinomialNB()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Assign the Naive Bayes with a pipeline of vectorization\nfrom sklearn.pipeline import make_pipeline\npipeline = make_pipeline(vector, mnb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# explore the pipeline steps\npipeline.steps","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# cross validate the full pipeline\nfrom sklearn.cross_validation import cross_val_score\ncross_val_score(pipeline, X, y, cv=7, scoring='accuracy').mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# pipeline steps are automatically assigned names by make_pipeline\npipeline.named_steps.keys()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# create a grid of parameters to search (and specify the pipeline step along with the parameter)\nparameters_grid = {}\nparameters_grid['countvectorizer__token_pattern'] = [r\"\\b\\w\\w+\\b\", r\"'([a-z ]+)'\"]\nparameters_grid['multinomialnb__alpha'] = [0.5, 1]\nparameters_grid","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# pass the pipeline (instead of the model) to GridSearchCV\nfrom sklearn.grid_search import GridSearchCV\ngridCV = GridSearchCV(pipeline, parameters_grid , cv=7, scoring='accuracy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# time the grid search\n%time gridCV.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# examine the score for each combination of parameters\ngridCV.grid_scores_","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# print the single best score and parameters that produced that score\nprint(gridCV.best_score_)\nprint(gridCV.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"from sklearn.grid_search import RandomizedSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# for the continuous parameters, distribution is always prefeered when compared to a list of options\nimport scipy as sp\nparameters_grid = {}\nparameters_grid['countvectorizer__token_pattern'] = [r\"\\b\\w\\w+\\b\", r\"'([a-z ]+)'\"]\nparameters_grid['countvectorizer__min_df'] = [1, 2, 3]\nparameters_grid['multinomialnb__alpha'] = sp.stats.uniform(scale=1)\nparameters_grid","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# define a random seed\nnp.random.seed(1)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# additional parameters are achieved thru number of searches (n_tier) and random_state\nrdm = RandomizedSearchCV(pipeline, parameters_grid, cv=5, scoring='accuracy', n_iter=5, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# time the randomized search\n%time rdm.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rdm.grid_scores_","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(rdm.best_score_)\nprint(rdm.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Making predictions for test data"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Assign X_new as the ingredients string\nX_new = new.ingredients_string\nX_new","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# what is the best model identified by RandomizedSearchCV\nrdm.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# RandomizedSearchCV/GridSearchCV now refits the best model and ready to make predictions for all the dataset\nnew_pred_class_rdm = rdm.predict(X_new)\nnew_pred_class_rdm","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# train_test_split\n\ntrain_features_new= train.drop('cuisine', axis = 1)\ntrain_cuisine_new = pd.DataFrame(train['cuisine'])\n\n# Split the 'features' and 'Yummly' data into training and testing sets\nX_train_new, X_test_new, y_train_new, y_test_new = train_test_split(train_features_new, \n                                                    train_cuisine_new , \n                                                    test_size = 0.2, \n                                                    random_state = 0)\n\n# Show the results of the split\nprint (\"Training set has {} samples.\".format(X_train_new.shape[0]))\nprint (\"Testing set has {} samples.\".format(X_test_new.shape[0]))\nprint (y_train_new.shape[0])\nprint (y_test_new.shape[0])\nX_train_new.shape\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_test_new.ingredients_string","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Metrics\n\n#test['cuisine'] = new_pred_class_rdm\npred_results_mnb = rdm.predict(X_test_new.ingredients_string)\n\n# Test Accuracy for Naive Bayes\nprint (\"Test Accuracy for Naive Bayes  :: \", accuracy_score(y_test, pred_results_mnb))\n\nprint (\"F-Score on Test for Naive Bayes :: \",fbeta_score(y_test, pred_results_mnb,average=None, beta = 0.5))\n\nclf_probs_mnb =   rdm.predict_proba(X_test_new.ingredients_string)\nprint(clf_probs_mnb)\n\nscore_mnb = log_loss(y_test, clf_probs_mnb)\nprint(\"Log Loss for Naive Bayes :: \",score_mnb)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# create a submission file (score: 0.75341)\npd.DataFrame({'id':new.id, 'cuisine':new_pred_class_rdm}).set_index('id').to_csv('../input/actual1_naive_bayes.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# create a document term matrix using the entire training data\nX_dtm = vector.fit_transform(X)\nX_dtm.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"type(X_dtm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# DF of the custom created features\nX_custom = train.loc[:, ['no_ingredients', 'ingredient_len']]\nX_custom.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# a sparse matrix from the above DF\nX_custom_sparse = sp.sparse.csr_matrix(X_custom)\ntype(X_custom_sparse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# combine the two sparse matrices\nX_dtm_custom = sp.sparse.hstack([X_dtm, X_custom_sparse])\nX_dtm_custom.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Converting a function into a transformer"},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# Create a function that takes a DataFrame & returns the custom created features\ndef get_custom(df):\n    return df.loc[:, ['no_ingredients', 'ingredient_len']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"get_custom(train).head()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"from sklearn.preprocessing import FunctionTransformer","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# create a stateless transformer from the get_custom function\nget_custom_ft = FunctionTransformer(get_custom, validate=False)\ntype(get_custom_ft)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# execute the function using the transform method\nget_custom_ft.transform(train).head()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# create a function that takes DF and returns the ingredients string\ndef get_txt(df):\n    return df.ingredients_string","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# create and test another transformer\nget_txt_ft = FunctionTransformer(get_txt, validate=False)\nget_txt_ft.transform(train).head()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"from sklearn.pipeline import make_union","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# create a document term matrix using the entire training data\nX_dtm = vector.fit_transform(X)\nX_dtm.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Replicate it as a FeatureUnion by  using transformer\nf_union = make_union(vector)\nX_dtm = f_union.fit_transform(X)\nX_dtm.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# properly combine the transformers into a FeatureUnion\nf_union = make_union(make_pipeline(get_txt_ft, vector), get_custom_ft)\nX_dtm_custom = f_union.fit_transform(train)\nX_dtm_custom.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cross Validation"},{"metadata":{"trusted":false},"cell_type":"code","source":"# is this proper cross validation?\ncross_val_score(mnb, X_dtm_custom, y, cv=5, scoring='accuracy').mean()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# define a pipeline of the FeatureUnion and Naive Bayes\npipeline = make_pipeline(f_union, mnb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# do proper cross validate the entire pipeline and pass it the DF\ncross_val_score(pipeline, train, y, cv=5, scoring='accuracy').mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Alternative way to specify `Pipeline` and `FeatureUnion`"},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# quick rewind to the pipeline I did earlier\nf_union = make_union(make_pipeline(get_txt_ft, vector), get_custom_ft)\npipeline = make_pipeline(f_union, mnb)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# repicate the pipeline without using the make_union or make_pipeline\nfrom sklearn.pipeline import Pipeline, FeatureUnion\npipeline = Pipeline([\n    ('featureunion', FeatureUnion([\n            ('pipeline', Pipeline([\n                    ('functiontransformer', get_txt_ft),\n                    ('countvectorizer', vector)\n                    ])),\n            ('functiontransformer', get_custom_ft)\n        ])),\n    ('multinomialnb', mnb)\n])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  The Nested Pipeline's Grid search "},{"metadata":{"trusted":false},"cell_type":"code","source":"# explore the pipeline steps\npipeline.steps","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# define a grid of parameters to search & create the pipeline steps along with the parameters\n\nparameters_grid = {}\nparameters_grid['featureunion__pipeline__countvectorizer__token_pattern'] = [r\"\\b\\w\\w+\\b\", r\"'([a-z ]+)'\"]\nparameters_grid['multinomialnb__alpha'] = [0.5, 1]\nparameters_grid","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"gridCV = GridSearchCV(pipeline, parameters_grid, cv=5, scoring='accuracy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"%time gridCV.fit(train, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(gridCV.best_score_)\nprint(gridCV.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 1: KNN model using only custom features"},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# define X and y\nfeature_columns = ['no_ingredients', 'ingredient_len']\nX = train[feature_columns]\ny = train.cuisine","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# use KNN with K=800\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=800)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# train KNN on all of the training data\nknn.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# create X_new as the custom created features\nX_new = new[feature_columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# find predicted probabilities for the new data\nnew_pred_proba_knn = knn.predict_proba(X_new)\nnew_pred_proba_knn.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# display the sample of predicted probabilities\nnew_pred_proba_knn[0, :]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# model classes\nzip(knn.classes_, new_pred_proba_knn[0, :])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 2: Naive Bayes model using default features"},{"metadata":{"trusted":false},"cell_type":"code","source":"# the best model earlier found by RandomizedSearchCV\nrdm.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# X_new as the ingredients string\nX_new = new.ingredients_string","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# calculate predicted probabilities of class membership for the new data\nnew_pred_proba_rdm = rdm.predict_proba(X_new)\nnew_pred_proba_rdm.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# sampel of predicted probabilities\nnew_pred_proba_rdm[0, :]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ensembling models 1 and 2"},{"metadata":{"trusted":false},"cell_type":"code","source":"# calculate the mean of the predicted probabilities for all rows\nnew_pred_proba = pd.DataFrame((new_pred_proba_knn + new_pred_proba_rdm) / 2, columns=knn.classes_)\nnew_pred_proba.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# find the field with the highest predicted probability\nnew_pred_proba_class = new_pred_proba.apply(np.argmax, axis=1)\nnew_pred_proba_class.head()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# create a submission file\npd.DataFrame({'id':new.id, 'cuisine':new_pred_proba_class}).set_index('id').to_csv('../input/actual2_ensembled_models.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 2","language":"python","name":"python2"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.13"}},"nbformat":4,"nbformat_minor":1}