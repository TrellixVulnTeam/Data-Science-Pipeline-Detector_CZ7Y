{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d6218f238392dc4f954476bcd262274afed2c6af"},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n# data preprocessing\ntrain_data = pd.read_json('../input/train.json')\ntest_data = pd.read_json('../input/test.json')\n\ntrain_data['ingredients'] = train_data['ingredients'].apply(lambda list: ','.join(list).lower())\ntest_data['ingredients'] = test_data['ingredients'].apply(lambda list: ','.join(list).lower())\n\nvectorizer = TfidfVectorizer(binary = True)\ntrain_X = vectorizer.fit_transform(train_data['ingredients'])\ntest_X = vectorizer.transform(test_data['ingredients'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6a161f9079681d17c904c55d390db5fc7f6a4a2f"},"cell_type":"code","source":"idxtuple = train_X.nonzero()\nfor i in range(16):\n    row = idxtuple[0][i]\n    col = idxtuple[1][i]\n    print('Recipe {0}: {1} = {2}'.format(row, vectorizer.get_feature_names()[col], train_X[row, col]))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Naive Bayes, score = 0.68634\n'''\nfrom sklearn.naive_bayes import MultinomialNB\n\nclf = MultinomialNB().fit(train_X, train_data['cuisine'])\n\ntest_pred = clf.predict(test_X)\nprint(test_pred)\nanswer = pd.DataFrame({'id': test_data['id'], 'cuisine': test_pred})\nanswer.to_csv('answer.csv', index = False)\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d44b74f4d9e9bdb5ed678216fc26bf16c2d13eed"},"cell_type":"code","source":"# SVC with panelty = 100, socre = 0.71721, but take too long time\n'''\nfrom sklearn.svm import SVC\n\nclf = SVC(C = 100).fit(train_X, train_data['cuisine'])\n\ntest_pred = clf.predict(test_X)\nprint(test_pred)\nanswer = pd.DataFrame({'id': test_data['id'], 'cuisine': test_pred})\nanswer.to_csv('answer.csv', index = False)\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"16b9e6b8b4d64e17e184e628b95f765a22c90134"},"cell_type":"code","source":"# LinearSVC with OvR classifier, score = 0.78761 and very fast\n'''\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import LinearSVC\nclf = OneVsRestClassifier(LinearSVC(random_state=0)).fit(train_X, train_data['cuisine'])\n\ntest_pred = clf.predict(test_X)\nprint(test_pred)\nanswer = pd.DataFrame({'id': test_data['id'], 'cuisine': test_pred})\nanswer.to_csv('answer.csv', index = False)\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b9d1e355e610a2938ce597aaaac6219678866c7c"},"cell_type":"code","source":"# LinearSVC with OvO classifier, score = 0.78821\n'''\nfrom sklearn.multiclass import OneVsOneClassifier\nfrom sklearn.svm import LinearSVC\nclf = OneVsOneClassifier(LinearSVC(random_state=0)).fit(train_X, train_data['cuisine'])\n\ntest_pred = clf.predict(test_X)\nprint(test_pred)\nanswer = pd.DataFrame({'id': test_data['id'], 'cuisine': test_pred})\nanswer.to_csv('answer.csv', index = False)\n'''","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a1d68cf81eae9731902c9eb5a892398ca81be53f"},"cell_type":"markdown","source":"以下的code是根據我的猜測實作的，看起來不像machine learning，而是單純透過統計學和邏輯推論組成的答案。我的想法如下：\n\n* 首先製作一個二維矩陣，column是不同的料理種類，row是不同的食材，以此矩陣統計各類食材在不同的料理當中被使用的頻率\n* 欲得出一種食材在某料理種類當中的特殊程度，可以透過分析該食材在不同料理當中出現的次數，如果在某料理當中出現該食材的次數特別多，表示這個食材很可能是該料理中的特色食材\n* 但是有些食材無論在哪種料理當中都很常出現，例如鹽巴(salt)、糖(sugar)、油(oil)等，這些食材對於一道菜是屬於什麼料理是幾乎沒有影響的，所以其實他們不是那麼重要\n* 相對的，如果資料集當中某種料理的數量特別多，那麼任何一種食材出現在該料理當中的機率都會相對於出現在其它料理當中的機率還要高，所以在比較同一樣食材在不同料理當中出現的頻率之前，必須先針對每一種料理下不同食材的出現次數做正規化（將次數轉換為百分比），這樣才可以在公平的基準點下比較\n* 在做完正規化之後，就可以比較某一樣食材在不同的料理間使用的狀況。這裡我希望將一樣食材用一個單位向量表示，這個向量的長度與料理的種類數量一樣（20維，因為有20種料理），每一個維度的值代表「此食材有多麼的OO料理」。比如味噌在日本料理當中大約有10%的比例被使用，但是在其它料理當中都是0%，所以這就是一個平行日本料理那個方向的單位向量，表示這個食材很日本料理。又比如鹽巴無論在那一種料理當中，被使用的頻率大約都是50%，那鹽巴就是一個指向各個維度正向的單位向量，無論在哪個維度上的投影都差不多長，也就是他沒有特別代表什麼料理。\n\n-> 這其實就是Tfidf的概念（。\n\n* 在產生出所有食材的向量之後，每一個食譜都可以視為是一群向量的疊加，所以我就把一個食譜當中有用到的食材的向量全部加總，得到一個很大的向量，用這個向量代表這個食譜。最後再尋找這個食譜在哪個維度的值是最大的，則這個食譜很有可能就是那個維度所對應的料理種類\n* 如果在testing data上遭遇了沒有見過的食材，則先假設這個食材的向量為0向量，即不考慮這個沒見過的東西對料理種類的影響，先以既有的知識去猜測。在predict完第一輪之後就會有一個粗略的結果，再根據這個結果去對沒有見過的食材做統計，並且分析他是屬於哪一種料理，再用這個結果去predict第二次。有點類似semi-supervised learning的感覺。\n"},{"metadata":{"trusted":true,"_uuid":"12d41ed7fe8f2f984666b180896c73cb653cf231"},"cell_type":"code","source":"# My toy model(?)\ntrain_data = pd.read_json('../input/train.json')\ntest_data = pd.read_json('../input/test.json')\n\ncuisine = train_data['cuisine'].value_counts()\ningredients = {}\nfor idx, row in train_data.iterrows():\n    for i in range(len(row['ingredients'])):\n        ingr = row['ingredients'][i].lower()\n        if row['cuisine'] in ingredients:\n            if ingr in ingredients[row['cuisine']]:\n                ingredients[row['cuisine']][ingr] += 1\n            else:\n                ingredients[row['cuisine']][ingr] = 1\n        else:\n            ingredients[row['cuisine']] = {}\n            ingredients[row['cuisine']][ingr] = 1\n\ningredients = pd.DataFrame(ingredients)\ningredients = ingredients.fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6c015c6a213d77cbc0461a81d31600c3d5ced960"},"cell_type":"code","source":"# term frequency\ningredients = ingredients.apply(lambda x: x.apply(lambda y: y / cuisine[x.name]))\n\n# unit-vectorize\ndef unit_vec(vec):\n    len = (vec.apply(lambda x: x * x).sum()) ** 0.5\n    return vec.apply(lambda x: x / len)\ningredients = ingredients.apply(unit_vec, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1aeb26715dca683eda9d33956851bafb7abb3103"},"cell_type":"code","source":"# get recipe vector on training data\ntrain_data_vector = []\nfor idx, row in train_data.iterrows():\n    train_data_vector.append({\n        'id': row['id'],\n        'cuisine': row['cuisine'],\n        'cuisine_vector': ingredients.loc[row['ingredients']].sum()\n    })\n\n# Evaluate\ntotal = len(train_data_vector)\ncorrect = 0\nfor i in range(total):\n    if train_data_vector[i]['cuisine_vector'].idxmax() == train_data_vector[i]['cuisine']:\n        correct += 1\n\nprint('Result on training data')\nprint(f'Total: {total}, Correct: {correct}, Accuracy: {correct / total}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1a7c0f06bdeef11cfabb44f6d1f71d122ec3bf10"},"cell_type":"code","source":"# get recipe vector on testing data and predict(?)\ntesting_data_vector = []\nfor idx, row in test_data.iterrows():\n    ingredients_known = [igdt for igdt in row['ingredients'] if igdt in ingredients.index]\n    vector = ingredients.loc[ingredients_known].sum()\n    cuisine = vector.idxmax()\n    testing_data_vector.append({\n        'id': row['id'],\n        'cuisine_vector': vector,\n        'cuisine': cuisine\n    })\nanswer = pd.DataFrame(testing_data_vector)[['id','cuisine']]\nanswer.to_csv('answer.csv', index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}