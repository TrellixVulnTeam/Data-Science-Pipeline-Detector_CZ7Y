{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nimport sys\nfrom importlib import reload\n\nreload(sys)\n#sys.setdefaultencoding('utf-8')\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout\nfrom nltk.stem.wordnet import WordNetLemmatizer\nimport re\nimport itertools\nimport os.path\nimport json\nfrom datetime import datetime\n\ndef k_to_one_hot(k_hot_vector):\n    # This function converts k-hot target vector to one-hot target matrix\n    \n    classes = np.unique(k_hot_vector)\n    one_hot_matrix = []\n    \n    for i in np.arange(len(classes)):\n        row = (k_hot_vector == classes[i]).astype(int, copy = False)\n        if len(one_hot_matrix) == 0:\n            one_hot_matrix = row\n        else:\n            one_hot_matrix = np.vstack((one_hot_matrix, row))\n            \n    return classes, one_hot_matrix.conj().transpose()\n    \n    \ndef read_data(filename):\n    # read data into lists\n    \n    with open(filename) as data_file:    \n        data = json.load(data_file)\n        \n    ids, cuisines, ingredients = [], [], []\n    if 'cuisine' in data[0].keys():\n        for i in range(len(data)):\n            ids.append(data[i]['id'])\n            cuisines.append(data[i]['cuisine'])\n            ingredients.append(data[i]['ingredients'])\n    else:\n        for i in range(len(data)):\n            ids.append(data[i]['id'])\n            ingredients.append(data[i]['ingredients'])    \n                \n    return ids, cuisines, ingredients\n    \n    \ndef create_submission(test_ids, guess):\n    # create submission in proper format\n    \n    sub = np.transpose(np.vstack((test_ids, guess)))\n    sub = np.vstack((['id', 'cuisine'], sub))\n    sub_file_name = 'submission_' + str(datetime.now())[0:16] +'.csv'\n    sub_file_name = sub_file_name.replace(' ', '_')\n    sub_file_name = sub_file_name.replace(':', '-')\n    np.savetxt(sub_file_name, sub, delimiter=\",\", fmt=\"%s\")\n    \n    return None  \n    \n\ndef remove_numbers(ing):\n    # remove numbers from ingredients\n    \n    return [[re.sub(\"\\d+\", \"\", x) for x in y] for y in ing]\n\n    \ndef remove_special_chars(ing):\n    # remove certain special characters from ingredients\n   \n    ing = [[x.replace(\"-\", \" \") for x in y] for y in ing] \n    ing = [[x.replace(\"&\", \" \") for x in y] for y in ing] \n    ing = [[x.replace(\"'\", \" \") for x in y] for y in ing] \n    ing = [[x.replace(\"''\", \" \") for x in y] for y in ing] \n    ing = [[x.replace(\"%\", \" \") for x in y] for y in ing] \n    ing = [[x.replace(\"!\", \" \") for x in y] for y in ing] \n    ing = [[x.replace(\"(\", \" \") for x in y] for y in ing] \n    ing = [[x.replace(\")\", \" \") for x in y] for y in ing] \n    ing = [[x.replace(\"/\", \" \") for x in y] for y in ing] \n    ing = [[x.replace(\"/\", \" \") for x in y] for y in ing] \n    ing = [[x.replace(\",\", \" \") for x in y] for y in ing] \n    ing = [[x.replace(\".\", \" \") for x in y] for y in ing] \n    ing = [[x.replace(u\"\\u2122\", \" \") for x in y] for y in ing] \n    ing = [[x.replace(u\"\\u00AE\", \" \") for x in y] for y in ing] \n    ing = [[x.replace(u\"\\u2019\", \" \") for x in y] for y in ing] \n\n    return ing\n    \n    \ndef make_lowercase(ing):\n    # make all letters lowercase for all ingredients\n    \n    return [[x.lower() for x in y] for y in ing]\n    \n    \ndef remove_extra_whitespace(ing):\n    # removes extra whitespaces\n    \n    return [[re.sub( '\\s+', ' ', x).strip() for x in y] for y in ing] \n    \n    \ndef stem_words(ing):\n    # word stemming for ingredients\n    \n    lmtzr = WordNetLemmatizer()\n    \n    def word_by_word(strng):\n        \n        return \" \".join([\"\".join(lmtzr.lemmatize(w)) for w in strng.split()])\n    \n    return [[word_by_word(x) for x in y] for y in ing] \n    \n    \ndef remove_units(ing):\n    # remove certain words from ingredients\n    \n    remove_list = ['g', 'lb', 's', 'n']\n        \n    def check_word(strng):\n        \n        s = strng.split()\n        resw  = [word for word in s if word.lower() not in remove_list]\n        \n        return ' '.join(resw)\n\n    return [[check_word(x) for x in y] for y in ing] \n    \n\ndef extract_feats(ingredients, uniques):\n    # each ingredient + each word as feature\n    \n    feats_whole = np.zeros((len(ingredients), len(uniques)))\n    for i in range(len(ingredients)):\n        for j in ingredients[i]:\n            feats_whole[i, uniques.index(j)] = 1\n            \n    new_uniques = []\n    for m in uniques:\n        new_uniques.append(m.split())\n    new_uniques = list(set(list(itertools.chain.from_iterable(new_uniques))))\n    \n    feats_each = np.zeros((len(ingredients), len(new_uniques))).astype(np.uint8)\n    for i in range(len(ingredients)):\n        for j in ingredients[i]:\n            for k in j.split():\n                feats_each[i, new_uniques.index(k)] = 1\n            \n    return np.hstack((feats_whole, feats_each)).astype(bool)\n    \n    \ndef load_model():\n    # load neural net model architectiure\n    \n    mdl = Sequential()\n    mdl.add(Dense(512, init='glorot_uniform', activation='relu', \n                                        input_shape=(train_feats.shape[1],)))\n    mdl.add(Dropout(0.5))\n    mdl.add(Dense(128, init='glorot_uniform', activation='relu'))\n    mdl.add(Dropout(0.5))\n    mdl.add(Dense(20, activation='softmax'))\n    mdl.compile(loss='categorical_crossentropy', optimizer='adagrad', metrics=['accuracy'])\n    \n    return mdl    \n\n    \nif __name__ == '__main__':\n    \n    # preprocess training set\n    print(\"\\nPreprocessing...\\n\")  \n    train_ids, train_cuisines, train_ingredients = read_data(\"../input/train.json\")\n    train_ingredients = make_lowercase(train_ingredients)\n    train_ingredients = remove_numbers(train_ingredients)\n    train_ingredients = remove_special_chars(train_ingredients)\n    train_ingredients = remove_extra_whitespace(train_ingredients)\n    train_ingredients = remove_units(train_ingredients)\n    train_ingredients = stem_words(train_ingredients)\n    \n    # preprocess test set\n    test_ids, test_cuisines, test_ingredients = read_data(\"../input/test.json\")\n    test_ingredients = make_lowercase(test_ingredients)\n    test_ingredients = remove_numbers(test_ingredients)\n    test_ingredients = remove_special_chars(test_ingredients)\n    test_ingredients = remove_extra_whitespace(test_ingredients)\n    test_ingredients = remove_units(test_ingredients)\n    test_ingredients = stem_words(test_ingredients)\n    \n    # encode   \n    print(\"Encoding...\\n\")  \n    le = LabelEncoder()\n    targets = le.fit_transform(train_cuisines)\n    classes, targets = k_to_one_hot(targets)\n    \n    # extract features\n    print(\"Feature extraction...\\n\") \n    uniques = list(set([item for sublist in train_ingredients + test_ingredients for item in sublist]))\n    train_feats = extract_feats(train_ingredients, uniques)\n    test_feats = extract_feats(test_ingredients, uniques)\n  \n    # train\n    n_ensemble = 8\n    for ens in range(n_ensemble):\n        print(\"\\n\\tTraining...\", ens)\n        model = load_model()\n        \n        # if model already exists, continue training\n        model_name = 'model' + str(ens) + '.hdf5'\n        if os.path.isfile(model_name):\n            model.load_weights(model_name)\n            \n        model.fit(train_feats, targets, epochs=1000, batch_size=4096)\n        model.save_weights(model_name, overwrite=True)\n\n    # create submission out of the ensemble\n    preds = []\n    for ens in range(n_ensemble):\n        print(\"\\nSubmission\", ens)\n        model = load_model()\n\n        model_name = 'model' + str(ens) + '.hdf5'\n        model.load_weights(model_name)            \n        preds.append(model.predict_proba(test_feats))\n\n    # final cuisine decision: argmax of sum of log probabilities  \n    print(\"\\nPredicting...\")      \n    preds = sum(np.log(preds))\n    guess = le.inverse_transform(np.argmax(preds, axis=1))\n    create_submission(test_ids, guess)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\n'''import os\nprint(os.listdir(\"../input\"))'''\n\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"},{"output_type":"stream","text":"\nPreprocessing...\n\nEncoding...\n\nFeature extraction...\n\n\n\tTraining... 0\nWARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\nWARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:166: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(512, activation=\"relu\", input_shape=(9980,), kernel_initializer=\"glorot_uniform\")`\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:168: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(128, activation=\"relu\", kernel_initializer=\"glorot_uniform\")`\n","name":"stderr"},{"output_type":"stream","text":"WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\nEpoch 1/1000\n39774/39774 [==============================] - 6s 155us/step - loss: 1.8721 - acc: 0.4662\nEpoch 2/1000\n39774/39774 [==============================] - 4s 89us/step - loss: 1.0306 - acc: 0.7106\nEpoch 3/1000\n39774/39774 [==============================] - 4s 89us/step - loss: 0.8070 - acc: 0.7750\nEpoch 4/1000\n39774/39774 [==============================] - 4s 89us/step - loss: 0.7108 - acc: 0.8009\nEpoch 5/1000\n39774/39774 [==============================] - 4s 88us/step - loss: 0.6360 - acc: 0.8183\nEpoch 6/1000\n39774/39774 [==============================] - 4s 89us/step - loss: 0.5852 - acc: 0.8323\nEpoch 7/1000\n39774/39774 [==============================] - 4s 94us/step - loss: 0.5460 - acc: 0.8450\nEpoch 8/1000\n39774/39774 [==============================] - 4s 95us/step - loss: 0.5126 - acc: 0.8521\nEpoch 9/1000\n39774/39774 [==============================] - 4s 94us/step - loss: 0.4842 - acc: 0.8616\nEpoch 10/1000\n39774/39774 [==============================] - 4s 88us/step - loss: 0.4600 - acc: 0.8691\nEpoch 11/1000\n39774/39774 [==============================] - 4s 88us/step - loss: 0.4337 - acc: 0.8746\nEpoch 12/1000\n39774/39774 [==============================] - 4s 89us/step - loss: 0.4142 - acc: 0.8795\nEpoch 13/1000\n39774/39774 [==============================] - 4s 88us/step - loss: 0.3971 - acc: 0.8841\nEpoch 14/1000\n39774/39774 [==============================] - 4s 88us/step - loss: 0.3821 - acc: 0.8894\nEpoch 15/1000\n39774/39774 [==============================] - 4s 88us/step - loss: 0.3664 - acc: 0.8939\nEpoch 16/1000\n39774/39774 [==============================] - 4s 88us/step - loss: 0.3532 - acc: 0.8966\nEpoch 17/1000\n39774/39774 [==============================] - 4s 88us/step - loss: 0.3407 - acc: 0.9019\nEpoch 18/1000\n39774/39774 [==============================] - 4s 88us/step - loss: 0.3266 - acc: 0.9040\nEpoch 19/1000\n39774/39774 [==============================] - 4s 88us/step - loss: 0.3171 - acc: 0.9078\nEpoch 20/1000\n39774/39774 [==============================] - 4s 89us/step - loss: 0.3029 - acc: 0.9110\nEpoch 21/1000\n39774/39774 [==============================] - 4s 88us/step - loss: 0.2943 - acc: 0.9162\nEpoch 22/1000\n39774/39774 [==============================] - 4s 88us/step - loss: 0.2842 - acc: 0.9187\nEpoch 23/1000\n39774/39774 [==============================] - 4s 89us/step - loss: 0.2797 - acc: 0.9191\nEpoch 24/1000\n39774/39774 [==============================] - 4s 88us/step - loss: 0.2684 - acc: 0.9219\nEpoch 25/1000\n39774/39774 [==============================] - 4s 88us/step - loss: 0.2608 - acc: 0.9242\nEpoch 26/1000\n39774/39774 [==============================] - 4s 88us/step - loss: 0.2516 - acc: 0.9281\nEpoch 27/1000\n39774/39774 [==============================] - 4s 88us/step - loss: 0.2489 - acc: 0.9280\nEpoch 28/1000\n39774/39774 [==============================] - 4s 88us/step - loss: 0.2409 - acc: 0.9304\nEpoch 29/1000\n39774/39774 [==============================] - 4s 89us/step - loss: 0.2325 - acc: 0.9317\nEpoch 30/1000\n39774/39774 [==============================] - 4s 95us/step - loss: 0.2267 - acc: 0.9345\nEpoch 31/1000\n39774/39774 [==============================] - 4s 95us/step - loss: 0.2224 - acc: 0.9356\nEpoch 32/1000\n39774/39774 [==============================] - 4s 92us/step - loss: 0.2153 - acc: 0.9377\nEpoch 33/1000\n39774/39774 [==============================] - 3s 88us/step - loss: 0.2098 - acc: 0.9394\nEpoch 34/1000\n39774/39774 [==============================] - 4s 88us/step - loss: 0.2048 - acc: 0.9408\nEpoch 35/1000\n39774/39774 [==============================] - 4s 89us/step - loss: 0.2016 - acc: 0.9418\nEpoch 36/1000\n39774/39774 [==============================] - 4s 88us/step - loss: 0.1953 - acc: 0.9446\nEpoch 37/1000\n39774/39774 [==============================] - 4s 88us/step - loss: 0.1903 - acc: 0.9442\nEpoch 38/1000\n39774/39774 [==============================] - 4s 88us/step - loss: 0.1848 - acc: 0.9462\nEpoch 39/1000\n39774/39774 [==============================] - 4s 88us/step - loss: 0.1819 - acc: 0.9473\nEpoch 40/1000\n39774/39774 [==============================] - 4s 88us/step - loss: 0.1784 - acc: 0.9474\nEpoch 41/1000\n39774/39774 [==============================] - 4s 88us/step - loss: 0.1747 - acc: 0.9509\nEpoch 42/1000\n39774/39774 [==============================] - 3s 88us/step - loss: 0.1704 - acc: 0.9520\nEpoch 43/1000\n39774/39774 [==============================] - 4s 88us/step - loss: 0.1697 - acc: 0.9517\nEpoch 44/1000\n32768/39774 [=======================>......] - ETA: 0s - loss: 0.1619 - acc: 0.9535","name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-6d174e4b3c00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4096\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}