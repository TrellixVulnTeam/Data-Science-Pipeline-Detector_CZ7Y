{"cells":[{"metadata":{"collapsed":true,"scrolled":false,"_uuid":"8cb1d106401b6541651465315d61734f4eefc19e"},"cell_type":"markdown","source":"# Kaggle competition - What's cooking\n### Classification problem to fit cuisine with recipe list of ingredients\n### Goal\n1. We aim to maximize the f1 score.\n     - First, create a metrix using tfidfVectorizer,\n     - Second, modeling using PCA\n2. Implement simple bot using word2vec."},{"metadata":{"trusted":true,"_uuid":"eab20d113b6483c3956c3dcf8db568349d4d4838"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n% matplotlib inline\nimport matplotlib.pyplot as plt\n\nfrom collections import Counter\nfrom pprint import pprint\n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_row', None)\n\nimport re\nfrom nltk.stem import WordNetLemmatizer\n\n# Grid search for optimal parameters of the model\nfrom sklearn.model_selection import GridSearchCV\n\n# Model modules\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nimport xgboost as xgb\nfrom sklearn.neural_network import MLPClassifier\n\n# modules for # estimate\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import cross_validation\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import classification_report\n\n# modules for encoding features\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import LabelEncoder\n\n# Modules for dividing a data set\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\n\nfrom gensim.models import word2vec","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b6d92c41ed314b95c7f28b3f8f5a54e3a7e7f0ad"},"cell_type":"markdown","source":"## I. DATA LOAD"},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"5acda37cb1a30a06fa54efc1333da6c03f1c19e7"},"cell_type":"code","source":"train = pd.read_json('../input/train.json', encoding = 'UTF-8')\ntest = pd.read_json('../input/test.json', encoding = 'UTF-8')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5a87514e29554588adfc1fd405ad945fb61ce99b"},"cell_type":"markdown","source":"COPY"},{"metadata":{"trusted":true,"_uuid":"404474d09c19866b4fa1bf565276da954906f378"},"cell_type":"code","source":"df = train.copy()\n\nprint(df.shape)\ndf.head(2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"96d1e0830270abbc6283dc55ed9ce9541ef82cdc"},"cell_type":"markdown","source":"## II. Data search (Before Pre-processing)"},{"metadata":{"_uuid":"afbe2f71511de78e22c8a7eacd11580bf0bffba9"},"cell_type":"markdown","source":"#### 1) Count"},{"metadata":{"trusted":true,"_uuid":"d06bc91ab1fc10f692d45aad1b6cc1bef8991fb8"},"cell_type":"code","source":"df.count()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"37103793094f10bfb1acbdf555ce97a2ea2785a3"},"cell_type":"markdown","source":"#### 2) Number of NaNs for each column"},{"metadata":{"trusted":true,"_uuid":"a760910b0ba2c5d31f5cb42c3892217af2bba892"},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3ceb3a211dda97380c63724b2df01f165517d07a"},"cell_type":"markdown","source":"#### 3) Cuisine Types"},{"metadata":{"trusted":true,"_uuid":"047ec56944dfaa267062e08dedb66d1394868632"},"cell_type":"code","source":"print('Cuisine is {}.'.format(len(df.cuisine.value_counts())))\ndf.cuisine.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"01e9cbec4872a825d566017d3d79ce76deb90144"},"cell_type":"code","source":"# cuisine type visualization\n\nplt.style.use('ggplot')\ndf.cuisine.value_counts().plot(kind = 'bar',\n                              title='Cuisine Types',\n                              figsize=(20,5),\n                              legend=True,\n                              fontsize=12)\nplt.ylabel(\"Number of Recipes\", fontsize=12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f93395c491c96d5fbe0cc1aaf5962ae8b545ad9e"},"cell_type":"markdown","source":"#### 4) Ingredients Types"},{"metadata":{"trusted":true,"_uuid":"6e07a10246c0e8ca1e0f9ff2ac9df5588c6e4f13","collapsed":true},"cell_type":"code","source":"### Work to count ingredients per recipe per row\n\n# Here, the ingredient will be counted one by one.\nbag_of_ingredients = [Counter (ingredient) for ingredient in df.ingredients]\n\n# Number of each ingredient by type\nsum_of_ingredients = sum (bag_of_ingredients, Counter ())\n\n########################################################################################\n\n### Work to put sum_of_ingredients in dataframe\n\n# dict -> list -> dataframe\nsum_of_ingredients_dict = dict (sum_of_ingredients)\nsum_of_ingredients_list = list (sum_of_ingredients_dict.items ())\n\ningredients_df = pd.DataFrame (sum_of_ingredients_list)\ningredients_df.columns = ['ingredient', 'count']\ningredients_df.tail (2)\n\nprint ('Before the preprocessing, the total ingredients are {}.'. format (len (ingredients_df)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e6bb35865621f459f0bdc1fdd47b30356fb2679d","collapsed":true},"cell_type":"code","source":"ingredients_df.head(20)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b79211f7d804f15a62e32dec4b2f884e3d3b7e6a"},"cell_type":"markdown","source":"## III. Pre-processing"},{"metadata":{"_uuid":"5522ccc4727d248baef85312774b30135580066e"},"cell_type":"markdown","source":"1. Lowercase upper case\n2. Removal of product name: Oscar Mayer Deli Fresh Smoked, '®, ™'\n3. Material shine & remove material condition: chopped, ground, fresh, powdered, sharp, crushed, grilled, roasted, sliced, cooked, shredded, cracked, minced, finely ...\n4. What is behind the ',' (comma) is thought to be unnecessary because it is a method of cleaning the material.\n5. Remove digits\n6. Remove special characters:%, -, (,), '.', Oz. , ....\n7. Use lemmatize to change the word to a circle"},{"metadata":{"trusted":true,"_uuid":"5c019b669923539cf69ba2a58e022d1669e094c9","collapsed":true},"cell_type":"code","source":"def pre_processing_(recipe):\n    \n    wnl = WordNetLemmatizer()\n    \n    # 1. lower 함수를 이용하여 대문자를 소문자로 변경\n    recipe = [str.lower(ingredient) for ingredient in recipe]\n    recipe = [delete_brand_(ingredient) for ingredient in recipe]\n    recipe = [delete_state_(ingredient) for ingredient in recipe]\n    recipe = [delete_comma_(ingredient) for ingredient in recipe]\n    recipe = [original_(ingredient) for ingredient in recipe]\n    recipe = [delete_space_(ingredient) for ingredient in recipe]\n\n    return recipe\n\n# 2. 상품명을 제거하는 함수\ndef delete_brand_(ingredient):\n\n    # '®'이 있는 브랜드\n    ingredient = re.sub(\"country crock|i can't believe it's not butter!|bertolli|oreo|hellmann's\"\n                        , '', ingredient)\n    ingredient = re.sub(\"red gold|hidden valley|original ranch|frank's|redhot|lipton\", '', ingredient)\n    ingredient = re.sub(\"recipe secrets|eggland's best|hidden valley|best foods|knorr|land o lakes\"\n                        , '', ingredient)\n    ingredient = re.sub(\"sargento|johnsonville|breyers|diamond crystal|taco bell|bacardi\", '', ingredient)\n    ingredient = re.sub(\"mccormick|crystal farms|yoplait|mazola|new york style panetini\", '', ingredient)\n    ingredient = re.sub(\"ragu|soy vay|tabasco|truvía|crescent recipe creations|spice islands\", '', ingredient)\n    ingredient = re.sub(\"wish-bone|honeysuckle white|pasta sides|fiesta sides\", '', ingredient)\n    ingredient = re.sub(\"veri veri teriyaki|artisan blends|home originals|greek yogurt|original ranch\"\n                        , '', ingredient)\n    ingredient = re.sub(\"jonshonville\", '', ingredient)\n\n    # '™'이 있는 브랜드\n    ingredient = re.sub(\"old el paso|pillsbury|progresso|betty crocker|green giant|hellmannâ€\", '', ingredient)\n\n    # 'oscar mayer deli fresh smoked' 브랜드\n    ingredient = re.sub(\"oscar mayer deli fresh smoked\", '', ingredient)\n\n    return ingredient\n\n# 3. 재료 손질, 상태를 제거하는 함수\ndef delete_state_(ingredient):\n\n    ingredient = re.sub('frozen|chopped|ground|fresh|powdered', '', ingredient)\n    ingredient = re.sub('sharp|crushed|grilled|roasted|sliced', '', ingredient)\n    ingredient = re.sub('cooked|shredded|cracked|minced|finely', '', ingredient)        \n     return ingredient\n\n# 4. 콤마 뒤에 있는 재료손질방법을 제거하는 함수\ndef delete_comma_(ingredient):\n\n    ingredient = ingredient.split(',')\n    ingredient = ingredient[0]\n\n    return ingredient\n\n## 그외 전처리 함수 (숫자제거, 특수문자제거, 원형으로변경)\ndef original_(ingredient):\n\n    # 숫자제거\n    ingredient = re.sub('[0-9]', '', ingredient)\n\n    # 특수문자 제거\n    ingredient = ingredient.replace(\"oz.\", '')\n    ingredient = re.sub('[&%()®™/]', '', ingredient)\n    ingredient = re.sub('[-.]', '', ingredient)\n\n    # lemmatize를 이용하여 단어를 원형으로 변경\n    ingredient = wnl.lemmatize(ingredient)\n\n    return ingredient\n\n# 양 끝 공백을 제거하는 함수\ndef delete_space_(ingredient):\n    ingredient = ingredient.strip()\n    return ingredient","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"e95156bedca75cda735bc25193dda18d9ac56300","collapsed":true},"cell_type":"code","source":"df['ingredients'] = df['ingredients'].apply(lambda x : pre_processing_(x))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"b7850c5cc7f6676ee088e90e9ed6855aad6c0a87"},"cell_type":"markdown","source":"\n## IV. Pre-Processing 후 Data set의 변화"},{"metadata":{"scrolled":true,"trusted":false,"collapsed":true,"_uuid":"e0f0439c767626105cc6c08c87879b14fb5bbf53"},"cell_type":"code","source":"%%time\n### 각 row 마다의 recipe 별 ingredient를 count하기 위한 작업\n\n# 여기서는 ingredient가 각 1개씩 count 될 것이다.\nbag_of_ingredients = [Counter(ingredient) for ingredient in df.ingredients]\n\n# 각 ingredients의 종류별 개수\nsum_of_ingredients = sum(bag_of_ingredients, Counter())\n\n########################################################################################\n\n### sum_of_ingredients를 dataframe에 넣기 위한 작업\n\n# dict -> list -> dataframe\nsum_of_ingredients_dict = dict(sum_of_ingredients)\nsum_of_ingredients_list = list(sum_of_ingredients_dict.items())\n\ningredients_df = pd.DataFrame(sum_of_ingredients_list)\ningredients_df.columns = ['ingredient', 'count']\ningredients_df.tail(2)\n\nprint('전처리 후 ingredient는 총 {}개 입니다.'.format(len(ingredients_df)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"458a8f7bb58c04383e9276349c80dd0880985259"},"cell_type":"markdown","source":"## Case1) Using tfidfVectorizer"},{"metadata":{"_uuid":"db49a93f8b00e96c959065c051846cd150d3476f"},"cell_type":"markdown","source":"### I. Feature Encoding \nMachine learning algorithm에 적용하기 위해 features를 encoding 해주는 작업"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"2142fc2e64718fbfeb661f27ad5d299e93b6a76c"},"cell_type":"code","source":"df['ingredients_train'] = df['ingredients'].apply(','.join)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f92613bf46074f808fa30d45360ecd99b6019b36"},"cell_type":"markdown","source":"#### 1. Encode our features to a matrix for using machine learning algorithms"},{"metadata":{"scrolled":true,"trusted":false,"collapsed":true,"_uuid":"81df80fa946f1b53f9884a792b77f06917a3701d"},"cell_type":"code","source":"\"\"\"\nTfidfVectorizer : 문서 집합으로부터 단어의 수를 세고, TF-IDF 방식으로 단어의 가중치를 조정한 카운트 행렬을 만든다.\n\"\"\"\n\ntfv = TfidfVectorizer()\nX = tfv.fit_transform(df['ingredients_train'].values)\n\nprint(list(tfv.vocabulary_.keys())[:10])","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false,"collapsed":true,"_uuid":"aaf13118799a34d6574edba0b5009c92923f77bc"},"cell_type":"code","source":"print(X.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"5e4abfb8992a096e8500a755369db74fbd5b6dfd"},"cell_type":"code","source":"print(type(X))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"af639aae1028b2eddf51181570d76b677a22da9d"},"cell_type":"code","source":"print(X[2999])\n# 구조파악하기","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"feaff234917e3bef8cd0b43105f0c82d127d0aa1"},"cell_type":"markdown","source":"#### 2. Encode the labels (target values)"},{"metadata":{"scrolled":true,"trusted":false,"collapsed":true,"_uuid":"fb48a82e2b87b1aec425cbb9d4c31ff7e707b6ad"},"cell_type":"code","source":"Lec = LabelEncoder()\ntrain_target_value = Lec.fit_transform(df['cuisine'].values)\n\nprint(train_target_value.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"3139b1171543cc5fe610c5c9d79c4c6cdf53b24d"},"cell_type":"code","source":"print(train_target_value[:20])","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false,"collapsed":true,"_uuid":"bf1304e506cec1b529d6aa366499d83d573d70bd"},"cell_type":"code","source":"print(Lec.classes_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"91a8502531f0429498f66eb48ae4f6d37ac22402"},"cell_type":"markdown","source":"#### 3. Data split (학습시키기 위해 Data set을 train과 test로 나눈다)"},{"metadata":{"scrolled":true,"trusted":false,"collapsed":true,"_uuid":"047aabe3fc97b7bb35970749ecfa99a4b94f6c48"},"cell_type":"code","source":"X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, train_target_value)\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"c0ce7bb1c9e16ee5be80a54cbbb465dcb543f158"},"cell_type":"markdown","source":"## II. Model Selection"},{"metadata":{"_uuid":"899707302fa8b435fe4d5531a9d18c14a0158b82"},"cell_type":"markdown","source":"#### 1) 사용한 모델 : Randomforest, SVM, KNN, Xgboost, Decision Tree, Neural network\n#### 2) 성능평가는 f1 score를 기준으로 한다.\n- why? 정밀도와 재현율을 같이 고려하므로 불균형한 분류 데이터셋에서 정확도보다 더 나은 지표가 될 수 있다."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"4b8c1ddd75cfea2b6bb95c136c63b526c0a98d87"},"cell_type":"code","source":"\"\"\" Random Forest Model \"\"\"\ndef RandomForestClassifier_():\n    \n    pipe = Pipeline([('classifier', RandomForestClassifier())])\n    hyperparameter_space = [{'classifier' : [RandomForestClassifier()], \n                             'classifier__n_estimators' : [350, 375, 400],\n                             'classifier__max_features' : ['sqrt', 'log2']}]\n    \n    grid = GridSearchCV(pipe, hyperparameter_space, cv=3)\n    grid.fit(X_train, y_train)\n\n    cuisine = ['brazilian', 'british', 'cajun_creole', 'chinese', 'filipino', 'french', 'greek',\n               'indian', 'irish', 'italian', 'jamaican', 'japanese', 'korean', 'mexican', 'moroccan',\n               'russian', 'southern_us', 'spanish', 'thai', 'vietnamese']\n\n    print (classification_report(y_test, grid.predict(X_test), digits=4, target_names=cuisine))\n    \n    return print(\"Best parameters:\\n{}\\n\".format(grid.best_params_), \n                 \"Best score : {}\\n\".format(grid.best_score_),\n                 \"Test score : {}\".format(grid.score(X_test, y_test)))\n    \n\"\"\" SVM Model \"\"\"\ndef SVM_():\n    pipe = Pipeline([('classifier', SVC())])\n    hyperparameter_space = [{'classifier': [SVC()],\n                             'classifier__gamma': ['auto'],\n                             'classifier__C' : [10, 15]}]\n    grid = GridSearchCV(pipe, hyperparameter_space, cv=3)\n    grid.fit(X_train, y_train)\n    \n    cuisine = ['brazilian', 'british', 'cajun_creole', 'chinese', 'filipino', 'french', 'greek',\n               'indian', 'irish', 'italian', 'jamaican', 'japanese', 'korean', 'mexican', 'moroccan',\n               'russian', 'southern_us', 'spanish', 'thai', 'vietnamese']\n\n    print (classification_report(y_test, grid.predict(X_test), digits=4, target_names=cuisine))\n    \n    return print(\"Best parameters:\\n{}\\n\".format(grid.best_params_), \n                 \"Best score : {}\\n\".format(grid.best_score_),\n                 \"Test score : {}\".format(grid.score(X_test, y_test)))\n\n\n\"\"\" KNN Model \"\"\"\ndef KNN_():\n    \n    knn = KNeighborsClassifier()\n    \n    pipe = Pipeline([('classifier', knn)])\n    hyperparameter_space = [{'classifier': [knn],\n                             'classifier__n_neighbors': [15, 20, 25],\n                             'classifier__leaf_size' : [20, 25, 30]}]\n    grid = GridSearchCV(pipe, hyperparameter_space, cv=3)\n    grid.fit(X_train, y_train)\n        \n        \n    cuisine = ['brazilian', 'british', 'cajun_creole', 'chinese', 'filipino', 'french', 'greek',\n               'indian', 'irish', 'italian', 'jamaican', 'japanese', 'korean', 'mexican', 'moroccan',\n               'russian', 'southern_us', 'spanish', 'thai', 'vietnamese']\n\n    print (classification_report(y_test, knn.predict(X_test), digits=4, target_names=cuisine))\n    \n    return print(\"Best parameters:\\n{}\\n\".format(grid.best_params_), \n                 \"Best score : {}\\n\".format(grid.best_score_),\n                 \"Test score : {}\".format(grid.score(X_test, y_test)))\n\n\"\"\" Xgboost Model\"\"\"\ndef Xgboost_():\n    pipe = Pipeline([('classifier', xgb.XGBClassifier())])\n    hyperparameter_space = [{'classifier': [xgb.XGBClassifier()],\n                             'classifier__max_depth': [3, 4, 5],\n                             'classifier__n_estimators' : [350, 375, 400]}]\n    grid = GridSearchCV(pipe, hyperparameter_space, cv=3)\n    grid.fit(X_train, y_train)\n    \n    cuisine = ['brazilian', 'british', 'cajun_creole', 'chinese', 'filipino', 'french', 'greek',\n               'indian', 'irish', 'italian', 'jamaican', 'japanese', 'korean', 'mexican', 'moroccan',\n               'russian', 'southern_us', 'spanish', 'thai', 'vietnamese']\n\n    print (classification_report(y_test, grid.predict(X_test), digits=4, target_names=cuisine))\n    \n    return print(\"Best parameters:\\n{}\\n\".format(grid.best_params_), \n                 \"Best score : {}\\n\".format(grid.best_score_),\n                 \"Test score : {}\".format(grid.score(X_test, y_test)))\n\n\n\"\"\" Decision Tree Model \"\"\"\ndef DecisionTree_():\n    pipe = Pipeline([('classifier', DecisionTreeClassifier())])\n    hyperparameter_space = [{'classifier': [DecisionTreeClassifier()],\n                             'classifier__max_depth': [50, 60, 70]}]\n    grid = GridSearchCV(pipe, hyperparameter_space, cv=3)\n    grid.fit(X_train, y_train)\n    \n    cuisine = ['brazilian', 'british', 'cajun_creole', 'chinese', 'filipino', 'french', 'greek',\n               'indian', 'irish', 'italian', 'jamaican', 'japanese', 'korean', 'mexican', 'moroccan',\n               'russian', 'southern_us', 'spanish', 'thai', 'vietnamese']\n\n    print (classification_report(y_test, grid.predict(X_test), digits=4, target_names=cuisine))\n    \n    return print(\"Best parameters:\\n{}\\n\".format(grid.best_params_), \n                 \"Best score : {}\\n\".format(grid.best_score_),\n                 \"Test score : {}\".format(grid.score(X_test, y_test)))\n\ndef Neural_network_():\n    nn = MLPClassifier(hidden_layer_sizes=(400,500,400))\n    nn.fit(X_train, y_train)\n    \n    cuisine = ['brazilian', 'british', 'cajun_creole', 'chinese', 'filipino', 'french', 'greek',\n               'indian', 'irish', 'italian', 'jamaican', 'japanese', 'korean', 'mexican', 'moroccan',\n               'russian', 'southern_us', 'spanish', 'thai', 'vietnamese']\n\n    print (classification_report(y_test, nn.predict(X_test), digits=4, target_names=cuisine))\n    return print(\"Test score : {}\".format(nn.score(X_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b72c3ac4fcced045e195c40af2779b193ccbafec"},"cell_type":"markdown","source":"#### 1. Random Forest"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"0dd51d3b95ba735ccd29775b1adfb3a9535664f5"},"cell_type":"code","source":"%%time\nrf = RandomForestClassifier_()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cf855a6c9083e2b53a4ea74af3c1e5d533d62b77"},"cell_type":"markdown","source":"#### 2. SVM"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"cd10e632602f4fd0b409ba3a6dcfec4d46414f5c"},"cell_type":"code","source":"%%time\nSVM = SVM_()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0fd1acd614e7c280f273c8fa4a4ee5f893bb86e6"},"cell_type":"markdown","source":"#### 3. KNN"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"3d0b82490377076b0378d89a5107a39ebd325b26"},"cell_type":"code","source":"%%time\nKNN = KNN_()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"35f01b08cb52633a9be054bae3f29d02f5b79a32"},"cell_type":"markdown","source":"#### 4. Xgboost"},{"metadata":{"scrolled":false,"trusted":false,"collapsed":true,"_uuid":"0b421ebd8353bc081951f3f2eba78240de3e537f"},"cell_type":"code","source":"%%time\nXgboost_()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"20031fcb9991c28e8165dc32cb806acec1d1026b"},"cell_type":"markdown","source":"#### 5. Decision Tree"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"69a9dce9ec1c6e67aa0e510e8d74073d9ca411b9"},"cell_type":"code","source":"%%time\nDecisionTree_()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"42d1b86c77d8bcc10f94def1d8b980075144f439"},"cell_type":"markdown","source":"#### 6. Neural Network"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"0a28bffc6b044d232f2218c774494967bf9ce4c5"},"cell_type":"code","source":"%%time\nNeural_network = Neural_network_()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"540b5338e5b91e321cd207cdcbc59e0e839b89ae"},"cell_type":"markdown","source":"## III. Conclusion"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"ab690688a3d8c4f4ef8088a28afacf50448ce24d"},"cell_type":"code","source":"# Feature importance\npd.Series(xgbr.feature_importances_).plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"710b1f702103930d86c30340bc7af5082e9056d7"},"cell_type":"markdown","source":"#### 결론\n- Decision tree와 SVM 같은 경우 parameter 값을 계속 조정해 봤지만 성능이 잘 올라가지 않았다.\n- 학습 시간이 제일 오래 걸렸지만(4시간 30분), f1_score가 약 0.779로 Xgboost의 성능이 가장 좋았다.\n- Neural network는 layer를 더 쌓고 계속 조절해도 Xgboost 보다는 성능이 좋아지지는 않았다. 하지만 0.01프로 차이이고, grid search를 쓰지 않았을때 Xgboost가 대략 50분 정도 걸렸던 것을 생각하면 Neural network 빠른 속도로 좋은 성능을 내는 모델이라 생각된다. (뉴럴네트워크의 layer를 100개 이상 쌓는경우 속도가 더 느려지겠지만, 레이어를 늘려도 성능 차이가 크게 없어서 layer를 더이상 늘리지 않았다.)"},{"metadata":{"_uuid":"3c48c0567011834d791a5bcc3077aa3af73d7d6a"},"cell_type":"markdown","source":"## Case2) Using PCA"},{"metadata":{"_uuid":"c63b4a7acfee1452abc2e6654b9bcbdadb7ac011"},"cell_type":"markdown","source":"1. 각 Ingredient를 column으로 변경한 뒤 PCA를 통해 차원을 축소한다.\n2. tfidfVectorize를 이용하여 학습을 했을 때, f1 score가 가장 높았던 Xgboost와 Neural-network 이용하여 학습한다."},{"metadata":{"_uuid":"7a11552d761039aa125cf7218518a4b9b85697e8"},"cell_type":"markdown","source":"## I. ingredients column의 recipe를 colum으로 변경하기"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"6bff6643caf441b15f0e1fbd73e2e867574a7346"},"cell_type":"code","source":"# 1. all_ingredients set에 ingredients들을 담는다.\nall_ingredients = set()\ndf['ingredients'].apply(lambda x : [all_ingredients.add(i) for i in list(x)])\n#print(all_ingredients)\n\n# 'ingredient' columns를 새로 만들면서, 각 ingredient가 해당 row의 recipe에 들어 있으면 True, 그렇지 않으면 False를 반환하게 함\nfor ingredient in all_ingredients:\n    df[ingredient] = df['ingredients'].apply(lambda x : ingredient in x)\n    \nlen(df.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"6c87125c0d4db4d34784ff1c30832d6a3a54e9ed"},"cell_type":"code","source":"%%time\n\ncolumn_list = []\nfor col in df.columns:\n    column_list.append(col)\n    \ncolumn_list.remove('id')\ncolumn_list.remove('ingredients')\ncolumn_list.remove('cuisine')\n\nlen(column_list)\nprint(column_list[:10])\n\ndf[column_list] = df[column_list].astype(int) # False는 0으로, True는 1로","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"a1bfee4de3b73013735a50c23453114aadc08fff"},"cell_type":"code","source":"# Copy\ndf_dummy = df.copy()\n\n# 'id'와 'ingredients' columns는 더이상 필요가 없으므로 지운다.\ndel df_dummy['id']\ndel df_dummy['ingredients']\n\n# 'cuisine' column을 지우기 위해 df_dummy를 copy\ndf_features = df_dummy.copy()\n\ndel df_features['cuisine']\ndf_features.tail(1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8939cc6bbe2fd6228b3760465b24a60891d5092f"},"cell_type":"markdown","source":"## II. Label Encoding"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"0bd1dceaa324da607675ff39a8a17cf347f5fd37"},"cell_type":"code","source":"Lec = LabelEncoder()\ntrain_target_value = Lec.fit_transform(df_dummy['cuisine'].values)\n\nprint(train_target_value.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"f7f06ec7c7a0e6579d72d1778306b30215e41e79"},"cell_type":"code","source":"print(train_target_value[:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"066300922853855242548c4e87d95198212da4ca"},"cell_type":"code","source":"print(Lec.classes_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fef486d02ac05d7ef4e13e40654770923de40e14"},"cell_type":"markdown","source":"## III. Data split"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"295675c2201ae944270d4c1eb2ccd513ffa41b6a"},"cell_type":"code","source":"X_train, X_test, y_train, y_test = cross_validation.train_test_split(df_features, train_target_value)\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"370b7dfc3fa8713209f90901fe9f9b663a9eec3b"},"cell_type":"markdown","source":"## IV. PCA"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"7d8d2b5aa0c2c6867519d26e207fe728c4bb4a49"},"cell_type":"code","source":"%%time\n# fit method를 호출하여 주성분을 찾는다. 주성분은 200개로 한다.\npca = PCA(n_components=200, whiten=True, random_state=0).fit(X_train)\n# transform method를 호출해 데이터를 회전시키고 차원을 축소\nX_train_pca = pca.transform(X_train)\nX_test_pca = pca.transform(X_test)\n\nprint(\"X_train_pca.shape: {}\".format(X_train_pca.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"ed1bde91c619ebd48e43e04078d679e037b6d2d1"},"cell_type":"code","source":"pca.components_","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0ab99cb1dd3228b0f020ec3adea46f9e51b24909"},"cell_type":"markdown","source":"이제 새로운 data는 처음 200개의 주성분에 해당하는 특성을 갖는다."},{"metadata":{"_uuid":"02162d6f1bba3536416d45349d9055365940eb3b"},"cell_type":"markdown","source":"## V. Model Selection"},{"metadata":{"_uuid":"c87a1cf04d9767a9f1bc04a5a717777527aaa93d"},"cell_type":"markdown","source":"### 1. Xgboost"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"9ea766160d6ff3989a6f6447a20142b5a842fac8"},"cell_type":"code","source":"def Xgboost_():\n    xgbr = xgb.XGBClassifier(\n        n_estimators = 400,\n        max_depth = 5\n    ).fit(X_train_pca, y_train)\n        \n    cuisine = ['brazilian', 'british', 'cajun_creole', 'chinese', 'filipino', 'french', 'greek',\n               'indian', 'irish', 'italian', 'jamaican', 'japanese', 'korean', 'mexican', 'moroccan',\n               'russian', 'southern_us', 'spanish', 'thai', 'vietnamese']\n\n    print (classification_report(y_test, xgbr.predict(X_test_pca), digits=4, target_names=cuisine))\n    return print(\"Test score : {}\".format(xgbr.score(X_test_pca, y_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"ed311ca7972f5b1bb8d570e1a7206e8b4b38f224"},"cell_type":"code","source":"%%time\nXgboost = Xgboost_()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2cb65de64fa6f236892c79b5d013b4bdf50d31e6"},"cell_type":"markdown","source":"### 2. Neural Network"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"9e73f9598c6a9b7f0694b9f289006b8a6f9e8f4c"},"cell_type":"code","source":"def Neural_network_():\n    nn = MLPClassifier(hidden_layer_sizes=(400,500,400))\n    nn.fit(X_train_pca, y_train)\n    \n    cuisine = ['brazilian', 'british', 'cajun_creole', 'chinese', 'filipino', 'french', 'greek',\n               'indian', 'irish', 'italian', 'jamaican', 'japanese', 'korean', 'mexican', 'moroccan',\n               'russian', 'southern_us', 'spanish', 'thai', 'vietnamese']\n\n    print (classification_report(y_test, nn.predict(X_test_pca), digits=4, target_names=cuisine))\n    return print(\"Test score : {}\".format(nn.score(X_test_pca, y_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"d996b169fa9f09414147e16e1de182f853af4128"},"cell_type":"code","source":"%%time\nNeural_Network = Neural_network_()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eccce3dcd3f6b67779348420960fe6910999dc60"},"cell_type":"markdown","source":"## VI. Conclusion"},{"metadata":{"_uuid":"54157d33c05932ad1f4d4c50d48e1ade91ad05b5"},"cell_type":"markdown","source":"- 성능을 더 높이고 싶어서 시도해보았지만, PCA를 이용하여 축소된 데이터를 가지고 학습시킨 경우 더 성능이 좋지 않았다.\n- PCA가 잘 작동할 때는 데이터에 중복이 많은 경우인데 중복된 컬럼이 별로 없었기 때문에 PCA로 요약하는 게 의미가 없어진거라고 판단했다."},{"metadata":{"_uuid":"1d5e7d473fce6d8efe44be8bd037973e957956dd"},"cell_type":"markdown","source":"## Case3) Simple Bot"},{"metadata":{"_uuid":"db278926edee2cc54e183d000a1baf3330595e5d"},"cell_type":"markdown","source":"- user가 선호하는 음식을 선택하면 그것을 통해서 user가 어느나라 음식을 선호하는지 파악하고, recipe를 추천하는 기능을 구현하고 싶었으나 각 recipe의 음식 이름을 찾을 수가 없었다. 그래서 방향을 전환하여 user가 cuisine을 선택하면 word2vec을 이용해서 user가 선택한 cuisine과 가장 유사한 ingredients를 알려주는 간단한 봇을 구현했다."},{"metadata":{"_uuid":"5de7a6193f54fa2a5dc044e6e70c46079a68117a"},"cell_type":"markdown","source":"## I. word2vec 적용"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"2d0ff88100ac57f73048df69cccc299236c9ad3a"},"cell_type":"code","source":"\"\"\" \n\nlogic : df['cuisine']과 df['ingredients']를 리스트 형식으로 만들고 싶지만, \n        df['ingredients']가 각각 리스트 형식으로 되어있기 때문에 몇가지 작업이 필요하다.\n\n1. df['ingredients'] 즉, 각 recipe 리스트를 하나의 리스트(called ingredient_list)에 넣는다.\n   ingredient_list를 data frame에 넣어준다 (왜? 각 ingredient를 column으로 만들어 주기 위해)\n2. df['cuisine']과 ingredient_list를 column으로 합친다. 그러면 하나의 data frame이 만들어짐\n3. iterrows() 함수를 이용하여 각 row를 list로 만들어 준다.\n\n\"\"\"\n\n\"\"\" 1번 작업 \"\"\"\ningredient_list = []\n\nfor elements in df['ingredients']:\n    ingredient_list.append(elements)\n    \n## ingredient_list의 각 원소를 리스트 형식\n#ingredient_list\n\ningredient_df = pd.DataFrame(ingredient_list)\n#ingredient_df\n\n\"\"\" 2번 작업 \"\"\"\ncuisine_list = []\n\nfor element in df['cuisine']:\n    cuisine_list.append(element)\n\ningredient_df.insert(0, \"cuisines\", cuisine_list)\n#ingredient_df.tail(2)\n\n\"\"\" 3번 작업 \"\"\"\ntemp = []\n\nfor row in ingredient_df.iterrows():\n    index, data = row\n    temp.append(data.tolist())\n    \n#temp\n\n\"\"\" 예상치 못하게, temp안에 None 값이 들어 간것을 확인했다. None 값을 제거한다. \"\"\"\nnew_temp = []\n\nfor list_element in temp:\n    new_element = [x for x in list_element if x is not None]\n    new_temp.append(new_element)\n    \n#new_temp[0]\n\n\"\"\" word2vec 학습 \"\"\"\nmodel = word2vec.Word2Vec(new_temp, workers = 4, \n                         size = 300, min_count = 3, window = 10)\n\nmodel.init_sims(replace=True)\n\n# 학습이 잘 되었는지 확인\nmodel.most_similar('korean')","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"717ab43c255fd38e7fd6310a5b1eb6f9008ab3f8"},"cell_type":"code","source":"cuisine_dict = dict(df.cuisine.value_counts().items())\ncuisine_list = list(cuisine_dict.keys())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"129f61bc64d3e08f89945fd841e49b48647e052b"},"cell_type":"markdown","source":"## II. Simple Bot"},{"metadata":{"scrolled":false,"trusted":false,"collapsed":true,"_uuid":"26c260fd28b8be996c98745f7cc9cd6a22c1dac2"},"cell_type":"code","source":"print(\"Bot: \" \n      + \"\\t\" + \"I can tell you common ingredients often used in the country.\" \n      + \"\\n\" + \"\\t\" + \"What kind of cuisine do you want to know?\" \n      + \"\\n\" + \"\\t\" + \"Cuisine list is here.\"\n      + \"\\n\" + \"\\t\" + \"====================[CUISINE LISE]====================\"\n      + \"\\n\" + \"\\t\" + \"italian, mexican, southern_us, indian, chinese\"\n      + \"\\n\" + \"\\t\" + \"french, cajun_creole, thai, japanese, greek\"\n      + \"\\n\" + \"\\t\" + \"spanish, korean, vietnamese', moroccan, british\"\n      + \"\\n\" + \"\\t\" + \"filipino, irish, jamaican, russian, brazilian\"\n      + \"\\n\" + \"\\t\" + \"======================================================\")\n    \nwhile True:\n\n    user_question = input(\"User: \" + \"\\t\").strip()\n    user_question_lower = user_question.lower()\n    \n    ingredient_list = []\n    for cuisine in cuisine_list:\n        \n        if cuisine in user_question:\n            for i in model.most_similar(cuisine):\n                if i[0] not in cuisine_list:\n                    ingredient_list.append(i[0])\n            print(\"Bot: \" + \"\\t\" + \"commonly used ingredients in {} food are {}\".format(cuisine, ingredient_list)\n                          + \"\\n\" + \"\\t\" + \"Do you want to know more? yes or no\")\n                        \n    if user_question == \"\":\n        print(\"Bot: \"+ \"\\t\" + \"What kind of cuisine do you want to know?\"\n                   + \"\\t\" + \"I can tell you common ingredients often used in the country.\" \n                   + \"\\n\" + \"\\t\" + \"What kind of cuisine do you want to know?\" \n                   + \"\\n\" + \"\\t\" + \"Cuisine list is here.\"\n                   + \"\\n\" + \"\\t\" + \"====================[CUISINE LISE]====================\"\n                   + \"\\n\" + \"\\t\" + \"italian, mexican, southern_us, indian, chinese\"\n                   + \"\\n\" + \"\\t\" + \"french, cajun_creole, thai, japanese, greek\"\n                   + \"\\n\" + \"\\t\" + \"spanish, korean, vietnamese', moroccan, british\"\n                   + \"\\n\" + \"\\t\" + \"filipino, irish, jamaican, russian, brazilian\"\n                   + \"\\n\" + \"\\t\" + \"======================================================\")\n        \n    elif user_question == \"yes\":\n        print(\"Bot: \"+ \"\\t\" + \"What kind of cuisine do you want to know?\"\n                   + \"\\t\" + \"I can tell you common ingredients often used in the country.\" \n                   + \"\\n\" + \"\\t\" + \"What kind of cuisine do you want to know?\" \n                   + \"\\n\" + \"\\t\" + \"Cuisine list is here.\"\n                   + \"\\n\" + \"\\t\" + \"====================[CUISINE LISE]====================\"\n                   + \"\\n\" + \"\\t\" + \"italian, mexican, southern_us, indian, chinese\"\n                   + \"\\n\" + \"\\t\" + \"french, cajun_creole, thai, japanese, greek\"\n                   + \"\\n\" + \"\\t\" + \"spanish, korean, vietnamese', moroccan, british\"\n                   + \"\\n\" + \"\\t\" + \"filipino, irish, jamaican, russian, brazilian\"\n                   + \"\\n\" + \"\\t\" + \"======================================================\")\n                        \n    elif user_question == \"no\":\n        print(\"Bot: \"+ \"\\t\" + \"See you again. Bye.\")\n        break","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6b34f66f3c59b3e10c7afb4736c8e409d1d39e41"},"cell_type":"markdown","source":"## III. Conclusion"},{"metadata":{"_uuid":"7f645a2e23f770224f739184d62f2ccee8499abf"},"cell_type":"markdown","source":"- seq2seq, 마르코프체인 등 챗봇을 구현할 수 있는 알고리즘을 적용할 수 있도록 더 공부하여 시도해 볼 것이다."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"93421acc7d9a1933011b3a39385dd346674d8b04"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}