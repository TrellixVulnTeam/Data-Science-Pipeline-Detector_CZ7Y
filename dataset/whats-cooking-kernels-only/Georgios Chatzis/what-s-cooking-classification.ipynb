{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-03T09:56:42.245544Z","iopub.execute_input":"2021-11-03T09:56:42.245936Z","iopub.status.idle":"2021-11-03T09:56:42.258032Z","shell.execute_reply.started":"2021-11-03T09:56:42.245821Z","shell.execute_reply":"2021-11-03T09:56:42.257159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Explore Dataset","metadata":{}},{"cell_type":"code","source":"df = pd.read_json('/kaggle/input/whats-cooking-kernels-only/train.json')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-03T09:56:42.259284Z","iopub.execute_input":"2021-11-03T09:56:42.259597Z","iopub.status.idle":"2021-11-03T09:56:42.661308Z","shell.execute_reply.started":"2021-11-03T09:56:42.259557Z","shell.execute_reply":"2021-11-03T09:56:42.660511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(8, 6))\ndf.cuisine.value_counts().plot.bar(title='Classes Counts')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-03T09:56:42.662635Z","iopub.execute_input":"2021-11-03T09:56:42.662949Z","iopub.status.idle":"2021-11-03T09:56:42.974208Z","shell.execute_reply.started":"2021-11-03T09:56:42.662921Z","shell.execute_reply":"2021-11-03T09:56:42.973511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nn_classes = len(df['cuisine'].unique())\nprint(\"Number of classes\", n_classes)\n\n# get the length of the tokens\ndf['length'] = df.ingredients.map(lambda x: len(x))\n\n# get the number of classes\nle = LabelEncoder()\ndf['categorical_label'] = le.fit_transform(df.cuisine)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-03T09:56:42.975923Z","iopub.execute_input":"2021-11-03T09:56:42.976141Z","iopub.status.idle":"2021-11-03T09:56:43.341438Z","shell.execute_reply.started":"2021-11-03T09:56:42.976117Z","shell.execute_reply":"2021-11-03T09:56:43.34056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# split dataset\ntrain_set, valid_set = train_test_split(df, test_size=0.15, stratify=df.cuisine, random_state=42)\n\nprint(train_set.shape)\nprint(valid_set.shape)\n\ntrain_sentences = [','.join(sentence) for sentence in train_set.ingredients.values.tolist()]\nvalid_sentences = [','.join(sentence) for sentence in valid_set.ingredients.values.tolist()]\n\n# get the labels\ny_train = train_set.categorical_label\ny_valid = valid_set.categorical_label\n\ntrain_sentences[:3]","metadata":{"execution":{"iopub.status.busy":"2021-11-03T09:56:43.343144Z","iopub.execute_input":"2021-11-03T09:56:43.343444Z","iopub.status.idle":"2021-11-03T09:56:43.478501Z","shell.execute_reply.started":"2021-11-03T09:56:43.343405Z","shell.execute_reply":"2021-11-03T09:56:43.477841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Text Vectorization","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\n\n# get sequence max length\nsequence_length = int(df['length'].max())\n\n# create vectorization layer\nvectorization_layer = tf.keras.layers.TextVectorization(max_tokens=None, output_mode='int', output_sequence_length=sequence_length, \n                                                        split=lambda x: tf.strings.split(x, ','), standardize=lambda x: tf.strings.lower(x))\nvectorization_layer.adapt(train_sentences)\n\n# create vectorization layer\nvectorizer = tf.keras.models.Sequential()\nvectorizer.add(tf.keras.Input(shape=(1,), dtype=tf.string))\nvectorizer.add(vectorization_layer)\n\n# get sequences\ntrain_sequences = vectorizer.predict(train_sentences)\nvalid_sequences = vectorizer.predict(valid_sentences)\n\nprint(train_sentences[:3])\nprint(train_sequences[:3])","metadata":{"execution":{"iopub.status.busy":"2021-11-03T09:56:43.479988Z","iopub.execute_input":"2021-11-03T09:56:43.480553Z","iopub.status.idle":"2021-11-03T09:56:47.818093Z","shell.execute_reply.started":"2021-11-03T09:56:43.480515Z","shell.execute_reply":"2021-11-03T09:56:47.817089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(vectorization_layer.get_vocabulary()))\nprint(vectorization_layer.get_vocabulary()[:10])","metadata":{"execution":{"iopub.status.busy":"2021-11-03T09:56:47.819595Z","iopub.execute_input":"2021-11-03T09:56:47.819826Z","iopub.status.idle":"2021-11-03T09:56:47.853978Z","shell.execute_reply.started":"2021-11-03T09:56:47.819799Z","shell.execute_reply":"2021-11-03T09:56:47.853267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create Classification Model","metadata":{}},{"cell_type":"code","source":"embedding_dim = 50\nvocab_size = vectorization_layer.vocabulary_size()\n\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=sequence_length, mask_zero=True),\n    tf.keras.layers.Conv1D(128, 5, activation='relu'),\n    tf.keras.layers.GlobalMaxPool1D(),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dense(n_classes, activation='softmax')\n])\n    \nmodel.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())","metadata":{"execution":{"iopub.status.busy":"2021-11-03T09:56:47.855695Z","iopub.execute_input":"2021-11-03T09:56:47.856011Z","iopub.status.idle":"2021-11-03T09:56:47.921252Z","shell.execute_reply.started":"2021-11-03T09:56:47.85597Z","shell.execute_reply":"2021-11-03T09:56:47.920425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=5)\ncheckpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"cooking_deep.h5\", save_best_only=True)\n\nhistory = model.fit(train_sequences, y_train, epochs=30, validation_data=(valid_sequences, y_valid),\n                    callbacks=[early_stopping_cb, checkpoint_cb])","metadata":{"execution":{"iopub.status.busy":"2021-11-03T09:56:47.923399Z","iopub.execute_input":"2021-11-03T09:56:47.92373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(len(loss))  # Get number of epochs\n\n# Plot training and validation loss per epoch\nplt.figure(figsize=(8, 6))\nplt.plot(epochs, loss, 'r', label=\"Training Loss\")\nplt.plot(epochs, val_loss, 'b', label=\"Validation Loss\")\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Make Predictions","metadata":{}},{"cell_type":"code","source":"model = tf.keras.models.load_model(\"cooking_deep.h5\")\nprint(model.evaluate(valid_sequences, y_valid))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import zipfile\nzip_ref = zipfile.ZipFile(\"/kaggle/input/whats-cooking-kernels-only/sample_submission.csv.zip\", 'r')\nzip_ref.extractall('/kaggle/temp')\nzip_ref.close()\n\npd.read_csv('/kaggle/temp/sample_submission.csv').head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_set = pd.read_json('/kaggle/input/whats-cooking-kernels-only/test.json')\ntest_sentences = [','.join(sentence) for sentence in test_set.ingredients.values.tolist()]\ntest_sequences = vectorizer.predict(test_sentences)\npredictions = model.predict(test_sequences)\npredictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_set[\"cuisine\"] = le.inverse_transform(np.argmax(predictions, axis=1))\ntest_set[['id', 'cuisine']].to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.read_csv('submission.csv').head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}