{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nfrom sklearn import svm\nfrom sklearn.model_selection import GridSearchCV, train_test_split, KFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import make_scorer, accuracy_score\nfrom sklearn.svm import SVC\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.naive_bayes import BernoulliNB\n\nimport pandas as pd\n\nimport numpy as np\nimport seaborn as sns\n \nimport matplotlib.pyplot as plt\nimport matplotlib.mlab as mlab\nimport matplotlib\nimport json\nimport os\nplt.style.use('ggplot')\nfrom matplotlib.pyplot import figure\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\nmatplotlib.rcParams['figure.figsize'] = (12,8)\n \npd.options.mode.chained_assignment = None\n\n#Leitura dos dados\ntrain = pd.read_json(\"/kaggle/input/whats-cooking-kernels-only/train.json\")\ntest = pd.read_json(\"/kaggle/input/whats-cooking-kernels-only/test.json\")\n# print(\"Tipagem dos dados\")\n# print(train.dtypes)\n \n# print(\"\\nVolume de dados (linhas x colunas)\")\n# print(train.shape)\n \n# print(\"\\nColunas numericas\")\ndf_numeric = train.select_dtypes(include=[np.number])\nnumeric_cols = df_numeric.columns.values\n# print(numeric_cols)\n \n \n# print(\"\\nColunas não numericas (STRING)\")\ndf_non_numeric = train.select_dtypes(exclude=[np.number])\nnon_numeric_cols = df_non_numeric.columns.values\nprint(non_numeric_cols)\n \ncolours = ['#8B0000', '#000000'] # Definição das cores para a plotagem (vermelho: dado presente, preto: dado não presente)\naux = sns.heatmap(train[train.columns].isnull(), cmap=sns.color_palette(colours))\n\ntrain['ingredient_text'] = train['ingredients'].apply(lambda x: ' '.join([i.replace(' ','_').lower() for i in x]))\ntest['ingredient_text'] = test['ingredients'].apply(lambda x: ' '.join([i.replace(' ','_').lower() for i in x]))\n\ndef generate_parameters(x,y,classifier,parameters): \n  grid = GridSearchCV(\n          classifier(), parameters, scoring='accuracy'\n      )\n  grid = grid.fit(x, y)\n  clf = grid.best_estimator_\n  return clf\n\ndef run_kfold(clf, train, test):\n    print(\"rodando kfold\")\n    kf = KFold(10)\n    print(kf)\n    output = []\n    fold = 0\n    for train_index, test_index in kf.split(train):\n        fold += 1\n        x_train, x_test = train[train_index], train[test_index]\n        y_train, y_test = test[train_index], test[test_index]\n        clf.fit(x_train, y_train)\n        predictions = clf.predict(x_test)\n        accuracy = accuracy_score(y_test, predictions)\n        output.append(accuracy)\n        print(\"Fold {0} acurácia: {1}\".format(fold, accuracy))     \n    mean_output = np.mean(output)\n    dp = np.std(output)\n    print(\"Acurácia média: {0}\".format(mean_output)) \n    print(\"Desvio padrão da acurácia: {0}\".format(dp))\n\n# x_all = train.drop(['cuisine'], axis=1)\n# y_all = train['cuisine']\n\nnum_test = 0.2\ncv = CountVectorizer()\nle = LabelEncoder()\n\nx_train = cv.fit_transform(train['ingredient_text'])\ny_train = le.fit_transform(train['cuisine'])\nx_test = cv.transform(test['ingredient_text'])\nprint(x_train)\n\nnb = BernoulliNB()\nparameters =[{\n    'alpha': [1.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],\n    'binarize': [0.0,None, 0.5],\n    'fit_prior': [True, False],    \n}]\n# print(\"Naive Bayes\")\nclf_nb = generate_parameters(x_train, y_train, BernoulliNB, parameters)\n\nclf_nb.fit(x_train, y_train)\n\np = clf_nb.predict(x_test)\np = le.inverse_transform(p)\nresult = pd.DataFrame({'id': test[\"id\"], 'cuisine': p})\nresult.to_csv(\"./submission.csv\", index=False)\nprint(result)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]}]}