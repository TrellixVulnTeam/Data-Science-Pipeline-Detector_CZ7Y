{"cells":[{"metadata":{},"cell_type":"markdown","source":"# A little start with the dataset and the ML\n\nI'm going to split this notebook in two parts:\n* Little study and preprocess of train data.\n* Cross-validation over Random Forest"},{"metadata":{},"cell_type":"markdown","source":"## Little Study of train data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nfrom tqdm import tqdm_notebook\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Leemos el Json original y lo visualizamos\ndata_path = '/kaggle/input/whats-cooking-kernels-only/'\njson_train_path = os.path.join(data_path, 'train.json')\njson_train = pd.read_json(json_train_path)\n\njson_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Missing values\n\nFirst of all we are going to search for missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"json_train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is no missing values."},{"metadata":{},"cell_type":"markdown","source":"### Class Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"json_train['cuisine'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(y='cuisine', data=json_train)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cleaning the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a list with all different ingredients \ntotal_ingredients_list = []\ntq = tqdm_notebook(total=json_train.shape[0])\nfor ing in json_train['ingredients']:\n    total_ingredients_list = total_ingredients_list + ing\n    tq.update(1)\ntq.close()\n\nprint('Total number of unique ingredients: {}'.format(len(np.unique(total_ingredients_list))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove a lot of things to clean the data:\n# - Remove (number  oz.)\n# - Remove all spaces in the beggining or in the end\n# - Remove all special characters\n\nimport string\nfrom nltk.corpus import stopwords\n\nchars = re.escape(string.punctuation)\nclean_ingredients_list = [re.sub(r'['+chars+']', '', \n                                 re.sub('[0-9]+','', c.replace(\"oz\",\"\"))).strip(' ').lower()\n                    for c in total_ingredients_list]\n\nstop_words = set(stopwords.words('english'))\ntq = tqdm_notebook(total=len(clean_ingredients_list))\nfor i, ingredients in enumerate(clean_ingredients_list):\n    cleaned_ingredients = [c for c in ingredients.split(' ') if c not in stop_words]\n    cleaned_ingredients = (' '.join(cleaned_ingredients)).strip(' ')\n    clean_ingredients_list[i] = cleaned_ingredients\n    tq.update(1)\ntq.close()\n# See some of the cleaned data\n# Var with all cleaned ingredients in the dataset\nclean_unique_ingredients_list = [c for c in list(np.unique(clean_ingredients_list)) if len(c)>0]  \nprint('Total of unique ingredients: ', len(clean_unique_ingredients_list))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creating a dataframe with one column per ingredient with 1 if the ingredient appears in this recipe and 0 if not appears"},{"metadata":{"trusted":true},"cell_type":"code","source":"classes_dict = {'greek':0, 'southern_us':1, 'filipino':2, 'indian':3, 'jamaican':4,\n                'spanish':5, 'italian':6, 'mexican':7, 'chinese':8, 'british':9, 'thai':10,\n                'vietnamese':11, 'cajun_creole':12, 'brazilian':13, 'french':14, 'japanese':15,\n                'irish':16, 'korean':17, 'moroccan':18, 'russian':19}\n\ningredients_encoded = np.zeros((json_train.shape[0], len(clean_unique_ingredients_list)+2), dtype=np.uint8)\n\ntq = tqdm_notebook(total=json_train.shape[0])\nfor i in range(json_train.shape[0]):\n    ingredients_encoded[i,0] = json_train['id'].values[i]\n    ingredients_encoded[i,1] = classes_dict[json_train['cuisine'][i]]\n    # first clean\n    clean_recipe = [re.sub(r'['+chars+']', '',\n                           re.sub('[0-9]+','', c.replace(\"oz\",\"\"))).strip(' ').lower()\n                    for c in json_train['ingredients'][i]]\n    \n    # delete stop words\n    for k, ingredients in enumerate(clean_recipe):\n        cleaned_ingredients = [c for c in ingredients.split(' ') if c not in stop_words]\n        cleaned_ingredients = (' '.join(cleaned_ingredients)).strip(' ')\n        clean_recipe[k] = cleaned_ingredients\n        \n    clean_recipe = [c for c in clean_recipe if len(c)>0]\n    for ingredient in clean_recipe:\n        ingredients_encoded[i,clean_unique_ingredients_list.index(ingredient)+2]= 1\n        \n    tq.update(1)\n    \ntq.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.DataFrame(data=ingredients_encoded[:,1:])\ndata","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check if everything is ok in two ways:\n* We are going to create an image with one column. In the title we print the name if the ingredient and the ocurrences in all the dataset. In the image each white pixel is an ocurrence.\n* We are going to create a list with each ingredient and the number of ocurrences and see the top 5."},{"metadata":{"trusted":true},"cell_type":"code","source":"column_to_show = 6677\n\nimage = data[column_to_show].values\nimage = np.reshape(image, (42,947))\n\nplt.figure(figsize=(10,30))\nplt.title(clean_unique_ingredients_list[column_to_show-1]+\n          ' {}'.format(np.sum(data[column_to_show].values)))\nplt.imshow(image*255, cmap='gray')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sum = np.sum(data[data.columns[1:]].values, axis=0)\nZ = [x for _,x in sorted(zip(sum, clean_unique_ingredients_list), reverse=True)]\nY = [y for y,x in sorted(zip(sum, clean_unique_ingredients_list), reverse=True)]\n\nprint('Top five: ',list(zip(Z,Y))[:5])\nprint('Inverse top five: ', list(zip(Z,Y))[-5:])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Models and other things\n\nPrepare the data to ML."},{"metadata":{"trusted":true},"cell_type":"code","source":"y = data[data.columns[0]]\nX = data[data.columns[1:]]\n# Split data in train-val\npercent_train = 80\n# OJO! para hacer los conjuntos de train y val, deberiamos tener cuidado de incluir todo tipo de cocina en ambos\n# estaria bien localizar los indices de cada tipo de cocina e incluir un 80% en train y un 80% para cada una\nidx_train = []\nidx_val = []\nfor cls in classes_dict.values():\n    indices = np.array(data.index[y==cls])\n    np.random.shuffle(indices)\n    idx_train += list(indices[:int(len(indices)*percent_train/100)])\n    idx_val += list(indices[int(len(indices)*percent_train/100):])\n\nnp.random.shuffle(idx_train)\nnp.random.shuffle(idx_val)\n    \nX_train = X.values[idx_train,:]\ny_train = y.values[idx_train]\nX_val = X.values[idx_val,:]\ny_val = y.values[idx_val]\n\nX_train.shape, y_train.shape, X_val.shape, y_val.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrf_best = RandomForestClassifier(bootstrap=True, \n                                 class_weight='balanced',\n                                 criterion='gini', \n                                 max_depth=100, \n                                 max_features='auto',\n                                 max_leaf_nodes=None, \n                                 min_impurity_decrease=0.0,\n                                 min_impurity_split=None, \n                                 min_samples_leaf=1,\n                                 min_samples_split=2, \n                                 min_weight_fraction_leaf=0.0,\n                                 n_estimators=400, \n                                 n_jobs=8, \n                                 oob_score=False,\n                                 random_state=None, verbose=0, warm_start=False)\n# Acc CV: 0.7225571464456166\n\nrf_best.fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Prepare test data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"json_test_path = os.path.join(data_path, 'test.json')\njson_test = pd.read_json(json_test_path)\n\njson_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ingredients_encoded_test = np.zeros((json_test.shape[0], len(clean_unique_ingredients_list)+1), dtype=np.uint8)\n\ntq = tqdm_notebook(total=json_test.shape[0])\nfor i in range(json_test.shape[0]):\n    ingredients_encoded_test[i,0] = json_test['id'].values[i]\n    #ingredients_encoded_test[i,1] = classes_dict[json_test['cuisine'][i]]\n    # first clean\n    clean_recipe = [re.sub(r'['+chars+']', '',\n                           re.sub('[0-9]+','', c.replace(\"oz\",\"\"))).strip(' ').lower()\n                    for c in json_test['ingredients'][i]]\n    # delete stop words\n    for k, ingredients in enumerate(clean_recipe):\n        cleaned_ingredients = [c for c in ingredients.split(' ') if c not in stop_words]\n        cleaned_ingredients = (' '.join(cleaned_ingredients)).strip(' ')\n        clean_recipe[k] = cleaned_ingredients\n        \n    clean_recipe = [c for c in clean_recipe if len(c)>0]\n    for ingredient in clean_recipe:\n        if ingredient not in clean_unique_ingredients_list:\n            continue\n        ingredients_encoded_test[i,clean_unique_ingredients_list.index(ingredient)+1]= 1\n        \n    tq.update(1)\n    \ntq.close()\n\ndata_test = pd.DataFrame(data=ingredients_encoded_test[:,1:])\ndata_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"column_to_show = 6676\n\nimage = data_test[column_to_show].values\nimage = np.reshape(image, (88,113))\n\nplt.figure(figsize=(10,30))\nplt.title(clean_unique_ingredients_list[column_to_show]+\n          ' {}'.format(np.sum(data_test[column_to_show].values)))\nplt.imshow(image*255, cmap='gray')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = data_test[data_test.columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = rf_best.predict(X_test)\ny_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv(os.path.join(data_path, 'sample_submission.csv'))\ny_pred_str = [list(classes_dict.keys())[list(classes_dict.values()).index(c)] for c in y_pred]\nprint(len(y_pred_str), sub.shape)\nsub['id'] = json_test['id']\nsub['cuisine'] = y_pred_str\n\nsub.to_csv(\"submission.csv\", index = None)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}