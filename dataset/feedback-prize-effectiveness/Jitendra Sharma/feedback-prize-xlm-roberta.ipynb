{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q pandarallel\n!pip install -q spacy \n!pip install -q spacy_cld\n!pip install -q pyspellchecker\n!python -m spacy download xx_ent_wiki_sm > /dev/null","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-20T02:43:42.992977Z","iopub.execute_input":"2022-06-20T02:43:42.99358Z","iopub.status.idle":"2022-06-20T02:44:54.305153Z","shell.execute_reply.started":"2022-06-20T02:43:42.993446Z","shell.execute_reply":"2022-06-20T02:44:54.304142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\nimport time\nimport random\n\nimport numpy as np\nimport pandas as pd\n\nimport spacy\nfrom spacy_cld import LanguageDetector\nimport xx_ent_wiki_sm\n\nfrom spellchecker import SpellChecker\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport tqdm\nfrom tqdm.notebook import tqdm\ntqdm.pandas()\n\nimport re\nimport nltk\n\nfrom pandarallel import pandarallel\npandarallel.initialize(progress_bar=True)\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom kaggle_datasets import KaggleDatasets\nimport transformers\nfrom transformers import TFAutoModel, AutoTokenizer\nfrom tqdm.notebook import tqdm\nfrom tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors\n\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\n\nfrom wordcloud import WordCloud, STOPWORDS\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, CSVLogger\nfrom tensorflow.keras.layers import Dense, Input, Dropout\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tokenizers import BertWordPieceTokenizer\nfrom colorama import Fore, Back, Style, init\nimport plotly.graph_objects as go\n\nfrom tensorflow.keras.layers import (Dense, Input, LSTM, Bidirectional, Activation, Conv1D, GRU,\n                          Embedding, Flatten, Dropout, Add, concatenate, MaxPooling1D,\n                         GlobalAveragePooling1D,  GlobalMaxPooling1D, GlobalMaxPool1D,\n                        SpatialDropout1D)\n\nfrom tensorflow.keras import (initializers, regularizers, constraints, \n                              optimizers, layers, callbacks)\nimport seaborn as sns\nsns.set(style=\"darkgrid\")","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:44:54.308023Z","iopub.execute_input":"2022-06-20T02:44:54.308364Z","iopub.status.idle":"2022-06-20T02:45:03.543977Z","shell.execute_reply.started":"2022-06-20T02:44:54.308322Z","shell.execute_reply":"2022-06-20T02:45:03.543364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TPU configuration","metadata":{}},{"cell_type":"code","source":"# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:45:03.545053Z","iopub.execute_input":"2022-06-20T02:45:03.545414Z","iopub.status.idle":"2022-06-20T02:45:10.215668Z","shell.execute_reply.started":"2022-06-20T02:45:03.545387Z","shell.execute_reply":"2022-06-20T02:45:10.214645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Configuration","metadata":{}},{"cell_type":"code","source":"AUTO = tf.data.experimental.AUTOTUNE\n\n# Data access\nGCS_DS_PATH = KaggleDatasets().get_gcs_path()\n\n# Configuration\nEPOCHS     = 50\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\nMAX_LEN    = 512\nMODEL      = 'jplu/tf-xlm-roberta-large'","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:45:10.217041Z","iopub.execute_input":"2022-06-20T02:45:10.217456Z","iopub.status.idle":"2022-06-20T02:45:10.681152Z","shell.execute_reply.started":"2022-06-20T02:45:10.217418Z","shell.execute_reply":"2022-06-20T02:45:10.680277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"dataset_tr = pd.read_csv('/kaggle/input/feedback-prize-effectiveness/train.csv')\ndataset_te = pd.read_csv('/kaggle/input/feedback-prize-effectiveness/test.csv')\nprint(dataset_tr.shape)\nprint(dataset_te.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:45:10.684613Z","iopub.execute_input":"2022-06-20T02:45:10.685014Z","iopub.status.idle":"2022-06-20T02:45:11.109831Z","shell.execute_reply.started":"2022-06-20T02:45:10.684978Z","shell.execute_reply":"2022-06-20T02:45:11.108866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Concate feedback text in the train dataset**","metadata":{}},{"cell_type":"code","source":"dataset_tr['text'] = dataset_tr['essay_id'].apply(lambda x: \\\n                                    open(f'/kaggle/input/feedback-prize-effectiveness/train/{x}.txt').read())\ndataset_tr.head(2)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:45:11.11103Z","iopub.execute_input":"2022-06-20T02:45:11.111253Z","iopub.status.idle":"2022-06-20T02:45:47.624838Z","shell.execute_reply.started":"2022-06-20T02:45:11.111227Z","shell.execute_reply":"2022-06-20T02:45:47.623956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Map the labels to Numerical Category**","metadata":{}},{"cell_type":"code","source":"effectiveness_map    = {\"Ineffective\":0, \"Adequate\":1,\"Effective\":2}\ndataset_tr[\"target\"] = dataset_tr[\"discourse_effectiveness\"].map(effectiveness_map)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:45:47.626081Z","iopub.execute_input":"2022-06-20T02:45:47.626329Z","iopub.status.idle":"2022-06-20T02:45:47.63853Z","shell.execute_reply.started":"2022-06-20T02:45:47.6263Z","shell.execute_reply":"2022-06-20T02:45:47.637835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tokenizer","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(MODEL)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:45:47.639699Z","iopub.execute_input":"2022-06-20T02:45:47.640491Z","iopub.status.idle":"2022-06-20T02:45:51.5616Z","shell.execute_reply.started":"2022-06-20T02:45:47.640431Z","shell.execute_reply":"2022-06-20T02:45:51.560497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def regular_encode(texts, tokenizer, maxlen=512):\n    enc_di = tokenizer.batch_encode_plus(\n        texts, \n        return_attention_masks = False, \n        return_token_type_ids  = False,\n        pad_to_max_length      = True,\n        max_length             = maxlen,\n    )\n\n    return np.array(enc_di['input_ids'])\n\ndef encode_data(df, ids, masks, tokenizer, maxlen=512):\n    for i, text in tqdm(enumerate(df['text'])):\n        tokenized_text = tokenizer.encode_plus(\n            text,\n            max_length=maxlen, \n            truncation=True, \n            padding='max_length', \n            add_special_tokens=True,\n            return_tensors='tf'\n        )\n        ids[i, :]   = tokenized_text.input_ids\n        masks[i, :] = tokenized_text.attention_mask\n    return ids, masks","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:45:51.563749Z","iopub.execute_input":"2022-06-20T02:45:51.564171Z","iopub.status.idle":"2022-06-20T02:45:51.572143Z","shell.execute_reply.started":"2022-06-20T02:45:51.56414Z","shell.execute_reply":"2022-06-20T02:45:51.570987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_tr['text'].shape","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:45:51.573692Z","iopub.execute_input":"2022-06-20T02:45:51.574019Z","iopub.status.idle":"2022-06-20T02:45:51.590431Z","shell.execute_reply.started":"2022-06-20T02:45:51.573987Z","shell.execute_reply":"2022-06-20T02:45:51.589667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# x_train = encode_data(dataset_tr['text'], tokenizer, maxlen=MAX_LEN)\n# x_train.shape\n\nX_input_ids  = np.zeros((len(dataset_tr), MAX_LEN))\nX_attn_masks = np.zeros((len(dataset_tr), MAX_LEN))\n\nX_input_ids, X_attn_masks = encode_data(dataset_tr, X_input_ids, X_attn_masks, tokenizer, maxlen=MAX_LEN)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:45:51.591525Z","iopub.execute_input":"2022-06-20T02:45:51.592236Z","iopub.status.idle":"2022-06-20T02:48:29.724986Z","shell.execute_reply.started":"2022-06-20T02:45:51.592202Z","shell.execute_reply":"2022-06-20T02:48:29.72422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare labels","metadata":{}},{"cell_type":"code","source":"labels = np.zeros((len(dataset_tr), 3))\nlabels[np.arange(len(dataset_tr)), dataset_tr['target'].values] = 1\n\nprint(labels.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:48:29.72628Z","iopub.execute_input":"2022-06-20T02:48:29.726522Z","iopub.status.idle":"2022-06-20T02:48:29.733389Z","shell.execute_reply.started":"2022-06-20T02:48:29.726494Z","shell.execute_reply":"2022-06-20T02:48:29.732463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def DatasetMapFunction(input_ids, attn_masks, labels):\n#     return {\n#         \\\n#         'input_ids': input_ids,\n#         'attention_mask': attn_masks\n#     }, labels\n\n# dataset = tf.data.Dataset.from_tensor_slices((X_input_ids, X_attn_masks, labels))\n# dataset = dataset.map(DatasetMapFunction)     # converting to required format for tensorflow dataset\n# dataset = dataset.shuffle(10000).batch(16, drop_remainder=True) # batch size, drop any left out tensor","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:48:29.734794Z","iopub.execute_input":"2022-06-20T02:48:29.735097Z","iopub.status.idle":"2022-06-20T02:48:29.744238Z","shell.execute_reply.started":"2022-06-20T02:48:29.73506Z","shell.execute_reply":"2022-06-20T02:48:29.743483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Split dataset","metadata":{}},{"cell_type":"code","source":"# p = 0.8\n# # for each 16 batch of data we will have len(df)//16 samples, take 80% of that for train.\n# train_size    = int((len(dataset_tr)//16)*p)\n# train_dataset = dataset.take(train_size)\n# val_dataset   = dataset.skip(train_size)\n\nX_train, X_test, y_train, y_test = train_test_split(X_input_ids, labels, test_size = 0.10, random_state = 0)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:48:30.796569Z","iopub.execute_input":"2022-06-20T02:48:30.796948Z","iopub.status.idle":"2022-06-20T02:48:30.86015Z","shell.execute_reply.started":"2022-06-20T02:48:30.796898Z","shell.execute_reply":"2022-06-20T02:48:30.859372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((X_train, y_train))\n    .repeat()\n    .shuffle(2048)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((X_test, y_test))\n    .batch(BATCH_SIZE)\n    .cache()\n    .prefetch(AUTO)\n)\n\n# test_dataset = (\n#     tf.data.Dataset\n#     .from_tensor_slices(x_test)\n#     .batch(BATCH_SIZE)\n# )","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:48:30.861378Z","iopub.execute_input":"2022-06-20T02:48:30.861607Z","iopub.status.idle":"2022-06-20T02:48:31.481256Z","shell.execute_reply.started":"2022-06-20T02:48:30.861581Z","shell.execute_reply":"2022-06-20T02:48:31.48046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"def build_model(transformer, max_len=512):\n    input_word_ids  = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token       = sequence_output[:, 0, :]\n    # 0 refers to output for the [CLS] token OR [all sentences,token(0 for CLS),hiddne units output]\n    out = Dense(3, activation='softmax')(cls_token)\n    \n    model = Model(inputs=input_word_ids, outputs=out)\n    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:48:31.482374Z","iopub.execute_input":"2022-06-20T02:48:31.482593Z","iopub.status.idle":"2022-06-20T02:48:31.489278Z","shell.execute_reply.started":"2022-06-20T02:48:31.482569Z","shell.execute_reply":"2022-06-20T02:48:31.488422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nwith strategy.scope():\n    transformer_layer = TFAutoModel.from_pretrained(MODEL)\n    model             = build_model(transformer_layer, max_len=MAX_LEN)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:48:31.490476Z","iopub.execute_input":"2022-06-20T02:48:31.49069Z","iopub.status.idle":"2022-06-20T02:51:17.15269Z","shell.execute_reply.started":"2022-06-20T02:48:31.490665Z","shell.execute_reply":"2022-06-20T02:51:17.152016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# input_ids  = tf.keras.layers.Input(shape=(512,), name='input_ids',      dtype='int32')\n# attn_masks = tf.keras.layers.Input(shape=(512,), name='attention_mask', dtype='int32')","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:51:17.154427Z","iopub.execute_input":"2022-06-20T02:51:17.154939Z","iopub.status.idle":"2022-06-20T02:51:17.159194Z","shell.execute_reply.started":"2022-06-20T02:51:17.154881Z","shell.execute_reply":"2022-06-20T02:51:17.158334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train Model","metadata":{}},{"cell_type":"code","source":"import time\nn_steps = X_train.shape[0] // BATCH_SIZE\n\n\nstart_time    =  time.time()\n\ntrain_history = model.fit(\n    train_dataset,\n    steps_per_epoch=n_steps,\n    validation_data=valid_dataset,\n    epochs=EPOCHS\n)\n\ntraining_time = time.time() - start_time\nprint('Training time: ', training_time, ' sec')","metadata":{"execution":{"iopub.status.busy":"2022-06-20T02:51:17.160613Z","iopub.execute_input":"2022-06-20T02:51:17.160843Z","iopub.status.idle":"2022-06-20T02:57:58.258817Z","shell.execute_reply.started":"2022-06-20T02:51:17.160817Z","shell.execute_reply":"2022-06-20T02:57:58.2577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.save('XLM_Roberta')\n# model.save('XLM_Roberta.h5')","metadata":{"execution":{"iopub.status.busy":"2022-06-20T03:01:26.300508Z","iopub.execute_input":"2022-06-20T03:01:26.300898Z","iopub.status.idle":"2022-06-20T03:01:26.305029Z","shell.execute_reply.started":"2022-06-20T03:01:26.300864Z","shell.execute_reply":"2022-06-20T03:01:26.304405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot training & validation accuracy values\nplt.plot(train_history.history['accuracy'])\nplt.plot(train_history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T03:01:41.844886Z","iopub.execute_input":"2022-06-20T03:01:41.845222Z","iopub.status.idle":"2022-06-20T03:01:42.152326Z","shell.execute_reply.started":"2022-06-20T03:01:41.845183Z","shell.execute_reply":"2022-06-20T03:01:42.151338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_steps = X_test.shape[0] // BATCH_SIZE\ntrain_history_2 = model.fit(\n    valid_dataset.repeat(),\n    steps_per_epoch=n_steps,\n    epochs=EPOCHS*2\n)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T03:01:43.48607Z","iopub.execute_input":"2022-06-20T03:01:43.486921Z","iopub.status.idle":"2022-06-20T03:03:57.910453Z","shell.execute_reply.started":"2022-06-20T03:01:43.486873Z","shell.execute_reply":"2022-06-20T03:03:57.909536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot training & validation accuracy values\nplt.plot(train_history_2.history['accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train'], loc='upper left')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T03:03:57.912537Z","iopub.execute_input":"2022-06-20T03:03:57.913471Z","iopub.status.idle":"2022-06-20T03:03:58.195854Z","shell.execute_reply.started":"2022-06-20T03:03:57.913423Z","shell.execute_reply":"2022-06-20T03:03:58.19529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hist_df1 = pd.DataFrame(train_history.history)\nhist_df2 = pd.DataFrame(train_history_2.history) \n\n# or save to csv: \nhist_file1 = 'train_history.csv'\nwith open(hist_file1, mode='w') as f:\n    hist_df1.to_csv(f)\n\nhist_file2 = 'valid_history.csv'\nwith open(hist_file2, mode='w') as f:\n    hist_df2.to_csv(f)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T03:03:58.196803Z","iopub.execute_input":"2022-06-20T03:03:58.197032Z","iopub.status.idle":"2022-06-20T03:03:58.209641Z","shell.execute_reply.started":"2022-06-20T03:03:58.197005Z","shell.execute_reply":"2022-06-20T03:03:58.208941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predictions","metadata":{}},{"cell_type":"code","source":"dataset_te['discourse_type'].shape","metadata":{"execution":{"iopub.status.busy":"2022-06-20T03:08:54.393398Z","iopub.execute_input":"2022-06-20T03:08:54.393761Z","iopub.status.idle":"2022-06-20T03:08:54.401082Z","shell.execute_reply.started":"2022-06-20T03:08:54.393721Z","shell.execute_reply":"2022-06-20T03:08:54.400161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test_input_ids  = np.zeros((len(dataset_te), 512))\nX_test_attn_masks = np.zeros((len(dataset_te), 512))\nfor i, text in enumerate(dataset_te['discourse_type']):\n        tokenized_text = tokenizer.encode_plus(\n            text,\n            max_length=MAX_LEN, \n            truncation=True, \n            padding='max_length', \n            add_special_tokens=True,\n            return_tensors='tf'\n        )\n        X_test_input_ids[i, :]  = tokenized_text.input_ids\n        X_test_attn_masks[i, :] = tokenized_text.attention_mask\n\n\npred_labels = model.predict(X_test_input_ids)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T03:09:13.179925Z","iopub.execute_input":"2022-06-20T03:09:13.180775Z","iopub.status.idle":"2022-06-20T03:09:31.011882Z","shell.execute_reply.started":"2022-06-20T03:09:13.180739Z","shell.execute_reply":"2022-06-20T03:09:31.010979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"sample_submission = pd.read_csv('/kaggle/input/feedback-prize-effectiveness/sample_submission.csv')\n\nsample_submission['discourse_id'] = dataset_te['discourse_id']\nsample_submission['Ineffective']  = pred_labels[:,0]\nsample_submission['Adequate']     = pred_labels[:,1]\nsample_submission['Effective']    = pred_labels[:,2]\nsample_submission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T03:10:20.267155Z","iopub.execute_input":"2022-06-20T03:10:20.267981Z","iopub.status.idle":"2022-06-20T03:10:20.289893Z","shell.execute_reply.started":"2022-06-20T03:10:20.267936Z","shell.execute_reply":"2022-06-20T03:10:20.288892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission","metadata":{"execution":{"iopub.status.busy":"2022-06-20T03:10:22.288643Z","iopub.execute_input":"2022-06-20T03:10:22.288967Z","iopub.status.idle":"2022-06-20T03:10:22.302341Z","shell.execute_reply.started":"2022-06-20T03:10:22.288931Z","shell.execute_reply":"2022-06-20T03:10:22.301369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}