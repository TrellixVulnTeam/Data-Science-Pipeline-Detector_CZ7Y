{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## **Introduction**\nThis notebook is a notebook that allows DeBERTa-v3-large to be trained on kaggle resources in large batch sizes.  \nThe following optimizations can be applied to increase the batch size on a limited number of GPUs.  \n* Freezing\n* Automatic Mixed Precision\n* 8-bit Optimizers\n* Gradient Checkpointing\n* Uniform Dynamic Padding\n\nWith this notebook setup, even with batch_size=8, the GPU usage is about 8 GB.  \n\n**Acknowledgements**  \nThis notebook is based on two great public notebooks.   \nWe would like to thank the authors.  \nIt is a great learning experience, and we encourage anyone interested to take a look at these notebooks.  \n* [Optimization approaches for Transformers](https://www.kaggle.com/code/vad13irt/optimization-approaches-for-transformers/notebook)\n* [Tez for feedback v2.0](https://www.kaggle.com/code/abhishek/tez-for-feedback-v2-0)","metadata":{}},{"cell_type":"markdown","source":"## **Install Liblarys**","metadata":{}},{"cell_type":"code","source":"!pip install tez\n!pip install AttrDict\n!pip install -q bitsandbytes-cuda110","metadata":{"execution":{"iopub.status.busy":"2022-06-24T05:23:37.494458Z","iopub.execute_input":"2022-06-24T05:23:37.494887Z","iopub.status.idle":"2022-06-24T05:24:07.00441Z","shell.execute_reply.started":"2022-06-24T05:23:37.4948Z","shell.execute_reply":"2022-06-24T05:24:07.003136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Import Libs**","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nfrom attrdict import AttrDict\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nimport torch\nimport torch.nn as nn\nfrom torch.utils.checkpoint import checkpoint\n\nfrom joblib import Parallel, delayed\nfrom sklearn import metrics\nfrom torch.nn import functional as F\nfrom tqdm import tqdm\nfrom transformers import (\n    AdamW,\n    AutoConfig,\n    AutoModel,\n    AutoTokenizer,\n    get_cosine_schedule_with_warmup,\n    get_linear_schedule_with_warmup,\n)\n\nfrom tez import Tez, TezConfig\nfrom tez.callbacks import EarlyStopping\n\nimport bitsandbytes as bnb","metadata":{"execution":{"iopub.status.busy":"2022-06-24T05:24:07.008347Z","iopub.execute_input":"2022-06-24T05:24:07.008636Z","iopub.status.idle":"2022-06-24T05:24:15.096597Z","shell.execute_reply.started":"2022-06-24T05:24:07.008606Z","shell.execute_reply":"2022-06-24T05:24:15.095684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Configs**","metadata":{}},{"cell_type":"code","source":"cfg = {\n    # Model Configs\n    \"model\": \"microsoft/deberta-v3-large\",\n    \"max_len\": 512,\n    \n    # Train Configs\n    \"fold_num\": 5,\n    \"val_fold\": 0,\n    \"lr\": 3e-6,\n    \"batch_size\": 8,\n    \"valid_batch_size\": 32,\n    \"epochs\": 1, # Set to 1 because it is a demo\n    \"accumulation_steps\": 1,\n    \"val_steps\": 375,\n    \n    # GPU Optimize Settings\n    \"gpu_optimize_config\": {\n        \"fp16\": True,\n        \"freezing\": True,\n        \"optim8bit\": True,\n        \"gradient_checkpoint\": True\n    },\n    \n    # Path\n    \"input\": \"/kaggle/input/feedback-prize-effectiveness\",\n    \"output\": \"/kaggle/working\"\n}\ncfg = AttrDict(cfg)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T05:24:15.097874Z","iopub.execute_input":"2022-06-24T05:24:15.098496Z","iopub.status.idle":"2022-06-24T05:24:15.106414Z","shell.execute_reply.started":"2022-06-24T05:24:15.098459Z","shell.execute_reply":"2022-06-24T05:24:15.104701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LABEL_MAPPING = {\"Ineffective\": 0, \"Adequate\": 1, \"Effective\": 2}","metadata":{"execution":{"iopub.status.busy":"2022-06-24T05:24:15.109379Z","iopub.execute_input":"2022-06-24T05:24:15.11037Z","iopub.status.idle":"2022-06-24T05:24:15.115547Z","shell.execute_reply.started":"2022-06-24T05:24:15.110334Z","shell.execute_reply":"2022-06-24T05:24:15.114525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Helper Functions**","metadata":{}},{"cell_type":"code","source":"def seed_everything(seed: int):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--config\", type=str, required=True)\n    parser.add_argument(\"--fold\", type=int, required=False, default=0)\n    #parser.add_argument(\"--model\", type=str, required=False, default=\"microsoft/deberta-base\")\n    #parser.add_argument(\"--lr\", type=float, required=False, default=3e-5)\n    parser.add_argument(\"--output\", type=str, default=\".\", required=False)\n    #parser.add_argument(\"--input\", type=str, default=\"../input\", required=False)\n    #parser.add_argument(\"--max_len\", type=int, default=1024, required=False)\n    #parser.add_argument(\"--batch_size\", type=int, default=2, required=False)\n    #parser.add_argument(\"--valid_batch_size\", type=int, default=16, required=False)\n    #parser.add_argument(\"--epochs\", type=int, default=5, required=False)\n    #parser.add_argument(\"--accumulation_steps\", type=int, default=1, required=False)\n    #parser.add_argument(\"--predict\", action=\"store_true\", required=False)\n    return parser.parse_args()\n\n\ndef _prepare_training_data_helper(cfg, tokenizer, df, is_train):\n    training_samples = []\n    for _, row in tqdm(df.iterrows(), total=len(df)):\n        idx = row[\"essay_id\"]\n        discourse_text = row[\"discourse_text\"]\n        discourse_type = row[\"discourse_type\"]\n\n        if is_train:\n            filename = os.path.join(cfg.input, \"train\", idx + \".txt\")\n        else:\n            filename = os.path.join(cfg.input, \"test\", idx + \".txt\")\n\n        with open(filename, \"r\") as f:\n            text = f.read()\n\n        encoded_text = tokenizer.encode_plus(\n            discourse_type + \" \" + discourse_text,\n            text,\n            add_special_tokens=False,\n        )\n        input_ids = encoded_text[\"input_ids\"]\n\n        sample = {\n            \"discourse_id\": row[\"discourse_id\"],\n            \"input_ids\": input_ids,\n            # \"discourse_text\": discourse_text,\n            # \"essay_text\": text,\n            # \"mask\": encoded_text[\"attention_mask\"],\n        }\n\n        if \"token_type_ids\" in encoded_text:\n            sample[\"token_type_ids\"] = encoded_text[\"token_type_ids\"]\n\n        label = row[\"discourse_effectiveness\"]\n\n        sample[\"label\"] = LABEL_MAPPING[label]\n\n        training_samples.append(sample)\n    return training_samples\n\n\ndef prepare_training_data(df, tokenizer, cfg, num_jobs, is_train):\n    training_samples = []\n\n    df_splits = np.array_split(df, num_jobs)\n\n    results = Parallel(n_jobs=num_jobs, backend=\"multiprocessing\")(\n        delayed(_prepare_training_data_helper)(cfg, tokenizer, df, is_train) for df in df_splits\n    )\n    for result in results:\n        training_samples.extend(result)\n\n    return training_samples","metadata":{"execution":{"iopub.status.busy":"2022-06-24T05:24:15.117221Z","iopub.execute_input":"2022-06-24T05:24:15.11763Z","iopub.status.idle":"2022-06-24T05:24:15.136854Z","shell.execute_reply.started":"2022-06-24T05:24:15.117597Z","shell.execute_reply":"2022-06-24T05:24:15.135968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Dataset**","metadata":{}},{"cell_type":"code","source":"class FeedbackDataset:\n    def __init__(self, samples, cfg, tokenizer):\n        self.samples = samples\n        self.cfg = cfg\n        self.tokenizer = tokenizer\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        ids = self.samples[idx][\"input_ids\"]\n        label = self.samples[idx][\"label\"]\n\n        input_ids = [self.tokenizer.cls_token_id] + ids\n\n        if len(input_ids) > self.cfg.max_len - 1:\n            input_ids = input_ids[: self.cfg.max_len - 1]\n\n        input_ids = input_ids + [self.tokenizer.sep_token_id]\n        mask = [1] * len(input_ids)\n\n        return {\n            \"ids\": input_ids,\n            \"mask\": mask,\n            # \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n            \"targets\": label,\n        }\n\n\nclass Collate:\n    def __init__(self, tokenizer, cfg):\n        self.tokenizer = tokenizer\n        self.cfg = cfg\n\n    def __call__(self, batch):\n        output = dict()\n        output[\"ids\"] = [sample[\"ids\"] for sample in batch]\n        output[\"mask\"] = [sample[\"mask\"] for sample in batch]\n        output[\"targets\"] = [sample[\"targets\"] for sample in batch]\n\n        # calculate max token length of this batch\n        batch_max = max([len(ids) for ids in output[\"ids\"]])\n\n        # add padding\n        if self.tokenizer.padding_side == \"right\":\n            output[\"ids\"] = [s + (batch_max - len(s)) * [self.tokenizer.pad_token_id] for s in output[\"ids\"]]\n            output[\"mask\"] = [s + (batch_max - len(s)) * [0] for s in output[\"mask\"]]\n        else:\n            output[\"ids\"] = [(batch_max - len(s)) * [self.tokenizer.pad_token_id] + s for s in output[\"ids\"]]\n            output[\"mask\"] = [(batch_max - len(s)) * [0] + s for s in output[\"mask\"]]\n\n        # convert to tensors\n        output[\"ids\"] = torch.tensor(output[\"ids\"], dtype=torch.long)\n        output[\"mask\"] = torch.tensor(output[\"mask\"], dtype=torch.long)\n        output[\"targets\"] = torch.tensor(output[\"targets\"], dtype=torch.long)\n\n        return output","metadata":{"execution":{"iopub.status.busy":"2022-06-24T05:24:15.139812Z","iopub.execute_input":"2022-06-24T05:24:15.140582Z","iopub.status.idle":"2022-06-24T05:24:15.155982Z","shell.execute_reply.started":"2022-06-24T05:24:15.14053Z","shell.execute_reply":"2022-06-24T05:24:15.15508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Model**","metadata":{}},{"cell_type":"code","source":"def freeze(module):\n    \"\"\"\n    Freezes module's parameters.\n    \"\"\"\n    \n    for parameter in module.parameters():\n        parameter.requires_grad = False\n        \n\ndef set_embedding_parameters_bits(embeddings_path, optim_bits=32):\n    \"\"\"\n    https://github.com/huggingface/transformers/issues/14819#issuecomment-1003427930\n    \"\"\"\n    \n    embedding_types = (\"word\", \"position\", \"token_type\")\n    for embedding_type in embedding_types:\n        attr_name = f\"{embedding_type}_embeddings\"\n        \n        if hasattr(embeddings_path, attr_name): \n            bnb.optim.GlobalOptimManager.get_instance().register_module_override(\n                getattr(embeddings_path, attr_name), 'weight', {'optim_bits': optim_bits}\n            )","metadata":{"execution":{"iopub.status.busy":"2022-06-24T05:24:15.157361Z","iopub.execute_input":"2022-06-24T05:24:15.157716Z","iopub.status.idle":"2022-06-24T05:24:15.170887Z","shell.execute_reply.started":"2022-06-24T05:24:15.15768Z","shell.execute_reply":"2022-06-24T05:24:15.16988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MeanPooling(nn.Module):\n    def __init__(self):\n        super(MeanPooling, self).__init__()\n\n    def forward(self, last_hidden_state, attention_mask):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings / sum_mask\n        return mean_embeddings","metadata":{"execution":{"iopub.status.busy":"2022-06-24T05:24:15.172257Z","iopub.execute_input":"2022-06-24T05:24:15.172692Z","iopub.status.idle":"2022-06-24T05:24:15.18411Z","shell.execute_reply.started":"2022-06-24T05:24:15.172656Z","shell.execute_reply":"2022-06-24T05:24:15.183296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FeedbackModel(nn.Module):\n    def __init__(self, model_name, num_train_steps, learning_rate, num_labels, steps_per_epoch, gpu_optimize_config):\n        super().__init__()\n        self.learning_rate = learning_rate\n        self.model_name = model_name\n        self.num_train_steps = num_train_steps\n        self.num_labels = num_labels\n        self.steps_per_epoch = steps_per_epoch\n        self.gpu_optimize_config = gpu_optimize_config\n\n        hidden_dropout_prob: float = 0.1\n        layer_norm_eps: float = 1e-7\n\n        model_config = AutoConfig.from_pretrained(model_name)\n\n        model_config.update(\n            {\n                \"output_hidden_states\": True,\n                \"hidden_dropout_prob\": hidden_dropout_prob,\n                \"layer_norm_eps\": layer_norm_eps,\n                \"add_pooling_layer\": False,\n                \"num_labels\": self.num_labels,\n            }\n        )\n        self.transformer = AutoModel.from_pretrained(model_name, config=model_config)\n        self.dropout = nn.Dropout(model_config.hidden_dropout_prob)\n        self.dropout1 = nn.Dropout(0.1)\n        self.dropout2 = nn.Dropout(0.2)\n        self.dropout3 = nn.Dropout(0.3)\n        self.dropout4 = nn.Dropout(0.4)\n        self.dropout5 = nn.Dropout(0.5)\n        self.pooler = MeanPooling()\n        self.output = nn.Linear(model_config.hidden_size, self.num_labels)\n\n        # Freeze\n        if self.gpu_optimize_config.freezing:\n            freeze(self.transformer.embeddings)\n            freeze(self.transformer.encoder.layer[:2])\n\n        # Gradient Checkpointing\n        if self.gpu_optimize_config.gradient_checkpoint:\n            self.transformer.gradient_checkpointing_enable()  \n\n\n    def optimizer_scheduler(self):\n        param_optimizer = list(self.named_parameters())\n        no_decay = [\"bias\", \"LayerNorm.weight\"]\n        optimizer_parameters = [\n            {\n                \"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay) and p.requires_grad],\n                \"weight_decay\": 0.001,\n            },\n            {\n                \"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay) and p.requires_grad],\n                \"weight_decay\": 0.0,\n            },\n        ]\n        opt = AdamW(optimizer_parameters, lr=self.learning_rate)\n        if self.gpu_optimize_config.gradient_checkpoint:\n            opt = bnb.optim.AdamW(optimizer_parameters, lr=self.learning_rate, optim_bits=8)\n            #set_embedding_parameters_bits(embeddings_path=self.transformer.embeddings)\n        sch = get_linear_schedule_with_warmup(\n            opt,\n            num_warmup_steps=0,\n            num_training_steps=self.num_train_steps,\n            last_epoch=-1,\n        )\n        return opt, sch\n\n    def loss(self, outputs, targets):\n        loss_fct = nn.CrossEntropyLoss()\n        loss = loss_fct(outputs, targets)\n        return loss\n\n    def monitor_metrics(self, outputs, targets):\n        device = targets.get_device()\n        mll = metrics.log_loss(\n            targets.cpu().detach().numpy(),\n            outputs.cpu().detach().numpy(),\n            labels=[0, 1, 2],\n        )\n        return {\"mll\": torch.tensor(mll, device=device)}\n\n    def forward(self, ids, mask, token_type_ids=None, targets=None):\n\n        if token_type_ids:\n            transformer_out = self.transformer(ids, mask, token_type_ids)\n        else:\n            transformer_out = self.transformer(ids, mask)\n        sequence_output = transformer_out.last_hidden_state\n        sequence_output = self.pooler(sequence_output, mask)\n        sequence_output = self.dropout(sequence_output)\n\n        logits1 = self.output(self.dropout1(sequence_output))\n        logits2 = self.output(self.dropout2(sequence_output))\n        logits3 = self.output(self.dropout3(sequence_output))\n        logits4 = self.output(self.dropout4(sequence_output))\n        logits5 = self.output(self.dropout5(sequence_output))\n\n        logits = (logits1 + logits2 + logits3 + logits4 + logits5) / 5\n        logits = torch.softmax(logits, dim=-1)\n        loss = 0\n\n        if targets is not None:\n            loss1 = self.loss(logits1, targets)\n            loss2 = self.loss(logits2, targets)\n            loss3 = self.loss(logits3, targets)\n            loss4 = self.loss(logits4, targets)\n            loss5 = self.loss(logits5, targets)\n            loss = (loss1 + loss2 + loss3 + loss4 + loss5) / 5\n            metric = self.monitor_metrics(logits, targets)\n            return logits, loss, metric\n\n        return logits, loss, {}","metadata":{"execution":{"iopub.status.busy":"2022-06-24T05:24:15.185509Z","iopub.execute_input":"2022-06-24T05:24:15.186051Z","iopub.status.idle":"2022-06-24T05:24:15.21049Z","shell.execute_reply.started":"2022-06-24T05:24:15.186004Z","shell.execute_reply":"2022-06-24T05:24:15.209552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Train**","metadata":{}},{"cell_type":"code","source":"# Initialize\nNUM_JOBS = 12\nseed_everything(42)\nos.makedirs(cfg.output, exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T05:24:15.214201Z","iopub.execute_input":"2022-06-24T05:24:15.214475Z","iopub.status.idle":"2022-06-24T05:24:15.227894Z","shell.execute_reply.started":"2022-06-24T05:24:15.214451Z","shell.execute_reply":"2022-06-24T05:24:15.227015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create fold\ndf = pd.read_csv(os.path.join(cfg.input, \"train.csv\"))\ngkf = GroupKFold(n_splits=cfg.fold_num)\nfor fold, ( _, val_) in enumerate(gkf.split(X=df, groups=df.essay_id)):\n    df.loc[val_ , \"kfold\"] = int(fold)\n\ndf[\"kfold\"] = df[\"kfold\"].astype(int)\ndf.groupby('kfold')['discourse_effectiveness'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-06-24T05:24:15.229355Z","iopub.execute_input":"2022-06-24T05:24:15.229709Z","iopub.status.idle":"2022-06-24T05:24:15.588338Z","shell.execute_reply.started":"2022-06-24T05:24:15.229674Z","shell.execute_reply":"2022-06-24T05:24:15.5875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# DataSet Preparation\ntrain_df = df[df[\"kfold\"] != cfg.val_fold].reset_index(drop=True)\nvalid_df = df[df[\"kfold\"] == cfg.val_fold].reset_index(drop=True)\n\ntokenizer = AutoTokenizer.from_pretrained(cfg.model, use_fast=True)\ntraining_samples = prepare_training_data(train_df, tokenizer, cfg, num_jobs=NUM_JOBS, is_train=True)\nvalid_samples = prepare_training_data(valid_df, tokenizer, cfg, num_jobs=NUM_JOBS, is_train=True)\n\ntraining_samples = list(sorted(training_samples, key=lambda d: len(d[\"input_ids\"])))\nvalid_samples = list(sorted(valid_samples, key=lambda d: len(d[\"input_ids\"])))\n\ntrain_dataset = FeedbackDataset(training_samples, cfg, tokenizer)\nvalid_dataset = FeedbackDataset(valid_samples, cfg, tokenizer)\n\nnum_train_steps = int(len(train_dataset) / cfg.batch_size / cfg.accumulation_steps * cfg.epochs)\n\ncollate_fn = Collate(tokenizer, cfg)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T05:24:15.58981Z","iopub.execute_input":"2022-06-24T05:24:15.590155Z","iopub.status.idle":"2022-06-24T05:27:21.984781Z","shell.execute_reply.started":"2022-06-24T05:24:15.590121Z","shell.execute_reply":"2022-06-24T05:27:21.983597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model Preparation\nmodel = FeedbackModel(\n    model_name=cfg.model,\n    num_train_steps=num_train_steps,\n    learning_rate=cfg.lr,\n    num_labels=3,\n    steps_per_epoch=len(train_dataset) / cfg.batch_size,\n    gpu_optimize_config=cfg.gpu_optimize_config,\n)\nmodel = Tez(model)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T05:27:21.986557Z","iopub.execute_input":"2022-06-24T05:27:21.986952Z","iopub.status.idle":"2022-06-24T05:27:54.551168Z","shell.execute_reply.started":"2022-06-24T05:27:21.986905Z","shell.execute_reply":"2022-06-24T05:27:54.550106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training\nes = EarlyStopping(\n    monitor=\"valid_loss\",\n    model_path=os.path.join(cfg.output, f\"model_f{cfg.val_fold}.bin\"),\n    patience=5,\n    mode=\"min\",\n    delta=0.001,\n    save_weights_only=True,\n)\n\ntrain_config = TezConfig(\n    training_batch_size=cfg.batch_size,\n    validation_batch_size=cfg.valid_batch_size,\n    gradient_accumulation_steps=cfg.accumulation_steps,\n    epochs=cfg.epochs,\n    fp16=cfg.gpu_optimize_config.fp16,\n    step_scheduler_after=\"batch\",\n    val_strategy=\"batch\",\n    val_steps=cfg.val_steps,\n)\n\nmodel.fit(\n    train_dataset,\n    valid_dataset=valid_dataset,\n    train_collate_fn=collate_fn,\n    valid_collate_fn=collate_fn,\n    callbacks=[es],\n    config=train_config,\n)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T05:27:54.552538Z","iopub.execute_input":"2022-06-24T05:27:54.553305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}