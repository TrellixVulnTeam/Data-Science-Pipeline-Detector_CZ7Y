{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <h><center>Feedback Prize - Predicting Effective Arguments</center></h>\n\n<img src='https://www.incimages.com/uploaded_files/image/1920x1080/getty_506903004_200013332000928076_348061.jpg'>\n\n\n\n# <center>**NLP Based Problem: Classification + TextData.....**</center>\n\n## <center>**Goal of the Competition:**</center> \n\nThe goal of this competition is to classify argumentative elements in student writing as \"effective,\" \"adequate,\" or \"ineffective.\" You will create a model trained on data that is representative of the 6th-12th grade population in the United States in order to minimize bias. Models derived from this competition will help pave the way for students to receive enhanced feedback on their argumentative writing. With automated guidance, students can complete more assignments and ultimately become more confident, proficient writers.","metadata":{}},{"cell_type":"markdown","source":"## ***Continue the US pattern matching competition approach:***\n\n1. https://www.kaggle.com/code/venkatkumar001/u-s-p-p-baseline-eda-dataprep\n2. Starter1: https://www.kaggle.com/code/venkatkumar001/nlp-starter1-almost-all-basic-concept\n3. Starter2: https://www.kaggle.com/code/venkatkumar001/nlp-starter2-hf-pretrain-finetune\n4. https://www.kaggle.com/code/venkatkumar001/transformeranatomy-encoder\n\n## ***Now, This feedback price competiton onward!***\n\n1. Starter3: https://www.kaggle.com/venkatkumar001/nlpstarter3-baseline-approach\n\n\n## ***So, I am trying to build baseline Approach of Competition data***\n\n# **Steps:**\n\n### **1. Import Necessary Library**\n\n### **2. Load and analysis the data**\n\n### **3. Preprocessing**\n\n### **4. Feature selection**\n\n### **5. Build the Model**\n\n### **6. Predict Output**\n\n### **7. Generate Submission file**\n\n","metadata":{}},{"cell_type":"markdown","source":"# <center>**Import Necessary Library**</center>","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time\nimport datetime\nfrom scipy import sparse\n\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import OneHotEncoder,LabelEncoder\nfrom sklearn.metrics import log_loss\n\nfrom sklearn import svm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import BernoulliNB,MultinomialNB\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score,recall_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nfrom string import punctuation\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom tqdm import tqdm\n\n%matplotlib inline\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <center>**Load and Analysis the data**</center>","metadata":{}},{"cell_type":"code","source":"!ls '../input/feedback-prize-effectiveness'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/feedback-prize-effectiveness/train.csv')\ntest = pd.read_csv('../input/feedback-prize-effectiveness/test.csv')\nsample = pd.read_csv('../input/feedback-prize-effectiveness/sample_submission.csv')\nprint(f'Train_Shape: {train.shape},Test_Shape: {test.shape},Sample_Shape: {sample.shape}')\ndisplay(train.sample(2))\ndisplay(test.sample(2))\ndisplay(sample.sample(2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.describe(include='object')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.countplot(x='discourse_id',data=train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.countplot(train['discourse_type'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.countplot(train['discourse_effectiveness'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.countplot(train['essay_id'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.discourse_type.unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Identify the imbalance of data - discourse_type**","metadata":{}},{"cell_type":"code","source":"#identify the imbalance one\n\ndef count_target(target_list):\n    target_dict = {}\n    for x in target_list:\n        count = len(train['discourse_type'] == x)\n        dict_t = dict({x:count})\n        target_dict.update(dict_t)\n    return target_dict\n\ntarget_list = ['Lead', 'Position', 'Claim', 'Evidence', 'Counterclaim',\n       'Rebuttal', 'Concluding Statement']\ncount_target(target_list)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <center>**Preprocessing - NLTK**</center>","metadata":{}},{"cell_type":"markdown","source":"## **1. Clean the discourse_text attribute and generate clean attribute**","metadata":{}},{"cell_type":"code","source":"def cleanup_text(text):\n    words = re.sub(pattern = '[^a-zA-Z]',repl = ' ', string = text)\n    words = words.lower()\n    return words\n\ncleanup_text('VK, is beast mode in my NLP competition ')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_preprocessed = train['discourse_text'].apply(cleanup_text)\ntext_preprocessed\ntrain['text_preprocessed'] = text_preprocessed\ntrain.sample(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_preprocessed = test['discourse_text'].apply(cleanup_text)\ntest_preprocessed\ntest['text_preprocessed'] =  test_preprocessed\ntest.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **2. Map the target data**","metadata":{}},{"cell_type":"code","source":"effectiveness_map = {'Ineffective' : 0, 'Adequate':1,'Effective':2}\ntrain['target'] = train['discourse_effectiveness'].map(effectiveness_map)\ntrain.sample(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **3. Preprocessing - Apply one hot encoding in (discourse type) , TFIDF vectorize in (discourse text and cleaning text)**","metadata":{}},{"cell_type":"code","source":"tf = TfidfVectorizer(ngram_range=(1,2),norm='l2', smooth_idf=True)\ntrain_discourse_tfidf = tf.fit_transform(train[\"discourse_text\"])\ntest_discourse_tfidf = tf.transform(test[\"discourse_text\"])\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf = TfidfVectorizer(ngram_range=(1,2),norm='l2', smooth_idf=True) # Load tf another time because it will learn the new vocabulary for 'text'\ntrain_text_tfidf = tf.fit_transform(train[\"text_preprocessed\"])\ntest_text_tfidf = tf.transform(test[\"text_preprocessed\"])\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#discourse_type\nohe = OneHotEncoder()\ntrain_type_ohe =  sparse.csr_matrix(ohe.fit_transform(train[\"discourse_type\"].values.reshape(-1,1)))\ntest_type_ohe =  sparse.csr_matrix(ohe.transform(test[\"discourse_type\"].values.reshape(-1,1)))\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **4. Stack the all three preprocess data**","metadata":{}},{"cell_type":"code","source":"#Stack each vector representations \ntrain_tfidf = sparse.hstack((train_type_ohe,train_discourse_tfidf,train_text_tfidf))\ntest_tfidf = sparse.hstack((test_type_ohe,test_discourse_tfidf,test_text_tfidf))\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_tfidf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <center>**Build the Model**</center>\n\n## **1. Logistic Regression**\n\n## **2. BernoulliNB**\n\n## **3. SVM**\n\n## **4. Boosting Methods**","metadata":{}},{"cell_type":"markdown","source":"## **1. LogisticRegression - First try** \n\n<img src='https://www.statisticalaid.com/wp-content/uploads/2021/05/tempsnip2.png'>","metadata":{}},{"cell_type":"code","source":"# clf1 = LogisticRegression(max_iter=500,penalty=\"l2\",C=1.0131816333513533)\n# clf1.fit(train_tfidf, train[\"target\"].values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **2.BernoulliNB-Second try**\n\n<img src='https://i.stack.imgur.com/e3KGO.png'>","metadata":{}},{"cell_type":"code","source":"# #Model\n# clf2 = BernoulliNB()\n# clf2.fit(train_tfidf, train[\"target\"].values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **3. SVM-Third try**\n\n<img src='https://www.researchgate.net/publication/304611323/figure/fig8/AS:668377215406089@1536364954428/Classification-of-data-by-support-vector-machine-SVM.png'>","metadata":{}},{"cell_type":"code","source":"# #Model\n# clf3 = svm.SVC(decision_function_shape='ovo')\n# clf3.fit(train_tfidf, train[\"target\"].values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. **Boosting method - Make a good result**\n\n<img src='https://cdn.educba.com/academy/wp-content/uploads/2019/11/bagging-and-boosting.png'>","metadata":{}},{"cell_type":"markdown","source":"# **CatBoost-Taken Long time to run**","metadata":{}},{"cell_type":"code","source":"# from catboost import CatBoostRegressor,CatBoostClassifier\n\n# #cat\n# catpara={\n#         'learning_rate': 0.001152,\n#         \"max_depth\": 3,\n#         'random_state':42,\n#         'n_estimators':1000\n#     }\n# cat = CatBoostClassifier(**catpara).fit(train_tfidf, train[\"target\"].values,verbose=False)","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **XGBoost-Make good result lets try better one**","metadata":{}},{"cell_type":"code","source":"# from xgboost import XGBRegressor,XGBClassifier\n\n# #Model hyperparameter of XGboostRegressor\n# xgb_params = {\n#         'learning_rate': 0.03628302216953097,\n#         'subsample': 0.7875490025178,\n#         'colsample_bytree': 0.11807135201147,\n#         'max_depth': 3,\n#         'booster': 'gbtree', \n#         'reg_lambda': 0.0008746338866473539,\n#         'reg_alpha': 23.13181079976304,\n#         'random_state':40,\n#         'n_estimators':5000\n        \n        \n#     }\n    \n# model= XGBClassifier(**xgb_params,\n#                        tree_method='gpu_hist',\n#                        predictor='gpu_predictor',\n#                        gpu_id=0)\n    \n# model.fit(train_tfidf, train[\"target\"].values)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **LSTM**","metadata":{}},{"cell_type":"code","source":"#Model hyperparameter of XGboostRegressor\n#lgb parameters\nfrom lightgbm import LGBMRegressor,LGBMClassifier\nimport lightgbm as lgb\n\nparams_lgb = {\n    \"boosting_type\": \"gbdt\",\n    \"objective\": \"multiclass\",\n    'subsample': 0.95312,\n    'learning_rate': 0.001635,\n    \"max_depth\": 3,\n    'random_state':12,\n    'n_estimators':15000,\n    }\n    \n\n\nmodel1= LGBMClassifier(**params_lgb )\n    \nmodel1.fit(train_tfidf, train[\"target\"].values)\n    \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Predict output - using Boosting Model**","metadata":{}},{"cell_type":"code","source":"test_predict = model1.predict_proba(test_tfidf)\ntest_predict","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample.loc[:,\"Ineffective\"] = test_predict[:,0]\nsample.loc[:,\"Adequate\"] = test_predict[:,1]\nsample.loc[:,\"Effective\"] = test_predict[:,2]\nsample.to_csv('submission.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('successfully execute all!')\nsample.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ***Next Process Coming Soon...........Try to apply KFOLD***\n\n## **\"if you see any errors and your opinion! feel free to share with me\"**\n\ncredit of this notebook: \n\n1. https://www.kaggle.com/code/chandraprajapati/feedback-prize-logistic-regression\n2. https://www.kaggle.com/code/venkatkumar001/nlp-starter1-almost-all-basic-concept\n3. https://www.kaggle.com/code/bhavikardeshna/logisticregression-feedback-price-effectiveness\n\n\n## <center>⭐️⭐️Thanks for visiting guys⭐️⭐️</center>","metadata":{}}]}