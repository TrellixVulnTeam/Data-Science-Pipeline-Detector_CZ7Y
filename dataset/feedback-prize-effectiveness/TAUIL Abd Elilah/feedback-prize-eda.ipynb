{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Content:**\n* [Load all dependencies we need](#section-two)\n* [EDA](#section-three)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-two\"></a>\n# Load all dependencies we need","metadata":{}},{"cell_type":"code","source":"import string\nimport os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport matplotlib.pyplot as plt\nfrom plotly import graph_objs as go\nfrom collections import Counter\nimport plotly.express as px\nimport seaborn as sns\nimport scipy as sp\nimport re\nimport csv\nfrom bs4 import BeautifulSoup\nfrom nltk.corpus import stopwords\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-25T00:37:43.437745Z","iopub.execute_input":"2022-05-25T00:37:43.438315Z","iopub.status.idle":"2022-05-25T00:37:44.30629Z","shell.execute_reply.started":"2022-05-25T00:37:43.438273Z","shell.execute_reply":"2022-05-25T00:37:44.305367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-three\"></a>\n# EDA","metadata":{}},{"cell_type":"code","source":"train             = pd.read_csv('../input/feedback-prize-effectiveness/train.csv')\ntest              = pd.read_csv('../input/feedback-prize-effectiveness/test.csv')\nsample_submission = pd.read_csv('../input/feedback-prize-effectiveness/sample_submission.csv')\n\nprint(train.shape)\nprint(test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-25T00:37:44.308287Z","iopub.execute_input":"2022-05-25T00:37:44.308558Z","iopub.status.idle":"2022-05-25T00:37:44.84544Z","shell.execute_reply.started":"2022-05-25T00:37:44.308527Z","shell.execute_reply":"2022-05-25T00:37:44.844313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So We have 36765 samples in the train set and 10 samples in the test set","metadata":{}},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2022-05-25T00:37:44.846942Z","iopub.execute_input":"2022-05-25T00:37:44.847301Z","iopub.status.idle":"2022-05-25T00:37:44.881374Z","shell.execute_reply.started":"2022-05-25T00:37:44.847247Z","shell.execute_reply":"2022-05-25T00:37:44.880646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are no null Values in the test set and train set.","metadata":{}},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-25T00:37:44.883273Z","iopub.execute_input":"2022-05-25T00:37:44.883534Z","iopub.status.idle":"2022-05-25T00:37:44.90067Z","shell.execute_reply.started":"2022-05-25T00:37:44.883502Z","shell.execute_reply":"2022-05-25T00:37:44.8995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Lets look at the distribution of discourse_effectiveness in the train set**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,6))\nsns.countplot(x='discourse_effectiveness',data=train)","metadata":{"execution":{"iopub.status.busy":"2022-05-25T00:37:44.902174Z","iopub.execute_input":"2022-05-25T00:37:44.902447Z","iopub.status.idle":"2022-05-25T00:37:45.140541Z","shell.execute_reply.started":"2022-05-25T00:37:44.902414Z","shell.execute_reply":"2022-05-25T00:37:45.13976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's draw a Funnel-Chart for better visualization","metadata":{}},{"cell_type":"code","source":"temp = train.groupby('discourse_effectiveness').count()['discourse_id'].reset_index().sort_values(by='discourse_id',ascending=False)\nfig = go.Figure(go.Funnelarea(\n    text =temp.discourse_effectiveness,\n    values = temp.discourse_id,\n    title = {\"position\": \"top center\", \"text\": \"Funnel-Chart of discourse_effectiveness Distribution\"}\n    ))\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-25T00:37:45.141785Z","iopub.execute_input":"2022-05-25T00:37:45.142232Z","iopub.status.idle":"2022-05-25T00:37:45.181874Z","shell.execute_reply.started":"2022-05-25T00:37:45.142186Z","shell.execute_reply":"2022-05-25T00:37:45.181222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's look at the distribution of discourse_type in the train set","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,6))\nsns.countplot(x='discourse_type',data=train)","metadata":{"execution":{"iopub.status.busy":"2022-05-25T00:37:45.183192Z","iopub.execute_input":"2022-05-25T00:37:45.183717Z","iopub.status.idle":"2022-05-25T00:37:45.448194Z","shell.execute_reply.started":"2022-05-25T00:37:45.183681Z","shell.execute_reply":"2022-05-25T00:37:45.447256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp = train.groupby('discourse_type').count()['discourse_id'].reset_index().sort_values(by='discourse_id',ascending=False)\nfig = go.Figure(go.Funnelarea(\n    text =temp.discourse_type,\n    values = temp.discourse_id,\n    title = {\"position\": \"top center\", \"text\": \"Funnel-Chart of discourse_type Distribution\"}\n    ))\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-25T00:37:45.449925Z","iopub.execute_input":"2022-05-25T00:37:45.450462Z","iopub.status.idle":"2022-05-25T00:37:45.492893Z","shell.execute_reply.started":"2022-05-25T00:37:45.450415Z","shell.execute_reply":"2022-05-25T00:37:45.492188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So the dataset is quite imbalance.","metadata":{}},{"cell_type":"markdown","source":"Now let's concatenate txt files with csv file","metadata":{}},{"cell_type":"code","source":"%%time\nfilenames = os.listdir('../input/feedback-prize-effectiveness/train/')\ntrain['essay'] = ''\nfor file in filenames:\n    raw_html = open('../input/feedback-prize-effectiveness/train/' + file)\n    cleantext = BeautifulSoup(raw_html, \"lxml\").text \n    output = re.sub('\\s+',' ', cleantext)      # saved the result using a variable\n    train['essay'][train['essay_id'] == file[:-4]] = output","metadata":{"execution":{"iopub.status.busy":"2022-05-25T00:37:45.494249Z","iopub.execute_input":"2022-05-25T00:37:45.494658Z","iopub.status.idle":"2022-05-25T00:38:30.524251Z","shell.execute_reply.started":"2022-05-25T00:37:45.49462Z","shell.execute_reply":"2022-05-25T00:38:30.523517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-25T00:38:30.526948Z","iopub.execute_input":"2022-05-25T00:38:30.527229Z","iopub.status.idle":"2022-05-25T00:38:30.54284Z","shell.execute_reply.started":"2022-05-25T00:38:30.527197Z","shell.execute_reply":"2022-05-25T00:38:30.541724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Number of uniques values in discourse_text column: {train.discourse_text.nunique()} in train set\")\nprint(f\"Number of uniques values in essay column: {train.essay.nunique()} in train set\")","metadata":{"execution":{"iopub.status.busy":"2022-05-25T00:38:30.544328Z","iopub.execute_input":"2022-05-25T00:38:30.54507Z","iopub.status.idle":"2022-05-25T00:38:30.730999Z","shell.execute_reply.started":"2022-05-25T00:38:30.545008Z","shell.execute_reply":"2022-05-25T00:38:30.730082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So in discourse_text They are just 36765-36691 = 74 duplicate values.","metadata":{}},{"cell_type":"markdown","source":"**Now let's look at how much words we have in discourse_text and essay**","metadata":{}},{"cell_type":"code","source":"%%time\nlen_essay          = []\nlen_discourse_text = []\nfor k in range(train.shape[0]):\n    len_essay.append(len(train['essay'][k]))\n    len_discourse_text.append(len(train['discourse_text'][k]))    \n\nplt.hist(len_essay)","metadata":{"execution":{"iopub.status.busy":"2022-05-25T00:38:30.73241Z","iopub.execute_input":"2022-05-25T00:38:30.733512Z","iopub.status.idle":"2022-05-25T00:38:31.696192Z","shell.execute_reply.started":"2022-05-25T00:38:30.733465Z","shell.execute_reply":"2022-05-25T00:38:31.6952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.hist(len_discourse_text)","metadata":{"execution":{"iopub.status.busy":"2022-05-25T00:38:31.697361Z","iopub.execute_input":"2022-05-25T00:38:31.697582Z","iopub.status.idle":"2022-05-25T00:38:32.052989Z","shell.execute_reply.started":"2022-05-25T00:38:31.697555Z","shell.execute_reply":"2022-05-25T00:38:32.052106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('The shortest essay has ' +str(min(len_essay))+' words' )\nprint('The longest essay has ' +str(max(len_essay))+' words' )\nprint('The mean words in the essay is ' +str(np.mean(len_essay)))\nprint()\nprint('The shortest discourse_text has ' +str(min(len_discourse_text))+' words' )\nprint('The longest discourse_text has ' +str(max(len_discourse_text))+' words' )\nprint('The mean words in the discourse_text is ' +str(np.mean(len_discourse_text)))","metadata":{"execution":{"iopub.status.busy":"2022-05-25T00:38:32.05473Z","iopub.execute_input":"2022-05-25T00:38:32.055635Z","iopub.status.idle":"2022-05-25T00:38:32.08309Z","shell.execute_reply.started":"2022-05-25T00:38:32.055583Z","shell.execute_reply":"2022-05-25T00:38:32.082094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So I guess there are some outliers in our data that we should take car of.","metadata":{}}]}