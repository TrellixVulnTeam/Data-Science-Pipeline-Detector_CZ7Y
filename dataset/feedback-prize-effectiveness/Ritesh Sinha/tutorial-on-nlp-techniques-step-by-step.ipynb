{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Overview\nThis notebook is targeted towards application of various nlp techniques towards this competition where the we are required to **estimate** the output. Here we have input data in form of a csv file which contain various features aka columns or predictors. There is also a response variable which needs to be calculated and submitted to the competition.\n\nHere we will be looking some of the NLP techniques applicable to this scenario.\n- CountVectorizer\n- tfidfVectorizer\n\nBut before that, some visualizations are also done to help in understanding the underlying data.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport matplotlib.pyplot as plt\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom nltk.stem.snowball import SnowballStemmer\nimport seaborn as sns\nfrom wordcloud import WordCloud, STOPWORDS\nstemmer = SnowballStemmer(\"english\")\ndf_master  = pd.read_csv('/kaggle/input/feedback-prize-effectiveness/train.csv')","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-24T17:30:14.284752Z","iopub.execute_input":"2022-06-24T17:30:14.285718Z","iopub.status.idle":"2022-06-24T17:30:14.610671Z","shell.execute_reply.started":"2022-06-24T17:30:14.285677Z","shell.execute_reply":"2022-06-24T17:30:14.60971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Following are the steps which are required to be followed when a problem related to text is presented to us:\n- Basic Exploratory Analysis to understand the data distribution.\n- The first step in any scenario, is to get the data. Here, the data is stored in csv file, so we first load this.\n- In Text processing scenario, the data needs to be numericalized, so that it can be fed into various alogorithms for building the models. We will start with a basic approach, using CountVectorizer.\n- To achieve this, a special data structure called **corpus** is created.\n- Please note that there is some basic preprocessing happens for textual data. As text data is likely to be dirty because of jargons, misspellings, this step is useful and necessary.","metadata":{}},{"cell_type":"code","source":"\ncorpus = []\nfor i in range(0, len(df_master)):\n  review = re.sub('[^a-zA-Z]', ' ', df_master['discourse_text'][i])\n  review = review.lower()\n  review = review.split()\n  ps = PorterStemmer()\n  all_stopwords = stopwords.words('english')\n  all_stopwords.remove('not')\n  review = [ps.stem(word) for word in review if not word in set(all_stopwords)]\n  review = ' '.join(review)\n  corpus.append(review)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-20T14:25:57.345355Z","iopub.execute_input":"2022-06-20T14:25:57.346222Z","iopub.status.idle":"2022-06-20T14:26:39.82567Z","shell.execute_reply.started":"2022-06-20T14:25:57.346168Z","shell.execute_reply":"2022-06-20T14:26:39.824764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Basic Exploratory Analysis","metadata":{}},{"cell_type":"code","source":"# Basic exploration\nplt.figure(figsize=(12,8))\nsns.countplot(x=\"discourse_effectiveness\", data=df_master)\nplt.ylabel('Count', fontsize=12)\nplt.xlabel('discourse_effectiveness', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.title(\"Frequency of discourse_effectiveness\", fontsize=15)\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-20T14:26:39.827477Z","iopub.execute_input":"2022-06-20T14:26:39.827963Z","iopub.status.idle":"2022-06-20T14:26:40.089147Z","shell.execute_reply.started":"2022-06-20T14:26:39.827925Z","shell.execute_reply":"2022-06-20T14:26:40.087908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sw = set(STOPWORDS)\ndef show_wordcloud(data, title = None):\n    wordcloud = WordCloud(\n        background_color='white',\n        stopwords=sw,\n        max_words=200,\n        max_font_size=40, \n        scale=3,\n        random_state=1 # chosen at random by flipping a coin; it was heads\n    ).generate(str(data))\n\n    fig = plt.figure(1, figsize=(12, 12))\n    plt.axis('off')\n    if title: \n        fig.suptitle(title, fontsize=20)\n        fig.subplots_adjust(top=2.3)\n    plt.imshow(wordcloud)\n    plt.show()","metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"execution":{"iopub.status.busy":"2022-06-20T14:26:46.856133Z","iopub.execute_input":"2022-06-20T14:26:46.856587Z","iopub.status.idle":"2022-06-20T14:26:46.864458Z","shell.execute_reply.started":"2022-06-20T14:26:46.856548Z","shell.execute_reply":"2022-06-20T14:26:46.86371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Examining Wordclouds\n\nWordclouds are a convenient way to visualizing text data. They give us the a visual representation of most occuring words which may help in choosing a corpus.\nWe observe that wordclouds for three categories are very different.","metadata":{}},{"cell_type":"markdown","source":"### WordCloud for terms appearing for `Adequate` category.","metadata":{}},{"cell_type":"code","source":"title = \"Adequate\"\ndf = df_master[df_master['discourse_effectiveness'] == title]\nshow_wordcloud(df['discourse_text'], title )","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-20T14:26:52.775999Z","iopub.execute_input":"2022-06-20T14:26:52.776576Z","iopub.status.idle":"2022-06-20T14:26:53.262044Z","shell.execute_reply.started":"2022-06-20T14:26:52.776541Z","shell.execute_reply":"2022-06-20T14:26:53.260676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### WordCloud for terms appearing for `Ineffective` category.","metadata":{}},{"cell_type":"code","source":"title = \"Ineffective\"\ndf = df_master[df_master['discourse_effectiveness'] == title]\nshow_wordcloud(df['discourse_text'], title )","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-20T14:27:00.386676Z","iopub.execute_input":"2022-06-20T14:27:00.388034Z","iopub.status.idle":"2022-06-20T14:27:00.84899Z","shell.execute_reply.started":"2022-06-20T14:27:00.387982Z","shell.execute_reply":"2022-06-20T14:27:00.847845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### WordCloud for terms appearing for `Effective` category.","metadata":{}},{"cell_type":"code","source":"title = \"Effective\"\ndf = df_master[df_master['discourse_effectiveness'] == title]\nshow_wordcloud(df['discourse_text'], title )","metadata":{"execution":{"iopub.status.busy":"2022-06-20T13:57:05.32328Z","iopub.execute_input":"2022-06-20T13:57:05.32371Z","iopub.status.idle":"2022-06-20T13:57:05.772774Z","shell.execute_reply.started":"2022-06-20T13:57:05.323673Z","shell.execute_reply":"2022-06-20T13:57:05.772092Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Building using CountVectorizer\nOnce the corpus is created, we actually do the feature building here using **CountVectorizer**. This is a powerful, yet simple technique. Here, simply a list of count of the words or terms is given in a document. Concepts of sparse matrix etc also useful to understand here. ","metadata":{}},{"cell_type":"code","source":"cv = CountVectorizer(max_features = 500)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Machine Learning\nOnce the feature building is done, Machine Learning can be performed, because we have the dataset in desired format. Here we split the dataset in training and testing samples. Testing samples will help to determine the performance of the model.","metadata":{}},{"cell_type":"code","source":"X = cv.fit_transform(corpus).toarray()\ny = df_master.iloc[:, -1].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.10, random_state = 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Gaussian Naive Bayes Algorithm\nA Gaussian Naive Bayes algorithm is a special type of NB algorithm. It's specifically used when the features have continuous values. It's also assumed that all the features are following a gaussian distribution i.e, normal distribution.","metadata":{}},{"cell_type":"code","source":"classifier = GaussianNB()\nclassifier.fit(X_train, y_train)\ny_pred = classifier.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluating the model based on CountVectorizer strategy\nAfter the model is built, it is evaluated on test set. This model shows about 32 percent of accuracy. Confusion Matrix is also printed for better understanding of model.","metadata":{}},{"cell_type":"code","source":"cm = confusion_matrix(y_test, y_pred)\nscore = accuracy_score(y_test, y_pred)\nprint(\"Confusion Matrix\")\nprint(cm)\nprint(score)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating the Submission File\nSince this is a competition so there is a need to have file called `submission.csv` which can be scored. Folloing steps do the prediction on the test set and generate the submission file.","metadata":{}},{"cell_type":"code","source":"df_submit  = pd.read_csv('/kaggle/input/feedback-prize-effectiveness/test.csv')\nsubmit_corpus = []\nfor i in range(0, len(df_submit)):\n  review = re.sub('[^a-zA-Z]', ' ', df_master['discourse_text'][i])\n  review = review.lower()\n  review = review.split()\n  ps = PorterStemmer()\n  all_stopwords = stopwords.words('english')\n  all_stopwords.remove('not')\n  review = [ps.stem(word) for word in review if not word in set(all_stopwords)]\n  review = ' '.join(review)\n  submit_corpus.append(review)\n\nX_submit = cv.transform(submit_corpus).toarray()\npreds = classifier.predict_proba(X_submit)\ndf_res = pd.DataFrame(preds)\ndf_res.columns  = classifier.classes_\npd.concat([df_submit['discourse_id'], df_res], axis = 1).to_csv(\"submission.csv\", index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Technique 2 - TF-IDF based text analyzer\ntf-idf is another popular technique which we will be looking at. As per wikipedia\n\nIn information retrieval, tf–idf short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. tf–idf is one of the most popular term-weighting schemes today. A survey conducted in 2015 showed that 83% of text-based recommender systems in digital libraries use tf–idf.","metadata":{}},{"cell_type":"code","source":"df_master  = pd.read_csv('/kaggle/input/feedback-prize-effectiveness/train.csv')\ndef clean_tokenize_orig(document):\n  review = re.sub('[^a-zA-Z]', ' ', document)\n  review = review.lower()\n  review = review.split()\n  ps = PorterStemmer()\n  all_stopwords = stopwords.words('english')\n  all_stopwords.remove('not')\n  review = [ps.stem(word) for word in review if not word in set(all_stopwords)]\n  review = ' '.join(review)\n  return(review)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading and Cleaning Data","metadata":{}},{"cell_type":"code","source":"df_master = df_master[['discourse_id','discourse_text','discourse_effectiveness']].dropna()\ntexts = df_master['discourse_text'].tolist()\ntexts[0] #one instance of dataset.\ncleaned_texts = list(map(clean_tokenize_orig, texts))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature creation using TFIDF vectorizer and Model Creation","metadata":{}},{"cell_type":"code","source":"tfidf_vectorizer = TfidfVectorizer(stop_words='english')\ntfidf_array = tfidf_vectorizer.fit_transform(cleaned_texts)\nX = tfidf_array.toarray()\ny = df_master.iloc[:, -1].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.10, random_state = 0)\nclassifier = GaussianNB()\nclassifier.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluating Model Performance","metadata":{}},{"cell_type":"code","source":"y_pred = classifier.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\nscore = accuracy_score(y_test, y_pred)\nprint(\"Confusion Matrix\")\nprint(cm)\nprint(round(score,2))","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We observe that Model built by CountVectorizer outperforms the tfidfVectorizer by a margin of 26 percent which is huge. But this should not undermine the effectiveness of tfidf as this has applicability in large areas and this should always be considered.\n\nThats it. We have seen couple of basic NLP techniques to analyze text data. I will be adding more in coming days. Stay tuned.","metadata":{}},{"cell_type":"markdown","source":"## Using Transformers","metadata":{}},{"cell_type":"code","source":"from transformers import TrainingArguments,Trainer\nfrom transformers import AutoModelForSequenceClassification,AutoTokenizer\nimport datasets\nfrom datasets import load_dataset, Dataset, DatasetDict","metadata":{"execution":{"iopub.status.busy":"2022-06-24T17:27:08.72325Z","iopub.execute_input":"2022-06-24T17:27:08.723684Z","iopub.status.idle":"2022-06-24T17:27:17.49309Z","shell.execute_reply.started":"2022-06-24T17:27:08.723646Z","shell.execute_reply":"2022-06-24T17:27:17.492092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_nm = 'microsoft/deberta-v3-small'\ntokz = AutoTokenizer.from_pretrained(model_nm)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T17:27:23.366457Z","iopub.execute_input":"2022-06-24T17:27:23.367177Z","iopub.status.idle":"2022-06-24T17:27:26.126433Z","shell.execute_reply.started":"2022-06-24T17:27:23.36714Z","shell.execute_reply":"2022-06-24T17:27:26.125713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds = Dataset.from_pandas(df_master)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T17:30:28.376116Z","iopub.execute_input":"2022-06-24T17:30:28.376519Z","iopub.status.idle":"2022-06-24T17:30:28.452966Z","shell.execute_reply.started":"2022-06-24T17:30:28.376484Z","shell.execute_reply":"2022-06-24T17:30:28.45222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds","metadata":{"execution":{"iopub.status.busy":"2022-06-24T17:30:33.472098Z","iopub.execute_input":"2022-06-24T17:30:33.472468Z","iopub.status.idle":"2022-06-24T17:30:33.481773Z","shell.execute_reply.started":"2022-06-24T17:30:33.472423Z","shell.execute_reply":"2022-06-24T17:30:33.481022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tok_func(x): return tokz(x[\"discourse_text\"])\ntok_ds = ds.map(tok_func, batched=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T17:47:17.023096Z","iopub.execute_input":"2022-06-24T17:47:17.023979Z","iopub.status.idle":"2022-06-24T17:47:35.673854Z","shell.execute_reply.started":"2022-06-24T17:47:17.023935Z","shell.execute_reply":"2022-06-24T17:47:35.67298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"row = tok_ds[0]\nrow['discourse_text'], row['input_ids']","metadata":{"execution":{"iopub.status.busy":"2022-06-24T17:32:51.508957Z","iopub.execute_input":"2022-06-24T17:32:51.50935Z","iopub.status.idle":"2022-06-24T17:32:51.517777Z","shell.execute_reply.started":"2022-06-24T17:32:51.509319Z","shell.execute_reply":"2022-06-24T17:32:51.516988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tok_ds = tok_ds.rename_columns({'discourse_effectiveness':'labels'})","metadata":{"execution":{"iopub.status.busy":"2022-06-24T17:47:41.272746Z","iopub.execute_input":"2022-06-24T17:47:41.273464Z","iopub.status.idle":"2022-06-24T17:47:41.280917Z","shell.execute_reply.started":"2022-06-24T17:47:41.273425Z","shell.execute_reply":"2022-06-24T17:47:41.280029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tok_ds.column_names","metadata":{"execution":{"iopub.status.busy":"2022-06-24T17:48:14.19807Z","iopub.execute_input":"2022-06-24T17:48:14.199081Z","iopub.status.idle":"2022-06-24T17:48:14.205426Z","shell.execute_reply.started":"2022-06-24T17:48:14.199022Z","shell.execute_reply":"2022-06-24T17:48:14.204842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tok_ds = tok_ds.remove_columns(tok_ds.column_names)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T17:47:44.498459Z","iopub.execute_input":"2022-06-24T17:47:44.498934Z","iopub.status.idle":"2022-06-24T17:47:44.507056Z","shell.execute_reply.started":"2022-06-24T17:47:44.49889Z","shell.execute_reply":"2022-06-24T17:47:44.505952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dds = tok_ds.train_test_split(0.20, seed=0)\ndds","metadata":{"execution":{"iopub.status.busy":"2022-06-24T17:47:47.576499Z","iopub.execute_input":"2022-06-24T17:47:47.577596Z","iopub.status.idle":"2022-06-24T17:47:47.585155Z","shell.execute_reply.started":"2022-06-24T17:47:47.57752Z","shell.execute_reply":"2022-06-24T17:47:47.584487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tok_ds = tok_ds.remove_columns(dds[\"train\"].column_names)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T17:44:15.242725Z","iopub.execute_input":"2022-06-24T17:44:15.243156Z","iopub.status.idle":"2022-06-24T17:44:15.252273Z","shell.execute_reply.started":"2022-06-24T17:44:15.243121Z","shell.execute_reply":"2022-06-24T17:44:15.251265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import TrainingArguments,Trainer","metadata":{"execution":{"iopub.status.busy":"2022-06-24T17:37:49.580424Z","iopub.execute_input":"2022-06-24T17:37:49.580846Z","iopub.status.idle":"2022-06-24T17:37:49.585123Z","shell.execute_reply.started":"2022-06-24T17:37:49.580812Z","shell.execute_reply":"2022-06-24T17:37:49.584169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bs = 128\nepochs = 4","metadata":{"execution":{"iopub.status.busy":"2022-06-24T17:37:59.348568Z","iopub.execute_input":"2022-06-24T17:37:59.348969Z","iopub.status.idle":"2022-06-24T17:37:59.352688Z","shell.execute_reply.started":"2022-06-24T17:37:59.348936Z","shell.execute_reply":"2022-06-24T17:37:59.351992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr = 8e-5","metadata":{"execution":{"iopub.status.busy":"2022-06-24T17:38:07.872502Z","iopub.execute_input":"2022-06-24T17:38:07.872969Z","iopub.status.idle":"2022-06-24T17:38:07.877875Z","shell.execute_reply.started":"2022-06-24T17:38:07.872933Z","shell.execute_reply":"2022-06-24T17:38:07.876591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"args = TrainingArguments('outputs', learning_rate=lr, warmup_ratio=0.1, lr_scheduler_type='cosine', fp16=False,\n    evaluation_strategy=\"epoch\", per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2,\n    num_train_epochs=epochs, weight_decay=0.01, report_to='none')","metadata":{"execution":{"iopub.status.busy":"2022-06-24T17:38:46.589649Z","iopub.execute_input":"2022-06-24T17:38:46.590132Z","iopub.status.idle":"2022-06-24T17:38:46.596868Z","shell.execute_reply.started":"2022-06-24T17:38:46.590082Z","shell.execute_reply":"2022-06-24T17:38:46.596148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def corr_d(eval_pred): return {'pearson': corr(*eval_pred)}","metadata":{"execution":{"iopub.status.busy":"2022-06-24T17:39:47.131914Z","iopub.execute_input":"2022-06-24T17:39:47.13347Z","iopub.status.idle":"2022-06-24T17:39:47.140063Z","shell.execute_reply.started":"2022-06-24T17:39:47.133403Z","shell.execute_reply":"2022-06-24T17:39:47.138618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels=1)\ntrainer = Trainer(model, args, train_dataset=dds['train'], eval_dataset=dds['test'],\n                  tokenizer=tokz, compute_metrics=corr_d)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T17:44:23.416054Z","iopub.execute_input":"2022-06-24T17:44:23.416446Z","iopub.status.idle":"2022-06-24T17:44:25.704836Z","shell.execute_reply.started":"2022-06-24T17:44:23.416415Z","shell.execute_reply":"2022-06-24T17:44:25.704021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train();","metadata":{"execution":{"iopub.status.busy":"2022-06-24T17:44:33.353377Z","iopub.execute_input":"2022-06-24T17:44:33.354385Z","iopub.status.idle":"2022-06-24T17:44:33.459666Z","shell.execute_reply.started":"2022-06-24T17:44:33.354341Z","shell.execute_reply":"2022-06-24T17:44:33.458281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}