{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objs as go\n\nimport plotly\nplotly.offline.init_notebook_mode(connected=True) \n\n#Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-27T00:08:38.731168Z","iopub.execute_input":"2022-05-27T00:08:38.731553Z","iopub.status.idle":"2022-05-27T00:08:39.742327Z","shell.execute_reply.started":"2022-05-27T00:08:38.731522Z","shell.execute_reply":"2022-05-27T00:08:39.740283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#That's a ReMix that I copied from the last Feedback Prize - Evaluating Student Writing\n\nhttps://www.kaggle.com/competitions/feedback-prize-2021/overview","metadata":{}},{"cell_type":"code","source":"#Code by Dino Wun https://www.kaggle.com/code/dinowun/eda-simplified-feedback-prize\n\n# ℹ️: You can copy file path in data sections if you would want!\nTRAIN_DIR = \"../input/feedback-prize-effectiveness/train\"\nTEST_DIR = \"../input/feedback-prize-effectiveness/test\"\ntrain_files = os.listdir(TRAIN_DIR)\ntest_files = os.listdir(TEST_DIR)\n\nfor file in range(len(train_files)):\n    train_files[file] = TRAIN_DIR + \"/\" + str(train_files[file])\n    \nfor file in range(len(test_files)):\n    test_files[file] = TEST_DIR + \"/\" + str(test_files[file])\n    \ntrain_df = pd.read_csv(\"../input/feedback-prize-effectiveness/train.csv\")\nsub_df = pd.read_csv(\"../input/feedback-prize-effectiveness/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-05-27T00:08:49.172734Z","iopub.execute_input":"2022-05-27T00:08:49.173495Z","iopub.status.idle":"2022-05-27T00:08:49.350681Z","shell.execute_reply.started":"2022-05-27T00:08:49.173435Z","shell.execute_reply":"2022-05-27T00:08:49.349728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Distribution of Discourse Type","metadata":{}},{"cell_type":"code","source":"#Code by Dino Wun https://www.kaggle.com/code/dinowun/eda-simplified-feedback-prize\n\nfig = px.bar(x=np.unique(train_df[\"discourse_type\"]), y=[list(train_df[\"discourse_type\"]).count(i) for i in np.unique(train_df[\"discourse_type\"])], color=np.unique(train_df[\"discourse_type\"]), color_continuous_scale=\"Mint\")\nfig.update_xaxes(title=\"Classes\")\nfig.update_yaxes(title=\"Number of Rows\")\nfig.update_layout(showlegend=True, \n                  title={\n                      'text':'Discourse Type Distribution', \n                      'y':0.95, \n                      'x':0.5, \n                      'xanchor':'center', \n                      'yanchor':'top'}, template=\"seaborn\")\nfig.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-27T00:08:56.028509Z","iopub.execute_input":"2022-05-27T00:08:56.028889Z","iopub.status.idle":"2022-05-27T00:08:56.274557Z","shell.execute_reply.started":"2022-05-27T00:08:56.028856Z","shell.execute_reply":"2022-05-27T00:08:56.273667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Distribution of Discourse Effectiveness","metadata":{}},{"cell_type":"code","source":"#Code by Dino Wun https://www.kaggle.com/code/dinowun/eda-simplified-feedback-prize\n\nfig = px.bar(x=np.unique(train_df[\"discourse_effectiveness\"]), y=[list(train_df[\"discourse_effectiveness\"]).count(i) for i in np.unique(train_df[\"discourse_effectiveness\"])], color=np.unique(train_df[\"discourse_effectiveness\"]), color_continuous_scale=\"Mint\")\nfig.update_xaxes(title=\"Classes\")\nfig.update_yaxes(title=\"Number of Rows\")\nfig.update_layout(showlegend=True, \n                  title={\n                      'text':'Enumerated class label of Discourse Effectiveness Distribution', \n                      'y':0.95, \n                      'x':0.5, \n                      'xanchor':'center', \n                      'yanchor':'top'}, template=\"seaborn\")\nfig.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-27T00:09:01.880789Z","iopub.execute_input":"2022-05-27T00:09:01.881194Z","iopub.status.idle":"2022-05-27T00:09:02.081376Z","shell.execute_reply.started":"2022-05-27T00:09:01.881163Z","shell.execute_reply":"2022-05-27T00:09:02.080646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Let's wordcloud the words!","metadata":{}},{"cell_type":"code","source":"#Code by Dino Wun https://www.kaggle.com/code/dinowun/eda-simplified-feedback-prize\n\nimport wordcloud\n\nwordcloud = wordcloud.WordCloud(stopwords=wordcloud.STOPWORDS, max_font_size=90, max_words=4500, width=600, height=400, background_color='black', colormap='Set3').generate(' '.join(txt for txt in train_df[\"discourse_text\"]))\nfig, ax = plt.subplots(figsize=(14,10))\nax.imshow(wordcloud, interpolation='bilinear')\nax.set_axis_off()\nplt.imshow(wordcloud);","metadata":{"_kg_hide-output":false,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-27T00:09:07.505577Z","iopub.execute_input":"2022-05-27T00:09:07.505954Z","iopub.status.idle":"2022-05-27T00:09:14.208481Z","shell.execute_reply.started":"2022-05-27T00:09:07.505916Z","shell.execute_reply":"2022-05-27T00:09:14.207716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(\"../input/feedback-prize-effectiveness/train.csv\")\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-27T00:09:21.463247Z","iopub.execute_input":"2022-05-27T00:09:21.463621Z","iopub.status.idle":"2022-05-27T00:09:21.619196Z","shell.execute_reply.started":"2022-05-27T00:09:21.463574Z","shell.execute_reply":"2022-05-27T00:09:21.618461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Distribution of Labels","metadata":{}},{"cell_type":"code","source":"#Code by Lonnie https://www.kaggle.com/code/lonnieqin/name-entity-recognition-with-transformer\n\ntrain_df[\"discourse_type\"].value_counts().plot(kind=\"bar\");","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-27T00:09:26.535232Z","iopub.execute_input":"2022-05-27T00:09:26.535567Z","iopub.status.idle":"2022-05-27T00:09:26.733552Z","shell.execute_reply.started":"2022-05-27T00:09:26.535538Z","shell.execute_reply":"2022-05-27T00:09:26.732632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Code by Lonnie https://www.kaggle.com/code/lonnieqin/name-entity-recognition-with-transformer\n\nvocab_size = 10000 # Vocabulary size\nsequence_length = 1024 # Sequence Length\nbatch_size = 128 # Batch size\nunk_token = \"<UNK>\" # Unknownd token\npadding_token = \"<PAD>\"\nnone_class = \"O\"\nvectorizer_path = \"vectorizer.json\"\n# Use output dataset for inference\noutput_dataset_path = \"../input/name-entity-with-transformer-output/\"\nmodel_path = \"model.h5\"\nembed_size = 128\nhidden_size = 64\nmodes = [\"training\", \"inference\"] # There is training and inference mode\nmode = modes[0]\nepochs = 30","metadata":{"execution":{"iopub.status.busy":"2022-05-27T00:09:41.825747Z","iopub.execute_input":"2022-05-27T00:09:41.826133Z","iopub.status.idle":"2022-05-27T00:09:41.832867Z","shell.execute_reply.started":"2022-05-27T00:09:41.826102Z","shell.execute_reply":"2022-05-27T00:09:41.831691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Code by Lonnie https://www.kaggle.com/code/lonnieqin/name-entity-recognition-with-transformer\n\ndiscourse_types = list(train_df[\"discourse_type\"].value_counts().index)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T00:09:46.465349Z","iopub.execute_input":"2022-05-27T00:09:46.465772Z","iopub.status.idle":"2022-05-27T00:09:46.475914Z","shell.execute_reply.started":"2022-05-27T00:09:46.465739Z","shell.execute_reply":"2022-05-27T00:09:46.474808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Code by Lonnie https://www.kaggle.com/code/lonnieqin/name-entity-recognition-with-transformer\n\nall_tags = [padding_token]\nfor discourse_type in discourse_types:\n    all_tags.append(\"B-\" + discourse_type)\nfor discourse_type in discourse_types:\n    all_tags.append(\"I-\" + discourse_type)\nall_tags.append(none_class)\npad_index = all_tags.index(padding_token)\nnone_index = all_tags.index(none_class)\ntag_index = dict([(tag, index) for (index, tag) in enumerate(all_tags)])\nindex_tag = dict([(tag_index[tag], tag.replace(\"B-\", \"\").replace(\"I-\", \"\")) for tag in tag_index])\nprint(all_tags)\nprint(tag_index)\nprint(index_tag)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T00:09:51.302017Z","iopub.execute_input":"2022-05-27T00:09:51.302401Z","iopub.status.idle":"2022-05-27T00:09:51.310375Z","shell.execute_reply.started":"2022-05-27T00:09:51.302367Z","shell.execute_reply":"2022-05-27T00:09:51.309486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Tokenization","metadata":{}},{"cell_type":"code","source":"#Code by Lonnie https://www.kaggle.com/code/lonnieqin/name-entity-recognition-with-transformer\n\ndef tokenize(content):\n    tokens = content.lower().split()\n    return tokens\n    \ndef calc_word_indices(full_text, discourse_start, discourse_end):\n    start_index = len(full_text[:discourse_start].split())\n    token_len = len(full_text[discourse_start:discourse_end].split())\n    output = list(range(start_index, start_index + token_len))\n    if output[-1] >= len(full_text.split()):\n        output = list(range(start_index, start_index + token_len-1))\n    return output\ndef get_range(item):\n    locations = [int(location) for location in item[\"predictionstring\"].split(\" \")]\n    return (locations[0], locations[-1])\ndef add_annotation(all_data, start_index, end_index, discourse_type):\n    for j in range(start_index, end_index): \n        if j == start_index:\n            all_data[-1][1][j] = tag_index[\"B-\" + discourse_type]\n        else:\n            all_data[-1][1][j] = tag_index[\"I-\" + discourse_type]","metadata":{"execution":{"iopub.status.busy":"2022-05-27T00:09:56.4082Z","iopub.execute_input":"2022-05-27T00:09:56.408595Z","iopub.status.idle":"2022-05-27T00:09:56.418916Z","shell.execute_reply.started":"2022-05-27T00:09:56.40856Z","shell.execute_reply":"2022-05-27T00:09:56.417731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Vectorization","metadata":{}},{"cell_type":"code","source":"#Code by Lonnie https://www.kaggle.com/code/lonnieqin/name-entity-recognition-with-transformer\n\nclass Vectorizer:\n    \n    def __init__(self, vocab_size = None, sequence_length = None, unk_token = \"<unk>\"):\n        self.vocab_size = vocab_size\n        self.sequence_length = sequence_length\n        self.unk_token = unk_token\n        \n    def fit_transform(self, sentences):\n        word_counter = dict()\n        for tokens in sentences:\n            for token in tokens: \n                if token in word_counter:\n                    word_counter[token] += 1\n                else:\n                    word_counter[token] = 1\n        word_counter = pd.DataFrame({\"key\": word_counter.keys(), \"count\": word_counter.values()})\n        word_counter.sort_values(by=\"count\", ascending=False, inplace=True)\n        vocab = set(word_counter[\"key\"][0:self.vocab_size-2])\n        word_index = dict()\n        begin_index = 1 \n        word_index[self.unk_token] = begin_index\n        begin_index += 1\n        Xs = []\n        for i in range(len(sentences)):\n            X = []\n            for token in sentences[i]:\n                if token not in word_index and token in vocab:\n                    word_index[token] = begin_index\n                    begin_index += 1\n                if token in word_index:\n                    X.append(word_index[token])\n                else:\n                    X.append(word_index[self.unk_token])\n                if len(X) == self.sequence_length:\n                    break\n            Xs.append(X)\n        self.word_index = word_index\n        self.vocab = vocab\n        return Xs\n    \n    def transform(self, sentences):\n        Xs = []\n        for i in range(len(sentences)):\n            X = []\n            for token in sentences[i]:\n                if token in self.word_index:\n                    X.append(self.word_index[token])\n                else:\n                    X.append(self.word_index[self.unk_token])\n                if len(X) == self.sequence_length:\n                    break\n            Xs.append(X)\n        return Xs\n    \n    def load(self, path):\n        with open(path, 'r') as f:\n            dic = json.load(f)\n            self.vocab_size = dic['vocab_size']\n            self.sequence_length = dic['sequence_length']\n            self.unk_token = dic['unk_token']\n            self.word_index = dic['word_index']\n            \n    def save(self, path):\n        with open(path, 'w') as f:\n            data = json.dumps({\n                \"vocab_size\": self.vocab_size, \n                \"sequence_length\": self.sequence_length, \n                \"unk_token\": self.unk_token,\n                \"word_index\": self.word_index\n            })\n            f.write(data)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T00:10:28.468131Z","iopub.execute_input":"2022-05-27T00:10:28.468577Z","iopub.status.idle":"2022-05-27T00:10:28.48504Z","shell.execute_reply.started":"2022-05-27T00:10:28.468544Z","shell.execute_reply":"2022-05-27T00:10:28.484102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Let's go to plan B since the code above requires another Dataset. ","metadata":{}},{"cell_type":"markdown","source":"#Preprocessing","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import stopwords\nimport re\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import KFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nfrom string import punctuation\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2022-05-27T00:10:35.740096Z","iopub.execute_input":"2022-05-27T00:10:35.740866Z","iopub.status.idle":"2022-05-27T00:10:36.11175Z","shell.execute_reply.started":"2022-05-27T00:10:35.740823Z","shell.execute_reply":"2022-05-27T00:10:36.110842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Code by Neha Pawar  https://www.kaggle.com/code/nehapawar/tfidf-random-forest-classifier\n\ndef clean_text(text, remove_stopwords=True, stem_words=False, lemma=True):\n    text = str(text).lower().split()\n    \n    if remove_stopwords:\n        stops = stopwords.words(\"english\")\n        text = [x for x in text if not x in stops]\n        \n    text = ' '.join(text)\n    \n    text = re.sub(r\"[-()\\\"#/<>!@&;*:<>{}`'+=~%|.!?,_]\", \" \", text)\n    text = re.sub(r\"\\]\", \" \", text)\n    text = re.sub(r\"\\[\", \" \", text)\n    text = re.sub(r\"\\/\", \" \", text)\n    text = re.sub(r\"\\\\\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"can't\", \"cannot \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\"  \", \" \", text)\n    text = re.sub(r\"   \", \" \", text)\n    text = re.sub(r\"   \", \" \", text)\n    text = re.sub(r\"0x00\", \"\", text)\n    \n    if stem_words:\n        text = text.split()\n        stemmer = SnowballStemmer('english')\n        stem_words = [stemmer.stem(x) for x in text]\n        text = \" \".join(text)\n        \n    if lemma:\n        text = text.split()\n        lem = WordNetLemmatizer()\n        lemmatized = [lem.lemmatize(x, \"v\") for x in text]\n        text = \" \".join(text)\n        \n    return text","metadata":{"execution":{"iopub.status.busy":"2022-05-27T00:10:41.176714Z","iopub.execute_input":"2022-05-27T00:10:41.17728Z","iopub.status.idle":"2022-05-27T00:10:41.190125Z","shell.execute_reply.started":"2022-05-27T00:10:41.177243Z","shell.execute_reply":"2022-05-27T00:10:41.189278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Code by Neha Pawar  https://www.kaggle.com/code/nehapawar/tfidf-random-forest-classifier\n\n%time\ntrain_df['cleaned_text'] = train_df.discourse_text.apply(clean_text)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T00:10:54.073032Z","iopub.execute_input":"2022-05-27T00:10:54.073406Z","iopub.status.idle":"2022-05-27T00:11:09.656484Z","shell.execute_reply.started":"2022-05-27T00:10:54.073376Z","shell.execute_reply":"2022-05-27T00:11:09.65561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Code by Neha Pawar  https://www.kaggle.com/code/nehapawar/tfidf-random-forest-classifier\n\ntrain_df.iloc[0]['discourse_text']","metadata":{"execution":{"iopub.status.busy":"2022-05-27T00:11:10.987844Z","iopub.execute_input":"2022-05-27T00:11:10.990115Z","iopub.status.idle":"2022-05-27T00:11:10.996883Z","shell.execute_reply.started":"2022-05-27T00:11:10.990054Z","shell.execute_reply":"2022-05-27T00:11:10.995936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Nasa Again! :)","metadata":{}},{"cell_type":"markdown","source":"#TFIDF Vectorizer","metadata":{}},{"cell_type":"code","source":"#Code by Neha Pawar  https://www.kaggle.com/code/nehapawar/tfidf-random-forest-classifier\n\ntfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=1000)\ntfidf_vect.fit(train_df['cleaned_text'])","metadata":{"execution":{"iopub.status.busy":"2022-05-27T00:11:19.564933Z","iopub.execute_input":"2022-05-27T00:11:19.565317Z","iopub.status.idle":"2022-05-27T00:11:20.440688Z","shell.execute_reply.started":"2022-05-27T00:11:19.565287Z","shell.execute_reply":"2022-05-27T00:11:20.439753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Code by Neha Pawar  https://www.kaggle.com/code/nehapawar/tfidf-random-forest-classifier\n\nTEST_PATH = \"../input/feedback-prize-effectiveness/test\"\n\ndef get_test_text(a_id):\n    a_file = f\"{TEST_PATH}/{a_id}.txt\"\n    with open(a_file, \"r\") as fp:\n        txt = fp.read()\n    return txt\n\ndef create_df_test():\n    test_ids = [f[:-4] for f in os.listdir(TEST_PATH)] #Remove the last 4 characters ('.txt') in the filenames such as '0FB0700DAF44.txt'.\n    test_data = []\n    for test_id in test_ids:\n        text = get_test_text(test_id)\n        sentences = nltk.sent_tokenize(text)\n        id_sentences = []\n        idx = 0 \n        for sentence in sentences:\n            id_sentence = []\n            words = sentence.split()\n            # I created this heuristic for mapping words in sentences to \"word indices\"\n            # This is not definitive and might have strong drawbacks and problems\n            for w in words:\n                id_sentence.append(idx)\n                idx+=1\n            id_sentences.append(id_sentence)\n        test_data += list(zip([test_id] * len(sentences), sentences, id_sentences))\n    df_test = pd.DataFrame(test_data, columns=['id', 'discourse_text', 'ids'])\n    return df_test","metadata":{"execution":{"iopub.status.busy":"2022-05-27T00:11:25.475684Z","iopub.execute_input":"2022-05-27T00:11:25.476313Z","iopub.status.idle":"2022-05-27T00:11:25.485824Z","shell.execute_reply.started":"2022-05-27T00:11:25.476277Z","shell.execute_reply":"2022-05-27T00:11:25.484828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Code by Neha Pawar  https://www.kaggle.com/code/nehapawar/tfidf-random-forest-classifier\n\ndf_test = create_df_test()\ndf_test.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-27T00:11:32.026427Z","iopub.execute_input":"2022-05-27T00:11:32.026893Z","iopub.status.idle":"2022-05-27T00:11:32.06993Z","shell.execute_reply.started":"2022-05-27T00:11:32.026848Z","shell.execute_reply":"2022-05-27T00:11:32.06919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Code by Neha Pawar  https://www.kaggle.com/code/nehapawar/tfidf-random-forest-classifier\n\ndf = train_df.append(df_test)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T00:11:37.02343Z","iopub.execute_input":"2022-05-27T00:11:37.023791Z","iopub.status.idle":"2022-05-27T00:11:37.036116Z","shell.execute_reply.started":"2022-05-27T00:11:37.023745Z","shell.execute_reply":"2022-05-27T00:11:37.035236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Handling with Nan to avoid an error below on test_tfidf","metadata":{}},{"cell_type":"code","source":"# categorical features with missing values\ncategorical_nan = [feature for feature in df.columns if df[feature].isna().sum()>0 and df[feature].dtypes=='O']\nprint(categorical_nan)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T00:11:42.203727Z","iopub.execute_input":"2022-05-27T00:11:42.204321Z","iopub.status.idle":"2022-05-27T00:11:42.24331Z","shell.execute_reply.started":"2022-05-27T00:11:42.204284Z","shell.execute_reply":"2022-05-27T00:11:42.242298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# replacing missing values in categorical features\nfor feature in categorical_nan:\n    df[feature] = df[feature].fillna('None')","metadata":{"execution":{"iopub.status.busy":"2022-05-27T00:11:46.383303Z","iopub.execute_input":"2022-05-27T00:11:46.383666Z","iopub.status.idle":"2022-05-27T00:11:46.421225Z","shell.execute_reply.started":"2022-05-27T00:11:46.383629Z","shell.execute_reply":"2022-05-27T00:11:46.42019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[categorical_nan].isna().sum()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-27T00:11:50.646021Z","iopub.execute_input":"2022-05-27T00:11:50.64664Z","iopub.status.idle":"2022-05-27T00:11:50.692551Z","shell.execute_reply.started":"2022-05-27T00:11:50.646603Z","shell.execute_reply":"2022-05-27T00:11:50.691723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Code by Neha Pawar  https://www.kaggle.com/code/nehapawar/tfidf-random-forest-classifier\n\ntrain_tfidf = tfidf_vect.fit_transform(df[:train_df.shape[1]]['cleaned_text'])\ntest_tfidf = tfidf_vect.transform(df[train_df.shape[1]:]['cleaned_text'])#Original is [0]","metadata":{"execution":{"iopub.status.busy":"2022-05-27T00:11:55.694643Z","iopub.execute_input":"2022-05-27T00:11:55.695159Z","iopub.status.idle":"2022-05-27T00:11:56.929029Z","shell.execute_reply.started":"2022-05-27T00:11:55.695097Z","shell.execute_reply":"2022-05-27T00:11:56.928062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Code by Neha Pawar  https://www.kaggle.com/code/nehapawar/tfidf-random-forest-classifier\n\nfeatures = tfidf_vect.get_feature_names()\nfeatures[:20]","metadata":{"execution":{"iopub.status.busy":"2022-05-27T00:12:01.616684Z","iopub.execute_input":"2022-05-27T00:12:01.617321Z","iopub.status.idle":"2022-05-27T00:12:01.623576Z","shell.execute_reply.started":"2022-05-27T00:12:01.617287Z","shell.execute_reply":"2022-05-27T00:12:01.622748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Random Forest","metadata":{}},{"cell_type":"code","source":"#Code by Neha Pawar  https://www.kaggle.com/code/nehapawar/tfidf-random-forest-classifier\n\nrf_model = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 42)\n#scores = cross_val_score(rf_model, train_tfidf, train_df['discourse_type'], scoring='accuracy', cv=cv)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T00:12:08.233681Z","iopub.execute_input":"2022-05-27T00:12:08.234279Z","iopub.status.idle":"2022-05-27T00:12:08.238467Z","shell.execute_reply.started":"2022-05-27T00:12:08.234242Z","shell.execute_reply":"2022-05-27T00:12:08.237473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Code by Neha Pawar  https://www.kaggle.com/code/nehapawar/tfidf-random-forest-classifier\n\n#%%time\n#rf_model.fit(train_tfidf, train_df['discourse_type'])","metadata":{"execution":{"iopub.status.busy":"2022-05-27T00:12:13.44849Z","iopub.execute_input":"2022-05-27T00:12:13.448876Z","iopub.status.idle":"2022-05-27T00:12:13.452625Z","shell.execute_reply.started":"2022-05-27T00:12:13.448842Z","shell.execute_reply":"2022-05-27T00:12:13.451617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#After the error above I gave up\n\nValueError: Found input variables with inconsistent numbers of samples: [6, 36765]","metadata":{"execution":{"iopub.status.busy":"2022-05-26T23:37:09.043379Z","iopub.execute_input":"2022-05-26T23:37:09.043939Z","iopub.status.idle":"2022-05-26T23:37:09.072155Z","shell.execute_reply.started":"2022-05-26T23:37:09.043899Z","shell.execute_reply":"2022-05-26T23:37:09.070896Z"}}},{"cell_type":"code","source":"#Code by Chris Deotte https://www.kaggle.com/code/cdeotte/rapids-umap-tfidf-kmeans-discovers-15-topics\n\nimport pandas as pd, os\nimport cudf, cuml, cupy\nfrom tqdm import tqdm\nimport numpy as np\nprint('RAPIDS',cudf.__version__)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T00:12:22.16214Z","iopub.execute_input":"2022-05-27T00:12:22.162875Z","iopub.status.idle":"2022-05-27T00:12:22.168637Z","shell.execute_reply.started":"2022-05-27T00:12:22.162836Z","shell.execute_reply":"2022-05-27T00:12:22.167402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://www.kaggle.com/raghavendrakotala/fine-tunned-on-roberta-base-as-ner-problem-0-533\ntrain_names, train_texts = [], []\nfor f in tqdm(list(os.listdir('../input/feedback-prize-effectiveness/train'))):\n    train_names.append(f.replace('.txt', ''))\n    train_texts.append(open('../input/feedback-prize-effectiveness/train/' + f, 'r').read())\ntrain_text_df = cudf.DataFrame({'id': train_names, 'text': train_texts})\ntrain_text_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-27T00:12:27.994722Z","iopub.execute_input":"2022-05-27T00:12:27.995606Z","iopub.status.idle":"2022-05-27T00:12:30.197306Z","shell.execute_reply.started":"2022-05-27T00:12:27.995567Z","shell.execute_reply":"2022-05-27T00:12:30.196405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Code by Chris Deotte https://www.kaggle.com/code/cdeotte/rapids-umap-tfidf-kmeans-discovers-15-topics\n\nfrom cuml.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer(stop_words='english', binary=True, max_features=25_000)\ntext_embeddings = tfidf.fit_transform( train_text_df.text ).toarray()","metadata":{"execution":{"iopub.status.busy":"2022-05-27T00:12:37.356804Z","iopub.execute_input":"2022-05-27T00:12:37.35743Z","iopub.status.idle":"2022-05-27T00:12:38.227395Z","shell.execute_reply.started":"2022-05-27T00:12:37.357394Z","shell.execute_reply":"2022-05-27T00:12:38.226578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Code by Chris Deotte https://www.kaggle.com/code/cdeotte/rapids-umap-tfidf-kmeans-discovers-15-topics\n\nfrom cuml import UMAP\numap = UMAP()\nembed_2d = umap.fit_transform(text_embeddings)\nembed_2d = cupy.asnumpy( embed_2d )","metadata":{"execution":{"iopub.status.busy":"2022-05-27T00:12:42.809484Z","iopub.execute_input":"2022-05-27T00:12:42.810183Z","iopub.status.idle":"2022-05-27T00:12:43.262023Z","shell.execute_reply.started":"2022-05-27T00:12:42.810124Z","shell.execute_reply":"2022-05-27T00:12:43.261024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Code by Chris Deotte https://www.kaggle.com/code/cdeotte/rapids-umap-tfidf-kmeans-discovers-15-topics\n\nfrom cuml import KMeans\nkmeans = cuml.KMeans(n_clusters=15)\nkmeans.fit(embed_2d)\ntrain_text_df['cluster'] = kmeans.labels_","metadata":{"execution":{"iopub.status.busy":"2022-05-27T00:12:48.172351Z","iopub.execute_input":"2022-05-27T00:12:48.172739Z","iopub.status.idle":"2022-05-27T00:12:48.189358Z","shell.execute_reply.started":"2022-05-27T00:12:48.172708Z","shell.execute_reply":"2022-05-27T00:12:48.188549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Code by Chris Deotte https://www.kaggle.com/code/cdeotte/rapids-umap-tfidf-kmeans-discovers-15-topics\n\nimport matplotlib.pyplot as plt\n\ncenters = kmeans.cluster_centers_\n\nplt.figure(figsize=(10,10))\nplt.scatter(embed_2d[:,0], embed_2d[:,1], s=1, c=kmeans.labels_)\nplt.title('UMAP Plot of Train Text using Tfidf features\\nRAPIDS Discovers the 15 essay topics!',size=16)\n\nfor k in range(len(centers)):\n    mm = cupy.mean( text_embeddings[train_text_df.cluster.values==k],axis=0 )\n    ii = cupy.argmax(mm)\n    top_word = tfidf.vocabulary_.iloc[ii]\n    plt.text(centers[k,0]-1,centers[k,1]+0.75,f'{k+1}-{top_word}',size=16)\n\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-27T00:12:53.226766Z","iopub.execute_input":"2022-05-27T00:12:53.227317Z","iopub.status.idle":"2022-05-27T00:12:53.940468Z","shell.execute_reply.started":"2022-05-27T00:12:53.227283Z","shell.execute_reply":"2022-05-27T00:12:53.939627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Code by Chris Deotte https://www.kaggle.com/code/cdeotte/rapids-umap-tfidf-kmeans-discovers-15-topics\n\nfor k in range(15):\n    mm = cupy.mean( text_embeddings[train_text_df.cluster.values==k],axis=0 )\n    ii = cupy.asnumpy( cupy.argsort(mm)[-5:][::-1] )\n    top_words = tfidf.vocabulary_.to_array()[ii]\n    print('#'*25)\n    print(f'### Essay Topic {k+1}')\n    print('### Top 5 Words',top_words)\n    print('#'*25)\n    tmp = train_text_df.loc[train_text_df.cluster==k].sample(3, random_state=123)\n    for j in range(3):\n        txt = tmp.iloc[j,1]\n        print('-'*10,f'Example {j+1}','-'*10)\n        print(txt,'\\n')","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-27T00:13:05.009748Z","iopub.execute_input":"2022-05-27T00:13:05.010494Z","iopub.status.idle":"2022-05-27T00:13:05.752109Z","shell.execute_reply.started":"2022-05-27T00:13:05.010456Z","shell.execute_reply":"2022-05-27T00:13:05.75121Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#That's all from Feedback Prize Effectiveness ReMix","metadata":{}},{"cell_type":"markdown","source":"![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcR_jATyVNm5La9Ppm6jN0oDTw8_ppt8x8BfnQ&usqp=CAU)pinterest.com","metadata":{}},{"cell_type":"markdown","source":"#Acknowledgements:\n\nChris Deotte https://www.kaggle.com/code/cdeotte/rapids-umap-tfidf-kmeans-discovers-15-topics  (Finally I could copy one of his brilliant codes)\n\nRaghavendrakuttala https://www.kaggle.com/raghavendrakotala/fine-tunned-on-roberta-base-as-ner-problem-0-533\n\nDino Wun https://www.kaggle.com/code/dinowun/eda-simplified-feedback-prize\n\nLonnie https://www.kaggle.com/code/lonnieqin/name-entity-recognition-with-transformer\n\nNeha Pawar  https://www.kaggle.com/code/nehapawar/tfidf-random-forest-classifier","metadata":{}}]}