{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom joblib import Parallel, delayed\nfrom transformers import AutoTokenizer\nimport os\nfrom shutil import rmtree\nfrom joblib import dump\nfrom tqdm.auto import tqdm\nimport warnings\nimport joblib\nimport torch","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-21T18:18:14.578297Z","iopub.execute_input":"2022-06-21T18:18:14.578758Z","iopub.status.idle":"2022-06-21T18:18:17.11354Z","shell.execute_reply.started":"2022-06-21T18:18:14.578644Z","shell.execute_reply":"2022-06-21T18:18:17.112437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"warnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-06-21T18:18:17.114984Z","iopub.execute_input":"2022-06-21T18:18:17.115709Z","iopub.status.idle":"2022-06-21T18:18:17.121122Z","shell.execute_reply.started":"2022-06-21T18:18:17.115673Z","shell.execute_reply":"2022-06-21T18:18:17.119757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LABEL_MAPPING = {\"Ineffective\": 0, \"Adequate\": 1, \"Effective\": 2}","metadata":{"execution":{"iopub.status.busy":"2022-06-21T18:18:17.122376Z","iopub.execute_input":"2022-06-21T18:18:17.12271Z","iopub.status.idle":"2022-06-21T18:18:17.132752Z","shell.execute_reply.started":"2022-06-21T18:18:17.122671Z","shell.execute_reply":"2022-06-21T18:18:17.131748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class args:\n    input = '../input/folds'\n    model = 'microsoft/deberta-v3-base'\n    max_len = 512","metadata":{"execution":{"iopub.status.busy":"2022-06-21T18:18:17.135597Z","iopub.execute_input":"2022-06-21T18:18:17.136841Z","iopub.status.idle":"2022-06-21T18:18:17.144307Z","shell.execute_reply.started":"2022-06-21T18:18:17.136792Z","shell.execute_reply":"2022-06-21T18:18:17.143062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def _prepare_training_data_helper(args, tokenizer, df, is_train):\n    training_samples = []\n    for _, row in tqdm(df.iterrows(), total=len(df)):\n        idx = row[\"essay_id\"]\n        discourse_text = row[\"discourse_text\"]\n        discourse_type = row[\"discourse_type\"]\n\n        if is_train:\n            filename = os.path.join(args.input, \"train\", idx + \".txt\")\n        else:\n            filename = os.path.join(args.input, \"test\", idx + \".txt\")\n\n        with open(filename, \"r\") as f:\n            text = f.read()\n\n        encoded_text = tokenizer.encode_plus(\n            discourse_type + \" \" + discourse_text,\n            text,\n            add_special_tokens=True,\n            padding=\"max_length\",\n            max_length=args.max_len,\n            return_attention_mask=True,\n            return_token_type_ids=True,\n            truncation=\"longest_first\",\n            return_tensors=\"pt\"\n        )\n        input_ids = encoded_text[\"input_ids\"]\n\n        sample = {\n            \"discourse_id\": row[\"discourse_id\"],\n            \"input_ids\": input_ids,\n            # \"discourse_text\": discourse_text,\n            # \"essay_text\": text,\n            \"attention_mask\": encoded_text[\"attention_mask\"],\n        }\n\n        if \"token_type_ids\" in encoded_text:\n            sample[\"token_type_ids\"] = encoded_text[\"token_type_ids\"]\n\n        label = row[\"discourse_effectiveness\"]\n\n        sample[\"label\"] = LABEL_MAPPING[label]\n\n        training_samples.append(sample)\n    return training_samples\n\n\ndef prepare_training_data(df, tokenizer, args, num_jobs, is_train):\n    training_samples = []\n\n    df_splits = np.array_split(df, num_jobs)\n\n    results = Parallel(n_jobs=num_jobs, backend=\"multiprocessing\")(\n        delayed(_prepare_training_data_helper)(args, tokenizer, df, is_train) for df in df_splits\n    )\n    for result in results:\n        training_samples.extend(result)\n\n    return training_samples\n","metadata":{"execution":{"iopub.status.busy":"2022-06-21T18:18:17.146356Z","iopub.execute_input":"2022-06-21T18:18:17.147239Z","iopub.status.idle":"2022-06-21T18:18:17.160596Z","shell.execute_reply.started":"2022-06-21T18:18:17.14719Z","shell.execute_reply":"2022-06-21T18:18:17.159414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_folds = 5\n\nfor i in range( n_folds ):\n    \n    print(f'Fold {i}')\n    \n    df = pd.read_csv(os.path.join(args.input, \"train_folds.csv\"))\n\n    train_df = df[df[\"kfold\"] != i].reset_index(drop=True)\n    valid_df = df[df[\"kfold\"] == i].reset_index(drop=True)\n\n    tokenizer = AutoTokenizer.from_pretrained(args.model)\n    training_samples = prepare_training_data(train_df, tokenizer, args, num_jobs=4, is_train=True)\n    valid_samples = prepare_training_data(valid_df, tokenizer, args, num_jobs=4, is_train=True)\n\n    if os.path.exists(f'fold_{i}/'):\n        rmtree(f'fold_{i}/')\n    \n    else:\n        os.mkdir(f'fold_{i}/')\n    \n    torch.save( training_samples, os.path.join(f'fold_{i}','train.pt') )\n    torch.save( valid_samples, os.path.join(f'fold_{i}','valid.pt') )\n    \n        ","metadata":{"execution":{"iopub.status.busy":"2022-06-21T18:18:17.162433Z","iopub.execute_input":"2022-06-21T18:18:17.162874Z","iopub.status.idle":"2022-06-21T18:45:15.104325Z","shell.execute_reply.started":"2022-06-21T18:18:17.16283Z","shell.execute_reply":"2022-06-21T18:45:15.103064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}