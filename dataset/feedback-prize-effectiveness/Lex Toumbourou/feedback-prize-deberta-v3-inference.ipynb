{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This is the companion Inference notebook to this [Training Notebook](https://www.kaggle.com/code/lextoumbourou/feedback-prize-eda-and-model-training).","metadata":{}},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\nfrom types import SimpleNamespace\nimport logging\n\nimport torch\nfrom tqdm.notebook import tqdm\nfrom datasets import Dataset\nfrom transformers import AutoModel, AutoModelForSequenceClassification, AutoTokenizer, AutoConfig, DataCollatorWithPadding\nfrom transformers import TrainingArguments, Trainer\nimport numpy as np\nimport pandas as pd\nimport sklearn\nimport glob, pandas as pd, numpy as np, re\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom scipy.special import softmax\n\n# To work around the aggressive HuggingFace log spam.\nlogging.disable(logging.WARNING)\n\n# From this Gist: https://gist.github.com/ihoromi4/b681a9088f348942b01711f251e5f964\ndef seed_everything(seed: int):\n    import random, os\n    import numpy as np\n    import torch\n    \n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True","metadata":{"execution":{"iopub.status.busy":"2022-06-26T04:49:49.743514Z","iopub.execute_input":"2022-06-26T04:49:49.743969Z","iopub.status.idle":"2022-06-26T04:49:58.891217Z","shell.execute_reply.started":"2022-06-26T04:49:49.743888Z","shell.execute_reply":"2022-06-26T04:49:58.890486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = 'cpu'\nif torch.cuda.is_available:\n    device = 'cuda'","metadata":{"execution":{"iopub.status.busy":"2022-06-26T04:49:58.892905Z","iopub.execute_input":"2022-06-26T04:49:58.893132Z","iopub.status.idle":"2022-06-26T04:49:58.897158Z","shell.execute_reply.started":"2022-06-26T04:49:58.893101Z","shell.execute_reply":"2022-06-26T04:49:58.896493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device","metadata":{"execution":{"iopub.status.busy":"2022-06-26T04:49:58.898304Z","iopub.execute_input":"2022-06-26T04:49:58.898677Z","iopub.status.idle":"2022-06-26T04:49:58.910971Z","shell.execute_reply.started":"2022-06-26T04:49:58.898643Z","shell.execute_reply":"2022-06-26T04:49:58.910146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Config","metadata":{"execution":{"iopub.status.busy":"2022-06-06T22:11:23.363932Z","iopub.status.idle":"2022-06-06T22:11:23.364284Z","shell.execute_reply.started":"2022-06-06T22:11:23.364113Z","shell.execute_reply":"2022-06-06T22:11:23.36413Z"}}},{"cell_type":"code","source":"config = SimpleNamespace()\n\nconfig.n_folds = 4\nconfig.seed = 420\nconfig.max_len = 512\nconfig.lr = 1e-5\nconfig.weight_decay = 0.01\nconfig.epochs = 4\nconfig.batch_size = 16\nconfig.warm_up_ratio = 0.1\nconfig.hidden_dropout_prob = 0.1\nconfig.output_path = Path('./')\nconfig.input_path = Path('../input/feedback-prize-effectiveness')\nconfig.model_path = Path('../input/feedback-prize-the-complete-overview')","metadata":{"execution":{"iopub.status.busy":"2022-06-26T04:49:58.912932Z","iopub.execute_input":"2022-06-26T04:49:58.913342Z","iopub.status.idle":"2022-06-26T04:49:58.920786Z","shell.execute_reply.started":"2022-06-26T04:49:58.913307Z","shell.execute_reply":"2022-06-26T04:49:58.920026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"code","source":"test_df = pd.read_csv(config.input_path / 'test.csv')","metadata":{"execution":{"iopub.status.busy":"2022-06-26T04:49:58.923563Z","iopub.execute_input":"2022-06-26T04:49:58.924192Z","iopub.status.idle":"2022-06-26T04:49:58.939349Z","shell.execute_reply.started":"2022-06-26T04:49:58.924155Z","shell.execute_reply":"2022-06-26T04:49:58.9386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Generate Topics","metadata":{}},{"cell_type":"markdown","source":"Load the topic model and libraries created in [this](https://www.kaggle.com/code/lextoumbourou/feedback-topics-identification) notebook.","metadata":{}},{"cell_type":"code","source":"topic_pred_df = pd.read_csv('../input/feedback-topics-identification/topic_model_feedback.csv')\ntopic_pred_df = topic_pred_df.drop(columns={'prob'})\ntopic_pred_df = topic_pred_df.rename(columns={'id': 'essay_id'})\n\ntopic_meta_df = pd.read_csv('../input/feedback-topics-identification/topic_model_metadata.csv')\ntopic_meta_df = topic_meta_df.rename(columns={'Topic': 'topic', 'Name': 'topic_name'}).drop(columns=['Count'])\ntopic_meta_df.topic_name = topic_meta_df.topic_name.apply(lambda n: ' '.join(n.split('_')[1:]))\n\ntopic_pred_df = topic_pred_df.merge(topic_meta_df, on='topic', how='left')","metadata":{"execution":{"iopub.status.busy":"2022-06-26T04:49:58.940841Z","iopub.execute_input":"2022-06-26T04:49:58.94112Z","iopub.status.idle":"2022-06-26T04:49:58.996652Z","shell.execute_reply.started":"2022-06-26T04:49:58.941082Z","shell.execute_reply":"2022-06-26T04:49:58.996042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nsys.path.append('../input/feedback-topics-identification/site-packages')\nfrom bertopic import BERTopic\n\ntopic_model = BERTopic.load(\"../input/feedback-topics-identification/feedback_2021_topic_model\")\n\nsws = stopwords.words(\"english\") + [\"n't\",  \"'s\", \"'ve\"]\nfls = glob.glob(\"../input/feedback-prize-effectiveness/test/*.txt\")\ndocs = []\nfor fl in tqdm(fls):\n    with open(fl) as f:\n        txt = f.read()\n        word_tokens = word_tokenize(txt)\n        txt = \" \".join([w for w in word_tokens if not w.lower() in sws])\n    docs.append(txt)\n\ntopics, probs = topic_model.transform(docs)\n\npred_topics = pd.DataFrame()\ndids = list(map(lambda fl: fl.split(\"/\")[-1].split(\".\")[0], fls))\npred_topics[\"id\"] = dids\npred_topics[\"topic\"] = topics\npred_topics['prob'] = probs\npred_topics = pred_topics.drop(columns={'prob'})\npred_topics = pred_topics.rename(columns={'id': 'essay_id'})\npred_topics = pred_topics.merge(topic_meta_df, on='topic', how='left')\npred_topics","metadata":{"execution":{"iopub.status.busy":"2022-06-26T04:49:58.997853Z","iopub.execute_input":"2022-06-26T04:49:58.998097Z","iopub.status.idle":"2022-06-26T04:50:57.887869Z","shell.execute_reply.started":"2022-06-26T04:49:58.998065Z","shell.execute_reply":"2022-06-26T04:50:57.887087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = test_df.merge(pred_topics, on='essay_id', how='left')","metadata":{"execution":{"iopub.status.busy":"2022-06-26T04:50:57.889478Z","iopub.execute_input":"2022-06-26T04:50:57.889966Z","iopub.status.idle":"2022-06-26T04:50:57.896923Z","shell.execute_reply.started":"2022-06-26T04:50:57.889927Z","shell.execute_reply":"2022-06-26T04:50:57.89609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-26T04:50:57.898578Z","iopub.execute_input":"2022-06-26T04:50:57.898928Z","iopub.status.idle":"2022-06-26T04:50:57.914147Z","shell.execute_reply.started":"2022-06-26T04:50:57.898891Z","shell.execute_reply":"2022-06-26T04:50:57.913506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare Data","metadata":{}},{"cell_type":"code","source":"labels = ['Adequate', 'Effective', 'Ineffective']\n\ntokenizer = AutoTokenizer.from_pretrained(config.model_path / 'fold_0')\ntokenizer.model_max_len = config.max_len","metadata":{"execution":{"iopub.status.busy":"2022-06-26T04:50:57.916857Z","iopub.execute_input":"2022-06-26T04:50:57.917251Z","iopub.status.idle":"2022-06-26T04:50:58.636563Z","shell.execute_reply.started":"2022-06-26T04:50:57.917211Z","shell.execute_reply":"2022-06-26T04:50:58.635817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_essay(essay_fns):\n    essay_cache = {}\n\n    output = []\n    for essay_fn in essay_fns:\n        if essay_fn not in essay_cache:\n            essay_txt = open(essay_fn).read().strip().lower()\n            essay_cache[essay_fn] = essay_txt\n        output.append(essay_cache[essay_fn])\n\n    return output\n\ndef add_inputs(df, basepath):\n    df['essay_fn'] = basepath + '/' + df.essay_id + '.txt'\n    df['inputs'] = df.discourse_type.str.lower() + ' ' + tokenizer.sep_token + ' ' + df.topic_name + ' ' + tokenizer.sep_token + ' ' + df.discourse_text.str.lower()\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-06-26T04:50:58.637866Z","iopub.execute_input":"2022-06-26T04:50:58.638124Z","iopub.status.idle":"2022-06-26T04:50:58.645567Z","shell.execute_reply.started":"2022-06-26T04:50:58.63809Z","shell.execute_reply":"2022-06-26T04:50:58.644767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenizer_func(x):\n    return tokenizer(x[\"inputs\"], get_essay(x['essay_fn']), truncation=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-26T04:50:58.647101Z","iopub.execute_input":"2022-06-26T04:50:58.647603Z","iopub.status.idle":"2022-06-26T04:50:58.659377Z","shell.execute_reply.started":"2022-06-26T04:50:58.647564Z","shell.execute_reply":"2022-06-26T04:50:58.658633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = add_inputs(test_df, str(config.input_path / 'test'))","metadata":{"execution":{"iopub.status.busy":"2022-06-26T04:50:58.660204Z","iopub.execute_input":"2022-06-26T04:50:58.660465Z","iopub.status.idle":"2022-06-26T04:50:58.673139Z","shell.execute_reply.started":"2022-06-26T04:50:58.660433Z","shell.execute_reply":"2022-06-26T04:50:58.672482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head(1)","metadata":{"execution":{"iopub.status.busy":"2022-06-26T04:50:58.674425Z","iopub.execute_input":"2022-06-26T04:50:58.67483Z","iopub.status.idle":"2022-06-26T04:50:58.694942Z","shell.execute_reply.started":"2022-06-26T04:50:58.674779Z","shell.execute_reply":"2022-06-26T04:50:58.694273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom transformers import AutoConfig, AutoModelForSequenceClassification\nfrom transformers.models.deberta_v2.modeling_deberta_v2 import ContextPooler\nfrom transformers.models.deberta_v2.modeling_deberta_v2 import StableDropout\nfrom transformers.modeling_outputs import TokenClassifierOutput\nfrom transformers import DebertaV2ForSequenceClassification\n\ndef get_dropouts(num, start_prob, increment):\n    return [StableDropout(start_prob + (increment * i)) for i in range(num)]  \n\nclass MeanPooling(nn.Module):\n    def __init__(self):\n        super(MeanPooling, self).__init__()\n        \n    def forward(self, last_hidden_state, attention_mask):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings / sum_mask\n        return mean_embeddings\n\nclass CustomModel(nn.Module):\n    def __init__(self, backbone):\n        super(CustomModel, self).__init__()\n        \n        self.model = backbone\n        self.config = self.model.config\n        self.num_labels = self.config.num_labels\n\n        # self.pooler = ContextPooler(self.config)\n        self.pooler = MeanPooling()\n        \n        self.classifier = nn.Linear(self.config.hidden_size, self.num_labels)\n    \n        self.dropouts = get_dropouts(num=5, start_prob=config.hidden_dropout_prob - 0.02, increment=0.01)\n    \n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        inputs_embeds=None,\n        labels=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None\n    ):\n        outputs = self.model.deberta(\n            input_ids,\n            token_type_ids=token_type_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        \n        encoder_layer = outputs[0]\n        pooled_output = self.pooler(encoder_layer, attention_mask)\n                      \n        # Multi-sample dropout.\n        num_dps = float(len(self.dropouts))\n        for ii, drop in enumerate(self.dropouts):\n            if ii == 0:\n                logits = (self.classifier(drop(pooled_output)) / num_dps)\n            else:\n                logits += (self.classifier(drop(pooled_output)) / num_dps)\n\n        loss = None\n        if labels is not None:\n            loss_fn = nn.CrossEntropyLoss()\n            logits = logits.view(-1, self.num_labels)\n            loss = loss_fn(logits, labels.view(-1))\n\n        output = (logits,) + outputs[1:]\n\n        return TokenClassifierOutput(loss=loss, logits=logits, hidden_states=outputs.hidden_states, attentions=outputs.attentions)","metadata":{"execution":{"iopub.status.busy":"2022-06-26T04:50:58.696278Z","iopub.execute_input":"2022-06-26T04:50:58.696513Z","iopub.status.idle":"2022-06-26T04:50:58.727141Z","shell.execute_reply.started":"2022-06-26T04:50:58.696482Z","shell.execute_reply":"2022-06-26T04:50:58.726527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_model():\n    model_config = AutoConfig.from_pretrained(config.model_path / 'backbone_config/config.json')\n    model = DebertaV2ForSequenceClassification(model_config)\n    \n    return CustomModel(model)","metadata":{"execution":{"iopub.status.busy":"2022-06-26T04:50:58.728182Z","iopub.execute_input":"2022-06-26T04:50:58.728512Z","iopub.status.idle":"2022-06-26T04:50:58.733566Z","shell.execute_reply.started":"2022-06-26T04:50:58.728467Z","shell.execute_reply":"2022-06-26T04:50:58.732865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = get_model()","metadata":{"execution":{"iopub.status.busy":"2022-06-26T04:50:58.73507Z","iopub.execute_input":"2022-06-26T04:50:58.735609Z","iopub.status.idle":"2022-06-26T04:51:03.323922Z","shell.execute_reply.started":"2022-06-26T04:50:58.735575Z","shell.execute_reply":"2022-06-26T04:51:03.323163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"all_test_data = np.zeros((config.n_folds, len(test_df), len(labels)))\n\nfor fold_num in range(config.n_folds):\n    print(f'Do fold {fold_num}')\n\n    tokenizer = AutoTokenizer.from_pretrained(config.model_path / f'fold_{fold_num}')\n    tokenizer.model_max_length = config.max_len\n\n    model = get_model()\n\n    state_dict = torch.load(config.model_path / f'fold_{fold_num}/pytorch_model.bin')\n    model.load_state_dict(state_dict)  \n\n    test_dataset = Dataset.from_pandas(test_df[['inputs', 'essay_fn']])\n    test_tok_dataset = test_dataset.map(tokenizer_func, batched=True, remove_columns=('inputs', 'essay_fn'))\n    \n    data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding='longest')\n\n    args = TrainingArguments(\n        output_dir=config.output_path,\n        learning_rate=config.lr,\n        lr_scheduler_type='cosine',\n        fp16=True,\n        evaluation_strategy='epoch',\n        per_device_train_batch_size=config.batch_size,\n        per_device_eval_batch_size=config.batch_size * 2,\n        report_to=\"none\",\n        save_strategy='no'\n    )\n    \n    trainer = Trainer(\n        model,\n        args,\n        tokenizer=tokenizer,\n        data_collator=data_collator\n    )\n    \n    outputs = trainer.predict(test_tok_dataset) \n    softmax_outputs = softmax(outputs.predictions, axis=1)\n    \n    all_test_data[fold_num] = softmax_outputs","metadata":{"execution":{"iopub.status.busy":"2022-06-26T04:51:03.325077Z","iopub.execute_input":"2022-06-26T04:51:03.325352Z","iopub.status.idle":"2022-06-26T04:51:56.47008Z","shell.execute_reply.started":"2022-06-26T04:51:03.325315Z","shell.execute_reply":"2022-06-26T04:51:56.469333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Make Submission","metadata":{}},{"cell_type":"code","source":"preds = np.mean(all_test_data, axis=0)\noutput_df = pd.concat([test_df[['discourse_id']], pd.DataFrame(preds, columns=labels)], axis=1)\noutput_df.to_csv('submission.csv', index=False)\npd.read_csv('submission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-06-26T04:52:04.696407Z","iopub.execute_input":"2022-06-26T04:52:04.696705Z","iopub.status.idle":"2022-06-26T04:52:04.717895Z","shell.execute_reply.started":"2022-06-26T04:52:04.696671Z","shell.execute_reply":"2022-06-26T04:52:04.717203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}