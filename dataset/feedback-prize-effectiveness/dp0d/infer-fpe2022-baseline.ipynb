{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!tar -xvf ../input/infer-fpe2022-baseline-input/deberta-v3-base-baseline.tar","metadata":{"execution":{"iopub.status.busy":"2022-06-27T03:05:15.408854Z","iopub.execute_input":"2022-06-27T03:05:15.410098Z","iopub.status.idle":"2022-06-27T03:05:20.144735Z","shell.execute_reply.started":"2022-06-27T03:05:15.410052Z","shell.execute_reply":"2022-06-27T03:05:20.143459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nprint(torch.cuda.is_available())","metadata":{"execution":{"iopub.status.busy":"2022-06-27T03:05:20.147828Z","iopub.execute_input":"2022-06-27T03:05:20.148343Z","iopub.status.idle":"2022-06-27T03:05:20.154596Z","shell.execute_reply.started":"2022-06-27T03:05:20.1483Z","shell.execute_reply":"2022-06-27T03:05:20.153571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# Directory settings\n# ====================================================\nimport os\n\nINPUT_DIR = '../input/feedback-prize-effectiveness/'\nOUTPUT_DIR = './'\nif not os.path.exists(OUTPUT_DIR):\n    os.makedirs(OUTPUT_DIR)\n    \n# essay dir\nTRAIN_DIR = \"../input/feedback-prize-effectiveness/train\"\nTEST_DIR = \"../input/feedback-prize-effectiveness/test\"","metadata":{"execution":{"iopub.status.busy":"2022-06-27T03:05:20.155799Z","iopub.execute_input":"2022-06-27T03:05:20.156336Z","iopub.status.idle":"2022-06-27T03:05:20.381918Z","shell.execute_reply.started":"2022-06-27T03:05:20.156298Z","shell.execute_reply":"2022-06-27T03:05:20.380678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# Library\n# ====================================================\nimport os\nimport gc\nimport cv2\nimport copy\nimport time\nimport random\nimport joblib\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig\nfrom tqdm import tqdm\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2022-06-27T03:05:20.383613Z","iopub.execute_input":"2022-06-27T03:05:20.384019Z","iopub.status.idle":"2022-06-27T03:05:21.024526Z","shell.execute_reply.started":"2022-06-27T03:05:20.383981Z","shell.execute_reply":"2022-06-27T03:05:21.023184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 导入相关的包\n\n\nMODEL_PATHS = [\n    './Loss-Fold-0.bin',\n    './Loss-Fold-1.bin',\n    './Loss-Fold-2.bin',\n]\n\n# 设置CFG参数\n\nCONFIG = dict(\n    seed = 666,\n    model_name = '../input/deberta-v3-base/deberta-v3-base',\n    test_batch_size = 8,\n    max_length = 512,\n    num_classes = 3,\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n)\n\nCONFIG[\"tokenizer\"] = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n\ndef get_essay(essay_id):\n    essay_path = os.path.join(TEST_DIR, f\"{essay_id}.txt\")\n    essay_text = open(essay_path, 'r').read()\n    return essay_text\n\ndf = pd.read_csv(f\"{INPUT_DIR}test.csv\")\ndf['essay_text'] = df['essay_id'].apply(get_essay)\ndf.head()\n\nwith open(\"../input/infer-fpe2022-baseline-input/le.pkl\", \"rb\") as fp:\n    encoder = joblib.load(fp)\n\nprint(encoder.classes_)\n\n\nclass FeedBackDataset(Dataset):\n    def __init__(self, df, tokenizer, max_length):\n        self.df = df\n        self.max_len = max_length\n        self.tokenizer = tokenizer\n        self.discourse = df['discourse_text'].values\n        self.essay = df['essay_text'].values\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index):\n        discourse = self.discourse[index]\n        essay = self.essay[index]\n        text = discourse + \" \" + self.tokenizer.sep_token + \" \" + essay\n        inputs = self.tokenizer.encode_plus(\n            text,\n            truncation=True,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length'\n        )\n\n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']\n\n        return {\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long)\n        }\n\ntest_dataset = FeedBackDataset(df, CONFIG['tokenizer'], max_length=CONFIG['max_length'])\ntest_loader = DataLoader(test_dataset, batch_size=CONFIG['test_batch_size'],\n                         num_workers=2, shuffle=False, pin_memory=True)\n\n\nclass MeanPooling(nn.Module):\n    def __init__(self):\n        super(MeanPooling, self).__init__()\n\n    def forward(self, last_hidden_state, attention_mask):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings / sum_mask\n        return mean_embeddings\n\n\nclass FeedBackModel(nn.Module):\n    def __init__(self, model_name):\n        super(FeedBackModel, self).__init__()\n        self.model = AutoModel.from_pretrained(model_name)\n        self.config = AutoConfig.from_pretrained(model_name)\n        self.drop = nn.Dropout(p=0.2)\n        self.pooler = MeanPooling()\n        self.fc = nn.Linear(self.config.hidden_size, CONFIG['num_classes'])\n\n    def forward(self, ids, mask):\n        out = self.model(input_ids=ids, attention_mask=mask,\n                         output_hidden_states=False)\n        out = self.pooler(out.last_hidden_state, mask)\n        out = self.drop(out)\n        outputs = self.fc(out)\n        return outputs\n\n\n@torch.no_grad()\ndef valid_fn(model, dataloader, device):\n    model.eval()\n\n    dataset_size = 0\n    running_loss = 0.0\n\n    preds = []\n\n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    for step, data in bar:\n        ids = data['ids'].to(device, dtype=torch.long)\n        mask = data['mask'].to(device, dtype=torch.long)\n\n        outputs = model(ids, mask)\n        outputs = F.softmax(outputs, dim=1)\n        preds.append(outputs.cpu().detach().numpy())\n\n    preds = np.concatenate(preds)\n    gc.collect()\n\n    return preds\n\n\ndef inference(model_paths, dataloader, device):\n    final_preds = []\n    for i, path in enumerate(model_paths):\n        model = FeedBackModel(CONFIG['model_name'])\n        model.to(CONFIG['device'])\n        model.load_state_dict(torch.load(path))\n\n#         print(f\"Load model and predictions {i + 1}\")\n        preds = valid_fn(model, dataloader, device)\n        final_preds.append(preds)\n\n    final_preds = np.array(final_preds)\n    final_preds = np.mean(final_preds, axis=0)\n    return final_preds\n\nmodel_preds = inference(MODEL_PATHS, test_loader, CONFIG['device'])\n\nsample_submission = pd.read_csv(\"../input/feedback-prize-effectiveness/sample_submission.csv\")\nprint(sample_submission.head())\n\nsample_submission['Adequate'] = model_preds[:, 0]\nsample_submission['Effective'] = model_preds[:, 1]\nsample_submission['Ineffective'] = model_preds[:, 2]\n\nprint(sample_submission.head())\nsample_submission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T03:05:21.031104Z","iopub.execute_input":"2022-06-27T03:05:21.031513Z","iopub.status.idle":"2022-06-27T03:05:35.06924Z","shell.execute_reply.started":"2022-06-27T03:05:21.031478Z","shell.execute_reply":"2022-06-27T03:05:35.067341Z"},"trusted":true},"execution_count":null,"outputs":[]}]}