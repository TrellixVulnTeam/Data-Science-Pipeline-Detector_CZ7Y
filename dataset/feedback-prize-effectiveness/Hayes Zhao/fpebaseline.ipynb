{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-30T15:00:49.308793Z","iopub.execute_input":"2022-06-30T15:00:49.310404Z","iopub.status.idle":"2022-06-30T15:00:51.028782Z","shell.execute_reply.started":"2022-06-30T15:00:49.31024Z","shell.execute_reply":"2022-06-30T15:00:51.02551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom copy import deepcopy\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig\nfrom tqdm import tqdm\nimport torch.nn as nn\nimport gc","metadata":{"execution":{"iopub.status.busy":"2022-06-30T15:00:51.030731Z","iopub.execute_input":"2022-06-30T15:00:51.032201Z","iopub.status.idle":"2022-06-30T15:00:59.276932Z","shell.execute_reply.started":"2022-06-30T15:00:51.032143Z","shell.execute_reply":"2022-06-30T15:00:59.275414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_substring_span(text, substring, min_length=10, fraction=0.999):\n    \"\"\"\n    Returns substring's span from the given text with the certain precision.\n    \"\"\"\n\n    position = text.find(substring)\n    substring_length = len(substring)\n    if position == -1:\n        half_length = int(substring_length * fraction) \n        half_substring = substring[:half_length]\n        half_substring_length = len(half_substring)\n        if half_substring_length < min_length:\n            return [-1, 0]\n        else:\n            return get_substring_span(text=text, \n                                    substring=half_substring, \n                                    min_length=min_length, \n                                    fraction=fraction)\n\n    span = [position, position+substring_length]\n    return span\n\n\ndef read_file(path):\n    with open(path, \"r\") as file:\n        data = file.read()\n\n    return data\n\ndef preprocess(data_frame, \n               essay_id_column=\"essay_id\", \n               essay_path_column=\"essay_path\", \n               essay_text_column=\"essay_text\", \n               discourse_text_column=\"discourse_text\", \n               compute_lengths=True, \n               directory=\"./\", \n               file_format=\"txt\"):\n\n    data_frame = deepcopy(data_frame)\n\n    data_frame[essay_path_column] = data_frame[essay_id_column].apply(lambda essay_id: os.path.join(directory, f\"{essay_id}.{file_format}\"))\n    data_frame[essay_text_column] = data_frame[essay_path_column].apply(lambda essay_path: read_file(essay_path))\n\n    data_frame[f\"{discourse_text_column}_span\"] = data_frame.apply(lambda sample: get_substring_span(text=sample[essay_text_column], \n                                                                                                     substring=sample[discourse_text_column]), axis=1)\n    if compute_lengths:\n        data_frame[f\"{essay_text_column}_length\"] = data_frame[essay_text_column].apply(lambda text: len(text.split()))\n        data_frame[f\"{discourse_text_column}_length\"] = data_frame[discourse_text_column].apply(lambda text: len(text.split()))\n\n    return data_frame","metadata":{"execution":{"iopub.status.busy":"2022-06-30T15:00:59.280175Z","iopub.execute_input":"2022-06-30T15:00:59.280953Z","iopub.status.idle":"2022-06-30T15:00:59.297728Z","shell.execute_reply.started":"2022-06-30T15:00:59.280881Z","shell.execute_reply":"2022-06-30T15:00:59.29614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_path = \"../input/feedback-prize-effectiveness/test.csv\"\ntest_directory = \"../input/feedback-prize-effectiveness/test\"\n\ntest = pd.read_csv(test_path)\ntest = preprocess(data_frame=test, directory=test_directory, compute_lengths=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T15:00:59.30149Z","iopub.execute_input":"2022-06-30T15:00:59.302655Z","iopub.status.idle":"2022-06-30T15:00:59.345027Z","shell.execute_reply.started":"2022-06-30T15:00:59.302612Z","shell.execute_reply":"2022-06-30T15:00:59.343777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name  = '../input/deberta-v3-large/deberta-v3-large'\nbatch_size = 2\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel_path = ['../input/fpe-deberta/Loss-Fold-0.bin',\n             '../input/fpe-deberta/Loss-Fold-1.bin',\n             '../input/fpe-deberta/Loss-Fold-2.bin',\n             '../input/fpe-deberta/Loss-Fold-3.bin',\n             '../input/fpe-deberta/Loss-Fold-4.bin']","metadata":{"execution":{"iopub.status.busy":"2022-06-30T15:00:59.346524Z","iopub.execute_input":"2022-06-30T15:00:59.346926Z","iopub.status.idle":"2022-06-30T15:00:59.424345Z","shell.execute_reply.started":"2022-06-30T15:00:59.346876Z","shell.execute_reply":"2022-06-30T15:00:59.421699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test","metadata":{"execution":{"iopub.status.busy":"2022-06-30T15:00:59.426522Z","iopub.execute_input":"2022-06-30T15:00:59.427101Z","iopub.status.idle":"2022-06-30T15:00:59.464678Z","shell.execute_reply.started":"2022-06-30T15:00:59.427056Z","shell.execute_reply":"2022-06-30T15:00:59.463435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FeedBackDataset(Dataset):\n    def __init__(self, df, tokenizer, max_length):\n        self.df = df\n        self.max_len = max_length\n        self.tokenizer = tokenizer\n        self.discourse = df['discourse_text'].values\n        self.essay = df['essay_text'].values\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index):\n        discourse = self.discourse[index]\n        essay = self.essay[index]\n        text = discourse + \" \" + self.tokenizer.sep_token + \" \" + essay\n        inputs = self.tokenizer.encode_plus(\n            text,\n            truncation=True,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length'\n        )\n\n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']\n\n        return {\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long)\n        }\n","metadata":{"execution":{"iopub.status.busy":"2022-06-30T15:00:59.466655Z","iopub.execute_input":"2022-06-30T15:00:59.467073Z","iopub.status.idle":"2022-06-30T15:00:59.478364Z","shell.execute_reply.started":"2022-06-30T15:00:59.467031Z","shell.execute_reply":"2022-06-30T15:00:59.476719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T15:00:59.48037Z","iopub.execute_input":"2022-06-30T15:00:59.481265Z","iopub.status.idle":"2022-06-30T15:01:00.343024Z","shell.execute_reply.started":"2022-06-30T15:00:59.481222Z","shell.execute_reply":"2022-06-30T15:01:00.341738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = FeedBackDataset(test, tokenizer, max_length=512)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T15:01:00.345118Z","iopub.execute_input":"2022-06-30T15:01:00.345591Z","iopub.status.idle":"2022-06-30T15:01:00.353954Z","shell.execute_reply.started":"2022-06-30T15:01:00.345546Z","shell.execute_reply":"2022-06-30T15:01:00.352391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_loader = DataLoader(test_dataset, batch_size=batch_size,\n                         num_workers=2, shuffle=False, pin_memory=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T15:01:00.360931Z","iopub.execute_input":"2022-06-30T15:01:00.362223Z","iopub.status.idle":"2022-06-30T15:01:00.368864Z","shell.execute_reply.started":"2022-06-30T15:01:00.362164Z","shell.execute_reply":"2022-06-30T15:01:00.367214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MeanPooling(nn.Module):\n    def __init__(self):\n        super(MeanPooling, self).__init__()\n\n    def forward(self, last_hidden_state, attention_mask):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings / sum_mask\n        return mean_embeddings\n\n\nclass FeedBackModel(nn.Module):\n    def __init__(self, model_name):\n        super(FeedBackModel, self).__init__()\n        self.model = AutoModel.from_pretrained(model_name)\n        self.config = AutoConfig.from_pretrained(model_name)\n        self.drop = nn.Dropout(p=0.2)\n        self.pooler = MeanPooling()\n        self.fc = nn.Linear(self.config.hidden_size, 3)\n\n    def forward(self, ids, mask):\n        out = self.model(input_ids=ids, attention_mask=mask,\n                         output_hidden_states=False)\n        out = self.pooler(out.last_hidden_state, mask)\n        out = self.drop(out)\n        outputs = self.fc(out)\n        return outputs","metadata":{"execution":{"iopub.status.busy":"2022-06-30T15:01:00.371185Z","iopub.execute_input":"2022-06-30T15:01:00.372375Z","iopub.status.idle":"2022-06-30T15:01:00.389746Z","shell.execute_reply.started":"2022-06-30T15:01:00.372232Z","shell.execute_reply":"2022-06-30T15:01:00.388185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def valid(model, dataloader, device):\n    model.eval()\n\n    dataset_size = 0\n    running_loss = 0.0\n\n    preds = []\n\n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    for step, data in bar:\n        ids = data['ids'].to(device, dtype=torch.long)\n        mask = data['mask'].to(device, dtype=torch.long)\n\n        outputs = model(ids, mask)\n        outputs = F.softmax(outputs, dim=1)\n        preds.append(outputs.cpu().detach().numpy())\n\n    preds = np.concatenate(preds)\n    gc.collect()\n\n    return preds","metadata":{"execution":{"iopub.status.busy":"2022-06-30T15:01:00.391795Z","iopub.execute_input":"2022-06-30T15:01:00.392256Z","iopub.status.idle":"2022-06-30T15:01:00.408971Z","shell.execute_reply.started":"2022-06-30T15:01:00.392213Z","shell.execute_reply":"2022-06-30T15:01:00.40742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def inference(model_paths, dataloader, device):\n    final_preds = []\n    for i, path in enumerate(model_paths):\n        model = FeedBackModel(model_name)\n        model.to(device)\n        model.load_state_dict(torch.load(path))\n        \n        print(f\"Getting predictions for model {i+1}\")\n        preds = valid(model, dataloader, device)\n        final_preds.append(preds)\n    \n    final_preds = np.array(final_preds)\n    final_preds = np.mean(final_preds, axis=0)\n    return final_preds","metadata":{"execution":{"iopub.status.busy":"2022-06-30T15:01:00.411551Z","iopub.execute_input":"2022-06-30T15:01:00.412097Z","iopub.status.idle":"2022-06-30T15:01:00.42452Z","shell.execute_reply.started":"2022-06-30T15:01:00.412053Z","shell.execute_reply":"2022-06-30T15:01:00.422998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_preds = inference(model_path, test_loader, device)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T15:01:00.426483Z","iopub.execute_input":"2022-06-30T15:01:00.427556Z","iopub.status.idle":"2022-06-30T15:03:02.437238Z","shell.execute_reply.started":"2022-06-30T15:01:00.427512Z","shell.execute_reply":"2022-06-30T15:03:02.435739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission = pd.read_csv(\"../input/feedback-prize-effectiveness/sample_submission.csv\")\nprint(sample_submission.head())\n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-30T15:03:02.439747Z","iopub.execute_input":"2022-06-30T15:03:02.441617Z","iopub.status.idle":"2022-06-30T15:03:02.510967Z","shell.execute_reply.started":"2022-06-30T15:03:02.441565Z","shell.execute_reply":"2022-06-30T15:03:02.509436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission['Adequate'] = model_preds[:, 0]\nsample_submission['Effective'] = model_preds[:, 1]\nsample_submission['Ineffective'] = model_preds[:, 2]\n\nprint(sample_submission)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T15:03:02.512893Z","iopub.execute_input":"2022-06-30T15:03:02.513937Z","iopub.status.idle":"2022-06-30T15:03:02.565767Z","shell.execute_reply.started":"2022-06-30T15:03:02.51389Z","shell.execute_reply":"2022-06-30T15:03:02.564022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T15:03:02.567959Z","iopub.execute_input":"2022-06-30T15:03:02.568683Z","iopub.status.idle":"2022-06-30T15:03:02.5855Z","shell.execute_reply.started":"2022-06-30T15:03:02.568635Z","shell.execute_reply":"2022-06-30T15:03:02.583976Z"},"trusted":true},"execution_count":null,"outputs":[]}]}