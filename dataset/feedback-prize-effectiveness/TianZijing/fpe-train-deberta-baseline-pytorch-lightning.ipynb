{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install ../input/pytorchlightning160/pytorch_lightning-1.6.0-py3-none-any.whl","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-30T04:11:58.829567Z","iopub.execute_input":"2022-06-30T04:11:58.830015Z","iopub.status.idle":"2022-06-30T04:12:12.286178Z","shell.execute_reply.started":"2022-06-30T04:11:58.829927Z","shell.execute_reply":"2022-06-30T04:12:12.284976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# Library\n# ====================================================\nimport os\nimport gc\nimport re\nimport ast\nimport sys\nimport copy\nimport json\nimport time\nimport math\nimport shutil\nimport string\nimport pickle\nimport random\nimport joblib\nimport itertools\nfrom pathlib import Path\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport scipy as sp\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\nfrom tqdm.auto import tqdm\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\nfrom sklearn.preprocessing import LabelEncoder\n\nimport torch\nprint(f\"torch.__version__: {torch.__version__}\")\nimport torch.nn as nn\nfrom torch.nn import Parameter\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD, AdamW\nfrom torch.utils.data import DataLoader, Dataset\nimport pytorch_lightning as pl\nprint(f\"pytorch_lightning.__version__: {pl.__version__}\")\n\n# import tokenizers\nimport transformers\n# print(f\"tokenizers.__version__: {tokenizers.__version__}\")\nprint(f\"transformers.__version__: {transformers.__version__}\")\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig, DataCollatorWithPadding\nfrom transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n%env TOKENIZERS_PARALLELISM=true\n\n# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:12:12.288776Z","iopub.execute_input":"2022-06-30T04:12:12.28916Z","iopub.status.idle":"2022-06-30T04:12:22.449474Z","shell.execute_reply.started":"2022-06-30T04:12:12.289123Z","shell.execute_reply":"2022-06-30T04:12:22.447551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    ## 常规设置\n\n    data_dir = '../input/feedback-prize-effectiveness'\n    output_dir = '.'\n    model = \"../input/debertav3small\"\n\n    debug = False\n    debug_size = 0\n    train = True\n    #     seed = 42\n    seed = 2022\n    n_fold = 4\n    trn_fold = [0]\n    print_freq = 50\n\n    ## 模型设置\n    fc_dropout = 0.2\n    fgm = False\n    fgm_epsilion = 0.1\n    label_smooth = False\n    smoothing = 0.05\n\n    ## 优化器设置\n    scheduler = 'cosine'  # ['linear', 'cosine']\n    #     batch_scheduler = True\n    num_cycles = 0.5\n    warmup_steps = 0.2\n    encoder_lr = 2e-5\n    decoder_lr_ratio = 1\n    #     min_lr = 1e-6\n    eps = 1e-6\n    betas = (0.9, 0.999)\n    weight_decay = 0.01\n\n    ## 数据设置\n    d_padding = False  # Dynamic padding\n    num_workers = 4\n    batch_size = 16\n    max_len = 512\n    pin_memory = True\n    target_size = 3\n\n    ## Trainer设置\n    # \"amp_level\" is used only when \"precision = 16\" and \"amp_backend = 'apex' \",\n    # i.e it's only relevant for only one type of configuration and is generally not required.\n    apex = False\n    apex_level = 'O1'\n    max_epochs = 4\n    gradient_accumulation_steps = 4\n    precision = 32\n    max_grad_norm = 1 if precision == 32 else 500\n    fast_dev_run = 0  # 快速检验，取 n 个train, val, test batches\n    num_sanity_val_steps = 0  # 在开始前取 n 个val batches\n    val_check_interval = 0.5\n\n    if debug:\n        d_padding = False\n        debug_size = 1000\n        max_epochs = 2\n        num_workers = 0\n        trn_fold = [0]\n        fast_dev_run = 0\n        num_sanity_val_steps = 0\n        val_check_interval = 0.2\n","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:12:22.451183Z","iopub.execute_input":"2022-06-30T04:12:22.451846Z","iopub.status.idle":"2022-06-30T04:12:22.463342Z","shell.execute_reply.started":"2022-06-30T04:12:22.451808Z","shell.execute_reply":"2022-06-30T04:12:22.460929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Utils","metadata":{}},{"cell_type":"code","source":"def criterion(outputs, labels):\n    return nn.CrossEntropyLoss()(outputs, labels)\n\ndef get_score(outputs, labels):\n    outputs = F.softmax(torch.tensor(outputs)).numpy()\n    return log_loss(labels, outputs)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:12:22.466175Z","iopub.execute_input":"2022-06-30T04:12:22.46687Z","iopub.status.idle":"2022-06-30T04:12:22.478963Z","shell.execute_reply.started":"2022-06-30T04:12:22.466832Z","shell.execute_reply":"2022-06-30T04:12:22.477845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## DataModule","metadata":{}},{"cell_type":"code","source":"class FPEDataModule(pl.LightningDataModule):\n    def __init__(self, config, tokenizer_path=None, prepare_train=True, prepare_test=True):\n        super().__init__()\n        self.prepare_data_per_node = False\n        self.seed = config.seed\n        self.debug = config.debug\n        self.debug_size = 0 if self.debug == False else config.debug_size\n        self.shuffle = (self.debug == False)\n        self.batch_size = config.batch_size\n        self.pin_memory = config.pin_memory\n        self.num_workers = config.num_workers\n        self.max_len = config.max_len\n        self.data_dir = config.data_dir\n        self.output_dir = config.output_dir\n        self.n_fold = config.n_fold\n\n        self.select_dataset()\n        if tokenizer_path != None:\n            self.tokenizer = self.get_tokenizer(tokenizer_path)\n        else:\n            self.tokenizer = self.get_tokenizer(config.model)\n\n        self.prepare_train = prepare_train\n        self.prepare_test = prepare_test\n\n    def get_tokenizer(self, tokenizer_path):\n        tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n        return tokenizer\n\n    def set_trn_fold(self, trn_fold):\n        self.trn_fold = trn_fold\n\n    def select_dataset(self, dataset_name='structure_1'):\n        # TODO\n        self.TrainDataset = FeedBackDataset\n        self.TestDataset = FeedBackDataset\n\n    def CV_split(self, dataset, n_splits=5, debug=False, debug_size=1000, seed=0):\n        # StratifiedKFold\n        dataset['label'] = dataset['discourse_effectiveness'].map({'Ineffective': 0, 'Adequate': 1, 'Effective': 2})\n        if debug:\n            dataset = dataset.sample(n=debug_size, random_state=seed).reset_index(drop=True)\n\n        Fold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n        for n, (train_index, val_index) in enumerate(Fold.split(dataset, dataset['label'])):\n            dataset.loc[val_index, 'fold'] = int(n)\n        dataset['fold'] = dataset['fold'].astype(int)\n\n        return dataset\n\n    def prepare_data(self):\n        if self.prepare_train == True:\n            train = pd.read_csv(Path(self.data_dir) / 'train.csv')\n            # 将数据切分成 n 折\n            train = self.CV_split(train, n_splits=self.n_fold, debug=self.debug,\n                                  debug_size=self.debug_size)\n            self.train = train\n            self.prepare_train = False\n            print('Train data prepared!')\n\n        if self.prepare_test == True:\n            self.test = pd.read_csv(Path(self.data_dir) / 'test.csv')\n            self.submission = pd.read_csv(Path(self.data_dir) / 'sample_submission.csv')\n            self.prepare_test = False\n            print('Test data prepared!')\n\n    def setup(self, stage='fit'):\n        if stage == 'fit':\n            self.build_fit_dataset(trn_fold=self.trn_fold)\n\n        elif stage == 'test':\n            self.build_test_dataset()\n\n        elif stage == 'predict':\n            self.build_predict_dataset()\n\n    def build_fit_dataset(self, trn_fold=None):\n        df = self.train\n        if trn_fold != None:\n            self.train_df = df[df['fold'] != trn_fold].reset_index(drop=True)\n            self.val_df = df[df['fold'] == trn_fold].reset_index(drop=True)\n            self.train_dataset = self.TrainDataset(self.train_df, self.tokenizer, self.max_len)\n            self.val_dataset = self.TrainDataset(self.val_df, self.tokenizer, self.max_len)\n\n    def build_test_dataset(self):\n        self.test_dataset = self.TestDataset(self.test, self.tokenizer, self.max_len)\n\n    def build_predict_dataset(self):\n        self.predict_dataset = self.TestDataset(self.test, self.tokenizer, self.max_len)\n\n    def train_dataloader(self):\n        loader = DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=self.num_workers,\n                            pin_memory=self.pin_memory, shuffle=self.shuffle, collate_fn=self.train_dataset.collate)\n        return loader\n\n    def val_dataloader(self):\n        loader = DataLoader(self.val_dataset, batch_size=self.batch_size * 4, num_workers=self.num_workers,\n                            shuffle=False, collate_fn=self.val_dataset.collate)\n        return loader\n\n    def test_dataloader(self):\n        loader = DataLoader(self.test_dataset, batch_size=self.batch_size * 4, num_workers=self.num_workers,\n                            shuffle=False)\n        return loader\n\n    def predict_dataloader(self):\n        loader = DataLoader(self.predict_dataset, batch_size=self.batch_size * 4, num_workers=self.num_workers,\n                            shuffle=False)\n        return loader","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:12:22.492299Z","iopub.execute_input":"2022-06-30T04:12:22.492722Z","iopub.status.idle":"2022-06-30T04:12:22.51955Z","shell.execute_reply.started":"2022-06-30T04:12:22.492688Z","shell.execute_reply":"2022-06-30T04:12:22.518389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset","metadata":{}},{"cell_type":"code","source":"class FeedBackDataset(Dataset):\n    def __init__(self, df, tokenizer, max_length):\n        self.df = df\n        self.max_len = max_length\n        self.tokenizer = tokenizer\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index):\n        item = self.df.iloc[index]\n        text = item['discourse_text'] + '[SEP]' + item['discourse_type']\n        label = item['label']\n        \n        inputs = self.tokenizer(\n            text,\n            truncation=True,\n            add_special_tokens=True,\n            max_length = self.max_len,\n        )\n        return {\n            'input_ids':inputs['input_ids'],\n            'attention_mask':inputs['attention_mask'],\n            'label':label\n            }\n    def collate(self, batch):\n        dcp = DataCollatorWithPadding(tokenizer=self.tokenizer)\n        return dcp(batch)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:12:22.521917Z","iopub.execute_input":"2022-06-30T04:12:22.523671Z","iopub.status.idle":"2022-06-30T04:12:22.533505Z","shell.execute_reply.started":"2022-06-30T04:12:22.523636Z","shell.execute_reply":"2022-06-30T04:12:22.532606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"class AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:12:22.535172Z","iopub.execute_input":"2022-06-30T04:12:22.535716Z","iopub.status.idle":"2022-06-30T04:12:22.5467Z","shell.execute_reply.started":"2022-06-30T04:12:22.535677Z","shell.execute_reply":"2022-06-30T04:12:22.545828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# MeanPooling trick\nclass MeanPooling(nn.Module):\n    def __init__(self):\n        super(MeanPooling, self).__init__()\n\n    def forward(self, last_hidden_state, attention_mask):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings / sum_mask\n        return mean_embeddings","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:12:22.547769Z","iopub.execute_input":"2022-06-30T04:12:22.549089Z","iopub.status.idle":"2022-06-30T04:12:22.558249Z","shell.execute_reply.started":"2022-06-30T04:12:22.549061Z","shell.execute_reply":"2022-06-30T04:12:22.557305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FPEModel(pl.LightningModule):\n    def __init__(self, config, model_config_path=None, pretrained=False, weight_path=None):\n        super().__init__()\n        self.save_hyperparameters('config')\n\n        if model_config_path:\n            self.model_config = AutoConfig.from_pretrained(model_config_path, output_hidden_states=True)\n            # self.model_config = torch.load(model_config_path)\n        else:\n            self.model_config = AutoConfig.from_pretrained(config.model, output_hidden_states=True)\n        if pretrained:\n            self.model = AutoModel.from_pretrained(config.model, config=self.model_config)\n        else:\n            self.model = AutoModel.from_config(self.model_config)\n\n        self.pooler = MeanPooling()\n\n        self.fc = nn.Linear(self.model_config.hidden_size, config.target_size)\n\n        # TODO multi_dropout / layer norm\n        self.dropout_0 = nn.Dropout(config.fc_dropout / 2.)\n        self.dropout_1 = nn.Dropout(config.fc_dropout / 1.5)\n        self.dropout_2 = nn.Dropout(config.fc_dropout)\n        self.dropout_3 = nn.Dropout(config.fc_dropout * 1.5)\n        self.dropout_4 = nn.Dropout(config.fc_dropout * 2.)\n\n        self.init_weight(self.fc)\n        self.set_metrics()\n\n        if config.label_smooth:\n            self.criterion = LabelSmoothLoss(smoothing=config.smoothing,\n                                             loss_func=nn.BCEWithLogitsLoss(reduction=\"mean\"))\n        else:\n            self.criterion = nn.CrossEntropyLoss(reduction=\"mean\")\n\n        if hasattr(self.hparams.config, 'fgm') and self.hparams.config.fgm:\n            self.automatic_optimization = False\n            self.fgm = FGM(self, config.fgm_epsilion)\n\n        if weight_path != None:\n            weight = torch.load(weight_path, map_location='cpu')\n            if 'state_dict' in weight.keys():\n                weight = weight['state_dict']\n            self.load_state_dict(weight)\n\n    def forward(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_state, pooler_output = outputs[0], outputs[1]\n        feature = self.pooler(last_hidden_state, inputs['attention_mask'])\n        output_0 = self.fc(self.dropout_0(feature))\n        output_1 = self.fc(self.dropout_1(feature))\n        output_2 = self.fc(self.dropout_2(feature))\n        output_3 = self.fc(self.dropout_3(feature))\n        output_4 = self.fc(self.dropout_4(feature))\n        return (output_0 + output_1 + output_2 + output_3 + output_4) / 5\n    \n    def set_metrics(self):\n        self.train_losses = AverageMeter()\n        self.val_losses = AverageMeter()\n        self.val_acc = AverageMeter()\n\n        self.train_losses.reset()\n        self.val_losses.reset()\n        self.val_acc.reset()\n\n    def init_weight(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.model_config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.model_config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n\n    def training_step(self, batch, batch_idx):\n        labels = batch.pop('labels')\n        y_preds = self.forward(batch).float()\n        loss = self.criterion(y_preds, labels)\n        self.train_losses.update(loss.item(), len(labels))\n        self.log('train/avg_loss', self.train_losses.avg)\n        # 因为 optimizer 有 3 组参数，所有 get_last_lr() 会返回含有 3 个元素的列表\n        en_lr = self.trainer.lr_scheduler_configs[0].scheduler.get_last_lr()[0]\n        de_lr = self.trainer.lr_scheduler_configs[0].scheduler.get_last_lr()[-1]\n        self.log('train/en_lr', en_lr, prog_bar=True)\n        self.log('train/de_lr', de_lr, prog_bar=True)\n\n        if (self.trainer.global_step) % self.hparams.config.print_freq == 0 and batch_idx % self.trainer.accumulate_grad_batches == 0:\n            self.print('Global step:{global_step}.'\n                       'Train Loss: {loss.val:.4f}(avg: {loss.avg:.4f}) '\n                       'Encoder LR: {en_lr:.3E}, Decoder LR: {de_lr:.3E}'\n                       .format(global_step=self.trainer.global_step,\n                               loss=self.train_losses,\n                               en_lr=en_lr,\n                               de_lr=de_lr))\n        return loss\n\n    def training_epoch_end(self, outs):\n        torch.cuda.empty_cache()\n\n    def on_validation_start(self) -> None:\n        self.start_val_time = time.time()\n\n    def validation_step(self, batch, batch_idx):\n        labels = batch.pop('labels')\n        y_preds = self.forward(batch).float()\n        loss = self.criterion(y_preds, labels)\n        self.val_losses.update(loss.item(), len(labels))\n        self.log('val/avg_loss', self.val_losses.avg)\n        return y_preds.cpu().numpy(), labels.cpu()\n\n    def validation_epoch_end(self, outs):\n        preds = np.concatenate([item[0] for item in outs])\n        val_labels = np.concatenate([item[1] for item in outs])\n\n        val_loss_avg = self.val_losses.avg\n        #  ======================== scoring ============================\n        log_loss = get_score(preds, val_labels)\n        self.log(f'val/loss_avg', val_loss_avg)\n        self.log(f'val/log_loss', log_loss)\n        self.print(f'Global step:{self.trainer.global_step}.\\n Val loss avg: {val_loss_avg:.4f}, log_loss: {log_loss:.4f}')\n\n        self.val_losses.reset()\n        self.val_acc.reset()\n\n        self.preds = preds  # 保存每折的预测，用于CV评分\n        # self.trainer.checkpoint_callback.current_score = torch.tensor(round(log_loss,4))\n\n    def on_validation_end(self) -> None:\n        ### callbacks里保存oof\n        current_score = self.trainer.callback_metrics[self.trainer.checkpoint_callback.monitor]\n        if current_score == self.trainer.checkpoint_callback.best_model_score:\n            self.oof_states = self.preds\n        eval_time = time.time() - self.start_val_time\n\n    def predict_step(self, batch, batch_idx, dataloader_idx=None):\n        inputs = batch\n        y_preds = self.forward(inputs)\n        return y_preds.sigmoid().cpu().numpy()\n\n    def configure_optimizers(self):\n        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n        encoder_lr = self.hparams.config.encoder_lr\n        decoder_lr = self.hparams.config.decoder_lr_ratio * encoder_lr\n        num_cycles = self.hparams.config.num_cycles\n        # end_lr = self.hparams.config.min_lr\n        weight_decay = self.hparams.config.weight_decay\n        eps = self.hparams.config.eps\n        betas = self.hparams.config.betas\n        optimizer_parameters = [\n            {'params': [p for n, p in self.model.named_parameters()\n                        if not any(nd in n for nd in no_decay)],\n             'lr': encoder_lr, 'weight_decay': weight_decay,\n             },\n            {'params': [p for n, p in self.model.named_parameters()\n                        if any(nd in n for nd in no_decay)],\n             'lr': encoder_lr, 'weight_decay': 0.0,\n             },\n            {'params': [p for n, p in self.named_parameters() if \"model\" not in n],\n             'lr': decoder_lr, 'weight_decay': 0.0,\n             }\n        ]\n        optimizer = AdamW(optimizer_parameters,\n                          lr=encoder_lr, eps=eps, betas=betas)\n\n        if self.trainer.max_steps == None or self.trainer.max_epochs != None:\n            max_steps = (\n                    len(self.trainer.datamodule.train_dataloader()) * self.trainer.max_epochs\n                    // self.hparams.config.gradient_accumulation_steps\n            )\n        else:\n            max_steps = self.trainer.max_steps\n\n        warmup_steps = self.hparams.config.warmup_steps\n        if isinstance(warmup_steps, float):\n            warmup_steps = int(warmup_steps * max_steps)\n\n        print(f'====== Max steps: {max_steps},\\t Warm up steps: {warmup_steps} =========')\n\n        if self.hparams.config.scheduler == 'linear':\n            scheduler = get_linear_schedule_with_warmup(\n                optimizer, num_warmup_steps=warmup_steps, num_training_steps=max_steps,\n            )\n        elif self.hparams.config.scheduler == 'cosine':\n            scheduler = get_cosine_schedule_with_warmup(\n                optimizer, num_warmup_steps=warmup_steps, num_training_steps=max_steps,\n                num_cycles=num_cycles\n            )\n        else:\n            scheduler = None\n        sched = {\n            'scheduler': scheduler, 'interval': 'step'\n        }\n        return ([optimizer], [sched])","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:12:22.56196Z","iopub.execute_input":"2022-06-30T04:12:22.564221Z","iopub.status.idle":"2022-06-30T04:12:22.604484Z","shell.execute_reply.started":"2022-06-30T04:12:22.564195Z","shell.execute_reply":"2022-06-30T04:12:22.603397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train","metadata":{}},{"cell_type":"code","source":"pl.seed_everything(CFG.seed)\ndm = FPEDataModule(CFG, prepare_test=False)\ndm.prepare_data()\n\nprint(f\"train epochs:{CFG.max_epochs},\\t batch_size:{CFG.batch_size} * {CFG.gradient_accumulation_steps}\")\nprint(f\"FGM:{CFG.fgm}_{CFG.fgm_epsilion}, \\t label_smooth:{CFG.label_smooth}_{CFG.smoothing},\")\nprint(f\"encoder_lr:{CFG.encoder_lr},\\t\", f\"decoder_lr:{CFG.encoder_lr * CFG.decoder_lr_ratio}\")\nprint(f\"precision:{CFG.precision}, grad_norm:{CFG.max_grad_norm}, \\t apex:{CFG.apex}_{CFG.apex_level}\")","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:12:22.606177Z","iopub.execute_input":"2022-06-30T04:12:22.606459Z","iopub.status.idle":"2022-06-30T04:12:23.686753Z","shell.execute_reply.started":"2022-06-30T04:12:22.606433Z","shell.execute_reply":"2022-06-30T04:12:23.685545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for train_fold in CFG.trn_fold:\n    fgm_p = 'fgm_' if CFG.fgm else ''\n    ls = 'ls_' if CFG.label_smooth else ''\n    prefix = f'{fgm_p}{ls}fold{train_fold}'\n    checkpoint_callback = pl.callbacks.ModelCheckpoint(\n        filename=prefix + 'step{step}-val_log_loss{val/log_loss:.4f}',\n        auto_insert_metric_name=False,\n        save_top_k=1, monitor='val/log_loss', mode='min', save_last=False, verbose=True, save_weights_only=True,\n    )\n    callbacks = [checkpoint_callback]\n    dm.set_trn_fold(train_fold)\n\n    model = FPEModel(CFG, model_config_path=None, pretrained=True)\n    print('Load model!')\n    trainer = pl.Trainer(\n        gpus=[0],\n        default_root_dir=f\"seed{CFG.seed}_fold{train_fold}_from_{CFG.model.split('/')[-1]}\",\n        log_every_n_steps=10,\n        amp_backend=\"apex\" if CFG.apex else \"native\",\n        amp_level=CFG.apex_level if CFG.apex else None,\n        precision=16 if CFG.apex else CFG.precision,\n        max_epochs=CFG.max_epochs,\n        callbacks=callbacks,\n        gradient_clip_val=None if CFG.fgm else CFG.max_grad_norm,\n        accumulate_grad_batches=None if CFG.fgm else CFG.gradient_accumulation_steps,\n        fast_dev_run=CFG.fast_dev_run,\n        num_sanity_val_steps=CFG.num_sanity_val_steps,\n        val_check_interval=CFG.val_check_interval,\n    )\n    # \"amp_level\" is used only when \"precision = 16\" and \"amp_backend = 'apex' \",\n    # i.e it's only relevant for only one type of configuration and is generally not required.\n\n    trainer.fit(model, datamodule=dm)\n    torch.save(model.oof_states, model.trainer.checkpoint_callback.dirpath + f'/fold{train_fold}.oof')","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:12:23.689216Z","iopub.execute_input":"2022-06-30T04:12:23.690243Z","iopub.status.idle":"2022-06-30T04:52:56.513847Z","shell.execute_reply.started":"2022-06-30T04:12:23.690199Z","shell.execute_reply":"2022-06-30T04:52:56.512742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}