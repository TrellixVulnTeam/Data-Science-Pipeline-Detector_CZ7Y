{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport re\nimport nltk\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # plotting charts\nfrom nltk.corpus import stopwords\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix,accuracy_score\nfrom keras.models import Sequential\nfrom keras import layers\nfrom keras.layers import Dense ,LSTM ,Dropout ,BatchNormalization,Bidirectional ,Embedding\nimport string\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom keras.preprocessing.text import Tokenizer\nfrom nltk.tokenize import TweetTokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.callbacks import EarlyStopping\nfrom tensorflow.keras.optimizers import Adam\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-28T19:03:50.050862Z","iopub.execute_input":"2022-06-28T19:03:50.051882Z","iopub.status.idle":"2022-06-28T19:03:50.939006Z","shell.execute_reply.started":"2022-06-28T19:03:50.051844Z","shell.execute_reply":"2022-06-28T19:03:50.937982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/feedback-prize-effectiveness/train.csv\")\ntest_data = pd.read_csv(\"/kaggle/input/feedback-prize-effectiveness/test.csv\")\nsubmission = pd.read_csv(\"/kaggle/input/feedback-prize-effectiveness/sample_submission.csv\")\ntrain_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-28T18:30:52.531148Z","iopub.execute_input":"2022-06-28T18:30:52.532309Z","iopub.status.idle":"2022-06-28T18:30:52.831704Z","shell.execute_reply.started":"2022-06-28T18:30:52.532269Z","shell.execute_reply":"2022-06-28T18:30:52.830643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#####################################\n##########Perform some Cleaning######\n#####################################\n\nnltk.download(\"stopwords\")\nnltk.download(\"wordnet\")\n\nlemmatizer = WordNetLemmatizer()\n\ndef word_lemmatize(text):\n    text = text.split()\n    text = [lemmatizer.lemmatize(w) for w in text]\n    text = ' '.join(text)\n    return text\n\ntrain_data['discourse_text'] = train_data['discourse_text'].apply(word_lemmatize)","metadata":{"execution":{"iopub.status.busy":"2022-06-28T18:30:52.833175Z","iopub.execute_input":"2022-06-28T18:30:52.833914Z","iopub.status.idle":"2022-06-28T18:31:42.283315Z","shell.execute_reply.started":"2022-06-28T18:30:52.833875Z","shell.execute_reply":"2022-06-28T18:31:42.28232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The maximum number of words to be used. (most frequent)\nMAX_NB_WORDS = 50000\nMAX_SEQUENCE_LENGTH = 250\nEMBEDDING_DIM = 100\ntokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\ntokenizer.fit_on_texts(train_data['discourse_text'].values)\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))\nX = tokenizer.texts_to_sequences(train_data['discourse_text'].values)\nX = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\nprint('Shape of data tensor:', X.shape)\nZ = tokenizer.texts_to_sequences(test_data['discourse_text'].values)\nZ = pad_sequences(Z, maxlen=MAX_SEQUENCE_LENGTH)\nprint('Shape of data tensor:', Z.shape)\nY = pd.get_dummies(train_data['discourse_effectiveness']).values\nprint('Shape of label tensor:', Y.shape)\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.1, random_state = 42)\nprint(X_train.shape,Y_train.shape)\nprint(X_test.shape,Y_test.shape)\nmodel = Sequential()\nmodel.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\nmodel.add(LSTM(100,return_sequences=True))\nmodel.add(Dropout(0.4))\nmodel.add(LSTM(100,return_sequences=True))\nmodel.add(Dropout(0.4))\nmodel.add(LSTM(100,return_sequences=True))\nmodel.add(Dropout(0.4))\nmodel.add(LSTM(100,return_sequences=False))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(3, activation='sigmoid'))\noptimizer = Adam(lr=0.001)\nmodel.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\nepochs = 10\nbatch_size = 32\nhistory = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])","metadata":{"execution":{"iopub.status.busy":"2022-06-28T18:31:42.285391Z","iopub.execute_input":"2022-06-28T18:31:42.285774Z","iopub.status.idle":"2022-06-28T18:32:41.472203Z","shell.execute_reply.started":"2022-06-28T18:31:42.285718Z","shell.execute_reply":"2022-06-28T18:32:41.471159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = (model.predict(Z))","metadata":{"execution":{"iopub.status.busy":"2022-06-28T18:50:34.153416Z","iopub.execute_input":"2022-06-28T18:50:34.154187Z","iopub.status.idle":"2022-06-28T18:50:34.25897Z","shell.execute_reply.started":"2022-06-28T18:50:34.154147Z","shell.execute_reply":"2022-06-28T18:50:34.257936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(results)\ndf = df.rename(columns={0:\"Adequate\",1:\"Ineffective\",2:\"Effective\"})\ndf_index = pd.DataFrame(test_data.discourse_id)\nsubmission = pd.concat([df_index,df],axis=1)\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-28T18:50:47.19381Z","iopub.execute_input":"2022-06-28T18:50:47.194399Z","iopub.status.idle":"2022-06-28T18:50:47.205964Z","shell.execute_reply.started":"2022-06-28T18:50:47.194361Z","shell.execute_reply":"2022-06-28T18:50:47.204917Z"},"trusted":true},"execution_count":null,"outputs":[]}]}