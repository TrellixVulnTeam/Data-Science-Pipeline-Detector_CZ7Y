{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tensorflow import keras\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        os.path.join(dirname, filename)\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n                     ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-29T17:03:58.237491Z","iopub.execute_input":"2022-06-29T17:03:58.238082Z","iopub.status.idle":"2022-06-29T17:04:06.106558Z","shell.execute_reply.started":"2022-06-29T17:03:58.237943Z","shell.execute_reply":"2022-06-29T17:04:06.105567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Model\ndef embedds(wordtoidx,vocab_size):\n        embeddings_index = {} \n        f = open('../input/glove6b/glove.6B.300d.txt', encoding=\"utf-8\")\n        for line in f:\n            values = line.split()\n            word = values[0]\n            coefs = np.asarray(values[1:], dtype='float32')\n            embeddings_index[word] = coefs\n        embedding_dim = 300\n        embedding_matrix = np.zeros((vocab_size, embedding_dim))\n        for word, i in wordtoidx.items():\n                embedding_vector = embeddings_index.get(word)\n                if embedding_vector is not None:\n                    embedding_matrix[i] = embedding_vector\n        return  embedding_matrix  ","metadata":{"execution":{"iopub.status.busy":"2022-06-29T17:13:54.199983Z","iopub.execute_input":"2022-06-29T17:13:54.200398Z","iopub.status.idle":"2022-06-29T17:13:54.210098Z","shell.execute_reply.started":"2022-06-29T17:13:54.200366Z","shell.execute_reply":"2022-06-29T17:13:54.208655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd_train=pd.read_csv(\"../input/feedback-prize-effectiveness/train.csv\")\ntext=pd_train['discourse_text']\nlen(text)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-29T17:04:06.120984Z","iopub.execute_input":"2022-06-29T17:04:06.121589Z","iopub.status.idle":"2022-06-29T17:04:06.484478Z","shell.execute_reply.started":"2022-06-29T17:04:06.121555Z","shell.execute_reply":"2022-06-29T17:04:06.483641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import string\nfrom textblob import TextBlob\nfrom gensim.parsing.preprocessing import STOPWORDS\nall_stopwords_gensim = STOPWORDS.union(set(['the', 'The']))\ntable = str.maketrans('', '', string.punctuation)\nbad_chars = [';', ':', '!', \"*\", \" \",\",\",\"'\"]\nMAX_LEN=0\nimport re\ndef correct_sentence_spelling(sentence):\n    \n    sentence = TextBlob(sentence)\n    \n    result = sentence.correct()\n    \n    print(result)\ndef decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase\ndef Data_preprocessing(text):\n    list_cleaned=[]\n    for i in range(len(text)):\n        words =text[i].translate(table)\n        #removing punctuation from our data\n        words=words.split()\n        stripped_filtered = [w.lower() for w in words if not w in all_stopwords_gensim]\n        stripped_filtered_2=[w for w in stripped_filtered ]\n        list_cleaned.append(stripped_filtered_2)\n    return list_cleaned\n","metadata":{"execution":{"iopub.status.busy":"2022-06-29T17:04:06.486407Z","iopub.execute_input":"2022-06-29T17:04:06.486669Z","iopub.status.idle":"2022-06-29T17:04:08.135951Z","shell.execute_reply.started":"2022-06-29T17:04:06.486638Z","shell.execute_reply":"2022-06-29T17:04:08.134991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# calculate the length of the description with the most words\ndef max_length(descriptions):\n    return max([len(d) for d in descriptions])\ndef create_tokenizer(descriptions):\n        tokenizer = Tokenizer()\n        tokenizer.fit_on_texts(descriptions)\n        return tokenizer","metadata":{"execution":{"iopub.status.busy":"2022-06-29T17:04:08.137372Z","iopub.execute_input":"2022-06-29T17:04:08.137633Z","iopub.status.idle":"2022-06-29T17:04:08.142974Z","shell.execute_reply.started":"2022-06-29T17:04:08.137604Z","shell.execute_reply":"2022-06-29T17:04:08.142007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.sequence import pad_sequences\n\ndef create_sequences(tokenizer, max_length, desc_list, vocab_size):\n        X1=list()\n        # walk through each description for the image\n        for l in range(len(desc_list)):\n            # encode the sequence\n            seq = tokenizer.texts_to_sequences([desc_list[l]])\n            # split one sequence into multiple X,y pairs\n            \n            in_seq = pad_sequences(seq[0:], maxlen=max_length)\n            #store\n            X1.append(in_seq)\n        return np.array(X1)","metadata":{"execution":{"iopub.status.busy":"2022-06-29T17:04:08.144204Z","iopub.execute_input":"2022-06-29T17:04:08.14448Z","iopub.status.idle":"2022-06-29T17:04:08.155615Z","shell.execute_reply.started":"2022-06-29T17:04:08.144443Z","shell.execute_reply":"2022-06-29T17:04:08.154663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelBinarizer\nencoder = LabelBinarizer()\ntext_type_output = encoder.fit_transform(pd_train['discourse_type'])\ntext_eff_output = encoder.fit_transform(pd_train['discourse_effectiveness'])\nprint(text_eff_output.shape,text_type_output.shape)\ndesc_list=Data_preprocessing(text)\ntokenizer = create_tokenizer(desc_list)\nwordtoidx=tokenizer.word_index\nvocab_size = len(tokenizer.word_index) + 1\nprint('Vocabulary Size: %d' % vocab_size,'samples Size: %d' % len(desc_list))\nmax_length=max_length(desc_list)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-29T17:12:01.523605Z","iopub.execute_input":"2022-06-29T17:12:01.523905Z","iopub.status.idle":"2022-06-29T17:12:03.536387Z","shell.execute_reply.started":"2022-06-29T17:12:01.523877Z","shell.execute_reply":"2022-06-29T17:12:03.535338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ninputs=create_sequences(tokenizer, max_length, desc_list, vocab_size)\n\nprint(inputs[1],desc_list[1])\n","metadata":{"execution":{"iopub.status.busy":"2022-06-29T17:12:10.987229Z","iopub.execute_input":"2022-06-29T17:12:10.987541Z","iopub.status.idle":"2022-06-29T17:12:12.829231Z","shell.execute_reply.started":"2022-06-29T17:12:10.98751Z","shell.execute_reply":"2022-06-29T17:12:12.828502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#now we gonna create the embedding matrx\nembedding_matrx=embedds(wordtoidx,vocab_size)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-29T17:14:08.235214Z","iopub.execute_input":"2022-06-29T17:14:08.23553Z","iopub.status.idle":"2022-06-29T17:15:00.17755Z","shell.execute_reply.started":"2022-06-29T17:14:08.235498Z","shell.execute_reply":"2022-06-29T17:15:00.176846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_length","metadata":{"execution":{"iopub.status.busy":"2022-06-29T17:15:00.178726Z","iopub.execute_input":"2022-06-29T17:15:00.179259Z","iopub.status.idle":"2022-06-29T17:15:00.184934Z","shell.execute_reply.started":"2022-06-29T17:15:00.179226Z","shell.execute_reply":"2022-06-29T17:15:00.184133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import LeakyReLU\nimport tensorflow as tf\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\nwith tpu_strategy.scope():\n    embedding_layer=layers.Embedding(vocab_size,300,mask_zero=True,weights=[embedding_matrx], input_length=360,trainable=False)\n    input_layer=layers.Input(shape=(1,360))\n    inputs_flattened=layers.Flatten()(input_layer)\n    input_layer2=layers.Input(shape=(7,))\n    input_layer2_2=layers.Dense(240)(input_layer2)\n    l_relu=LeakyReLU()(input_layer2_2)\n    embeds=embedding_layer(inputs_flattened)\n    lstm_layer=layers.LSTM(240,return_sequences=True)(embeds)\n    drop_out=layers.Dropout(0.1)(lstm_layer)\n    gru_layer=layers.GRU(240)(drop_out)\n    dnn_layer=layers.Dense(240,'relu')(gru_layer)\n    drop_out2=layers.Dropout(0.1)(dnn_layer)\n    add_layer=layers.Add()([drop_out2,l_relu])\n    output1=layers.Dense(3,'softmax')(add_layer)\n    model_fb=Model([input_layer,input_layer2],output1)\n    model_fb.compile( optimizer='adam', loss='categorical_crossentropy',metrics='acc')\n    \nprint(model_fb.summary())","metadata":{"execution":{"iopub.status.busy":"2022-06-29T17:25:13.104384Z","iopub.execute_input":"2022-06-29T17:25:13.104702Z","iopub.status.idle":"2022-06-29T17:25:20.920458Z","shell.execute_reply.started":"2022-06-29T17:25:13.104672Z","shell.execute_reply":"2022-06-29T17:25:20.919558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 16 * tpu_strategy.num_replicas_in_sync\nmodel_history=model_fb.fit([inputs,text_type_output],text_eff_output,validation_split=0.01,epochs=20, batch_size=BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2022-06-29T17:25:36.038256Z","iopub.execute_input":"2022-06-29T17:25:36.039159Z","iopub.status.idle":"2022-06-29T17:30:35.219392Z","shell.execute_reply.started":"2022-06-29T17:25:36.039122Z","shell.execute_reply":"2022-06-29T17:30:35.218633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dict_types=dict()\nfor x in range(len(pd_train['discourse_type'])):\n    dict_types[pd_train['discourse_type'][x]]=text_type_output[x]\nmodel_fb.save('feedbacks.h5')\nimport pickle\nwith open('tokenizer.pickle', 'wb') as handle:\n    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n    \n","metadata":{"execution":{"iopub.status.busy":"2022-06-29T17:38:05.951206Z","iopub.execute_input":"2022-06-29T17:38:05.951579Z","iopub.status.idle":"2022-06-29T17:38:06.75408Z","shell.execute_reply.started":"2022-06-29T17:38:05.951534Z","shell.execute_reply":"2022-06-29T17:38:06.753061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#   ***preprocessing on test data***","metadata":{}},{"cell_type":"code","source":"\npd_test=pd.read_csv(\"../input/feedback-prize-effectiveness/test.csv\")\ntext_test=pd_test['discourse_text']\ndesc_list_test=Data_preprocessing(text_test)\ninputs_test=create_sequences(tokenizer, max_length, desc_list_test, vocab_size)\ntext_test","metadata":{"execution":{"iopub.status.busy":"2022-06-29T17:39:56.878121Z","iopub.execute_input":"2022-06-29T17:39:56.878479Z","iopub.status.idle":"2022-06-29T17:39:56.893686Z","shell.execute_reply.started":"2022-06-29T17:39:56.878445Z","shell.execute_reply":"2022-06-29T17:39:56.892765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs_test.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-29T17:39:58.637982Z","iopub.execute_input":"2022-06-29T17:39:58.638783Z","iopub.status.idle":"2022-06-29T17:39:58.644838Z","shell.execute_reply.started":"2022-06-29T17:39:58.638731Z","shell.execute_reply":"2022-06-29T17:39:58.644081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs2=[]\nfor i in pd_test['discourse_type']:\n    inputs2.append(dict_types[i])\n    \ninputs2=np.array(inputs2)","metadata":{"execution":{"iopub.status.busy":"2022-06-29T17:38:15.66799Z","iopub.execute_input":"2022-06-29T17:38:15.668767Z","iopub.status.idle":"2022-06-29T17:38:15.674155Z","shell.execute_reply.started":"2022-06-29T17:38:15.668723Z","shell.execute_reply":"2022-06-29T17:38:15.673076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs2","metadata":{"execution":{"iopub.status.busy":"2022-06-29T17:40:10.071222Z","iopub.execute_input":"2022-06-29T17:40:10.07155Z","iopub.status.idle":"2022-06-29T17:40:10.078485Z","shell.execute_reply.started":"2022-06-29T17:40:10.07152Z","shell.execute_reply":"2022-06-29T17:40:10.077585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicts=model_fb.predict([inputs_test,inputs2])\nprint(predicts)","metadata":{"execution":{"iopub.status.busy":"2022-06-29T18:10:16.924997Z","iopub.execute_input":"2022-06-29T18:10:16.925782Z","iopub.status.idle":"2022-06-29T18:10:17.361533Z","shell.execute_reply.started":"2022-06-29T18:10:16.925742Z","shell.execute_reply":"2022-06-29T18:10:17.360566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicts=pd.DataFrame(predicts)\npredicts.rename({0: 'Adequate',1: 'Effective',2:'Ineffective'},axis='columns',inplace=True)\npredicts","metadata":{"execution":{"iopub.status.busy":"2022-06-29T18:13:31.594901Z","iopub.execute_input":"2022-06-29T18:13:31.595747Z","iopub.status.idle":"2022-06-29T18:13:31.609369Z","shell.execute_reply.started":"2022-06-29T18:13:31.595709Z","shell.execute_reply":"2022-06-29T18:13:31.608448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicts.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-29T18:18:17.226668Z","iopub.execute_input":"2022-06-29T18:18:17.227296Z","iopub.status.idle":"2022-06-29T18:18:17.23327Z","shell.execute_reply.started":"2022-06-29T18:18:17.227253Z","shell.execute_reply":"2022-06-29T18:18:17.232614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}