{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# BERT Model \n\nBERT and other Transformer encoder architectures have been wildly successful on a variety of tasks in NLP (natural language processing). They compute vector-space representations of natural language that are suitable for use in deep learning models. The BERT family of models uses the Transformer encoder architecture to process each token of input text in the full context of all tokens before and after, hence the name: Bidirectional Encoder Representations from Transformers.\n\nBERT models are usually pre-trained on a large corpus of text, then fine-tuned for specific tasks.\n\n\n<p style=\"text-align:center;\"><img src=\"https://cdn-images-1.medium.com/max/1000/1*-oQKmzvHrzqeSQEnM9f_kQ.png\" alt=\"BertModel\" height=600 width = 600></p>","metadata":{}},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom transformers import TFBertModel\nimport transformers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-28T18:07:49.339733Z","iopub.execute_input":"2022-05-28T18:07:49.340281Z","iopub.status.idle":"2022-05-28T18:07:57.212079Z","shell.execute_reply.started":"2022-05-28T18:07:49.340196Z","shell.execute_reply":"2022-05-28T18:07:57.21128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading DataFrame","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/feedback-prize-effectiveness/train.csv\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-28T18:07:57.213768Z","iopub.execute_input":"2022-05-28T18:07:57.214372Z","iopub.status.idle":"2022-05-28T18:07:57.505665Z","shell.execute_reply.started":"2022-05-28T18:07:57.214337Z","shell.execute_reply":"2022-05-28T18:07:57.504814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['discourse_type'].unique()","metadata":{"execution":{"iopub.status.busy":"2022-05-28T18:07:57.507598Z","iopub.execute_input":"2022-05-28T18:07:57.50826Z","iopub.status.idle":"2022-05-28T18:07:57.523559Z","shell.execute_reply.started":"2022-05-28T18:07:57.508222Z","shell.execute_reply":"2022-05-28T18:07:57.522746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"AUTO = tf.data.experimental.AUTOTUNE\n# Configuration\nEPOCHS = 3\nBATCH_SIZE = 16\nMAX_LEN = 256","metadata":{"execution":{"iopub.status.busy":"2022-05-28T18:07:57.527216Z","iopub.execute_input":"2022-05-28T18:07:57.527472Z","iopub.status.idle":"2022-05-28T18:07:57.531315Z","shell.execute_reply.started":"2022-05-28T18:07:57.52745Z","shell.execute_reply":"2022-05-28T18:07:57.530477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Bert Encoder","metadata":{}},{"cell_type":"code","source":"def bert_encode(texts, tokenizer, max_len=MAX_LEN):\n    input_ids = []\n    token_type_ids = []\n    attention_mask = []\n    \n    for text in texts:\n        token = tokenizer(text, max_length=max_len, truncation=True, padding='max_length',\n                         add_special_tokens=True)\n        input_ids.append(token['input_ids'])\n        token_type_ids.append(token['token_type_ids'])\n        attention_mask.append(token['attention_mask'])\n    \n    return np.array(input_ids), np.array(token_type_ids), np.array(attention_mask)","metadata":{"execution":{"iopub.status.busy":"2022-05-28T18:07:57.532443Z","iopub.execute_input":"2022-05-28T18:07:57.532852Z","iopub.status.idle":"2022-05-28T18:07:57.541578Z","shell.execute_reply.started":"2022-05-28T18:07:57.532817Z","shell.execute_reply":"2022-05-28T18:07:57.540824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# First load the real tokenizer\ntokenizer = transformers.BertTokenizer.from_pretrained('../input/huggingface-bert-variants/bert-base-cased/bert-base-cased')\n# Save the loaded tokenizer locally\ntokenizer.save_pretrained('.')","metadata":{"execution":{"iopub.status.busy":"2022-05-28T18:07:57.542756Z","iopub.execute_input":"2022-05-28T18:07:57.543293Z","iopub.status.idle":"2022-05-28T18:07:57.647562Z","shell.execute_reply.started":"2022-05-28T18:07:57.543254Z","shell.execute_reply":"2022-05-28T18:07:57.646836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sep = tokenizer.sep_token\nsep","metadata":{"execution":{"iopub.status.busy":"2022-05-28T18:07:57.648681Z","iopub.execute_input":"2022-05-28T18:07:57.649043Z","iopub.status.idle":"2022-05-28T18:07:57.65537Z","shell.execute_reply.started":"2022-05-28T18:07:57.649005Z","shell.execute_reply":"2022-05-28T18:07:57.654353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['inputs'] = df.discourse_type + sep +df.discourse_text","metadata":{"execution":{"iopub.status.busy":"2022-05-28T18:07:57.656778Z","iopub.execute_input":"2022-05-28T18:07:57.657573Z","iopub.status.idle":"2022-05-28T18:07:57.683363Z","shell.execute_reply.started":"2022-05-28T18:07:57.657546Z","shell.execute_reply":"2022-05-28T18:07:57.682574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_label = {\"discourse_effectiveness\": {\"Ineffective\": 0, \"Adequate\": 1, \"Effective\": 2}}\ndf = df.replace(new_label)\ndf = df.rename(columns = {\"discourse_effectiveness\": \"label\"})","metadata":{"execution":{"iopub.status.busy":"2022-05-28T18:07:57.684773Z","iopub.execute_input":"2022-05-28T18:07:57.685143Z","iopub.status.idle":"2022-05-28T18:07:57.729435Z","shell.execute_reply.started":"2022-05-28T18:07:57.685107Z","shell.execute_reply":"2022-05-28T18:07:57.72861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-28T18:07:57.732388Z","iopub.execute_input":"2022-05-28T18:07:57.732781Z","iopub.status.idle":"2022-05-28T18:07:57.743538Z","shell.execute_reply.started":"2022-05-28T18:07:57.732754Z","shell.execute_reply":"2022-05-28T18:07:57.7427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_valid, y_train, y_valid = train_test_split(df['inputs'], df['label'], test_size=0.12, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-05-28T18:07:57.744999Z","iopub.execute_input":"2022-05-28T18:07:57.745575Z","iopub.status.idle":"2022-05-28T18:07:58.352594Z","shell.execute_reply.started":"2022-05-28T18:07:57.745538Z","shell.execute_reply":"2022-05-28T18:07:58.35182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = bert_encode(X_train.astype(str), tokenizer)\nX_valid = bert_encode(X_valid.astype(str), tokenizer)\n\ny_train = y_train.values\ny_valid = y_valid.values","metadata":{"execution":{"iopub.status.busy":"2022-05-28T18:07:58.35398Z","iopub.execute_input":"2022-05-28T18:07:58.354315Z","iopub.status.idle":"2022-05-28T18:08:53.271544Z","shell.execute_reply.started":"2022-05-28T18:07:58.354281Z","shell.execute_reply":"2022-05-28T18:08:53.270763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((X_train, y_train))\n    .repeat()\n    .shuffle(2048)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((X_valid, y_valid))\n    .batch(BATCH_SIZE)\n    .cache()\n    .prefetch(AUTO)\n)","metadata":{"execution":{"iopub.status.busy":"2022-05-28T18:08:53.272904Z","iopub.execute_input":"2022-05-28T18:08:53.273259Z","iopub.status.idle":"2022-05-28T18:08:58.866417Z","shell.execute_reply.started":"2022-05-28T18:08:53.273223Z","shell.execute_reply":"2022-05-28T18:08:58.865511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Building Model","metadata":{}},{"cell_type":"code","source":"def build_model(bert_model, max_len=MAX_LEN):    \n    input_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n    token_type_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"token_type_ids\")\n    attention_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\")\n\n    sequence_output = bert_model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)[0]\n    clf_output = sequence_output[:, 0, :]\n    clf_output = Dropout(.1)(clf_output)\n    out = Dense(3, activation='softmax')(clf_output)\n    \n    model = Model(inputs=[input_ids, token_type_ids, attention_mask], outputs=out)\n    model.compile(Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2022-05-28T18:08:58.869586Z","iopub.execute_input":"2022-05-28T18:08:58.870621Z","iopub.status.idle":"2022-05-28T18:08:58.879548Z","shell.execute_reply.started":"2022-05-28T18:08:58.870579Z","shell.execute_reply":"2022-05-28T18:08:58.878538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntransformer_layer = (TFBertModel.from_pretrained('../input/huggingface-bert-variants/bert-base-cased/bert-base-cased'))\nmodel = build_model(transformer_layer, max_len=MAX_LEN)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-05-28T18:08:58.88095Z","iopub.execute_input":"2022-05-28T18:08:58.881405Z","iopub.status.idle":"2022-05-28T18:09:15.008135Z","shell.execute_reply.started":"2022-05-28T18:08:58.881366Z","shell.execute_reply":"2022-05-28T18:09:15.007253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_history = model.fit(\n    train_dataset,\n    steps_per_epoch=200,\n    validation_data=valid_dataset,\n    epochs=10\n)","metadata":{"execution":{"iopub.status.busy":"2022-05-28T18:09:15.009418Z","iopub.execute_input":"2022-05-28T18:09:15.011577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv(\"../input/feedback-prize-effectiveness/test.csv\")\ntest['text'] = test.discourse_type + sep +test.discourse_text\ntest.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_text = bert_encode(test.text.astype(str), tokenizer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.read_csv(\"../input/feedback-prize-effectiveness/sample_submission.csv\")\nsub.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = model.predict(test_text, verbose=1)\npreds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub['Ineffective'] = preds[:,0]\nsub['Adequate'] = preds[:,1]\nsub['Effective'] = preds[:,2]\nsub","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}