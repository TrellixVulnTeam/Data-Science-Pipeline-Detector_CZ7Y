{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Read train and test csv and import the all essay texts in new column \"text\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-10T11:22:18.590705Z","iopub.execute_input":"2022-06-10T11:22:18.59106Z","iopub.status.idle":"2022-06-10T11:22:33.159049Z","shell.execute_reply.started":"2022-06-10T11:22:18.59103Z","shell.execute_reply":"2022-06-10T11:22:33.158152Z"}}},{"cell_type":"code","source":"import pandas as pd\ndf_train = pd.read_csv('/kaggle/input/feedback-prize-effectiveness/train.csv')\ndf_test = pd.read_csv('/kaggle/input/feedback-prize-effectiveness/test.csv')\ndf_train[\"text\"] = df_train[\"essay_id\"].apply(lambda x: open(f'/kaggle/input/feedback-prize-effectiveness/train/{x}.txt').read())\n#df_test[\"text\"] = df_test[\"essay_id\"].apply(lambda x: open(f'/kaggle/input/feedback-prize-effectiveness/test/{x}.txt').read())\ndf_train.head()\n","metadata":{"execution":{"iopub.status.busy":"2022-06-12T08:22:52.115957Z","iopub.execute_input":"2022-06-12T08:22:52.116721Z","iopub.status.idle":"2022-06-12T08:23:17.902303Z","shell.execute_reply.started":"2022-06-12T08:22:52.116308Z","shell.execute_reply":"2022-06-12T08:23:17.901527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Convert Discourse Effectiveness to numeric codes - 0, 1, 2","metadata":{}},{"cell_type":"code","source":"effectiveness_map = {\"Ineffective\":0, \"Adequate\":1,\"Effective\":2}\ndf_train[\"target\"] = df_train[\"discourse_effectiveness\"].map(effectiveness_map)","metadata":{"execution":{"iopub.status.busy":"2022-06-12T08:23:17.903638Z","iopub.execute_input":"2022-06-12T08:23:17.905007Z","iopub.status.idle":"2022-06-12T08:23:17.917448Z","shell.execute_reply.started":"2022-06-12T08:23:17.904966Z","shell.execute_reply":"2022-06-12T08:23:17.916769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load the Bert Base Tokenizer","metadata":{}},{"cell_type":"code","source":"from transformers import BertTokenizer\n#initialize tokenizer\ntokenizer = BertTokenizer.from_pretrained('../input/huggingface-bert-variants/bert-base-cased/bert-base-cased')","metadata":{"execution":{"iopub.status.busy":"2022-06-10T11:22:33.171434Z","iopub.execute_input":"2022-06-10T11:22:33.172013Z","iopub.status.idle":"2022-06-10T11:22:33.212535Z","shell.execute_reply.started":"2022-06-10T11:22:33.171979Z","shell.execute_reply":"2022-06-10T11:22:33.211749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Check if Discourse Effectiveness is related with Discourse Type","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nsns.set_theme(style=\"darkgrid\")\nsns.set(rc={\"figure.figsize\": (10, 10)})\n\nsns.countplot(x='discourse_type', hue='discourse_effectiveness', data = df_train)","metadata":{"execution":{"iopub.status.busy":"2022-06-10T12:44:42.743212Z","iopub.execute_input":"2022-06-10T12:44:42.744103Z","iopub.status.idle":"2022-06-10T12:44:43.17194Z","shell.execute_reply.started":"2022-06-10T12:44:42.744067Z","shell.execute_reply":"2022-06-10T12:44:43.171124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### It looks like Discourse Effectiveness and Discourse Type have some relation\n### Add Discourse Type to part of input text to the model","metadata":{}},{"cell_type":"code","source":"df_train['text']  = df_train['discourse_type'] + tokenizer.sep_token + df_train['text']\ndf_test['text']  = df_train['discourse_type'] + tokenizer.sep_token + df_train['discourse_text']","metadata":{"execution":{"iopub.status.busy":"2022-06-10T11:22:33.214426Z","iopub.execute_input":"2022-06-10T11:22:33.214852Z","iopub.status.idle":"2022-06-10T11:22:33.279556Z","shell.execute_reply.started":"2022-06-10T11:22:33.214815Z","shell.execute_reply":"2022-06-10T11:22:33.27746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Initialize Input Ids and Attention Masks tensors","metadata":{}},{"cell_type":"code","source":"import numpy as np\nX_input_ids = np.zeros((len(df_train), 256))\nX_attn_masks = np.zeros((len(df_train), 256))","metadata":{"execution":{"iopub.status.busy":"2022-06-10T11:22:33.280934Z","iopub.execute_input":"2022-06-10T11:22:33.281404Z","iopub.status.idle":"2022-06-10T11:22:33.297388Z","shell.execute_reply.started":"2022-06-10T11:22:33.281368Z","shell.execute_reply":"2022-06-10T11:22:33.296577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Function to Bert Encode the input text\n#### This function will convert the input text to input ids and attention masks using bert tokenizer","metadata":{}},{"cell_type":"code","source":"def encode_data(df, ids, masks, tokenizer):\n    for i, text in tqdm(enumerate(df['text'])):\n        tokenized_text = tokenizer.encode_plus(\n            text,\n            max_length=256, \n            truncation=True, \n            padding='max_length', \n            add_special_tokens=True,\n            return_tensors='tf'\n        )\n        ids[i, :] = tokenized_text.input_ids\n        masks[i, :] = tokenized_text.attention_mask\n    return ids, masks","metadata":{"execution":{"iopub.status.busy":"2022-06-10T11:22:33.298625Z","iopub.execute_input":"2022-06-10T11:22:33.299035Z","iopub.status.idle":"2022-06-10T11:22:33.309694Z","shell.execute_reply.started":"2022-06-10T11:22:33.299011Z","shell.execute_reply":"2022-06-10T11:22:33.308935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Encode the train text","metadata":{}},{"cell_type":"code","source":"from tqdm.auto import tqdm\n\nX_input_ids, X_attn_masks = encode_data(df_train, X_input_ids, X_attn_masks, tokenizer)","metadata":{"execution":{"iopub.status.busy":"2022-06-10T11:22:33.311079Z","iopub.execute_input":"2022-06-10T11:22:33.311452Z","iopub.status.idle":"2022-06-10T11:30:17.082504Z","shell.execute_reply.started":"2022-06-10T11:22:33.311417Z","shell.execute_reply":"2022-06-10T11:30:17.081585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Prepare the y label tensor (Discourse effectiveness)","metadata":{}},{"cell_type":"code","source":"labels = np.zeros((len(df_train), 3))\nlabels[np.arange(len(df_train)), df_train['target'].values] = 1\nlabels","metadata":{"execution":{"iopub.status.busy":"2022-06-10T11:30:17.084007Z","iopub.execute_input":"2022-06-10T11:30:17.084393Z","iopub.status.idle":"2022-06-10T11:30:17.093765Z","shell.execute_reply.started":"2022-06-10T11:30:17.084356Z","shell.execute_reply":"2022-06-10T11:30:17.093064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Creating a data pipeline using tensorflow dataset utility, creates batches of data for easy loading","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\n\ndef DatasetMapFunction(input_ids, attn_masks, labels):\n    return {\n        'input_ids': input_ids,\n        'attention_mask': attn_masks\n    }, labels\n\ndataset = tf.data.Dataset.from_tensor_slices((X_input_ids, X_attn_masks, labels))\n\ndataset = dataset.map(DatasetMapFunction)     # converting to required format for tensorflow dataset\ndataset = dataset.shuffle(10000).batch(16, drop_remainder=True) # batch size, drop any left out tensor","metadata":{"execution":{"iopub.status.busy":"2022-06-10T11:30:17.094958Z","iopub.execute_input":"2022-06-10T11:30:17.096029Z","iopub.status.idle":"2022-06-10T11:30:17.35796Z","shell.execute_reply.started":"2022-06-10T11:30:17.095992Z","shell.execute_reply":"2022-06-10T11:30:17.357271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Split the train dataset into training and validation dataset in 80:20","metadata":{}},{"cell_type":"code","source":"p = 0.8\ntrain_size = int((len(df_train)//16)*p) # for each 16 batch of data we will have len(df)//16 samples, take 80% of that for train.\ntrain_dataset = dataset.take(train_size)\nval_dataset = dataset.skip(train_size)","metadata":{"execution":{"iopub.status.busy":"2022-06-10T11:30:17.411505Z","iopub.execute_input":"2022-06-10T11:30:17.411831Z","iopub.status.idle":"2022-06-10T11:30:17.419041Z","shell.execute_reply.started":"2022-06-10T11:30:17.411798Z","shell.execute_reply":"2022-06-10T11:30:17.418335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load Bert Base Cased Model","metadata":{}},{"cell_type":"code","source":"from transformers import TFBertModel\nmodel = TFBertModel.from_pretrained('../input/huggingface-bert-variants/bert-base-cased/bert-base-cased') # bert base model with pretrained weights","metadata":{"execution":{"iopub.status.busy":"2022-06-10T11:30:17.420714Z","iopub.execute_input":"2022-06-10T11:30:17.421367Z","iopub.status.idle":"2022-06-10T11:30:23.444533Z","shell.execute_reply.started":"2022-06-10T11:30:17.421257Z","shell.execute_reply":"2022-06-10T11:30:23.443785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Define the model with 2 input layers for input_ids and attn_masks","metadata":{}},{"cell_type":"code","source":"input_ids = tf.keras.layers.Input(shape=(256,), name='input_ids', dtype='int32')\nattn_masks = tf.keras.layers.Input(shape=(256,), name='attention_mask', dtype='int32')\n\nbert_embds = model.bert(input_ids, attention_mask=attn_masks)[1] # 0 -> activation layer (3D), 1 -> pooled output layer (2D)\nintermediate_layer = tf.keras.layers.Dense(512, activation='relu', name='intermediate_layer')(bert_embds)\noutput_layer = tf.keras.layers.Dense(3, activation='softmax', name='output_layer')(intermediate_layer) # softmax -> calcs probs of classes\n\ndiscourse_model = tf.keras.Model(inputs=[input_ids, attn_masks], outputs=output_layer)\ndiscourse_model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-06-10T11:30:23.445817Z","iopub.execute_input":"2022-06-10T11:30:23.446229Z","iopub.status.idle":"2022-06-10T11:30:24.787348Z","shell.execute_reply.started":"2022-06-10T11:30:23.446191Z","shell.execute_reply":"2022-06-10T11:30:24.785857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Set the Loss, Optimizer and Metrics parameters for the model","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam\ndiscourse_model.compile(optimizer=Adam(learning_rate=1e-5, decay=1e-6), \n                        loss='categorical_crossentropy', \n                        metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2022-06-10T11:30:24.788705Z","iopub.execute_input":"2022-06-10T11:30:24.78909Z","iopub.status.idle":"2022-06-10T11:30:24.802368Z","shell.execute_reply.started":"2022-06-10T11:30:24.789053Z","shell.execute_reply":"2022-06-10T11:30:24.801537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Fit the model\n##### Epoch was set to 5 as the validation accuracy seems to deteriorate for greater epochs","metadata":{}},{"cell_type":"code","source":"history = discourse_model.fit(\n    train_dataset,\n    steps_per_epoch=200,\n    validation_data=val_dataset,\n    epochs=5\n)","metadata":{"execution":{"iopub.status.busy":"2022-06-10T11:30:24.803748Z","iopub.execute_input":"2022-06-10T11:30:24.804041Z","iopub.status.idle":"2022-06-10T11:41:54.921058Z","shell.execute_reply.started":"2022-06-10T11:30:24.804017Z","shell.execute_reply":"2022-06-10T11:41:54.9201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Encode the train dataset and Predict","metadata":{}},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('../input/huggingface-bert-variants/bert-base-cased/bert-base-cased')\ndf_test['text']  = df_test['discourse_type'] + tokenizer.sep_token + df_test['text']\nX_test_input_ids = np.zeros((len(df_test), 256))\nX_test_attn_masks = np.zeros((len(df_test), 256))\nX_test_input_ids, X_test_attn_masks = encode_data(df_test, X_test_input_ids, X_test_attn_masks, tokenizer)\n\npred_labels = discourse_model.predict([X_test_input_ids, X_test_attn_masks] )","metadata":{"execution":{"iopub.status.busy":"2022-06-10T11:43:16.819937Z","iopub.execute_input":"2022-06-10T11:43:16.820841Z","iopub.status.idle":"2022-06-10T11:43:17.193416Z","shell.execute_reply.started":"2022-06-10T11:43:16.820802Z","shell.execute_reply":"2022-06-10T11:43:17.192714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Generate the Sample Submission file","metadata":{}},{"cell_type":"code","source":"sample_submission = pd.read_csv('/kaggle/input/feedback-prize-effectiveness/sample_submission.csv')\nsample_submission.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-10T11:41:57.771406Z","iopub.execute_input":"2022-06-10T11:41:57.772007Z","iopub.status.idle":"2022-06-10T11:41:57.794745Z","shell.execute_reply.started":"2022-06-10T11:41:57.77197Z","shell.execute_reply":"2022-06-10T11:41:57.79405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission['discourse_id'] = df_test['discourse_id']\nsample_submission['Ineffective'] = pred_labels[:,0]\nsample_submission['Adequate'] = pred_labels[:,1]\nsample_submission['Effective'] = pred_labels[:,2]\nsample_submission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-10T11:44:06.80189Z","iopub.execute_input":"2022-06-10T11:44:06.802348Z","iopub.status.idle":"2022-06-10T11:44:06.813232Z","shell.execute_reply.started":"2022-06-10T11:44:06.802284Z","shell.execute_reply":"2022-06-10T11:44:06.812138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission","metadata":{},"execution_count":null,"outputs":[]}]}