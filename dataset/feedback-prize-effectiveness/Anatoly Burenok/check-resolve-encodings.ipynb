{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#### What can we gain from from resolving encoding errors?\n\nDiscussion here\n> https://www.kaggle.com/code/brandonhu0215/feedback-deberta-large-lb0-619/comments\n\n@vad13irt\n> I checked it too, but on the validation performance, and results were worse. It is weird.\n\n@ivanaerlic\n> Yeah, I got worse results with resolve_encodings_and_normalize too.","metadata":{}},{"cell_type":"markdown","source":"# 1. Import & Def & Set & Load","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\nimport codecs\nfrom text_unidecode import unidecode\nfrom typing import Tuple\n\nfrom transformers import AutoModel, AutoTokenizer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-29T22:24:53.632373Z","iopub.execute_input":"2022-06-29T22:24:53.632724Z","iopub.status.idle":"2022-06-29T22:25:06.483744Z","shell.execute_reply.started":"2022-06-29T22:24:53.632652Z","shell.execute_reply":"2022-06-29T22:25:06.482139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def replace_encoding_with_utf8(error: UnicodeError) -> Tuple[bytes, int]:\n    return error.object[error.start : error.end].encode(\"utf-8\"), error.end\n\n\ndef replace_decoding_with_cp1252(error: UnicodeError) -> Tuple[str, int]:\n    return error.object[error.start : error.end].decode(\"cp1252\"), error.end\n\n\ncodecs.register_error(\"replace_encoding_with_utf8\", replace_encoding_with_utf8)\ncodecs.register_error(\"replace_decoding_with_cp1252\", replace_decoding_with_cp1252)\n\n\ndef resolve_encodings_and_normalize(text: str) -> str:\n    text = (\n        text.encode(\"raw_unicode_escape\")\n        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n        .encode(\"cp1252\", errors=\"replace_encoding_with_utf8\")\n        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n    )\n    \n    text = unidecode(text)\n    \n    return text","metadata":{"execution":{"iopub.status.busy":"2022-06-29T22:25:06.486058Z","iopub.execute_input":"2022-06-29T22:25:06.487725Z","iopub.status.idle":"2022-06-29T22:25:06.497904Z","shell.execute_reply.started":"2022-06-29T22:25:06.487684Z","shell.execute_reply":"2022-06-29T22:25:06.496837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_LEN = 150\nCHECK_ROW = 15\nRANDOM_STATE = 42\n\npd.set_option('display.max_colwidth', MAX_LEN)","metadata":{"execution":{"iopub.status.busy":"2022-06-29T22:25:06.499326Z","iopub.execute_input":"2022-06-29T22:25:06.499657Z","iopub.status.idle":"2022-06-29T22:25:06.525851Z","shell.execute_reply.started":"2022-06-29T22:25:06.499623Z","shell.execute_reply":"2022-06-29T22:25:06.524818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_path = \"../input/feedback-prize-effectiveness/train.csv\"\ncols_list = ['essay_id', 'discourse_text']\n\ndf_origin = pd.read_csv(data_path, usecols=cols_list)\n\ndf_origin.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-29T22:25:06.528155Z","iopub.execute_input":"2022-06-29T22:25:06.528518Z","iopub.status.idle":"2022-06-29T22:25:06.888479Z","shell.execute_reply.started":"2022-06-29T22:25:06.528491Z","shell.execute_reply":"2022-06-29T22:25:06.887237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Update text & Select rows","metadata":{}},{"cell_type":"code","source":"df = df_origin.copy()\n\ndf['discourse_text'] = df['discourse_text'].str.strip()\ndf['discourse_text_UPD'] = df['discourse_text'].apply(resolve_encodings_and_normalize)\n\nlen_mask = df['discourse_text'].str.len() < MAX_LEN\ndiff_mask = (df['discourse_text'] != df['discourse_text_UPD'])\n\ndf = df.loc[diff_mask & len_mask, :]\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-29T22:25:06.889972Z","iopub.execute_input":"2022-06-29T22:25:06.890368Z","iopub.status.idle":"2022-06-29T22:25:09.746198Z","shell.execute_reply.started":"2022-06-29T22:25:06.890333Z","shell.execute_reply":"2022-06-29T22:25:09.744832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(diff_mask.sum())  # df['discourse_text'] != df['discourse_text_UPD']\nprint(len(df))          # diff_mask & len_mask","metadata":{"execution":{"iopub.status.busy":"2022-06-29T22:25:09.747603Z","iopub.execute_input":"2022-06-29T22:25:09.747916Z","iopub.status.idle":"2022-06-29T22:25:09.755112Z","shell.execute_reply.started":"2022-06-29T22:25:09.747883Z","shell.execute_reply":"2022-06-29T22:25:09.75427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Load tokenizers","metadata":{}},{"cell_type":"code","source":"tokenizers_info = [\n    ('deberta', '../input/feedback-deberta-large-051/tokenizer'),\n    ('roberta', '../input/roberta-base')\n]","metadata":{"execution":{"iopub.status.busy":"2022-06-29T22:25:09.75632Z","iopub.execute_input":"2022-06-29T22:25:09.757536Z","iopub.status.idle":"2022-06-29T22:25:09.801981Z","shell.execute_reply.started":"2022-06-29T22:25:09.757477Z","shell.execute_reply":"2022-06-29T22:25:09.800495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizers_dict = {}\n\nfor x in tokenizers_info:\n    name, path = x\n    tokenizers_dict[name] = AutoTokenizer.from_pretrained(path)\n    \nprint(tokenizers_dict.keys())","metadata":{"execution":{"iopub.status.busy":"2022-06-29T22:25:09.804017Z","iopub.execute_input":"2022-06-29T22:25:09.804538Z","iopub.status.idle":"2022-06-29T22:25:10.174768Z","shell.execute_reply.started":"2022-06-29T22:25:09.804508Z","shell.execute_reply":"2022-06-29T22:25:10.173599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Check tokenizers","metadata":{}},{"cell_type":"code","source":"samples = df.sample(n=CHECK_ROW, random_state=RANDOM_STATE).sort_index()\n\nfor row in samples.iterrows():\n    indx, data = row\n    discourse_text = data.discourse_text\n    discourse_text_UPD = data.discourse_text_UPD\n\n    print(f'\\n\\tindex: {indx}')        \n    \n    for x in tokenizers_dict.keys():\n        print(f'\\n\\t=== === tokenizer: {x} === ===')\n        print()\n        print('Origin text: ', repr(discourse_text))\n        print('tokens:      ', tokenizers_dict.get(x).tokenize(discourse_text))        \n        print('input_ids:   ', tokenizers_dict.get(x)(discourse_text)['input_ids'])\n        print()\n        print('Updated text:', repr(discourse_text_UPD))\n        print('tokens:      ', tokenizers_dict.get(x).tokenize(discourse_text_UPD))        \n        print('input_ids:   ', tokenizers_dict.get(x)(discourse_text_UPD)['input_ids'])\n","metadata":{"execution":{"iopub.status.busy":"2022-06-29T22:25:10.176387Z","iopub.execute_input":"2022-06-29T22:25:10.177067Z","iopub.status.idle":"2022-06-29T22:25:10.221703Z","shell.execute_reply.started":"2022-06-29T22:25:10.177037Z","shell.execute_reply":"2022-06-29T22:25:10.220762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}