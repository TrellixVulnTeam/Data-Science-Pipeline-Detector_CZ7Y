{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport pandas as pd\nimport os\nimport random\nimport string\nimport matplotlib.pyplot as plt\nimport argparse\nimport time\nimport tensorflow as tf\n\nfrom tensorflow.python.keras.preprocessing import sequence\nfrom tensorflow.python.keras.preprocessing import text\nfrom tensorflow.python.keras import models\nfrom tensorflow.python.keras import initializers\nfrom tensorflow.python.keras import regularizers\n\nfrom tensorflow.python.keras.layers import Dense\nfrom tensorflow.python.keras.layers import Dropout\nfrom tensorflow.python.keras.layers import Embedding\nfrom tensorflow.python.keras.layers import SeparableConv1D\nfrom tensorflow.python.keras.layers import MaxPooling1D\nfrom tensorflow.python.keras.layers import GlobalAveragePooling1D\n\ninput_path='../input/feedback-prize-effectiveness/'\ntest_path=os.path.join(input_path,'test/')\ntrain_path=os.path.join(input_path,'train/')\n\ntrain_df=pd.read_csv(os.path.join(input_path,'train.csv'))\ntest_df=pd.read_csv(os.path.join(input_path,'test.csv'))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-02T02:01:40.145692Z","iopub.execute_input":"2022-06-02T02:01:40.146063Z","iopub.status.idle":"2022-06-02T02:01:40.306306Z","shell.execute_reply.started":"2022-06-02T02:01:40.146032Z","shell.execute_reply":"2022-06-02T02:01:40.305507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#How many training examples do we have? \ntrain_len=len(train_df)\n\n#How common is each rating in the training set? \nadq=train_df[\"discourse_effectiveness\"].value_counts()['Adequate']\neff=train_df[\"discourse_effectiveness\"].value_counts()['Effective']\nineff=train_df[\"discourse_effectiveness\"].value_counts()['Ineffective']\n\n#Calculate the percentages\npct_adq = adq/train_len\npct_eff = eff/train_len\npct_ineff = ineff/train_len\n\nprint(pct_adq,pct_eff,pct_ineff)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-02T02:01:43.966297Z","iopub.execute_input":"2022-06-02T02:01:43.967239Z","iopub.status.idle":"2022-06-02T02:01:43.990726Z","shell.execute_reply.started":"2022-06-02T02:01:43.967191Z","shell.execute_reply":"2022-06-02T02:01:43.989964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#inspect the training data\ntrain_df","metadata":{"execution":{"iopub.status.busy":"2022-06-02T02:01:46.958608Z","iopub.execute_input":"2022-06-02T02:01:46.958981Z","iopub.status.idle":"2022-06-02T02:01:46.979647Z","shell.execute_reply.started":"2022-06-02T02:01:46.958947Z","shell.execute_reply":"2022-06-02T02:01:46.978816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#inspect the discourse types\n#Note, for this basic model we will ignore the different types\ntrain_df.discourse_type.unique()","metadata":{"execution":{"iopub.status.busy":"2022-06-02T02:01:50.315759Z","iopub.execute_input":"2022-06-02T02:01:50.316522Z","iopub.status.idle":"2022-06-02T02:01:50.326249Z","shell.execute_reply.started":"2022-06-02T02:01:50.316486Z","shell.execute_reply":"2022-06-02T02:01:50.325369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Adapted from \n#https://developers.google.com/machine-learning/guides/text-classification/step-2\n\nlbl_dict={ 'Adequate':0,\n            'Effective':1,\n            'Ineffective':2 }\n\nstoplist = set('for a of the and to in'.split(' ')) #optional: Note, this particular list didn't improve the model\n\ndef fix_text(txt):\n    fix_txt=''\n    #Optional - remove punctuation\n    #Note: Removing punctuation here does not improve the model\n#    txt=txt.translate(str.maketrans('', '', string.punctuation))\n    # Lowercase each item, optionally filter out stopwords\n    words=[word for word in txt.lower().split()] ## optionally, add--> if word not in stoplist]\n    for word in words:\n        fix_txt+=(word+' ')\n    return fix_txt\n\ndef load_train_val(df,seed=123):\n    len_df=len(df)\n    train_txt=[]\n    train_lbl=[]\n    #Embed the type into the text at end, so never truncated\n    #& encode the effectiveness label as integer\n    for i in range(len_df):\n        txt=fix_text(df.iloc[i].discourse_text)\n        train_txt.append(txt)\n        lbl=lbl_dict[df.iloc[i].discourse_effectiveness]\n        train_lbl.append(lbl)\n    #Shuffle all text and labels\n    random.seed(seed)\n    random.shuffle(train_txt)\n    random.seed(seed)\n    random.shuffle(train_lbl)\n    #Split the training and validation sets\n    train_len=int(len_df*.8)\n    val_txt=train_txt[train_len:]\n    train_txt=train_txt[:train_len]\n    val_lbl=train_lbl[train_len:]\n    train_lbl=train_lbl[:train_len]\n    #Return the training and validation sets\n    return ((train_txt,np.array(train_lbl)),(val_txt,np.array(val_lbl)))\n","metadata":{"execution":{"iopub.status.busy":"2022-06-02T02:01:54.21535Z","iopub.execute_input":"2022-06-02T02:01:54.216005Z","iopub.status.idle":"2022-06-02T02:01:54.225154Z","shell.execute_reply.started":"2022-06-02T02:01:54.215971Z","shell.execute_reply":"2022-06-02T02:01:54.224359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(tr_txt,tr_lbl),(val_txt,val_lbl)=load_train_val(train_df)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T02:01:59.986233Z","iopub.execute_input":"2022-06-02T02:01:59.986586Z","iopub.status.idle":"2022-06-02T02:02:04.699672Z","shell.execute_reply.started":"2022-06-02T02:01:59.986558Z","shell.execute_reply":"2022-06-02T02:02:04.698881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#inspect some of the data\nprint(tr_txt[0],tr_lbl[0])\nprint(len(tr_txt))\nprint(val_txt[0],val_lbl[0])\nprint(len(val_txt))","metadata":{"execution":{"iopub.status.busy":"2022-06-02T02:02:08.048896Z","iopub.execute_input":"2022-06-02T02:02:08.049252Z","iopub.status.idle":"2022-06-02T02:02:08.05637Z","shell.execute_reply.started":"2022-06-02T02:02:08.049222Z","shell.execute_reply":"2022-06-02T02:02:08.055584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Adapted From \n#https://developers.google.com/machine-learning/guides/text-classification/step-2\n\ndef get_num_words_per_sample(sample_texts):\n    \"\"\"Returns the median number of words per sample given corpus.\n\n    # Arguments\n        sample_texts: list, sample texts.\n\n    # Returns\n        int, median number of words per sample.\n    \"\"\"\n    num_words = [len(s.split()) for s in sample_texts]\n    return np.median(num_words)\n\ndef plot_sample_length_distribution(sample_texts):\n    \"\"\"Plots the sample length distribution.\n\n    # Arguments\n        samples_texts: list, sample texts.\n    \"\"\"\n    plt.hist([len(s.split()) for s in sample_texts], 50)\n    plt.xlabel('Length of a sample in words')\n    plt.ylabel('Number of samples')\n    plt.title('Sample length distribution')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-02T02:09:36.317588Z","iopub.execute_input":"2022-06-02T02:09:36.317983Z","iopub.status.idle":"2022-06-02T02:09:36.324958Z","shell.execute_reply.started":"2022-06-02T02:09:36.317939Z","shell.execute_reply":"2022-06-02T02:09:36.324122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#inspect \nprint(get_num_words_per_sample(tr_txt))\nplot_sample_length_distribution(tr_txt)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T02:09:42.964234Z","iopub.execute_input":"2022-06-02T02:09:42.964581Z","iopub.status.idle":"2022-06-02T02:09:43.488524Z","shell.execute_reply.started":"2022-06-02T02:09:42.964553Z","shell.execute_reply":"2022-06-02T02:09:43.487765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With ~37,000 training samples and there is a median of 30 words per sample, the ratio of # samples / # of words per sample is greater than 1,500 but less than 15,000.  \n\nAccording to Google's analysis posted here, \nhttps://developers.google.com/machine-learning/guides/text-classification/step-2-5\nwe will likely get best results by tokenizing the text as sequences and using sepCNN model to classify them, using a fine-tuned pre-trained embedding. \n\nFor this notebook, I have not included a pre-trained embedding.\n","metadata":{}},{"cell_type":"code","source":"#Adapted from:\n#https://developers.google.com/machine-learning/guides/text-classification/step-3\n\n# Vectorization parameters\n# Limit on the number of features\nTOP_K = 10000 #can try different values here\n\n# Limit on the length of text sequences. Sequences longer than this\n# will be truncated.\nMAX_SEQUENCE_LENGTH = 100 #can try different values here\n\ndef sequence_vectorize(train_texts, val_texts):\n    \"\"\"Vectorizes texts as sequence vectors.\n    1 text = 1 sequence vector with fixed length.\n    # Arguments\n        train_texts: list, training text strings.\n        val_texts: list, validation text strings.\n    # Returns\n        x_train, x_val, word_index: vectorized training and validation\n            texts and word index dictionary.\n    \"\"\"\n    # Create vocabulary with training texts.\n    tokenizer = text.Tokenizer(num_words=TOP_K)\n    tokenizer.fit_on_texts(train_texts)\n\n    # Vectorize training and validation texts.\n    x_train = tokenizer.texts_to_sequences(train_texts)\n    x_val = tokenizer.texts_to_sequences(val_texts)\n\n    # Fix sequence length to max value. Sequences shorter than the length are\n    # padded in the beginning and sequences longer are truncated\n    # at the beginning.\n    x_train = sequence.pad_sequences(x_train, maxlen=MAX_SEQUENCE_LENGTH)\n    x_val = sequence.pad_sequences(x_val, maxlen=MAX_SEQUENCE_LENGTH)\n    return x_train, x_val, tokenizer","metadata":{"execution":{"iopub.status.busy":"2022-06-02T02:03:37.502444Z","iopub.execute_input":"2022-06-02T02:03:37.502796Z","iopub.status.idle":"2022-06-02T02:03:37.509428Z","shell.execute_reply.started":"2022-06-02T02:03:37.502766Z","shell.execute_reply":"2022-06-02T02:03:37.508515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#From:\n#https://developers.google.com/machine-learning/guides/text-classification/step-4\n\ndef _get_last_layer_units_and_activation(num_classes):\n    \"\"\"Gets the # units and activation function for the last network layer.\n\n    # Arguments\n        num_classes: int, number of classes.\n\n    # Returns\n        units, activation values.\n    \"\"\"\n    if num_classes == 2:\n        activation = 'sigmoid'\n        units = 1\n    else:\n        activation = 'softmax'\n        units = num_classes\n    return units, activation\n\n\ndef sepcnn_model(blocks,\n                 filters,\n                 kernel_size,\n                 embedding_dim,\n                 dropout_rate,\n                 pool_size,\n                 input_shape,\n                 num_classes,\n                 num_features,\n                 use_pretrained_embedding=False,\n                 is_embedding_trainable=False,\n                 embedding_matrix=None):\n    \"\"\"Creates an instance of a separable CNN model.\n\n    # Arguments\n        blocks: int, number of pairs of sepCNN and pooling blocks in the model.\n        filters: int, output dimension of the layers.\n        kernel_size: int, length of the convolution window.\n        embedding_dim: int, dimension of the embedding vectors.\n        dropout_rate: float, percentage of input to drop at Dropout layers.\n        pool_size: int, factor by which to downscale input at MaxPooling layer.\n        input_shape: tuple, shape of input to the model.\n        num_classes: int, number of output classes.\n        num_features: int, number of words (embedding input dimension).\n        use_pretrained_embedding: bool, true if pre-trained embedding is on.\n        is_embedding_trainable: bool, true if embedding layer is trainable.\n        embedding_matrix: dict, dictionary with embedding coefficients.\n\n    # Returns\n        A sepCNN model instance.\n    \"\"\"\n    op_units, op_activation = _get_last_layer_units_and_activation(num_classes)\n    model = models.Sequential()\n\n    # Add embedding layer. If pre-trained embedding is used add weights to the\n    # embeddings layer and set trainable to input is_embedding_trainable flag.\n    if use_pretrained_embedding:\n        model.add(Embedding(input_dim=num_features,\n                            output_dim=embedding_dim,\n                            input_length=input_shape[0],\n                            weights=[embedding_matrix],\n                            trainable=is_embedding_trainable))\n    else:\n        model.add(Embedding(input_dim=num_features,\n                            output_dim=embedding_dim,\n                            input_length=input_shape[0]))\n\n    for _ in range(blocks-1):\n        model.add(Dropout(rate=dropout_rate))\n        model.add(SeparableConv1D(filters=filters,\n                                  kernel_size=kernel_size,\n                                  activation='relu',\n                                  bias_initializer='random_uniform',\n                                  depthwise_initializer='random_uniform',\n                                  padding='same'))\n        model.add(SeparableConv1D(filters=filters,\n                                  kernel_size=kernel_size,\n                                  activation='relu',\n                                  bias_initializer='random_uniform',\n                                  depthwise_initializer='random_uniform',\n                                  padding='same'))\n        model.add(MaxPooling1D(pool_size=pool_size))\n\n    model.add(SeparableConv1D(filters=filters * 2,\n                              kernel_size=kernel_size,\n                              activation='relu',\n                              bias_initializer='random_uniform',\n                              depthwise_initializer='random_uniform',\n                              padding='same'))\n    model.add(SeparableConv1D(filters=filters * 2,\n                              kernel_size=kernel_size,\n                              activation='relu',\n                              bias_initializer='random_uniform',\n                              depthwise_initializer='random_uniform',\n                              padding='same'))\n    model.add(GlobalAveragePooling1D())\n    model.add(Dropout(rate=dropout_rate))\n    model.add(Dense(op_units, activation=op_activation))\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-06-02T02:03:45.753554Z","iopub.execute_input":"2022-06-02T02:03:45.754193Z","iopub.status.idle":"2022-06-02T02:03:45.769382Z","shell.execute_reply.started":"2022-06-02T02:03:45.754153Z","shell.execute_reply":"2022-06-02T02:03:45.767983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Adapted From: \n#https://github.com/google/eng-edu/blob/main/ml/guides/text_classification/train_sequence_model.py\n\n\"\"\"Module to train sequence model.\n\nVectorizes training and validation texts into sequences and uses that for\ntraining a sequence model - a sepCNN model. We use sequence model for text\nclassification when the ratio of number of samples to number of words per\nsample for the given dataset is very large (>~15K).\n\"\"\"\n\nFLAGS = None\n\ndef train_sequence_model(data,\n                         learning_rate=1e-3,\n                         epochs=1000,       #we'll set to auto terminate when validation loss stabilizes\n                         batch_size=128,\n                         blocks=2,\n                         filters=64,\n                         dropout_rate=0.2,\n                         embedding_dim=200,  #can try different parameters here\n                         kernel_size=6,      #can try different CNN kernel sizes here\n                         pool_size=3):\n    \"\"\"Trains sequence model on the given dataset.\n\n    # Arguments\n        data: tuples of training and test texts and labels.\n        learning_rate: float, learning rate for training model.\n        epochs: int, number of epochs.\n        batch_size: int, number of samples per batch.\n        blocks: int, number of pairs of sepCNN and pooling blocks in the model.\n        filters: int, output dimension of sepCNN layers in the model.\n        dropout_rate: float: percentage of input to drop at Dropout layers.\n        embedding_dim: int, dimension of the embedding vectors.\n        kernel_size: int, length of the convolution window.\n        pool_size: int, factor by which to downscale input at MaxPooling layer.\n\n    # Raises\n        ValueError: If validation data has label values which were not seen\n            in the training data.\n    \"\"\"\n    # Get the data.\n    (train_texts, train_labels), (val_texts, val_labels) = data\n\n    # Verify that validation labels are in the same range as training labels.\n    num_classes = 3 #Hardcoded for this exercise\n    unexpected_labels = [v for v in val_labels if v not in range(num_classes)]\n    if len(unexpected_labels):\n        raise ValueError('Unexpected label values found in the validation set:'\n                         ' {unexpected_labels}. Please make sure that the '\n                         'labels in the validation set are in the same range '\n                         'as training labels.'.format(\n                             unexpected_labels=unexpected_labels))\n\n    # Vectorize texts.\n    x_train, x_val, tokenizer = sequence_vectorize(\n            train_texts, val_texts)\n\n    # Number of features will be the embedding input dimension. Add 1 for the\n    # reserved index 0.\n    num_features = min(len(tokenizer.word_index) + 1, TOP_K)\n\n    # Create model instance.\n    model = sepcnn_model(blocks=blocks,\n                        filters=filters,\n                        kernel_size=kernel_size,\n                        embedding_dim=embedding_dim,\n                        dropout_rate=dropout_rate,\n                        pool_size=pool_size,\n                        input_shape=x_train.shape[1:],\n                        num_classes=num_classes,\n                        num_features=num_features)\n\n    # Compile model with learning parameters.\n#    optimizer = tf.keras.optimizers.Adam(); I got errors so use 'adam'\n    model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n\n    # Create callback for early stopping on validation loss. If the loss does\n    # not decrease in two consecutive tries, stop training.\n    callbacks = [tf.keras.callbacks.EarlyStopping(\n        monitor='val_loss', patience=2)]   #This auto terminates when validation loss stabilizes\n\n    # Train and validate model.\n    history = model.fit(\n            x_train,\n            train_labels,\n            epochs=epochs,\n            callbacks=callbacks,\n            validation_data=(x_val, val_labels),\n            verbose=2,  # Logs once per epoch.\n            batch_size=batch_size)\n\n    # Save model.\n    model.save('effective_arg_sepcnn_model.h5')\n    return model, tokenizer","metadata":{"execution":{"iopub.status.busy":"2022-06-02T02:03:55.078616Z","iopub.execute_input":"2022-06-02T02:03:55.079015Z","iopub.status.idle":"2022-06-02T02:03:55.091529Z","shell.execute_reply.started":"2022-06-02T02:03:55.078982Z","shell.execute_reply":"2022-06-02T02:03:55.090785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Load the data\ndata = load_train_val(train_df)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T02:04:04.203821Z","iopub.execute_input":"2022-06-02T02:04:04.204182Z","iopub.status.idle":"2022-06-02T02:04:09.185096Z","shell.execute_reply.started":"2022-06-02T02:04:04.204153Z","shell.execute_reply":"2022-06-02T02:04:09.184242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Train the model\nmodel, tokenizer = train_sequence_model(data)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T02:04:14.49302Z","iopub.execute_input":"2022-06-02T02:04:14.493671Z","iopub.status.idle":"2022-06-02T02:04:44.26523Z","shell.execute_reply.started":"2022-06-02T02:04:14.493635Z","shell.execute_reply":"2022-06-02T02:04:44.264371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#prepare model inputs in same format as training\ntest_txt=[]\nfor i in range(len(test_df)):\n    txt=fix_text(test_df.iloc[i].discourse_text)\n    test_txt.append(txt)\ntest=tokenizer.texts_to_sequences(test_txt)\ntest=sequence.pad_sequences(test, maxlen=MAX_SEQUENCE_LENGTH)\n#make the predictions\npred=model.predict(test)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T02:05:02.095645Z","iopub.execute_input":"2022-06-02T02:05:02.096444Z","iopub.status.idle":"2022-06-02T02:05:02.144842Z","shell.execute_reply.started":"2022-06-02T02:05:02.096393Z","shell.execute_reply":"2022-06-02T02:05:02.144107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#inspect\ntest_txt","metadata":{"execution":{"iopub.status.busy":"2022-06-02T02:05:05.522374Z","iopub.execute_input":"2022-06-02T02:05:05.522756Z","iopub.status.idle":"2022-06-02T02:05:05.52794Z","shell.execute_reply.started":"2022-06-02T02:05:05.522727Z","shell.execute_reply":"2022-06-02T02:05:05.527181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Prepare the data frame for our subsmission\nsub=pd.DataFrame(columns={'discourse_id','Ineffective','Adequate','Effective'})\nsub.discourse_id=test_df.discourse_id\nfor i in range(len(sub)):\n    sub.Adequate[i]=pred[i][lbl_dict['Adequate']]\n    sub.Effective[i]=pred[i][lbl_dict['Effective']]\n    sub.Ineffective[i]=pred[i][lbl_dict['Ineffective']]","metadata":{"execution":{"iopub.status.busy":"2022-06-02T02:05:19.377413Z","iopub.execute_input":"2022-06-02T02:05:19.377769Z","iopub.status.idle":"2022-06-02T02:05:19.392014Z","shell.execute_reply.started":"2022-06-02T02:05:19.377739Z","shell.execute_reply":"2022-06-02T02:05:19.391255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Write the submission file\nsub=sub[[\"discourse_id\",\"Ineffective\",\"Adequate\",\"Effective\"]]\nsub.to_csv('submission.csv',index=False)\nsub\n","metadata":{"execution":{"iopub.status.busy":"2022-06-02T02:05:24.899646Z","iopub.execute_input":"2022-06-02T02:05:24.9003Z","iopub.status.idle":"2022-06-02T02:05:24.918313Z","shell.execute_reply.started":"2022-06-02T02:05:24.90026Z","shell.execute_reply":"2022-06-02T02:05:24.917563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#inspect\npred","metadata":{"execution":{"iopub.status.busy":"2022-06-01T23:41:04.402011Z","iopub.execute_input":"2022-06-01T23:41:04.402399Z","iopub.status.idle":"2022-06-01T23:41:04.408492Z","shell.execute_reply.started":"2022-06-01T23:41:04.402368Z","shell.execute_reply":"2022-06-01T23:41:04.407571Z"},"trusted":true},"execution_count":null,"outputs":[]}]}