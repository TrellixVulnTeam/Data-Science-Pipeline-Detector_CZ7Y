{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"I already noticed Logistic Regression and TfiDf as a code article, So i have decided to improve that. We will import the needed libraries and load the dataset. Hopefully the normal stuffs. LOL\n\nI copied this notebook to enhance it.\nhttps://www.kaggle.com/code/kvsnoufal/logisticregression-tfidf-5-folds?scriptVersionId=97758635","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np # linear algebra\nimport os\ndftr = pd.read_csv(\"../input/feedback-prize-effectiveness/train.csv\")\nprint(dftr.shape,dftr.discourse_id.nunique())\ndfte = pd.read_csv(\"../input/feedback-prize-effectiveness/test.csv\")\nprint(dfte.shape,dfte.discourse_id.nunique())\ndfs = pd.read_csv(\"../input/feedback-prize-effectiveness/sample_submission.csv\")\n\ndftr[\"text\"] = dftr[\"essay_id\"].apply(lambda x: open(f'../input/feedback-prize-effectiveness/train/{x}.txt').read())\ndfte[\"text\"] = dfte[\"essay_id\"].apply(lambda x: open(f'../input/feedback-prize-effectiveness/test/{x}.txt').read())\nFOLDS = 5\nfrom sklearn.model_selection import StratifiedKFold\ntarget_map = {\"Adequate\":1,\"Effective\":2,\"Ineffective\":0}\ndftr[\"target\"] = dftr[\"discourse_effectiveness\"].map(target_map)\ndftr = dftr.reset_index(drop=True)\n# print(dftr[\"target\"].value_counts())\nskf = StratifiedKFold(n_splits=FOLDS,shuffle=True,random_state=FOLDS)\nfor i,(train_index, test_index) in enumerate(skf.split(dftr, dftr[\"target\"])):\n    dftr.loc[test_index,\"fold\"] = i\nprint(dftr.fold.value_counts())    \n","metadata":{"execution":{"iopub.status.busy":"2022-06-11T14:37:17.922731Z","iopub.execute_input":"2022-06-11T14:37:17.923295Z","iopub.status.idle":"2022-06-11T14:37:34.307093Z","shell.execute_reply.started":"2022-06-11T14:37:17.92325Z","shell.execute_reply":"2022-06-11T14:37:34.306132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Below we used the TfidfVectorizer, OneHotEncoder and the LogisticRegression. I will have to change some vectorization technicles and algorithms to get more efficiency.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import OneHotEncoder,LabelEncoder\nfrom scipy import sparse\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import log_loss\nimport numpy as np\npreds = []\nfor fold in range(FOLDS):\n    dftr_ = dftr[dftr[\"fold\"]!=fold]\n    dfev_ = dftr[dftr[\"fold\"]==fold]\n    tf = TfidfVectorizer(ngram_range=(1,2))\n    tr_text_feats = tf.fit_transform(dftr_[\"discourse_text\"])\n    ev_text_feats = tf.transform(dfev_[\"discourse_text\"])\n    te_text_feats = tf.transform(dfte[\"discourse_text\"])\n    tf = TfidfVectorizer(ngram_range=(1,2))\n    tr_text_feats2 = tf.fit_transform(dftr_[\"text\"])\n    ev_text_feats2 = tf.transform(dfev_[\"text\"])\n    te_text_feats2 = tf.transform(dfte[\"text\"])\n    ohe = OneHotEncoder()\n    tr_feats1 = sparse.csr_matrix(ohe.fit_transform(dftr_[\"discourse_type\"].values.reshape(-1,1)))\n    ev_feats1 = sparse.csr_matrix(ohe.transform(dfev_[\"discourse_type\"].values.reshape(-1,1)))\n    te_feats1 = sparse.csr_matrix(ohe.transform(dfte[\"discourse_type\"].values.reshape(-1,1)))\n    tr_feats = sparse.hstack((tr_feats1,tr_text_feats,tr_text_feats2))\n    ev_feats = sparse.hstack((ev_feats1,ev_text_feats,ev_text_feats2))\n    te_feats = sparse.hstack((te_feats1,te_text_feats,te_text_feats2))\n    clf = LogisticRegression(max_iter=500,penalty=\"l2\",C=1.0131816333513533)\n    clf.fit(tr_feats, dftr_[\"target\"].values)\n    ev_preds = clf.predict_proba(ev_feats)\n    ev_loss = log_loss(dfev_[\"target\"].values,ev_preds)\n    print(\"Fold : {} EV score: {}\".format(fold,ev_loss))\n    preds.append(clf.predict_proba(te_feats))\n    # break","metadata":{"execution":{"iopub.status.busy":"2022-06-11T10:39:39.101716Z","iopub.execute_input":"2022-06-11T10:39:39.10211Z","iopub.status.idle":"2022-06-11T11:01:55.266371Z","shell.execute_reply.started":"2022-06-11T10:39:39.102079Z","shell.execute_reply":"2022-06-11T11:01:55.265003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us try another vectorization mechnism. Count Vector.\n\nWe will replace the TfidVectorizer with the CountVectorizer. Reducing the max_iter parameter in the Logistric Regression will reduce the processing time.","metadata":{}},{"cell_type":"code","source":"all_preds = np.array(preds).mean(0)\nprint(all_preds.shape)\ndfs.loc[:,\"Ineffective\"] = all_preds[:,0]\ndfs.loc[:,\"Adequate\"] = all_preds[:,1]\ndfs.loc[:,\"Effective\"] = all_preds[:,2]\ndfs.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-11T17:05:57.760166Z","iopub.execute_input":"2022-06-11T17:05:57.761011Z","iopub.status.idle":"2022-06-11T17:05:57.774707Z","shell.execute_reply.started":"2022-06-11T17:05:57.760972Z","shell.execute_reply":"2022-06-11T17:05:57.773605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us move further to creating our submission.csv file with the needed details.","metadata":{}},{"cell_type":"code","source":"dfs.to_csv('submission.csv',index=None)","metadata":{"execution":{"iopub.status.busy":"2022-06-11T17:05:58.620908Z","iopub.execute_input":"2022-06-11T17:05:58.621793Z","iopub.status.idle":"2022-06-11T17:05:58.627096Z","shell.execute_reply.started":"2022-06-11T17:05:58.621755Z","shell.execute_reply":"2022-06-11T17:05:58.62612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Well done. Nice Job.","metadata":{}}]}