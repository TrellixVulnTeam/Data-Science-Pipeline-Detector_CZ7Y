{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pandas_profiling as pp\n\ncolumns_required = ['ip', 'app', 'device', 'os', 'channel', 'click_time', 'is_attributed']\npath= '~/.kaggle/competitions/talkingdata-adtracking-fraud-detection/'\n# Load subset of the training data\nX_train = pd.read_csv('../input/train.csv',usecols=columns_required,parse_dates=['click_time'])\n\n# Show the head of the table\nX_train.head()\nX_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# segregating the click_time column into day, hour, minute and second","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train['day'] = X_train['click_time'].dt.day.astype('uint8')\nX_train['hour'] = X_train['click_time'].dt.hour.astype('uint8')\nX_train['minute'] = X_train['click_time'].dt.minute.astype('uint8')\nX_train['second'] = X_train['click_time'].dt.second.astype('uint8')\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.loc[:, 'device'] = X_train.loc[:, 'device'].astype(np.int16)\nX_train.loc[:, 'os'] = X_train.loc[:, 'os'].astype(np.int16)\nX_train.loc[:, 'channel'] = X_train.loc[:, 'channel'].astype(np.int16)\nX_train.loc[:, 'is_attributed'] = X_train.loc[:, 'is_attributed'].astype(np.int8)\nX_train.loc[:, 'ip'] = X_train.loc[:, 'ip'].astype(np.int8)\nX_train.loc[:, 'app'] = X_train.loc[:, 'app'].astype(np.int8)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pp.ProfileReport(X_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns_required = ['ip', 'app', 'device', 'os', 'channel', 'click_time']\n\n# Load subset of the training data\nX_test = pd.read_csv('../input/test.csv',nrows=100000,usecols=columns_required,parse_dates=['click_time'])\nX_test.fillna(X_test.mean(), inplace=True)\n# Show the head of the table\nX_test.head()\nX_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test['day'] = X_test['click_time'].dt.day.astype('uint8')\nX_test['hour'] = X_test['click_time'].dt.hour.astype('uint8')\nX_test['minute'] = X_test['click_time'].dt.minute.astype('uint8')\nX_test['second'] = X_test['click_time'].dt.second.astype('uint8')\nX_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.loc[:, 'device'] = X_test.loc[:, 'device'].astype(np.int16)\nX_test.loc[:, 'os'] = X_test.loc[:, 'os'].astype(np.int16)\nX_test.loc[:, 'channel'] = X_test.loc[:, 'channel'].astype(np.int16)\nX_test.loc[:, 'ip'] = X_test.loc[:, 'ip'].astype(np.int8)\nX_test.loc[:, 'app'] = X_test.loc[:, 'app'].astype(np.int8)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ATTRIBUTION_CATEGORIES = [        \n    # Group-1 Features \n    ['ip'], ['app'], ['device'], ['os'], ['channel'],\n    \n    # Group-2 Features\n    ['app', 'channel'],\n    ['app', 'os'],\n    ['app', 'device'],\n    \n    # Group-3 Features\n    ['channel', 'os'],\n    ['channel', 'device'],\n    ['os', 'device']\n]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find frequency of is_attributed for each unique value in column in train data\nfreqs = {}\nfor cols in ATTRIBUTION_CATEGORIES:\n    \n    # New feature name\n    new_feature = '_'.join(cols)+'_confRate'    \n    \n    # Perform the groupby\n    group_object = X_train.groupby(cols)\n    \n    # Group sizes    \n    group_sizes = group_object.size()\n    log_group = np.log(100000) \n    print(\">> Calculating confidence-weighted rate for: {}.\\n   Saving to: {}. Group Max /Mean / Median / Min: {} / {} / {} / {}\".format(\n        cols, new_feature, \n        group_sizes.max(), \n        np.round(group_sizes.mean(), 2),\n        np.round(group_sizes.median(), 2),\n        group_sizes.min()\n    ))\n    \n    # Aggregation function\n    def rate_calculation(x):\n        \"\"\"Calculate the attributed rate. Scale by confidence\"\"\"\n        rate = x.sum() / float(x.count())\n        conf = np.min([1, np.log(x.count()) / log_group])\n        return rate * conf\n    \n    # Merge operation\n    X_train = X_train.merge(\n        group_object['is_attributed']. \\\n            apply(rate_calculation). \\\n            reset_index(). \\\n            rename( \n                index=str,\n                columns={'is_attributed': new_feature}\n            )[cols + [new_feature]],\n        on=cols, how='left'\n    )\n    \nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Test Data\nfreqs = {}\nfor cols in ATTRIBUTION_CATEGORIES:\n    \n    # New feature name\n    new_feature = '_'.join(cols)+'_confRate'    \n    \n    # Perform the groupby\n    group_object = X_test.groupby(cols)\n    \n    # Group sizes    \n    group_sizes = group_object.size()\n    log_group = np.log(100000) # 1000 views -> 60% confidence, 100 views -> 40% confidence \n    print(\">> Calculating confidence-weighted rate for: {}.\\n   Saving to: {}. Group Max /Mean / Median / Min: {} / {} / {} / {}\".format(\n        cols, new_feature, \n        group_sizes.max(), \n        np.round(group_sizes.mean(), 2),\n        np.round(group_sizes.median(), 2),\n        group_sizes.min()\n    ))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define all the groupby transformations\nGROUPBY_AGGREGATIONS = [\n    \n    # Group-1 - GroupBy Features    \n    # Variance in day, for ip-app-channel\n    {'groupby': ['ip','app','channel'], 'select': 'day', 'agg': 'var'},\n    # Variance in hour, for ip-app-os\n    {'groupby': ['ip','app','os'], 'select': 'hour', 'agg': 'var'},\n    # Variance in hour, for ip-day-channel\n    {'groupby': ['ip','day','channel'], 'select': 'hour', 'agg': 'var'},\n    # Count, for ip-day-hour\n    {'groupby': ['ip','day','hour'], 'select': 'channel', 'agg': 'count'},\n    # Count, for ip-app\n    {'groupby': ['ip', 'app'], 'select': 'channel', 'agg': 'count'},        \n    # Count, for ip-app-os\n    {'groupby': ['ip', 'app', 'os'], 'select': 'channel', 'agg': 'count'},\n    # Count, for ip-app-day-hour\n    {'groupby': ['ip','app','day','hour'], 'select': 'channel', 'agg': 'count'},\n    # Mean hour, for ip-app-channel\n    {'groupby': ['ip','app','channel'], 'select': 'hour', 'agg': 'mean'}, \n    \n    # Group-2 - GroupBy Features \n    # Average clicks on app by distinct users; is it an app they return to?\n    {'groupby': ['app'], \n     'select': 'ip', \n     'agg': lambda x: float(len(x)) / len(x.unique()), \n     'agg_name': 'AvgViewPerDistinct'\n    },\n    # How popular is the app or channel?\n    {'groupby': ['app'], 'select': 'channel', 'agg': 'count'},\n    {'groupby': ['channel'], 'select': 'app', 'agg': 'count'},\n    \n    # Group-3 - GroupBy Features     \n    # Reference from https://www.kaggle.com/bk0000/non-blending-lightgbm-model-lb-0-977 \n    {'groupby': ['ip'], 'select': 'channel', 'agg': 'nunique'}, \n    {'groupby': ['ip'], 'select': 'app', 'agg': 'nunique'}, \n    {'groupby': ['ip','day'], 'select': 'hour', 'agg': 'nunique'}, \n    {'groupby': ['ip','app'], 'select': 'os', 'agg': 'nunique'}, \n    {'groupby': ['ip'], 'select': 'device', 'agg': 'nunique'}, \n    {'groupby': ['app'], 'select': 'channel', 'agg': 'nunique'}, \n    {'groupby': ['ip', 'device', 'os'], 'select': 'app', 'agg': 'nunique'}, \n    {'groupby': ['ip','device','os'], 'select': 'app', 'agg': 'cumcount'}, \n    {'groupby': ['ip'], 'select': 'app', 'agg': 'cumcount'}, \n    {'groupby': ['ip'], 'select': 'os', 'agg': 'cumcount'}, \n    {'groupby': ['ip','day','channel'], 'select': 'hour', 'agg': 'var'}   \n    \n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply all the groupby transformations\nfor spec in GROUPBY_AGGREGATIONS:\n    \n    # Name of the aggregation we're applying\n    agg_name = spec['agg_name'] if 'agg_name' in spec else spec['agg']\n    \n    # Name of new feature\n    new_feature = '{}_{}_{}'.format('_'.join(spec['groupby']), agg_name, spec['select'])\n    \n    # Info\n    print(\"Grouping by {}, and aggregating {} with {}\".format(\n        spec['groupby'], spec['select'], agg_name\n    ))\n    \n    # Unique list of features to select\n    all_features = list(set(spec['groupby'] + [spec['select']]))\n     # Perform the groupby\n    gp = X_train[all_features]. \\\n        groupby(spec['groupby'])[spec['select']]. \\\n        agg(spec['agg']). \\\n        reset_index(). \\\n        rename(index=str, columns={spec['select']: new_feature})\n        \n    # Merge back to X_total\n    if 'cumcount' == spec['agg']:\n        X_train[new_feature] = gp[0].values\n    else:\n        X_train = X_train.merge(gp, on=spec['groupby'], how='left')\n        \n     # Clear memory\n    del gp\n    gc.collect()\n\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Test Data \n#Apply all the groupby transformations\nfor spec in GROUPBY_AGGREGATIONS:\n    \n    # Name of the aggregation we're applying\n    agg_name = spec['agg_name'] if 'agg_name' in spec else spec['agg']\n    \n    # Name of new feature\n    new_feature = '{}_{}_{}'.format('_'.join(spec['groupby']), agg_name, spec['select'])\n    \n    # Info\n    print(\"Grouping by {}, and aggregating {} with {}\".format(\n        spec['groupby'], spec['select'], agg_name\n    ))\n    \n    # Unique list of features to select\n    all_features = list(set(spec['groupby'] + [spec['select']]))\n     # Perform the groupby\n    gp = X_test[all_features]. \\\n        groupby(spec['groupby'])[spec['select']]. \\\n        agg(spec['agg']). \\\n        reset_index(). \\\n        rename(index=str, columns={spec['select']: new_feature})\n        \n    # Merge back to X_total\n    if 'cumcount' == spec['agg']:\n        X_test[new_feature] = gp[0].values\n    else:\n        X_test= X_test.merge(gp, on=spec['groupby'], how='left')\n        \n     # Clear memory\n    del gp\n    gc.collect()\n\nX_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train Data\nGROUP_BY_NEXT_CLICKS = [\n    \n    # Group-1\n    {'groupby': ['ip']},\n    {'groupby': ['ip', 'app']},\n    {'groupby': ['ip', 'channel']},\n    {'groupby': ['ip', 'os']},\n    \n    # Group-3\n    {'groupby': ['ip', 'app', 'device', 'os', 'channel']},\n    {'groupby': ['ip', 'os', 'device']},\n    {'groupby': ['ip', 'os', 'device', 'app']}\n]\n\n# Calculate the time to next click for each group\nfor t in GROUP_BY_NEXT_CLICKS:\n    \n    # Name of new feature\n    new_feature = '{}_nextClick'.format('_'.join(t['groupby']))    \n    \n    # Unique list of features to select\n    all_features = t['groupby'] + ['click_time']\n    \n    # Run calculation\n    print(f\">> Grouping by {t['groupby']}, and saving time to next click in: {new_feature}\")\n    X_train[new_feature] = X_train[all_features].groupby(t['groupby']).click_time.transform(lambda x: x.diff().shift(-1)).dt.seconds\n    \nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Test Data\nGROUP_BY_NEXT_CLICKS = [\n    \n    # V1\n    {'groupby': ['ip']},\n    {'groupby': ['ip', 'app']},\n    {'groupby': ['ip', 'channel']},\n    {'groupby': ['ip', 'os']},\n    \n    # V3\n    {'groupby': ['ip', 'app', 'device', 'os', 'channel']},\n    {'groupby': ['ip', 'os', 'device']},\n    {'groupby': ['ip', 'os', 'device', 'app']}\n]\n\n# Calculate the time to next click for each group\nfor t in GROUP_BY_NEXT_CLICKS:\n    \n    # Name of new feature\n    new_feature = '{}_nextClick'.format('_'.join(t['groupby']))    \n    \n    # Unique list of features to select\n    all_features = t['groupby'] + ['click_time']\n    \n    # Run calculation\n    print(f\">> Grouping by {t['groupby']}, and saving time to next click in: {new_feature}\")\n    X_test[new_feature] = X_test[all_features].groupby(t['groupby']).click_time.transform(lambda x: x.diff().shift(-1)).dt.seconds\n    \nX_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"HISTORY_CLICKS = {\n    'identical_clicks': ['ip', 'app', 'device', 'os', 'channel'],\n    'app_clicks': ['ip', 'app']\n}\n\n# Go through different group-by combinations\nfor fname, fset in HISTORY_CLICKS.items():\n    \n    # Clicks in the past\n    X_train['prev_'+fname] = X_train. \\\n        groupby(fset). \\\n        cumcount(). \\\n        rename('prev_'+fname)\n        \n    # Clicks in the future\n    X_train['future_'+fname] = X_train.iloc[::-1]. \\\n        groupby(fset). \\\n        cumcount(). \\\n        rename('future_'+fname).iloc[::-1]\n\n# Count cumulative subsequent clicks\nX_train.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test Data\nHISTORY_CLICKS = {\n    'identical_clicks': ['ip', 'app', 'device', 'os', 'channel'],\n    'app_clicks': ['ip', 'app']\n}\n\n# Go through different group-by combinations\nfor fname, fset in HISTORY_CLICKS.items():\n    \n    # Clicks in the past\n    X_test['prev_'+fname] = X_test. \\\n        groupby(fset). \\\n        cumcount(). \\\n        rename('prev_'+fname)\n        \n    # Clicks in the future\n    X_test['future_'+fname] = X_test.iloc[::-1]. \\\n        groupby(fset). \\\n        cumcount(). \\\n        rename('future_'+fname).iloc[::-1]\n\n# Count cumulative subsequent clicks\nX_test.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split into X and y\nX_train.fillna(X_train.mean(), inplace=True)\ny_train = X_train['is_attributed']\nX_train= X_train.drop('is_attributed', axis=1).select_dtypes(include=[np.number])\n# Oversampling to decrease imbalance in labels\nfrom imblearn.over_sampling import SMOTE\nsm = SMOTE()\nX_train, y_train = sm.fit_sample(X_train, y_train)\n#Shuffle the data to train well\nfrom sklearn.utils import shuffle\nshuffle(X_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nX_train = SelectKBest(chi2, k=42).fit_transform(abs(X_train), y_train)\nX_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_valid, y_train, y_valid= train_test_split(X_train,y_train,test_size=0.3, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a model\n# Params from: https://www.kaggle.com/aharless/swetha-s-xgboost-revised\nimport xgboost as xgb\nclf_xgBoost = xgb.XGBClassifier(max_depth = 4,subsample = 0.8,colsample_bytree = 0.7,colsample_bylevel = 0.7,scale_pos_weight = 9,\n    min_child_weight = 0,reg_alpha = 0.01,n_jobs = -1, objective = 'binary:logistic')\n# Fit the models\nclf_xgBoost.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\ny_pred=clf_xgBoost.predict(X_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_auc_score(y_valid, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nclf1=RandomForestClassifier(n_jobs=-1,criterion=\"entropy\",min_samples_leaf=1,min_samples_split=8, \\\n                                                n_estimators=15,max_features=None,random_state=100)\nclf1.fit(X_train,y_train)\n                        \n    \n    \ny_pred_rf=clf1.predict(X_valid)\nprint(y_pred_rf)\n    \n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_auc_score(y_valid, y_pred_rf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.drop('click_time',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.fillna(X_test.mean(), inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_t=clf1.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create submission file\nsubmission = pd.DataFrame({'click_id':[i for i in range(len(y_pred_t))],'is_attributed':y_pred_t})\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"nbformat":4,"nbformat_minor":1}