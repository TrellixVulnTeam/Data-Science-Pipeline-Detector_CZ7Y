{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/824024445/KaggleCases/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0kaggle%E6%A1%88%E4%BE%8B%EF%BC%9A%E9%A3%8E%E6%8E%A7%E8%AF%84%E5%88%86%E5%8D%A1%E6%A8%A1%E5%9E%8B%EF%BC%88Give_Me_Some_Credit%EF%BC%89.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github"}},{"cell_type":"markdown","source":"#### 1.Define the problem","metadata":{}},{"cell_type":"markdown","source":"The credit scoring algorithm, used to guess the probability of default, is a method used by banks to determine whether a loan should be granted. Improve the current level of credit scores by predicting the likelihood that someone will face financial distress in the next two years","metadata":{}},{"cell_type":"code","source":"#Data collation and analysis\nimport pandas as pd\nimport numpy as np\n\n#Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#Machine learning\nimport time\nimport os\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.1 Read Data","metadata":{"id":"REU3OiPRUghR"}},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/GiveMeSomeCredit/cs-training.csv')\ntest_df = pd.read_csv('/kaggle/input/GiveMeSomeCredit/cs-test.csv')\ncombine=[train_df, test_df]\ntrain_df.head()","metadata":{"id":"J9e2Ujw6o-IH","outputId":"52156e0f-14c4-4422-efae-6cfa2a2edcd9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.2 Observe the data","metadata":{"id":"uyZWNrToPerD"}},{"cell_type":"markdown","source":"#### 2.2.1 info()","metadata":{"id":"_8lx_CO6PlP4"}},{"cell_type":"code","source":"train_df.info()","metadata":{"id":"cpGf5JW4qy0M","outputId":"475de8a9-7776-4897-83c6-0755f5230656"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"observed:\n-\"MonthlyIncome\" and \"NumberOfDependents\" have null values. Need to deal with null values in data cleaning.","metadata":{"id":"jS-5OIraP1HH"}},{"cell_type":"code","source":"test_df.info()","metadata":{"id":"-Xb8oX4liHFN","outputId":"c1b26a7d-38f4-4489-af35-9bedbb185829"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.2.2 decribe()","metadata":{"id":"m2_mIsFAPrHu"}},{"cell_type":"code","source":"#decribe() View the information of numeric data. There is no non-numeric data, \n#so do not use describe(include=['O']) to view non-numeric data.\ntrain_df.describe()","metadata":{"id":"6oOhZU0lLa3H","outputId":"665b8a22-40fa-475c-c266-b42205a33bce"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"observed:\n-\"NumberOfDependents\" More than 50% of people have no family members, and the discrete value is large, \nso select the mode to fill in null","metadata":{"id":"INvbxUwkQT10"}},{"cell_type":"markdown","source":"#### 2.2.3 corr() Find association","metadata":{"id":"rFznNXxNPuCZ"}},{"cell_type":"code","source":"#Find associations (also often used when cleaning data later, to compare the effect)\ncorr_matrix = train_df.corr()\nprint(corr_matrix[\"SeriousDlqin2yrs\"].sort_values(ascending=False))\n\n# The following code graphically shows the correlation between each feature\nfig, ax = plt.subplots(figsize=(12,12))\nsns.heatmap(corr_matrix,xticklabels=corr_matrix.columns,yticklabels=corr_matrix.columns,annot=True)","metadata":{"id":"2tDiDy8TL231","outputId":"7c33dd5c-d29a-4c44-948a-23b02723f4b0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Find the correlation between SeriousDlqin2yrs (target value, the smaller the better) and other features\n-For the remaining features, if the correlation between each feature exceeds 60%, you can use only one feature to model. In this case, NumberOfTime30-59DaysPastDueNotWorse, NumberOfTimes90DaysLate, NumberOfTime60-89DaysPastDueNotWorse can be modeled with only one","metadata":{"id":"2GqJVRd6VmPh"}},{"cell_type":"markdown","source":"### 2.3 Data cleaning","metadata":{"id":"IQgPio2pPNFg"}},{"cell_type":"markdown","source":"#### 2.3.1 Null value processing","metadata":{"id":"DlWiJr_1Y-NY"}},{"cell_type":"code","source":"for data in combine:\n  data[\"MonthlyIncome\"].fillna(data[\"MonthlyIncome\"].mean(), inplace=True)\n  data[\"NumberOfDependents\"].fillna(data[\"MonthlyIncome\"].mode()[0], inplace=True)\n\n#Check the replaced data and confirm that there is no empty value\ntrain_df.info()","metadata":{"id":"U2dyCaGOPPTv","outputId":"7298674a-5c3f-4f11-8c48-4b9e7e403a4c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.3.2 Outlier handling","metadata":{"id":"zsN_paEsZEYm"}},{"cell_type":"markdown","source":"NumberOfDependents","metadata":{"id":"xB_UAlDH1vdO"}},{"cell_type":"code","source":"#As you can see, the number of family members is actually 6,670, and the number is still a lot, accounting for 2.6%, \n#first fill it with the average value\ntrain_df.NumberOfDependents.value_counts()","metadata":{"id":"5UuvjauItkJZ","outputId":"94e1670f-03fc-4476-dee5-46d4e9f15bbe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##Look at the correlation between the number of family members and the target value before filling, \n##in order to see the effect, the correlation before processing is -0.013881 \n##(I list all the correlations in order to check the correlations of several others at any time)\ncorr_matrix = train_df.corr()\ncorr_matrix[\"SeriousDlqin2yrs\"].sort_values(ascending=False)","metadata":{"id":"vgj9OT86y3NS","outputId":"76e00300-0273-48c9-e969-164f6e473ae6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for data in combine:\n  data[\"NumberOfDependents\"][data[\"NumberOfDependents\"]>30] = 0\n  \ncorr_matrix = train_df.corr()\ncorr_matrix[\"SeriousDlqin2yrs\"].sort_values(ascending=False)\n\n#After modifying the abnormal value, the correlation of \"NumberOfDependents\" reached 0.046869","metadata":{"id":"gKy07nr0vJnm","outputId":"a8f48a1b-33eb-4950-e6b7-7cbc93108cdc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"age","metadata":{"id":"9WgIGkCf1rgU"}},{"cell_type":"code","source":"train_df = train_df[train_df[\"age\"]>18]\ntest_df = test_df[test_df[\"age\"]>18]\n\ncombine = [train_df, test_df]\ntrain_df[train_df[\"age\"]<18]","metadata":{"id":"PX7RL2DglY8R","outputId":"04c3bc5c-e1c7-4008-813f-2f70e3a3d2dd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.3.3 Create new features","metadata":{"id":"XCD3J68X3A-r"}},{"cell_type":"code","source":"for data in combine:\n  data[\"CombinedDefaulted\"] = data[\"NumberOfTimes90DaysLate\"] + data[\"NumberOfTime60-89DaysPastDueNotWorse\"] + data[\"NumberOfTime30-59DaysPastDueNotWorse\"]\n  data.loc[(data[\"CombinedDefaulted\"] >= 1), \"CombinedDefaulted\"] = 1\n  data[\"CombinedCreditLoans\"] = data[\"NumberOfOpenCreditLinesAndLoans\"] + data[\"NumberRealEstateLoansOrLines\"]\n\n  data[\"CombinedCreditLoans\"] = data[\"NumberOfOpenCreditLinesAndLoans\"] + data[\"NumberRealEstateLoansOrLines\"]\n  data.loc[(data[\"CombinedCreditLoans\"] <= 5), \"CombinedCreditLoans\"] = 0\n  data.loc[(data[\"CombinedCreditLoans\"] > 5), \"CombinedCreditLoans\"] = 1\n","metadata":{"id":"G06vLwAY3DmQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.corr()[\"SeriousDlqin2yrs\"][[\"CombinedDefaulted\", \"CombinedCreditLoans\"]]","metadata":{"id":"PiGX_Bzs4S8O","outputId":"bd6ec406-036a-461d-88b1-8931c2e675cc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3 Models and predictions","metadata":{"id":"P5jANJKJZG33"}},{"cell_type":"markdown","source":"The data set needs to be divided again. The previous test is used for the final test. There is no target value. After submitting to kaggle, it will return you an AUC score, which is equivalent to evaluating generalization ability. And now we must first evaluate the current model by ourselves","metadata":{"id":"vUyHB3efjVKf"}},{"cell_type":"code","source":"attributes=[\"SeriousDlqin2yrs\", 'age','NumberOfTime30-59DaysPastDueNotWorse','NumberOfDependents','MonthlyIncome',\"CombinedDefaulted\",\"CombinedCreditLoans\"]\nsol=['SeriousDlqin2yrs']\n\nattributes2 = [\"Unnamed: 0\", 'age','NumberOfTime30-59DaysPastDueNotWorse','NumberOfDependents','MonthlyIncome',\"CombinedDefaulted\",\"CombinedCreditLoans\"]\nsol=['SeriousDlqin2yrs']\n\ntrain_df = train_df[attributes]\ntest_df = test_df[attributes2]\n","metadata":{"id":"-PYcKox7jUEq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.1 Logistic regression","metadata":{"id":"PtILeMjLdqBV"}},{"cell_type":"markdown","source":"For quick testing, I wrote a class","metadata":{"id":"Ao4GUMXE-etT"}},{"cell_type":"code","source":"class Tester():\n    def __init__(self, target):\n        self.target = target\n        self.datasets = {}\n        self.models = {}\n        self.cache = {} # We added a simple cache to speed up\n\n    def addDataset(self, name, df):\n        self.datasets[name] = df.copy()\n\n    def addModel(self, name, model):\n        self.models[name] = model\n        \n    def clearModels(self):\n        self.models = {}\n\n    def clearCache(self):\n        self.cache = {}\n    \n    def testModelWithDataset(self, m_name, df_name, sample_len, cv):\n        if (m_name, df_name, sample_len, cv) in self.cache:\n            return self.cache[(m_name, df_name, sample_len, cv)]\n\n        clf = self.models[m_name]\n        \n        if not sample_len: \n            sample = self.datasets[df_name]\n        else: sample = self.datasets[df_name].sample(sample_len)\n            \n        X = sample.drop([self.target], axis=1)\n        Y = sample[self.target]\n\n        s = cross_validate(clf, X, Y, scoring=['roc_auc'], cv=cv, n_jobs=-1)\n        self.cache[(m_name, df_name, sample_len, cv)] = s\n\n        return s\n\n    def runTests(self, sample_len=80000, cv=4):\n        # Test the added model on all added data sets\n        scores = {}\n        for m_name in self.models:\n            for df_name in self.datasets:\n                # print('Testing %s' % str((m_name, df_name)), end='')\n                start = time.time()\n\n                score = self.testModelWithDataset(m_name, df_name, sample_len, cv)\n                scores[(m_name, df_name)] = score\n                \n                end = time.time()\n                \n                # print(' -- %0.2fs ' % (end - start))\n\n        print('--- Top 10 Results ---')\n        for score in sorted(scores.items(), key=lambda x: -1 * x[1]['test_roc_auc'].mean())[:10]:\n            auc = score[1]['test_roc_auc']\n            print(\"%s --> AUC: %0.4f (+/- %0.4f)\" % (str(score[0]), auc.mean(), auc.std()))\n    ","metadata":{"id":"q5dzr04l-d5A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We will use test objects in all models\ntester = Tester('SeriousDlqin2yrs')\n\n# Add data set\ntester.addDataset('Drop Missing', train_df.dropna())\n\n# Add model\nrfc = RandomForestClassifier(n_estimators=15, max_depth = 6, random_state=0)\nlog = LogisticRegression()\ntester.addModel('Simple Random Forest', rfc)\ntester.addModel('Simple Logistic Regression', log)\n\n# test\ntester.runTests()","metadata":{"id":"yiJCg-yJBHG5","outputId":"ca01cd44-952a-4b5d-9028-d17e68c40bce"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = train_df.drop(['SeriousDlqin2yrs'], axis=1)\nY_train = train_df['SeriousDlqin2yrs']\n\nX_test = test_df.drop([\"Unnamed: 0\"], axis=1)\nrfc.fit(X_train, Y_train)\nY_pred = rfc.predict_proba(X_test)\n","metadata":{"id":"WuKPZWkbBybM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame({\n        \"Id\": test_df[\"Unnamed: 0\"],\n        \"Probability\": pd.DataFrame(Y_pred)[1]\n    })\n\nsubmission.to_csv('submission.csv', index=False)","metadata":{"id":"O_duLLrpHV0k"},"execution_count":null,"outputs":[]}]}