{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport graphviz\n\nfrom sklearn import preprocessing,model_selection\nimport itertools\n\nimport xgboost as xgb\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nprint(os.listdir(\"/kaggle/input/GiveMeSomeCredit\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/GiveMeSomeCredit/cs-training.csv')\ntest_df = pd.read_csv('/kaggle/input/GiveMeSomeCredit/cs-test.csv')\nprint (\"training dataset shape is {}\".format(train_df.shape))\nprint (\"testing dataset shape is {}\".format(test_df.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_names = train_df.columns.values\ncol_names[0] = 'ID' ## rename first column to ID\ntrain_df.columns = col_names ## assign new column name to training dataset\ntest_df.columns = col_names ## assign new column name to testing dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check column type"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_df.dtypes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test_df.dtypes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check distribution of each features to see outlier\n\n\"MonthlyIncome\" and \"NumberOfDependents\" are removed here as they have nan values"},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove ID, target variable Dlqin2yrs and variables with missing values\nfeature_list=list(train_df.columns.values)\nremove_list = ['ID','SeriousDlqin2yrs','MonthlyIncome','NumberOfDependents']\nfor each in remove_list:\n    feature_list.remove(each)\n\nfor each in feature_list:\n    sns.distplot(train_df[each])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Distribution of following features are highly skewed.\n\n* RevolvingUtilizationOfUnsecuredLines\n* NumberOfTime30-59DaysPastDueNotWorse\n* DebtRatio\n* NumberOfTimes30DaysLate\n* NumberRealEstateLoansOrLines\n* NumberOfTime60-89DaysPastDueNotWorse\n\nTake a log transformation to see if distribution can be less skewed."},{"metadata":{"trusted":true},"cell_type":"code","source":"print (train_df.columns.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"log_trans_list = train_df.columns.values[[2,4,5,8,9,10]]\nlog_trans_list\nfor each in log_trans_list:\n    train_df[each] = np.log(1+train_df[each].values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Distribution after log transformation"},{"metadata":{"trusted":true},"cell_type":"code","source":"for each in feature_list:\n    sns.distplot(train_df[each])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The distribution after transformation is much less skewed. We may able to put them into machine learning algorithm later.\n\nRemove nan values in \"MonthlyIncome\" and \"NumberOfDependents\" to check their distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"partial_train_df = train_df[['MonthlyIncome','NumberOfDependents']]\n#partial_train_df.dropna(how='any')\npartial_train_df = partial_train_df.dropna(how='any')\n\nsns.distplot(partial_train_df['MonthlyIncome'])\nplt.show()\nsns.distplot(partial_train_df['NumberOfDependents'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"monthlyIncome is highly skewed. let us take log transformation on both then check their distribution again"},{"metadata":{"trusted":true},"cell_type":"code","source":"partial_train_df['MonthlyIncome'] = np.log(1+partial_train_df['MonthlyIncome'].values)\npartial_train_df['NumberOfDependents'] = np.log(1+partial_train_df['NumberOfDependents'].values)\n\n\nsns.distplot(partial_train_df['MonthlyIncome'])\nplt.show()\nsns.distplot(partial_train_df['NumberOfDependents'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Post transformation looks better than before. I will keep log transformation on both at this time."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['MonthlyIncome'] = np.log(1+train_df['MonthlyIncome'].values)\ntrain_df['NumberOfDependents'] = np.log(1+train_df['NumberOfDependents'].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grouped_df = train_df.groupby('age')\ndlinq_age = grouped_df['SeriousDlqin2yrs'].aggregate([np.mean,'count']).reset_index()\nprint(dlinq_age)\ndlinq_age.columns =['age','DlqinFreq','count']\nsns.regplot(x='age',y='DlqinFreq',data=dlinq_age)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the plot above, we can see:\n\n* DlinFreq is negatively associated with age in general\n* age of 0,99 and 101 looks like outliers\n* DlinFreq looks like a quardratic function of age. Put a higher order of age maybe helpful\n\nRemove outlier in age and create new feature $age^2$"},{"metadata":{"trusted":true},"cell_type":"code","source":"## remove outlier\ntrain_df = train_df[train_df['age'] != 0]\ntrain_df = train_df[train_df['age'] !=99]\ntrain_df = train_df[train_df['age'] !=101]\ngrouped_df = train_df.groupby('age')\ndlinq_age = grouped_df['SeriousDlqin2yrs'].aggregate([np.mean,'count']).reset_index()\ndlinq_age.columns =['age','DlqinFreq','count']\nsns.regplot(x='age',y='DlqinFreq',data=dlinq_age)\nplt.show()\n\n## create new features\ntrain_df['age_sqr'] = train_df['age'].values^2 \n## apply the same operation on testing set\ntest_df['age_sqr'] = test_df['age'].values^2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Split the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_y = train_df['SeriousDlqin2yrs']\n#'RevolvingUtilizationOfUnsecuredLines'\ntrain_X = train_df.drop(['SeriousDlqin2yrs','ID'],axis=1,inplace=False)\ntest_X = test_df.drop(['SeriousDlqin2yrs','ID'],axis=1,inplace=False)\nprint(type(train_y))\n\nskf = model_selection.StratifiedKFold(n_splits=5,random_state=100)\nxgb_params = {\n'eta':0.03,\n'max_depth':4,\n'sub_sample':0.9,\n'colsample_bytree':0.5,\n'objective':'binary:logistic',\n'eval_metric':'auc',\n'silent':0\n}\n\nprint(train_X.shape)\nprint(train_X.columns)\nprint(test_X.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_iteration =[]\nbest_score= []\ntraining_score = []\nfor train_ind,val_ind in skf.split(train_X,train_y):\n    #print (set(train_y))\n    #print (type(train_y))\n    X_train,X_val = train_X.iloc[train_ind,],train_X.iloc[val_ind,]\n    y_train,y_val = train_y.iloc[train_ind],train_y.iloc[val_ind]\n    #print (set(train_y))\n    #print (max(train_ind),min(train_ind),max(val_ind),min(val_ind))\n    #print (train_ind,val_ind)\n    #print(set(y_train))\n    dtrain = xgb.DMatrix(X_train,y_train,feature_names = X_train.columns)\n    dval = xgb.DMatrix(X_val,y_val,feature_names = X_val.columns)\n    model = xgb.train(xgb_params,dtrain,num_boost_round=1000,\n                      evals=[(dtrain,'train'),(dval,'val')],verbose_eval=True,early_stopping_rounds=30)\n    best_iteration.append(model.attributes()['best_iteration'])\n    best_score.append(model.attributes()['best_score'])\n    # training_score.append(model.attributes()['best_msg'].split()[1][-8:])\n    xgb.plot_importance(model)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def xgbCV(eta=[0.05],max_depth=[6],sub_sample=[0.9],colsample_bytree=[0.9]):\n    train_y = train_df['SeriousDlqin2yrs'] # label for training data\n    train_X = train_df.drop(['SeriousDlqin2yrs','ID'],axis=1,inplace=False) # feature for training data\n    test_X = test_df.drop(['SeriousDlqin2yrs','ID'],axis=1,inplace=False) # feature for testing data\n    skf = model_selection.StratifiedKFold(n_splits=5,random_state=100) # stratified sampling\n    train_performance ={} \n    val_performance={}\n    for each_param in itertools.product(eta,max_depth,sub_sample,colsample_bytree): # iterative over each combination in parameter space\n        xgb_params = {\n                    'eta':each_param[0],\n                    'max_depth':each_param[1],\n                    'sub_sample':each_param[2],\n                    'colsample_bytree':each_param[3],\n                    'objective':'binary:logistic',\n                    'eval_metric':'auc',\n                    'silent':0\n                    }\n        best_iteration =[]\n        best_score=[]\n        training_score=[]\n        for train_ind,val_ind in skf.split(train_X,train_y): # five fold stratified cross validation\n            X_train,X_val = train_X.iloc[train_ind,],train_X.iloc[val_ind,] # train X and train y\n            y_train,y_val = train_y.iloc[train_ind],train_y.iloc[val_ind] # validation X and validation y\n            dtrain = xgb.DMatrix(X_train,y_train,feature_names = X_train.columns) # convert into DMatrix (xgb library data structure)\n            dval = xgb.DMatrix(X_val,y_val,feature_names = X_val.columns) # convert into DMatrix (xgb library data structure)\n            model = xgb.train(xgb_params,dtrain,num_boost_round=1000, \n                              evals=[(dtrain,'train'),(dval,'val')],verbose_eval=False,early_stopping_rounds=30) # train the model\n            best_iteration.append(model.attributes()['best_iteration']) # best iteration regarding AUC in valid set\n            best_score.append(model.attributes()['best_score']) # best score regarding AUC in valid set\n            training_score.append(model.attributes()['best_msg'].split()[1][10:]) # best score regarding AUC in training set\n        valid_mean = (np.asarray(best_score).astype(np.float).mean()) # mean AUC in valid set\n        train_mean = (np.asarray(training_score).astype(np.float).mean()) # mean AUC in training set\n        val_performance[each_param] =  train_mean\n        train_performance[each_param] =  valid_mean\n        print (\"Parameters are {}. Training performance is {:.4f}. Validation performance is {:.4f}\".format(each_param,train_mean,valid_mean))\n    return (train_performance,val_performance)\n#xgbCV(eta=[0.01,0.02,0.03,0.04,0.05],max_depth=[4,6,8,10],colsample_bytree=[0.3,0.5,0.7,0.9]) \nxgbCV(eta=[0.04],max_depth=[4],colsample_bytree=[0.5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_X.columns)\nany(train_X.columns == test_X.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = xgb.DMatrix(train_X,train_y,feature_names=train_X.columns)\ntest = xgb.DMatrix(test_X,feature_names=test_X.columns)\nxgb_params = {\n                    'eta':0.03,\n                    'max_depth':4,\n                    'sub_sample':0.9,\n                    'colsample_bytree':0.5,\n                    'objective':'binary:logistic',\n                    'eval_metric':'auc',\n                    'silent':0\n                    }\n\nfinal_model = xgb.train(xgb_params,train,num_boost_round=500)\nypred = final_model.predict(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb.plot_importance(final_model)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SUB_1 = pd.DataFrame({'Id':test_df.ID.values,'Probability':ypred})\nSUB_1.to_csv('Submission.csv',index=False)\nSUB_1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":4}