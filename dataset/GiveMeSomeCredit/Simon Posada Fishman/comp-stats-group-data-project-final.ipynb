{"cells":[{"metadata":{"_uuid":"7ae3c53a0713ae5039e538b005637c78eb4ef78d"},"cell_type":"markdown","source":"<center> <h1> Math 154: Group Data Project </h1> </center>\n<center> <h2> Give Me Some Credit Kaggle Competition </h2> </center>\n<center> <i>  Seena Huang, Cecelia Sanborn, Harry Bendekgey, Simon Posada Fishman </i> </center>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":false,"_kg_hide-input":false},"cell_type":"code","source":"# Main tools we used:\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Models:\nfrom sklearn import svm\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Python utilities:\nimport time\nimport os\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8224d3a94a819b5be904a1104c3be7d29e29bc85"},"cell_type":"markdown","source":"# Data Exploration\n\nFirst let's take a look at what our predictors are:"},{"metadata":{"trusted":true,"_uuid":"c8c293d97840a52a7eaf293eb59293b10297981e"},"cell_type":"code","source":"df = pd.read_csv('../input/cs-training.csv')\nlist(df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3d678c200f151daffa2a24aca586813736bfa589"},"cell_type":"markdown","source":"We also want to know how balanced the data set is, so we check what portion defaulted:"},{"metadata":{"trusted":true,"_uuid":"21654918b2bafa931c1582fb595c1f8357ac3443"},"cell_type":"code","source":"df.SeriousDlqin2yrs.mean()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7aed923fd0ae0407319e6311f7759f2d3256258a"},"cell_type":"markdown","source":"\"If you're over 55 and need a mortgage, the important thing to know if that lenders can't deny you a loan based on your age. But age can factor into your mortgage equation.\" ([Source](https://www.chicagotribune.com/news/ct-xpm-2005-11-20-0511200488-story.html))\n\nThis should be our first concern: making sure we're in legally okay territory. In order to solve this, we need to confirm that in our data set age has a negative influence on probability of defaulting. Then we can assume that the models we build reflect that trend, and won't deny someone a loan simply because they are old. "},{"metadata":{"trusted":true,"_uuid":"3d86a58291df787b39530032e18231792775c191","_kg_hide-output":false},"cell_type":"code","source":"age_hist = df['age'].hist(by=df['SeriousDlqin2yrs'], bins=20, layout=(2,1))\nage_hist[0].set_xlim((0,100))\nage_hist[0].set_title('SeriousDlqin2yrs = 0')\nage_hist[1].set_xlim((0,100))\nage_hist[1].set_title('SeriousDlqin2yrs = 1')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d708204f916c8a194b2fe9d9d9d50e2c0da04888"},"cell_type":"markdown","source":"Nice! Looking at the distribution of people that defaulted vs those who did not shows us that, generally, younger people were more responsible for defaulting. So we're good to go, legally. \n\nLet's take a look at our data. We can start with Debt Ratio, the ratio of debt to assets. "},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"7f08281893f6ab709a4f24ce7400d6da3198ad46"},"cell_type":"code","source":"df.DebtRatio.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bca18e3bddc7155f3f8b610a6c0cfd433844e994"},"cell_type":"markdown","source":"It is very concerning that someone owes 330,000 times what they own. Maybe that person is a single outlier? Let's take a look:"},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"93da3d25cc75e1525279ddae6bab23e5d27a4e98"},"cell_type":"code","source":"df.DebtRatio.quantile([.975])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2c5bfadd8d77346870612035fa4fa8b14abc66f0"},"cell_type":"markdown","source":"Apparently 2.5% of the dataset owes more than 3,500 times what they own. We need to investigate further to see if these are outliers or not. We can see:"},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"a90330b4fdee3c9a9f5f7aa767c5b9c14b66ff88"},"cell_type":"code","source":"df[df['DebtRatio'] > 3489.025][['SeriousDlqin2yrs','MonthlyIncome']].describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"80b7cd4f633e21657fd7f72f0398f13a344309cb"},"cell_type":"markdown","source":"We see two particularly concerning things here. The first is that of 4,000 records with DebtRatio > 3,500, only 185 of them have a value for monthly income. Further, the people who do have monthly income seem to either have a monthly income of either 1 or 0. We see that: "},{"metadata":{"trusted":true,"_uuid":"334999e1cfda488e869b140620da2675ce46fc3c"},"cell_type":"code","source":"df[(df['DebtRatio'] > 3489.025) & (df['SeriousDlqin2yrs'] == df['MonthlyIncome'])].shape[0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6172ec7c1af1ecdff38890f57cc56f7b0b29f32c"},"cell_type":"markdown","source":"...of those 185 entries, 164 of them have the same value for 2 year default rate and monthly income, indicating that there is a data-entry error. \n\nThe second problem is that despite owing thousands of times what they own, these people aren't defaulting any more than the general population. We can conclude that these entries must be data-entry errors, so we will remove them from our model.\n\nNow let's take a look at NumberOfTimes90DaysLate."},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"f7545bedb5554d1ec40b94e7cc6daa2520558501"},"cell_type":"code","source":"df.groupby('NumberOfTimes90DaysLate').NumberOfTimes90DaysLate.count()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f54b9e2e0a7bf381eaf2a3fa989220ff3341463e"},"cell_type":"markdown","source":"It is interesting that no one is between 17 and 96 times late, but hundreds of people are 98 times late. We can take a look at these few hundred records:"},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"e591cbebab47c48fcefeae3733882c4d6280a2b1"},"cell_type":"code","source":"df[df['NumberOfTimes90DaysLate'] > 95][['SeriousDlqin2yrs','NumberOfTime60-89DaysPastDueNotWorse','NumberOfTime30-59DaysPastDueNotWorse','NumberOfTimes90DaysLate']].describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bd0bbe755f0fc3ef6b498a5538551ea53cbba71f"},"cell_type":"markdown","source":"Somehow, all these (300) people were 30-59 days late 96/98 times, 60-89 days late 96/98 times, and 90+ days late 96/98 times. This is really concerning.  However, the data might not be garbage, because (as expected) these people are defaulting at a massive rate (55%) compared to the population (6%). So we don't want to throw away this data. We can Winsorize by replacing all the 96/98s with 20s to make them not extreme outliers and see if that improves the models. We don't expect that to improve the random forests, which are robust to outliers, but it might improve the SVMs."},{"metadata":{"_uuid":"23d217a5afacf8dd529a5eb573bdc778207cc704"},"cell_type":"markdown","source":"Finally, let's take a look at revolving utilization of unsecured lines. It represents the ratio of money owed to credit limit, so it shouldn't get much above 1. First let's look at values close to 1 and see at what rate they're defaulting. We start with 0.9 to 4.0:"},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"b016580b052c600d946e3296799009d5e354be98"},"cell_type":"code","source":"df[(df['RevolvingUtilizationOfUnsecuredLines'] > .9) & (df['RevolvingUtilizationOfUnsecuredLines'] <= 4)].SeriousDlqin2yrs.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1875cc0fee5c64fb01162ea646ff35ef9bdbe1e2"},"cell_type":"markdown","source":"These 20,000 people are defaulting at a rate of almost 1-in-4. This is important! But what about people with more RUUL? Let's consider 4 to 10:"},{"metadata":{"trusted":true,"_uuid":"410537aeaff228b14a39692bb25fc9cdb3802524"},"cell_type":"code","source":"df[(df['RevolvingUtilizationOfUnsecuredLines'] > 4) & (df['RevolvingUtilizationOfUnsecuredLines'] <= 10)].SeriousDlqin2yrs.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cef91a9f505e640a3ee0421e94e05e7d3038e972"},"cell_type":"markdown","source":"There are only 23 records in this region, but they're still defaulting at a high rate. What about if we go even higher, and consider those with RUUL > 10?"},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"9e49ec19a827bcfa3cee35b28396df88a91355c1"},"cell_type":"code","source":"df[df['RevolvingUtilizationOfUnsecuredLines'] > 10].describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3750b5555e0452eaf0738261799c1fc8781ddcad"},"cell_type":"markdown","source":"These 241 people are not defaulting any more than anyone else, despite some of them owing hundreds of thousands of times their credit limits. These seem to be inconsistent with the rest of the data, so we can remove them from our model.\n\nFinally, let's take a look at using regression to fill in the missing values for MonthlyIncome:"},{"metadata":{"trusted":true,"_uuid":"5272387c6c55d734dd54f4df96d919b62ce813e7"},"cell_type":"code","source":"not_missing = df.dropna()\ntarget = 'MonthlyIncome'\npredictors = [c for c in list(not_missing) if c not in [target, 'Unnamed: 0','SeriousDlqin2yrs']]\nX_data = not_missing[predictors]\ny_data = not_missing[target]\nregr = LinearRegression().fit(X_data, y_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"8e6fe0b30985da9b519ac64404128de1edb1d21c"},"cell_type":"code","source":"regr.score(X_data, y_data)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"04bcac3790949599db8c326ff4bca17c485226a2"},"cell_type":"markdown","source":"This is a terrible $R^2$. It seems that our best bet is to just replace the missing values with the median (instead of the mean, because there are some extreme outliers in MonthlyIncome, who are making millions of dollars a month). The other column with missing values is NumberOfDependents. >50% of the non-missing values have 0 dependents, and if someone leaves the field blank it is likely to be due to not having any dependents, we'll replace these values with 0."},{"metadata":{"_uuid":"48b6794bb0be19bfd0a837db05bfcbdb67418afd"},"cell_type":"markdown","source":"## Datasets"},{"metadata":{"_uuid":"9ab79a63c4549b05a295ff9de178d55b1e697f2c"},"cell_type":"markdown","source":"Based on all of these insights, we built some datasets that will potentially improve our model performance:"},{"metadata":{"trusted":true,"_uuid":"2d09cadc9504003806e21abca73558987cf69682","scrolled":true},"cell_type":"code","source":"# Median Fill, Outliers Removed\nremoved_debt_outliers = df.drop(df[df['DebtRatio'] > 3489.025].index)\nremoved_debt_outliers = removed_debt_outliers.fillna(removed_debt_outliers.median())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f9920add1e1d3fd446fbc304027734c8cc6f4bb5"},"cell_type":"code","source":"# Removed utilization outliers\ndfus = removed_debt_outliers.drop(removed_debt_outliers[removed_debt_outliers['RevolvingUtilizationOfUnsecuredLines'] > 10].index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ae845f31cdd759d4e7834e8883f49e6838b15bb9"},"cell_type":"code","source":"# Removed 98s\ndfn98 = dfus.copy()\ndfn98.loc[dfn98['NumberOfTime30-59DaysPastDueNotWorse'] > 90, 'NumberOfTime30-59DaysPastDueNotWorse'] = 18\ndfn98.loc[dfn98['NumberOfTime60-89DaysPastDueNotWorse'] > 90, 'NumberOfTime60-89DaysPastDueNotWorse'] = 18\ndfn98.loc[dfn98['NumberOfTimes90DaysLate'] > 90, 'NumberOfTimes90DaysLate'] = 18","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"74eb4978884a4817d59334baff9ba92d208f9025"},"cell_type":"markdown","source":"We will revisit these in the next section."},{"metadata":{"_uuid":"ab8264612cf3d0f23dab7534567dd322b72f8e86"},"cell_type":"markdown","source":"# Model Testing"},{"metadata":{"_uuid":"684d64de7c59354c4ea80c5c6ee986bd91474ec5"},"cell_type":"markdown","source":"To be able to rapidly test our models and datasets and see how they compare against each other, we developed a tester program that automatically runs k-fold cross-validation on scikit models accross different datasets. "},{"metadata":{"trusted":true,"_uuid":"1173fed2cf7e3b55d078caee0a1babf0c5d67127"},"cell_type":"code","source":"# A utility class to test all of our models on different datasets\nclass Tester():\n    def __init__(self, target):\n        self.target = target\n        self.datasets = {}\n        self.models = {}\n        self.cache = {} # we added a simple cache to speed things up\n\n    def addDataset(self, name, df):\n        self.datasets[name] = df.copy()\n\n    def addModel(self, name, model):\n        self.models[name] = model\n        \n    def clearModels(self):\n        self.models = {}\n\n    def clearCache(self):\n        self.cache = {}\n    \n    def testModelWithDataset(self, m_name, df_name, sample_len, cv):\n        if (m_name, df_name, sample_len, cv) in self.cache:\n            return self.cache[(m_name, df_name, sample_len, cv)]\n\n        clf = self.models[m_name]\n        \n        if not sample_len: \n            sample = self.datasets[df_name]\n        else: sample = self.datasets[df_name].sample(sample_len)\n            \n        X = sample.drop([self.target], axis=1)\n        Y = sample[self.target]\n\n        s = cross_validate(clf, X, Y, scoring=['roc_auc'], cv=cv, n_jobs=-1)\n        self.cache[(m_name, df_name, sample_len, cv)] = s\n\n        return s\n\n    def runTests(self, sample_len=80000, cv=4):\n        # Tests the added models on all the added datasets\n        scores = {}\n        for m_name in self.models:\n            for df_name in self.datasets:\n                # print('Testing %s' % str((m_name, df_name)), end='')\n                start = time.time()\n\n                score = self.testModelWithDataset(m_name, df_name, sample_len, cv)\n                scores[(m_name, df_name)] = score\n                \n                end = time.time()\n                \n                # print(' -- %0.2fs ' % (end - start))\n\n        print('--- Top 10 Results ---')\n        for score in sorted(scores.items(), key=lambda x: -1 * x[1]['test_roc_auc'].mean())[:10]:\n            auc = score[1]['test_roc_auc']\n            print(\"%s --> AUC: %0.4f (+/- %0.4f)\" % (str(score[0]), auc.mean(), auc.std()))\n\n            \n# We will use a tester object across all models\ntester = Tester('SeriousDlqin2yrs')\n\n# You can add datasets like this:\ntester.addDataset('Drop Missing', df.dropna())\n\n# And models like this:\nrfc = RandomForestClassifier(n_estimators=15, max_depth = 6, random_state=0)\ntester.addModel('Simple Random Forest', rfc)\ntester.addModel('Simple SVM', svm.LinearSVC())\n\n# You can then use it to run the tests\ntester.runTests()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2935548777cefe9f9e2c2b576e025c31b08227a2"},"cell_type":"markdown","source":"Now that we have this tester up and running, let's go back to the datasets we created in our data exploration section. We can easily add them to the tester program to see how our models perform on them."},{"metadata":{"trusted":true,"_uuid":"5b084b9f9bdd50d9347a7699486badd446e2f29b"},"cell_type":"code","source":"tester.addDataset('Median Fill', df.fillna(df.median()))\ntester.addDataset('Median Fill, Outliers Removed', removed_debt_outliers)\ntester.addDataset('Removed 98s', dfn98)\ntester.addDataset('Removed utilization outliers', dfus)\n\ntester.runTests()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f0c68450ab10f4c665d441ef3ba31dd98ee584aa"},"cell_type":"markdown","source":"We noticed that the modified datasets have a big influence on the SVM AUC scores, but they don't affect the Random Forests as much. However, there's a noticeable gain in performance between simply dropping the missing values and the modified datasets accross all models. "},{"metadata":{"_uuid":"6aadbfc0c9da9ecd23c13449ecc855f809abe8c2"},"cell_type":"markdown","source":"## Random Forests"},{"metadata":{"_uuid":"d6ebe7b7dadba1900fdebac884cfd05938a4230d"},"cell_type":"markdown","source":"Random Forests seemed to be the best bet, so we decided to move forward with them and figure out the best tuning parameters. Running two for-loops allowed us to find the best value for max_depth and number of estimators, with our best model being the random forest with a depth of 9 and 16 estimators. When we use the data set that removes the RUUL outliers, it gives us an AUC of .8662. "},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"84f91d25303f97eac166997ebd338da994be92a9"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nfor i in range(5,10):\n    for j in range(10,20):\n        rfc = RandomForestClassifier(n_estimators=j,max_depth = i, random_state=0)\n        tester.addModel('Random Forest '+'d: '+str(i)+' est: '+str(j)  ,rfc)\n\ntester.runTests()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cc4b105704e580db4e821ccbeac15d7b07b4eb8a"},"cell_type":"markdown","source":"## K-Nearest Neighbors"},{"metadata":{"_uuid":"2cc41df6b8334f4700985d6d1bad8948b22daf02"},"cell_type":"markdown","source":"We were also curious to see how a simpler learner would perform, so we tested a simple K-Nearest Neighbors model. Unsurprisingly, the results were not that great."},{"metadata":{"trusted":true,"_uuid":"f5b3faecf9e2d767454d723ffed2b37a70ec9cf8"},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\ntester.clearModels()\n\nfor i in range(5, 10):\n    neigh = KNeighborsClassifier(n_neighbors=i)\n    tester.addModel('KNN k=%d' % i, neigh)\n    \ntester.runTests(50000)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6485a69d74ac2fb3090b68984fa8f772684b9a85"},"cell_type":"markdown","source":"## Neural Network"},{"metadata":{"_uuid":"b6b12dfa4b4f8f6844b82e90ecd0ee3992d3dcba"},"cell_type":"markdown","source":"On the other end of the spectrum, we were also curious about trying out really fancy models, so we created and ran a neural network on the data. While K-Nearest-Neighbors was the weakest model we could think to run, a neural network was the strongest thing we could think to do. We expected its performance to be quite high, but discovered exactly the opposite. When initially training the network, we were excited to see it classifying 93% of examples correctly until we realized that it was guessing the same value every time. Essentially, the neural network was not learning a real decision boundary but was instead capitalizing on the imbalance on our data to achieve a high accuracy score. In order to combat this, we attempted to weight our classes differently, but unfortunately were unsuccessful. If we had wanted to, we could have pursued undersampling in order to balance the sample. However, we decided it would be best to not throw away large portions of our data. Because of this in combination with the fact that we do not fully understand the inner workings of a neural network, we decided to use a model whose theory we did understand and that enabled us to take advantage of all of the data."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}