{"cells":[{"metadata":{},"cell_type":"markdown","source":"> \n## 导入包<a class=\"anchor\" id=\"1\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import AdaBoostClassifier\nimport math\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport lightgbm as lgb\nimport seaborn as sns\nimport sklearn.metrics as metrics\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import f1_score\nimport os\nfrom imblearn.ensemble import EasyEnsembleClassifier\nfrom sklearn.model_selection import KFold,StratifiedKFold\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nrandom_seed=1024","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_threshold(y,cv_pred):\n    best_score=0\n    for i in range(0,1000,10):\n        threshold=i/1000\n        pred=np.round(cv_pred-threshold+0.5)\n        score=f1_score(y,pred,average='macro')\n        if score>best_score:\n            best_score=score\n            best_threshold=threshold\n    return best_threshold,best_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cv_lgb(X_train,y_train,X_test,y_test,is_unbalance=False):\n    test_pred = np.zeros((X_test.shape[0],))\n    cv_pred = np.zeros((X_train.shape[0],))\n    skf = StratifiedKFold(n_splits=5, random_state=random_seed, shuffle=True)\n    for index, (train_index, test_index) in enumerate(skf.split(X_train, y_train)):\n        train_x, val_x, train_y, val_y = X_train.iloc[train_index], X_train.iloc[test_index], y_train.iloc[train_index], y_train.iloc[test_index]\n        model = lgb.LGBMClassifier(n_estimators=5000,\n                                    max_depth=4,\n                                    boosting_type=\"gbdt\",\n                                    #subsample=0.7,\n                                    colsample_bytree=0.7,\n                                    learning_rate=0.01,\n                                    random_state=random_seed,\n                                    bagging_seed= random_seed,\n                                    feature_fraction_seed= random_seed,\n                                    early_stopping_rounds=100,\n                                    metric='auc',\n                                    is_unbalance=is_unbalance\n            )\n        clf=model.fit(train_x,train_y, eval_set=(val_x,val_y), verbose=0, )\n        cv_pred[test_index] =clf.predict_proba(val_x)[:,1]\n        test_pred += clf.predict_proba(X_test)[:,1] / 5\n    auc=roc_auc_score(y_test,test_pred)\n    print('auc : ',round(auc,4))\n    fpr,tpr,thresholds= roc_curve(y_test,test_pred)\n    print('ks : ',round(max(tpr-fpr),4))\n    threshold,best_score=get_threshold(y_train,cv_pred)\n    print('best cv f1',round(best_score,4))\n    print('best threshold',threshold)\n    test_pred_label=np.round(test_pred-threshold+0.5)\n    score=f1_score(y_test,test_pred_label,average='macro')\n    print('best test f1',round(score,4))\n    #get_threshold(y_test,test_pred)\n    return ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_kaggle_data():\n    path='../input/GiveMeSomeCredit/'\n    df_train = pd.read_csv(path+\"/cs-training.csv\",index_col=0)\n    df_test = pd.read_csv(path+\"/cs-test.csv\",index_col=0)\n    df_train=df_train.fillna(0)\n    df_test=df_test.fillna(0)\n    #total_1 = int(df_train['SeriousDlqin2yrs'].sum()*0.1)\n    #df_train = pd.concat([df_train[df_train['SeriousDlqin2yrs']==1].sample(total_1,random_state=42),df_train[df_train['SeriousDlqin2yrs']==0]])\n    df_train=df_train.sample(frac=1)\n    print(df_train.shape,df_train['SeriousDlqin2yrs'].mean())\n    #train test split\n    X_train, X_test, y_train, y_test = train_test_split(df_train.drop(['SeriousDlqin2yrs'],axis=1), df_train['SeriousDlqin2yrs'], test_size=0.2, random_state=random_seed)\n    print('train bad rate : ',y_train.mean(),'test bad rate : ',y_test.mean())\n    return X_train, X_test, y_train, y_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test=load_kaggle_data()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#base model\nprint('base model')\ncv_lgb(X_train,y_train,X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#base model is_unbalance\nprint('base model is_unbalance')\ncv_lgb(X_train,y_train,X_test,y_test,is_unbalance=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#over sampling\nprint('over sampling')\nfrom imblearn.over_sampling import RandomOverSampler\nros = RandomOverSampler(random_state=0)\nX_train_balanced, y_train_balanced = ros.fit_resample(X_train, y_train)\nprint(y_train_balanced.value_counts())\ncv_lgb(X_train_balanced, y_train_balanced,X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#under sampling\nprint('under sampling')\nfrom imblearn.under_sampling import RandomUnderSampler\nrus = RandomUnderSampler(random_state=0)\nX_train_balanced, y_train_balanced = rus.fit_resample(X_train, y_train)\nprint(y_train_balanced.value_counts())\ncv_lgb(X_train_balanced, y_train_balanced,X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#EasyEnsemble\nprint('EasyEnsemble')\nmodel5 = EasyEnsembleClassifier(n_estimators=20, random_state=random_seed, base_estimator=lgb.LGBMClassifier(random_state=random_seed))\nmodel5.fit(X_train,y_train)\ny_hat = model5.predict_proba(X_test)[:,1]\nauc=roc_auc_score(y_test,y_hat)\nprint('auc : ',round(auc,4))\nfpr,tpr,thresholds= roc_curve(y_test,y_hat)\nprint('ks : ',round(max(tpr-fpr),4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#SMOTE\nprint('SMOTE')\nfrom imblearn.over_sampling import SMOTE \nsm = SMOTE(random_state=42)\nX_train_balanced, y_train_balanced = sm.fit_resample(X_train, y_train)\nmodel6 = lgb.LGBMClassifier(random_state=random_seed,is_unbalance=True)\ncv_lgb(X_train_balanced, y_train_balanced,X_test,y_test)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}