{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Try to predict credit"},{"metadata":{},"cell_type":"markdown","source":"## Agenda\n- Data loading and Data checking\n- EDA\n- Data preprocessing\n- Machine learning (Logistic regression, KNeithborsClassfier, Multilayer perceptron) and Validation\n- Submit"},{"metadata":{},"cell_type":"markdown","source":"### Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Basic libraries\nimport numpy as np\nimport pandas as pd\nimport warnings\nwarnings.simplefilter('ignore')\n\n# Directry check\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Data preprocessing\nimport datetime\nimport re\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Visualization\nfrom matplotlib import pyplot as plt\nimport folium\nimport seaborn as sns\n\n# Over sampling method, SMOTE\nfrom imblearn.over_sampling import SMOTE\n\n# Logistic regression\nfrom sklearn.linear_model import LogisticRegression\n\n# KNeithborsClassfier\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# parameter opimization\nfrom sklearn.model_selection import GridSearchCV\n\n# Multilayer perceptron\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\n\n# Validation\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.metrics import roc_curve, auc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data loading and Data chacking"},{"metadata":{"trusted":true},"cell_type":"code","source":"sample = pd.read_csv(\"/kaggle/input/GiveMeSomeCredit/sampleEntry.csv\")\ndf_train = pd.read_csv(\"/kaggle/input/GiveMeSomeCredit/cs-training.csv\", header=0)\ndata_dict = pd.read_excel(\"/kaggle/input/GiveMeSomeCredit/Data Dictionary.xls\")\ndf_test = pd.read_csv(\"/kaggle/input/GiveMeSomeCredit/cs-test.csv\", header=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dict","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Variable Name\tDescription\tType<br>\n- SeriousDlqin2yrs<br>\tPerson experienced 90 days past due delinquency or worse \tY/N\n- RevolvingUtilizationOfUnsecuredLines<br>\tTotal balance on credit cards and personal lines of credit except real estate and no installment debt like car loans divided by the sum of credit limits\tpercentage\n- age<br>\tAge of borrower in years\tinteger\n- NumberOfTime30-59DaysPastDueNotWorse<br>\tNumber of times borrower has been 30-59 days past due but no worse in the last 2 years.\tinteger\n- DebtRatio<br>\tMonthly debt payments, alimony,living costs divided by monthy gross income\tpercentage\n- MonthlyIncome<br>\tMonthly income\treal\n- NumberOfOpenCreditLinesAndLoans<br>\tNumber of Open loans (installment like car loan or mortgage) and Lines of credit (e.g. credit cards)\tinteger\n- NumberOfTimes90DaysLate<br>\tNumber of times borrower has been 90 days or more past due.\tinteger\n- NumberRealEstateLoansOrLines<br>\tNumber of mortgage and real estate loans including home equity lines of credit\tinteger\n- NumberOfTime60-89DaysPastDueNotWorse<br>\tNumber of times borrower has been 60-89 days past due but no worse in the last 2 years.\tinteger\n- NumberOfDependents<br>\tNumber of dependents in family excluding themselves (spouse, children etc.)\tinteger\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rename columns\ncolnames=[\"Id\", \"Probability\", \"RUUL\", \"age\", \"Time_30\", \"D_ratio\", \"M_income\", \"Ooen_loan\", \"Times_90\", \"E_loan\", \"Time_60\", \"Dependents\"]\n\ndf_train.columns=colnames\ndf_test.columns=colnames","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# data size\nprint(\"train data size:{}\".format(df_train.shape))\nprint(\"test data size:{}\".format(df_test.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# data info\nprint(\"train data info:\\n{}\".format(df_train.info()))\nprint(\"-\"*50)\nprint(\"test data info:\\n{}\".format(df_test.info()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Null data check\nprint(\"train data\")\nprint(df_train.isnull().sum())\nprint(\"-\"*50)\nprint(\"test data\")\nprint(df_test.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I must consider null data preprocessing for \"M_income\" and \"Dependents\"."},{"metadata":{},"cell_type":"markdown","source":"## EDA"},{"metadata":{},"cell_type":"markdown","source":"### Trage value distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"with plt.style.context(\"fivethirtyeight\"):\n    sns.countplot(df_train[\"Probability\"])\n    plt.title(\"Traget value count plot\\n(0:negative, 1:positive)\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"positive ratio is lower, so if i create a predition model, i need to adjust th dataset with sampling method. "},{"metadata":{},"cell_type":"markdown","source":"## Explanation parameters distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"negative_df = df_train.query(\"Probability==0\")\npositive_df = df_train.query(\"Probability==1\")\n\nwith plt.style.context(\"fivethirtyeight\"):\n    fig, ax = plt.subplots(2,5,figsize=(30,12))\n    plt.subplots_adjust(wspace=0.2, hspace=0.3)\n    for i in range(0,5):\n        for j in range(0,5):\n            sns.distplot(negative_df.iloc[:,2+i], ax=ax[0,i], kde=False, color=\"blue\", label=\"negative\")\n            sns.distplot(positive_df.iloc[:,2+i], ax=ax[0,i], kde=False, color=\"red\", label=\"positive\")\n            ax[0,i].set_title(\"{}\".format(negative_df.columns[2+i]))\n            ax[0,i].set_yscale(\"log\")\n            ax[0,i].legend(labels=[\"negetive\", \"positive\"])\n            sns.distplot(negative_df.iloc[:,7+j], ax=ax[1,j], kde=False, color=\"blue\", label=\"negative\")\n            sns.distplot(positive_df.iloc[:,7+j], ax=ax[1,j], kde=False, color=\"red\", label=\"positive\")\n            ax[1,j].set_title(\"{}\".format(negative_df.columns[7+j]))\n            ax[1,j].set_yscale(\"log\")\n            ax[1,j].legend(labels=[\"negetive\", \"positive\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- RUUL & D_ratio & M_income can be separated between negative and positive.\n- Time30 & Times90 & Time 60 have a outer data, but count is not smaii. Maybe they have important information.\n- Other data have outer data, too. and count is so small. Prediction may be optimized by excepting outer data."},{"metadata":{},"cell_type":"markdown","source":"## Explanation parameters correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot\nwith plt.style.context(\"fivethirtyeight\"):\n    sns.pairplot(df_train.sample(5000).iloc[:, 2:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# correlation\nmatrix = df_train.sample(5000).iloc[:, 2:].fillna(0) # tempolary fill na =0\ncols = df_train.sample(5000).iloc[:, 2:].columns\ncm = np.corrcoef(matrix.T)\n\nwith plt.style.context(\"fivethirtyeight\"):\n    sns.set(font_scale=1.0)\n    plt.figure(figsize=(10,10))\n    hm = sns.heatmap(cm,\n                cbar=True,\n                annot=True,\n                square=True,\n                cmap=\"RdBu_r\",\n                fmt=\".2f\",\n                annot_kws={\"size\":10},\n                yticklabels=cols,\n                xticklabels=cols,\n                vmax=1,\n                vmin=-1,\n                center=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Time30 & Times90 & Time 60 have a strong correlation, but we can see from plot, they are made by outer plot data.\n- No other correlation is found"},{"metadata":{},"cell_type":"markdown","source":"## Data preprocessing\n### Direction\n- Null data, data distribution are not normalized, so they were filled medan data.\n- I decided that outer data is dropped.\n- Inbalanced data preprocessing is needed. I try with SMOTE method."},{"metadata":{},"cell_type":"markdown","source":"### Null data of M_income and Dependents"},{"metadata":{"trusted":true},"cell_type":"code","source":"medi_m_income = df_train[\"M_income\"].median()\nmedi_dependents = df_train[\"Dependents\"].median()\n\n# M_income, train_data and test_data\ndf_train[\"M_income\"].fillna(medi_m_income, inplace=True)\ndf_train[\"M_income\"].fillna(medi_m_income, inplace=True)\n# Dependents, train_data and test_data\ndf_train[\"Dependents\"].fillna(medi_dependents, inplace=True)\ndf_train[\"Dependents\"].fillna(medi_dependents, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### outer data dropped distribution\n- \"RUUL\" and \"D_ratio\" and \"M_income\" and \"E_loan\" and \"Dependents\" : Over Quantile 99.99%(1500 data) are dropped.\n- \"age\" : 0 is one data, this is dropped."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_ml = df_train.copy()\n\n# age 0 is dropped\ndf_train_ml = df_train_ml.query(\"age != 0\")\n\n# Other data preprocessing, over quantile99.99% is dropped.\nRUUL_99 = df_train_ml[\"RUUL\"].quantile(0.9999)\nD_ratio_99 = df_train_ml[\"D_ratio\"].quantile(0.9999)\nM_income_99 = df_train_ml[\"M_income\"].quantile(0.9999)\nE_loan_99 = df_train_ml[\"E_loan\"].quantile(0.9999)\nDependents_99 = df_train_ml[\"Dependents\"].quantile(0.9999)\n\ndf_train_ml = df_train_ml[df_train_ml[\"RUUL\"]<RUUL_99]\ndf_train_ml = df_train_ml[df_train_ml[\"D_ratio\"]<D_ratio_99]\ndf_train_ml = df_train_ml[df_train_ml[\"M_income\"]<M_income_99]\ndf_train_ml = df_train_ml[df_train_ml[\"E_loan\"]<E_loan_99]\ndf_train_ml = df_train_ml[df_train_ml[\"Dependents\"]<Dependents_99]\n\ndf_train_ml = df_train_ml","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Recheck the distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"negative_df = df_train_ml.query(\"Probability==0\")\npositive_df = df_train_ml.query(\"Probability==1\")\n\nwith plt.style.context(\"fivethirtyeight\"):\n    fig, ax = plt.subplots(2,5,figsize=(30,12))\n    plt.subplots_adjust(wspace=0.2, hspace=0.3)\n    for i in range(0,5):\n        for j in range(0,5):\n            sns.distplot(negative_df.iloc[:,2+i], ax=ax[0,i], kde=False, color=\"blue\", label=\"negative\")\n            sns.distplot(positive_df.iloc[:,2+i], ax=ax[0,i], kde=False, color=\"red\", label=\"positive\")\n            ax[0,i].set_title(\"{}\".format(negative_df.columns[2+i]))\n            ax[0,i].set_yscale(\"log\")\n            ax[0,i].legend(labels=[\"negetive\", \"positive\"])\n            sns.distplot(negative_df.iloc[:,7+j], ax=ax[1,j], kde=False, color=\"blue\", label=\"negative\")\n            sns.distplot(positive_df.iloc[:,7+j], ax=ax[1,j], kde=False, color=\"red\", label=\"positive\")\n            ax[1,j].set_title(\"{}\".format(negative_df.columns[7+j]))\n            ax[1,j].set_yscale(\"log\")\n            ax[1,j].legend(labels=[\"negetive\", \"positive\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The distribution of the data has improved considerably"},{"metadata":{},"cell_type":"markdown","source":"### For Machine learning, SMOTE method is applied."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data\nX = df_train_ml[['RUUL', 'age', 'Time_30', 'D_ratio', 'M_income','Ooen_loan', 'Times_90', 'E_loan', 'Time_60', 'Dependents']]\ny = df_train_ml['Probability']\n\n# Data standarlization\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\n# Create StandardScaler instance and fit_trainsform\nsc = StandardScaler()\nX_train_std = sc.fit_transform(X_train)\nX_test_std = sc.fit_transform(X_test)\n\n# Create SMOTE instance\nsmote = SMOTE(sampling_strategy=\"auto\", random_state=10)\n\n# data split\nX_train_resampled, y_train_resampled = smote.fit_sample(X_train_std, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Machine learning (Logistic regression, KNeithborsClassfier, Multilayer perceptron) and Validation"},{"metadata":{},"cell_type":"markdown","source":"### Logistic regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create logistic regression instance\nlr = LogisticRegression()\n\n# Grid search\nparam_range = [0.001, 0.01, 0.1, 1.0, 10, 100]\npenalty = ['l1', 'l2']\nparam_grid = [{\"C\":param_range, \"penalty\":penalty}]\n\ngs = GridSearchCV(estimator=lr, param_grid=param_grid, scoring=\"recall\", cv=10, n_jobs=-1)\ngs = gs.fit(X_train_resampled, y_train_resampled)\n\nprint(gs.best_score_.round(3))\nprint(gs.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_lr = gs.best_estimator_\nprint('Test accuracy: %.3f' % clf_lr.score(X_test_std, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = clf_lr.predict(X_test_std)\ny_pred_train = clf_lr.predict(X_train_std)\n\n# Validation of model\nprint(\"confusion_matrix = \\n\", confusion_matrix(y_true=y_test, y_pred=y_pred))\n\nprint(\"*accuracy_train = %.3f\" % accuracy_score(y_true=y_train, y_pred=y_pred_train))\nprint(\"accuracy = %.3f\" % accuracy_score(y_true=y_test, y_pred=y_pred))\n\nprint(\"*precision_train = %.3f\" % precision_score(y_true=y_train, y_pred=y_pred_train))\nprint(\"precision = %.3f\" % precision_score(y_true=y_test, y_pred=y_pred))\n\nprint(\"*recall_train = %.3f\" % recall_score(y_true=y_train, y_pred=y_pred_train))\nprint(\"recall = %.3f\" % recall_score(y_true=y_test, y_pred=y_pred))\n\nprint(\"*f1_score_train = %.3f\" % f1_score(y_true=y_train, y_pred=y_pred_train))\nprint(\"f1_score = %.3f\" % f1_score(y_true=y_test, y_pred=y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ROC curve and AUC\ny_score = clf_lr.predict_proba(X_test_std)[:, 1]\n\nfpr, tpr, thresholds = roc_curve(y_true=y_test, y_score=y_score)\n# Visualization\nwith plt.style.context(\"fivethirtyeight\"):\n    plt.figure(figsize=(10,6))\n    plt.plot(fpr, tpr, label=\"roc curve (area = %.3f)\" % auc(fpr, tpr))\n    plt.plot([0,1], [0,1], linestyle='--', label='random')\n    plt.plot([0,0,1], [0,1,1], linestyle='--', label=\"ideal\")\n    plt.legend()\n    plt.xlabel(\"false positive rate\")\n    plt.ylabel(\"true positive rate\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### KNeithborsClassfier"},{"metadata":{"trusted":true},"cell_type":"code","source":"# KNeithborsClassfier\nknn = KNeighborsClassifier(metric='minkowski')\n\n# Grid search\nparam_range = [5, 10, 15, 20]\nparam_grid = [{\"n_neighbors\":param_range, \"p\":[1,2]}]\n\ngs_knn = GridSearchCV(estimator=knn, param_grid=param_grid, scoring=\"accuracy\", cv=10, n_jobs=-1)\ngs_knn = gs_knn.fit(X_train_std, y_train)\n\nprint(gs_knn.best_score_.round(3))\nprint(gs_knn.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_knn = gs.best_estimator_\nprint('Test accuracy: %.3f' % clf_knn.score(X_test_std, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = clf_knn.predict(X_test_std)\ny_pred_train = clf_knn.predict(X_train_std)\n\n# Validation of model\nprint(\"confusion_matrix = \\n\", confusion_matrix(y_true=y_test, y_pred=y_pred))\n\nprint(\"*accuracy_train = %.3f\" % accuracy_score(y_true=y_train, y_pred=y_pred_train))\nprint(\"accuracy = %.3f\" % accuracy_score(y_true=y_test, y_pred=y_pred))\n\nprint(\"*precision_train = %.3f\" % precision_score(y_true=y_train, y_pred=y_pred_train))\nprint(\"precision = %.3f\" % precision_score(y_true=y_test, y_pred=y_pred))\n\nprint(\"*recall_train = %.3f\" % recall_score(y_true=y_train, y_pred=y_pred_train))\nprint(\"recall = %.3f\" % recall_score(y_true=y_test, y_pred=y_pred))\n\nprint(\"*f1_score_train = %.3f\" % f1_score(y_true=y_train, y_pred=y_pred_train))\nprint(\"f1_score = %.3f\" % f1_score(y_true=y_test, y_pred=y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ROC curve and AUC\ny_score = clf_knn.predict_proba(X_test_std)[:, 1]\n\nfpr, tpr, thresholds = roc_curve(y_true=y_test, y_score=y_score)\n# Visualization\nwith plt.style.context(\"fivethirtyeight\"):\n    plt.figure(figsize=(10,6))\n    plt.plot(fpr, tpr, label=\"roc curve (area = %.3f)\" % auc(fpr, tpr))\n    plt.plot([0,1], [0,1], linestyle='--', label='random')\n    plt.plot([0,0,1], [0,1,1], linestyle='--', label=\"ideal\")\n    plt.legend()\n    plt.xlabel(\"false positive rate\")\n    plt.ylabel(\"true positive rate\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Multilayer Perceptron"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_resampled = np.expand_dims(y_train_resampled, axis=1)\ny_test = np.expand_dims(y_test, axis=1)\n\n# shape check\nprint(\"X_train_resampled: %s, y_train_resampled: %s\" % (X_train_resampled.shape, y_train_resampled.shape))\nprint(\"X_test: %s, y_test: %s\" % (X_test.shape, y_test.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model\nmodel = Sequential()\nmodel.add(Dense(8, activation='relu', input_dim=10))\nmodel.add(Dense(5, activation='relu'))\nmodel.add(Dense(3, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer=SGD(lr=0.01), loss='mse')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mc = ModelCheckpoint(\"model_01.h2\", monitor=\"val_loss\", save_best_only=True, verbose=1)\nes = EarlyStopping(monitor='val_loss', patience=10)\n\nhist = model.fit(X_train_resampled, y_train_resampled,\n                 callbacks=[mc, es],\n                 epochs=100, batch_size=16,\n                 validation_split=0.2, verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualization loss plot\ntrain_loss = hist.history[\"loss\"]\nval_loss = hist.history[\"val_loss\"]\nwith plt.style.context(\"fivethirtyeight\"):\n    plt.figure(figsize=(8, 4))\n    plt.plot(range(len(train_loss)), train_loss, label='train_loss')\n    plt.plot(range(len(val_loss)), val_loss, label='valid_loss')\n    plt.xlabel('epoch', fontsize=16)\n    plt.ylabel('loss', fontsize=16)\n    plt.yscale('log')\n    plt.legend(fontsize=16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test data prediction\ny_pred = model.predict(X_test_std)\ny_pred = [num[0] for num in y_pred]\ny_pred_df = pd.DataFrame({\"y_pred\":y_pred})\n\n# train data prediction\ny_pred_train = model.predict(X_train_std)\ny_pred_train = [num[0] for num in y_pred_train]\ny_pred_train_df = pd.DataFrame({\"y_pred\":y_pred_train})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# y_pred flg\ndef prediction_flg(x):\n    if x[\"y_pred\"] > 0.5:\n        res=1\n    else:\n        res=0\n    return res\n\ny_pred_df[\"y_flg\"] = y_pred_df.apply(prediction_flg, axis=1)\ny_pred_train_df[\"y_flg\"] = y_pred_train_df.apply(prediction_flg, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = y_pred_df[\"y_flg\"]\ny_pred_train = y_pred_train_df[\"y_flg\"]\n\n# Validation of model\nprint(\"confusion_matrix = \\n\", confusion_matrix(y_true=y_test, y_pred=y_pred))\n\nprint(\"*accuracy_train = %.3f\" % accuracy_score(y_true=y_train, y_pred=y_pred_train))\nprint(\"accuracy = %.3f\" % accuracy_score(y_true=y_test, y_pred=y_pred))\n\nprint(\"*precision_train = %.3f\" % precision_score(y_true=y_train, y_pred=y_pred_train))\nprint(\"precision = %.3f\" % precision_score(y_true=y_test, y_pred=y_pred))\n\nprint(\"*recall_train = %.3f\" % recall_score(y_true=y_train, y_pred=y_pred_train))\nprint(\"recall = %.3f\" % recall_score(y_true=y_test, y_pred=y_pred))\n\nprint(\"*f1_score_train = %.3f\" % f1_score(y_true=y_train, y_pred=y_pred_train))\nprint(\"f1_score = %.3f\" % f1_score(y_true=y_test, y_pred=y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ROC curve and AUC\ny_score = model.predict_proba(X_test_std)[:, 0]\n\nfpr, tpr, thresholds = roc_curve(y_true=y_test, y_score=y_score)\n# Visualization\nwith plt.style.context(\"fivethirtyeight\"):\n    plt.figure(figsize=(10,6))\n    plt.plot(fpr, tpr, label=\"roc curve (area = %.3f)\" % auc(fpr, tpr))\n    plt.plot([0,1], [0,1], linestyle='--', label='random')\n    plt.plot([0,0,1], [0,1,1], linestyle='--', label=\"ideal\")\n    plt.legend()\n    plt.xlabel(\"false positive rate\")\n    plt.ylabel(\"true positive rate\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Best ML model is Multilayer perceptron, sosubmitting with this model."},{"metadata":{},"cell_type":"markdown","source":"## Submit"},{"metadata":{},"cell_type":"markdown","source":"Test data prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_Test = df_test[['RUUL', 'age', 'Time_30', 'D_ratio', 'M_income','Ooen_loan', 'Times_90', 'E_loan', 'Time_60', 'Dependents']]\n\nX_Test_std = sc.fit_transform(X_Test)\n\n# submit Test data prediction\ny_Pred = model.predict(X_Test_std)\ny_Pred = [num[0] for num in y_Pred]\ny_Pred_df = pd.DataFrame({\"y_pred\":y_Pred})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit = pd.DataFrame({\"Id\":df_test[\"Id\"],\n                     \"Probability\":y_Pred_df[\"y_pred\"]})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}