{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"I made Q-learning starter notebook.\n\nIt's first time to try reinforcement learning, so if you find mistake, please tell me by comment.\nAlso, I'm glad if you share me opinion to make good state key by comment.","metadata":{}},{"cell_type":"raw","source":"!pip install kaggle-environments","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"from kaggle_environments import make\n\nenv = make(\"kore_fleets\", debug=True)\nprint(env.name, env.version)","metadata":{"execution":{"iopub.status.busy":"2022-04-01T00:34:29.296662Z","iopub.execute_input":"2022-04-01T00:34:29.297366Z","iopub.status.idle":"2022-04-01T00:34:29.341851Z","shell.execute_reply.started":"2022-04-01T00:34:29.297329Z","shell.execute_reply":"2022-04-01T00:34:29.340922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from kaggle_environments.envs.kore_fleets.helpers import *\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport random\nimport seaborn as sns\nfrom tqdm import tqdm\nimport itertools\nimport pickle","metadata":{"execution":{"iopub.status.busy":"2022-04-01T00:34:29.761763Z","iopub.execute_input":"2022-04-01T00:34:29.762038Z","iopub.status.idle":"2022-04-01T00:34:29.767406Z","shell.execute_reply.started":"2022-04-01T00:34:29.762009Z","shell.execute_reply":"2022-04-01T00:34:29.766474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Copy from [Kore Intro Part 3: Expanding the Empire!](https://www.kaggle.com/code/bovard/kore-intro-part-3-expanding-the-empire) to use when learning.","metadata":{}},{"cell_type":"code","source":"%%writefile pilot.py\n   \nfrom kaggle_environments.envs.kore_fleets.helpers import *\nfrom random import randint\n\n# a flight plan\ndef build_flight_plan(dir_idx, size):\n    flight_plan = \"\"\n    for i in range(4):\n        flight_plan += Direction.from_index((dir_idx + i) % 4).to_char()\n        if not i == 3:\n            flight_plan += str(size)\n    return flight_plan\n\ndef agent(obs, config):\n    board = Board(obs, config)\n    me=board.current_player\n\n    me = board.current_player\n    turn = board.step\n    spawn_cost = board.configuration.spawn_cost\n    kore_left = me.kore\n\n    for shipyard in me.shipyards:\n        if shipyard.ship_count >= 50:\n            flight_plan = build_flight_plan(randint(0, 3), randint(2, 9))\n            action = ShipyardAction.launch_fleet_with_flight_plan(50, flight_plan)\n            shipyard.next_action = action\n        elif kore_left >= spawn_cost:\n            action = ShipyardAction.spawn_ships(1)\n            shipyard.next_action = action\n\n    return me.next_actions","metadata":{"execution":{"iopub.status.busy":"2022-04-01T00:34:30.000609Z","iopub.execute_input":"2022-04-01T00:34:30.001047Z","iopub.status.idle":"2022-04-01T00:34:30.007602Z","shell.execute_reply.started":"2022-04-01T00:34:30.000945Z","shell.execute_reply":"2022-04-01T00:34:30.006995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Start Q-learning","metadata":{}},{"cell_type":"markdown","source":"I refer to this article [kaggleで強化学習をやってみた](https://yukoishizaki.hatenablog.com/entry/2020/04/05/202935) to make QTable and QLearningAgent.","metadata":{}},{"cell_type":"code","source":"# Making list of shipyard actions.\nnum_ship_list = [3,5,8,13,21,34,55,91]\n\nactions_list = [f'spawn_{i}' for i in range(1, 9)]\nactions_list += [f'straight_{i}_{s}' for i in [2, 5, 9] for s in num_ship_list]\nactions_list += [f'cycle_{i}_{s}_{d}' for i in [2, 5, 9] for s in num_ship_list[4:] for d in range(4) ]\nactions_list += [f'invade_{s}' for s in num_ship_list[4:]]\nactions_list += [f'build_{s}' for s in num_ship_list[6:]]\nactions_list","metadata":{"execution":{"iopub.status.busy":"2022-04-01T00:34:30.326356Z","iopub.execute_input":"2022-04-01T00:34:30.326689Z","iopub.status.idle":"2022-04-01T00:34:30.33947Z","shell.execute_reply.started":"2022-04-01T00:34:30.326654Z","shell.execute_reply":"2022-04-01T00:34:30.338605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Q table\nclass QTable():\n    def __init__(self, actions):\n        self.Q = {}\n        self.actions = actions\n    \n    def get_state_key(self, state):\n        # Use number of kore owned and ship number as state key.\n        board = Board(state, env.configuration)\n        me=board.current_player\n        me_obser = me._observation\n        \n        turn = board.step\n        kore_left = me.kore\n        \n        ship_number = 0\n        for value in me_obser[1].values():\n            ship_number += value[1]\n        \n        fleet_number = 0\n        for value in me_obser[2].values():\n            fleet_number += value[2]\n        return f'{int(np.log(kore_left+1))}_{int(ship_number / 2)}'\n        \n    def get_q_values(self, state):\n        # Output an array of Q values for all actions for the state\n        state_key = self.get_state_key(state)\n        if state_key not in self.Q.keys():\n            self.Q[state_key] = [0] * len(self.actions)\n        return self.Q[state_key]\n    \n    def update(self, state, action_str, add_q):\n        state_key = self.get_state_key(state)\n        self.Q[state_key] = [q + add_q if self.actions[idx] == action_str else q for idx, q in enumerate(self.Q[state_key])]","metadata":{"execution":{"iopub.status.busy":"2022-04-01T00:34:30.407812Z","iopub.execute_input":"2022-04-01T00:34:30.408113Z","iopub.status.idle":"2022-04-01T00:34:30.41897Z","shell.execute_reply.started":"2022-04-01T00:34:30.408079Z","shell.execute_reply":"2022-04-01T00:34:30.418288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Helper of actions\ndef pos_to_index(pos):\n    x = pos[0]\n    y = pos[1]\n    x = (x + 31) if x < 0 else x\n    x = (x - 31) if x > 30 else x\n    y = (y + 31) if y < 0 else y\n    y = (y - 31) if y > 30 else y\n\n    return x * 31 + y\n\n# Get straight score\ndef get_straight_score(pos, kore, length, direction):\n    score = 0\n    for i in range(length):\n        pos += direction.to_point()\n        score += kore[pos_to_index(pos)]\n    return score\n\ndef get_max_straight_plan(pos, kore, length):\n    scores = []\n    for i in range(4):\n        direction = Direction.from_index(i)\n        scores.append(get_straight_score(pos, kore, length, direction))\n    scores = sorted(scores, reverse=True)\n#     direction_index = scores.index(scores[np.random.randint(2)])\n    direction_index = scores.index(scores[0])\n    \n    direction = Direction.from_index(direction_index)\n    flight_plan = direction.to_char()\n    flight_plan += str(length)\n    flight_plan += direction.opposite().to_char()\n    return flight_plan\n\n\n# Get maximum cycle\ndef get_max_cycle_plan(pos, kore, length, direct_i):\n    direction = Direction.from_index(direct_i) \n    each_lengths = [i for i in range(1,length)]\n    each_lengths = [i for i in itertools.product(each_lengths, repeat=2)]\n    each_lengths = [i + tuple([i[0], i[1]]) for i in each_lengths]\n    \n    # Get max score\n    scores = []\n    for rotate_i in range(2):\n        for each_length in each_lengths:\n            each_pos = pos\n            each_direction = direction\n            score = 0\n            for direct_i in range(4):\n                for i in range(each_length[direct_i]):\n                    each_pos += each_direction.to_point()\n                    score += kore[pos_to_index(each_pos)]\n                if rotate_i == 0:\n                    each_direction = each_direction.rotate_right()\n                else:\n                    each_direction = each_direction.rotate_left()\n            scores.append(score)\n    \n    # Get flight plan\n    rotate_right = True\n    best_index = scores.index(max(scores))\n    if best_index >= len(each_lengths):\n        best_index -= len(each_lengths)\n        rotate_right = False\n        \n    best_each_length = each_lengths[best_index]\n    each_direction = direction\n    flight_plan = \"\"\n    for i in range(4):\n        flight_plan += each_direction.to_char()\n        \n        length = best_each_length[i] - 1\n        if i != 3 and length > 0:\n            flight_plan += str(length)\n        if rotate_right:\n            each_direction = each_direction.rotate_right()\n        else:\n            each_direction = each_direction.rotate_left()\n    return flight_plan\n\n\ndef get_closest_enemy_shipyard(board, position, me):\n    min_dist = 1000000\n    enemy_shipyard = None\n    for shipyard in board.shipyards.values():\n        if shipyard.player_id == me.id:\n            continue\n        dist = position.distance_to(shipyard.position, board.configuration.size)\n        if dist < min_dist:\n            min_dist = dist\n            enemy_shipyard = shipyard\n    return enemy_shipyard","metadata":{"execution":{"iopub.status.busy":"2022-04-01T00:34:30.49087Z","iopub.execute_input":"2022-04-01T00:34:30.491498Z","iopub.status.idle":"2022-04-01T00:34:30.513144Z","shell.execute_reply.started":"2022-04-01T00:34:30.491459Z","shell.execute_reply":"2022-04-01T00:34:30.512296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert action string to ShipyardAction.\ndef shipyard_action(action_str, pos, kore, board=None, me=None):\n    inst_list = action_str.split('_')\n    if inst_list[0] == 'spawn':\n        return ShipyardAction.spawn_ships(int(inst_list[1]))\n    \n    elif inst_list[0] == 'straight':\n        flight_plan = get_max_straight_plan(pos, kore, int(inst_list[1]))\n        return ShipyardAction.launch_fleet_with_flight_plan(int(inst_list[2]), flight_plan)\n    \n    elif inst_list[0] == 'cycle':\n        flight_plan = get_max_cycle_plan(pos, kore, int(inst_list[1]), int(inst_list[3]))\n        return ShipyardAction.launch_fleet_with_flight_plan(int(inst_list[2]), flight_plan)\n    \n    elif inst_list[0] == 'invade':\n        closest_enemy_shipyard = get_closest_enemy_shipyard(board, pos, me)\n        if not closest_enemy_shipyard:\n            return None\n        enemy_pos = closest_enemy_shipyard.position\n        my_pos = pos\n        flight_plan = \"N\" if enemy_pos.y > my_pos.y else \"S\"\n        flight_plan += str(abs(enemy_pos.y - my_pos.y) - 1)\n        flight_plan += \"W\" if enemy_pos.x < my_pos.x else \"E\"\n        if (abs(enemy_pos.y - my_pos.y) - 1) < 0:\n            return None\n        if not all([c in \"NESWC0123456789\" for c in flight_plan]):\n            return None\n\n        return ShipyardAction.launch_fleet_with_flight_plan(int(inst_list[1]), flight_plan)\n\n    elif inst_list[0] == 'build':\n        length = 6\n        flight_plan = get_max_straight_plan(pos, kore, length)\n        flight_plan = flight_plan[:-1]\n        direction = Direction.from_char(flight_plan[0])\n        direction = direction.rotate_right()\n        flight_plan += direction.to_char()\n        flight_plan += 'C'\n        return ShipyardAction.launch_fleet_with_flight_plan(int(inst_list[1]), flight_plan)","metadata":{"execution":{"iopub.status.busy":"2022-04-01T00:34:30.574042Z","iopub.execute_input":"2022-04-01T00:34:30.574337Z","iopub.status.idle":"2022-04-01T00:34:30.58788Z","shell.execute_reply.started":"2022-04-01T00:34:30.574308Z","shell.execute_reply":"2022-04-01T00:34:30.586875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"env = make(\"kore_fleets\", debug=True)\ntrainer = env.train([None, \"./pilot.py\", \"./pilot.py\", \"./pilot.py\"])\n\nclass QLearningAgent():\n    def __init__(self, env, epsilon=0.99):\n        self.env = env\n        self.actions = actions_list.copy()\n        self.q_table = QTable(self.actions)\n        self.epsilon = epsilon\n        self.reward_log = []    \n        \n    def policy(self, state, shipyard, kore_left):\n        # Require: spawn_cost, kore_left,\n        possible_actions = self.actions.copy()\n        for action in self.actions:\n            inst_list = action.split('_')\n            if inst_list[0] == 'spawn':\n                if (int(inst_list[1]) * 10) >= kore_left or int(inst_list[1]) > shipyard.max_spawn:\n                    possible_actions.remove(action)\n            elif inst_list[0] == 'straight' or inst_list[0] == 'cycle':\n                if int(inst_list[2]) > shipyard.ship_count:\n                    possible_actions.remove(action)\n            elif inst_list[0] == 'invade' or inst_list[0] == 'build':\n                if int(inst_list[1]) > shipyard.ship_count:\n                    possible_actions.remove(action)\n        if len(possible_actions) == 0:\n            return None\n        # Epsilon-Greedy\n        if np.random.random() < self.epsilon:\n            return random.choice(possible_actions)\n        else:\n            # Select max Q value action.\n            q_values = self.q_table.get_q_values(state)\n            selected_items = [q if self.actions[idx] in possible_actions else -1e7 for idx, q in enumerate(q_values)]\n            return self.actions[int(np.argmax(selected_items))]\n        \n    def learn(self, trainer, episode_cnt=100, gamma=0.6, \n              learn_rate=0.3, epsilon_decay_rate=0.99, min_epsilon=0.1):\n        for episode in tqdm(range(episode_cnt)):\n            state = trainer.reset()\n            self.epsilon = max(min_epsilon, self.epsilon * epsilon_decay_rate) \n            while not env.done:\n                # Update q to each shipyard.\n                board = Board(state, self.env.configuration)\n                spawn_cost = board.configuration.spawn_cost\n\n                me = board.current_player\n                turn = board.step\n                kore_left = me.kore\n\n                observation = board.observation\n                kore = observation['kore']\n                \n                actions_str = []\n                # loop through all shipyards you control\n                for shipyard in me.shipyards:\n                    spawn_max = shipyard.max_spawn\n                    action_str = self.policy(state, shipyard, kore_left)\n                    actions_str.append(action_str)\n                    if action_str == None:\n                        actions_str.append(None)\n                        continue\n                    shipyard.next_action = shipyard_action(action_str, shipyard.position, kore, board, me)\n                next_state, reward, done, info = trainer.step(me.next_actions)\n                \n                reward = kore_left\n                for value in me._observation[2].values():\n                    reward += (value[2] * 12)\n                reward += (len(me.shipyards) * 60)\n                gain = reward + gamma * max(self.q_table.get_q_values(next_state))\n                \n                # Calucurate error and update Q-table.\n                for i, shipyard in enumerate(me.shipyards):\n                    action_str = actions_str[i]\n                    if action_str == None:\n                        continue\n                    estimate = self.q_table.get_q_values(state)[self.actions.index(action_str)]\n                    self.q_table.update(state, action_str, learn_rate * (gain - estimate))\n\n                state = next_state\n            self.reward_log.append(reward)","metadata":{"execution":{"iopub.status.busy":"2022-04-01T00:34:30.758314Z","iopub.execute_input":"2022-04-01T00:34:30.758565Z","iopub.status.idle":"2022-04-01T00:34:30.877378Z","shell.execute_reply.started":"2022-04-01T00:34:30.758538Z","shell.execute_reply":"2022-04-01T00:34:30.876438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"episode_cnt=100\nepsilon_decay_rate=0.995\nx = [i for i in range(episode_cnt)]\ny = [max(0.99*(epsilon_decay_rate**i), 0.1) for i in range(episode_cnt)]\nplt.plot(x, y)","metadata":{"execution":{"iopub.status.busy":"2022-04-01T00:39:20.002968Z","iopub.execute_input":"2022-04-01T00:39:20.0036Z","iopub.status.idle":"2022-04-01T00:39:20.238981Z","shell.execute_reply.started":"2022-04-01T00:39:20.003557Z","shell.execute_reply":"2022-04-01T00:39:20.237762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 学習\nqa = QLearningAgent(env)\nqa.learn(trainer, episode_cnt=episode_cnt, epsilon_decay_rate=epsilon_decay_rate)\n# qa.learn(trainer, episode_cnt=1000)\n\nsns.set(style='darkgrid')\n# pd.DataFrame({'Average Reward': qa.reward_log}).rolling(10).mean().plot(figsize=(10,5))\npd.DataFrame({'Average Reward': qa.reward_log}).plot(figsize=(10,5))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-01T00:35:26.802947Z","iopub.execute_input":"2022-04-01T00:35:26.803272Z","iopub.status.idle":"2022-04-01T00:36:51.037835Z","shell.execute_reply.started":"2022-04-01T00:35:26.803236Z","shell.execute_reply":"2022-04-01T00:36:51.036882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sort by increasing Q value and tie the list of indexes to each state.\ntmp_dict_q_table = qa.q_table.Q.copy()\ndict_q_table = dict()\n\nfor k in tmp_dict_q_table:\n    if np.count_nonzero(tmp_dict_q_table[k]) > 0:\n        dict_q_table[k] = np.argsort(tmp_dict_q_table[k]).argsort().tolist()","metadata":{"execution":{"iopub.status.busy":"2022-04-01T00:36:51.03976Z","iopub.execute_input":"2022-04-01T00:36:51.040137Z","iopub.status.idle":"2022-04-01T00:36:51.060139Z","shell.execute_reply.started":"2022-04-01T00:36:51.040055Z","shell.execute_reply":"2022-04-01T00:36:51.059259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('dict.pkl','wb') as f:\n    pickle.dump(dict_q_table, f)","metadata":{"execution":{"iopub.status.busy":"2022-04-01T00:36:51.061357Z","iopub.execute_input":"2022-04-01T00:36:51.061559Z","iopub.status.idle":"2022-04-01T00:36:51.067783Z","shell.execute_reply.started":"2022-04-01T00:36:51.061534Z","shell.execute_reply":"2022-04-01T00:36:51.066758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# with open(\"../input/kore2022dict/dict.pkl\", mode=\"rb\") as f:\n#     dict_q_table = pickle.load(f)","metadata":{"execution":{"iopub.status.busy":"2022-04-01T00:36:51.069869Z","iopub.execute_input":"2022-04-01T00:36:51.07011Z","iopub.status.idle":"2022-04-01T00:36:51.080454Z","shell.execute_reply.started":"2022-04-01T00:36:51.070074Z","shell.execute_reply":"2022-04-01T00:36:51.079546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_agent = '''\nfrom kaggle_environments.envs.kore_fleets.helpers import *\nimport numpy as np\nimport itertools\nimport random\n# Helper\ndef pos_to_index(pos):\n    x = pos[0]\n    y = pos[1]\n    x = (x + 31) if x < 0 else x\n    x = (x - 31) if x > 30 else x\n    y = (y + 31) if y < 0 else y\n    y = (y - 31) if y > 30 else y\n\n    return x * 31 + y\n\n# Get straight score\ndef get_straight_score(pos, kore, length, direction):\n    score = 0\n    for i in range(length):\n        pos += direction.to_point()\n        score += kore[pos_to_index(pos)]\n    return score\n\ndef get_max_straight_plan(pos, kore, length):\n    scores = []\n    for i in range(4):\n        direction = Direction.from_index(i)\n        scores.append(get_straight_score(pos, kore, length, direction))\n    scores = sorted(scores, reverse=True)\n    direction_index = scores.index(scores[0])\n\n    direction = Direction.from_index(direction_index)\n    flight_plan = direction.to_char()\n    flight_plan += str(length)\n    flight_plan += direction.opposite().to_char()\n    return flight_plan\n\n\n# Get maximum cycle\ndef get_max_cycle_plan(pos, kore, length, direct_i):\n    direction = Direction.from_index(direct_i) \n    each_lengths = [i for i in range(1,length)]\n    each_lengths = [i for i in itertools.product(each_lengths, repeat=2)]\n    each_lengths = [i + tuple([i[0], i[1]]) for i in each_lengths]\n\n    # Get max score\n    scores = []\n    for rotate_i in range(2):\n        for each_length in each_lengths:\n            each_pos = pos\n            each_direction = direction\n            score = 0\n            for direct_i in range(4):\n                for i in range(each_length[direct_i]):\n                    each_pos += each_direction.to_point()\n                    score += kore[pos_to_index(each_pos)]\n                if rotate_i == 0:\n                    each_direction = each_direction.rotate_right()\n                else:\n                    each_direction = each_direction.rotate_left()\n            scores.append(score)\n\n    # Get flight plan\n    rotate_right = True\n    best_index = scores.index(max(scores))\n    if best_index >= len(each_lengths):\n        best_index -= len(each_lengths)\n        rotate_right = False\n\n    best_each_length = each_lengths[best_index]\n    each_direction = direction\n    flight_plan = \"\"\n    for i in range(4):\n        flight_plan += each_direction.to_char()\n\n        length = best_each_length[i] - 1\n        if i != 3 and length > 0:\n            flight_plan += str(length)\n        if rotate_right:\n            each_direction = each_direction.rotate_right()\n        else:\n            each_direction = each_direction.rotate_left()\n    return flight_plan\n\ndef get_closest_enemy_shipyard(board, position, me):\n    min_dist = 1000000\n    enemy_shipyard = None\n    for shipyard in board.shipyards.values():\n        if shipyard.player_id == me.id:\n            continue\n        dist = position.distance_to(shipyard.position, board.configuration.size)\n        if dist < min_dist:\n            min_dist = dist\n            enemy_shipyard = shipyard\n    return enemy_shipyard\n\n\ndef shipyard_action(action_str, pos, kore, board, me):\n    inst_list = action_str.split('_')\n    if inst_list[0] == 'spawn':\n        return ShipyardAction.spawn_ships(int(inst_list[1]))\n    \n    elif inst_list[0] == 'straight':\n        flight_plan = get_max_straight_plan(pos, kore, int(inst_list[1]))\n        return ShipyardAction.launch_fleet_with_flight_plan(int(inst_list[2]), flight_plan)\n    \n    elif inst_list[0] == 'cycle':\n        flight_plan = get_max_cycle_plan(pos, kore, int(inst_list[1]), int(inst_list[3]))\n        return ShipyardAction.launch_fleet_with_flight_plan(int(inst_list[2]), flight_plan)\n    \n    elif inst_list[0] == 'invade':\n        closest_enemy_shipyard = get_closest_enemy_shipyard(board, pos, me)\n        if not closest_enemy_shipyard:\n            return None\n        enemy_pos = closest_enemy_shipyard.position\n        my_pos = pos\n        flight_plan = \"N\" if enemy_pos.y > my_pos.y else \"S\"\n        flight_plan += str(abs(enemy_pos.y - my_pos.y) - 1)\n        flight_plan += \"W\" if enemy_pos.x < my_pos.x else \"E\"\n        if (abs(enemy_pos.y - my_pos.y) - 1) < 0:\n            return None\n        if not all([c in \"NESWC0123456789\" for c in flight_plan]):\n            return None\n\n        return ShipyardAction.launch_fleet_with_flight_plan(int(inst_list[1]), flight_plan)\n\n    elif inst_list[0] == 'build':\n        length = 6\n        flight_plan = get_max_straight_plan(pos, kore, length)\n        flight_plan = flight_plan[:-1]\n        direction = Direction.from_char(flight_plan[0])\n        direction = direction.rotate_right()\n        flight_plan += direction.to_char()\n        flight_plan += 'C'\n        return ShipyardAction.launch_fleet_with_flight_plan(int(inst_list[1]), flight_plan)\n    \n    \n\nnum_ship_list = [3,5,8,13,21,34,55,91]\n\nactions_list = [f'spawn_{i}' for i in range(1, 9)]\nactions_list += [f'straight_{i}_{s}' for i in [2, 5, 9] for s in num_ship_list]\nactions_list += [f'cycle_{i}_{s}_{d}' for i in [2, 5, 9] for s in num_ship_list[4:] for d in range(4) ]\nactions_list += [f'invade_{s}' for s in num_ship_list[4:]]\nactions_list += [f'build_{s}' for s in num_ship_list[6:]]\n\ndef agent(obs, config):\n    q_table = ''' \\\n    + str(dict_q_table).replace(' ', '') \\\n    + '''\n\n    board = Board(obs, config)\n    me=board.current_player\n    me_obser = me._observation\n\n    turn = board.step\n    kore_left = me.kore\n    \n    observation = board.observation\n    kore = observation['kore']\n\n    ship_number = 0\n    for value in me_obser[1].values():\n        ship_number += value[1]\n    fleet_number = 0\n    for value in me_obser[2].values():\n        fleet_number += value[2]\n    state_key = f'{int(ship_number)}_{int(np.log(fleet_number+1))}'\n\n    actions_str = []\n    # loop through all shipyards you control\n    for shipyard in me.shipyards:\n        possible_actions = actions_list.copy()\n        for action in actions_list:\n            inst_list = action.split('_')\n            if inst_list[0] == 'spawn':\n                if (int(inst_list[1]) * 10) > kore_left or int(inst_list[1]) > shipyard.max_spawn:\n                    possible_actions.remove(action)\n            elif inst_list[0] == 'straight' or inst_list[0] == 'cycle':\n                if int(inst_list[2]) > shipyard.ship_count:\n                    possible_actions.remove(action)\n            elif inst_list[0] == 'invade' or inst_list[0] == 'build':\n                if int(inst_list[1]) > shipyard.ship_count:\n                    possible_actions.remove(action)\n        if len(possible_actions) == 0:\n            continue\n        if state_key not in q_table.keys():\n            action_str = random.choice(possible_actions)\n            shipyard.next_action = shipyard_action(action_str, shipyard.position, kore, board, me)\n            continue\n\n        q_values = q_table[state_key]\n        # Determine if it is a possible action\n        selected_items = [q if actions_list[idx] in possible_actions else -100 for idx, q in enumerate(q_values)]\n        action_str = actions_list[int(np.argmax(selected_items))]\n        actions_str.append(action_str)\n        if action_str == None:\n            actions_str.append(None)\n            continue\n        shipyard.next_action = shipyard_action(action_str, shipyard.position, kore, board, me)\n    return me.next_actions\n    '''\n\nwith open('submission.py', 'w') as f:\n    f.write(my_agent)","metadata":{"execution":{"iopub.status.busy":"2022-04-01T00:36:51.081865Z","iopub.execute_input":"2022-04-01T00:36:51.082297Z","iopub.status.idle":"2022-04-01T00:36:51.101071Z","shell.execute_reply.started":"2022-04-01T00:36:51.082249Z","shell.execute_reply":"2022-04-01T00:36:51.100107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"env.run([\"./submission.py\", \"./pilot.py\", \"./pilot.py\", \"./pilot.py\"])\nenv.render(mode=\"ipython\", width=1000, height=800)","metadata":{"execution":{"iopub.status.busy":"2022-04-01T00:36:51.10291Z","iopub.execute_input":"2022-04-01T00:36:51.104044Z","iopub.status.idle":"2022-04-01T00:37:25.385197Z","shell.execute_reply.started":"2022-04-01T00:36:51.104007Z","shell.execute_reply":"2022-04-01T00:37:25.384191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I made a q-leraning starter notebook, but I can't train this agent well.\nI try 700 times in 11 hours by competing my best model which don't includes combat action. However, that agent's public score is 874.7.\n\n<br />\n\nThere are several hypothetical reasons why it did not work.\n1. The way the state is created is wrong.\n2. I need to train more times such as 10000.\n3. I need to let agent observe the board more using deep q-laning, etc.\n4. In the end, the rule-based approach is stronger.","metadata":{}}]}