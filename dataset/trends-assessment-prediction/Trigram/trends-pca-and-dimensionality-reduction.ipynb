{"cells":[{"metadata":{},"cell_type":"markdown","source":"# <center>TRENDS: PCA and Dimensionality Reduction</center>\n\n![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F1537731%2Fa5fdbe17ca91e6713d2880887232c81a%2FScreen%20Shot%202019-12-09%20at%2011.25.31%20AM.png?generation=1575920121028151&alt=media)\n\n---\n\n*Author: **Nikhil Praveen***"},{"metadata":{},"cell_type":"markdown","source":"## Table of contents\n\n* 1. Introduction to the data\n```\n    + Imports\n    + Data description\n```\n* 2. Overview of Methods\n* 3. Simple EDA\n   ```\n   + Starting the EDA\n   + Some fancy Plotly visuals\n   + Dimensionality reduction techniques\n       + PCA\n           + Normal PCA\n           + Incremental PCA\n           + Kernel PCA\n       + TruncatedSVD\n       + NMF\n       + Selection\n   + What to infer?\n   ```"},{"metadata":{},"cell_type":"markdown","source":"# 1. Introduction"},{"metadata":{},"cell_type":"markdown","source":"In this competition, we are asked to predict multiple assessments as well as age from many different features given in the data. Let's look at our data files:"},{"metadata":{},"cell_type":"markdown","source":"## Imports"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport plotly.express as px\nimport seaborn as sns\nimport plotly.io as pio\n\npio.templates.default = \"plotly_dark\"\n\nimport sklearn\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.model_selection import GroupKFold\nfrom typing import List, Tuple\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\nfig = make_subplots(rows=1, cols=2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"PATH = '../input/trends-assessment-prediction/'\nfnc = pd.read_csv(PATH+'fnc.csv')\nts = pd.read_csv(PATH+'train_scores.csv')\nloading = pd.read_csv(PATH+'loading.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data description"},{"metadata":{},"cell_type":"markdown","source":"From the official page, the descriptions we get for each of the data files are as follows:\n\n+ **fnc.csv** (corresponding to neuron networks)\n```\n    + SCN - Sub-cortical Network\n    + ADN - Auditory Network\n    + SMN - Sensorimotor Network\n    + VSN - Visual Network\n    + CON - Cognitive-control Network    \n    + DMN - Default-mode Network\n    + CBN - Cerebellar Network\n```\n+ **loading.csv** (parts of the brain)\n```\n    + IC_01 - Cerebellum\n    + IC_07 - Precuneus+PCC\n    + IC_05 - Calcarine\n    + IC_16 - Middle Occipital?\n    + IC_26 - Inf+Mid Frontal\n    + IC_06 - Calcarine\n    + IC_10 - MTG\n    + IC_09 - IPL+AG\n    + IC_18 - Cerebellum\n    + IC_12 - SMA\n    + IC_24 - IPL+Postcentral\n    + IC_15 - STG\n    + IC_13 - Temporal Pole\n    + IC_17 - Cerebellum\n    + IC_02 - ACC+mpfc\n    + IC_08 - Frontal\n    + IC_03 - Caudate\n    + IC_21 - Temporal Pole + Cerebellum\n    + IC_28 - Calcarine\n    + IC_11 - Frontal\n    + IC_20 - MCC\n    + IC_30 - Inf Frontal\n    + IC_22 - Insula + Caudate\n    + IC_29 - MTG\n    + IC_14 - Temporal Pole + Fusiform\n```"},{"metadata":{},"cell_type":"markdown","source":"# 2. Overview of methods\n\nIn this notebook, I will use the following methods:\n* **Dimensionality reduction**:\n    We have high dimensionality here, so I will try to apply: \n        * PCA\n        * TruncatedSVD\n        \n*NOTE: THE FOLLOWING WILL COME IN A LATER KERNEL.*\n* **Feature maker**<br>\n    I shall use sklearn's `BaseEstimator` class and add some dummy methods to make it work in the pipeline. Here's some sample code for the feature maker:\n    ```\n    class FeatureMaker(BaseEstimator, TransformerMixin):\n        def __init__(self, df):\n            '''And so on and so forth and what have'''\n        def fit(self):\n            pass\n        def transform():\n            # and so on and so forth\n    ```\n* **Imputer**<br>\n    After performing the feature engineering with our simple function, the Imputer from `sklearn` will be able to handle the many missing values.\n* **K-Fold**<br>\n    It is pretty obvious why one would use CV in a Kaggle competition.\n* **HistGradient Boosting Regressor**<br>\n    OK, this is an experimental `sklearn` regressor which I have decided to attempt in this competition."},{"metadata":{},"cell_type":"markdown","source":"# 3. Simple EDA"},{"metadata":{},"cell_type":"markdown","source":"## Starting the EDA"},{"metadata":{},"cell_type":"markdown","source":"First, we look at the dataset:"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"id1 = fnc[\"Id\"]\nfnc = fnc.drop([\"Id\"], axis=1)\nfnc.head(3)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"ts.head(3)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"loading.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fancy Plotly\n\nIf we want to look at building models, we will have to look at fancy Plotly instead of good old-fashioned Seaborn and Matplotlib.\n\nOur first 3d plot explores the relationships between `age` and the two `domain1_var` variables."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import plotly.graph_objects as go\n\nx, y, z = np.array(ts[\"age\"]), np.array(ts[\"domain1_var1\"]), np.array(ts[\"domain1_var2\"])\n\nfig = go.Figure(data=[go.Scatter3d(\n    x=x,\n    y=y,\n    z=z,\n    mode='markers',\n    marker=dict(\n        size=12,\n        color=z,                # set color to an array/list of desired values\n        colorscale='Viridis',   # choose a colorscale\n        opacity=0.8\n    )\n)])\n\n# tight layout\nfig.update_layout(margin=dict(l=0, r=0, b=0, t=0))\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's look at the domain2 variables:"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"x, y, z = np.array(ts[\"age\"]), np.array(ts[\"domain2_var1\"]), np.array(ts[\"domain2_var2\"])\n\nfig = go.Figure(data=[go.Scatter3d(\n    x=x,\n    y=y,\n    z=z,\n    mode='markers',\n    marker=dict(\n        size=12,\n        color=z,                # set color to an array/list of desired values\n        colorscale='Viridis',   # choose a colorscale\n        opacity=0.8\n    )\n)])\n\n# tight layout\nfig.update_layout(margin=dict(l=0, r=0, b=0, t=0))\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What about the relation between domain1 and domain2 variables? That would be interesting."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from plotly.subplots import make_subplots\n\nx, y, z = np.array(ts[\"domain1_var1\"]), np.array(ts[\"domain2_var1\"]), np.array(ts[\"domain2_var2\"])\nx2, y2, z2 = np.array(ts[\"domain1_var2\"]), np.array(ts[\"domain2_var1\"]), np.array(ts[\"domain2_var2\"])\n\nfig = make_subplots(rows=1, cols=1)\n\nfig.add_trace(go.Scatter3d(\n    x=x,\n    y=y,\n    z=z,\n    mode='markers',\n    marker=dict(\n        size=12,\n        color=z,                # set color to an array/list of desired values\n        colorscale='Viridis',   # choose a colorscale\n        opacity=0.8\n    )\n))\n\n# tight layout\nfig.update_layout(margin=dict(l=0, r=0, b=0, t=0))\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = make_subplots(rows=1, cols=1)\n\nfig.add_trace(go.Scatter3d(\n    x=x2,\n    y=y2,\n    z=z2,\n    mode='markers',\n    marker=dict(\n        size=12,\n        color=z,                # set color to an array/list of desired values\n        colorscale='Viridis',   # choose a colorscale\n        opacity=0.8\n    ),\n))\n\n\n# tight layout\nfig.update_layout(margin=dict(l=0, r=0, b=0, t=0))\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dimensionality reduction\n\nThe main one we have to reduce is the data from `fnc.csv` as well as the data from `loading.csv`. For this purpose, we shall try PCA and TruncatedSVD on this dataset and pick which one is better."},{"metadata":{},"cell_type":"markdown","source":"### PCA"},{"metadata":{},"cell_type":"markdown","source":"#### 1. Normal PCA"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\npca = PCA(n_components=2)\npca.fit(fnc)\ntransformed_pca = pca.fit_transform(fnc)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"pd.DataFrame(transformed_pca)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"One one hand, there is a lot of variance between components 0 and 1. What's the explained variance ratio?"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"pca.explained_variance_ratio_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, not off to a great start. Let's look at the other PCA methods."},{"metadata":{},"cell_type":"markdown","source":"#### 2. Incremental PCA"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import IncrementalPCA\npca = IncrementalPCA(n_components=5)\npca.fit(fnc)\ntransformed_pca = pca.fit_transform(fnc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(transformed_pca)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is a very minimal difference between our first attempt at PCA and this one. Honestly, I didn't expect the difference to be too much between these two."},{"metadata":{"trusted":true},"cell_type":"code","source":"pca.explained_variance_ratio_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is virtually **no variance** between these PCA and IncrementalPCA. Looks like it's time to move on to our good old friend TruncatedSVD."},{"metadata":{},"cell_type":"markdown","source":"### Truncated SVD"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import TruncatedSVD\nsvd = TruncatedSVD(n_components=5, n_iter=7, random_state=42)\nsvd.fit(fnc)\nx = pd.DataFrame(svd.fit_transform(fnc))\nx","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that the first component is carrying most of the weight in both PCA and SVD. This leads to the absence of a well-balanced dataset. Let's check for skew:"},{"metadata":{"trusted":true},"cell_type":"code","source":"skewValue = x.skew(axis=1)\nskewValue","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems like Incremental PCA is the best choice here for dimensionality reduction. Let's perform PCA on the full dataset:"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import IncrementalPCA\npca = IncrementalPCA(n_components=5)\npca.fit(fnc)\ntransformed_pca = pca.fit_transform(fnc)\npd.DataFrame(transformed_pca).to_csv('PCAData.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n\n# Please upvote if you liked this kernel!\n\n---"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}