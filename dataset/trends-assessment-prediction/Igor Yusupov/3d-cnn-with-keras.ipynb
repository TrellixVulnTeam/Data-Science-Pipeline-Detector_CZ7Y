{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport pandas as pd\nimport os\nimport math\nimport h5py\nimport random\nimport nilearn as nl\nfrom nilearn import image, datasets, plotting\nfrom nilearn.image import get_data\nfrom random import randint\nfrom tqdm.notebook import tqdm\nfrom sklearn.impute import KNNImputer\nfrom tqdm.notebook import tqdm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"source = \"../input/trends-assessment-prediction\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img_train = []\nfor name in os.listdir(source+'/fMRI_train'):\n    img_train.append(int(name[:5]))\nimg_train.sort()\ntrain = {}\nfor i, name in enumerate(img_train):\n    train[i] = name","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"impute = KNNImputer(n_neighbors=40)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = pd.read_csv(\"/kaggle/input/trends-assessment-prediction/train_scores.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = impute.fit_transform(y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = pd.DataFrame(y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = {}\nfor i in range(len(img_train)):\n    y[img_train[i]] = y_train[y_train[0] == img_train[i]].drop([0], axis=1).to_numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_batch(k, size):\n    image = img_train[k:k+size]\n    batch = np.zeros((size,53, 52, 63, 53))\n    y_train = np.zeros((size,5))\n    for i in range(len(image)):\n        batch[i] = h5py.File(source+f'/fMRI_train/{image[i]}.mat', 'r')['SM_feature'][()]\n        y_train[i] = y[image[i]]\n    \n    return batch, y_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv3D, Dense, Dropout, Flatten, BatchNormalization, PReLU, Reshape , MaxPooling3D,MaxPool3D\nimport tensorflow.keras.backend as K","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def weighted_NAE(yTrue,yPred):\n    weights = K.constant([.3, .175, .175, .175, .175])\n\n    return K.sum(weights*K.sum(K.abs(yTrue-yPred))/K.sum(yPred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Conv3D(200, (3,3,3), input_shape = (53, 52, 63, 53), activation='relu', data_format='channels_first'))\nmodel.add(MaxPool3D((2,2,2)))\nmodel.add(Dropout(0.2))\nmodel.add(Conv3D(200, (3,3,3), activation='relu', data_format='channels_first'))\nmodel.add(MaxPool3D((2,2,2)))\nmodel.add(BatchNormalization()) \n\nmodel.add(Conv3D(200, (3,3,3), activation ='relu', data_format='channels_first'))\nmodel.add(MaxPool3D((2,2,2)))\nmodel.add(Dropout(0.2))\nmodel.add(Conv3D(200, (3,3,3), activation='relu', data_format='channels_first'))\nmodel.add(MaxPool3D((2,2,2)))\n\nmodel.add(Flatten())\nmodel.add(BatchNormalization()) \nmodel.add(Dense(8192, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(4096, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(2048, activation='relu'))\nmodel.add(BatchNormalization()) \nmodel.add(Dropout(0.2))\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(5, activation='relu'))\nmodel.compile(loss='mse',\n              optimizer='adam',\n              metrics='mse')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 10\nnb_batchs = 10\n\nfor e in range(150):\n    print(f'epoch {e}/150')\n    loss = 0.\n    acc = 0.\n    for batch in range(nb_batchs):\n        train_x, train_y = get_batch(batch, batch_size)\n        loss_batch, acc_batch = model.train_on_batch(train_x, train_y)\n        loss += loss_batch\n        acc += acc_batch\n    print(f'Loss: {loss / nb_batchs}')\n    print(f'Acc: {acc / nb_batchs}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('model.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img_test = []\nfor name in os.listdir(source+'/fMRI_test'):\n    img_test.append(int(name[:5]))\nimg_test.sort()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t = h5py.File(source+f'/fMRI_test/{img_test[0]}.mat', 'r')['SM_feature'][()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.predict(np.array([t])).flatten()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_model = np.array([])\nfor img in tqdm(img_test):\n    t = h5py.File(source+f'/fMRI_test/{img}.mat', 'r')['SM_feature'][()]\n    if len(pred_model) == 0:\n        pred_model = model.predict(np.array([t])).flatten()\n    else:\n        pred_model = np.append(pred_model, model.predict(np.array([t])).flatten())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission = pd.read_csv(\"/kaggle/input/trends-assessment-prediction/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred=pd.DataFrame()\npred[\"Id\"]=sample_submission.Id\npred[\"Predicted\"]=pred_model\npred.to_csv('out.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Due to limited computing power, network training was only in 20 pictures. I think that if you train model on the entire dataset, then the quality will be high. Good luck","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}