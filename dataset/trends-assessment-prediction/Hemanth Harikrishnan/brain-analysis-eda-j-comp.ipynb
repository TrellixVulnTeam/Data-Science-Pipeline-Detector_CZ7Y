{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pynrrd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nimport os\nfrom random import randint\n\nimport numpy as np\nimport pandas as pd\n\nimport nibabel as nib\nimport pydicom as pdm\nimport nilearn as nl\nimport nilearn.plotting as nlplt\nimport nrrd\nimport h5py\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nimport matplotlib.animation as anim\n\nimport imageio\nfrom skimage.transform import resize\nfrom skimage.util import montage\n\nfrom IPython.display import Image as show_gif\n\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\nfrom glob import glob\nfrom os.path import join as opj\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\nsns.set_context('notebook')\n\nfrom tqdm.notebook import tqdm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_filename = '../input/trends-assessment-prediction/fMRI_train/10001.mat'\nmatlab_file = h5py.File(sample_filename)\nprint(matlab_file.keys())\nprint(matlab_file.values())\nprint(matlab_file['SM_feature'][()].shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading reference image\nfmri_mask = nl.image.load_img('../input/trends-assessment-prediction/fMRI_mask.nii')\n\n# Reorienting the axis of 3D spatial map\nspatial_maps = np.moveaxis(matlab_file['SM_feature'][()], [0, 1, 2, 3], [3, 2, 1, 0]) \n\n# Loading 3D spatial maps\nspatial_maps_niimg = nl.image.new_img_like(ref_niimg=fmri_mask,\n                                           data=spatial_maps,\n                                           affine=fmri_mask.affine,\n                                           copy_header=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img = img = list(nl.image.iter_img(spatial_maps_niimg))[0]\nview = nlplt.view_img_on_surf(img,\n                              title=f'10009.mat Spatial Map 0 view_img_on_surf',\n                              title_fontsize=20,\n                              threshold=1,\n                              black_bg=False)\nview.open_in_browser()\nview","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = '/kaggle/input/trends-assessment-prediction/'\n#targets = pd.read_csv(opj(path, 'train_scores.csv')).set_index('Id')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the target features\ntargets = pd.read_csv('/kaggle/input/trends-assessment-prediction/train_scores.csv').set_index('Id')\ntargets.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# How many null values does each target have\nn_nulls = targets.isnull().sum()\ndisplay(n_nulls)\nn_nulls.plot.barh();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observation 1\nSo the first hunch is correct. Domain 1 and domain 2 seem to be connected, as they both contain the same amount of missing values.","metadata":{}},{"cell_type":"code","source":"targets.dropna(inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(targets.corr()*100, square=True, annot=True, fmt='.0f');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observation 2\nWhile there is a slight correlation within domain 2, there doesn't seem to be one within domain 1. So, the targets were measured in the same domain, but do not necessarily encode a connected property. Except for this domain2 connection","metadata":{}},{"cell_type":"code","source":"targets.plot(lw=0, marker='.', markersize=1, subplots=True, figsize=(14, 8));","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Just looking at the targets, most seems to be normal. Except for age, something is unique.So let us have a closer look.","metadata":{}},{"cell_type":"code","source":"targets['age'].plot(lw=0, marker='.', markersize=1, figsize=(14, 4));","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"targets['age'].nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#distribution of age over target\nplt.plot(targets['age'].sort_values().values);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If we try to predict this feature, it might be worth it to restrict the predictions to these 33 unique values.","metadata":{}},{"cell_type":"code","source":"sns.pairplot(targets, plot_kws=dict(s=5, alpha=0.5));","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see again the stratification of age, but what is more interesting is the relationship within domain2","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(6, 6))\nd2 = targets.dropna().iloc[:, 3:].values\nplt.scatter(d2[:, 0], d2[:, 1], s=3);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Function to rotate a point around the origin (0, 0).","metadata":{}},{"cell_type":"code","source":"def rotate_origin(x, y, radians):\n    xx = x * np.cos(radians) + y * np.sin(radians)\n    yy = -x * np.sin(radians) + y * np.cos(radians)\n    return np.array([xx, yy]).T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### To get the best rotation, we have to plot the length of unique values that can be notices in variable 2 with every rotation and take the angle at which the number of unique values are minimum.","metadata":{}},{"cell_type":"code","source":"# Function to plot unique values in a given range\n\ndef plot_unique(start,end,d2,noOfVals):\n    n_uniques = []\n    for r in np.linspace(start, end, noOfVals):\n        d22_rot = rotate_origin(d2[:, 0], d2[:, 1], r)[:, 1]\n        n_uniques.append([r, len(np.unique(np.round(d22_rot, 6)))])\n    n_uniques = np.array(n_uniques)\n\n    plt.figure(figsize=(14, 2))\n    plt.scatter(n_uniques[:, 0], n_uniques[:, 1], s=3);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Trying for values from 0.85 to 0.95 radians\nplot_unique(0.85,0.95,d2,5000)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# As we can see sudden drop, trying for values from 0.905 to 0.910 radians\nplot_unique(0.905,0.910,d2,5000)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Trying for values from 0.90771 to 0.907715 radians\nplot_unique(0.90771,0.907715,d2,5000)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So the best value to rotate is taken as 0.90771256655","metadata":{}},{"cell_type":"code","source":"# rgets = pd.read_csv(opj(path, 'train_scores.csv')).set_index('Id')\n# Let's also create the rotated domain2 targets\nrot = 0.90771256655\nd2 = rotate_origin(targets.iloc[:, 3].values, targets.iloc[:, 4].values, rot)\ntargets['d21_rot'] = d2[:, 0]\ntargets['d22_rot'] = d2[:, 1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Plotting Distribution of Target variables","metadata":{}},{"cell_type":"code","source":"from scipy.stats import norm\nfor col in targets.columns:\n    plt.figure(figsize=(8, 2))\n    sns.distplot(targets[col], fit=norm, kde=True)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Age is almost normally distributed but for the other targets power transformations can be applied.","metadata":{}},{"cell_type":"code","source":"# Let's apply the power transformation to make the value distribution gaussian\npow_age = 1.0\npow_d1v1 = 1.5\npow_d1v2 = 1.5\npow_d2v1 = 1.5\npow_d2v2 = 1.5\npow_d21 = 1.5\npow_d22 = 1\npowers = [pow_age, pow_d1v1, pow_d1v2, pow_d2v1, pow_d2v2, pow_d21, pow_d22 ]\n\nfrom scipy.stats import norm\nfor i, col in enumerate(targets.columns):\n    plt.figure(figsize=(8, 2))\n    sns.distplot(np.power(targets[col], powers[i]), fit=norm, kde=True)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i, col in enumerate(targets.columns):\n    targets[col] = np.power(targets[col], powers[i])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Scaling","metadata":{}},{"cell_type":"markdown","source":"Loading the targets and doing necessary rotation and power transformations as done before without excluding the NULL values.","metadata":{}},{"cell_type":"code","source":"targets = pd.read_csv(opj(path, 'train_scores.csv')).set_index('Id')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating the rotated domain2 targets\nrot = 0.90771256655\nd2 = rotate_origin(targets.iloc[:, 3].values, targets.iloc[:, 4].values, rot)\ntargets['d21_rot'] = d2[:, 0]\ntargets['d22_rot'] = d2[:, 1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pow_age = 1.0\npow_d1v1 = 1.5\npow_d1v2 = 1.5\npow_d2v1 = 1.5\npow_d2v2 = 1.5\npow_d21 = 1.5\npow_d22 = 1\npowers = [pow_age, pow_d1v1, pow_d1v2, pow_d2v1, pow_d2v2, pow_d21, pow_d22 ]\n\nfor i, col in enumerate(targets.columns):\n    targets[col] = np.power(targets[col], powers[i])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\n# And last but not least, let's scale the target features using ab\nscaler = StandardScaler()\ntargets.iloc[:, :] = scaler.fit_transform(targets)\ntargets.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract ID to separate train and test set\ntrain_id = targets.index.values\nsample_submission = pd.read_csv(opj(path, 'sample_submission.csv'))\ntest_id = np.unique(sample_submission.Id.str.split('_', expand=True)[0].astype('int'))\nprint(train_id.shape, test_id.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load ICs from the loading file and separate them into train and test set\ndf_ic = pd.read_csv(opj(path, 'loading.csv'))\nic_train = df_ic[df_ic.Id.isin(train_id)].set_index('Id')\nic_test = df_ic[df_ic.Id.isin(test_id)].set_index('Id')\nprint(ic_train.shape, ic_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load FNCs from file and separate them into train and test set\ndf_fnc = pd.read_csv(opj(path, 'fnc.csv'))\nfnc_train = df_fnc[df_fnc.Id.isin(train_id)].set_index('Id')\nfnc_test = df_fnc[df_fnc.Id.isin(test_id)].set_index('Id')\nprint(fnc_train.shape, fnc_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature Exploration","metadata":{}},{"cell_type":"code","source":"def plot_corr_matrix(df_train, df_test, c_restrict=200):\n\n    # Correlation matrix for ICA components\n    fig, ax = plt.subplots(ncols=3, figsize=(20, 10))\n    abs_max = 1.0\n    sns.heatmap(df_train.iloc[:, :c_restrict].corr(), square=True, vmin=-abs_max, vmax=abs_max, cbar=False, ax=ax[0]);\n    sns.heatmap(df_test.iloc[:, :c_restrict].corr(), square=True, vmin=-abs_max, vmax=abs_max, cbar=False, ax=ax[1]);\n    sns.heatmap(df_train.iloc[:, :c_restrict].corr()-df_test.iloc[:, :c_restrict].corr(),\n                square=True, vmin=-0.33, vmax=0.33, cbar=False, ax=ax[2]);\n    ax[0].set_title('Train')\n    ax[1].set_title('Test')\n    ax[2].set_title('Difference (Train - Test)');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Correlation matrix for IC features\nplot_corr_matrix(ic_train, ic_test, c_restrict=100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Correlation matrix for FNC features\nplot_corr_matrix(fnc_train, fnc_test, c_restrict=100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_corr_matrix_target(targets, df_train, c_restrict=100):\n\n    # Merge target and feature matrix\n    df_temp = pd.merge(targets.reset_index(), df_train.reset_index())\n    df_temp = df_temp.set_index('Id').iloc[:, :c_restrict]\n    \n    # Correlation matrix for ICA components\n    plt.figure(figsize=(16, 3))\n    sns.heatmap(df_temp.corr().iloc[:7, 7:], square=True,\n                vmin=-0.5, vmax=0.5, cbar=False, cmap='Spectral');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Correlation between IC features and targets\nplot_corr_matrix_target(targets, ic_train, c_restrict=100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Correlation between FNC features and targets\nplot_corr_matrix_target(targets, fnc_train, c_restrict=100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observation 1\nFor both feature types, the correlation with age seems to be the highest. Let's explore this in a bit more detail. What is the highest correlation features can reach with the 5 targets?","metadata":{}},{"cell_type":"code","source":"# Show highest correlation with target variables and IC dataset\ndf_corr = pd.concat([np.abs(ic_train.corrwith(targets.iloc[:, i])).sort_values(ascending=False).reset_index(drop=True) for i in range(7)], axis=1)\ndf_corr.columns = targets.columns\ndf_corr.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show highest correlation with target variables and FNC dataset\ndf_corr = pd.concat([np.abs(fnc_train.corrwith(targets.iloc[:, i])).sort_values(ascending=False).reset_index(drop=True) for i in range(7)], axis=1)\ndf_corr.columns = targets.columns\ndf_corr.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Explore between features and targets\n# Number of columns to investigate\nn_invest = 10\nsns.pairplot(ic_train.iloc[:, :n_invest], diag_kind=\"kde\", corner=True);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.pairplot(fnc_train.iloc[:, :n_invest], diag_kind=\"kde\", corner=True);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_markers(key, df_temp, ncolmarker=5, split_at=5, plot_max=15):\n\n    # Restrict dataframe to first X features\n    df_temp = df_temp.iloc[:, :plot_max]\n\n    # Compute dataset selecters\n    ncolumns = np.arange(df_temp.shape[1])\n    selecter = np.split(ncolumns, ncolumns[::split_at][1:])\n\n    for s in selecter:\n\n        print(key, s)\n        df_temp.iloc[:, s].plot(kind='line',subplots=True, sharex=True, marker='.', lw=0,\n                                ms=10, markeredgecolor='k', markeredgewidth=0.3,\n                     figsize=(5 * ncolmarker, 4 * df_temp.iloc[:, s].shape[1]//ncolmarker), layout=(-1,ncolmarker));\n        plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_markers('Visualization of IC features:', ic_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Conclusion 1\nThe datasets seem to contain a few outliers. We will take care of them at the very end.","metadata":{}},{"cell_type":"markdown","source":"## Prediction","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\n!pip install pycaret --quiet","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import KFold\n\nfrom pycaret.regression import *","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Loading datasets and making train and test datasets","metadata":{}},{"cell_type":"code","source":"BASE_PATH = '../input/trends-assessment-prediction'\n\nfnc_df = pd.read_csv(f\"{BASE_PATH}/fnc.csv\")\nloading_df = pd.read_csv(f\"{BASE_PATH}/loading.csv\")\nlabels_df = pd.read_csv(f\"{BASE_PATH}/train_scores.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fnc_features, loading_features = list(fnc_df.columns[1:]), list(loading_df.columns[1:])\ndf = fnc_df.merge(loading_df, on=\"Id\")\nlabels_df[\"is_train\"] = True\ndf = df.merge(labels_df, on=\"Id\", how=\"left\")\n\ntest_df = df[df[\"is_train\"] != True].copy()\ndf = df[df[\"is_train\"] == True].copy()\nprint(f'Shape of train data: {df.shape}, Shape of test data: {test_df.shape}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_cols = ['age', 'domain1_var1', 'domain1_var2', 'domain2_var1', 'domain2_var2']\ndf.drop(['is_train'], axis=1, inplace=True)\ntest_df = test_df.drop(target_cols + ['is_train'], axis=1)\n\n\n# Giving less importance to FNC features since they are easier to overfit due to high dimensionality.\nFNC_SCALE = 1/500\ndf[fnc_features] *= FNC_SCALE\ntest_df[fnc_features] *= FNC_SCALE","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As pycaret supports only a single column prediction at a time, a utility function is defined to get values of a given col.","metadata":{}},{"cell_type":"code","source":"def get_train_data(target):\n    other_targets = [tar for tar in target_cols if tar != target]\n    train_df = df.drop( other_targets, axis=1)\n    return train_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Not using a few types of regression as it takes a lot of time","metadata":{}},{"cell_type":"code","source":"blacklist_models = ['ransac', 'tr', 'rf', 'et', 'ada', 'gbr', 'xgboost', 'catboost']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Firstly taking the Age column for prediction","metadata":{}},{"cell_type":"code","source":"target = target_cols[0]\ntrain_df = get_train_data(target)\n\nsetup_reg = setup(\n    data = train_df,\n    target = target,\n    train_size=0.8,\n    numeric_imputation = 'mean',\n    silent = True\n)\n\ncompare_models(\n    exclude = blacklist_models,\n    fold = 7,\n    sort = 'MAE',\n    turbo = True\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Creating a Bayesian Ridge model","metadata":{}},{"cell_type":"code","source":"br_age = create_model(\n    estimator='br',\n    fold=7\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Tuning the Bayesian Ridge model to optimize on MAE (metric for the competition)","metadata":{}},{"cell_type":"code","source":"tuned_br_age = tune_model(\n    br_age,\n    optimize = 'MAE'\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Plotting the Learning Curve","metadata":{}},{"cell_type":"code","source":"plot_model(tuned_br_age,plot = 'learning')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Plotting the Residuals","metadata":{}},{"cell_type":"code","source":"plot_model(tuned_br_age, plot = 'residuals')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Plotting the feature importance","metadata":{}},{"cell_type":"code","source":"plot_model(tuned_br_age,plot = 'feature')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Prediction on age for test dataset","metadata":{}},{"cell_type":"code","source":"predictions =  predict_model(tuned_br_age, data=test_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions[['Id','Label']].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Predicting the other targets:","metadata":{}},{"cell_type":"markdown","source":"#### domain1_var1","metadata":{}},{"cell_type":"code","source":"target = target_cols[1]\ntrain_df = get_train_data(target)\n\nsetup_reg = setup(\n    data = train_df,\n    target = target,\n    train_size=0.8,\n    numeric_imputation = 'mean',\n    silent = True\n)\n\ncompare_models(\n    exclude = blacklist_models,\n    fold = 7,\n    sort = 'MAE',\n    turbo = True\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### domain1_var2","metadata":{}},{"cell_type":"code","source":"target = target_cols[2]\ntrain_df = get_train_data(target)\n\nsetup_reg = setup(\n    data = train_df,\n    target = target,\n    train_size=0.8,\n    numeric_imputation = 'mean',\n    silent = True\n)\n\ncompare_models(\n    exclude = blacklist_models,\n    fold = 7,\n    sort = 'MAE',\n    turbo = True\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### domain2_var1","metadata":{}},{"cell_type":"code","source":"target = target_cols[3]\ntrain_df = get_train_data(target)\n\nsetup_reg = setup(\n    data = train_df,\n    target = target,\n    train_size=0.8,\n    numeric_imputation = 'mean',\n    silent = True\n)\n\ncompare_models(\n    exclude = blacklist_models,\n    fold = 7,\n    sort = 'MAE',\n    turbo = True\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### domain2_var2","metadata":{}},{"cell_type":"code","source":"target = target_cols[4]\ntrain_df = get_train_data(target)\n\nsetup_reg = setup(\n    data = train_df,\n    target = target,\n    train_size=0.8,\n    numeric_imputation = 'mean',\n    silent = True\n)\n\ncompare_models(\n    exclude = blacklist_models,\n    fold = 7,\n    sort = 'MAE',\n    turbo = True\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**OBSERVATIONS:**\n1. age          - Bayesian Ridge\n2. domain1_var1 - Linear Regression\n3. domain1_var2 - Lasso Least Angle Regression\n4. domain2_var1 - Linear Regression\n5. domain2_var2 - Linear Regression\n","metadata":{}},{"cell_type":"markdown","source":"#### Function to tune and ensemble (Bagging) best model for each target:","metadata":{}},{"cell_type":"code","source":"models = []\n\ntarget_models_dict = {\n    'age': 'br',\n    'domain1_var1':'lr',\n    'domain1_var2':'llar',\n    'domain2_var1':'lr',\n    'domain2_var2':'lr',\n}\n\ndef tune_and_ensemble(target):\n    train_df = get_train_data(target)    \n    exp_reg = setup(\n        data = train_df,\n        target = target,\n        train_size=0.8,\n        numeric_imputation = 'mean',\n        silent = True\n    )\n    model_name = target_models_dict[target]\n    mod = create_model(model_name,fold=7)\n    tuned_model = tune_model(mod, fold=7, optimize = 'MAE')\n    model = ensemble_model(tuned_model, fold=7, optimize = 'MAE', choose_better = True)\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for target in target_cols:\n    model = tune_and_ensemble(target)\n    models.append(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Function to finalize and save model","metadata":{}},{"cell_type":"code","source":"models","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def finalize_model_pipeline(model, target):\n    # this will train the model on houldout data\n    finalize_model(model)\n    save_model(model, f'{target}_{target_models_dict[target]}', verbose=True)\n    # making predictions on test data\n    predictions = predict_model(model, data=test_df)\n    test_df[target] = predictions['Label'].values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for index, target in enumerate(target_cols):\n    model = models[index]\n    finalize_model_pipeline(model,target)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Creating submission csv file","metadata":{}},{"cell_type":"code","source":"sub_df = pd.melt(test_df[[\"Id\", \"age\", \"domain1_var1\", \"domain1_var2\", \"domain2_var1\", \"domain2_var2\"]], id_vars=[\"Id\"], value_name=\"Predicted\")\nsub_df[\"Id\"] = sub_df[\"Id\"].astype(\"str\") + \"_\" +  sub_df[\"variable\"].astype(\"str\")\n\nsub_df = sub_df.drop(\"variable\", axis=1).sort_values(\"Id\")\nassert sub_df.shape[0] == test_df.shape[0]*5\n\nsub_df.to_csv(\"submission1.csv\", index=False)\n\nsub_df.head(15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models[0].get_params()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"age_train_df = get_train_data('age')\npreds = predict_model(models[0], data=age_train_df)\npreds\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.mean(np.sum(np.abs(preds['age'] - preds['Label']), axis=0)/np.sum(preds['age'], axis=0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = []\nop = 'Label'\noverall = 0.0\nfor i in range(5):\n    train_df = get_train_data(target_cols[i])\n    preds = predict_model(models[i], data=train_df)\n    predictions.append(preds)\n    score = np.mean(np.sum(np.abs(preds[target_cols[i]] - preds[op]), axis=0)/np.sum(preds[target_cols[i]], axis=0))\n    overall+=score\n    print(f'{target_cols[i]}: \\t{score}')\nprint(f\"Overall score = {overall/5}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}