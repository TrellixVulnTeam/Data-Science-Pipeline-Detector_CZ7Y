{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom nilearn import plotting, image \nimport nibabel as nb\nimport h5py\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm.auto import tqdm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# First Look at the Data\n\nHi there, \nI am currently finishing my PhD in cognitive neuroscience, so this is a nice distraction next to the writing process, so a little bit of domain knowledge might be here :) \n\nOn the other hand, age prediction, resting state analysis, and structural imaging are not my area of expertise - so I don't know much about the different ICA algorithms used here. \n\nI will try to provide you with some high-level overview over the different kinds of data. And will update this notebook, when I have the time and get around to read up on some of the literature. \n\nWe got many different files here, which are of course described on the Web-Page but let's see what we can learn by looking at it. \nAlso if you don't know too much nilearn yet, it's a great toolset and maybe you find some useful functions here!\n\n(final version so far, still missing links to more and better information - which I still might add later)\n\nAlso please forgive me some weird sentences and typos :) "},{"metadata":{},"cell_type":"markdown","source":"# The brain mask\n> fMRI_mask.nii - a 3D binary spatial map\n\nWell, that one's is the most obvious: \nIf you want to know what part's of the nifti are inside the brain - especially across many many participants - so here is the brain mask. \nFor this often differen algorithms are applied in pre-processing. For example, removing part's of the skull etc. But also a lot of realignment in the scanner. We don't have too many information right now what the dimensions are, but just as a note here: Functional connectivity is often prone to a lot of movement artifacts."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Nifti MASK image\nbrain_mask = nb.load('../input/trends-assessment-prediction/fMRI_mask.nii')\nplotting.plot_roi(brain_mask, title='fMRI_mask.nii');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# fnc.csv\n> static FNC correlation features for both train and test samples \n\nFirst up, for people who do not know a lot about brain imaging studies.\n#### What is resting state\nIn resting state measures participants lie in the MRI scanner with their eyes opened or closed (there is/was a huge debate on what is best way...) and are told to do nothing in particular. Sometimes with the addition not to think about anything specific or meditate. This usually goes on for several minutes - it can be really hard not to fall asleep - trust me. While participants lie in the scanner, typically the [**B**lood **O**xygen **L**evel **D**ependent](https://en.wikipedia.org/wiki/Blood-oxygen-level-dependent_imaging) signal is measured, which serves as a proxy of brain activity. \n\nThis the \"task\" sounds really error-prone and unspecific, this kind of measuring has been the key for many insights in human neuroscience and the results are surprisingly stable. \n\nIn resting state you have 4D data, a 3D image (made up of voxels) measured over time. \n\n### What has been done? \n\n> The second set are static functional network connectivity (FNC) matrices. These are the subject-level cross-correlation values among 53 component timecourses estimated from GIG-ICA of resting state functional MRI (fMRI).\n\nThis is different - but not uncommon - to typical resting state function connectivity. In classical **F**unctional **C**onnectivity you would use a brain atlas, and average all voxels in a brain region. Then you calculate the correlation (or other measures) between the different brain regions and get a brain connectivity matrix.\n\nHere an example of an atlas (well one you wouldn't typically use, but it serves the purpose): "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from nilearn import datasets\naal = datasets.fetch_atlas_aal();\n# This is just supposed to be an example - so it's not too important\ntry:\n    plotting.plot_roi(aal['maps'], title='Example of a Brain Atlas (aal)');\nexcept:\n    print(\"Probably time out\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this challenge, however, the approach is a bit different. Using ICA, different spatial maps have been create to extract brain networks which are typically present in the resting brain. \nAs described - the maps were create on a different dataset to avoid information leakage. For each component the time-series has been correlated with the time-series of a different component. So we get (53 * 53 - 53) / 2 = 1378 featues. "},{"metadata":{"trusted":true},"cell_type":"code","source":"fnc_10 = next(pd.read_csv('../input/trends-assessment-prediction/fnc.csv', low_memory=True, chunksize=5))\nfnc_10.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For every feature we get a description of the connections (i.e. which component with which component).\n\nLet's extract the different names. "},{"metadata":{"trusted":true},"cell_type":"code","source":"fnc10_cols = fnc_10.columns.to_list()[1:]\nfnc10_cols_filtered = [i.split('_')[0] for i in fnc10_cols]\nprint(np.unique(fnc10_cols_filtered))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ICN_numbers.csv"},{"metadata":{},"cell_type":"markdown","source":"We see there are a number of different names with numbers.\nUsing the file `ICN_numbers.csv`:\n> ICN_numbers.txt - intrinsic connectivity network numbers for each fMRI spatial map; matches FNC names  \n\nWe could now assign the different names to indices in the matrices (for example in `fMRI_train`) \n\nBut what do the names mean. Shooting from the hip we have, and that's it for me. As I said, I am not an expert and those abbreviations are a bit ambigous for me:\n1. DMN - Default Mode Network (the idle, non-task network)\nBased on the plots below we can try to name them:\n2. SMN - somatosensory - motor - network\n3. VSN - visual network\n4. CON - still not really sure\n5. ADN - auditory\n6. CBN - something in the cerebelleum\n7. SCN - something sub-cortical "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's extract the indices for the different networks\n# Network index:\nntwk_idx = {}\nnetwork_names = np.unique([i[:3] for i in fnc10_cols_filtered])\nfor ii in network_names:\n    ntwk_idx[ii] = np.unique([np.int(i.split('(')[-1].split(')')[0]) for i in fnc10_cols_filtered if ii in i])\n    \n# Look up matrix index\nicn_number = pd.read_csv('../input/trends-assessment-prediction/ICN_numbers.csv')\n\nicn_idx = {}\n\nfor jj in ntwk_idx.keys():\n    icn_idx[jj] = np.array(icn_number.index[icn_number.ICN_number.isin(ntwk_idx[jj])])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# SM_features\n> fMRI_train - a folder containing 53 3D spatial maps for train samples in .mat format  \n\nWe are doing a few leaps and jumps here, but all these aspects of the data are connected. \n\nIf I understand the description correctly this data is again something different from the functional network connectivity above. \n\nWhile the networks in `fnc.csv` has been estimated on a different dataset, the ICA maps in the different mat files seem to be estimated from resting state of each participant\n\n> The third set of features are the component spatial maps (SM). These are the subject-level 3D images of 53 spatial networks estimated from GIG-ICA of resting state functional MRI (fMRI).\n\nLet's load some data, and maybe we can fill the gaps in the numerated list above."},{"metadata":{"trusted":true},"cell_type":"code","source":"# We load the data using h5py\ntest_mat1 = h5py.File('../input/trends-assessment-prediction/fMRI_test/11000.mat', mode='r')\nprint(test_mat1.keys())\ntest_mat1 = np.array(test_mat1.get('SM_feature'))\nprint('Dimensions of ICA feature map')\nprint(test_mat1.shape)\nprint('Dimenions of the brain mask')\nprint(brain_mask.shape)\n\n## Let's also load a second participant\ntest_mat2 = h5py.File('../input/trends-assessment-prediction/fMRI_test/10006.mat', mode='r')\ntest_mat2 = np.array(test_mat2.get('SM_feature'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If we want to visualize the maps, we need to know some information about the dimensions of the data. Plotting the brain-mask which was provied, we see that the dimensions are not quite in the correct order. \nSecondly, we have two dimensions of size 53, so we need to play around to find the correct ordering, "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Somehow nilearn is not happy with plotting matrices anymore - so we have to create a nifti first:\ndef map_for_plotting(mat, brain_mask):\n    # Assuming that we provide a 3D image\n    # image.new_img_like creates a nifti by applying informaiton from the soure image (here brain_mask),\n    # like the affine to a matrix.\n    return image.new_img_like(brain_mask, mat.transpose([2, 1, 0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's extract the indices for the different average networks\nsample_maps1 = {}\nsample_maps2 = {}\nfor ii in icn_idx.keys():\n    # indices -1 because matlab\n    sample_maps1[ii] = map_for_plotting(test_mat1[icn_idx[ii] -1].mean(0), brain_mask)\n    sample_maps2[ii] = map_for_plotting(test_mat2[icn_idx[ii] -1].mean(0), brain_mask)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Average SM Feature Maps"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(len(sample_maps1), 2, figsize=(20, 10))\n\nfor n, ii in enumerate(sample_maps1.keys()):\n    # We are plotting glass brains here - a nice way to visualize brain maps\n    plotting.plot_glass_brain(sample_maps1[ii], title=ii, axes=axes[n, 0], plot_abs=False)\n    plotting.plot_glass_brain(sample_maps2[ii], title=ii, axes=axes[n, 1], plot_abs=False)\naxes[0, 0].set_title('Networks for Participant 1');\naxes[0, 1].set_title('Networks for Participant 2');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here are the networks for two participants plotted next to each other. As I alluded too earlier resting state networks are quite stable and similar, even across participants. \nOn the other hand, they also display some distinction (I mean it is possible to identify participants based on their network connectivity alone).\n\nI think there will be some information here for the different prediction targets but for some first pilotting of models I am going to skip this data and see how far I am coming. \nAs I said before, I am not too familiar with the literature (right now) and these maps could be **the** key features to focus later on, but for some very first analyses it might be possible to get good results with the connectivity data alone. (Just wildly guessing)"},{"metadata":{},"cell_type":"markdown","source":"# Back to FNC data\nWe now have some idea about the different networks involved in the functional network connectivity. We could now try to reshape the FNC data to a connectivity matrix. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# This is probably totally inefficient - but let's try it\nicn_mat_idx = icn_number.T.to_dict('list')\n# Reverse the matrix:\nicn_mat_idx = {i[0]: j for i, j in zip(icn_mat_idx.values(), icn_mat_idx.keys())}\n# Map names to indices\nname_matrix = {}\n\nfor fnco in fnc10_cols:\n    name_matrix[fnco] = ([np.int(icn_mat_idx[np.int(i.split(')')[0])]) for i in fnco.split('(')[1:]])\n    \n# And now create a sample connectivity matrix:\ncon_matrix1 = np.zeros((53, 53))\ncon_matrix2 = np.zeros((53, 53))\n\nfor n in fnc10_cols:\n    r_, c_ = name_matrix[n]\n    con_matrix1[c_, r_] = fnc_10.iloc[0, :][n]\n    con_matrix2[c_, r_] = fnc_10.iloc[1, :][n]\n# And now add the transpose - its symmetrix\ncon_matrix1 += con_matrix1.T\ncon_matrix2 += con_matrix2.T\n\n# Prepare labeling:\ncol_halves = np.array([jj.split('_')[-1]  for jj in name_matrix.keys()])\n_, idx = np.unique(col_halves, return_index=True)\ncol_labels = col_halves[np.sort(idx)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1, 2,figsize=(20, 7.5))\n\nsns.heatmap(con_matrix1, cmap='coolwarm', square=True, ax=ax[0], \n            xticklabels=col_labels, \n            yticklabels=col_labels, cbar=False, center=0, vmin=-1, vmax=1)\n\nsns.heatmap(con_matrix2, cmap='coolwarm', square=True, ax=ax[1], \n            xticklabels=col_labels, \n            yticklabels=col_labels, cbar=False, center=0, vmin=-1, vmax=1)\n\nax[0].set_title('Example 1')\nax[1].set_title('Example 2');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I am not totally sure about the labels and whether everything is in the correct location, but it doesn't seem too shabby.\n\nThe two example matrix look quite as expected: We have a high connectivity (i.e., correlation) between components with the same name. And some connectivity between. \n\nWe also see that different participants have different expressions. For example, participant 2 has a higher interconnectivity in the visual network (VSN) when compared to participant 2.\n\nMy first bet, is that these networks will provide much of the information we need for this challenge."},{"metadata":{},"cell_type":"markdown","source":"# Loading.csv\n\n> loading.csv - sMRI SBM loadings for both train and test samples\n\nThis is unfortunately I cannot tell you that much about, except:\n\n> The first set of features are source-based morphometry (SBM) loadings. These are subject-level weights from a group-level ICA decomposition of gray  matter concentration maps from structural MRI (sMRI) scans.\n\nWe again get different values of ICA deompositions, this time for structural images, they look something like this:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plotting.plot_anat(datasets.load_mni152_template(), title='MNI template');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On a technical note - structural images use a different contrast than BOLD images (all the data description above, i.e., functional connectivity). \nThe aim of these images is to provide a better anatomical picture of the brain. Due to the measurement procedures, BOLD images usually have a realtively low resolution, as you want to squeeze in as many data-points along time as possible. For structural images the measurement can take several minutes for one image, achieving a higher spatial resolution. \n\nSo for this type of data we receive information about the concentration of gray-matter in the brain. These do not describe much about the dynamics or interaction of the brain, but can tell different stories about how a person's life went. If I remember correctly, there was a study that playing tetris increases gray-matter density and volume in some brain area many years back. I don't know how reliable this particular study was, let's say you will find traces of some longer lasting process here. \n\nParticularly interesting for this study is, I think, that there is a reduction in gray matter volume over age. So something might be here for you to find :) "},{"metadata":{},"cell_type":"markdown","source":"# Target variables - age and assessments\nAfter getting an  overview of the different kinds of data in the challenge, we can now have a look at the target variables in the training set.  These are stored in this .csv\n\n> train_scores.csv - age and assessment values for train samples\n\nWe have age (which has been manipulated for privacy concerns) and multiple assessment variables (could be: Depression or mental health, working memory, memory, intelligence, personality...). Also, we know there are different sites in this datasets, which induces many possible biases like:\n* Different fMRI scanners (manufacturers, models, scanner) have an effect on the measured data\n* Different sample population\n* Different measurement times (maybe one site is earlier in the morning, the other in the evening)\n* Different weather (yup, has been found to have a slight effect one measurements) \n\nSo we need to make sure that our predictions are robust enough! \n\n## Distributions"},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = pd.read_csv('../input/trends-assessment-prediction/train_scores.csv')\nscores = scores.set_index('Id')\nscores.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores.isna().sum(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And we already see that there are quite some missing values in the sample, especially for the first domain. In this quick and dirty approach I will just replace the values by 0 for now, but will need to look into a better imputation method later.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = scores.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(2, 3, figsize=(15, 5))\n\nfor ax, data in zip(axes.flatten()[:5], scores.columns[:5]):\n    deciles = np.percentile(scores[data].values, [10, 20, 30, 40, 50, 60, 70, 80, 90])\n    \n    sns.distplot(scores[data], ax=ax)\n    # Ugly, but whatever\n    for de in deciles:\n        ax.axvline(de, c='black')\n    \n    ax.set_title(data)\n    \naxes.flatten()[-1].set_axis_off()\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We se that most of the data seems to relatively normally distributed. As you can see in this plot, I also included the deciles of the distribution, the `NaN` filling, however added a left skew to the distritbutions of the assessment scores. \n\nTo not bias our thinking about the whole dataset too much, I will use the deciles to discretize the data distribution, which will I then use to create a stratified sub-sample (will also be helpful for cross-validation)"},{"metadata":{},"cell_type":"markdown","source":"## Correlation between target values\nDirectly running into an issue - the discretization is too fine, i.e. some combinations occur only once. \n\nA look at the correlation could help to find a better, less fine, discretization."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(scores.corr())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Correlations in domain1 is quite high, so dropping one might be quite safe. For domain2 the two variables are not too much correlated, which is unfortunated. For a better split I will, however, drop domain2_var1 from the stratification, as domain2_var2 has less correlation with age. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add discretization to data:\nfor sc in scores.columns[:5]:\n    deciles = np.percentile(scores.loc[:, sc], [20, 40,  60, 80])\n    discr = np.digitize(scores.loc[:, sc], deciles)\n    scores.loc[:, sc + '_discrete'] = discr.astype(str)\n    \n# Everything to one variable:\nscores.loc[:, 'stratify'] = (scores['age_discrete'] + '_'\n                             + scores['domain1_var1_discrete'] + '_' \n                             + scores['domain2_var2_discrete'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores.stratify.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not great, but could be sufficient. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# And now draw a stratified sample, we will statistically analyse 20% of the data\nfrom sklearn.model_selection import train_test_split\n\ntrain_idx, _ = train_test_split(scores.index, train_size=0.2, random_state=223, stratify=scores.stratify)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores_stat = scores.loc[train_idx]\n\nfig, axes = plt.subplots(2, 3, figsize=(15, 5))\n\nfor ax, data in zip(axes.flatten()[:5], scores_stat.columns[:5]):\n    deciles = np.percentile(scores_stat[data].values, [10, 20, 30, 40, 50, 60, 70, 80, 90])\n    \n    sns.distplot(scores_stat[data], ax=ax)\n    sns.distplot(scores[data], ax=ax)\n    ax.legend(['Subsample', 'Original'])\n    # Ugly, but whatever\n    for de in deciles:\n        ax.axvline(de, c='black')\n    \n    ax.set_title(data)\n\n    \naxes.flatten()[-1].set_axis_off()\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The deciles are calculated based on the subsample here. This stratification approach seems to provide a good representation of the original dataset. So we can start to do some basic statistics and learn a bit about the relationship between features and targets. "},{"metadata":{},"cell_type":"markdown","source":"# Statistical Analysis\n\n## FNC - correlations\nLet's investigate the functional connectivity, in a non-spatial mass-univariate analysis, we can for example look at the correlation between functional connectivity values across participants and the target variables. Based on certain patterns, we could think later about constricting our analysis of the `SM` maps for some targets. "},{"metadata":{"trusted":true},"cell_type":"code","source":"fnc = pd.read_csv('../input/trends-assessment-prediction/fnc.csv', index_col='Id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fnc_sample = fnc.loc[train_idx, :]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correlations: \ncorr_df = []\nfor col_score in ['age', 'domain1_var1', 'domain1_var2', 'domain2_var1', 'domain2_var2']:\n    tmp_corr = fnc_sample.corrwith(scores_stat.loc[:, col_score]).to_frame().transpose()\n    corr_df.append(tmp_corr)\n\nfig, axes = plt.subplots(1, 5, figsize=(15, 5), sharex=True, sharey=True)\n\nfor col_score, ax, tmp_df in zip(['age', 'domain1_var1', 'domain1_var2', 'domain2_var1', 'domain2_var2'], axes.flatten(), corr_df):\n    ax.hist(tmp_df.transpose().values.ravel())\n    ax.set_title(col_score)\nplt.suptitle('Histograms of pearson correlations');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have really low correlation values, so maybe plotting them in a connectivity matrix won't provide the best visualization, so I am thresholding the matrices at the 80% percentile of the absolute correlation values. "},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(2, 3, figsize=(15,10), sharex=True, sharey=True)\n\nfor col_score, ax, tmp_corr in zip(['age', 'domain1_var1', 'domain1_var2', 'domain2_var1', 'domain2_var2'], axes.flatten(), corr_df):\n\n    perc_ = np.percentile(np.abs(tmp_corr.values.ravel()), 90)\n    tmp_matrix = np.zeros((53, 53))\n    for n in fnc10_cols:\n        r_, c_ = name_matrix[n]\n        tmp_matrix[c_, r_] = tmp_corr.iloc[0, :][n]\n            \n    tmp_matrix[np.abs(tmp_matrix) < perc_] = 0\n    tmp_matrix += tmp_matrix.T\n\n    sns.heatmap(tmp_matrix, cmap='coolwarm', square=True, ax=ax, \n            xticklabels=col_labels, \n            yticklabels=col_labels, cbar=False, center=0, vmin=-0.25, vmax=0.25)\n\n    ax.set_title(col_score)\n    \naxes.flatten()[-1].set_axis_off()\nprint(perc_)\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"There seems to be a lot of noise in here. But for example:\n* Patterns in domain2_var2 and age seem to be going into opposite directions\n* The VSN network seems to be important for age. Negative correlation between SMN and higher correlations with DMN\n* SCN and correlations to SMN/ADN show stronger patterns in many of the variables."},{"metadata":{},"cell_type":"markdown","source":"### Applying dimensionality might help us find multi-variate patterns"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\n\npca = PCA(n_components=0.8, whiten=True, svd_solver='full')\npca.fit(fnc_sample.values)\n\ncomponents = pca.transform(fnc_sample.values)\ncomponents = pd.DataFrame(components, index=scores_stat.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(1,3, figsize=(15,5))\n\naxes[0].plot(pca.explained_variance_ratio_[:21])\naxes[0].set(title='Elbowplot of PCA components', ylabel='Explained Variance', xlabel='Components')\n\ntmp_matrix1 = np.zeros((53, 53))\ntmp_matrix2 = np.zeros((53, 53))\nfor i, n in enumerate(fnc10_cols):\n    r_, c_ = name_matrix[n]\n    tmp_matrix1[c_, r_] = pca.components_[0, i]\n    tmp_matrix2[c_, r_] = pca.components_[1, i]\n\ntmp_matrix1 += tmp_matrix1.T\ntmp_matrix2 += tmp_matrix2.T\n\nsns.heatmap(tmp_matrix1, cmap='coolwarm', square=True, ax=axes[1], \n        xticklabels=col_labels, \n        yticklabels=col_labels, cbar=False, center=0)\naxes[1].set(title='Component 0')\naxes[1].set_xticklabels(axes[1].get_xmajorticklabels(),  fontsize=6)\n\nsns.heatmap(tmp_matrix2, cmap='coolwarm', square=True, ax=axes[2], \n        xticklabels=col_labels, \n        yticklabels=col_labels, cbar=False, center=0)\naxes[2].set(title='Component 1')\naxes[2].set_xticklabels(axes[2].get_xmajorticklabels(), fontsize=6)\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using an elbowplot on the explained variance scores, doesnt really provide us with too much hope, to really find interesting patterns in the PCA analyis. Explained variance of the different components seems to taper off really quickly. Only the first one explainin a reasonable amount."},{"metadata":{"trusted":true},"cell_type":"code","source":"pca_corr = []\nfor kk in range(10):\n    pca_corr.append(scores_stat[['age', 'domain1_var1', 'domain1_var2', 'domain2_var1', 'domain2_var2']].corrwith(components.loc[:,kk]))\n\npd.concat(pca_corr, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our hunch is confirmed. Only the first components seems to have a small correlation with age"},{"metadata":{},"cell_type":"markdown","source":"## T1 images - structural components\n\nSo here we have only 26 components. Let's do the same correlation game again!"},{"metadata":{"trusted":true},"cell_type":"code","source":"loadings_samp = pd.read_csv('../input/trends-assessment-prediction/loading.csv', index_col='Id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loadings_samp = loadings_samp.loc[train_idx, :]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correlations: \nfig, axes = plt.subplots(1, 5, figsize=(15,5))\nload_corr = []\nfor ax, col_score in zip(axes.flatten(), ['age', 'domain1_var1', 'domain1_var2', 'domain2_var1', 'domain2_var2']):\n    tmp_corr = loadings_samp.corrwith(scores_stat.loc[:, col_score]).to_frame().transpose()\n    load_corr.append(tmp_corr)\n    ax.hist(tmp_corr.values.ravel())\n    ax.set_title(col_score)\n    \nload_corr = pd.concat(load_corr)\nload_corr.loc[:, 'Assessment'] = ['age', 'domain1_var1', 'domain1_var2', 'domain2_var1', 'domain2_var2'] \nload_corr.set_index('Assessment', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we seem to be a little bit more lucky, there are some ICs that are anti correlated with age! A hunch, which I mentioned before, as grey matter density seems to decrease with higher age. \nSome areas in domain2_var2 seem also be related with grey-matter values. "},{"metadata":{},"cell_type":"markdown","source":"# Looking at SM maps"},{"metadata":{},"cell_type":"markdown","source":"I'm trying to make this approach a bit easier. Nilearn does not only have great plotting capabilites, but I can also be used to extract great amounts of data. \n\nI mentioned atlasses and parcellations in the FNC section before. To reduce the huge amounts of data in the SM files, I am using a relatively low parcellation to extract for each of the networks average values per region.\n\nIn an ADHD prediction challenge, I had the feeling that too high resolutions are not really helpful, so I am going with the middle way. I am not too familiar with the new trend in atlasses - so I took one I heard about at one time. "},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    basc_data = datasets.fetch_atlas_basc_multiscale_2015(version='sym', data_dir=None, resume=True, verbose=1)\nexcept:\n    print(\"Probably time out\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"basc_197 = nb.load(basc_data['scale197'])\nplotting.plot_roi(basc_197)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 197 regions in the data. We have 53 components, so we get to 10441 different features. Which is a lot less than using all pixels, but still a lot. To extract the data, we can use a masker function from nilearn"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nilearn import input_data\n# We also use the brain_mask from the beginning\nbasc197_masker = input_data.NiftiLabelsMasker(basc_197, mask_img=brain_mask)\n\ndef load_matlab(participant_id, masker, path='../input/trends-assessment-prediction/fMRI_train/'):\n    mat = np.array(h5py.File(f'{path}{participant_id}.mat', mode='r').get('SM_feature'))\n    mat = masker.fit_transform(nb.Nifti1Image(mat.transpose([3,2,1,0]), affine=masker.mask_img.affine))\n    return mat.flatten()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This takes ages about (like 8 min ... so time 5 for the whole data set)\nfrom joblib import Parallel, delayed\n\nsm_data = Parallel(n_jobs=-1)(delayed(load_matlab)(ii, basc197_masker) for ii in tqdm(list(train_idx)))\nsm_data = np.stack(sm_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So now we have the data... But I am not really sure what do to with this. Let's try feature reduction and look at some correlations. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import FastICA\npca_2 = PCA(n_components=0.6, whiten=True)\npca_2.fit(sm_data)\n\ncomponents2 = pca_2.fit_transform(sm_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(1,1, figsize=(15,5))\n\naxes.plot(pca_2.explained_variance_ratio_[:30])\naxes.set(title='Elbowplot of PCA components', ylabel='Explained Variance', xlabel='Components')\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The correlation game"},{"metadata":{"trusted":true},"cell_type":"code","source":"components2 = pd.DataFrame(components2, index=scores_stat.index)\npca2_corr = []\nfor kk in range(20):\n    pca2_corr.append(scores_stat[['age', 'domain1_var1', 'domain1_var2', 'domain2_var1', 'domain2_var2']].corrwith(components2.loc[:,kk]))\n\npca2_scorr = pd.concat(pca2_corr, axis=1)\npca2_scorr","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on the elbow plots the explained variance ratios are decreasing quite quickly, which is expected based on the input-data. "},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(1, 5, figsize=(15,5), sharex=True, sharey=True)\n\nfor n, ax in enumerate(axes.flatten()):\n    ax.plot(pca2_scorr.iloc[n, :])\n    ax.set_title(pca2_scorr.index[n])\n    ax.axhline(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again some components seem to be quite highly correlated with age, the other assessment variables are not so high (as in the other analysis). \n\nThis is not a really meaningful analysis, but might inform you, whether to include the individual score in the analysis. "},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\nWe have a lot of data in different forms. Integrating all the data in one single model migth solve difficult, especially as FNC connectivity and SM values provide many many different features. \n\n* Age seems to be a variable that appears to be highly correlated with many of the features in the data.\n\nAll the other variables are less expressed. \n\nFurthermore, we seem to able to stratify the data based on a subset of deciles and a couple of the variables for a potentially good cross-validation. "},{"metadata":{},"cell_type":"markdown","source":"# Predictions\nWell for version 11 I said this is the final version. But sometimes I also like to run some basic models :)\n\nAlso to see if the partition we are using helps us in terms of getting a good estimate of the prediction error. \n\nI am not going to use the `SM` maps, only loadings and FNC data - it's easier I guess."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Not the best approach but let's load some data again and delete some\nimport gc\ntry:\n    del fnc\n    del sm_data\n    del pca\n    del pca_2\nexcept:\n    pass\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading data again - we have the scores with the stratifier variable alread\nfnc = pd.read_csv('../input/trends-assessment-prediction/fnc.csv', index_col='Id')\nloading = pd.read_csv('../input/trends-assessment-prediction/loading.csv', index_col='Id')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As a very baseline model let's use a RidgeRegression - I am not the biggest fan of SVMs (don't really know why)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import RidgeCV\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.decomposition import PCA","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fnc_train = fnc.loc[scores.index, :]\nloading_train = loading.loc[scores.index, :]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test whether indices align\nassert np.all(fnc_train.index == loading_train.index) \nassert np.all(fnc_train.index == scores.index) \nassert np.all(loading_train.index == scores.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SKF = StratifiedKFold(n_splits=4)\ntargets = ['age', 'domain1_var1', 'domain1_var2', 'domain2_var1', 'domain2_var2']\nweighting = [.3, .175, .175, .175, .175]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# I hope I did the calculation correctly\ndef absolute_normalized_error(y_true, y_pred, multioutput):\n    output_errors = np.sum(np.abs(y_pred - y_true), axis=0) / np.sum(y_pred, axis=0)    \n    return np.average(output_errors, weights=multioutput)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"REG_FNC = make_pipeline(PCA(n_components=50, whiten=False), RobustScaler(), RidgeCV(alphas=np.logspace(-5, 5, 11))) # Some dimensionality reduction might be in order\nREG_LOA = make_pipeline(RobustScaler(), RidgeCV(alphas=np.logspace(-5, 5, 11))) # Not so much here\n\ntrues, preds_fnc, preds_load, preds_comb = [], [], [], []\nscores_fnc, scores_load, scores_comb = [], [], []\n\nfor tr, te in SKF.split(fnc_train, scores.stratify):\n    REG_FNC.fit(fnc_train.iloc[tr, :].values, scores.iloc[tr][targets])\n    REG_LOA.fit(loading_train.iloc[tr, :].values, scores.iloc[tr][targets])\n    \n    preds_fnc.append(REG_FNC.predict(fnc_train.iloc[te,:]))\n    preds_load.append(REG_LOA.predict(loading_train.iloc[te,:]))\n    preds_comb.append((preds_fnc[-1] + preds_load[-1]) / 2)\n    trues.append(scores.iloc[te][targets])\n    scores_fnc.append(absolute_normalized_error(trues[-1], preds_fnc[-1],  multioutput=weighting))\n    scores_load.append(absolute_normalized_error(trues[-1], preds_load[-1],  multioutput=weighting))\n    scores_comb.append(absolute_normalized_error(trues[-1], preds_comb[-1],  multioutput=weighting))\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Error based on FNC: {np.mean(scores_fnc)} +/- {np.std(scores_fnc)}')\nprint(f'Error based on Load: {np.mean(scores_load)} +/- {np.std(scores_load)}')\nprint(f'Error based on Load: {np.mean(scores_comb)} +/- {np.std(scores_comb)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Score is currently below baseline, but let's see how we do on the leaderboard."},{"metadata":{"trusted":true},"cell_type":"code","source":"REG_FNC.fit(fnc_train, scores[targets])\nREG_LOA.fit(loading_train, scores[targets])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the test data\nsample_submission = pd.read_csv('../input/trends-assessment-prediction/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extract the test index\ntest_index = sample_submission.Id.str.split('_', expand=True)[0].unique().astype('int')\nfnc_test = fnc.loc[test_index, :]\nloading_test = loading.loc[test_index, :]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate the average prediction value\nprediction = (REG_FNC.predict(fnc_test) + REG_LOA.predict(loading_test)) / 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Submit prediction\npredictions = pd.DataFrame(prediction, index=test_index, columns=targets).reset_index()\npredictions = predictions.rename(columns={'index': 'Id'})\npredictions = predictions.melt(id_vars='Id', value_vars=targets, value_name='Predicted')\npredictions.loc[:, 'Id'] = predictions.loc[:, 'Id'].astype(str) + '_' + predictions.loc[:, 'variable']\npredictions = predictions[['Id', 'Predicted']]\npredictions.to_csv('ridge_baseline_submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}