{"cells":[{"metadata":{},"cell_type":"markdown","source":"# TReNDS Neuroimaging - Data exploration"},{"metadata":{},"cell_type":"markdown","source":"So this is a documentation of my travels down this rabbit hole. I try to write down all my thoughts when exploring data, that's how I learn. Also, I'm hoping that you guys can help me know just how wrong I am. I don't think you are going to find any contest-winning ideas here, I find most ML models scary and try to stick with Linear Regression as far as I can.\n\nMy apologies to @PAB97 who is the original author of this Notebook. I stole it and I've mostly modified it by adding my own mistakes. Actually, I think I have already modified it beyond recognition. Sorry.\n\n**This kernel will be a work in progress, and I will keep on updating it as the competition progresses and I gain more insight about the data.**\n\nIf you find this kernel useful, please consider upvoting it, it motivates me to write more quality content."},{"metadata":{},"cell_type":"markdown","source":"## <a href='#2'>Data exploration</a>"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# Importing dependencies\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\n\nfrom tqdm.notebook import tqdm\nfrom glob import glob\nimport gc\n\nimport nilearn as nl\nimport nilearn.plotting as nlplt\nimport nibabel as nib\n\nimport h5py\n\nimport lightgbm as lgb\n\nfrom scipy.stats import skew, kurtosis\n\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.model_selection import cross_validate, train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\nimport os, random\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense\n\nfrom tensorflow.keras.utils import Sequence","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Loading train scores\n\nMAIN_DATA_PATH = '/kaggle/input/trends-assessment-prediction/'\n\ntrain_scores_df = pd.read_csv(MAIN_DATA_PATH + 'train_scores.csv')\nicn_numbers_df = pd.read_csv(MAIN_DATA_PATH + 'ICN_numbers.csv')\nloading_df = pd.read_csv(MAIN_DATA_PATH + 'loading.csv')\nfnc_df = pd.read_csv(MAIN_DATA_PATH + 'fnc.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## SBM Loadings\n\nPer the competition documentation,\n\n> The first set of features are source-based morphometry (SBM) loadings. These are subject-level weights from a group-level ICA (independent component analysis) decomposition of gray matter concentration maps from structural MRI (sMRI) scans.\n\nLet's think about this for a bit. There are a few interesting code words here. These are sMRI scans, as oppose to the fMRI below. These scans reveal the structure of the brain, i.e. where all the gray goo are located. The fMRI show sort of show how the goo is working in different situation.\n\nSo, thinking back, we are trying to predict the age and some secret features of subjects. Does the structure of the brain change with age? And is it important in other, secret aspects? This is exciting!\n\nIn this context we are looking at a process called Independent Component Analysis. The goal of this process is to find a set of features, 'components', that have the least to do with each other. The classical example is the coctail problem, where you want to find out what people at the coctail party are saying (I honestly can't think of many problems I'm less interested in finding the solution to; coctail parties are boring). \n\nYou have signals from some microphones. If you can find a set of features that are really independent and super not-random then you can be fairly sure that these features represent the individual voices of the people at the party. I guess the theory here is there is no correlation between what different people say at coctail parties, which is a deep insight in itself. \n\nSo we're thinking that there are some hidden features that describe our brains. These features are mixed in some secret way and result of that is what shows up in the sMRI scan. The ICA gives us an indication of what the features might be.\n\nIn this case, we are working on group level ICA. I think that refers to the process of taking a lot of this sMRI scans of lots of people (not just these subjects) and find the features where these subjects stand out. So these values represent the features where these subjects stand out the most.\n\nA dumb analogy could be that by analyzing lots and lots of coctail party conversations you could note that most of them are about the weather, the economy, humblebragging about oneself and outright bragging about one's children. An individual coctail party could be described by how the relative amounts and intensities of these respective conversations differ from whats normal. \"Remember the party at the Robinsons, where we talked soo much about the weather? So nice!\"\n\nI haven't been to many coctail parties myself, for some reason I'm never invited."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"loading_df.head()\nloading_melted = loading_df.melt(id_vars='Id')\nloading_melted.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's have look at the distributions of these values."},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.FacetGrid(loading_melted, col='variable', col_wrap=3, height=6, sharex=False, sharey=False)\ng.map(sns.distplot, 'value')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loading_df.describe()\nplt.figure(figsize=(20,5))\nsns.violinplot(data=loading_melted, x='variable', y='value')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we have some features that honestly make no sense to me what so ever. I was hoping I would end up being able to look at a sMRI scan after this and say something like: \"Hmm, that IC_14 sure looks worrying.\"\n\nBefore we continue to the even cooler features, from the fabeled **f**MRI scans, let's see if we already can make some predictions."},{"metadata":{},"cell_type":"markdown","source":"### <a href='#2-1'>Target distributions</a>"},{"metadata":{},"cell_type":"markdown","source":"The train_scores.csv file contains the targets that we need to predict."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_scores_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Plot the distribution of the target variables\n\ntrain_scores_df_melted = train_scores_df.melt(id_vars='Id')\n\ng = sns.FacetGrid(train_scores_df_melted, col='variable', height=4)\ng.map(sns.distplot, 'value')\n\nplt.subplots_adjust(top=0.85)\ng.set_titles('{col_name}')\ng.set_xlabels('')\ng.fig.suptitle('Target distributions')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's now build our training set composed of multiple sets of features."},{"metadata":{"trusted":true},"cell_type":"code","source":"features_df = pd.merge(train_scores_df, loading_df, on=['Id'], how='left')\nfeatures_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since we still have relatively few features, let's investigate the correlation between target variables and features."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(12, 8))\ncols = features_df.columns[1:]\ntarget_cols = features_df.columns[1:6]\nfeature_cols = features_df.columns[6:]\nsns.heatmap(features_df[cols].corr(), ax=ax, cmap='RdYlGn')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"OK, we know some stuff now. Let's see what a simple old Linear Regression model gives us."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = features_df[feature_cols]\ny = features_df['age']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n\nlinreg = LinearRegression()\nlinreg.fit(X_train, y_train)\n\ny_pred = linreg.predict(X_test)\n\nrmse = mean_squared_error(y_test, y_pred) ** 0.5\n\nprint(f\"R^2:\", linreg.score(X_test, y_test))\nprint(f\"RMSE: {rmse}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"OK, done! I can't possibly imagine a better result. The prize money is all mine.\n\nAlso, let's see if there is some features that are more important than others. I really like Lasso regularization for this. We're still in the good ol' linear regression domain, but we see if we can penalize some parameters and still get a decent result.\n\nThe Lasso isn't great for actual modeling, I think, but it has the really nice feature of either penalizing a feature completely or not at all. This can give you a shortlist of the most interesting features."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Lasso\n\nlasso = Lasso(alpha=0.05, normalize=True)\n\nlasso.fit(X,y)\n\nplt.style.use('ggplot')\nplt.figure(figsize=(12,8))\nplt.plot(X.columns, lasso.coef_)\n\n_ = plt.xticks(rotation=70)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I honestly love this. Just for fun, let's see what happens when you focus only on these three features for our Linear Regression."},{"metadata":{"trusted":true},"cell_type":"code","source":"cool_features = ['IC_06', 'IC_15', 'IC_22']\n\nlinreg = LinearRegression()\nlinreg.fit(X_train[cool_features], y_train)\n\ny_pred = linreg.predict(X_test[cool_features])\n\nrmse = mean_squared_error(y_test, y_pred) ** 0.5\n\nprint(f\"R^2:\", linreg.score(X_test[cool_features], y_test))\nprint(f\"RMSE: {rmse}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ok, so let's think about these numbers. The $R^2$ is the fraction $SS_{res} \\over SS_{tot}$. That numerator is how much my model differs from the observed values and the denominator is how much the actual values tend to differ. The result is how much of the variation in the target features is captured in my model.\n\nThe RMSE is the square root of the Mean Squared Error. The MSE is the $SS_{res}$ from above, that is how much the model differs from the observed values. The neat thing with taking the square root of the MSE, is that you get the same dimension as the target variable. This gives us the rather comprehensible insight that our model is off by ten years or so on average when estimating the age of the subjects. I get worse results every day when I try to estimate the age of people.\n\nThis is the reason I choose to focus mainly on age for now. I don't know what domain 1 etc is, so at this exploratory stage I have no idea how to think about the values. Baby steps...\n\n(Also, we all know that they keep them values secret in order to hide that they are analyzing the passengers from Flight 828, and I'm NOT taking part in that dirty business.)\n\nAnyway, a $R^2$ of 34% is not great, but I really like the fact that is not that much lower when we use just three features. Often, when the features make more sense, these kind of insights can really make this EDA phase exciting. "},{"metadata":{},"cell_type":"markdown","source":"## Submission\n\nLet's do something we can submit. We'll go with Ridge Regularized Linear Regression for now, and try to forecast all the target variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import RidgeCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\noutputs = list()\n\nfor target in target_cols:\n    scaler = StandardScaler()\n    ridge = RidgeCV(alphas=np.logspace(5, -5, 11), cv=5)\n\n    pipeline = Pipeline(steps=[\n        ('scaler', scaler),\n        ('regression', ridge)\n    ])\n\n    y = features_df[target]\n    y = y.fillna(y.mean())\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n\n    pipeline.fit(X_train, y_train)\n\n    y_pred = pipeline.predict(X_test)\n\n    rmse = mean_squared_error(y_test, y_pred) ** 0.5\n    print(f\"R^2 for {target}:\", pipeline.score(X_test, y_test))\n    print(f\"RMSE for {target}: {rmse}\")\n    print(f\"Best alpha for {target}\", ridge.alpha_)\n    \n    predictions = pipeline.predict(loading_df.iloc[:,1:])\n    ids = loading_df.Id.astype(str) + '_' + target\n\n    outputs.append(pd.DataFrame({'Id': ids,\n                                'Predicted': predictions}))\n\noutput = pd.concat(outputs)\noutput","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We don't get great results for the mysterious variables. This shouldn't come as a surprise - look at the heatmap above. There doesn't seem initially to be that much of a correlation between those variables and our features."},{"metadata":{},"cell_type":"markdown","source":"## <a href=\"5-1\">Submission</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission = pd.read_csv(\"/kaggle/input/trends-assessment-prediction/sample_submission.csv\")\nsample_submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission = pd.read_csv(\"/kaggle/input/trends-assessment-prediction/sample_submission.csv\")\noutput = sample_submission.drop('Predicted',axis=1).merge(output,on='Id',how='left')\noutput.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}