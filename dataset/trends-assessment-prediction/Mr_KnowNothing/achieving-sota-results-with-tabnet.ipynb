{"cells":[{"metadata":{},"cell_type":"markdown","source":"# About this Notebook\nIf anyone of you have read my previous kernels , you might know how much I love the EDA part , but it struck me that writing on one particular thing would not help me grow , so I have decided to explore untreaded territories to explore new things. For this competition people are mostly using Rapids and the tabular data . I have hardly seen any kernels using only images and both images and tabular data .\n\nFew days ago I saw Abhishek's post on LinkedIn about Tabnet and I was really curious about it , I wanted to apply the idea here on Trends data but it had already been done and didn't give good results so I dropped it.\n\nAfter watching Sebastian on Abhishek talks , I realized that Tabnet's potential isn't being fully utilized .\n\n**This notebook presents a fully structured working pipeline for training n-folds Tabnet Regressor for this competition . This Notebook achieves 0.1620 without a lot of efforts and this notbook could beat Rapids SVM's and achieve the benchmark 0.1595 with some tweaks . I also explain the pros and cons of using Tabnets (although I don't find a lot cons ðŸ˜œ )**\n\nHere is the [link](https://arxiv.org/pdf/1908.07442.pdf) to Tabnet Paper\n\n<font color='red'>If you like my efforts please leava an upvote .As I am not planning on doing this competition for now , if you all like my efforts I plan to release more public kernels on Tabnet with higher scores</font>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Token of Gratitude\n\n* For the part other than modelling I have used most of the code from this wonderful [kernel](https://www.kaggle.com/aerdem4/rapids-svm-on-trends-neuroimaging) by Ahmet , Thank you for writing it \n* A big thanks to team of Pytorch-Tabnet for writing such a beautiful implementations with so much functionalities . The repo can be found [here](https://github.com/dreamquark-ai/tabnet)\nThe documentation is very nicely written and Sebastien has also provided with example notebooks to help understand the model and usage better. Everything can be found at above mentioned repo","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Advantages of Tabnet\n\nTabnet gives us the following advantages :-\n* The best thing which I found is Tabnet allows us to train a MULTIREGRESSOR and we don't to create separate models for every class\n\n* It uses attention for selecting out the set of features to focus on for a given particular data point and we can even visualize that to see which parts get attention for a particular decision . We can also play with the number of features we want the Tabnet to focus to.\n* It uses backprop for improving decisions and weights thus providing a greater control to us\n* We can use the fine-tuning techniques that have worked for us and all the deep-learning concepts like LR annealing , Custom loss,etc\n* The headache of feature selection is vanished as Tabnet does that on its own.\n* It achieves SOTA results wothout any feature engg, finetuning with just  the defaults , wonder what it can do with sufficient feature engineering and finetuning\n\nThere are a lot of more advantages and ideas that I have for Tabnet which I plan to release in the future\n\nIf you want to learn more about Tabnet and it's inner workings please refer to this [video](https://www.youtube.com/watch?v=ysBaZO8YmX8)","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"!pip install pytorch-tabnet","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Preliminaries\nimport numpy as np\nimport pandas as pd \nimport os\nimport random\n\n#Visuals\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#Torch and Tabnet\nimport torch\nfrom pytorch_tabnet.tab_model import TabNetRegressor\n\n#Sklearn only for splitting\nfrom sklearn.model_selection import KFold","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Configuration","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"NUM_FOLDS = 7  # you can specify your folds here\nseed = 2020   # seed for reproducible results","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Seed Everything\n\nSeeding Everything for Reproducible Results","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seed_everything(seed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Metric\n\nSince Tabnet allows us to create a MULTIREGRESSOR , we don't have to create multiple models and loop through them . I have modified the metric to account for that","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def metric(y_true, y_pred):\n    \n    overall_score = 0\n    \n    weights = [.3, .175, .175, .175, .175]\n    \n    for i,w in zip(range(y_true.shape[1]),weights):\n        ind_score = np.mean(np.sum(np.abs(y_true[:,i] - y_pred[:,i]), axis=0)/np.sum(y_true[:,i], axis=0))\n        overall_score += w*ind_score\n    \n    return overall_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preparation\n\nMostly Taken from Ahmet's kernel","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fnc_df = pd.read_csv(\"../input/trends-assessment-prediction/fnc.csv\")\nloading_df = pd.read_csv(\"../input/trends-assessment-prediction/loading.csv\")\n\nfnc_features, loading_features = list(fnc_df.columns[1:]), list(loading_df.columns[1:])\ndf = fnc_df.merge(loading_df, on=\"Id\")\nfeatures = fnc_features + loading_features\n\n\nlabels_df = pd.read_csv(\"../input/trends-assessment-prediction/train_scores.csv\")\ntarget_features = list(labels_df.columns[1:])\nlabels_df[\"is_train\"] = True\n\n\ndf = df.merge(labels_df, on=\"Id\", how=\"left\")\n\ntest_df = df[df[\"is_train\"] != True].copy()\ndf = df[df[\"is_train\"] == True].copy()\n\ndf.shape, test_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating FOLDS\n\ndf = df.dropna().reset_index(drop=True)\ndf[\"kfold\"] = -1\n\ndf = df.sample(frac=1,random_state=2020).reset_index(drop=True)\n\nkf = KFold(n_splits=NUM_FOLDS)\n\nfor fold, (trn_, val_) in enumerate(kf.split(X=df, y=df)):\n    df.loc[val_, 'kfold'] = fold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Giving less importance to FNC features since they are easier to overfit due to high dimensionality.\nFNC_SCALE = 1/500\n\ndf[fnc_features] *= FNC_SCALE\ntest_df[fnc_features] *= FNC_SCALE","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = TabNetRegressor(n_d=16,\n                       n_a=16,\n                       n_steps=4,\n                       gamma=1.9,\n                       n_independent=4,\n                       n_shared=5,\n                       seed=seed,\n                       optimizer_fn = torch.optim.Adam,\n                       scheduler_params = {\"milestones\": [150,250,300,350,400,450],'gamma':0.2},\n                       scheduler_fn=torch.optim.lr_scheduler.MultiStepLR)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Engine","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test = np.zeros((test_df.shape[0],len(target_features), NUM_FOLDS))  #A 3D TENSOR FOR STORING RESULTS OF ALL FOLDS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run(fold):\n    df_train = df[df.kfold != fold]\n    df_valid = df[df.kfold == fold]\n    \n    X_train = df_train[features].values\n    Y_train = df_train[target_features].values\n    \n    X_valid = df_valid[features].values\n    Y_valid = df_valid[target_features].values\n    \n    y_oof = np.zeros((df_valid.shape[0],len(target_features)))   # Out of folds validation\n    \n    print(\"--------Training Begining for fold {}-------------\".format(fold+1))\n     \n    model.fit(X_train = X_train,\n             y_train = Y_train,\n             X_valid = X_valid,\n             y_valid = Y_valid,\n             max_epochs = 1000,\n             patience =70)\n              \n    \n    print(\"--------Validating For fold {}------------\".format(fold+1))\n    \n    y_oof = model.predict(X_valid)\n    y_test[:,:,fold] = model.predict(test_df[features].values)\n    \n    val_score = metric(Y_valid,y_oof)\n    \n    print(\"Validation score: {:<8.5f}\".format(val_score))\n    \n    # VISUALIZTION\n    plt.figure(figsize=(12,6))\n    plt.plot(model.history['train']['loss'])\n    plt.plot(model.history['valid']['loss'])\n    \n    #Plotting Metric\n    #plt.plot([-x for x in model.history['train']['metric']])\n    #plt.plot([-x for x in model.history['valid']['metric']])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### I am hiding the output of training please unhide the output to look at the results and Loss plots for any fold","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"run(fold=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"run(fold=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"run(fold=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"run(fold=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"run(fold=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"run(fold=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"run(fold=6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating Submission","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test = y_test.mean(axis=-1) # Taking mean of all the fold predictions\ntest_df[target_features] = y_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = test_df[[\"Id\", \"age\", \"domain1_var1\", \"domain1_var2\", \"domain2_var1\", \"domain2_var2\"]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df = pd.melt(test_df, id_vars=[\"Id\"], value_name=\"Predicted\")\nsub_df[\"Id\"] = sub_df[\"Id\"].astype(\"str\") + \"_\" +  sub_df[\"variable\"].astype(\"str\")\n\nsub_df = sub_df.drop(\"variable\", axis=1).sort_values(\"Id\")\nassert sub_df.shape[0] == test_df.shape[0]*5\nsub_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# End Notes:\n* Tabnet allows us to have a greater control over training and predictions\n* With Tabnet we can integrate Image and Tabular data with some ideas\n* I have dropped the missing values in the targets and used raw data without any pre-processing/feature engineering ,etc\n* I would be glad to see interesting results if someone fine tunes  it further","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}