{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Investigation of the targets\n\nThe TReNDS dataset came with 5 different targets:\n- Age\n- Domain1: Var1 and Var2\n- Domain2: Var1 and Var2\n\nThe assumption is that the Var1 and Var2 in the wo domains are somehow related to each other. Either through type of measurement or something else.\n\nSo, before digging too much into feature engineering and modeling, I first investigated the target features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\nsns.set_context('notebook')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. Take a first look at the targets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the target features\ntargets = pd.read_csv('/kaggle/input/trends-assessment-prediction/train_scores.csv').set_index('Id')\ntargets.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# How many null values does each target have\nn_nulls = targets.isnull().sum()\ndisplay(n_nulls)\nn_nulls.plot.barh();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observation 1\n\nSo the first hunch is correct. Domain 1 and domain 2 seem to be connected, as they both contain the same amount of missing values.\n\nBefore we do any more explorations, let's get rid of these missing values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"targets.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"But how much are they connected? Let's look at a correlation plot:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(targets.corr()*100, square=True, annot=True, fmt='.0f');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observation 2\n\nInteresting, while there is a slight correlation within domain 2, there doesn't seem to be one within domain 1. So, the targets were measured in the same domain, but do not necessarily encode a connected property. Except for this domain2 connection... We will see more why.\n\nAlso, why do both var2 have a correlation with age? Haven't figured this one out yet.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Let's visualize the targets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"targets.plot(lw=0, marker='.', markersize=1, subplots=True, figsize=(14, 8));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Just looking at the targets, most seems to be normal. Except for age, something is off here... Let's have a closer look.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"targets['age'].plot(lw=0, marker='.', markersize=1, figsize=(14, 4));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observation 3\n\nAge seems to be discrete. Let's take a look at the unique values. How many unique values does this target contain?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"targets['age'].nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And how are they distributed over the target?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(targets['age'].sort_values().values);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Decision 1\n\nInteresting! So, if we try to predict this feature, it might be worth it to restrict the predictions to these 33 unique values.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Visualize target relationships\n\nLet's use a simple pairplot to see how the features vary together.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(targets, plot_kws=dict(s=5, alpha=0.5));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observation 4\n\nWe can see again the stratification of age, but what is more interesting is the relationship within domain2. What's happening there?","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Investigation of domain2","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(6, 6))\nd2 = targets.dropna().iloc[:, 3:].values\nplt.scatter(d2[:, 0], d2[:, 1], s=3);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Yes, something is definitely off. What about this strange line on the left? Also, is there some stratification happening here as well?\n\nLet's digg deeper. What about rotating these two features?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def rotate_origin(x, y, radians):\n    \"\"\"Rotates a point around the origin (0, 0).\"\"\"\n    xx = x * np.cos(radians) + y * np.sin(radians)\n    yy = -x * np.sin(radians) + y * np.cos(radians)\n    return np.array([xx, yy]).T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's rotate the domain 2 variables by 1 radian\nd2_rot = rotate_origin(d2[:, 0], d2[:, 1], 1)\nplt.figure(figsize=(6, 6))\nplt.scatter(d2_rot[:, 0], d2_rot[:, 1], s=3);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that does something, but how to best figure out what the best rotation is?\n\nHmm... it seems there are some lines in the data with respect to var2. So, let's rotate different values and see if we can find these lines.\n\nThe idea, rotate the points and check for the number of unique values with respect to var2. The implemetation is very sensitive to the right target region:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's explore between 0.85 and 0.95 radians\nn_uniques = []\nfor r in np.linspace(0.85, 0.95, 5000):\n    d22_rot = rotate_origin(d2[:, 0], d2[:, 1], r)[:, 1]\n    n_uniques.append([r, len(np.unique(np.round(d22_rot, 6)))])\nn_uniques = np.array(n_uniques)\n\nplt.figure(figsize=(14, 2))\nplt.scatter(n_uniques[:, 0], n_uniques[:, 1], s=3);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is something there. Let's enhance!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's explore between 0.905 and 0.910 radians\nn_uniques = []\nfor r in np.linspace(0.905, 0.910, 5000):\n    d22_rot = rotate_origin(d2[:, 0], d2[:, 1], r)[:, 1]\n    n_uniques.append([r, len(np.unique(np.round(d22_rot, 6)))])\nn_uniques = np.array(n_uniques)\n\nplt.figure(figsize=(14, 2))\nplt.scatter(n_uniques[:, 0], n_uniques[:, 1], s=3);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And repeat, and repeat and repeat...","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"d2_rot","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's explore between 0.90771 and 0.907715 radians\nn_uniques = []\nfor r in np.linspace(0.90771, 0.907715, 5000):\n    d22_rot = rotate_origin(d2[:, 0], d2[:, 1], r)[:, 1]\n    n_uniques.append([r, len(np.unique(np.round(d22_rot, 6)))])\nn_uniques = np.array(n_uniques)\n\nplt.figure(figsize=(14, 2))\nplt.scatter(n_uniques[:, 0], n_uniques[:, 1], s=3);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After a long search, we find that `radians=0.90771256655` does the trick.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rot = 0.90771256655\n\nd22_rot = rotate_origin(d2[:, 0], d2[:, 1], rot)[:, 1]\nn_unique_entries = len(np.unique(np.round(d22_rot, 6)))\n\nprint('Optimal rotation leads to %d unique entries on domain2_var2' % n_unique_entries)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And what does this look like?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's rotate the domain 2 variables by 1 radian\nd2_rot = rotate_origin(d2[:, 0], d2[:, 1], rot)\nplt.figure(figsize=(8, 8))\nplt.scatter(d2_rot[:, 0], d2_rot[:, 1], s=2);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observation 5\n\nThere seems to be a \"perfect\" rotation for domain2. I've tried to further investigate this (e.g. the \"outliers\" on the left or on the top\" or finding the offset to \"align the features with respect to var 2. But this never worked, as it seems to have some internal overlap of grading systems.\n\nWe can see this once we some in a bit.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's rotate the domain 2 variables by 1 radian\nd2_rot = rotate_origin(d2[:, 0], d2[:, 1], rot)\nplt.figure(figsize=(8, 8))\nplt.scatter(d2_rot[:, 0], d2_rot[:, 1], s=4)\nplt.xlim(40, 100)\nplt.ylim(-30, 0);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, let's add these rotated features to the other five targets. And plot the correlation matrix once more.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"targets.loc[:, 'd21_rot'] = d2_rot[:, 0]\ntargets.loc[:, 'd22_rot'] = d2_rot[:, 1]\n\nsns.heatmap(targets.corr()*100, square=True, annot=True, fmt='.0f');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"While this is super interesting, I unfortunately was never able to truely profit from this insight. As the prediction of these rotated features need to be super exact so that the inverse rotation leads to useful target values. I've tried some NeuralNets with multi loss functions to predict domain2 targets with and without rotation, but wasn't able to make it work. But more about this in another post.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"For the curious ones, here's also the \"step\" plot for domain2_var2 after rotation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(targets['d22_rot'].sort_values().values);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## What about the value distributions in general?\n\nA target is often best predicted if it follows a gaussian distribution, so let's take a look at the distributions.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import norm\nfor col in targets.columns:\n    plt.figure(figsize=(8, 2))\n    sns.distplot(targets[col], fit=norm, kde=True)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observation 6\n\nAge seems to be normal distributed, while the others might profit from a transformation. Using a similiar iterative approach as for finding the optimal rotation, I tried to find the optimal power transformation for all features:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pow_age = 0.93\npow_d1v1 = 1.25\npow_d1v2 = 1.72\npow_d2v1 = 1.37\npow_d2v2 = 1.39\npow_d21 = 1.85\npow_d22 = 1\n\npowers = [pow_age, pow_d1v1, pow_d1v2, pow_d2v1, pow_d2v2, pow_d21, pow_d22 ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import norm\nfor i, col in enumerate(targets.columns):\n    plt.figure(figsize=(8, 2))\n    sns.distplot(np.power(targets[col], powers[i]), fit=norm, kde=True)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"While this looks all great and nice, this approach turned out to lead to worse results (probably due to overfitting). What I've ended up with is a simple transformation of 1.5 for target 1 to 4:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pow_age = 1.0\npow_d1v1 = 1.5\npow_d1v2 = 1.5\npow_d2v1 = 1.5\npow_d2v2 = 1.5\npow_d21 = 1.5\npow_d22 = 1\n\npowers = [pow_age, pow_d1v1, pow_d1v2, pow_d2v1, pow_d2v2, pow_d21, pow_d22 ]\n\nfrom scipy.stats import norm\nfor i, col in enumerate(targets.columns):\n    plt.figure(figsize=(8, 2))\n    sns.distplot(np.power(targets[col], powers[i]), fit=norm, kde=True)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"### Decision 2\n\nThe 5 target features were transformed with a power of 1, 1.5, 1.5, 1.5, 1.5.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Decision 3\n\nAt the very end, these targets were also scaled using a standard scaler.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}