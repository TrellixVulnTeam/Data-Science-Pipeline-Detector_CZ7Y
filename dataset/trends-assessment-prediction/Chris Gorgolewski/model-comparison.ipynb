{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\n!cp ../input/rapids/rapids.0.13.0 /opt/conda/envs/rapids.tar.gz\n!cd /opt/conda/envs/ && tar -xzvf rapids.tar.gz > /dev/null\nsys.path = [\"/opt/conda/envs/rapids/lib/python3.6/site-packages\"] + sys.path\nsys.path = [\"/opt/conda/envs/rapids/lib/python3.6\"] + sys.path\nsys.path = [\"/opt/conda/envs/rapids/lib\"] + sys.path\n!cp /opt/conda/envs/rapids/lib/libxgboost.so /opt/conda/lib/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport cudf","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_scores_df = pd.read_csv(\"/kaggle/input/trends-assessment-prediction/train_scores.csv\", index_col='Id')\ntrain_scores_df['is_train'] = True\ntrain_scores_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loading_df = pd.read_csv(\"/kaggle/input/trends-assessment-prediction/loading.csv\", index_col='Id')\nloading_features = list(loading_df.columns.values)\nfnc_df = pd.read_csv(\"/kaggle/input/trends-assessment-prediction/fnc.csv\", index_col='Id')\nfnc_features = list(fnc_df.columns.values)\nfeatures_df = loading_df.join(fnc_df)\n#dropping IC_20 since it shows large site effects\nall_features = list(set(features_df.columns.values) - set(['IC_20']))\nfeatures_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_df = train_scores_df.merge(features_df, how='outer', left_index=True, right_index=True)\nprint(combined_df.shape)\ncombined_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_df.is_train.fillna(False, inplace=True)\ncombined_df.is_train.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import make_scorer\ndef MAPE(true, predicted, **kwargs):\n    absolute_error = np.abs(predicted-true).sum()\n    normalized_error = absolute_error/true.sum()\n    return normalized_error\n\nMAPE_scorer = make_scorer(MAPE, greater_is_better=False)\n\ndef weighted_absolute_error(true, predicted, weight):\n    return weight*MAPE(true, predicted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.dummy import DummyRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom cuml import SVR as cuSVR\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import KFold\n\nkf = KFold(n_splits=5, shuffle=True, random_state=0)\nlabel_weights = {'age': .3, \n                 'domain1_var1': .175, \n                 'domain1_var2': .175, \n                 'domain2_var1': .175, \n                 'domain2_var2': .175}\n\ntrain_and_val_df = combined_df[combined_df.is_train]\ntest_df = combined_df[~combined_df.is_train]\n\ndef evaluate_model(model_fn, model_name):\n    fold_scores = []\n    for fold_i, (train_index, val_index) in enumerate(kf.split(train_and_val_df)):\n        weighted_absolute_errors = []\n        for label in label_weights.keys():\n            train_df = train_and_val_df.iloc[train_index]\n            train_df = train_df[~train_df[label].isna()]\n\n            val_df = train_and_val_df.iloc[val_index]\n            val_df = val_df[~val_df[label].isna()]\n\n            preprocess, model = model_fn()\n            preprocess.fit(train_df, train_df[label])\n            \n            model.fit(np.asfortranarray(preprocess.transform(train_df)), \n                      np.array(train_df[label].values, order='F'))\n            \n            predicted = np.array(model.predict(np.asfortranarray(preprocess.transform(val_df))))\n\n            weighted_absolute_errors.append(weighted_absolute_error(true=val_df[label], \n                                                                    predicted=predicted,\n                                                                    weight=label_weights[label]))\n        score = sum(weighted_absolute_errors)\n        print(\"[%s] Fold %d, score: %f\"%(model_name, fold_i, score))\n        fold_scores.append(score)\n\n    print(\"[%s] Average score: %f\"%(model_name, np.mean(fold_scores)))\n\ndef create_dummy_model():\n    model = DummyRegressor('median')\n    preprocess = Pipeline([\n        ('union', ColumnTransformer([('scale', RobustScaler(), loading_features)]))\n    ])\n    return preprocess, model\n          \nevaluate_model(create_dummy_model, 'median')\n\ndef create_linear_regression_model():\n    regressor = LinearRegression()\n    pipeline = Pipeline([\n        ('union', ColumnTransformer([('scale', RobustScaler(), loading_features)])),\n        (\"dummy_regressor\", regressor)\n    ])\n    return pipeline\n        \n#evaluate_model(create_linear_regression_model, 'linear')\n\ndef create_gb_regression_model():\n    regressor = GradientBoostingRegressor()\n    pipeline = Pipeline([\n        ('union', ColumnTransformer([('scale', RobustScaler(), loading_features)])),\n        (\"dummy_regressor\", regressor)\n    ])\n    return pipeline\n\n#evaluate_model(create_gb_regression_model, 'gb')\n          \ndef create_sv_regression_model():\n    regressor = SVR()\n    pipeline = Pipeline([\n        ('union', ColumnTransformer([('scale', RobustScaler(), loading_features)])),\n        (\"dummy_regressor\", regressor)\n    ])\n    return pipeline\n\n#evaluate_model(create_sv_regression_model, 'sv')\n\ndef create_cusvr_model():\n    model = cuSVR(cache_size=3000.0)\n    preprocess = Pipeline([\n        ('union', ColumnTransformer([('scale', RobustScaler(), loading_features)]))\n    ])\n    return preprocess, model\n          \nevaluate_model(create_cusvr_model, 'cuSVR')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.model_selection import GridSearchCV\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\ndef model_factory(C, factor):\n    def create_svr_model_v2():\n        def down_scale(x):\n            return x/factor\n\n        PREPROCESSING_STAGE_NAME = 'preprocessing'\n        REGRESSION_STAGE_NAME = 'regression'\n        preprocess = Pipeline([\n            (PREPROCESSING_STAGE_NAME, \n             ColumnTransformer([('down_scale_fnc', FunctionTransformer(down_scale), fnc_features),\n                                ('scale_others', 'passthrough', loading_features)])),\n        ])\n\n        model = cuSVR(C=C, cache_size=3000.0)\n        return preprocess, model\n    \n    return create_svr_model_v2\n\nfor C in [1, 10, 100]:\n    for factor in [200.0, 250.0, 500.0]:\n        evaluate_model(model_factory(C, factor), \"C=%f, factor=%f\"%(C,factor))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_selection import SelectKBest, f_regression\nfrom sklearn.preprocessing import StandardScaler\n\n\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\ndef model_factory_v3(C, k):\n\n    def create_svr_model_v3():\n        PREPROCESSING_STAGE_NAME = 'preprocessing'\n        preprocess = Pipeline([\n            (PREPROCESSING_STAGE_NAME, \n             ColumnTransformer([('top_k', SelectKBest(f_regression, k=k), fnc_features),\n                                ('others', 'passthrough', loading_features)])),\n            ('scaling', StandardScaler())\n        ])\n\n        model = cuSVR(C=C, cache_size=3000.0)\n        return preprocess, model\n    \n    return create_svr_model_v3\n\nfor C in [1, 10, 100]:\n    for k in [10, 100, 200]:\n        evaluate_model(model_factory_v3(C, k), \"C=%f, k=%f\"%(C,k))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_selection import SelectKBest, f_regression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\ndef model_factory_v4(C, k):\n\n    def create_svr_model_v4():\n        PREPROCESSING_STAGE_NAME = 'preprocessing'\n        preprocess = Pipeline([\n            (PREPROCESSING_STAGE_NAME, \n             ColumnTransformer([('top_k', PCA(n_components=k), fnc_features),\n                                ('others', 'passthrough', loading_features)])),\n            ('scaling', StandardScaler())\n        ])\n\n        model = cuSVR(C=C, cache_size=3000.0)\n        return preprocess, model\n    \n    return create_svr_model_v4\n\nfor C in [1, 10, 100]:\n    for k in [10, 100, 200]:\n        evaluate_model(model_factory_v4(C, k), \"C=%f, k=%f\"%(C,k))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_selection import SelectKBest, f_regression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\ndef model_factory_v5(C, k, factor):\n\n    def create_svr_model_v5():\n        def down_scale(x):\n            return x/factor\n\n        PREPROCESSING_STAGE_NAME = 'preprocessing'\n        preprocess = Pipeline([\n            (PREPROCESSING_STAGE_NAME, \n             ColumnTransformer([('top_k', Pipeline([('PCA', PCA(n_components=k)), \n                                                    ('scale', FunctionTransformer(down_scale))]), fnc_features),\n                                ('others', 'passthrough', loading_features)]))\n        ])\n\n        model = cuSVR(C=C, cache_size=3000.0)\n        return preprocess, model\n    \n    return create_svr_model_v5\n\nfor C in [10, 20]:\n    for k in [200, 300, 400]:\n        for factor in [250]:\n            evaluate_model(model_factory_v5(C, k, factor), \"C=%f, k=%f, factor=%f\"%(C,k, factor))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for C in [5, 10, 15]:\n    for k in [500, 600, 700]:\n        for factor in [250, 350]:\n            evaluate_model(model_factory_v5(C, k, factor), \"C=%f, k=%f, factor=%f\"%(C,k, factor))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for C in [15]:\n    for k in [800, 900]:\n        for factor in [450, 550]:\n            evaluate_model(model_factory_v5(C, k, factor), \"C=%f, k=%f, factor=%f\"%(C,k, factor))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output_dfs = []\nfor label in label_weights.keys():\n    train_df = train_and_val_df\n    train_df = train_df[~train_df[label].isna()]\n\n    preprocess, model = model_factory_v5(C=10.000000, k=700, factor=250.0)()\n    preprocess.fit(train_df, train_df[label])\n\n    model.fit(np.asfortranarray(preprocess.transform(train_df)), \n              np.array(train_df[label].values, order='F'))\n\n    predicted = np.array(model.predict(np.asfortranarray(preprocess.transform(test_df))))\n\n    output_dfs.append(pd.DataFrame(predicted, index=test_df.index, columns=[label]))\n\noutput_df = pd.concat(output_dfs, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"melt_df = output_df.reset_index().melt(id_vars=['Id'], \n                                         value_vars=['age', 'domain1_var1', 'domain1_var2', 'domain2_var1', 'domain2_var2'],\n                                         value_name='Predicted')\nmelt_df['Id'] = melt_df['Id'].astype('str') + '_' + melt_df['variable']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"melt_df.to_csv(\"submission.csv\", index=False, columns=['Id', 'Predicted'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!head submission.csv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}