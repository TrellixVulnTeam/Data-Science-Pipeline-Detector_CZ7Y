{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Simple Baseline (not using 3D spatial map)\n* NN model using fnc and loading\n* optimize weighted normalized absolute errors directly\n* 5-fold averaging"},{"metadata":{},"cell_type":"markdown","source":"## prepare"},{"metadata":{},"cell_type":"markdown","source":"### import"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from pathlib import Path\nfrom typing import List, Tuple, Dict, Union\nfrom functools import partial\n\nimport time\nimport os\nimport sys\nimport random\nimport shutil\n\nfrom sklearn.model_selection import KFold\n\nimport numpy as np \nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import chainer\nfrom chainer import links, functions\nfrom chainer import datasets, iterators, optimizers, serializers\nfrom chainer import training, reporter, cuda\nfrom chainercv.links import PickableSequentialChain","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### definition"},{"metadata":{},"cell_type":"markdown","source":"#### model"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"class LinearActiv(chainer.Chain):\n    \"\"\"linear -> activation (-> batch norm -> dropout)\"\"\"\n\n    def __init__(\n        self, in_size: int, out_size: int,\n        dropout_rate=None, use_bn=False, activ=functions.relu\n    ) -> None:\n        \"\"\"Initialize.\"\"\"\n        super(LinearActiv, self).__init__()\n        layers = chainer.Sequential(links.Linear(in_size, out_size))\n        if activ is not None:\n            layers.append(activ)\n        if use_bn:\n            layers.append(links.BatchNormalization(out_size))\n        if dropout_rate is not None:\n            layers.append(partial(functions.dropout, ratio=dropout_rate))\n\n        with self.init_scope():\n            self.la = layers\n\n    def __call__(self, x: chainer.Variable) -> chainer.Variable:\n        \"\"\"Forward.\"\"\"\n        return self.la(x)\n    \n\nclass MLP(chainer.Chain):\n    \"\"\"Multi Layer Perceptron.\"\"\"\n\n    def __init__(\n        self, in_dim: int, hidden_dims: List[int],\n        drop_rates: List[float]=None, use_bn=False, use_tail_as_out=True\n    ) -> None:\n        \"\"\"initialize.\"\"\"\n        super(MLP, self).__init__()\n        hidden_dims = [in_dim] + hidden_dims\n        drop_rates = [None] * len(hidden_dims) if drop_rates is None else drop_rates\n        layers = [\n            LinearActiv(\n                hidden_dims[i], hidden_dims[i + 1], drop_rates[i], use_bn)\n            for i in range(len(hidden_dims) - 2)]\n\n        if use_tail_as_out:\n            layers.append(links.Linear(hidden_dims[-2], hidden_dims[-1]))\n        else:\n            layers.append(\n                LinearActiv(\n                    hidden_dims[-2], hidden_dims[-1], drop_rates[-1], use_bn))\n\n        with self.init_scope():\n            self.layers = chainer.Sequential(*layers)\n\n    def __call__(self, x: chainer.Variable) -> chainer.Variable:\n        \"\"\"Forward.\"\"\"\n        return self.layers(x)\n\n\nclass CustomMLP(chainer.Chain):\n    \"\"\"Simple MLP model.\"\"\"\n    \n    def __init__(\n        self, left_mlp: MLP, right_mlp: MLP, tail_mlp: MLP\n    ) -> None:\n        \"\"\"Initialize.\"\"\"\n        super(CustomMLP, self).__init__()\n        with self.init_scope():\n            self.left = left_mlp\n            self.right = right_mlp\n            self.tail = tail_mlp\n        \n    def __call__(self, x_left: chainer.Variable, x_right: chainer.Variable) -> chainer.Variable:\n        \"\"\"Forward.\"\"\"\n        h_left = self.left(x_left)\n        h_right = self.right(x_right)\n        h = functions.concat([h_left, h_right])\n        h = self.tail(h)\n        return h","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### wrapper"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"class Regressor(links.Classifier):\n    \"\"\"Wrapper for regression model.\"\"\"\n\n    def __init__(\n        self, predictor, lossfun, evalfun_dict\n    ):\n        \"\"\"Initialize\"\"\"\n        super(Regressor, self).__init__(predictor, lossfun)\n        self.compute_accuracy = False\n        self.evalfun_dict = evalfun_dict\n        for name, func in self.evalfun_dict.items():\n            setattr(self, name, None)\n            \n    def evaluate(self, *in_arrs: Tuple[chainer.Variable]) -> None:\n        \"\"\"Calc loss and evaluation metric.\"\"\"\n        for name in self.evalfun_dict.keys():\n            setattr(self, name, None)\n        loss = self(*in_arrs)\n\n        for name, evalfun in self.evalfun_dict.items():\n            setattr(self, name, evalfun(self.y, in_arrs[-1]))\n            reporter.report({name: getattr(self, name)}, self)\n        del loss","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### loss "},{"metadata":{"trusted":true},"cell_type":"code","source":"def normalized_absolute_error(y_pred: chainer.Variable, t: np.ndarray):\n    \"\"\"\n    \\sum_{i} |y_pred_{i} - t_{i}| / \\sum_{i} t_{i}\n    \"\"\"\n    return functions.sum(functions.absolute(y_pred - t)) / functions.sum(t)\n\n\nclass WeightedNormalizedAbsoluteError:\n    \"\"\"Metric for this competition\"\"\"\n    \n    def __init__(self, weights: List[float]=[.3, .175, .175, .175, .175]):\n        \"\"\"Initialize.\"\"\"\n        self.weights = weights\n        self.pred_num = len(weights)\n        \n    def __call__(self, y_pred: chainer.Variable, t: np.ndarray) ->  chainer.Variable:\n        \"\"\"Forward.\"\"\"\n        loss = 0\n        for i, weight in enumerate(self.weights):\n            loss += weight * normalized_absolute_error(y_pred[:, i], t[:, i])\n            \n        return loss\n    \n\nclass SelectNormalizedAbsoluteError:\n    \"\"\"For checking each features loss\"\"\"\n    \n    def __init__(self, index: int):\n        \"\"\"Initialize.\"\"\"\n        self.index = index\n        \n    def __call__(self, y_pred: chainer.Variable, t: np.ndarray) ->  chainer.Variable:\n        \"\"\"Forward.\"\"\"\n        return normalized_absolute_error(y_pred[:, self.index], t[:, self.index])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### others"},{"metadata":{"trusted":true},"cell_type":"code","source":"def set_random_seed(seed=42):\n    \"\"\"Fix Seeds.\"\"\"\n    # set Python random seed\n    random.seed(seed)\n\n    # set NumPy random seed\n    np.random.seed(seed)\n\n    # set Chainer(CuPy) random seed\n    cuda.cupy.random.seed(seed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### read data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"COMPETITION_NAME = \"trends-assessment-prediction\"\nROOT = Path(\".\").resolve().parents[0]\n\nINPUT_ROOT = ROOT / \"input\"\nRAW_DATA = INPUT_ROOT / COMPETITION_NAME\nTRAIN_IMAGES = RAW_DATA / \"fMRI_train\"\nTEST_IMAGES = RAW_DATA / \"fMRI_test\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fnc = pd.read_csv(RAW_DATA / \"fnc.csv\")\nicn_numbers = pd.read_csv(RAW_DATA / \"ICN_numbers.csv\")\nloading = pd.read_csv(RAW_DATA / \"loading.csv\")\nreveal_ID_site2 = pd.read_csv(RAW_DATA / \"reveal_ID_site2.csv\")\n\ntrain_scores = pd.read_csv(RAW_DATA / \"train_scores.csv\")\nsample_sub = pd.read_csv(RAW_DATA / \"sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_sub.shape[0] / 5 ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## training"},{"metadata":{},"cell_type":"markdown","source":"### init model"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# # init model\ndef init_model(is_train=True):\n    model = CustomMLP(\n        left_mlp=MLP(\n            in_dim=1378, hidden_dims=[1024, 768], drop_rates=[0.5, 0.5], use_tail_as_out=False),\n        right_mlp=MLP(\n            in_dim=26, hidden_dims=[64, 768], drop_rates=[0.5, 0.5], use_tail_as_out=False),\n        tail_mlp=MLP(\n            in_dim=1536, hidden_dims=[1024, 5], drop_rates=[0.5, 0.0]),\n    )\n    if not is_train:\n        return model\n\n    # # set trainning wrapper\n    train_model = Regressor(\n        predictor=model,\n        lossfun=WeightedNormalizedAbsoluteError(weights=[.3, .175, .175, .175, .175]),\n        evalfun_dict={\n            \"NAE_Age\": SelectNormalizedAbsoluteError(0),\n            \"NAE_Domain1Var1\": SelectNormalizedAbsoluteError(1),\n            \"NAE_Domain1Var2\": SelectNormalizedAbsoluteError(2),\n            \"NAE_Domain2Var1\": SelectNormalizedAbsoluteError(3),\n            \"NAE_Domain2Var2\": SelectNormalizedAbsoluteError(4)}\n    )\n    return train_model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### make trainer"},{"metadata":{"trusted":true},"cell_type":"code","source":"DEVICE = 0","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def create_trainer(train_model, train_dataset, val_dataset, output_dir, device):\n    # # set optimizer\n    optimizer = optimizers.AdamW(alpha=0.001, weight_decay_rate=0.0)\n    optimizer.setup(train_model)\n    \n    # # make iterator\n    train_iter = iterators.MultiprocessIterator(\n        train_dataset, 64, n_processes=2)\n    val_iter = iterators.MultiprocessIterator(\n        val_dataset, 64, repeat=False, shuffle=False, n_processes=2)\n    \n    # # init trainer\n    updater = training.StandardUpdater(train_iter, optimizer, device=device)\n\n    stop_trigger = training.triggers.EarlyStoppingTrigger(\n        check_trigger=(1, 'epoch'), monitor='val/main/loss', mode=\"min\",\n        patients=20, max_trigger=(200, 'epoch'), verbose=True)\n\n    trainer = training.trainer.Trainer(\n        updater, stop_trigger=stop_trigger, out=output_dir)\n    \n    # # set extentions\n    lr_attr_name = \"alpha\"\n    log_trigger = (1, \"epoch\")\n    logging_attributes = [\n        \"epoch\", \"elapsed_time\", \"main/loss\", \"val/main/loss\",\n        \"val/main/NAE_Age\",\n        \"val/main/NAE_Domain1Var1\", \"val/main/NAE_Domain1Var2\",\n        \"val/main/NAE_Domain2Var1\", \"val/main/NAE_Domain2Var2\"]\n\n    # # # evaluator\n    eval_target = trainer.updater.get_optimizer('main').target\n    trainer.extend(\n        training.extensions.Evaluator(\n            val_iter, eval_target, device=device, eval_func=eval_target.evaluate),\n        name='val',trigger=(1, 'epoch'))\n\n    # # # log.\n    trainer.extend(\n        training.extensions.observe_lr(observation_key=lr_attr_name), trigger=log_trigger)\n    trainer.extend(\n        training.extensions.LogReport(logging_attributes, trigger=log_trigger), trigger=log_trigger)\n    trainer.extend(training.extensions.PrintReport(logging_attributes), trigger=log_trigger)\n\n    # # # save snapshot\n    trainer.extend(\n        training.extensions.snapshot_object(\n            trainer.updater.get_optimizer('main').target.predictor,\n            'model_snapshot_{.updater.epoch}.npz'),\n        trigger=training.triggers.MinValueTrigger(\"val/main/loss\", (1, \"epoch\")))\n    \n    return trainer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### run trainer"},{"metadata":{"trusted":true},"cell_type":"code","source":"val_score_list = []","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### prepare data"},{"metadata":{"trusted":true},"cell_type":"code","source":"features = pd.merge(fnc,loading, on=\"Id\", how=\"inner\") \ntrain_all = train_scores.merge(features, on=\"Id\", how=\"left\")\n\n# #For convenience in trainning, fill NA by mean values. \n\nfor i in range(5):\n    train_all.iloc[:, i + 1] = train_all.iloc[:, i + 1].fillna(train_all.iloc[:, i + 1].mean())\n\nkf = KFold(n_splits=5, shuffle=True, random_state=1086)\ntrain_val_splits = list(kf.split(X=train_scores.Id))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### fold0"},{"metadata":{"trusted":true},"cell_type":"code","source":"fold_id = 0\ntrain_index, val_index = train_val_splits[fold_id]\ntrain = train_all.iloc[train_index]\nval = train_all.iloc[val_index]\n\ntrain_dataset = datasets.TupleDataset(\n    train.iloc[:, 6:1384].values.astype(\"f\"),  # fnc\n    train.iloc[:, 1384:].values.astype(\"f\"),  # loading\n    train.iloc[:, 1:6].values.astype(\"f\"),  # label\n)\nval_dataset = datasets.TupleDataset(\n    val.iloc[:, 6:1384].values.astype(\"f\"),  # fnc\n    val.iloc[:, 1384:].values.astype(\"f\"),  # loading\n    val.iloc[:, 1:6].values.astype(\"f\"),  # label\n)\nset_random_seed(1086)\ntrain_model = init_model()\ntrainer = create_trainer(train_model, train_dataset, val_dataset, \"training_result_fold{}\".format(fold_id), DEVICE)\ntrainer.run()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_epoch = trainer.updater.epoch - trainer.stop_trigger.count\nprint(best_epoch, trainer.stop_trigger.best)\nval_score_list.append([fold_id, trainer.stop_trigger.best, best_epoch,])\nshutil.copyfile(\n    \"training_result_fold{}/model_snapshot_{}.npz\".format(fold_id, best_epoch), \"best_model_fold{},npz\".format(fold_id))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### fold1"},{"metadata":{"trusted":true},"cell_type":"code","source":"fold_id = 1\ntrain_index, val_index = train_val_splits[fold_id]\ntrain = train_all.iloc[train_index]\nval = train_all.iloc[val_index]\n\ntrain_dataset = datasets.TupleDataset(\n    train.iloc[:, 6:1384].values.astype(\"f\"),  # fnc\n    train.iloc[:, 1384:].values.astype(\"f\"),  # loading\n    train.iloc[:, 1:6].values.astype(\"f\"),  # label\n)\nval_dataset = datasets.TupleDataset(\n    val.iloc[:, 6:1384].values.astype(\"f\"),  # fnc\n    val.iloc[:, 1384:].values.astype(\"f\"),  # loading\n    val.iloc[:, 1:6].values.astype(\"f\"),  # label\n)\nset_random_seed(1086)\ntrain_model = init_model()\ntrainer = create_trainer(train_model, train_dataset, val_dataset, \"training_result_fold{}\".format(fold_id), DEVICE)\ntrainer.run()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_epoch = trainer.updater.epoch - trainer.stop_trigger.count\nprint(best_epoch, trainer.stop_trigger.best)\nval_score_list.append([fold_id, trainer.stop_trigger.best, best_epoch,])\nshutil.copyfile(\n    \"training_result_fold{}/model_snapshot_{}.npz\".format(fold_id, best_epoch), \"best_model_fold{},npz\".format(fold_id))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fold_id = 2\ntrain_index, val_index = train_val_splits[fold_id]\ntrain = train_all.iloc[train_index]\nval = train_all.iloc[val_index]\n\ntrain_dataset = datasets.TupleDataset(\n    train.iloc[:, 6:1384].values.astype(\"f\"),  # fnc\n    train.iloc[:, 1384:].values.astype(\"f\"),  # loading\n    train.iloc[:, 1:6].values.astype(\"f\"),  # label\n)\nval_dataset = datasets.TupleDataset(\n    val.iloc[:, 6:1384].values.astype(\"f\"),  # fnc\n    val.iloc[:, 1384:].values.astype(\"f\"),  # loading\n    val.iloc[:, 1:6].values.astype(\"f\"),  # label\n)\nset_random_seed(1086)\ntrain_model = init_model()\ntrainer = create_trainer(train_model, train_dataset, val_dataset, \"training_result_fold{}\".format(fold_id), DEVICE)\ntrainer.run()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_epoch = trainer.updater.epoch - trainer.stop_trigger.count\nprint(best_epoch, trainer.stop_trigger.best)\nval_score_list.append([fold_id, trainer.stop_trigger.best, best_epoch,])\nshutil.copyfile(\n    \"training_result_fold{}/model_snapshot_{}.npz\".format(fold_id, best_epoch), \"best_model_fold{},npz\".format(fold_id))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fold_id = 3\ntrain_index, val_index = train_val_splits[fold_id]\ntrain = train_all.iloc[train_index]\nval = train_all.iloc[val_index]\n\ntrain_dataset = datasets.TupleDataset(\n    train.iloc[:, 6:1384].values.astype(\"f\"),  # fnc\n    train.iloc[:, 1384:].values.astype(\"f\"),  # loading\n    train.iloc[:, 1:6].values.astype(\"f\"),  # label\n)\nval_dataset = datasets.TupleDataset(\n    val.iloc[:, 6:1384].values.astype(\"f\"),  # fnc\n    val.iloc[:, 1384:].values.astype(\"f\"),  # loading\n    val.iloc[:, 1:6].values.astype(\"f\"),  # label\n)\nset_random_seed(1086)\ntrain_model = init_model()\ntrainer = create_trainer(train_model, train_dataset, val_dataset, \"training_result_fold{}\".format(fold_id), DEVICE)\ntrainer.run()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_epoch = trainer.updater.epoch - trainer.stop_trigger.count\nprint(best_epoch, trainer.stop_trigger.best)\nval_score_list.append([fold_id, trainer.stop_trigger.best, best_epoch,])\nshutil.copyfile(\n    \"training_result_fold{}/model_snapshot_{}.npz\".format(fold_id, best_epoch), \"best_model_fold{},npz\".format(fold_id))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fold_id = 4\ntrain_index, val_index = train_val_splits[fold_id]\ntrain = train_all.iloc[train_index]\nval = train_all.iloc[val_index]\n\ntrain_dataset = datasets.TupleDataset(\n    train.iloc[:, 6:1384].values.astype(\"f\"),  # fnc\n    train.iloc[:, 1384:].values.astype(\"f\"),  # loading\n    train.iloc[:, 1:6].values.astype(\"f\"),  # label\n)\nval_dataset = datasets.TupleDataset(\n    val.iloc[:, 6:1384].values.astype(\"f\"),  # fnc\n    val.iloc[:, 1384:].values.astype(\"f\"),  # loading\n    val.iloc[:, 1:6].values.astype(\"f\"),  # label\n)\nset_random_seed(1086)\ntrain_model = init_model()\ntrainer = create_trainer(train_model, train_dataset, val_dataset, \"training_result_fold{}\".format(fold_id), DEVICE)\ntrainer.run()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_epoch = trainer.updater.epoch - trainer.stop_trigger.count\nprint(best_epoch, trainer.stop_trigger.best)\nval_score_list.append([fold_id, trainer.stop_trigger.best, best_epoch,])\nshutil.copyfile(\n    \"training_result_fold{}/model_snapshot_{}.npz\".format(fold_id, best_epoch), \"best_model_fold{},npz\".format(fold_id))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(\n    val_score_list,\n    columns=[\"fold\", \"score\", \"best_epoch\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## inference"},{"metadata":{"trusted":true},"cell_type":"code","source":"def inference_test_data(\n    model: Union[chainer.Chain, PickableSequentialChain],\n    test_iter: chainer.iterators.MultiprocessIterator, gpu_device: int=-1\n) -> Tuple[np.ndarray]:\n    \"\"\"Oridinary Inference.\"\"\"\n    test_pred_list = []\n    test_label_list = []\n    iter_num = 0\n    epoch_test_start = time.time()\n\n    while True:\n        test_batch = test_iter.next()\n        iter_num += 1\n        print(\"\\rtmp_iteration: {:0>5}\".format(iter_num), end=\"\")\n        in_arrays = chainer.dataset.concat_examples(test_batch, gpu_device)\n\n        # Forward the test data\n        with chainer.no_backprop_mode() and chainer.using_config(\"train\", False):\n            prediction_test = model(*in_arrays[:-1])\n            test_pred_list.append(prediction_test)\n            test_label_list.append(in_arrays[-1])\n            prediction_test.unchain_backward()\n\n        if test_iter.is_new_epoch:\n            print(\" => test end: {:.2f} sec\".format(time.time() - epoch_test_start))\n            test_iter.reset()\n            break\n\n    test_pred_all = cuda.to_cpu(functions.concat(test_pred_list, axis=0).data)\n    test_label_all = cuda.to_cpu(functions.concat(test_label_list, axis=0).data)\n    del test_pred_list\n    del test_label_list\n    return test_pred_all, test_label_all","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### make dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.DataFrame({}, columns=train_scores.columns.tolist())\ntest[\"Id\"] = sample_sub[\"Id\"].apply(lambda x: int(x.split('_')[0])).unique()\ntest = test.fillna(-1)\ntest = test.merge(features, on=\"Id\", how=\"left\")\n\ntest_label = test.iloc[:, 1:6].values.astype(\"f\")\ntest_fnc = test.iloc[:, 6:1384].values.astype(\"f\")\ntest_loading = test.iloc[:, 1384:].values.astype(\"f\")\n\ntest_dataset = datasets.TupleDataset(test_fnc, test_loading, test_label)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds_arr = np.zeros((5, len(test), 5))\nfor fold_id in range(5):\n    test_iter = iterators.MultiprocessIterator(\n        test_dataset, 64, repeat=False, shuffle=False, n_processes=2)\n    model = init_model(is_train=False)\n    model.to_gpu(DEVICE)\n    serializers.load_npz(\"best_model_fold{},npz\".format(fold_id), model)\n    test_pred, _ = inference_test_data(model, test_iter, DEVICE)\n    test_preds_arr[fold_id] =  test_pred\n    del test_iter\n    del test_pred\n    del model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred = test_preds_arr.mean(axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## submit"},{"metadata":{"trusted":true},"cell_type":"code","source":"5877 * 5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_sub = test.iloc[:, :6].copy()\ntest_sub.iloc[:, 1:] = test_pred\n\ntest_sub = pd.melt(test_sub, id_vars=\"Id\", value_name=\"Predicted\")\ntest_sub[\"Id\"] = test_sub[\"Id\"].astype(\"str\") + \"_\" +  test_sub[\"variable\"]\n\ntest_sub = pd.merge(sample_sub[\"Id\"], test_sub[[\"Id\", \"Predicted\"]], how=\"left\")\ntest_sub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_sub.Predicted.isnull().value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_sub.tail()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}