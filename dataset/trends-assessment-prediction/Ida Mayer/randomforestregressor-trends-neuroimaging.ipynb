{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2021-06-30T11:40:37.211722Z","iopub.execute_input":"2021-06-30T11:40:37.212009Z","iopub.status.idle":"2021-06-30T11:40:37.217628Z","shell.execute_reply.started":"2021-06-30T11:40:37.211982Z","shell.execute_reply":"2021-06-30T11:40:37.216574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This data set comes from the TReNDS neuroimaging dataset on kaggle, which can be found at:\n# https://www.kaggle.com/c/trends-assessment-prediction\n\n# these are the independent variables\ndf = pd.read_csv(\"../input/trends-assessment-prediction/loading.csv\")\nfeatures = list(df.columns[1:])\n\n# these are the dependent variables\nlabels_df = pd.read_csv(\"../input/trends-assessment-prediction/train_scores.csv\")\n\ndf = df.merge(labels_df, on=\"Id\", how=\"left\")\ndf = df.dropna()\n","metadata":{"execution":{"iopub.status.busy":"2021-06-30T11:40:37.219608Z","iopub.execute_input":"2021-06-30T11:40:37.220082Z","iopub.status.idle":"2021-06-30T11:40:37.327032Z","shell.execute_reply.started":"2021-06-30T11:40:37.220042Z","shell.execute_reply":"2021-06-30T11:40:37.326308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# How a regression tree works: create a step function by learning the optimal boundary between steps\n# by minimizing sum of square residuals against every possible boundary in the data\n# then use the average value (or a linear regression?) for each step, to calculate the prediction of y\n# it's typical to truncate step splitting at steps of size <=20 to prevent overfitting\n\n# How a random forest works: a random forest is a collection of decision trees where the boundaries\n# are learned from only a randomly selected subset of k of the possible regressors  at each step,\n# on top of the bootstrapped data set. Then take an average of all the trees' predictions.\n\ntrain_df, test_df = train_test_split(df, test_size=0.33, shuffle=True)\n\n# Create the random forest regressor\n# a bootstrapped dataset is sample that was randomly sampled with replacement from some source dataset\nmodel = RandomForestRegressor(n_estimators=100, criterion=\"mse\", bootstrap = True)\nmodel.fit(train_df[features], train_df[\"age\"])\n\nprint(\"Accuracy score of Random Forest Regressor on age in training set (R2)\")\nprint(model.score(train_df[features], train_df[\"age\"]))","metadata":{"execution":{"iopub.status.busy":"2021-06-30T11:40:37.328377Z","iopub.execute_input":"2021-06-30T11:40:37.328661Z","iopub.status.idle":"2021-06-30T11:40:43.498365Z","shell.execute_reply.started":"2021-06-30T11:40:37.328628Z","shell.execute_reply":"2021-06-30T11:40:43.497399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now we can create predictions and evaluate our model\n\ny_predicted = model.predict(test_df[features])\n\nprint(\"Accuracy score of Random Forest Regressor on predicting age in test set (R2)\")\nprint(model.score(test_df[features], test_df[\"age\"]))","metadata":{"execution":{"iopub.status.busy":"2021-06-30T11:40:43.500227Z","iopub.execute_input":"2021-06-30T11:40:43.500448Z","iopub.status.idle":"2021-06-30T11:40:43.597188Z","shell.execute_reply.started":"2021-06-30T11:40:43.500422Z","shell.execute_reply":"2021-06-30T11:40:43.596019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# It would be nice to have a visual of how well we predicted ages, so convert our prediction to a\n# classification by decade of age, and plot of confusion matrix to show the accuracy of our prediction\ncm = metrics.confusion_matrix(test_df[\"age\"].round(-1).astype(int), y_predicted.round(-1).astype(int))\n\nplt.figure(figsize=(10,7))\nsns.heatmap(cm, annot=True, fmt = \"d\")\nplt.xlabel('Predicted (Age in decades)')\nplt.ylabel('Actual (Age in decades)')","metadata":{"execution":{"iopub.status.busy":"2021-06-30T11:40:43.598892Z","iopub.execute_input":"2021-06-30T11:40:43.599267Z","iopub.status.idle":"2021-06-30T11:40:44.048913Z","shell.execute_reply.started":"2021-06-30T11:40:43.599228Z","shell.execute_reply":"2021-06-30T11:40:44.048174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###########################################################################################################\n#                                     Collaborative Model Comparison                                      #\n# After completing our individual projects above, students who used the same kaggle dataset collaborated  #\n# to produce a model comparison between these different regression models, which can be shown below:      #\n#                                                                                                         #\n\n# run cross validation on my model\nseed = 7\nscoring = 'r2'\nkfold = model_selection.KFold(n_splits=10, random_state=seed)\ncv_results = model_selection.cross_val_score(model, df[features], df[\"age\"].astype(int), cv=kfold, scoring=scoring)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T11:40:44.049984Z","iopub.execute_input":"2021-06-30T11:40:44.050177Z","iopub.status.idle":"2021-06-30T11:42:07.976293Z","shell.execute_reply.started":"2021-06-30T11:40:44.050154Z","shell.execute_reply":"2021-06-30T11:42:07.975308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compare to the same cross validation produced by other models\n# Cross validated results were gathered for every model above and are summarized here:\nresults = [\n    [0.48038686, 0.44722758, 0.47229537, 0.43149557, 0.48267628, 0.47303389,\n       0.47288577, 0.43150015, 0.50556724, 0.51195019], # LinearRegression\n    [0.4330323 , 0.4415893 , 0.43576118, 0.41260509, 0.45900908,\n       0.44424805, 0.4300585 , 0.41139041, 0.47555739, 0.480079  ], #SVR\n    [0.5222,0.5109,0.5340,0.4915,0.5440,0.5819,0.5384,0.5855,0.5720,0.5368], #Ridge Regression\n    [0.43819714, 0.40940325, 0.45465441, 0.40138206, 0.43437831,\n       0.44386384, 0.42308454, 0.39606003, 0.48182481, 0.43259663], #RandomForestRegressor\n    [0.48602632, 0.50945147, 0.54282255, 0.51462939, 0.54125916,\n        0.50663771, 0.49722094, 0.45733866, 0.53932954, 0.56163937], # MLPRegressor\n    [0.02901306, 0.15196958, 0.08661957, 0.1940907 , 0.11258754,\n        0.11474175, 0.15555495, 0.10880699, 0.03113268, 0.22320099] # BaggingRegressor\n]\n\n# boxplot algorithm comparison\nfig = plt.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels([\"LR\", \"SVR\", \"Ridge\", \"RFR\", \"MLP\", \"BR\"])\nplt.show()\n\nprint(\"Based on this analysis we determine that the Ridge regression was the most successful at predicting age.\")","metadata":{"execution":{"iopub.status.busy":"2021-06-30T11:43:55.548824Z","iopub.execute_input":"2021-06-30T11:43:55.54911Z","iopub.status.idle":"2021-06-30T11:43:55.730727Z","shell.execute_reply.started":"2021-06-30T11:43:55.549065Z","shell.execute_reply":"2021-06-30T11:43:55.729988Z"},"trusted":true},"execution_count":null,"outputs":[]}]}