{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\n> This is my first kernel shared :)\n\n- The main idea of this kernel is to share some observations on the dataset and some recommandations on the cross-validation folders. \n- The Pipeline is: firstly use FaceNet to have features extracted, then use these features to train a traditional machine learning model, and use this model to predict on test set.\n\nThe FaceNet idea was inspired by [Khoi Nguyen](https://www.kaggle.com/suicaokhoailang) and his [kernel](https://www.kaggle.com/suicaokhoailang/facenet-baseline-in-keras-0-749-lb). \n\nI am new to Deep Learning, so firstly I only used deep net as feature extractor, then use traditional way to train the model. Here we can use other deep net, like VGGFace, to replace FaceNet as feature extractor. After feature extraction, we can test on very different traditional machine learning models.","metadata":{}},{"cell_type":"markdown","source":"# Load useful libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom imageio import imread\nfrom skimage.transform import resize\n\nfrom keras.models import load_model\nfrom tqdm._tqdm_notebook import tqdm_notebook\nfrom sklearn.model_selection import GroupKFold\n\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-18T05:52:04.754563Z","iopub.execute_input":"2022-01-18T05:52:04.754904Z","iopub.status.idle":"2022-01-18T05:52:10.349171Z","shell.execute_reply.started":"2022-01-18T05:52:04.75481Z","shell.execute_reply":"2022-01-18T05:52:10.348443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\nwarnings.filterwarnings('ignore') #this one works good!","metadata":{"execution":{"iopub.status.busy":"2022-01-18T05:52:10.350695Z","iopub.execute_input":"2022-01-18T05:52:10.351265Z","iopub.status.idle":"2022-01-18T05:52:10.358342Z","shell.execute_reply.started":"2022-01-18T05:52:10.351225Z","shell.execute_reply":"2022-01-18T05:52:10.357604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Get image pairs\n\nFind all the possible image pairs for training.","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(\"../input/recognizing-faces-in-the-wild/train_relationships.csv\")\ntrain_df.head()","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.status.busy":"2022-01-18T05:52:10.359694Z","iopub.execute_input":"2022-01-18T05:52:10.360037Z","iopub.status.idle":"2022-01-18T05:52:10.391418Z","shell.execute_reply.started":"2022-01-18T05:52:10.35998Z","shell.execute_reply":"2022-01-18T05:52:10.390722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Find all the train images\n\ndef findAllTrain(train_folder):\n    train_li=[]\n    for fam in os.listdir(train_folder):\n        for pers in os.listdir(os.path.join(train_folder,fam)):\n            for pic in os.listdir(os.path.join(train_folder,fam,pers)):\n                train_li.append(os.path.join(fam,pers,pic))\n    \n    return train_li\n\ntrain_fd = '../input/dataconvert/train'\n\ntrain_file_li=findAllTrain(train_fd)\n\nprint('There are {} images in the train dataset.'.\n      format(len(train_file_li)))\n\n#Create a dict to store all the train images\ntrain_file_dict=dict(zip(train_file_li,range(len(train_file_li))))\n\n# Create a DataFrame to store all the train images\ntrain_file_df = pd.DataFrame()\ntrain_file_df['image_fp']=train_file_li\ntrain_file_df.sample(5)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T05:52:10.392605Z","iopub.execute_input":"2022-01-18T05:52:10.392981Z","iopub.status.idle":"2022-01-18T05:52:18.943655Z","shell.execute_reply.started":"2022-01-18T05:52:10.392945Z","shell.execute_reply":"2022-01-18T05:52:18.942912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Find all the image pairs with kinship\n\ntrain_fd = '../input/dataconvert/train'\n\nindex_p1_li=[]\nindex_p2_li=[]\n\nfor idx, row in tqdm_notebook(train_df.iterrows(), total=len(train_df)):\n    if os.path.isdir(os.path.join(train_fd,row['p1'])) and os.path.isdir(os.path.join(train_fd,row['p2'])): # some folders do not exist !!\n        for p1_pic in os.listdir(os.path.join(train_fd,row['p1'])):\n            for p2_pic in os.listdir(os.path.join(train_fd,row['p2'])):\n                index_f1=train_file_dict[os.path.join(row['p1'].split('/')[0],row['p1'].split('/')[1],p1_pic)]\n                index_f2=train_file_dict[os.path.join(row['p2'].split('/')[0],row['p2'].split('/')[1],p2_pic)]\n                if index_f1<index_f2: # force the image pairs to have the same order of persons\n                    index_p1_li.append(index_f1)\n                    index_p2_li.append(index_f2)\n                else:\n                    index_p1_li.append(index_f2)\n                    index_p2_li.append(index_f1)\n                    \ntrain_pairs_kinship=pd.DataFrame()\ntrain_pairs_kinship['p1']=index_p1_li\ntrain_pairs_kinship['p2']=index_p2_li\n\nindex_p1_li=[]\nindex_p2_li=[]\n\nprint('Total image pairs with kinship: {}'.format(len(train_pairs_kinship)))\ntrain_pairs_kinship.sample(5)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T05:52:18.945954Z","iopub.execute_input":"2022-01-18T05:52:18.946217Z","iopub.status.idle":"2022-01-18T05:52:33.486993Z","shell.execute_reply.started":"2022-01-18T05:52:18.946183Z","shell.execute_reply":"2022-01-18T05:52:33.486256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**How about the image pairs from the same person? Should they be used as positive samples (with kinship)? I think YES.**\n\nBecause basicly we are training a model to identify the similarity of two images, the same person's images can bring us more postive samples. ","metadata":{}},{"cell_type":"code","source":"# make image pairs of the same person\n# for example: for this person \"F0002\\MID1\", there are 10 images in the folder, so it can make 10*9/2=45 pairs.\n\ndef make_pair_same_person(source,pre_path):\n    res_p1_li = []\n    res_p2_li = []\n    for p1 in range(len(source)):\n        for p2 in range(p1+1,len(source)):\n            index_f1=train_file_dict[os.path.join(pre_path,source[p1])]\n            index_f2=train_file_dict[os.path.join(pre_path,source[p2])]\n            if index_f1<index_f2: # force the image pairs to have the same order of persons\n                res_p1_li.append(index_f1)\n                res_p2_li.append(index_f2)\n            else:\n                res_p1_li.append(index_f2)\n                res_p2_li.append(index_f1)\n            \n    return (res_p1_li,res_p2_li)\n\nindex_p1_li = []\nindex_p2_li = []\nfor fam in os.listdir(train_fd):\n    for pers in os.listdir(os.path.join(train_fd,fam)):\n        res_temp = make_pair_same_person([pic for pic in os.listdir(os.path.join(train_fd,fam,pers))],os.path.join(fam,pers))\n        index_p1_li.extend(res_temp[0])\n        index_p2_li.extend(res_temp[1])\n\ntrain_pairs_same=pd.DataFrame()\ntrain_pairs_same['p1']=index_p1_li\ntrain_pairs_same['p2']=index_p2_li\n\nindex_p1_li = []\nindex_p2_li = []\n\nprint('Total image pairs of same person: {}'.format(len(train_pairs_same)))\n\ntrain_pairs_same.sample(5)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T05:52:33.488137Z","iopub.execute_input":"2022-01-18T05:52:33.488758Z","iopub.status.idle":"2022-01-18T05:52:34.808995Z","shell.execute_reply.started":"2022-01-18T05:52:33.488719Z","shell.execute_reply":"2022-01-18T05:52:34.808321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(12,2))\ny2show=[len(train_pairs_kinship),len(train_pairs_same)]\nplt.barh(range(2),y2show,0.35)\nplt.title('Image pair number')\nplt.yticks(range(2), ('With kinship', 'From same person'),)\nplt.box(on=None)\nplt.xticks([], [])\nfor i, v in enumerate(y2show):\n    ax.text(v+1000, i-0.05, str(v), color='blue', fontweight='bold')","metadata":{"execution":{"iopub.status.busy":"2022-01-18T05:52:34.811161Z","iopub.execute_input":"2022-01-18T05:52:34.811483Z","iopub.status.idle":"2022-01-18T05:52:34.926955Z","shell.execute_reply.started":"2022-01-18T05:52:34.811454Z","shell.execute_reply":"2022-01-18T05:52:34.926147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_pairs_kinship=pd.concat([train_pairs_kinship,train_pairs_same],ignore_index=True) # Combine them together\ntrain_pairs_same=None # to free RAM\nprint('Total POSITIVE image pairs: {}'.format(len(train_pairs_kinship)))","metadata":{"execution":{"iopub.status.busy":"2022-01-18T05:52:34.928079Z","iopub.execute_input":"2022-01-18T05:52:34.928312Z","iopub.status.idle":"2022-01-18T05:52:34.940874Z","shell.execute_reply.started":"2022-01-18T05:52:34.928275Z","shell.execute_reply":"2022-01-18T05:52:34.939985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get all the possible image pairs\n\nindex_p1_li = []\nindex_p2_li = []\n\nfor p1 in tqdm_notebook(range(len(train_file_li))):    \n    for p2 in range(p1+1,len(train_file_li)):\n        index_p1_li.append(p1)\n        index_p2_li.append(p2)\n\ntrain_pairs_all=pd.DataFrame()\ntrain_pairs_all['p1']=index_p1_li\nindex_p1_li = []\ntrain_pairs_all['p2']=index_p2_li\nindex_p2_li = []\n\nprint('Total image pairs: {}'.format(len(train_pairs_all)))\n\ntrain_pairs_all.sample(5)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T05:52:34.942465Z","iopub.execute_input":"2022-01-18T05:52:34.942903Z","iopub.status.idle":"2022-01-18T05:54:37.386866Z","shell.execute_reply.started":"2022-01-18T05:52:34.94286Z","shell.execute_reply":"2022-01-18T05:54:37.386179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add a col \"is_related\": 1 if POS, 0 if NEG\n\nkin_index=np.arange(len(train_pairs_all))[train_pairs_all.merge(train_pairs_kinship, on=['p1','p2'],how='left', indicator=True)['_merge']=='both']\ntrain_pairs_all['is_related']=0\ntrain_pairs_all.loc[kin_index,'is_related']=1\nkin_index=None # to free RAM\ntrain_pairs_kinship=None # to free RAM","metadata":{"execution":{"iopub.status.busy":"2022-01-18T05:54:37.387963Z","iopub.execute_input":"2022-01-18T05:54:37.388698Z","iopub.status.idle":"2022-01-18T05:55:19.055808Z","shell.execute_reply.started":"2022-01-18T05:54:37.388658Z","shell.execute_reply":"2022-01-18T05:55:19.055046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(12,2))\ny2show=[train_pairs_all.query('is_related == 0').shape[0],train_pairs_all.query('is_related == 1').shape[0]]\nplt.barh(range(2),y2show,0.35)\nplt.title('Image pair number')\nplt.yticks(range(2), ('No Kinship (NEG)','With kinship (POS)'))\nplt.box(on=None)\nplt.xticks([], [])\nfor i, v in enumerate(y2show):\n    ax.text(v+1000, i-0.05, str(v), color='blue', fontweight='bold')","metadata":{"execution":{"iopub.status.busy":"2022-01-18T05:55:19.057206Z","iopub.execute_input":"2022-01-18T05:55:19.057441Z","iopub.status.idle":"2022-01-18T05:55:22.98796Z","shell.execute_reply.started":"2022-01-18T05:55:19.05741Z","shell.execute_reply":"2022-01-18T05:55:22.987021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"The number of negative samples is {:.0f} times of positive samples!\".\n      format(train_pairs_all.query('is_related == 0').shape[0]/train_pairs_all.query('is_related == 1').shape[0]))","metadata":{"execution":{"iopub.status.busy":"2022-01-18T05:55:22.989513Z","iopub.execute_input":"2022-01-18T05:55:22.989843Z","iopub.status.idle":"2022-01-18T05:55:25.758517Z","shell.execute_reply.started":"2022-01-18T05:55:22.98979Z","shell.execute_reply":"2022-01-18T05:55:25.757739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create folders for cross-validation\n\nIt is very important to have good cross-validation folders, in order to:\n- Avoid data leakage\n- Optimize model parameters\n- Close the gap between your validation set and LB score\n\nThe main idea is: **The same family does NOT appear in two different folds!** \n\nThis is the same as **GroupKFold** in Scikit-Learn. However, we can not use GroupKFold directly in this case. If we define one \"family\" as one \"group\", then it will be difficult to define the family ID for negative samples (image pair with NO kinship), because the 2 persons in negative image pairs can be from 2 different families.\n\nSo we have to create our own group number for NEG samples. Firstly, use GroupKFold to seperate POS samples into N folders (use family ID as group). Then we can get a family list for each folder. And this family list can be used to get NEG samples for each folders. For example, the 2 persons in a NEG sample are from family-1 and family-2, and both families are in folder-A's family list, then this NEG sample can be assigned to folder-A.\n\nHowever, this method has a problem on this dataset!! Let's see below:","metadata":{}},{"cell_type":"code","source":"# Get family ID for each image\n\ntrain_file_df['fam']=-1\ntrain_file_df['fam']=train_file_df['image_fp'].apply(lambda x: int(x[1:5]))\ntrain_file_df.reset_index(inplace=True)\ntrain_file_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T05:55:25.759791Z","iopub.execute_input":"2022-01-18T05:55:25.760097Z","iopub.status.idle":"2022-01-18T05:55:25.78337Z","shell.execute_reply.started":"2022-01-18T05:55:25.760033Z","shell.execute_reply":"2022-01-18T05:55:25.782316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('There are {} families in the train set.'.format(len(train_file_df.fam.unique())))","metadata":{"execution":{"iopub.status.busy":"2022-01-18T05:55:25.789684Z","iopub.execute_input":"2022-01-18T05:55:25.79007Z","iopub.status.idle":"2022-01-18T05:55:25.796744Z","shell.execute_reply.started":"2022-01-18T05:55:25.790014Z","shell.execute_reply":"2022-01-18T05:55:25.795758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get family ID for each POSimage pair (use p1 only)\ntrain_pairs_kinship = train_pairs_all.query('is_related == 1')\ntrain_pairs_kinship=train_pairs_kinship.merge(train_file_df[['index','fam']], left_on='p1',right_on='index',how='left').drop(columns=['index'])\ntrain_pairs_kinship.sample(5)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T05:55:25.798432Z","iopub.execute_input":"2022-01-18T05:55:25.799352Z","iopub.status.idle":"2022-01-18T05:55:26.414055Z","shell.execute_reply.started":"2022-01-18T05:55:25.799314Z","shell.execute_reply":"2022-01-18T05:55:26.413336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(12,5))\nsns.countplot(x='fam',data=train_pairs_kinship,\n              order=train_pairs_kinship.fam.value_counts().iloc[:20].index)\nplt.title('Top 20 families (image pair with kinship | POS samples)')\nax.text(12, 30000, 'Average POS samples per family is {:.0f}'.format(len(train_pairs_kinship)/len(train_pairs_kinship.fam.unique())),fontsize=12)\nfor i, v in enumerate(train_pairs_kinship['fam'].value_counts()[:20]):\n    ax.text(i-0.4, v+500, str(v),color='gray')\nplt.box(on=None)\nplt.yticks([]);","metadata":{"execution":{"iopub.status.busy":"2022-01-18T05:55:26.415366Z","iopub.execute_input":"2022-01-18T05:55:26.415644Z","iopub.status.idle":"2022-01-18T05:55:26.760026Z","shell.execute_reply.started":"2022-01-18T05:55:26.415607Z","shell.execute_reply":"2022-01-18T05:55:26.758936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Family 601 contains {:.0f}% of image pair of all the POS samples!'.format(train_pairs_kinship['fam'].value_counts().tolist()[0]/len(train_pairs_kinship)*100))\nprint('Family 9 contains {:.0f}% of image pair of all the POS samples.'.format(train_pairs_kinship['fam'].value_counts().tolist()[1]/len(train_pairs_kinship)*100))","metadata":{"execution":{"iopub.status.busy":"2022-01-18T05:55:26.761666Z","iopub.execute_input":"2022-01-18T05:55:26.761923Z","iopub.status.idle":"2022-01-18T05:55:26.774545Z","shell.execute_reply.started":"2022-01-18T05:55:26.761886Z","shell.execute_reply":"2022-01-18T05:55:26.773694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As shown above, **the family 601 represents 35% of POS samples**.\n\nWhat happens? If you open the family folder, you'll find that it's **British Royal Family** ! Of course!\n\nThis kernel [EDA with Plotly-Smart, Cute and Pretty People](https://www.kaggle.com/gowrishankarin/eda-with-plotly-smart-cute-and-pretty-people) by [Gowri Shankar](https://www.kaggle.com/gowrishankarin) shows great visualizations on this.\n\nWhy does it cause a problem to create our CV folders? \n\nWe want each folder to have equivalent number of samples. If we cut our samples into 3 or more folders, and we don't want the same family appears in two different folds (to avoid data leakage), so the British Royal Family will take one whole folder. It will bias the cross validation score. \n\nSo, we may reduce the POS sample number per family to a certain limit, like 3000. If the number is above the limit, only use 3000 random samples from tha family. ","metadata":{}},{"cell_type":"code","source":"limit_number = 3000\n\nindex_li = train_pairs_kinship['fam'].value_counts()[lambda x:x<=limit_number].index\ntrain_fam_lim_df = train_pairs_kinship[train_pairs_kinship['fam'].isin(index_li)]\n\nfor i in train_pairs_kinship['fam'].value_counts()[lambda x:x>limit_number].index:\n    df_temp = train_pairs_kinship.query('fam == {}'.format(i)).sample(limit_number,replace=False,random_state=2019)\n    train_fam_lim_df = pd.concat([train_fam_lim_df, df_temp])\n    \ntrain_fam_lim_df=train_fam_lim_df.reset_index() # Reset index for GroupKFold method\n\nprint('Number of POS samples in the selected dataset: {}'.format(len(train_fam_lim_df)))","metadata":{"execution":{"iopub.status.busy":"2022-01-18T05:55:26.776197Z","iopub.execute_input":"2022-01-18T05:55:26.776484Z","iopub.status.idle":"2022-01-18T05:55:26.820512Z","shell.execute_reply.started":"2022-01-18T05:55:26.776445Z","shell.execute_reply":"2022-01-18T05:55:26.819707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(12,5))\nsns.countplot(x='fam',data=train_fam_lim_df,order=train_fam_lim_df.fam.value_counts().iloc[:20].index)\nplt.title('Top 20 families (image pair with kinship | POS samples)')\nax.text(12, 2500, 'Average POS samples per family is {:.0f}'.format(len(train_pairs_kinship)/len(train_fam_lim_df.fam.unique())),fontsize=12)\nfor i, v in enumerate(train_fam_lim_df['fam'].value_counts()[:20]):\n    ax.text(i-0.4, v+100, str(v),color='gray')\nplt.box(on=None)\nplt.yticks([]);","metadata":{"execution":{"iopub.status.busy":"2022-01-18T05:55:26.821885Z","iopub.execute_input":"2022-01-18T05:55:26.822157Z","iopub.status.idle":"2022-01-18T05:55:27.158536Z","shell.execute_reply.started":"2022-01-18T05:55:26.822122Z","shell.execute_reply":"2022-01-18T05:55:27.157698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gkf = GroupKFold(n_splits=6) # Group 6 as test set, Group0-5 as CV folders.\n\ntrain_fam=train_fam_lim_df['fam']\n\nfam_group=np.ones(max(train_fam_lim_df['fam'])+1)*(-1)\nfam_group=fam_group.astype(int)\n\nfor idx,( _, test_index) in enumerate(gkf.split(X=train_fam,groups=train_fam)):\n    print(\"Group {}: {}\".format(idx,np.unique(train_fam[test_index])))\n    fam_group[np.unique(train_fam[test_index])]=idx\n    print('-'*85)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T05:55:27.160496Z","iopub.execute_input":"2022-01-18T05:55:27.160719Z","iopub.status.idle":"2022-01-18T05:55:27.225089Z","shell.execute_reply.started":"2022-01-18T05:55:27.16069Z","shell.execute_reply":"2022-01-18T05:55:27.223563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"6 groups have been created, now it's time to add NEG samples to each group.\n\nIs it the best to have the equal number of POS and NEG samples? Or shall we have more portion for NEG samples, like 2:1? I will test other portions later.","metadata":{}},{"cell_type":"code","source":"# Get group ID for each image\n\ntrain_file_df['group']=train_file_df['fam'].apply(\n    lambda x: fam_group[x])\n\ntrain_file_df.sample(5)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T05:55:27.22661Z","iopub.execute_input":"2022-01-18T05:55:27.226893Z","iopub.status.idle":"2022-01-18T05:55:27.255549Z","shell.execute_reply.started":"2022-01-18T05:55:27.226858Z","shell.execute_reply":"2022-01-18T05:55:27.254806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get group ID for each image pair\n\ntqdm_notebook.pandas()\ngroup_li=train_file_df['group'].tolist()\n\ntrain_pairs_all['group1']=train_pairs_all['p1'].progress_apply(lambda x: group_li[x])\ntrain_pairs_all['group2']=train_pairs_all['p2'].progress_apply(lambda x: group_li[x])\ntmp_li = (train_pairs_all['group1']==train_pairs_all['group2'])*(train_pairs_all['group1']+1)-1\ntrain_pairs_all.drop(columns=['group1','group2'],inplace=True)\ntrain_pairs_all['group']=tmp_li\ntmp_li=None # to free RAM\ntrain_pairs_all.sample(5)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T05:55:27.257985Z","iopub.execute_input":"2022-01-18T05:55:27.258277Z","iopub.status.idle":"2022-01-18T06:00:40.767592Z","shell.execute_reply.started":"2022-01-18T05:55:27.258243Z","shell.execute_reply":"2022-01-18T06:00:40.766873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(12,5))\nsns.countplot(y='group',data=train_pairs_all,orient='v')\nplt.title('Number of image pair in each group')\nax.text(30000000,1, '\"-1\" means no group is assigned.',fontsize=16)\nax.text(25000000,2,\n        '{:.0f}% of image pairs have no group assigned.'.format(train_pairs_all.query('group == -1').shape[0]/len(train_pairs_all)*100),\n        fontsize=16)\nplt.box(on=None)\nplt.xticks([]);","metadata":{"execution":{"iopub.status.busy":"2022-01-18T06:00:40.769019Z","iopub.execute_input":"2022-01-18T06:00:40.7693Z","iopub.status.idle":"2022-01-18T06:00:48.625429Z","shell.execute_reply.started":"2022-01-18T06:00:40.769264Z","shell.execute_reply":"2022-01-18T06:00:48.624586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop group==-1\ntrain_pairs_all = train_pairs_all[train_pairs_all['group']!=-1]\n# Shuffle\ntrain_pairs_all = train_pairs_all.sample(frac=1,random_state=2019)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T06:00:48.626922Z","iopub.execute_input":"2022-01-18T06:00:48.627378Z","iopub.status.idle":"2022-01-18T06:00:50.750275Z","shell.execute_reply.started":"2022-01-18T06:00:48.627338Z","shell.execute_reply":"2022-01-18T06:00:50.749484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset_df=train_fam_lim_df\ntrain_dataset_df['group']=train_dataset_df['fam'].apply(lambda x: fam_group[x])\ntrain_dataset_df.drop(columns=['index','fam'],inplace=True)\ntrain_dataset_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T06:00:50.751775Z","iopub.execute_input":"2022-01-18T06:00:50.752018Z","iopub.status.idle":"2022-01-18T06:00:50.908593Z","shell.execute_reply.started":"2022-01-18T06:00:50.751985Z","shell.execute_reply":"2022-01-18T06:00:50.907814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"group_num = train_fam_lim_df.groupby('group')['is_related'].count().tolist()\nportion=1 # get equal number of NEG / POS\ndf_temp = pd.concat(\n    [t.head(int(group_num[g]*portion)) for g, t in train_pairs_all.query('is_related == 0').groupby('group', sort=False, as_index=False)],\n    ignore_index=True)\n\ntrain_dataset_df=pd.concat([train_dataset_df,df_temp],ignore_index=True)\ntrain_dataset_df.shape","metadata":{"execution":{"iopub.status.busy":"2022-01-18T06:00:50.909802Z","iopub.execute_input":"2022-01-18T06:00:50.912211Z","iopub.status.idle":"2022-01-18T06:00:52.074115Z","shell.execute_reply.started":"2022-01-18T06:00:50.912165Z","shell.execute_reply":"2022-01-18T06:00:52.073223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# to free RAM\ntrain_pairs_all=None\ntrain_pairs_kinship=None","metadata":{"execution":{"iopub.status.busy":"2022-01-18T06:00:52.075591Z","iopub.execute_input":"2022-01-18T06:00:52.075857Z","iopub.status.idle":"2022-01-18T06:00:52.081362Z","shell.execute_reply.started":"2022-01-18T06:00:52.075822Z","shell.execute_reply":"2022-01-18T06:00:52.080392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(12,5))\nsns.countplot(y='group',data=train_dataset_df,orient='v',hue='is_related')\nplt.title('Number of image pair in each group')\nplt.box(on=None)\nplt.xticks([]);","metadata":{"execution":{"iopub.status.busy":"2022-01-18T06:00:52.082792Z","iopub.execute_input":"2022-01-18T06:00:52.083278Z","iopub.status.idle":"2022-01-18T06:00:52.300482Z","shell.execute_reply.started":"2022-01-18T06:00:52.083236Z","shell.execute_reply":"2022-01-18T06:00:52.299687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T06:00:52.301917Z","iopub.execute_input":"2022-01-18T06:00:52.302354Z","iopub.status.idle":"2022-01-18T06:00:52.31624Z","shell.execute_reply.started":"2022-01-18T06:00:52.302313Z","shell.execute_reply":"2022-01-18T06:00:52.315427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prewhiten(x):\n    if x.ndim == 4:\n        axis = (1, 2, 3)\n        size = x[0].size\n    elif x.ndim == 3:\n        axis = (0, 1, 2)\n        size = x.size\n    else:\n        raise ValueError('Dimension should be 3 or 4')\n\n    mean = np.mean(x, axis=axis, keepdims=True)\n    std = np.std(x, axis=axis, keepdims=True)\n    std_adj = np.maximum(std, 1.0/np.sqrt(size))\n    y = (x - mean) / std_adj\n    return y\n\ndef l2_normalize(x, axis=-1, epsilon=1e-10):\n    output = x / np.sqrt(np.maximum(np.sum(np.square(x), axis=axis, keepdims=True), epsilon))\n    return output\n","metadata":{"execution":{"iopub.status.busy":"2022-01-18T06:00:52.31758Z","iopub.execute_input":"2022-01-18T06:00:52.31805Z","iopub.status.idle":"2022-01-18T06:00:52.329334Z","shell.execute_reply.started":"2022-01-18T06:00:52.317986Z","shell.execute_reply":"2022-01-18T06:00:52.328221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Landmark","metadata":{}},{"cell_type":"code","source":"from keras.models import Sequential,load_model\nfrom PIL import Image \nfrom skimage.color import rgb2gray\n\nmodeland = load_model('../input/modelsisi/Landmark_modelsisi.h5')\n\ndef load_and_align_images96(filepaths, margin,image_size = 96):\n    \n    aligned_images = []\n    for filepath in filepaths:\n        img = imread(filepath)\n        gs = rgb2gray(img)\n        aligned = resize(gs, (image_size, image_size), mode='reflect')\n        \n        aligned_images.append(aligned)\n            \n    return np.array(aligned_images)\n\n\ndef calc_embs1(filepaths, margin=10, batch_size=512):\n    pd = []\n    for start in tqdm_notebook(range(0, len(filepaths), batch_size)):\n        aligned_images = prewhiten(load_and_align_images96(filepaths[start:start+batch_size], margin))\n        \n        print(aligned_images.shape )\n        \n        pd.append(modeland.predict(aligned_images.reshape(-1,96,96,1)))\n    embs = l2_normalize(np.concatenate(pd))\n\n    return embs\n\n\ntrain_Land= calc_embs1([os.path.join(\"../input/dataconvert/train\", f) for f in train_file_df['image_fp']])\nprint(train_Land.shape)\nlandmark_fitur =pd.DataFrame(train_Land,columns=['fe'+str(i) for i in range(8)])\nlandmark_fitur ","metadata":{"execution":{"iopub.status.busy":"2022-01-18T06:00:52.331016Z","iopub.execute_input":"2022-01-18T06:00:52.331672Z","iopub.status.idle":"2022-01-18T06:03:31.330207Z","shell.execute_reply.started":"2022-01-18T06:00:52.33163Z","shell.execute_reply":"2022-01-18T06:03:31.329487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LBP","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\nlb = load_model('../input/modelsisi/LBP_modelsis.h5',compile = False)\ndef Binarypattern(im):                               # creating function to get local binary pattern\n    img= np.zeros_like(im)\n    n=3                                              # taking kernel of size 3*3\n    for i in range(0,im.shape[0]-n):                 # for image height\n        for j in range(0,im.shape[1]-n):               # for image width\n            x  = im[i:i+n,j:j+n]                     # reading the entire image in 3*3 format\n            center       = x[1,1]                    # taking the center value for 3*3 kernel\n            img1        = (x >= center)*1.0          # checking if neighbouring values of center value is greater or less than center value\n            img1_vector = img1.T.flatten()           # getting the image pixel values \n            img1_vector = np.delete(img1_vector,4)  \n            digit = np.where(img1_vector)[0]         \n            if len(digit) >= 1:                     # converting the neighbouring pixels according to center pixel value\n                num = np.sum(2**digit)              # if n> center assign 1 and if n<center assign 0\n            else:                                    # if 1 then multiply by 2^digit and if 0 then making value 0 and aggregating all the values of kernel to get new center value\n                num = 0\n            img[i+1,j+1] = num\n    return(img)\ndef create_LBP_features(data):\n    Feature_data = np.zeros(data.shape)\n\n    for i in range(len(data)):\n        img = data[i]\n        imgLBP=Binarypattern(img)  \n        Feature_data[i] = imgLBP\n    \n    return Feature_data\n\ndef load_and_align_images48(filepaths, margin,image_size = 48):\n    \n    aligned_images = []\n    for filepath in filepaths:\n        img = imread(filepath)\n        gs = rgb2gray(img)\n        aligned = resize(gs, (image_size, image_size), mode='reflect')\n        \n        aligned_images.append(aligned)\n            \n    return np.array(aligned_images)\n\ndef calc_embs2(filepaths, margin=10, batch_size=512):\n    pd = []\n    for start in tqdm_notebook(range(0, len(filepaths), batch_size)):\n        aligned_images = prewhiten(load_and_align_images48(filepaths[start:start+batch_size], margin))\n        Feature_X_train = create_LBP_features(aligned_images)\n        print( Feature_X_train.shape )\n        \n        pd.append(lb.predict( Feature_X_train.reshape(-1,48,48,1)))\n    embs = l2_normalize(np.concatenate(pd))\n\n    return embs\n\ntrain_lbp= calc_embs2([os.path.join(\"../input/dataconvert/train\", f) for f in train_file_df['image_fp']])\nprint(train_lbp.shape)\nLBP_fitur = pd.DataFrame(train_lbp,columns=['fe'+str(i) for i in range(7)])\nLBP_fitur","metadata":{"execution":{"iopub.status.busy":"2022-01-18T06:03:31.331667Z","iopub.execute_input":"2022-01-18T06:03:31.33213Z","iopub.status.idle":"2022-01-18T06:16:30.090501Z","shell.execute_reply.started":"2022-01-18T06:03:31.332087Z","shell.execute_reply":"2022-01-18T06:16:30.089549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Use Facenet to calculate 128 features","metadata":{}},{"cell_type":"code","source":"model_path = '../input/facenet-keras/facenet_keras.h5'\nmodel_facenet_keras = load_model(model_path)\nmodel_facenet_keras.summary()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-01-18T06:16:30.09225Z","iopub.execute_input":"2022-01-18T06:16:30.092711Z","iopub.status.idle":"2022-01-18T06:16:35.193829Z","shell.execute_reply.started":"2022-01-18T06:16:30.092673Z","shell.execute_reply":"2022-01-18T06:16:35.193113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prewhiten(x):\n    if x.ndim == 4:\n        axis = (1, 2, 3)\n        size = x[0].size\n    elif x.ndim == 3:\n        axis = (0, 1, 2)\n        size = x.size\n    else:\n        raise ValueError('Dimension should be 3 or 4')\n\n    mean = np.mean(x, axis=axis, keepdims=True)\n    std = np.std(x, axis=axis, keepdims=True)\n    std_adj = np.maximum(std, 1.0/np.sqrt(size))\n    y = (x - mean) / std_adj\n    return y\n\ndef l2_normalize(x, axis=-1, epsilon=1e-10):\n    output = x / np.sqrt(np.maximum(np.sum(np.square(x), axis=axis, keepdims=True), epsilon))\n    return output\n\ndef load_and_align_images(filepaths, margin,image_size = 160):\n    \n    aligned_images = []\n    for filepath in filepaths:\n        img = imread(filepath)\n        aligned = resize(img, (image_size, image_size), mode='reflect')\n        aligned_images.append(aligned)\n            \n    return np.array(aligned_images)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T06:16:35.194861Z","iopub.execute_input":"2022-01-18T06:16:35.195127Z","iopub.status.idle":"2022-01-18T06:16:35.206722Z","shell.execute_reply.started":"2022-01-18T06:16:35.195092Z","shell.execute_reply":"2022-01-18T06:16:35.204653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calc_embs3(filepaths, margin=10, batch_size=512):\n    pd = []\n    for start in tqdm_notebook(range(0, len(filepaths), batch_size)):\n        aligned_images = prewhiten(load_and_align_images(filepaths[start:start+batch_size], margin))\n        print(aligned_images.shape )\n        \n        pd.append(model_facenet_keras.predict_on_batch(aligned_images))\n    embs = l2_normalize(np.concatenate(pd))\n    print(np.concatenate(pd))\n    return embs\n\ntrain_embs = calc_embs3([os.path.join(\"../input/dataconvert/train\", f) for f in train_file_df['image_fp']])\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-01-18T06:16:35.207958Z","iopub.execute_input":"2022-01-18T06:16:35.208724Z","iopub.status.idle":"2022-01-18T06:20:02.590806Z","shell.execute_reply.started":"2022-01-18T06:16:35.208688Z","shell.execute_reply":"2022-01-18T06:20:02.590041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"face = pd.DataFrame(train_embs,columns=['fe'+str(i) for i in range(128)])\nface","metadata":{"execution":{"iopub.status.busy":"2022-01-18T06:20:02.59245Z","iopub.execute_input":"2022-01-18T06:20:02.592985Z","iopub.status.idle":"2022-01-18T06:20:02.627492Z","shell.execute_reply.started":"2022-01-18T06:20:02.592942Z","shell.execute_reply":"2022-01-18T06:20:02.626614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gab = pd.concat([face,landmark_fitur,LBP_fitur],axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T06:20:02.628819Z","iopub.execute_input":"2022-01-18T06:20:02.629183Z","iopub.status.idle":"2022-01-18T06:20:02.641335Z","shell.execute_reply.started":"2022-01-18T06:20:02.629145Z","shell.execute_reply":"2022-01-18T06:20:02.64053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_file_df=pd.concat([train_file_df, gab],axis=1)\ntrain_file_df","metadata":{"execution":{"iopub.status.busy":"2022-01-18T06:20:02.642707Z","iopub.execute_input":"2022-01-18T06:20:02.64347Z","iopub.status.idle":"2022-01-18T06:20:02.68044Z","shell.execute_reply.started":"2022-01-18T06:20:02.643429Z","shell.execute_reply":"2022-01-18T06:20:02.679689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# preprosesing validasi","metadata":{}},{"cell_type":"code","source":"# Use absolute distance as final features\n\np1_df = train_dataset_df.merge(train_file_df, left_on='p1',right_on='index',how='left').iloc[:,8:]\np2_df = train_dataset_df.merge(train_file_df, left_on='p2',right_on='index',how='left').iloc[:,8:]\n\ntrain_dataset_df = pd.concat([train_dataset_df, abs(p1_df-p2_df)],axis=1)\np1_df=None\np2_df=None\ntrain_dataset_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T06:20:02.682633Z","iopub.execute_input":"2022-01-18T06:20:02.682937Z","iopub.status.idle":"2022-01-18T06:20:03.205999Z","shell.execute_reply.started":"2022-01-18T06:20:02.682898Z","shell.execute_reply":"2022-01-18T06:20:03.205301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset_df.dtypes","metadata":{"execution":{"iopub.status.busy":"2022-01-18T06:20:03.207108Z","iopub.execute_input":"2022-01-18T06:20:03.208178Z","iopub.status.idle":"2022-01-18T06:20:03.217298Z","shell.execute_reply.started":"2022-01-18T06:20:03.208137Z","shell.execute_reply":"2022-01-18T06:20:03.216538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset_df.to_csv('hasil.csv')","metadata":{"execution":{"iopub.status.busy":"2022-01-18T06:20:03.224449Z","iopub.execute_input":"2022-01-18T06:20:03.224674Z","iopub.status.idle":"2022-01-18T06:20:53.191253Z","shell.execute_reply.started":"2022-01-18T06:20:03.224649Z","shell.execute_reply":"2022-01-18T06:20:53.190512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# shuffle the dataset\ntrain_dataset_df1=train_dataset_df.sample(frac=1,random_state=2019).reset_index(drop=True)\ntrain_dataset_df1.head()\n","metadata":{"execution":{"iopub.status.busy":"2022-01-18T06:20:53.192596Z","iopub.execute_input":"2022-01-18T06:20:53.192844Z","iopub.status.idle":"2022-01-18T06:20:53.487043Z","shell.execute_reply.started":"2022-01-18T06:20:53.192812Z","shell.execute_reply":"2022-01-18T06:20:53.486209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X=train_dataset_df1.iloc[:,4:]\ny=train_dataset_df1.iloc[:,2]\nprint(X.shape, y.shape)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T06:20:53.488822Z","iopub.execute_input":"2022-01-18T06:20:53.489282Z","iopub.status.idle":"2022-01-18T06:20:53.559504Z","shell.execute_reply.started":"2022-01-18T06:20:53.489236Z","shell.execute_reply":"2022-01-18T06:20:53.558691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X","metadata":{"execution":{"iopub.status.busy":"2022-01-18T06:20:53.560873Z","iopub.execute_input":"2022-01-18T06:20:53.561729Z","iopub.status.idle":"2022-01-18T06:20:53.618329Z","shell.execute_reply.started":"2022-01-18T06:20:53.561689Z","shell.execute_reply":"2022-01-18T06:20:53.616131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y","metadata":{"execution":{"iopub.status.busy":"2022-01-18T06:20:53.622328Z","iopub.execute_input":"2022-01-18T06:20:53.622622Z","iopub.status.idle":"2022-01-18T06:20:53.638437Z","shell.execute_reply.started":"2022-01-18T06:20:53.62258Z","shell.execute_reply":"2022-01-18T06:20:53.637691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train=X[train_dataset_df1['group']!=5]\nX_test=X[train_dataset_df1['group']==5]\ny_train=train_dataset_df1['is_related'][train_dataset_df1['group']!=5]\ny_test=train_dataset_df1['is_related'][train_dataset_df1['group']==5]\n\ny_train_group=train_dataset_df1['group'][train_dataset_df1['group']!=5]\n\nX_train.shape,X_test.shape,y_train.shape,y_test.shape,y_train_group.shape","metadata":{"execution":{"iopub.status.busy":"2022-01-18T06:20:53.639751Z","iopub.execute_input":"2022-01-18T06:20:53.640299Z","iopub.status.idle":"2022-01-18T06:20:53.831312Z","shell.execute_reply.started":"2022-01-18T06:20:53.640259Z","shell.execute_reply":"2022-01-18T06:20:53.830088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# group kfolder\ngroup_kfold = GroupKFold(n_splits=5)\n# this is a check of GroupKFold result\n\nfor train_index, test_index in group_kfold.split(X_train, y_train, y_train_group):\n    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    print(np.unique(y_train_group.to_numpy()[train_index]))\n    print(np.unique(y_train_group.to_numpy()[test_index]))\n    print('-'*20)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T06:20:53.832567Z","iopub.execute_input":"2022-01-18T06:20:53.833045Z","iopub.status.idle":"2022-01-18T06:20:53.972754Z","shell.execute_reply.started":"2022-01-18T06:20:53.832991Z","shell.execute_reply":"2022-01-18T06:20:53.972086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model=LogisticRegression(solver='liblinear')\nres=cross_validate(model,X_train,y_train,cv=group_kfold,n_jobs=1,groups=y_train_group,scoring=('accuracy', 'roc_auc'))\nprint(\"Mean ROC_AUC score: {:.4f} (std: {:.4f})\".format(res['test_roc_auc'].mean(),res['test_roc_auc'].std()))","metadata":{"execution":{"iopub.status.busy":"2022-01-18T06:20:53.974046Z","iopub.execute_input":"2022-01-18T06:20:53.977401Z","iopub.status.idle":"2022-01-18T06:21:46.771179Z","shell.execute_reply.started":"2022-01-18T06:20:53.977355Z","shell.execute_reply":"2022-01-18T06:21:46.770434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test","metadata":{"execution":{"iopub.status.busy":"2022-01-18T06:21:46.77249Z","iopub.execute_input":"2022-01-18T06:21:46.77316Z","iopub.status.idle":"2022-01-18T06:21:46.824476Z","shell.execute_reply.started":"2022-01-18T06:21:46.773125Z","shell.execute_reply":"2022-01-18T06:21:46.82376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test on Test Set\nmodel.fit(X_train,y_train)\nprint(\"ROC_AUC socre on test set: {:.3f}\".format(roc_auc_score(y_test,model.predict_proba(X_test)[:,1])))","metadata":{"execution":{"iopub.status.busy":"2022-01-18T06:21:46.828745Z","iopub.execute_input":"2022-01-18T06:21:46.830971Z","iopub.status.idle":"2022-01-18T06:21:59.885357Z","shell.execute_reply.started":"2022-01-18T06:21:46.830927Z","shell.execute_reply":"2022-01-18T06:21:59.871202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate embs for test images\ntest_images = os.listdir(\"../input/dataconvert/test\")\n\ntest_embs1 = calc_embs1([os.path.join(\"../input/dataconvert/test\", f) for f in test_images])\ntest_embs2 = calc_embs2([os.path.join(\"../input/dataconvert/test\", f) for f in test_images])\ntest_embs3 = calc_embs3([os.path.join(\"../input/dataconvert/test\", f) for f in test_images])\n\nimg2idx = dict()\nfor idx, img in enumerate(test_images):\n    img2idx[img] = idx","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-01-18T06:21:59.887238Z","iopub.execute_input":"2022-01-18T06:21:59.88762Z","iopub.status.idle":"2022-01-18T06:31:25.142576Z","shell.execute_reply.started":"2022-01-18T06:21:59.887578Z","shell.execute_reply":"2022-01-18T06:31:25.14172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_embs1.shape,test_embs2.shape,test_embs3.shape","metadata":{"execution":{"iopub.status.busy":"2022-01-18T06:32:59.511268Z","iopub.execute_input":"2022-01-18T06:32:59.511591Z","iopub.status.idle":"2022-01-18T06:32:59.522946Z","shell.execute_reply.started":"2022-01-18T06:32:59.511555Z","shell.execute_reply":"2022-01-18T06:32:59.522098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_embs = np.hstack((test_embs1,test_embs2,test_embs3))\ntest_embs.shape","metadata":{"execution":{"iopub.status.busy":"2022-01-18T06:34:35.722275Z","iopub.execute_input":"2022-01-18T06:34:35.722769Z","iopub.status.idle":"2022-01-18T06:34:35.729635Z","shell.execute_reply.started":"2022-01-18T06:34:35.722727Z","shell.execute_reply":"2022-01-18T06:34:35.728841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv(\"../input/recognizing-faces-in-the-wild/sample_submission.csv\")\nprint(test_df.head())\ntest_np = []\nfor idx, row in tqdm_notebook(test_df.iterrows(), total=len(test_df)):\n    imgs = [test_embs[img2idx[img]] for img in row.img_pair.split(\"-\")]\n    \n    test_np.append(abs(imgs[0]-imgs[1]))\n    \ntest_np = np.array(test_np)\n# Predict\nmodel.fit(X,y)\nprobs = model.predict_proba(test_np)[:,1]\n\nsub_df = pd.read_csv(\"../input/recognizing-faces-in-the-wild/sample_submission.csv\")\nsub_df.is_related = probs\nsub_df.hist();","metadata":{"execution":{"iopub.status.busy":"2022-01-18T06:34:38.315489Z","iopub.execute_input":"2022-01-18T06:34:38.316284Z","iopub.status.idle":"2022-01-18T06:34:54.076279Z","shell.execute_reply.started":"2022-01-18T06:34:38.316233Z","shell.execute_reply":"2022-01-18T06:34:54.075564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# use this function to show some image pairs.\ndef show2pic(fd,paire):\n    plt.figure(figsize=(7,10))\n    plt.subplot(121)\n    plt.imshow(imread(os.path.join(fd,paire.split('-')[0])))\n    plt.axis('off')\n    plt.title(paire.split('-')[0])\n    plt.subplot(122)\n    plt.imshow(imread(os.path.join(fd,paire.split('-')[1])))\n    plt.axis('off')\n    plt.title(paire.split('-')[1])\n    \n#sub_df.sort_values('is_related',ascending=False).head(10)\n# here is an example of the top 5th result.\n\nshow2pic('../input/dataconvert/test',sub_df.loc[2,'img_pair'])","metadata":{"execution":{"iopub.status.busy":"2022-01-18T06:37:09.661599Z","iopub.execute_input":"2022-01-18T06:37:09.661856Z","iopub.status.idle":"2022-01-18T06:37:09.886596Z","shell.execute_reply.started":"2022-01-18T06:37:09.661826Z","shell.execute_reply":"2022-01-18T06:37:09.885814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df.sort_values('is_related',ascending=False).head(10)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T06:35:03.710145Z","iopub.execute_input":"2022-01-18T06:35:03.710615Z","iopub.status.idle":"2022-01-18T06:35:03.725343Z","shell.execute_reply.started":"2022-01-18T06:35:03.710576Z","shell.execute_reply":"2022-01-18T06:35:03.7246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}