{"cells":[{"metadata":{},"cell_type":"markdown","source":"The goal of this problem is to find if two faces are related or not. My idea is to use VGG Face model to extract features of faces from images, and then find how close the second image's features are. This is a theoretical idea that I think could work so I've just given it a try."},{"metadata":{},"cell_type":"markdown","source":"# Importing Neccessary Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\n\nimport matplotlib.pyplot as plt\n\nimport torch\nfrom torchvision import datasets, transforms, models\nfrom torch import nn, optim\nfrom torch.autograd import Variable\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom torch.utils.data import Dataset, DataLoader\nfrom skimage import io, transform\nfrom PIL import Image\nimport torchvision.transforms.functional as TF\nimport itertools\nimport torch.utils.data as data_utils","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#Checking if CUDA is available or not\ntrain_on_gpu = torch.cuda.is_available()\n\nif not train_on_gpu:\n    print('CUDA is not available.  Training on CPU ...')\nelse:\n    print('CUDA is available!  Training on GPU ...')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Initializing VGG Face Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Vgg_face_dag(nn.Module):\n\n    def __init__(self):\n        super(Vgg_face_dag, self).__init__()\n        self.meta = {'mean': [129.186279296875, 104.76238250732422, 93.59396362304688],\n                     'std': [1, 1, 1],\n                     'imageSize': [224, 224, 3]}\n        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n        self.relu1_1 = nn.ReLU(inplace=True)\n        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n        self.relu1_2 = nn.ReLU(inplace=True)\n        self.pool1 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n        self.relu2_1 = nn.ReLU(inplace=True)\n        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n        self.relu2_2 = nn.ReLU(inplace=True)\n        self.pool2 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n        self.relu3_1 = nn.ReLU(inplace=True)\n        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n        self.relu3_2 = nn.ReLU(inplace=True)\n        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n        self.relu3_3 = nn.ReLU(inplace=True)\n        self.pool3 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n        self.relu4_1 = nn.ReLU(inplace=True)\n        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n        self.relu4_2 = nn.ReLU(inplace=True)\n        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n        self.relu4_3 = nn.ReLU(inplace=True)\n        self.pool4 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n        self.relu5_1 = nn.ReLU(inplace=True)\n        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n        self.relu5_2 = nn.ReLU(inplace=True)\n        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n        self.relu5_3 = nn.ReLU(inplace=True)\n        self.pool5 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n        self.fc6 = nn.Linear(in_features=25088, out_features=4096, bias=True)\n        self.relu6 = nn.ReLU(inplace=True)\n        self.dropout6 = nn.Dropout(p=0.5)\n        self.fc7 = nn.Linear(in_features=4096, out_features=4096, bias=True)\n        self.relu7 = nn.ReLU(inplace=True)\n        self.dropout7 = nn.Dropout(p=0.5)\n        self.fc8 = nn.Linear(in_features=4096, out_features=2622, bias=True)\n\n    def forward(self, x0):\n        x1 = self.conv1_1(x0)\n        x2 = self.relu1_1(x1)\n        x3 = self.conv1_2(x2)\n        x4 = self.relu1_2(x3)\n        x5 = self.pool1(x4)\n        x6 = self.conv2_1(x5)\n        x7 = self.relu2_1(x6)\n        x8 = self.conv2_2(x7)\n        x9 = self.relu2_2(x8)\n        x10 = self.pool2(x9)\n        x11 = self.conv3_1(x10)\n        x12 = self.relu3_1(x11)\n        x13 = self.conv3_2(x12)\n        x14 = self.relu3_2(x13)\n        x15 = self.conv3_3(x14)\n        x16 = self.relu3_3(x15)\n        x17 = self.pool3(x16)\n        x18 = self.conv4_1(x17)\n        x19 = self.relu4_1(x18)\n        x20 = self.conv4_2(x19)\n        x21 = self.relu4_2(x20)\n        x22 = self.conv4_3(x21)\n        x23 = self.relu4_3(x22)\n        x24 = self.pool4(x23)\n        x25 = self.conv5_1(x24)\n        x26 = self.relu5_1(x25)\n        x27 = self.conv5_2(x26)\n        x28 = self.relu5_2(x27)\n        x29 = self.conv5_3(x28)\n        x30 = self.relu5_3(x29)\n        x31_preflatten = self.pool5(x30)\n        x31 = x31_preflatten.view(x31_preflatten.size(0), -1)\n        x32 = self.fc6(x31)\n        x33 = self.relu6(x32)\n        x34 = self.dropout6(x33)\n        x35 = self.fc7(x34)\n        x36 = self.relu7(x35)\n        x37 = self.dropout7(x36)\n        x38 = self.fc8(x37)\n        return x38\n\ndef vgg_face_dag(weights_path=None, **kwargs):\n    \"\"\"\n    load imported model instance\n\n    Args:\n        weights_path (str): If set, loads model weights from the given path\n    \"\"\"\n    model = Vgg_face_dag()\n    if weights_path:\n        state_dict = torch.load(weights_path)\n        model.load_state_dict(state_dict)\n    return model\n\n#Parkhi, Omkar M., Andrea Vedaldi, and Andrew Zisserman. \"Deep Face Recognition.\" BMVC. Vol. 1. No. 3. 2015.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Initializing Face Model\nface_model = vgg_face_dag()\nface_model\nif train_on_gpu:\n    face_model.cuda()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Checking available Training Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(os.listdir('../input'))\nprint(os.listdir('../input/train/F0765'))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So the train data has folders of families and inside them are folders of people. The data of relations between images is given in file train_relationships.csv"},{"metadata":{},"cell_type":"markdown","source":"# Pre processing relationships data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/train_relationships.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see the data has two columns of people that are related. Let's split these columns into three i.e. Family, Person1 and Person2 "},{"metadata":{"trusted":true},"cell_type":"code","source":"new = df[\"p1\"].str.split(\"/\", n = 1, expand = True)\n\n# making separate first name column from new data frame \ndf[\"Family\"]= new[0]\n# making separate last name column from new data frame \ndf[\"Person1\"]= new[1]\n\n# Dropping old Name columns\ndf.drop(columns =[\"p1\"], inplace = True)\n\nnew = df[\"p2\"].str.split(\"/\", n = 1, expand = True)\n\n# making separate first name column from new data frame \ndf[\"Family2\"]= new[0]\n# making separate last name column from new data frame \ndf[\"Person2\"]= new[1]\n\n# Dropping old Name columns\ndf.drop(columns =[\"p2\"], inplace = True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Family column is redundant so it's not needed"},{"metadata":{"trusted":true},"cell_type":"code","source":"del df['Family2']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can also create some more data from Families for people that are not related. Then we can use this data to train our model on classes 'Related' and 'Not Related'. 'Not Related' instances can be numerous since there are many families that are not correlated. However, that would give a lot of data, We can create new data from people within families that are not related. Eg. A son and daughter would be related to their father and their mother, However, the mother and father won't be related themselves"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"#A new column in the existing dataframe with all values as 1, since these people are all related\ndf['Related'] = 1\n\n#Creating a dictionary, and storing members of each family\ndf_dict = {}\nfor index, row in df.iterrows():\n    if row['Family'] in df_dict:\n        df_dict[row['Family']].append(row['Person1'])\n    else:\n        df_dict[row['Family']] = [row['Person1']]\n        \n#For each family in this dictionary, we'll first make pairs of people\n#For each pair, we'll check if they're related in our existing Dataset\n#If they're not in the dataframe, means we'll create a row with both persons and related value 0\ni=1\nfor key in df_dict:\n    pair = list(itertools.combinations(df_dict[key], 2))\n    for item in pair:\n        if len(df[(df['Family']==key)&(df['Person1']==item[0])&(df['Person2']==item[1])])==0 \\\n        and len(df[(df['Family']==key)&(df['Person1']==item[1])&(df['Person2']==item[0])])==0:\n            new = {'Family':key,'Person1':item[0],'Person2':item[1],'Related':0}\n            df=df.append(new,ignore_index=True)\n        \n#Storing rows only where Person1 and Person2 are not same\ndf = df[(df['Person1']!=df['Person2'])]\n\n#len(df[(df['Related']==1)])\n\nprint(df['Related'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking Dataframe contents once\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating Custom Dataset"},{"metadata":{},"cell_type":"markdown","source":"We can load a custom dataset such that when our dataloader iterates, it iterates the index in dataframe, and returns a pair of people, with their corresponding class i.e. if they're 'Related' or 'Non Related'"},{"metadata":{"trusted":true},"cell_type":"code","source":"class FamilyDataset(Dataset):\n    \"\"\"Family Dataset.\"\"\"\n\n    def __init__(self, df, root_dir, transform=None):\n        \"\"\"\n        Args:\n            df (string): Path to the csv file with annotations.\n            root_dir (string): Directory with all the images.\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        \"\"\"\n        self.relations = df\n        self.root_dir = root_dir\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.relations)\n    \n    def __getpair__(self,idx):\n        pair = self.root_dir+self.relations.iloc[idx,0] + '/' + self.relations.iloc[idx,1],\\\n        self.root_dir+self.relations.iloc[idx,0] + '/' + self.relations.iloc[idx,2]\n        return pair\n    \n    def __getlabel__(self,idx):\n        return self.relations.iloc[idx,3]\n    \n    def __getitem__(self, idx):\n        pair =  self.__getpair__(idx)\n        label = self.__getlabel__(idx)\n        \n        return idx,pair,label","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Initializing Classification Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\n\n# define the NN architecture\nclass ClassificationNet(nn.Module):\n    def __init__(self):\n        super(ClassificationNet, self).__init__()\n                \n        hidden_1 = 3500\n        hidden_2 = 3000\n        hidden_3 = 2500\n        hidden_4 = 2000\n        hidden_5 = 1600\n        hidden_6 = 1200\n        hidden_7 = 1000\n        hidden_8 = 450\n        hidden_9 = 200\n        hidden_10 = 100\n        hidden_11 = 50\n        hidden_12 = 20\n        \n        output = 2\n        \n        self.fc1 = nn.Linear(2622, hidden_1, bias=True)\n        \n        self.fc2 = nn.Linear(hidden_1, hidden_2)\n        self.fc3 = nn.Linear(hidden_2, hidden_3)\n        self.fc4 = nn.Linear(hidden_3, hidden_4)\n        self.fc5 = nn.Linear(hidden_4, hidden_5)\n        self.fc6 = nn.Linear(hidden_5, hidden_6)\n        self.fc7 = nn.Linear(hidden_6, hidden_7)\n        self.fc8 = nn.Linear(hidden_7, hidden_8)\n        self.fc9 = nn.Linear(hidden_8, hidden_9)\n        self.fc10 = nn.Linear(hidden_9, hidden_10)\n        self.fc11 = nn.Linear(hidden_10, hidden_11)\n        self.fc12 = nn.Linear(hidden_11, hidden_12)\n        self.fc13 = nn.Linear(hidden_12, output)\n        \n        # dropout layer (p=0.2)\n        # dropout prevents overfitting of data\n        self.dropout = nn.Dropout(0.2)\n\n    def forward(self, x):\n        \n        # add hidden layer, with relu activation function\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        \n        x = F.relu(self.fc2(x))\n        x = self.dropout(x)\n        \n        x = F.relu(self.fc3(x))\n        x = self.dropout(x)\n        \n        x = F.relu(self.fc4(x))\n        x = self.dropout(x)\n        \n        x = F.relu(self.fc5(x))\n        x = self.dropout(x)\n        \n        x = F.relu(self.fc6(x))\n        x = self.dropout(x)\n        \n        x = F.relu(self.fc7(x))\n        x = self.dropout(x)\n        \n        x = F.relu(self.fc8(x))\n        x = self.dropout(x)\n        \n        x = F.relu(self.fc9(x))\n        x = self.dropout(x)\n        \n        x = F.relu(self.fc10(x))\n        x = self.dropout(x)\n        \n        x = F.relu(self.fc11(x))\n        x = self.dropout(x)\n        \n        x = F.relu(self.fc12(x))\n        x = self.dropout(x)\n        \n        x = self.fc13(x)\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TODO: Define transforms for the training data and testing data\ntransform = transforms.Compose([transforms.Resize(255),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n    ])\n\ntrain_dataset= FamilyDataset(df=df,root_dir=\"../input/train/\",transform=transform)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = ClassificationNet()\nif train_on_gpu:\n    model.cuda()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Initializing Trainloader and Validloader"},{"metadata":{"trusted":true},"cell_type":"code","source":"# number of subprocesses to use for data loading\nnum_workers = 8\n\n# percentage of training set to use as validation\nvalid_size = 0.2\n\n# obtain training indices that will be used for validation\nnum_train = len(train_dataset)\nindices = list(range(num_train))\nnp.random.shuffle(indices)\nsplit = int(np.floor(valid_size * num_train))\ntrain_idx, valid_idx = indices[split:], indices[:split]\n\n# define samplers for obtaining training and validation batches\ntrain_sampler = SubsetRandomSampler(train_idx)\nvalid_sampler = SubsetRandomSampler(valid_idx)\n\ntrain_loader = torch.utils.data.DataLoader(train_dataset,sampler=train_sampler,num_workers=num_workers)\nvalid_loader = torch.utils.data.DataLoader(train_dataset,sampler=valid_sampler,num_workers=num_workers)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Initializing Loss Function & Optimizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"# specify loss function (categorical cross-entropy)\ncriterion = nn.CrossEntropyLoss()\n\n# specify optimizer (stochastic gradient descent) and learning rate = 0.01\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# number of epochs to train the model\nn_epochs = 2\n\n# initialize tracker for minimum validation loss\nvalid_loss_min = np.Inf # set initial \"min\" to infinity\n\nfor epoch in range(n_epochs):\n    # monitor training loss\n    train_loss = 0.0\n    valid_loss = 0.0\n    train_class_correct = list(0 for i in range(2))\n    train_class_total = list(0 for i in range(2))\n    ###################\n    # train the model #\n    ###################\n    model.train()\n    batch=0\n    v_batch=0\n    for i,data, target in train_loader:\n        train_loss = 0.0\n        if os.path.exists(data[0][0]) and os.path.exists(data[1][0]):\n            count=0\n            for person1 in os.listdir(data[0][0]):\n                for person2 in os.listdir(data[1][0]):\n\n                    image1 = Image.open(data[0][0]+'/'+ person1)\n                    x1 = TF.to_tensor(image1)\n                    x1.unsqueeze_(0)\n\n                    image2 = Image.open(data[1][0]+'/'+ person2)\n                    x2 = TF.to_tensor(image2)\n                    x2.unsqueeze_(0)\n\n                    if train_on_gpu:\n                        x1 = x1.cuda()\n                        x2 = x2.cuda()\n\n                    vgg1 = face_model.forward(x1)\n                    vgg2 = face_model.forward(x2)\n\n                    face_distance = vgg1-vgg2\n\n                    if train_on_gpu:\n                        target = target.cuda()\n                        face_distance = face_distance.cuda()\n\n                    optimizer.zero_grad()\n                    output = model(face_distance)\n                    _,pred = torch.max(output,1)\n                    # calculate the loss\n                    loss = criterion(output, target)\n                    # backward pass: compute gradient of the loss with respect to model parameters\n                    loss.backward()\n                    # perform a single optimization step (parameter update)\n                    optimizer.step()\n                    # update running training loss\n                    \n                    correct = pred.eq(target.view_as(pred))\n                    \n                    for i in range(len(target)):\n                        label = target.data\n                        train_class_correct[label] += correct.item()\n                        train_class_total[label] += 1\n                    \n                    \n                    train_loss += loss.item()*len(data)\n                    count+=1\n            batch+=1\n            print('\\r', 'Family', batch, 'Output', output, 'Training Loss: {:.6f}', train_loss/count, end='')\n            \n            if batch%30==0:\n                for i in range(2):\n                    if train_class_total[i] > 0:\n                        print('\\nTraining Accuracy of %5s: %2d%% (%2d/%2d)' % (\n                            str(i), 100 * train_class_correct[i] / train_class_total[i],\n                            np.sum(train_class_correct[i]), np.sum(train_class_total[i])))\n\n                print('\\nTraining Accuracy (Overall): %2d%% (%2d/%2d)' % (\n                    100. * np.sum(train_class_correct) / np.sum(train_class_total),\n                    np.sum(train_class_correct), np.sum(train_class_total)))       \n     \n#             ######################    \n#             # validate the model #\n#             ######################\n            \n    \n    valid_class_correct = list(0 for i in range(2))\n    valid_class_total = list(0 for i in range(2))\n    model.eval() # prep model for evaluation\n    for i,data, target in valid_loader:\n        valid_loss = 0.0\n        if os.path.exists(data[0][0]) and os.path.exists(data[1][0]):\n            count=0\n            for person1 in os.listdir(data[0][0]):\n                for person2 in os.listdir(data[1][0]):\n                    image1 = Image.open(data[0][0]+'/'+ person1)\n                    x1 = TF.to_tensor(image1)\n                    x1.unsqueeze_(0)\n\n                    image2 = Image.open(data[1][0]+'/'+ person2)\n                    x2 = TF.to_tensor(image2)\n                    x2.unsqueeze_(0)\n\n                    if train_on_gpu:\n                        x1 = x1.cuda()\n                        x2 = x2.cuda()\n\n                    vgg1 = face_model.forward(x1)\n                    vgg2 = face_model.forward(x2)\n\n                    face_distance = vgg1-vgg2\n\n                    if train_on_gpu:\n                        target = target.cuda()\n                        face_distance = face_distance.cuda()\n\n                    output = model(face_distance)\n                    # calculate the loss\n                    loss = criterion(output, target)\n                    valid_loss += loss.item()*len(data)\n                    \n                    #Check Predicted Class\n                    _, pred = torch.max(output, 1)\n\n                    #Compare predicted class to the correct class\n                    correct = pred.eq(target.view_as(pred))\n                    \n                    for i in range(len(target)):\n                        label = target.data\n                        valid_class_correct[label]+= correct.item()\n                        valid_class_total[label] += 1\n        \n                    count+=1\n\n            # print training/validation statistics \n            # calculate average loss over an epoch\n        v_batch+=1\n        print('\\r', 'Family', v_batch, 'Validation Loss: {:.6f}', valid_loss/count, end='')\n        if v_batch%30==0:\n            for i in range(2):\n                if valid_class_total[i] > 0:\n                    print('\\nValidation Accuracy of %5s: %2d%% (%2d/%2d)' % (\n                        str(i), 100 * valid_class_correct[i] / valid_class_total[i],\n                        np.sum(valid_class_correct[i]), np.sum(valid_class_total[i])))\n\n            print('\\nValidation Accuracy (Overall): %2d%% (%2d/%2d)' % (\n                100. * np.sum(valid_class_correct) / np.sum(valid_class_total),\n                np.sum(valid_class_correct), np.sum(valid_class_total)))\n            \n        \n        # save model if validation loss has decreased\n        if valid_loss <= valid_loss_min:\n            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n            valid_loss_min,\n            valid_loss))\n            torch.save(model.state_dict(), 'model.pt')\n            valid_loss_min = valid_loss","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that our model is trained, we'll see how the sample submissions are"},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\nsample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new = sample_submission[\"img_pair\"].str.split(\"-\", n = 1, expand = True)\n\n# making separate first name column from new data frame \nsample_submission[\"Person1\"]= new[0]\n# making separate last name column from new data frame \nsample_submission[\"Person2\"]= new[1]\n\n# Dropping old Name columns\nsample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(sample_submission)):\n    person1 = sample_submission.loc[i,'Person1']\n    person2 = sample_submission.loc[i,'Person2']\n    \n    if os.path.exists('../input/test/'+person1) and os.path.exists('../input/test/'+person2):\n        image1 = Image.open('../input/test/'+person1)\n        x1 = TF.to_tensor(image1)\n        x1.unsqueeze_(0)\n\n        image2 = Image.open('../input/test/'+person2)\n        x2 = TF.to_tensor(image2)\n        x2.unsqueeze_(0)\n\n        if train_on_gpu:\n            x1 = x1.cuda()\n            x2 = x2.cuda()\n\n        vgg1 = face_model.forward(x1)\n        vgg2 = face_model.forward(x2)\n\n        face_distance = vgg1-vgg2\n        output = model(face_distance)\n        _,pred = torch.max(output,1)\n        \n        sample_submission.loc[i,'is_related'] = pred.item()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.drop(columns =[\"Person1\",\"Person2\"], inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission['is_related'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output= sample_submission.to_csv('output.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Helper Functions"},{"metadata":{},"cell_type":"markdown","source":"To download output and view images"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import the modules we'll need\nfrom IPython.display import HTML\nimport base64\n\n# function that takes in a dataframe and creates a text link to  \n# download it (will only work for files < 2MB or so)\ndef create_download_link(df, title = \"Download CSV file\", filename = \"data.csv\"):  \n    csv = df.to_csv(index=False)\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text/csv;base64,{payload}\" target=\"_blank\">{title}</a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"create_download_link(sample_submission)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def imshow(image, ax=None, title=None, normalize=True):\n    \"\"\"Imshow for Tensor.\"\"\"\n    if ax is None:\n        fig, ax = plt.subplots()\n    image = image.numpy().transpose((1, 2, 0))\n\n    if normalize:\n        mean = np.array([0.485, 0.456, 0.406])\n        std = np.array([0.229, 0.224, 0.225])\n        image = std * image + mean\n        image = np.clip(image, 0, 1)\n\n    ax.imshow(image)\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.spines['left'].set_visible(False)\n    ax.spines['bottom'].set_visible(False)\n    ax.tick_params(axis='both', length=0)\n    ax.set_xticklabels('')\n    ax.set_yticklabels('')\n\n    return ax","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}