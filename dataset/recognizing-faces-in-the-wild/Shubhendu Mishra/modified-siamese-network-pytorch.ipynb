{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n\n%matplotlib inline\nimport torchvision\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader,Dataset\nimport matplotlib.pyplot as plt\nimport torchvision.utils\nimport numpy as np\nimport random\nfrom PIL import Image\nimport torch\nfrom torch.autograd import Variable\nimport PIL.ImageOps    \nimport torch.nn as nn\nfrom torch import optim\nimport torch.nn.functional as F\nfrom torch.utils.data.sampler import SubsetRandomSampler\nimport itertools\nimport os\nimport pandas as pd\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(os.listdir(\"../input/pytorch-pretrained-models-for-face-detection/\"))\nprint(os.listdir(\"../input/recognizing-faces-in-the-wild/\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking if CUDA is available or not\ntrain_on_gpu = torch.cuda.is_available()\n\nif not train_on_gpu:\n    print('CUDA is not available.  Training on CPU ...')\nelse:\n    print('CUDA is available!  Training on GPU ...')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Helper Functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def imshow(img,text=None,should_save=False):\n    npimg = img.numpy()\n    plt.axis(\"off\")\n    if text:\n        plt.text(75, 8, text, style='italic',fontweight='bold',\n            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()    \n\ndef show_plot(iteration,loss):\n    plt.plot(iteration,loss)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pre processing relationships Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/recognizing-faces-in-the-wild/train_relationships.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see the data has two columns of people that are related. Let's split these columns into three i.e. Family, Person1 and Person2"},{"metadata":{"trusted":true},"cell_type":"code","source":"new = df[\"p1\"].str.split(\"/\", n = 1, expand = True)\n\n# making separate first name column from new data frame \ndf[\"Family1\"]= new[0]\n# making separate last name column from new data frame \ndf[\"Person1\"]= new[1]\n\n# Dropping old Name columns\ndf.drop(columns =[\"p1\"], inplace = True)\n\nnew = df[\"p2\"].str.split(\"/\", n = 1, expand = True)\n\n# making separate first name column from new data frame \ndf[\"Family2\"]= new[0]\n# making separate last name column from new data frame \ndf[\"Person2\"]= new[1]\n\n# Dropping old Name columns\ndf.drop(columns =[\"p2\"], inplace = True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is essential to check if all the folders present in the above DataFrame exist, else it'll be an error while processing the dataset. We should remove the rows that are duplicate and the rows that don't have any folder existing corresponding to their values"},{"metadata":{"trusted":true},"cell_type":"code","source":"root_dir = '../input/recognizing-faces-in-the-wild/train/'\ntemp = []\nfor index, row in df.iterrows():\n    if os.path.exists(root_dir+row.Family1+'/'+row.Person1) and os.path.exists(root_dir+row.Family2+'/'+row.Person2):\n        continue\n    else:\n        temp.append(index)\n        \nprint(len(temp))\ndf = df.drop(temp, axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 236 rows that don't have any folders corresponding to them so they have been removed"},{"metadata":{},"cell_type":"markdown","source":"We can also create some more data from Families for people that are not related. Then we can use this data to train our model on classes 'Related' and 'Not Related'. 'Not Related' instances can be numerous since there are many families that are not correlated. However, that would give a lot of data, We can create new data from people within families that are not related. Eg. A son and daughter would be related to their father and their mother, However, the mother and father won't be related themselves"},{"metadata":{"trusted":true},"cell_type":"code","source":"#A new column in the existing dataframe with all values as 1, since these people are all related\ndf['Related'] = 1\n\n#Creating a dictionary, and storing members of each family\ndf_dict = {}\nfor index, row in df.iterrows():\n    if row['Family1'] in df_dict:\n        df_dict[row['Family1']].append(row['Person1'])\n    else:\n        df_dict[row['Family1']] = [row['Person1']]\n        \n#For each family in this dictionary, we'll first make pairs of people\n#For each pair, we'll check if they're related in our existing Dataset\n#If they're not in the dataframe, means we'll create a row with both persons and related value 0\ni=1\nfor key in df_dict:\n    pair = list(itertools.combinations(df_dict[key], 2))\n    for item in pair:\n        if len(df[(df['Family1']==key)&(df['Person1']==item[0])&(df['Person2']==item[1])])==0 \\\n        and len(df[(df['Family1']==key)&(df['Person1']==item[1])&(df['Person2']==item[0])])==0:\n            new = {'Family1':key,'Person1':item[0],'Family2':key,'Person2':item[1],'Related':0}\n            df=df.append(new,ignore_index=True)\n        \n#Storing rows only where Person1 and Person2 are not same\ndf = df[(df['Person1']!=df['Person2'])]\n\n#len(df[(df['Related']==1)])\n\nprint(df['Related'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From value_counts() it is visible that data is imbalanced and there are many more instances of 1 than 0. So, let's create some more instances of class 0 between two families, such that dataframe becomes balanced"},{"metadata":{"trusted":true},"cell_type":"code","source":"extra = df['Related'].value_counts()[1]-df['Related'].value_counts()[0]\nwhile extra>=0:\n    rows = df.sample(n=2)\n    first = rows.iloc[0,:]\n    second = rows.iloc[1,:]\n    \n    if first.Family1!=second.Family1 and first.Family2!=second.Family2:\n        new1 = {'Family1':first.Family1,'Person1':first.Person1,'Family2':second.Family1,'Person2':second.Person1,'Related':0}\n        extra=extra-1\n        if extra==0:\n            break\n        new2 = {'Family1':first.Family2,'Person1':first.Person2,'Family2':second.Family2,'Person2':second.Person2,'Related':0}\n        extra=extra-1\n        \n        df=df.append(new1,ignore_index=True)\n        df=df.append(new2,ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now all rows with 1 are together and all rows with 0 are together, Rows should be shuffled"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.sample(frac=1).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Related'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dataset is now balanced"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Custom Dataset Class"},{"metadata":{"trusted":true},"cell_type":"code","source":"class FamilyDataset(Dataset):\n    \"\"\"Family Dataset.\"\"\"\n\n    def __init__(self, df, root_dir, transform=None):\n        \"\"\"\n        Args:\n            df (string): Path to the csv file with annotations.\n            root_dir (string): Directory with all the images.\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        \"\"\"\n        self.relations = df\n        self.root_dir = root_dir\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.relations)\n    \n    def __getpair__(self,idx):\n        pair = self.root_dir+self.relations.iloc[idx,0] + '/' + self.relations.iloc[idx,1],\\\n        self.root_dir+self.relations.iloc[idx,2] + '/' + self.relations.iloc[idx,3]\n        return pair\n    \n    def __getlabel__(self,idx):\n        return self.relations.iloc[idx,4]\n    \n    def __getitem__(self, idx):\n        pair =  self.__getpair__(idx)\n        label = self.__getlabel__(idx)\n        \n        first = random.choice(os.listdir(pair[0]))\n        second = random.choice(os.listdir(pair[1]))\n        \n        img0 = Image.open(pair[0] + '/' + first)\n        img1 = Image.open(pair[1] + '/'  + second)\n#         img0 = img0.convert(\"L\")\n#         img1 = img1.convert(\"L\")\n        \n        if self.transform is not None:\n            img0 = self.transform(img0)\n            img1 = self.transform(img1)\n            \n        return idx,img0,img1,label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df,valid_df = np.split(df, [int(.8*len(df))])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transform = transforms.Compose([transforms.Resize(255),\n    transforms.CenterCrop(224),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5], [0.5])])\n\nvalid_transform = transforms.Compose([transforms.Resize(255),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5], [0.5])])\n\ntrain_dataset= FamilyDataset(df=train_df,root_dir=\"../input/recognizing-faces-in-the-wild/train/\",transform=train_transform)\nvalid_dataset = FamilyDataset(df=valid_df,root_dir=\"../input/recognizing-faces-in-the-wild/train/\",transform=valid_transform)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualising Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"vis_dataloader = DataLoader(train_dataset,\n                        shuffle=True,\n                        num_workers=8,\n                        batch_size=8)\ndataiter = iter(vis_dataloader)\n\nexample_batch = next(dataiter)\nconcatenated = torch.cat((example_batch[1],example_batch[2]),0)\nimshow(torchvision.utils.make_grid(concatenated))\nprint(example_batch[3].numpy())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Defining Neural Network"},{"metadata":{"trusted":true},"cell_type":"code","source":"# class SiameseNetwork(nn.Module):\n#     def __init__(self):\n#         super(SiameseNetwork, self).__init__()\n#         self.cnn1 = nn.Sequential(\n#             nn.ReflectionPad2d(1),\n#             nn.Conv2d(1, 4, kernel_size=3),\n#             nn.ReLU(inplace=True),\n#             nn.BatchNorm2d(4),\n            \n#             nn.ReflectionPad2d(1),\n#             nn.Conv2d(4, 8, kernel_size=3),\n#             nn.ReLU(inplace=True),\n#             nn.BatchNorm2d(8),\n\n#             nn.ReflectionPad2d(1),\n#             nn.Conv2d(8, 8, kernel_size=3),\n#             nn.ReLU(inplace=True),\n#             nn.BatchNorm2d(8),\n\n#         )\n\n#         self.fc1 = nn.Sequential(\n#             nn.Linear(8*224*224, 1024),\n#             nn.ReLU(inplace=True),\n\n#             nn.Linear(1024, 1024),\n#             nn.ReLU(inplace=True),\n            \n#             nn.Linear(1024, 512),\n#             nn.ReLU(inplace=True),\n            \n#             nn.Linear(512, 512),\n#             nn.ReLU(inplace=True),\n            \n#             nn.Linear(512, 50),\n#             nn.ReLU(inplace=True),\n            \n#             nn.Linear(50, 25))\n        \n#         self.fc2 = nn.Sequential(\n#             nn.Linear(25,2)\n#         )\n        \n#     def forward_once(self, x):\n#         output = self.cnn1(x)\n#         output = output.view(output.size()[0], -1)\n#         output = self.fc1(output)\n#         return output\n\n#     def forward(self, input1, input2):\n#         output1 = self.forward_once(input1)\n#         output2 = self.forward_once(input2)\n        \n#         euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)\n        \n#         output = self.fc2(output1-output2)\n        \n#         return output","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# VGG Face Net"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Vgg_face_dag(nn.Module):\n\n    def __init__(self):\n        super(Vgg_face_dag, self).__init__()\n        self.meta = {'mean': [129.186279296875, 104.76238250732422, 93.59396362304688],\n                     'std': [1, 1, 1],\n                     'imageSize': [224, 224, 3]}\n        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n        self.relu1_1 = nn.ReLU(inplace=True)\n        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n        self.relu1_2 = nn.ReLU(inplace=True)\n        self.pool1 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n        self.relu2_1 = nn.ReLU(inplace=True)\n        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n        self.relu2_2 = nn.ReLU(inplace=True)\n        self.pool2 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n        self.relu3_1 = nn.ReLU(inplace=True)\n        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n        self.relu3_2 = nn.ReLU(inplace=True)\n        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n        self.relu3_3 = nn.ReLU(inplace=True)\n        self.pool3 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n        self.relu4_1 = nn.ReLU(inplace=True)\n        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n        self.relu4_2 = nn.ReLU(inplace=True)\n        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n        self.relu4_3 = nn.ReLU(inplace=True)\n        self.pool4 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n        self.relu5_1 = nn.ReLU(inplace=True)\n        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n        self.relu5_2 = nn.ReLU(inplace=True)\n        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n        self.relu5_3 = nn.ReLU(inplace=True)\n        self.pool5 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n        self.fc6 = nn.Linear(in_features=25088, out_features=4096, bias=True)\n        self.relu6 = nn.ReLU(inplace=True)\n        self.dropout6 = nn.Dropout(p=0.5)\n        self.fc7 = nn.Linear(in_features=4096, out_features=4096, bias=True)\n        self.relu7 = nn.ReLU(inplace=True)\n        self.dropout7 = nn.Dropout(p=0.5)\n        self.fc8 = nn.Linear(in_features=4096, out_features=2622, bias=True)\n\n    def forward_once(self, x0):\n        x1 = self.conv1_1(x0)\n        x2 = self.relu1_1(x1)\n        x3 = self.conv1_2(x2)\n        x4 = self.relu1_2(x3)\n        x5 = self.pool1(x4)\n        x6 = self.conv2_1(x5)\n        x7 = self.relu2_1(x6)\n        x8 = self.conv2_2(x7)\n        x9 = self.relu2_2(x8)\n        x10 = self.pool2(x9)\n        x11 = self.conv3_1(x10)\n        x12 = self.relu3_1(x11)\n        x13 = self.conv3_2(x12)\n        x14 = self.relu3_2(x13)\n        x15 = self.conv3_3(x14)\n        x16 = self.relu3_3(x15)\n        x17 = self.pool3(x16)\n        x18 = self.conv4_1(x17)\n        x19 = self.relu4_1(x18)\n        x20 = self.conv4_2(x19)\n        x21 = self.relu4_2(x20)\n        x22 = self.conv4_3(x21)\n        x23 = self.relu4_3(x22)\n        x24 = self.pool4(x23)\n        x25 = self.conv5_1(x24)\n        x26 = self.relu5_1(x25)\n        x27 = self.conv5_2(x26)\n        x28 = self.relu5_2(x27)\n        x29 = self.conv5_3(x28)\n        x30 = self.relu5_3(x29)\n        x31_preflatten = self.pool5(x30)\n        x31 = x31_preflatten.view(x31_preflatten.size(0), -1)\n        x32 = self.fc6(x31)\n        x33 = self.relu6(x32)\n        x34 = self.dropout6(x33)\n        x35 = self.fc7(x34)\n        x36 = self.relu7(x35)\n        x37 = self.dropout7(x36)\n        x38 = self.fc8(x37)\n        return x38\n    \n    def forward(self,input1,input2):\n        output1 = self.forward_once(input1)\n        output2 = self.forward_once(input2)\n        \n        #euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)\n        difference = output1 - output2\n        return difference\n\ndef vgg_face_dag(weights_path=None, **kwargs):\n    \"\"\"\n    load imported model instance\n\n    Args:\n        weights_path (str): If set, loads model weights from the given path\n    \"\"\"\n    model = Vgg_face_dag()\n    if weights_path:\n        state_dict = torch.load(weights_path)\n        model.load_state_dict(state_dict)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vggnet = vgg_face_dag(weights_path=\"../input/pytorch-pretrained-models-for-face-detection/VGG Face\")\nvggnet = vggnet.cuda()\nfor param in vggnet.parameters(): \n    param.requires_grad = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# if train_on_gpu:\n#     net = SiameseNetwork().cuda()\n# else:\n#     net= SiameseNetwork()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Classification Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Sequential(\n            nn.Linear(2622, 1024),\n            nn.ReLU(inplace=True),\n            \n            nn.Linear(1024, 512),\n            nn.ReLU(inplace=True),\n            \n            nn.Linear(512, 256),\n            nn.ReLU(inplace=True),\n            \n            nn.Linear(256, 128),\n            nn.ReLU(inplace=True),\n\n            nn.Linear(128, 2))\n  \n    def forward(self, x):\n        x = self.fc1(x)\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model = Model().cuda()\n#net = nn.Sequential(vgg_face_dag(weights_path=\"../input/pytorch-pretrained-models-for-face-detection/VGG Face\").cuda(),Model().cuda())\nnet = Model().cuda()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Optimizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"# specify loss function (categorical cross-entropy)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.008, momentum=0.9)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Configuration Class"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Config():\n    training_dir = \"../input/train/\"\n    testing_dir = \"../input/test/\"\n    batch_size = 64\n    train_number_epochs = 150\n    num_workers = 8","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training and Validation Dataloader"},{"metadata":{"trusted":true},"cell_type":"code","source":"# # percentage of training set to use as validation\n# valid_size = 0.2\n\n# # obtain training indices that will be used for validation\n# num_train = len(train_dataset)\n# indices = list(range(num_train))\n# np.random.shuffle(indices)\n# split = int(np.floor(valid_size * num_train))\n# train_idx, valid_idx = indices[split:], indices[:split]\n\n# # define samplers for obtaining training and validation batches\n# train_sampler = SubsetRandomSampler(train_idx)\n# valid_sampler = SubsetRandomSampler(valid_idx)\n\ntrain_loader = torch.utils.data.DataLoader(train_dataset,shuffle=True,num_workers=Config.num_workers, batch_size = Config.batch_size)\nvalid_loader = torch.utils.data.DataLoader(valid_dataset,shuffle=True,num_workers=Config.num_workers, batch_size = Config.batch_size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_counter = []\ntrain_loss_history = []\ntrain_iteration_number= 0\n\nvalid_counter = []\nvalid_loss_history = []\nvalid_iteration_number= 0\n\n# initialize tracker for minimum validation loss\nvalid_loss_min = np.Inf # set initial \"min\" to infinity\n\ntrain_class_correct = list(0 for i in range(2))\ntrain_class_total = list(0 for i in range(2))\n\nvalid_class_correct = list(0 for i in range(2))\nvalid_class_total = list(0 for i in range(2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"for epoch in range(0,Config.train_number_epochs):\n    train_loss = 0.0\n    valid_loss = 0.0\n    net.train()\n    for i, data in enumerate(train_loader,0):\n        row, img0, img1 , label = data\n        row, img0, img1 , label = row.cuda(), img0.cuda(), img1.cuda() , label.cuda()\n        \n        optimizer.zero_grad()\n        output1= vggnet(img0,img1)\n        output = net(output1)\n        _, pred= torch.max(output,1)\n\n        loss = criterion(output,label)\n        loss.backward()\n        \n        optimizer.step()\n        \n        correct = pred.eq(label.view_as(pred))\n        for j in range(len(label)):\n                        target = label[j].data\n                        train_class_correct[target] += correct[j].item()\n                        train_class_total[target] += 1\n                        \n        if i%30 == 0:\n            print(\"Epoch number {}\\n Current loss {}\\n\".format(epoch+1,loss.item()))\n            train_iteration_number +=30\n            train_counter.append(train_iteration_number)\n            train_loss_history.append(loss.item())\n\n            for i in range(2):\n                if train_class_total[i] > 0:\n                        print('\\nTraining Accuracy of %5s: %2d%% (%2d/%2d)' % (\n                            str(i), 100 * train_class_correct[i] / train_class_total[i],\n                            np.sum(train_class_correct[i]), np.sum(train_class_total[i])))\n\n            print('\\nTraining Accuracy (Overall): %2d%% (%2d/%2d)' % (\n                100. * np.sum(train_class_correct) / np.sum(train_class_total),\n                np.sum(train_class_correct), np.sum(train_class_total)))\n    \n    net.eval()\n    for i, data in enumerate(valid_loader,0):\n        row, img0, img1 , label = data\n        row, img0, img1 , label = row.cuda(), img0.cuda(), img1.cuda() , label.cuda()\n        \n        output1= vggnet(img0,img1)\n        output = net(output1)\n        #combined = torch.cat([vgg,res1,sen1],1)\n        #output= net(combined)\n        _, pred= torch.max(output,1)\n\n        loss = criterion(output,label)\n        \n        correct = pred.eq(label.view_as(pred))\n        for j in range(len(label)):\n                        target = label[j].data\n                        valid_class_correct[target] += correct[j].item()\n                        valid_class_total[target] += 1\n        valid_loss += loss.item()                \n        if i%30 == 0:\n            print(\"Epoch number {}\\n Current loss {}\\n\".format(epoch+1,loss.item()))\n            valid_iteration_number +=30\n            valid_counter.append(valid_iteration_number)\n            valid_loss_history.append(loss.item())\n\n            for i in range(2):\n                if train_class_total[i] > 0:\n                        print('\\nValdiation Accuracy of %5s: %2d%% (%2d/%2d)' % (\n                            str(i), 100 * valid_class_correct[i] / valid_class_total[i],\n                            np.sum(valid_class_correct[i]), np.sum(valid_class_total[i])))\n\n            print('\\nValdiation Accuracy (Overall): %2d%% (%2d/%2d)' % (\n                100. * np.sum(valid_class_correct) / np.sum(valid_class_total),\n                np.sum(valid_class_correct), np.sum(valid_class_total)))\n            \n    if valid_loss <= valid_loss_min:\n        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n        valid_loss_min,\n        valid_loss))\n        torch.save(net.state_dict(), 'model.pt')\n        valid_loss_min = valid_loss","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training and Validation Loss"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('\\n Training Loss History')\nshow_plot(train_counter,train_loss_history)\n\nprint('\\n Validation Loss History')\nshow_plot(valid_counter,valid_loss_history)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading Saved Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"net.load_state_dict(torch.load('model.pt'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that our model is trained, we'll see how the sample submissions are"},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission = pd.read_csv(\"../input/recognizing-faces-in-the-wild/sample_submission.csv\")\nsample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new = sample_submission[\"img_pair\"].str.split(\"-\", n = 1, expand = True)\n\n# making separate first name column from new data frame \nsample_submission[\"Person1\"]= new[0]\n# making separate last name column from new data frame \nsample_submission[\"Person2\"]= new[1]\n\n# Dropping old Name columns\nsample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Custom Test Dataset\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"class FamilyTestDataset(Dataset):\n    \"\"\"Family Dataset.\"\"\"\n\n    def __init__(self, df, root_dir, transform=None):\n        \"\"\"\n        Args:\n            df (string): Path to the csv file with annotations.\n            root_dir (string): Directory with all the images.\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        \"\"\"\n        self.relations = df\n        self.root_dir = root_dir\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.relations)\n    \n    def __getpair__(self,idx):\n        pair = self.root_dir+self.relations.iloc[idx,2],\\\n        self.root_dir+self.relations.iloc[idx,3]\n        return pair\n    \n    def __getlabel__(self,idx):\n        return self.relations.iloc[idx,4]\n    \n    def __getitem__(self, idx):\n        pair =  self.__getpair__(idx)\n        \n        img0 = Image.open(pair[0])\n        img1 = Image.open(pair[1])\n#         img0 = img0.convert(\"L\")\n#         img1 = img1.convert(\"L\")\n        \n        if self.transform is not None:\n            img0 = self.transform(img0)\n            img1 = self.transform(img1)\n            \n        return idx,img0,img1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transform = transforms.Compose([transforms.Resize(255),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5], [0.5])])\n\ntest_dataset= FamilyTestDataset(df=sample_submission,root_dir=\"../input/recognizing-faces-in-the-wild/test/\",transform=transform)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_loader = torch.utils.data.DataLoader(test_dataset,shuffle=True,num_workers=Config.num_workers, batch_size = Config.batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"net.eval()\nfor i, data in enumerate(test_loader,0):\n    row, img0, img1 = data\n    row, img0, img1 = row.cuda(), img0.cuda(), img1.cuda()\n   \n    output1= vggnet(img0,img1)\n    output = net(output1)\n    #output= net(img0,img1)\n    _, pred= torch.max(output,1)\n     \n    count=0\n    for item in row:\n        sample_submission.loc[item,'is_related'] = pred[count].item()\n        count+=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.drop(columns =[\"Person1\",\"Person2\"], inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission['is_related'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output= sample_submission.to_csv('output.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import the modules we'll need\nfrom IPython.display import HTML\nimport base64\n\n# function that takes in a dataframe and creates a text link to  \n# download it (will only work for files < 2MB or so)\ndef create_download_link(df, title = \"Download CSV file\", filename = \"data.csv\"):  \n    csv = df.to_csv(index=False)\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text/csv;base64,{payload}\" target=\"_blank\">{title}</a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"create_download_link(sample_submission)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}