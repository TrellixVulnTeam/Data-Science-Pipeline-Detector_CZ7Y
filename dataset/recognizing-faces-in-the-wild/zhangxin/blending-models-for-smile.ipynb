{"cells":[{"metadata":{"trusted":true},"cell_type":"markdown","source":"In this script, I blend two model for increasing the accuracy."},{"metadata":{},"cell_type":"markdown","source":"# 1 model1--VGGFace + CV"},{"metadata":{},"cell_type":"markdown","source":"## 1.1 Train"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nimport gc\n\nfrom collections import defaultdict\nfrom glob import glob\nfrom random import choice, sample\nfrom keras.preprocessing import image\nimport cv2\nfrom tqdm import tqdm_notebook\nimport numpy as np\nimport pandas as pd\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\nfrom keras.layers import Input, Dense, GlobalMaxPool2D, GlobalAvgPool2D, Concatenate, Multiply, Dropout, Subtract\nfrom keras.models import Model\nfrom keras.optimizers import Adam\n\n!pip install git+https://github.com/rcmalli/keras-vggface.git\n    \nfrom keras_vggface.utils import preprocess_input\nfrom keras_vggface.vggface import VGGFace\n\ntrain_file_path = \"../input/train_relationships.csv\"\ntrain_folders_path = \"../input/train/\"\n\n# val_famillies_list = [\"F01\", \"F02\", \"F03\", \"F04\", \"F05\", \"F06\", \"F07\", \"F08\", \"F09\"]\nval_famillies_list = [\"F07\", \"F08\", \"F09\"]\n\nall_images = glob(train_folders_path + \"*/*/*.jpg\")\nrelationships = pd.read_csv(train_file_path)\n\ndef get_train_val(family_name, relationships=relationships):\n    # Get val_person_image_map\n    val_famillies = family_name\n    train_images = [x for x in all_images if val_famillies not in x]\n    val_images = [x for x in all_images if val_famillies in x]\n\n    train_person_to_images_map = defaultdict(list)\n\n    ppl = [x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2] for x in all_images]\n\n    for x in train_images:\n        train_person_to_images_map[x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2]].append(x)\n\n    val_person_to_images_map = defaultdict(list)\n\n    for x in val_images:\n        val_person_to_images_map[x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2]].append(x)\n        \n    # Get the train and val dataset\n#     relationships = pd.read_csv(train_file_path)\n    relationships = list(zip(relationships.p1.values, relationships.p2.values))\n    relationships = [x for x in relationships if x[0] in ppl and x[1] in ppl]\n\n    train = [x for x in relationships if val_famillies not in x[0]]\n    val = [x for x in relationships if val_famillies in x[0]]\n    \n    return train, val, train_person_to_images_map, val_person_to_images_map\n\ndef read_img(path):\n    img = image.load_img(path, target_size=(197, 197))\n    img = np.array(img).astype(np.float)\n    return preprocess_input(img, version=2)\n\ndef gen(list_tuples, person_to_images_map, batch_size=16):\n    ppl = list(person_to_images_map.keys())\n    while True:\n        batch_tuples = sample(list_tuples, batch_size // 2)\n        labels = [1] * len(batch_tuples)\n        while len(batch_tuples) < batch_size:\n            p1 = choice(ppl)\n            p2 = choice(ppl)\n\n            if p1 != p2 and (p1, p2) not in list_tuples and (p2, p1) not in list_tuples:\n                batch_tuples.append((p1, p2))\n                labels.append(0)\n\n        for x in batch_tuples:\n            if not len(person_to_images_map[x[0]]):\n                print(x[0])\n\n        X1 = [choice(person_to_images_map[x[0]]) for x in batch_tuples]\n        X1 = np.array([read_img(x) for x in X1])\n\n        X2 = [choice(person_to_images_map[x[1]]) for x in batch_tuples]\n        X2 = np.array([read_img(x) for x in X2])\n\n        yield [X1, X2], labels\n\n\ndef baseline_model():\n    input_1 = Input(shape=(197, 197, 3))\n    input_2 = Input(shape=(197, 197, 3))\n\n    base_model = VGGFace(model='resnet50', include_top=False)\n\n    for x in base_model.layers[:-3]:\n        x.trainable = True\n    for x in base_model.layers[-3:]:\n        x.trainable=False\n\n    x1 = base_model(input_1)\n    x2 = base_model(input_2)\n\n#     x1_ = Reshape(target_shape=(7*7, 2048))(x1)\n#     x2_ = Reshape(target_shape=(7*7, 2048))(x2)\n#     #\n#     x_dot = Dot(axes=[2, 2], normalize=True)([x1_, x2_])\n#     x_dot = Flatten()(x_dot)\n\n    x1 = Concatenate(axis=-1)([GlobalMaxPool2D()(x1), GlobalAvgPool2D()(x1)])\n    x2 = Concatenate(axis=-1)([GlobalMaxPool2D()(x2), GlobalAvgPool2D()(x2)])\n\n    x3 = Subtract()([x1, x2])\n    x3 = Multiply()([x3, x3])\n\n    x1_ = Multiply()([x1, x1])\n    x2_ = Multiply()([x2, x2])\n    x4 = Subtract()([x1_, x2_])\n    x = Concatenate(axis=-1)([x4, x3])\n\n    x = Dense(100, activation=\"relu\")(x)\n    x = Dropout(0.3)(x)\n    x = Dense(25, activation=\"relu\")(x)\n    x = Dropout(0.3)(x)\n    out = Dense(1, activation=\"sigmoid\")(x)\n\n    model = Model([input_1, input_2], out)\n\n    model.compile(loss=\"binary_crossentropy\", metrics=['acc'], optimizer=Adam(0.00001))\n\n    model.summary()\n\n    return model\n\nmodel1 = baseline_model()\nn_val_famillies_list = len(val_famillies_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_acc_list = []\ndef train_model1():\n    for i in tqdm_notebook(range(n_val_famillies_list)):\n        train, val, train_person_to_images_map, val_person_to_images_map = get_train_val(val_famillies_list[i])\n        file_path = f\"vgg_face_{i}.h5\"\n        checkpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n        reduce_on_plateau = ReduceLROnPlateau(monitor=\"val_acc\", mode=\"max\", factor=0.1, patience=10, verbose=1)\n        es = EarlyStopping(monitor=\"val_acc\", min_delta = 0.001, patience=20, verbose=1)\n        callbacks_list = [checkpoint, reduce_on_plateau, es]\n\n        history = model1.fit_generator(gen(train, train_person_to_images_map, batch_size=16), \n                                      use_multiprocessing=True,\n                                      validation_data=gen(val, val_person_to_images_map, batch_size=16), \n                                      epochs=200, verbose=0,\n                                      workers=4, callbacks=callbacks_list, \n                                      steps_per_epoch=300, validation_steps=150)\n        val_acc_list.append(np.max(history.history['val_acc']))\n        \ntrain_model1()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_acc_list","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.2 Inference"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_rate_list = val_acc_list / np.sum(val_acc_list)\nmodel_rate_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nmodel1_weight = np.matmul(val_acc_list, model_rate_list)\nmodel1_weight","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"test_path = \"../input/test/\"\n\nsubmission = pd.read_csv('../input/sample_submission.csv')\n\ndef chunker(seq, size=32):\n    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n\ndef get_pred1():\n    preds_for_sub = np.zeros(submission.shape[0])\n    for i in tqdm_notebook(range(n_val_famillies_list)):\n    # for i in tqdm_notebook(range(1)):\n        file_path = f\"vgg_face_{i}.h5\"\n        model1.load_weights(file_path)\n        # Get the predictions\n        predictions = []\n\n        for batch in tqdm_notebook(chunker(submission.img_pair.values)):\n            X1 = [x.split(\"-\")[0] for x in batch]\n            X1 = np.array([read_img(test_path + x) for x in X1])\n\n            X2 = [x.split(\"-\")[1] for x in batch]\n            X2 = np.array([read_img(test_path + x) for x in X2])\n\n            pred = model1.predict([X1, X2]).ravel().tolist()\n            predictions += pred\n        preds_for_sub += np.array(predictions) * model_rate_list[i]\n        return preds_for_sub\n    \npreds_model1 = get_pred1()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# del train, val, train_person_to_images_map, val_person_to_images_map\n# del preds_for_sub, model\n# del X1, X2\n# del pred\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2 model2--Kinship Detection with VGG16\n\nhttps://www.kaggle.com/arjunrao2000/kinship-detection-with-vgg16"},{"metadata":{"trusted":true},"cell_type":"code","source":"import h5py\nfrom collections import defaultdict\nfrom glob import glob\nfrom random import choice, sample\n\nimport cv2\nimport numpy as np\nimport pandas as pd\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\nfrom keras.layers import Input, Dense, Flatten, GlobalMaxPool2D, GlobalAvgPool2D, Concatenate, Multiply, Dropout, Subtract, Add, Conv2D\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nfrom keras_vggface.utils import preprocess_input\nfrom keras_vggface.vggface import VGGFace","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_file_path = \"../input/train_relationships.csv\"\ntrain_folders_path = \"../input/train/\"\n# val_famillies_list = [\"F01\", \"F02\", \"F03\", \"F04\", \"F05\", \"F06\", \"F07\", \"F08\", \"F09\"]\nval_famillies_list = [\"F07\", \"F08\", \"F09\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def baseline_model():\n    input_1 = Input(shape=(197, 197, 3))\n    input_2 = Input(shape=(197, 197, 3))\n\n    base_model = VGGFace(model='resnet50', include_top=False)\n\n    for layer in base_model.layers[:-3]:\n        layer.trainable = True\n\n    x1 = base_model(input_1)\n    x2 = base_model(input_2)\n\n    merged_add = Add()([x1, x2])\n    merged_sub = Subtract()([x1,x2])\n    \n    merged_add = Conv2D(100 , [1,1] )(merged_add)\n    merged_sub = Conv2D(100 , [1,1] )(merged_sub)\n    \n    merged = Concatenate(axis=-1)([merged_add, merged_sub])\n    \n    merged = Flatten()(merged)\n\n    merged = Dense(100, activation=\"relu\")(merged)\n    merged = Dropout(0.3)(merged)\n    merged = Dense(25, activation=\"relu\")(merged)\n    merged = Dropout(0.3)(merged)\n    out = Dense(1, activation=\"sigmoid\")(merged)\n\n    model = Model([input_1, input_2], out)\n\n    model.compile(loss=\"binary_crossentropy\", metrics=['acc'], optimizer=Adam(0.00001))\n\n    model.summary()\n\n    return model\n\n\nmodel2 = baseline_model()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_acc_list = []\ndef train_model2():\n    for i in tqdm_notebook(range(n_val_famillies_list)):\n        train, val, train_person_to_images_map, val_person_to_images_map = get_train_val(val_famillies_list[i])\n        file_path = f\"vgg_face_{i}.h5\"\n        checkpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n        reduce_on_plateau = ReduceLROnPlateau(monitor=\"val_acc\", mode=\"max\", factor=0.1, patience=10, verbose=0)\n        es = EarlyStopping(monitor=\"val_acc\", min_delta = 0.001, patience=20, verbose=1)\n        callbacks_list = [checkpoint, reduce_on_plateau, es]\n\n        history = model2.fit_generator(gen(train, train_person_to_images_map, batch_size=16), \n                                      use_multiprocessing=True,\n                                      validation_data=gen(val, val_person_to_images_map, batch_size=16), \n                                      epochs=200, verbose=1,\n                                      workers=4, callbacks=callbacks_list, \n                                      steps_per_epoch=300, validation_steps=150)\n        val_acc_list.append(np.max(history.history['val_acc']))\n\ntrain_model2()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_rate_list = val_acc_list / np.sum(val_acc_list)\nmodel_rate_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model2_weight = np.matmul(val_acc_list, model_rate_list)\nmodel2_weight","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_path = \"../input/test/\"\n\nsubmission = pd.read_csv('../input/sample_submission.csv')\n\ndef chunker(seq, size=32):\n    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n\ndef get_pred2():\n    preds_for_sub = np.zeros(submission.shape[0])\n    for i in tqdm_notebook(range(n_val_famillies_list)):\n    # for i in tqdm_notebook(range(1)):\n        file_path = f\"vgg_face_{i}.h5\"\n        model2.load_weights(file_path)\n        # Get the predictions\n        predictions = []\n\n        for batch in tqdm_notebook(chunker(submission.img_pair.values)):\n            X1 = [x.split(\"-\")[0] for x in batch]\n            X1 = np.array([read_img(test_path + x) for x in X1])\n\n            X2 = [x.split(\"-\")[1] for x in batch]\n            X2 = np.array([read_img(test_path + x) for x in X2])\n\n            pred = model2.predict([X1, X2]).ravel().tolist()\n            predictions += pred\n        preds_for_sub += np.array(predictions) * model_rate_list[i]\n        return preds_for_sub\n    \npreds_model2 = get_pred2()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Blending the models above"},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = (preds_model1 * model1_weight + preds_model2 * model2_weight) / (model1_weight + model2_weight)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission['is_related'] = preds\nsubmission.to_csv(\"vgg_face.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}