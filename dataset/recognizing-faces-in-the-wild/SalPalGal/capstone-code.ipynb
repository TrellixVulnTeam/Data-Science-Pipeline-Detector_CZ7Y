{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\n\nfrom collections import defaultdict\nfrom glob import glob\nfrom random import choice, sample\nfrom keras.preprocessing import image\nimport cv2\nfrom tqdm import tqdm_notebook\nimport numpy as np\nimport pandas as pd\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\nfrom keras.layers import Input, Dense, GlobalMaxPool2D, GlobalAvgPool2D, Concatenate, Multiply, Dropout, Subtract\nfrom keras.models import Model\nfrom keras.optimizers import Adam","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-05T07:57:02.178213Z","iopub.execute_input":"2021-06-05T07:57:02.178501Z","iopub.status.idle":"2021-06-05T07:57:04.60916Z","shell.execute_reply.started":"2021-06-05T07:57:02.178452Z","shell.execute_reply":"2021-06-05T07:57:04.608426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.listdir()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T08:02:29.823318Z","iopub.execute_input":"2021-06-05T08:02:29.823611Z","iopub.status.idle":"2021-06-05T08:02:29.831498Z","shell.execute_reply.started":"2021-06-05T08:02:29.823562Z","shell.execute_reply":"2021-06-05T08:02:29.830405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# os.chdir(\"kaggle/input/kerasvggface1/keras-vggface-master\")\nos.chdir(\"..\")\nos.getcwd()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T08:06:15.436973Z","iopub.execute_input":"2021-06-05T08:06:15.437266Z","iopub.status.idle":"2021-06-05T08:06:15.442753Z","shell.execute_reply.started":"2021-06-05T08:06:15.437219Z","shell.execute_reply":"2021-06-05T08:06:15.442017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!pip install git+https://github.com/hamidomar/keras-vggface","metadata":{"execution":{"iopub.status.busy":"2021-06-05T08:04:09.363996Z","iopub.execute_input":"2021-06-05T08:04:09.364334Z","iopub.status.idle":"2021-06-05T08:04:09.371008Z","shell.execute_reply.started":"2021-06-05T08:04:09.364289Z","shell.execute_reply":"2021-06-05T08:04:09.370345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# code is working without this as well so it doesn't really matter\n# !cp -r ../input/kerasvggface1/* ./","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.status.busy":"2021-06-05T08:06:19.558315Z","iopub.execute_input":"2021-06-05T08:06:19.558603Z","iopub.status.idle":"2021-06-05T08:06:20.186189Z","shell.execute_reply.started":"2021-06-05T08:06:19.558552Z","shell.execute_reply":"2021-06-05T08:06:20.185458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#run this cell before the next\nos.chdir(\"/kaggle/input/kerasvggface1/keras-vggface-master\")","metadata":{"execution":{"iopub.status.busy":"2021-06-05T08:06:28.0018Z","iopub.execute_input":"2021-06-05T08:06:28.002111Z","iopub.status.idle":"2021-06-05T08:06:28.006417Z","shell.execute_reply.started":"2021-06-05T08:06:28.002058Z","shell.execute_reply":"2021-06-05T08:06:28.005312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras_vggface.utils import preprocess_input\nfrom keras_vggface.vggface import VGGFace","metadata":{"execution":{"iopub.status.busy":"2021-06-05T08:06:28.846248Z","iopub.execute_input":"2021-06-05T08:06:28.846538Z","iopub.status.idle":"2021-06-05T08:06:28.916629Z","shell.execute_reply.started":"2021-06-05T08:06:28.846488Z","shell.execute_reply":"2021-06-05T08:06:28.915948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_file_path = \"../input/rfiwsmile/recognizing-faces-in-the-wild/train_relationships.csv\"\ntrain_folders_path = \"../input/rfiwsmile/recognizing-faces-in-the-wild/test\"","metadata":{"execution":{"iopub.status.busy":"2021-06-05T08:06:55.049127Z","iopub.execute_input":"2021-06-05T08:06:55.04942Z","iopub.status.idle":"2021-06-05T08:06:55.054582Z","shell.execute_reply.started":"2021-06-05T08:06:55.049373Z","shell.execute_reply":"2021-06-05T08:06:55.053832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.getcwd()","metadata":{"execution":{"iopub.status.busy":"2021-06-03T19:40:22.036492Z","iopub.execute_input":"2021-06-03T19:40:22.036784Z","iopub.status.idle":"2021-06-03T19:40:22.041542Z","shell.execute_reply.started":"2021-06-03T19:40:22.036739Z","shell.execute_reply":"2021-06-03T19:40:22.040816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_dirs = os.listdir('../input/train/')\n# len(train_dirs), train_dirs","metadata":{"execution":{"iopub.status.busy":"2021-06-03T19:40:23.20527Z","iopub.execute_input":"2021-06-03T19:40:23.205623Z","iopub.status.idle":"2021-06-03T19:40:23.209144Z","shell.execute_reply.started":"2021-06-03T19:40:23.205567Z","shell.execute_reply":"2021-06-03T19:40:23.208274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_famillies_list = [\"F07\", \"F08\", \"F09\"]\n# val_famillies_list = [\"F09\"]","metadata":{"execution":{"iopub.status.busy":"2021-06-05T08:06:59.275779Z","iopub.execute_input":"2021-06-05T08:06:59.276067Z","iopub.status.idle":"2021-06-05T08:06:59.283069Z","shell.execute_reply.started":"2021-06-05T08:06:59.276021Z","shell.execute_reply":"2021-06-05T08:06:59.282181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cwd ='/kaggle/input'\n#  \"/input/rfiwsmile/recognizing-faces-in-the-wild/train_relationships.csv\"","metadata":{"execution":{"iopub.status.busy":"2021-06-03T19:40:27.173452Z","iopub.execute_input":"2021-06-03T19:40:27.173745Z","iopub.status.idle":"2021-06-03T19:40:27.177433Z","shell.execute_reply.started":"2021-06-03T19:40:27.173697Z","shell.execute_reply":"2021-06-03T19:40:27.176262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nall_images = glob(train_folders_path + \"*/*/*/*/*\")","metadata":{"execution":{"iopub.status.busy":"2021-06-05T08:08:30.688056Z","iopub.execute_input":"2021-06-05T08:08:30.688346Z","iopub.status.idle":"2021-06-05T08:09:14.330643Z","shell.execute_reply.started":"2021-06-05T08:08:30.688298Z","shell.execute_reply":"2021-06-05T08:09:14.329187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_train_val(family_name):\n    # Get val_person_image_map\n    val_famillies = family_name\n    train_images = [x for x in all_images if val_famillies not in x] # train_img if val_fam not given\n    val_images = [x for x in all_images if val_famillies in x] # val_img if val_fam given\n\n    train_person_to_images_map = defaultdict(list) # initialises a default dict which will not give key error\n\n    ppl = [x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2] for x in all_images] # EXTRACT NAME VALUES\n\n    for x in train_images:\n        train_person_to_images_map[x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2]].append(x) \n\n    val_person_to_images_map = defaultdict(list)\n\n    for x in val_images:\n        val_person_to_images_map[x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2]].append(x)\n        \n    # Get the train and val dataset\n    relationships = pd.read_csv(train_file_path)\n    relationships = list(zip(relationships.p1.values, relationships.p2.values))\n    relationships = [x for x in relationships if x[0] in ppl and x[1] in ppl]\n\n    train = [x for x in relationships if val_famillies not in x[0]]\n    val = [x for x in relationships if val_famillies in x[0]]\n    \n    return train, val, train_person_to_images_map, val_person_to_images_map","metadata":{"execution":{"iopub.status.busy":"2021-06-05T08:07:03.548328Z","iopub.execute_input":"2021-06-05T08:07:03.548615Z","iopub.status.idle":"2021-06-05T08:07:03.559513Z","shell.execute_reply.started":"2021-06-05T08:07:03.548567Z","shell.execute_reply":"2021-06-05T08:07:03.558707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_img(path):\n    img = image.load_img(path, target_size=(197, 197))\n    img = np.array(img).astype(np.float)\n    return preprocess_input(img, version=2)\n\ndef gen(list_tuples, person_to_images_map, batch_size=16):\n    ppl = list(person_to_images_map.keys())\n    while True:\n        batch_tuples = sample(list_tuples, batch_size // 2)\n        labels = [1] * len(batch_tuples)\n        while len(batch_tuples) < batch_size:\n            p1 = choice(ppl)\n            p2 = choice(ppl)\n\n            if p1 != p2 and (p1, p2) not in list_tuples and (p2, p1) not in list_tuples:\n                batch_tuples.append((p1, p2))\n                labels.append(0)\n\n        for x in batch_tuples:\n            if not len(person_to_images_map[x[0]]):\n                print(x[0])\n\n        X1 = [choice(person_to_images_map[x[0]]) for x in batch_tuples]\n        X1 = np.array([read_img(x) for x in X1])\n\n        X2 = [choice(person_to_images_map[x[1]]) for x in batch_tuples]\n        X2 = np.array([read_img(x) for x in X2])\n\n        yield [X1, X2], labels\n\nimport tensorflow as tf\nfrom keras import backend as K\ndef focal_loss(gamma=2., alpha=.25):\n    def focal_loss_fixed(y_true, y_pred):\n        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(K.epsilon()+pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0 + K.epsilon()))\n    return focal_loss_fixed\n\n\ndef baseline_model():\n    input_1 = Input(shape=(197, 197, 3))\n    input_2 = Input(shape=(197, 197, 3))\n\n    base_model = VGGFace(model='resnet50', include_top=False)\n\n    for x in base_model.layers[:-3]:\n        x.trainable = True\n    for x in base_model.layers[-3:]:\n        x.trainable=False\n\n    x1 = base_model(input_1)\n    x2 = base_model(input_2)\n\n#     x1_ = Reshape(target_shape=(7*7, 2048))(x1)\n#     x2_ = Reshape(target_shape=(7*7, 2048))(x2)\n#     #\n#     x_dot = Dot(axes=[2, 2], normalize=True)([x1_, x2_])\n#     x_dot = Flatten()(x_dot)\n\n    x1 = Concatenate(axis=-1)([GlobalMaxPool2D()(x1), GlobalAvgPool2D()(x1)])\n    x2 = Concatenate(axis=-1)([GlobalMaxPool2D()(x2), GlobalAvgPool2D()(x2)])\n\n    x3 = Subtract()([x1, x2])\n    x3 = Multiply()([x3, x3])\n\n    x1_ = Multiply()([x1, x1])\n    x2_ = Multiply()([x2, x2])\n    x4 = Subtract()([x1_, x2_])\n    x = Concatenate(axis=-1)([x4, x3])\n    x = Dropout(0.01)(x)\n    x = Dense(100, activation=\"relu\")(x)\n    x = Dropout(0.01)(x)\n    out = Dense(1, activation=\"sigmoid\")(x)\n\n    model = Model([input_1, input_2], out)\n    \n    # loss=\"binary_crossentropy\"\n    model.compile(loss=[focal_loss(alpha=.25, gamma=2)], \n                  metrics=['acc'], \n                  optimizer=Adam(0.00003))\n\n    model.summary()\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-06-05T08:07:04.300642Z","iopub.execute_input":"2021-06-05T08:07:04.301005Z","iopub.status.idle":"2021-06-05T08:07:04.324814Z","shell.execute_reply.started":"2021-06-05T08:07:04.300954Z","shell.execute_reply":"2021-06-05T08:07:04.32381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.chdir(\"../..\")\nos.getcwd()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T08:07:50.678182Z","iopub.execute_input":"2021-06-05T08:07:50.678479Z","iopub.status.idle":"2021-06-05T08:07:50.685822Z","shell.execute_reply.started":"2021-06-05T08:07:50.678429Z","shell.execute_reply":"2021-06-05T08:07:50.682892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = baseline_model()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T08:07:55.071924Z","iopub.execute_input":"2021-06-05T08:07:55.072214Z","iopub.status.idle":"2021-06-05T08:08:14.923143Z","shell.execute_reply.started":"2021-06-05T08:07:55.072167Z","shell.execute_reply":"2021-06-05T08:08:14.922426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. Train","metadata":{}},{"cell_type":"code","source":"n_val_famillies_list = len(val_famillies_list)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T08:08:22.980278Z","iopub.execute_input":"2021-06-05T08:08:22.98061Z","iopub.status.idle":"2021-06-05T08:08:22.9853Z","shell.execute_reply.started":"2021-06-05T08:08:22.980556Z","shell.execute_reply":"2021-06-05T08:08:22.984395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_val_famillies_list","metadata":{"execution":{"iopub.status.busy":"2021-06-05T08:08:23.145842Z","iopub.execute_input":"2021-06-05T08:08:23.146105Z","iopub.status.idle":"2021-06-05T08:08:23.152942Z","shell.execute_reply.started":"2021-06-05T08:08:23.146059Z","shell.execute_reply":"2021-06-05T08:08:23.150442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in tqdm_notebook(range(n_val_famillies_list)):\n    train, val, train_person_to_images_map, val_person_to_images_map = get_train_val(val_famillies_list[i])\n    file_path = f\"/kaggle/working/vgg_face_0.h5\"\n    checkpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n    reduce_on_plateau = ReduceLROnPlateau(monitor=\"val_acc\", mode=\"max\", factor=0.5, patience=10, verbose=1)\n    es = EarlyStopping(monitor=\"val_acc\", min_delta = 0.001, patience=15, verbose=1)\n    callbacks_list = [checkpoint, reduce_on_plateau, es]\n\n    history = model.fit_generator(gen(train, train_person_to_images_map, batch_size=32), \n                                  use_multiprocessing=True,\n                                  validation_data=gen(val, val_person_to_images_map, batch_size=32), \n                                  epochs=100, verbose=1,\n                                  workers=4, callbacks=callbacks_list, \n                                  steps_per_epoch=400, \n                                  validation_steps=500)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T08:09:31.029153Z","iopub.execute_input":"2021-06-05T08:09:31.029456Z","iopub.status.idle":"2021-06-05T13:45:20.46348Z","shell.execute_reply.started":"2021-06-05T08:09:31.029407Z","shell.execute_reply":"2021-06-05T13:45:20.462086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('val_loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-03T21:52:21.978211Z","iopub.execute_input":"2021-06-03T21:52:21.982244Z","iopub.status.idle":"2021-06-03T21:52:23.909264Z","shell.execute_reply.started":"2021-06-03T21:52:21.982179Z","shell.execute_reply":"2021-06-03T21:52:23.908523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Inference","metadata":{}},{"cell_type":"code","source":"test_path = \"../input/newtest/test/\"\n\nsubmission = pd.read_csv('../input/rfiwsmile/recognizing-faces-in-the-wild/sample_submission.csv')\n\ndef chunker(seq, size=32):\n    return (seq[pos:pos + size] for pos in range(0, len(seq), size))","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:05:31.410325Z","iopub.execute_input":"2021-06-05T14:05:31.410625Z","iopub.status.idle":"2021-06-05T14:05:31.439612Z","shell.execute_reply.started":"2021-06-05T14:05:31.410573Z","shell.execute_reply":"2021-06-05T14:05:31.438944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.chdir(\"/kaggle/working\")","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:05:31.836354Z","iopub.execute_input":"2021-06-05T14:05:31.836637Z","iopub.status.idle":"2021-06-05T14:05:31.840453Z","shell.execute_reply.started":"2021-06-05T14:05:31.836592Z","shell.execute_reply":"2021-06-05T14:05:31.83967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_for_sub = np.zeros(submission.shape[0])\nfor i in tqdm_notebook(range(n_val_famillies_list)):\n    file_path = \"vgg_face_0.h5\"\n    model.load_weights(file_path)\n    # Get the predictions\n    predictions = []\n\n    for batch in tqdm_notebook(chunker(submission.img_pair.values)):\n        X1 = [x.split(\"-\")[0] for x in batch]\n        X1 = np.array([read_img(test_path + x) for x in X1])\n\n        X2 = [x.split(\"-\")[1] for x in batch]\n        X2 = np.array([read_img(test_path + x) for x in X2])\n\n        pred = model.predict([X1, X2]).ravel().tolist()\n        predictions += pred\n    preds_for_sub += np.array(predictions) / n_val_famillies_list","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-05T14:05:32.621709Z","iopub.execute_input":"2021-06-05T14:05:32.622006Z","iopub.status.idle":"2021-06-05T14:08:41.60627Z","shell.execute_reply.started":"2021-06-05T14:05:32.621959Z","shell.execute_reply":"2021-06-05T14:08:41.605455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('val_loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T13:45:20.469207Z","iopub.execute_input":"2021-06-05T13:45:20.47161Z","iopub.status.idle":"2021-06-05T13:45:22.952828Z","shell.execute_reply.started":"2021-06-05T13:45:20.471538Z","shell.execute_reply":"2021-06-05T13:45:22.951601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission['is_related'] = preds_for_sub\nsubmission.to_csv(\"vgg_face.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:10:29.740293Z","iopub.execute_input":"2021-06-05T14:10:29.740591Z","iopub.status.idle":"2021-06-05T14:10:30.093738Z","shell.execute_reply.started":"2021-06-05T14:10:29.740542Z","shell.execute_reply":"2021-06-05T14:10:30.09299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import HTML\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame(submission)\n\ndf.to_csv('submission.csv', index=False)\n\ndef create_download_link(title = \"Download CSV file\", filename = \"data.csv\"):  \n    html = '<a href={filename}>{title}</a>'\n    html = html.format(title=title,filename=filename)\n    return HTML(html)\n\n# create a link to download the dataframe which was saved with .to_csv method\ncreate_download_link(filename='submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:10:32.890779Z","iopub.execute_input":"2021-06-05T14:10:32.891096Z","iopub.status.idle":"2021-06-05T14:10:32.925857Z","shell.execute_reply.started":"2021-06-05T14:10:32.891043Z","shell.execute_reply":"2021-06-05T14:10:32.924827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}