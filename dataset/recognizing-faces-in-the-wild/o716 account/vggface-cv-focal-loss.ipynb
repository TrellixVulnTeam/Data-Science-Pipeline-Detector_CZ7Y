{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\n\nfrom collections import defaultdict\nfrom glob import glob\nfrom random import choice, sample\nfrom keras.preprocessing import image\nimport cv2\nfrom tqdm import tqdm_notebook\nimport numpy as np\nimport pandas as pd\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\nfrom keras.layers import Input, Dense, GlobalMaxPool2D, GlobalAvgPool2D, Concatenate, Multiply, Dropout, Subtract\nfrom keras.models import Model\nfrom keras.optimizers import Adam","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-01T04:39:51.790211Z","iopub.execute_input":"2021-06-01T04:39:51.790485Z","iopub.status.idle":"2021-06-01T04:39:53.93852Z","shell.execute_reply.started":"2021-06-01T04:39:51.790437Z","shell.execute_reply":"2021-06-01T04:39:53.937775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.getcwd()","metadata":{"execution":{"iopub.status.busy":"2021-06-01T04:40:03.514703Z","iopub.execute_input":"2021-06-01T04:40:03.515047Z","iopub.status.idle":"2021-06-01T04:40:03.521346Z","shell.execute_reply.started":"2021-06-01T04:40:03.514993Z","shell.execute_reply":"2021-06-01T04:40:03.520619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!pip install git+https://github.com/hamidomar/keras-vggface","metadata":{"execution":{"iopub.status.busy":"2021-06-01T04:40:07.664606Z","iopub.execute_input":"2021-06-01T04:40:07.664924Z","iopub.status.idle":"2021-06-01T04:40:07.669153Z","shell.execute_reply.started":"2021-06-01T04:40:07.664871Z","shell.execute_reply":"2021-06-01T04:40:07.667885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" !cp -r ../input/kerasvggface1/* ./","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.status.busy":"2021-06-01T04:40:08.791669Z","iopub.execute_input":"2021-06-01T04:40:08.792005Z","iopub.status.idle":"2021-06-01T04:40:09.681014Z","shell.execute_reply.started":"2021-06-01T04:40:08.79195Z","shell.execute_reply":"2021-06-01T04:40:09.679926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.chdir(\"../input/kerasvggface1/keras-vggface-master\")","metadata":{"execution":{"iopub.status.busy":"2021-06-01T04:40:11.000146Z","iopub.execute_input":"2021-06-01T04:40:11.000473Z","iopub.status.idle":"2021-06-01T04:40:11.005081Z","shell.execute_reply.started":"2021-06-01T04:40:11.000413Z","shell.execute_reply":"2021-06-01T04:40:11.004287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras_vggface.utils import preprocess_input\nfrom keras_vggface.vggface import VGGFace","metadata":{"execution":{"iopub.status.busy":"2021-06-01T04:40:12.015768Z","iopub.execute_input":"2021-06-01T04:40:12.016135Z","iopub.status.idle":"2021-06-01T04:40:12.033954Z","shell.execute_reply.started":"2021-06-01T04:40:12.016082Z","shell.execute_reply":"2021-06-01T04:40:12.033313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_file_path = \"../input/rfiwsmile/recognizing-faces-in-the-wild/train_relationships.csv\"\ntrain_folders_path = \"../input/rfiwsmile/recognizing-faces-in-the-wild/test\"","metadata":{"execution":{"iopub.status.busy":"2021-06-01T04:40:12.935117Z","iopub.execute_input":"2021-06-01T04:40:12.935434Z","iopub.status.idle":"2021-06-01T04:40:12.940263Z","shell.execute_reply.started":"2021-06-01T04:40:12.935385Z","shell.execute_reply":"2021-06-01T04:40:12.939223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_file_path = \"../input/rfiwsmile/recognizing-faces-in-the-wild/train_relationships.csv\"\ntrain_folders_path = \"../input/rfiwsmile/recognizing-faces-in-the-wild/test\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.chdir(\"input\")\nos.getcwd()","metadata":{"execution":{"iopub.status.busy":"2021-06-01T04:41:03.025956Z","iopub.execute_input":"2021-06-01T04:41:03.026251Z","iopub.status.idle":"2021-06-01T04:41:03.03268Z","shell.execute_reply.started":"2021-06-01T04:41:03.0262Z","shell.execute_reply":"2021-06-01T04:41:03.031939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_dirs = os.listdir('../input/train/')\n# len(train_dirs), train_dirs","metadata":{"execution":{"iopub.status.busy":"2021-06-01T04:41:08.29018Z","iopub.execute_input":"2021-06-01T04:41:08.290484Z","iopub.status.idle":"2021-06-01T04:41:08.294611Z","shell.execute_reply.started":"2021-06-01T04:41:08.290434Z","shell.execute_reply":"2021-06-01T04:41:08.293589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_famillies_list = [\"F07\", \"F08\", \"F09\"]\n# val_famillies_list = [\"F09\"]","metadata":{"execution":{"iopub.status.busy":"2021-06-01T04:41:08.585248Z","iopub.execute_input":"2021-06-01T04:41:08.5856Z","iopub.status.idle":"2021-06-01T04:41:08.590813Z","shell.execute_reply.started":"2021-06-01T04:41:08.585542Z","shell.execute_reply":"2021-06-01T04:41:08.590055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cwd ='/kaggle/input'\n#  \"/input/rfiwsmile/recognizing-faces-in-the-wild/train_relationships.csv\"","metadata":{"execution":{"iopub.status.busy":"2021-06-01T04:41:08.898056Z","iopub.execute_input":"2021-06-01T04:41:08.89841Z","iopub.status.idle":"2021-06-01T04:41:08.902623Z","shell.execute_reply.started":"2021-06-01T04:41:08.898358Z","shell.execute_reply":"2021-06-01T04:41:08.901506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nall_images = glob(train_folders_path + \"*/*/*/*/*\")","metadata":{"execution":{"iopub.status.busy":"2021-06-01T04:41:09.353519Z","iopub.execute_input":"2021-06-01T04:41:09.353823Z","iopub.status.idle":"2021-06-01T04:41:27.357181Z","shell.execute_reply.started":"2021-06-01T04:41:09.35376Z","shell.execute_reply":"2021-06-01T04:41:27.356361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_train_val(family_name):\n    # Get val_person_image_map\n    val_famillies = family_name\n    train_images = [x for x in all_images if val_famillies not in x] # train_img if val_fam not given\n    val_images = [x for x in all_images if val_famillies in x] # val_img if val_fam given\n\n    train_person_to_images_map = defaultdict(list) # initialises a default dict which will not give key error\n\n    ppl = [x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2] for x in all_images] # EXTRACT NAME VALUES\n\n    for x in train_images:\n        train_person_to_images_map[x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2]].append(x) \n\n    val_person_to_images_map = defaultdict(list)\n\n    for x in val_images:\n        val_person_to_images_map[x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2]].append(x)\n        \n    # Get the train and val dataset\n    relationships = pd.read_csv(train_file_path)\n    relationships = list(zip(relationships.p1.values, relationships.p2.values))\n    relationships = [x for x in relationships if x[0] in ppl and x[1] in ppl]\n\n    train = [x for x in relationships if val_famillies not in x[0]]\n    val = [x for x in relationships if val_famillies in x[0]]\n    \n    return train, val, train_person_to_images_map, val_person_to_images_map","metadata":{"execution":{"iopub.status.busy":"2021-06-01T04:41:43.556628Z","iopub.execute_input":"2021-06-01T04:41:43.556943Z","iopub.status.idle":"2021-06-01T04:41:43.567144Z","shell.execute_reply.started":"2021-06-01T04:41:43.556889Z","shell.execute_reply":"2021-06-01T04:41:43.566404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_img(path):\n    img = image.load_img(path, target_size=(197, 197))\n    img = np.array(img).astype(np.float)\n    return preprocess_input(img, version=2)\n\ndef gen(list_tuples, person_to_images_map, batch_size=16):\n    ppl = list(person_to_images_map.keys())\n    while True:\n        batch_tuples = sample(list_tuples, batch_size // 2)\n        labels = [1] * len(batch_tuples)\n        while len(batch_tuples) < batch_size:\n            p1 = choice(ppl)\n            p2 = choice(ppl)\n\n            if p1 != p2 and (p1, p2) not in list_tuples and (p2, p1) not in list_tuples:\n                batch_tuples.append((p1, p2))\n                labels.append(0)\n\n        for x in batch_tuples:\n            if not len(person_to_images_map[x[0]]):\n                print(x[0])\n\n        X1 = [choice(person_to_images_map[x[0]]) for x in batch_tuples]\n        X1 = np.array([read_img(x) for x in X1])\n\n        X2 = [choice(person_to_images_map[x[1]]) for x in batch_tuples]\n        X2 = np.array([read_img(x) for x in X2])\n\n        yield [X1, X2], labels\n\nimport tensorflow as tf\nfrom keras import backend as K\ndef focal_loss(gamma=2., alpha=.25):\n    def focal_loss_fixed(y_true, y_pred):\n        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(K.epsilon()+pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0 + K.epsilon()))\n    return focal_loss_fixed\n\n\ndef baseline_model():\n    input_1 = Input(shape=(197, 197, 3))\n    input_2 = Input(shape=(197, 197, 3))\n\n    base_model = VGGFace(model='resnet50', include_top=False)\n\n    for x in base_model.layers[:-3]:\n        x.trainable = True\n    for x in base_model.layers[-3:]:\n        x.trainable=False\n\n    x1 = base_model(input_1)\n    x2 = base_model(input_2)\n\n#     x1_ = Reshape(target_shape=(7*7, 2048))(x1)\n#     x2_ = Reshape(target_shape=(7*7, 2048))(x2)\n#     #\n#     x_dot = Dot(axes=[2, 2], normalize=True)([x1_, x2_])\n#     x_dot = Flatten()(x_dot)\n\n    x1 = Concatenate(axis=-1)([GlobalMaxPool2D()(x1), GlobalAvgPool2D()(x1)])\n    x2 = Concatenate(axis=-1)([GlobalMaxPool2D()(x2), GlobalAvgPool2D()(x2)])\n\n    x3 = Subtract()([x1, x2])\n    x3 = Multiply()([x3, x3])\n\n    x1_ = Multiply()([x1, x1])\n    x2_ = Multiply()([x2, x2])\n    x4 = Subtract()([x1_, x2_])\n    x = Concatenate(axis=-1)([x4, x3])\n\n    x = Dense(100, activation=\"relu\")(x)\n    x = Dropout(0.01)(x)\n    out = Dense(1, activation=\"sigmoid\")(x)\n\n    model = Model([input_1, input_2], out)\n    \n    # loss=\"binary_crossentropy\"\n    model.compile(loss=[focal_loss(alpha=.25, gamma=2)], \n                  metrics=['acc'], \n                  optimizer=Adam(0.00003))\n\n    model.summary()\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-06-01T04:41:44.245011Z","iopub.execute_input":"2021-06-01T04:41:44.245318Z","iopub.status.idle":"2021-06-01T04:41:44.269465Z","shell.execute_reply.started":"2021-06-01T04:41:44.245267Z","shell.execute_reply":"2021-06-01T04:41:44.268723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.getcwd()","metadata":{"execution":{"iopub.status.busy":"2021-06-01T04:41:52.943348Z","iopub.execute_input":"2021-06-01T04:41:52.943646Z","iopub.status.idle":"2021-06-01T04:41:52.948748Z","shell.execute_reply.started":"2021-06-01T04:41:52.943597Z","shell.execute_reply":"2021-06-01T04:41:52.947997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = baseline_model()","metadata":{"execution":{"iopub.status.busy":"2021-06-01T04:41:55.536949Z","iopub.execute_input":"2021-06-01T04:41:55.537247Z","iopub.status.idle":"2021-06-01T04:42:16.053321Z","shell.execute_reply.started":"2021-06-01T04:41:55.5372Z","shell.execute_reply":"2021-06-01T04:42:16.052628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. Train","metadata":{}},{"cell_type":"code","source":"n_val_famillies_list = len(val_famillies_list)","metadata":{"execution":{"iopub.status.busy":"2021-06-01T04:42:16.055462Z","iopub.execute_input":"2021-06-01T04:42:16.055739Z","iopub.status.idle":"2021-06-01T04:42:16.068819Z","shell.execute_reply.started":"2021-06-01T04:42:16.055693Z","shell.execute_reply":"2021-06-01T04:42:16.067164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_val_famillies_list","metadata":{"execution":{"iopub.status.busy":"2021-06-01T04:42:16.070582Z","iopub.execute_input":"2021-06-01T04:42:16.070939Z","iopub.status.idle":"2021-06-01T04:42:16.081438Z","shell.execute_reply.started":"2021-06-01T04:42:16.070892Z","shell.execute_reply":"2021-06-01T04:42:16.080444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.chdir(\"input\")\nos.getcwd()","metadata":{"execution":{"iopub.status.busy":"2021-05-31T17:24:50.777721Z","iopub.execute_input":"2021-05-31T17:24:50.778019Z","iopub.status.idle":"2021-05-31T17:24:50.786175Z","shell.execute_reply.started":"2021-05-31T17:24:50.777968Z","shell.execute_reply":"2021-05-31T17:24:50.785236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in tqdm_notebook(range(n_val_famillies_list)):\n    train, val, train_person_to_images_map, val_person_to_images_map = get_train_val(val_famillies_list[i])\n    file_path = f\"/kaggle/working/vgg_face_{i}.h5\"\n    checkpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n    reduce_on_plateau = ReduceLROnPlateau(monitor=\"val_acc\", mode=\"max\", factor=0.5, patience=10, verbose=1)\n    es = EarlyStopping(monitor=\"val_acc\", min_delta = 0.001, patience=16, verbose=1)\n    callbacks_list = [checkpoint, reduce_on_plateau, es]\n\n    history = model.fit_generator(gen(train, train_person_to_images_map, batch_size=32), \n                                  use_multiprocessing=True,\n                                  validation_data=gen(val, val_person_to_images_map, batch_size=32), \n                                  epochs=100, verbose=1,\n                                  workers=4, callbacks=callbacks_list, \n                                  steps_per_epoch=400, \n                                  validation_steps=500)","metadata":{"execution":{"iopub.status.busy":"2021-06-01T04:42:16.083398Z","iopub.execute_input":"2021-06-01T04:42:16.083841Z","iopub.status.idle":"2021-06-01T09:25:14.698805Z","shell.execute_reply.started":"2021-06-01T04:42:16.083637Z","shell.execute_reply":"2021-06-01T09:25:14.697834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('acc')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-01T09:28:42.291527Z","iopub.execute_input":"2021-06-01T09:28:42.291869Z","iopub.status.idle":"2021-06-01T09:28:42.622879Z","shell.execute_reply.started":"2021-06-01T09:28:42.291785Z","shell.execute_reply":"2021-06-01T09:28:42.622073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Inference","metadata":{}},{"cell_type":"code","source":"test_path = \"../input/newtest/test/\"\n\nsubmission = pd.read_csv('../input/rfiwsmile/recognizing-faces-in-the-wild/sample_submission.csv')\n\ndef chunker(seq, size=32):\n    return (seq[pos:pos + size] for pos in range(0, len(seq), size))","metadata":{"execution":{"iopub.status.busy":"2021-06-01T09:49:49.88228Z","iopub.execute_input":"2021-06-01T09:49:49.882625Z","iopub.status.idle":"2021-06-01T09:49:49.911326Z","shell.execute_reply.started":"2021-06-01T09:49:49.882565Z","shell.execute_reply":"2021-06-01T09:49:49.91057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.chdir(\"/kaggle/working\")","metadata":{"execution":{"iopub.status.busy":"2021-06-01T09:55:31.001662Z","iopub.execute_input":"2021-06-01T09:55:31.001986Z","iopub.status.idle":"2021-06-01T09:55:31.005821Z","shell.execute_reply.started":"2021-06-01T09:55:31.001929Z","shell.execute_reply":"2021-06-01T09:55:31.00505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.listdir()","metadata":{"execution":{"iopub.status.busy":"2021-06-01T09:55:46.987819Z","iopub.execute_input":"2021-06-01T09:55:46.988131Z","iopub.status.idle":"2021-06-01T09:55:46.993258Z","shell.execute_reply.started":"2021-06-01T09:55:46.988074Z","shell.execute_reply":"2021-06-01T09:55:46.992382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_for_sub = np.zeros(submission.shape[0])\nfor i in tqdm_notebook(range(n_val_famillies_list)):\n    file_path = \"./vgg_face_1.h5\"\n    model.load_weights(file_path)\n    # Get the predictions\n    predictions = []\n\n    for batch in tqdm_notebook(chunker(submission.img_pair.values)):\n        X1 = [x.split(\"-\")[0] for x in batch]\n        X1 = np.array([read_img(test_path + x) for x in X1])\n\n        X2 = [x.split(\"-\")[1] for x in batch]\n        X2 = np.array([read_img(test_path + x) for x in X2])\n\n        pred = model.predict([X1, X2]).ravel().tolist()\n        predictions += pred\n    preds_for_sub += np.array(predictions) / n_val_famillies_list","metadata":{"execution":{"iopub.status.busy":"2021-06-01T10:09:09.03841Z","iopub.execute_input":"2021-06-01T10:09:09.038741Z","iopub.status.idle":"2021-06-01T10:11:49.297161Z","shell.execute_reply.started":"2021-06-01T10:09:09.038687Z","shell.execute_reply":"2021-06-01T10:11:49.29631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission['is_related'] = preds_for_sub\nsubmission.to_csv(\"vgg_face.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-01T10:11:49.2989Z","iopub.execute_input":"2021-06-01T10:11:49.299345Z","iopub.status.idle":"2021-06-01T10:11:49.330254Z","shell.execute_reply.started":"2021-06-01T10:11:49.299293Z","shell.execute_reply":"2021-06-01T10:11:49.329641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import HTML\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame(submission)\n\ndf.to_csv('submission.csv', index=False)\n\ndef create_download_link(title = \"Download CSV file\", filename = \"data.csv\"):  \n    html = '<a href={filename}>{title}</a>'\n    html = html.format(title=title,filename=filename)\n    return HTML(html)\n\n# create a link to download the dataframe which was saved with .to_csv method\ncreate_download_link(filename='submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-06-01T10:11:49.331604Z","iopub.execute_input":"2021-06-01T10:11:49.332112Z","iopub.status.idle":"2021-06-01T10:11:49.365253Z","shell.execute_reply.started":"2021-06-01T10:11:49.331882Z","shell.execute_reply":"2021-06-01T10:11:49.364334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}