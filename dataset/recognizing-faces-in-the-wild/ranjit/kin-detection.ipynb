{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import defaultdict\nfrom glob import glob\nfrom random import choice, sample\nimport cv2\nimport numpy as np\nimport pandas as pd\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau,EarlyStopping\nfrom keras.layers import Input, Dense, Flatten, GlobalMaxPool2D, GlobalAvgPool2D, Concatenate, Multiply, Dropout, Subtract, Add, Conv2D, Flatten\nfrom keras.models import Model\nfrom keras.layers import BatchNormalization\nfrom keras.preprocessing import image\nfrom keras.optimizers import Adam\nimport h5py\nfrom keras.layers import LeakyReLU\nfrom keras import regularizers\nimport gc\nimport psutil","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_file_path = \"../input/train_relationships.csv\"\ntrain_folders_path = \"../input/train/\"\nval_famillies = \"F09\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_images = glob(train_folders_path + \"*/*/*.jpg\")\ntrain_images = [x for x in all_images if val_famillies not in x]\nval_images = [x for x in all_images if val_famillies in x]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_person_to_images_map = defaultdict(list)\nppl = [x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2] for x in all_images]\n\nfor x in train_images:\n    train_person_to_images_map[x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2]].append(x)\n\nval_person_to_images_map = defaultdict(list)\n\nfor x in val_images:\n    val_person_to_images_map[x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2]].append(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"relationships = pd.read_csv(train_file_path)\nrelationships = list(zip(relationships.p1.values, relationships.p2.values))\nrelationships = [x for x in relationships if x[0] in ppl and x[1] in ppl]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = [x for x in relationships if val_famillies not in x[0]]\nval = [x for x in relationships if val_famillies in x[0]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_img(path):\n    img = image.load_img(path, target_size=(197, 197))\n    img = np.array(img).astype(np.float)\n    return preprocess_input(img, version=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def gen(list_tuples, person_to_images_map, batch_size=16):\n    ppl = list(person_to_images_map.keys())\n    while True:\n        batch_tuples = sample(list_tuples, batch_size // 2)\n        labels = [1] * len(batch_tuples)\n        while len(batch_tuples) < batch_size:\n            p1 = choice(ppl)\n            p2 = choice(ppl)\n\n            if p1 != p2 and (p1, p2) not in list_tuples and (p2, p1) not in list_tuples:\n                batch_tuples.append((p1, p2))\n                labels.append(0)\n\n        for x in batch_tuples:\n            if not len(person_to_images_map[x[0]]):\n                print(x[0])\n\n        X1 = [choice(person_to_images_map[x[0]]) for x in batch_tuples]\n        X1 = np.array([read_img(x) for x in X1])\n\n        X2 = [choice(person_to_images_map[x[1]]) for x in batch_tuples]\n        X2 = np.array([read_img(x) for x in X2])\n\n        yield [X1, X2], labels\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def baseline_model():\n    input_1 = Input(shape=(197, 197, 3))\n    input_2 = Input(shape=(197, 197, 3))\n\n    base_model = VGGFace(model='resnet50', include_top=False)\n\n    for x in base_model.layers[:-3]:\n        x.trainable = True\n\n    x1 = base_model(input_1)\n    x2 = base_model(input_2)\n\n    x1 = Concatenate(axis=-1)([GlobalAvgPool2D()(x1), GlobalAvgPool2D()(x1)])\n    x2 = Concatenate(axis=-1)([GlobalAvgPool2D()(x2), GlobalAvgPool2D()(x2)])\n\n    x3 = Subtract()([x1, x2])\n    x3 = Multiply()([x3, x3])\n\n    \n    x1_ = Multiply()([x1, x1])\n    x2_ = Multiply()([x2, x2])\n    x4  = Subtract()([x1_, x2_])    \n    x   = Concatenate(axis=-1)([x4, x3])\n    x   = Dense(256, activation=\"relu\",kernel_regularizer=regularizers.l2(0.01))(x)\n    x   = BatchNormalization()(x)\n    x   = Dense(256, activation=\"relu\",kernel_regularizer=regularizers.l2(0.01))(x)\n    x   = Dropout(0.01)(x) \n    x   = Dense(128, activation=\"relu\",kernel_regularizer=regularizers.l2(0.01))(x)\n    x   = BatchNormalization()(x)\n    x   = Dense(128, activation=\"relu\",kernel_regularizer=regularizers.l2(0.01))(x)\n    x   = Dropout(0.01)(x)    \n    x   = Dense(100, activation=\"relu\",kernel_regularizer=regularizers.l2(0.01))(x)    \n    out = Dense(1, activation=\"sigmoid\")(x)\n\n    model = Model([input_1, input_2], out)\n    model.compile(loss=\"binary_crossentropy\", metrics=['acc'], optimizer=Adam(0.00001))\n    model.summary()\n\n    return model\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install git+https://github.com/rcmalli/keras-vggface.git\nfrom keras_vggface.utils import preprocess_input\nfrom keras_vggface.vggface import VGGFace","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"available RAM:\", psutil.virtual_memory())\ngc.collect()\nprint(\"available RAM:\", psutil.virtual_memory())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfile_path = \"vgg_face.h5\"\n\ncheckpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, \n                             save_best_only=True, mode='max')\nearly_stop = EarlyStopping(monitor='val_loss', min_delta=0.0001, \n                          patience=15, verbose=0, mode='auto')\nreduce_on_plateau = ReduceLROnPlateau(monitor=\"val_acc\", mode=\"max\", factor=0.2, \n                                      patience=10, verbose=1)\n\ncallbacks_list = [checkpoint,early_stop]\n\ncurr_model = baseline_model()\ncurr_model_hist=curr_model.fit_generator(gen(train, train_person_to_images_map, batch_size=16), \n                            use_multiprocessing=True,\n                    validation_data=gen(val, val_person_to_images_map, batch_size=16), \n                            epochs=50, \n                            verbose=1,workers = 4, \n                            callbacks=callbacks_list,\n                            steps_per_epoch=200,\n                            validation_steps=100)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\ndef plot_accuracy(y):\n    if(y == True):\n        plt.plot(curr_model_hist.history['acc'])\n        plt.plot(curr_model_hist.history['val_acc'])\n        plt.legend(['train', 'test'], loc='lower right')\n        plt.title('accuracy plot - train vs test')\n        plt.xlabel('epoch')\n        plt.ylabel('accuracy')\n        plt.show()\n    else:\n        pass\n    return\n\ndef plot_loss(y):\n    if(y == True):\n        plt.plot(curr_model_hist.history['loss'])\n        plt.plot(curr_model_hist.history['val_loss'])\n        plt.legend(['training loss', 'validation loss'], loc = 'upper right')\n        plt.title('loss plot - training vs vaidation')\n        plt.xlabel('epoch')\n        plt.ylabel('loss')\n        plt.show()\n    else:\n        pass\n    return\n\n\nplot_accuracy(True)\nplot_loss(True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"available RAM:\\n\", psutil.virtual_memory())\ngc.collect()\nprint(\"available RAM:\\n\", psutil.virtual_memory())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_path = \"../input/test/\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def chunker(seq, size=32):\n    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n\n\nfrom tqdm import tqdm\n\nsubmission = pd.read_csv('../input/sample_submission.csv')\n\npredictions = []\n\nfor batch in tqdm(chunker(submission.img_pair.values)):\n    X1 = [x.split(\"-\")[0] for x in batch]\n    X1 = np.array([read_img(test_path + x) for x in X1])\n\n    X2 = [x.split(\"-\")[1] for x in batch]\n    X2 = np.array([read_img(test_path + x) for x in X2])\n\n    pred = curr_model.predict([X1, X2]).ravel().tolist()\n    predictions += pred\n\nsubmission['is_related'] = predictions\n\nsubmission.to_csv(\"vgg_face.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}