{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 'You seem familiar' - One-shot approach to tackle kinship problem\n\nIn this notebook we are going to solve this kinship problem using the popular one-shot learning approach and build a embedding generator to find the cosine-similarity amongst the images of people from a family. Playing with cosine similarity, we are going to design a model that is able to recognize the people of a family and people related by blood."},{"metadata":{},"cell_type":"markdown","source":"### Understanding the dataset\nThe dataset comprises of files seperated into training and testing folders with a relation dataframe _train_relationships.csv_ pointing to the images of blood related members.\nFiles stored in the train folder are as follows:\n    \n```bash\n./train\n./train/F@@@@      - @@@@ denoting the family number\n./train/F@@@@/MID$ - $ denoting the member of the family\n```\nExample:\n\n```bash\n   +---------------+------------+\n   | F0002/MID1    | F0002/MID3 |\n   | F0002/MID2\t| F0002/MID3 |\n   +---------------+------------+\n```\nThis represents that member _MID1_ and _MID2_ of family _F0002_ is in blood relation with _MID3_ whereas _MID1_ and _MID2_ are not in blood-relationship with each other."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# calling basic imports\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Plotting library\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n# Additional support libraries\nimport os\nprint(os.listdir(\"../input\"))\n\n# Library for reading images \nfrom PIL import Image\n\n# Random\nimport random\nfrom tqdm import tqdm_notebook","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Reading the dataset"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train  = pd.read_csv('../input/recognizing-faces-in-the-wild/train_relationships.csv')\ntrain.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN_BASE = '../input/recognizing-faces-in-the-wild/train/'\nfamilies = sorted(os.listdir(TRAIN_BASE))\nprint('We have {} families in the dataset'.format(len(families)))\nprint(families[:5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"members = {i:sorted(os.listdir(TRAIN_BASE+i)) for i in families}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TEST_BASE='../input/recognizing-faces-in-the-wild/test/'\ntest_images_names = os.listdir(TEST_BASE)\ntest_images_names[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualizing the dataset\n\nIn this part of the kernel, we are going to visualize the dataset images to get the gist of it. The dataset structure has been discussed above and here we are going to visualize the images.\n\nFirst let's being with building the support functions to view the dataset images."},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_img(PATH): return np.array(Image.open(PATH))\n\ndef plots(ims, figsize=(12,6), rows=1, titles=None):\n    f = plt.figure(figsize=figsize)\n    for i in range(len(ims)):\n        sp = f.add_subplot(rows, len(ims)//rows, i+1)\n        sp.axis('Off')\n        if titles is not None: sp.set_title(titles[i], fontsize=16)\n        plt.imshow(ims[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_relations(df, BASE='../input/recognizing-faces-in-the-wild/train/', rows=1, titles=None):\n    tdf = df[:rows]\n    tdf1 = tdf.p1\n    tdf2 = tdf.p2\n    figsize=(5,3*rows)\n    f = plt.figure(figsize=figsize)\n    x = 0\n    for i in range(rows):\n        sp = f.add_subplot(rows, 2, x+1)\n        sp.axis('Off')\n        x+=1\n        image_path = os.path.join(BASE,tdf1[i])\n        im = os.listdir(image_path)[-1]\n        sp.set_title(tdf1[i], fontsize=16)\n        plt.imshow(load_img(os.path.join(image_path, im)))\n        sp = f.add_subplot(rows, 2, x+1)\n        x+=1\n        sp.axis('Off')\n        image_path = os.path.join(BASE,tdf2[i])\n        im = os.listdir(image_path)[-1]\n        sp.set_title(tdf2[i], fontsize=16)\n        plt.imshow(load_img(os.path.join(image_path, im)))\n        \nplot_relations(train, rows=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_images=np.array([load_img(os.path.join(TEST_BASE,image)) for image in test_images_names])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_images.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plots(test_images[:15], rows=3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Average test faces"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(test_images.sum(axis=0)//test_images.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"avg_face = []\nu=0\nfor family in families[u:u+1]:\n    for member in os.listdir(os.path.join(TRAIN_BASE,family)):\n        for image in os.listdir(os.path.join(TRAIN_BASE, family, member)):\n            avg_face.append(load_img(os.path.join(TRAIN_BASE, family, member, image)))\navg_face=np.array(avg_face)\nplt.imshow(avg_face.sum(axis=0)//avg_face.shape[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Building the model\n\nIn order to generate the face embeddings, we are going to use the _vgg_face_ model trained on the faces dataset to generate images. We are going to use the _channel_first_ method in keras and change every image to channel first as per required. In order to change the keras configration to channel first, we are going to alter the _keras.json_ file in user home or by simply altering the backend."},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import backend as K","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"K.set_image_data_format('channels_first')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nfrom keras.models import Model\nfrom keras.layers import Input, Convolution2D, ZeroPadding2D, MaxPooling2D, Flatten, Dropout, Activation","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we are using the VGG16 as the base model here, so we are going to build VGG16 as per the diagram below\n\n![vgg16](https://d2mxuefqeaa7sj.cloudfront.net/s_8C760A111A4204FB24FFC30E04E069BD755C4EEFD62ACBA4B54BBA2A78E13E8C_1491022251600_VGGNet.png)\n\nI have changed the last layer to 2622 as per the VGG16 pretrained configrations that could be found here: [VGG16_facenet_keras](https://drive.google.com/uc?id=0B4ChsjFJvew3NkF0dTc1OGxsOFU&export=download)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def vgg_face(weights_path=None):\n    img = Input(shape=(3, 224, 224))\n\n    pad1_1 = ZeroPadding2D(padding=(1, 1))(img)\n    conv1_1 = Convolution2D(64, (3, 3), activation='relu', name='conv1_1')(pad1_1)\n    pad1_2 = ZeroPadding2D(padding=(1, 1))(conv1_1)\n    conv1_2 = Convolution2D(64, (3, 3), activation='relu', name='conv1_2')(pad1_2)\n    pool1 = MaxPooling2D((2, 2), strides=(2, 2))(conv1_2)\n\n    pad2_1 = ZeroPadding2D((1, 1))(pool1)\n    conv2_1 = Convolution2D(128, (3, 3), activation='relu', name='conv2_1')(pad2_1)\n    pad2_2 = ZeroPadding2D((1, 1))(conv2_1)\n    conv2_2 = Convolution2D(128, (3, 3), activation='relu', name='conv2_2')(pad2_2)\n    pool2 = MaxPooling2D((2, 2), strides=(2, 2))(conv2_2)\n\n    pad3_1 = ZeroPadding2D((1, 1))(pool2)\n    conv3_1 = Convolution2D(256, (3, 3), activation='relu', name='conv3_1')(pad3_1)\n    pad3_2 = ZeroPadding2D((1, 1))(conv3_1)\n    conv3_2 = Convolution2D(256, (3, 3), activation='relu', name='conv3_2')(pad3_2)\n    pad3_3 = ZeroPadding2D((1, 1))(conv3_2)\n    conv3_3 = Convolution2D(256, (3, 3), activation='relu', name='conv3_3')(pad3_3)\n    pool3 = MaxPooling2D((2, 2), strides=(2, 2))(conv3_3)\n\n    pad4_1 = ZeroPadding2D((1, 1))(pool3)\n    conv4_1 = Convolution2D(512, (3, 3), activation='relu', name='conv4_1')(pad4_1)\n    pad4_2 = ZeroPadding2D((1, 1))(conv4_1)\n    conv4_2 = Convolution2D(512, (3, 3), activation='relu', name='conv4_2')(pad4_2)\n    pad4_3 = ZeroPadding2D((1, 1))(conv4_2)\n    conv4_3 = Convolution2D(512, (3, 3), activation='relu', name='conv4_3')(pad4_3)\n    pool4 = MaxPooling2D((2, 2), strides=(2, 2))(conv4_3)\n\n    pad5_1 = ZeroPadding2D((1, 1))(pool4)\n    conv5_1 = Convolution2D(512, (3, 3), activation='relu', name='conv5_1')(pad5_1)\n    pad5_2 = ZeroPadding2D((1, 1))(conv5_1)\n    conv5_2 = Convolution2D(512, (3, 3), activation='relu', name='conv5_2')(pad5_2)\n    pad5_3 = ZeroPadding2D((1, 1))(conv5_2)\n    conv5_3 = Convolution2D(512, (3, 3), activation='relu', name='conv5_3')(pad5_3)\n    pool5 = MaxPooling2D((2, 2), strides=(2, 2))(conv5_3)\n\n    fc6 = Convolution2D(4096, (7, 7), activation='relu', name='fc6')(pool5)\n    fc6_drop = Dropout(rate = 0.5)(fc6)\n    fc7 = Convolution2D(4096,( 1, 1), activation='relu', name='fc7')(fc6_drop)\n    fc7_drop = Dropout(rate = 0.5)(fc7)\n    fc8 = Convolution2D(2622, (1, 1), name='fc8')(fc7_drop)\n    flat = Flatten()(fc8)\n    out = Activation('softmax')(flat)\n\n    model = Model(input=img, output=out)\n\n    if weights_path:\n        model.load_weights(weights_path)\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vgg_facenet = vgg_face('../input/vgg16-facenet-model/vgg-face-keras.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vgg_facenet.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's try to generate the embeddings for 2 images from the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"im = Image.open('../input/recognizing-faces-in-the-wild/train/F0002/MID1/P00009_face3.jpg')\nim = np.array(im).astype(np.float32)\nim2 = Image.open('../input/recognizing-faces-in-the-wild/train/F0002/MID3/P00014_face1.jpg')\nim2 = np.array(im2).astype(np.float32)\nim = im.transpose((2,0,1))\nim = np.expand_dims(im, axis=0)\nim2 = im2.transpose((2,0,1))\nim2 = np.expand_dims(im2, axis=0)\nnp.concatenate([im,im2]).shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"out = vgg_facenet.predict(np.concatenate([im,im2]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cosine_similarity(a,b):\n    return np.sum(np.multiply(a,b))/np.multiply( np.sqrt(np.sum(np.power(a,2))),(np.sqrt(np.sum(np.power(b,2)))))\n\ndef distance(x, y):\n    return np.linalg.norm(x - y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(cosine_similarity(out[0], out[1]), distance(out[0], out[1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now finally reading the testset from the test_folder and predicting on the values from sample submission."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_images = os.listdir(TEST_BASE)\ntest = np.array([load_img(os.path.join(TEST_BASE, i)) for i in test_images])\ntest_emb = vgg_facenet.predict(test.transpose(0,3,1,2))\nprint(test.shape, test_emb.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_mapping = {img:idx for idx, img in enumerate(test_images)}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('../input/recognizing-faces-in-the-wild/sample_submission.csv')\nreq_mapping = [i.split('-') for i in submission.img_pair]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vector_distances=[]\nfor i in req_mapping:\n    a = i[0]\n    b = i[1]\n    dis = distance(test_emb[image_mapping[a]], test_emb[image_mapping[b]])\n    vector_distances.append(dis)\nvector_distances=np.array(vector_distances)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_sum = vector_distances.sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now from the vector_distances we have calulated above, we are now going to transform these distances into probablity of being same."},{"metadata":{"trusted":true},"cell_type":"code","source":"probs = []\nfor dist in vector_distances:\n    prob = np.sum(vector_distances[np.where(vector_distances <= dist)[0]])/total_sum\n    probs.append(1 - prob)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vector_distances.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.sum(vector_distances[np.where(vector_distances <= dist)[0]])/total_sum","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.is_related = probs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_emb.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sklearn.manifold\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tsne = sklearn.manifold.TSNE(n_components=2, random_state=0)\nall_emb_matrix_2d = tsne.fit_transform(test_emb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"points = pd.DataFrame(\n    [\n        (name, coords[0], coords[1])\n        for name, coords in [\n            (img, all_emb_matrix_2d[image_mapping[img]])\n            for img in test_images\n        ]\n    ],\n    columns=[\"name\", \"x\", \"y\"]\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_context(\"poster\")\npoints.plot.scatter(\"x\", \"y\", s=10, figsize=(15, 8))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_region(x_bounds, y_bounds):\n    slice = points[\n        (x_bounds[0] <= points.x) &\n        (points.x <= x_bounds[1]) & \n        (y_bounds[0] <= points.y) &\n        (points.y <= y_bounds[1])\n    ]\n    \n    ax = slice.plot.scatter(\"x\", \"y\", s=35, figsize=(10, 8))\n    for i, point in slice.iterrows():\n        ax.text(point.x + 0.005, point.y + 0.005, point.word, fontsize=11)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_region(x_bounds=(70.0, 80.0), y_bounds=(-10.0, 0.0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(test[image_mapping['face'+'03198'+'.jpg']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(test[image_mapping['face'+'05866'+'.jpg']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}