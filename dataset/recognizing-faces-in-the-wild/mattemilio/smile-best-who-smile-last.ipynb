{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Model combining Facenet and VGG in Keras\n\nThis is my single best scoring model (0.893 public leaderboard, 0.900 private leaderboard)\nI used this model in combination with other 36 models to get my final submission.\nFrom the 37 models that I used, 7 are public kernels (reference below)\n\nhttps://www.kaggle.com/shivamsarawagi/wildimagedetection-0-875\n\nhttps://www.kaggle.com/hsinwenchang/vggface-baseline-197x197\n\nhttps://www.kaggle.com/arjunrao2000/kinship-detection-with-vgg16\n\nhttps://www.kaggle.com/leonbora/kinship-recognition-transfer-learning-vggface\n\nhttps://www.kaggle.com/janpreets/just-another-feature-extractor-0-824-lb\n\nhttps://www.kaggle.com/tenffe/vggface-cv-focal-loss\n\nhttps://www.kaggle.com/vaishvik25/smile \n\n\nI used pretrained Facenet model from this repo https://github.com/nyoki-mtl/keras-facenet). \n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport os\nimport matplotlib.pyplot as plt\nimport cv2\nfrom imageio import imread\nfrom skimage.transform import resize\nfrom keras.models import load_model\nimport pandas as pd\nfrom tqdm import tqdm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install git+https://github.com/rcmalli/keras-vggface.git","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import h5py\nfrom collections import defaultdict\nfrom glob import glob\nfrom random import choice, sample\n\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\nfrom keras import backend as K\nfrom keras.layers import Input, Dense, Flatten, GlobalMaxPool2D, GlobalAvgPool2D, Concatenate, Multiply, Dropout, Subtract, Add, Conv2D, Lambda, Reshape\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nfrom keras_vggface.utils import preprocess_input\nfrom keras_vggface.vggface import VGGFace","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_file_path = \"../input/recognizing-faces-in-the-wild/train_relationships.csv\"\ntrain_folders_path = \"../input/recognizing-faces-in-the-wild/train/\"\nval_families = \"F09\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_images = glob(train_folders_path + \"*/*/*.jpg\")\nprint(all_images[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_images = [x for x in all_images if val_families not in x]\nval_images = [x for x in all_images if val_families in x]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ppl = [x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2] for x in all_images]\nprint(ppl[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_person_to_images_map = defaultdict(list)\n\nfor x in train_images:\n    train_person_to_images_map[x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2]].append(x)\n\nval_person_to_images_map = defaultdict(list)\n\nfor x in val_images:\n    val_person_to_images_map[x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2]].append(x)\n\nrelationships = pd.read_csv(train_file_path)\nrelationships = list(zip(relationships.p1.values, relationships.p2.values))\nrelationships = [x for x in relationships if x[0] in ppl and x[1] in ppl]\n\ntrain = [x for x in relationships if val_families not in x[0]]\nval = [x for x in relationships if val_families in x[0]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b8da009689331982f628256abc151cbbd7e288a1"},"cell_type":"code","source":"def prewhiten(x):\n    if x.ndim == 4:\n        axis = (1, 2, 3)\n        size = x[0].size\n    elif x.ndim == 3:\n        axis = (0, 1, 2)\n        size = x.size\n    else:\n        raise ValueError('Dimension should be 3 or 4')\n\n    mean = np.mean(x, axis=axis, keepdims=True)\n    std = np.std(x, axis=axis, keepdims=True)\n    std_adj = np.maximum(std, 1.0/np.sqrt(size))\n    y = (x - mean) / std_adj\n    return y","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"model_path = '../input/facenet-keras/facenet_keras.h5'\nmodel_fn = load_model(model_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for layer in model_fn.layers[:-3]:\n    layer.trainable = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_vgg = VGGFace(model='resnet50', include_top=False)\nfor layer in model_vgg.layers[:-3]:\n    layer.trainable = True","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Define image size for facenet and vgg"},{"metadata":{"trusted":true},"cell_type":"code","source":"IMG_SIZE_FN = 160\nIMG_SIZE_VGG = 224","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_img_fn(path):\n    img = cv2.imread(path)\n    img = cv2.resize(img,(IMG_SIZE_FN,IMG_SIZE_FN))\n    img = np.array(img).astype(np.float)\n    return prewhiten(img)\n\ndef read_img_vgg(path):\n    img = cv2.imread(path)\n    img = cv2.resize(img,(IMG_SIZE_VGG,IMG_SIZE_VGG))\n    img = np.array(img).astype(np.float)\n    return preprocess_input(img, version=2)\n\ndef gen(list_tuples, person_to_images_map, batch_size=16):\n    ppl = list(person_to_images_map.keys())\n    while True:\n        batch_tuples = sample(list_tuples, batch_size // 2)\n        labels = [1] * len(batch_tuples)\n        while len(batch_tuples) < batch_size:\n            p1 = choice(ppl)\n            p2 = choice(ppl)\n\n            if p1 != p2 and (p1, p2) not in list_tuples and (p2, p1) not in list_tuples:\n                batch_tuples.append((p1, p2))\n                labels.append(0)\n\n        for x in batch_tuples:\n            if not len(person_to_images_map[x[0]]):\n                print(x[0])\n\n        X1 = [choice(person_to_images_map[x[0]]) for x in batch_tuples]\n        X1_FN = np.array([read_img_fn(x) for x in X1])\n        X1_VGG = np.array([read_img_vgg(x) for x in X1])\n\n        X2 = [choice(person_to_images_map[x[1]]) for x in batch_tuples]\n        X2_FN = np.array([read_img_fn(x) for x in X2])\n        X2_VGG = np.array([read_img_vgg(x) for x in X2])\n\n        yield [X1_FN, X2_FN, X1_VGG, X2_VGG], labels\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def signed_sqrt(x):\n    return K.sign(x)*K.sqrt(K.abs(x)+1e-9)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def baseline_model():\n    input_1 = Input(shape=(IMG_SIZE_FN, IMG_SIZE_FN, 3))\n    input_2 = Input(shape=(IMG_SIZE_FN, IMG_SIZE_FN, 3))\n    input_3 = Input(shape=(IMG_SIZE_VGG, IMG_SIZE_VGG, 3))\n    input_4 = Input(shape=(IMG_SIZE_VGG, IMG_SIZE_VGG, 3))\n\n    x1 = model_fn(input_1)\n    x2 = model_fn(input_2)\n    x3 = model_vgg(input_3)\n    x4 = model_vgg(input_4)\n    \n    x1 = Reshape((1, 1 ,128))(x1)\n    x2 = Reshape((1, 1 ,128))(x2)\n    x1 = Concatenate(axis=-1)([GlobalMaxPool2D()(x1), GlobalAvgPool2D()(x1)])\n    x2 = Concatenate(axis=-1)([GlobalMaxPool2D()(x2), GlobalAvgPool2D()(x2)])\n\n    x1t = Lambda(lambda tensor  : K.square(tensor))(x1)\n    x2t = Lambda(lambda tensor  : K.square(tensor))(x2)\n    x3t = Lambda(lambda tensor  : K.square(tensor))(x3)\n    x4t = Lambda(lambda tensor  : K.square(tensor))(x4)\n    \n    merged_add_fn = Add()([x1, x2])\n    merged_add_vgg = Add()([x3, x4])\n    merged_sub1_fn = Subtract()([x1,x2])\n    merged_sub1_vgg = Subtract()([x3,x4])\n    merged_sub2_fn = Subtract()([x2,x1])\n    merged_sub2_vgg = Subtract()([x4,x3])\n    merged_mul1_fn = Multiply()([x1,x2])\n    merged_mul1_vgg = Multiply()([x3,x4])\n    merged_sq1_fn = Add()([x1t,x2t])\n    merged_sq1_vgg = Add()([x3t,x4t])\n    merged_sqrt_fn = Lambda(lambda tensor  : signed_sqrt(tensor))(merged_mul1_fn)\n    merged_sqrt_vgg = Lambda(lambda tensor  : signed_sqrt(tensor))(merged_mul1_vgg)\n\n    \n    merged_add_vgg = Conv2D(128 , [1,1] )(merged_add_vgg)\n    merged_sub1_vgg = Conv2D(128 , [1,1] )(merged_sub1_vgg)\n    merged_sub2_vgg = Conv2D(128 , [1,1] )(merged_sub2_vgg)\n    merged_mul1_vgg = Conv2D(128 , [1,1] )(merged_mul1_vgg)\n    merged_sq1_vgg = Conv2D(128 , [1,1] )(merged_sq1_vgg)\n    merged_sqrt_vgg = Conv2D(128 , [1,1] )(merged_sqrt_vgg)\n    \n    merged = Concatenate(axis=-1)([Flatten()(merged_add_vgg), (merged_add_fn), Flatten()(merged_sub1_vgg), (merged_sub1_fn),\n                                   Flatten()(merged_sub2_vgg), (merged_sub2_fn), Flatten()(merged_mul1_vgg), (merged_mul1_fn), \n                                   Flatten()(merged_sq1_vgg), (merged_sq1_fn), Flatten()(merged_sqrt_vgg), (merged_sqrt_fn)])\n    \n    merged = Dense(100, activation=\"relu\")(merged)\n    merged = Dropout(0.1)(merged)\n    merged = Dense(25, activation=\"relu\")(merged)\n    merged = Dropout(0.1)(merged)\n    out = Dense(1, activation=\"sigmoid\")(merged)\n\n    model = Model([input_1, input_2, input_3, input_4], out)\n\n    model.compile(loss=\"binary_crossentropy\", metrics=['acc'], optimizer=Adam(0.00001))\n\n    model.summary()\n\n    return model\n\n\nfile_path = \"facenet_vgg.h5\"\n\ncheckpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n\nreduce_on_plateau = ReduceLROnPlateau(monitor=\"val_acc\", mode=\"max\", factor=0.1, patience=20, verbose=1)\n\ncallbacks_list = [checkpoint, reduce_on_plateau]\n\nmodel = baseline_model() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit_generator(gen(train, train_person_to_images_map, batch_size=16), use_multiprocessing=True,\n                    validation_data=gen(val, val_person_to_images_map, batch_size=16), epochs=150, verbose=1,\n                    workers = 4, callbacks=callbacks_list, steps_per_epoch=200, validation_steps=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.utils import plot_model\nplot_model(model, to_file='model.png',show_shapes = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_path = \"../input/recognizing-faces-in-the-wild/test/\"\n\n\ndef chunker(seq, size=32):\n    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n\n\nfrom tqdm import tqdm\n\nsubmission = pd.read_csv('../input/recognizing-faces-in-the-wild/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = []\n\nfor batch in tqdm(chunker(submission.img_pair.values)):\n    X1 = [x.split(\"-\")[0] for x in batch]\n    X1_FN = np.array([read_img_fn(test_path + x) for x in X1])\n    X1_VGG = np.array([read_img_vgg(test_path + x) for x in X1])\n \n    X2 = [x.split(\"-\")[1] for x in batch]\n    X2_FN = np.array([read_img_fn(test_path + x) for x in X2])\n    X2_VGG = np.array([read_img_vgg(test_path + x) for x in X2])\n    \n    pred = model.predict([X1_FN, X2_FN, X1_VGG, X2_VGG]).ravel().tolist()\n    \n    predictions += pred\n\nsubmission['is_related'] = predictions\n\nsubmission.to_csv(\"facenetvgg.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}