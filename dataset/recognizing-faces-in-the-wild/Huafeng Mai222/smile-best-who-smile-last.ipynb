{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport os\nimport matplotlib.pyplot as plt\nimport cv2\nfrom imageio import imread\nfrom skimage.transform import resize\nfrom keras.models import load_model\nimport pandas as pd\nfrom tqdm import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-06T22:21:37.013437Z","iopub.execute_input":"2022-06-06T22:21:37.013878Z","iopub.status.idle":"2022-06-06T22:21:39.913561Z","shell.execute_reply.started":"2022-06-06T22:21:37.01379Z","shell.execute_reply":"2022-06-06T22:21:39.912736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_path = '../input/facenet-keras/facenet_keras.h5'\nmodel = load_model(model_path)","metadata":{"execution":{"iopub.status.busy":"2022-06-06T22:21:39.915429Z","iopub.execute_input":"2022-06-06T22:21:39.915713Z","iopub.status.idle":"2022-06-06T22:22:02.244086Z","shell.execute_reply.started":"2022-06-06T22:21:39.915666Z","shell.execute_reply":"2022-06-06T22:22:02.243224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(os.listdir(\"../input\"))","metadata":{"execution":{"iopub.status.busy":"2022-06-06T22:22:02.245396Z","iopub.execute_input":"2022-06-06T22:22:02.24586Z","iopub.status.idle":"2022-06-06T22:22:02.258681Z","shell.execute_reply.started":"2022-06-06T22:22:02.245793Z","shell.execute_reply":"2022-06-06T22:22:02.257975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install git+https://github.com/rcmalli/keras-vggface.git","metadata":{"execution":{"iopub.status.busy":"2022-06-06T22:22:02.259957Z","iopub.execute_input":"2022-06-06T22:22:02.260418Z","iopub.status.idle":"2022-06-06T22:22:13.165269Z","shell.execute_reply.started":"2022-06-06T22:22:02.260366Z","shell.execute_reply":"2022-06-06T22:22:13.164212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import h5py\nfrom collections import defaultdict\nfrom glob import glob\nfrom random import choice, sample\n\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\nfrom keras import backend as K\nfrom keras.layers import Input, Dense, Flatten, GlobalMaxPool2D, GlobalAvgPool2D, Concatenate, Multiply, Dropout, Subtract, Add, Conv2D, Lambda, Reshape\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nfrom keras_vggface.utils import preprocess_input\nfrom keras_vggface.vggface import VGGFace","metadata":{"execution":{"iopub.status.busy":"2022-06-06T22:22:13.16914Z","iopub.execute_input":"2022-06-06T22:22:13.16948Z","iopub.status.idle":"2022-06-06T22:22:13.183819Z","shell.execute_reply.started":"2022-06-06T22:22:13.16942Z","shell.execute_reply":"2022-06-06T22:22:13.182689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!unzip -q ../input/recognizing-faces-in-the-wild/train.zip -d train\n!unzip -q ../input/recognizing-faces-in-the-wild/test.zip -d test","metadata":{"execution":{"iopub.status.busy":"2022-06-06T22:22:13.18794Z","iopub.execute_input":"2022-06-06T22:22:13.188428Z","iopub.status.idle":"2022-06-06T22:22:16.689395Z","shell.execute_reply.started":"2022-06-06T22:22:13.18837Z","shell.execute_reply":"2022-06-06T22:22:16.688315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_file_path = \"../input/recognizing-faces-in-the-wild/train_relationships.csv\"\ntrain_folders_path = \"./train/\"\nval_families = \"F09\"","metadata":{"execution":{"iopub.status.busy":"2022-06-06T22:22:16.692018Z","iopub.execute_input":"2022-06-06T22:22:16.692333Z","iopub.status.idle":"2022-06-06T22:22:16.696735Z","shell.execute_reply.started":"2022-06-06T22:22:16.692281Z","shell.execute_reply":"2022-06-06T22:22:16.695964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_images = glob(train_folders_path + \"*/*/*.jpg\")","metadata":{"execution":{"iopub.status.busy":"2022-06-06T22:22:16.698219Z","iopub.execute_input":"2022-06-06T22:22:16.698796Z","iopub.status.idle":"2022-06-06T22:22:16.812884Z","shell.execute_reply.started":"2022-06-06T22:22:16.698742Z","shell.execute_reply":"2022-06-06T22:22:16.811981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(all_images[1])#./train/F0339/MID4/P03587_face3.jpg","metadata":{"execution":{"iopub.status.busy":"2022-06-06T22:22:16.814242Z","iopub.execute_input":"2022-06-06T22:22:16.814534Z","iopub.status.idle":"2022-06-06T22:22:16.820195Z","shell.execute_reply.started":"2022-06-06T22:22:16.814486Z","shell.execute_reply":"2022-06-06T22:22:16.819263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_images = [x for x in all_images if val_families not in x]\nval_images = [x for x in all_images if val_families in x]","metadata":{"execution":{"iopub.status.busy":"2022-06-06T22:22:16.821742Z","iopub.execute_input":"2022-06-06T22:22:16.822504Z","iopub.status.idle":"2022-06-06T22:22:16.831646Z","shell.execute_reply.started":"2022-06-06T22:22:16.822445Z","shell.execute_reply":"2022-06-06T22:22:16.830672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ppl = [x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2] for x in all_images]\nprint(ppl[0])","metadata":{"execution":{"iopub.status.busy":"2022-06-06T22:22:16.833191Z","iopub.execute_input":"2022-06-06T22:22:16.833702Z","iopub.status.idle":"2022-06-06T22:22:16.85426Z","shell.execute_reply.started":"2022-06-06T22:22:16.833523Z","shell.execute_reply":"2022-06-06T22:22:16.853235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_person_to_images_map = defaultdict(list)\n\nfor x in train_images:\n    train_person_to_images_map[x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2]].append(x)\n\nval_person_to_images_map = defaultdict(list)\n\nfor x in val_images:\n    val_person_to_images_map[x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2]].append(x)\n\nrelationships = pd.read_csv(train_file_path)\nrelationships = list(zip(relationships.p1.values, relationships.p2.values))\nrelationships = [x for x in relationships if x[0] in ppl and x[1] in ppl]\n\ntrain = [x for x in relationships if val_families not in x[0]]\nval = [x for x in relationships if val_families in x[0]]","metadata":{"execution":{"iopub.status.busy":"2022-06-06T22:22:16.856032Z","iopub.execute_input":"2022-06-06T22:22:16.85648Z","iopub.status.idle":"2022-06-06T22:22:17.909493Z","shell.execute_reply.started":"2022-06-06T22:22:16.856358Z","shell.execute_reply":"2022-06-06T22:22:17.90864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prewhiten(x):\n    if x.ndim == 4:\n        axis = (1, 2, 3)\n        size = x[0].size\n    elif x.ndim == 3:\n        axis = (0, 1, 2)\n        size = x.size\n    else:\n        raise ValueError('Dimension should be 3 or 4')\n\n    mean = np.mean(x, axis=axis, keepdims=True)\n    std = np.std(x, axis=axis, keepdims=True)\n    std_adj = np.maximum(std, 1.0/np.sqrt(size))\n    y = (x - mean) / std_adj\n    return y","metadata":{"_uuid":"b8da009689331982f628256abc151cbbd7e288a1","execution":{"iopub.status.busy":"2022-06-06T22:22:17.911006Z","iopub.execute_input":"2022-06-06T22:22:17.911477Z","iopub.status.idle":"2022-06-06T22:22:17.920748Z","shell.execute_reply.started":"2022-06-06T22:22:17.91139Z","shell.execute_reply":"2022-06-06T22:22:17.919799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_path = '../input/facenet-keras/facenet_keras.h5'\nmodel_fn = load_model(model_path)","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.status.busy":"2022-06-06T22:22:17.922315Z","iopub.execute_input":"2022-06-06T22:22:17.922879Z","iopub.status.idle":"2022-06-06T22:22:40.408631Z","shell.execute_reply.started":"2022-06-06T22:22:17.922816Z","shell.execute_reply":"2022-06-06T22:22:40.407731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for layer in model_fn.layers[:-3]:\n    layer.trainable = True","metadata":{"execution":{"iopub.status.busy":"2022-06-06T22:22:40.410173Z","iopub.execute_input":"2022-06-06T22:22:40.410458Z","iopub.status.idle":"2022-06-06T22:22:40.415909Z","shell.execute_reply.started":"2022-06-06T22:22:40.410409Z","shell.execute_reply":"2022-06-06T22:22:40.415162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_vgg = VGGFace(model='resnet50', include_top=False)\nfor layer in model_vgg.layers[:-3]:\n    layer.trainable = True","metadata":{"execution":{"iopub.status.busy":"2022-06-06T22:22:40.417381Z","iopub.execute_input":"2022-06-06T22:22:40.41795Z","iopub.status.idle":"2022-06-06T22:23:18.325824Z","shell.execute_reply.started":"2022-06-06T22:22:40.417895Z","shell.execute_reply":"2022-06-06T22:23:18.325056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Define image size for facenet and vgg","metadata":{}},{"cell_type":"code","source":"IMG_SIZE_FN = 160\nIMG_SIZE_VGG = 224","metadata":{"execution":{"iopub.status.busy":"2022-06-06T22:23:18.32721Z","iopub.execute_input":"2022-06-06T22:23:18.327496Z","iopub.status.idle":"2022-06-06T22:23:18.339489Z","shell.execute_reply.started":"2022-06-06T22:23:18.327448Z","shell.execute_reply":"2022-06-06T22:23:18.33858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_img_fn(path):\n    img = cv2.imread(path)\n    img = cv2.resize(img,(IMG_SIZE_FN,IMG_SIZE_FN))\n    img = np.array(img).astype(np.float)\n    return prewhiten(img)\n\ndef read_img_vgg(path):\n    img = cv2.imread(path)\n    img = cv2.resize(img,(IMG_SIZE_VGG,IMG_SIZE_VGG))\n    img = np.array(img).astype(np.float)\n    return preprocess_input(img, version=2)\n\ndef gen(list_tuples, person_to_images_map, batch_size=16):\n    ppl = list(person_to_images_map.keys())\n    while True:\n        batch_tuples = sample(list_tuples, batch_size // 2)\n        labels = [1] * len(batch_tuples)\n        while len(batch_tuples) < batch_size:\n            p1 = choice(ppl)\n            p2 = choice(ppl)\n\n            if p1 != p2 and (p1, p2) not in list_tuples and (p2, p1) not in list_tuples:\n                batch_tuples.append((p1, p2))\n                labels.append(0)\n\n        for x in batch_tuples:\n            if not len(person_to_images_map[x[0]]):\n                print(x[0])\n\n        X1 = [choice(person_to_images_map[x[0]]) for x in batch_tuples]\n        X1_FN = np.array([read_img_fn(x) for x in X1])\n        X1_VGG = np.array([read_img_vgg(x) for x in X1])\n\n        X2 = [choice(person_to_images_map[x[1]]) for x in batch_tuples]\n        X2_FN = np.array([read_img_fn(x) for x in X2])\n        X2_VGG = np.array([read_img_vgg(x) for x in X2])\n\n        yield [X1_FN, X2_FN, X1_VGG, X2_VGG], labels                    \n       # yield [X1_VGG, X2_VGG], labels\n","metadata":{"execution":{"iopub.status.busy":"2022-06-06T22:23:18.3414Z","iopub.execute_input":"2022-06-06T22:23:18.342225Z","iopub.status.idle":"2022-06-06T22:23:18.357608Z","shell.execute_reply.started":"2022-06-06T22:23:18.34217Z","shell.execute_reply":"2022-06-06T22:23:18.356883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def signed_sqrt(x):\n    return K.sign(x)*K.sqrt(K.abs(x)+1e-9)","metadata":{"execution":{"iopub.status.busy":"2022-06-06T22:23:18.359223Z","iopub.execute_input":"2022-06-06T22:23:18.359717Z","iopub.status.idle":"2022-06-06T22:23:18.369351Z","shell.execute_reply.started":"2022-06-06T22:23:18.359509Z","shell.execute_reply":"2022-06-06T22:23:18.368567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def baseline_model():\n    input_1 = Input(shape=(IMG_SIZE_FN, IMG_SIZE_FN, 3))\n    input_2 = Input(shape=(IMG_SIZE_FN, IMG_SIZE_FN, 3))\n    input_3 = Input(shape=(IMG_SIZE_VGG, IMG_SIZE_VGG, 3))\n    input_4 = Input(shape=(IMG_SIZE_VGG, IMG_SIZE_VGG, 3))\n\n    x1 = model_fn(input_1)\n    x2 = model_fn(input_2)\n    x3 = model_vgg(input_3)\n    x4 = model_vgg(input_4)\n    \n    x1 = Reshape((1, 1 ,128))(x1)\n    x2 = Reshape((1, 1 ,128))(x2)\n    x1 = Concatenate(axis=-1)([GlobalMaxPool2D()(x1), GlobalAvgPool2D()(x1)])\n    x2 = Concatenate(axis=-1)([GlobalMaxPool2D()(x2), GlobalAvgPool2D()(x2)])\n\n    x1t = Lambda(lambda tensor  : K.square(tensor))(x1)\n    x2t = Lambda(lambda tensor  : K.square(tensor))(x2)\n    x3t = Lambda(lambda tensor  : K.square(tensor))(x3)\n    x4t = Lambda(lambda tensor  : K.square(tensor))(x4)\n    \n    merged_add_fn = Add()([x1, x2])\n    merged_add_vgg = Add()([x3, x4])\n    merged_sub1_fn = Subtract()([x1,x2])\n    merged_sub1_vgg = Subtract()([x3,x4])\n    merged_sub2_fn = Subtract()([x2,x1])\n    merged_sub2_vgg = Subtract()([x4,x3])\n    merged_mul1_fn = Multiply()([x1,x2])\n    merged_mul1_vgg = Multiply()([x3,x4])\n    merged_sq1_fn = Add()([x1t,x2t])\n    merged_sq1_vgg = Add()([x3t,x4t])\n    merged_sqrt_fn = Lambda(lambda tensor  : signed_sqrt(tensor))(merged_mul1_fn)\n    merged_sqrt_vgg = Lambda(lambda tensor  : signed_sqrt(tensor))(merged_mul1_vgg)\n\n    \n    merged_add_vgg = Conv2D(128 , [1,1] )(merged_add_vgg)\n    merged_sub1_vgg = Conv2D(128 , [1,1] )(merged_sub1_vgg)\n    merged_sub2_vgg = Conv2D(128 , [1,1] )(merged_sub2_vgg)\n    merged_mul1_vgg = Conv2D(128 , [1,1] )(merged_mul1_vgg)\n    merged_sq1_vgg = Conv2D(128 , [1,1] )(merged_sq1_vgg)\n    merged_sqrt_vgg = Conv2D(128 , [1,1] )(merged_sqrt_vgg)\n    \n    merged = Concatenate(axis=-1)([Flatten()(merged_add_vgg), (merged_add_fn), Flatten()(merged_sub1_vgg), (merged_sub1_fn),\n                                   Flatten()(merged_sub2_vgg), (merged_sub2_fn), Flatten()(merged_mul1_vgg), (merged_mul1_fn), \n                                   Flatten()(merged_sq1_vgg), (merged_sq1_fn), Flatten()(merged_sqrt_vgg), (merged_sqrt_fn)])\n    \n    merged = Dense(100, activation=\"relu\")(merged)\n    merged = Dropout(0.1)(merged)\n    merged = Dense(25, activation=\"relu\")(merged)\n    merged = Dropout(0.1)(merged)\n    out = Dense(1, activation=\"sigmoid\")(merged)\n\n    model = Model([input_1, input_2, input_3, input_4], out)\n\n    model.compile(loss=\"binary_crossentropy\", metrics=['acc'], optimizer=Adam(0.00001))\n\n    model.summary()\n\n    return model\n\n\n\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-06T22:23:18.372158Z","iopub.execute_input":"2022-06-06T22:23:18.372931Z","iopub.status.idle":"2022-06-06T22:23:18.396308Z","shell.execute_reply.started":"2022-06-06T22:23:18.372613Z","shell.execute_reply":"2022-06-06T22:23:18.395519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def faceNet():\n    input_1 = Input(shape=(IMG_SIZE_FN, IMG_SIZE_FN, 3))\n    input_2 = Input(shape=(IMG_SIZE_FN, IMG_SIZE_FN, 3))\n   \n    x1 = model_fn(input_1)\n    x2 = model_fn(input_2)\n    \n    x1 = Reshape((1, 1 ,128))(x1)\n    x2 = Reshape((1, 1 ,128))(x2)\n    x1 = Concatenate(axis=-1)([GlobalMaxPool2D()(x1), GlobalAvgPool2D()(x1)])\n    x2 = Concatenate(axis=-1)([GlobalMaxPool2D()(x2), GlobalAvgPool2D()(x2)])\n\n    x1t = Lambda(lambda tensor  : K.square(tensor))(x1)\n    x2t = Lambda(lambda tensor  : K.square(tensor))(x2)\n    \n    merged_add_fn = Add()([x1, x2])\n    merged_sub1_fn = Subtract()([x1,x2])\n    merged_sub2_fn = Subtract()([x2,x1])\n    merged_mul1_fn = Multiply()([x1,x2])\n    merged_sq1_fn = Add()([x1t,x2t])\n    merged_sqrt_fn = Lambda(lambda tensor  : signed_sqrt(tensor))(merged_mul1_fn)\n\n    \n    \n    merged = Concatenate(axis=-1)([ (merged_add_fn), (merged_sub1_fn),(merged_sub2_fn), (merged_mul1_fn), \n                                   (merged_sq1_fn),(merged_sqrt_fn)])\n    \n    merged = Dense(100, activation=\"relu\")(merged)\n    merged = Dropout(0.1)(merged)\n    merged = Dense(25, activation=\"relu\")(merged)\n    merged = Dropout(0.1)(merged)\n    out = Dense(1, activation=\"sigmoid\")(merged)\n\n    model = Model([input_1, input_2], out)\n\n    model.compile(loss=\"binary_crossentropy\", metrics=['acc'], optimizer=Adam(0.00001))\n\n    model.summary()\n\n    return model\n","metadata":{"execution":{"iopub.status.busy":"2022-06-06T22:23:18.39888Z","iopub.execute_input":"2022-06-06T22:23:18.399561Z","iopub.status.idle":"2022-06-06T22:23:18.415169Z","shell.execute_reply.started":"2022-06-06T22:23:18.399438Z","shell.execute_reply":"2022-06-06T22:23:18.414358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def vgg():\n    input_3 = Input(shape=(IMG_SIZE_VGG, IMG_SIZE_VGG, 3))\n    input_4 = Input(shape=(IMG_SIZE_VGG, IMG_SIZE_VGG, 3))\n\n  \n    x3 = model_vgg(input_3)\n    x4 = model_vgg(input_4)\n    \n\n    x3t = Lambda(lambda tensor  : K.square(tensor))(x3)\n    x4t = Lambda(lambda tensor  : K.square(tensor))(x4)\n    \n  \n    merged_add_vgg = Add()([x3, x4])\n    merged_sub1_vgg = Subtract()([x3,x4])\n    merged_sub2_vgg = Subtract()([x4,x3])\n    merged_mul1_vgg = Multiply()([x3,x4])\n    merged_sq1_vgg = Add()([x3t,x4t])\n    merged_sqrt_vgg = Lambda(lambda tensor  : signed_sqrt(tensor))(merged_mul1_vgg)\n\n    \n    merged_add_vgg = Conv2D(128 , [1,1] )(merged_add_vgg)\n    merged_sub1_vgg = Conv2D(128 , [1,1] )(merged_sub1_vgg)\n    merged_sub2_vgg = Conv2D(128 , [1,1] )(merged_sub2_vgg)\n    merged_mul1_vgg = Conv2D(128 , [1,1] )(merged_mul1_vgg)\n    merged_sq1_vgg = Conv2D(128 , [1,1] )(merged_sq1_vgg)\n    merged_sqrt_vgg = Conv2D(128 , [1,1] )(merged_sqrt_vgg)\n    \n    merged = Concatenate(axis=-1)([ Flatten()(merged_sub1_vgg), Flatten()(merged_sub2_vgg),  Flatten()(merged_mul1_vgg), \n                                   Flatten()(merged_sq1_vgg),  Flatten()(merged_sqrt_vgg)])\n    \n    merged = Dense(100, activation=\"relu\")(merged)\n    merged = Dropout(0.1)(merged)\n    merged = Dense(25, activation=\"relu\")(merged)\n    merged = Dropout(0.1)(merged)\n    out = Dense(1, activation=\"sigmoid\")(merged)\n\n    model = Model([ input_3, input_4], out)\n\n    model.compile(loss=\"binary_crossentropy\", metrics=['acc'], optimizer=Adam(0.00001))\n\n    model.summary()\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-06-06T22:23:18.417149Z","iopub.execute_input":"2022-06-06T22:23:18.417727Z","iopub.status.idle":"2022-06-06T22:23:18.435446Z","shell.execute_reply.started":"2022-06-06T22:23:18.417531Z","shell.execute_reply":"2022-06-06T22:23:18.434274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file_path = \"facenet_vgg.h5\"\n\ncheckpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n\nreduce_on_plateau = ReduceLROnPlateau(monitor=\"val_acc\", mode=\"max\", factor=0.1, patience=20, verbose=1)\n\ncallbacks_list = [checkpoint, reduce_on_plateau]","metadata":{"execution":{"iopub.status.busy":"2022-06-06T22:23:18.437487Z","iopub.execute_input":"2022-06-06T22:23:18.438207Z","iopub.status.idle":"2022-06-06T22:23:18.446527Z","shell.execute_reply.started":"2022-06-06T22:23:18.437971Z","shell.execute_reply":"2022-06-06T22:23:18.445448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model=baseline_model()\n#model=faceNet() \n#model=vgg()","metadata":{"execution":{"iopub.status.busy":"2022-06-06T22:23:18.44844Z","iopub.execute_input":"2022-06-06T22:23:18.449113Z","iopub.status.idle":"2022-06-06T22:23:50.06633Z","shell.execute_reply.started":"2022-06-06T22:23:18.448815Z","shell.execute_reply":"2022-06-06T22:23:50.065494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history=model.fit_generator(gen(train, train_person_to_images_map, batch_size=16), use_multiprocessing=True,\n                    validation_data=gen(val, val_person_to_images_map, batch_size=16), epochs=200, verbose=1,\n                    workers = 4, callbacks=callbacks_list, steps_per_epoch=200, validation_steps=100)","metadata":{"execution":{"iopub.status.busy":"2022-06-06T22:23:50.067765Z","iopub.execute_input":"2022-06-06T22:23:50.068079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  \"Accuracy\"\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n# \"Loss\"\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.utils import plot_model\nplot_model(model, to_file='model.png',show_shapes = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_path = \"./test/\"\n\n\ndef chunker(seq, size=32):\n    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n\n\nfrom tqdm import tqdm\n\nsubmission = pd.read_csv('../input/recognizing-faces-in-the-wild/sample_submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = []\n\nfor batch in tqdm(chunker(submission.img_pair.values)):\n    X1 = [x.split(\"-\")[0] for x in batch]\n   # print(test_path + X1[0])\n    X1_FN = np.array([read_img_fn(test_path + x) for x in X1])\n    X1_VGG = np.array([read_img_vgg(test_path + x) for x in X1])\n \n    X2 = [x.split(\"-\")[1] for x in batch]\n    X2_FN = np.array([read_img_fn(test_path + x) for x in X2])\n    X2_VGG = np.array([read_img_vgg(test_path + x) for x in X2])\n    \n    pred = model.predict([X1_FN, X2_FN]).ravel().tolist()\n    \n    predictions += pred\n\nsubmission['is_related'] = predictions\n\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(submission.head(10))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}