{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Log\n- Version 5:\n    - Use VGGFace to replace FaceNet: 2048 features instead of 128 features.\n    - Do not use very similar pictures of the same person, in order to save RAM. Use FaceNet to pick similar pictures."},{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\n> This is my first kernel shared :)\n\n- The main idea of this kernel is to share some observations on the dataset and some recommandations on the cross-validation folders. \n- The Pipeline is: firstly use FaceNet to have features extracted, then use these features to train a traditional machine learning model, and use this model to predict on test set.\n\nThe FaceNet idea was inspired by [Khoi Nguyen](https://www.kaggle.com/suicaokhoailang) and his [kernel](https://www.kaggle.com/suicaokhoailang/facenet-baseline-in-keras-0-749-lb). \n\nI am new to Deep Learning, so firstly I only used deep net as feature extractor, then use traditional way to train the model. Here we can use other deep net, like VGGFace, to replace FaceNet as feature extractor. After feature extraction, we can test on very different traditional machine learning models."},{"metadata":{},"cell_type":"markdown","source":"# Load useful libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom imageio import imread\nfrom skimage.transform import resize\nfrom keras.models import load_model\nfrom tqdm._tqdm_notebook import tqdm_notebook\nfrom sklearn.model_selection import GroupKFold\n\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\nwarnings.filterwarnings('ignore') #this one works good!","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Use nearest neighbor to remove similar pictures"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/recognizing-faces-in-the-wild/train_relationships.csv\")\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find all the train images\n\ndef findAllTrain(train_folder):\n    train_li=[]\n    for fam in os.listdir(train_folder):\n        for pers in os.listdir(os.path.join(train_folder,fam)):\n            for pic in os.listdir(os.path.join(train_folder,fam,pers)):\n                train_li.append(os.path.join(fam,pers,pic))\n    \n    return train_li\n\ntrain_fd = '../input/recognizing-faces-in-the-wild/train'\n\ntrain_file_li=findAllTrain(train_fd)\n\nprint('There are {} images in the train dataset.'.\n      format(len(train_file_li)))\n\n#Create a dict to store all the train images\ntrain_file_dict=dict(zip(train_file_li,range(len(train_file_li))))\n\n# Create a DataFrame to store all the train images\ntrain_file_df = pd.DataFrame()\ntrain_file_df['image_fp']=train_file_li\ntrain_file_df.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install git+https://github.com/rcmalli/keras-vggface.git","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras_vggface.utils import preprocess_input\nfrom keras_vggface.vggface import VGGFace\nfrom keras.preprocessing import image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convolution Features\nvgg_features = VGGFace(include_top=False, input_shape=(224, 224, 3), pooling='avg', model='resnet50') # pooling: None, avg or max\n\n# After this point you can use your model to predict.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef load_images(filepaths,target_size=(224, 224)):\n    \n    aligned_images = []\n    for filepath in filepaths:\n        img = image.load_img(filepath, target_size=target_size)\n        x_ = image.img_to_array(img)\n        aligned_images.append(x_)\n            \n    return preprocess_input(np.array(aligned_images),version=2)\n\ndef calc_embs(filepaths,batch_size=512,target_size=(224, 224)):\n    pd = []\n    for start in tqdm_notebook(range(0, len(filepaths), batch_size)):\n        aligned_images = load_images(filepaths[start:start+batch_size],target_size=target_size)\n        pd.append(vgg_features.predict_on_batch(aligned_images))\n    embs = np.concatenate(pd)\n\n    return embs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_size=2048","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate embs for train images\n\ntrain_embs = calc_embs([os.path.join(\"../input/recognizing-faces-in-the-wild/train\", f) for f in train_file_df['image_fp']])\ntrain_file_df=pd.concat([train_file_df, pd.DataFrame(train_embs,columns=['fe'+str(i) for i in range(feature_size)])],axis=1)\ntrain_file_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get family ID for each image\ntrain_file_df['fam_person']=\" \"\ntrain_file_df['fam_person']=train_file_df['image_fp'].apply(lambda x: x[:x.find('P')-1])\ntrain_file_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import NearestNeighbors\n\nradius=60\nneigh = NearestNeighbors(radius=radius)\n\nfor person in train_file_df['fam_person'].unique():\n    if person=='F0601/MID1':\n        print(person)\n        print(len(train_file_df.loc[train_file_df['fam_person']==person,:]))\n        person_df = train_file_df.loc[train_file_df['fam_person']==person,:]\n        neigh.fit(person_df.iloc[:,1:2049])\n#         rng = neigh.radius_neighbors([person_df.iloc[0,1:2049]])\n#         print(len(rng[0][0])-1)\n        print(person_df.apply(lambda x: len(neigh.radius_neighbors([x[1:2049]])[0][0])-1,axis=1))\n        print(person_df.apply(lambda x: neigh.radius_neighbors([x[1:2049]]),axis=1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show2pic('../input/recognizing-faces-in-the-wild/train/',\n         train_file_df.loc[2427,'image_fp'] + '-' + train_file_df.loc[2437,'image_fp'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show2pic('../input/recognizing-faces-in-the-wild/train/',\n         train_file_df.loc[3313,'image_fp'] + '-' + train_file_df.loc[3319,'image_fp'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# use this function to show some image pairs.\ndef show2pic(fd,paire):\n    plt.figure(figsize=(7,10))\n    plt.subplot(121)\n    plt.imshow(imread(os.path.join(fd,paire.split('-')[0])))\n    plt.axis('off')\n    plt.title(paire.split('-')[0])\n    plt.subplot(122)\n    plt.imshow(imread(os.path.join(fd,paire.split('-')[1])))\n    plt.axis('off')\n    plt.title(paire.split('-')[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show2pic('../input/recognizing-faces-in-the-wild/train/',\n         train_file_df.loc[3330,'image_fp'] + '-' + train_file_df.loc[3336,'image_fp'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Get image pairs\n\nFind all the possible image pairs for training."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/recognizing-faces-in-the-wild/train_relationships.csv\")\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find all the train images\n\ndef findAllTrain(train_folder):\n    train_li=[]\n    for fam in os.listdir(train_folder):\n        for pers in os.listdir(os.path.join(train_folder,fam)):\n            for pic in os.listdir(os.path.join(train_folder,fam,pers)):\n                train_li.append(os.path.join(fam,pers,pic))\n    \n    return train_li\n\ntrain_fd = '../input/recognizing-faces-in-the-wild/train'\n\ntrain_file_li=findAllTrain(train_fd)\n\nprint('There are {} images in the train dataset.'.\n      format(len(train_file_li)))\n\n#Create a dict to store all the train images\ntrain_file_dict=dict(zip(train_file_li,range(len(train_file_li))))\n\n# Create a DataFrame to store all the train images\ntrain_file_df = pd.DataFrame()\ntrain_file_df['image_fp']=train_file_li\ntrain_file_df.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find all the image pairs with kinship\n\ntrain_fd = '../input/recognizing-faces-in-the-wild/train'\n\nindex_p1_li=[]\nindex_p2_li=[]\n\nfor idx, row in tqdm_notebook(train_df.iterrows(), total=len(train_df)):\n    if os.path.isdir(os.path.join(train_fd,row['p1'])) and os.path.isdir(os.path.join(train_fd,row['p2'])): # some folders do not exist !!\n        for p1_pic in os.listdir(os.path.join(train_fd,row['p1'])):\n            for p2_pic in os.listdir(os.path.join(train_fd,row['p2'])):\n                index_f1=train_file_dict[os.path.join(row['p1'].split('/')[0],row['p1'].split('/')[1],p1_pic)]\n                index_f2=train_file_dict[os.path.join(row['p2'].split('/')[0],row['p2'].split('/')[1],p2_pic)]\n                if index_f1<index_f2: # force the image pairs to have the same order of persons\n                    index_p1_li.append(index_f1)\n                    index_p2_li.append(index_f2)\n                else:\n                    index_p1_li.append(index_f2)\n                    index_p2_li.append(index_f1)\n                    \ntrain_pairs_kinship=pd.DataFrame()\ntrain_pairs_kinship['p1']=index_p1_li\ntrain_pairs_kinship['p2']=index_p2_li\n\nindex_p1_li=[]\nindex_p2_li=[]\n\nprint('Total image pairs with kinship: {}'.format(len(train_pairs_kinship)))\ntrain_pairs_kinship.sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**How about the image pairs from the same person? Should they be used as positive samples (with kinship)? I think YES.**\n\nBecause basicly we are training a model to identify the similarity of two images, the same person's images can bring us more postive samples. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# make image pairs of the same person\n# for example: for this person \"F0002\\MID1\", there are 10 images in the folder, so it can make 10*9/2=45 pairs.\n\ndef make_pair_same_person(source,pre_path):\n    res_p1_li = []\n    res_p2_li = []\n    for p1 in range(len(source)):\n        for p2 in range(p1+1,len(source)):\n            index_f1=train_file_dict[os.path.join(pre_path,source[p1])]\n            index_f2=train_file_dict[os.path.join(pre_path,source[p2])]\n            if index_f1<index_f2: # force the image pairs to have the same order of persons\n                res_p1_li.append(index_f1)\n                res_p2_li.append(index_f2)\n            else:\n                res_p1_li.append(index_f2)\n                res_p2_li.append(index_f1)\n            \n    return (res_p1_li,res_p2_li)\n\nindex_p1_li = []\nindex_p2_li = []\nfor fam in os.listdir(train_fd):\n    for pers in os.listdir(os.path.join(train_fd,fam)):\n        res_temp = make_pair_same_person([pic for pic in os.listdir(os.path.join(train_fd,fam,pers))],os.path.join(fam,pers))\n        index_p1_li.extend(res_temp[0])\n        index_p2_li.extend(res_temp[1])\n\ntrain_pairs_same=pd.DataFrame()\ntrain_pairs_same['p1']=index_p1_li\ntrain_pairs_same['p2']=index_p2_li\n\nindex_p1_li = []\nindex_p2_li = []\n\nprint('Total image pairs of same person: {}'.format(len(train_pairs_same)))\n\ntrain_pairs_same.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(12,2))\ny2show=[len(train_pairs_kinship),len(train_pairs_same)]\nplt.barh(range(2),y2show,0.35)\nplt.title('Image pair number')\nplt.yticks(range(2), ('With kinship', 'From same person'),)\nplt.box(on=None)\nplt.xticks([], [])\nfor i, v in enumerate(y2show):\n    ax.text(v+1000, i-0.05, str(v), color='blue', fontweight='bold')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_pairs_kinship=pd.concat([train_pairs_kinship,train_pairs_same],ignore_index=True) # Combine them together\ntrain_pairs_same=None # to free RAM\nprint('Total POSITIVE image pairs: {}'.format(len(train_pairs_kinship)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get all the possible image pairs\n\nindex_p1_li = []\nindex_p2_li = []\n\nfor p1 in tqdm_notebook(range(len(train_file_li))):    \n    for p2 in range(p1+1,len(train_file_li)):\n        index_p1_li.append(p1)\n        index_p2_li.append(p2)\n\ntrain_pairs_all=pd.DataFrame()\ntrain_pairs_all['p1']=index_p1_li\nindex_p1_li = []\ntrain_pairs_all['p2']=index_p2_li\nindex_p2_li = []\n\nprint('Total image pairs: {}'.format(len(train_pairs_all)))\n\ntrain_pairs_all.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add a col \"is_related\": 1 if POS, 0 if NEG\n\nkin_index=np.arange(len(train_pairs_all))[train_pairs_all.merge(train_pairs_kinship, on=['p1','p2'],how='left', indicator=True)['_merge']=='both']\ntrain_pairs_all['is_related']=0\ntrain_pairs_all.loc[kin_index,'is_related']=1\nkin_index=None # to free RAM\ntrain_pairs_kinship=None # to free RAM","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(12,2))\ny2show=[train_pairs_all.query('is_related == 0').shape[0],train_pairs_all.query('is_related == 1').shape[0]]\nplt.barh(range(2),y2show,0.35)\nplt.title('Image pair number')\nplt.yticks(range(2), ('No Kinship (NEG)','With kinship (POS)'))\nplt.box(on=None)\nplt.xticks([], [])\nfor i, v in enumerate(y2show):\n    ax.text(v+1000, i-0.05, str(v), color='blue', fontweight='bold')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The number of negative samples is {:.0f} times of positive samples!\".\n      format(train_pairs_all.query('is_related == 0').shape[0]/train_pairs_all.query('is_related == 1').shape[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create folders for cross-validation\n\nIt is very important to have good cross-validation folders, in order to:\n- Avoid data leakage\n- Optimize model parameters\n- Close the gap between your validation set and LB score\n\nThe main idea is: **The same family does NOT appear in two different folds!** \n\nThis is the same as **GroupKFold** in Scikit-Learn. However, we can not use GroupKFold directly in this case. If we define one \"family\" as one \"group\", then it will be difficult to define the family ID for negative samples (image pair with NO kinship), because the 2 persons in negative image pairs can be from 2 different families.\n\nSo we have to create our own group number for NEG samples. Firstly, use GroupKFold to seperate POS samples into N folders (use family ID as group). Then we can get a family list for each folder. And this family list can be used to get NEG samples for each folders. For example, the 2 persons in a NEG sample are from family-1 and family-2, and both families are in folder-A's family list, then this NEG sample can be assigned to folder-A.\n\nHowever, this method has a problem on this dataset!! Let's see below:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get family ID for each image\n\ntrain_file_df['fam']=-1\ntrain_file_df['fam']=train_file_df['image_fp'].apply(lambda x: int(x[1:5]))\ntrain_file_df.reset_index(inplace=True)\ntrain_file_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('There are {} families in the train set.'.format(len(train_file_df.fam.unique())))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get family ID for each POSimage pair (use p1 only)\ntrain_pairs_kinship = train_pairs_all.query('is_related == 1')\ntrain_pairs_kinship=train_pairs_kinship.merge(train_file_df[['index','fam']], left_on='p1',right_on='index',how='left').drop(columns=['index'])\ntrain_pairs_kinship.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(12,5))\nsns.countplot(x='fam',data=train_pairs_kinship,\n              order=train_pairs_kinship.fam.value_counts().iloc[:20].index)\nplt.title('Top 20 families (image pair with kinship | POS samples)')\nax.text(12, 30000, 'Average POS samples per family is {:.0f}'.format(len(train_pairs_kinship)/len(train_pairs_kinship.fam.unique())),fontsize=12)\nfor i, v in enumerate(train_pairs_kinship['fam'].value_counts()[:20]):\n    ax.text(i-0.4, v+500, str(v),color='gray')\nplt.box(on=None)\nplt.yticks([]);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Family 601 contains {:.0f}% of image pair of all the POS samples!'.format(train_pairs_kinship['fam'].value_counts().tolist()[0]/len(train_pairs_kinship)*100))\nprint('Family 9 contains {:.0f}% of image pair of all the POS samples.'.format(train_pairs_kinship['fam'].value_counts().tolist()[1]/len(train_pairs_kinship)*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train_pairs_kinship.fam.value_counts());","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As shown above, **the family 601 represents 35% of POS samples**.\n\nWhat happens? If you open the family folder, you'll find that it's **British Royal Family** ! Of course!\n\nThis kernel [EDA with Plotly-Smart, Cute and Pretty People](https://www.kaggle.com/gowrishankarin/eda-with-plotly-smart-cute-and-pretty-people) by [Gowri Shankar](https://www.kaggle.com/gowrishankarin) shows great visualizations on this.\n\nWhy does it cause a problem to create our CV folders? \n\nWe want each folder to have equivalent number of samples. If we cut our samples into 3 or more folders, and we don't want the same family appears in two different folds (to avoid data leakage), so the British Royal Family will take one whole folder. It will bias the cross validation score. \n\nSo, we may reduce the POS sample number per family to a certain limit, like 3000. If the number is above the limit, only use 3000 random samples from tha family. "},{"metadata":{"trusted":true},"cell_type":"code","source":"limit_number = 300 # Only use 300 because of lack of RAM for 2048 features\n\nindex_li = train_pairs_kinship['fam'].value_counts()[lambda x:x<=limit_number].index\ntrain_fam_lim_df = train_pairs_kinship[train_pairs_kinship['fam'].isin(index_li)]\n\nfor i in train_pairs_kinship['fam'].value_counts()[lambda x:x>limit_number].index:\n    df_temp = train_pairs_kinship.query('fam == {}'.format(i)).sample(limit_number,replace=False,random_state=2019)\n    train_fam_lim_df = pd.concat([train_fam_lim_df, df_temp])\n    \ntrain_fam_lim_df=train_fam_lim_df.reset_index() # Reset index for GroupKFold method\n\nprint('Number of POS samples in the selected dataset: {}'.format(len(train_fam_lim_df)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train_fam_lim_df.fam.value_counts());","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gkf = GroupKFold(n_splits=6) # Group 6 as test set, Group0-5 as CV folders.\n\ntrain_fam=train_fam_lim_df['fam']\n\nfam_group=np.ones(max(train_fam_lim_df['fam'])+1)*(-1)\nfam_group=fam_group.astype(int)\n\nfor idx,( _, test_index) in enumerate(gkf.split(X=train_fam,groups=train_fam)):\n    print(\"Group {}: {}\".format(idx,np.unique(train_fam[test_index])))\n    fam_group[np.unique(train_fam[test_index])]=idx\n    print('-'*85)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"6 groups have been created, now it's time to add NEG samples to each group.\n\nIs it the best to have the equal number of POS and NEG samples? Or shall we have more portion for NEG samples, like 2:1? I will test other portions later."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get group ID for each image\n\ntrain_file_df['group']=train_file_df['fam'].apply(\n    lambda x: fam_group[x])\n\ntrain_file_df.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get group ID for each image pair\n\ntqdm_notebook.pandas()\ngroup_li=train_file_df['group'].tolist()\n\ntrain_pairs_all['group1']=train_pairs_all['p1'].progress_apply(lambda x: group_li[x])\ntrain_pairs_all['group2']=train_pairs_all['p2'].progress_apply(lambda x: group_li[x])\ntmp_li = (train_pairs_all['group1']==train_pairs_all['group2'])*(train_pairs_all['group1']+1)-1\ntrain_pairs_all.drop(columns=['group1','group2'],inplace=True)\ntrain_pairs_all['group']=tmp_li\ntmp_li=None # to free RAM\ntrain_pairs_all.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(12,5))\nsns.countplot(y='group',data=train_pairs_all,orient='v')\nplt.title('Number of image pair in each group')\nax.text(30000000,1, '\"-1\" means no group is assigned.',fontsize=16)\nax.text(25000000,2,\n        '{:.0f}% of image pairs have no group assigned.'.format(train_pairs_all.query('group == -1').shape[0]/len(train_pairs_all)*100),\n        fontsize=16)\nplt.box(on=None)\nplt.xticks([]);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop group==-1\ntrain_pairs_all = train_pairs_all[train_pairs_all['group']!=-1]\n# Shuffle\ntrain_pairs_all = train_pairs_all.sample(frac=1,random_state=2019)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset_df=train_fam_lim_df\ntrain_dataset_df['group']=train_dataset_df['fam'].apply(lambda x: fam_group[x])\ntrain_dataset_df.drop(columns=['index','fam'],inplace=True)\ntrain_dataset_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"group_num = train_fam_lim_df.groupby('group')['is_related'].count().tolist()\nportion=1 # get equal number of NEG / POS\ndf_temp = pd.concat(\n    [t.head(int(group_num[g]*portion)) for g, t in train_pairs_all.query('is_related == 0').groupby('group', sort=False, as_index=False)],\n    ignore_index=True)\n\ntrain_dataset_df=pd.concat([train_dataset_df,df_temp],ignore_index=True)\ntrain_dataset_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# to free RAM\ntrain_pairs_all=None\ntrain_pairs_kinship=None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(12,5))\nsns.countplot(y='group',data=train_dataset_df,orient='v',hue='is_related')\nplt.title('Number of image pair in each group')\nplt.box(on=None)\nplt.xticks([]);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Use VGGFace to calculate 2048 features"},{"metadata":{},"cell_type":"markdown","source":"Thanks : [VGGFace Baseline 197X197](https://www.kaggle.com/hsinwenchang/vggface-baseline-197x197) for the example of using VGGFace."},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install git+https://github.com/rcmalli/keras-vggface.git","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras_vggface.utils import preprocess_input\nfrom keras_vggface.vggface import VGGFace\nfrom keras.preprocessing import image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convolution Features\nvgg_features = VGGFace(include_top=False, input_shape=(224, 224, 3), pooling='avg', model='resnet50') # pooling: None, avg or max\n\n# After this point you can use your model to predict.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_images(filepaths,target_size=(224, 224)):\n    \n    aligned_images = []\n    for filepath in filepaths:\n        img = image.load_img(filepath, target_size=target_size)\n        x_ = image.img_to_array(img)\n        aligned_images.append(x_)\n            \n    return preprocess_input(np.array(aligned_images),version=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def calc_embs(filepaths,batch_size=512,target_size=(224, 224)):\n    pd = []\n    for start in tqdm_notebook(range(0, len(filepaths), batch_size)):\n        aligned_images = load_images(filepaths[start:start+batch_size],target_size=target_size)\n        pd.append(vgg_features.predict_on_batch(aligned_images))\n    embs = np.concatenate(pd)\n\n    return embs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# feature_size=128\nfeature_size=2048","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate embs for train images\n\ntrain_embs = calc_embs([os.path.join(\"../input/recognizing-faces-in-the-wild/train\", f) for f in train_file_df['image_fp']])\ntrain_file_df=pd.concat([train_file_df, pd.DataFrame(train_embs,columns=['fe'+str(i) for i in range(feature_size)])],axis=1)\ntrain_file_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_embs = None # to free RAM","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use absolute distance as final features\n\np1_df = train_dataset_df.merge(train_file_df, left_on='p1',right_on='index',how='left').iloc[:,8:]\np2_df = train_dataset_df.merge(train_file_df, left_on='p2',right_on='index',how='left').iloc[:,8:]\n\ntrain_dataset_df = pd.concat([train_dataset_df, abs(p1_df-p2_df)],axis=1)\np1_df=None\np2_df=None\ntrain_dataset_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# shuffle the dataset\ntrain_dataset_df=train_dataset_df.sample(frac=1,random_state=2019).reset_index(drop=True)\ntrain_dataset_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X=train_dataset_df.iloc[:,4:]\n# y=train_dataset_df.iloc[:,2]\n# X.shape, y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X=None\n# y=None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_train=X[train_dataset_df['group']!=5]\n# X_test=X[train_dataset_df['group']==5]\n# y_train=train_dataset_df['is_related'][train_dataset_df['group']!=5]\n# y_test=train_dataset_df['is_related'][train_dataset_df['group']==5]\n\n# y_train_group=train_dataset_df['group'][train_dataset_df['group']!=5]\n\n# X_train.shape,X_test.shape,y_train.shape,y_test.shape,y_train_group.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_train=train_dataset_df.query('group != 5').iloc[:,4:]\n# X_test=train_dataset_df.query('group == 5').iloc[:,4:]\n# y_train=train_dataset_df.query('group != 5')['is_related']\n# y_test=train_dataset_df.query('group == 5')['is_related']\n\n# y_train_group=train_dataset_df.query('group != 5')['group']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_train=None\n# X_test=None\n# y_train=None\n# y_test=None\n# y_train_group=None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# group kfolder\ngroup_kfold = GroupKFold(n_splits=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # this is a check of GroupKFold result\n\n# for train_index, test_index in group_kfold.split(X_train, y_train, y_train_group):\n#     #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n#     print(np.unique(y_train_group.as_matrix()[train_index]))\n#     print(np.unique(y_train_group.as_matrix()[test_index]))\n#     print('-'*20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # this is a check of GroupKFold result\n\n# for train_index, test_index in group_kfold.split(train_dataset_df.query('group != 5').iloc[:,4:], \n#                                                  train_dataset_df.query('group != 5')['is_related'], \n#                                                  train_dataset_df.query('group != 5')['group']):\n#     #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n#     print(np.unique(train_dataset_df.query('group != 5')['group'].as_matrix()[train_index]))\n#     print(np.unique(train_dataset_df.query('group != 5')['group'].as_matrix()[test_index]))\n#     print('-'*20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Only LogisticRegression model is tested. You can use other more advanced model, like lightgbm, to train on the same dataset. And use GridSearchCV to tweat super parameters."},{"metadata":{"trusted":true},"cell_type":"code","source":"model=LogisticRegression(random_state=2019)\nres=cross_validate(model,train_dataset_df.query('group != 5').iloc[:,4:],\n                   train_dataset_df.query('group != 5')['is_related'],\n                   cv=group_kfold,n_jobs=1,\n                   groups=train_dataset_df.query('group != 5')['group'],\n                   scoring=('accuracy', 'roc_auc'))\nprint(\"Mean ROC_AUC score: {:.4f} (std: {:.4f})\".format(res['test_roc_auc'].mean(),res['test_roc_auc'].std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from lightgbm import LGBMClassifier\n\nmodel = LGBMClassifier(random_state=2019,\n                       n_jobs=-1,\n                      n_estimators=1000,\n                      num_leaves=40,\n                      max_depth=12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model = LGBMClassifier(random_state=2019,n_jobs=1)\n# res=cross_validate(model,train_dataset_df.query('group != 5').iloc[:,4:],\n#                    train_dataset_df.query('group != 5')['is_related'],\n#                    cv=group_kfold,n_jobs=1,\n#                    groups=train_dataset_df.query('group != 5')['group'],\n#                    scoring=('accuracy', 'roc_auc'))\n# print(\"Mean ROC_AUC score: {:.4f} (std: {:.4f})\".format(res['test_roc_auc'].mean(),res['test_roc_auc'].std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test on Test Set\nmodel.fit(train_dataset_df.query('group != 5').iloc[:,4:],train_dataset_df.query('group != 5')['is_related'])\nprint(\"ROC_AUC socre on test set: {:.3f}\".format(roc_auc_score(train_dataset_df.query('group == 5')['is_related'],\n                                                               model.predict_proba(train_dataset_df.query('group == 5').iloc[:,4:])[:,1])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predict and Export result"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate embs for test images\ntest_images = os.listdir(\"../input/recognizing-faces-in-the-wild/test/\")\ntest_embs = calc_embs([os.path.join(\"../input/recognizing-faces-in-the-wild/test/\", f) for f in test_images])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img2idx = dict()\nfor idx, img in enumerate(test_images):\n    img2idx[img] = idx","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv(\"../input/recognizing-faces-in-the-wild/sample_submission.csv\")\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_np = []\nfor idx, row in tqdm_notebook(test_df.iterrows(), total=len(test_df)):\n    imgs = [test_embs[img2idx[img]] for img in row.img_pair.split(\"-\")]\n    test_np.append(abs(imgs[0]-imgs[1]))\ntest_np = np.array(test_np)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict\nmodel.fit(X,y)\nprobs = model.predict_proba(test_np)[:,1]\n\nsub_df = pd.read_csv(\"../input/recognizing-faces-in-the-wild/sample_submission.csv\")\nsub_df.is_related = probs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df.hist();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# use this function to show some image pairs.\ndef show2pic(fd,paire):\n    plt.figure(figsize=(7,10))\n    plt.subplot(121)\n    plt.imshow(imread(os.path.join(fd,paire.split('-')[0])))\n    plt.axis('off')\n    plt.title(paire.split('-')[0])\n    plt.subplot(122)\n    plt.imshow(imread(os.path.join(fd,paire.split('-')[1])))\n    plt.axis('off')\n    plt.title(paire.split('-')[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df.sort_values('is_related',ascending=False).head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# here is an example of the top 5th result.\n\nshow2pic('../input/recognizing-faces-in-the-wild/test/',sub_df.loc[4636,'img_pair'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# export result to csv file\nsub_df.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\n\n# These are the usual ipython objects, including this one you are creating\nipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n\n# Get a sorted list of the objects and their sizes\nsorted([(x, sys.getsizeof(globals().get(x))/1e6) for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}