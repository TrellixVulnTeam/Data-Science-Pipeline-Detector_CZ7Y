{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from collections import defaultdict\nfrom glob import glob\nfrom random import choice, sample\n\nimport cv2\nimport numpy as np\nimport pandas as pd\nfrom keras.applications.resnet50 import ResNet50, preprocess_input\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\nfrom keras.layers import Input, Dense, GlobalMaxPool2D, GlobalAvgPool2D, Concatenate, Multiply, Dropout, Subtract, BatchNormalization\nfrom keras.models import Model\nfrom keras.optimizers import adam","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_file_path = \"../input/train_relationships.csv\"\ntrain_folders_path = \"../input/train/\"\nval_famillies = \"F09\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_images = glob(train_folders_path + \"*/*/*.jpg\")\n\ntrain_images = [x for x in all_images if val_famillies not in x]\nval_images = [x for x in all_images if val_famillies in x]\n#train_images","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_person_to_images_map = defaultdict(list)\n\nppl = [x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2] for x in all_images]\n\nfor x in train_images:\n    train_person_to_images_map[x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2]].append(x)\n#train_person_to_images_map","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_person_to_images_map = defaultdict(list)\n\nfor x in val_images:\n    val_person_to_images_map[x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2]].append(x)\n#val_person_to_images_map","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"relationships = pd.read_csv(train_file_path)\nrelationships = list(zip(relationships.p1.values, relationships.p2.values))\nrelationships = [x for x in relationships if x[0] in ppl and x[1] in ppl]\n#relationships","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = [x for x in relationships if val_famillies not in x[0]]\nval = [x for x in relationships if val_famillies in x[0]]\n#train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_img(path):\n    img = cv2.imread(path)\n    img = cv2.resize(img,(197,197))\n    return preprocess_input(img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def gen(list_tuples, person_to_images_map, batch_size=16):\n    ppl = list(person_to_images_map.keys())\n    while True:\n        batch_tuples = sample(list_tuples, batch_size // 2)\n        labels = [1] * len(batch_tuples)\n        while len(batch_tuples) < batch_size:\n            p1 = choice(ppl)\n            p2 = choice(ppl)\n\n            if p1 != p2 and (p1, p2) not in list_tuples and (p2, p1) not in list_tuples:\n                batch_tuples.append((p1, p2))\n                labels.append(0)\n\n        for x in batch_tuples:\n            if not len(person_to_images_map[x[0]]):\n                print(x[0])\n\n        X1 = [choice(person_to_images_map[x[0]]) for x in batch_tuples]\n        X1 = np.array([read_img(x) for x in X1])\n\n        X2 = [choice(person_to_images_map[x[1]]) for x in batch_tuples]\n        X2 = np.array([read_img(x) for x in X2])\n\n        yield [X1, X2], labels\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install git+https://github.com/rcmalli/keras-vggface.git\n# Release Version\n!pip install keras_vggface","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.engine import  Model\nfrom keras.layers import Input\nfrom keras_vggface.vggface import VGGFace","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"def model1():\n    vgg = VGGFace(model='senet50',include_top=False,input_shape=(224, 224, 3))\n    out = vgg.get_layer('conv5_1_1x1_proj/bn').output\n    model = Model(vgg.input,out)\n    return model\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def baseline_model():\n    input_1 = Input(shape=(197, 197, 3))\n    input_2 = Input(shape=(197, 197, 3))\n\n    base_model = VGGFace(model='resnet50',include_top=False)\n\n    for x in base_model.layers[:-3]:\n        x.trainable = True\n\n    x1 = base_model(input_1)\n    x2 = base_model(input_2)\n    \n    # x1_ = Reshape(target_shape=(7*7, 2048))(x1)\n    # x2_ = Reshape(target_shape=(7*7, 2048))(x2)\n    #\n    # x_dot = Dot(axes=[2, 2], normalize=True)([x1_, x2_])\n    # x_dot = Flatten()(x_dot)\n\n    x1 = Concatenate(axis=-1)([GlobalMaxPool2D()(x1), GlobalAvgPool2D()(x1)])\n    x2 = Concatenate(axis=-1)([GlobalMaxPool2D()(x2), GlobalAvgPool2D()(x2)])\n\n    x3 = Subtract()([x1, x2])\n    x3 = Dropout(0.25)(x3)\n    x3 = Multiply()([x3, x3])\n\n    x = Multiply()([x1, x2])\n    x = Dropout(0.5)(x)\n    x = Concatenate(axis=-1)([x, x3])\n\n    x = Dense(128, activation=\"relu\")(x)\n    x = Dropout(0.2)(x)\n    x = Dense(64, activation=\"relu\")(x)\n    x = Dropout(0.01)(x)\n    out = Dense(1, activation=\"sigmoid\")(x)\n\n    model = Model([input_1, input_2], out)\n\n    model.compile(loss=\"binary_crossentropy\", metrics=['acc'], optimizer=adam(lr=0.00001))\n\n    model.summary()\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#VGGFace(model='vgg16',include_top=False).summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"file_path = \"baseline.h5\"\n\ncheckpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n\nreduce_on_plateau = ReduceLROnPlateau(monitor=\"val_acc\", mode=\"max\", factor=0.1, patience=10, verbose=1)\n\ncallbacks_list = [checkpoint, reduce_on_plateau]\n\nmodel = baseline_model()\n# model.load_weights(file_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit_generator(gen(train, train_person_to_images_map, batch_size=16), use_multiprocessing=True,\n                    validation_data=gen(val, val_person_to_images_map, batch_size=16), epochs=100, verbose=1,\n                    workers=4, callbacks=callbacks_list, steps_per_epoch=200, validation_steps=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_path = \"../input/test/\"\n\n\ndef chunker(seq, size=32):\n    return (seq[pos:pos + size] for pos in range(0, len(seq), size))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\n\nsubmission = pd.read_csv('../input/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = []\n\nfor batch in tqdm(chunker(submission.img_pair.values)):\n    X1 = [x.split(\"-\")[0] for x in batch]\n    X1 = np.array([read_img(test_path + x) for x in X1])\n\n    X2 = [x.split(\"-\")[1] for x in batch]\n    X2 = np.array([read_img(test_path + x) for x in X2])\n\n    pred = model.predict([X1, X2]).ravel().tolist()\n    predictions += pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission['is_related'] = predictions\n\nsubmission.to_csv(\"baseline.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}