{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"'''# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in sorted(os.walk('/kaggle/input')):\n    for filename in filenames:\n        #if filename.endswith('.jpg'):\n        #    break\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#There was not enough time to train this full model, but here is what was done.\n'''import torch\n\nprint(torch.cuda.current_device())\nprint(torch.cuda.device(0))\nprint(torch.cuda.device_count())\nprint(torch.cuda.get_device_name(0))\nprint(torch.cuda.is_available())\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np  \nimport pandas as pd\nimport torch\nimport os\nfrom torch import nn\nfrom torch import optim\n#import torch.nn.functional as F\nfrom torchvision import datasets, transforms, models","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''import cv2\nimport re\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom keras.preprocessing.image import img_to_array, image\nfrom keras.applications.resnet50 import preprocess_input, decode_predictions, resnet50\nfrom keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\nfrom keras.layers import Dropout, Flatten, Dense, Activation\nfrom keras.models import Sequential \nfrom sklearn.model_selection import train_test_split\nfrom keras import optimizers\nfrom keras.applications.vgg16 import VGG16\nfrom keras.applications.vgg19 import VGG19\nfrom keras.models import Model\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras import backend as K'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dir = '/kaggle/input/iwildcam-2020-fgvc7/test/'\ntrain_dir = '/kaggle/input/iwildcam-2020-fgvc7/train/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_filenames = []\ntest_filenames = []\ntrain_filepaths = []\ntest_filepaths = []\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        #print(dirname)\n        #print(filename)\n        #print(dirname==train_dir)\n        #print(dirname==test_dir)\n        #break\n        if dirname.endswith('train'):\n            train_filenames.append(filename[:-4])\n            #train_filepaths.append(os.path.join(dirname, filename))\n        if dirname.endswith('test'):\n            test_filenames.append(filename[:-4])\n            #test_filepaths.append(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_filenames\nlen(test_filenames)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''import os\nfrom PIL import Image \nfn_nopen_train = []\n#im = Image.open('/kaggle/input/iwildcam-2020-fgvc7/train/87022118-21bc-11ea-a13a-137349068a90.jpg')\n\nfor filename in train_filepaths:\n    try:\n        #path = r'/kaggle/input/iwildcam-2020-fgvc7/train/87022118-21bc-11ea-a13a-137349068a90.jpg'\n        Image.open(filename)\n    except:\n        print('didnt open ' + filename)\n        fn_nopen_train.append(filename)\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nfn_nopen_test = []\n#im = Image.open('/kaggle/input/iwildcam-2020-fgvc7/train/87022118-21bc-11ea-a13a-137349068a90.jpg')\n\nfor filename in test_filepaths:\n    try:\n        #path = r'/kaggle/input/iwildcam-2020-fgvc7/train/87022118-21bc-11ea-a13a-137349068a90.jpg'\n        Image.open(filename)\n    except:\n        print('didnt open ' + filename)\n        fn_nopen_test.append(filename)'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_filepaths \n#test_filepaths","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''fn_nopen_train = ['/kaggle/input/iwildcam-2020-fgvc7/train/883572ba-21bc-11ea-a13a-137349068a90.jpg',\n '/kaggle/input/iwildcam-2020-fgvc7/train/8792549a-21bc-11ea-a13a-137349068a90.jpg',\n '/kaggle/input/iwildcam-2020-fgvc7/train/99136aa6-21bc-11ea-a13a-137349068a90.jpg',\n '/kaggle/input/iwildcam-2020-fgvc7/train/8f17b296-21bc-11ea-a13a-137349068a90.jpg',\n '/kaggle/input/iwildcam-2020-fgvc7/train/896c1198-21bc-11ea-a13a-137349068a90.jpg',\n '/kaggle/input/iwildcam-2020-fgvc7/train/87022118-21bc-11ea-a13a-137349068a90.jpg']'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nopen_train = ['/kaggle/input/iwildcam-2020-fgvc7/train/883572ba-21bc-11ea-a13a-137349068a90.jpg',\n '/kaggle/input/iwildcam-2020-fgvc7/train/8792549a-21bc-11ea-a13a-137349068a90.jpg',\n '/kaggle/input/iwildcam-2020-fgvc7/train/99136aa6-21bc-11ea-a13a-137349068a90.jpg',\n '/kaggle/input/iwildcam-2020-fgvc7/train/8f17b296-21bc-11ea-a13a-137349068a90.jpg',\n '/kaggle/input/iwildcam-2020-fgvc7/train/896c1198-21bc-11ea-a13a-137349068a90.jpg',\n '/kaggle/input/iwildcam-2020-fgvc7/train/87022118-21bc-11ea-a13a-137349068a90.jpg']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nopen_train_id = [i[40:-4] for i in nopen_train]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nopen_train_id","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_filenames\n#len(train_filenames)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#len(test_filenames)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import json\n\nwith open('/kaggle/input/iwildcam-2020-fgvc7/iwildcam2020_train_annotations.json', 'r', errors='ignore') as f:\n    train_annotations = json.load(f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"samp_sub = pd.read_csv('/kaggle/input/iwildcam-2020-fgvc7/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"samp_sub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('/kaggle/input/iwildcam-2020-fgvc7/iwildcam2020_test_information.json', 'r', errors='ignore') as f:\n    test_information = json.load(f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#with open('/kaggle/input/iwildcam-2020-fgvc7/iwildcam2020_megadetector_results.json', 'r', errors='ignore') as f:\n#    megadetector_results = json.load(f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_annotations.keys()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test_information.keys()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#megadetector_results.keys()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_ann = pd.DataFrame(train_annotations['annotations'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_ann[train_ann['image_id'].isin(nopen_train_id)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#train_allopen = train_ann.drop(train_ann[train_ann['image_id'].isin(nopen_train_id)].index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_allopen","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_fn_allopen =   [x for x in train_filenames if x not in nopen_train_id]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#len(train_fn_allopen)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_ann_image_index = train_allopen.set_index('image_id')\ntrain_ann_image_index = train_ann.set_index('image_id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_ann_image_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_ann_filename_order = train_ann_image_index.loc[train_fn_allopen]\ntrain_ann_filename_order = train_ann_image_index.loc[train_filenames]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train_ann_filename_order.reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_y = train['category_id'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['category_id'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''def convert_to_tensor(path_img):\n    img = image.load_img(path_img, target_size = (224,224))\n    img_arr = image.img_to_array(img)\n    return np.expand_dims(img_arr, axis = 0)'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''def convert_all_tensor(paths_imgs):\n    tensor_list = [convert_to_tensor(i) for i in paths_imgs]\n    return np.vstack(tensor_list)'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_tensors = convert_all_tensor(train_filepaths).astype('float64')/255\n#test_tensors = convert_all_tensor(test_filepaths).astype('float64')/255","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''import os\nfrom PIL import Image \nfn_nopen_train = []\n#im = Image.open('/kaggle/input/iwildcam-2020-fgvc7/train/87022118-21bc-11ea-a13a-137349068a90.jpg')\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        try:\n            #path = r'/kaggle/input/iwildcam-2020-fgvc7/train/87022118-21bc-11ea-a13a-137349068a90.jpg'\n            Image.open(train_dir + filename)\n        except:\n            print('didnt open' + train_dir + filename)\n            fn_nopen_train.append(train_dir+ filename)'''\n            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#open(path, 'rb')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#pil_loader(path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(PILLOW_VERSION)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.utils.data as data\n\nfrom PIL import Image\n\nimport os.path\n\nIMG_EXTENSIONS = [\n    '.jpg', '.JPG', '.jpeg', '.JPEG',\n    '.png', '.PNG', '.ppm', '.PPM', '.bmp', '.BMP',\n]\n\n\ndef is_image_file(filename):\n    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n\n\ndef find_classes(dir):\n    classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))]\n    classes.sort()\n    class_to_idx = {classes[i]: i for i in range(len(classes))}\n    return classes, class_to_idx\n\n\ndef make_dataset(dir, class_to_idx):\n    images = []\n    dir = os.path.expanduser(dir)\n    for target in sorted(os.listdir(dir)):\n        #d = os.path.join(dir, target)\n        #if not os.path.isdir(d):\n        #    continue\n\n        #for root, _, fnames in sorted(os.walk(d)):\n         #   for fname in sorted(fnames):\n        if is_image_file(target):\n            path = os.path.join(dir, target)\n            item = (path,0) #, class_to_idx[target])\n            images.append(item)\n            #images.append(path) #avoid subfolders\n\n    return images\n\n\ndef pil_loader(path):\n    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n    with open(path, 'rb') as f:        \n        with Image.open(f) as img:\n            return img.convert('RGB')\n\n\ndef accimage_loader(path):\n    import accimage\n    try:\n        return accimage.Image(path)\n    except IOError:\n        # Potentially a decoding problem, fall back to PIL.Image\n        return pil_loader(path)\n\n\ndef default_loader(path):\n    from torchvision import get_image_backend\n    if get_image_backend() == 'accimage':\n        return accimage_loader(path)\n    else:\n        return pil_loader(path)\n\n\nclass ImageFolder(data.Dataset):\n    \"\"\"A generic data loader where the images are arranged in this way: ::\n\n        root/dog/xxx.png\n        root/dog/xxy.png\n        root/dog/xxz.png\n\n        root/cat/123.png\n        root/cat/nsdf3.png\n        root/cat/asd932_.png\n\n    Args:\n        root (string): Root directory path.\n        transform (callable, optional): A function/transform that  takes in an PIL image\n            and returns a transformed version. E.g, ``transforms.RandomCrop``\n        target_transform (callable, optional): A function/transform that takes in the\n            target and transforms it.\n        loader (callable, optional): A function to load an image given its path.\n\n     Attributes:\n        classes (list): List of the class names.\n        class_to_idx (dict): Dict with items (class_name, class_index).\n        imgs (list): List of (image path, class_index) tuples\n    \"\"\"\n\n    def __init__(self, root, transform=None, target_transform=None,\n                 loader=default_loader):\n        classes, class_to_idx = find_classes(root)\n        imgs = make_dataset(root, class_to_idx)\n        if len(imgs) == 0:\n            raise(RuntimeError(\"Found 0 images in subfolders of: \" + root + \"\\n\"\n                               \"Supported image extensions are: \" + \",\".join(IMG_EXTENSIONS)))\n\n        self.root = root\n        self.imgs = imgs\n        self.classes = classes\n        self.class_to_idx = class_to_idx\n        self.transform = transform\n        self.target_transform = target_transform\n        self.loader = loader\n\n\n    def __getitem__(self, index):\n        \"\"\"\n        Args:\n            index (int): Index\n\n        Returns:\n            tuple: (image, target) where target is class_index of the target class.\n        \"\"\"\n        path, target = self.imgs[index]\n        try:\n            img = self.loader(path)\n        except:\n            img = self.loader(train_dir+'8ccf922e-21bc-11ea-a13a-137349068a90.jpg')\n        if self.transform is not None:\n            img = self.transform(img)\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        return img, target #no targets given, so none returned\n\n\n    def __len__(self):\n        return len(self.imgs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#[d for d in os.listdir(train_dir) if os.path.isdir(os.path.join(train_dir, d))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''for target in sorted(os.listdir(train_dir)):\n    print(target)\n    break'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''for target in sorted(os.listdir(train_dir)):\n    d = os.path.join(train_dir, target)\n    if not os.path.isdir(d):\n        print('y')\n    print(d)\n    break'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''classes = [d for d in os.listdir('/kaggle/input/iwildcam-2020-fgvc7/') if os.path.isdir(os.path.join('/kaggle/input/iwildcam-2020-fgvc7/', d))]\nclasses.sort()\nclass_to_idx = {classes[i]: i for i in range(len(classes))}\nimages = []\ntd = os.path.expanduser('/kaggle/input/iwildcam-2020-fgvc7/')\nfor target in sorted(os.listdir(td)):\n    d = os.path.join(td, target)\n    if not os.path.isdir(d):\n        continue\n\n    for root, _, fnames in sorted(os.walk(d)):\n        for fname in sorted(fnames):\n            if is_image_file(fname):\n                path = os.path.join(root, fname)\n                item = (path, class_to_idx[target])\n                images.append(item)\n                #images.append(path)\n            break\n        break       \n    break\n\nprint(images)              '''  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''classes = [d for d in os.listdir('/kaggle/input/iwildcam-2020-fgvc7/') if os.path.isdir(os.path.join('/kaggle/input/iwildcam-2020-fgvc7/', d))]\nclasses.sort()\nclass_to_idx = {classes[i]: i for i in range(len(classes))}\nimages = []\ntd = os.path.expanduser('/kaggle/input/iwildcam-2020-fgvc7/')\nfor target in sorted(os.listdir(td)):\n    d = os.path.join(td, target)\n    if is_image_file(fname):\n        path = os.path.join(root, fname)\n        images.append(path)\n      \n    break\n\nprint(images)'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_torch_y = torch.tensor(train_y, dtype=torch.long ).view(2247, 97)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_torch_y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_cat = pd.DataFrame(train_annotations['categories'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_cat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_imgs = pd.DataFrame(train_annotations['images'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_imgs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_annotations['info']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test_information['info']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test_imgs = pd.DataFrame(test_information['images'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test_imgs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test_cat = pd.DataFrame(test_information['categories'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test_cat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test_cat['count'].sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#megadetector_cat = pd.Series(megadetector_results['detection_categories'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#megadetector_cat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#megadetector_imgs = pd.DataFrame(megadetector_results['images'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#megadetector_imgs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#megadetector_results['info']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''def collate_fn(batch):\n    batch = list(filter(lambda x: x is not None, batch))\n    return torch.utils.data.dataloader.default_collate(batch)'''\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_transforms_train = transforms.Compose([transforms.RandomResizedCrop(224),\n                                       transforms.ToTensor(),\n                                       transforms.Normalize([0.485, 0.456, 0.406], \n                                                            [0.229, 0.224, 0.225])])\n\n\n\nimage_datasets_train =  ImageFolder(train_dir, transform=data_transforms_train)\n\n\n\ndataloaders_train = torch.utils.data.DataLoader(image_datasets_train, batch_size=97)#,collate_fn=collate_fn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#next(iter(dataloaders_train))[0].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_transforms_test = transforms.Compose([transforms.Resize(256),\n                                      transforms.CenterCrop(224),\n                                      transforms.ToTensor(),\n                                      transforms.Normalize([0.485, 0.456, 0.406], \n                                                           [0.229, 0.224, 0.225])])\n\nimage_datasets_test = ImageFolder(test_dir, transform=data_transforms_test)\n\ndataloaders_test = torch.utils.data.DataLoader(image_datasets_test, batch_size=82)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel = models.vgg16_bn(pretrained=True)\n\nfor param in model.parameters():\n    param.requires_grad = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.classifier = nn.Sequential(nn.Linear(25088, 1024),\n                                 nn.ReLU(),\n                                 nn.Dropout(0.2),\n                                 nn.Linear(1024, 572),\n                                 nn.LogSoftmax(dim=1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"criterion = nn.NLLLoss()\noptimizer = optim.Adam(model.classifier.parameters(), lr=0.003)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 2\nsteps = 0\nrunning_loss = 0\nprint_every = 100\nfor epoch in range(epochs):\n    for inputs, labels in zip(dataloaders_train,train_torch_y):\n        steps += 1\n        inputs, labels = inputs[0].to(device), labels.to(device)\n\n        optimizer.zero_grad()\n\n        logps = model.forward(inputs)\n        loss = criterion(logps, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n\n        if steps % print_every == 0:\n\n            print(f\"Epoch {epoch+1}/{epochs}.. \"\n                  f\"Train loss: {running_loss/print_every:.3f}.. \")\n            running_loss = 0\n     \n            if steps % 200 == 0:\n                break #to get out of loop before memory is full","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.eval()\ntop_c = []\ntop_probs = []\nwith torch.no_grad():\n    for inputs, labels in dataloaders_test:\n        inputs  = inputs.to(device) \n        logps = model.forward(inputs)\n                           \n        ps = torch.exp(logps)\n        top_p, top_class = ps.topk(1, dim=1)\n        top_c.append(top_class)\n        #top_probs.append(top_p)\nmodel.train()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#top_c","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topc_list = [torch.Tensor.cpu(i).numpy().tolist() for i in top_c]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from functools import reduce\ntopc_list1 = reduce(lambda x,y: x+y, topc_list)\ntopc_list2 = reduce(lambda x,y: x+y, topc_list1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(topc_list2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(topc_list2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Image.open(train_dir+'86760c00-21bc-11ea-a13a-137349068a90.jpg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topc_ser = pd.Series(topc_list2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#topc_ser","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = samp_sub.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_image_index = sub.set_index('Id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sub_image_index.index.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_fn_sub = [i for i in test_filenames if i in sub_image_index.index.values]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"not_in_sub = [i for i in test_filenames if i not in sub_image_index.index.values]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(not_in_sub)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"extra_index = [test_filenames.index(i) for i in not_in_sub]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topc_ser_drop_extra = topc_ser.drop(extra_index).reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#topc_ser_drop_extra","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(test_fn_sub)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_filename_order = sub_image_index.loc[test_fn_sub]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sub_filename_order","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_filename_order['Category'] = topc_ser_drop_extra[0].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#topc_ser_drop_extra[0].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_filename_order_cat = sub_filename_order.reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sub_filename_order_cat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_filename_order_cat.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}