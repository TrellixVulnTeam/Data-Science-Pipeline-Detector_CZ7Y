{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#Import Statements\n\nimport pandas as pd\nimport numpy as np\n\nimport tensorflow as tf\nprint(tf.__version__)\n\nimport re\nfrom tqdm import tqdm\n\nfrom scipy.stats import spearmanr\n\nimport warnings\nwarnings.simplefilter('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **READING THE GIVEN DATASETS**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"PATH = '../input/google-quest-challenge/'\nPATH_w2vec_300d = '../input/glove-300d/'\n\ndf_train = pd.read_csv(PATH+'train.csv')\ndf_test = pd.read_csv(PATH+'test.csv')\ndf_sub = pd.read_csv(PATH+'sample_submission.csv')\nprint('Train Shape =', df_train.shape)\nprint('Test Shape =', df_test.shape)\n\noutput_categories = list(df_train.columns[11:])\ninput_categories = list(df_train.columns[[1,2,5]])\nprint('\\nOutput Categories:\\n\\t', output_categories)\nprint('\\nInput Categories:\\n\\t', input_categories)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **PREPROCESSING**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Preprocessing Text Data\n\nstopwords= ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n            'won', \"won't\", 'wouldn', \"wouldn't\"]\n\n#Utility Methods\ndef decontracted(phrase): # https://stackoverflow.com/a/47091490/4084039\n    # specific\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase\n\ndef preprocess_text(text_data):\n    preprocessed_text = []\n    # tqdm is for printing the status bar\n    for sentance in tqdm(text_data):\n        sent = decontracted(sentance)\n        sent = sent.replace('\\\\r', ' ')\n        sent = sent.replace('\\\\n', ' ')\n        sent = sent.replace('\\\\\"', ' ')\n        sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n        # https://gist.github.com/sebleier/554280\n        sent = ' '.join(e for e in sent.split() if e.lower() not in stopwords)\n        preprocessed_text.append(sent.lower().strip())\n    return preprocessed_text\n\ndef perform_preprocessing(text_array):\n    #Changing all characters to lower case\n    lower_text_array = pd.Series(text_array).str.lower()\n    #Calling utility method preprocess_text to perform some more preprocessing\n    preprocessed_text_array = preprocess_text(lower_text_array)\n\n    return pd.Series(preprocessed_text_array)\n\n\n#Preprocessing Train Input Columns\ndf_train['Preproc_Question_Title'] = perform_preprocessing(df_train['question_title'].values)\ndf_train['Preproc_Question_Body'] = perform_preprocessing(df_train['question_body'].values)\ndf_train['Preproc_Answer'] = perform_preprocessing(df_train['answer'].values)\n  \n#Preprocessing Test Input Columns\ndf_test['Preproc_Question_Title'] = perform_preprocessing(df_test['question_title'].values)\ndf_test['Preproc_Question_Body'] = perform_preprocessing(df_test['question_body'].values)\ndf_test['Preproc_Answer'] = perform_preprocessing(df_test['answer'].values)\n\nprint(\"\\n\")\nprint(\"=\"*70 + \"Question Title\" + \"=\"*70)\nprint(\"Before Preprocessing:\\n\", df_train['question_title'][0])\nprint(\"\\nAfter Preprocessing:\\n\", df_train['Preproc_Question_Title'][0])\n\nprint(\"=\"*70 + \"Question Body\" + \"=\"*70)\nprint(\"Before Preprocessing:\\n\", df_train['question_body'][0])\nprint(\"\\nAfter Preprocessing:\\n\", df_train['Preproc_Question_Body'][0])\n\nprint(\"=\"*70 + \"Answer\" + \"=\"*70)\nprint(\"Before Preprocessing:\\n\", df_train['answer'][0])\nprint(\"\\nAfter Preprocessing:\\n\", df_train['Preproc_Answer'][0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ****Architecture - 1****\n\n<img src='https://i.imgur.com/7oTktOf.png'>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Prepare Embedding for 3 inputs together\n\ndef prepare_embedding(input_series_train, input_series_test, column_name):\n\n    print(\"=\"*70 + column_name + \"=\"*70)\n\n    #Tokenization of text data to numbers\n    tokenizer_obj = tf.keras.preprocessing.text.Tokenizer()\n    tokenizer_obj.fit_on_texts(input_series_train.values)\n\n    word_index = tokenizer_obj.word_index\n    print('Found %s unique tokens.' % len(word_index))\n\n    #Encoded inputs\n    train_sequences = tokenizer_obj.texts_to_sequences(input_series_train.values)\n    test_sequences = tokenizer_obj.texts_to_sequences(input_series_test.values)\n    print(\"Train Sequences Length\", len(train_sequences))\n    print(\"Test Sequences Length\", len(test_sequences))\n\n    #Selecting max_length of words in an essay\n    MAX_SEQUENCE_LENGTH = int(np.percentile(pd.Series(train_sequences).apply(lambda x: len(x)), 98))\n    print(\"Around 96 percentile of \" + column_name + \" have length of words less than \", MAX_SEQUENCE_LENGTH)\n\n    #Padding of Word Sequences\n    vocab_size = len(word_index)+1\n    train_sequences_pad = tf.keras.preprocessing.sequence.pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n    test_sequences_pad = tf.keras.preprocessing.sequence.pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n    print(\"Shape of padded train sequences: \", train_sequences_pad.shape)\n    print(\"Shape of padded test sequences: \", test_sequences_pad.shape)\n\n    #Preparing Embedding Layer using Glove vector (300 dimension)\n\n    # Loading Glove embedding layer\n    embeddings_index = {}\n    f = open(PATH_w2vec_300d+'glove-840B-300d-char_embed.txt')\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\n    f.close()\n\n    print('Found %s word vectors.' % len(embeddings_index))\n\n    #*--*create a weight matrix for words in training docs*--*\n    embedding_matrix = np.zeros((vocab_size, 300))\n    for word, i in word_index.items():\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n\n    return vocab_size, embedding_matrix, MAX_SEQUENCE_LENGTH, train_sequences_pad, test_sequences_pad\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Merge all the text columns into 1 as 'Total_Text'\n\ndf_train['Total_Text'] = df_train[\"Preproc_Question_Title\"].map(str) + df_train[\"Preproc_Question_Body\"].map(str) + df_train['Preproc_Answer'].map(str)\ndf_test['Total_Text'] = df_test[\"Preproc_Question_Title\"].map(str) + df_test[\"Preproc_Question_Body\"].map(str) + df_test['Preproc_Answer'].map(str)\n\n#Calling prepare_embedding method to generate embedding for 3 inputs for both train and test\nvocab_size, embedding_matrix, MAX_SEQUENCE_LENGTH, _, train_sequences_pad_qt = \\\nprepare_embedding(df_train['Total_Text'], df_train['question_title'], 'Question Title Train')\nvocab_size, embedding_matrix, MAX_SEQUENCE_LENGTH, _, test_sequences_pad_qt = \\\nprepare_embedding(df_train['Total_Text'], df_test['question_title'], 'Question Title Test')\n\nvocab_size, embedding_matrix, MAX_SEQUENCE_LENGTH, _, train_sequences_pad_qb = \\\nprepare_embedding(df_train['Total_Text'], df_train['question_body'], 'Question Body Train')\nvocab_size, embedding_matrix, MAX_SEQUENCE_LENGTH, _, test_sequences_pad_qb = \\\nprepare_embedding(df_train['Total_Text'], df_test['question_body'], 'Question Body Test')\n\nvocab_size, embedding_matrix, MAX_SEQUENCE_LENGTH, _, train_sequences_pad_ans = \\\nprepare_embedding(df_train['Total_Text'], df_train['answer'], 'Answer Train')\nvocab_size, embedding_matrix, MAX_SEQUENCE_LENGTH, _, test_sequences_pad_ans = \\\nprepare_embedding(df_train['Total_Text'], df_test['answer'], 'Answer Test')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observation:**\n\n*1) Here we are training our embedding layer on the entire corpus of text data (i.e. concat of Question Title, Question Body and Answer) and then extracting embedding for train and test inputs separately.*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Splitting into train and validation\nvalidation_sequences_pad_qt = train_sequences_pad_qt[5000:]\ntrain_sequences_pad_qt = train_sequences_pad_qt[:5000]\n\nvalidation_sequences_pad_qb = train_sequences_pad_qb[5000:]\ntrain_sequences_pad_qb = train_sequences_pad_qb[:5000]\n\nvalidation_sequences_pad_ans = train_sequences_pad_ans[5000:]\ntrain_sequences_pad_ans = train_sequences_pad_ans[:5000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating Embedding Layers for Total_Text:\n\nembedding_layer_total_text = tf.keras.layers.Embedding(vocab_size,\n                                            300,\n                                            weights=[embedding_matrix],\n                                            input_length=MAX_SEQUENCE_LENGTH,\n                                            name = 'Shared_Embedding_Layer',\n                                            trainable=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model(): \n\n    #Path 1 \n    input_question_title = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,), name = 'IP_Question_Title')\n    embedded_question_title = embedding_layer_total_text(input_question_title)\n    LSTM_layer_question_title = tf.keras.layers.LSTM(100, kernel_initializer='glorot_uniform', name = 'Question_Title_LSTM')(embedded_question_title) #32\n    flatten = tf.keras.layers.Flatten(name='flatten_Question_Title')(LSTM_layer_question_title)\n\n    #Path 2\n    input_question_body = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,), name = 'IP_Question_Body')\n    embedded_question_body = embedding_layer_total_text(input_question_body)\n    LSTM_layer_question_body = tf.keras.layers.LSTM(100, kernel_initializer='glorot_uniform', name = 'Question_Body_LSTM')(embedded_question_body) #32\n    flatten_1 = tf.keras.layers.Flatten(name='flatten_Question_Body')(LSTM_layer_question_body)\n\n    #Path 3\n    input_answer = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,), name = 'IP_Answer')\n    embedded_answer = embedding_layer_total_text(input_answer)\n    LSTM_layer_answer = tf.keras.layers.LSTM(100, kernel_initializer='glorot_uniform', name = 'Answer_LSTM')(embedded_answer) #32\n    flatten_2 = tf.keras.layers.Flatten(name='flatten_Answer')(LSTM_layer_answer)\n\n    #Concat\n    concat = tf.keras.layers.concatenate([flatten, flatten_1, flatten_2], axis=1, name='concatenate')\n\n    #Dense & Dropout - 1\n    dense_1 = tf.keras.layers.Dense(64, activation='relu', name='Dense_1')(concat) #64 #tanh\n    dropout_1 = tf.keras.layers.Dropout(0.5, name='Dropout_1')(dense_1) #Taking Dropout Rate = 0.5\n\n    #Dense & Dropout - 2\n    dense_2 = tf.keras.layers.Dense(32, activation='relu', name='Dense_2')(dropout_1) #64 #tanh\n    dropout_2 = tf.keras.layers.Dropout(0.5, name='Dropout_2')(dense_2) #Taking Dropout Rate = 0.5\n\n    # -----------------------------------------------------TRY--------------------------------------------------------------\n    #Dense & Dropout - 3\n    dense_3 = tf.keras.layers.Dense(64, activation='relu', name='Dense_3')(dropout_2) #64 #tanh\n    dropout_3 = tf.keras.layers.Dropout(0.2, name='Dropout_3')(dense_3) #Taking Dropout Rate = 0.5\n    # -----------------------------------------------------TRY--------------------------------------------------------------\n\n    #Output Layer\n    dense_4 = tf.keras.layers.Dense(4, activation='relu', name='Dense_4')(dropout_3) #64 #tanh\n    preds = tf.keras.layers.Dense(30, activation='sigmoid', name='Output')(dense_4)\n\n    model_created = tf.keras.models.Model(inputs = [input_question_title, input_question_body, input_answer], outputs = [preds], name='Model_Google_QUEST')\n\n    return model_created\n\n#Calling create_model method and printing summary of model\nmodel_Google_QUEST = create_model()\nprint(model_Google_QUEST.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting Architecture_1 of Google QUEST:\ntf.keras.utils.plot_model(model_Google_QUEST, to_file='Arch1_v2.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Metrics and Callbacks  \n\ndef compute_spearmanr(trues, preds):\n    rhos = []\n    for col_trues, col_pred in zip(trues.T, preds.T):\n        rhos.append(\n            spearmanr(col_trues, col_pred + np.random.normal(0, 1e-7, col_pred.shape[0])).correlation)\n    return np.nanmean(rhos)\n\n\nclass CustomCallback(tf.keras.callbacks.Callback):\n    \n    def on_train_begin(self, logs={}):\n        self.train_data = {'IP_Question_Title': train_sequences_pad_qt, 'IP_Question_Body': train_sequences_pad_qb, 'IP_Answer': train_sequences_pad_ans}\n        self.train_target = df_train[output_categories].values[:5000]\n\n        self.validation_data = {'IP_Question_Title': validation_sequences_pad_qt, 'IP_Question_Body': validation_sequences_pad_qb, 'IP_Answer': validation_sequences_pad_ans}\n        self.validation_target = df_train[output_categories].values[5000:]\n\n        self.valid_predictions = []\n        self.test_predictions = []\n        \n    def on_epoch_end(self, epoch, logs={}):\n        self.valid_predictions.append(\n            self.model.predict(self.validation_data))\n        \n        rho_val = compute_spearmanr(\n            self.validation_target, np.average(self.valid_predictions, axis=0))\n        \n        print(\"\\nvalidation rho: %.4f\" % rho_val)\n        \n        # if self.fold is not None:\n        #     self.model.save_weights(f'bert-base-{fold}-{epoch}.h5py')\n        \n        # self.test_predictions.append(\n        #     self.model.predict(self.test_inputs, batch_size=self.batch_size)\n\ncustom_callback = CustomCallback()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Compile and fit Model:\n\ntrain_data = {'IP_Question_Title': train_sequences_pad_qt, 'IP_Question_Body': train_sequences_pad_qb, 'IP_Answer': train_sequences_pad_ans}\ntrain_target = df_train[output_categories].values[:5000]\n\ntest_data = {'IP_Question_Title': validation_sequences_pad_qt, 'IP_Question_Body': validation_sequences_pad_qb, 'IP_Answer': validation_sequences_pad_ans}\ntest_target = df_train[output_categories].values[5000:]\n\noptimizer_adam = tf.keras.optimizers.Adam(learning_rate=0.01)\nmodel_Google_QUEST.compile(loss='mean_squared_error', optimizer=optimizer_adam)\nmodel_Google_QUEST.fit(train_data, train_target, validation_data = (test_data, test_target),\n           epochs=100, batch_size=64, verbose=1, callbacks=[custom_callback])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Prepare submission file:\ntest_prediction = model_Google_QUEST.predict({'IP_Question_Title': test_sequences_pad_qt, 'IP_Question_Body': test_sequences_pad_qb, 'IP_Answer': test_sequences_pad_ans})\nsubmission_df = pd.concat([pd.DataFrame(df_test['qa_id']), pd.DataFrame(test_prediction, columns=output_categories)], axis=1)\nsubmission_df.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}