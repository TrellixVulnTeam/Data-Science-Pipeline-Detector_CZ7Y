{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"#Import Statements\nimport pandas as pd\nimport numpy as np\nimport gc\nimport os\nfrom math import floor, ceil\nfrom tqdm.notebook import tqdm\n\nfrom sklearn.model_selection import GroupKFold\nfrom scipy.stats import spearmanr\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport bert_tokenization as tokenization\nimport tensorflow.keras.backend as K","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#*======================================================Read data and tokenizer========================================================*\n\n#Read tokenizer and data, as well as defining the maximum sequence length that will be used for the input to Bert (maximum is usually 512 tokens)\n\nPATH = '../input/google-quest-challenge/'\nBERT_PATH = '../input/bert-base-from-tfhub/bert_en_uncased_L-12_H-768_A-12'\ntokenizer = tokenization.FullTokenizer(BERT_PATH+'/assets/vocab.txt', True)\nMAX_SEQUENCE_LENGTH = 512\n\ndf_train = pd.read_csv(PATH+'train.csv')\ndf_test = pd.read_csv(PATH+'test.csv')\ndf_sub = pd.read_csv(PATH+'sample_submission.csv')\nprint('Train Shape =', df_train.shape)\nprint('Test Shape =', df_test.shape)\n\noutput_categories = list(df_train.columns[11:])\ninput_categories = list(df_train.columns[[1,2,5]])\nprint('\\nOutput Categories:\\n\\t', output_categories)\nprint('\\nInput Categories:\\n\\t', input_categories)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observations**\n\n*1) Our train dataset consists of 6079 rows and test dataset consists of 476 rows.*\n\n*2) We have total 30 output labels and 3 input columns i.e. Question Title, Question Body and Answer.*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#*======================================================Preprocessing Functions========================================================*\n\n# These are some functions that will be used to preprocess the raw text data into useable Bert inputs.\n\ndef _get_masks(tokens, max_seq_length):\n    \"\"\"Mask for padding\"\"\"\n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n\ndef _get_segments(tokens, max_seq_length):\n    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    segments = []\n    first_sep = True\n    current_segment_id = 0\n    for token in tokens:\n        segments.append(current_segment_id)\n        if token == \"[SEP]\":\n            if first_sep:\n                first_sep = False \n            else:\n                current_segment_id = 1\n    return segments + [0] * (max_seq_length - len(tokens))\n\ndef _get_ids(tokens, tokenizer, max_seq_length):\n    \"\"\"Token ids from Tokenizer vocab\"\"\"\n    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n    return input_ids\n\ndef _trim_input(title, question, answer, max_sequence_length, \n                t_max_len=30, q_max_len=239, a_max_len=239):\n\n    t = tokenizer.tokenize(title)\n    q = tokenizer.tokenize(question)\n    a = tokenizer.tokenize(answer)\n    \n    t_len = len(t)\n    q_len = len(q)\n    a_len = len(a)\n\n    if (t_len+q_len+a_len+4) > max_sequence_length:\n        \n        if t_max_len > t_len:\n            t_new_len = t_len\n            a_max_len = a_max_len + floor((t_max_len - t_len)/2)\n            q_max_len = q_max_len + ceil((t_max_len - t_len)/2)\n        else:\n            t_new_len = t_max_len\n      \n        if a_max_len > a_len:\n            a_new_len = a_len \n            q_new_len = q_max_len + (a_max_len - a_len)\n        elif q_max_len > q_len:\n            a_new_len = a_max_len + (q_max_len - q_len)\n            q_new_len = q_len\n        else:\n            a_new_len = a_max_len\n            q_new_len = q_max_len\n            \n            \n        if t_new_len+a_new_len+q_new_len+4 != max_sequence_length:\n            raise ValueError(\"New sequence length should be %d, but is %d\" \n                             % (max_sequence_length, (t_new_len+a_new_len+q_new_len+4)))\n        \n        t = t[:t_new_len]\n        q = q[:q_new_len]\n        a = a[:a_new_len]\n    \n    return t, q, a\n\ndef _convert_to_bert_inputs(title, question, answer, tokenizer, max_sequence_length):\n    \"\"\"Converts tokenized input to ids, masks and segments for BERT\"\"\"\n    \n    stoken = [\"[CLS]\"] + title + [\"[SEP]\"] + question + [\"[SEP]\"] + answer + [\"[SEP]\"]\n\n    input_ids = _get_ids(stoken, tokenizer, max_sequence_length)\n    input_masks = _get_masks(stoken, max_sequence_length)\n    input_segments = _get_segments(stoken, max_sequence_length)\n\n    return [input_ids, input_masks, input_segments]\n\ndef compute_input_arays(df, columns, tokenizer, max_sequence_length):\n    input_ids, input_masks, input_segments = [], [], []\n    for _, instance in tqdm(df[columns].iterrows()):\n        t, q, a = instance.question_title, instance.question_body, instance.answer\n\n        t, q, a = _trim_input(t, q, a, max_sequence_length)\n\n        ids, masks, segments = _convert_to_bert_inputs(t, q, a, tokenizer, max_sequence_length)\n        input_ids.append(ids)\n        input_masks.append(masks)\n        input_segments.append(segments)\n        \n    return [np.asarray(input_ids, dtype=np.int32), \n            np.asarray(input_masks, dtype=np.int32), \n            np.asarray(input_segments, dtype=np.int32)]\n\n\ndef compute_output_arrays(df, columns):\n    return np.asarray(df[columns])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observations:**\n\n*1) _get_masks method returns a array containing 1 for all trainable tokens and 0 for extra tokens to make up the Max Sequence Length.*\n\n*2) _get_segments method will populate 0 for question title and question body tokens and treat it as one segment. Whereas for answers tokens it will populate 1, treating it as separate segment.*\n\n*3) _get_ids method is used to give token ids for each word in question title, body, answer and 0 for padding tokens.*\n\n*4) _trim_input method is use to adjust length of question title, body and answer sequence so that in all cases summation of length of all these tokens is equal to Max Sequence Length always.*\n\n*5) _convert_to_bert_inputs method calls above explained utility mehtod and also forms final sequence by attaching [SEP] and [CLS] tokens.*\n\n*6) compute_input_arays method is the main method to convert input data into input token, mask and segment array for input to BERT model.*\n\n*7) compute_output_arrays method converts training output label columns and it's data compatible for training.*\n\n\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#*======================================================Metric & Callbacks========================================================*\n\n\ndef compute_spearmanr(trues, preds):\n    rhos = []\n    for col_trues, col_pred in zip(trues.T, preds.T):\n        rhos.append(\n            spearmanr(col_trues, col_pred + np.random.normal(0, 1e-7, col_pred.shape[0])).correlation)\n    return np.mean(rhos)\n\n\nclass CustomCallback(tf.keras.callbacks.Callback):\n    \n    def __init__(self, valid_data, test_data, batch_size=16, fold=None):\n\n        self.valid_inputs = valid_data[0]\n        self.valid_outputs = valid_data[1]\n        self.test_inputs = test_data\n        \n        self.batch_size = batch_size\n        self.fold = fold\n        \n    def on_train_begin(self, logs={}):\n        self.valid_predictions = []\n        self.test_predictions = []\n        \n    def on_epoch_end(self, epoch, logs={}):\n        self.valid_predictions.append(\n            self.model.predict(self.valid_inputs, batch_size=self.batch_size))\n        \n        rho_val = compute_spearmanr(\n            self.valid_outputs, np.average(self.valid_predictions, axis=0))\n        \n        print(\"\\nvalidation rho: %.4f\" % rho_val)\n        \n        if self.fold is not None:\n            self.model.save_weights(f'bert-base-{fold}-{epoch}.h5py')\n        \n        self.test_predictions.append(\n            self.model.predict(self.test_inputs, batch_size=self.batch_size)\n        )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observations:**\n\n*1) compute_spearmanr is a custom method to monitor competition metric i.e. Spearman Rank Correlation Coefficient.*\n\n*2) CustomCallback class saves model weights for each fold and also computes Spearman metric over validation data.*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#*======================================================Model Architecture========================================================*\n\n\ndef bert_model():\n    \n    input_word_ids = tf.keras.layers.Input(\n        (MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_word_ids')\n    input_masks = tf.keras.layers.Input(\n        (MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_masks')\n    input_segments = tf.keras.layers.Input(\n        (MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_segments')\n    \n    bert_layer = hub.KerasLayer(BERT_PATH, trainable=True, name = 'BERT_Layer')\n    \n    _, sequence_output = bert_layer([input_word_ids, input_masks, input_segments])\n    \n    x = tf.keras.layers.GlobalAveragePooling1D()(sequence_output)\n    x = tf.keras.layers.Dropout(0.2)(x)\n    out = tf.keras.layers.Dense(30, activation=\"sigmoid\", name=\"dense_output\")(x)\n\n    model = tf.keras.models.Model(\n        inputs=[input_word_ids, input_masks, input_segments], outputs=out)\n    \n    return model ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = bert_model()\ntf.keras.utils.plot_model(model, to_file='Google_QUEST_Model_Arch.png')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observations:**\n\n*1) BERT takes 3 inputs i.e. ids, masks and segments array.*\n\n*2) Here we are using sequence output from the BERT model which is further given to simple NN we have defined above.*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#*======================================================Divide Train Data into Folds========================================================*\n\ndef train_and_predict(model, train_data, valid_data, test_data, \n                      learning_rate, epochs, batch_size, loss_function, fold):\n        \n    custom_callback = CustomCallback(\n        valid_data=(valid_data[0], valid_data[1]), \n        test_data=test_data,\n        batch_size=batch_size,\n        fold=None)\n\n    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n    model.compile(loss=loss_function, optimizer=optimizer)\n    model.fit(train_data[0], train_data[1], epochs=epochs, \n              batch_size=batch_size, callbacks=[custom_callback])\n    \n    return custom_callback\n\n\n#Obtain inputs and targets, as well as the indices of the train/validation splits\n\ngkf = GroupKFold(n_splits=10).split(X=df_train.question_body, groups=df_train.question_body)\n\noutputs = compute_output_arrays(df_train, output_categories)\ninputs = compute_input_arays(df_train, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)\ntest_inputs = compute_input_arays(df_test, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observations:**\n\n*1) train_and_predict method is used to train on several folds and predict on validation data.*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#*======================================================Training, Validation and Testing========================================================*\n\n# Loops over the folds in gkf and trains each fold for 5 epochs with a learning rate of 1e-5 and batch_size of 8.\n\nhistories = []\nfor fold, (train_idx, valid_idx) in enumerate(gkf):\n    \n    #Doing it only for 3 folds to keep training time less\n    if fold < 3:\n        K.clear_session()\n        model = bert_model()\n\n        train_inputs = [inputs[i][train_idx] for i in range(3)]\n        train_outputs = outputs[train_idx]\n\n        valid_inputs = [inputs[i][valid_idx] for i in range(3)]\n        valid_outputs = outputs[valid_idx]\n\n        # history contains two lists of valid and test preds respectively:\n        history = train_and_predict(model, \n                          train_data=(train_inputs, train_outputs), \n                          valid_data=(valid_inputs, valid_outputs),\n                          test_data=test_inputs, \n                          learning_rate=3e-5, epochs=5, batch_size=8,\n                          loss_function='binary_crossentropy', fold=fold)\n\n        histories.append(history)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#*==============================================Post Processing and Submission Generation========================================================*\n\n# First the test predictions are read from the list of lists of `histories`. Then each test prediction list (in lists) is averaged. Then a mean of the averages is computed to get a single prediction for each data point. Finally, this is saved to `submission.csv`\n\ntest_predictions = [histories[i].test_predictions for i in range(len(histories))]\ntest_predictions = [np.average(test_predictions[i], axis=0) for i in range(len(test_predictions))]\ntest_predictions = np.mean(test_predictions, axis=0)\n\ndf_sub.iloc[:, 1:] = test_predictions\n\ndf_sub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}