{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#Import Statements\n\nimport pandas as pd\nimport numpy as np\n\nimport tensorflow as tf\nprint(tf.__version__)\n\nimport re\nfrom tqdm import tqdm\n\nfrom scipy.stats import spearmanr\n\nimport warnings\nwarnings.simplefilter('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **READING THE GIVEN DATASETS**","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"PATH = '../input/google-quest-challenge/'\nPATH_w2vec_300d = '../input/glove-300d/'\n\ndf_train = pd.read_csv(PATH+'train.csv')\ndf_test = pd.read_csv(PATH+'test.csv')\ndf_sub = pd.read_csv(PATH+'sample_submission.csv')\nprint('Train Shape =', df_train.shape)\nprint('Test Shape =', df_test.shape)\n\noutput_categories = list(df_train.columns[11:])\ninput_categories = list(df_train.columns[[1,2,5]])\nprint('\\nOutput Categories:\\n\\t', output_categories)\nprint('\\nInput Categories:\\n\\t', input_categories)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# PREPROCESSING","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Preprocessing Text Data\n\nstopwords= ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n            'won', \"won't\", 'wouldn', \"wouldn't\"]\n\n#Utility Methods\ndef decontracted(phrase): # https://stackoverflow.com/a/47091490/4084039\n    # specific\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase\n\ndef preprocess_text(text_data):\n    preprocessed_text = []\n    # tqdm is for printing the status bar\n    for sentance in tqdm(text_data):\n        sent = decontracted(sentance)\n        sent = sent.replace('\\\\r', ' ')\n        sent = sent.replace('\\\\n', ' ')\n        sent = sent.replace('\\\\\"', ' ')\n        sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n        # https://gist.github.com/sebleier/554280\n        sent = ' '.join(e for e in sent.split() if e.lower() not in stopwords)\n        preprocessed_text.append(sent.lower().strip())\n    return preprocessed_text\n\ndef perform_preprocessing(text_array):\n    #Changing all characters to lower case\n    lower_text_array = pd.Series(text_array).str.lower()\n    #Calling utility method preprocess_text to perform some more preprocessing\n    preprocessed_text_array = preprocess_text(lower_text_array)\n\n    return pd.Series(preprocessed_text_array)\n\n\n#Preprocessing Train Input Columns\ndf_train['Preproc_Question_Title'] = perform_preprocessing(df_train['question_title'].values)\ndf_train['Preproc_Question_Body'] = perform_preprocessing(df_train['question_body'].values)\ndf_train['Preproc_Answer'] = perform_preprocessing(df_train['answer'].values)\n  \n#Preprocessing Test Input Columns\ndf_test['Preproc_Question_Title'] = perform_preprocessing(df_test['question_title'].values)\ndf_test['Preproc_Question_Body'] = perform_preprocessing(df_test['question_body'].values)\ndf_test['Preproc_Answer'] = perform_preprocessing(df_test['answer'].values)\n\nprint(\"\\n\")\nprint(\"=\"*70 + \"Question Title\" + \"=\"*70)\nprint(\"Before Preprocessing:\\n\", df_train['question_title'][0])\nprint(\"\\nAfter Preprocessing:\\n\", df_train['Preproc_Question_Title'][0])\n\nprint(\"=\"*70 + \"Question Body\" + \"=\"*70)\nprint(\"Before Preprocessing:\\n\", df_train['question_body'][0])\nprint(\"\\nAfter Preprocessing:\\n\", df_train['Preproc_Question_Body'][0])\n\nprint(\"=\"*70 + \"Answer\" + \"=\"*70)\nprint(\"Before Preprocessing:\\n\", df_train['answer'][0])\nprint(\"\\nAfter Preprocessing:\\n\", df_train['Preproc_Answer'][0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ****Architecture - 2****\n\n<img src='https://i.imgur.com/j2bqj6k.png'>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Prepare Embedding for 3 inputs separately\n\ndef prepare_embedding(input_series_train, input_series_test, column_name):\n\n    print(\"=\"*70 + column_name + \"=\"*70)\n\n    #Tokenization of text data to numbers\n    tokenizer_obj = tf.keras.preprocessing.text.Tokenizer()\n    tokenizer_obj.fit_on_texts(input_series_train.values)\n\n    word_index = tokenizer_obj.word_index\n    print('Found %s unique tokens.' % len(word_index))\n\n    #Encoded inputs\n    train_sequences = tokenizer_obj.texts_to_sequences(input_series_train.values)\n    test_sequences = tokenizer_obj.texts_to_sequences(input_series_test.values)\n    print(\"Train Sequences Length\", len(train_sequences))\n    print(\"Test Sequences Length\", len(test_sequences))\n\n    #Selecting max_length of words in an essay\n    MAX_SEQUENCE_LENGTH = int(np.percentile(pd.Series(train_sequences).apply(lambda x: len(x)), 96))\n    print(\"Around 96 percentile of \" + column_name + \" have length of words less than \", MAX_SEQUENCE_LENGTH)\n\n    #Padding of Word Sequences\n    vocab_size = len(word_index)+1\n    train_sequences_pad = tf.keras.preprocessing.sequence.pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n    test_sequences_pad = tf.keras.preprocessing.sequence.pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n    print(\"Shape of padded train sequences: \", train_sequences_pad.shape)\n    print(\"Shape of padded test sequences: \", test_sequences_pad.shape)\n\n    #Preparing Embedding Layer using Glove vector (300 dimension)\n\n    # Loading Glove embedding layer\n    embeddings_index = {}\n    f = open(PATH_w2vec_300d+'glove-840B-300d-char_embed.txt')\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\n    f.close()\n\n    print('Found %s word vectors.' % len(embeddings_index))\n\n    #*--*create a weight matrix for words in training docs*--*\n    embedding_matrix = np.zeros((vocab_size, 300))\n    for word, i in word_index.items():\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n\n    return vocab_size, embedding_matrix, MAX_SEQUENCE_LENGTH, train_sequences_pad, test_sequences_pad\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# #Calling prepare_embedding method to generate embedding for 3 inputs for both train and test\n# vocab_size_question_title, embedding_matrix_question_title, MAX_SEQUENCE_LENGTH_question_title, train_sequences_pad_qt, test_sequences_pad_qt = \\\n# prepare_embedding(df_train['question_title'], df_test['question_title'], 'Question Title')\n\n# vocab_size_question_body, embedding_matrix_question_body, MAX_SEQUENCE_LENGTH_question_body, train_sequences_pad_qb, test_sequences_pad_qb = \\\n# prepare_embedding(df_train['question_body'], df_test['question_body'], 'Question Body')\n\n# vocab_size_answer, embedding_matrix_answer, MAX_SEQUENCE_LENGTH_answer, train_sequences_pad_ans, test_sequences_pad_ans = \\\n# prepare_embedding(df_train['answer'], df_test['answer'], 'Answer')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Merge all the text columns into 1 as 'Total_Text'\n\ndf_train['Total_Text'] = df_train[\"Preproc_Question_Title\"].map(str) + df_train[\"Preproc_Question_Body\"].map(str) + df_train['Preproc_Answer'].map(str)\ndf_test['Total_Text'] = df_test[\"Preproc_Question_Title\"].map(str) + df_test[\"Preproc_Question_Body\"].map(str) + df_test['Preproc_Answer'].map(str)\n\n#Calling prepare_embedding method to generate embedding for 3 inputs for both train and test\nvocab_size, embedding_matrix, MAX_SEQUENCE_LENGTH, _, train_sequences_pad_qt = \\\nprepare_embedding(df_train['Total_Text'], df_train['question_title'], 'Question Title Train')\nvocab_size, embedding_matrix, MAX_SEQUENCE_LENGTH, _, test_sequences_pad_qt = \\\nprepare_embedding(df_train['Total_Text'], df_test['question_title'], 'Question Title Test')\n\nvocab_size, embedding_matrix, MAX_SEQUENCE_LENGTH, _, train_sequences_pad_qb = \\\nprepare_embedding(df_train['Total_Text'], df_train['question_body'], 'Question Body Train')\nvocab_size, embedding_matrix, MAX_SEQUENCE_LENGTH, _, test_sequences_pad_qb = \\\nprepare_embedding(df_train['Total_Text'], df_test['question_body'], 'Question Body Test')\n\nvocab_size, embedding_matrix, MAX_SEQUENCE_LENGTH, _, train_sequences_pad_ans = \\\nprepare_embedding(df_train['Total_Text'], df_train['answer'], 'Answer Train')\nvocab_size, embedding_matrix, MAX_SEQUENCE_LENGTH, _, test_sequences_pad_ans = \\\nprepare_embedding(df_train['Total_Text'], df_test['answer'], 'Answer Test')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Splitting into train and validation\nvalidation_sequences_pad_qt = train_sequences_pad_qt[5000:]\ntrain_sequences_pad_qt = train_sequences_pad_qt[:5000]\n\nvalidation_sequences_pad_qb = train_sequences_pad_qb[5000:]\ntrain_sequences_pad_qb = train_sequences_pad_qb[:5000]\n\nvalidation_sequences_pad_ans = train_sequences_pad_ans[5000:]\ntrain_sequences_pad_ans = train_sequences_pad_ans[:5000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating Embedding Layers for all 3 inputs:\n\nembedding_layer_total_text = tf.keras.layers.Embedding(vocab_size,\n                                            300,\n                                            weights=[embedding_matrix],\n                                            input_length=MAX_SEQUENCE_LENGTH,\n                                            name = 'Shared_Embedding_Layer',\n                                            trainable=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model_2(): \n\n    #Path 1 \n    input_question_title = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,), name = 'IP_Question_Title')\n    embedded_question_title = embedding_layer_total_text(input_question_title)\n\n    #Path 2\n    input_question_body = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,), name = 'IP_Question_Body')\n    embedded_question_body = embedding_layer_total_text(input_question_body)\n\n    #Path 3\n    input_answer = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,), name = 'IP_Answer')\n    embedded_answer = embedding_layer_total_text(input_answer)\n\n    tower_1 = tf.keras.layers.Conv1D(64, 5, activation='relu')(embedded_question_title) #Kernel Size(M) = 3\n    tower_2 = tf.keras.layers.Conv1D(64, 5, activation='relu')(embedded_question_body) #Kernel Size(N) = 5\n    tower_3 = tf.keras.layers.Conv1D(64, 5, activation='relu')(embedded_answer) #Kernel Size(O) = 7\n\n    concat = tf.keras.layers.concatenate([tower_1, tower_2, tower_3], axis=1)\n    max_pool = tf.keras.layers.MaxPooling1D(9)(concat)#9\n\n    tower_1a = tf.keras.layers.Conv1D(64, 5, activation='relu')(max_pool) #Kernel Size(i) = 5\n    tower_2b = tf.keras.layers.Conv1D(64, 7, activation='relu')(max_pool) #Kernel Size(j) = 7\n    tower_3c = tf.keras.layers.Conv1D(64, 9, activation='relu')(max_pool) #Kernel Size(k) = 9 \n\n    concat2 = tf.keras.layers.concatenate([tower_1a, tower_2b, tower_3c], axis=1)\n    max_pool2 = tf.keras.layers.MaxPooling1D(9)(concat2)#9\n\n    convP = tf.keras.layers.Conv1D(64, 9, activation='relu')(max_pool2) #Kernel Size(P) = 9\n    flatten = tf.keras.layers.Flatten()(convP)\n    dropout = tf.keras.layers.Dropout(0.7)(flatten) #Taking Dropout Rate = 0.2\n\n    dense = tf.keras.layers.Dense(128, activation='relu')(dropout) #128\n    preds = tf.keras.layers.Dense(30, activation='sigmoid', name='Output')(dense)\n\n    model_created = tf.keras.models.Model([input_question_title, input_question_body, input_answer], preds, name='Model_Google_QUEST')\n\n    return model_created\n\n#Calling create_model method and printing summary of model\nmodel_Google_QUEST = create_model_2()\nprint(model_Google_QUEST.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting Architecture_1 of Google QUEST:\ntf.keras.utils.plot_model(model_Google_QUEST, to_file='Arch_2_v2.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Metrics and Callbacks  \n\ndef compute_spearmanr(trues, preds):\n    rhos = []\n    for col_trues, col_pred in zip(trues.T, preds.T):\n        rhos.append(\n            spearmanr(col_trues, col_pred + np.random.normal(0, 1e-7, col_pred.shape[0])).correlation)\n    return np.nanmean(rhos)\n\n\nclass CustomCallback(tf.keras.callbacks.Callback):\n    \n    def on_train_begin(self, logs={}):\n        self.train_data = {'IP_Question_Title': train_sequences_pad_qt, 'IP_Question_Body': train_sequences_pad_qb, 'IP_Answer': train_sequences_pad_ans}\n        self.train_target = df_train[output_categories].values[:5000]\n\n        self.validation_data = {'IP_Question_Title': validation_sequences_pad_qt, 'IP_Question_Body': validation_sequences_pad_qb, 'IP_Answer': validation_sequences_pad_ans}\n        self.validation_target = df_train[output_categories].values[5000:]\n\n        self.valid_predictions = []\n        self.test_predictions = []\n        \n    def on_epoch_end(self, epoch, logs={}):\n        self.valid_predictions.append(\n            self.model.predict(self.validation_data))\n        \n        rho_val = compute_spearmanr(\n            self.validation_target, np.average(self.valid_predictions, axis=0))\n        \n        print(\"\\nvalidation rho: %.4f\" % rho_val)\n        \n        # if self.fold is not None:\n        #     self.model.save_weights(f'bert-base-{fold}-{epoch}.h5py')\n        \n        # self.test_predictions.append(\n        #     self.model.predict(self.test_inputs, batch_size=self.batch_size)\n\ncustom_callback = CustomCallback()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Compile and fit Model:\n\ntrain_data = {'IP_Question_Title': train_sequences_pad_qt, 'IP_Question_Body': train_sequences_pad_qb, 'IP_Answer': train_sequences_pad_ans}\ntrain_target = df_train[output_categories].values[:5000]\n\ntest_data = {'IP_Question_Title': validation_sequences_pad_qt, 'IP_Question_Body': validation_sequences_pad_qb, 'IP_Answer': validation_sequences_pad_ans}\ntest_target = df_train[output_categories].values[5000:]\n\noptimizer_adam = tf.keras.optimizers.Adam(learning_rate=0.01)\nmodel_Google_QUEST.compile(loss='mean_squared_error', optimizer=optimizer_adam)\nmodel_Google_QUEST.fit(train_data, train_target, validation_data = (test_data, test_target),\n           epochs=100, batch_size=64, verbose=1, callbacks=[custom_callback])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_prediction = model_Google_QUEST.predict({'IP_Question_Title': train_sequences_pad_qt, 'IP_Question_Body': train_sequences_pad_qb, 'IP_Answer': train_sequences_pad_ans})\nvalidation_prediction = model_Google_QUEST.predict({'IP_Question_Title': validation_sequences_pad_qt, 'IP_Question_Body': validation_sequences_pad_qb, 'IP_Answer': validation_sequences_pad_ans})\ntest_prediction = model_Google_QUEST.predict({'IP_Question_Title': test_sequences_pad_qt, 'IP_Question_Body': test_sequences_pad_qb, 'IP_Answer': test_sequences_pad_ans})\n\n#Train Spearman Rank Correlation\nprint(\"Train Spearman Rank Correlation: \",compute_spearmanr(df_train[output_categories].values[:5000], train_prediction))\n\n#Validation Spearman Rank Correlation\nprint(\"Validation Spearman Rank Correlation: \",compute_spearmanr(df_train[output_categories].values[5000:], validation_prediction))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Prepare submission file:\ntest_prediction = model_Google_QUEST.predict({'IP_Question_Title': test_sequences_pad_qt, 'IP_Question_Body': test_sequences_pad_qb, 'IP_Answer': test_sequences_pad_ans})\nsubmission_df = pd.concat([pd.DataFrame(df_test['qa_id']), pd.DataFrame(test_prediction, columns=output_categories)], axis=1)\nsubmission_df.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}