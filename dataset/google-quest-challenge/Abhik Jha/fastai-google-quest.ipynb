{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\n\n%reload_ext autoreload\n%autoreload 2\n%matplotlib inline\n\nfrom fastai import *\nfrom fastai.text import *\nfrom fastai.tabular import *\n\nfrom pathlib import Path\nfrom typing import *\n\nimport torch\nimport torch.optim as optim\n\nimport gc\ngc.collect()\n\nimport re\nimport os\nimport re\nimport gc\nimport pickle  \nimport random\nimport keras\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport keras.backend as K\n\nfrom keras.models import Model\nfrom keras.layers import Dense, Input, Dropout, Lambda\nfrom keras.optimizers import Adam\nfrom keras.callbacks import Callback\nfrom scipy.stats import spearmanr, rankdata\nfrom os.path import join as path_join\nfrom numpy.random import seed\nfrom urllib.parse import urlparse\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom bayes_opt import BayesianOptimization\nfrom lightgbm import LGBMRegressor\nfrom nltk.tokenize import wordpunct_tokenize\nfrom nltk.stem.snowball import EnglishStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom functools import lru_cache\nfrom tqdm import tqdm as tqdm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nSEED = 999\nseed_everything(SEED)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/google-quest-challenge/train.csv\")\ntest = pd.read_csv(\"../input/google-quest-challenge/test.csv\")\nsub = pd.read_csv(\"../input/google-quest-challenge/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape, test.shape, sub.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£',\n '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', '\\n', '\\xa0', '\\t',\n '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '\\u3000', '\\u202f',\n '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '«',\n '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\nmispell_dict = {\"aren't\" : \"are not\",\n\"can't\" : \"cannot\",\n\"couldn't\" : \"could not\",\n\"couldnt\" : \"could not\",\n\"didn't\" : \"did not\",\n\"doesn't\" : \"does not\",\n\"doesnt\" : \"does not\",\n\"don't\" : \"do not\",\n\"hadn't\" : \"had not\",\n\"hasn't\" : \"has not\",\n\"haven't\" : \"have not\",\n\"havent\" : \"have not\",\n\"he'd\" : \"he would\",\n\"he'll\" : \"he will\",\n\"he's\" : \"he is\",\n\"i'd\" : \"I would\",\n\"i'd\" : \"I had\",\n\"i'll\" : \"I will\",\n\"i'm\" : \"I am\",\n\"isn't\" : \"is not\",\n\"it's\" : \"it is\",\n\"it'll\":\"it will\",\n\"i've\" : \"I have\",\n\"let's\" : \"let us\",\n\"mightn't\" : \"might not\",\n\"mustn't\" : \"must not\",\n\"shan't\" : \"shall not\",\n\"she'd\" : \"she would\",\n\"she'll\" : \"she will\",\n\"she's\" : \"she is\",\n\"shouldn't\" : \"should not\",\n\"shouldnt\" : \"should not\",\n\"that's\" : \"that is\",\n\"thats\" : \"that is\",\n\"there's\" : \"there is\",\n\"theres\" : \"there is\",\n\"they'd\" : \"they would\",\n\"they'll\" : \"they will\",\n\"they're\" : \"they are\",\n\"theyre\":  \"they are\",\n\"they've\" : \"they have\",\n\"we'd\" : \"we would\",\n\"we're\" : \"we are\",\n\"weren't\" : \"were not\",\n\"we've\" : \"we have\",\n\"what'll\" : \"what will\",\n\"what're\" : \"what are\",\n\"what's\" : \"what is\",\n\"what've\" : \"what have\",\n\"where's\" : \"where is\",\n\"who'd\" : \"who would\",\n\"who'll\" : \"who will\",\n\"who're\" : \"who are\",\n\"who's\" : \"who is\",\n\"who've\" : \"who have\",\n\"won't\" : \"will not\",\n\"wouldn't\" : \"would not\",\n\"you'd\" : \"you would\",\n\"you'll\" : \"you will\",\n\"you're\" : \"you are\",\n\"you've\" : \"you have\",\n\"'re\": \" are\",\n\"wasn't\": \"was not\",\n\"we'll\":\" will\",\n\"didn't\": \"did not\",\n\"tryin'\":\"trying\"}\n\n\ndef clean_text(x):\n    x = str(x)\n    for punct in puncts:\n        x = x.replace(punct, f' {punct} ')\n    return x\n\n\ndef clean_numbers(x):\n    x = re.sub('[0-9]{5,}', '#####', x)\n    x = re.sub('[0-9]{4}', '####', x)\n    x = re.sub('[0-9]{3}', '###', x)\n    x = re.sub('[0-9]{2}', '##', x)\n    return x\n\n\ndef _get_mispell(mispell_dict):\n    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n    return mispell_dict, mispell_re\n\n\ndef replace_typical_misspell(text):\n    mispellings, mispellings_re = _get_mispell(mispell_dict)\n\n    def replace(match):\n        return mispellings[match.group(0)]\n\n    return mispellings_re.sub(replace, text)\n\n\ndef clean_data(df, columns: list):\n    for col in columns:\n        df[col] = df[col].apply(lambda x: clean_numbers(x))\n        df[col] = df[col].apply(lambda x: clean_text(x.lower()))\n        df[col] = df[col].apply(lambda x: replace_typical_misspell(x))\n\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_cols_questions = ['question_asker_intent_understanding',\n       'question_body_critical', 'question_conversational',\n       'question_expect_short_answer', 'question_fact_seeking',\n       'question_has_commonly_accepted_answer',\n       'question_interestingness_others', 'question_interestingness_self',\n       'question_multi_intent', 'question_not_really_a_question',\n       'question_opinion_seeking', 'question_type_choice',\n       'question_type_compare', 'question_type_consequence',\n       'question_type_definition', 'question_type_entity',\n       'question_type_instructions', 'question_type_procedure',\n       'question_type_reason_explanation', 'question_type_spelling',\n       'question_well_written']\n\ntarget_cols_answers = ['answer_helpful',\n       'answer_level_of_information', 'answer_plausible', 'answer_relevance',\n       'answer_satisfaction', 'answer_type_instructions',\n       'answer_type_procedure', 'answer_type_reason_explanation',\n       'answer_well_written']\n\ntargets = target_cols_questions + target_cols_answers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = clean_data(train, ['answer', 'question_body', 'question_title'])\ntest = clean_data(test, ['answer', 'question_body', 'question_title'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"find = re.compile(r\"^[^.]*\")\n\ntrain['netloc_1'] = train['url'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\ntest['netloc_1'] = test['url'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\n\ntrain['netloc_2'] = train['question_user_page'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\ntest['netloc_2'] = test['question_user_page'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\n\ntrain['netloc_3'] = train['answer_user_page'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\ntest['netloc_3'] = test['answer_user_page'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.host.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.netloc_1.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# WordVec with Fastai Tabular"},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_tfidf = train.copy()\n# test_tfidf = test.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import gensim\n\n# w2v_model = gensim.models.KeyedVectors.load_word2vec_format('../input/word2vec-google/GoogleNews-vectors-negative300.bin', \n#                                                             binary=True, unicode_errors='ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# #https://www.kaggle.com/sediment/a-gentle-introduction-eda-tfidf-word2vec/data#Benchmark-with-Word2Vec\n\n# from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n\n# TFIDF_SVD_WORDVEC_DIM = 300\n\n# def get_text_feats(df, col):\n\n#     def tokenize_downcase_filtering(x):\n#         words = TOKENIZER.tokenize(x)\n#         lower_case = map(lambda w: w.lower(), words)\n#         content_words = filter(lambda w: w not in STOPWORDS, lower_case)\n#         return ' '.join(content_words)\n\n#     rows = df[col].map(tokenize_downcase_filtering).values.tolist()\n#     tfidf = TfidfVectorizer(tokenizer=lambda x: x.split(' '))  # dont use sklearn default tokenization tool \n#     tfidf_weights = tfidf.fit_transform(rows)\n#     svd = TruncatedSVD(n_components=TFIDF_SVD_WORDVEC_DIM, n_iter=10)  # reduce dimensionality\n#     dense_tfidf_repr_mat = svd.fit_transform(tfidf_weights)\n    \n#     word2vec_repr_mat = np.zeros((len(df), w2v_model.vector_size))\n#     for i, row in enumerate(rows):\n#         word2vec_accum = np.zeros((w2v_model.vector_size, ))\n#         word_cnt = 0\n#         for w in row.split(' '):\n#             if w in w2v_model.wv:\n#                 word2vec_accum += w2v_model.wv[w]\n#                 word_cnt += 1\n\n#         # compute the average for the wordvec of each non-sptop word\n#         if word_cnt != 0:\n#             word2vec_repr_mat[i] = word2vec_accum / word_cnt\n\n#     return  np.concatenate([word2vec_repr_mat, dense_tfidf_repr_mat], axis=1)  # word2vec + tfidf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from nltk.tokenize import RegexpTokenizer\n# from nltk.corpus import stopwords\n\n# TOKENIZER = RegexpTokenizer(r'\\w+')\n# STOPWORDS = set(stopwords.words('english'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # let's build features\n# df_all = pd.concat((train_tfidf, test_tfidf))\n# df_all['question_title_len'] = df_all['question_title'].map(lambda x: len(TOKENIZER.tokenize(x)))\n# df_all['question_body_len'] = df_all['question_body'].map(lambda x: len(TOKENIZER.tokenize(x)))\n# df_all['answer_len'] = df_all['answer'].map(lambda x: len(TOKENIZER.tokenize(x)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# text_cols = [\n#     'question_title',\n#     'question_body',\n#     'answer'\n# ]\n\n# text_len_cols = ['question_title_len', 'question_body_len', 'answer_len']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.decomposition import TruncatedSVD\n\n# data = []\n# for col in text_cols:\n#     data.append(get_text_feats(df_all, col))\n\n# data.append(df_all[text_len_cols].values)\n# data = np.concatenate(data, axis=1)\n\n# train_feats = data[:len(train_tfidf)]\n# test_feats = data[len(train_tfidf):]\n\n\n# print(train_feats.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(test_feats.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# del w2v_model\n# gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_wordvec = pd.DataFrame(data = train_feats)\n# train_wordvec.columns = [str(col) + '_col' for col in train_wordvec.columns]\n# print(train_wordvec.shape)\n# train_wordvec.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test_wordvec = pd.DataFrame(data = test_feats)\n# test_wordvec.columns = [str(col) + '_col' for col in test_wordvec.columns]\n# print(test_wordvec.shape)\n# test_wordvec.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tabular_cols = ['question_user_name', 'answer_user_name', \n#                'netloc_1', 'netloc_2', 'netloc_3',\n#                'category', 'host']\n\n# train_select = train[tabular_cols + targets]\n# test_select = test[tabular_cols]\n\n# train_tfidf_final = pd.concat([train_wordvec, train_select], axis=1)\n\n# test_tfidf_final = pd.concat([test_wordvec, test_select], axis=1)\n\n# gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# valid_sz = 2000\n# valid_idx = range(len(train_wordvec)-valid_sz, len(train_wordvec))\n# valid_idx","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cont_names = train_wordvec.columns\n# cat_names = tabular_cols\n# dep_var = targets\n# procs = [FillMissing, Categorify, Normalize]\n\n# test_tab = TabularList.from_df(test_tfidf_final, cat_names=cat_names, cont_names=cont_names, procs=procs)\n\n# data = (TabularList.from_df(train_tfidf_final, procs = procs, cont_names=cont_names, cat_names=cat_names)\n#         .split_by_idx(valid_idx)\n#         .label_from_df(cols=dep_var)\n#         .add_test(test_tab)\n#         .databunch(bs=32))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from fastai.callbacks import *\n\n# auroc = AUROC()\n\n# learn_wordvec = tabular_learner(data, layers=[800, 400, 200, 100], \n#                         ps=[0.5, 0.5, 0.25, 0.25], emb_drop=0.5)\n# learn_wordvec.lr_find()\n# learn_wordvec.recorder.plot(suggestion=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lr = 5e-2\n# learn_wordvec.fit_one_cycle(7, max_lr=lr,  pct_start=0.5, wd = 0.75)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# learn_wordvec.lr_find()\n# learn_wordvec.recorder.plot(suggestion=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lr = 1e-4\n# learn_wordvec.fit_one_cycle(7, max_lr=lr,  pct_start=0.5, wd = 0.75)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# learn_wordvec.lr_find()\n# learn_wordvec.recorder.plot(suggestion=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lr = 1e-5\n# learn_wordvec.fit_one_cycle(7, max_lr=lr,  pct_start=0.5, wd = 1.)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pred_test_wordvec, lbl_test_wordvec = learn_wordvec.get_preds(ds_type=DatasetType.Test)\n# print(pred_test_wordvec.shape)\n# pred_test_wordvec","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pred_test_wordvec = np.clip(pred_test_wordvec, 0.00001, 0.999999)\n# pred_test_wordvec.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# del df_all, train_tfidf_final, train_tfidf, test_tfidf, test_tfidf_final, train_wordvec, test_wordvec\n# gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TFIDF - Tabular"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_tfidf = train.copy()\ntest_tfidf = test.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stemmer = EnglishStemmer()\n\n@lru_cache(30000)\ndef stem_word(text):\n    return stemmer.stem(text)\n\n\nlemmatizer = WordNetLemmatizer()\n\n@lru_cache(30000)\ndef lemmatize_word(text):\n    return lemmatizer.lemmatize(text)\n\n\ndef reduce_text(conversion, text):\n    return \" \".join(map(conversion, wordpunct_tokenize(text.lower())))\n\n\ndef reduce_texts(conversion, texts):\n    return [reduce_text(conversion, str(text))\n            for text in tqdm(texts)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_tfidf['question_body'] = reduce_texts(stem_word, train_tfidf['question_body'])\ntest_tfidf['question_body'] = reduce_texts(stem_word, test_tfidf['question_body'])\n\ntrain_tfidf['question_title'] = reduce_texts(stem_word, train_tfidf['question_title'])\ntest_tfidf['question_title'] = reduce_texts(stem_word, test_tfidf['question_title'])\n\ntrain_tfidf['answer'] = reduce_texts(stem_word, train_tfidf['answer'])\ntest_tfidf['answer'] = reduce_texts(stem_word, test_tfidf['answer'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_text_1 = train_tfidf['question_body']\ntest_text_1 = test_tfidf['question_body']\nall_text_1 = pd.concat([train_text_1, test_text_1])\n\ntrain_text_2 = train_tfidf['answer']\ntest_text_2 = test_tfidf['answer']\nall_text_2 = pd.concat([train_text_2, test_text_2])\n\ntrain_text_3 = train_tfidf['question_title']\ntest_text_3 = test_tfidf['question_title']\nall_text_3 = pd.concat([train_text_3, test_text_3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re, string\nre_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\ndef tokenize(s): return re_tok.sub(r' \\1 ', s).split()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn import linear_model\nfrom sklearn.model_selection import train_test_split\nimport scipy\nfrom sklearn.metrics import log_loss\nimport xgboost as xgb\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.sparse import hstack\nfrom sklearn.decomposition import TruncatedSVD\n\n\nword_vectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    analyzer='word',\n    token_pattern=r'\\w{1,}',\n    stop_words='english',\n    ngram_range=(1, 2),\n    max_features=80000,\n    tokenizer=tokenize)\nword_vectorizer.fit(all_text_1)\n\ntrain_word_features_1 = word_vectorizer.transform(train_text_1)\ntest_word_features_1 = word_vectorizer.transform(test_text_1)\n\nword_vectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    analyzer='word',\n    token_pattern=r'\\w{1,}',\n    stop_words='english',\n    ngram_range=(1, 2),\n    max_features=80000,\n    tokenizer=tokenize)\nword_vectorizer.fit(all_text_2)\n\ntrain_word_features_2 = word_vectorizer.transform(train_text_2)\ntest_word_features_2 = word_vectorizer.transform(test_text_2)\n\nword_vectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    analyzer='word',\n    token_pattern=r'\\w{1,}',\n    stop_words='english',\n    ngram_range=(1, 2),\n    max_features=80000,\n    tokenizer=tokenize)\nword_vectorizer.fit(all_text_3)\n\ntrain_word_features_3 = word_vectorizer.transform(train_text_3)\ntest_word_features_3 = word_vectorizer.transform(test_text_3)\n\nchar_vectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    analyzer='char',\n    stop_words='english',\n    token_pattern=r'\\w{1,}',\n    ngram_range=(1, 4),\n    max_features=50000,\n    tokenizer=tokenize)\nchar_vectorizer.fit(all_text_1)\n\ntrain_char_features_1 = char_vectorizer.transform(train_text_1)\ntest_char_features_1 = char_vectorizer.transform(test_text_1)\n\nchar_vectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    analyzer='char',\n    stop_words='english',\n    token_pattern=r'\\w{1,}',\n    ngram_range=(1, 4),\n    max_features=50000,\n    tokenizer=tokenize)\nchar_vectorizer.fit(all_text_2)\n\ntrain_char_features_2 = char_vectorizer.transform(train_text_2)\ntest_char_features_2 = char_vectorizer.transform(test_text_2)\n\nchar_vectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    analyzer='char',\n    stop_words='english',\n    ngram_range=(1, 4),\n    max_features=50000,\n    tokenizer=tokenize)\nchar_vectorizer.fit(all_text_3)\n\ntrain_char_features_3 = char_vectorizer.transform(train_text_3)\ntest_char_features_3 = char_vectorizer.transform(test_text_3)\n\ntrain_features = hstack([train_char_features_1, train_word_features_1, train_char_features_2, train_word_features_2,train_char_features_3, train_word_features_3])\ntest_features = hstack([test_char_features_1, test_word_features_1, test_char_features_2, test_word_features_2,test_char_features_3, test_word_features_3])\n\npca = TruncatedSVD(n_components=300, n_iter=10)\ntf_idf_text_train = pca.fit_transform(train_features)\ntf_idf_text_test = pca.fit_transform(test_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_tfidf = pd.DataFrame(data = tf_idf_text_train)\ntrain_tfidf.columns = [str(col) + '_col' for col in train_tfidf.columns]\nprint(train_tfidf.shape)\ntrain_tfidf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_tfidf = pd.DataFrame(data = tf_idf_text_test)\ntest_tfidf.columns = [str(col) + '_col' for col in test_tfidf.columns]\nprint(test_tfidf.shape)\ntest_tfidf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tabular_cols = ['question_user_name', 'answer_user_name', \n               'netloc_1', 'netloc_2', 'netloc_3',\n               'category', 'host']\n\ntrain_select = train[tabular_cols + targets]\ntest_select = test[tabular_cols]\n\ntrain_tfidf_final = pd.concat([train_tfidf, train_select], axis=1)\n\ntest_tfidf_final = pd.concat([test_tfidf, test_select], axis=1)\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_sz = 2000\nvalid_idx = range(len(train_tfidf)-valid_sz, len(train_tfidf))\nvalid_idx","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_tfidf.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cont_names = train_tfidf.columns\ncat_names = tabular_cols\ndep_var = targets\nprocs = [FillMissing, Categorify, Normalize]\n\ntest_tab = TabularList.from_df(test_tfidf_final, cat_names=cat_names, cont_names=cont_names, procs=procs)\n\ndata = (TabularList.from_df(train_tfidf_final, procs = procs, cont_names=cont_names, cat_names=cat_names)\n        .split_by_idx(valid_idx)\n        .label_from_df(cols=dep_var)\n        .add_test(test_tab)\n        .databunch(bs=32))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from fastai.callbacks import *\n\nauroc = AUROC()\n\nlearn_tfidf = tabular_learner(data, layers=[800, 400, 200, 100], \n                        ps=[0.5, 0.5, 0.25, 0.25], emb_drop=0.5)\nlearn_tfidf.lr_find()\nlearn_tfidf.recorder.plot(suggestion=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = 5e-2\nlearn_tfidf.fit_one_cycle(7, max_lr=lr,  pct_start=0.5, wd = 0.75)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_tfidf.lr_find()\nlearn_tfidf.recorder.plot(suggestion=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr=1e-4\nlearn_tfidf.fit_one_cycle(7, max_lr=lr,  pct_start=0.5, wd = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_tfidf.lr_find()\nlearn_tfidf.recorder.plot(suggestion=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr=1e-5\nlearn_tfidf.fit_one_cycle(7, max_lr=lr,  pct_start=0.5, wd = 1.)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_test_tfidf, lbl_test_tfidf = learn_tfidf.get_preds(ds_type=DatasetType.Test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_test_tfidf = np.clip(pred_test_tfidf, 0.00001, 0.999999)\npred_test_tfidf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_test_tfidf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tree Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"X, y  = train_tfidf_final.iloc[:, :-30], train_tfidf_final.iloc[:, -30:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Categorical boolean mask\ncategorical_feature_mask = X.dtypes==object\n# filter categorical columns using mask and turn it into a list\ncategorical_cols = X.columns[categorical_feature_mask].tolist()\n\n# import labelencoder\nfrom sklearn.preprocessing import LabelEncoder\n# instantiate labelencoder object\nle = LabelEncoder()\n\n# apply le on categorical feature columns\nX[categorical_cols] = X[categorical_cols].apply(lambda col: le.fit_transform(col))\nX[categorical_cols].head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.multioutput import MultiOutputRegressor\n\n\nregr_multirf = MultiOutputRegressor(LGBMRegressor(boosting_type='gbdt', num_leaves=31, max_depth=5, learning_rate=0.1, \n                                                  n_estimators=100, min_child_samples=20, subsample=0.8, \n                                                  subsample_freq=0, colsample_bytree=0.8, \n                                                  reg_alpha=1., reg_lambda=1., random_state=42, silent=False))\n\nregr_multirf.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = test_tfidf_final\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Categorical boolean mask\ncategorical_feature_mask = X.dtypes==object\n# filter categorical columns using mask and turn it into a list\ncategorical_cols = X.columns[categorical_feature_mask].tolist()\n\n# import labelencoder\nfrom sklearn.preprocessing import LabelEncoder\n# instantiate labelencoder object\nle = LabelEncoder()\n\n# apply le on categorical feature columns\nX[categorical_cols] = X[categorical_cols].apply(lambda col: le.fit_transform(col))\nX[categorical_cols].head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_test_tree = regr_multirf.predict(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_test_tree = np.clip(pred_test_tree, 0.00001, 0.999999)\npred_test_tree.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_test_tree","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train-Val split"},{"metadata":{"trusted":true},"cell_type":"code","source":"train, val = train_test_split(train, test_size=0.2, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape, val.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\ntext = train.question_title.values\nwordcloud = WordCloud(\n    width = 3000,\n    height = 2000,\n    background_color = 'black',\n    stopwords = STOPWORDS).generate(str(text))\nfig = plt.figure(\n    figsize = (40, 30),\n    facecolor = 'k',\n    edgecolor = 'k')\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\ntext = train.question_body.values\nwordcloud = WordCloud(\n    width = 3000,\n    height = 2000,\n    background_color = 'black',\n    stopwords = STOPWORDS).generate(str(text))\nfig = plt.figure(\n    figsize = (40, 30),\n    facecolor = 'k',\n    edgecolor = 'k')\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\ntext = train.answer.values\nwordcloud = WordCloud(\n    width = 3000,\n    height = 2000,\n    background_color = 'black',\n    stopwords = STOPWORDS).generate(str(text))\nfig = plt.figure(\n    figsize = (40, 30),\n    facecolor = 'k',\n    edgecolor = 'k')\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# who asked the most questions\n\nqn_asker = train.question_user_name.value_counts()\nqn_asker","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nfrom matplotlib import pyplot as plt\nimport matplotlib.style as style\nstyle.use('seaborn-poster')\nstyle.use('ggplot')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"qn_asker.loc[qn_asker>10].sort_values().plot(kind = 'barh', figsize=(15,15)).legend(loc='best')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"qn_answerer = train.answer_user_name.value_counts()\nqn_answerer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"qn_answerer.loc[qn_answerer>10].sort_values().plot(kind = 'barh', figsize=(15,15)).legend(loc='best')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"category = train.category.value_counts()\ncategory","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## lets see some distributions of questions targets\nplt.figure(figsize=(20, 5))\n\nsns.distplot(train[target_cols_questions[0]], hist= False , rug= False ,kde=True, label =target_cols_questions[0],axlabel =False )\nsns.distplot(train[target_cols_questions[1]], hist= False , kde=True, rug= False,label =target_cols_questions[1],axlabel =False)\nsns.distplot(train[target_cols_questions[2]], hist= False , kde=True, rug= False,label =target_cols_questions[2],axlabel =False)\nsns.distplot(train[target_cols_questions[3]], hist= False , kde=True, rug= False,label =target_cols_questions[3],axlabel =False)\nsns.distplot(train[target_cols_questions[4]], hist= False , kde=True, rug= False,label =target_cols_questions[4],axlabel =False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## lets see some distributions of questions targets\nplt.figure(figsize=(20, 5))\n\nsns.distplot(train[target_cols_answers[0]], hist= False , rug= False ,kde=True, label =target_cols_answers[0],axlabel =False )\nsns.distplot(train[target_cols_answers[1]], hist= False , kde=True, rug= False,label =target_cols_answers[1],axlabel =False)\nsns.distplot(train[target_cols_answers[2]], hist= False , kde=True, rug= False,label =target_cols_answers[2],axlabel =False)\nsns.distplot(train[target_cols_answers[3]], hist= False , kde=True, rug= False,label =target_cols_answers[3],axlabel =False)\nsns.distplot(train[target_cols_answers[4]], hist= False , kde=True, rug= False,label =target_cols_answers[4],axlabel =False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fastai NLP Modelling"},{"metadata":{},"cell_type":"markdown","source":"# Language Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"bs, bptt = 32, 80\n\ndata_lm = TextLMDataBunch.from_df('.', train, val, test,\n                  include_bos=False,\n                  include_eos=False,\n                  text_cols=['question_title', 'question_body', 'answer'],\n                  label_cols=targets,\n                  bs=bs,\n                  mark_fields=True,\n                  collate_fn=partial(pad_collate, pad_first=False, pad_idx=0),\n             )\n\ndata_lm.save('data_lm.pkl')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# src_lm = ItemLists(path, TextList.from_df(train, path=\".\", cols = [ 'question_title', \"question_body\", 'answer']), \n#                    TextList.from_df(val, path=\".\", cols = [ 'question_title', \"question_body\", 'answer']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# data_lm = src_lm.label_for_lm().databunch(bs=32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = \".\"\ndata_lm = load_data(path, 'data_lm.pkl', bs=bs, bptt=bptt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = \".\"\ndata_bwd = load_data(path, 'data_lm.pkl', bs=bs, bptt = bptt, backwards=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_lm.show_batch()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_bwd.show_batch()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"awd_lstm_lm_config = dict( emb_sz=400, n_hid=1150, n_layers=3, pad_token=1, qrnn=False, bidir=False, output_p=0.1,\n                          hidden_p=0.15, input_p=0.25, embed_p=0.02, weight_p=0.2, tie_weights=True, out_bias=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"awd_lstm_clas_config = dict(emb_sz=400, n_hid=1150, n_layers=3, pad_token=1, qrnn=False, bidir=False, output_p=0.4,\n                       hidden_p=0.2, input_p=0.6, embed_p=0.1, weight_p=0.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Forward Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.5,\n                               config=awd_lstm_lm_config, pretrained = False)\nlearn = learn.to_fp16(clip=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fnames = ['../input/awd-lstm/lstm_wt103.pth','../input/awd-lstm/itos_wt103.pkl']\nlearn.load_pretrained(*fnames, strict=False)\nlearn.freeze()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.lr_find()\nlearn.recorder.plot(suggestion=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(2, max_lr=slice(5e-3, 5e-2), moms=(0.8, 0.7), pct_start=0.3, wd =(1e-7, 1e-5, 1e-4, 1e-3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.save('fit_head')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.unfreeze()\nlearn.lr_find()\nlearn.recorder.plot(suggestion=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(10, max_lr = slice(1e-4, 5e-2), moms=(0.8, 0.7), pct_start=0.3, wd =(1e-7, 1e-5, 1e-4,  1e-2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.recorder.plot_losses()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.save('fine-tuned')\nlearn.load('fine-tuned')\nlearn.save_encoder('fine-tuned-fwd')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(learn.model[0].encoder)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\npca = PCA(n_components=2)\npca.fit(learn.model[0].encoder.weight.data)\nembedding_weights = pca.transform(learn.model[0].encoder.weight.data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,15))\nplt.scatter(embedding_weights[:, 0], embedding_weights[:, 1])\n\nfor i, word in enumerate(data_lm.vocab.itos[:50]):\n    plt.annotate(word, xy=(embedding_weights[i, 0], embedding_weights[i, 1]))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Backward Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"learn = language_model_learner(data_bwd, AWD_LSTM, drop_mult=0.5,\n                               config=awd_lstm_lm_config, pretrained = False)\nlearn = learn.to_fp16(clip=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fnames = ['../input/awd-lstm/lstm_wt103.pth','../input/awd-lstm/itos_wt103.pkl']\nlearn.load_pretrained(*fnames, strict=False)\nlearn.freeze()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.lr_find()\nlearn.recorder.plot(suggestion=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(2, max_lr=slice(5e-2, 1e-1), moms=(0.8, 0.7), pct_start=0.3, wd =(1e-7, 1e-5, 1e-4, 1e-3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.save('fit_head-bwd')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.unfreeze()\nlearn.lr_find()\nlearn.recorder.plot(suggestion=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(10, max_lr = slice(1e-4, 1e-3), moms=(0.8, 0.7), pct_start=0.3, wd =(1e-7, 1e-5, 1e-4,  1e-2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.recorder.plot_losses()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.save('fine-tuned-bwd')\nlearn.load('fine-tuned-bwd')\nlearn.save_encoder('fine-tuned-bwd')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Regression Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"text_cols = ['question_title', \"question_body\", 'answer']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_cls = TextClasDataBunch.from_df('.', train, val, test, vocab = data_lm.vocab,\n                  include_bos=False,\n                  include_eos=False,\n                  text_cols=text_cols,\n                  label_cols=targets,\n                  bs=bs,\n                  mark_fields=True,\n                  collate_fn=partial(pad_collate, pad_first=False, pad_idx=0),\n             )\n\ndata_cls.save('data_cls.pkl')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_cls = load_data(path, 'data_cls.pkl', bs=bs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_cls.show_batch()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_cls_bwd = load_data(path, 'data_cls.pkl', bs=bs, backwards=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_cls_bwd.show_batch()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class L1LossFlat(nn.MSELoss):\n    def forward(self, input:Tensor, target:Tensor) -> Rank0Tensor:\n        return super().forward(input.view(-1), target.view(-1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Forward Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"learn = text_classifier_learner(data_cls, AWD_LSTM, drop_mult=0.5,config=awd_lstm_clas_config, pretrained = False)\nlearn.load_encoder('fine-tuned-fwd')\nlearn = learn.to_fp16(clip=0.1)\n#learn.loss_func = L1LossFlat()\nfnames = ['../input/awd-lstm/lstm_wt103.pth','../input/awd-lstm/itos_wt103.pkl']\nlearn.load_pretrained(*fnames, strict=False)\nlearn.freeze()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.lr_find()\nlearn.recorder.plot(suggestion=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(2, max_lr=slice(1e-2, 1e-1), moms=(0.8, 0.7), pct_start=0.3, wd =(1e-7, 1e-5, 1e-4, 1e-3, 1e-2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.save('first-head')\nlearn.load('first-head')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.freeze_to(-2)\nlearn.fit_one_cycle(2, slice(1e-1/(2.6**4),1e-1), moms=(0.8,0.7), pct_start=0.3, wd =(1e-7, 1e-5, 1e-4, 1e-3, 1e-2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.save('second')\nlearn.load('second')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.freeze_to(-3)\nlearn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3), moms=(0.8,0.7), pct_start=0.3, wd =(1e-7, 1e-5, 1e-4, 1e-3, 1e-2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.save('third')\nlearn.load('third')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.unfreeze()\nlearn.lr_find()\nlearn.recorder.plot(suggestion=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(5, slice(1e-5/(2.6**4),1e-4), moms=(0.8,0.7), pct_start=0.3, wd =(1e-7, 1e-5, 1e-4, 1e-3, 1e-2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.recorder.plot_losses()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.save('fwd-cls')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Backward Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_bwd = text_classifier_learner(data_cls_bwd, AWD_LSTM, drop_mult=0.5, config=awd_lstm_clas_config, pretrained = False)\nlearn_bwd.load_encoder('fine-tuned-bwd')\nlearn_bwd = learn_bwd.to_fp16(clip=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fnames = ['../input/awd-lstm/lstm_wt103.pth','../input/awd-lstm/itos_wt103.pkl']\nlearn_bwd.load_pretrained(*fnames, strict=False)\nlearn_bwd.freeze()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_bwd.lr_find()\nlearn_bwd.recorder.plot(suggestion=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_bwd.fit_one_cycle(2, max_lr=slice(5e-2, 1e-1), moms=(0.8, 0.7), pct_start=0.3, wd =(1e-7, 1e-5, 1e-4, 1e-3, 1e-2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_bwd.save('first-head-bwd')\nlearn_bwd.load('first-head-bwd')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_bwd.freeze_to(-2)\nlearn_bwd.fit_one_cycle(2, slice(1e-1/(2.6**4),1e-1), moms=(0.8,0.7), pct_start=0.3, wd =(1e-7, 1e-5, 1e-4, 1e-3, 1e-2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_bwd.save('second-bwd')\nlearn_bwd.load('second-bwd')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_bwd.freeze_to(-3)\nlearn_bwd.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3), moms=(0.8,0.7), pct_start=0.3, wd =(1e-7, 1e-5, 1e-4, 1e-3, 1e-2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_bwd.save('third-bwd')\nlearn_bwd.load('third-bwd')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_bwd.unfreeze()\nlearn_bwd.lr_find()\nlearn_bwd.recorder.plot(suggestion=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_bwd.fit_one_cycle(5, slice(1e-3/(2.6**4),1e-3), moms=(0.8,0.7), pct_start=0.3, wd =(1e-7, 1e-5, 1e-4, 1e-3, 1e-2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_bwd.recorder.plot_losses()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_bwd.save('bwd-cls')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_fwd_val, lbl_fwd_val = learn.get_preds(ds_type=DatasetType.Valid,ordered=True)\npred_bwd_val, lbl_bwd_val = learn_bwd.get_preds(ds_type=DatasetType.Valid,ordered=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_fwd_test, lbl_fwd_test = learn.get_preds(ds_type=DatasetType.Test,ordered=True)\npred_bwd_test, lbl_bwd_test = learn_bwd.get_preds(ds_type=DatasetType.Test,ordered=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_test_tree = torch.from_numpy(pred_test_tree)\nfinal_preds_test = (0.30 * pred_fwd_test + 0.30 * pred_bwd_test + 0.30 * pred_test_tfidf  + 0.10* pred_test_tree)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def get_ordered_preds(learn, ds_type, preds):\n#   np.random.seed(42)\n#   sampler = [i for i in learn.data.dl(ds_type).sampler]\n#   reverse_sampler = np.argsort(sampler)\n#   preds = [p[reverse_sampler] for p in preds]\n#   return preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# val_raw_preds = learn.get_preds(ds_type=DatasetType.Valid)\n# val_preds_fwd = get_ordered_preds(learn, DatasetType.Valid, val_raw_preds)\n\n# val_raw_preds = learn_bwd.get_preds(ds_type=DatasetType.Valid)\n# val_preds_bwd = get_ordered_preds(learn_bwd, DatasetType.Valid, val_raw_preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# final_preds = (pred_fwd + pred_bwd)/2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from scipy.stats import spearmanr\n# score = 0\n# for i in range(30):\n#     score += np.nan_to_num(spearmanr(val[targets].values[:, i], final_preds_val[:, i]).correlation) / 30\n# score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test_raw_preds = learn.get_preds(ds_type=DatasetType.Test)\n# test_preds_fwd = get_ordered_preds(learn, DatasetType.Test, test_raw_preds)\n\n# test_raw_preds = learn_bwd.get_preds(ds_type=DatasetType.Test)\n# test_preds_bwd = get_ordered_preds(learn_bwd, DatasetType.Test, test_raw_preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.head()\n\nsub.iloc[:, 1:] = final_preds_test.numpy()\nsub.to_csv('submission.csv', index=False)\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(6, 5, figsize=(18, 15))\naxes = axes.ravel()\nbins = np.linspace(0, 1, 20)\n\nfor i, col in enumerate(targets):\n    ax = axes[i]\n    sns.distplot(train[col], label=col, bins=bins, ax=ax, color='blue')\n    sns.distplot(sub[col], label=col, bins=bins, ax=ax, color='orange')\n    # ax.set_title(col)\n    ax.set_xlim([0, 1])\nplt.tight_layout()\nplt.show()\nplt.close()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}