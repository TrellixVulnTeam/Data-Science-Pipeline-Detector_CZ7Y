{"cells":[{"metadata":{},"cell_type":"markdown","source":"The data for this competition includes questions and answers from various StackExchange properties. Your task is to predict target values of 30 labels for each question-answer pair.\n\nThe list of 30 target labels are the same as the column names in the samplesubmission.csv file. Target labels with the prefix question relate to the question_title and/or questionbody features in the data. Target labels with the prefix answer relate to the answer feature.\n\nEach row contains a single question and a single answer to that question, along with additional features. The training data contains rows with some duplicated questions (but with different answers). The test data does not contain any duplicated questions.\n\nThis is not a binary prediction challenge. Target labels are aggregated from multiple raters, and can have continuous values in the range [0,1]. Therefore, predictions must also be in that range.\n\nSince this is a synchronous re-run competition, you only have access to the Public test set. For planning purposes, the re-run test set is no larger than 10,000 rows, and less than 8 Mb uncompressed."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. Load data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('../input/google-quest-challenge/train.csv')\ndf_test = pd.read_csv('../input/google-quest-challenge/test.csv')\ndf_sub = pd.read_csv('../input/google-quest-challenge/sample_submission.csv')\n\ninput_columns = list(df_train.columns[[1,2,5]])\noutput_columns = list(df_train.columns[11:])\n\nprint('train shape =', df_train.shape)\nprint('test shape =', df_test.shape)\n\n# show columns\nprint('\\n ', len(input_columns),'  inputs :', *[f'\\n\\t{x}' for x in input_columns])\nprint('\\n ', len(output_columns),' outputs:', *[f'\\n\\t{x}' for x in output_columns])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Load BERT"},{"metadata":{},"cell_type":"markdown","source":"BERT requires specifically formatted inputs. For each tokenized input sentence, we need to create:\n\n* **input ids**: a sequence of integers identifying each input token to its index number in the BERT tokenizer vocabulary\n* **segment mask**: (optional) a sequence of 1s and 0s used to identify whether the input is one sentence or two sentences long. For one sentence inputs, this is simply a sequence of 0s. For two sentence inputs, there is a 0 for each token of the first sentence, followed by a 1 for each token of the second sentence\n* **attention mask**: (optional) a sequence of 1s and 0s, with 1s for all input tokens and 0s for all padding tokens (we'll detail this in the next paragraph)\n* **labels**: a single value between 0 and 1\n\nFollowing instructions from tensorflow hub [here](https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/1)"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_hub as hub\n\nmax_seq_length = 512 # Your choice here.\n\ninput_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"input_word_ids\")\ninput_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"input_mask\")\nsegment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"segment_ids\")\n\n# TF Hub model uses L=12 hidden layers (i.e., Transformer blocks), \n# a hidden size of H=768, and A=12 attention heads\n# Inputs have been \"cased\", meaning that the distinction between \n# lower and upper case as well as accent markers have been preserved\nbert_layer = hub.KerasLayer(\"/kaggle/input/bert-en-cased-l12-h768-a12-1/bert-en-cased-l12-h768-a12-v1\")\n\npooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Import tokenizer using the original vocab file\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import bert_tokenization as tokenization\n\nvocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Test BERT embedding generator model"},{"metadata":{},"cell_type":"markdown","source":"Generating segments and masks based on the original BERT"},{"metadata":{"trusted":true},"cell_type":"code","source":"# See BERT paper: https://arxiv.org/pdf/1810.04805.pdf\n# And BERT implementation convert_single_example() at https://github.com/google-research/bert/blob/master/run_classifier.py\n\ndef get_masks(tokens, max_seq_length):\n    \"\"\"Mask for padding\"\"\"\n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n\n\ndef get_segments(tokens, max_seq_length):\n    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    segments = []\n    current_segment_id = 0\n    for token in tokens:\n        segments.append(current_segment_id)\n        if token == \"[SEP]\":\n            current_segment_id = 1\n    return segments + [0] * (max_seq_length - len(tokens))\n\n\ndef get_ids(tokens, tokenizer, max_seq_length):\n    \"\"\"Token ids from Tokenizer vocab\"\"\"\n    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n    return input_ids\n\n# And BERT implementation truncate_seq_pair() at https://github.com/google-research/bert/blob/master/run_classifier.py\ndef _truncate_seq_pair(tokens_a, tokens_b, max_length):\n    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n    # This is a simple heuristic which will always truncate the longer sequence\n    # one token at a time. This makes more sense than truncating an equal percent\n    # of tokens from each, since if one sequence is very short then each token\n    # that's truncated likely contains more information than a longer sequence.\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_length:\n            break\n        if len(tokens_a) > len(tokens_b):\n            tokens_a.pop()\n        else:\n            tokens_b.pop()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Although we can have variable length input sentences, BERT does requires our input arrays to be the same size. We address this by first choosing a maximum sentence length, and then padding and truncating our inputs until every input sequence is of the same length.\n\nTo \"pad\" our inputs in this context means that if a sentence is shorter than the maximum sentence length, we simply add 0s to the end of the sequence until it is the maximum sentence length.\n\nIf a sentence is longer than the maximum sentence length, then we simply truncate the end of the sequence, discarding anything that does not fit into our maximum sentence length.\n\nWe pad and truncate our sequences so that they all become of length MAX_LEN (\"post\" indicates that we want to pad and truncate at the end of the sequence, as opposed to the beginning) pad_sequences is a utility function that we're borrowing from Keras. It simply handles the truncating and padding of Python lists."},{"metadata":{"trusted":true},"cell_type":"code","source":"inputs = df_train[input_columns]\ntitle = inputs.question_title[0]\nbody = inputs.question_body[0]\n\n# Tokenizing the sentence\ntitle_tokens = tokenizer.tokenize(title)\nbody_tokens = tokenizer.tokenize(body)\n\n# Adding separator tokens according to the paper\nstokens = [\"[CLS]\"] + title_tokens + [\"[SEP]\"] + body_tokens + [\"[SEP]\"]\n\n# Get the model inputs from the tokens\nsample_ids = get_ids(stokens, tokenizer, max_seq_length)\nsample_masks = get_masks(stokens, max_seq_length)\nsample_segments = get_segments(stokens, max_seq_length)\n\n# print (first 20)\nsample = (stokens,sample_ids,sample_masks,sample_segments)\nb = [[print(f'\\n {len(x)} :', x[:20])] for x in sample]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Build models for training"},{"metadata":{"trusted":true},"cell_type":"code","source":"max_seq_length = 512 # Your choice here.\n\nimport math\n\ndef compute_input_arrays(df, tokenizer, max_sequence_length):\n    input_ids, input_masks, input_segments = [], [], []\n    for _, row in df.iterrows():\n        # Tokenizing the sentence\n        tokens_a = tokenizer.tokenize(row[0])\n        tokens_b = tokenizer.tokenize(row[1])\n        # Modifies `tokens_a` and `tokens_b` in place so that the total\n        # length is less than the specified length.\n        # Account for [CLS], [SEP], [SEP] with \"- 3\"\n        _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n        # Adding separator tokens according to the paper\n        stoken = [\"[CLS]\"] + tokens_a + [\"[SEP]\"] + tokens_b + [\"[SEP]\"]\n        ids = get_ids(stoken, tokenizer, max_sequence_length)\n        masks = get_masks(stoken, max_sequence_length)\n        segments = get_segments(stoken, max_sequence_length)\n        # Get the model inputs from the tokens\n        input_ids.append(ids)\n        input_masks.append(masks)\n        input_segments.append(segments)\n    return [np.asarray(input_ids, dtype=np.int16), \n            np.asarray(input_masks, dtype=np.int16), \n            np.asarray(input_segments, dtype=np.int16)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_rate = 0.00002\nbatch_size = 16\nepochs = 8","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model1():\n    input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"segment_ids\")\n    # TF Hub model uses L=12 hidden layers (i.e., Transformer blocks), \n    # a hidden size of H=768, and A=12 attention heads\n    # Inputs have been \"cased\", meaning that the distinction between \n    # lower and upper case as well as accent markers have been preserved\n    bert_layer = hub.KerasLayer(\"/kaggle/input/bert-en-cased-l12-h768-a12-1/bert-en-cased-l12-h768-a12-v1\")\n    pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])    \n    x = tf.keras.layers.GlobalAveragePooling1D()(sequence_output)\n    x = tf.keras.layers.Dropout(0.2)(x)\n    x = tf.keras.layers.Dense(42, activation='relu')(x)\n    x = tf.keras.layers.Dense(21, activation=\"sigmoid\", name=\"dense_output\")(x)\n    model = tf.keras.models.Model(\n        inputs=[input_word_ids, input_mask, segment_ids], \n        outputs=x)\n    optimizer = tf.keras.optimizers.Adam(learning_rate=learn_rate)\n    model.compile(loss='binary_crossentropy', optimizer=optimizer)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_train(model, inputs, outputs):\n    input_arrays = compute_input_arrays(inputs, tokenizer, max_seq_length)\n    output_arrays = np.asarray(outputs) # only question related columns\n    model.fit(input_arrays, output_arrays, epochs=epochs, batch_size=batch_size)\n    \ndef model_predict(model, inputs):\n    compute_inputs = compute_input_arrays(inputs, tokenizer, max_seq_length)\n    return model.predict(compute_inputs, batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = model1()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1_inputs = ['question_title','question_body']\nmodel1_outputs = output_columns[:21]\n    \nmodel_train(model, df_train[model1_inputs], df_train[model1_outputs])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### update Model1 prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model_predict(model, df_test[model1_inputs])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sub.iloc[:, 1:22] = predictions","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 2"},{"metadata":{"trusted":true},"cell_type":"code","source":"def model2():\n    input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"segment_ids\")\n    # TF Hub model uses L=12 hidden layers (i.e., Transformer blocks), \n    # a hidden size of H=768, and A=12 attention heads\n    # Inputs have been \"cased\", meaning that the distinction between \n    # lower and upper case as well as accent markers have been preserved\n    bert_layer = hub.KerasLayer(\"/kaggle/input/bert-en-cased-l12-h768-a12-1/bert-en-cased-l12-h768-a12-v1\")\n    pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])    \n    x = tf.keras.layers.GlobalAveragePooling1D()(sequence_output)\n    x = tf.keras.layers.Dropout(0.2)(x)\n    x = tf.keras.layers.Dense(18, activation='relu')(x)\n    x = tf.keras.layers.Dense(9, activation=\"sigmoid\", name=\"dense_output\")(x)\n    model = tf.keras.models.Model(\n        inputs=[input_word_ids, input_mask, segment_ids], \n        outputs=x)\n    optimizer = tf.keras.optimizers.Adam(learning_rate=learn_rate)\n    model.compile(loss='binary_crossentropy', optimizer=optimizer)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = model2()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model2_inputs = ['question_body','answer']\nmodel2_outputs = output_columns[21:]\n\nmodel_train(model, df_train[model2_inputs], df_train[model2_outputs])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model2_predictions = model_predict(model, df_test[model2_inputs])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model2_predictions.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sub.iloc[:, 22:] = model2_predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}