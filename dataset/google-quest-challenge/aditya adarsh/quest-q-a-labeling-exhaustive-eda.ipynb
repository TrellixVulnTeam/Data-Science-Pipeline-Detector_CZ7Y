{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Case Study 1\n\n# Google QUEST Q&A Labeling\nImproving automated understanding of complex question answer content.\n"},{"metadata":{"jupyter":{"source_hidden":true},"trusted":true,"_kg_hide-input":true,"_kg_hide-output":false},"cell_type":"code","source":"from io import BytesIO\nimport requests\nfrom PIL import Image\nresponse = requests.get(\"https://external-content.duckduckgo.com/iu/?u=http%3A%2F%2F1.bp.blogspot.com%2F-qKqxEyuXEQo%2FVD71mHi8sDI%2FAAAAAAAAPhg%2FVwDyvAlXDnY%2Fs1600%2Frgicontransparent.png&f=1&nofb=1\")\nimg = Image.open(BytesIO(response.content))\nimg.resize((300,300), Image.ANTIALIAS)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Workflow\n\n#### 1. Problem Statement and source\n#### 2. Business Constrain and Dataset Loading\n#### 3. Machine learning Formulation and Evalution mtric\n#### 4. Loading Dataset\n#### 5. Exploratory Data Analyses\n#### 6. Insights and Findings\n#### 7. Preprocessing and Feature engineering \n#### 8. Modeling and Hyperparameter tuning \n#### 9. Results and Conclusion\n"},{"metadata":{},"cell_type":"markdown","source":"\n"},{"metadata":{},"cell_type":"markdown","source":"#### Special Thanks to these Kaggle kernals and refers \n\n* https://www.kaggle.com/dimitreoliveira/google-quest-eda-and-use-baseline\n\n* https://www.kaggle.com/codename007/start-from-here-quest-complete-eda-fe\n\n* https://www.kaggle.com/kabure/qa-eda-and-nlp-modelling-insights-and-data-vis"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ignore all your warnings\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Loading Libraries\nimport datetime\nimport pandas as pd\nimport numpy as np\nimport nltk\nimport string\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import f1_score\nimport pylab \nimport scipy.stats as stats\nfrom scipy.stats import boxcox\n\nimport re\nimport pickle\nfrom sklearn.model_selection import train_test_split\n\nfrom tqdm import tqdm\nimport os\nfrom wordcloud import WordCloud\nfrom matplotlib_venn import venn2\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom textblob import TextBlob","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Problem Statement\n#### Source(kaggle): https://www.kaggle.com/c/google-quest-challenge/overview"},{"metadata":{},"cell_type":"markdown","source":"## Problem Defination"},{"metadata":{},"cell_type":"markdown","source":"Computers are really good at answering questions with single, verifiable answers. But, humans are often still better at answering questions about opinions, recommendations, or personal experiences.\n\nHumans are better at addressing subjective questions that require a deeper, multidimensional understanding of context - something computers aren't trained to do well…yet.. Questions can take many forms - some have multi-sentence elaborations, others may be simple curiosity or a fully developed problem. They can have multiple intents, or seek advice and opinions. Some may be helpful and others interesting. Some are simple right or wrong."},{"metadata":{"jupyter":{"source_hidden":true},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"response = requests.get(\"https://storage.googleapis.com/kaggle-media/competitions/google-research/human_computable_dimensions_1.png\")\nImage.open(BytesIO(response.content))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Unfortunately, it’s hard to build better subjective question-answering algorithms because of a lack of data and predictive models. That’s why the CrowdSource team at Google Research, a group dedicated to advancing NLP and other types of ML science via crowdsourcing, has collected data on a number of these quality scoring aspects.\n\nIn this competition, you’re challenged to use this new dataset to build predictive algorithms for different subjective aspects of question-answering. The question-answer pairs were gathered from nearly 70 different websites, in a \"common-sense\" fashion. Our raters received minimal guidance and training, and relied largely on their subjective interpretation of the prompts. As such, each prompt was crafted in the most intuitive fashion so that raters could simply use their common-sense to complete the task. By lessening our dependency on complicated and opaque rating guidelines, we hope to increase the re-use value of this data set. What you see is what you get!\n\nDemonstrating these subjective labels can be predicted reliably can shine a new light on this research area. Results from this competition will inform the way future intelligent Q&A systems will get built, hopefully contributing to them becoming more human-like."},{"metadata":{},"cell_type":"markdown","source":"## About Dataset"},{"metadata":{},"cell_type":"markdown","source":"* The data for this competition includes questions and answers from various StackExchange properties. Your task is to predict target values of 30 labels for each question-answer pair.\n\n* The list of 30 target labels are the same as the column names in the sample_submission.csv file. Target labels with the prefix question_ relate to the question_title and/or question_body features in the data. Target labels with the prefix answer_ relate to the answer feature.\n\n* Each row contains a single question and a single answer to that question, along with additional features. The training data contains rows with some duplicated questions (but with different answers). The test data does not contain any duplicated questions.\n\n* This is not a binary prediction challenge. Target labels are aggregated from multiple raters, and can have continuous values in the range [0,1]. Therefore, predictions must also be in that range.\n\n* Since this is a synchronous re-run competition, you only have access to the Public test set. For planning purposes, the re-run test set is no larger than 10,000 rows, and less than 8 Mb uncompressed.\n\n* Additional information about the labels and collection method will be provided by the competition sponsor in the forum.\n"},{"metadata":{},"cell_type":"markdown","source":"#### File descriptions\n* train.csv - the training data (target labels are the last 30 columns)\n* test.csv - the test set (you must predict 30 labels for each test set row)\n* sample_submission.csv - a sample submission file in the correct format; column names are the 30 target labels"},{"metadata":{},"cell_type":"markdown","source":"# 2. Business Objective and Constrains"},{"metadata":{},"cell_type":"markdown","source":"#### 1. Provide tremendous value in differentiating subjective aspects of question-answering (like humans)\n\n#### 2. No Low Latency required \n\n#### 3. Model Interpretability is helpful\n\n#### 4. Feature Importance is helpful\n\n"},{"metadata":{},"cell_type":"markdown","source":"# 3. Machine Learning Formulation and Evaluation Metric\n"},{"metadata":{},"cell_type":"markdown","source":"## 3.1. Multi Label Regression problem\n\nIt is clearly mentioned in Kaggle Competition page:  \n\"This is not a binary prediction challenge. Target labels are aggregated from multiple raters, and can have continuous values in the range [0,1]. Therefore, predictions must also be in that range.\"\n\n#### Predict the continous value between range [0,1] for 30 target labels\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# This image is created by me for better understanding of problem\nresponse = requests.get(\"https://i.postimg.cc/NFNFYPPG/regression.png\")\nImage.open(BytesIO(response.content))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.2 Evaluation Metric"},{"metadata":{},"cell_type":"markdown","source":"###  'Mean column-wise Spearman's correlation coefficient'"},{"metadata":{},"cell_type":"markdown","source":" Submissions are evaluated on the mean column-wise Spearman's correlation coefficient.\n \n The Spearman's rank correlation is computed for each target column, and the mean of these values is calculated for the submission score"},{"metadata":{},"cell_type":"markdown","source":"#### Defination:\n\n\"The Spearman's rank-order correlation is the nonparametric version of the Pearson product-moment correlation. Spearman's correlation coefficient, (ρ, also signified by rs) measures the strength and direction of association between two ranked variables.\""},{"metadata":{},"cell_type":"markdown","source":"#### But why Spearman's correlation coefficient?"},{"metadata":{},"cell_type":"markdown","source":"Reason\n\n"},{"metadata":{},"cell_type":"markdown","source":"# 4. Loading The Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os \nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have 3 csv files available\n* 1. train.csv\n* 2. test.csv\n* 3. sample_submission.csv\n\n#### 'train.csv' contain all the independent variable and 30 target labels on which we need to train the model.\n\n#### 'test.csv' contain only independent variable on which we have to predict.\n\n#### 'sample_submission.csv' contain the format of how to predict all the target values."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/google-quest-challenge/train.csv\")\ntest_df = pd.read_csv(\"../input/google-quest-challenge/test.csv\")\nsample_submission = pd.read_csv(\"../input/google-quest-challenge/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_df data\nprint(f\"shape of train_df: {train_df.shape} \\n{'='*50}\")\ntrain_df.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test_df data\nprint(f\"shape of test_df: {test_df.shape} \\n{'='*50}\")\ntest_df.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.1 Dependent variables or Target Labels"},{"metadata":{"trusted":true},"cell_type":"code","source":"target_vars = sample_submission.columns[1:]\nfor idx,target in enumerate(target_vars):\n    print(idx+1,\":\",target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sample of Target variable\ntrain_df[target_vars].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.2 Independent variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_columns = [columns for columns in train_df.columns if columns not in sample_submission.columns[1:]]\nfor idx,x_var in enumerate(x_columns):\n    print(idx+1,\":\",x_var)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sample of dependent variables\ntrain_df[x_columns].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Exploratory Data Analyses"},{"metadata":{},"cell_type":"markdown","source":"## 5.1. Overview of train_df"},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_df data\nprint(f\"shape of train_df: {train_df.shape} \\n{'='*50}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This includes both independent variables and target variables\ntrain_df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observations\n* There is around 6k datapoints in training data\n* There is no null values in any of the columns but there is some repeated questions present in data which is answered by different users ( clearly mentioned on Kaggle Competition page )"},{"metadata":{},"cell_type":"markdown","source":"## 5.2. Sample Datapoint (train_df)"},{"metadata":{},"cell_type":"markdown","source":"#### Sample"},{"metadata":{"trusted":true},"cell_type":"code","source":"# sample\nsample=train_df.iloc[0]\nsample_question = sample[['qa_id', 'question_title', 'question_body']]\nsample_answer = sample[['answer']]\nsample_question_target_labels = sample[['question_asker_intent_understanding',\n       'question_body_critical', 'question_conversational',\n       'question_expect_short_answer', 'question_fact_seeking',\n       'question_has_commonly_accepted_answer',\n       'question_interestingness_others', 'question_interestingness_self',\n       'question_multi_intent', 'question_not_really_a_question',\n       'question_opinion_seeking', 'question_type_choice',\n       'question_type_compare', 'question_type_consequence',\n       'question_type_definition', 'question_type_entity',\n       'question_type_instructions', 'question_type_procedure',\n       'question_type_reason_explanation', 'question_type_spelling',\n       'question_well_written']]\nsample_answer_target_labels = sample[['answer_helpful',\n       'answer_level_of_information', 'answer_plausible', 'answer_relevance',\n       'answer_satisfaction', 'answer_type_instructions',\n       'answer_type_procedure', 'answer_type_reason_explanation',\n       'answer_well_written']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Sample Question"},{"metadata":{"jupyter":{"source_hidden":true},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"for i in sample_question.index:\n    print(i,\":\",sample_question[i],\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Sample Answer"},{"metadata":{"jupyter":{"source_hidden":true},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(sample_answer[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Sample Question Target Labels"},{"metadata":{"jupyter":{"source_hidden":true},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(sample_question_target_labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Sample Answer Target Labels"},{"metadata":{"jupyter":{"source_hidden":true},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(sample_answer_target_labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.3. EDA: Target Labels"},{"metadata":{"jupyter":{"source_hidden":true},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"target_vars = sample_submission.columns[1:]\nfor idx,target in enumerate(target_vars):\n    print(idx+1,\":\",target)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Each of the Target Labels name is self explanatory\n* All the 30 Taget Labes has values between range [0,1]"},{"metadata":{},"cell_type":"markdown","source":"### 5.3.1. Distribution of target labels"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(28,20))\nfor idx,target in enumerate(target_vars): \n    sns.distplot(train_df[target],ax=plt.subplot(5,6,idx+1))\n    plt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observation:\n\n1. Almost all taget labels are imbalanced and skewed but some are highly skewed like 'question_not_really' , 'question_type_spelling' etc. So we need to take special care for those labels.\n\n2. Wee need to make sure not to overfit on highly skewed target labels.\n    "},{"metadata":{},"cell_type":"markdown","source":"### 5.3.2. Correlation between target labels"},{"metadata":{},"cell_type":"markdown","source":"#### Note:\n\n* keep this in mind that \"Correlation is not necessary mean causation\". "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"corr = train_df[target_vars].corr()\n\n# plot the heatmap\nplt.figure(figsize=(16,14))\nsns.heatmap(corr, \n        xticklabels=corr.columns,\n        yticklabels=corr.columns,      \n        vmin=-1, vmax=1, center=0,)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observation\n\n1. There are some obvious high positive correlation e.g.\n    * answer_type_instructions is highly correlated to question_type_instructions.\n    * answer_type_procedure is highly correlated to question_type_procedure.\n    * answer_type_reason_explanation  is highly correlated to question_type_reason_explanation.\n    * question_interestingness_self is highly co related with question_interestingness_others \n    \n    \n2. Some other high posive correlation \n    * answer_level_of_information is highly correlated to answer_helpful.\n    * answer_plausable is highly correlated to answer_helpful.\n    * answer_satisfaction is highly correlated to answer_helpful , answer_level_of_information , answer_plausable and answer_relavance.\n    \n    \n3. There are also some high negative correlation is present\n    * question_fact_seeking is negatively correlated question_opinion_seeking.\n    * heatmap indicating light red color refers to all negatively correlated labels.\n    "},{"metadata":{},"cell_type":"markdown","source":"### 5.3.3 Plot of Target labels with high Correlation (either positive or negative)"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Distribution plots for highly positive Correlated labels\nhigh_pos_corr_feat = [['answer_type_procedure','question_fact_seeking'],\n[\"question_type_instructions\",\"answer_type_instructions\"],\n['question_type_reason_explanation','answer_type_reason_explanation'],\n['question_interestingness_self','question_interestingness_others'],\n['answer_level_of_information','answer_helpful'],\n['answer_plausible','answer_helpful'],\n['answer_satisfaction','answer_helpful']]\n\nprint(\"****Distribtion Plots for postitive Correlation Target labels****\\n\\n\")\nfor idx, feat in enumerate(high_pos_corr_feat):\n    print(f\"plot: {idx+1} \")\n    plt.figure(figsize=(8,4))\n    sns.distplot(train_df[f'{feat[0]}'], label=f\"{feat[0]}\")\n    sns.distplot(train_df[f'{feat[1]}'], label=f\"{feat[1]}\")\n    plt.legend()\n    plt.xlabel(None)\n    plt.title(f\"Distribuion of  {feat[0]}  V/S  {feat[1]}\\n\")\n    plt.grid()\n    plt.show()\n\n\n## Distribution plots for highly negative Correlated labels\nhigh_neg_corr_feat = [['question_fact_seeking','question_opinion_seeking']]\n\nprint(\"\\n\\n****Distribtion Plots for high Negative Correlation Target labels****\\n\")\nfor idx, feat in enumerate(high_neg_corr_feat):\n    print(f\"plot: {idx+1} \")\n    plt.figure(figsize=(8,4))\n    sns.distplot(train_df[f'{feat[0]}'], label=f\"{feat[0]}\")\n    sns.distplot(train_df[f'{feat[1]}'], label=f\"{feat[1]}\")\n    plt.legend()\n    plt.xlabel(None)\n    plt.title(f\"Distribuion of  {feat[0]}  V/S  {feat[1]}\\n\")\n    plt.grid()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* we can see that few of the targets which have high positive correlation have very similar distributions and some are even overlaping.\n* Same can be observed for targets which have high negative correlation."},{"metadata":{},"cell_type":"markdown","source":"## 5.4. EDA: Dependent Variable Questions_title"},{"metadata":{},"cell_type":"markdown","source":"### 5.4.0. Utility Fuctions"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Utility function to plot lineplot and distplot using seaborn\ndef plot_sns(data,feature,color='lightblue',title=None,subtitle=None):\n    \n    \"\"\"   \n    Utility function to plot lineplot and distplot using seaborn\n    \n    plot_sns(data,feature,color='lightblue',title=None,subtitle=None):\n    \n    data = data \n    feature = coulum name\n    color = color of plot\n    title = Either 'length' or 'number' based on which to plot. Otherwise by default='None'\n    subtitle = Either 'train_df' or 'test_df'. Otherwise by default='None'  \n    \n    \"\"\"    \n    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 6))\n    \n    # line plot\n    sns.lineplot(np.arange(len(data)),data,ax=ax1,color=color)    \n    if title=='number':\n        ax1.set(xlabel=f\"Idx of {feature}\", ylabel=f\"Number of words in {feature}\", title=f'Number of words in {feature} in {subtitle}\\n')\n    elif title=='length':\n        ax1.set(xlabel=f\"Idx of {feature}\", ylabel=f\"Length of {feature}\", title=f'Length of {feature} in {subtitle}\\n')   \n    ax1.grid()\n\n    # distribution plot\n    sns.distplot(data,ax=ax2,color=color)\n    if title=='number':\n        ax2.set(xlabel=f\"Idx of {feature}\", ylabel=f\"Number of words in {feature}\", title=f'Number of words in {feature} in {subtitle}\\n')\n    elif title=='length':\n        ax2.set(xlabel=f\"Idx of {feature}\", ylabel=f\"Length of {feature}\", title=f'Length of {feature} in {subtitle}\\n')   \n    ax2.grid()\n    plt.show()\n\n#=======================================================================================================================================================================================    \n# Utility function to plot bar graph for both train and test using seaborn\ndef plot_bar(train_data,test_data,feature=None,x_label=None, y_label=None):\n    \n    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 6))\n\n    # for train_df\n    sns.barplot(train_data,np.arange(len(train_data)),ax=ax1)\n    ax1.set(xlabel=f\"{x_label}\", ylabel=f\"{y_label} {feature}\", title='train_df\\n')\n    ax1.grid()\n    \n    # for test_df\n    sns.barplot(test_data,np.arange(len(test_data)),ax=ax2)\n    ax2.set(xlabel=f\"{x_label}\", ylabel=f\"{y_label} {feature}\", title='test_df\\n')\n    ax2.grid()\n    plt.show()\n    \n#=======================================================================================================================================================================================  \n# Utility function to plot requency of most popular words\ndef word_frequency_plot(dataframe, title=None):\n    list_of_all_words = []\n    for sent in dataframe:\n        list_of_all_words.extend(sent.split())\n\n    top_50_words = pd.Series(list_of_all_words).value_counts()[:50]\n    top_50_words_prob_dist = top_50_words.values/sum(top_50_words.values)\n\n    #  plot of frequency of polpular words in train\n    plt.figure(figsize=(16,7))\n    sns.barplot(top_50_words.index, top_50_words_prob_dist)\n    plt.xlabel(\"words\")\n    plt.ylabel(\"frequency\")\n    plt.title(f\"Frequency of most popular words {title}\\n\")\n    plt.xticks(rotation=70)\n    plt.grid()\n    plt.show()\n\n#=======================================================================================================================================================================================\n# Utility function to check if feature or variable follows Normal distribution using Q-Q Plot   \ndef q_q_plot(train_data, test_data, feature_name=None):\n    \"\"\"\n    # code refer: https://stackoverflow.com/a/13865874\n    \"\"\"\n    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 5))\n    \n    measurements = train_data\n    stats.probplot(measurements, dist=\"norm\", plot=ax1)\n    ax1.set(title=f'train : Q-Q Plot for {feature_name} \\n')\n\n    measurements = test_data\n    stats.probplot(measurements, dist=\"norm\", plot=ax2)\n    ax2.set(title=f'test : Q-Q Plot for {feature_name} \\n')\n    plt.show()\n        \n#=======================================================================================================================================================================================    \n# Utility function for box plot\ndef box_plot(train_data, test_data, feature_name=None):\n    \n    # for train data\n    plt.figure(figsize=(26,4))\n    sns.violinplot(train_data,color='darkred')\n    plt.title(f'Train : violinplot Plot for {feature_name} \\n')\n    plt.xlabel(f\"{feature_name}\")\n    plt.ylabel(f\"Distribution\")   \n    plt.grid()\n    plt.show()\n    \n    # for test data\n    plt.figure(figsize=(26,4))\n    sns.violinplot(test_data,color='orangered')\n    plt.title(f'Test : violinplot Plot for {feature_name} \\n')\n    plt.xlabel(f\"{feature_name}\")\n    plt.ylabel(f\"Distribution\")   \n    plt.grid()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.4.1. Number of repeated question_title in train and test"},{"metadata":{},"cell_type":"markdown","source":"#### Which Question is most popular?\nWhich question has maximum number of answers?"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Counts of repeated question_title in train\npd.DataFrame(train_df['question_title'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Counts of repeated question_title in test\npd.DataFrame(test_df['question_title'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of repeated question_title in train\nn_repeated_question_title_train = train_df['question_title'].value_counts().values\n\n# Number of repeated question_title in test\nn_repeated_question_title_test = test_df['question_title'].value_counts().values\n\n# plot for Number of repeated question_title in train and test\nplot_bar(n_repeated_question_title_train, n_repeated_question_title_test,feature='question_title',x_label=\"Number of times repeated same question \",y_label=\"Counts of repeated\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Many of the questions occured only 1 time but still very large numbers of question occured more than once in the train data.\n* Maximum number of times a question occured repeatedly in train data is 12.\n* Test data do not have any repeative occurance of questions."},{"metadata":{},"cell_type":"markdown","source":"### 5.4.2 Length of question_title in train and test "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Length of question_title in train\nlen_question_title_train = sorted(train_df['question_title'].apply(lambda x: len(x)),reverse=True)\n\n# Length of question_title in test\nlen_question_title_test = sorted(test_df['question_title'].apply(lambda x: len(x)),reverse=True)\n\n# plot for train_df\nplot_sns(len_question_title_train,\"question_title\",color='darkblue',title='length',subtitle='train_df')\n\n# plot for test_df\nplot_sns(len_question_title_test,\"question_title\",color='lightblue',title='length',subtitle='test_df')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 5.4.2.1 Box plot of Length of question_title in train and test"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Box plot of Length of question_title in train and test\nbox_plot(len_question_title_train, len_question_title_test, \"question_title\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The length of most of question_title in train and test lies between ~ (20,100) \n* Very few question_title has very large length.\n* It also looks like the length of question_title follows Normal distribution with slight skeweness toward the right . Let's find it out using Q-Q plot."},{"metadata":{},"cell_type":"markdown","source":"#### 5.4.2.2.  Checking weather len_question_title follows normal distribution using Q-Q plot\n\nIf len_question_title follows Normal or Gaussian distribution , We can make use of these features in Logistic Regression like model which is the generalisation of gaussian Naive Bayes for better results"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking weather len_question_title follows normal distribution using Q-Q plot\nq_q_plot(len_question_title_train, len_question_title_test, \"len_question_title\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observation\n* No, Length of question_title does not follows normal distribution."},{"metadata":{},"cell_type":"markdown","source":"### 5.4.3 Number of words in question_title in train and test "},{"metadata":{"trusted":true},"cell_type":"code","source":"# number of words in question_title in train\nn_words_in_question_title_train = sorted(train_df['question_title'].apply(lambda x: len(x.split(\" \"))),reverse=True)\n\n# number of words in question_title in test\nn_words_in_question_title_test = sorted(test_df['question_title'].apply(lambda x: len(x.split(\" \"))),reverse=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot for train_df\nplot_sns(n_words_in_question_title_train,\"question_title\",color='darkred',title='number',subtitle='train_df')\n\n# plot for test_df\nplot_sns(n_words_in_question_title_train,\"question_title\",color='orangered',title='number',subtitle='test_df')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 5.4.3.1 Box plot of number of words in question_title in train and test"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Box plot of Length of question_title in train and test\nbox_plot(n_words_in_question_title_train, n_words_in_question_title_test, \"question_title\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Number of words in most of the question_title in train and test lies between ~ (2,20) words \n* Very few question_title has number of words greater than 20 making the plot skewed toward the right.\n* It also looks like the number of words in question_title follows Normal distribution with slight skew toward the right. Let's find it out using Q-Q plot."},{"metadata":{},"cell_type":"markdown","source":"#### 5.4.3.2 Checking weather n_words_in_question_title follows normal distribution or not using Q-Q plot"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking weather  n_words_in_question_title follows normal distribution using Q-Q plot\nq_q_plot(n_words_in_question_title_train, n_words_in_question_title_test, \"n_words_in_question_title\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Number of words in question title does not follows normal distribution."},{"metadata":{},"cell_type":"markdown","source":"### 5.4.4 WordCloud of question_title in train and test "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# refer: https://www.datacamp.com/community/tutorials/wordcloud-python\n\n# For train_df\ntext_train = \" \".join(word for word in train_df['question_title'])\n\n# Create and generate a word cloud image:\nwordcloud = WordCloud().generate(text_train)\n\n# Display the generated image:\nplt.figure(figsize=(9,6))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.title(\"WordCloud of question_title in train\\n\")\nplt.axis(\"off\")\nplt.show()\n\n#===================================================================\n\n# For test_df\ntext_test = \" \".join(word for word in test_df['question_title'])\n\n# Create and generate a word cloud image:\nwordcloud = WordCloud().generate(text_test)\n\n# Display the generated image:\nplt.figure(figsize=(9,6))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.title(\"WordCloud of question_title in test\")\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Ploting wordclouds of words without preprocessing roughly shows that there is huge similarity in words of question_title in both train and test data.\n* Also most of the keywords looks alike they are in the contex of programing. So while doing preprocessing, we need to keep this in mind that we do not remove programing syntax from question_title thinking as unnecessary word."},{"metadata":{},"cell_type":"markdown","source":"### 5.4.5 Frequency of most popular words in train and test"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Frequency of most popular 50 words in train_df\nword_frequency_plot(train_df['question_title'], title='train')\n\n# Frequency of most popular words in test_df\nword_frequency_plot(test_df['question_title'], title='test')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* As I have not removed punctuations from question_title, most frequent words are punctuations. So we need to plot this graph again after removing punctuations from question_title for accurate word frequency plot."},{"metadata":{},"cell_type":"markdown","source":"###  5.4.6 Common question_title in both train and test\n\nrefer: https://www.kaggle.com/codename007/start-from-here-quest-complete-eda-fe"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(9,5))\nvenn2([set(train_df['question_title'].unique()), set(test_df['question_title'].unique())], set_labels = ('Train set', 'Test set') )\nplt.title(\"Common question_title in training and test data\", fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n* There is no common question which is occured in both train and test."},{"metadata":{},"cell_type":"markdown","source":"## 5.5 EDA: Dependent variable question_body"},{"metadata":{},"cell_type":"markdown","source":"### 5.5.1 Number of repeated question_body  in train and test"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Counts of repeated questions in train\ntrain_df['question_body'].value_counts().values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Counts of repeated questions in test\ntest_df['question_body'].value_counts().values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of repeated question_body in train\nn_repeated_question_body_train = train_df['question_body'].value_counts().values\n\n# Number of repeated question_body in test\nn_repeated_question_body_test = test_df['question_body'].value_counts().values\n\n# plot for Number of repeated question_body in train and test\nplot_bar(n_repeated_question_body_train, n_repeated_question_body_test,feature='question_title',x_label=\"Number of times repeated same question \",y_label=\"Counts of repeated\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Obviously the plot for repeated question_body and repeated question_title are same. \n* This also indicates there is no repeated question having different question_title but same question_body or viceversa."},{"metadata":{},"cell_type":"markdown","source":"### 5.5.2 Length of question_body in train and test "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Length of question_title in train\nlen_question_body_train = sorted(train_df['question_body'].apply(lambda x: len(x)),reverse=True)\n\n# Length of question_title in test\nlen_question_body_test = sorted(test_df['question_body'].apply(lambda x: len(x)),reverse=True)\n\n# plot for train_df\nplot_sns(len_question_body_train,\"question_body\",color='darkblue',title='length',subtitle='train_df')\n\n# plot for test_df\nplot_sns(len_question_body_test,\"question_body\",color='lightblue',title='length',subtitle='test_df')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 5.5.2.1 Box plot of length of question_body"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Box plot of length of question_body\nbox_plot(len_question_body_train, len_question_body_test, \"len_question_body\" )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The length of most of question_body in train and test is less than 2000 \n* the distribution length of question_body is highly skewed towards right indicates that there are few question_body which has very large length( > 2000) .\n* It looks like length of question_body follows log normal distribution. If yes, we can transform it to nomal distribution using Box-Cox transform and apply linear model on top of transformed feature for better result. "},{"metadata":{},"cell_type":"markdown","source":"#### 5.5.2.2 Box Cox transform of length of question_body and Q-Q plot of transformed feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"box_cox_len_question_body_train = boxcox(len_question_body_train)[0]\nbox_cox_len_question_body_test =  boxcox(len_question_body_test)[0]\n\ntrain_df['box_cox_len_question_body'] = box_cox_len_question_body_train\ntest_df['box_cox_len_question_body'] = box_cox_len_question_body_test\n\n# Checking weather box cox transformed len_question_body_box_cox follows normal distribution or not using Q-Q plot\nq_q_plot(box_cox_len_question_body_train, box_cox_len_question_body_test, \"box-cox transformed length of question_body \")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Yes, It kinda look that Box Cox transfromed feature of len_question_body_box_cox_train roughly follows normal distribution\n"},{"metadata":{},"cell_type":"markdown","source":"### 5.5.3 Number of words in question_body in train and test "},{"metadata":{"trusted":true},"cell_type":"code","source":"# number of words in question_title in train\nn_words_in_question_body_train = sorted(train_df['question_body'].apply(lambda x: len(x.split(\" \"))),reverse=True)\n\n# number of words in question_title in test\nn_words_in_question_body_test = sorted(test_df['question_body'].apply(lambda x: len(x.split(\" \"))),reverse=True)\n\n# plot for train_df\nplot_sns(n_words_in_question_body_train,\"question_body\",color='darkred',title='number',subtitle='train_df')\n\n# plot for test_df\nplot_sns(n_words_in_question_body_test,\"question_body\",color='orangered',title='number',subtitle='test_df')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 5.5.3.1. Box plot of number of words in question_body"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Box plot of length of question_body\nbox_plot(n_words_in_question_body_train, n_words_in_question_body_test, \"n_words_in_question_body\" )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observation: "},{"metadata":{},"cell_type":"markdown","source":"* The number of words in question_body in train and test is less than 500 \n* the distribution number of words in question_body is highly skewed towards right indicates that there are few question_body which has very high number of words in question_body( greater than 500 and upto 7000 words). This could be because of some questions are from programming contex and the syntax of words maybe counted as indiviaudal word.\n* It also looks like number of words in question_body follows log normal distribution. If yes, we can transform it to nomal distribution using Box-Cox transform and apply linear model on top of transformed feature for better result. "},{"metadata":{},"cell_type":"markdown","source":"#### 5.5.3.2 Box Cox transform of number of words in question_body and Q-Q plot of transformed feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"box_cox_n_words_in_question_body_train = boxcox(n_words_in_question_body_train)[0]\nbox_cox_n_words_in_question_body_test =  boxcox(n_words_in_question_body_test)[0]\n\n# Saving box_cox_n_words_in_question_body as feature\ntrain_df['box_cox_n_words_in_question_body'] = box_cox_n_words_in_question_body_train \ntest_df['box_cox_n_words_in_question_body'] = box_cox_n_words_in_question_body_test\n\n# Checking weather box cox transformed len_question_body_box_cox follows normal distribution or not using Q-Q plot\nq_q_plot(box_cox_n_words_in_question_body_train, box_cox_n_words_in_question_body_test, \"box-cox transformed number of words in question_body \")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Yes, It kinda look that Box Cox transfromed feature of  number of words in question_body roughly follows normal distribution\n"},{"metadata":{},"cell_type":"markdown","source":"### 5.5.4 WordCloud of question_body in train and test "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# refer: https://www.datacamp.com/community/tutorials/wordcloud-python\n\n# For train_df\ntext_train = \" \".join(word for word in train_df['question_body'])\n\n# Create and generate a word cloud image:\nwordcloud = WordCloud().generate(text_train)\n\n# Display the generated image:\nplt.figure(figsize=(9,6))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.title(\"WordCloud of question_title in train\\n\")\nplt.axis(\"off\")\nplt.show()\n\n#===================================================================\n\n# For test_df\ntext_test = \" \".join(word for word in test_df['question_body'])\n\n# Create and generate a word cloud image:\nwordcloud = WordCloud().generate(text_test)\n\n# Display the generated image:\nplt.figure(figsize=(9,6))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.title(\"WordCloud of question_title in test\")\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Wordclouds of words in question_body without preprocessing roughly shows that there is huge similarity in words of question_body in both train and test data.\n\n* Just like keywords in question_title, here also  most of the keywords are in the contex of programing. So while doing preprocessing, we need to keep this in mind that we do not remove programing syntax from question_body."},{"metadata":{},"cell_type":"markdown","source":"### 5.5.5 Frequency of most popular worlds in question_body of train and test"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Frequency of most popular worlds in question_body of train\nword_frequency_plot(train_df['question_body'], title='train')\n\n# Frequency of most popular worlds in question_body of test_df\nword_frequency_plot(test_df['question_body'], title='test')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* As punctuations are not removed from question_body, most frequent words are either punctuations or programming syntax. So we need to plot this graph again after removing punctuations for accurate word frequency plot."},{"metadata":{},"cell_type":"markdown","source":"## 5.6 EDA : question_user_name feature"},{"metadata":{},"cell_type":"markdown","source":"### 5.6.1 Distribution of question_user_name and number of question asked by user in train and test"},{"metadata":{"trusted":true},"cell_type":"code","source":"# for train_df\nn_count_question_user_name_train=train_df['question_user_name'].value_counts().values\n\n# for test_df\nn_count_question_user_name_test=test_df['question_user_name'].value_counts().values\n\n# plot for Distribution of counts question_user_name in train and test\nplot_bar(n_count_question_user_name_train, n_count_question_user_name_test,feature='question_user_name',x_label=\"Number of questions  \",y_label=\"Counts of \")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.6.2 Which user has asked most number of unique question based on question_title?"},{"metadata":{"trusted":true},"cell_type":"code","source":"# for train_df\nn_user_unique_question_train = train_df.drop_duplicates(subset=['question_title'])['question_user_name'].value_counts()\n\n# for test_df\nn_user_unique_question_test = test_df.drop_duplicates(subset=['question_title'])['question_user_name'].value_counts()\n\n# plot for Which user has most number of unique question based on question_title?\nplot_bar(n_user_unique_question_train.values, n_user_unique_question_test.values,feature='question_user_name',x_label=\"Number of questions  \",y_label=\"Counts of \")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Most users has asked only one question in both train and test dataset.\n* Few users has asked more than 1 questions (upto 9) in the train and (upto 5) in te test"},{"metadata":{},"cell_type":"markdown","source":"#### Top 10 user who has asked most number of unique question"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Top 10 user who has asked most number of unique question\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 6))\n\nsns.barplot(n_user_unique_question_train[:10].values,n_user_unique_question_train[:10].index,ax=ax1)\nax1.set(xlabel = \"number of unique question\", ylabel=f\"top 10  question_user_name\", title='train_df\\n')\nax1.grid()\n\n\nsns.barplot(n_user_unique_question_test[:10].values,n_user_unique_question_test[:10].index , ax=ax2)\nax2.set(xlabel = \"number of unique question\", ylabel=f\"top 10  question_user_name\", title='test_df\\n')\nax2.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Description of number of unique question asked by user (train)"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_user_unique_question_train.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Description of number of unique question asked by user (test)"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_user_unique_question_test.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.6.3 Common unique question_user_name in train and test (Venn Diagram)"},{"metadata":{},"cell_type":"markdown","source":"refer: https://www.kaggle.com/codename007/start-from-here-quest-complete-eda-fe\n\n* A Venn diagram uses overlapping circles or other shapes to illustrate the logical relationships between two or more sets of items. Often, they serve to graphically organize things, highlighting how the items are similar and different."},{"metadata":{},"cell_type":"markdown","source":"#### Unique number of question_user_name"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Unique number of question_user_name in Train : {len(train_df['question_user_name'].unique())}\")\nprint(f\"Unique number of question_user_name in Test : {len(test_df['question_user_name'].unique())}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Venn plot of  Common unique question_user_name in train and test\n# refer: https://www.kaggle.com/codename007/start-from-here-quest-complete-eda-fe\n\nplt.figure(figsize=(9,5))\n\nvenn2([set(train_df.question_user_name.unique()), set(test_df.question_user_name.unique())], set_labels = ('Train set', 'Test set') )\nplt.title(\"Common question_user_name in training and test data\", fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* There are 37  question_user_name who is present in both train and test data"},{"metadata":{},"cell_type":"markdown","source":"## 5.6.4. User behaviour on question_title\n* How many number of words user usually ask in question_title? \n* What is the minimum number of words in question_title asked by each user?\n* Similary \"max_title_len\" , \"mean_title_len\" , \"median_title_len\" ?"},{"metadata":{},"cell_type":"markdown","source":"#### For train_df"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finding the unique question users asked unique questions based on question_title\nunique_question_user_with_unique_questions = pd.DataFrame(train_df.groupby(['question_user_name',\"question_title\"])[\"question_title\"].unique())\nunique_question_user_with_unique_questions.reset_index(level=1,drop=True,inplace=True)\nunique_question_user_with_unique_questions.reset_index(inplace=True)\n\n# Find the number of words in each question_title asked by unique question_user\nnumber_of_words = unique_question_user_with_unique_questions['question_title'].apply(lambda x : len(x[0].split()))\nunique_question_user_with_unique_questions[\"number_of_words\"] = number_of_words\n\n# Apply aggrigation function( ) to find \"sum\",\"min\",\"max\",\"mean\",\"median\" of question_title asked by each unique question_user\nunique_question_user_with_unique_questions_title_agg_train = unique_question_user_with_unique_questions.groupby('question_user_name')['number_of_words'].agg([\"sum\",\"min\",\"max\",\"mean\",\"median\"])\nunique_question_user_with_unique_questions_title_agg_train = unique_question_user_with_unique_questions_title_agg_train.reset_index(level=0)\n\n# Renaming column names\nunique_question_user_with_unique_questions_title_agg_train.rename({'sum': 'sum_title_len', 'min': 'min_title_len', 'max': 'max_title_len', 'mean': 'mean_title_len', 'median': \"median_title_len\"}, axis=1, inplace=True)\n\n# merging to train dataframe\ntrain_df = pd.merge(left=train_df ,right=unique_question_user_with_unique_questions_title_agg_train ,how='inner',on=\"question_user_name\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### for test_df\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finding the unique question users asked unique questions based on question_title\nunique_question_user_with_unique_questions = pd.DataFrame(test_df.groupby(['question_user_name',\"question_title\"])[\"question_title\"].unique())\nunique_question_user_with_unique_questions.reset_index(level=1,drop=True,inplace=True)\nunique_question_user_with_unique_questions.reset_index(inplace=True)\n\n# Find the number of words in each question_title asked by unique question_user\nnumber_of_words = unique_question_user_with_unique_questions['question_title'].apply(lambda x : len(x[0].split()))\nunique_question_user_with_unique_questions[\"number_of_words\"] = number_of_words\n\n# Apply aggrigation function( ) to find \"sum\",\"min\",\"max\",\"mean\",\"median\" of question_title asked by each unique question_user\nunique_question_user_with_unique_questions_title_agg_test = unique_question_user_with_unique_questions.groupby('question_user_name')['number_of_words'].agg([\"sum\",\"min\",\"max\",\"mean\",\"median\"])\nunique_question_user_with_unique_questions_title_agg_test = unique_question_user_with_unique_questions_title_agg_test.reset_index(level=0)\n\n# Renaming column names\nunique_question_user_with_unique_questions_title_agg_test.rename({'sum': 'sum_title_len', 'min': 'min_title_len', 'max': 'max_title_len', 'mean': 'mean_title_len', 'median': \"median_title_len\"}, axis=1, inplace=True)\n\n# merging to test dataframe\ntest_df = pd.merge(left=test_df ,right=unique_question_user_with_unique_questions_title_agg_test ,how='inner',on=\"question_user_name\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Ploting user behaviour based on queston_titile"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Ploting user behaviour\nuser_behaviour_column_on_queston_titile = ['sum_title_len', 'min_title_len', 'max_title_len', 'mean_title_len','median_title_len']\n\nfor idx,column in enumerate(user_behaviour_column_on_queston_titile):\n    \n    train_set = unique_question_user_with_unique_questions_title_agg_train.sort_values(by= column ,ascending=False)[column]\n    test_set = unique_question_user_with_unique_questions_title_agg_test.sort_values(by= column ,ascending=False)[column]\n    \n    print(f\"\\n{idx+1}: Plot for {column}\")\n    \n    # Ploting lineplot for trainset\n    f, (ax1, ax2 , ax3 , ax4 ) = plt.subplots(1, 4, figsize=(24,5))\n    \n    sns.lineplot(np.arange(len(train_set)), train_set,ax=ax1, color = \"darkred\")\n    ax1.set(xlabel = \"Count of unique_users\", ylabel=f\"{column}\", title='train_df\\n')\n    ax1.grid()\n\n    # Ploting lineplot for testset\n    sns.lineplot(np.arange(len(test_set)), test_set,ax=ax2 ,color = \"orangered\")\n    ax2.set(xlabel = \"Count of unique_users\", ylabel=f\"{column}\", title='test_df\\n')\n    ax2.grid()\n    \n    # Ploting distplot for trainset\n    sns.distplot(train_set,ax=ax3, color = \"darkred\")\n    ax3.set(xlabel = \"Count of unique_users\", ylabel=f\"{column}\", title='train_df\\n')\n    ax3.grid()\n\n    # Ploting distplot for testset\n    sns.distplot( test_set, ax=ax4 ,color = \"orangered\")\n    ax4.set(xlabel = \"Count of unique_users\", ylabel=f\"{column}\", title='test_df\\n')\n    ax4.grid()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* All of the plots are either sligtly or heavely skewed toward right \n* All these plots shows explicit behaviour of question_user_name on question_title and some of the plots even looks alike pareto distribution(80-20 rule), e.g. `Plot for sum_title_len`.\n* Sum of user behaviour on queston_titile plots also looks like they follows log normal distribution.\n* Let's Transfrom those features and check weather transformed feature follows Noraml distribution using Q-Q Plot.  "},{"metadata":{},"cell_type":"markdown","source":"#### 5.6.4.1  Q-Q Plot of Box Cox transformed user_behaviour_column_on_queston_titile features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Q-Q Plot of Box Cox transformed user_behaviour_column_on_queston_titile features\nfor idx,column_name in enumerate(user_behaviour_column_on_queston_titile):\n    \n    # Box Cox Transform\n    boxcox_transformed_feature_train = boxcox(train_df[column_name])[0]\n    boxcox_transformed_feature_test = boxcox(test_df[column_name])[0]\n    \n    # Saving the transformed user_behaviour_column_on_queston_titile column in dataframe\n    train_df[f\"boxcox_transformed_{column_name}\"] = boxcox_transformed_feature_train\n    test_df[f\"boxcox_transformed_{column_name}\"] = boxcox_transformed_feature_test\n    \n    print(f\"\\n{idx+1}: Q-Q Plot for box cox transformed feature of {column_name}\\n\")\n    \n    # Q-Q Plot of transformed Feature\n    q_q_plot(boxcox_transformed_feature_train, boxcox_transformed_feature_test, f\"box cox transformed {column_name}\")\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Looking At above plots, It is not clear exactly that transformed features follows normal distribution or not.\n* Let's just keep box cox transformed features and test these features importance while modeling"},{"metadata":{},"cell_type":"markdown","source":"## 5.6.5. Question User behaviour on question_body\n* How many number of words user usually ask in question_body? \n* What is the minimum number of words in question_title asked by each user?\n* Similary \"max_title_len\" , \"mean_title_len\" , \"median_title_len\" ?"},{"metadata":{},"cell_type":"markdown","source":"#### For train_df"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finding the unique question users asked unique questions based on question_title\nunique_question_user_with_unique_questions = pd.DataFrame(train_df.groupby(['question_user_name',\"question_title\"])[\"question_body\"].unique())\nunique_question_user_with_unique_questions.reset_index(level=1,drop=True,inplace=True)\nunique_question_user_with_unique_questions.reset_index(inplace=True)\n\n# Find the number of words in each question_title asked by unique question_user\nnumber_of_words = unique_question_user_with_unique_questions['question_body'].apply(lambda x : len(x[0].split()))\nunique_question_user_with_unique_questions[\"number_of_words\"] = number_of_words\n\n# Apply aggrigation function( ) to find \"sum\",\"min\",\"max\",\"mean\",\"median\" of question_title asked by each unique question_user\nunique_question_user_with_unique_questions_body_agg_train = unique_question_user_with_unique_questions.groupby('question_user_name')['number_of_words'].agg([\"sum\",\"min\",\"max\",\"mean\",\"median\"])\nunique_question_user_with_unique_questions_body_agg_train = unique_question_user_with_unique_questions_body_agg_train.reset_index(level=0)\n\n# Renaming column names\nunique_question_user_with_unique_questions_body_agg_train.rename({'sum': 'sum_body_len', 'min': 'min_body_len', 'max': 'max_body_len', 'mean': 'mean_body_len', 'median': \"median_body_len\"}, axis=1, inplace=True)\n\n# merging to train dataframe\ntrain_df = pd.merge(left=train_df ,right=unique_question_user_with_unique_questions_body_agg_train ,how='inner',on=\"question_user_name\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### for test_df"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finding the unique question users asked unique questions based on question_title\nunique_question_user_with_unique_questions = pd.DataFrame(test_df.groupby(['question_user_name',\"question_title\"])[\"question_body\"].unique())\nunique_question_user_with_unique_questions.reset_index(level=1,drop=True,inplace=True)\nunique_question_user_with_unique_questions.reset_index(inplace=True)\n\n# Find the number of words in each question_title asked by unique question_user\nnumber_of_words = unique_question_user_with_unique_questions['question_body'].apply(lambda x : len(x[0].split()))\nunique_question_user_with_unique_questions[\"number_of_words\"] = number_of_words\n\n# Apply aggrigation function( ) to find \"sum\",\"min\",\"max\",\"mean\",\"median\" of question_title asked by each unique question_user\nunique_question_user_with_unique_questions_body_agg_test = unique_question_user_with_unique_questions.groupby('question_user_name')['number_of_words'].agg([\"sum\",\"min\",\"max\",\"mean\",\"median\"])\nunique_question_user_with_unique_questions_body_agg_test = unique_question_user_with_unique_questions_body_agg_test.reset_index(level=0)\n\n# Renaming column names\nunique_question_user_with_unique_questions_body_agg_test.rename({'sum': 'sum_body_len', 'min': 'min_body_len', 'max': 'max_body_len', 'mean': 'mean_body_len', 'median': \"median_body_len\"}, axis=1, inplace=True)\n\n# merging to test dataframe\ntest_df = pd.merge(left=test_df ,right=unique_question_user_with_unique_questions_body_agg_test ,how='inner',on=\"question_user_name\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_question_user_with_unique_questions_body_agg_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Ploting user behaviour based on queston_body"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Ploting user behaviour\nuser_behaviour_column_on_queston_body = ['sum_body_len', 'min_body_len', 'max_body_len', 'mean_body_len','median_body_len']\n\nfor idx,column in enumerate(user_behaviour_column_on_queston_body):\n    \n    train_set = unique_question_user_with_unique_questions_body_agg_train.sort_values(by= column ,ascending=False)[column]\n    test_set = unique_question_user_with_unique_questions_body_agg_test.sort_values(by= column ,ascending=False)[column]\n    \n    print(f\"\\n{idx+1}: Plot for {column}\")\n    \n    # Ploting lineplot for trainset\n    f, (ax1, ax2 , ax3 , ax4 ) = plt.subplots(1, 4, figsize=(24,5))\n    \n    sns.lineplot(np.arange(len(train_set)), train_set,ax=ax1, color = \"darkred\")\n    ax1.set(xlabel = \"Count of unique_users\", ylabel=f\"{column}\", title='train_df\\n')\n    ax1.grid()\n\n    # Ploting lineplot for testset\n    sns.lineplot(np.arange(len(test_set)), test_set,ax=ax2 ,color = \"orangered\")\n    ax2.set(xlabel = \"Count of unique_users\", ylabel=f\"{column}\", title='test_df\\n')\n    ax2.grid()\n    \n    # Ploting distplot for trainset\n    sns.distplot(train_set,ax=ax3, color = \"darkred\")\n    ax3.set(xlabel = \"Count of unique_users\", ylabel=f\"{column}\", title='train_df\\n')\n    ax3.grid()\n\n    # Ploting distplot for testset\n    sns.distplot( test_set, ax=ax4 ,color = \"orangered\")\n    ax4.set(xlabel = \"Count of unique_users\", ylabel=f\"{column}\", title='test_df\\n')\n    ax4.grid()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* All the plots of users behaviour on queston_body are heavely skewed toward right.\n* Transfrom these log normal alike features using box cox and check weather transformed features follows Noraml distribution or not using Q-Q plot"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Q-Q Plot of Box Cox transformed user_behaviour_column_on_queston_body features\nfor idx,column_name in enumerate(user_behaviour_column_on_queston_body):\n    \n    print(f\"Feature name: {column_name}\")\n    \n    # Continues the loop if box cox fails to transform\n    if sum(train_df[f'{column_name}']<1)>0:\n        print(f\"{idx+1}: Box Cox Transformation can not be applied on feature '{column_name}' because all the values of data must be positive for transformation\\n\\n\")\n        continue\n\n    # Box Cox Transform\n    boxcox_transformed_feature_train = boxcox(train_df[column_name])[0]\n    boxcox_transformed_feature_test = boxcox(test_df[column_name])[0]\n    \n             \n    # Saving the transformed user_behaviour_column_on_queston_body column in dataframe\n    train_df[f\"boxcox_transformed_{column_name}\"] = boxcox_transformed_feature_train\n    test_df[f\"boxcox_transformed_{column_name}\"] = boxcox_transformed_feature_test\n    \n    print(f\"{idx+1}: Q-Q Plot for box cox transformed feature of {column_name}\\n\")\n    \n    # Q-Q Plot of transformed Feature\n    q_q_plot(boxcox_transformed_feature_train, boxcox_transformed_feature_test, f\"box cox transformed {column_name}\")\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Most of these Q-Q Plots do looks alike tey follows normal distribution. So just keep these transformed feature"},{"metadata":{},"cell_type":"markdown","source":"## 5.7. EDA : answer Feature"},{"metadata":{},"cell_type":"markdown","source":"### 5.7.1. Any same or repeated answer in train and test"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_repeated_answer_train = sum(train_df['answer'].value_counts().values>1)\nn_repeated_answer_test = sum(test_df['answer'].value_counts().values>1)\n\nprint(f\"Number of repeated answer in train: {n_repeated_answer_train}\")\nprint(f\"Number of repeated answer in test: {n_repeated_answer_test}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.7.2. Length of answer in train and test "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Length of answer in train\nlen_answer_train = sorted(train_df['answer'].apply(lambda x: len(x)),reverse=True)\n\n# Length of answer in test\nlen_answer_test = sorted(test_df['answer'].apply(lambda x: len(x)),reverse=True)\n\n# plot for train_df\nplot_sns(len_answer_train,\"answer\",color='darkblue',title='length',subtitle='train_df')\n\n# plot for test_df\nplot_sns(len_answer_test,\"answer\",color='lightblue',title='length',subtitle='test_df')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 5.7.2.1 Box plot of length of answers"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Box plot of length of question_body\nbox_plot(len_answer_train, len_answer_test, \"length of answer \" )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The length of most of answer in train and test is less than 2000 \n* the distribution length of answer is highly skewed towards right indicates that there are few answer which has very large length( > 2000) . This could be becuase of reason that programming contex question has also the answer in coding format and having large set of syntax.\n* It looks like length of answer follows log normal distribution. If yes, we can transform it to nomal distribution using Box-Cox transform and apply linear model on top of transformed feature for better result.  Let's Find it out."},{"metadata":{},"cell_type":"markdown","source":"#### 5.7.2.2 Box Cox transform of length of answer and Q-Q plot of transformed feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"box_cox_len_answer_train = boxcox(len_answer_train)[0]\nbox_cox_len_answer_test =  boxcox(len_answer_test)[0]\n\ntrain_df['box_cox_len_answer'] = box_cox_len_answer_train\ntest_df['box_cox_len_answer'] = box_cox_len_answer_test\n\n# Checking weather box cox transformed len_question_body_box_cox follows normal distribution or not using Q-Q plot\nq_q_plot(box_cox_len_question_body_train, box_cox_len_question_body_test, \"box-cox transformed length of answer  \")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Yes, It looks that Box Cox transfromed feature of len_answer follows normal distribution\n"},{"metadata":{},"cell_type":"markdown","source":"### 5.7.3. Number of words in answer in train and test "},{"metadata":{"trusted":true},"cell_type":"code","source":"# number of words in answer in train\nn_words_in_answer_train = sorted(train_df['answer'].apply(lambda x: len(x.split(\" \"))),reverse=True)\n\n# number of words in answer in test\nn_words_in_answer_test = sorted(test_df['answer'].apply(lambda x: len(x.split(\" \"))),reverse=True)\n\n# plot for train_df\nplot_sns(n_words_in_answer_train,\"answer\",color='darkred',title='number',subtitle='train_df')\n\n# plot for test_df\nplot_sns(n_words_in_answer_test,\"answer\",color='orangered',title='number',subtitle='test_df')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 5.7.3.1. Box plot of number of words in question_body"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Box plot of length of question_body\nbox_plot(n_words_in_answer_train, n_words_in_answer_test, \"number of words in answer\" )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The number of words in answer in train and test is less than 400 \n\n* the distribution number of words in answer is highly skewed towards right indicates that there are few question_body which has very high number of words in answer greater than 400 and upto 8000 words in train and upto 2500 in test.\n\n* This could be because of some questions are from programming contex and the syntax of words maybe counted as indiviaudal word.\n\n* It also looks like number of words in question_body follows log normal distribution. If yes, we can transform it to nomal distribution using Box-Cox transform and apply linear model on top of transformed feature for better result. "},{"metadata":{},"cell_type":"markdown","source":"#### 5.7.3.2 Box Cox transform of number of words in answer and Q-Q plot of transformed feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"box_cox_n_words_in_answer_train = boxcox(n_words_in_answer_train)[0]\nbox_cox_n_words_in_answer_test =  boxcox(n_words_in_answer_test)[0]\n\ntrain_df['box_cox_n_words_in_answer'] = box_cox_n_words_in_answer_train\ntest_df['box_cox_n_words_in_answer'] = box_cox_n_words_in_answer_test\n\n# Checking weather box cox transformed len_question_body_box_cox follows normal distribution or not using Q-Q plot\nq_q_plot(box_cox_n_words_in_answer_train, box_cox_n_words_in_answer_test, \"box-cox transformed of n_words_in_answer \")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* It looks like box-cox transformed of n_words_in_answer follows normal distribution. So keep this feature and test this feature importance while modeling  "},{"metadata":{},"cell_type":"markdown","source":"### 5.7.4. WordCloud of answer in train and test "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# refer: https://www.datacamp.com/community/tutorials/wordcloud-python\n\n# For train_df\ntext_train = \" \".join(word for word in train_df['answer'])\n\n# Create and generate a word cloud image:\nwordcloud = WordCloud().generate(text_train)\n\n# Display the generated image:\nplt.figure(figsize=(9,6))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.title(\"WordCloud of answer in train\\n\")\nplt.axis(\"off\")\nplt.show()\n\n#===================================================================\n\n# For test_df\ntext_test = \" \".join(word for word in test_df['answer'])\n\n# Create and generate a word cloud image:\nwordcloud = WordCloud().generate(text_test)\n\n# Display the generated image:\nplt.figure(figsize=(9,6))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.title(\"WordCloud of answer in test\")\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Wordclouds of words in answer without preprocessing roughly shows that there is huge similarity in words of question_body in both train and test data.\n\n* Just like keywords in question_body, here also  most of the keywords are in the contex of programing. So while doing preprocessing, we need to keep this in mind that we do not remove programing syntax from answer."},{"metadata":{},"cell_type":"markdown","source":"### 5.7.5. Frequency of most popular worlds in answer of train and test"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Frequency of most popular worlds in answer of train\nword_frequency_plot(train_df['answer'], title='train')\n\n# Frequency of most popular worlds in answer of test_df\nword_frequency_plot(test_df['answer'], title='test')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* As punctuations are not removed from anser, most frequent words are either punctuations or programming syntax. So we need to plot this graph again after removing punctuations for accurate word frequency plot."},{"metadata":{},"cell_type":"markdown","source":"## 5.8. EDA : answer_user_name feature"},{"metadata":{},"cell_type":"markdown","source":"### 5.8.1. Distribution of answer_user and number of answer answerd by user in train and test"},{"metadata":{},"cell_type":"markdown","source":"#### 5.8.1.1. Number of unique answer_user in train and test"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of unique answer_user in train and test\nprint(f'Number of unique answer_user in train: {len(train_df[\"answer_user_name\"].unique())}')\nprint(f'Number of unique answer_user in test: {len(test_df[\"answer_user_name\"].unique())}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 5.8.1.2. Common answer_user in both train and test"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(9,5))\nvenn2([set(train_df.answer_user_name.unique()), set(test_df.answer_user_name.unique())], set_labels = ('Train set', 'Test set') )\nplt.title(\"Common  answer_user in both train and test\", fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* There are 29 answer_user_name which is common in both train and test."},{"metadata":{},"cell_type":"markdown","source":"#### 5.8.1.3. Number of answer given by users"},{"metadata":{"trusted":true},"cell_type":"code","source":"# for train_df\nn_count_answer_user_name_train=train_df['answer_user_name'].value_counts().values\n\n# for test_df\nn_count_answer_user_name_test=test_df['answer_user_name'].value_counts().values\n\n# plot for Distribution of counts answer_user_name in train and test\nplot_bar(n_count_answer_user_name_train, n_count_answer_user_name_test,feature='answer_user_name',x_label=\"Number of answer  \",y_label=\"Counts of \")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.8.2. Number of users who has answered most number of unique question?"},{"metadata":{"trusted":true},"cell_type":"code","source":"# for train_df\nn_user_unique_answer_train = train_df.drop_duplicates(subset=['question_title'])['answer_user_name'].value_counts()\n\n# for test_df\nn_user_unique_answer_test = test_df.drop_duplicates(subset=['question_title'])['answer_user_name'].value_counts()\n\n# plot of Which user has answered most number of unique question based on question_title?\nplot_bar(n_user_unique_answer_train.values, n_user_unique_answer_test.values,feature='answer_user_name',x_label=\"Number of questions  \",y_label=\"Counts of \")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* This plot and above this plot indicates that there are few users in dataset who has answered the same question more than once but answer is different.\n\n* Just like we have seen in question_users, here also mostly answer_users has answerd only on question.\n \n* And very few answer_users has answerd large number of questions upto 15 in train data and upto 6 in test data ."},{"metadata":{},"cell_type":"markdown","source":"#### Lets find out those users  who has answered the same question more than once but having different answer (train_df)?"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Users who has answered the same question more than once but differently\nuser_answered_same_ques_twice = pd.DataFrame(train_df.groupby(['question_title','answer_user_name'])['answer_user_name'].agg(['count'])).sort_values(by='count',ascending=False)\nuser_answered_same_ques_twice = user_answered_same_ques_twice.reset_index(level=1)\nuser_answered_same_ques_twice.head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 5.8.2.2. Number of unique users who has answerd the same question more than once?"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f' Number of unique users who has answerd the same question more than once: {sum(user_answered_same_ques_twice[\"count\"]>1)} out of {len(train_df[\"answer_user_name\"].unique())}\\\n ({round(sum(user_answered_same_ques_twice[\"count\"]>1) / len(train_df[\"answer_user_name\"].unique()),4)})%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 5.8.2.3. Top 10 user who has answered most number of unique question"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Top 10 user who has asked most number of unique question\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 6))\n\nsns.barplot(n_user_unique_answer_train[:10].values,n_user_unique_answer_train[:10].index,ax=ax1)\nax1.set(xlabel = \"number of unique question\", ylabel=f\"top 10 answer_user_name\", title='train_df\\n')\nax1.grid()\n\n\nsns.barplot(n_user_unique_answer_test[:10].values,n_user_unique_answer_test[:10].index , ax=ax2)\nax2.set(xlabel = \"number of unique question\", ylabel=f\"top 10  answer_user_name\", title='test_df\\n')\nax2.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4.8.2.4. Description of number of unique question asked by user (train)"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_user_unique_answer_train.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4.8.2.5. Description of number of unique question asked by user (test)"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_user_unique_answer_test.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observation\n\n* \n"},{"metadata":{},"cell_type":"markdown","source":"## 5.8.4 answer_user behaviour based on answer body\n* How many number of words user usually answered in an answer? \n* What is the minimum number of words in answer answered by each user?\n* Similary \"max_title_len\" , \"mean_title_len\" , \"median_title_len\" ?"},{"metadata":{},"cell_type":"markdown","source":"#### For train_df"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find the number of words in each answer \ntemp = train_df[['answer_user_name','answer']]\nnumber_of_words = temp['answer'].apply(lambda x : len(x[0].split()))\ntemp[\"number_of_words\"] = number_of_words\n\n# Apply aggrigation function( ) to find \"sum\",\"min\",\"max\",\"mean\",\"median\" of answer answered by each unique answer_user\nanswer_user_agg_behaviour_train = temp.groupby('answer_user_name')['number_of_words'].agg([\"sum\",\"min\",\"max\",\"mean\",\"median\"])\nanswer_user_agg_behaviour_train = answer_user_agg_behaviour_train.reset_index(level=0)\n\n# Renaming column names\nanswer_user_agg_behaviour_train.rename({'sum': 'sum_answer_len', 'min': 'min_answer_len', 'max': 'max_answer_len', 'mean': 'mean_answer_len', 'median': \"median_answer_len\"}, axis=1, inplace=True)\n\n# merging to train dataframe\ntrain_df = pd.merge(left=train_df ,right=answer_user_agg_behaviour_train ,how='inner',on=\"answer_user_name\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### for test_df\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find the number of words in each answer \ntemp = test_df[['answer_user_name','answer']]\nnumber_of_words = temp['answer'].apply(lambda x : len(x[0].split()))\ntemp[\"number_of_words\"] = number_of_words\n\n# Apply aggrigation function( ) to find \"sum\",\"min\",\"max\",\"mean\",\"median\" of answer answered by each unique answer_user\nanswer_user_agg_behaviour_test = temp.groupby('answer_user_name')['number_of_words'].agg([\"sum\",\"min\",\"max\",\"mean\",\"median\"])\nanswer_user_agg_behaviour_test = answer_user_agg_behaviour_test.reset_index(level=0)\n\n# Renaming column names\nanswer_user_agg_behaviour_test.rename({'sum': 'sum_answer_len', 'min': 'min_answer_len', 'max': 'max_answer_len', 'mean': 'mean_answer_len', 'median': \"median_answer_len\"}, axis=1, inplace=True)\n\n# merging to train dataframe\ntest_df = pd.merge(left=test_df ,right=answer_user_agg_behaviour_test ,how='inner',on=\"answer_user_name\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Ploting answer_user behaviour based on answer"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Ploting user behaviour\nuser_behaviour_column_on_answer = ['sum_answer_len', 'sum_answer_len', 'min_answer_len', 'mean_answer_len','median_answer_len']\n\nfor idx,column in enumerate(user_behaviour_column_on_answer):\n    \n    train_set = answer_user_agg_behaviour_train.sort_values(by= column ,ascending=False)[column]\n    test_set = answer_user_agg_behaviour_test.sort_values(by= column ,ascending=False)[column]\n    \n    print(f\"\\n{idx+1}: Plot for {column}\")\n    \n    # Ploting lineplot for trainset\n    f, (ax1, ax2 , ax3 , ax4 ) = plt.subplots(1, 4, figsize=(24,5))\n    \n    sns.lineplot(np.arange(len(train_set)), train_set,ax=ax1, color = \"darkred\")\n    ax1.set(xlabel = \"Count of unique_users\", ylabel=f\"{column}\", title='train_df\\n')\n    ax1.grid()\n\n    # Ploting lineplot for testset\n    sns.lineplot(np.arange(len(test_set)), test_set,ax=ax2 ,color = \"orangered\")\n    ax2.set(xlabel = \"Count of unique_users\", ylabel=f\"{column}\", title='test_df\\n')\n    ax2.grid()\n    \n    # Ploting distplot for trainset\n    sns.distplot(train_set,ax=ax3, color = \"darkred\")\n    ax3.set(xlabel = \"Count of unique_users\", ylabel=f\"{column}\", title='train_df\\n')\n    ax3.grid()\n\n    # Ploting distplot for testset\n    sns.distplot( test_set, ax=ax4 ,color = \"orangered\")\n    ax4.set(xlabel = \"Count of unique_users\", ylabel=f\"{column}\", title='test_df\\n')\n    ax4.grid()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Few of the plots of users behaviour on answer are heavely skewed toward right.\n* Top two plots also very similar to log normal distribution\n* Transfrom these log normal alike features using box cox and check weather transformed features follows Noraml distribution or not using Q-Q plot"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Q-Q Plot of Box Cox transformed user_behaviour_column_on_answer features\nfor idx,column_name in enumerate(user_behaviour_column_on_answer[:2]):\n    \n    print(f\"Feature name: {column_name}\")\n    \n    # Continues the loop if box cox fails to transform\n    if sum(train_df[f'{column_name}']<1)>0:\n        print(f\"{idx+1}: Box Cox Transformation can not be applied on feature '{column_name}' because all the values of data must be positive for transformation\\n\\n\")\n        continue\n\n    # Box Cox Transform\n    boxcox_transformed_feature_train = boxcox(train_df[column_name])[0]\n    boxcox_transformed_feature_test = boxcox(test_df[column_name])[0]\n    \n             \n    # Saving the transformed user_behaviour_column_on_answer column in dataframe\n    train_df[f\"boxcox_transformed_{column_name}\"] = boxcox_transformed_feature_train\n    test_df[f\"boxcox_transformed_{column_name}\"] = boxcox_transformed_feature_test\n    \n    print(f\"{idx+1}: Q-Q Plot for box cox transformed feature of {column_name}\\n\")\n    \n    # Q-Q Plot of transformed Feature\n    q_q_plot(boxcox_transformed_feature_train, boxcox_transformed_feature_test, f\"box cox transformed {column_name}\")\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Guess we cannot tranform these user behaviour features using box cox thus we cannot plot Q-Q plot. Therefore cannot use these transformed featues"},{"metadata":{},"cell_type":"markdown","source":"## 5.8.8. Common users who has asked the question and answered by himself in train_df and test_df"},{"metadata":{},"cell_type":"markdown","source":"#### Common users who has asked question and answered by himself in train_df"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Venn diagram for train_df\nplt.figure(figsize=(16,8))\nplt.subplot(211)\nvenn2([set(train_df.question_user_name.unique()), set(train_df.answer_user_name.unique())], set_labels = ('question_user set', 'answer_user set') )\nplt.title(\"Common users who has asked the question and answered by himself in train data\", fontsize=15)\n\n# Venn diagram for test_df\nplt.subplot(212)\nvenn2([set(test_df.question_user_name.unique()), set(test_df.answer_user_name.unique())], set_labels = ('question_user set', 'answer_user set') )\nplt.title(\"Common users who has asked the question and answered by himself in test data\", fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* There are few users who like to ask the question and answer it by self. Can be clearly seen in above plot."},{"metadata":{},"cell_type":"markdown","source":"## 5.9. EDA: category Feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"# unique categories\nprint(train_df['category'].unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Distribution For train_df"},{"metadata":{"trusted":true},"cell_type":"code","source":"# For train_df\nprint(f\"Unique number of category: {len(train_df['category'].unique())}\\n\")\n\ncategory_dist_df_train = pd.DataFrame(train_df['category'].unique(),columns=['category'])\ncategory_dist_df_train[\"values_count\"] = train_df['category'].value_counts().values\ncategory_dist_df_train[\"distribution\"] = train_df['category'].value_counts().values/sum(train_df['category'].value_counts().values)\ncategory_dist_df_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Distribution For test_df"},{"metadata":{"trusted":true},"cell_type":"code","source":"# For test_df\nprint(f\"Unique number of category: {len(test_df['category'].unique())}\\n\")\n\ncategory_dist_df_test = pd.DataFrame(test_df['category'].unique(),columns=['category'])\ncategory_dist_df_test[\"values_count\"] = test_df['category'].value_counts().values\ncategory_dist_df_test[\"distribution\"] = test_df['category'].value_counts().values/sum(test_df['category'].value_counts().values)\ncategory_dist_df_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plot (pie chart) for category distribution in train and test"},{"metadata":{"trusted":true},"cell_type":"code","source":"f, (ax1, ax2  ) = plt.subplots(1, 2, figsize=(24,7))\n\n# Categories dist for train_df\nax1.pie(category_dist_df_train.values_count, labels=category_dist_df_train.category, shadow=True, autopct='%.1f%%')\nax1.set( title='Categories distribution: train\\n')\n\n\n# Categories dist for test_df\nax2.pie(category_dist_df_test.values_count, labels=category_dist_df_test.category, shadow=True, autopct='%.1f%%')\nax2.set(title='Categories distribution: test\\n')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The distribution of Categories is not similar in train set and test set\n* LIGE_ARTS has majority of distribution in train but CULTURE category holds majority of distribution in test set."},{"metadata":{},"cell_type":"markdown","source":"## 4.10. EDA: Host featue"},{"metadata":{},"cell_type":"markdown","source":"### 4.10.1. Common number of Host in train and test"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Venn diagram for train_df\nplt.figure(figsize=(16,8))\nplt.subplot(111)\nvenn2([set(train_df.host.unique()), set(test_df.host.unique())], set_labels = ('Train set', 'Test set') )\nplt.title(\"Common number of Host in train and test\", fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Most of Categories is common in both train and test set."},{"metadata":{},"cell_type":"markdown","source":"### 5.10.2. Distribution of host in train and test"},{"metadata":{"trusted":true},"cell_type":"code","source":"# For train_df\nhost_dist_df_train = pd.DataFrame(train_df['host'].value_counts().index,columns=['host'])\nhost_dist_df_train[\"values_count\"] = train_df['host'].value_counts().values\nhost_dist_df_train[\"distribution\"] = train_df['host'].value_counts().values/sum(train_df['category'].value_counts().values)\n\n# For test_df\nhost_dist_df_test = pd.DataFrame(test_df['host'].value_counts().index,columns=['host'])\nhost_dist_df_test[\"values_count\"] = test_df['host'].value_counts().values\nhost_dist_df_test[\"distribution\"] = test_df['host'].value_counts().values/sum(train_df['category'].value_counts().values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Plot for train_df"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Unique number of host in train: {len(train_df['host'].unique())}\\n\")\n\n# plot for distribution of host in train\nplt.figure(figsize=(28,8))\nsns.barplot(x=host_dist_df_train['host'], y=host_dist_df_train['distribution'])\nplt.title(\"Host distribution: Train\\n\")\nplt.xticks(rotation=85)\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Plot for test_df"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Unique number of host in train: {len(test_df['host'].unique())}\\n\")\n\n# plot for distribution of host in train\nplt.figure(figsize=(28,8))\nsns.barplot(x=host_dist_df_test['host'], y=host_dist_df_test['distribution'])\nplt.title(\"Host distribution: Test\\n\")\nplt.xticks(rotation=85)\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Host stackoverflow.com has majority of distribution in both train set and test set. This could be the reason why most of the keywords in WordClouds has programing syntax."},{"metadata":{},"cell_type":"markdown","source":"### 5.10.3. Top 10 host where number of users  has asked the question and answered by himself"},{"metadata":{},"cell_type":"markdown","source":"#### For train_df"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_self_question_answer_users_df_train = pd.DataFrame(columns=['n_of_self_question_answer_users'])\n\nfor idx, host_name in enumerate(train_df['host'].unique()):\n    ques_user =set(train_df[train_df['host']==host_name]['question_user_name'])\n    ans_user =set(train_df[train_df['host']==host_name]['answer_user_name'])\n    n_self_question_answer_users = len(ques_user.intersection(ans_user))\n    n_self_question_answer_users_df_train.loc[host_name] = n_self_question_answer_users\n\nn_self_question_answer_users_df_train.sort_values(by ='n_of_self_question_answer_users' ,ascending=False,inplace=True)\n\n\n# plot for distribution of top 10 host where number of users  has asked the question and answered by himself\nplt.figure(figsize=(16,7))\nsns.barplot(x = n_self_question_answer_users_df_train.head(10).index, y = n_self_question_answer_users_df_train['n_of_self_question_answer_users'].head(10))\nplt.title(\"Host distribution: Train\\n\")\nplt.xticks(rotation=85)\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### For test_df"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_self_question_answer_users_df_test = pd.DataFrame(columns=['n_of_self_question_answer_users'])\n\nfor idx, host_name in enumerate(test_df['host'].unique()):\n    ques_user =set(test_df[test_df['host']==host_name]['question_user_name'])\n    ans_user =set(test_df[test_df['host']==host_name]['answer_user_name'])\n    n_self_question_answer_users = len(ques_user.intersection(ans_user))\n    n_self_question_answer_users_df_test.loc[host_name] = n_self_question_answer_users\n\nn_self_question_answer_users_df_test.sort_values(by ='n_of_self_question_answer_users' ,ascending=False,inplace=True)\n\n\n# plot for distribution of top 10 host where number of users  has asked the question and answered by himself\nplt.figure(figsize=(16,7))\nsns.barplot(x = n_self_question_answer_users_df_test.head(10).index, y = n_self_question_answer_users_df_test['n_of_self_question_answer_users'].head(10))\nplt.title(\"Host distribution: Test\\n\")\nplt.xticks(rotation=85)\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* From this plot it can be interprated that people tend to self question and answer on the host website 'stackoverflow.com' most.\n\n* `Stack Overflow is a question and answer site for professional and enthusiast programmers`"},{"metadata":{},"cell_type":"markdown","source":"\n\n"},{"metadata":{},"cell_type":"markdown","source":"### Special thanks to this kernal for below plots\nhttps://www.kaggle.com/kabure/qa-eda-and-nlp-modelling-insights-and-data-vis"},{"metadata":{},"cell_type":"markdown","source":"## 5.10.Some More Importand EDA plots"},{"metadata":{},"cell_type":"markdown","source":"### 5.10.1. Number of unique questions by category"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# refer: https://www.kaggle.com/kabure/qa-eda-and-nlp-modelling-insights-and-data-vis\n\nhost = train_df.groupby(['host'])['url'].nunique().sort_values(ascending=False)\ncategory = train_df.groupby(['category'])['url'].nunique().sort_values(ascending=False)\n\nplt.figure(figsize=(14,10))\nplt.suptitle('Unique URL by Host and Categories', size=22)\n\nplt.subplot(211)\ng0 = sns.barplot(x=category.index, y=category.values, color='blue')\ng0.set_title(\"Unique Questions by category\", fontsize=22)\ng0.set_xlabel(\"Category Name\", fontsize=19)\ng0.set_ylabel(\"Total Count\", fontsize=19)\n#g1.set_xticklabels(g1.get_xticklabels(),rotation=45)\nfor p in g0.patches:\n    height = p.get_height()\n    g0.text(p.get_x()+p.get_width()/2.,\n            height + 3,\n            '{:1.1f}%'.format(height/category.sum()*100),\n            ha=\"center\",fontsize=11) \n\nplt.subplot(212)\ng1 = sns.barplot(x=host[:20].index, y=host[:20].values, color='blue')\ng1.set_title(\"TOP 20 HOSTS with more UNIQUE questions\", fontsize=22)\ng1.set_xlabel(\"Host Name\", fontsize=19)\ng1.set_ylabel(\"Total Count\", fontsize=19)\ng1.set_xticklabels(g1.get_xticklabels(),rotation=85)\nfor p in g1.patches:\n    height = p.get_height()\n    g1.text(p.get_x()+p.get_width()/2.,\n            height + 3,\n            '{:1.1f}%'.format(height/host.sum()*100),\n            ha=\"center\",fontsize=11) \n    \nplt.subplots_adjust(hspace = 0.3, top = 0.90)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### First chart: \nThe most common category in Stackexchange fórums are:\n\n1 - Technology(41.8%)\n\n2 - Stackoverflow (21.2%)\n\n3 - Culture (14.7%)\n\n4 - Science (11.2)\n\n5 - Life Arts(10.9)\n\n* It's not so unbalanced, so it can be useful to train the model.\n\n\n#### Second Chart: \n* The most common category is the Stackoverflow with 20.6% of the total open topics(questions);\n\n* After the StackOverflow we can see that only 7 categories have a ratio highest than 2%, we have in total 59;\n\n* The group of Stackoverflow, eletronics, superuser, serverfault, english, math, tex, physics and askubuntu together has almost 46% of the total questions."},{"metadata":{},"cell_type":"markdown","source":"### 5.10.2 Number of users who has asked question and answered by themself (by category) in train"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# refer: https://www.kaggle.com/kabure/qa-eda-and-nlp-modelling-insights-and-data-vis\n\nimport matplotlib.gridspec as gridspec # to do the grid of plots\n\ngrid = gridspec.GridSpec(3, 3)\nplt.figure(figsize=(16,3*4))\n\nplt.suptitle('Intersection QA USERS \\nQuestions and Answers by different CATEGORIES', size=20)\n\nfor n, col in enumerate(train_df['category'].value_counts().index):\n    ax = plt.subplot(grid[n])\n    venn2([set(train_df[train_df.category == col]['question_user_name'].value_counts(dropna=False).index), \n           set(train_df[train_df.category == col]['answer_user_name'].value_counts(dropna=False).index)],\n      set_labels=('Question Users', 'Answer Users'), )\n    ax.set_title(str(col), fontsize=15)\n    ax.set_xlabel('')\n    #plt.subplots_adjust(top = 0.98, wspace=.9, hspace=.9)\n    \nplt.subplots_adjust(top = 0.9, hspace=.1)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.10.3. Number of users who has asked question and answered by themself (by Host) in train"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# refer: https://www.kaggle.com/kabure/qa-eda-and-nlp-modelling-insights-and-data-vis\n\ngrid = gridspec.GridSpec(5, 3)\nplt.figure(figsize=(16,4.5*4))\n\nplt.suptitle('Intersection QA USERS - TOP 15 \\nQuestions and Answers by different HOSTS', size=20)\ntop_host = train_df['host'].value_counts()[:15].index\nfor n, col in enumerate(top_host):\n    ax = plt.subplot(grid[n])\n    venn2([set(train_df[train_df.host == col]['question_user_name'].value_counts(dropna=False).index), \n           set(train_df[train_df.host == col]['answer_user_name'].value_counts(dropna=False).index)],\n      set_labels=('Question Users', 'Answer Users'), )\n    ax.set_title(str(col), fontsize=15)\n    ax.set_xlabel('')\n    #plt.subplots_adjust(top = 0.98, wspace=.9, hspace=.9)\n    \nplt.subplots_adjust(top = 0.9, hspace=.1)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* It's interesting to note that programmers host has almost 1:3 ratio between QA Users, and only one in the intersection.\n\n* Only for curiosity, we can take a look on one some of the intersection users to see what type of questions was done by them"},{"metadata":{},"cell_type":"markdown","source":"### 5.10.4 Title and Body Lenghts of questions by each category"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tokenize each item in the review column\nfrom nltk import word_tokenize\nword_tokens = [word_tokenize(question) for question in train_df.question_body]\n\n# Create an empty list to store the length of the reviews\nlen_tokens = []\n\n# Iterate over the word_tokens list and determine the length of each item\nfor i in range(len(word_tokens)):\n     len_tokens.append(len(word_tokens[i]))\n\n# Create a new feature for the lengh of each review\ntrain_df['question_n_words'] = len_tokens","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"grid = gridspec.GridSpec(5, 3)\nplt.figure(figsize=(16,6*4))\n\nplt.suptitle('Title and Question Lenghts by Different Categories \\nThe Mean in RED - Also 5% and 95% lines', size=20)\ncount=0\ntop_cats = train_df['category'].value_counts().index\nfor n, col in enumerate(top_cats):\n    for i, q_t in enumerate(['question_title', 'question_body', 'question_n_words']):\n        ax = plt.subplot(grid[count])\n        if q_t == 'question_n_words':\n            sns.distplot(train_df[train_df['category'] == col][q_t], bins = 50, \n                         color='g', label=\"RED - 50%\") \n            ax.set_title(f\"Distribution of {str(col)} \\nQuestion #Total Words Distribution\", fontsize=15)\n            ax.axvline(train_df[train_df['category'] == col][q_t].quantile(.95))\n            ax.axvline(train_df[train_df['category'] == col][q_t].quantile(.05))\n            mean_val = train_df[train_df['category'] == col][q_t].mean()\n            ax.axvline(mean_val, color='red' )\n            ax.set_xlabel('')            \n        else:\n            sns.distplot(train_df[train_df['category'] == col][q_t].str.len(), bins = 50, \n                         color='g', label=\"RED - 50%\") \n            ax.set_title(f\"Distribution of {str(col)} \\n{str(q_t)}\", fontsize=15)\n            ax.axvline(train_df[train_df['category'] == col][q_t].str.len().quantile(.95))\n            ax.axvline(train_df[train_df['category'] == col][q_t].str.len().quantile(.05))\n            mean_val = train_df[train_df['category'] == col][q_t].str.len().mean()\n            ax.axvline(mean_val, color='red' )\n            #ax.text(x=mean_val*1.1, y=.02, s='Holiday in US', alpha=0.7, color='#334f8d')\n            ax.set_xlabel('')\n        count+=1\n        \nplt.subplots_adjust(top = 0.90, hspace=.4, wspace=.15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The quantiles of the distributions are very similar in all categories in both title and question lenghts."},{"metadata":{},"cell_type":"markdown","source":"### 5.10.5. PCA visualisation of target labels hue by categories\n\nrefer : https://www.kaggle.com/kabure/qa-eda-and-nlp-modelling-insights-and-data-vis\n\n#### This is to just visualise how important is categorical feature in distintuising taget labels?  (like pseudo feature importance)\n\nApplying Machine learning model and then finding feature importance would definaltely be a better choice nonetheless."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scaling targets or labels with mean=0, and variance=1 (getting targets ready for PCA)\nsc=StandardScaler(with_mean=True)\nscalar_targets = sc.fit_transform(train_df[target_vars])\n\n# Pca fitting and transform\npca = PCA()\npca_component = pca.fit_transform(scalar_targets)\n\n# variance explained by top 2 eigen vector values\nprint(f\"variance explained by top 2 eigen vector values: {round(sum(pca.explained_variance_ratio_[:2]),2)} %\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca__target_component_1 = pca_component[:,0]\npca__target_component_2 = pca_component[:,1]\n\n# Ploting\nplt.figure(figsize=(12,7))\nsns.scatterplot(pca__target_component_1, pca__target_component_2, hue=train_df['category'])\nplt.title(\"Visualisation of pca__target_component_1 V/S pca__target_component_2 \")\nplt.xlabel(\"pca__target_component_1\")\nplt.ylabel(\"pca__target_component_2\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* PCA plot of targets using category feature is definately showing some patterns.\n* Category feature would definately be helpful in distinguish datapoints and regressing target values."},{"metadata":{},"cell_type":"markdown","source":"### 5.10.6. WordClouds of Questions Texts in each Categories"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS\n\nstopwords = set(STOPWORDS)\n\nfor idx, cat in enumerate(train_df['category'].value_counts().index):\n    \n    f, (ax1, ax2 ) = plt.subplots(1, 2, figsize=(16,5))\n    \n    wordcloud_question = WordCloud(\n        background_color='black',\n        stopwords=stopwords,\n        max_words=250,\n        max_font_size=100, \n        width=400, height=280,\n        random_state=42,\n    ).generate(\" \".join(train_df[train_df['category'] == cat]['question_body'].astype(str)))\n    \n    print(f\" {idx+1}: category {cat}\")\n    ax1.imshow(wordcloud_question)\n    ax1.set(title=f'Category: {cat}\\n Question_body')\n    ax1.axis('off')\n    \n    wordcloud_anser = WordCloud(\n        background_color='black',\n        stopwords=stopwords,\n        max_words=250,\n        max_font_size=100, \n        width=400, height=280,\n        random_state=42,\n    ).generate(\" \".join(train_df[train_df['category'] == cat]['answer'].astype(str)))\n    \n    ax2.imshow(wordcloud_anser)\n    ax2.set(title=f'Category: {cat}\\n Answer')\n    ax2.axis('off')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* This plot is showing some good insights. The WordClouds of Questions and Answers is almost similar of each of the categories.\n* This means that if question is in the programing contex or belong to stackoverflow category then answer is also in the same contex. ( Isn't it common sense :) )\n* Similary we can plot Wordclouds of each host as well. but we will plot it after preprocessing the text and revisit all the wordcloud plots again."},{"metadata":{},"cell_type":"markdown","source":"### 5.10.7 Polarity and subjectivity analyses of question_title\n\nBig thangs to this kernal: https://www.kaggle.com/kabure/qa-eda-and-nlp-modelling-insights-and-data-vis"},{"metadata":{"trusted":true},"cell_type":"code","source":"pol = lambda x: TextBlob(x).sentiment.polarity\nsub = lambda x: TextBlob(x).sentiment.subjectivity\n\ntrain_df['q_title_polarity'] = train_df['question_title'].apply(pol)\ntrain_df['q_title_subjectivity'] = train_df['question_title'].apply(sub)\n\ntrain_df[['question_title', 'category', 'q_title_polarity', 'q_title_subjectivity']].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Polarity and subjectivity plot\nplt.figure(figsize=(12,5))\ng = sns.scatterplot(x='q_title_polarity', y='q_title_subjectivity', \n                    data=train_df, hue='category')\ng.set_title(\"Sentiment Analyzis (Polarity x Subjectivity) of question_title by 'Category' Feature\", fontsize=16)\ng.set_xlabel(\"Polarity distribution\",fontsize=18)\ng.set_ylabel(\"Subjective \",fontsize=18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.10.8 Polarity and subjectivity analyses of question_body\n\nrefer: https://www.kaggle.com/kabure/qa-eda-and-nlp-modelling-insights-and-data-vis"},{"metadata":{"trusted":true},"cell_type":"code","source":"pol = lambda x: TextBlob(x).sentiment.polarity\nsub = lambda x: TextBlob(x).sentiment.subjectivity\n\ntrain_df['q_body_polarity'] = train_df['question_body'].apply(pol)\ntrain_df['q_body_subjectivity'] = train_df['question_body'].apply(sub)\n\ntrain_df[['question_body', 'category', 'q_body_polarity', 'q_body_subjectivity']].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Polarity and subjectivity plot\nplt.figure(figsize=(12,5))\ng = sns.scatterplot(x='q_body_polarity', y='q_body_subjectivity', \n                    data=train_df, hue='category')\ng.set_title(\"Sentiment Analyzis (Polarity x Subjectivity) of question_body by 'Category' Feature\", fontsize=16)\ng.set_xlabel(\"Polarity distribution\",fontsize=18)\ng.set_ylabel(\"Subjective \",fontsize=18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.10.9 Polarity and subjectivity analyses of answer\n\nrefer: https://www.kaggle.com/kabure/qa-eda-and-nlp-modelling-insights-and-data-vis"},{"metadata":{"trusted":true},"cell_type":"code","source":"pol = lambda x: TextBlob(x).sentiment.polarity\nsub = lambda x: TextBlob(x).sentiment.subjectivity\n\ntrain_df['answer_polarity'] = train_df['answer'].apply(pol)\ntrain_df['answer_subjectivity'] = train_df['answer'].apply(sub)\n\ntrain_df[['answer', 'category', 'answer_polarity', 'answer_subjectivity']].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Polarity and subjectivity plot\nplt.figure(figsize=(12,5))\ng = sns.scatterplot(x='answer_polarity', y='answer_subjectivity', \n                    data=train_df, hue='category')\ng.set_title(\"Sentiment Analyzis (Polarity x Subjectivity) of answer by 'Category' Feature\", fontsize=16)\ng.set_xlabel(\"Polarity distribution\",fontsize=18)\ng.set_ylabel(\"Subjective \",fontsize=18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n"},{"metadata":{},"cell_type":"markdown","source":"## Saving all the transformed features and behaviour features into pickle file"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Saving all the transformed features and behaviour features into pickle file\n\"\"\"train_df.to_pickle(\"train_df.pkl\")\ntest_df.to_pickle(\"test_df.pkl\")\"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### All the features after all the box cox transformation and user behaviour analyses"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_feat = [col for col in train_df.columns if col not in target_vars]\nprint(\" All the features after all the box cox transformation and user behaviour analyses:\\n\")\nfor idx,f in enumerate(all_feat):\n    print(f\"{idx+1}: {f}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6. Insights and Findings"},{"metadata":{"trusted":false},"cell_type":"markdown","source":"## Note:\n### THIS KERNEL IS NOT FINISHED\n\n* Stay tuned  for more machine learning and deppe leaning and NLP concepts in Part 2\n\n*** If this kernel was useful for you, please upvote the kernel"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":1}