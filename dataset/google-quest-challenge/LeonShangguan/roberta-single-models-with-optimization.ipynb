{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Pytorch BERT baseline**"},{"metadata":{},"cell_type":"markdown","source":"### bert-base (uncased-v2, cased-v2) swa + xlnet (5 folds) + bert-base-uncased (question + answer) swa + bert-base-cased (question + answer) swa + xlnet (question + answer) swa + roberta (question + answer) + postprocessing (seefun's version)"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"!pip install ../input/sacremoses/sacremoses-master/\n!pip install ../input/transformers/transformers-master/","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Required Imports\n\nI've added imports that will be used in training too"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport gc\nimport matplotlib.pyplot as plt\nDATA_DIR = '../input/google-quest-challenge'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls ../input","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir(\"../input/roberta-transformers-pytorch/roberta-base\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir(\"../input/qaxlnetbasecasedaugdiffswaanswer\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir(\"../input/qaxlnetbasecasedaugdiffswaquestion\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir(\"../input/qabertbaseuncasedaugdiffswaanswer\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir(\"../input/qabertbaseuncasedaugdiffswaquestion\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir(\"../input/qabertbasecasedaugdiffswaanswer\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir(\"../input/qabertbasecasedaugdiffswaquestion\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir(\"../input/qabertbasecasedaugdiffv2swa\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir(\"../input/qabertuncasedaugdiffv2swa\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir(\"../input/qaxlnetbasecasedaugdiff\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir(\"../input/qarobertabasecasedaugdiffswaquestion\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv(f'{DATA_DIR}/sample_submission.csv')\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TARGET_COLUMNS = sub.columns.values[1:].tolist()\nTARGET_COLUMNS","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Define dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(f'{DATA_DIR}/train.csv')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(f'{DATA_DIR}/test.csv')\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport html\n#import torch.utils.data as data\nfrom torchvision import datasets, models, transforms\nfrom transformers import *\nfrom sklearn.utils import shuffle\nimport random\nfrom math import floor, ceil\nfrom sklearn.model_selection import GroupKFold\n\nMAX_LEN = 512\n#MAX_Q_LEN = 250\n#MAX_A_LEN = 259\nSEP_TOKEN_ID = 102\n\nclass QuestDataset(torch.utils.data.Dataset):\n    def __init__(self, df, model_type=\"bert-base-cased\", max_len=512, content=\"Question_Answer\", train_mode=True, labeled=True):\n        self.df = df\n        self.train_mode = train_mode\n        self.labeled = labeled\n        self.max_len = max_len\n        self.content = content\n        bert_tokenizer_path = '../input/pretrained-bert-models-for-pytorch/' + model_type + '-vocab.txt'\n        xlnet_tokenizer_path = '../input/xlnet-pretrained-models-pytorch/' + model_type + '-spiece.model'\n        roberta_tokenizer_path = '../input/roberta-transformers-pytorch/roberta-base/vocab.json'\n        roberta_tokenizer_merges_file = '../input/roberta-transformers-pytorch/roberta-base/merges.txt'\n        if model_type == \"bert-base-uncased\":\n            self.tokenizer = BertTokenizer.from_pretrained(bert_tokenizer_path)\n        elif model_type == \"bert-base-cased\":\n            self.tokenizer = BertTokenizer.from_pretrained(bert_tokenizer_path)\n        elif model_type == \"xlnet-base-cased\":\n            self.tokenizer = XLNetTokenizer.from_pretrained(xlnet_tokenizer_path)\n        elif model_type == \"roberta-base\":\n            self.tokenizer = RobertaTokenizer(vocab_file=roberta_tokenizer_path, merges_file=roberta_tokenizer_merges_file)\n\n    def __getitem__(self, index):\n        row = self.df.iloc[index]\n        token_ids, seg_ids = self.get_token_ids(row)\n        if self.labeled:\n            labels = self.get_label(row)\n            return token_ids, seg_ids, labels\n        else:\n            return token_ids, seg_ids\n\n    def __len__(self):\n        return len(self.df)\n\n    def select_tokens(self, tokens, max_num):\n        if len(tokens) <= max_num:\n            return tokens\n        if self.train_mode:\n            num_remove = len(tokens) - max_num\n            remove_start = random.randint(0, len(tokens)-num_remove-1)\n            return tokens[:remove_start] + tokens[remove_start + num_remove:]\n        else:\n            return tokens[:max_num//2] + tokens[-(max_num - max_num//2):]\n        \n    def trim_input_single_content(self, title, content, max_sequence_length=512, \n                t_max_len=30, c_max_len=512-30-4, num_token=3):\n        \n        content = html.unescape(content)\n        title = html.unescape(title)\n        \n        t = self.tokenizer.tokenize(title)\n        c = self.tokenizer.tokenize(content)\n\n        t_len = len(t)\n        c_len = len(c)\n\n        if (t_len+c_len+num_token) > max_sequence_length:\n\n            if t_max_len > t_len:\n                t_new_len = t_len\n                c_max_len = c_max_len + floor((t_max_len - t_len)/2)\n            else:\n                t_new_len = t_max_len\n\n            if c_max_len > c_len:\n                c_new_len = c_len \n            else:\n                c_new_len = c_max_len\n\n\n            if t_new_len+c_new_len+num_token > max_sequence_length:\n                raise ValueError(\"New sequence length should be less or equal than %d, but is %d\" \n                                 % (max_sequence_length, (t_new_len+c_new_len+num_token)))\n            \n            # truncate\n            if len(t) - t_new_len > 0:\n                t = t[:t_new_len//4] + t[len(t)-t_new_len+t_new_len//4:]\n            else:\n                t = t[:t_new_len]\n\n            if len(c) - c_new_len > 0:\n                c = c[:c_new_len//4] + c[len(c)-c_new_len+c_new_len//4:]\n            else:\n                c = c[:c_new_len]\n\n        # some bad cases\n        if (len(t) + len(c) + num_token > max_sequence_length):\n            more_token = len(t) + len(c) + num_token - max_sequence_length\n            c = c[:(len(c)-more_token)]\n        \n        return t, c\n            \n    def trim_input(self, title, question, answer, max_sequence_length=MAX_LEN, \n                t_max_len=30, q_max_len=239, a_max_len=239, num_token=4):\n\n        question = html.unescape(question)\n        answer = html.unescape(answer)\n        title = html.unescape(title)\n        \n        t = self.tokenizer.tokenize(title)\n        q = self.tokenizer.tokenize(question)\n        a = self.tokenizer.tokenize(answer)\n\n        t_len = len(t)\n        q_len = len(q)\n        a_len = len(a)\n\n        if (t_len+q_len+a_len+num_token) > max_sequence_length:\n\n            if t_max_len > t_len:\n                t_new_len = t_len\n                a_max_len = a_max_len + floor((t_max_len - t_len)/2)\n                q_max_len = q_max_len + ceil((t_max_len - t_len)/2)\n            else:\n                t_new_len = t_max_len\n\n            if a_max_len > a_len:\n                a_new_len = a_len \n                q_new_len = q_max_len + (a_max_len - a_len)\n            elif q_max_len > q_len:\n                a_new_len = a_max_len + (q_max_len - q_len)\n                q_new_len = q_len\n            else:\n                a_new_len = a_max_len\n                q_new_len = q_max_len\n\n\n            if t_new_len+a_new_len+q_new_len+num_token > max_sequence_length:\n                raise ValueError(\"New sequence length should be %d, but is %d\" \n                                 % (max_sequence_length, (t_new_len+a_new_len+q_new_len+num_token)))\n\n            \n            # truncate\n            if len(t) - t_new_len > 0:\n                t = t[:t_new_len//4] + t[len(t)-t_new_len+t_new_len//4:]\n            else:\n                t = t[:t_new_len]\n\n            if len(q) - q_new_len > 0:\n                q = q[:q_new_len//4] + q[len(q)-q_new_len+q_new_len//4:]\n            else:\n                q = q[:q_new_len]\n\n            if len(a) - a_new_len > 0:\n                a = a[:a_new_len//4] + a[len(a)-a_new_len+a_new_len//4:]\n            else:\n                a = a[:a_new_len]\n\n        return t, q, a\n        \n    def get_token_ids(self, row):\n        \n        num_token = 4\n        \n        if self.content == \"Question\":\n            num_token -= 1\n        elif self.content == \"Answer\":\n            num_token -= 1\n        \n        if self.content == \"Question_Answer\":   \n            t_max_len=30\n            q_max_len=int((self.max_len-t_max_len-num_token)/2)\n            a_max_len=(self.max_len-t_max_len - num_token - int((self.max_len-t_max_len-num_token)/2))\n        elif self.content == \"Question\":\n            t_max_len=30\n            q_max_len=self.max_len-t_max_len-num_token\n            a_max_len=0\n        elif self.content == \"Answer\":\n            t_max_len=30\n            q_max_len=0\n            a_max_len=self.max_len-t_max_len-num_token  \n        else:\n            raise NotImplementedError\n        \n        if self.content == \"Question_Answer\":\n            t_tokens, q_tokens, a_tokens = self.trim_input(row.question_title, row.question_body, row.answer, max_sequence_length=self.max_len, \\\n                t_max_len=t_max_len, q_max_len=q_max_len, a_max_len=a_max_len, num_token=num_token)\n        elif self.content == \"Question\":\n            t_tokens, c_tokens = self.trim_input_single_content(row.question_title, row.question_body, max_sequence_length=self.max_len, \\\n                t_max_len=t_max_len, c_max_len=q_max_len, num_token=num_token)\n        elif self.content == \"Answer\":\n            t_tokens, c_tokens = self.trim_input_single_content(row.question_title, row.answer, max_sequence_length=self.max_len, \\\n                t_max_len=t_max_len, c_max_len=a_max_len, num_token=num_token)\n        else:\n            raise NotImplementedError\n\n        if self.content == \"Question_Answer\":\n            tokens = ['[CLS]'] + t_tokens + ['[SEP]'] + q_tokens + ['[SEP]'] + a_tokens + ['[SEP]']\n        elif ((self.content == \"Question\") or (self.content == \"Answer\")):\n            tokens = ['[CLS]'] + t_tokens + ['[SEP]'] + c_tokens + ['[SEP]']\n        else:\n            raise NotImplementedError\n                \n        token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n        if len(token_ids) < self.max_len:\n            token_ids += [0] * (self.max_len - len(token_ids))\n        ids = torch.tensor(token_ids)\n        seg_ids = self.get_seg_ids(ids)\n        \n        return ids, seg_ids\n    \n    def get_seg_ids(self, ids):\n        seg_ids = torch.zeros_like(ids)\n        seg_idx = 0\n        first_sep = True\n        for i, e in enumerate(ids):\n            seg_ids[i] = seg_idx\n            if e == self.tokenizer.sep_token_id:\n                if first_sep:\n                    first_sep = False\n                else:\n                    seg_idx = 1\n        pad_idx = torch.nonzero(ids == 0)\n        seg_ids[pad_idx] = 0\n\n        return seg_ids\n\n    def get_label(self, row):\n        #print(row[TARGET_COLUMNS].values)\n        return torch.tensor(row[TARGET_COLUMNS].values.astype(np.float32))\n\n    def collate_fn(self, batch):\n        token_ids = torch.stack([x[0] for x in batch])\n        seg_ids = torch.stack([x[1] for x in batch])\n    \n        if self.labeled:\n            labels = torch.stack([x[2] for x in batch])\n            return token_ids, seg_ids, labels\n        else:\n            return token_ids, seg_ids\n\ndef get_test_loader(model_type=\"bert-base-cased\", max_len=512, content=\"Question_Answer\", batch_size=4):\n    df = pd.read_csv(f'{DATA_DIR}/test.csv')\n    ds_test = QuestDataset(df, model_type, max_len=max_len, content=content, train_mode=False, labeled=False)\n    loader = torch.utils.data.DataLoader(ds_test, batch_size=batch_size, shuffle=False, num_workers=2, collate_fn=ds_test.collate_fn, drop_last=False)\n    loader.num = len(df)\n    \n    return loader, ds_test.tokenizer\n        \ndef get_train_val_loaders(model_type=\"bert-base-cased\", max_len=512, content=\"Question_Answer\", batch_size=4, val_batch_size=4, ifold=0):\n    df = pd.read_csv(f'{DATA_DIR}/train.csv')\n    df = shuffle(df, random_state=42)\n    #split_index = int(len(df) * (1-val_percent))\n    gkf = GroupKFold(n_splits=5).split(X=df.question_body, groups=df.question_body)\n    for fold, (train_idx, valid_idx) in enumerate(gkf):\n        if fold == ifold:\n            df_train = df.iloc[train_idx]\n            df_val = df.iloc[valid_idx]\n            break\n\n    #print(df_val.head())\n    #df_train = df[:split_index]\n    #df_val = df[split_index:]\n\n    print(df_train.shape)\n    print(df_val.shape)\n\n    ds_train = QuestDataset(df_train, model_type, max_len=max_len, content=content)\n    train_loader = torch.utils.data.DataLoader(ds_train, batch_size=batch_size, shuffle=True, num_workers=2, collate_fn=ds_train.collate_fn, drop_last=True)\n    train_loader.num = len(df_train)\n\n    ds_val = QuestDataset(df_val, model_type, max_len=max_len, content=content, train_mode=False)\n    val_loader = torch.utils.data.DataLoader(ds_val, batch_size=val_batch_size, shuffle=False, num_workers=2, collate_fn=ds_val.collate_fn, drop_last=False)\n    val_loader.num = len(df_val)\n    val_loader.df = df_val\n\n    return train_loader, val_loader, ds_train.tokenizer\n\ndef test_train_loader():\n    loader, _, _ = get_train_val_loaders(\"xlnet-base-cased\", 512, \"Question\", 4, 4, 1)\n    for ids, seg_ids, labels in loader:\n        print(ids)\n        print(seg_ids.numpy())\n        print(labels)\n        break\ndef test_test_loader():\n    loader, _ = get_test_loader(\"roberta-base\", 512, \"Question\", 4)\n    for ids, seg_ids in loader:\n        print(ids)\n        print(seg_ids)\n        break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_test_loader()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_train_loader()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Build Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import *\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass QuestModel(nn.Module):\n    def __init__(self, model_type=\"xlnet-base-cased\", tokenizer=None, n_classes=30, hidden_layers=[-1, -3, -5, -7, -9]):\n        super(QuestModel, self).__init__()\n        self.model_name = 'QuestModel'\n        self.model_type = model_type\n        self.hidden_layers = hidden_layers\n        if model_type == \"bert-base-uncased\":\n            bert_model_config = '../input/pretrained-bert-models-for-pytorch/bert-base-uncased/bert_config.json'\n            bert_config = BertConfig.from_json_file(bert_model_config)\n            bert_config.output_hidden_states = True\n            model_path = os.path.join('../input/pretrained-bert-models-for-pytorch/' + model_type)\n            self.bert_model = BertModel.from_pretrained(model_path, config=bert_config)   \n        elif model_type == \"bert-base-cased\":\n            bert_model_config = '../input/pretrained-bert-models-for-pytorch/bert-base-cased/bert_config.json'\n            bert_config = BertConfig.from_json_file(bert_model_config)\n            bert_config.output_hidden_states = True\n            model_path = os.path.join('../input/pretrained-bert-models-for-pytorch/' + model_type)\n            self.bert_model = BertModel.from_pretrained(model_path, config=bert_config)   \n        elif model_type == \"xlnet-base-cased\":\n            xlnet_model_config = '../input/xlnet-pretrained-models-pytorch/xlnet-base-cased-config.json'\n            xlnet_config = XLNetConfig.from_json_file(xlnet_model_config)\n            xlnet_config.output_hidden_states = True\n            xlnet_config.hidden_dropout_prob = 0\n            model_path = os.path.join('../input/xlnet-pretrained-models-pytorch/' + model_type + '-pytorch_model.bin')\n            self.xlnet_model = XLNetModel.from_pretrained(model_path, config=xlnet_config)   \n        elif model_type == \"xlnet-large-cased\":\n            xlnet_model_config = '../input/xlnet-pretrained-models-pytorch/xlnet-large-cased-config.json'\n            xlnet_config = XLNetConfig.from_json_file(xlnet_model_config)\n            xlnet_config.output_hidden_states = True\n            xlnet_config.hidden_dropout_prob = 0\n            model_path = os.path.join('../input/xlnet-pretrained-models-pytorch/' + model_type + '-pytorch_model.bin')\n            self.xlnet_model = XLNetModel.from_pretrained(model_path, config=xlnet_config)  \n        elif model_type == \"roberta-base\":\n            roberta_model_config = '../input/roberta-transformers-pytorch/roberta-base/config.json'\n            roberta_config = RobertaConfig.from_json_file(roberta_model_config)\n            roberta_config.output_hidden_states = True\n            roberta_config.hidden_dropout_prob = 0\n            model_path = os.path.join('../input/roberta-transformers-pytorch/roberta-base/pytorch_model.bin')\n            self.roberta_model = RobertaModel.from_pretrained(model_path, config=roberta_config)  \n            self.roberta_model.resize_token_embeddings(len(tokenizer)) \n        \n        if model_type == \"bert-base-uncased\":\n            self.hidden_size = 768\n        elif model_type == \"bert-large-uncased\":\n            self.hidden_size = 1024\n        elif model_type == \"bert-base-cased\":\n            self.hidden_size = 768\n        elif model_type == \"xlnet-base-cased\":\n            self.hidden_size = 768\n        elif model_type == \"xlnet-large-cased\":\n            self.hidden_size = 1024\n        elif model_type == \"roberta-base\":\n            self.hidden_size = 768\n        else:\n            raise NotImplementedError\n            \n        self.fc_1 = nn.Linear(self.hidden_size * len(hidden_layers), self.hidden_size)\n        self.fc = nn.Linear(self.hidden_size, n_classes)\n            \n        self.selu = nn.SELU()\n        self.relu = nn.ReLU()\n        self.tanh = nn.Tanh()\n        self.dropouts = nn.ModuleList([\n            nn.Dropout(0.5) for _ in range(5)\n        ])\n\n    def forward(self, ids, seg_ids):\n        attention_mask = (ids > 0)\n        \n        if ((self.model_type == \"bert-base-uncased\") \\\n            or (self.model_type == \"bert-base-cased\") \\\n            or (self.model_type == \"bert-large-uncased\") \\\n            or (self.model_type == \"bert-large-cased\")):\n        \n            outputs = self.bert_model(input_ids=ids, token_type_ids=seg_ids, attention_mask=attention_mask)\n            hidden_states = outputs[2]\n            \n            # pooled_out = outputs[1] #  N * 768\n        \n            # sequence_out = torch.unsqueeze(outputs[0][:, 0], dim=-1) # N * 512 * 768 * 1, hidden_states[-1]\n            # fuse_hidden = sequence_out\n            \n            # 13 (embedding + 12 transformers) for base\n            # 26 (embedding + 25 transformers) for large\n            \n            # concat hidden\n            for i in range(len(self.hidden_layers)):\n                if i == 0:\n                    hidden_layer = self.hidden_layers[i]\n                    # hidden_state = torch.mean(hidden_states[hidden_layer], dim=1)\n                    hidden_state = hidden_states[hidden_layer][:, 0]\n                    fuse_hidden = torch.unsqueeze(hidden_state, dim=-1) # N * 768 * 1\n                else:\n                    hidden_layer = self.hidden_layers[i]\n                    # hidden_state = torch.mean(hidden_states[hidden_layer], dim=1)\n                    hidden_state = hidden_states[hidden_layer][:, 0]\n                    h = torch.unsqueeze(hidden_state, dim=-1) # N * 768 * 1\n                    fuse_hidden = torch.cat([fuse_hidden, h], dim=-1)\n                    \n            fuse_hidden = fuse_hidden.reshape(fuse_hidden.shape[0], -1)\n            h = self.relu(self.fc_1(fuse_hidden))\n        \n        elif ((self.model_type == \"xlnet-base-cased\") \\\n            or (self.model_type == \"xlnet-large-cased\")):\n\n            attention_mask = attention_mask.float()\n            outputs = self.xlnet_model(input_ids=ids, token_type_ids=seg_ids, attention_mask=attention_mask)\n            hidden_states = outputs[1]\n            \n            # last_hidden_out = outputs[0]\n            # mem = outputs[1], when config.mem_len > 0\n            \n            # concat hidden, summary_type=\"first\", first_dropout = 0\n            for i in range(len(self.hidden_layers)):\n                if i == 0:\n                    hidden_layer = self.hidden_layers[i]\n                    # hidden_state = hidden_states[hidden_layer].mean(dim=1)\n                    hidden_state = hidden_states[hidden_layer][:, 0]\n                    fuse_hidden = torch.unsqueeze(hidden_state, dim=-1) # N * 768 * 1\n                else:\n                    hidden_layer = self.hidden_layers[i]\n                    # hidden_state = hidden_states[hidden_layer].mean(dim=1)\n                    hidden_state = hidden_states[hidden_layer][:, 0]\n                    h = torch.unsqueeze(hidden_state, dim=-1) # N * 768 * 1\n                    fuse_hidden = torch.cat([fuse_hidden, h], dim=-1)\n        \n            fuse_hidden = fuse_hidden.reshape(fuse_hidden.shape[0], -1)\n            h = self.relu(self.fc_1(fuse_hidden))\n        elif (self.model_type == \"roberta-base\"):\n\n            attention_mask = attention_mask.float()\n            outputs = self.roberta_model(input_ids=ids, token_type_ids=seg_ids, attention_mask=attention_mask)\n            # outputs = self.roberta_model(input_ids=ids, attention_mask=attention_mask)\n            hidden_states = outputs[2]\n            \n            for i in range(len(self.hidden_layers)):\n                if i == 0:\n                    hidden_layer = self.hidden_layers[i]\n                    # hidden_state = hidden_states[hidden_layer].mean(dim=1)\n                    hidden_state = hidden_states[hidden_layer][:, 0]\n                    fuse_hidden = torch.unsqueeze(hidden_state, dim=-1) # N * 768 * 1\n                else:\n                    hidden_layer = self.hidden_layers[i]\n                    # hidden_state = hidden_states[hidden_layer].mean(dim=1)\n                    hidden_state = hidden_states[hidden_layer][:, 0]\n                    h = torch.unsqueeze(hidden_state, dim=-1) # N * 768 * 1\n                    fuse_hidden = torch.cat([fuse_hidden, h], dim=-1)\n        \n            fuse_hidden = fuse_hidden.reshape(fuse_hidden.shape[0], -1)\n            h = self.relu(self.fc_1(fuse_hidden))\n            \n            \n            \n        for j, dropout in enumerate(self.dropouts):\n            \n            if j == 0:\n                logit = self.fc(dropout(h))\n            else:\n                logit += self.fc(dropout(h))\n                \n        return logit / len(self.dropouts)\n    \ndef test_model(model_type=\"bert-base-cased\", hidden_layers=[-1, -3, -5, -7, -9]):\n    x = torch.tensor([[1,2,3,4,5, 0, 0], [1,2,3,4,5, 0, 0]])\n    seg_ids = torch.tensor([[0,0,0,0,0, 0, 0], [0,0,0,0,0, 0, 0]])\n    model = QuestModel(model_type=model_type, hidden_layers=hidden_layers)\n\n    y = model(x, seg_ids)\n    print(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_model(model_type=\"bert-base-cased\", hidden_layers=[-3, -4, -5, -6, -7])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_bert_base_uncased_models():\n    models = []\n    for i in range(10):\n        model = QuestModel(model_type=\"bert-base-uncased\", hidden_layers=[-1, -3, -5, -7, -9])\n        model.load_state_dict(torch.load(f'../input/qabertuncasedaugdiffv2swa/fold_{i}_checkpoint_swa.pth'))\n        model.eval()\n        models.append(model)\n    return models\n\ndef create_bert_base_cased_models():\n    models = []\n    for i in range(10):\n        model = QuestModel(model_type=\"bert-base-cased\", hidden_layers=[-1, -3, -5, -7, -9])\n        model.load_state_dict(torch.load(f'../input/qabertbasecasedaugdiffv2swa/fold_{i}_checkpoint_swa.pth'))\n        model.eval()\n        models.append(model)\n    return models\n\ndef create_xlnet_base_cased_models():\n    models = []\n    for i in range(5):\n        model = QuestModel(model_type=\"xlnet-base-cased\", hidden_layers=[-3, -4, -5, -6, -7])\n        model.load_state_dict(torch.load(f'../input/qaxlnetbasecasedaugdiff/fold_{i}_checkpoint.pth'))\n        model.eval()\n        models.append(model)\n    return models\n\ndef create_xlnet_base_cased_question_models():\n    models = []\n    for i in range(5):\n        model = QuestModel(model_type=\"xlnet-base-cased\", n_classes=21, hidden_layers=[-3, -4, -5, -6, -7])\n        model.load_state_dict(torch.load(f'../input/qaxlnetbasecasedaugdiffswaquestion/fold_{i}_checkpoint_swa.pth'))\n        model.eval()\n        models.append(model)\n    return models\n\ndef create_xlnet_base_cased_answer_models():\n    models = []\n    for i in range(5):\n        model = QuestModel(model_type=\"xlnet-base-cased\", n_classes=9, hidden_layers=[-3, -4, -5, -6, -7])\n        model.load_state_dict(torch.load(f'../input/qaxlnetbasecasedaugdiffswaanswer/fold_{i}_checkpoint_swa.pth'))\n        model.eval()\n        models.append(model)\n    return models\n\n\ndef create_bert_base_uncased_question_models():\n    models = []\n    for i in range(5):\n        model = QuestModel(model_type=\"bert-base-uncased\", n_classes=21, hidden_layers=[-3, -4, -5, -6, -7])\n        model.load_state_dict(torch.load(f'../input/qabertbaseuncasedaugdiffswaquestion/fold_{i}_checkpoint_swa.pth'))\n        model.eval()\n        models.append(model)\n    return models\n\ndef create_bert_base_uncased_answer_models():\n    models = []\n    for i in range(5):\n        model = QuestModel(model_type=\"bert-base-uncased\", n_classes=9, hidden_layers=[-3, -4, -5, -6, -7])\n        model.load_state_dict(torch.load(f'../input/qabertbaseuncasedaugdiffswaanswer/fold_{i}_checkpoint_swa.pth'))\n        model.eval()\n        models.append(model)\n    return models\n\ndef create_bert_base_cased_question_models():\n    models = []\n    for i in range(5):\n        model = QuestModel(model_type=\"bert-base-cased\", n_classes=21, hidden_layers=[-2, -4, -6, -8, -10])\n        model.load_state_dict(torch.load(f'../input/qabertbasecasedaugdiffswaquestion/fold_{i}_checkpoint_swa.pth'))\n        model.eval()\n        models.append(model)\n    return models\n\ndef create_bert_base_cased_answer_models():\n    models = []\n    for i in range(5):\n        model = QuestModel(model_type=\"bert-base-cased\", n_classes=9, hidden_layers=[-2, -4, -6, -8, -10])\n        model.load_state_dict(torch.load(f'../input/qabertbasecasedaugdiffswaanswer/fold_{i}_checkpoint_swa.pth'))\n        model.eval()\n        models.append(model)\n    return models\n\ndef create_roberta_base_question_models(tokenizer):\n    models = []\n    for i in range(5):\n        model = QuestModel(model_type=\"roberta-base\", tokenizer=tokenizer, n_classes=21, hidden_layers=[-3, -4, -5, -6, -7])\n        model.load_state_dict(torch.load(f'../input/qarobertabasecasedaugdiffswaquestion/fold_{i}_checkpoint_swa.pth'))\n        model.eval()\n        models.append(model)\n    return models\n\ndef create_roberta_base_answer_models(tokenizer):\n    models = []\n    for i in range(5):\n        model = QuestModel(model_type=\"roberta-base\", tokenizer=tokenizer, n_classes=9, hidden_layers=[-3, -4, -5, -6, -7])\n        model.load_state_dict(torch.load(f'../input/qarobertabasecasedaugdiffswaanswer/fold_{i}_checkpoint_swa.pth'))\n        model.eval()\n        models.append(model)\n    return models","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\nimport torch\ndef predict(models, test_loader):\n    all_scores = []\n    with torch.no_grad():\n        for ids, seg_ids in tqdm(test_loader, total=test_loader.num // test_loader.batch_size):\n            ids, seg_ids = ids.cuda(), seg_ids.cuda()\n            scores = []\n            for model in models:\n                model = model.cuda()\n                outputs = torch.sigmoid(model(ids, seg_ids)).cpu()\n                scores.append(outputs)\n            all_scores.append(torch.mean(torch.stack(scores), 0))\n\n    all_scores = torch.cat(all_scores, 0).numpy()\n    \n    return all_scores","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## predict with xlnet-base-cased"},{"metadata":{"trusted":true},"cell_type":"code","source":"# test_loader, _ = get_test_loader(model_type=\"xlnet-base-cased\", batch_size=32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# xlnet_base_cased_models = create_xlnet_base_cased_models()\n# xlnet_base_cased_preds = predict(xlnet_base_cased_models, test_loader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# del xlnet_base_cased_models, test_loader\n# torch.cuda.empty_cache()\n# gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## predict with xlnet-base-cased question and answer"},{"metadata":{"trusted":true},"cell_type":"code","source":"# test_loader, _ = get_test_loader(model_type=\"xlnet-base-cased\", content=\"Question\", batch_size=32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# xlnet_base_cased_question_models = create_xlnet_base_cased_question_models()\n# xlnet_base_cased_question_preds = predict(xlnet_base_cased_question_models, test_loader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# del xlnet_base_cased_question_models, test_loader\n# torch.cuda.empty_cache()\n# gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test_loader, _ = get_test_loader(model_type=\"xlnet-base-cased\", content=\"Answer\", batch_size=32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# xlnet_base_cased_answer_models = create_xlnet_base_cased_answer_models()\n# xlnet_base_cased_answer_preds = predict(xlnet_base_cased_answer_models, test_loader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# del xlnet_base_cased_answer_models, test_loader\n# torch.cuda.empty_cache()\n# gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# xlnet_base_cased_question_answer_preds = np.concatenate([xlnet_base_cased_question_preds, xlnet_base_cased_answer_preds], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## predict with roberta-base question and answer"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_loader, tokenizer = get_test_loader(model_type=\"roberta-base\", content=\"Question\", batch_size=32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roberta_base_question_models = create_roberta_base_question_models(tokenizer)\nroberta_base_question_preds = predict(roberta_base_question_models, test_loader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del roberta_base_question_models, test_loader, tokenizer\ntorch.cuda.empty_cache()\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_loader, tokenizer = get_test_loader(model_type=\"roberta-base\", content=\"Answer\", batch_size=32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roberta_base_answer_models = create_roberta_base_answer_models(tokenizer)\nroberta_base_answer_preds = predict(roberta_base_answer_models, test_loader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del roberta_base_answer_models, test_loader, tokenizer\ntorch.cuda.empty_cache()\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roberta_base_question_answer_preds = np.concatenate([roberta_base_question_preds, roberta_base_answer_preds], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## predict with bert-base-cased question and answer"},{"metadata":{"trusted":true},"cell_type":"code","source":"# test_loader, _ = get_test_loader(model_type=\"bert-base-cased\", content=\"Question\", batch_size=32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# bert_base_cased_question_models = create_bert_base_cased_question_models()\n# bert_base_cased_question_preds = predict(bert_base_cased_question_models, test_loader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# del bert_base_cased_question_models, test_loader\n# torch.cuda.empty_cache()\n# gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test_loader, _ = get_test_loader(model_type=\"bert-base-cased\", content=\"Answer\", batch_size=32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# bert_base_cased_answer_models = create_bert_base_cased_answer_models()\n# bert_base_cased_answer_preds = predict(bert_base_cased_answer_models, test_loader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# del bert_base_cased_answer_models, test_loader\n# torch.cuda.empty_cache()\n# gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# bert_base_cased_question_answer_preds = np.concatenate([bert_base_cased_question_preds, bert_base_cased_answer_preds], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## predict with bert-base-uncased question and answer"},{"metadata":{"trusted":true},"cell_type":"code","source":"# test_loader, _ = get_test_loader(model_type=\"bert-base-uncased\", content=\"Question\", batch_size=32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# bert_base_uncased_question_models = create_bert_base_uncased_question_models()\n# bert_base_uncased_question_preds = predict(bert_base_uncased_question_models, test_loader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# del bert_base_uncased_question_models, test_loader\n# torch.cuda.empty_cache()\n# gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test_loader, _ = get_test_loader(model_type=\"bert-base-uncased\", content=\"Answer\", batch_size=32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# bert_base_uncased_answer_models = create_bert_base_uncased_answer_models()\n# bert_base_uncased_answer_preds = predict(bert_base_uncased_answer_models, test_loader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# del bert_base_uncased_answer_models, test_loader\n# torch.cuda.empty_cache()\n# gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# bert_base_uncased_question_answer_preds = np.concatenate([bert_base_uncased_question_preds, bert_base_uncased_answer_preds], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## predict with bert-base-cased"},{"metadata":{"trusted":true},"cell_type":"code","source":"# test_loader, _ = get_test_loader(model_type=\"bert-base-cased\", batch_size=32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# bert_base_cased_models = create_bert_base_cased_models()\n# bert_base_cased_preds = predict(bert_base_cased_models, test_loader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# del bert_base_cased_models, test_loader\n# torch.cuda.empty_cache()\n# gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## predict with bert-base-uncased"},{"metadata":{"trusted":true},"cell_type":"code","source":"# test_loader, _ = get_test_loader(model_type=\"bert-base-uncased\", batch_size=32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# bert_base_uncased_models = create_bert_base_uncased_models()\n# bert_base_uncased_preds = predict(bert_base_uncased_models, test_loader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# del bert_base_uncased_models, test_loader\n# torch.cuda.empty_cache()\n# gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# preds = bert_base_uncased_question_answer_preds\n# preds = ((bert_base_uncased_preds + bert_base_cased_preds)/2.0 \\\n#          + (xlnet_base_cased_preds + xlnet_base_cased_question_answer_preds)/2.0 \\\n#          + (bert_base_uncased_question_answer_preds + bert_base_cased_question_answer_preds)/2.0 \\\n#           ) / 3.0\n# preds = bert_base_uncased_preds\npreds = roberta_base_question_answer_preds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Generate Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"# sub[TARGET_COLUMNS] = bert_base_uncased_preds\n# sub.to_csv('submission_bert_base_uncased.csv', index=False)\n# sub[TARGET_COLUMNS] = bert_base_cased_preds\n# sub.to_csv('submission_bert_base_cased.csv', index=False)\n# sub[TARGET_COLUMNS] = xlnet_base_cased_preds\n# sub.to_csv('submission_xlnet_base_cased.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pred = np.copy(preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test = pd.read_csv(f'{DATA_DIR}/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import pandas as pd\n# optimization_results = pd.read_csv(\"../input/optyxx/optimization_resultsX.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ALL_COLUMNS = ['question_asker_intent_understanding', 'question_body_critical', 'question_conversational',\n#                      'question_expect_short_answer', 'question_fact_seeking', 'question_has_commonly_accepted_answer',\n#                      'question_interestingness_others', 'question_interestingness_self', 'question_multi_intent',\n#                      'question_not_really_a_question', 'question_opinion_seeking', 'question_type_choice',\n#                      'question_type_compare', 'question_type_consequence', 'question_type_definition',\n#                      'question_type_entity', 'question_type_instructions', 'question_type_procedure',\n#                      'question_type_reason_explanation', 'question_type_spelling', 'question_well_written',\n#                      'answer_helpful', 'answer_level_of_information', 'answer_plausible', 'answer_relevance',\n#                      'answer_satisfaction', 'answer_type_instructions', 'answer_type_procedure',\n#                      'answer_type_reason_explanation', 'answer_well_written']\n\n# OPTIMIZED_COLUMNS = [\n#     'question_conversational',\n#     'question_has_commonly_accepted_answer',\n#     'question_not_really_a_question',\n#     'question_type_choice',\n#     'question_type_compare',\n#     'question_type_consequence',\n#     'question_type_definition',\n#     'question_type_entity',\n#     'question_type_instructions',\n#     'question_interestingness_self', \n#     'answer_satisfaction'\n# ]\n\n# NON_OPTIMIZED_COLUMNS = list(set(ALL_COLUMNS) - set(OPTIMIZED_COLUMNS))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for col in NON_OPTIMIZED_COLUMNS:\n#     coeffs = optimization_results.loc[optimization_results.col==col, 'coeffs']\n        \n#     changerow = int(len(test) * coeffs)\n#     colidx = NON_OPTIMIZED_COLUMNS.index(col)\n    \n#     if optimization_results.loc[optimization_results.col==col, 'choice'].values =='decrease':\n#         rowidx = pred[:, colidx].argsort()[:changerow]\n#         pred[rowidx, colidx] = pred[rowidx, colidx] * 0.9\n#     elif optimization_results.loc[optimization_results.col==col, 'choice'].values =='increase':\n#         rowidx = pred[:, colidx].argsort()[-changerow:]\n#         pred[rowidx, colidx] = pred[rowidx, colidx] * 1.1\n#     else:\n#         pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub[TARGET_COLUMNS] = preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(f'{DATA_DIR}/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = test.set_index('qa_id').join(sub.set_index('qa_id'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Postprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n    \ndef postprocessing(oof_df):\n   \n    scaler = MinMaxScaler()\n    \n    # type 1 column [0, 0.333333, 0.5, 0.666667, 1]\n    # type 2 column [0, 0.333333, 0.666667]\n    # type 3 column [0.333333, 0.444444, 0.5, 0.555556, 0.666667, 0.777778, 0.8333333, 0.888889, 1]\n    # type 4 column [0.200000, 0.266667, 0.300000, 0.333333, 0.400000, \\\n    # 0.466667, 0.5, 0.533333, 0.600000, 0.666667, 0.700000, \\\n    # 0.733333, 0.800000, 0.866667, 0.900000, 0.933333, 1]\n    \n    # comment some columns based on oof result\n    \n    ################################################# handle type 1 columns\n    type_one_column_list = [\n       'question_conversational', \\\n       'question_has_commonly_accepted_answer', \\\n       'question_not_really_a_question', \\\n       'question_type_choice', \\\n       'question_type_compare', \\\n       'question_type_consequence', \\\n       'question_type_definition', \\\n       'question_type_entity', \\\n       'question_type_instructions', \n    ]\n    \n    oof_df[type_one_column_list] = scaler.fit_transform(oof_df[type_one_column_list])\n    \n    tmp = oof_df.copy(deep=True)\n    \n    for column in type_one_column_list:\n        \n        oof_df.loc[tmp[column] <= 0.16667, column] = 0\n        oof_df.loc[(tmp[column] > 0.16667) & (tmp[column] <= 0.41667), column] = 0.333333\n        oof_df.loc[(tmp[column] > 0.41667) & (tmp[column] <= 0.58333), column] = 0.500000\n        oof_df.loc[(tmp[column] > 0.58333) & (tmp[column] <= 0.73333), column] = 0.666667\n        oof_df.loc[(tmp[column] > 0.73333), column] = 1\n    \n    \n    \n    ################################################# handle type 2 columns      \n#     type_two_column_list = [\n#         'question_type_spelling'\n#     ]\n    \n#     for column in type_two_column_list:\n#         if sum(tmp[column] > 0.15)>0:\n#             oof_df.loc[tmp[column] <= 0.15, column] = 0\n#             oof_df.loc[(tmp[column] > 0.15) & (tmp[column] <= 0.45), column] = 0.333333\n#             oof_df.loc[(tmp[column] > 0.45), column] = 0.666667\n#         else:\n#             t1 = max(int(len(tmp[column])*0.0013),2)\n#             t2 = max(int(len(tmp[column])*0.0008),1)\n#             thred1 = sorted(list(tmp[column]))[-t1]\n#             thred2 = sorted(list(tmp[column]))[-t2]\n#             oof_df.loc[tmp[column] <= thred1, column] = 0\n#             oof_df.loc[(tmp[column] > thred1) & (tmp[column] <= thred2), column] = 0.333333\n#             oof_df.loc[(tmp[column] > thred2), column] = 0.666667\n    \n    \n    \n    ################################################# handle type 3 columns      \n    type_three_column_list = [\n       'question_interestingness_self', \n    ]\n    scaler = MinMaxScaler(feature_range=(0, 1))\n    oof_df[type_three_column_list] = scaler.fit_transform(oof_df[type_three_column_list])\n    tmp[type_three_column_list] = scaler.fit_transform(tmp[type_three_column_list])\n    \n    for column in type_three_column_list:\n        oof_df.loc[tmp[column] <= 0.385, column] = 0.333333\n        oof_df.loc[(tmp[column] > 0.385) & (tmp[column] <= 0.47), column] = 0.444444\n        oof_df.loc[(tmp[column] > 0.47) & (tmp[column] <= 0.525), column] = 0.5\n        oof_df.loc[(tmp[column] > 0.525) & (tmp[column] <= 0.605), column] = 0.555556\n        oof_df.loc[(tmp[column] > 0.605) & (tmp[column] <= 0.715), column] = 0.666667\n        oof_df.loc[(tmp[column] > 0.715) & (tmp[column] <= 0.8), column] = 0.833333\n        oof_df.loc[(tmp[column] > 0.8) & (tmp[column] <= 0.94), column] = 0.888889\n        oof_df.loc[(tmp[column] > 0.94), column] = 1\n        \n        \n        \n    ################################################# handle type 4 columns      \n    type_four_column_list = [\n        'answer_satisfaction'\n    ]\n    scaler = MinMaxScaler(feature_range=(0.2, 1))\n    oof_df[type_four_column_list] = scaler.fit_transform(oof_df[type_four_column_list])\n    tmp[type_four_column_list] = scaler.fit_transform(tmp[type_four_column_list])\n    \n    for column in type_four_column_list:\n        \n        oof_df.loc[tmp[column] <= 0.233, column] = 0.200000\n        oof_df.loc[(tmp[column] > 0.233) & (tmp[column] <= 0.283), column] = 0.266667\n        oof_df.loc[(tmp[column] > 0.283) & (tmp[column] <= 0.315), column] = 0.300000\n        oof_df.loc[(tmp[column] > 0.315) & (tmp[column] <= 0.365), column] = 0.333333\n        oof_df.loc[(tmp[column] > 0.365) & (tmp[column] <= 0.433), column] = 0.400000\n        oof_df.loc[(tmp[column] > 0.433) & (tmp[column] <= 0.483), column] = 0.466667\n        oof_df.loc[(tmp[column] > 0.483) & (tmp[column] <= 0.517), column] = 0.500000\n        oof_df.loc[(tmp[column] > 0.517) & (tmp[column] <= 0.567), column] = 0.533333\n        oof_df.loc[(tmp[column] > 0.567) & (tmp[column] <= 0.633), column] = 0.600000\n        oof_df.loc[(tmp[column] > 0.633) & (tmp[column] <= 0.683), column] = 0.666667\n        oof_df.loc[(tmp[column] > 0.683) & (tmp[column] <= 0.715), column] = 0.700000\n        oof_df.loc[(tmp[column] > 0.715) & (tmp[column] <= 0.767), column] = 0.733333\n        oof_df.loc[(tmp[column] > 0.767) & (tmp[column] <= 0.833), column] = 0.800000\n        oof_df.loc[(tmp[column] > 0.883) & (tmp[column] <= 0.915), column] = 0.900000\n        oof_df.loc[(tmp[column] > 0.915) & (tmp[column] <= 0.967), column] = 0.933333\n        oof_df.loc[(tmp[column] > 0.967), column] = 1\n    \n    \n    ################################################# round to i / 90 (i from 0 to 90)\n    oof_values = oof_df[TARGET_COLUMNS].values\n    DEGREE = len(oof_df)//45*9\n#     if degree:\n#         DEGREE = degree\n#     DEGREE = 90\n    oof_values = np.around(oof_values * DEGREE) / DEGREE  ### 90 To be changed\n    oof_df[TARGET_COLUMNS] = oof_values\n    \n    return oof_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = postprocessing(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for column in TARGET_COLUMNS:\n    print(test[column].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Assign postprocessed result"},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = test[TARGET_COLUMNS].reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub[ sub[TARGET_COLUMNS] > 1.0] = 1.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(f'{DATA_DIR}/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n=test['url'].apply(lambda x:(('ell.stackexchange.com' in x) or ('english.stackexchange.com' in x))).tolist()\nspelling=[]\nfor x in n:\n    if x:\n        spelling.append(0.5)\n    else:\n        spelling.append(0.)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub['question_type_spelling'] = spelling","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"086421f7eec44c769f08f5f68fafafdc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0e7c81c0da784e04b530236bad005c33":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"18b951cd8c1447eaa1e12f972ac37d42":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"IntProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"IntProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"  9%","description_tooltip":null,"layout":"IPY_MODEL_9e743ecd26cd4877a539f9bbbd23d114","max":308,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e34fafda1e234d9c981322beff1068e7","value":29}},"35645b239e914aee807cfdc234fc5b75":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dfdee9b109bc4176a0bc3a26ae38f7a3","placeholder":"​","style":"IPY_MODEL_086421f7eec44c769f08f5f68fafafdc","value":" 29/308 [7:12:46&lt;66:18:40, 855.63s/it]"}},"554271f3ed5d48efa844608a0b72fdac":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"685ecca481e3463d83a2465f15f7b33f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"IntProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"IntProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"100%","description_tooltip":null,"layout":"IPY_MODEL_80ae0d356c3e4b1bbe511c5f5a2694b5","max":4059,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a34804b28be74d7c9972a0a13410ba77","value":4059}},"7f49d7d685554687bc2d131959058cb3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"80ae0d356c3e4b1bbe511c5f5a2694b5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"840e74b11eb44a58b1ed0433e924dc82":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9e743ecd26cd4877a539f9bbbd23d114":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a34804b28be74d7c9972a0a13410ba77":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"a61c8f56dbd74e29a155fad0d7095f79":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_18b951cd8c1447eaa1e12f972ac37d42","IPY_MODEL_35645b239e914aee807cfdc234fc5b75"],"layout":"IPY_MODEL_7f49d7d685554687bc2d131959058cb3"}},"c9ab84b07a32426282543b5ff054ddec":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_840e74b11eb44a58b1ed0433e924dc82","placeholder":"​","style":"IPY_MODEL_554271f3ed5d48efa844608a0b72fdac","value":" 4059/4059 [14:53&lt;00:00,  4.54it/s]"}},"dfdee9b109bc4176a0bc3a26ae38f7a3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e34fafda1e234d9c981322beff1068e7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"fd692cff30e343aa8afa007a05d66acc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_685ecca481e3463d83a2465f15f7b33f","IPY_MODEL_c9ab84b07a32426282543b5ff054ddec"],"layout":"IPY_MODEL_0e7c81c0da784e04b530236bad005c33"}}},"version_major":2,"version_minor":0}}},"nbformat":4,"nbformat_minor":1}