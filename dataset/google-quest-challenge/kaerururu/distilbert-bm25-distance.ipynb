{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install ../input/sacremoses/sacremoses-master/ > /dev/null\n\nimport os\nimport sys\nimport glob\nimport torch\n\nsys.path.insert(0, \"../input/transformers/transformers-master/\")\nimport transformers\nimport numpy as np\nimport pandas as pd\nimport math","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport pandas as pd\nimport argparse\nimport collections\nimport datetime\nimport gc\nimport glob\nimport logging\nimport math\nimport operator\nimport os \nimport pickle\nimport pkg_resources\nimport random\nimport re\nimport scipy.stats as stats\nimport seaborn as sns\nimport shutil\nimport sys\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.utils.data\nimport torch.nn.functional as F\nfrom contextlib import contextmanager\nfrom collections import OrderedDict\n# from nltk.stem import PorterStemmer\nfrom sklearn import metrics\nfrom sklearn import model_selection\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\nfrom torch.nn import CrossEntropyLoss, MSELoss\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import (Dataset,DataLoader, RandomSampler, SequentialSampler,\n                              TensorDataset)\n%load_ext autoreload\n%autoreload 2\n%matplotlib inline\n# from tqdm import tqdm, tqdm_notebook, trange\nfrom tqdm._tqdm_notebook import tqdm_notebook as tqdm\ntqdm.pandas()\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\nimport warnings\nwarnings.filterwarnings('ignore')\n# from apex import amp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data.distributed import DistributedSampler\n\ntry:\n    from torch.utils.tensorboard import SummaryWriter\nexcept:\n    from tensorboardX import SummaryWriter\n\nfrom transformers import (WEIGHTS_NAME, BertConfig, BertModel,\n                                  BertForQuestionAnswering, BertTokenizer,\n                                  DistilBertTokenizer, DistilBertModel)\n\nfrom transformers import AdamW # , WarmupLinearSchedule\nfrom transformers.tokenization_bert import (BasicTokenizer,\n                                                    whitespace_tokenize)\n\nSEED = 1129\n\ndef seed_everything(seed=1129):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\nseed_everything(SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mispell_dict = {\n    \"I'd\": 'I would',\n    \"I'll\": 'I will',\n    \"I'm\": 'I am',\n    \"I've\": 'I have',\n    \"ain't\": 'is not',\n    \"aren't\": 'are not',\n    \"can't\": 'cannot',\n    'cancelled': 'canceled',\n    'centre': 'center',\n    'colour': 'color',\n    \"could've\": 'could have',\n    \"couldn't\": 'could not',\n    \"didn't\": 'did not',\n    \"doesn't\": 'does not',\n    \"don't\": 'do not',\n    'enxiety': 'anxiety',\n    'favourite': 'favorite',\n    \"hadn't\": 'had not',\n    \"hasn't\": 'has not',\n    \"haven't\": 'have not',\n    \"he'd\": 'he would',\n    \"he'll\": 'he will',\n    \"he's\": 'he is',\n    \"here's\": 'here is',\n    \"how's\": 'how is',\n    \"i'd\": 'i would',\n    \"i'll\": 'i will',\n    \"i'm\": 'i am',\n    \"i've\": 'i have',\n    \"isn't\": 'is not',\n    \"it'll\": 'it will',\n    \"it's\": 'it is',\n    'labour': 'labor',\n    \"let's\": 'let us',\n    \"might've\": 'might have',\n    \"must've\": 'must have',\n    'organisation': 'organization',\n    \"she'd\": 'she would',\n    \"she'll\": 'she will',\n    \"she's\": 'she is',\n    \"shouldn't\": 'should not',\n    \"that's\": 'that is',\n    'theatre': 'theater',\n    \"there's\": 'there is',\n    \"they'd\": 'they would',\n    \"they'll\": 'they will',\n    \"they're\": 'they are',\n    \"they've\": 'they have',\n    'travelling': 'traveling',\n    \"wasn't\": 'was not',\n    'watsapp': 'whatsapp',\n    \"we'd\": 'we would',\n    \"we'll\": 'we will',\n    \"we're\": 'we are',\n    \"we've\": 'we have',\n    \"weren't\": 'were not',\n    \"what's\": 'what is',\n    \"where's\": 'where is',\n    \"who'll\": 'who will',\n    \"who's\": 'who is',\n    \"who've\": 'who have',\n    \"won't\": 'will not',\n    \"would've\": 'would have',\n    \"wouldn't\": 'would not',\n    \"you'd\": 'you would',\n    \"you'll\": 'you will',\n    \"you're\": 'you are',\n    \"you've\": 'you have',\n    '，': ',',\n    '／': '/',\n    '？': '?'\n}\n\n\npuncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£',\n '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', '\\n', '\\xa0', '\\t',\n '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '\\u3000', '\\u202f',\n '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '«',\n '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n\n\ndef clean_puncts(x):\n    x = str(x)\n    for punct in puncts:\n        x = x.replace(punct, f' {punct} ')\n    return x\n\ndef clean_text(x):\n    x = str(x)        \n    x = x.lower()\n    x = x.replace('\\n', '') # 改行削除\n    x = x.replace('\\t', '') # タブ削除\n    x = x.replace('\\r', '')\n    x = re.sub(re.compile(r'[!-\\/:-@[-`{-~]'), ' ', x) \n    x = re.sub(r'\\[math\\]', ' LaTex math ', x) # LaTex削除\n    x = re.sub(r'\\[\\/math\\]', ' LaTex math ', x) # LaTex削除\n    x = re.sub(r'\\\\', ' LaTex ', x) # LaTex削除   \n    x = re.sub(' +', ' ', x)\n    return x\n\ndef clean_text_simple(x):\n    x = str(x)        \n    x = x.lower()\n    x = x.replace('\\n', '') # 改行削除\n    x = x.replace('\\t', '') # タブ削除\n    x = x.replace('\\r', '')\n    x = x.replace('?', '.')\n    x = x.replace('!', '.')\n    x = re.sub(' +', ' ', x)\n    return x\n\ndef _get_mispell(mispell_dict):\n    mispell_re = re.compile('(%s)' % '|'.join(['\\s*'.join(key) \n                                               for key in mispell_dict.keys()]))\n    return mispell_dict, mispell_re\n\nmispellings, mispellings_re = _get_mispell(mispell_dict)\n\ndef replace_typical_misspell(text):\n    def replace(match):\n        return mispellings[re.sub('\\s', '', match.group(0))]\n    return mispellings_re.sub(replace, text)\n\n\ndef preprocess(data):\n    data = data.progress_apply(lambda x: clean_puncts(x))\n    data = data.progress_apply(lambda x: replace_typical_misspell(x))\n    data = data.progress_apply(lambda x: clean_text(x))\n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/google-quest-challenge/train.csv\")\ntest = pd.read_csv(\"../input/google-quest-challenge/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from urllib.parse import urlparse\n\n# ===================\n# create url features\n# ===================\n\nfind = re.compile(r\"^[^.]*\")\n\ntrain['netloc'] = train['url'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\ntest['netloc'] = test['url'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\n\nfeatures = ['netloc', 'category']\nmerged = pd.concat([train[features], test[features]])\nohe = OneHotEncoder()\nohe.fit(merged)\n\nnetloc_category_features_train = ohe.transform(train[features]).toarray()\nnetloc_category_features_test = ohe.transform(test[features]).toarray()\n\nprint(netloc_category_features_train.shape, netloc_category_features_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_sentence_features(train, col):\n    train[col + '_num_chars'] = train[col].apply(len)\n    train[col + '_num_capitals'] = train[col].apply(lambda x: sum(1 for c in x if c.isupper()))\n    train[col + '_caps_vs_length'] = train.apply(lambda row: row[col + '_num_chars'] / (row[col + '_num_capitals']+1e-5), axis=1)\n    train[col + '_num_exclamation_marks'] = train[col].apply(lambda x: x.count('!'))\n    train[col + '_num_question_marks'] = train[col].apply(lambda x: x.count('?'))\n    train[col + '_num_punctuation'] = train[col].apply(lambda x: sum(x.count(w) for w in '.,;:'))\n    train[col + '_num_symbols'] = train[col].apply(lambda x: sum(x.count(w) for w in '*&$%'))\n    train[col + '_num_words'] = train[col].apply(lambda x: len(x.split()))\n    train[col + '_num_unique_words'] = train[col].apply(lambda comment: len(set(w for w in comment.split())))\n    train[col + '_words_vs_unique'] = train[col + '_num_unique_words'] / train[col + '_num_words'] \n    return train\n\n\nfor col in tqdm(['question_body', 'question_title', 'answer']):\n    train = get_sentence_features(train, col)\n    test = get_sentence_features(test, col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_num_features = [\n       'question_body_num_chars',\n       'question_body_num_capitals', 'question_body_caps_vs_length',\n       'question_body_num_exclamation_marks',\n       'question_body_num_question_marks', 'question_body_num_punctuation',\n       'question_body_num_symbols', 'question_body_num_words',\n       'question_body_num_unique_words', 'question_body_words_vs_unique',\n       'question_title_num_chars', 'question_title_num_capitals',\n       'question_title_caps_vs_length', 'question_title_num_exclamation_marks',\n       'question_title_num_question_marks', 'question_title_num_punctuation',\n       'question_title_num_symbols', 'question_title_num_words',\n       'question_title_num_unique_words', 'question_title_words_vs_unique',\n       'answer_num_chars', 'answer_num_capitals', 'answer_caps_vs_length',\n       'answer_num_exclamation_marks', 'answer_num_question_marks',\n       'answer_num_punctuation', 'answer_num_symbols', 'answer_num_words',\n       'answer_num_unique_words', 'answer_words_vs_unique'\n]\n\nfor feature in tqdm(all_num_features):\n    scaler = StandardScaler()\n    scaler.fit(train[feature].append(test[feature]).astype(np.float64).values[:, np.newaxis])\n    \n    train[feature + '_scaled'] = scaler.transform(train[feature].astype(np.float64).values[:, np.newaxis])\n    test[feature + '_scaled'] = scaler.transform(test[feature].astype(np.float64).values[:, np.newaxis])\n    \n    train[feature + '_scaled'] = np.nan_to_num(train[feature + '_scaled'], -99)\n    test[feature + '_scaled'] = np.nan_to_num(test[feature + '_scaled'], -99)\n    \n    del train[feature], test[feature]\n    gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['question_title'] = preprocess(train['question_title'])\ntrain['question_body'] = preprocess(train['question_body'])\ntrain['answer'] = preprocess(train['answer'])\ntest['question_title'] = preprocess(test['question_title'])\ntest['question_body'] = preprocess(test['question_body'])\ntest['answer'] = preprocess(test['answer'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_cols = ['question_asker_intent_understanding',\n       'question_body_critical', 'question_conversational',\n       'question_expect_short_answer', 'question_fact_seeking',\n       'question_has_commonly_accepted_answer',\n       'question_interestingness_others', 'question_interestingness_self',\n       'question_multi_intent', 'question_not_really_a_question',\n       'question_opinion_seeking', 'question_type_choice',\n       'question_type_compare', 'question_type_consequence',\n       'question_type_definition', 'question_type_entity',\n       'question_type_instructions', 'question_type_procedure',\n       'question_type_reason_explanation', 'question_type_spelling',\n       'question_well_written', 'answer_helpful',\n       'answer_level_of_information', 'answer_plausible', 'answer_relevance',\n       'answer_satisfaction', 'answer_type_instructions',\n       'answer_type_procedure', 'answer_type_reason_explanation',\n       'answer_well_written']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def chunks(l, n):\n    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n    for i in range(0, len(l), n):\n        yield l[i:i + n]\n        \n\ndef fetch_vectors(string_list, batch_size=64):\n    # inspired by https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/\n    DEVICE = torch.device(\"cuda\")\n    tokenizer = transformers.DistilBertTokenizer.from_pretrained(\"../input/distilbertbaseuncased/\")\n    model = transformers.DistilBertModel.from_pretrained(\"../input/distilbertbaseuncased/\")\n    model.to(DEVICE)\n\n    fin_features = []\n    for data in tqdm(chunks(string_list, batch_size)):\n        tokenized = []\n        for x in data:\n            x = \" \".join(x.strip().split()[:300])\n            tok = tokenizer.encode(x, add_special_tokens=True)\n            tokenized.append(tok[:512])\n\n        max_len = 512\n        padded = np.array([i + [0] * (max_len - len(i)) for i in tokenized])\n        attention_mask = np.where(padded != 0, 1, 0)\n        input_ids = torch.tensor(padded).to(DEVICE)\n        attention_mask = torch.tensor(attention_mask).to(DEVICE)\n\n        with torch.no_grad():\n            last_hidden_states = model(input_ids, attention_mask=attention_mask)\n\n        features = last_hidden_states[0][:, 0, :].cpu().numpy()\n        fin_features.append(features)\n\n    fin_features = np.vstack(fin_features)\n    return fin_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_question_body_dense = fetch_vectors(train['question_body'].values)\ntrain_question_title_dense = fetch_vectors(train['question_title'].values)\ntrain_answer_dense = fetch_vectors(train['answer'].values)\ntrain_category_dense = fetch_vectors(train['category'].values)\ntrain_netloc_dense = fetch_vectors(train['netloc'].values)\n\ntest_question_body_dense = fetch_vectors(test['question_body'].values)\ntest_question_title_dense = fetch_vectors(test['question_title'].values)\ntest_answer_dense = fetch_vectors(test['answer'].values)\ntest_category_dense = fetch_vectors(test['category'].values)\ntest_netloc_dense = fetch_vectors(test['netloc'].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_question_body_dense[0][:3], test_question_body_dense[0][:3], \\\ntrain_question_title_dense[0][:3], test_question_title_dense[0][:3], \\\ntrain_answer_dense[0][:3], test_answer_dense[0][:3], \\\ntrain_category_dense[0][:3], test_category_dense[0][:3], \\\ntrain_netloc_dense[0][:3], test_netloc_dense[0][:3], \\","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import scipy as sp\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.utils.validation import check_is_fitted\nfrom sklearn.feature_extraction.text import _document_frequency\nfrom sklearn.pipeline import make_pipeline, make_union\n\n\nclass BM25Transformer(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Parameters\n    ----------\n    use_idf : boolean, optional (default=True)\n    k1 : float, optional (default=2.0)\n    b  : float, optional (default=0.75)\n    References\n    ----------\n    Okapi BM25: a non-binary model - Introduction to Information Retrieval\n    http://nlp.stanford.edu/IR-book/html/htmledition/okapi-bm25-a-non-binary-model-1.html\n    \"\"\"\n    def __init__(self, use_idf=True, k1=2.0, b=0.75):\n        self.use_idf = use_idf\n        self.k1 = k1\n        self.b = b\n\n    def fit(self, X):\n        \"\"\"\n        Parameters\n        ----------\n        X : sparse matrix, [n_samples, n_features] document-term matrix\n        \"\"\"\n        if not sp.sparse.issparse(X):\n            X = sp.sparse.csc_matrix(X)\n        if self.use_idf:\n            n_samples, n_features = X.shape\n            df = _document_frequency(X)\n            idf = np.log((n_samples - df + 0.5) / (df + 0.5))\n            self._idf_diag = sp.sparse.spdiags(idf, diags=0, m=n_features, n=n_features)\n\n        doc_len = X.sum(axis=1)\n        self._average_document_len = np.average(doc_len)\n\n        return self\n\n    def transform(self, X, copy=True):\n        \"\"\"\n        Parameters\n        ----------\n        X : sparse matrix, [n_samples, n_features] document-term matrix\n        copy : boolean, optional (default=True)\n        \"\"\"\n        if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n            # preserve float family dtype\n            X = sp.sparse.csr_matrix(X, copy=copy)\n        else:\n            # convert counts or binary occurrences to floats\n            X = sp.sparse.csr_matrix(X, dtype=np.float, copy=copy)\n\n        n_samples, n_features = X.shape\n\n        # Document length (number of terms) in each row\n        # Shape is (n_samples, 1)\n        doc_len = X.sum(axis=1)\n        # Number of non-zero elements in each row\n        # Shape is (n_samples, )\n        sz = X.indptr[1:] - X.indptr[0:-1]\n\n        # In each row, repeat `doc_len` for `sz` times\n        # Shape is (sum(sz), )\n        # Example\n        # -------\n        # dl = [4, 5, 6]\n        # sz = [1, 2, 3]\n        # rep = [4, 5, 5, 6, 6, 6]\n        rep = np.repeat(np.asarray(doc_len), sz)\n\n        # Compute BM25 score only for non-zero elements\n        nom = self.k1 + 1\n        denom = X.data + self.k1 * (1 - self.b + self.b * rep / self._average_document_len)\n        data = X.data * nom / denom\n\n        X = sp.sparse.csr_matrix((data, X.indices, X.indptr), shape=X.shape)\n\n        if self.use_idf:\n            check_is_fitted(self, '_idf_diag', 'idf vector is not fitted')\n\n            expected_n_features = self._idf_diag.shape[0]\n            if n_features != expected_n_features:\n                raise ValueError(\"Input has n_features=%d while the model\"\n                                 \" has been trained with n_features=%d\" % (\n                                     n_features, expected_n_features))\n            X = X * self._idf_diag\n\n        return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len_train = len(train)\nn_components = 50\n\n\nall_data = pd.concat([train, test], sort=True)\n\nvectorizer = make_pipeline(\n                TfidfVectorizer(ngram_range=(1, 3)),\n                make_union(\n                    TruncatedSVD(n_components=n_components, random_state=SEED),\n                    make_pipeline(\n                        BM25Transformer(use_idf=True, k1=2.0, b=0.75),\n                        TruncatedSVD(n_components=n_components, random_state=SEED)\n                    ),\n                    n_jobs=1,\n                ),\n             )\nall_question_title = vectorizer.fit_transform(all_data[\"question_title\"]).astype(np.float32)\nall_question_body = vectorizer.fit_transform(all_data[\"question_body\"]).astype(np.float32)\nall_answer = vectorizer.fit_transform(all_data[\"answer\"]).astype(np.float32)\n\nquestion_title = all_question_title[:len_train]\nquestion_title_test = all_question_title[len_train:]\nquestion_body = all_question_body[:len_train]\nquestion_body_test = all_question_body[len_train:]\nanswer = all_answer[:len_train]\nanswer_test = all_answer[len_train:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"question_title.shape, question_title_test.shape, \\\nquestion_body.shape, question_body_test.shape, \\\nanswer.shape, answer_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"l2_dist = lambda x, y: np.power(x - y, 2).sum(axis=1)\n\ncos_dist = lambda x, y: (x*y).sum(axis=1)\n\n\ndist_features_train = np.array([\n    l2_dist(train_question_body_dense, train_answer_dense),\n    l2_dist(train_question_title_dense, train_answer_dense),\n    l2_dist(train_question_body_dense, train_question_title_dense),\n    cos_dist(train_question_body_dense, train_question_title_dense),\n    cos_dist(train_question_body_dense, train_question_title_dense),\n    cos_dist(train_question_body_dense, train_question_title_dense),\n]).T\n\ndist_features_test = np.array([\n    l2_dist(test_question_body_dense, test_answer_dense),\n    l2_dist(test_question_title_dense, test_answer_dense),\n    l2_dist(test_question_body_dense, test_question_title_dense),\n    cos_dist(test_question_body_dense, test_question_title_dense),\n    cos_dist(test_question_body_dense, test_question_title_dense),\n    cos_dist(test_question_body_dense, test_question_title_dense),\n]).T\n\ndist_features_train.shape, dist_features_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = np.concatenate([question_title, question_body, answer,\n                                 np.vstack(train_question_body_dense),\n                                 np.vstack(train_question_title_dense),\n                                 np.vstack(train_answer_dense),\n                                 np.vstack(train_category_dense),\n                                 np.vstack(train_netloc_dense),\n                                 netloc_category_features_train,\n                                 np.hstack([train[[x + '_scaled' for x in all_num_features]].values]),\n                                 dist_features_train,\n                               ], axis = 1)\ntest_features = np.concatenate([question_title_test, question_body_test, answer_test,\n                                np.vstack(test_question_body_dense),\n                                np.vstack(test_question_title_dense),\n                                np.vstack(test_answer_dense),\n                                np.vstack(test_category_dense),\n                                np.vstack(test_netloc_dense),\n                                netloc_category_features_test,\n                                np.hstack([test[[x + '_scaled' for x in all_num_features]].values]),\n                                dist_features_test,\n                               ], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow.keras as keras \n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation, Dropout\nfrom sklearn.model_selection import KFold\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom scipy.stats import spearmanr\n\nnum_folds = 5\nfold_scores = []\nkf = KFold(n_splits = num_folds, shuffle = True, random_state = 42)\ntest_preds = np.zeros((len(test_features), len(target_cols)))\nfor train_index, val_index in kf.split(train_features):\n    train_X = train_features[train_index, :]\n    train_y = train[target_cols].iloc[train_index]\n    \n    val_X = train_features[val_index, :]\n    val_y = train[target_cols].iloc[val_index]\n    \n    model = Sequential([\n        Dense(2048, input_shape=(train_features.shape[1],)),\n        Activation('relu'),\n        Dense(1024),\n        Activation('relu'),\n        Dense(len(target_cols)),\n        Activation('sigmoid'),\n    ])\n    \n    es = EarlyStopping(monitor='val_loss', min_delta=0, patience=6, verbose=0, mode='auto', baseline=None, restore_best_weights=True)\n    model.compile(optimizer='adam',\n                  loss='binary_crossentropy')\n    \n    model.fit(np.asarray(train_X), np.asarray(train_y), epochs = 50, validation_data=(np.asarray(val_X), np.asarray(val_y)), callbacks = [es])\n    preds = model.predict(val_X)\n    overall_score = 0\n    for col_index, col in enumerate(target_cols):\n        overall_score += spearmanr(preds[:, col_index], val_y[col].values).correlation/len(target_cols)\n        print(col, spearmanr(preds[:, col_index], val_y[col].values).correlation)\n    fold_scores.append(overall_score)\n    print(overall_score)\n\n    test_preds += model.predict(test_features)/num_folds\n    \nprint(fold_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv(\"../input/google-quest-challenge/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col_index, col in enumerate(target_cols):\n    sub[col] = test_preds[:, col_index]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv(\"submission.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}