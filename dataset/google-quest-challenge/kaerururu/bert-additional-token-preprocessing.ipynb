{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# https://www.kaggle.com/c/google-quest-challenge/discussion/123770","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install ../input/sacremoses/sacremoses-master/ > /dev/null","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport pandas as pd\nimport argparse\nimport collections\nimport datetime\nimport gc\nimport glob\nimport logging\nimport math\nimport operator\nimport os \nimport pickle\nimport pkg_resources\nimport random\nimport re\nimport scipy.stats as stats\nimport seaborn as sns\nimport shutil\nimport sys\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.utils.data\nimport torch.nn.functional as F\nfrom contextlib import contextmanager\nfrom collections import OrderedDict\n# from nltk.stem import PorterStemmer\nfrom sklearn import metrics\nfrom sklearn import model_selection\nfrom sklearn.model_selection import KFold, GroupKFold\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\nfrom torch.nn import CrossEntropyLoss, MSELoss\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import (Dataset,DataLoader, RandomSampler, SequentialSampler,\n                              TensorDataset)\n%load_ext autoreload\n%autoreload 2\n%matplotlib inline\n# from tqdm import tqdm, tqdm_notebook, trange\nfrom tqdm._tqdm_notebook import tqdm_notebook as tqdm\ntqdm.pandas()\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\nimport warnings\nwarnings.filterwarnings('ignore')\n# from apex import amp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sys.path.insert(0, \"../input/transformers/transformers-master/\")\n\nfrom torch.utils.data.distributed import DistributedSampler\n\ntry:\n    from torch.utils.tensorboard import SummaryWriter\nexcept:\n    from tensorboardX import SummaryWriter\n\nfrom transformers import (WEIGHTS_NAME, BertConfig, BertModel, PreTrainedModel, BertPreTrainedModel,\n                          AlbertModel, AlbertForSequenceClassification, BertForSequenceClassification,\n                          AlbertTokenizer, AlbertConfig,\n                                  BertForQuestionAnswering, BertTokenizer,\n                                  XLMConfig, XLMForQuestionAnswering,\n                                  XLMTokenizer, XLNetConfig,\n                                  XLNetForQuestionAnswering,\n                                  XLNetTokenizer, DistilBertTokenizer, DistilBertModel)\n\nfrom transformers import AdamW # , WarmupLinearSchedule\nfrom transformers.tokenization_bert import (BasicTokenizer,\n                                                    whitespace_tokenize)\n\nfrom transformers.modeling_bert import BertLayer, BertEmbeddings, BertEncoder, BertPooler\n\nSEED = 1129\n\ndef seed_everything(seed=1129):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\nseed_everything(SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class BertForSequenceClassification_v2(BertPreTrainedModel):\n\n    def __init__(self, config, num_labels=30):\n\n        super(BertForSequenceClassification_v2, self).__init__(config)\n\n        # config.output_hidden_states=True (make sure)\n        self.num_labels = num_labels\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, self.num_labels)\n        self.init_weights()\n\n    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None,\n                position_ids=None, head_mask=None, inputs_embeds=None, labels=None, \n                extra_feats=None):\n\n        outputs = self.bert(input_ids,\n                            attention_mask=attention_mask,\n                            token_type_ids=token_type_ids,\n                            position_ids=position_ids,\n                            head_mask=head_mask,\n                            inputs_embeds=inputs_embeds)\n\n        # sequence_output = outputs[0]\n        # pooled_output = outputs[1]\n\n        hidden_states = outputs[2] #hidden_states: 12 layers tuples each is of (batch_size, sequence_length, hidden_size) + embedding``\n        # print(seq[-1].shape, seq[-1][:, 0].shape)\n\n        # we are taking zero because in my understanding that's the [CLS] token...\n        # idea is to pool last 4 layers as well instead of just the last one, since it's too close to the output\n        # layers, it might not be that efficient as it's more regulated by the o/p's..\n\n        h12 = hidden_states[-1][:, 0].reshape((-1, 1, 768))\n        h11 = hidden_states[-2][:, 0].reshape((-1, 1, 768))\n        h10 = hidden_states[-3][:, 0].reshape((-1, 1, 768))\n        h9  = hidden_states[-4][:, 0].reshape((-1, 1, 768))\n\n        all_h = torch.cat([h9, h10, h11, h12], 1)\n        mean_pool = torch.mean(all_h, 1)\n\n        pooled_output = self.dropout(mean_pool)\n        logits = self.classifier(pooled_output)\n        # outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n\n        return logits","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load pre-trained model tokenizer (vocabulary)\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True, additional_special_tokens = [\"[UNK]\", \"[SEP]\", \"[PAD]\", \"[CLS]\", \"[MASK]\", \"[ANS]\", \"[QBODY]\"])\n\n# Tokenize input (dummy example)\ntext = \"[CLS] Who was Jim Henson ? [QBODY] [ANS] Jim Henson was a puppeteer [SEP]\"\ntokenized_text = tokenizer.tokenize(text)\n#outputs\nprint(tokenized_text)\n#['[CLS]', 'who', 'was', 'jim', 'henson', '?', '[ANS]', '[QBODY]', 'jim', 'henson', 'was', 'a', 'puppet', '##eer', '[SEP]']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tokenizer.vocab[\"[ANS]\"] = -1\n# tokenizer.vocab[\"[QBODY]\"] = -1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Just in case someone is adding new tokens to transformers (Hugging Face)\n### Let's load a model and tokenizer\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased')\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n### Do some stuff to our model and tokenizer\n# Ex: add new tokens to the vocabulary and embeddings of our model (another way)\ntokenizer.add_tokens(['[ANS]', '[QBODY]'])\nmodel.resize_token_embeddings(len(tokenizer))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer.vocab[\"who\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train our model\n# train(model)\n\n### Now let's save our model and tokenizer to a directory\nmodel.save_pretrained('/kaggle/working')\ntokenizer.save_pretrained('/kaggle/working')\n### Reload the model and the tokenizer\nmodel = BertForSequenceClassification.from_pretrained('/kaggle/working')\ntokenizer = BertTokenizer.from_pretrained('/kaggle/working')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text = \"[CLS] Who was Jim Henson ? [QBODY] [ANS] Jim Henson was a puppeteer [SEP]\"\ntokenized_text = tokenizer.tokenize(text)\n#outputs\nprint(tokenized_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in tokenized_text:\n    try:\n        print(tokenizer.vocab[i])\n    except KeyError:\n        print(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}