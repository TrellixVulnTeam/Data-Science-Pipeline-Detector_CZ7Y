{"cells":[{"metadata":{},"cell_type":"markdown","source":"## If you like the kernel, consider upvoting it and the associated datasets:\n\nhttps://www.kaggle.com/abhishek/transformers\n\nhttps://www.kaggle.com/abhishek/sacremoses\n\nhttps://www.kaggle.com/abhishek/distilbertbaseuncased","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# OOF, Out of Folds에 대한 설명\n# https://daewonyoon.tistory.com/287","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Most of the code in this kernel comes directly from:\n\nhttps://www.kaggle.com/abazdyrev/use-features-oof\n\nPlease consider upvoting it!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# 필요 패키지 설치\n!pip install ../input/sacremoses/sacremoses-master/ > /dev/null","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport sys\nimport glob\nimport torch\n\n# 트랜스포머 패키지 임포트\nsys.path.insert(0, \"../input/transformers/transformers-master/\")\nimport transformers\nimport numpy as np\nimport pandas as pd\nimport math","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# n-사이즈의 말뭉치를 생성하는 제너레이터 정의\n# 제너레이터란?\n#\"제너레이터는 반복자(iterator)와 같은 루프의 작용을 컨트롤하기 위해 쓰여지는 특별한 함수 또는 루틴이다. 사실 모든 제너레이터는 반복자이다. 제너레이터는 배열이나 리스트를 리턴하는 함수와 비슷하며, \n#호출을 할 수 있는 파라메터를 가지고 있고, 연속적인 값들을 만들어 낸다. 하지만 한번에 모든 값을 포함한 배열을 만들어서 리턴하는 대신에 \n#yield 구문을 이용해 한 번 호출될 때마다 하나의 값만을 리턴하고, 이런 이유로 일반 반복자에 비해 아주 작은 메모리를 필요로 한다. 간단히 얘기하면 제너레이터는 반복자와 같은 역할을 하는 함수이다.\"\n# 즉, 전체 값을 출력하는 대신 특정 부분까지 출력하고, 그 지점을 기억한 뒤 나중에 이어서 출력하는 함수.\n\n\n\n\ndef generator_square(nums):\n    for i in nums:\n        yield i * i\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"emp = [1,2,3,4,5]\ngen=generator_square(emp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(next(gen))\n# print(next(gen))\n# print(next(gen))\n# print(next(gen))\n# print(next(gen))\n# print(next(gen))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef chunks(l, n):\n    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n    for i in range(0, len(l), n):\n        yield l[i:i + n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 데이터 전처리 함수\n\ndef fetch_vectors(string_list, batch_size=64):\n    # inspired by https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/\n    \n    # cuda로 GPU 사용 \n    DEVICE = torch.device(\"cuda\")\n    \n    # 기학습된 distillBERT tokenizer 호출, 트랜스포머 기반 기학습 모델 호출\n    tokenizer = transformers.DistilBertTokenizer.from_pretrained(\"../input/distilbertbaseuncased/\")\n    model = transformers.DistilBertModel.from_pretrained(\"../input/distilbertbaseuncased/\")\n    \n    # 모델 학습/활용시 GPU 사용\n    model.to(DEVICE)\n    \n    # 해당 함수의 입력값으로 받은 string_list와 batch_size로 위에 정의한 chunks를 실행하고, 그 출력물로 나온 data로 토큰화 \n    fin_features = []\n    # chunks 제너레이터로 입력받은 문장을 64글자 단위로 쪼개어 줌. 그 쪼개어진 문장(청크)을 data로 삼고, 이 청크마다 다음 작업을 실행\n    # data의 예시는 \"I am trying to understand what kinds of places the spam values o\"\n    \n    for data in chunks(string_list, batch_size):\n        tokenized = []\n        \n        \n        \n      \n        \n        \n        # 글자를 하나씩 쪼개어 x로 취급한 뒤, \n        for x in data:\n            # strip으로 스트링 끝부분의 공백을 제거하고, 띄어쓰기 단위로 단어를 list로 만듦(split함수). 그 중 300개 까지의 단어를 가져와 x에 저장\n            x = \" \".join(x.strip().split()[:300])\n            # 이 x를 디스틸 버트 토크나이저 함수로 토큰화 -> [101,1501,102]의 형태로 저장됨\n            \n            \n            # BPE 방식이란?\n            # 문장을 subwords로 쪼개고, 그 중 자주 사용되는 뭉치를 unit으로 사용하게끔 하는 방식. 이를 통해 용량을 아낄 수 있다.\n\n\n            # WPM은 tokenizer의 일종으로, 문장을 쪼개어 token으로 만들어 주는 작업을 수행함\n            # 이러한 BPE 방식과 유사하게, Word Piece Model은 언어에 대한 지식이 없어도 빈출하는 substring을 단어로 학습하게 됨. 이러한 방식을 통해 OOV 문제를 효과적으로 처리 가능\n            # WPM은 빈도기반 BPE와는 달리 likelihood 기반.\n\n            tok = tokenizer.encode(x, add_special_tokens=True)\n            # 토크나이즈 된 결과(tok)의 512번째 토큰까지를 tokenized에 저장\n            tokenized.append(tok[:512])\n            \n\n        # 최대 길이를 지정하고(padding할 최대 길이)\n    \n        max_len = 512\n\n        \n        # tokenize한 리스트에서 토큰을 하나씩 꺼냄. 최대 길이(512)에서 이 꺼낸 토큰의 길이를 뺀 만큼을 0으로 채워줌\n        # post padding 방식임을 알 수 있음. (keras.sequence.pad_sequences 의 padding argument 참조)\n        padded = np.array([i + [0] * (max_len - len(i)) for i in tokenized])\n        \n        # BERT 모델은 세가지 임베딩을 입력으로 받음 \n        #-> 1) 토큰 임베딩(입력 토큰들을 참조한 임베딩) 2) 세그멘트 임베딩(첫번째 문장인지, 두번째 문장인지) 3) 포지션 임베딩(문장 내 토큰의 절대적 위치)\n        \n        \n        # 어텐션마스크를 padding한 결과물에서 0이 아닌(즉 패딩으로 채워넣은 값이 아닌 진짜 토큰 값) 부분에 1로 표시\n        attention_mask = np.where(padded != 0, 1, 0)\n        \n        # padding 한 결과물을 토치의 텐서로 입력해줌.\n        input_ids = torch.tensor(padded).to(DEVICE)\n        # 어텐션 마스크도 토치 텐서로 입력 \n        attention_mask = torch.tensor(attention_mask).to(DEVICE)\n        \n        # 텐서의 미분값 계산을 지시하는 requires_grad 인자를 일시적으로 off 상태로 만들어주는 구문 (with torch.no_grad())\n        # 마지막 히든 스테이트에 model의 결과물을 저장\n        with torch.no_grad():\n            last_hidden_states = model(input_ids, attention_mask=attention_mask)\n            \n            \n        \n        # 피처값들을 fin_features 리스트에 덧붙여줌\n        features = last_hidden_states[0][:, 0, :].cpu().numpy()\n        fin_features.append(features)\n    \n    # np.vstack 함수로 vertical하게 matrix/vector를 붙여주는 연산\n    fin_features = np.vstack(fin_features)\n    return fin_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv(\"../input/google-quest-challenge/train.csv\").fillna(\"none\")\ndf_test = pd.read_csv(\"../input/google-quest-challenge/test.csv\").fillna(\"none\")\n\nsample = pd.read_csv(\"../input/google-quest-challenge/sample_submission.csv\")\ntarget_cols = list(sample.drop(\"qa_id\", axis=1).columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# fetch_vectors 뜯어 보기","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(type(df_train.question_body.values))\n# print(df_train.question_body.values[1])\n\nexample=chunks(df_train.question_body.values[1],64)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 문장을 64글자씩 쪼개어주는 것을 알 수 있음\n# print(next(example))\n# print(next(example))\n# print(next(example))\n# print(next(example))\n# print(next(example))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cuda로 GPU 사용 \nDEVICE = torch.device(\"cuda\")\n\n# 기학습된 distillbert tokenizer 호출, 트랜스포머 기반 모델 호출\ntokenizer = transformers.DistilBertTokenizer.from_pretrained(\"../input/distilbertbaseuncased/\")\nmodel = transformers.DistilBertModel.from_pretrained(\"../input/distilbertbaseuncased/\")\n\n# 모델 학습/활용시 GPU 사용\nmodel.to(DEVICE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x=\"expressed words are unshown   \"\n# print(x)\n# print(x.strip())\n# print(x.strip().split())\n# print(\" \".join(x.strip().split()[:300]))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# BPE 방식이란?\n# 문장을 subwords로 쪼개고, 그 중 자주 사용되는 뭉치를 unit으로 사용하게끔 하는 방식. 이를 통해 용량을 아낄 수 있다.\n\n\n# WPM은 tokenizer의 일종으로, 문장을 쪼개어 token으로 만들어 주는 작업을 수행함\n# 이러한 BPE 방식과 유사하게, Word Piece Model은 언어에 대한 지식이 없어도 빈출하는 substring을 단어로 학습하게 됨. 이러한 방식을 통해 OOV 문제를 효과적으로 처리 가능\n# WPM은 빈도기반 BPE와는 달리 likelihood 기반.\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data=\"I am trying to understand what kinds of places the spam values o\"\ntokenized=[]\nfin_features = []\n     \nfor x in data:\n    print(x)\n    x = \" \".join(x.strip().split()[:300])\n    print(x)\n    tok = tokenizer.encode(x, add_special_tokens=True)\n    print(tok)\n    tokenized.append(tok[:512])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenized","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_len = 512\n\n# tokenize한 리스트에서 토큰을 하나씩 꺼냄. 최대 길이(512)에서 이 꺼낸 토큰의 길이를 뺀 만큼을 0으로 채워줌\n# post padding 방식임을 알 수 있음. (keras.sequence.pad_sequences 의 padding argument 참조)\npadded = np.array([i + [0] * (max_len - len(i)) for i in tokenized])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(tokenized)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"padded","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# \n\nattention_mask = np.where(padded != 0, 1, 0)\ninput_ids = torch.tensor(padded).to(DEVICE)\nattention_mask = torch.tensor(attention_mask).to(DEVICE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with torch.no_grad():\n    last_hidden_states = model(input_ids, attention_mask=attention_mask)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(last_hidden_states)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"last_hidden_states[0][:, 0, :]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = last_hidden_states[0][:, 0, :].cpu().numpy()\nfin_features.append(features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fin_features = np.vstack(fin_features)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fin_features[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":">>> a = np.array([1, 2, 3])\n>>> b = np.array([2, 3, 4])\n>>> np.vstack((a,b))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### petch_vectors 뜯어보기 끝\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 실행하기","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_question_body_dense = fetch_vectors(df_train.question_body.values)\ntrain_answer_dense = fetch_vectors(df_train.answer.values)\n\ntest_question_body_dense = fetch_vectors(df_test.question_body.values)\ntest_answer_dense = fetch_vectors(df_test.answer.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_question_body_dense","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_question_body_dense.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport re\nimport gc\nimport pickle  \nimport random\nimport keras\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport keras.backend as K\n\nfrom keras.models import Model\nfrom keras.layers import Dense, Input, Dropout, Lambda\nfrom keras.optimizers import Adam\nfrom keras.callbacks import Callback\nfrom scipy.stats import spearmanr, rankdata\nfrom os.path import join as path_join\nfrom numpy.random import seed\nfrom urllib.parse import urlparse\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import MultiTaskElasticNet\n\nseed(42)\ntf.random.set_seed(42)\nrandom.seed(42)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data_dir = '../input/google-quest-challenge/'\ntrain = pd.read_csv(path_join(data_dir, 'train.csv'))\ntest = pd.read_csv(path_join(data_dir, 'test.csv'))\nprint(train.shape, test.shape)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"targets = [\n        'question_asker_intent_understanding',\n        'question_body_critical',\n        'question_conversational',\n        'question_expect_short_answer',\n        'question_fact_seeking',\n        'question_has_commonly_accepted_answer',\n        'question_interestingness_others',\n        'question_interestingness_self',\n        'question_multi_intent',\n        'question_not_really_a_question',\n        'question_opinion_seeking',\n        'question_type_choice',\n        'question_type_compare',\n        'question_type_consequence',\n        'question_type_definition',\n        'question_type_entity',\n        'question_type_instructions',\n        'question_type_procedure',\n        'question_type_reason_explanation',\n        'question_type_spelling',\n        'question_well_written',\n        'answer_helpful',\n        'answer_level_of_information',\n        'answer_plausible',\n        'answer_relevance',\n        'answer_satisfaction',\n        'answer_type_instructions',\n        'answer_type_procedure',\n        'answer_type_reason_explanation',\n        'answer_well_written'    \n    ]\n\ninput_columns = ['question_title', 'question_body', 'answer']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[targets]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> # Features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# ^: 일치하지 않는 것, .: 하나의 문자 ---> 즉, 하나의 문자가 아닌 것\n# *: 0개 이상 \n\n# 하나의 문자가 아닌 것중에, 다양한 0개 이상의 문자 앞에 나오는...?\n\n\nfind = re.compile(r\"^[^.]*\")\n\ntrain['netloc'] = train['url'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"re.findall(find,'d.d.sdasd.asasdasdd')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"urlparse(train['url'][0]).netloc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"re.findall(find,urlparse(train['url'][0]).netloc)[0]\n\nurlparse(train['url'][1]).netloc\n\nre.findall(find,urlparse(train['url'][1]).netloc)\n\nurlparse(train['url'][100]).netloc\n\nre.findall(find,urlparse(train['url'][100]).netloc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['netloc']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['netloc'] = test['url'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# onehotencoder로 label 달아주기\n\n# 나중에 모델 인풋으로 사용하게 되는데, URL에 있던 링크의 위치와 질문 카테고리를 모델의 input으로 사용하기 위함임.\n\nfeatures = ['netloc', 'category']\nmerged = pd.concat([train[features], test[features]])\nohe = OneHotEncoder()\nohe.fit(merged)\n\nfeatures_train = ohe.transform(train[features]).toarray()\nfeatures_test = ohe.transform(test[features]).toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[features]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(features_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# universal sentence encoder 모듈 호출 \n# The Universal Sentence Encoder encodes text into high dimensional vectors that can be used for text classification, semantic similarity, clustering and other natural language tasks.\n# tensorflow hub에서 저 인코더를 다운 받아 embed에 저장\n\nmodule_url = \"../input/universalsentenceencoderlarge4/\"\nembed = hub.load(module_url)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"module_url","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#학습용 embedding, 테스트용 embedding\n\nembeddings_train = {}\nembeddings_test = {}\n\n# 인풋으로 들어오는 칼럼의 text(질문 제목, 질문 내용, 대답) 대해,\n# print(input_columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text = 'question_title'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(text)\n\ntrain_text = train[text].str.replace('?', '.').str.replace('!', '.').tolist()\ntest_text = test[text].str.replace('?', '.').str.replace('!', '.').tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"curr_train_emb = []\ncurr_test_emb = []\nbatch_size = 4\nind = 0\nwhile ind*batch_size < len(train_text):\n    curr_train_emb.append(embed(train_text[ind*batch_size: (ind + 1)*batch_size])[\"outputs\"].numpy())\n    ind += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nind=0\nprint(train_text[ind*batch_size: (ind + 1)*batch_size])\nind=1\nprint(train_text[ind*batch_size: (ind + 1)*batch_size])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embed(train_text[ind*batch_size: (ind + 1)*batch_size])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 인덱스 * 배치사이즈가 train_text의 전체 길이보다 작다면,\n# curr_train_emb라는 리스트에 train_text의 문장을 4개씩 끊어서 embedding 함수에 넣어 나온 결과를 차곡차곡 쌓아줌\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ind = 0\nwhile ind*batch_size < len(test_text):\n    curr_test_emb.append(embed(test_text[ind*batch_size: (ind + 1)*batch_size])[\"outputs\"].numpy())\n    ind += 1    \n\nembeddings_train[text + '_embedding'] = np.vstack(curr_train_emb)\nembeddings_test[text + '_embedding'] = np.vstack(curr_test_emb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"curr_test_emb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embeddings_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for text in input_columns:\n    print(text)\n    # 물음표와 느낌표를 마침표로 바꿔주고, list로 변환\n    train_text = train[text].str.replace('?', '.').str.replace('!', '.').tolist()\n    test_text = test[text].str.replace('?', '.').str.replace('!', '.').tolist()\n    \n    #\n    curr_train_emb = []\n    curr_test_emb = []\n    batch_size = 4\n    ind = 0\n    while ind*batch_size < len(train_text):\n        curr_train_emb.append(embed(train_text[ind*batch_size: (ind + 1)*batch_size])[\"outputs\"].numpy())\n        ind += 1\n        \n    ind = 0\n    while ind*batch_size < len(test_text):\n        curr_test_emb.append(embed(test_text[ind*batch_size: (ind + 1)*batch_size])[\"outputs\"].numpy())\n        ind += 1    \n        \n    embeddings_train[text + '_embedding'] = np.vstack(curr_train_emb)\n    embeddings_test[text + '_embedding'] = np.vstack(curr_test_emb)\n    \ndel embed\nK.clear_session()\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# l2 norm, aka 유클리디언 거리\nl2_dist = lambda x, y: np.power(x - y, 2).sum(axis=1)\n\n# 코사인 거리\ncos_dist = lambda x, y: (x*y).sum(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 질문 제목, 질문 내용, 대답 내용을 임베딩한 벡터간의 거리를 유클리드 및 코사인 거리로 정리해 train 셋으로 만듦\n# 테스트로도 만들어 줌.\n\n\ndist_features_train = np.array([\n    l2_dist(embeddings_train['question_title_embedding'], embeddings_train['answer_embedding']),\n    l2_dist(embeddings_train['question_body_embedding'], embeddings_train['answer_embedding']),\n    l2_dist(embeddings_train['question_body_embedding'], embeddings_train['question_title_embedding']),\n    cos_dist(embeddings_train['question_title_embedding'], embeddings_train['answer_embedding']),\n    cos_dist(embeddings_train['question_body_embedding'], embeddings_train['answer_embedding']),\n    cos_dist(embeddings_train['question_body_embedding'], embeddings_train['question_title_embedding'])\n]).T\n\n\ndist_features_test = np.array([\n    l2_dist(embeddings_test['question_title_embedding'], embeddings_test['answer_embedding']),\n    l2_dist(embeddings_test['question_body_embedding'], embeddings_test['answer_embedding']),\n    l2_dist(embeddings_test['question_body_embedding'], embeddings_test['question_title_embedding']),\n    cos_dist(embeddings_test['question_title_embedding'], embeddings_test['answer_embedding']),\n    cos_dist(embeddings_test['question_body_embedding'], embeddings_test['answer_embedding']),\n    cos_dist(embeddings_test['question_body_embedding'], embeddings_test['question_title_embedding'])\n]).T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"[item for k, item in embeddings_train.items()][1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 이 모든 거리값과 피처를 학습과 테스트용 인풋데이터로 뭉쳐줌\n\nX_train = np.hstack([item for k, item in embeddings_train.items()] + [features_train, dist_features_train])\nX_test = np.hstack([item for k, item in embeddings_test.items()] + [features_test, dist_features_test])\ny_train = train[targets].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 총 3개의 데이터를 쌓아서 인풋으로 넣음\n# X_train 역시 3개를 쌓은 건데, embeddings_train은 다음과 같은 입력(train_text)을 embedding한 결과임.\n# features_train은 질문의 카테고리, 웹 주소 등을 ohe로 펼쳐 만든 행렬\n# dis_features_test는 질문 제목, 질문 내용, 대답 내용을 임베딩한 벡터간의 거리를 유클리드 및 코사인 거리로 계산한 결과\n\n# print(train_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 이렇게 만든 X_train 에, ['question_title', 'question_body', 'answer'] 이 세가지를 USE 방식으로 임베딩한 결과물까지 붙여 최종 인풋으로 만듦\n\n\nX_train = np.hstack((X_train, train_question_body_dense, train_answer_dense))\nX_test = np.hstack((X_test, test_question_body_dense, test_answer_dense))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_question_body_dense","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compatible with tensorflow backend\nclass SpearmanRhoCallback(Callback):\n    # Spearman Rho는 순위 상관관계의 비모수적인 추정법\n    # 이 대회에서 사용하는 metric임\n    # train early stopping을 위해 이 대회의 metric을 기준으로 개선이 일어나지 않으면 학습을 중단시키는 방식으로 callback 사용\n    \n    def __init__(self, training_data, validation_data, patience, model_name):\n        self.x = training_data[0]\n        self.y = training_data[1]\n        self.x_val = validation_data[0]\n        self.y_val = validation_data[1]\n        \n        self.patience = patience\n        self.value = -1\n        self.bad_epochs = 0\n        self.model_name = model_name\n\n    def on_train_begin(self, logs={}):\n        return\n\n    def on_train_end(self, logs={}):\n        return\n\n    def on_epoch_begin(self, epoch, logs={}):\n        return\n\n    def on_epoch_end(self, epoch, logs={}):\n        y_pred_val = self.model.predict(self.x_val)\n        rho_val = np.mean([spearmanr(self.y_val[:, ind], y_pred_val[:, ind] + np.random.normal(0, 1e-7, y_pred_val.shape[0])).correlation for ind in range(y_pred_val.shape[1])])\n        if rho_val >= self.value:\n            self.value = rho_val\n        else:\n            self.bad_epochs += 1\n        if self.bad_epochs >= self.patience:\n            print(\"Epoch %05d: early stopping Threshold\" % epoch)\n            self.model.stop_training = True\n            #self.model.save_weights(self.model_name)\n        print('\\rval_spearman-rho: %s' % (str(round(rho_val, 4))), end=100*' '+'\\n')\n        return rho_val\n\n    def on_batch_begin(self, batch, logs={}):\n        return\n\n    def on_batch_end(self, batch, logs={}):\n        return","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.shape[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 모델 구성부분\n\ndef create_model():\n    inps = Input(shape=(X_train.shape[1],))\n    x = Dense(512, activation='elu')(inps)\n    x = Dropout(0.2)(x)\n    x = Dense(y_train.shape[1], activation='sigmoid')(x)\n    model = Model(inputs=inps, outputs=x)\n    model.compile(\n        optimizer=Adam(lr=1e-4),\n        loss=['binary_crossentropy']\n    )\n    model.summary()\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_predictions = []\n\nkf = KFold(n_splits=5, random_state=42, shuffle=True)\n\n\nfor ind, (tr, val) in enumerate(kf.split(X_train)):\n    X_tr = X_train[tr]\n    y_tr = y_train[tr]\n    X_vl = X_train[val]\n    y_vl = y_train[val]\n    \n    model = create_model()\n    model.fit(\n        X_tr, y_tr, epochs=100, batch_size=32, validation_data=(X_vl, y_vl), verbose=True, \n        callbacks=[SpearmanRhoCallback(training_data=(X_tr, y_tr), validation_data=(X_vl, y_vl),\n                                       patience=5, model_name=f'best_model_batch{ind}.h5')]\n    )\n    all_predictions.append(model.predict(X_test))\n    \nmodel = create_model()\nmodel.fit(X_train, y_train, epochs=33, batch_size=32, verbose=False)\nall_predictions.append(model.predict(X_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Seed 값을 다르게 해놓고, MultiTaskElasticNet으로 학습\n\nkf = KFold(n_splits=5, random_state=2019, shuffle=True)\nfor ind, (tr, val) in enumerate(kf.split(X_train)):\n    X_tr = X_train[tr]\n    y_tr = y_train[tr]\n    X_vl = X_train[val]\n    y_vl = y_train[val]\n    \n    model = MultiTaskElasticNet(alpha=0.001, random_state=42, l1_ratio=0.5)\n    model.fit(X_tr, y_tr)\n    all_predictions.append(model.predict(X_test))\n    \nmodel = MultiTaskElasticNet(alpha=0.001, random_state=42, l1_ratio=0.5)\nmodel.fit(X_train, y_train)\nall_predictions.append(model.predict(X_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# rankdata : 리스트 내에서 순위를 매겨, 순위 리스트를 출력함\n\ntest_preds = np.array([np.array([rankdata(c) for c in p.T]).T for p in all_predictions]).mean(axis=0)\nmax_val = test_preds.max() + 1\ntest_preds = test_preds/max_val + 1e-12","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv(path_join(data_dir, 'sample_submission.csv'))\nsubmission[targets] = test_preds\nsubmission.to_csv(\"submission.csv\", index = False)\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}