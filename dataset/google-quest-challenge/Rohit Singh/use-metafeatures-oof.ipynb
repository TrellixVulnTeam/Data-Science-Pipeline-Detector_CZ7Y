{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport re\nimport gc\nimport pickle  \nimport random\nimport keras\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport keras.backend as K\n\nfrom keras.models import Model\nfrom keras.layers import Dense, Input, Dropout, Lambda\nfrom keras.optimizers import Adam\nfrom keras.callbacks import Callback\nfrom scipy.stats import spearmanr, rankdata\nfrom os.path import join as path_join\nfrom numpy.random import seed\nfrom urllib.parse import urlparse\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import MultiTaskElasticNet\nfrom nltk.corpus import stopwords\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nstop_words = set(stopwords.words('english'))\nimport string\n\nseed(42)\ntf.random.set_seed(42)\nrandom.seed(42)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data_dir = '../input/google-quest-challenge/'\ntrain = pd.read_csv(path_join(data_dir, 'train.csv'))\ntest = pd.read_csv(path_join(data_dir, 'test.csv'))\nprint(train.shape, test.shape)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"targets = [\n        'question_asker_intent_understanding',\n        'question_body_critical',\n        'question_conversational',\n        'question_expect_short_answer',\n        'question_fact_seeking',\n        'question_has_commonly_accepted_answer',\n        'question_interestingness_others',\n        'question_interestingness_self',\n        'question_multi_intent',\n        'question_not_really_a_question',\n        'question_opinion_seeking',\n        'question_type_choice',\n        'question_type_compare',\n        'question_type_consequence',\n        'question_type_definition',\n        'question_type_entity',\n        'question_type_instructions',\n        'question_type_procedure',\n        'question_type_reason_explanation',\n        'question_type_spelling',\n        'question_well_written',\n        'answer_helpful',\n        'answer_level_of_information',\n        'answer_plausible',\n        'answer_relevance',\n        'answer_satisfaction',\n        'answer_type_instructions',\n        'answer_type_procedure',\n        'answer_type_reason_explanation',\n        'answer_well_written'    \n    ]\n\ninput_columns = ['question_title', 'question_body', 'answer']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> # Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"find = re.compile(r\"^[^.]*\")\n\ntrain['netloc'] = train['url'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\ntest['netloc'] = test['url'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\n\n# train['self_answered'] =  train.apply(lambda x: 1 if x.question_user_name == x.answer_user_name else 0, axis=1)\n# test['self_answered'] =  test.apply(lambda x: 1 if x.question_user_name == x.answer_user_name else 0, axis=1)\n\n\nfeatures = ['netloc', 'category']\nmerged = pd.concat([train[features], test[features]])\nohe = OneHotEncoder()\nohe.fit(merged)\n\nfeatures_train = ohe.transform(train[features]).toarray()\nfeatures_test = ohe.transform(test[features]).toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"module_url = \"../input/universalsentenceencoderlarge4/\"\nembed = hub.load(module_url)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://stackoverflow.com/a/47091490/4084039\nimport re\nfrom tqdm import tqdm\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import sent_tokenize, word_tokenize\n\nps = PorterStemmer()\n\ndef decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase\n\n\ndef preprocess_text(df, column_name):\n    preprocessed = []\n    # tqdm is for printing the status bar\n    for sentance in tqdm(df[column_name].values):\n        sent = decontracted(sentance)\n        sent = sent.replace('\\\\r', ' ')\n        sent = sent.replace('\\\\\"', ' ')\n        sent = sent.replace('\\\\n', ' ')\n        sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n        sent = ' '.join(e for e in sent.split() if e.lower() not in stop_words)\n        # porter stemming to root word\n#         sent = ' '.join([ps.stem(word) for word in sent.split()])                    \n        preprocessed.append(sent.lower().strip())\n    return preprocessed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nembeddings_train = {}\nembeddings_test = {}\nfor text in input_columns:\n    print(text)\n    \n#     train_text = preprocess_text(train, text)\n#     test_text = preprocess_text(test, text)\n    \n    train_text = train[text].str.replace('?', '.').str.replace('!', '.').tolist()\n    test_text = test[text].str.replace('?', '.').str.replace('!', '.').tolist()\n    \n    curr_train_emb = []\n    curr_test_emb = []\n    batch_size = 4\n    ind = 0\n    while ind*batch_size < len(train_text):\n        curr_train_emb.append(embed(train_text[ind*batch_size: (ind + 1)*batch_size])[\"outputs\"].numpy())\n        ind += 1\n        \n    ind = 0\n    while ind*batch_size < len(test_text):\n        curr_test_emb.append(embed(test_text[ind*batch_size: (ind + 1)*batch_size])[\"outputs\"].numpy())\n        ind += 1    \n        \n    embeddings_train[text + '_embedding'] = np.vstack(curr_train_emb)\n    embeddings_test[text + '_embedding'] = np.vstack(curr_test_emb)\n    \n# del embed\nK.clear_session()\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# num of words in text\ntrain['num_words_ques_title'] = train['question_title'].apply(lambda x: len(x.split()))\ntrain['num_words_ques_body'] = train['question_body'].apply(lambda x: len(x.split()))\ntrain['num_words_answer'] = train['answer'].apply(lambda x: len(x.split()))\n\ntest['num_words_ques_title'] = test['question_title'].apply(lambda x: len(x.split()))\ntest['num_words_ques_body'] = test['question_body'].apply(lambda x: len(x.split()))\ntest['num_words_answer'] = test['answer'].apply(lambda x: len(x.split()))\n\n\n# num of unique words in text\ntrain['num_uniq_words_ques_title'] = train['question_title'].apply(lambda x: len(np.unique(x.split())))\ntrain['num_uniq_words_ques_body'] = train['question_body'].apply(lambda x: len(np.unique(x.split())))\ntrain['num_uniq_words_answer'] = train['answer'].apply(lambda x: len(np.unique(x.split())))\n\ntest['num_uniq_words_ques_title'] = test['question_title'].apply(lambda x: len(np.unique(x.split())))\ntest['num_uniq_words_ques_body'] = test['question_body'].apply(lambda x: len(np.unique(x.split())))\ntest['num_uniq_words_answer'] = test['answer'].apply(lambda x: len(np.unique(x.split())))\n\n\n# # num of characters in text\n# train['num_chars_ques_title'] = train['question_title'].apply(lambda x: len(x))\n# train['num_chars_ques_body'] = train['question_body'].apply(lambda x: len(x))\n# train['num_chars_answer'] = train['answer'].apply(lambda x: len(x))\n\n# test['num_chars_ques_title'] = test['question_title'].apply(lambda x: len(x))\n# test['num_chars_ques_body'] = test['question_body'].apply(lambda x: len(x))\n# test['num_chars_answer'] = test['answer'].apply(lambda x: len(x))\n\n\n# num of stop_words in text\ntrain['num_stop_words_ques_title'] = train['question_title'].apply(lambda x: len([word for word in x.split() if word in stop_words]))\ntrain['num_stop_words_ques_body'] = train['question_body'].apply(lambda x: len([word for word in x.split() if word in stop_words]))\ntrain['num_stop_words_answer'] = train['answer'].apply(lambda x: len([word for word in x.split() if word in stop_words]))\n\n\ntest['num_stop_words_ques_title'] = test['question_title'].apply(lambda x: len([word for word in x.split() if word in stop_words]))\ntest['num_stop_words_ques_body'] = test['question_body'].apply(lambda x: len([word for word in x.split() if word in stop_words]))\ntest['num_stop_words_answer'] = test['answer'].apply(lambda x: len([word for word in x.split() if word in stop_words]))\n\n\n# num of punctuations in text\ntrain['num_puncts_ques_title'] = train['question_title'].apply(lambda x: len([char for char in x if char in string.punctuation]))\ntrain['num_puncts_ques_body'] = train['question_body'].apply(lambda x: len([char for char in x if char in string.punctuation]))\ntrain['num_puncts_answer'] = train['answer'].apply(lambda x: len([char for char in x if char in string.punctuation]))\n\ntest['num_puncts_ques_title'] = test['question_title'].apply(lambda x: len([char for char in x if char in string.punctuation]))\ntest['num_puncts_ques_body'] = test['question_body'].apply(lambda x: len([char for char in x if char in string.punctuation]))\ntest['num_puncts_answer'] = test['answer'].apply(lambda x: len([char for char in x if char in string.punctuation]))\n\n\n# # num of upper case words in text\n# train['num_upper_words'] = train['text'].apply(lambda x: len([word for word in x.split() if word.isupper()]))\n# test['num_upper_words'] = test['text'].apply(lambda x: len([word for word in x.split() if word.isupper()]))\n\n\n# # num of title case words in text\n# train['num_title_words'] = train['text'].apply(lambda x: len([word for word in x.split() if word.istitle()]))\n# test['num_title_words'] = test['text'].apply(lambda x: len([word for word in x.split() if word.istitle()]))\n\n\n# Average length of words in text\ntrain['mean_len_words_ques_title'] = train['question_title'].apply(lambda x: np.mean([len(word) for word in x.split()]))\ntrain['mean_len_words_ques_body'] = train['question_body'].apply(lambda x: np.mean([len(word) for word in x.split()]))\ntrain['mean_len_words_ques_answer'] = train['answer'].apply(lambda x: np.mean([len(word) for word in x.split()]))\n\n\ntest['mean_len_words_ques_title'] = test['question_title'].apply(lambda x: np.mean([len(word) for word in x.split()]))\ntest['mean_len_words_ques_body'] = test['question_body'].apply(lambda x: np.mean([len(word) for word in x.split()]))\ntest['mean_len_words_ques_answer'] = test['answer'].apply(lambda x: np.mean([len(word) for word in x.split()]))\n\n\n# Adding text sentiment features\n\nanalyzer = SentimentIntensityAnalyzer()\n\n# # pos sentiment\ntrain['text_sent_pos_ques_title'] = train['question_title'].apply(lambda x: analyzer.polarity_scores(x)['pos'])\ntrain['text_sent_pos_ques_body'] = train['question_body'].apply(lambda x: analyzer.polarity_scores(x)['pos'])\ntrain['text_sent_pos_answer'] = train['answer'].apply(lambda x: analyzer.polarity_scores(x)['pos'])\n\ntest['text_sent_pos_ques_title'] = test['question_title'].apply(lambda x: analyzer.polarity_scores(x)['pos'])\ntest['text_sent_pos_ques_body'] = test['question_body'].apply(lambda x: analyzer.polarity_scores(x)['pos'])\ntest['text_sent_pos_answer'] = test['answer'].apply(lambda x: analyzer.polarity_scores(x)['pos'])\n\n# neg sentiment\ntrain['text_sent_neg_ques_title'] = train['question_title'].apply(lambda x: analyzer.polarity_scores(x)['neg'])\ntrain['text_sent_neg_ques_body'] = train['question_body'].apply(lambda x: analyzer.polarity_scores(x)['neg'])\ntrain['text_sent_neg_answer'] = train['answer'].apply(lambda x: analyzer.polarity_scores(x)['neg'])\n\ntest['text_sent_neg_ques_title'] = test['question_title'].apply(lambda x: analyzer.polarity_scores(x)['neg'])\ntest['text_sent_neg_ques_body'] = test['question_body'].apply(lambda x: analyzer.polarity_scores(x)['neg'])\ntest['text_sent_neg_answer'] = test['answer'].apply(lambda x: analyzer.polarity_scores(x)['neg'])\n\n# neu sentiment\ntrain['text_sent_neu_ques_title'] = train['question_title'].apply(lambda x: analyzer.polarity_scores(x)['neu'])\ntrain['text_sent_neu_ques_body'] = train['question_body'].apply(lambda x: analyzer.polarity_scores(x)['neu'])\ntrain['text_sent_neu_answer'] = train['answer'].apply(lambda x: analyzer.polarity_scores(x)['neu'])\n\ntest['text_sent_neu_ques_title'] = test['question_title'].apply(lambda x: analyzer.polarity_scores(x)['neu'])\ntest['text_sent_neu_ques_body'] = test['question_body'].apply(lambda x: analyzer.polarity_scores(x)['neu'])\ntest['text_sent_neu_answer'] = test['answer'].apply(lambda x: analyzer.polarity_scores(x)['neu'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def standardize_num_features(feat_name):\n    standardizer = StandardScaler(with_mean=False)\n    embeddings_train[feat_name] = standardizer.fit_transform(train[feat_name].values.reshape(-1,1))\n    embeddings_test[feat_name] = standardizer.transform(test[feat_name].values.reshape(-1,1))\n\n\nstandardize_num_features('num_words_ques_title') \nstandardize_num_features('num_words_ques_body') \nstandardize_num_features('num_words_answer')\n\n\nstandardize_num_features('num_uniq_words_ques_title')\nstandardize_num_features('num_words_ques_body')\nstandardize_num_features('num_uniq_words_answer')\n\n\nstandardize_num_features('num_stop_words_ques_title')\nstandardize_num_features('num_stop_words_ques_body')\nstandardize_num_features('num_stop_words_answer')\n\nstandardize_num_features('num_puncts_ques_title')\nstandardize_num_features('num_puncts_ques_body')\nstandardize_num_features('num_puncts_answer')\n\n\nstandardize_num_features('text_sent_pos_ques_title')\nstandardize_num_features('text_sent_pos_ques_body')\nstandardize_num_features('text_sent_pos_answer')\n\nstandardize_num_features('text_sent_neg_ques_title')\nstandardize_num_features('text_sent_neg_ques_body')\nstandardize_num_features('text_sent_neg_answer')\n\nstandardize_num_features('text_sent_neu_ques_title')\nstandardize_num_features('text_sent_neu_ques_body')\nstandardize_num_features('text_sent_neu_answer')\n\n\n# # num words in text\n# # embeddings_train['num_words_ques_title'] = standardizer.fit_transform(train['num_words_ques_title'].values.reshape(-1,1))\n# # embeddings_test['num_words_ques_title'] = standardizer.transform(test['num_words_ques_title'].values.reshape(-1,1))\n# # embeddings_train['num_words_ques_body'] = standardizer.fit_transform(train['num_words_ques_body'].values.reshape(-1,1))\n# # embeddings_test['num_words_ques_body'] = standardizer.transform(test['num_words_ques_body'].values.reshape(-1,1))\n# embeddings_train['num_words_answer'] = standardizer.fit_transform(train['num_words_answer'].values.reshape(-1,1))\n# embeddings_test['num_words_answer'] = standardizer.transform(test['num_words_answer'].values.reshape(-1,1))\n\n# # num unique words in text\n# # embeddings_train['num_uniq_words_ques_title'] = standardizer.fit_transform(train['num_uniq_words_ques_title'].values.reshape(-1,1))\n# # embeddings_test['num_uniq_words_ques_title'] = standardizer.transform(test['num_uniq_words_ques_title'].values.reshape(-1,1))\n# # embeddings_train['num_uniq_words_ques_body'] = standardizer.fit_transform(train['num_uniq_words_ques_body'].values.reshape(-1,1))\n# # embeddings_test['num_uniq_words_ques_body'] = standardizer.transform(test['num_uniq_words_ques_body'].values.reshape(-1,1))\n# embeddings_train['num_uniq_words_answer'] = standardizer.fit_transform(train['num_uniq_words_answer'].values.reshape(-1,1))\n# embeddings_test['num_uniq_words_answer'] = standardizer.transform(test['num_uniq_words_answer'].values.reshape(-1,1))\n\n# # num of stop_words in text\n# # embeddings_train['num_stop_words_ques_title'] = standardizer.fit_transform(train['num_stop_words_ques_title'].values.reshape(-1,1))\n# # embeddings_test['num_stop_words_ques_title'] = standardizer.transform(test['num_stop_words_ques_title'].values.reshape(-1,1))\n# # embeddings_train['num_stop_words_ques_body'] = standardizer.fit_transform(train['num_stop_words_ques_body'].values.reshape(-1,1))\n# # embeddings_test['num_stop_words_ques_body'] = standardizer.transform(test['num_stop_words_ques_body'].values.reshape(-1,1))\n# embeddings_train['num_stop_words_answer'] = standardizer.fit_transform(train['num_stop_words_answer'].values.reshape(-1,1))\n# embeddings_test['num_stop_words_answer'] = standardizer.transform(test['num_stop_words_answer'].values.reshape(-1,1))\n\n# # num of puncts in text\n# # embeddings_train['num_puncts_ques_title'] = standardizer.fit_transform(train['num_puncts_ques_title'].values.reshape(-1,1))\n# # embeddings_test['num_puncts_ques_title'] = standardizer.transform(test['num_puncts_ques_title'].values.reshape(-1,1))\n# # embeddings_train['num_puncts_ques_body'] = standardizer.fit_transform(train['num_puncts_ques_body'].values.reshape(-1,1))\n# # embeddings_test['num_puncts_ques_body'] = standardizer.transform(test['num_puncts_ques_body'].values.reshape(-1,1))\n# embeddings_train['num_puncts_answer'] = standardizer.fit_transform(train['num_puncts_answer'].values.reshape(-1,1))\n# embeddings_test['num_puncts_answer'] = standardizer.transform(test['num_puncts_answer'].values.reshape(-1,1))\n\n# embeddings_train['text_sent_pos_ques_title'] = standardizer.fit_transform(train['text_sent_pos_ques_title'].values.reshape(-1,1))\n# embeddings_test['text_sent_pos_ques_title'] = standardizer.transform(test['text_sent_pos_ques_title'].values.reshape(-1,1))\n\n\n# embeddings_train['text_sent_neg_ques_title'] = standardizer.fit_transform(train['text_sent_neg_ques_title'].values.reshape(-1,1))\n# embeddings_test['text_sent_neg_ques_title'] = standardizer.transform(test['text_sent_neg_ques_title'].values.reshape(-1,1))\n\n# embeddings_train['text_sent_neu_ques_title'] = standardizer.fit_transform(train['text_sent_neu_ques_title'].values.reshape(-1,1))\n# embeddings_test['text_sent_neu_ques_title'] = standardizer.transform(test['text_sent_neu_ques_title'].values.reshape(-1,1))\n\n# text_sent_neg_ques_title\n# text_sent_pos_answer\n# text_sent_pos_ques_body\n\n# embeddings_train['num_words_ques_title'] = train['num_words_ques_title']\n# embeddings_train['num_words_ques_body'] = train['num_words_ques_body']\n# embeddings_train['num_words_answer'] = train['num_words_answer']\n\n# embeddings_test['num_words_ques_title'] = np.vstack(test['num_words_ques_title'])\n# embeddings_test['num_words_ques_body'] = np.vstack(test['num_words_ques_body'])\n# embeddings_test['num_words_answer'] = np.vstack(test['num_words_answer'])\n\n\n# # num of unique words in text\n# embeddings_train['num_uniq_words_ques_title'] =  np.vstack(train['num_uniq_words_ques_title'])\n# embeddings_train['num_uniq_words_ques_body'] = np.vstack(train['num_uniq_words_ques_body'])\n# embeddings_train['num_uniq_words_answer'] = np.vstack(train['num_uniq_words_answer'])\n\n# embeddings_test['num_uniq_words_ques_title'] = np.vstack(test['num_uniq_words_ques_title'])\n# embeddings_test['num_uniq_words_ques_body'] = np.vstack(test['num_uniq_words_ques_body'])\n# embeddings_test['num_uniq_words_answer'] = np.vstack(test['num_uniq_words_answer'])\n\n\n# # num of characters in text\n# embeddings_train['num_chars_ques_title'] =  np.vstack(train['num_chars_ques_title'])\n# embeddings_train['num_chars_ques_body'] = np.vstack(train['num_chars_ques_body'])\n# embeddings_train['num_chars_answer'] = np.vstack(train['num_chars_answer'])\n\n# embeddings_test['num_chars_ques_title'] = np.vstack(test['num_chars_ques_title'])\n# embeddings_test['num_chars_ques_body'] = np.vstack(test['num_chars_ques_body'])\n# embeddings_test['num_chars_answer'] = np.vstack(test['num_chars_answer'])\n\n\n# # num of stop_words in text\n# embeddings_train['num_stop_words_ques_title'] =  np.vstack(train['num_stop_words_ques_title'])\n# embeddings_train['num_stop_words_ques_body'] = np.vstack(train['num_stop_words_ques_body'])\n# embeddings_train['num_stop_words_answer'] = np.vstack(train['num_stop_words_answer'])\n\n# embeddings_test['num_stop_words_ques_title'] = np.vstack(test['num_stop_words_ques_title'])\n# embeddings_test['num_stop_words_ques_body'] = np.vstack(test['num_stop_words_ques_body'])\n# embeddings_test['num_stop_words_answer'] = np.vstack(test['num_stop_words_answer'])\n\n\n# # num of punctuations in text\n# embeddings_train['num_puncts_ques_title'] =  np.vstack(train['num_puncts_ques_title'])\n# embeddings_train['num_puncts_ques_body'] = np.vstack(train['num_puncts_ques_body'])\n# embeddings_train['num_puncts_answer'] = np.vstack(train['num_puncts_answer'])\n\n# embeddings_test['num_puncts_ques_title'] = np.vstack(test['num_puncts_ques_title'])\n# embeddings_test['num_puncts_ques_body'] = np.vstack(test['num_puncts_ques_body'])\n# embeddings_test['num_puncts_answer'] = np.vstack(test['num_puncts_answer'])\n\n\n\n# # Average length of words in text\n# embeddings_train['mean_len_words_ques_title'] =  np.vstack(train['mean_len_words_ques_title'])\n# embeddings_train['mean_len_words_ques_body'] = np.vstack(train['mean_len_words_ques_body'])\n# embeddings_train['mean_len_words_ques_answer'] = np.vstack(train['mean_len_words_ques_answer'])\n\n# embeddings_test['mean_len_words_ques_title'] = np.vstack(test['mean_len_words_ques_title'])\n# embeddings_test['mean_len_words_ques_body'] = np.vstack(test['mean_len_words_ques_body'])\n# embeddings_test['mean_len_words_ques_answer'] = np.vstack(test['mean_len_words_ques_answer'])\n\n\n# # pos sentiment\n# embeddings_train['text_sent_pos_ques_title'] =  np.vstack(train['text_sent_pos_ques_title'])\n# embeddings_train['text_sent_pos_ques_body'] = np.vstack(train['text_sent_pos_ques_body'])\n# embeddings_train['text_sent_pos_answer'] = np.vstack(train['text_sent_pos_answer'])\n\n# embeddings_test['text_sent_pos_ques_title'] = np.vstack(test['text_sent_pos_ques_title'])\n# embeddings_test['text_sent_pos_ques_body'] = np.vstack(test['text_sent_pos_ques_body'])\n# embeddings_test['text_sent_pos_answer'] = np.vstack(test['text_sent_pos_answer'])\n\n# # neg sentiment\n# embeddings_train['text_sent_neg_ques_title'] =  np.vstack(train['text_sent_neg_ques_title'])\n# embeddings_train['text_sent_neg_ques_body'] = np.vstack(train['text_sent_neg_ques_body'])\n# embeddings_train['text_sent_neg_answer'] = np.vstack(train['text_sent_neg_answer'])\n\n# embeddings_test['text_sent_neg_ques_title'] = np.vstack(test['text_sent_neg_ques_title'])\n# embeddings_test['text_sent_neg_ques_body'] = np.vstack(test['text_sent_neg_ques_body'])\n# embeddings_test['text_sent_neg_answer'] = np.vstack(test['text_sent_neg_answer'])\n\n# # neu sentiment\n# embeddings_train['text_sent_neu_ques_title'] =  np.vstack(train['text_sent_neu_ques_title'])\n# embeddings_train['text_sent_neu_ques_body'] = np.vstack(train['text_sent_neu_ques_body'])\n# embeddings_train['text_sent_neu_answer'] = np.vstack(train['text_sent_neu_answer'])\n\n# embeddings_test['text_sent_neu_ques_title'] = np.vstack(test['text_sent_neu_ques_title'])\n# embeddings_test['text_sent_neu_ques_body'] = np.vstack(test['text_sent_neu_ques_body'])\n# embeddings_test['text_sent_neu_answer'] = np.vstack(test['text_sent_neu_answer'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embeddings_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"l2_dist = lambda x, y: np.power(x - y, 2).sum(axis=1)\ncos_dist = lambda x, y: (x*y).sum(axis=1)\n\ndist_features_train = np.array([\n    l2_dist(embeddings_train['question_title_embedding'], embeddings_train['answer_embedding']),\n    l2_dist(embeddings_train['question_body_embedding'], embeddings_train['answer_embedding']),\n    l2_dist(embeddings_train['question_body_embedding'], embeddings_train['question_title_embedding']),\n    cos_dist(embeddings_train['question_title_embedding'], embeddings_train['answer_embedding']),\n    cos_dist(embeddings_train['question_body_embedding'], embeddings_train['answer_embedding']),\n    cos_dist(embeddings_train['question_body_embedding'], embeddings_train['question_title_embedding']),\n]).T\n\ndist_features_test = np.array([\n    l2_dist(embeddings_test['question_title_embedding'], embeddings_test['answer_embedding']),\n    l2_dist(embeddings_test['question_body_embedding'], embeddings_test['answer_embedding']),\n    l2_dist(embeddings_test['question_body_embedding'], embeddings_test['question_title_embedding']),\n    cos_dist(embeddings_test['question_title_embedding'], embeddings_test['answer_embedding']),\n    cos_dist(embeddings_test['question_body_embedding'], embeddings_test['answer_embedding']),\n    cos_dist(embeddings_test['question_body_embedding'], embeddings_test['question_title_embedding'])\n]).T\n\nX_train = np.hstack([item for k, item in embeddings_train.items()] + [features_train, dist_features_train])\nX_test = np.hstack([item for k, item in embeddings_test.items()] + [features_test, dist_features_test])\ny_train = train[targets].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compatible with tensorflow backend\nclass SpearmanRhoCallback(Callback):\n    def __init__(self, training_data, validation_data, patience, model_name):\n        self.x = training_data[0]\n        self.y = training_data[1]\n        self.x_val = validation_data[0]\n        self.y_val = validation_data[1]\n        \n        self.patience = patience\n        self.value = -1\n        self.bad_epochs = 0\n        self.model_name = model_name\n\n    def on_train_begin(self, logs={}):\n        return\n\n    def on_train_end(self, logs={}):\n        return\n\n    def on_epoch_begin(self, epoch, logs={}):\n        return\n\n    def on_epoch_end(self, epoch, logs={}):\n        y_pred_val = self.model.predict(self.x_val)\n        rho_val = np.mean([spearmanr(self.y_val[:, ind], y_pred_val[:, ind] + np.random.normal(0, 1e-7, y_pred_val.shape[0])).correlation for ind in range(y_pred_val.shape[1])])\n        if rho_val >= self.value:\n            self.value = rho_val\n            self.model.save_weights(self.model_name)\n        else:\n            self.bad_epochs += 1\n        if self.bad_epochs >= self.patience:\n            print(\"Epoch %05d: early stopping Threshold\" % epoch)\n            self.model.stop_training = True\n        print('\\rval_spearman-rho: %s' % (str(round(rho_val, 4))), end=100*' '+'\\n')\n        return rho_val\n\n    def on_batch_begin(self, batch, logs={}):\n        return\n\n    def on_batch_end(self, batch, logs={}):\n        return","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers.normalization import BatchNormalization\n\n# def create_model():\n#     inps = Input(shape=(X_train.shape[1],))\n    \n#     x = Dense(512, activation='relu')(inps)\n#     x = Dropout(0.5)(x)\n# #     x = Dense(256, activation='relu')(x) \n# #     x = Dropout(rate=0.5)(x)\n#     x = Dense(128, activation='relu')(x)\n# #     x = Dropout(0.5)(x)\n#     x = BatchNormalization()(x)\n#     x = Dense(y_train.shape[1], activation='sigmoid')(x)\n#     model = Model(inputs=inps, outputs=x)\n#     model.compile(\n#         optimizer=Adam(lr=1e-4),\n#         loss=['binary_crossentropy']\n#     )\n#     model.summary()\n#     return model\n\n\ndef create_model():\n    inps = Input(shape=(X_train.shape[1],))\n    x = Dense(512, activation='relu')(inps) \n    x = Dropout(rate=0.5)(x)\n    x = Dense(y_train.shape[1], activation='sigmoid')(x)\n    model = Model(inputs=inps, outputs=x)\n    model.compile(\n        optimizer=Adam(lr=1e-4),\n        loss=['binary_crossentropy']\n    )\n    model.summary()\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_predictions = []\n\nkf = KFold(n_splits=5, random_state=42, shuffle=True)\nfor ind, (tr, val) in enumerate(kf.split(X_train)):\n    X_tr = X_train[tr]\n    y_tr = y_train[tr]\n    X_vl = X_train[val]\n    y_vl = y_train[val]\n    \n    model = create_model()\n    model.fit(\n        X_tr, y_tr, epochs=150, batch_size=32, validation_data=(X_vl, y_vl), verbose=True, \n        callbacks=[SpearmanRhoCallback(training_data=(X_tr, y_tr), validation_data=(X_vl, y_vl),\n                                       patience=5, model_name=u'best_model_batch.h5')]\n    )\n    model.load_weights('best_model_batch.h5')\n    all_predictions.append(model.predict(X_test))\n    \n    os.remove('best_model_batch.h5')\n    \nmodel = create_model()\nmodel.fit(X_train, y_train, epochs=30, batch_size=32, verbose=False)\nall_predictions.append(model.predict(X_test))\n    \nkf = KFold(n_splits=5, random_state=2019, shuffle=True)\nfor ind, (tr, val) in enumerate(kf.split(X_train)):\n    X_tr = X_train[tr]\n    y_tr = y_train[tr]\n    X_vl = X_train[val]\n    y_vl = y_train[val]\n    \n    model = MultiTaskElasticNet(alpha=0.001, random_state=42, l1_ratio=0.5)\n    model.fit(X_tr, y_tr)\n    all_predictions.append(model.predict(X_test))\n    \nmodel = MultiTaskElasticNet(alpha=0.001, random_state=42, l1_ratio=0.5)\nmodel.fit(X_train, y_train)\nall_predictions.append(model.predict(X_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds = np.array([np.array([rankdata(c) for c in p.T]).T for p in all_predictions]).mean(axis=0)\nmax_val = test_preds.max() + 1\ntest_preds = test_preds/max_val + 1e-12","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for column_ind in range(y_train.shape[1]):\n    curr_column = y_train[:, column_ind]\n    values = np.unique(curr_column)\n    map_quantiles = []\n    for val in values:\n        occurrence = np.mean(curr_column == val)\n        cummulative = sum(el['occurrence'] for el in map_quantiles)\n        map_quantiles.append({'value': val, 'occurrence': occurrence, 'cummulative': cummulative})\n            \n    for quant in map_quantiles:\n        pred_col = test_preds[:, column_ind]\n        q1, q2 = np.quantile(pred_col, quant['cummulative']), np.quantile(pred_col, min(quant['cummulative'] + quant['occurrence'], 1))\n        pred_col[(pred_col >= q1) & (pred_col <= q2)] = quant['value']\n        test_preds[:, column_ind] = pred_col","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv(path_join(data_dir, 'sample_submission.csv'))\nsubmission[targets] = test_preds\nsubmission.to_csv(\"submission.csv\", index = False)\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}