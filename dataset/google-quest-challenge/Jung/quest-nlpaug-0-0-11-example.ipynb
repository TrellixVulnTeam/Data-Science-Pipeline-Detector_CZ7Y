{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Short Introduction\nHi everybody, in the past few days, I played around with this great text augmentation repo : https://github.com/makcedward/nlpaug .\nI think that it has a potential, so I would love to share how to use it here. Please give the repo a star, and also please consider upvote the corresponding Kaggle dataset : https://www.kaggle.com/ratthachat/nlpaug0011\n\nNote that we can use wordnet-based or glove-based word augmentation offline. But I still cound't find how to use bert-family-based offline yet. So I turn on the internet for this kernel.\nNote also that this is only an early development version, so we shall see more and more very nice features soon!\n\nI am sorry I have not much time to write a good kernel. I will have to go for a family trip soon! Hope you all a happy long new year holliday!!!"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install ../input/sacremoses/sacremoses-master/ > /dev/null\n\nimport os\nimport sys\nimport glob\nimport torch\n\nsys.path.insert(0, \"../input/transformers/transformers-master/\")\nimport transformers\nimport numpy as np\nimport pandas as pd\nimport math","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls ../input/nlpaug0011/nlpaug-master","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install ../input/nlpaug0011/nlpaug-master #> /dev/null","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(transformers.__version__)\nprint(torch.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport re\nimport gc\nimport pickle  \nimport random\nimport tensorflow.keras as keras\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow.keras.backend as K\n\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import Callback\nfrom scipy.stats import spearmanr, rankdata, entropy\nfrom os.path import join as path_join\nfrom numpy.random import seed\nfrom urllib.parse import urlparse\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import KFold, GroupKFold\nfrom sklearn.linear_model import MultiTaskElasticNet\n\nfrom tqdm import tqdm_notebook\n\n\nSEED = 42\n\nseed(SEED)\ntf.random.set_seed(SEED )\nrandom.seed(SEED )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Test NLPAUG"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport nlpaug.augmenter.char as nac\nimport nlpaug.augmenter.word as naw\nimport nlpaug.augmenter.sentence as nas\nimport nlpaug.flow as naf\n\nfrom nlpaug.util import Action","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text = 'The quick brown fox jumps over the lazy dog .'\n# !ls -a ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aug_syn = naw.SynonymAug(aug_src='wordnet')\n# augmented_text = aug_syn.augment(text)\nprint(\"Original:\")\nprint(text)\nprint(\"Augmented Synonym Text:\")\nfor ii in range(5):\n    augmented_text = aug_syn.augment(text)\n    print(augmented_text)\n\naug = naw.AntonymAug()\n_text = 'Good boy is very good'\n\nprint(\"Original:\")\nprint(_text)\nprint(\"Augmented Antonym Text:\")\nfor ii in range(5):\n    augmented_text = aug.augment(_text)\n    print(augmented_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls ../input/nlpword2vecembeddingspretrained -l","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model_type: word2vec, glove or fasttext\naug_w2v = naw.WordEmbsAug(\n#     model_type='word2vec', model_path='../input/nlpword2vecembeddingspretrained/GoogleNews-vectors-negative300.bin',\n    model_type='glove', model_path='../input/nlpword2vecembeddingspretrained/glove.6B.300d.txt',\n    action=\"substitute\")\nprint(\"Original:\")\nprint(text)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aug_w2v.aug_p=0.1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Augmented Text:\")\nfor ii in range(5):\n    augmented_text = aug_w2v.augment(text)\n    print(augmented_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#BERT Augmentator\nTOPK=20 #default=100\nACT = 'insert' #\"substitute\"\n\naug_bert = naw.ContextualWordEmbsAug(\n    model_path='distilbert-base-uncased', \n    #device='cuda',\n    action=ACT, top_k=TOPK)\nprint(\"Original:\")\nprint(text)\nprint(\"Augmented Text:\")\nfor ii in range(5):\n    augmented_text = aug_bert.augment(text)\n    print(augmented_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aug = nas.ContextualWordEmbsForSentenceAug(\n#     model_path='gpt2'\n    model_path='xlnet-base-cased',\n#     model_path='distilgpt2', \n    top_k=TOPK\n)\n\nprint(\"Original:\")\nprint(text)\nprint(\"Augmented Text:\")\nfor ii in range(5):\n    augmented_text = aug.augment(text)\n    print(augmented_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# try Compose & Sometimes\n# make offline augmentation + pseudo label from my best ensemble\n# re-train\n\ntext = \"I have a question about programming language. Which is the best between python and R?\"\ntext = \"What is your recommended book on Bayesian Statistics?\"\n# text = \"How do you make a binary image in Photoshop?\"\n# text = \"Can an affidavit be used in Beit Din?\"\n\naug = naf.Sequential([\n    aug_bert,aug_w2v\n])\n\naug.augment(text, n=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aug2 = naf.Sometimes([\n    aug_bert,aug_w2v\n],aug_p=0.5, pipeline_p=0.5)\n\naug2.augment(text, n=10) # seems Sometimes has a bug, it still EveryTime, but results look better than sequential\n# However, in the manual aug_p pipeline_p are not clearly defined (have to look at the source)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Try augmentation for training data"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# around 3-4/5-7 mins for Distil/BertBase [300-350words to 512subwords] respectively\ntrain = pd.read_csv(\"../input/google-quest-challenge/train.csv\").fillna(\"none\")\ntest = pd.read_csv(\"../input/google-quest-challenge/test.csv\").fillna(\"none\")\n\nsample = pd.read_csv(\"../input/google-quest-challenge/sample_submission.csv\")\ntarget_cols = list(sample.drop(\"qa_id\", axis=1).columns)\ntargets = target_cols\ninput_columns = ['question_title', 'question_body', 'answer']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"texts = train['question_title'].values\n\nfor ii in tqdm_notebook(range(-7,-1)):\n    print(texts[ii])\n    print(aug.augment(texts[ii],n=1),'\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}