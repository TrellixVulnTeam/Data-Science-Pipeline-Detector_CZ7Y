{"cells":[{"metadata":{},"cell_type":"markdown","source":"ensembling 3 models\n\n- BERT fine-tuned model\n- Universal Sentence Encoder + Dense NN\n- Universal Sentence Encoder + ElasticNet"},{"metadata":{},"cell_type":"markdown","source":"# Install libraries from kaggle dataset\n\nFirst of all, we install `transformers`, `iterstrats` and its dependencies from **Kaggle dataset**, not from PyPI.\nThis is because the internet connection is forbidden in this competition rule."},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install ../input/sacremoses/sacremoses-master/ > /dev/null\n!pip install ../input/transformers/transformers-2.3.0/ > /dev/null\n!pip install ../input/iterative-stratification/iterative-stratification-master/ > /dev/null","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Import"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport random\nimport logging\nfrom typing import List, Dict, Tuple, Any","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, MinMaxScaler\nfrom sklearn.model_selection import GroupKFold\nimport joblib\nimport tensorflow as tf\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Sequential, Model, load_model\nfrom tensorflow.keras.layers import Dense, Activation, Dropout, Input, GlobalAveragePooling1D, Lambda\nfrom tensorflow.keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\nimport tensorflow.keras.backend as K\nimport tensorflow_hub\nimport transformers\nfrom transformers import TFBertModel, BertTokenizer\nfrom scipy.stats import spearmanr\nfrom tqdm import tqdm\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nfrom sklearn.linear_model import MultiTaskElasticNet","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Magic word"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.options.display.max_columns = None\ntf.get_logger().setLevel(logging.ERROR)\n\nrandom.seed(31)\nnp.random.seed(31)\ntf.random.set_seed(31)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load CSV"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/google-quest-challenge/train.csv')\ntarget_cols = pd.read_csv('../input/google-quest-challenge/sample_submission.csv').columns[1:].tolist()\nfeatures_train, targets_train = train.drop(columns=target_cols), train.loc[:, target_cols]\ndel train\n\nfeatures_test = pd.read_csv('../input/google-quest-challenge/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reusable class"},{"metadata":{"trusted":true},"cell_type":"code","source":"class BaseTransformer(BaseEstimator, TransformerMixin):\n    \n    def fit(self, x: pd.DataFrame, y = None):\n        return self\n    \n    def transform(self, x: pd.DataFrame) -> pd.DataFrame:\n        return x\n\n\nclass ColumnTransformer(BaseTransformer):\n    \n    def __init__(self, defs: Dict[str, BaseTransformer]):\n        self.defs = defs\n    \n    def fit(self, x: pd.DataFrame, y: np.ndarray = None):\n        for col, transformer in self.defs.items():\n            transformer.fit(x[col], y)\n        return self\n        \n    def transform(self, x: pd.DataFrame) -> pd.DataFrame:\n        xp = x.copy()\n        for col, transformer in self.defs.items():\n            xp[col] = transformer.transform(x[col])\n        return xp\n    \n    def fit_transform(self, x: pd.DataFrame, y: np.ndarray = None) -> pd.DataFrame:\n        xp = x.copy()\n        for col, transformer in self.defs.items():\n            if hasattr(transformer, 'fit_transform'):\n                xp[col] = transformer.fit_transform(x[col], y)\n            else:\n                xp[col] = transformer.fit(x[col], y).transform(x[col])\n        return xp\n\n\nclass WrappedOneHotEncoder(BaseTransformer):\n    \n    def __init__(self, col: str):\n        self.col = col\n        self.oe = OneHotEncoder(drop='first', sparse=False)\n    \n    def fit(self, x: pd.DataFrame, y = None):\n        self.oe.fit(x.loc[:, [self.col]])\n        return self\n    \n    def transform(self, x: pd.DataFrame) -> pd.DataFrame:\n        sparse_matrix = self.oe.transform(x.loc[:, [self.col]])\n        columns = ['{0}_onehot_{1}'.format(self.col, i) for i in range(sparse_matrix.shape[1])]\n        onehot = pd.DataFrame(sparse_matrix, index=x.index, columns=columns)\n        return pd.concat([x, onehot], axis=1)\n\n\nclass WrappedMinMaxScaler(BaseTransformer):\n    \n    def __init__(self, minmax: Tuple[float, float] = (1e-5, 1-1e-5)):\n        self.mms = MinMaxScaler(minmax)\n    \n    def fit_transform(self, x: pd.DataFrame) -> pd.DataFrame:\n        array = self.mms.fit_transform(x)\n        return pd.DataFrame(array, index=x.index, columns=x.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reusable functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def colwise_spearmanr(y_true: np.ndarray, y_pred: np.ndarray, cols: List[str]) -> Dict[str, float]:\n    return {c: spearmanr(y_true[:, i], y_pred[:, i]).correlation for i, c in enumerate(cols)}\n\n\ndef average_spearmanr(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n    return np.average([\n        spearmanr(y_t, y_p).correlation for y_t, y_p in zip(y_true.T, y_pred.T)\n    ])\n\n\ndef mini_batch(l: List, batch_size):\n    for i in range(0, len(l), batch_size):\n        yield l[i:i + batch_size]\n\n\ndef rank_average(arrays: List[np.ndarray]) -> np.ndarray:\n    rank_sum = np.sum([pd.DataFrame(a).rank().values for a in arrays], axis=0)\n    return rank_sum / (len(arrays) * arrays[0].shape[0])\n\n\ndef pd_average(df_ys: List[pd.DataFrame]) -> pd.DataFrame:\n    return pd.DataFrame(\n        np.average([df_y.values for df_y in df_ys], axis=0),\n        columns=df_ys[0].columns,\n        index=df_ys[0].index,\n    )\n\n\ndef pd_rank_average(df_ys: List[pd.DataFrame]) -> pd.DataFrame:\n    return pd.DataFrame(\n        rank_average([df_y.values for df_y in df_ys]),\n        columns=df_ys[0].columns,\n        index=df_ys[0].index,\n    )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Transformer"},{"metadata":{"trusted":true},"cell_type":"code","source":"class SecondLevelDomainExtracter(BaseTransformer):\n    \n    def transform(self, s_in: pd.Series) -> pd.Series:\n        s = s_in.str.extract(r'(^|.*\\.)([^\\.]+\\.[^\\.]+$)').iloc[:, 1]\n        s.name = s_in.name\n        return s\n\n\nclass UniversalSentenceEncoderEncoder(BaseTransformer):\n    \n    def __init__(self, col: str, model, batch_size: int = 16):\n        self.col = col\n        self.model = model\n        self.batch_size = batch_size\n    \n    def transform(self, df: pd.DataFrame) -> pd.DataFrame:\n        texts = df.loc[:, self.col].str.replace('?', '.').str.replace('!', '.').values\n        pbar_total = len(df) // self.batch_size + 1\n        pbar_desc = '{0} -> USE'.format(self.col)\n        use_features = np.vstack([\n            self.model(texts_batch)['outputs'].numpy() for texts_batch in tqdm(\n                mini_batch(texts, self.batch_size),\n                total=pbar_total,\n                desc=pbar_desc\n            )\n        ])\n        columns = ['{0}_use{1}'.format(self.col, i) for i in range(use_features.shape[1])]\n        return pd.concat([df, pd.DataFrame(use_features, index=df.index, columns=columns)], axis=1)\n\n    \nclass DistanceEngineerer(BaseTransformer):\n    \n    def __init__(self, col1: str, col2: str):\n        self.col1 = col1\n        self.col2 = col2\n    \n    @staticmethod\n    def extract_matrix(df: pd.DataFrame, base_col: str) -> np.ndarray:\n        columns = [c for c in df.columns if re.fullmatch(base_col + r'\\d+', c) is not None]\n        return df.loc[:, columns].values\n    \n    def transform(self, df_in: pd.DataFrame) -> pd.DataFrame:\n        df = df_in.copy()\n        a1 = self.extract_matrix(df, self.col1)\n        a2 = self.extract_matrix(df, self.col2)\n        assert a1.shape == a2.shape\n        l2_distance = np.power(a1 - a2, 2).sum(axis=1)\n        cos_distance = (a1 * a2).sum(axis=1)\n        df.loc[:, 'l2dist_{0}-{1}'.format(self.col1, self.col2)] = l2_distance\n        df.loc[:, 'cosdist_{0}-{1}'.format(self.col1, self.col2)] = cos_distance\n        return df\n\n\nclass QATokenizer(BaseTransformer):\n    \n    def __init__(self, tokenizer, max_len: int = 512):\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        \n    def tokenize(self, question_title: str, question_body: str, answer: str) -> Tuple[List[int], List[int], List[int]]:\n        question = '{0}[SEP]{1}'.format(question_title, question_body)\n        return self.tokenizer.encode_plus(question, answer, max_length=self.max_len, pad_to_max_length=True)\n    \n    def transform(self, df_in: pd.DataFrame) -> pd.DataFrame:        \n        tokenized = [self.tokenize(\n            row['question_title'],\n            row['question_body'],\n            row['answer']\n        ) for _, row in df_in.iterrows()]\n        input_ids = [d['input_ids'] for d in tokenized]\n        attention_mask = [d['attention_mask'] for d in tokenized]\n        token_type_ids = [d['token_type_ids'] for d in tokenized]\n        \n        return pd.DataFrame(np.hstack([input_ids, attention_mask, token_type_ids]), index=df_in.index)\n\n\nclass ColumnDropper(BaseTransformer):\n    \n    def __init__(self, cols: List[str]):\n        self.cols = cols\n        \n    def transform(self, df_in: pd.DataFrame) -> pd.DataFrame:\n        return df_in.drop(columns=self.cols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Estimators"},{"metadata":{"trusted":true},"cell_type":"code","source":"class BaseEnsembleCV:\n    \n    def __init__(self, n_splits: int = 5, verbose: bool = False, name: str = 'base_ecv'):\n        self.n_splits = n_splits\n        self.verbose = verbose\n        self.name = name\n    \n    def fit_fold(self, x_train, y_train, x_val, y_val, **params):\n        raise NotImplementedError()\n        \n    def save_fold(self, model, i_fold, dist):\n        raise NotImplementedError()\n    \n    def load_fold(self, i_fold, src):\n        raise NotImplementedError()\n        \n    def fit(self, df_x: pd.DataFrame, df_y: pd.DataFrame, groups = None, **params):\n        \n        x = df_x.values\n        y = df_y.values\n\n        if groups is None:\n            # thanks to [Neuron Engineer's kernel](https://www.kaggle.com/ratthachat/quest-cv-analysis-on-different-splitting-methods)\n            kfold = MultilabelStratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state=31).split(x, y)\n        else:\n            kfold = GroupKFold(n_splits=self.n_splits).split(x, y, groups=groups)\n                    \n        iterator = enumerate(kfold)\n        if self.verbose:\n            iterator = tqdm(iterator, total=self.n_splits, desc='k-fold')\n\n        self.models = []\n        _y_oof = np.zeros(y.shape)\n\n        for i_fold, (i_train, i_val) in iterator:\n\n            x_train, x_val = x[i_train], x[i_val]\n            y_train, y_val = y[i_train], y[i_val]\n            model = self.fit_fold(x_train, y_train, x_val, y_val, i_fold, **params)\n            self.models.append(model)\n            _y_oof[i_val] = model.predict(x_val)\n        _score = colwise_spearmanr(y, _y_oof, df_y.columns)\n        self.y_oof = pd.DataFrame(_y_oof, index=df_y.index, columns=df_y.columns)\n        self.score = pd.Series(_score)            \n        self.save()\n\n        return self\n    \n    def predict(self, df_x: pd.DataFrame) -> pd.DataFrame:\n        x = df_x.values\n        y = np.average([\n            m.predict(x) for m in self.models\n        ], axis=0)\n        return pd.DataFrame(y, index=df_x.index, columns=target_cols)\n    \n    def save(self, dist: str = '.'):\n        for i_fold, m in enumerate(self.models):\n            self.save_fold(m, i_fold, dist)\n        self.y_oof.to_csv('{0}/{1}.y_oof.csv'.format(dist, self.name), index=False)\n        self.score.to_csv('{0}/{1}.score.csv'.format(dist, self.name), header=True)\n    \n    def load(self, src: str = '.'):\n        self.models = []\n        for i_fold in range(self.n_splits):\n            self.models.append(self.load_fold(i_fold, src))\n        self.y_oof = pd.read_csv('{0}/{1}.y_oof.csv'.format(src, self.name))\n        self.score = pd.read_csv('{0}/{1}.score.csv'.format(src, self.name), index_col=0).iloc[:, 0]\n        return self","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ProgressBar(Callback):\n    \n    def __init__(self, pbar):\n        self.pbar = pbar\n        \n    def on_epoch_end(self, epoch, logs={}):\n        self.pbar.update()\n        self.pbar.set_postfix(logs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dense NN Estimator"},{"metadata":{"trusted":true},"cell_type":"code","source":"class DenseECV(BaseEnsembleCV):\n    \n    def __init__(self, name: str = 'dense_ecv'):\n        super().__init__(name=name)\n    \n    def fit_fold(self, x_train, y_train, x_val, y_val, i_fold: int, **in_params):\n        default_parmas = dict(\n            lr=0.0001,\n            epochs=50,\n            batch_size=32,\n        )\n        params = {**default_parmas, **in_params}\n        K.clear_session()\n        model = Sequential([\n            Dense(512, input_shape=(x_train.shape[1],)),\n            Activation('relu'),\n            Dropout(0.2),\n            Dense(y_train.shape[1]),\n            Activation('sigmoid'),\n        ])\n        es = EarlyStopping(patience=5)\n        model.compile(\n            optimizer=Adam(lr=params['lr']),\n            loss='binary_crossentropy',\n        )\n        with tqdm(desc='k-fold {0}/{1}'.format(i_fold+1, self.n_splits), total=params['epochs']) as pbar:\n            model.fit(\n                x_train, y_train,\n                epochs=params['epochs'],\n                batch_size=params['batch_size'],\n                validation_data=(x_val, y_val),\n                callbacks=[es, ProgressBar(pbar)],\n                verbose=0,\n            )\n        return model\n    \n    def save_fold(self, model, i_fold, dist):\n        model.save('{0}/{1}.model_{2}.h5'.format(dist, self.name, i_fold))\n    \n    def load_fold(self, i_fold, src):\n        return load_model('{0}/{1}.model_{2}.h5'.format(src, self.name, i_fold))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### MultiTask ElasticNet Estimatorhistory"},{"metadata":{"trusted":true},"cell_type":"code","source":"class ElasticNetECV(BaseEnsembleCV):\n    \n    def __init__(self, verbose: bool = True, name: str = 'elasticnet_ecv'):\n        super().__init__(verbose=verbose, name=name)\n        \n    def fit_fold(self, x_train, y_train, *_, **in_params):\n        default_params = dict(\n            alpha=0.001,\n            l1_ratio=0.5,\n            selection='random',\n        )\n        params = {**default_params, **in_params}\n        model = MultiTaskElasticNet(random_state=31, **params)\n        return model.fit(x_train, y_train)\n    \n    def save_fold(self, model, i_fold, dist):\n        joblib.dump(model, '{0}/{1}.model_{2}.joblib'.format(dist, self.name, i_fold))\n    \n    def load_fold(self, i_fold, src):\n        return joblib.load('{0}/{1}.model_{2}.joblib'.format(src, self.name, i_fold))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### BERT Fine Tuning Estimator"},{"metadata":{"trusted":true},"cell_type":"code","source":"class BertFineTuningECV(BaseEnsembleCV):\n    \n    def __init__(self, pretrained_path: str, seq_len: int = 512, name: str = 'bert_ecv'):\n        super().__init__(name=name)\n        self.pretrained_path = pretrained_path\n        self.seq_len = seq_len\n    \n    def build_model(self, params) -> Model:\n        s = self.seq_len\n        with tf.device('/cpu:0'):  # avoid OOM error\n            x = Input(self.seq_len * 3, dtype=tf.int32, name='x')\n            input_word_ids = Lambda(lambda x: x[:, :s], output_shape=(s,))(x)\n            attention_mask = Lambda(lambda x: x[:, s:s*2], output_shape=(s,))(x)\n            token_type_ids = Lambda(lambda x: x[:, s*2:], output_shape=(s,))(x)\n            bert_layer = TFBertModel.from_pretrained(self.pretrained_path)\n            last_hidden_layer, _ = bert_layer([input_word_ids, attention_mask, token_type_ids])\n            pooled = GlobalAveragePooling1D()(last_hidden_layer)\n            pooled = Dropout(0.2)(pooled)\n            y = Dense(30, activation='sigmoid', name='output')(pooled)\n            model = Model(inputs=x, outputs=y)\n        model.compile(\n            optimizer=Adam(lr=params['lr']),\n            loss='binary_crossentropy',\n        )\n        return model\n\n    def fit_fold(self, x_train, y_train, x_val, y_val, i_fold, **in_params):\n        default_params = dict(\n            lr=0.00003,\n            epochs=5,\n            batch_size=8,\n        )\n        params = {**default_params, **in_params}\n        es = EarlyStopping(patience=5)\n        cp = ModelCheckpoint('{0}.model_{1}.h5'.format(self.name, i_fold), save_best_only=True, save_weights_only=True)\n\n        K.clear_session()        \n        model = self.build_model(params)\n        h = model.fit(\n            x_train, y_train,\n            epochs=params['epochs'],\n            batch_size=params['batch_size'],\n            validation_data=(x_val, y_val),\n            callbacks=[es, cp],\n            verbose=1,\n        )\n        pd.DataFrame(h.history).to_csv('{0}_history_{1}.csv'.format(self.name, i_fold), index=False)\n        return model\n    \n    def save_fold(self, model, i_fold, dist):\n        return\n        \n    def load_fold(self, i_fold, src):\n        K.clear_session()\n        model = self.build_model(dict(lr=0.00003))\n        model.load_weights('{0}/{1}.model_{2}.h5'.format(src, self.name, i_fold))\n        return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"use_model = tensorflow_hub.load('../input/universalsentenceencoderlarge4')\nbert_tokenizer = BertTokenizer.from_pretrained('../input/transformers/bert-base-uncased')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"use_preprocess = Pipeline(steps=[\n    \n    # Universal Sentence Encoder\n    ('USE_question_title', UniversalSentenceEncoderEncoder('question_title', use_model)),\n    ('USE_question_body', UniversalSentenceEncoderEncoder('question_body', use_model)),\n    ('USE_answer', UniversalSentenceEncoderEncoder('answer', use_model)),\n    \n    # distance\n    ('distance_use_question_title-question_body', DistanceEngineerer('question_title_use', 'question_body_use')),\n    ('distance_use_question_title-answer', DistanceEngineerer('question_title_use', 'answer_use')),\n    ('distance_use_question_body-answer', DistanceEngineerer('question_body_use', 'answer_use')),\n    \n    # one-hot encode & drop columns\n    ('onehost_encode_and_drop_columns', Pipeline(steps=[\n\n        # abc.example.com -> example.com\n        ('extrace_sld', ColumnTransformer({\n            'host': SecondLevelDomainExtracter(),\n        })),\n\n        # one-hot encode\n        ('onehot_encode_host', WrappedOneHotEncoder('host')),\n        ('onehot_encode_category', WrappedOneHotEncoder('category')),\n\n    ]).fit(features_train)),\n    \n    ('drop_columns', ColumnDropper([\n        'qa_id', 'category', 'host', 'question_title', 'question_body', 'question_user_name', 'question_user_page',\n        'answer', 'answer_user_name', 'answer_user_page', 'url',\n    ])),\n])\n\nuse_densenn = Pipeline(steps=[\n    ('preprocess', use_preprocess),\n    ('estimate', DenseECV()),\n])\n\nuse_elasticnet = Pipeline(steps=[    \n    ('preprocess', use_preprocess),\n    ('estimate', ElasticNetECV())\n])\n\nbert_finetuning = Pipeline(steps=[    \n    ('tokenize', QATokenizer(bert_tokenizer)),\n    ('estimate', BertFineTuningECV('../input/transformers/bert-base-uncased')),\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"question_title = features_train['question_title']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# BERT Fine Tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"# _ = bert_finetuning.fit(features_train, targets_train, estimate__groups=question_title)\n_ = bert_finetuning['estimate'].load('../input/google-quest-challenge-trained-models')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bert_finetuning['estimate'].score.mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# UniversalSentenceEncder -> Dense NN"},{"metadata":{"trusted":true},"cell_type":"code","source":"# _ = use_densenn.fit(features_train, targets_train, estimate__groups=question_title)\n_ = use_densenn['estimate'].load('../input/google-quest-challenge-trained-models')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"use_densenn['estimate'].score.mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# UniversalSentenceEncder -> ElasticNet"},{"metadata":{"trusted":true},"cell_type":"code","source":"# _ = use_elasticnet.fit(features_train, targets_train, estimate__groups=question_title)\n_ = use_elasticnet['estimate'].load('../input/google-quest-challenge-trained-models')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"use_elasticnet['estimate'].score.mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ensemble"},{"metadata":{"trusted":true},"cell_type":"code","source":"average_spearmanr(\n    pd_average([\n        bert_finetuning['estimate'].y_oof,\n        use_densenn['estimate'].y_oof,\n        use_elasticnet['estimate'].y_oof,        \n    ]).values,\n    targets_train.values\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"average_spearmanr(\n    pd_rank_average([\n        bert_finetuning['estimate'].y_oof,\n        use_densenn['estimate'].y_oof,\n        use_elasticnet['estimate'].y_oof,        \n    ]).values,\n    targets_train.values\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"def to_submission(y: pd.DataFrame) -> pd.DataFrame:\n    return pd.concat([features_test.loc[:, 'qa_id'], y], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"WrappedMinMaxScaler().fit_transform(pd_average([\n    bert_finetuning.predict(features_test),\n    use_densenn.predict(features_test),\n    use_elasticnet.predict(features_test),\n])).pipe(to_submission).to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"}},"nbformat":4,"nbformat_minor":1}