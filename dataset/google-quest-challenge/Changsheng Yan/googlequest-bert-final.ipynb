{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!pip install ../input/transformers280/transformers/ > /dev/null","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom transformers import BertTokenizer, TFBertModel, BertConfig\nfrom scipy.stats import spearmanr\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nfrom keras.callbacks import Callback\nimport tensorflow.keras.backend as K\nfrom sklearn.model_selection import GroupKFold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_sequence_length = 380   # Higher will case google colab crashes\nn_epoch = 3                 # 3 or 4 would be good enough\nlearning_rate = 2e-5        # 5e-5, 3e-5, 2e-5\nn_fold = 5                  # Train Valid split, i.e. (1/n_fold) for validation on each fold\nbatch_size = 8              # Larger will cause google colab crashes\ndropout_rate = 0.1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_version = 'bertv11-3e/best_model_2.h5'\nDATA_PATH = '../input/google-quest-challenge'\nBERT_PATH = '../input/bertbaseuncased/bert-base-uncased'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = BertTokenizer(os.path.join(BERT_PATH, 'bert-base-uncased-vocab.txt'))\n\ndf_train = pd.read_csv(os.path.join(DATA_PATH, 'train.csv'))\ndf_test = pd.read_csv(os.path.join(DATA_PATH, 'test.csv'))\n\ninput_columns = list(df_train[['question_title', 'question_body', 'answer']].columns)\noutput_labels = list(df_train.columns[11:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_sequence(str1, str2, length, truncation_strategy='longest_first'):\n    \"\"\"\n    Process sequence or sequence pair into ids, masks and segments.\n    \"\"\"\n\n    inputs = tokenizer.encode_plus(str1, str2,\n        add_special_tokens=True,\n        max_length=length,\n        truncation_strategy=truncation_strategy)\n    \n    id =  inputs[\"input_ids\"]\n    mask = [1] * len(id)\n    segment = inputs[\"token_type_ids\"]\n    padding_length = length - len(id)\n    id = id + ([0] * padding_length)\n    mask = mask + ([0] * padding_length)\n    segment = segment + ([0] * padding_length)\n    \n    return id, mask, segment\n\ndef convert_to_bert_inputs(title, question, answer, tokenizer, max_sequence_length):\n    \"\"\"\n    Preprocess text input into tokens, then encode then into ids, masks and segments as input for the BERT transformer.\n    \"\"\"    \n    id_q, mask_q, segment_q = process_sequence(title + ' ' + question, None , max_sequence_length)\n    \n    id_a, mask_a, segment_a = process_sequence(answer, None, max_sequence_length)\n    \n    return id_q, mask_q, segment_q, id_a, mask_a, segment_a","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_input(df, columns, tokenizer, max_sequence_length):\n    ids_q, masks_q, segments_q = [], [], []\n    ids_a, masks_a, segments_a = [], [], []\n    for _, instance in tqdm(df[columns].iterrows()):\n        t, q, a = instance.question_title, instance.question_body, instance.answer\n\n        id_q, mask_q, segment_q, id_a, mask_a, segment_a = \\\n        convert_to_bert_inputs(t, q, a, tokenizer, max_sequence_length)\n        \n        ids_q.append(id_q)\n        masks_q.append(mask_q)\n        segments_q.append(segment_q)\n\n        ids_a.append(id_a)\n        masks_a.append(mask_a)\n        segments_a.append(segment_a)\n        \n    return [np.asarray(ids_q, dtype=np.int32), \n            np.asarray(masks_q, dtype=np.int32), \n            np.asarray(segments_q, dtype=np.int32),\n            np.asarray(ids_a, dtype=np.int32), \n            np.asarray(masks_a, dtype=np.int32), \n            np.asarray(segments_a, dtype=np.int32)]\n\ndef compute_output(df, columns):\n    return np.asarray(df[columns])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model():\n    q_id = tf.keras.layers.Input((max_sequence_length,), dtype=tf.int32)\n    a_id = tf.keras.layers.Input((max_sequence_length,), dtype=tf.int32)\n    \n    q_mask = tf.keras.layers.Input((max_sequence_length,), dtype=tf.int32)\n    a_mask = tf.keras.layers.Input((max_sequence_length,), dtype=tf.int32)\n    \n    q_seg = tf.keras.layers.Input((max_sequence_length,), dtype=tf.int32)\n    a_seg = tf.keras.layers.Input((max_sequence_length,), dtype=tf.int32)\n    \n    config = BertConfig() \n    # print(config)\n    config.output_hidden_states = False # Set to True to obtain hidden states\n    \n    # normally \".from_pretrained('bert-base-uncased')\", but because of no internet, the \n    # pretrained model has been downloaded manually and uploaded to kaggle. \n    bert_model = TFBertModel.from_pretrained(os.path.join(BERT_PATH, 'bert-base-uncased-tf_model.h5'), config=config)\n    \n    # Get the hidden embedding of the question/answer sequence.\n    q_emb = bert_model(q_id, attention_mask=q_mask, token_type_ids=q_seg)[0]\n    a_emb = bert_model(a_id, attention_mask=a_mask, token_type_ids=a_seg)[0]\n    \n    q = tf.keras.layers.GlobalAveragePooling1D()(q_emb)\n    a = tf.keras.layers.GlobalAveragePooling1D()(a_emb)\n    \n    x = tf.keras.layers.Concatenate()([q, a])\n\n    x = tf.keras.layers.Dense(1500)(x)\n    x = tf.keras.layers.Dense(1500)(x)\n    \n    x = tf.keras.layers.Dropout(dropout_rate)(x)\n    \n    x = tf.keras.layers.Dense(30, activation='sigmoid')(x)\n\n    model = tf.keras.models.Model(inputs=[q_id, q_mask, q_seg, a_id, a_mask, a_seg], outputs=x)\n    \n    return model\n  \ndef compute_rho(trues, preds):\n    rhos = []\n    for tcol, pcol in zip(np.transpose(trues), np.transpose(preds)):\n        rhos.append(spearmanr(tcol, pcol).correlation)\n    return np.nanmean(rhos)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntest_preds = []\nK.clear_session()\nmodel = create_model()\nmodel.load_weights(os.path.join('../input/', model_version))\ntest_inputs = compute_input(df_test, input_columns, tokenizer, max_sequence_length)\ntest_preds.append(model.predict(test_inputs))\n\ndf_submission = pd.read_csv(os.path.join(DATA_PATH, 'sample_submission.csv'))\ndf_submission.iloc[:, 1:] = np.average(test_preds, axis=0) # for weighted average set weights=[...]\ndf_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}