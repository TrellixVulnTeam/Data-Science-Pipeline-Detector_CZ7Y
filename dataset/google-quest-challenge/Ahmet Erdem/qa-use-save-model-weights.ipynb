{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_hub as hub\nimport numpy as np\nimport pandas as pd\nfrom nltk import tokenize\n\ntokenize.sent_tokenize(\"This is a sentence. This is another sentence.\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"embed_fn = hub.load('../input/universalsentenceencoderlarge4/')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/google-quest-challenge/train.csv\")\noutputs = df.columns[11:]\n\ndef count_words(data):\n    return len(str(data).split())\n\ndef count_words_unique(data):\n    return len(np.unique(str(data).split()))\n\ndef questionowords(data):\n    start_words = ['who', 'what', 'when', 'where', 'why', 'how', 'is', 'am','are','was','were','can','could','may','should','shall','does', 'do','did']\n    sents = tokenize.sent_tokenize(data)\n    qw = 0\n    for sent in sents:\n        if sent.lower().startswith(tuple(start_words)):\n            qw+=1\n    return qw\n\ndef questionmarks(data):\n    sents = tokenize.sent_tokenize(data)\n    qm = 0\n    for sent in sents:\n        qm += sent.count(\"?\")  \n    return qm\n\n\ndef get_numeric_features(df):\n    df[\"qt_wc\"] = df[\"question_title\"].apply(count_words)\n    df[\"qb_wc\"] = df[\"question_body\"].apply(count_words)\n    df[\"a_wc\"] = df[\"answer\"].apply(count_words)\n    df[\"qt_wcu\"] = df[\"question_title\"].apply(count_words_unique)\n    df[\"qb_wcu\"] = df[\"question_body\"].apply(count_words_unique)\n    df[\"a_wcu\"] = df[\"answer\"].apply(count_words_unique)\n\n\n    df['qb_qw'] = df['question_body'].apply(questionowords)\n    df['qt_qw'] = df['question_title'].apply(questionowords)\n    df['qb_qm'] = df['question_body'].apply(questionmarks)\n    df['qt_qm'] = df['question_title'].apply(questionmarks)\n    return df\n\ndf = get_numeric_features(df)\n\nfeatures = [\"qt_wc\", \"qb_wc\", \"a_wc\", \"qt_wcu\", \"qb_wcu\", \"a_wcu\",\n            \"qb_qw\", \"qt_qw\", \"qb_qm\", \"qt_qm\"]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm_notebook\n\nMAX_SEQ = 30\n\ndef get_sentences(x):\n    sentences = [s for s in tokenize.sent_tokenize(x) if s != \"\"]\n    if len(sentences) > MAX_SEQ:\n        return sentences[:MAX_SEQ]\n    return sentences + [\"\"]*(MAX_SEQ - len(sentences))\n\n\ndef get_use(df):\n    QT = embed_fn(df[\"question_title\"].values)[\"outputs\"].numpy()\n\n    A = np.zeros((df.shape[0], MAX_SEQ, 512), dtype=np.float32)\n    for i, x in tqdm_notebook(list(enumerate(df[\"answer\"].values))):\n        A[i] = embed_fn(get_sentences(x))[\"outputs\"].numpy()\n\n    QB = np.zeros((df.shape[0], MAX_SEQ, 512), dtype=np.float32)\n    for i, x in tqdm_notebook(list(enumerate(df[\"question_body\"].values))):\n        QB[i] = embed_fn(get_sentences(x))[\"outputs\"].numpy()\n\n    return QT, A, QB\n\nQT, A, QB = get_use(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow.keras.layers as KL\n\n\ndef nn_block(input_layer, size, dropout_rate, activation):\n    out_layer = KL.Dense(size, activation=None)(input_layer)\n    #out_layer = KL.BatchNormalization()(out_layer)\n    out_layer = KL.Activation(activation)(out_layer)\n    out_layer = KL.Dropout(dropout_rate)(out_layer)\n    return out_layer\n\ndef cnn_block(input_layer, size, dropout_rate, activation):\n    out_layer = KL.Conv1D(size, 1, activation=None)(input_layer)\n    #out_layer = KL.LayerNormalization()(out_layer)\n    out_layer = KL.Activation(activation)(out_layer)\n    out_layer = KL.Dropout(dropout_rate)(out_layer)\n    return out_layer\n    \ndef get_model():\n    qt_input = KL.Input(shape=(QT.shape[1],))\n\n    a_input = KL.Input(shape=(A.shape[1], A.shape[2]))\n    qb_input = KL.Input(shape=(QB.shape[1], QB.shape[2]))\n\n    dummy_input = KL.Input(shape=(1,))\n\n    a_emb = KL.Flatten()(KL.Embedding(2, 8)(dummy_input))\n    qb_emb = KL.Flatten()(KL.Embedding(2, 8)(dummy_input))\n\n    embs = KL.concatenate([KL.RepeatVector(MAX_SEQ)(a_emb), KL.RepeatVector(MAX_SEQ)(qb_emb)], axis=-2)\n\n    x = KL.concatenate([KL.SpatialDropout1D(0.7)(KL.RepeatVector(2*MAX_SEQ)(qt_input)), \n                        KL.SpatialDropout1D(0.3)(KL.concatenate([a_input, qb_input], axis=-2))])\n    x = KL.concatenate([x, embs])\n\n    x = cnn_block(x, 256, 0.1, \"relu\")\n    x = KL.concatenate([KL.GlobalAvgPool1D()(x), KL.GlobalMaxPool1D()(x)])\n\n    feature_input = KL.Input(shape=(len(features),))\n\n    hidden_layer = KL.concatenate([KL.BatchNormalization()(feature_input), x])\n    hidden_layer = nn_block(hidden_layer, 128, 0.1, \"relu\")\n\n    out = KL.Dense(len(outputs), activation=\"sigmoid\")(hidden_layer)\n\n    model = tf.keras.models.Model(inputs=[qt_input, a_input, qb_input, feature_input, dummy_input], outputs=out)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold, GroupKFold\nfrom sklearn.metrics import mean_squared_error\nfrom tensorflow.keras.optimizers import Nadam\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tensorflow.keras.backend import epsilon\nimport tensorflow.keras.backend as K\nimport os\n\nNUM_FOLDS = 10\nBATCH_SIZE = 32\nMODEL_FOLDER = \"use_models/\"\nos.mkdir(MODEL_FOLDER)\n\nmodels_w = []\n\ny = df[outputs].copy()\nfor col in outputs:\n    y[col] = y[col].rank(method=\"average\")\ny = MinMaxScaler().fit_transform(y.values)\n\ny_oof = np.zeros(y.shape)\n\nkfold = GroupKFold(NUM_FOLDS)\nfor fold, (train_ind, val_ind) in enumerate(kfold.split(y, y, groups=df[\"question_body\"].values)):\n    model_path = \"{folder}model{fold}.h5\".format(folder=MODEL_FOLDER, fold=fold)\n    print(model_path)\n    train_df, val_df = df.iloc[train_ind].copy(), df.iloc[val_ind].copy()\n    y_train, y_val = y[train_ind], y[val_ind]\n\n    model = get_model()\n    for lr, epochs in [(0.0002, 4), (0.002, 4), (0.0005, 3), (0.0002, 3)]: \n        model.compile(loss=\"binary_crossentropy\", optimizer=Nadam(lr=lr))\n        hist = model.fit([QT[train_ind], A[train_ind], QB[train_ind], train_df[features].values, np.ones(train_df.shape[0])], y_train,\n                         batch_size=BATCH_SIZE, epochs=epochs,\n                         validation_data=([QT[val_ind], A[val_ind], QB[val_ind], val_df[features].values, np.ones(val_df.shape[0])], y_val),\n                         verbose=0, shuffle=True)\n\n    y_oof[val_ind, :] = model.predict([QT[val_ind], A[val_ind], QB[val_ind], val_df[features].values, np.ones(val_df.shape[0])], batch_size=BATCH_SIZE, verbose=0)\n    model.save_weights(model_path)\n    K.clear_session()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import spearmanr\n\ndef evaluate(y, y_pred, verbose=False):\n    score = 0\n    for i in range(y.shape[1]):\n        col_score = spearmanr(y[:, i], y_pred[:, i])[0]\n        if verbose:\n            print(outputs[i], np.round(col_score, 3))\n        score += col_score/y.shape[1]\n    return np.round(score, 3)\n\nevaluate(y, y_oof, True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"use_df = pd.DataFrame(y_oof, columns=outputs)\nuse_df[\"qa_id\"] = df[\"qa_id\"].values\n\nuse_df.to_csv(\"use_oof.csv\", index=False)\nuse_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}