{"cells":[{"metadata":{},"cell_type":"markdown","source":"In this notebook i show one approach for predicting the 'question_type_spelling' label \nby building a custom feature using L2distance between USE embeddings of\n* a magic sentence and \n* input 'answer' data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow_hub as hub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PATH = \"../input/google-quest-challenge/\"\n# we are using the latest model file from tfhub repo\nUSE_PATH = \"https://tfhub.dev/google/universal-sentence-encoder-large/5\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reading in the input data\ndf_train = pd.read_csv(PATH+'train.csv')\nprint('training data shape \\t= ', df_train.shape)\n\n# we are focussing here only on the 'answer' column so filtering only that for processing\n#input_categories = list(df_train.columns[[1,2,5]])\ninput_categories = list(df_train.columns[[5]])\nprint('input categories \\t= ', input_categories)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We find how many rows match for this feature 'question_type_spelling' in training data"},{"metadata":{"trusted":true},"cell_type":"code","source":"abc = df_train['question_type_spelling'] > 0\nprint('Total rows with positive values for this feature = ',len(df_train[abc]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# loading the model file\nembed = hub.load(USE_PATH)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Compute the USE embedding vector for each of the rows in input 'answer' column"},{"metadata":{"trusted":true},"cell_type":"code","source":"if 1:\n  embeddings_train = {}\n  for text in input_categories:\n    print('Generating Embeddings for input category = ', text)\n    train_text = df_train[text].str.replace('?', '.').str.replace('!', '.').tolist()\n    \n    curr_train_emb = []\n    batch_size = 4\n    ind = 0\n    while ind*batch_size < len(train_text):\n        curr_train_emb.append(embed(train_text[ind*batch_size: (ind + 1)*batch_size]).numpy())\n        ind += 1\n        \n    embeddings_train[text + '_embedding'] = np.vstack(curr_train_emb)  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I define a magic sentence, which can be used to compare with the targets in answer column. \nThe goal here is find the closest rows which match the meaning or context for predicting the maximum labels in 'question_type_spelling'"},{"metadata":{"trusted":true},"cell_type":"code","source":"magic_sentence = \"how to pronounce English words\"\n#magic_sentence = \"pronounce and speak English word\"\n#magic_sentence = \"how to pronounce and spell English word \"\n#magic_sentence = \"How to pronounce English spelling word\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# copying the embedding vector to match the rows in train_df\nif 1:\n  curr_train_emb = []\n  spell_vector = embed([magic_sentence])\n  for i in range(0,df_train.shape[0]):\n      curr_train_emb.append(spell_vector.numpy())\n        \n  spell_embeddings_train = {}      \n  spell_embeddings_train['spell_embedding'] = np.vstack(curr_train_emb)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"l2_dist = lambda x, y: np.power(x - y, 2).sum(axis=1)\ncos_dist = lambda x, y: (x*y).sum(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# computing the l2 distance and cosine distance between the pairs\ndist_features_train = np.array([\n    l2_dist(embeddings_train['answer_embedding'], spell_embeddings_train['spell_embedding']),\n    cos_dist(embeddings_train['answer_embedding'], spell_embeddings_train['spell_embedding']),\n]).T\n\ndf_comp = pd.DataFrame(np.hstack([dist_features_train]),\n                       columns=['L2', 'Cosine'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sorting based on the L2 distance and analyzing the top n rows. During prediction we can use a threshold based on L2distance that gives the best accuracy."},{"metadata":{"trusted":true},"cell_type":"code","source":"lowest_l2_index = df_comp.sort_values('L2').head(12).index\nprint(lowest_l2_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for row in lowest_l2_index:\n    print(row, round(df_train.loc[row]['question_type_spelling'],3), df_train.loc[row]['question_title'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some Observations:\n* we are able to get 5 of 12 (accuracy of ~40%)\n* the data seems very noisy with inconsisent labelling\n\nRefer prediction output\n\n1. question title for 930 and 5199 are same, but one is +ve example and another -ve\n2. same disparity for question 2680 and 3579\n3. row 592 is -ve example, but has +ve labels in training set rows 362 & 4082 given below"},{"metadata":{},"cell_type":"markdown","source":"**For reference printing below the 11 rows in training set which had positive values for 'question_type_spelling'**"},{"metadata":{"trusted":true},"cell_type":"code","source":"rows = df_train['question_type_spelling'] > 0\ndf_train[rows]['question_title']","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}