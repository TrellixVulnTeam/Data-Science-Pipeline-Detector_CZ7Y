{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nfrom transformers import BertTokenizer, BertModel, RobertaModel, RobertaTokenizer\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport torch\nimport torch.nn as nn \nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nimport torch.optim as optim\nfrom scipy.stats import spearmanr\nfrom datetime import datetime\nfrom sklearn.model_selection import GroupKFold,KFold\nimport gc\nimport seaborn as sns\nimport transformers\nimport re\nfrom collections import Counter as ct\nimport html\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))]\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/google-quest-challenge/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"words = []\nfor w in train.question_title + train.question_body + train.answer:\n    words = words + w.split()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"counter = ct(words) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.question_title = train.question_title.apply(lambda x: \" \".join(\"[UNK]\" if counter[w] < 2 else w for w in x.split()))\ntrain.question_body = train.question_body.apply(lambda x: \" \".join(\"[UNK]\" if counter[w] < 2 else w for w in x.split()))\ntrain.answer = train.answer.apply(lambda x: \" \".join(\"[UNK]\" if counter[w] < 2 else w for w in x.split()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.question_title = train.question_title.apply(html.unescape)\ntrain.question_body = train.question_body.apply(html.unescape)\ntrain.answer = train.answer.apply(html.unescape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"targets = torch.tensor(train[['question_asker_intent_understanding',\n       'question_body_critical', 'question_conversational',\n       'question_expect_short_answer', 'question_fact_seeking',\n       'question_has_commonly_accepted_answer',\n       'question_interestingness_others', 'question_interestingness_self',\n       'question_multi_intent', 'question_not_really_a_question',\n       'question_opinion_seeking', 'question_type_choice',\n       'question_type_compare', 'question_type_consequence',\n       'question_type_definition', 'question_type_entity',\n       'question_type_instructions', 'question_type_procedure',\n       'question_type_reason_explanation', 'question_type_spelling',\n       'question_well_written', 'answer_helpful',\n       'answer_level_of_information', 'answer_plausible', 'answer_relevance',\n       'answer_satisfaction', 'answer_type_instructions',\n       'answer_type_procedure', 'answer_type_reason_explanation',\n       'answer_well_written']].values,dtype=torch.float32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class bertdataset:\n    def __init__(self, qtitle, qbody, answer, targets,tokenizer, max_length=512):\n        self.qtitle = qtitle\n        self.qbody = qbody\n        self.answer = answer\n        self.targets = targets\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.answer)\n\n    def __getitem__(self, item):\n        \n        question_title = self.qtitle[item]\n        question_body = self.qbody[item]\n        answer_text = self.answer[item]\n        \n        question_title = \" \".join(question_title.split())\n        question_body = \" \".join(question_body.split())\n        answer_text = \" \".join(answer_text.split())\n\n        inputs_q = self.tokenizer.encode_plus(\"[CLS]\" + question_title + \"[QBODY]\" + question_body + \"[SEP]\",           \n            pad_to_max_length=True,\n            max_length=self.max_length,\n        )\n        ids_q = inputs_q[\"input_ids\"]\n        token_type_ids_q = inputs_q[\"token_type_ids\"]\n        mask_q = inputs_q[\"attention_mask\"]\n        \n        inputs_a = self.tokenizer.encode_plus(\n            \"[CLS]\" + answer_text + \"[SEP]\",\n            pad_to_max_length=True,\n            max_length=self.max_length,\n        )\n        ids_a = inputs_a[\"input_ids\"]\n        token_type_ids_a = inputs_a[\"token_type_ids\"]\n        mask_a = inputs_a[\"attention_mask\"]\n        \n        return {\n        'ids_q': torch.tensor(ids_q, dtype=torch.long),\n        'mask_q': torch.tensor(mask_q, dtype=torch.long),\n        'token_type_ids_q': torch.tensor(token_type_ids_q, dtype=torch.long),\n        'ids_a': torch.tensor(ids_a, dtype=torch.long),\n        'mask_a': torch.tensor(mask_a, dtype=torch.long),\n        'token_type_ids_a': torch.tensor(token_type_ids_a, dtype=torch.long),\n        'targets': self.targets[item]\n        }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model\nclass nlp(nn.Module):\n    def __init__(self,bert_path):\n        super(nlp,self).__init__()\n        self.bert_path = bert_path\n        self.bert_model = BertModel.from_pretrained(self.bert_path, output_hidden_states=True)\n        self.drop = nn.Dropout(0.2)\n        self.dense = nn.Linear(768*2, 30)\n\n    def forward(self,ids_q,mask_q,token_type_ids_q,ids_a,mask_a,token_type_ids_a):\n        hidden_layers_q = self.bert_model(ids_q,attention_mask=mask_q,token_type_ids=token_type_ids_q)[2]\n        hidden_layers_a = self.bert_model(ids_a,attention_mask=mask_a,token_type_ids=token_type_ids_a)[2]\n        \n        \n        q12,a12 = hidden_layers_q[-1][:,0].view(-1,1,768),hidden_layers_a[-1][:,0].view(-1,1,768)\n        q11,a11 = hidden_layers_q[-2][:,0].view(-1,1,768),hidden_layers_a[-2][:,0].view(-1,1,768)\n        q10,a10 = hidden_layers_q[-3][:,0].view(-1,1,768),hidden_layers_a[-3][:,0].view(-1,1,768)\n        q9,a9 = hidden_layers_q[-4][:,0].view(-1,1,768),hidden_layers_a[-4][:,0].view(-1,1,768)\n        \n\n        q = torch.mean(torch.cat((q12,q11,q10,q9),axis = 1),axis = 1)\n        a = torch.mean(torch.cat((a12,a11,a10,a9),axis = 1),axis = 1)\n        \n        x = torch.cat((q,a),1)\n        x = self.dense(self.drop(x))\n        \n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',do_lower_case=True,\n#                                          add_specials_tokens = [\"[QBODY]\",\"[UNK]\"])\n\n# text = \"[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]\"\n\n\n# inputs_q = tokenizer.encode_plus(\n#             text,           \n#             pad_to_max_length=True,\n#             max_length=512,\n#             return_tensors = 'pt'\n#         )\n# ids_q = inputs_q[\"input_ids\"].type(torch.LongTensor).cuda()\n# token_type_ids_q = inputs_q[\"token_type_ids\"].type(torch.LongTensor).cuda()\n# mask_q = inputs_q[\"attention_mask\"].type(torch.LongTensor).cuda()\n\n# ids_q = torch.stack((ids_q,ids_q,ids_q)).squeeze(1)\n# token_type_ids_q = torch.stack((token_type_ids_q,token_type_ids_q,token_type_ids_q)).squeeze(1)\n# mask_q = torch.stack((mask_q,mask_q,mask_q)).squeeze(1)\n\n# inputs_a = tokenizer.encode_plus(\n#     text,\n#     pad_to_max_length=True,\n#     max_length=512,\n#     return_tensors = 'pt'\n# )\n# ids_a = inputs_a[\"input_ids\"].type(torch.LongTensor).cuda()\n# token_type_ids_a = inputs_a[\"token_type_ids\"].type(torch.LongTensor).cuda()\n# mask_a = inputs_a[\"attention_mask\"].type(torch.LongTensor).cuda()\n\n# ids_a = torch.stack((ids_a,ids_a,ids_a)).squeeze(1)\n# token_type_ids_a = torch.stack((token_type_ids_a,token_type_ids_a,token_type_ids_a)).squeeze(1)\n# mask_a = torch.stack((mask_a,mask_a,mask_a)).squeeze(1)\n\n# model = nlp('bert-base-uncased').cuda()\n# x,y = model(ids_q,mask_q,token_type_ids_q,ids_a,mask_a,token_type_ids_a)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_loop(dataset,model,optimizer,batch_size=6,epochs=1):\n    \n    batches = DataLoader(dataset,shuffle=False,batch_size=batch_size,num_workers=4)\n    criterion = nn.BCEWithLogitsLoss(reduce='mean')\n    \n    model.train()\n    for i in range(epochs):\n            for j,batch in enumerate(batches):\n                ids_q_batch = batch['ids_q'].type(torch.LongTensor).cuda()\n                mask_q_batch = batch['mask_q'].type(torch.LongTensor).cuda()\n                segments_q_batch = batch['token_type_ids_q'].type(torch.LongTensor).cuda()\n                ids_a_batch = batch['ids_a'].type(torch.LongTensor).cuda()\n                mask_a_batch = batch['mask_a'].type(torch.LongTensor).cuda()\n                segments_a_batch = batch['token_type_ids_a'].type(torch.LongTensor).cuda()\n                optimizer.zero_grad()\n                output = model(ids_q = ids_q_batch,\n                               mask_q = mask_q_batch,\n                               token_type_ids_q = segments_q_batch,\n                               ids_a = ids_a_batch,\n                               mask_a = mask_a_batch,\n                               token_type_ids_a = segments_a_batch)\n                target_batch = batch['targets'].cuda()\n                loss = criterion(output,target_batch)\n                loss.backward()\n                optimizer.step()\n                \n    loss = None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def eval_loop(dataset,model):\n    batches = DataLoader(dataset,shuffle=False,batch_size=6,num_workers=4)\n\n    model.eval()\n    pred_fold = []\n    target_fold = []\n    score_fold = 0\n    \n    with torch.no_grad():\n        for j,batch in enumerate(batches):\n\n            ids_q_batch = batch['ids_q'].type(torch.LongTensor).cuda()\n            mask_q_batch = batch['mask_q'].type(torch.LongTensor).cuda()\n            segments_q_batch = batch['token_type_ids_q'].type(torch.LongTensor).cuda()\n            ids_a_batch = batch['ids_a'].type(torch.LongTensor).cuda()\n            mask_a_batch = batch['mask_a'].type(torch.LongTensor).cuda()\n            segments_a_batch = batch['token_type_ids_a'].type(torch.LongTensor).cuda()\n            output = model(ids_q = ids_q_batch,\n                           mask_q = mask_q_batch,\n                           token_type_ids_q = segments_q_batch,\n                           ids_a = ids_a_batch,\n                           mask_a = mask_a_batch,\n                           token_type_ids_a = segments_a_batch)\n\n            out = torch.sigmoid(output).cpu().numpy()\n            target_fold.append(batch['targets'].numpy())\n            \n            pred_fold.append(out)\n            \n    pred_fold = np.vstack(pred_fold)\n    target_fold = np.vstack(target_fold)  \n    \n    for i in range(30):\n        score_fold += spearmanr(target_fold[:,i],pred_fold[:,i]).correlation\n        \n    return pred_fold,score_fold/30","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def test_loop(dataset,model):\n#     batches = DataLoader(dataset,shuffle=False,batch_size=4,num_workers=4)\n\n#     model.eval()\n#     pred = []\n\n#     with torch.no_grad():\n#         for j,batch in enumerate(batches):\n\n#             ids_batch = batch['ids'].type(torch.LongTensor).cuda()\n#             mask_batch = batch['mask'].type(torch.LongTensor).cuda()\n#             segments_batch = batch['segments'].type(torch.LongTensor).cuda()\n#             out = torch.sigmoid(model(ids = ids_batch,\n#                                       mask = mask_batch,\n#                                       token_type_ids = segments_batch)).cpu().numpy()\n            \n#             pred.append(out)\n\n#     return np.vstack(pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cross_val(tokenizer,cv=3):\n    oof_predictions = np.zeros((6079,30))\n    folds = GroupKFold(n_splits=cv)\n    \n    \n    \n    for fold,(train_index,valid_index) in enumerate(folds.split(X=train.question_body, groups=train.question_body)):\n\n        qtitle_train = train.iloc[train_index].question_title.values.astype(str).tolist()\n        qbody_train = train.iloc[train_index].question_body.values.astype(str).tolist()\n        answer_train = train.iloc[train_index].answer.values.astype(str).tolist()\n\n        train_loader = bertdataset(qtitle=qtitle_train,\n                                    qbody=qbody_train,\n                                    answer=answer_train,\n                                    targets=targets[train_index],\n                                    tokenizer=tokenizer)\n\n        qtitle_valid = train.iloc[valid_index].question_title.values.astype(str).tolist()\n        qbody_valid = train.iloc[valid_index].question_body.values.astype(str).tolist()\n        answer_valid = train.iloc[valid_index].answer.values.astype(str).tolist()\n\n        valid_loader = bertdataset(qtitle=qtitle_valid,\n                                    qbody=qbody_valid,\n                                    answer=answer_valid,\n                                    targets=targets[valid_index],\n                                    tokenizer=tokenizer)\n\n\n        model = nlp('bert-base-uncased').cuda()\n\n        optimizer = optim.AdamW(model.parameters(), lr=3e-5)\n\n        print(f'Fold {fold} started at ' + datetime.now().strftime(\"%H:%M\"))\n\n        train_loop(train_loader,model,optimizer,epochs=4)\n        \n        print('Training last layers a little bit more ' + datetime.now().strftime(\"%H:%M\"))\n        \n        for p in model.bert_model.parameters():\n            p.requires_grad = False\n            \n        optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=3e-5)\n        train_loop(train_loader,model,optimizer,batch_size=6,epochs=3)\n\n        print(f'Fold {fold} training finished, predicting... ' + datetime.now().strftime(\"%H:%M\"))\n\n        p,s = eval_loop(valid_loader,model) \n        oof_predictions[valid_index] = p\n#       test_predictions[fold] = (test_loop(test_loader,model))\n\n        print(f'Fold {fold} finished at ' + datetime.now().strftime(\"%H:%M\") + f' with score: {s}')\n\n        torch.save(model.state_dict(),f\"fold_{fold}_r.pt\")\n\n        model = None\n        optimizer = None\n        torch.cuda.empty_cache()\n\n    return oof_predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',do_lower_case=True,\n                                         add_specials_tokens = [\"[QBODY]\",\"[UNK]\"])\noof_predictions = cross_val(tokenizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# n = train['url'].apply(lambda x:(('ell.stackexchange.com' in x) or ('english.stackexchange.com' in x))).tolist()\n# spelling=[]\n\n# for x in n:\n#     if x:\n#         spelling.append(0.5)\n#     else:\n#         spelling.append(0.)\n\n# spearmanr(train['question_type_spelling'],np.array(spelling)).correlation","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}