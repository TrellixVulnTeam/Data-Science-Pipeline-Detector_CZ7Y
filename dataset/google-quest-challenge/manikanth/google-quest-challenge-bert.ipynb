{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Aim of this Challenge:** \n\nCreate intelligent question and answer systems that can reliably predict context without relying on complicated and opaque rating guidelines.","metadata":{"editable":false}},{"cell_type":"markdown","source":"# The Business Problem:\n\n\nTo create a more human-like question and answering system can answer the provided question having the intuitive understanding of the question. This can attract users and address their question more human-like and this can also increase the number of user participation in the question answering forms and create human-like conversation chat boxes.\n","metadata":{"editable":false}},{"cell_type":"markdown","source":"# Exploring dataset","metadata":{"editable":false}},{"cell_type":"code","source":"# importing the required libraries \n\nimport pandas as pd\nimport  numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:23:34.43346Z","iopub.execute_input":"2022-02-22T00:23:34.433892Z","iopub.status.idle":"2022-02-22T00:23:35.338938Z","shell.execute_reply.started":"2022-02-22T00:23:34.433801Z","shell.execute_reply":"2022-02-22T00:23:35.338098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = pd.read_csv('/kaggle/input/google-quest-challenge/train.csv')\ntest_dataset = pd.read_csv('/kaggle/input/google-quest-challenge/test.csv')\nsample_submission_dataset = pd.read_csv('/kaggle/input/google-quest-challenge/sample_submission.csv')\n\nprint(\"Train shape:\", train_dataset.shape)\nprint(\"Test shape:\", test_dataset.shape)\nprint(\"Sample submission shape:\", sample_submission_dataset.shape)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:23:35.340458Z","iopub.execute_input":"2022-02-22T00:23:35.340698Z","iopub.status.idle":"2022-02-22T00:23:35.762864Z","shell.execute_reply.started":"2022-02-22T00:23:35.340669Z","shell.execute_reply":"2022-02-22T00:23:35.761111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observations:\n* In train dataset we have 41 column and 6079 rows(instances/training points).\n* in test dataset we have only 11 column and 476 rows(instances/test points).\n* in submission dataset we have 31 column and 476 rows.","metadata":{"editable":false}},{"cell_type":"code","source":"# Check for train data samples\ntrain_dataset.head(2)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:23:35.764261Z","iopub.execute_input":"2022-02-22T00:23:35.764646Z","iopub.status.idle":"2022-02-22T00:23:35.79654Z","shell.execute_reply.started":"2022-02-22T00:23:35.764607Z","shell.execute_reply":"2022-02-22T00:23:35.795773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# getting basic info from training data\ntrain_dataset.info()","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:23:35.798955Z","iopub.execute_input":"2022-02-22T00:23:35.799251Z","iopub.status.idle":"2022-02-22T00:23:35.827041Z","shell.execute_reply.started":"2022-02-22T00:23:35.799217Z","shell.execute_reply":"2022-02-22T00:23:35.826372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> **Observations:** There are 10 features and no null values and 10 are having type as object and 30 labels are having type as float64 \n\n### Features:\n 1   question_title                         \n 2   question_body                           \n 3   question_user_name                      \n 4   question_user_page                     \n 5   answer                                 \n 6   answer_user_name                      \n 7   answer_user_page                        \n 8   url                                     \n 9   category                                \n 10  host      ","metadata":{"editable":false}},{"cell_type":"code","source":"# Describing the train data\ntrain_dataset.describe()","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:23:35.828259Z","iopub.execute_input":"2022-02-22T00:23:35.828504Z","iopub.status.idle":"2022-02-22T00:23:35.908053Z","shell.execute_reply.started":"2022-02-22T00:23:35.82847Z","shell.execute_reply":"2022-02-22T00:23:35.907389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Observations:** \n* In the above 41 columns, 10 are feature and 30 are the class labels and one column qa_id is the unique ID for every instance.\n* **21 class** labels are for **questions** that is the label  that starts with \"question_...\"\n* **9 class** labels are for **answers** that is the label  which starts with \"answer_...\"\n\n* Total we have **30 Class Lables**","metadata":{"editable":false}},{"cell_type":"code","source":"# Let's see the list of column names\n\nlist(train_dataset.columns[1:])","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:23:35.909235Z","iopub.execute_input":"2022-02-22T00:23:35.909915Z","iopub.status.idle":"2022-02-22T00:23:35.916518Z","shell.execute_reply.started":"2022-02-22T00:23:35.909884Z","shell.execute_reply":"2022-02-22T00:23:35.915769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset.head()","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:23:35.918127Z","iopub.execute_input":"2022-02-22T00:23:35.91876Z","iopub.status.idle":"2022-02-22T00:23:35.941828Z","shell.execute_reply.started":"2022-02-22T00:23:35.918699Z","shell.execute_reply":"2022-02-22T00:23:35.941075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Checking density of words & characters present in the `question_title` feature","metadata":{"editable":false}},{"cell_type":"code","source":"import seaborn as sns\n\n\ndef word_count(sentense):\n    sentense = sentense.strip()\n\n    return len(sentense.split(\" \"))\n\n\nfig, ax = plt.subplots(1,2, figsize = ( 20 , 5))\n\n\nquestion_title_lengths_train = train_dataset['question_title'].apply(len)\nquestion_title_lengths_test = test_dataset['question_title'].apply(len)\nquestion_title_lengths_train_words = train_dataset['question_title'].apply(word_count)\nquestion_title_lengths_test_words = test_dataset['question_title'].apply(word_count)\n\n\nsns.histplot(question_title_lengths_train, label=\"Train\", kde=True, stat=\"density\", linewidth=0,  color=\"red\", ax=ax[0])\nsns.histplot(question_title_lengths_test, label=\"Test\", kde=True, stat=\"density\", linewidth=0,  color=\"blue\", ax=ax[0])\nsns.histplot(question_title_lengths_train_words, label=\"Train\", kde=True, stat=\"density\", linewidth=0,  color=\"red\", ax=ax[1])\nsns.histplot(question_title_lengths_test_words, label=\"Test\", kde=True, stat=\"density\", linewidth=0,  color=\"blue\", ax=ax[1])\n\n# Set label for x-axis\nax[0].set_xlabel( \"No. of characters\" , size = 12 )\n  \n# Set label for y-axis\nax[0].set_ylabel( \"Density of character\" , size = 12 )\n  \n# Set title for plot\nax[0].set_title( \"Density of characters in 'question_title' feature\\n\" , size = 15 )\n\nax[0].legend()\n\n\n# Set label for x-axis\nax[1].set_xlabel( \"No. of Words\" , size = 12 )\n  \n# Set label for y-axis\nax[1].set_ylabel( \"Density of Words\" , size = 12 )\n  \n# Set title for plot\nax[1].set_title( \"Density of Words in 'question_title' feature\\n\" , size = 15 )\n\nax[1].legend()\n\n\n\nplt.show();\n","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:23:35.94314Z","iopub.execute_input":"2022-02-22T00:23:35.943384Z","iopub.status.idle":"2022-02-22T00:23:36.696005Z","shell.execute_reply.started":"2022-02-22T00:23:35.943351Z","shell.execute_reply":"2022-02-22T00:23:36.695364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observation: \n* Both train and test having the same distribution of characters and words. \n* Most of the words lies in range 5-10 both train and test. \n* Most of the characters lies in the range 40-60 train and test. ","metadata":{"editable":false}},{"cell_type":"markdown","source":"## Checking density of words & characters present in the `question_body` feature","metadata":{"editable":false}},{"cell_type":"code","source":"import seaborn as sns\n\n\ndef word_count(sentense):\n    sentense = sentense.strip()\n\n    return len(sentense.split(\" \"))\n\n\nfig, ax = plt.subplots(1,2, figsize = ( 20 , 5))\n\n\nquestion_body_lengths_train = train_dataset['question_body'].apply(len)\nquestion_body_lengths_test = test_dataset['question_body'].apply(len)\nquestion_body_lengths_train_words = train_dataset['question_body'].apply(word_count)\nquestion_body_lengths_test_words = test_dataset['question_body'].apply(word_count)\n\n\nsns.histplot(question_body_lengths_train, label=\"Train\", kde=True, stat=\"density\", linewidth=0,  color=\"red\", ax=ax[0])\nsns.histplot(question_body_lengths_test, label=\"Test\", kde=True, stat=\"density\", linewidth=0,  color=\"blue\", ax=ax[0])\nsns.histplot(question_body_lengths_train_words, label=\"Train\", kde=True, stat=\"density\", linewidth=0,  color=\"red\", ax=ax[1])\nsns.histplot(question_body_lengths_test_words, label=\"Test\", kde=True, stat=\"density\", linewidth=0,  color=\"blue\", ax=ax[1])\n\n# Set label for x-axis\nax[0].set_xlabel( \"No. of characters\" , size = 12 )\n  \n# Set label for y-axis\nax[0].set_ylabel( \"Density of character\" , size = 12 )\n  \n# Set title for plot\nax[0].set_title( \"Density of characters in 'question_body' feature\\n\" , size = 15 )\n\nax[0].legend()\n\n\n# Set label for x-axis\nax[1].set_xlabel( \"No. of Words\" , size = 12 )\n  \n# Set label for y-axis\nax[1].set_ylabel( \"Density of Words\" , size = 12 )\n  \n# Set title for plot\nax[1].set_title( \"Density of Words in 'question_body' feature\\n\" , size = 15 )\n\nax[1].legend()\n\n\n\nplt.show();\n","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:23:36.697238Z","iopub.execute_input":"2022-02-22T00:23:36.697596Z","iopub.status.idle":"2022-02-22T00:23:39.563261Z","shell.execute_reply.started":"2022-02-22T00:23:36.697561Z","shell.execute_reply":"2022-02-22T00:23:39.56258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observation:\n* We can observe that the distribution of both words and characters are very much right skewed.\n* Most of the characters in question_body lies below 2500.\n* Most of the words in question_body lies below 1000.","metadata":{"editable":false}},{"cell_type":"markdown","source":"## Similarly we will check for `answer` feature","metadata":{"editable":false}},{"cell_type":"code","source":"import seaborn as sns\n\n\ndef word_count(sentense):\n    sentense = sentense.strip()\n    return len(sentense.split(\" \"))\n\n\nfig, ax = plt.subplots(1,2, figsize = ( 20 , 5))\n\n\nanswer_lengths_train = train_dataset['answer'].apply(len)\nanswer_lengths_test = test_dataset['answer'].apply(len)\nanswer_lengths_train_words = train_dataset['answer'].apply(word_count)\nanswer_lengths_test_words = test_dataset['answer'].apply(word_count)\n\n\nsns.histplot(answer_lengths_train, label=\"Train\", kde=True, stat=\"density\", linewidth=0,  color=\"red\", ax=ax[0])\nsns.histplot(answer_lengths_test, label=\"Test\", kde=True, stat=\"density\", linewidth=0,  color=\"blue\", ax=ax[0])\nsns.histplot(answer_lengths_train_words, label=\"Train\", kde=True, stat=\"density\", linewidth=0,  color=\"red\", ax=ax[1])\nsns.histplot(answer_lengths_test_words, label=\"Test\", kde=True, stat=\"density\", linewidth=0,  color=\"blue\", ax=ax[1])\n\n# Set label for x-axis\nax[0].set_xlabel( \"No. of characters\" , size = 12 )\n  \n# Set label for y-axis\nax[0].set_ylabel( \"Density of character\" , size = 12 )\n  \n# Set title for plot\nax[0].set_title( \"Density of characters in 'answer' feature\\n\" , size = 15 )\n\nax[0].legend()\n\n\n# Set label for x-axis\nax[1].set_xlabel( \"No. of Words\" , size = 12 )\n  \n# Set label for y-axis\nax[1].set_ylabel( \"Density of Words\" , size = 12 )\n  \n# Set title for plot\nax[1].set_title( \"Density of Words in 'answer' feature\\n\" , size = 15 )\n\nax[1].legend()\n\n\n\nplt.show();\n","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:23:39.566418Z","iopub.execute_input":"2022-02-22T00:23:39.566616Z","iopub.status.idle":"2022-02-22T00:23:42.272197Z","shell.execute_reply.started":"2022-02-22T00:23:39.566591Z","shell.execute_reply":"2022-02-22T00:23:42.271527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observation:\n* As similar to question_body we can find that answer distribution is also skewed.\n* Their may be some extreme outlier instance that words/char length are very high in both question_body and answer features.","metadata":{"editable":false}},{"cell_type":"markdown","source":"## Analyzing `question_body` and `answer` features sequence length","metadata":{"editable":false}},{"cell_type":"code","source":"for i in range(0,101,10):\n    print(f'{i}th percentile of question_body input sequence {np.percentile(question_body_lengths_train_words, i)}')\nprint()\nfor i in range(90,101):\n    print(f'{i}th percentile of question_body input sequence {np.percentile(question_body_lengths_train_words, i)}')\nprint()\nfor i in [99.1,99.2,99.3,99.4,99.5,99.6,99.7,99.8,99.9,100]:\n    print(f'{i}th percentile of question_body input sequence {np.percentile(question_body_lengths_train_words, i)}')","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:23:42.273467Z","iopub.execute_input":"2022-02-22T00:23:42.274966Z","iopub.status.idle":"2022-02-22T00:23:42.299168Z","shell.execute_reply.started":"2022-02-22T00:23:42.274925Z","shell.execute_reply":"2022-02-22T00:23:42.298375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Observation:** 99.9% the of words in question body lies below **3220**","metadata":{"editable":false}},{"cell_type":"code","source":"for i in range(0,101,10):\n    print(f'{i}th percentile of answer input sequence {np.percentile(answer_lengths_train_words, i)}')\nprint()\nfor i in range(90,101):\n    print(f'{i}th percentile of answer input sequence {np.percentile(answer_lengths_train_words, i)}')\nprint()\nfor i in [99.1,99.2,99.3,99.4,99.5,99.6,99.7,99.8,99.9,100]:\n    print(f'{i}th percentile of answer input sequence {np.percentile(answer_lengths_train_words, i)}')","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:23:42.300416Z","iopub.execute_input":"2022-02-22T00:23:42.300637Z","iopub.status.idle":"2022-02-22T00:23:42.324349Z","shell.execute_reply.started":"2022-02-22T00:23:42.300606Z","shell.execute_reply":"2022-02-22T00:23:42.323719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Observation:** 99.9% of words in answer feature lies below **2200**","metadata":{"editable":false}},{"cell_type":"markdown","source":"# Analyzing `category` Feature","metadata":{"editable":false}},{"cell_type":"code","source":"train_dataset['category'].unique()","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:23:42.325421Z","iopub.execute_input":"2022-02-22T00:23:42.325906Z","iopub.status.idle":"2022-02-22T00:23:42.33361Z","shell.execute_reply.started":"2022-02-22T00:23:42.32587Z","shell.execute_reply":"2022-02-22T00:23:42.332753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_category_feature_count = train_dataset['category'].value_counts()\ntest_category_feature_count = test_dataset['category'].value_counts()\n\nprint(\"Train category:\\n\",train_category_feature_count)\nprint()\nprint(\"Test category:\\n\",test_category_feature_count)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:23:42.335413Z","iopub.execute_input":"2022-02-22T00:23:42.335815Z","iopub.status.idle":"2022-02-22T00:23:42.345479Z","shell.execute_reply.started":"2022-02-22T00:23:42.335775Z","shell.execute_reply":"2022-02-22T00:23:42.344818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"figure, ax = plt.subplots(1,2, figsize=(12, 6))\n\ntrain_category_feature_count.plot(kind='bar', ax=ax[0])\ntest_category_feature_count.plot(kind='bar', ax=ax[1])\n\nax[0].set_title('Train')\nax[0].set_xlabel( \"unique category\" , size = 12 )\nax[0].set_ylabel( \"count\" , size = 12 )\n\nax[1].set_title('Test')\nax[1].set_xlabel( \"unique category\" , size = 12 )\nax[1].set_ylabel( \"count\" , size = 12 )\n\nplt.show()","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:23:42.347193Z","iopub.execute_input":"2022-02-22T00:23:42.347887Z","iopub.status.idle":"2022-02-22T00:23:42.638269Z","shell.execute_reply.started":"2022-02-22T00:23:42.347843Z","shell.execute_reply":"2022-02-22T00:23:42.637557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sample stack over flow question and answer\ntrain_dataset[train_dataset['category'] == 'STACKOVERFLOW'].values[11]","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:23:42.639509Z","iopub.execute_input":"2022-02-22T00:23:42.639757Z","iopub.status.idle":"2022-02-22T00:23:42.651978Z","shell.execute_reply.started":"2022-02-22T00:23:42.639724Z","shell.execute_reply":"2022-02-22T00:23:42.651196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sample science question and answer \ntrain_dataset[train_dataset['category'] == 'SCIENCE'].values[11]","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:23:42.653356Z","iopub.execute_input":"2022-02-22T00:23:42.653617Z","iopub.status.idle":"2022-02-22T00:23:42.664101Z","shell.execute_reply.started":"2022-02-22T00:23:42.653583Z","shell.execute_reply":"2022-02-22T00:23:42.662769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sample life art and culture question and answer\ntrain_dataset[train_dataset['category'] == 'LIFE_ARTS'].values[11]","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:23:42.66549Z","iopub.execute_input":"2022-02-22T00:23:42.665937Z","iopub.status.idle":"2022-02-22T00:23:42.675417Z","shell.execute_reply.started":"2022-02-22T00:23:42.665902Z","shell.execute_reply":"2022-02-22T00:23:42.674428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sample life art and culture question and answer\ntrain_dataset[train_dataset['category'] == 'CULTURE'].values[11]","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:23:42.676981Z","iopub.execute_input":"2022-02-22T00:23:42.677216Z","iopub.status.idle":"2022-02-22T00:23:42.68743Z","shell.execute_reply.started":"2022-02-22T00:23:42.677185Z","shell.execute_reply":"2022-02-22T00:23:42.686521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observation:\n* Five unique category are present in the category feature.\n* **Technology** and **Stackoverflow** are the highest count and both are related topics.\n* **Life_arts** as the lowest count category.\n* Distribution of train and test category are the same.\n* **Life_arts & culture** follow general english syntax & structure.\n* **Science** utilizes latex with expressions prepended and appended with symbol: $\n* **Technology & stackoverflow** have code snippets & logs.","metadata":{"editable":false}},{"cell_type":"markdown","source":"# Word cloud","metadata":{"editable":false}},{"cell_type":"code","source":"from wordcloud import WordCloud\n\n\ndef plot_wordcloud(text, ax, title=None):\n    wordcloud = WordCloud(max_font_size=None, background_color='white', width=1200, height=1000).generate(text)\n    ax.imshow(wordcloud)\n    if title is not None:\n        ax.set_title(title, size = 15)\n    ax.axis(\"off\")","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:23:42.688446Z","iopub.execute_input":"2022-02-22T00:23:42.688633Z","iopub.status.idle":"2022-02-22T00:23:42.739634Z","shell.execute_reply.started":"2022-02-22T00:23:42.688611Z","shell.execute_reply":"2022-02-22T00:23:42.739034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n\n# word cloud for train data\ntext = ' '.join(train_dataset['question_title'].values)\nplot_wordcloud(text, axes[0][0], 'Train Question title')\n\ntext = ' '.join(train_dataset['question_body'].values)\nplot_wordcloud(text, axes[0][1], 'Train Question body')\n\ntext = ' '.join(train_dataset['answer'].values)\nplot_wordcloud(text, axes[0][2], 'Train Answer')\n\n\n# word cloud for Test data\ntext = ' '.join(test_dataset['question_title'].values)\nplot_wordcloud(text, axes[1][0], 'Test Question title')\n\ntext = ' '.join(test_dataset['question_body'].values)\nplot_wordcloud(text, axes[1][1], 'Test Question body')\n\ntext = ' '.join(test_dataset['answer'].values)\nplot_wordcloud(text, axes[1][2], 'Test Answer')\n\nplt.tight_layout()\nfig.show()","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:23:42.740835Z","iopub.execute_input":"2022-02-22T00:23:42.741091Z","iopub.status.idle":"2022-02-22T00:24:05.618678Z","shell.execute_reply.started":"2022-02-22T00:23:42.741058Z","shell.execute_reply":"2022-02-22T00:24:05.61702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observation:\n* We can observe that some of words match between train and test set.\nReference: https://www.kaggle.com/corochann/google-quest-first-data-introduction?scriptVersionId=23910525&cellId=34","metadata":{"editable":false}},{"cell_type":"markdown","source":"# Analyzing labels ","metadata":{"editable":false}},{"cell_type":"code","source":"for label in train_dataset.columns[11:]:\n    print(f\"{label:.20}: no. of unique label values: {len(train_dataset[label].unique())}\")","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:24:05.619651Z","iopub.execute_input":"2022-02-22T00:24:05.619901Z","iopub.status.idle":"2022-02-22T00:24:05.63552Z","shell.execute_reply.started":"2022-02-22T00:24:05.619868Z","shell.execute_reply":"2022-02-22T00:24:05.634943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observation:\n* The output label are regression(real) values but the distribution is not continuous.\n* Except for `answer_satisfaction` label rest every label are having unique values some are with 9 unique values and some are of 5 unique values.\n* Using this insights we can use post pocessing to get better scoring ","metadata":{"editable":false}},{"cell_type":"code","source":"for label in train_dataset.columns[11:]:\n    sns.histplot(train_dataset[label], label=label, kde=False)\n    plt.show()","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:24:05.636737Z","iopub.execute_input":"2022-02-22T00:24:05.637126Z","iopub.status.idle":"2022-02-22T00:24:12.628217Z","shell.execute_reply.started":"2022-02-22T00:24:05.637092Z","shell.execute_reply":"2022-02-22T00:24:12.627558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observation:\n* **Label values are imbalance** like for some of the label values are having only one values ex: **question_type_spelling**, **question_not_really_question** etc that is the distribution of label are very dissimilar.","metadata":{"editable":false}},{"cell_type":"markdown","source":"### correlation between target variables","metadata":{"editable":false}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(20,20))   \nsns.heatmap(train_dataset[11:].corr(), linewidths=1, ax=ax, annot_kws={\"fontsize\":40})\nplt.show();","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:24:12.629324Z","iopub.execute_input":"2022-02-22T00:24:12.630884Z","iopub.status.idle":"2022-02-22T00:24:13.623184Z","shell.execute_reply.started":"2022-02-22T00:24:12.630835Z","shell.execute_reply":"2022-02-22T00:24:13.622569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observations:\nFrom the above heatmap of correleation we can observe that `answer_helpful`, `answer_level_of_information`, `answer_plausible`, `answer_releveance` and `answer_satification` have some correlation between them.","metadata":{"editable":false}},{"cell_type":"markdown","source":"## Analyzing `host` feature","metadata":{"editable":false}},{"cell_type":"code","source":"print(f\"Total unique host present in the dataset {len(train_dataset['host'].unique())}\")","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:24:13.624432Z","iopub.execute_input":"2022-02-22T00:24:13.624854Z","iopub.status.idle":"2022-02-22T00:24:13.631291Z","shell.execute_reply.started":"2022-02-22T00:24:13.62481Z","shell.execute_reply":"2022-02-22T00:24:13.630443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_host_feature_count = train_dataset['host'].value_counts()\n\n\nfigure, ax = plt.subplots( figsize=(20, 5))\n\ntrain_host_feature_count.plot(kind='bar', ax=ax)\n\nax.set_title('Train dataset - count of Q&A collected from each website', size=20)\nax.set_xlabel( \"Host\" , size = 12 )\nax.set_ylabel( \"Count\" , size = 12 )\n\nplt.show()","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:24:13.632793Z","iopub.execute_input":"2022-02-22T00:24:13.633479Z","iopub.status.idle":"2022-02-22T00:24:14.890234Z","shell.execute_reply.started":"2022-02-22T00:24:13.633443Z","shell.execute_reply":"2022-02-22T00:24:14.889581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_host_feature_count = test_dataset['host'].value_counts()\nfigure, ax = plt.subplots( figsize=(20, 5))\ntest_host_feature_count.plot(kind='bar', ax=ax)\nax.set_title('Test dataset - count of Q&A collected from each website', size=20)\nax.set_xlabel( \"Host\" , size = 12 )\nax.set_ylabel( \"Count\" , size = 12 )\nplt.show()\n","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:24:14.891592Z","iopub.execute_input":"2022-02-22T00:24:14.892388Z","iopub.status.idle":"2022-02-22T00:24:15.955949Z","shell.execute_reply.started":"2022-02-22T00:24:14.892331Z","shell.execute_reply":"2022-02-22T00:24:15.95358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observation:\n* All question and answer in the dataset are extracted from **63 websites**.\n* Most of the question and answer are from **stackoverflow.com** as we observe from the  `category` feature analysis that most of the caterogy fall under **technology and stackoverflow**.","metadata":{"editable":false}},{"cell_type":"markdown","source":"","metadata":{"editable":false}},{"cell_type":"markdown","source":"# Spliting the data in to train and validation","metadata":{"editable":false}},{"cell_type":"code","source":"y_columns = ['question_asker_intent_understanding',\n       'question_body_critical', 'question_conversational',\n       'question_expect_short_answer', 'question_fact_seeking',\n       'question_has_commonly_accepted_answer',\n       'question_interestingness_others', 'question_interestingness_self',\n       'question_multi_intent', 'question_not_really_a_question',\n       'question_opinion_seeking', 'question_type_choice',\n       'question_type_compare', 'question_type_consequence',\n       'question_type_definition', 'question_type_entity',\n       'question_type_instructions', 'question_type_procedure',\n       'question_type_reason_explanation', 'question_type_spelling',\n       'question_well_written', 'answer_helpful',\n       'answer_level_of_information', 'answer_plausible', 'answer_relevance',\n       'answer_satisfaction', 'answer_type_instructions',\n       'answer_type_procedure', 'answer_type_reason_explanation',\n       'answer_well_written']\n\ny = train_dataset[y_columns]\nX = train_dataset.drop(y_columns,axis=1)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:24:15.962575Z","iopub.execute_input":"2022-02-22T00:24:15.962964Z","iopub.status.idle":"2022-02-22T00:24:15.973349Z","shell.execute_reply.started":"2022-02-22T00:24:15.962927Z","shell.execute_reply":"2022-02-22T00:24:15.972767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.shape, y.shape","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:24:15.975053Z","iopub.execute_input":"2022-02-22T00:24:15.975407Z","iopub.status.idle":"2022-02-22T00:24:15.993121Z","shell.execute_reply.started":"2022-02-22T00:24:15.975376Z","shell.execute_reply":"2022-02-22T00:24:15.992484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n\nX_train_dataset, X_valid_dataset, y_train_dataset, y_valid_dataset = train_test_split(X,y, test_size=0.10)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:24:15.994257Z","iopub.execute_input":"2022-02-22T00:24:15.994606Z","iopub.status.idle":"2022-02-22T00:24:16.169665Z","shell.execute_reply.started":"2022-02-22T00:24:15.994572Z","shell.execute_reply":"2022-02-22T00:24:16.168914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_dataset.shape, X_valid_dataset.shape, y_train_dataset.shape, y_valid_dataset.shape","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:24:16.173757Z","iopub.execute_input":"2022-02-22T00:24:16.175653Z","iopub.status.idle":"2022-02-22T00:24:16.184521Z","shell.execute_reply.started":"2022-02-22T00:24:16.175606Z","shell.execute_reply":"2022-02-22T00:24:16.183862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_dataset","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:24:16.188027Z","iopub.execute_input":"2022-02-22T00:24:16.188458Z","iopub.status.idle":"2022-02-22T00:24:16.21611Z","shell.execute_reply.started":"2022-02-22T00:24:16.188421Z","shell.execute_reply":"2022-02-22T00:24:16.213775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  **Preprocessing Text Feature**","metadata":{"editable":false}},{"cell_type":"code","source":"# https://stackoverflow.com/a/47091490/4084039\nimport re\n\ndef decontracted(phrase):\n    phrase = re.sub(r\"(W|w)on(\\'|\\’)t \", \"will not \", phrase)\n    phrase = re.sub(r\"(C|c)an(\\'|\\’)t \", \"can not \", phrase)\n    phrase = re.sub(r\"(Y|y)(\\'|\\’)all \", \"you all \", phrase)\n    phrase = re.sub(r\"(Y|y)a(\\'|\\’)ll \", \"you all \", phrase)\n    phrase = re.sub(r\"(I|i)(\\'|\\’)m \", \"i am \", phrase)\n    phrase = re.sub(r\"(A|a)isn(\\'|\\’)t \", \"is not \", phrase)\n    phrase = re.sub(r\"n(\\'|\\’)t \", \" not \", phrase)\n    phrase = re.sub(r\"(\\'|\\’)re \", \" are \", phrase)\n    phrase = re.sub(r\"(\\'|\\’)d \", \" would \", phrase)\n    phrase = re.sub(r\"(\\'|\\’)ll \", \" will \", phrase)\n    phrase = re.sub(r\"(\\'|\\’)t \", \" not \", phrase)\n    phrase = re.sub(r\"(\\'|\\’)ve \", \" have \", phrase)\n    \n    return phrase\n\n\ndef clean_text(x):\n\n    x = str(x)\n    for punct in \"/-'\":\n        x = x.replace(punct, ' ')\n    for punct in '&':\n        x = x.replace(punct, f' {punct} ')\n    for punct in '?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~' + '“”’':\n        x = x.replace(punct, '')\n    return x\n\ndef clean_numbers(x):\n\n    x = re.sub('[0-9]{5,}', '12345', x)\n    x = re.sub('[0-9]{4}', '1234', x)\n    x = re.sub('[0-9]{3}', '123', x)\n    x = re.sub('[0-9]{2}', '12', x)\n    return x","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:24:16.218705Z","iopub.execute_input":"2022-02-22T00:24:16.221299Z","iopub.status.idle":"2022-02-22T00:24:16.242069Z","shell.execute_reply.started":"2022-02-22T00:24:16.221263Z","shell.execute_reply":"2022-02-22T00:24:16.241446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://gist.github.com/sebleier/554280\n# we are removing the words from the stop words list: 'no', 'nor', 'not'\nstopwords= ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n            'won', \"won't\", 'wouldn', \"wouldn't\"]","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:24:16.248224Z","iopub.execute_input":"2022-02-22T00:24:16.248473Z","iopub.status.idle":"2022-02-22T00:24:16.269514Z","shell.execute_reply.started":"2022-02-22T00:24:16.248442Z","shell.execute_reply":"2022-02-22T00:24:16.268555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Combining all the above stundents \nfrom tqdm import tqdm\ndef preprocess_text(text_data):\n    preprocessed_text = []\n    # tqdm is for printing the status bar\n    for sentance in tqdm(text_data):\n        sent = decontracted(sentance)\n        sent = clean_text(sentance)\n        sent = clean_numbers(sentance)\n        sent = sent.replace('\\\\r', ' ')\n        sent = sent.replace('\\\\n', ' ')\n        sent = sent.replace('\\\\\"', ' ')\n        sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n        # https://gist.github.com/sebleier/554280\n        sent = ' '.join(e for e in sent.split() if e.lower() not in stopwords)\n        preprocessed_text.append(sent.lower().strip())\n    return preprocessed_text","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:24:16.270683Z","iopub.execute_input":"2022-02-22T00:24:16.271336Z","iopub.status.idle":"2022-02-22T00:24:16.280761Z","shell.execute_reply.started":"2022-02-22T00:24:16.2713Z","shell.execute_reply":"2022-02-22T00:24:16.279731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_dataset['preprocessed_question_title'] = preprocess_text(X_train_dataset['question_title'].values)\nX_train_dataset['preprocessed_question_body'] = preprocess_text(X_train_dataset['question_body'].values)\nX_train_dataset['preprocessed_answer'] = preprocess_text(X_train_dataset['answer'].values)\n\n\nX_valid_dataset['preprocessed_question_title'] = preprocess_text(X_valid_dataset['question_title'].values)\nX_valid_dataset['preprocessed_question_body'] = preprocess_text(X_valid_dataset['question_body'].values)\nX_valid_dataset['preprocessed_answer'] = preprocess_text(X_valid_dataset['answer'].values)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:24:16.284571Z","iopub.execute_input":"2022-02-22T00:24:16.285327Z","iopub.status.idle":"2022-02-22T00:24:25.358033Z","shell.execute_reply.started":"2022-02-22T00:24:16.284829Z","shell.execute_reply":"2022-02-22T00:24:25.356959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset['preprocessed_question_title'] = preprocess_text(test_dataset['question_title'].values)\ntest_dataset['preprocessed_question_body'] = preprocess_text(test_dataset['question_body'].values)\ntest_dataset['preprocessed_answer'] = preprocess_text(test_dataset['answer'].values)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:24:25.359356Z","iopub.execute_input":"2022-02-22T00:24:25.359631Z","iopub.status.idle":"2022-02-22T00:24:26.057752Z","shell.execute_reply.started":"2022-02-22T00:24:25.359594Z","shell.execute_reply":"2022-02-22T00:24:26.057067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### question_title text after preprocessing","metadata":{"editable":false}},{"cell_type":"code","source":"# Text before preprocessing\nX_train_dataset['question_title'].values[0]","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:24:26.059088Z","iopub.execute_input":"2022-02-22T00:24:26.059493Z","iopub.status.idle":"2022-02-22T00:24:26.064746Z","shell.execute_reply.started":"2022-02-22T00:24:26.059454Z","shell.execute_reply":"2022-02-22T00:24:26.064043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Text after preprocessing\nX_train_dataset['preprocessed_question_title'].values[0]","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:24:26.065975Z","iopub.execute_input":"2022-02-22T00:24:26.066364Z","iopub.status.idle":"2022-02-22T00:24:26.075165Z","shell.execute_reply.started":"2022-02-22T00:24:26.066313Z","shell.execute_reply":"2022-02-22T00:24:26.074382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### question_body after preprocessing","metadata":{"editable":false}},{"cell_type":"code","source":"# Text before preprocessing\nX_train_dataset['question_body'].values[0]","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:24:26.076367Z","iopub.execute_input":"2022-02-22T00:24:26.077355Z","iopub.status.idle":"2022-02-22T00:24:26.085311Z","shell.execute_reply.started":"2022-02-22T00:24:26.077312Z","shell.execute_reply":"2022-02-22T00:24:26.084656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Text after preprocessing\nX_train_dataset['preprocessed_question_body'].values[0]","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:24:26.086699Z","iopub.execute_input":"2022-02-22T00:24:26.087368Z","iopub.status.idle":"2022-02-22T00:24:26.093531Z","shell.execute_reply.started":"2022-02-22T00:24:26.08733Z","shell.execute_reply":"2022-02-22T00:24:26.092773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Answer after preprocessing","metadata":{"editable":false}},{"cell_type":"code","source":"# Text before preprocessing\nX_train_dataset['answer'].values[0]","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:24:26.094884Z","iopub.execute_input":"2022-02-22T00:24:26.095741Z","iopub.status.idle":"2022-02-22T00:24:26.103047Z","shell.execute_reply.started":"2022-02-22T00:24:26.095685Z","shell.execute_reply":"2022-02-22T00:24:26.102282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Text after preprocessing\nX_train_dataset['preprocessed_answer'].values[0]","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:24:26.104315Z","iopub.execute_input":"2022-02-22T00:24:26.104705Z","iopub.status.idle":"2022-02-22T00:24:26.113122Z","shell.execute_reply.started":"2022-02-22T00:24:26.104669Z","shell.execute_reply":"2022-02-22T00:24:26.112197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"editable":false},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Feature engineering:**","metadata":{"editable":false}},{"cell_type":"markdown","source":"## Text count based features:\n\n1. Number of characters in the **question_title**\n2. Number of characters in the **question_body**\n3. Number of characters in the **answer**\n4. Number of words in the **question_title**\n5. Number of words in the **question_body**\n6. Number of words in the **answer**\n7. Number of unique words in the **question_title**\n8. Number of unique words in the **question_body**\n9. Number of unique words in the **answer**\n","metadata":{"editable":false}},{"cell_type":"code","source":"def word_count(sentense):\n    sentense = sentense.strip()\n\n    return len(sentense.split(\" \"))\n\ndef unique_word_count(sentense):\n    sentense = sentense.strip()\n\n    return len(set(sentense.split(\" \")))\n","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:24:26.114609Z","iopub.execute_input":"2022-02-22T00:24:26.114939Z","iopub.status.idle":"2022-02-22T00:24:26.12287Z","shell.execute_reply.started":"2022-02-22T00:24:26.114871Z","shell.execute_reply":"2022-02-22T00:24:26.122133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\n# Number of characters in the text\nX_train_dataset[\"question_title_num_chars\"] = X_train_dataset[\"question_title\"].apply(len)\nX_train_dataset[\"question_body_num_chars\"] = X_train_dataset[\"question_body\"].apply(len)\nX_train_dataset[\"answer_num_chars\"] = X_train_dataset[\"answer\"].apply(len)\n\n# Feature engineering for validation dataset\nX_valid_dataset[\"question_title_num_chars\"] = X_valid_dataset[\"question_title\"].apply(len)\nX_valid_dataset[\"question_body_num_chars\"] = X_valid_dataset[\"question_body\"].apply(len)\nX_valid_dataset[\"answer_num_chars\"] = X_valid_dataset[\"answer\"].apply(len)\n\ntest_dataset[\"question_title_num_chars\"] = test_dataset[\"question_title\"].apply(len)\ntest_dataset[\"question_body_num_chars\"] = test_dataset[\"question_body\"].apply(len)\ntest_dataset[\"answer_num_chars\"] = test_dataset[\"answer\"].apply(len)\n\n#########################################################################################################\n# Number of words in the text\nX_train_dataset[\"question_title_num_words\"] = X_train_dataset[\"question_title\"].apply(word_count)\nX_train_dataset[\"question_body_num_words\"] = X_train_dataset[\"question_body\"].apply(word_count)\nX_train_dataset[\"answer_num_words\"] = X_train_dataset[\"answer\"].apply(word_count)\n\n# validation dataset features\nX_valid_dataset[\"question_title_num_words\"] = X_valid_dataset[\"question_title\"].apply(word_count)\nX_valid_dataset[\"question_body_num_words\"] = X_valid_dataset[\"question_body\"].apply(word_count)\nX_valid_dataset[\"answer_num_words\"] = X_valid_dataset[\"answer\"].apply(word_count)\n\ntest_dataset[\"question_title_num_words\"] = test_dataset[\"question_title\"].apply(word_count)\ntest_dataset[\"question_body_num_words\"] = test_dataset[\"question_body\"].apply(word_count)\ntest_dataset[\"answer_num_words\"] = test_dataset[\"answer\"].apply(word_count)\n\n\n#######################################################################################################\n# Number of unique words in the text\nX_train_dataset[\"question_title_num_unique_words\"] = X_train_dataset[\"question_title\"].apply(unique_word_count)\nX_train_dataset[\"question_body_num_unique_words\"] = X_train_dataset[\"question_body\"].apply(unique_word_count)\nX_train_dataset[\"answer_num_unique_words\"] = X_train_dataset[\"answer\"].apply(unique_word_count)\n\n# Validation dataset\nX_valid_dataset[\"question_title_num_unique_words\"] = X_valid_dataset[\"question_title\"].apply(unique_word_count)\nX_valid_dataset[\"question_body_num_unique_words\"] = X_valid_dataset[\"question_body\"].apply(unique_word_count)\nX_valid_dataset[\"answer_num_unique_words\"] = X_valid_dataset[\"answer\"].apply(unique_word_count)\n\ntest_dataset[\"question_title_num_unique_words\"] = test_dataset[\"question_title\"].apply(unique_word_count)\ntest_dataset[\"question_body_num_unique_words\"] = test_dataset[\"question_body\"].apply(unique_word_count)\ntest_dataset[\"answer_num_unique_words\"] = test_dataset[\"answer\"].apply(unique_word_count)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:24:26.124596Z","iopub.execute_input":"2022-02-22T00:24:26.124819Z","iopub.status.idle":"2022-02-22T00:24:26.516775Z","shell.execute_reply.started":"2022-02-22T00:24:26.124785Z","shell.execute_reply":"2022-02-22T00:24:26.516067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TF-IDF based features:\n\n* Word Level N-Gram TF-IDF of **question_title**\n* Word Level N-Gram TF-IDF of **question_body**\n* Word Level N-Gram TF-IDF of **answer**","metadata":{"editable":false}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\n\nvectorizer = TfidfVectorizer(min_df=2)\ntsvd = TruncatedSVD(n_components = 128, n_iter=5)\n\n\nqt_tfidf = vectorizer.fit_transform(X_train_dataset['preprocessed_question_title'].values)\ntfidf_question_title_train = tsvd.fit_transform(qt_tfidf)\n\n\nqb_tfidf = vectorizer.fit_transform(X_train_dataset['preprocessed_question_body'].values)\ntfidf_question_body_train = tsvd.fit_transform(qb_tfidf)\n\n\nans_tfidf = vectorizer.fit_transform(X_train_dataset['preprocessed_answer'].values)\ntfidf_answer_train = tsvd.fit_transform(ans_tfidf)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:24:26.517874Z","iopub.execute_input":"2022-02-22T00:24:26.518108Z","iopub.status.idle":"2022-02-22T00:24:31.202235Z","shell.execute_reply.started":"2022-02-22T00:24:26.518076Z","shell.execute_reply":"2022-02-22T00:24:31.20122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"qt_tfidf = vectorizer.fit_transform(X_valid_dataset['preprocessed_question_title'].values)\ntfidf_question_title_valid = tsvd.fit_transform(qt_tfidf)\n\n\nqb_tfidf = vectorizer.fit_transform(X_valid_dataset['preprocessed_question_body'].values)\ntfidf_question_body_valid = tsvd.fit_transform(qb_tfidf)\n\n\nans_tfidf = vectorizer.fit_transform(X_valid_dataset['preprocessed_answer'].values)\ntfidf_answer_valid = tsvd.fit_transform(ans_tfidf)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:24:31.207991Z","iopub.execute_input":"2022-02-22T00:24:31.210558Z","iopub.status.idle":"2022-02-22T00:24:31.84721Z","shell.execute_reply.started":"2022-02-22T00:24:31.208469Z","shell.execute_reply":"2022-02-22T00:24:31.846195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"qt_tfidf = vectorizer.fit_transform(test_dataset['preprocessed_question_title'].values)\ntfidf_question_title_test = tsvd.fit_transform(qt_tfidf)\n\n\nqb_tfidf = vectorizer.fit_transform(test_dataset['preprocessed_question_body'].values)\ntfidf_question_body_test = tsvd.fit_transform(qb_tfidf)\n\n\nans_tfidf = vectorizer.fit_transform(test_dataset['preprocessed_answer'].values)\ntfidf_answer_test = tsvd.fit_transform(ans_tfidf)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:24:31.85237Z","iopub.execute_input":"2022-02-22T00:24:31.854784Z","iopub.status.idle":"2022-02-22T00:24:32.59847Z","shell.execute_reply.started":"2022-02-22T00:24:31.852832Z","shell.execute_reply":"2022-02-22T00:24:32.597526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_dataset[\"tfidf_question_title\"] = list(tfidf_question_title_train)\nX_train_dataset[\"tfidf_question_body\"] = list(tfidf_question_body_train)\nX_train_dataset[\"tfidf_answer\"] = list(tfidf_answer_train)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:24:32.599776Z","iopub.execute_input":"2022-02-22T00:24:32.600178Z","iopub.status.idle":"2022-02-22T00:24:32.631426Z","shell.execute_reply.started":"2022-02-22T00:24:32.600142Z","shell.execute_reply":"2022-02-22T00:24:32.630704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_valid_dataset[\"tfidf_question_title\"] = list(tfidf_question_title_valid)\nX_valid_dataset[\"tfidf_question_body\"] = list(tfidf_question_body_valid)\nX_valid_dataset[\"tfidf_answer\"] = list(tfidf_answer_valid)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:24:32.632608Z","iopub.execute_input":"2022-02-22T00:24:32.632985Z","iopub.status.idle":"2022-02-22T00:24:32.645581Z","shell.execute_reply.started":"2022-02-22T00:24:32.63295Z","shell.execute_reply":"2022-02-22T00:24:32.644498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset[\"tfidf_question_title\"] = list(tfidf_question_title_test)\ntest_dataset[\"tfidf_question_body\"] = list(tfidf_question_body_test)\ntest_dataset[\"tfidf_answer\"] = list(tfidf_answer_test)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:24:32.651813Z","iopub.execute_input":"2022-02-22T00:24:32.65403Z","iopub.status.idle":"2022-02-22T00:24:32.668693Z","shell.execute_reply.started":"2022-02-22T00:24:32.653987Z","shell.execute_reply":"2022-02-22T00:24:32.668022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Features using web scraping \n\n\n## `answer_user_page` features:\n","metadata":{"editable":false}},{"cell_type":"code","source":"!pip install bs4","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:24:32.674777Z","iopub.execute_input":"2022-02-22T00:24:32.676974Z","iopub.status.idle":"2022-02-22T00:24:43.061447Z","shell.execute_reply.started":"2022-02-22T00:24:32.676932Z","shell.execute_reply":"2022-02-22T00:24:43.060547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom bs4 import BeautifulSoup\nfrom urllib import request\n\n\ndef get_user_rating(url):\n    try:\n        get = request.urlopen(url).read()\n        src = BeautifulSoup(get, 'html.parser')\n        #print(src)\n        reputation, gold = [], []\n        silver, bronze = [], []\n        reputation = int(''.join(src.find_all(\"div\", class_ = 'fs-body3 fc-dark')[0].text.strip().split(',')))\n        try:\n            gold = int(''.join(src.find_all('div', class_='fs-title fw-bold fc-black-800')[0].text.strip().split(',')))\n        except:\n            gold = 0\n\n        try:    \n            silver = int(''.join(src.find_all('div', class_='fs-title fw-bold fc-black-800')[1].text.strip().split(',')))\n        except:\n            silver = 0\n\n        try:\n            bronze = int(''.join(src.find_all('div', class_='fs-title fw-bold fc-black-800')[2].text.strip().split(',')))\n        except:\n            bronze = 0\n\n        output = [reputation, gold, silver, bronze]\n    except:\n        output = [0]*4\n\n    return output\n'''\ndata = []\nfor url in tqdm(X_train_dataset['answer_user_page']):\n    #print(url)\n    data.append(get_user_rating(url))\n    columns = ['reputation', 'gold', 'silver', 'bronze']  \nscraped = pd.DataFrame(data, columns=columns)\nscraped.to_csv(f'train_web_scrap_features.csv', index=False)\n\ndata = []\nfor url in tqdm(X_valid_dataset['answer_user_page']):\n    #print(url)\n    data.append(get_user_rating(url))\n    columns = ['reputation', 'gold', 'silver', 'bronze']  \nscraped = pd.DataFrame(data, columns=columns)\nscraped.to_csv(f'valid_web_scrap_features.csv', index=False)\n'''\n\ntrain_web_scraping_feature = pd.read_csv('../input/feature-engineering/train_web_scrap_features.csv')\nvalid_web_scraping_feature = pd.read_csv('../input/feature-engineering/valid_web_scrap_features.csv')","metadata":{"execution":{"iopub.status.busy":"2022-02-22T00:24:43.063201Z","iopub.execute_input":"2022-02-22T00:24:43.063456Z","iopub.status.idle":"2022-02-22T00:24:43.352428Z","shell.execute_reply.started":"2022-02-22T00:24:43.063421Z","shell.execute_reply":"2022-02-22T00:24:43.35156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_web_scraping_feature","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:24:43.357079Z","iopub.execute_input":"2022-02-22T00:24:43.359266Z","iopub.status.idle":"2022-02-22T00:24:43.377612Z","shell.execute_reply.started":"2022-02-22T00:24:43.359226Z","shell.execute_reply":"2022-02-22T00:24:43.377013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_web_scraping_feature","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:24:43.381727Z","iopub.execute_input":"2022-02-22T00:24:43.383787Z","iopub.status.idle":"2022-02-22T00:24:43.400646Z","shell.execute_reply.started":"2022-02-22T00:24:43.38375Z","shell.execute_reply":"2022-02-22T00:24:43.400056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### References for feature engineering:\n* https://www.kaggle.com/c/google-quest-challenge/discussion/130041 - meta features.\n* https://www.kaggle.com/codename007/start-from-here-quest-complete-eda-fe?scriptVersionId=25618132&cellId=65 - tfidf, count based features.\n* https://towardsdatascience.com/hands-on-transformers-kaggle-google-quest-q-a-labeling-affd3dad7bcb - web scraping features","metadata":{"editable":false}},{"cell_type":"markdown","source":"> **Note:** We have experimented with bi-direction LSTM as base model and acheived spearman score of 0.2712, then we tried with universal sentense encoder and achevied an spearman score of 0.37123 as the best score. Now lets experiment with transformed based models","metadata":{"editable":false}},{"cell_type":"code","source":"from prettytable import PrettyTable\n\n\nmyTable = PrettyTable([\"USE Model\", \"Features\", \"Spearman scroe\"])\n\n\nmyTable.add_row([\"Bi-LSTM\", \"Three basic features\", \"0.27561\"])\nmyTable.add_row([\"Bi-LSTM\", \"Three basic features + 18 FE Features(meta, TF-IDF, Web scraping)\", \"0.00253\"])\nmyTable.add_row([\"Bi-LSTM\", \"Three basic features + 13 FE Features(meta, Web scraping)\", \"0.01255\"])\nmyTable.add_row([\"Bi-LSTM\", \"Three basic features + 9 FE Features(meta features)\", \"0.28656\"])\nmyTable.add_row([\"Bi-LSTM\", \"Three basic features + 13 FE features with 100 dim embeddings(meta, Web scraping)\", \"-0.0041\"])\nmyTable.add_row([\"USE\", \"Three basic features\", \"0.33029\"])\nmyTable.add_row([\"USE\", \"Three basic features + 9 Meta Features\", \"0.37133\"])\nmyTable.add_row([\"USE\", \"Three basic features + L2 distance feature + 9 meta Features\", \"0.36575\"])\nmyTable.add_row([\"USE\", \"Three basic features + cosine distance + 9 Meta features\", \"0.37153\"])\nmyTable.add_row([\"USE\", \"Three basic features + L2 distance +cosine distance + 9 Meta features\", \"0.37061\"])\n\n\nprint(myTable)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:24:43.404492Z","iopub.execute_input":"2022-02-22T00:24:43.406528Z","iopub.status.idle":"2022-02-22T00:24:43.422647Z","shell.execute_reply.started":"2022-02-22T00:24:43.40649Z","shell.execute_reply":"2022-02-22T00:24:43.421954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install bert-for-tf2","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:24:43.42776Z","iopub.execute_input":"2022-02-22T00:24:43.429858Z","iopub.status.idle":"2022-02-22T00:24:56.959087Z","shell.execute_reply.started":"2022-02-22T00:24:43.429816Z","shell.execute_reply":"2022-02-22T00:24:56.957984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import GroupKFold\nimport matplotlib.pyplot as plt\nimport tensorflow_hub as hub\nimport tensorflow as tf\nfrom bert import bert_tokenization as tokenization\nimport tensorflow.keras.backend as K\nimport gc\nimport os\nfrom scipy.stats import spearmanr\nfrom math import floor, ceil\n\nnp.set_printoptions(suppress=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T00:24:56.960991Z","iopub.execute_input":"2022-02-22T00:24:56.96128Z","iopub.status.idle":"2022-02-22T00:25:02.155144Z","shell.execute_reply.started":"2022-02-22T00:24:56.961241Z","shell.execute_reply":"2022-02-22T00:25:02.154384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hub_url_bert = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3\"\nbert_layer = hub.KerasLayer(hub_url_bert, trainable=False)\n","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:25:02.1565Z","iopub.execute_input":"2022-02-22T00:25:02.156765Z","iopub.status.idle":"2022-02-22T00:25:22.371002Z","shell.execute_reply.started":"2022-02-22T00:25:02.156729Z","shell.execute_reply":"2022-02-22T00:25:22.369686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n\n\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n\nprint(\"Vocab size:\", len(tokenizer.vocab))","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:25:22.37654Z","iopub.execute_input":"2022-02-22T00:25:22.378944Z","iopub.status.idle":"2022-02-22T00:25:22.558613Z","shell.execute_reply.started":"2022-02-22T00:25:22.378897Z","shell.execute_reply":"2022-02-22T00:25:22.557768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_dataset.shape, X_valid_dataset.shape, test_dataset.shape","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:25:22.562562Z","iopub.execute_input":"2022-02-22T00:25:22.562831Z","iopub.status.idle":"2022-02-22T00:25:22.574362Z","shell.execute_reply.started":"2022-02-22T00:25:22.562796Z","shell.execute_reply":"2022-02-22T00:25:22.573514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_dataset.columns","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:25:22.578457Z","iopub.execute_input":"2022-02-22T00:25:22.580398Z","iopub.status.idle":"2022-02-22T00:25:22.589817Z","shell.execute_reply.started":"2022-02-22T00:25:22.58036Z","shell.execute_reply":"2022-02-22T00:25:22.589132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Transforming input features for bert model","metadata":{"editable":false}},{"cell_type":"markdown","source":"### Functions to get `Input Ids` , `Input mask`, `Input segment` for bert","metadata":{"editable":false}},{"cell_type":"code","source":"def extract_masks(tokens, max_seq_length):\n    \n    \"\"\"Mask for padding\"\"\"\n    \n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n\n\n\ndef extract_segments(tokens, max_seq_length):\n    \n    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n    \n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    segments = []\n    first_sep = True\n    current_segment_id = 0\n    for token in tokens:\n        segments.append(current_segment_id)\n        if token == \"[SEP]\":\n            if first_sep:\n                first_sep = False \n            else:\n                current_segment_id = 1\n    return segments + [0] * (max_seq_length - len(tokens))\n\n\n\ndef extract_ids(tokens, tokenizer, max_seq_length):\n    \n    \"\"\"Token ids from Tokenizer vocab\"\"\"\n    \n    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n    return input_ids","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:25:22.594231Z","iopub.execute_input":"2022-02-22T00:25:22.594785Z","iopub.status.idle":"2022-02-22T00:25:22.60891Z","shell.execute_reply.started":"2022-02-22T00:25:22.59475Z","shell.execute_reply":"2022-02-22T00:25:22.608227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### In the below `_traim_input` function:\n\n* if the input sentence has the number of tokens > 512, the \nsentence is trimmed down to 512. To trim the number of tokens, 256 tokens from \nthe start and 256 tokens from the end are kept and the remaining tokens are dropped.\n\n> **Ex.** suppose an answer has 700 tokens, to trim this down to 512, 256 tokens from the\nbeginning are taken and 256 tokens from the end are taken and concatenated to make \n512 tokens. The remaining [700-(256+256) = 288] tokens that are in the middle of the \nanswer are dropped. \n\n* The logic makes sense because in large texts, the beginning part\nusually describes what the text is all about and the end part describes the conclusion\nof the text. This is also closely related to the target features that we need to predict.","metadata":{"editable":false}},{"cell_type":"code","source":"def _trim_input(title, question, answer, max_sequence_length, \n                t_max_len=30, q_max_len=239, a_max_len=239):\n\n    t = tokenizer.tokenize(title)\n    q = tokenizer.tokenize(question)\n    a = tokenizer.tokenize(answer)\n    \n    t_len = len(t)\n    q_len = len(q)\n    a_len = len(a)\n\n    if (t_len+q_len+a_len+4) > max_sequence_length:\n        \n        if t_max_len > t_len:\n            t_new_len = t_len\n            a_max_len = a_max_len + floor((t_max_len - t_len)/2)\n            q_max_len = q_max_len + ceil((t_max_len - t_len)/2)\n        else:\n            t_new_len = t_max_len\n      \n        if a_max_len > a_len:\n            a_new_len = a_len \n            q_new_len = q_max_len + (a_max_len - a_len)\n        elif q_max_len > q_len:\n            a_new_len = a_max_len + (q_max_len - q_len)\n            q_new_len = q_len\n        else:\n            a_new_len = a_max_len\n            q_new_len = q_max_len\n            \n            \n        if t_new_len+a_new_len+q_new_len+4 != max_sequence_length:\n            raise ValueError(\"New sequence length should be %d, but is %d\" \n                             % (max_sequence_length, (t_new_len+a_new_len+q_new_len+4)))\n        \n        t = t[:t_new_len]\n        q = q[:q_new_len]\n        a = a[:a_new_len]\n    \n    return t, q, a","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:25:22.61335Z","iopub.execute_input":"2022-02-22T00:25:22.614836Z","iopub.status.idle":"2022-02-22T00:25:22.636112Z","shell.execute_reply.started":"2022-02-22T00:25:22.614798Z","shell.execute_reply":"2022-02-22T00:25:22.632775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### In the below `_convert_to_bert_inputs` function\n\n* Concatinate the three text features in to one single features and convert the input to bert compatable inputs","metadata":{"editable":false}},{"cell_type":"code","source":"def _convert_to_bert_inputs(title, question, answer, tokenizer, max_sequence_length):\n    \"\"\"Converts tokenized input to ids, masks and segments for BERT\"\"\"\n    \n    text = [\"[CLS]\"] + title + [\"[SEP]\"] + question + [\"[SEP]\"] + answer + [\"[SEP]\"]\n\n    input_ids = extract_ids(text, tokenizer, max_sequence_length)\n    input_masks = extract_masks(text, max_sequence_length)\n    input_segments = extract_segments(text, max_sequence_length)\n\n    return [input_ids, input_masks, input_segments]","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:25:22.639735Z","iopub.execute_input":"2022-02-22T00:25:22.640891Z","iopub.status.idle":"2022-02-22T00:25:22.651016Z","shell.execute_reply.started":"2022-02-22T00:25:22.640845Z","shell.execute_reply":"2022-02-22T00:25:22.650296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Transforming bert training dataset to bert compatible input\n\ninput_ids, input_masks, input_segments = [], [], []\nmax_sequence_length = 512\nfor _, instance in tqdm(X_train_dataset.iterrows()):\n    t, q, a = instance.question_title, instance.question_body, instance.answer\n\n    t, q, a = _trim_input(t, q, a, max_sequence_length)\n\n    ids, masks, segments = _convert_to_bert_inputs(t, q, a, tokenizer, max_sequence_length)\n    input_ids.append(ids)\n    input_masks.append(masks)\n    input_segments.append(segments)\n\nX_train_bert =  [np.asarray(input_ids, dtype=np.int32), \n                np.asarray(input_masks, dtype=np.int32), \n                np.asarray(input_segments, dtype=np.int32)]","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:25:22.655526Z","iopub.execute_input":"2022-02-22T00:25:22.657547Z","iopub.status.idle":"2022-02-22T00:26:04.902144Z","shell.execute_reply.started":"2022-02-22T00:25:22.657511Z","shell.execute_reply":"2022-02-22T00:26:04.901295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Transforming bert validation dataset to bert compatible input\n\ninput_ids, input_masks, input_segments = [], [], []\nmax_sequence_length = 512\nfor _, instance in tqdm(X_valid_dataset.iterrows()):\n    t, q, a = instance.question_title, instance.question_body, instance.answer\n\n    t, q, a = _trim_input(t, q, a, max_sequence_length)\n\n    ids, masks, segments = _convert_to_bert_inputs(t, q, a, tokenizer, max_sequence_length)\n    input_ids.append(ids)\n    input_masks.append(masks)\n    input_segments.append(segments)\n\nX_valid_bert =  [np.asarray(input_ids, dtype=np.int32), \n                np.asarray(input_masks, dtype=np.int32), \n                np.asarray(input_segments, dtype=np.int32)]\n\n\n","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:26:04.903571Z","iopub.execute_input":"2022-02-22T00:26:04.904401Z","iopub.status.idle":"2022-02-22T00:26:09.488529Z","shell.execute_reply.started":"2022-02-22T00:26:04.904351Z","shell.execute_reply":"2022-02-22T00:26:09.487724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Transforming bert test dataset to bert compatible input\n\ninput_ids, input_masks, input_segments = [], [], []\nmax_sequence_length = 512\nfor _, instance in tqdm(test_dataset.iterrows()):\n    t, q, a = instance.question_title, instance.question_body, instance.answer\n\n    t, q, a = _trim_input(t, q, a, max_sequence_length)\n\n    ids, masks, segments = _convert_to_bert_inputs(t, q, a, tokenizer, max_sequence_length)\n    input_ids.append(ids)\n    input_masks.append(masks)\n    input_segments.append(segments)\n\nX_test_bert =  [np.asarray(input_ids, dtype=np.int32), \n                np.asarray(input_masks, dtype=np.int32), \n                np.asarray(input_segments, dtype=np.int32)]","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:26:09.490806Z","iopub.execute_input":"2022-02-22T00:26:09.491403Z","iopub.status.idle":"2022-02-22T00:26:13.622108Z","shell.execute_reply.started":"2022-02-22T00:26:09.49136Z","shell.execute_reply":"2022-02-22T00:26:13.621369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(X_train_bert), X_train_bert[0].shape, X_train_bert[1].shape, X_train_bert[2].shape","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:26:13.623437Z","iopub.execute_input":"2022-02-22T00:26:13.623868Z","iopub.status.idle":"2022-02-22T00:26:13.631615Z","shell.execute_reply.started":"2022-02-22T00:26:13.623825Z","shell.execute_reply":"2022-02-22T00:26:13.630736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fine-tuning bert model","metadata":{"editable":false}},{"cell_type":"code","source":"from scipy.stats import spearmanr\n\nclass SpearmanCallback(tf.keras.callbacks.Callback):\n    def __init__(self, validation_data):\n        self.x_val = validation_data[0]\n        self.y_val = validation_data[1]\n\n    def on_epoch_end(self, epoch, logs={}):\n        y_pred_val = self.model.predict(self.x_val)\n        rho_val = np.mean([spearmanr(self.y_val[:, ind], y_pred_val[:, ind] + np.random.normal(0, 1e-7, y_pred_val.shape[0])).correlation for ind in range(y_pred_val.shape[1])])\n        print(rho_val)\n        print('\\nval_spearman-corr: %s' % (str(round(rho_val, 6))), end=100*' '+'\\n')\n        return rho_val","metadata":{"execution":{"iopub.status.busy":"2022-02-22T00:26:13.633266Z","iopub.execute_input":"2022-02-22T00:26:13.63359Z","iopub.status.idle":"2022-02-22T00:26:13.645344Z","shell.execute_reply.started":"2022-02-22T00:26:13.633553Z","shell.execute_reply":"2022-02-22T00:26:13.644535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.backend.clear_session()\n\nmax_seq_length = 512\n\ninput_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"input_word_ids\")\n\ninput_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"input_mask\")\n\nsegment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"segment_ids\")\n\nbert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\", trainable=True)\npooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n\nbert_model = tf.keras.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=sequence_output)\n","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:26:13.64657Z","iopub.execute_input":"2022-02-22T00:26:13.647848Z","iopub.status.idle":"2022-02-22T00:26:25.208235Z","shell.execute_reply.started":"2022-02-22T00:26:13.64781Z","shell.execute_reply":"2022-02-22T00:26:25.207473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ninput_word_ids = tf.keras.layers.Input(\n    (512,), dtype=tf.int32, name='input_word_ids')\ninput_masks = tf.keras.layers.Input(\n    (512,), dtype=tf.int32, name='input_masks')\ninput_segments = tf.keras.layers.Input(\n    (512,), dtype=tf.int32, name='input_segments')\n\n\nsequence_output = bert_model([input_word_ids, input_masks, input_segments])\n\nx = tf.keras.layers.GlobalAveragePooling1D()(sequence_output)\nx = tf.keras.layers.Dropout(0.2)(x)\nout = tf.keras.layers.Dense(30, activation=\"sigmoid\", name=\"dense_output\")(x)\n\nmodel = tf.keras.Model(\n    inputs=[input_word_ids, input_masks, input_segments], outputs=out\n)\n    \nmodel.summary()","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:45:49.416959Z","iopub.execute_input":"2022-02-22T00:45:49.417237Z","iopub.status.idle":"2022-02-22T00:45:49.671477Z","shell.execute_reply.started":"2022-02-22T00:45:49.417208Z","shell.execute_reply":"2022-02-22T00:45:49.67068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.utils.plot_model(model)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:45:52.985276Z","iopub.execute_input":"2022-02-22T00:45:52.985573Z","iopub.status.idle":"2022-02-22T00:45:53.216491Z","shell.execute_reply.started":"2022-02-22T00:45:52.985539Z","shell.execute_reply":"2022-02-22T00:45:53.21563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.array(y_valid_dataset)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:45:54.416927Z","iopub.execute_input":"2022-02-22T00:45:54.417694Z","iopub.status.idle":"2022-02-22T00:45:54.424619Z","shell.execute_reply.started":"2022-02-22T00:45:54.417653Z","shell.execute_reply":"2022-02-22T00:45:54.423701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"custom_callback = SpearmanCallback(\n        validation_data=(X_valid_bert, np.array(y_valid_dataset))\n)\n","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T00:45:55.687177Z","iopub.execute_input":"2022-02-22T00:45:55.687607Z","iopub.status.idle":"2022-02-22T00:45:55.693524Z","shell.execute_reply.started":"2022-02-22T00:45:55.68757Z","shell.execute_reply":"2022-02-22T00:45:55.692686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train_dataset = np.asarray(y_train_dataset)\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)) # 3e-5\n\nhistory = model.fit(X_train_bert, y_train_dataset, epochs=10, \n          validation_data=(X_valid_bert, np.array(y_valid_dataset)),\n              batch_size=4, callbacks=[custom_callback])","metadata":{"execution":{"iopub.status.busy":"2022-02-22T00:46:12.839325Z","iopub.execute_input":"2022-02-22T00:46:12.839607Z","iopub.status.idle":"2022-02-22T01:56:11.890189Z","shell.execute_reply.started":"2022-02-22T00:46:12.839578Z","shell.execute_reply":"2022-02-22T01:56:11.888835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(history.history).plot()\nplt.title(\"Fine tuning Bert model\")","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T01:56:11.892785Z","iopub.execute_input":"2022-02-22T01:56:11.893557Z","iopub.status.idle":"2022-02-22T01:56:12.223707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tweaking bert model\n\n* Increase the dropouts rate for avoiding overfitting.\n* Adding regularization to the layers to reduce overfitting.\n* Adding one or more dense layers before final dense layer.","metadata":{"editable":false}},{"cell_type":"code","source":"# Building the model\n\n\ninput_word_ids = tf.keras.layers.Input(\n    (512,), dtype=tf.int32, name='input_word_ids')\ninput_masks = tf.keras.layers.Input(\n    (512,), dtype=tf.int32, name='input_masks')\ninput_segments = tf.keras.layers.Input(\n    (512,), dtype=tf.int32, name='input_segments')\n\n\nsequence_output = bert_model([input_word_ids, input_masks, input_segments])\n\nx = tf.keras.layers.GlobalAveragePooling1D()(sequence_output)\nx = tf.keras.layers.Dropout(0.5)(x) # increased from 0.2 to 0.5\nx = tf.keras.layers.Dense(128,  kernel_regularizer= tf.keras.regularizers.l1())(x)\nx = tf.keras.layers.Dense(64, kernel_regularizer= tf.keras.regularizers.l1())(x) \n\nx = tf.keras.layers.Dense(32)(x)\n\nout = tf.keras.layers.Dense(30, activation=\"sigmoid\", name=\"dense_output\")(x)\n\nmodel = tf.keras.Model(\n    inputs=[input_word_ids, input_masks, input_segments], outputs=out\n)\n    \nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-02-22T02:24:55.17157Z","iopub.execute_input":"2022-02-22T02:24:55.172127Z","iopub.status.idle":"2022-02-22T02:24:55.412503Z","shell.execute_reply.started":"2022-02-22T02:24:55.172085Z","shell.execute_reply":"2022-02-22T02:24:55.411775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.utils.plot_model(model)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T02:24:55.41461Z","iopub.execute_input":"2022-02-22T02:24:55.415296Z","iopub.status.idle":"2022-02-22T02:24:55.636449Z","shell.execute_reply.started":"2022-02-22T02:24:55.415255Z","shell.execute_reply":"2022-02-22T02:24:55.635607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"custom_callback = SpearmanCallback(\n        validation_data=(X_valid_bert, np.array(y_valid_dataset))\n)\n","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T02:24:55.639202Z","iopub.execute_input":"2022-02-22T02:24:55.640031Z","iopub.status.idle":"2022-02-22T02:24:55.645172Z","shell.execute_reply.started":"2022-02-22T02:24:55.639987Z","shell.execute_reply":"2022-02-22T02:24:55.644508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(loss='binary_crossentropy',\n              optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)) # 3e-5\n\nhistory = model.fit(X_train_bert, y_train_dataset, epochs=30, \n          validation_data=(X_valid_bert, np.array(y_valid_dataset)),\n              batch_size=4, callbacks=[custom_callback])","metadata":{"execution":{"iopub.status.busy":"2022-02-22T02:24:55.688423Z","iopub.execute_input":"2022-02-22T02:24:55.688925Z","iopub.status.idle":"2022-02-22T03:35:24.950538Z","shell.execute_reply.started":"2022-02-22T02:24:55.688899Z","shell.execute_reply":"2022-02-22T03:35:24.949577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(history.history).plot()\nplt.title(\"Tweaking Bert model\")","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-22T03:35:24.953778Z","iopub.execute_input":"2022-02-22T03:35:24.954945Z","iopub.status.idle":"2022-02-22T03:35:25.219998Z","shell.execute_reply.started":"2022-02-22T03:35:24.954906Z","shell.execute_reply":"2022-02-22T03:35:25.219293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"editable":false},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Reference: \n* https://www.kaggle.com/abhinand05/bert-for-humans-tutorial-baseline-version-2","metadata":{"editable":false}},{"cell_type":"code","source":"","metadata":{"editable":false},"execution_count":null,"outputs":[]}]}