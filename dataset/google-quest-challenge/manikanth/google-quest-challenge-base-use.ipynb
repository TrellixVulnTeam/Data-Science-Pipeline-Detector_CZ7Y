{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Aim of this Challenge:** \n\nCreate intelligent question and answer systems that can reliably predict context without relying on complicated and opaque rating guidelines.","metadata":{"editable":false}},{"cell_type":"markdown","source":"# The Business Problem:\n\n\nTo create a more human-like question and answering system can answer the provided question having the intuitive understanding of the question. This can attract users and address their question more human-like and this can also increase the number of user participation in the question answering forms and create human-like conversation chat boxes.\n","metadata":{"editable":false}},{"cell_type":"markdown","source":"# Exploring dataset","metadata":{"editable":false}},{"cell_type":"code","source":"# importing the required libraries \n\nimport pandas as pd\nimport  numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:02:15.648746Z","iopub.execute_input":"2022-02-20T13:02:15.649312Z","iopub.status.idle":"2022-02-20T13:02:16.489566Z","shell.execute_reply.started":"2022-02-20T13:02:15.649213Z","shell.execute_reply":"2022-02-20T13:02:16.488823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = pd.read_csv('/kaggle/input/google-quest-challenge/train.csv')\ntest_dataset = pd.read_csv('/kaggle/input/google-quest-challenge/test.csv')\nsample_submission_dataset = pd.read_csv('/kaggle/input/google-quest-challenge/sample_submission.csv')\n\nprint(\"Train shape:\", train_dataset.shape)\nprint(\"Test shape:\", test_dataset.shape)\nprint(\"Sample submission shape:\", sample_submission_dataset.shape)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:02:16.491052Z","iopub.execute_input":"2022-02-20T13:02:16.491653Z","iopub.status.idle":"2022-02-20T13:02:16.934902Z","shell.execute_reply.started":"2022-02-20T13:02:16.491614Z","shell.execute_reply":"2022-02-20T13:02:16.934171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observations:\n* In train dataset we have 41 column and 6079 rows(instances/training points).\n* in test dataset we have only 11 column and 476 rows(instances/test points).\n* in submission dataset we have 31 column and 476 rows.","metadata":{"editable":false}},{"cell_type":"code","source":"# Check for train data samples\ntrain_dataset.head(2)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:02:16.938568Z","iopub.execute_input":"2022-02-20T13:02:16.938944Z","iopub.status.idle":"2022-02-20T13:02:16.970343Z","shell.execute_reply.started":"2022-02-20T13:02:16.93891Z","shell.execute_reply":"2022-02-20T13:02:16.969541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# getting basic info from training data\ntrain_dataset.info()","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:02:16.972391Z","iopub.execute_input":"2022-02-20T13:02:16.972715Z","iopub.status.idle":"2022-02-20T13:02:17.001337Z","shell.execute_reply.started":"2022-02-20T13:02:16.972676Z","shell.execute_reply":"2022-02-20T13:02:17.00045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> **Observations:** There are 10 features and no null values and 10 are having type as object and 30 labels are having type as float64 \n\n### Features:\n 1   question_title                         \n 2   question_body                           \n 3   question_user_name                      \n 4   question_user_page                     \n 5   answer                                 \n 6   answer_user_name                      \n 7   answer_user_page                        \n 8   url                                     \n 9   category                                \n 10  host      ","metadata":{"editable":false}},{"cell_type":"code","source":"# Describing the train data\ntrain_dataset.describe()","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:02:17.002758Z","iopub.execute_input":"2022-02-20T13:02:17.003023Z","iopub.status.idle":"2022-02-20T13:02:17.084912Z","shell.execute_reply.started":"2022-02-20T13:02:17.002988Z","shell.execute_reply":"2022-02-20T13:02:17.084192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Observations:** \n* In the above 41 columns, 10 are feature and 30 are the class labels and one column qa_id is the unique ID for every instance.\n* **21 class** labels are for **questions** that is the label  that starts with \"question_...\"\n* **9 class** labels are for **answers** that is the label  which starts with \"answer_...\"\n\n* Total we have **30 Class Lables**","metadata":{"editable":false}},{"cell_type":"code","source":"# Let's see the list of column names\n\nlist(train_dataset.columns[1:])","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:02:17.086165Z","iopub.execute_input":"2022-02-20T13:02:17.088051Z","iopub.status.idle":"2022-02-20T13:02:17.09529Z","shell.execute_reply.started":"2022-02-20T13:02:17.088006Z","shell.execute_reply":"2022-02-20T13:02:17.094537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset.head()","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:02:17.096708Z","iopub.execute_input":"2022-02-20T13:02:17.09716Z","iopub.status.idle":"2022-02-20T13:02:17.127763Z","shell.execute_reply.started":"2022-02-20T13:02:17.097121Z","shell.execute_reply":"2022-02-20T13:02:17.12694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Checking density of words & characters present in the `question_title` feature","metadata":{"editable":false}},{"cell_type":"code","source":"import seaborn as sns\n\n\ndef word_count(sentense):\n    sentense = sentense.strip()\n\n    return len(sentense.split(\" \"))\n\n\nfig, ax = plt.subplots(1,2, figsize = ( 20 , 5))\n\n\nquestion_title_lengths_train = train_dataset['question_title'].apply(len)\nquestion_title_lengths_test = test_dataset['question_title'].apply(len)\nquestion_title_lengths_train_words = train_dataset['question_title'].apply(word_count)\nquestion_title_lengths_test_words = test_dataset['question_title'].apply(word_count)\n\n\nsns.histplot(question_title_lengths_train, label=\"Train\", kde=True, stat=\"density\", linewidth=0,  color=\"red\", ax=ax[0])\nsns.histplot(question_title_lengths_test, label=\"Test\", kde=True, stat=\"density\", linewidth=0,  color=\"blue\", ax=ax[0])\nsns.histplot(question_title_lengths_train_words, label=\"Train\", kde=True, stat=\"density\", linewidth=0,  color=\"red\", ax=ax[1])\nsns.histplot(question_title_lengths_test_words, label=\"Test\", kde=True, stat=\"density\", linewidth=0,  color=\"blue\", ax=ax[1])\n\n# Set label for x-axis\nax[0].set_xlabel( \"No. of characters\" , size = 12 )\n  \n# Set label for y-axis\nax[0].set_ylabel( \"Density of character\" , size = 12 )\n  \n# Set title for plot\nax[0].set_title( \"Density of characters in 'question_title' feature\\n\" , size = 15 )\n\nax[0].legend()\n\n\n# Set label for x-axis\nax[1].set_xlabel( \"No. of Words\" , size = 12 )\n  \n# Set label for y-axis\nax[1].set_ylabel( \"Density of Words\" , size = 12 )\n  \n# Set title for plot\nax[1].set_title( \"Density of Words in 'question_title' feature\\n\" , size = 15 )\n\nax[1].legend()\n\n\n\nplt.show();\n","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:02:17.129224Z","iopub.execute_input":"2022-02-20T13:02:17.129491Z","iopub.status.idle":"2022-02-20T13:02:17.943766Z","shell.execute_reply.started":"2022-02-20T13:02:17.129456Z","shell.execute_reply":"2022-02-20T13:02:17.942934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observation: \n* Both train and test having the same distribution of characters and words. \n* Most of the words lies in range 5-10 both train and test. \n* Most of the characters lies in the range 40-60 train and test. ","metadata":{"editable":false}},{"cell_type":"markdown","source":"## Checking density of words & characters present in the `question_body` feature","metadata":{"editable":false}},{"cell_type":"code","source":"import seaborn as sns\n\n\ndef word_count(sentense):\n    sentense = sentense.strip()\n\n    return len(sentense.split(\" \"))\n\n\nfig, ax = plt.subplots(1,2, figsize = ( 20 , 5))\n\n\nquestion_body_lengths_train = train_dataset['question_body'].apply(len)\nquestion_body_lengths_test = test_dataset['question_body'].apply(len)\nquestion_body_lengths_train_words = train_dataset['question_body'].apply(word_count)\nquestion_body_lengths_test_words = test_dataset['question_body'].apply(word_count)\n\n\nsns.histplot(question_body_lengths_train, label=\"Train\", kde=True, stat=\"density\", linewidth=0,  color=\"red\", ax=ax[0])\nsns.histplot(question_body_lengths_test, label=\"Test\", kde=True, stat=\"density\", linewidth=0,  color=\"blue\", ax=ax[0])\nsns.histplot(question_body_lengths_train_words, label=\"Train\", kde=True, stat=\"density\", linewidth=0,  color=\"red\", ax=ax[1])\nsns.histplot(question_body_lengths_test_words, label=\"Test\", kde=True, stat=\"density\", linewidth=0,  color=\"blue\", ax=ax[1])\n\n# Set label for x-axis\nax[0].set_xlabel( \"No. of characters\" , size = 12 )\n  \n# Set label for y-axis\nax[0].set_ylabel( \"Density of character\" , size = 12 )\n  \n# Set title for plot\nax[0].set_title( \"Density of characters in 'question_body' feature\\n\" , size = 15 )\n\nax[0].legend()\n\n\n# Set label for x-axis\nax[1].set_xlabel( \"No. of Words\" , size = 12 )\n  \n# Set label for y-axis\nax[1].set_ylabel( \"Density of Words\" , size = 12 )\n  \n# Set title for plot\nax[1].set_title( \"Density of Words in 'question_body' feature\\n\" , size = 15 )\n\nax[1].legend()\n\n\n\nplt.show();\n","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:02:17.947995Z","iopub.execute_input":"2022-02-20T13:02:17.948267Z","iopub.status.idle":"2022-02-20T13:02:21.003725Z","shell.execute_reply.started":"2022-02-20T13:02:17.948229Z","shell.execute_reply":"2022-02-20T13:02:21.003057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observation:\n* We can observe that the distribution of both words and characters are very much right skewed.\n* Most of the characters in question_body lies below 2500.\n* Most of the words in question_body lies below 1000.","metadata":{"editable":false}},{"cell_type":"markdown","source":"## Similarly we will check for `answer` feature","metadata":{"editable":false}},{"cell_type":"code","source":"import seaborn as sns\n\n\ndef word_count(sentense):\n    sentense = sentense.strip()\n    return len(sentense.split(\" \"))\n\n\nfig, ax = plt.subplots(1,2, figsize = ( 20 , 5))\n\n\nanswer_lengths_train = train_dataset['answer'].apply(len)\nanswer_lengths_test = test_dataset['answer'].apply(len)\nanswer_lengths_train_words = train_dataset['answer'].apply(word_count)\nanswer_lengths_test_words = test_dataset['answer'].apply(word_count)\n\n\nsns.histplot(answer_lengths_train, label=\"Train\", kde=True, stat=\"density\", linewidth=0,  color=\"red\", ax=ax[0])\nsns.histplot(answer_lengths_test, label=\"Test\", kde=True, stat=\"density\", linewidth=0,  color=\"blue\", ax=ax[0])\nsns.histplot(answer_lengths_train_words, label=\"Train\", kde=True, stat=\"density\", linewidth=0,  color=\"red\", ax=ax[1])\nsns.histplot(answer_lengths_test_words, label=\"Test\", kde=True, stat=\"density\", linewidth=0,  color=\"blue\", ax=ax[1])\n\n# Set label for x-axis\nax[0].set_xlabel( \"No. of characters\" , size = 12 )\n  \n# Set label for y-axis\nax[0].set_ylabel( \"Density of character\" , size = 12 )\n  \n# Set title for plot\nax[0].set_title( \"Density of characters in 'answer' feature\\n\" , size = 15 )\n\nax[0].legend()\n\n\n# Set label for x-axis\nax[1].set_xlabel( \"No. of Words\" , size = 12 )\n  \n# Set label for y-axis\nax[1].set_ylabel( \"Density of Words\" , size = 12 )\n  \n# Set title for plot\nax[1].set_title( \"Density of Words in 'answer' feature\\n\" , size = 15 )\n\nax[1].legend()\n\n\n\nplt.show();\n","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:02:21.006737Z","iopub.execute_input":"2022-02-20T13:02:21.008027Z","iopub.status.idle":"2022-02-20T13:02:23.726376Z","shell.execute_reply.started":"2022-02-20T13:02:21.007985Z","shell.execute_reply":"2022-02-20T13:02:23.725716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observation:\n* As similar to question_body we can find that answer distribution is also skewed.\n* Their may be some extreme outlier instance that words/char length are very high in both question_body and answer features.","metadata":{"editable":false}},{"cell_type":"markdown","source":"## Analyzing `question_body` and `answer` features sequence length","metadata":{"editable":false}},{"cell_type":"code","source":"for i in range(0,101,10):\n    print(f'{i}th percentile of question_body input sequence {np.percentile(question_body_lengths_train_words, i)}')\nprint()\nfor i in range(90,101):\n    print(f'{i}th percentile of question_body input sequence {np.percentile(question_body_lengths_train_words, i)}')\nprint()\nfor i in [99.1,99.2,99.3,99.4,99.5,99.6,99.7,99.8,99.9,100]:\n    print(f'{i}th percentile of question_body input sequence {np.percentile(question_body_lengths_train_words, i)}')","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:02:23.727631Z","iopub.execute_input":"2022-02-20T13:02:23.730038Z","iopub.status.idle":"2022-02-20T13:02:23.752393Z","shell.execute_reply.started":"2022-02-20T13:02:23.729997Z","shell.execute_reply":"2022-02-20T13:02:23.751318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Observation:** 99.9% the of words in question body lies below **3220**","metadata":{"editable":false}},{"cell_type":"code","source":"for i in range(0,101,10):\n    print(f'{i}th percentile of answer input sequence {np.percentile(answer_lengths_train_words, i)}')\nprint()\nfor i in range(90,101):\n    print(f'{i}th percentile of answer input sequence {np.percentile(answer_lengths_train_words, i)}')\nprint()\nfor i in [99.1,99.2,99.3,99.4,99.5,99.6,99.7,99.8,99.9,100]:\n    print(f'{i}th percentile of answer input sequence {np.percentile(answer_lengths_train_words, i)}')","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:02:23.753669Z","iopub.execute_input":"2022-02-20T13:02:23.753906Z","iopub.status.idle":"2022-02-20T13:02:23.776396Z","shell.execute_reply.started":"2022-02-20T13:02:23.753858Z","shell.execute_reply":"2022-02-20T13:02:23.775765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Observation:** 99.9% of words in answer feature lies below **2200**","metadata":{"editable":false}},{"cell_type":"markdown","source":"# Analyzing `category` Feature","metadata":{"editable":false}},{"cell_type":"code","source":"train_dataset['category'].unique()","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:02:23.7776Z","iopub.execute_input":"2022-02-20T13:02:23.777815Z","iopub.status.idle":"2022-02-20T13:02:23.784751Z","shell.execute_reply.started":"2022-02-20T13:02:23.777784Z","shell.execute_reply":"2022-02-20T13:02:23.784028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_category_feature_count = train_dataset['category'].value_counts()\ntest_category_feature_count = test_dataset['category'].value_counts()\n\nprint(\"Train category:\\n\",train_category_feature_count)\nprint()\nprint(\"Test category:\\n\",test_category_feature_count)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:02:23.786233Z","iopub.execute_input":"2022-02-20T13:02:23.786631Z","iopub.status.idle":"2022-02-20T13:02:23.797259Z","shell.execute_reply.started":"2022-02-20T13:02:23.786594Z","shell.execute_reply":"2022-02-20T13:02:23.796458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"figure, ax = plt.subplots(1,2, figsize=(12, 6))\n\ntrain_category_feature_count.plot(kind='bar', ax=ax[0])\ntest_category_feature_count.plot(kind='bar', ax=ax[1])\n\nax[0].set_title('Train')\nax[0].set_xlabel( \"unique category\" , size = 12 )\nax[0].set_ylabel( \"count\" , size = 12 )\n\nax[1].set_title('Test')\nax[1].set_xlabel( \"unique category\" , size = 12 )\nax[1].set_ylabel( \"count\" , size = 12 )\n\nplt.show()","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:02:23.798978Z","iopub.execute_input":"2022-02-20T13:02:23.7993Z","iopub.status.idle":"2022-02-20T13:02:24.125597Z","shell.execute_reply.started":"2022-02-20T13:02:23.799268Z","shell.execute_reply":"2022-02-20T13:02:24.124916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sample stack over flow question and answer\ntrain_dataset[train_dataset['category'] == 'STACKOVERFLOW'].values[11]","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:02:24.1268Z","iopub.execute_input":"2022-02-20T13:02:24.127066Z","iopub.status.idle":"2022-02-20T13:02:24.142123Z","shell.execute_reply.started":"2022-02-20T13:02:24.127022Z","shell.execute_reply":"2022-02-20T13:02:24.141478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sample science question and answer \ntrain_dataset[train_dataset['category'] == 'SCIENCE'].values[11]","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:02:24.143286Z","iopub.execute_input":"2022-02-20T13:02:24.143601Z","iopub.status.idle":"2022-02-20T13:02:24.156628Z","shell.execute_reply.started":"2022-02-20T13:02:24.143567Z","shell.execute_reply":"2022-02-20T13:02:24.15576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sample life art and culture question and answer\ntrain_dataset[train_dataset['category'] == 'LIFE_ARTS'].values[11]","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:02:24.158087Z","iopub.execute_input":"2022-02-20T13:02:24.159104Z","iopub.status.idle":"2022-02-20T13:02:24.169514Z","shell.execute_reply.started":"2022-02-20T13:02:24.159066Z","shell.execute_reply":"2022-02-20T13:02:24.168707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sample life art and culture question and answer\ntrain_dataset[train_dataset['category'] == 'CULTURE'].values[11]","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:02:24.170942Z","iopub.execute_input":"2022-02-20T13:02:24.171508Z","iopub.status.idle":"2022-02-20T13:02:24.181801Z","shell.execute_reply.started":"2022-02-20T13:02:24.171471Z","shell.execute_reply":"2022-02-20T13:02:24.181086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observation:\n* Five unique category are present in the category feature.\n* **Technology** and **Stackoverflow** are the highest count and both are related topics.\n* **Life_arts** as the lowest count category.\n* Distribution of train and test category are the same.\n* **Life_arts & culture** follow general english syntax & structure.\n* **Science** utilizes latex with expressions prepended and appended with symbol: $\n* **Technology & stackoverflow** have code snippets & logs.","metadata":{"editable":false}},{"cell_type":"markdown","source":"# Word cloud","metadata":{"editable":false}},{"cell_type":"code","source":"from wordcloud import WordCloud\n\n\ndef plot_wordcloud(text, ax, title=None):\n    wordcloud = WordCloud(max_font_size=None, background_color='white', width=1200, height=1000).generate(text)\n    ax.imshow(wordcloud)\n    if title is not None:\n        ax.set_title(title, size = 15)\n    ax.axis(\"off\")","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:02:24.183217Z","iopub.execute_input":"2022-02-20T13:02:24.183629Z","iopub.status.idle":"2022-02-20T13:02:24.229552Z","shell.execute_reply.started":"2022-02-20T13:02:24.183594Z","shell.execute_reply":"2022-02-20T13:02:24.228888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n\n# word cloud for train data\ntext = ' '.join(train_dataset['question_title'].values)\nplot_wordcloud(text, axes[0][0], 'Train Question title')\n\ntext = ' '.join(train_dataset['question_body'].values)\nplot_wordcloud(text, axes[0][1], 'Train Question body')\n\ntext = ' '.join(train_dataset['answer'].values)\nplot_wordcloud(text, axes[0][2], 'Train Answer')\n\n\n# word cloud for Test data\ntext = ' '.join(test_dataset['question_title'].values)\nplot_wordcloud(text, axes[1][0], 'Test Question title')\n\ntext = ' '.join(test_dataset['question_body'].values)\nplot_wordcloud(text, axes[1][1], 'Test Question body')\n\ntext = ' '.join(test_dataset['answer'].values)\nplot_wordcloud(text, axes[1][2], 'Test Answer')\n\nplt.tight_layout()\nfig.show()","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:02:24.230793Z","iopub.execute_input":"2022-02-20T13:02:24.231225Z","iopub.status.idle":"2022-02-20T13:02:47.79825Z","shell.execute_reply.started":"2022-02-20T13:02:24.231187Z","shell.execute_reply":"2022-02-20T13:02:47.795948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observation:\n* We can observe that some of words match between train and test set.\nReference: https://www.kaggle.com/corochann/google-quest-first-data-introduction?scriptVersionId=23910525&cellId=34","metadata":{"editable":false}},{"cell_type":"markdown","source":"# Analyzing labels ","metadata":{"editable":false}},{"cell_type":"code","source":"for label in train_dataset.columns[11:]:\n    print(f\"{label:.20}: no. of unique label values: {len(train_dataset[label].unique())}\")","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:02:47.799256Z","iopub.execute_input":"2022-02-20T13:02:47.799495Z","iopub.status.idle":"2022-02-20T13:02:47.816979Z","shell.execute_reply.started":"2022-02-20T13:02:47.799459Z","shell.execute_reply":"2022-02-20T13:02:47.816352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observation:\n* The output label are regression(real) values but the distribution is not continuous.\n* Except for `answer_satisfaction` label rest every label are having unique values some are with 9 unique values and some are of 5 unique values.\n* Using this insights we can use post pocessing to get better scoring ","metadata":{"editable":false}},{"cell_type":"code","source":"for label in train_dataset.columns[11:]:\n    sns.histplot(train_dataset[label], label=label, kde=False)\n    plt.show()","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:02:47.818419Z","iopub.execute_input":"2022-02-20T13:02:47.818896Z","iopub.status.idle":"2022-02-20T13:02:55.165526Z","shell.execute_reply.started":"2022-02-20T13:02:47.818839Z","shell.execute_reply":"2022-02-20T13:02:55.164741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observation:\n* **Label values are imbalance** like for some of the label values are having only one values ex: **question_type_spelling**, **question_not_really_question** etc that is the distribution of label are very dissimilar.","metadata":{"editable":false}},{"cell_type":"markdown","source":"### correlation between target variables","metadata":{"editable":false}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(20,20))   \nsns.heatmap(train_dataset[11:].corr(), linewidths=1, ax=ax, annot_kws={\"fontsize\":40})\nplt.show();","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:02:55.166836Z","iopub.execute_input":"2022-02-20T13:02:55.167142Z","iopub.status.idle":"2022-02-20T13:02:56.180604Z","shell.execute_reply.started":"2022-02-20T13:02:55.1671Z","shell.execute_reply":"2022-02-20T13:02:56.179844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observations:\nFrom the above heatmap of correleation we can observe that `answer_helpful`, `answer_level_of_information`, `answer_plausible`, `answer_releveance` and `answer_satification` have some correlation between them.","metadata":{"editable":false}},{"cell_type":"markdown","source":"## Analyzing `host` feature","metadata":{"editable":false}},{"cell_type":"code","source":"print(f\"Total unique host present in the dataset {len(train_dataset['host'].unique())}\")","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:02:56.181841Z","iopub.execute_input":"2022-02-20T13:02:56.18304Z","iopub.status.idle":"2022-02-20T13:02:56.19034Z","shell.execute_reply.started":"2022-02-20T13:02:56.182998Z","shell.execute_reply":"2022-02-20T13:02:56.189335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_host_feature_count = train_dataset['host'].value_counts()\n\n\nfigure, ax = plt.subplots( figsize=(20, 5))\n\ntrain_host_feature_count.plot(kind='bar', ax=ax)\n\nax.set_title('Train dataset - count of Q&A collected from each website', size=20)\nax.set_xlabel( \"Host\" , size = 12 )\nax.set_ylabel( \"Count\" , size = 12 )\n\nplt.show()","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:02:56.192348Z","iopub.execute_input":"2022-02-20T13:02:56.193053Z","iopub.status.idle":"2022-02-20T13:02:57.583457Z","shell.execute_reply.started":"2022-02-20T13:02:56.192946Z","shell.execute_reply":"2022-02-20T13:02:57.582795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_host_feature_count = test_dataset['host'].value_counts()\nfigure, ax = plt.subplots( figsize=(20, 5))\ntest_host_feature_count.plot(kind='bar', ax=ax)\nax.set_title('Test dataset - count of Q&A collected from each website', size=20)\nax.set_xlabel( \"Host\" , size = 12 )\nax.set_ylabel( \"Count\" , size = 12 )\nplt.show()\n","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:02:57.584508Z","iopub.execute_input":"2022-02-20T13:02:57.584857Z","iopub.status.idle":"2022-02-20T13:02:58.639454Z","shell.execute_reply.started":"2022-02-20T13:02:57.584822Z","shell.execute_reply":"2022-02-20T13:02:58.638725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observation:\n* All question and answer in the dataset are extracted from **63 websites**.\n* Most of the question and answer are from **stackoverflow.com** as we observe from the  `category` feature analysis that most of the caterogy fall under **technology and stackoverflow**.","metadata":{"editable":false}},{"cell_type":"markdown","source":"","metadata":{"editable":false}},{"cell_type":"markdown","source":"# Spliting the data in to train and validation","metadata":{"editable":false}},{"cell_type":"code","source":"y_columns = ['question_asker_intent_understanding',\n       'question_body_critical', 'question_conversational',\n       'question_expect_short_answer', 'question_fact_seeking',\n       'question_has_commonly_accepted_answer',\n       'question_interestingness_others', 'question_interestingness_self',\n       'question_multi_intent', 'question_not_really_a_question',\n       'question_opinion_seeking', 'question_type_choice',\n       'question_type_compare', 'question_type_consequence',\n       'question_type_definition', 'question_type_entity',\n       'question_type_instructions', 'question_type_procedure',\n       'question_type_reason_explanation', 'question_type_spelling',\n       'question_well_written', 'answer_helpful',\n       'answer_level_of_information', 'answer_plausible', 'answer_relevance',\n       'answer_satisfaction', 'answer_type_instructions',\n       'answer_type_procedure', 'answer_type_reason_explanation',\n       'answer_well_written']\n\ny = train_dataset[y_columns]\nX = train_dataset.drop(y_columns,axis=1)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:02:58.645446Z","iopub.execute_input":"2022-02-20T13:02:58.645647Z","iopub.status.idle":"2022-02-20T13:02:58.655146Z","shell.execute_reply.started":"2022-02-20T13:02:58.645621Z","shell.execute_reply":"2022-02-20T13:02:58.653907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.shape, y.shape","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:02:58.656378Z","iopub.execute_input":"2022-02-20T13:02:58.657083Z","iopub.status.idle":"2022-02-20T13:02:58.669365Z","shell.execute_reply.started":"2022-02-20T13:02:58.657045Z","shell.execute_reply":"2022-02-20T13:02:58.668686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n\nX_train_dataset, X_valid_dataset, y_train_dataset, y_valid_dataset = train_test_split(X,y, test_size=0.10)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:02:58.670932Z","iopub.execute_input":"2022-02-20T13:02:58.671202Z","iopub.status.idle":"2022-02-20T13:02:58.82234Z","shell.execute_reply.started":"2022-02-20T13:02:58.671167Z","shell.execute_reply":"2022-02-20T13:02:58.821518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_dataset.shape, X_valid_dataset.shape, y_train_dataset.shape, y_valid_dataset.shape","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:02:58.824565Z","iopub.execute_input":"2022-02-20T13:02:58.825436Z","iopub.status.idle":"2022-02-20T13:02:58.833364Z","shell.execute_reply.started":"2022-02-20T13:02:58.825392Z","shell.execute_reply":"2022-02-20T13:02:58.832597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_dataset","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:02:58.834741Z","iopub.execute_input":"2022-02-20T13:02:58.835575Z","iopub.status.idle":"2022-02-20T13:02:58.859625Z","shell.execute_reply.started":"2022-02-20T13:02:58.835534Z","shell.execute_reply":"2022-02-20T13:02:58.858806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  **Preprocessing Text Feature**","metadata":{"editable":false}},{"cell_type":"code","source":"# https://stackoverflow.com/a/47091490/4084039\nimport re\n\ndef decontracted(phrase):\n    phrase = re.sub(r\"(W|w)on(\\'|\\’)t \", \"will not \", phrase)\n    phrase = re.sub(r\"(C|c)an(\\'|\\’)t \", \"can not \", phrase)\n    phrase = re.sub(r\"(Y|y)(\\'|\\’)all \", \"you all \", phrase)\n    phrase = re.sub(r\"(Y|y)a(\\'|\\’)ll \", \"you all \", phrase)\n    phrase = re.sub(r\"(I|i)(\\'|\\’)m \", \"i am \", phrase)\n    phrase = re.sub(r\"(A|a)isn(\\'|\\’)t \", \"is not \", phrase)\n    phrase = re.sub(r\"n(\\'|\\’)t \", \" not \", phrase)\n    phrase = re.sub(r\"(\\'|\\’)re \", \" are \", phrase)\n    phrase = re.sub(r\"(\\'|\\’)d \", \" would \", phrase)\n    phrase = re.sub(r\"(\\'|\\’)ll \", \" will \", phrase)\n    phrase = re.sub(r\"(\\'|\\’)t \", \" not \", phrase)\n    phrase = re.sub(r\"(\\'|\\’)ve \", \" have \", phrase)\n    \n    return phrase\n\n\ndef clean_text(x):\n\n    x = str(x)\n    for punct in \"/-'\":\n        x = x.replace(punct, ' ')\n    for punct in '&':\n        x = x.replace(punct, f' {punct} ')\n    for punct in '?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~' + '“”’':\n        x = x.replace(punct, '')\n    return x\n\ndef clean_numbers(x):\n\n    x = re.sub('[0-9]{5,}', '12345', x)\n    x = re.sub('[0-9]{4}', '1234', x)\n    x = re.sub('[0-9]{3}', '123', x)\n    x = re.sub('[0-9]{2}', '12', x)\n    return x","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:02:58.860968Z","iopub.execute_input":"2022-02-20T13:02:58.861325Z","iopub.status.idle":"2022-02-20T13:02:58.878762Z","shell.execute_reply.started":"2022-02-20T13:02:58.861283Z","shell.execute_reply":"2022-02-20T13:02:58.87771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://gist.github.com/sebleier/554280\n# we are removing the words from the stop words list: 'no', 'nor', 'not'\nstopwords= ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n            'won', \"won't\", 'wouldn', \"wouldn't\"]","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:02:58.880138Z","iopub.execute_input":"2022-02-20T13:02:58.880427Z","iopub.status.idle":"2022-02-20T13:02:58.895046Z","shell.execute_reply.started":"2022-02-20T13:02:58.880388Z","shell.execute_reply":"2022-02-20T13:02:58.894125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Combining all the above stundents \nfrom tqdm import tqdm\ndef preprocess_text(text_data):\n    preprocessed_text = []\n    # tqdm is for printing the status bar\n    for sentance in tqdm(text_data):\n        sent = decontracted(sentance)\n        sent = clean_text(sentance)\n        sent = clean_numbers(sentance)\n        sent = sent.replace('\\\\r', ' ')\n        sent = sent.replace('\\\\n', ' ')\n        sent = sent.replace('\\\\\"', ' ')\n        sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n        # https://gist.github.com/sebleier/554280\n        sent = ' '.join(e for e in sent.split() if e.lower() not in stopwords)\n        preprocessed_text.append(sent.lower().strip())\n    return preprocessed_text","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:02:58.89678Z","iopub.execute_input":"2022-02-20T13:02:58.897128Z","iopub.status.idle":"2022-02-20T13:02:58.907361Z","shell.execute_reply.started":"2022-02-20T13:02:58.897046Z","shell.execute_reply":"2022-02-20T13:02:58.906468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_dataset['preprocessed_question_title'] = preprocess_text(X_train_dataset['question_title'].values)\nX_train_dataset['preprocessed_question_body'] = preprocess_text(X_train_dataset['question_body'].values)\nX_train_dataset['preprocessed_answer'] = preprocess_text(X_train_dataset['answer'].values)\n\n\nX_valid_dataset['preprocessed_question_title'] = preprocess_text(X_valid_dataset['question_title'].values)\nX_valid_dataset['preprocessed_question_body'] = preprocess_text(X_valid_dataset['question_body'].values)\nX_valid_dataset['preprocessed_answer'] = preprocess_text(X_valid_dataset['answer'].values)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:02:58.909002Z","iopub.execute_input":"2022-02-20T13:02:58.909427Z","iopub.status.idle":"2022-02-20T13:03:08.450676Z","shell.execute_reply.started":"2022-02-20T13:02:58.909386Z","shell.execute_reply":"2022-02-20T13:03:08.449867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset['preprocessed_question_title'] = preprocess_text(test_dataset['question_title'].values)\ntest_dataset['preprocessed_question_body'] = preprocess_text(test_dataset['question_body'].values)\ntest_dataset['preprocessed_answer'] = preprocess_text(test_dataset['answer'].values)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:03:08.451975Z","iopub.execute_input":"2022-02-20T13:03:08.452302Z","iopub.status.idle":"2022-02-20T13:03:09.157906Z","shell.execute_reply.started":"2022-02-20T13:03:08.452263Z","shell.execute_reply":"2022-02-20T13:03:09.157199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### question_title text after preprocessing","metadata":{"editable":false}},{"cell_type":"code","source":"# Text before preprocessing\nX_train_dataset['question_title'].values[0]","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:03:09.159267Z","iopub.execute_input":"2022-02-20T13:03:09.159546Z","iopub.status.idle":"2022-02-20T13:03:09.16548Z","shell.execute_reply.started":"2022-02-20T13:03:09.159493Z","shell.execute_reply":"2022-02-20T13:03:09.1648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Text after preprocessing\nX_train_dataset['preprocessed_question_title'].values[0]","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:03:09.166732Z","iopub.execute_input":"2022-02-20T13:03:09.167343Z","iopub.status.idle":"2022-02-20T13:03:09.176145Z","shell.execute_reply.started":"2022-02-20T13:03:09.167306Z","shell.execute_reply":"2022-02-20T13:03:09.175319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### question_body after preprocessing","metadata":{"editable":false}},{"cell_type":"code","source":"# Text before preprocessing\nX_train_dataset['question_body'].values[0]","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:03:09.177578Z","iopub.execute_input":"2022-02-20T13:03:09.178033Z","iopub.status.idle":"2022-02-20T13:03:09.187524Z","shell.execute_reply.started":"2022-02-20T13:03:09.177995Z","shell.execute_reply":"2022-02-20T13:03:09.18677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Text after preprocessing\nX_train_dataset['preprocessed_question_body'].values[0]","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:03:09.189008Z","iopub.execute_input":"2022-02-20T13:03:09.189504Z","iopub.status.idle":"2022-02-20T13:03:09.19651Z","shell.execute_reply.started":"2022-02-20T13:03:09.189467Z","shell.execute_reply":"2022-02-20T13:03:09.195731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Answer after preprocessing","metadata":{"editable":false}},{"cell_type":"code","source":"# Text before preprocessing\nX_train_dataset['answer'].values[0]","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:03:09.197718Z","iopub.execute_input":"2022-02-20T13:03:09.198423Z","iopub.status.idle":"2022-02-20T13:03:09.2074Z","shell.execute_reply.started":"2022-02-20T13:03:09.198367Z","shell.execute_reply":"2022-02-20T13:03:09.206715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Text after preprocessing\nX_train_dataset['preprocessed_answer'].values[0]","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:03:09.208811Z","iopub.execute_input":"2022-02-20T13:03:09.209113Z","iopub.status.idle":"2022-02-20T13:03:09.216609Z","shell.execute_reply.started":"2022-02-20T13:03:09.209081Z","shell.execute_reply":"2022-02-20T13:03:09.215782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"editable":false},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Feature engineering:**","metadata":{"editable":false}},{"cell_type":"markdown","source":"## Text count based features:\n\n1. Number of characters in the **question_title**\n2. Number of characters in the **question_body**\n3. Number of characters in the **answer**\n4. Number of words in the **question_title**\n5. Number of words in the **question_body**\n6. Number of words in the **answer**\n7. Number of unique words in the **question_title**\n8. Number of unique words in the **question_body**\n9. Number of unique words in the **answer**\n","metadata":{"editable":false}},{"cell_type":"code","source":"def word_count(sentense):\n    sentense = sentense.strip()\n\n    return len(sentense.split(\" \"))\n\ndef unique_word_count(sentense):\n    sentense = sentense.strip()\n\n    return len(set(sentense.split(\" \")))\n","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:03:09.217887Z","iopub.execute_input":"2022-02-20T13:03:09.218506Z","iopub.status.idle":"2022-02-20T13:03:09.226507Z","shell.execute_reply.started":"2022-02-20T13:03:09.218469Z","shell.execute_reply":"2022-02-20T13:03:09.225836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\n# Number of characters in the text\nX_train_dataset[\"question_title_num_chars\"] = X_train_dataset[\"question_title\"].apply(len)\nX_train_dataset[\"question_body_num_chars\"] = X_train_dataset[\"question_body\"].apply(len)\nX_train_dataset[\"answer_num_chars\"] = X_train_dataset[\"answer\"].apply(len)\n\n# Feature engineering for validation dataset\nX_valid_dataset[\"question_title_num_chars\"] = X_valid_dataset[\"question_title\"].apply(len)\nX_valid_dataset[\"question_body_num_chars\"] = X_valid_dataset[\"question_body\"].apply(len)\nX_valid_dataset[\"answer_num_chars\"] = X_valid_dataset[\"answer\"].apply(len)\n\ntest_dataset[\"question_title_num_chars\"] = test_dataset[\"question_title\"].apply(len)\ntest_dataset[\"question_body_num_chars\"] = test_dataset[\"question_body\"].apply(len)\ntest_dataset[\"answer_num_chars\"] = test_dataset[\"answer\"].apply(len)\n\n#########################################################################################################\n# Number of words in the text\nX_train_dataset[\"question_title_num_words\"] = X_train_dataset[\"question_title\"].apply(word_count)\nX_train_dataset[\"question_body_num_words\"] = X_train_dataset[\"question_body\"].apply(word_count)\nX_train_dataset[\"answer_num_words\"] = X_train_dataset[\"answer\"].apply(word_count)\n\n# validation dataset features\nX_valid_dataset[\"question_title_num_words\"] = X_valid_dataset[\"question_title\"].apply(word_count)\nX_valid_dataset[\"question_body_num_words\"] = X_valid_dataset[\"question_body\"].apply(word_count)\nX_valid_dataset[\"answer_num_words\"] = X_valid_dataset[\"answer\"].apply(word_count)\n\ntest_dataset[\"question_title_num_words\"] = test_dataset[\"question_title\"].apply(word_count)\ntest_dataset[\"question_body_num_words\"] = test_dataset[\"question_body\"].apply(word_count)\ntest_dataset[\"answer_num_words\"] = test_dataset[\"answer\"].apply(word_count)\n\n\n#######################################################################################################\n# Number of unique words in the text\nX_train_dataset[\"question_title_num_unique_words\"] = X_train_dataset[\"question_title\"].apply(unique_word_count)\nX_train_dataset[\"question_body_num_unique_words\"] = X_train_dataset[\"question_body\"].apply(unique_word_count)\nX_train_dataset[\"answer_num_unique_words\"] = X_train_dataset[\"answer\"].apply(unique_word_count)\n\n# Validation dataset\nX_valid_dataset[\"question_title_num_unique_words\"] = X_valid_dataset[\"question_title\"].apply(unique_word_count)\nX_valid_dataset[\"question_body_num_unique_words\"] = X_valid_dataset[\"question_body\"].apply(unique_word_count)\nX_valid_dataset[\"answer_num_unique_words\"] = X_valid_dataset[\"answer\"].apply(unique_word_count)\n\ntest_dataset[\"question_title_num_unique_words\"] = test_dataset[\"question_title\"].apply(unique_word_count)\ntest_dataset[\"question_body_num_unique_words\"] = test_dataset[\"question_body\"].apply(unique_word_count)\ntest_dataset[\"answer_num_unique_words\"] = test_dataset[\"answer\"].apply(unique_word_count)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:03:09.228066Z","iopub.execute_input":"2022-02-20T13:03:09.22853Z","iopub.status.idle":"2022-02-20T13:03:09.62514Z","shell.execute_reply.started":"2022-02-20T13:03:09.228492Z","shell.execute_reply":"2022-02-20T13:03:09.624438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TF-IDF based features:\n\n* Word Level N-Gram TF-IDF of **question_title**\n* Word Level N-Gram TF-IDF of **question_body**\n* Word Level N-Gram TF-IDF of **answer**","metadata":{"editable":false}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\n\nvectorizer = TfidfVectorizer(min_df=2)\ntsvd = TruncatedSVD(n_components = 128, n_iter=5)\n\n\nqt_tfidf = vectorizer.fit_transform(X_train_dataset['preprocessed_question_title'].values)\ntfidf_question_title_train = tsvd.fit_transform(qt_tfidf)\n\n\nqb_tfidf = vectorizer.fit_transform(X_train_dataset['preprocessed_question_body'].values)\ntfidf_question_body_train = tsvd.fit_transform(qb_tfidf)\n\n\nans_tfidf = vectorizer.fit_transform(X_train_dataset['preprocessed_answer'].values)\ntfidf_answer_train = tsvd.fit_transform(ans_tfidf)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:03:09.626823Z","iopub.execute_input":"2022-02-20T13:03:09.627314Z","iopub.status.idle":"2022-02-20T13:03:14.600303Z","shell.execute_reply.started":"2022-02-20T13:03:09.627277Z","shell.execute_reply":"2022-02-20T13:03:14.599316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"qt_tfidf = vectorizer.fit_transform(X_valid_dataset['preprocessed_question_title'].values)\ntfidf_question_title_valid = tsvd.fit_transform(qt_tfidf)\n\n\nqb_tfidf = vectorizer.fit_transform(X_valid_dataset['preprocessed_question_body'].values)\ntfidf_question_body_valid = tsvd.fit_transform(qb_tfidf)\n\n\nans_tfidf = vectorizer.fit_transform(X_valid_dataset['preprocessed_answer'].values)\ntfidf_answer_valid = tsvd.fit_transform(ans_tfidf)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:03:14.605763Z","iopub.execute_input":"2022-02-20T13:03:14.608268Z","iopub.status.idle":"2022-02-20T13:03:15.585299Z","shell.execute_reply.started":"2022-02-20T13:03:14.608217Z","shell.execute_reply":"2022-02-20T13:03:15.583318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"qt_tfidf = vectorizer.fit_transform(test_dataset['preprocessed_question_title'].values)\ntfidf_question_title_test = tsvd.fit_transform(qt_tfidf)\n\n\nqb_tfidf = vectorizer.fit_transform(test_dataset['preprocessed_question_body'].values)\ntfidf_question_body_test = tsvd.fit_transform(qb_tfidf)\n\n\nans_tfidf = vectorizer.fit_transform(test_dataset['preprocessed_answer'].values)\ntfidf_answer_test = tsvd.fit_transform(ans_tfidf)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:03:15.58644Z","iopub.execute_input":"2022-02-20T13:03:15.586835Z","iopub.status.idle":"2022-02-20T13:03:16.202296Z","shell.execute_reply.started":"2022-02-20T13:03:15.586797Z","shell.execute_reply":"2022-02-20T13:03:16.201298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_dataset[\"tfidf_question_title\"] = list(tfidf_question_title_train)\nX_train_dataset[\"tfidf_question_body\"] = list(tfidf_question_body_train)\nX_train_dataset[\"tfidf_answer\"] = list(tfidf_answer_train)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:03:16.203461Z","iopub.execute_input":"2022-02-20T13:03:16.204023Z","iopub.status.idle":"2022-02-20T13:03:16.219033Z","shell.execute_reply.started":"2022-02-20T13:03:16.20398Z","shell.execute_reply":"2022-02-20T13:03:16.218281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_valid_dataset[\"tfidf_question_title\"] = list(tfidf_question_title_valid)\nX_valid_dataset[\"tfidf_question_body\"] = list(tfidf_question_body_valid)\nX_valid_dataset[\"tfidf_answer\"] = list(tfidf_answer_valid)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:03:16.224124Z","iopub.execute_input":"2022-02-20T13:03:16.226767Z","iopub.status.idle":"2022-02-20T13:03:16.237023Z","shell.execute_reply.started":"2022-02-20T13:03:16.226721Z","shell.execute_reply":"2022-02-20T13:03:16.236137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset[\"tfidf_question_title\"] = list(tfidf_question_title_test)\ntest_dataset[\"tfidf_question_body\"] = list(tfidf_question_body_test)\ntest_dataset[\"tfidf_answer\"] = list(tfidf_answer_test)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:03:16.241696Z","iopub.execute_input":"2022-02-20T13:03:16.243911Z","iopub.status.idle":"2022-02-20T13:03:16.253263Z","shell.execute_reply.started":"2022-02-20T13:03:16.243853Z","shell.execute_reply":"2022-02-20T13:03:16.25246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Features using web scraping \n\n\n## `answer_user_page` features:\n","metadata":{"editable":false}},{"cell_type":"code","source":"!pip install bs4","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:03:16.258398Z","iopub.execute_input":"2022-02-20T13:03:16.260758Z","iopub.status.idle":"2022-02-20T13:03:27.159121Z","shell.execute_reply.started":"2022-02-20T13:03:16.26071Z","shell.execute_reply":"2022-02-20T13:03:27.15835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom tqdm.notebook import tqdm\nfrom bs4 import BeautifulSoup\nfrom urllib import request\n\n\ndef get_user_rating(url):\n    try:\n        get = request.urlopen(url).read()\n        src = BeautifulSoup(get, 'html.parser')\n        #print(src)\n        reputation, gold = [], []\n        silver, bronze = [], []\n        reputation = int(''.join(src.find_all(\"div\", class_ = 'fs-body3 fc-dark')[0].text.strip().split(',')))\n        try:\n            gold = int(''.join(src.find_all('div', class_='fs-title fw-bold fc-black-800')[0].text.strip().split(',')))\n        except:\n            gold = 0\n\n        try:    \n            silver = int(''.join(src.find_all('div', class_='fs-title fw-bold fc-black-800')[1].text.strip().split(',')))\n        except:\n            silver = 0\n\n        try:\n            bronze = int(''.join(src.find_all('div', class_='fs-title fw-bold fc-black-800')[2].text.strip().split(',')))\n        except:\n            bronze = 0\n\n        output = [reputation, gold, silver, bronze]\n    except:\n        output = [0]*4\n\n    return output\n'''\ndata = []\nfor url in tqdm(X_train_dataset['answer_user_page']):\n    #print(url)\n    data.append(get_user_rating(url))\n    columns = ['reputation', 'gold', 'silver', 'bronze']  \nscraped = pd.DataFrame(data, columns=columns)\nscraped.to_csv(f'train_web_scrap_features.csv', index=False)\n\ndata = []\nfor url in tqdm(X_valid_dataset['answer_user_page']):\n    #print(url)\n    data.append(get_user_rating(url))\n    columns = ['reputation', 'gold', 'silver', 'bronze']  \nscraped = pd.DataFrame(data, columns=columns)\nscraped.to_csv(f'valid_web_scrap_features.csv', index=False)\n'''\n\ntrain_web_scraping_feature = pd.read_csv('../input/feature-engineering/train_web_scrap_features.csv')\nvalid_web_scraping_feature = pd.read_csv('../input/feature-engineering/valid_web_scrap_features.csv')","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:03:27.162713Z","iopub.execute_input":"2022-02-20T13:03:27.16295Z","iopub.status.idle":"2022-02-20T13:03:27.361716Z","shell.execute_reply.started":"2022-02-20T13:03:27.162923Z","shell.execute_reply":"2022-02-20T13:03:27.360931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_web_scraping_feature","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:03:27.363005Z","iopub.execute_input":"2022-02-20T13:03:27.363248Z","iopub.status.idle":"2022-02-20T13:03:27.376566Z","shell.execute_reply.started":"2022-02-20T13:03:27.363215Z","shell.execute_reply":"2022-02-20T13:03:27.375793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_web_scraping_feature","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:03:27.378407Z","iopub.execute_input":"2022-02-20T13:03:27.37872Z","iopub.status.idle":"2022-02-20T13:03:27.389998Z","shell.execute_reply.started":"2022-02-20T13:03:27.378682Z","shell.execute_reply":"2022-02-20T13:03:27.389302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### References for feature engineering:\n* https://www.kaggle.com/c/google-quest-challenge/discussion/130041 - meta features.\n* https://www.kaggle.com/codename007/start-from-here-quest-complete-eda-fe?scriptVersionId=25618132&cellId=65 - tfidf, count based features.\n* https://towardsdatascience.com/hands-on-transformers-kaggle-google-quest-q-a-labeling-affd3dad7bcb - web scraping features","metadata":{"editable":false}},{"cell_type":"markdown","source":"# Converting **`question_title`**  Text -> Vector","metadata":{"editable":false}},{"cell_type":"code","source":"import tensorflow as tf\n\nquestion_title_tk = tf.keras.preprocessing.text.Tokenizer(filters = \" \")\nquestion_title_tk.fit_on_texts(X_train_dataset['question_title'].values)\n\nvocab_size_question_title = len(question_title_tk.word_index) + 1","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:03:27.39146Z","iopub.execute_input":"2022-02-20T13:03:27.39181Z","iopub.status.idle":"2022-02-20T13:03:32.014409Z","shell.execute_reply.started":"2022-02-20T13:03:27.391774Z","shell.execute_reply":"2022-02-20T13:03:32.01363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Converting **`question_title`** text feature to tokens ","metadata":{"editable":false}},{"cell_type":"code","source":"tokenized_question_title_train = question_title_tk.texts_to_sequences(X_train_dataset['question_title'].values)\ntokenized_question_title_valid = question_title_tk.texts_to_sequences(X_valid_dataset['question_title'].values)\ntokenized_question_title_test = question_title_tk.texts_to_sequences(test_dataset['question_title'].values)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:03:32.015552Z","iopub.execute_input":"2022-02-20T13:03:32.015803Z","iopub.status.idle":"2022-02-20T13:03:32.079922Z","shell.execute_reply.started":"2022-02-20T13:03:32.01577Z","shell.execute_reply":"2022-02-20T13:03:32.079101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"max length in the question title feature\",max([(len(title)) for title in tokenized_question_title_train]))","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:03:32.081376Z","iopub.execute_input":"2022-02-20T13:03:32.081896Z","iopub.status.idle":"2022-02-20T13:03:32.08859Z","shell.execute_reply.started":"2022-02-20T13:03:32.081844Z","shell.execute_reply":"2022-02-20T13:03:32.087729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Padding **`question_title`** tokens to have all question_title in same lenght (i.e: 30)","metadata":{"editable":false}},{"cell_type":"code","source":"tokenized_question_title_train = tf.keras.preprocessing.sequence.pad_sequences(tokenized_question_title_train, maxlen=30, dtype='int32', padding='post')\ntokenized_question_title_valid = tf.keras.preprocessing.sequence.pad_sequences(tokenized_question_title_valid, maxlen=30, dtype='int32', padding='post')\ntokenized_question_title_test = tf.keras.preprocessing.sequence.pad_sequences(tokenized_question_title_test, maxlen=30, dtype='int32', padding='post')","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:03:32.089813Z","iopub.execute_input":"2022-02-20T13:03:32.090169Z","iopub.status.idle":"2022-02-20T13:03:32.141024Z","shell.execute_reply.started":"2022-02-20T13:03:32.090133Z","shell.execute_reply":"2022-02-20T13:03:32.140292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_question_title_train.shape, tokenized_question_title_valid.shape, tokenized_question_title_test.shape","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:03:32.142424Z","iopub.execute_input":"2022-02-20T13:03:32.142904Z","iopub.status.idle":"2022-02-20T13:03:32.148842Z","shell.execute_reply.started":"2022-02-20T13:03:32.142852Z","shell.execute_reply":"2022-02-20T13:03:32.148174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Converting **`question_body`** Feature Text -> Vector","metadata":{"editable":false}},{"cell_type":"code","source":"question_body_tk = tf.keras.preprocessing.text.Tokenizer(filters = \" \")\nquestion_body_tk.fit_on_texts(X_train_dataset['question_body'].values)\n\nvocab_size_question_body = len(question_body_tk.word_index) + 1","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:03:32.150344Z","iopub.execute_input":"2022-02-20T13:03:32.150847Z","iopub.status.idle":"2022-02-20T13:03:32.769793Z","shell.execute_reply.started":"2022-02-20T13:03:32.150809Z","shell.execute_reply":"2022-02-20T13:03:32.768996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" Converting **`question_body`** text feature to tokens ","metadata":{"editable":false}},{"cell_type":"code","source":"tokenized_question_body_train = question_body_tk.texts_to_sequences(X_train_dataset['question_body'].values)\ntokenized_question_body_valid = question_body_tk.texts_to_sequences(X_valid_dataset['question_body'].values)\ntokenized_question_body_test = question_body_tk.texts_to_sequences(test_dataset['question_body'].values)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:03:32.771251Z","iopub.execute_input":"2022-02-20T13:03:32.771492Z","iopub.status.idle":"2022-02-20T13:03:33.327634Z","shell.execute_reply.started":"2022-02-20T13:03:32.771458Z","shell.execute_reply":"2022-02-20T13:03:33.326862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"question_body_max_len = max([(len(body)) for body in tokenized_question_body_train])\nprint(\"max length in the question body feature\",question_body_max_len)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:03:33.329063Z","iopub.execute_input":"2022-02-20T13:03:33.32954Z","iopub.status.idle":"2022-02-20T13:03:33.335832Z","shell.execute_reply.started":"2022-02-20T13:03:33.329501Z","shell.execute_reply":"2022-02-20T13:03:33.335148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Padding **`question_body`** tokens to have all question_body in same lenght (i.e: 1397)","metadata":{"editable":false}},{"cell_type":"code","source":"tokenized_question_body_train = tf.keras.preprocessing.sequence.pad_sequences(tokenized_question_body_train, maxlen=question_body_max_len, dtype='int32', padding='post')\ntokenized_question_body_valid = tf.keras.preprocessing.sequence.pad_sequences(tokenized_question_body_valid, maxlen=question_body_max_len, dtype='int32', padding='post')\ntokenized_question_body_test = tf.keras.preprocessing.sequence.pad_sequences(tokenized_question_body_test, maxlen=question_body_max_len, dtype='int32', padding='post')","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:03:33.336986Z","iopub.execute_input":"2022-02-20T13:03:33.337398Z","iopub.status.idle":"2022-02-20T13:03:33.61438Z","shell.execute_reply.started":"2022-02-20T13:03:33.337363Z","shell.execute_reply":"2022-02-20T13:03:33.613556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_question_body_train.shape, tokenized_question_body_valid.shape, tokenized_question_body_test.shape","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:03:33.615789Z","iopub.execute_input":"2022-02-20T13:03:33.61633Z","iopub.status.idle":"2022-02-20T13:03:33.622227Z","shell.execute_reply.started":"2022-02-20T13:03:33.616289Z","shell.execute_reply":"2022-02-20T13:03:33.62146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Converting **`answer`** Feature Text -> Vector","metadata":{"editable":false}},{"cell_type":"code","source":"answer_tk = tf.keras.preprocessing.text.Tokenizer(filters = \" \")\nanswer_tk.fit_on_texts(X_train_dataset['answer'].values)\n\nvocab_size_answer = len(answer_tk.word_index) + 1","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:03:33.625261Z","iopub.execute_input":"2022-02-20T13:03:33.625772Z","iopub.status.idle":"2022-02-20T13:03:34.688431Z","shell.execute_reply.started":"2022-02-20T13:03:33.625736Z","shell.execute_reply":"2022-02-20T13:03:34.687656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Converting **`answer`** text feature to tokens ","metadata":{"editable":false}},{"cell_type":"code","source":"tokenized_answer_train = answer_tk.texts_to_sequences(X_train_dataset['answer'].values)\ntokenized_answer_valid = answer_tk.texts_to_sequences(X_valid_dataset['answer'].values)\ntokenized_answer_test = answer_tk.texts_to_sequences(test_dataset['answer'].values)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:03:34.689756Z","iopub.execute_input":"2022-02-20T13:03:34.690021Z","iopub.status.idle":"2022-02-20T13:03:35.169277Z","shell.execute_reply.started":"2022-02-20T13:03:34.689986Z","shell.execute_reply":"2022-02-20T13:03:35.168559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"answer_max_len = max([(len(answer)) for answer in tokenized_answer_train])\nprint(\"max length in the answer feature\",answer_max_len)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:03:35.170651Z","iopub.execute_input":"2022-02-20T13:03:35.170931Z","iopub.status.idle":"2022-02-20T13:03:35.177111Z","shell.execute_reply.started":"2022-02-20T13:03:35.170896Z","shell.execute_reply":"2022-02-20T13:03:35.176289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Padding **`answer`** tokens to have all answer in same lenght (i.e: 2332)","metadata":{"editable":false}},{"cell_type":"code","source":"tokenized_answer_train = tf.keras.preprocessing.sequence.pad_sequences(tokenized_answer_train, maxlen=answer_max_len, dtype='int32', padding='post')\ntokenized_answer_valid = tf.keras.preprocessing.sequence.pad_sequences(tokenized_answer_valid, maxlen=answer_max_len, dtype='int32', padding='post')\ntokenized_answer_test = tf.keras.preprocessing.sequence.pad_sequences(tokenized_answer_test, maxlen=answer_max_len, dtype='int32', padding='post')","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:03:35.17842Z","iopub.execute_input":"2022-02-20T13:03:35.178839Z","iopub.status.idle":"2022-02-20T13:03:35.392163Z","shell.execute_reply.started":"2022-02-20T13:03:35.178802Z","shell.execute_reply":"2022-02-20T13:03:35.391388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_question_title_train.shape, tokenized_question_body_train.shape, tokenized_answer_train.shape","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:03:35.39364Z","iopub.execute_input":"2022-02-20T13:03:35.393927Z","iopub.status.idle":"2022-02-20T13:03:35.401553Z","shell.execute_reply.started":"2022-02-20T13:03:35.393888Z","shell.execute_reply":"2022-02-20T13:03:35.400893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_question_title_valid.shape, tokenized_question_body_valid.shape, tokenized_answer_valid.shape","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:03:35.402772Z","iopub.execute_input":"2022-02-20T13:03:35.403615Z","iopub.status.idle":"2022-02-20T13:03:35.411246Z","shell.execute_reply.started":"2022-02-20T13:03:35.403579Z","shell.execute_reply":"2022-02-20T13:03:35.410405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_question_title_test.shape, tokenized_question_body_test.shape, tokenized_answer_test.shape","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:03:35.412707Z","iopub.execute_input":"2022-02-20T13:03:35.413052Z","iopub.status.idle":"2022-02-20T13:03:35.421386Z","shell.execute_reply.started":"2022-02-20T13:03:35.412958Z","shell.execute_reply":"2022-02-20T13:03:35.420551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train_dataset.shape","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:03:35.422718Z","iopub.execute_input":"2022-02-20T13:03:35.42312Z","iopub.status.idle":"2022-02-20T13:03:35.429747Z","shell.execute_reply.started":"2022-02-20T13:03:35.423076Z","shell.execute_reply":"2022-02-20T13:03:35.428849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_valid_dataset.shape","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:03:35.431317Z","iopub.execute_input":"2022-02-20T13:03:35.431575Z","iopub.status.idle":"2022-02-20T13:03:35.438481Z","shell.execute_reply.started":"2022-02-20T13:03:35.431542Z","shell.execute_reply":"2022-02-20T13:03:35.437635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Embedding weights for **question_title**","metadata":{"editable":false}},{"cell_type":"code","source":"import numpy as np\nfrom tqdm.notebook import tqdm\n\nglove_file = open('../input/glove-embeddings/glove.6B.300d.txt', encoding='utf8')\n\nembeddings_index = dict()   \nfor line in tqdm(glove_file):\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nglove_file.close()\nprint('Loaded %s word vectors.' % len(embeddings_index))","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:03:35.440134Z","iopub.execute_input":"2022-02-20T13:03:35.440475Z","iopub.status.idle":"2022-02-20T13:04:22.62554Z","shell.execute_reply.started":"2022-02-20T13:03:35.440433Z","shell.execute_reply":"2022-02-20T13:04:22.624813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create a weight matrix for words in training docs\n\nembedding_matrix_question_title = np.zeros((vocab_size_question_title, 300))\nfor word, i in question_title_tk.word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix_question_title[i] = embedding_vector","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:04:22.626905Z","iopub.execute_input":"2022-02-20T13:04:22.627513Z","iopub.status.idle":"2022-02-20T13:04:22.657463Z","shell.execute_reply.started":"2022-02-20T13:04:22.627475Z","shell.execute_reply":"2022-02-20T13:04:22.656718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_matrix_question_title.shape","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:04:22.660549Z","iopub.execute_input":"2022-02-20T13:04:22.660992Z","iopub.status.idle":"2022-02-20T13:04:22.665971Z","shell.execute_reply.started":"2022-02-20T13:04:22.660961Z","shell.execute_reply":"2022-02-20T13:04:22.665246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Embedding weights for **question_body**","metadata":{"editable":false}},{"cell_type":"code","source":"# create a weight matrix for words in training docs\n\nembedding_matrix_question_body = np.zeros((vocab_size_question_body, 300))\nfor word, i in question_body_tk.word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix_question_body[i] = embedding_vector","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:04:22.667661Z","iopub.execute_input":"2022-02-20T13:04:22.668197Z","iopub.status.idle":"2022-02-20T13:04:22.764679Z","shell.execute_reply.started":"2022-02-20T13:04:22.668159Z","shell.execute_reply":"2022-02-20T13:04:22.763923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_matrix_question_body.shape","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:04:22.766524Z","iopub.execute_input":"2022-02-20T13:04:22.767058Z","iopub.status.idle":"2022-02-20T13:04:22.772662Z","shell.execute_reply.started":"2022-02-20T13:04:22.767013Z","shell.execute_reply":"2022-02-20T13:04:22.771931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Embedding weights for **answer**","metadata":{"editable":false}},{"cell_type":"code","source":"# create a weight matrix for words in training docs\n\nembedding_matrix_answer = np.zeros((vocab_size_answer, 300))\nfor word, i in question_body_tk.word_index.items():\n    \n    \n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix_answer[i] = embedding_vector","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:04:22.774236Z","iopub.execute_input":"2022-02-20T13:04:22.774528Z","iopub.status.idle":"2022-02-20T13:04:22.870949Z","shell.execute_reply.started":"2022-02-20T13:04:22.774493Z","shell.execute_reply":"2022-02-20T13:04:22.870177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_matrix_answer.shape","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:04:22.872991Z","iopub.execute_input":"2022-02-20T13:04:22.873479Z","iopub.status.idle":"2022-02-20T13:04:22.878989Z","shell.execute_reply.started":"2022-02-20T13:04:22.873441Z","shell.execute_reply":"2022-02-20T13:04:22.87801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PLot the history of the model\n\n","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:04:22.890366Z","iopub.execute_input":"2022-02-20T13:04:22.890706Z","iopub.status.idle":"2022-02-20T13:04:22.894068Z","shell.execute_reply.started":"2022-02-20T13:04:22.890677Z","shell.execute_reply":"2022-02-20T13:04:22.89336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Building Base LSTM Model ","metadata":{"editable":false}},{"cell_type":"code","source":"############################ INPUT 1 - Question title ############################################################\ninput_1 = tf.keras.layers.Input(shape=(30,))\nembed_1 = tf.keras.layers.Embedding(vocab_size_question_title, \n                                    300, \n                                    weights=[embedding_matrix_question_title],\n                                    input_length=30,\n                                    embeddings_initializer=tf.keras.initializers.RandomNormal(mean=0, stddev=1, seed=23), \n                                    trainable=False)(input_1)\nlstm1 = tf.keras.layers.Bidirectional(\n    tf.keras.layers.LSTM(24,activation='tanh',kernel_initializer=tf.keras.initializers.glorot_uniform(seed=26),\n          return_sequences=True))(embed_1)\nprint(\"lstm_1 :\",lstm1.shape)\nflat_1 = tf.keras.layers.Flatten()(lstm1)\n\n\n############################ INPUT 2 - Question body ##############################################################\ninput_2 = tf.keras.layers.Input(shape=(question_body_max_len,))\nembed_2 = tf.keras.layers.Embedding(vocab_size_question_body,\n                    300, \n                    weights=[embedding_matrix_question_body], \n                    input_length=question_body_max_len, \n                    embeddings_initializer=tf.keras.initializers.RandomNormal(mean=0, stddev=1, seed=24), \n                    trainable=False)(input_2)\nlstm2 = tf.keras.layers.Bidirectional(\n    tf.keras.layers.LSTM(24,activation='tanh',kernel_initializer=tf.keras.initializers.glorot_uniform(seed=27),\n          return_sequences=True))(embed_2)\nprint(\"lstm_2 :\",lstm2.shape)\nflat_2 = tf.keras.layers.Flatten()(lstm2)\n\n\n############################ INPUT 3 - Answer ################################################################\ninput_3 = tf.keras.layers.Input(shape=(answer_max_len,))\nembed_3 = tf.keras.layers.Embedding(vocab_size_answer, 300, weights=[embedding_matrix_answer], \n                    input_length=answer_max_len, \n                    embeddings_initializer=tf.keras.initializers.RandomNormal(mean=0, stddev=1, seed=25), \n                    trainable=False)(input_3)\nlstm3 = tf.keras.layers.Bidirectional(\n    tf.keras.layers.LSTM(24,activation='tanh',kernel_initializer=tf.keras.initializers.glorot_uniform(seed=28),\n          return_sequences=True))(embed_3)\nprint(\"lstm_3 :\",lstm3.shape)\nflat_3 = tf.keras.layers.Flatten()(lstm3)\n\n\n\n\nprint(\"Flat_1 shape :\",flat_1.shape)\nprint(\"Flat_2 shape :\",flat_2.shape)\nprint(\"Flat_3 shape :\",flat_3.shape)\n\nconcat = tf.keras.layers.Concatenate()([flat_1,flat_2,flat_3])\nprint(concat.shape)\ndense = tf.keras.layers.Dense(32,activation = 'relu',kernel_initializer=tf.keras.initializers.he_normal(seed=40))(concat)\n\noutput = tf.keras.layers.Dense(30,activation = 'sigmoid',kernel_initializer=tf.keras.initializers.glorot_uniform(seed=45))(dense)\n\nmodel = tf.keras.Model(inputs = [input_1, input_2, input_3], outputs = output)\n\nmodel.summary()","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:04:22.895505Z","iopub.execute_input":"2022-02-20T13:04:22.895999Z","iopub.status.idle":"2022-02-20T13:04:26.834776Z","shell.execute_reply.started":"2022-02-20T13:04:22.895961Z","shell.execute_reply":"2022-02-20T13:04:26.834046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.utils.plot_model(model)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:04:26.836945Z","iopub.execute_input":"2022-02-20T13:04:26.837442Z","iopub.status.idle":"2022-02-20T13:04:27.60495Z","shell.execute_reply.started":"2022-02-20T13:04:26.837398Z","shell.execute_reply":"2022-02-20T13:04:27.604147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.stats import spearmanr\n\nlist_1 = [1,2,3,4,5]\nlist_2 = [2,3,4,5,6]\nspearmanr(list_1, list_2)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:04:27.606789Z","iopub.execute_input":"2022-02-20T13:04:27.607091Z","iopub.status.idle":"2022-02-20T13:04:27.619289Z","shell.execute_reply.started":"2022-02-20T13:04:27.607052Z","shell.execute_reply":"2022-02-20T13:04:27.618313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.stats import spearmanr\n\nclass SpearmanCallback(tf.keras.callbacks.Callback):\n    def __init__(self, validation_data):\n        self.x_val = validation_data[0]\n        self.y_val = validation_data[1]\n\n    def on_epoch_end(self, epoch, logs={}):\n        y_pred_val = self.model.predict(self.x_val)\n        rho_val = np.mean([spearmanr(self.y_val[:, ind], y_pred_val[:, ind] + np.random.normal(0, 1e-7, y_pred_val.shape[0])).correlation for ind in range(y_pred_val.shape[1])])\n        print(rho_val)\n        print('\\nval_spearman-corr: %s' % (str(round(rho_val, 6))), end=100*' '+'\\n')\n        return rho_val","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:04:27.620814Z","iopub.execute_input":"2022-02-20T13:04:27.621251Z","iopub.status.idle":"2022-02-20T13:04:27.630044Z","shell.execute_reply.started":"2022-02-20T13:04:27.621211Z","shell.execute_reply":"2022-02-20T13:04:27.629084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n                   optimizer=tf.keras.optimizers.Adam()\n            )","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:04:27.63128Z","iopub.execute_input":"2022-02-20T13:04:27.632044Z","iopub.status.idle":"2022-02-20T13:04:27.652357Z","shell.execute_reply.started":"2022-02-20T13:04:27.632007Z","shell.execute_reply":"2022-02-20T13:04:27.651546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_valid_dataset = np.array(y_valid_dataset)\ny_train_dataset = np.array(y_train_dataset)\ncustom_callback = SpearmanCallback(validation_data=([tokenized_question_title_valid, tokenized_question_body_valid, tokenized_answer_valid ], y_valid_dataset))","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:04:27.654872Z","iopub.execute_input":"2022-02-20T13:04:27.655409Z","iopub.status.idle":"2022-02-20T13:04:27.661917Z","shell.execute_reply.started":"2022-02-20T13:04:27.655373Z","shell.execute_reply":"2022-02-20T13:04:27.661166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#history = model.fit([tokenized_question_title_train, tokenized_question_body_train, tokenized_answer_train], y_train_dataset, \n#           epochs=10,  \n#           validation_data=([tokenized_question_title_valid, tokenized_question_body_valid, tokenized_answer_valid ], y_valid_dataset), \n#           callbacks=[custom_callback])","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:04:27.663458Z","iopub.execute_input":"2022-02-20T13:04:27.663933Z","iopub.status.idle":"2022-02-20T13:04:27.671109Z","shell.execute_reply.started":"2022-02-20T13:04:27.663895Z","shell.execute_reply":"2022-02-20T13:04:27.670377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#pd.DataFrame(history.history).plot()\n#plt.title(\"Base model with 3 features\")","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:04:27.672761Z","iopub.execute_input":"2022-02-20T13:04:27.672976Z","iopub.status.idle":"2022-02-20T13:04:27.679465Z","shell.execute_reply.started":"2022-02-20T13:04:27.67295Z","shell.execute_reply":"2022-02-20T13:04:27.678764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Observation:** \nAfter 11 epochs we have acheived an spearman score of **0.27447** with three question_title, question_body, answer features ","metadata":{"editable":false}},{"cell_type":"markdown","source":"# Base LSTM model applying 18 Feature Engineering features\n\n* **3 text features** - question_title, question_body, answer\n* **9 Feature engineering features (9 dim)** -  question_title_num_chars, question_body_num_chars, answer_num_chars,  question_title_num_words, question_body_num_words, answer_num_words, question_title_num_unique_words, question_body_num_unique_words, answer_num_unique_words\n* **3 TF-IDF features (384 dim)** - TF-IDF quesion_title, TF_IDF quesiton_body, TF-IDF answer.\n* **4 Web scraping features (4 dim)** - reputation, \tgold, \tsilver, \tbronze.","metadata":{"editable":false}},{"cell_type":"code","source":"feature_engineer_columns = ['question_title_num_chars', 'question_body_num_chars',\n       'answer_num_chars', 'question_title_num_words',\n       'question_body_num_words', 'answer_num_words',\n       'question_title_num_unique_words', 'question_body_num_unique_words',\n       'answer_num_unique_words']\n\n\ntfidf_features = ['tfidf_question_title','tfidf_question_body', 'tfidf_answer']\n\n\ntrain_web_scraping_feature.shape, valid_web_scraping_feature.shape","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:04:27.681105Z","iopub.execute_input":"2022-02-20T13:04:27.681321Z","iopub.status.idle":"2022-02-20T13:04:27.691649Z","shell.execute_reply.started":"2022-02-20T13:04:27.681287Z","shell.execute_reply":"2022-02-20T13:04:27.690905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_feature_eng = np.array(X_train_dataset[feature_engineer_columns])\nX_valid_feature_eng = np.array(X_valid_dataset[feature_engineer_columns])","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:04:27.693197Z","iopub.execute_input":"2022-02-20T13:04:27.693458Z","iopub.status.idle":"2022-02-20T13:04:27.715401Z","shell.execute_reply.started":"2022-02-20T13:04:27.693424Z","shell.execute_reply":"2022-02-20T13:04:27.714675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_feature_eng.shape","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:04:27.717277Z","iopub.execute_input":"2022-02-20T13:04:27.717472Z","iopub.status.idle":"2022-02-20T13:04:27.723805Z","shell.execute_reply.started":"2022-02-20T13:04:27.717449Z","shell.execute_reply":"2022-02-20T13:04:27.723112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_valid_feature_eng.shape","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:04:27.725179Z","iopub.execute_input":"2022-02-20T13:04:27.726062Z","iopub.status.idle":"2022-02-20T13:04:27.733548Z","shell.execute_reply.started":"2022-02-20T13:04:27.726011Z","shell.execute_reply":"2022-02-20T13:04:27.732683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###  TF-IDF Features","metadata":{"editable":false}},{"cell_type":"code","source":"tfidf_question_title_train.shape, tfidf_question_body_train.shape, tfidf_answer_train.shape","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:04:27.735016Z","iopub.execute_input":"2022-02-20T13:04:27.735906Z","iopub.status.idle":"2022-02-20T13:04:27.743017Z","shell.execute_reply.started":"2022-02-20T13:04:27.735853Z","shell.execute_reply":"2022-02-20T13:04:27.742125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfidf_question_title_valid.shape, tfidf_question_body_valid.shape, tfidf_answer_valid.shape","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:04:27.744544Z","iopub.execute_input":"2022-02-20T13:04:27.745401Z","iopub.status.idle":"2022-02-20T13:04:27.751811Z","shell.execute_reply.started":"2022-02-20T13:04:27.745363Z","shell.execute_reply":"2022-02-20T13:04:27.750939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfidf_train_features = np.hstack((tfidf_question_title_train, tfidf_question_body_train, tfidf_answer_train))\ntfidf_valid_features = np.hstack((tfidf_question_title_valid, tfidf_question_body_valid, tfidf_answer_valid))","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:04:27.753657Z","iopub.execute_input":"2022-02-20T13:04:27.754446Z","iopub.status.idle":"2022-02-20T13:04:27.76846Z","shell.execute_reply.started":"2022-02-20T13:04:27.75441Z","shell.execute_reply":"2022-02-20T13:04:27.76772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfidf_train_features.shape, tfidf_valid_features.shape","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:04:27.769568Z","iopub.execute_input":"2022-02-20T13:04:27.771028Z","iopub.status.idle":"2022-02-20T13:04:27.776946Z","shell.execute_reply.started":"2022-02-20T13:04:27.770987Z","shell.execute_reply":"2022-02-20T13:04:27.776069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"############################ INPUT 1 - Question title ############################################################\ninput_1 = tf.keras.layers.Input(shape=(30,))\nembed_1 = tf.keras.layers.Embedding(vocab_size_question_title, \n                                    300, \n                                    weights=[embedding_matrix_question_title],\n                                    input_length=30,\n                                    embeddings_initializer=tf.keras.initializers.RandomNormal(mean=0, stddev=1, seed=23), \n                                    trainable=False)(input_1)\nlstm1 = tf.keras.layers.Bidirectional(\n    tf.keras.layers.LSTM(24,activation='tanh',kernel_initializer=tf.keras.initializers.glorot_uniform(seed=26),\n          return_sequences=True))(embed_1)\nprint(\"lstm_1 :\",lstm1.shape)\nflat_1 = tf.keras.layers.Flatten()(lstm1)\n\n\n############################ INPUT 2 - Question body ##############################################################\ninput_2 = tf.keras.layers.Input(shape=(question_body_max_len,))\nembed_2 = tf.keras.layers.Embedding(vocab_size_question_body,\n                    300, \n                    weights=[embedding_matrix_question_body], \n                    input_length=question_body_max_len, \n                    embeddings_initializer=tf.keras.initializers.RandomNormal(mean=0, stddev=1, seed=24), \n                    trainable=False)(input_2)\nlstm2 = tf.keras.layers.Bidirectional(\n    tf.keras.layers.LSTM(24,activation='tanh',kernel_initializer=tf.keras.initializers.glorot_uniform(seed=27),\n          return_sequences=True))(embed_2)\nprint(\"lstm_2 :\",lstm2.shape)\nflat_2 = tf.keras.layers.Flatten()(lstm2)\n\n\n############################ INPUT 3 - Answer ################################################################\ninput_3 = tf.keras.layers.Input(shape=(answer_max_len,))\nembed_3 = tf.keras.layers.Embedding(vocab_size_answer, 300, weights=[embedding_matrix_answer], \n                    input_length=answer_max_len, \n                    embeddings_initializer=tf.keras.initializers.RandomNormal(mean=0, stddev=1, seed=25), \n                    trainable=False)(input_3)\nlstm3 = tf.keras.layers.Bidirectional(\n    tf.keras.layers.LSTM(24,activation='tanh',kernel_initializer=tf.keras.initializers.glorot_uniform(seed=28),\n          return_sequences=True))(embed_3)\nprint(\"lstm_3 :\",lstm3.shape)\nflat_3 = tf.keras.layers.Flatten()(lstm3)\n\n#########################  INPUT 4 - Feature Engineering features ############################################\ninput_4 = tf.keras.layers.Input(shape=(9,))\nx = tf.keras.layers.Dense(64, activation='relu')(input_4)\nflat_4 = tf.keras.layers.Dense(32, activation='relu')(x)\n\n\n####################### INPUT 5 - TF-IDF Features ###########################################################\ninput_5 = tf.keras.layers.Input(shape=(384,))\nx = tf.keras.layers.Dense(128, activation='relu')(input_5)\nx = tf.keras.layers.Dense(64, activation='relu')(x)\nflat_5 = tf.keras.layers.Dense(32, activation='relu')(x)\n\n\n##################### Input 6 - Web scraping feature ##########################################################\ninput_6 = tf.keras.layers.Input(shape=(4,))\nx = tf.keras.layers.Dense(128, activation='relu')(input_6)\nx = tf.keras.layers.Dense(64, activation='relu')(x)\nflat_6 = tf.keras.layers.Dense(32, activation='relu')(x)\n\n\n\n\n\n\nprint(\"Flat_1 shape :\",flat_1.shape)\nprint(\"Flat_2 shape :\",flat_2.shape)\nprint(\"Flat_3 shape :\",flat_3.shape)\nprint(\"Flat_4 shape :\",flat_4.shape)\nprint(\"Flat_5 shape :\",flat_5.shape)\n\n\nconcat = tf.keras.layers.Concatenate()([flat_1,flat_2,flat_3, flat_4, flat_5, flat_6])\nprint(concat.shape)\ndense = tf.keras.layers.Dense(32,activation = 'relu',kernel_initializer=tf.keras.initializers.he_normal(seed=40))(concat)\n\noutput = tf.keras.layers.Dense(30,activation = 'sigmoid',kernel_initializer=tf.keras.initializers.glorot_uniform(seed=45))(dense)\n\nmodel = tf.keras.Model(inputs = [input_1, input_2, input_3, input_4, input_5, input_6], outputs = output)\n\nmodel.summary()","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:04:27.778754Z","iopub.execute_input":"2022-02-20T13:04:27.779282Z","iopub.status.idle":"2022-02-20T13:04:29.747881Z","shell.execute_reply.started":"2022-02-20T13:04:27.779244Z","shell.execute_reply":"2022-02-20T13:04:29.747136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.utils.plot_model(model)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:04:29.749143Z","iopub.execute_input":"2022-02-20T13:04:29.749392Z","iopub.status.idle":"2022-02-20T13:04:29.96657Z","shell.execute_reply.started":"2022-02-20T13:04:29.749357Z","shell.execute_reply":"2022-02-20T13:04:29.965792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.stats import spearmanr\n\nclass SpearmanCallback(tf.keras.callbacks.Callback):\n    def __init__(self, validation_data):\n        self.x_val = validation_data[0]\n        self.y_val = validation_data[1]\n\n    def on_epoch_end(self, epoch, logs={}):\n        y_pred_val = self.model.predict(self.x_val)\n        rho_val = np.mean([spearmanr(self.y_val[:, ind], y_pred_val[:, ind] + np.random.normal(0, 1e-7, y_pred_val.shape[0])).correlation for ind in range(y_pred_val.shape[1])])\n        print('\\nval_spearman-corr: %s' % (str(round(rho_val, 6))), end=100*' '+'\\n')\n        return rho_val","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:04:29.968096Z","iopub.execute_input":"2022-02-20T13:04:29.968887Z","iopub.status.idle":"2022-02-20T13:04:29.978148Z","shell.execute_reply.started":"2022-02-20T13:04:29.968828Z","shell.execute_reply":"2022-02-20T13:04:29.977424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n                   optimizer=tf.keras.optimizers.Adam()\n            )","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:04:29.979571Z","iopub.execute_input":"2022-02-20T13:04:29.979975Z","iopub.status.idle":"2022-02-20T13:04:29.995471Z","shell.execute_reply.started":"2022-02-20T13:04:29.979935Z","shell.execute_reply":"2022-02-20T13:04:29.994638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_valid_dataset = np.array(y_valid_dataset)\ncustom_callback = SpearmanCallback(validation_data=([tokenized_question_title_valid, tokenized_question_body_valid, tokenized_answer_valid, X_valid_feature_eng, tfidf_valid_features, valid_web_scraping_feature ], y_valid_dataset))","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:04:29.998211Z","iopub.execute_input":"2022-02-20T13:04:29.998559Z","iopub.status.idle":"2022-02-20T13:04:30.003518Z","shell.execute_reply.started":"2022-02-20T13:04:29.998529Z","shell.execute_reply":"2022-02-20T13:04:30.002761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#history = model.fit([tokenized_question_title_train, tokenized_question_body_train, tokenized_answer_train, X_train_feature_eng, tfidf_train_features, train_web_scraping_feature], y_train_dataset, \n#           epochs=10,  \n#           validation_data=([tokenized_question_title_valid, tokenized_question_body_valid, tokenized_answer_valid, X_valid_feature_eng, tfidf_valid_features, valid_web_scraping_feature], y_valid_dataset), \n#           callbacks=[custom_callback])","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:04:30.005149Z","iopub.execute_input":"2022-02-20T13:04:30.005478Z","iopub.status.idle":"2022-02-20T13:04:30.011212Z","shell.execute_reply.started":"2022-02-20T13:04:30.005444Z","shell.execute_reply":"2022-02-20T13:04:30.010385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Observation:** \nIn the above model we have trained with all 18 feature engineering features but by using all the features we have observed that performance has decreased a lot comparing to basic three features","metadata":{"editable":false}},{"cell_type":"code","source":"#pd.DataFrame(history.history).plot()\n#plt.title(\"Base Model with 18 FE features\")","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:04:30.01229Z","iopub.execute_input":"2022-02-20T13:04:30.012537Z","iopub.status.idle":"2022-02-20T13:04:30.01984Z","shell.execute_reply.started":"2022-02-20T13:04:30.012503Z","shell.execute_reply":"2022-02-20T13:04:30.018859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Base LSTM model applying 13 Feature Engineering features\n\n> Removing TF-IDF features as in the above model the performance has decreased comparing to the base model with only three features as tf-idf has more dimension \n\n\n* **3 text features** - question_title, question_body, answer\n* **9 Feature engineering features (9 dim)** -  question_title_num_chars, question_body_num_chars, answer_num_chars,  question_title_num_words, question_body_num_words, answer_num_words, question_title_num_unique_words, question_body_num_unique_words, answer_num_unique_words\n* **4 Web scraping features (4 dim)** - reputation, \tgold, \tsilver, \tbronze.","metadata":{"editable":false}},{"cell_type":"code","source":"############################ INPUT 1 - Question title ############################################################\ninput_1 = tf.keras.layers.Input(shape=(30,))\nembed_1 = tf.keras.layers.Embedding(vocab_size_question_title, \n                                    300, \n                                    weights=[embedding_matrix_question_title],\n                                    input_length=30,\n                                    embeddings_initializer=tf.keras.initializers.RandomNormal(mean=0, stddev=1, seed=23), \n                                    trainable=False)(input_1)\nlstm1 = tf.keras.layers.Bidirectional(\n    tf.keras.layers.LSTM(24,activation='tanh',kernel_initializer=tf.keras.initializers.glorot_uniform(seed=26),\n          return_sequences=True))(embed_1)\nprint(\"lstm_1 :\",lstm1.shape)\nflat_1 = tf.keras.layers.Flatten()(lstm1)\n\n\n############################ INPUT 2 - Question body ##############################################################\ninput_2 = tf.keras.layers.Input(shape=(question_body_max_len,))\nembed_2 = tf.keras.layers.Embedding(vocab_size_question_body,\n                    300, \n                    weights=[embedding_matrix_question_body], \n                    input_length=question_body_max_len, \n                    embeddings_initializer=tf.keras.initializers.RandomNormal(mean=0, stddev=1, seed=24), \n                    trainable=False)(input_2)\nlstm2 = tf.keras.layers.Bidirectional(\n    tf.keras.layers.LSTM(24,activation='tanh',kernel_initializer=tf.keras.initializers.glorot_uniform(seed=27),\n          return_sequences=True))(embed_2)\nprint(\"lstm_2 :\",lstm2.shape)\nflat_2 = tf.keras.layers.Flatten()(lstm2)\n\n\n############################ INPUT 3 - Answer ################################################################\ninput_3 = tf.keras.layers.Input(shape=(answer_max_len,))\nembed_3 = tf.keras.layers.Embedding(vocab_size_answer, 300, weights=[embedding_matrix_answer], \n                    input_length=answer_max_len, \n                    embeddings_initializer=tf.keras.initializers.RandomNormal(mean=0, stddev=1, seed=25), \n                    trainable=False)(input_3)\nlstm3 = tf.keras.layers.Bidirectional(\n    tf.keras.layers.LSTM(24,activation='tanh',kernel_initializer=tf.keras.initializers.glorot_uniform(seed=28),\n          return_sequences=True))(embed_3)\nprint(\"lstm_3 :\",lstm3.shape)\nflat_3 = tf.keras.layers.Flatten()(lstm3)\n\n#########################  INPUT 4 - Feature Engineering features ############################################\ninput_4 = tf.keras.layers.Input(shape=(9,))\nx = tf.keras.layers.Dense(64, activation='relu')(input_4)\nflat_4 = tf.keras.layers.Dense(32, activation='relu')(x)\n\n\n##################### Input 6 - Web scraping feature ##########################################################\ninput_5 = tf.keras.layers.Input(shape=(4,))\nx = tf.keras.layers.Dense(128, activation='relu')(input_5)\nx = tf.keras.layers.Dense(64, activation='relu')(x)\nflat_5 = tf.keras.layers.Dense(32, activation='relu')(x)\n\nprint(\"Flat_1 shape :\",flat_1.shape)\nprint(\"Flat_2 shape :\",flat_2.shape)\nprint(\"Flat_3 shape :\",flat_3.shape)\nprint(\"Flat_4 shape :\",flat_4.shape)\nprint(\"Flat_5 shape :\",flat_5.shape)\n\n\nconcat = tf.keras.layers.Concatenate()([flat_1,flat_2,flat_3, flat_4, flat_5])\nprint(concat.shape)\ndense = tf.keras.layers.Dense(32,activation = 'relu',kernel_initializer=tf.keras.initializers.he_normal(seed=40))(concat)\n\noutput = tf.keras.layers.Dense(30,activation = 'sigmoid',kernel_initializer=tf.keras.initializers.glorot_uniform(seed=45))(dense)\n\nmodel = tf.keras.Model(inputs = [input_1, input_2, input_3, input_4, input_5], outputs = output)\n\nmodel.summary()","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:04:30.021607Z","iopub.execute_input":"2022-02-20T13:04:30.02223Z","iopub.status.idle":"2022-02-20T13:04:31.605833Z","shell.execute_reply.started":"2022-02-20T13:04:30.022126Z","shell.execute_reply":"2022-02-20T13:04:31.605067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.utils.plot_model(model)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:04:31.607304Z","iopub.execute_input":"2022-02-20T13:04:31.60754Z","iopub.status.idle":"2022-02-20T13:04:31.843858Z","shell.execute_reply.started":"2022-02-20T13:04:31.607505Z","shell.execute_reply":"2022-02-20T13:04:31.84291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n                   optimizer=tf.keras.optimizers.Adam()\n            )","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:04:31.845757Z","iopub.execute_input":"2022-02-20T13:04:31.846066Z","iopub.status.idle":"2022-02-20T13:04:31.858408Z","shell.execute_reply.started":"2022-02-20T13:04:31.84602Z","shell.execute_reply":"2022-02-20T13:04:31.857724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_valid_dataset = np.array(y_valid_dataset)\ncustom_callback = SpearmanCallback(validation_data=([tokenized_question_title_valid, tokenized_question_body_valid, tokenized_answer_valid, X_valid_feature_eng, valid_web_scraping_feature ], y_valid_dataset))","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:04:31.859804Z","iopub.execute_input":"2022-02-20T13:04:31.860213Z","iopub.status.idle":"2022-02-20T13:04:31.864748Z","shell.execute_reply.started":"2022-02-20T13:04:31.860177Z","shell.execute_reply":"2022-02-20T13:04:31.864103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#history = model.fit([tokenized_question_title_train, tokenized_question_body_train, tokenized_answer_train, X_train_feature_eng, train_web_scraping_feature], y_train_dataset, \n#           epochs=10,  \n#           validation_data=([tokenized_question_title_valid, tokenized_question_body_valid, tokenized_answer_valid, X_valid_feature_eng, valid_web_scraping_feature], y_valid_dataset), \n#           callbacks=[custom_callback])","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:04:31.866132Z","iopub.execute_input":"2022-02-20T13:04:31.866653Z","iopub.status.idle":"2022-02-20T13:04:31.872649Z","shell.execute_reply.started":"2022-02-20T13:04:31.866496Z","shell.execute_reply":"2022-02-20T13:04:31.871758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#pd.DataFrame(history.history).plot()\n#plt.title(\"Base model with 13 FE features\")","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:04:31.874447Z","iopub.execute_input":"2022-02-20T13:04:31.875294Z","iopub.status.idle":"2022-02-20T13:04:31.881553Z","shell.execute_reply.started":"2022-02-20T13:04:31.875257Z","shell.execute_reply":"2022-02-20T13:04:31.880856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Observation:**\nThe model still not performed better comparing to the model with three basic text feature even after removing the **TF-IDF** features","metadata":{"editable":false}},{"cell_type":"markdown","source":"# Base LSTM model applying 9 Feature Engineering features\n\n> Removing **TF-IDF features** as in the above model the performance has decreased comparing to the base model with only three features as tf-idf has more dimension.\n\n> Removing **Web scraping features** \n\n\n* **3 text features** - question_title, question_body, answer\n* **9 Feature engineering features (9 dim)** -  question_title_num_chars, question_body_num_chars, answer_num_chars,  question_title_num_words, question_body_num_words, answer_num_words, question_title_num_unique_words, question_body_num_unique_words, answer_num_unique_words\n","metadata":{"editable":false}},{"cell_type":"code","source":"############################ INPUT 1 - Question title ############################################################\ninput_1 = tf.keras.layers.Input(shape=(30,))\nembed_1 = tf.keras.layers.Embedding(vocab_size_question_title, \n                                    300, \n                                    weights=[embedding_matrix_question_title],\n                                    input_length=30,\n                                    embeddings_initializer=tf.keras.initializers.RandomNormal(mean=0, stddev=1, seed=23), \n                                    trainable=False)(input_1)\nlstm1 = tf.keras.layers.Bidirectional(\n    tf.keras.layers.LSTM(24,activation='tanh',kernel_initializer=tf.keras.initializers.glorot_uniform(seed=26),\n          return_sequences=True))(embed_1)\nprint(\"lstm_1 :\",lstm1.shape)\nflat_1 = tf.keras.layers.Flatten()(lstm1)\n\n\n############################ INPUT 2 - Question body ##############################################################\ninput_2 = tf.keras.layers.Input(shape=(question_body_max_len,))\nembed_2 = tf.keras.layers.Embedding(vocab_size_question_body,\n                    300, \n                    weights=[embedding_matrix_question_body], \n                    input_length=question_body_max_len, \n                    embeddings_initializer=tf.keras.initializers.RandomNormal(mean=0, stddev=1, seed=24), \n                    trainable=False)(input_2)\nlstm2 = tf.keras.layers.Bidirectional(\n    tf.keras.layers.LSTM(24,activation='tanh',kernel_initializer=tf.keras.initializers.glorot_uniform(seed=27),\n          return_sequences=True))(embed_2)\nprint(\"lstm_2 :\",lstm2.shape)\nflat_2 = tf.keras.layers.Flatten()(lstm2)\n\n\n############################ INPUT 3 - Answer ################################################################\ninput_3 = tf.keras.layers.Input(shape=(answer_max_len,))\nembed_3 = tf.keras.layers.Embedding(vocab_size_answer, 300, weights=[embedding_matrix_answer], \n                    input_length=answer_max_len, \n                    embeddings_initializer=tf.keras.initializers.RandomNormal(mean=0, stddev=1, seed=25), \n                    trainable=False)(input_3)\nlstm3 = tf.keras.layers.Bidirectional(\n    tf.keras.layers.LSTM(24,activation='tanh',kernel_initializer=tf.keras.initializers.glorot_uniform(seed=28),\n          return_sequences=True))(embed_3)\nprint(\"lstm_3 :\",lstm3.shape)\nflat_3 = tf.keras.layers.Flatten()(lstm3)\n\n#########################  INPUT 4 - Feature Engineering features ############################################\ninput_4 = tf.keras.layers.Input(shape=(9,))\nx = tf.keras.layers.Dense(64, activation='relu')(input_4)\nflat_4 = tf.keras.layers.Dense(32, activation='relu')(x)\n\n\n\n\nprint(\"Flat_1 shape :\",flat_1.shape)\nprint(\"Flat_2 shape :\",flat_2.shape)\nprint(\"Flat_3 shape :\",flat_3.shape)\nprint(\"Flat_4 shape :\",flat_4.shape)\n\n\nconcat = tf.keras.layers.Concatenate()([flat_1,flat_2,flat_3, flat_4])\nprint(concat.shape)\ndense = tf.keras.layers.Dense(32,activation = 'relu',kernel_initializer=tf.keras.initializers.he_normal(seed=40))(concat)\n\noutput = tf.keras.layers.Dense(30,activation = 'sigmoid',kernel_initializer=tf.keras.initializers.glorot_uniform(seed=45))(dense)\n\nmodel = tf.keras.Model(inputs = [input_1, input_2, input_3, input_4], outputs = output)\n\nmodel.summary()","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:04:31.883012Z","iopub.execute_input":"2022-02-20T13:04:31.883589Z","iopub.status.idle":"2022-02-20T13:04:33.610666Z","shell.execute_reply.started":"2022-02-20T13:04:31.883551Z","shell.execute_reply":"2022-02-20T13:04:33.609942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.utils.plot_model(model)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:04:33.612003Z","iopub.execute_input":"2022-02-20T13:04:33.612252Z","iopub.status.idle":"2022-02-20T13:04:33.81898Z","shell.execute_reply.started":"2022-02-20T13:04:33.612216Z","shell.execute_reply":"2022-02-20T13:04:33.818165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n                   optimizer=tf.keras.optimizers.Adam()\n            )","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:04:33.820949Z","iopub.execute_input":"2022-02-20T13:04:33.821258Z","iopub.status.idle":"2022-02-20T13:04:33.83481Z","shell.execute_reply.started":"2022-02-20T13:04:33.821218Z","shell.execute_reply":"2022-02-20T13:04:33.834093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"custom_callback = SpearmanCallback(validation_data=([tokenized_question_title_valid, tokenized_question_body_valid, tokenized_answer_valid, X_valid_feature_eng ], y_valid_dataset))","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:04:33.836543Z","iopub.execute_input":"2022-02-20T13:04:33.837036Z","iopub.status.idle":"2022-02-20T13:04:33.841642Z","shell.execute_reply.started":"2022-02-20T13:04:33.836995Z","shell.execute_reply":"2022-02-20T13:04:33.840927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#history = model.fit([tokenized_question_title_train, tokenized_question_body_train, tokenized_answer_train, X_train_feature_eng], y_train_dataset, \n#           epochs=10,  \n#           validation_data=([tokenized_question_title_valid, tokenized_question_body_valid, tokenized_answer_valid, X_valid_feature_eng], y_valid_dataset), \n#           callbacks=[custom_callback])","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:04:33.842945Z","iopub.execute_input":"2022-02-20T13:04:33.843782Z","iopub.status.idle":"2022-02-20T13:04:33.851523Z","shell.execute_reply.started":"2022-02-20T13:04:33.843722Z","shell.execute_reply":"2022-02-20T13:04:33.850792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#pd.DataFrame(history.history).plot()\n#plt.title(\"Base model with 9 FE features\")","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:04:33.852802Z","iopub.execute_input":"2022-02-20T13:04:33.853282Z","iopub.status.idle":"2022-02-20T13:04:33.859918Z","shell.execute_reply.started":"2022-02-20T13:04:33.853184Z","shell.execute_reply":"2022-02-20T13:04:33.859222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Observation:**\nThe model with meta feature engineering has acheived better performance comparing to the TF-IDF, Web scraping features by acheiving the height spearman value as **0.2871** at 14th epoch and the model started overfitting the train data after 5th epoch as the training loss decrease but validation loss increasing","metadata":{"editable":false}},{"cell_type":"markdown","source":"# Overall Observations:\n\n* Model with basic three features (question_title, quesiton_body, answer) + Meta features has acheived best performance comparing to the model with TF-IDF and web scraping features.\n* The best base model has acheived an spearman score of **0.2871**.\n* There is no need for training for 30 epochs as after 10 epochs the validation loss is increasing so the model will be overfitting to the training data.\n* TF-IDF and Web scraping feature are not important to get best performance, meta features are the important features.","metadata":{"editable":false}},{"cell_type":"code","source":"","metadata":{"editable":false},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Base Model with 100 dim glove \n","metadata":{"editable":false}},{"cell_type":"markdown","source":"# Embedding 100 dim weights for **question_title**","metadata":{"editable":false}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom tqdm.notebook import tqdm\n\nglove_file = open('../input/glove-embeddings/glove.6B.100d.txt', encoding='utf8')\n\nembeddings_index_100 = dict()   \nfor line in tqdm(glove_file):\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nglove_file.close()\nprint('Loaded %s word vectors.' % len(embeddings_index))","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:04:33.861378Z","iopub.execute_input":"2022-02-20T13:04:33.861699Z","iopub.status.idle":"2022-02-20T13:04:51.795496Z","shell.execute_reply.started":"2022-02-20T13:04:33.861665Z","shell.execute_reply":"2022-02-20T13:04:51.794822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create a weight matrix for words in training docs\n\nembedding_100_matrix_question_title = np.zeros((vocab_size_question_title, 100))\nfor word, i in question_title_tk.word_index.items():\n    embedding_vector = embeddings_index_100.get(word)\n    if embedding_vector is not None:\n        embedding_matrix_question_title[i] = embedding_vector","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:04:51.796854Z","iopub.execute_input":"2022-02-20T13:04:51.797315Z","iopub.status.idle":"2022-02-20T13:04:51.809592Z","shell.execute_reply.started":"2022-02-20T13:04:51.797275Z","shell.execute_reply":"2022-02-20T13:04:51.808886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_100_matrix_question_title.shape","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:04:51.810824Z","iopub.execute_input":"2022-02-20T13:04:51.811174Z","iopub.status.idle":"2022-02-20T13:04:51.817572Z","shell.execute_reply.started":"2022-02-20T13:04:51.811136Z","shell.execute_reply":"2022-02-20T13:04:51.816715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Embedding 100 dim weights for **question_body**","metadata":{"editable":false}},{"cell_type":"code","source":"# create a weight matrix for words in training docs\n\nembedding_100_matrix_question_body = np.zeros((vocab_size_question_body, 100))\nfor word, i in question_body_tk.word_index.items():\n    embedding_vector = embeddings_index_100.get(word)\n    if embedding_vector is not None:\n        embedding_matrix_question_body[i] = embedding_vector","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:04:51.820086Z","iopub.execute_input":"2022-02-20T13:04:51.820975Z","iopub.status.idle":"2022-02-20T13:04:51.847144Z","shell.execute_reply.started":"2022-02-20T13:04:51.820939Z","shell.execute_reply":"2022-02-20T13:04:51.84619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_100_matrix_question_body.shape","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:04:51.848607Z","iopub.execute_input":"2022-02-20T13:04:51.848953Z","iopub.status.idle":"2022-02-20T13:04:51.859435Z","shell.execute_reply.started":"2022-02-20T13:04:51.848919Z","shell.execute_reply":"2022-02-20T13:04:51.858706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Embedding 100 dim weights for **answer**","metadata":{"editable":false}},{"cell_type":"code","source":"# create a weight matrix for words in training docs\n\nembedding_100_matrix_answer = np.zeros((vocab_size_answer, 100))\nfor word, i in question_body_tk.word_index.items():\n    embedding_vector = embeddings_index_100.get(word)\n    if embedding_vector is not None:\n        embedding_matrix_answer[i] = embedding_vector","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:04:51.861039Z","iopub.execute_input":"2022-02-20T13:04:51.861307Z","iopub.status.idle":"2022-02-20T13:04:51.886572Z","shell.execute_reply.started":"2022-02-20T13:04:51.861272Z","shell.execute_reply":"2022-02-20T13:04:51.885843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_100_matrix_answer.shape","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:04:51.887519Z","iopub.execute_input":"2022-02-20T13:04:51.887718Z","iopub.status.idle":"2022-02-20T13:04:51.893583Z","shell.execute_reply.started":"2022-02-20T13:04:51.887694Z","shell.execute_reply":"2022-02-20T13:04:51.892819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Building base model with 100 dim embeddings\n\n","metadata":{"editable":false}},{"cell_type":"code","source":"############################ INPUT 1 - Question title ############################################################\ninput_1 = tf.keras.layers.Input(shape=(30,))\nembed_1 = tf.keras.layers.Embedding(vocab_size_question_title, \n                                    100, \n                                    weights=[embedding_100_matrix_question_title],\n                                    input_length=30,\n                                    embeddings_initializer=tf.keras.initializers.RandomNormal(mean=0, stddev=1, seed=23), \n                                    trainable=False)(input_1)\nlstm1 = tf.keras.layers.Bidirectional(\n    tf.keras.layers.LSTM(24,activation='tanh',kernel_initializer=tf.keras.initializers.glorot_uniform(seed=26),\n          return_sequences=True))(embed_1)\nprint(\"lstm_1 :\",lstm1.shape)\nflat_1 = tf.keras.layers.Flatten()(lstm1)\n\n\n############################ INPUT 2 - Question body ##############################################################\ninput_2 = tf.keras.layers.Input(shape=(question_body_max_len,))\nembed_2 = tf.keras.layers.Embedding(vocab_size_question_body,\n                    100, \n                    weights=[embedding_100_matrix_question_body], \n                    input_length=question_body_max_len, \n                    embeddings_initializer=tf.keras.initializers.RandomNormal(mean=0, stddev=1, seed=24), \n                    trainable=False)(input_2)\nlstm2 = tf.keras.layers.Bidirectional(\n    tf.keras.layers.LSTM(24,activation='tanh',kernel_initializer=tf.keras.initializers.glorot_uniform(seed=27),\n          return_sequences=True))(embed_2)\nprint(\"lstm_2 :\",lstm2.shape)\nflat_2 = tf.keras.layers.Flatten()(lstm2)\n\n\n############################ INPUT 3 - Answer ################################################################\ninput_3 = tf.keras.layers.Input(shape=(answer_max_len,))\nembed_3 = tf.keras.layers.Embedding(vocab_size_answer, 100, weights=[embedding_100_matrix_answer], \n                    input_length=answer_max_len, \n                    embeddings_initializer=tf.keras.initializers.RandomNormal(mean=0, stddev=1, seed=25), \n                    trainable=False)(input_3)\nlstm3 = tf.keras.layers.Bidirectional(\n    tf.keras.layers.LSTM(24,activation='tanh',kernel_initializer=tf.keras.initializers.glorot_uniform(seed=28),\n          return_sequences=True))(embed_3)\nprint(\"lstm_3 :\",lstm3.shape)\nflat_3 = tf.keras.layers.Flatten()(lstm3)\n\n#########################  INPUT 4 - Feature Engineering features ############################################\ninput_4 = tf.keras.layers.Input(shape=(9,))\nx = tf.keras.layers.Dense(64, activation='relu')(input_4)\nx = tf.keras.layers.Dropout(0.2)(x)\nflat_4 = tf.keras.layers.Dense(32, activation='relu')(x)\n\n\n##################### Input 5 - Web scraping feature ##########################################################\ninput_5 = tf.keras.layers.Input(shape=(4,))\nx = tf.keras.layers.Dense(128, activation='relu')(input_5)\nx = tf.keras.layers.Dense(64, activation='relu')(x)\nx = tf.keras.layers.Dropout(0.2)(x)\nflat_5 = tf.keras.layers.Dense(32, activation='relu')(x)\n\nprint(\"Flat_1 shape :\",flat_1.shape)\nprint(\"Flat_2 shape :\",flat_2.shape)\nprint(\"Flat_3 shape :\",flat_3.shape)\nprint(\"Flat_4 shape :\",flat_4.shape)\nprint(\"Flat_5 shape :\",flat_5.shape)\n\n\nconcat = tf.keras.layers.Concatenate()([flat_1,flat_2,flat_3, flat_4, flat_5])\nprint(concat.shape)\ndense = tf.keras.layers.Dense(32,activation = 'relu',kernel_initializer=tf.keras.initializers.he_normal(seed=40))(concat)\n\noutput = tf.keras.layers.Dense(30,activation = 'sigmoid',kernel_initializer=tf.keras.initializers.glorot_uniform(seed=45))(dense)\n\nmodel = tf.keras.Model(inputs = [input_1, input_2, input_3, input_4, input_5], outputs = output)\n\nmodel.summary()","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:04:51.895283Z","iopub.execute_input":"2022-02-20T13:04:51.895799Z","iopub.status.idle":"2022-02-20T13:04:53.417387Z","shell.execute_reply.started":"2022-02-20T13:04:51.895762Z","shell.execute_reply":"2022-02-20T13:04:53.416706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.utils.plot_model(model)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:04:53.418766Z","iopub.execute_input":"2022-02-20T13:04:53.419024Z","iopub.status.idle":"2022-02-20T13:04:53.645572Z","shell.execute_reply.started":"2022-02-20T13:04:53.418989Z","shell.execute_reply":"2022-02-20T13:04:53.644789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n                   optimizer=tf.keras.optimizers.Adam()\n            )","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:04:53.64759Z","iopub.execute_input":"2022-02-20T13:04:53.647896Z","iopub.status.idle":"2022-02-20T13:04:53.663417Z","shell.execute_reply.started":"2022-02-20T13:04:53.647841Z","shell.execute_reply":"2022-02-20T13:04:53.662373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_valid_dataset = np.array(y_valid_dataset)\ncustom_callback = SpearmanCallback(validation_data=([tokenized_question_title_valid, tokenized_question_body_valid, tokenized_answer_valid, X_valid_feature_eng, valid_web_scraping_feature ], y_valid_dataset))","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:04:53.664861Z","iopub.execute_input":"2022-02-20T13:04:53.665525Z","iopub.status.idle":"2022-02-20T13:04:53.670524Z","shell.execute_reply.started":"2022-02-20T13:04:53.665484Z","shell.execute_reply":"2022-02-20T13:04:53.669897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#history = model.fit([tokenized_question_title_train, tokenized_question_body_train, tokenized_answer_train, X_train_feature_eng, train_web_scraping_feature], y_train_dataset, \n#           epochs=10,  \n#           validation_data=([tokenized_question_title_valid, tokenized_question_body_valid, tokenized_answer_valid, X_valid_feature_eng, valid_web_scraping_feature], y_valid_dataset), \n#           callbacks=[custom_callback])","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:04:53.67222Z","iopub.execute_input":"2022-02-20T13:04:53.672736Z","iopub.status.idle":"2022-02-20T13:04:53.682085Z","shell.execute_reply.started":"2022-02-20T13:04:53.672697Z","shell.execute_reply":"2022-02-20T13:04:53.681309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#pd.DataFrame(history.history).plot()\n#plt.title(\"Base model with 100 dim embeddings\")","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:04:53.685129Z","iopub.execute_input":"2022-02-20T13:04:53.685404Z","iopub.status.idle":"2022-02-20T13:04:53.69082Z","shell.execute_reply.started":"2022-02-20T13:04:53.685377Z","shell.execute_reply":"2022-02-20T13:04:53.689977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from prettytable import PrettyTable\n\n\nmyTable = PrettyTable([\"Base Model\", \"Features\", \"Spearman scroe\"])\n\n\nmyTable.add_row([\"Bi-LSTM\", \"Three basic features\", \"0.2756\"])\nmyTable.add_row([\"Bi-LSTM\", \"Three basic features + 18 FE Features(meta, TF-IDF, Web scraping)\", \"0.00256\"])\nmyTable.add_row([\"Bi-LSTM\", \"Three basic features + 13 FE Features(meta, Web scraping)\", \"0.012556\"])\nmyTable.add_row([\"Bi-LSTM\", \"Three basic features + 9 FE Features(meta features)\", \"0.2865\"])\nmyTable.add_row([\"Bi-LSTM\", \"Three basic features + 13 FE features with 100 dim embeddings(meta, Web scraping)\", \"-0.004126\"])\n\nprint(myTable)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:04:53.692126Z","iopub.execute_input":"2022-02-20T13:04:53.692519Z","iopub.status.idle":"2022-02-20T13:04:53.703636Z","shell.execute_reply.started":"2022-02-20T13:04:53.692482Z","shell.execute_reply":"2022-02-20T13:04:53.702602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Observations:\n\n* From the above we can observe that base model with three basic + meta features provided high score of **0.2865** comparing to all the other model.\n* The model with 100 dim embeddings with meta and web scraping features has also not reached the score of base model with base feature.\n* so, we can conclude that base model with meta features are gives the high score comparing to other models that we have experimented with.\n\n* As the training data is less and for training neural network model we need huge data so we will try with  transfer learning model like bert, albert, XLnet etc..","metadata":{"editable":false}},{"cell_type":"markdown","source":"# Universal sentense encoder","metadata":{"editable":false}},{"cell_type":"markdown","source":"The Universal Sentence Encoder makes getting sentence level embeddings as easy as it has historically been to lookup the embeddings for individual words. The sentence embeddings can then be trivially used to compute sentence level meaning similarity as well as to enable better performance on downstream classification tasks using less supervised training data.\n\n> **Reference:** https://www.tensorflow.org/hub/tutorials/semantic_similarity_with_tf_hub_universal_encoder","metadata":{"editable":false}},{"cell_type":"code","source":"import tensorflow as tf\n\nimport tensorflow_hub as hub\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport re\nimport seaborn as sns","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:04:53.705148Z","iopub.execute_input":"2022-02-20T13:04:53.705552Z","iopub.status.idle":"2022-02-20T13:04:54.076379Z","shell.execute_reply.started":"2022-02-20T13:04:53.705516Z","shell.execute_reply":"2022-02-20T13:04:54.075519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\nuni_sen_embed = hub.load(module_url)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:04:54.078418Z","iopub.execute_input":"2022-02-20T13:04:54.079141Z","iopub.status.idle":"2022-02-20T13:05:12.923256Z","shell.execute_reply.started":"2022-02-20T13:04:54.079094Z","shell.execute_reply":"2022-02-20T13:05:12.922461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@title Compute a representation for each message, showing various lengths supported.\nword = \"Elephant\"\nsentence = \"I am a sentence for which I would like to get its embedding.\"\nparagraph = (\n    \"Universal Sentence Encoder embeddings also support short paragraphs. \"\n    \"There is no hard limit on how long the paragraph is. Roughly, the longer \"\n    \"the more 'diluted' the embedding will be.\")\nmessages = [word, sentence, paragraph]\n\nmessage_embeddings = uni_sen_embed(messages)\nfor i, message_embedding in enumerate(np.array(message_embeddings).tolist()):\n  print(\"Message: {}\".format(messages[i]))\n  print(\"Embedding size: {}\".format(len(message_embedding)))\n  message_embedding_snippet = \", \".join(\n      (str(x) for x in message_embedding[:3]))\n  print(\"Embedding: [{}, ...]\\n\".format(message_embedding_snippet))","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:05:12.924471Z","iopub.execute_input":"2022-02-20T13:05:12.924722Z","iopub.status.idle":"2022-02-20T13:05:14.320333Z","shell.execute_reply.started":"2022-02-20T13:05:12.924689Z","shell.execute_reply":"2022-02-20T13:05:14.319548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observartion:**  we can observe that for single word the embedding is 512 and even for varying sentence the embedding size is 512","metadata":{"editable":false}},{"cell_type":"code","source":"def plot_similarity(labels, features, rotation):\n    corr = np.inner(features, features)\n    sns.set(font_scale=1.2)\n    g = sns.heatmap(\n      corr,\n      xticklabels=labels,\n      yticklabels=labels,\n      vmin=0,\n      vmax=1,\n      cmap=\"YlOrRd\")\n    g.set_xticklabels(labels, rotation=rotation)\n    g.set_title(\"Semantic Textual Similarity\")\n\ndef run_and_plot(messages_):\n    message_embeddings_ = uni_sen_embed(messages_)\n    plot_similarity(messages_, message_embeddings_, 90)\n\n    \n ################ sample example of sentense level embeddings using USE ###############################3\nmessages = [\n    # Smartphones\n    \"I like my phone\",\n    \"My phone is not good.\",\n    \"Your cellphone looks great.\",\n\n    # Weather\n    \"Will it snow tomorrow?\",\n    \"Recently a lot of hurricanes have hit the US\",\n    \"Global warming is real\",\n\n    # Food and health\n    \"An apple a day, keeps the doctors away\",\n    \"Eating strawberries is healthy\",\n    \"Is paleo better than keto?\",\n\n    # Asking about age\n    \"How old are you?\",\n    \"what is your age?\",\n]\n\nrun_and_plot(messages)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:05:14.321806Z","iopub.execute_input":"2022-02-20T13:05:14.322081Z","iopub.status.idle":"2022-02-20T13:05:14.783433Z","shell.execute_reply.started":"2022-02-20T13:05:14.322037Z","shell.execute_reply":"2022-02-20T13:05:14.782754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation:** we can observe that similar sentense are grouped together","metadata":{"editable":false}},{"cell_type":"code","source":"X_train_dataset.shape, X_valid_dataset.shape, test_dataset.shape","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:05:14.784904Z","iopub.execute_input":"2022-02-20T13:05:14.785388Z","iopub.status.idle":"2022-02-20T13:05:14.791293Z","shell.execute_reply.started":"2022-02-20T13:05:14.785351Z","shell.execute_reply":"2022-02-20T13:05:14.790508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embeddings_train = {}\nembeddings_valid = {}\nembeddings_test = {}\n\n\nfor text in ['question_title', 'question_body', 'answer']:\n    print(text)\n    train_text = X_train_dataset[text].str.replace('?', '.').str.replace('!', '.').tolist()\n    valid_text = X_valid_dataset[text].str.replace('?', '.').str.replace('!', '.').tolist()\n    test_text = test_dataset[text].str.replace('?', '.').str.replace('!', '.').tolist()\n    \n    curr_train_emb = []\n    curr_valid_emb = []\n    curr_test_emb = []\n\n    batch_size = 4\n    ind = 0\n    while ind*batch_size < len(train_text):\n        curr_train_emb.append(uni_sen_embed(train_text[ind*batch_size: (ind + 1)*batch_size]).numpy())\n        ind += 1\n        \n    ind = 0\n    while ind*batch_size < len(test_text):\n        curr_test_emb.append(uni_sen_embed(test_text[ind*batch_size: (ind + 1)*batch_size]).numpy())\n        ind += 1    \n        \n    ind = 0\n    while ind*batch_size < len(valid_text):\n        curr_valid_emb.append(uni_sen_embed(valid_text[ind*batch_size: (ind + 1)*batch_size]).numpy())\n        ind += 1    \n    \n    embeddings_train[text + '_USE_embedding'] = np.vstack(curr_train_emb)\n    embeddings_valid[text + '_USE_embedding'] = np.vstack(curr_valid_emb)\n    embeddings_test[text + '_USE_embedding'] = np.vstack(curr_test_emb)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:05:14.793051Z","iopub.execute_input":"2022-02-20T13:05:14.793591Z","iopub.status.idle":"2022-02-20T13:05:56.854083Z","shell.execute_reply.started":"2022-02-20T13:05:14.793554Z","shell.execute_reply":"2022-02-20T13:05:56.853314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embeddings_test['question_title_USE_embedding']","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:05:56.855848Z","iopub.execute_input":"2022-02-20T13:05:56.85626Z","iopub.status.idle":"2022-02-20T13:05:56.862814Z","shell.execute_reply.started":"2022-02-20T13:05:56.856224Z","shell.execute_reply":"2022-02-20T13:05:56.862163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = np.hstack([item for k, item in embeddings_train.items()])\nX_valid = np.hstack([item for k, item in embeddings_valid.items()])\nX_test = np.hstack([item for k, item in embeddings_test.items()])","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:05:56.864238Z","iopub.execute_input":"2022-02-20T13:05:56.864678Z","iopub.status.idle":"2022-02-20T13:05:56.890766Z","shell.execute_reply.started":"2022-02-20T13:05:56.864641Z","shell.execute_reply":"2022-02-20T13:05:56.889932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Building the model\n\ninput_1 = tf.keras.layers.Input(shape=(X_train.shape[1],))\nx = tf.keras.layers.Dense(512, activation='elu')(input_1)\nx = tf.keras.layers.Dropout(0.2)(x)\nx = tf.keras.layers.Dense(218, activation='relu')(x)\nx = tf.keras.layers.BatchNormalization()(x)\noutput = tf.keras.layers.Dense(30, activation='sigmoid')(x)\n\n\nmodel = tf.keras.Model(inputs=input_1, outputs=output)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:05:56.894376Z","iopub.execute_input":"2022-02-20T13:05:56.894618Z","iopub.status.idle":"2022-02-20T13:05:56.947081Z","shell.execute_reply.started":"2022-02-20T13:05:56.894583Z","shell.execute_reply":"2022-02-20T13:05:56.946317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer= tf.keras.optimizers.Adam(lr=1e-4),\n        loss=['binary_crossentropy']\n    )\n\nmodel.summary()","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:05:56.948441Z","iopub.execute_input":"2022-02-20T13:05:56.948696Z","iopub.status.idle":"2022-02-20T13:05:56.965353Z","shell.execute_reply.started":"2022-02-20T13:05:56.94866Z","shell.execute_reply":"2022-02-20T13:05:56.964359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.utils.plot_model(model)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:05:56.967141Z","iopub.execute_input":"2022-02-20T13:05:56.967627Z","iopub.status.idle":"2022-02-20T13:05:57.148679Z","shell.execute_reply.started":"2022-02-20T13:05:56.967578Z","shell.execute_reply":"2022-02-20T13:05:57.147815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_valid_dataset = np.array(y_valid_dataset)\ncustom_callback = SpearmanCallback(validation_data=(X_valid, y_valid_dataset))","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:05:57.150508Z","iopub.execute_input":"2022-02-20T13:05:57.15098Z","iopub.status.idle":"2022-02-20T13:05:57.155487Z","shell.execute_reply.started":"2022-02-20T13:05:57.150938Z","shell.execute_reply":"2022-02-20T13:05:57.154737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(X_train, y_train_dataset,\n          epochs =100,\n          validation_data=(X_valid, y_valid_dataset),\n                    callbacks=[custom_callback]\n         )","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:05:57.15701Z","iopub.execute_input":"2022-02-20T13:05:57.157522Z","iopub.status.idle":"2022-02-20T13:07:04.76449Z","shell.execute_reply.started":"2022-02-20T13:05:57.157486Z","shell.execute_reply":"2022-02-20T13:07:04.76372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(history.history).plot()\nplt.title(\"USE with basic Three features\")","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:07:04.766468Z","iopub.execute_input":"2022-02-20T13:07:04.766944Z","iopub.status.idle":"2022-02-20T13:07:05.056438Z","shell.execute_reply.started":"2022-02-20T13:07:04.766906Z","shell.execute_reply":"2022-02-20T13:07:05.055626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Observations:** \n* For 12 epochs we were able to acheive an spearman score of **0.33029** after this model is overfitting so no need to train more than 15 epochs.\n* With basic three features (quesiton_title, quesiton_body, answer) using USE embeddings we are able to acheive spearman score of 0.33029 which is better than base+9 Meta feature model. \n\n\n## Lets do some more experiments on USE as below\n    1. USE + 9 Meta feature\n    2. USE + L2 distance similarity features\n    3. USE + cosine similarity features\n    4. USE + all distance similarity features + 9 meta featues\n","metadata":{}},{"cell_type":"markdown","source":"### **USE + 9 Meta features Model**","metadata":{"editable":false}},{"cell_type":"code","source":"# Building the model\n\n\n######################## USE Embeddings #########################\ninput_1 = tf.keras.layers.Input(shape=(X_train.shape[1],), name='USE_Embeddings')\n\n####################### 9 Meta features #########################\ninput_2 = tf.keras.layers.Input(shape=(9,))\nx  = tf.keras.layers.Dense(68, activation='relu')(input_2)\nx = tf.keras.layers.Dense(64, activation='relu')(input_2)\nx = tf.keras.layers.Dropout(0.2)(x)\nflat_2 = tf.keras.layers.Dense(32, activation='relu')(x)\n\nconcat = tf.keras.layers.Concatenate()([input_1,flat_2])\n\n\nx = tf.keras.layers.Dense(512, activation='elu')(concat)\nx = tf.keras.layers.Dropout(0.2)(x)\nx = tf.keras.layers.Dense(218, activation='relu')(x)\nx = tf.keras.layers.BatchNormalization()(x)\noutput = tf.keras.layers.Dense(30, activation='sigmoid')(x)\n\n\nmodel = tf.keras.Model(inputs=[input_1,input_2], outputs=output)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:07:05.05825Z","iopub.execute_input":"2022-02-20T13:07:05.058673Z","iopub.status.idle":"2022-02-20T13:07:05.132317Z","shell.execute_reply.started":"2022-02-20T13:07:05.058633Z","shell.execute_reply":"2022-02-20T13:07:05.131556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer= tf.keras.optimizers.Adam(lr=1e-4),\n        loss=['binary_crossentropy']\n    )\n\nmodel.summary()","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:07:05.133625Z","iopub.execute_input":"2022-02-20T13:07:05.134011Z","iopub.status.idle":"2022-02-20T13:07:05.150337Z","shell.execute_reply.started":"2022-02-20T13:07:05.133977Z","shell.execute_reply":"2022-02-20T13:07:05.149677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.utils.plot_model(model)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:07:05.151445Z","iopub.execute_input":"2022-02-20T13:07:05.151736Z","iopub.status.idle":"2022-02-20T13:07:05.343349Z","shell.execute_reply.started":"2022-02-20T13:07:05.151698Z","shell.execute_reply":"2022-02-20T13:07:05.342522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_valid_dataset = np.array(y_valid_dataset)\ncustom_callback = SpearmanCallback(validation_data=([X_valid,X_valid_feature_eng] , y_valid_dataset))","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:07:05.345136Z","iopub.execute_input":"2022-02-20T13:07:05.345683Z","iopub.status.idle":"2022-02-20T13:07:05.351217Z","shell.execute_reply.started":"2022-02-20T13:07:05.345644Z","shell.execute_reply":"2022-02-20T13:07:05.350148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit([X_train, X_train_feature_eng], y_train_dataset,\n          epochs =100,\n          validation_data=([X_valid, X_valid_feature_eng],y_valid_dataset),\n                    callbacks=[custom_callback]\n         )","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:07:05.352552Z","iopub.execute_input":"2022-02-20T13:07:05.35334Z","iopub.status.idle":"2022-02-20T13:08:28.074165Z","shell.execute_reply.started":"2022-02-20T13:07:05.353302Z","shell.execute_reply":"2022-02-20T13:08:28.073354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(history.history).plot()\nplt.title(\"USE with basic Three features + 9 Meta features\")","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:08:28.075792Z","iopub.execute_input":"2022-02-20T13:08:28.076078Z","iopub.status.idle":"2022-02-20T13:08:28.359745Z","shell.execute_reply.started":"2022-02-20T13:08:28.076033Z","shell.execute_reply":"2022-02-20T13:08:28.358951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Observation:** \n* Using USE + 9 meta feature we have acheived sperman score of **0.3713** which is better than simple USE + basic features.\n* The loss curve is smothly decreasing curve and after 90th epoch the model starting to overfit.\n* Lets try with L2 distance similarity features which can be computed using USE embeddings ","metadata":{}},{"cell_type":"markdown","source":"# **USE + L2 distance similarity + 9 Meta Features Model**","metadata":{"editable":false}},{"cell_type":"code","source":"# Computing L2 distance similarity\n\nl2_distance_similarity = lambda x, y: np.power(x - y, 2).sum(axis=1)\n\n\nsim_distance_features_train = np.array([\n    l2_distance_similarity(embeddings_train['question_title_USE_embedding'], embeddings_train['answer_USE_embedding']),\n    l2_distance_similarity(embeddings_train['question_body_USE_embedding'], embeddings_train['answer_USE_embedding']),\n    l2_distance_similarity(embeddings_train['question_body_USE_embedding'], embeddings_train['question_title_USE_embedding']),\n    ]).T\n\n\nsim_distance_features_valid = np.array([\n    l2_distance_similarity(embeddings_valid['question_title_USE_embedding'], embeddings_valid['answer_USE_embedding']),\n    l2_distance_similarity(embeddings_valid['question_body_USE_embedding'], embeddings_valid['answer_USE_embedding']),\n    l2_distance_similarity(embeddings_valid['question_body_USE_embedding'], embeddings_valid['question_title_USE_embedding']),\n    ]).T\n\n\nsim_distance_features_test = np.array([\n    l2_distance_similarity(embeddings_test['question_title_USE_embedding'], embeddings_test['answer_USE_embedding']),\n    l2_distance_similarity(embeddings_test['question_body_USE_embedding'], embeddings_test['answer_USE_embedding']),\n    l2_distance_similarity(embeddings_test['question_body_USE_embedding'], embeddings_test['question_title_USE_embedding']),\n    ]).T\n","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:08:28.361183Z","iopub.execute_input":"2022-02-20T13:08:28.361537Z","iopub.status.idle":"2022-02-20T13:08:28.563712Z","shell.execute_reply.started":"2022-02-20T13:08:28.361495Z","shell.execute_reply":"2022-02-20T13:08:28.562865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = np.hstack([item for k, item in embeddings_train.items()] + [sim_distance_features_train])\nX_valid = np.hstack([item for k, item in embeddings_valid.items()] + [sim_distance_features_valid])\nX_test = np.hstack([item for k, item in embeddings_test.items()] + [sim_distance_features_test])","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:08:28.56497Z","iopub.execute_input":"2022-02-20T13:08:28.566Z","iopub.status.idle":"2022-02-20T13:08:28.590527Z","shell.execute_reply.started":"2022-02-20T13:08:28.565958Z","shell.execute_reply":"2022-02-20T13:08:28.589711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:08:28.591822Z","iopub.execute_input":"2022-02-20T13:08:28.592206Z","iopub.status.idle":"2022-02-20T13:08:28.600316Z","shell.execute_reply.started":"2022-02-20T13:08:28.592165Z","shell.execute_reply":"2022-02-20T13:08:28.599463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Building the model\n\n\n######################## USE Embeddings #########################\ninput_1 = tf.keras.layers.Input(shape=(X_train.shape[1],), name='USE_Embeddings')\n\n####################### 9 Meta features #########################\ninput_2 = tf.keras.layers.Input(shape=(9,))\nx  = tf.keras.layers.Dense(68, activation='relu')(input_2)\nx = tf.keras.layers.Dense(64, activation='relu')(input_2)\nx = tf.keras.layers.Dropout(0.2)(x)\nflat_2 = tf.keras.layers.Dense(32, activation='relu')(x)\n\nconcat = tf.keras.layers.Concatenate()([input_1,flat_2])\n\n\nx = tf.keras.layers.Dense(512, activation='elu')(concat)\nx = tf.keras.layers.Dropout(0.2)(x)\nx = tf.keras.layers.Dense(218, activation='relu')(x)\nx = tf.keras.layers.BatchNormalization()(x)\noutput = tf.keras.layers.Dense(30, activation='sigmoid')(x)\n\n\nmodel = tf.keras.Model(inputs=[input_1,input_2], outputs=output)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:08:28.601885Z","iopub.execute_input":"2022-02-20T13:08:28.602333Z","iopub.status.idle":"2022-02-20T13:08:28.678167Z","shell.execute_reply.started":"2022-02-20T13:08:28.60227Z","shell.execute_reply":"2022-02-20T13:08:28.677327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer= tf.keras.optimizers.Adam(lr=1e-4),\n        loss=['binary_crossentropy']\n    )\n\nmodel.summary()","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:08:28.679952Z","iopub.execute_input":"2022-02-20T13:08:28.680399Z","iopub.status.idle":"2022-02-20T13:08:28.697015Z","shell.execute_reply.started":"2022-02-20T13:08:28.680359Z","shell.execute_reply":"2022-02-20T13:08:28.6961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.utils.plot_model(model)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:08:28.698478Z","iopub.execute_input":"2022-02-20T13:08:28.698685Z","iopub.status.idle":"2022-02-20T13:08:28.898646Z","shell.execute_reply.started":"2022-02-20T13:08:28.69866Z","shell.execute_reply":"2022-02-20T13:08:28.897787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_valid_dataset = np.array(y_valid_dataset)\ncustom_callback = SpearmanCallback(validation_data=([X_valid,X_valid_feature_eng] , y_valid_dataset))","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:08:28.900335Z","iopub.execute_input":"2022-02-20T13:08:28.900902Z","iopub.status.idle":"2022-02-20T13:08:28.907376Z","shell.execute_reply.started":"2022-02-20T13:08:28.900849Z","shell.execute_reply":"2022-02-20T13:08:28.90622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit([X_train, X_train_feature_eng], y_train_dataset,\n          epochs =100,\n          validation_data=([X_valid, X_valid_feature_eng],y_valid_dataset),\n                    callbacks=[custom_callback]\n         )","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:08:28.909277Z","iopub.execute_input":"2022-02-20T13:08:28.910066Z","iopub.status.idle":"2022-02-20T13:09:51.494218Z","shell.execute_reply.started":"2022-02-20T13:08:28.910017Z","shell.execute_reply":"2022-02-20T13:09:51.493416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(history.history).plot()\nplt.title(\"USE with basic Three features + L2 distance similarity features + 9 Meta features\")","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:09:51.495692Z","iopub.execute_input":"2022-02-20T13:09:51.495959Z","iopub.status.idle":"2022-02-20T13:09:51.775681Z","shell.execute_reply.started":"2022-02-20T13:09:51.495918Z","shell.execute_reply":"2022-02-20T13:09:51.774936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Observation:**\n* The score has not improved as much as above USE + 9 Meta featues model but the same score has been acheived by both the model but this model took more epochs to reach high score of **0.36124** at 22nd epoch and the after 20 epochs the model started overfitting to the train data as seen in the above curve.","metadata":{"editable":false}},{"cell_type":"markdown","source":"# **USE + cosine distance + 9 Meta features**\nNow lets performe the same experiment instead of l2 distance we will try with cosine distance","metadata":{"editable":false}},{"cell_type":"code","source":"# Computing cosine distance\n\ncos_dist = lambda x, y: (x*y).sum(axis=1)\n\ncos_distance_features_train = np.array([\n    cos_dist(embeddings_train['question_title_USE_embedding'], embeddings_train['answer_USE_embedding']),\n    cos_dist(embeddings_train['question_body_USE_embedding'], embeddings_train['answer_USE_embedding']),\n    cos_dist(embeddings_train['question_body_USE_embedding'], embeddings_train['question_title_USE_embedding']),\n    ]).T\n\n\ncos_distance_features_valid = np.array([\n    cos_dist(embeddings_valid['question_title_USE_embedding'], embeddings_valid['answer_USE_embedding']),\n    cos_dist(embeddings_valid['question_body_USE_embedding'], embeddings_valid['answer_USE_embedding']),\n    cos_dist(embeddings_valid['question_body_USE_embedding'], embeddings_valid['question_title_USE_embedding']),\n    ]).T\n\n\ncos_distance_features_test = np.array([\n    cos_dist(embeddings_test['question_title_USE_embedding'], embeddings_test['answer_USE_embedding']),\n    cos_dist(embeddings_test['question_body_USE_embedding'], embeddings_test['answer_USE_embedding']),\n    cos_dist(embeddings_test['question_body_USE_embedding'], embeddings_test['question_title_USE_embedding']),\n    ]).T\n","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:09:51.777131Z","iopub.execute_input":"2022-02-20T13:09:51.777369Z","iopub.status.idle":"2022-02-20T13:09:51.800491Z","shell.execute_reply.started":"2022-02-20T13:09:51.777335Z","shell.execute_reply":"2022-02-20T13:09:51.799741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = np.hstack([item for k, item in embeddings_train.items()] + [cos_distance_features_train])\nX_valid = np.hstack([item for k, item in embeddings_valid.items()] + [cos_distance_features_valid])\nX_test = np.hstack([item for k, item in embeddings_test.items()] + [cos_distance_features_test])","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:09:51.802002Z","iopub.execute_input":"2022-02-20T13:09:51.802272Z","iopub.status.idle":"2022-02-20T13:09:51.826193Z","shell.execute_reply.started":"2022-02-20T13:09:51.802236Z","shell.execute_reply":"2022-02-20T13:09:51.825426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:09:51.827641Z","iopub.execute_input":"2022-02-20T13:09:51.827922Z","iopub.status.idle":"2022-02-20T13:09:51.833798Z","shell.execute_reply.started":"2022-02-20T13:09:51.827887Z","shell.execute_reply":"2022-02-20T13:09:51.832942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Building the model\n\n\n######################## USE Embeddings #########################\ninput_1 = tf.keras.layers.Input(shape=(X_train.shape[1],), name='USE_Embeddings')\n\n####################### 9 Meta features #########################\ninput_2 = tf.keras.layers.Input(shape=(9,))\nx  = tf.keras.layers.Dense(68, activation='relu')(input_2)\nx = tf.keras.layers.Dense(64, activation='relu')(input_2)\nx = tf.keras.layers.Dropout(0.2)(x)\nflat_2 = tf.keras.layers.Dense(32, activation='relu')(x)\n\nconcat = tf.keras.layers.Concatenate()([input_1,flat_2])\n\n\nx = tf.keras.layers.Dense(512, activation='elu')(concat)\nx = tf.keras.layers.Dropout(0.2)(x)\nx = tf.keras.layers.Dense(218, activation='relu')(x)\nx = tf.keras.layers.BatchNormalization()(x)\noutput = tf.keras.layers.Dense(30, activation='sigmoid')(x)\n\n\nmodel = tf.keras.Model(inputs=[input_1,input_2], outputs=output)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:09:51.835423Z","iopub.execute_input":"2022-02-20T13:09:51.835679Z","iopub.status.idle":"2022-02-20T13:09:51.909194Z","shell.execute_reply.started":"2022-02-20T13:09:51.835645Z","shell.execute_reply":"2022-02-20T13:09:51.908408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer= tf.keras.optimizers.Adam(lr=1e-4),\n        loss=['binary_crossentropy']\n    )\n\nmodel.summary()","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:09:51.910531Z","iopub.execute_input":"2022-02-20T13:09:51.910791Z","iopub.status.idle":"2022-02-20T13:09:51.930895Z","shell.execute_reply.started":"2022-02-20T13:09:51.910757Z","shell.execute_reply":"2022-02-20T13:09:51.926933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.utils.plot_model(model)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:09:51.932241Z","iopub.execute_input":"2022-02-20T13:09:51.93251Z","iopub.status.idle":"2022-02-20T13:09:52.154258Z","shell.execute_reply.started":"2022-02-20T13:09:51.932475Z","shell.execute_reply":"2022-02-20T13:09:52.153299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_valid_dataset = np.array(y_valid_dataset)\ncustom_callback = SpearmanCallback(validation_data=([X_valid,X_valid_feature_eng] , y_valid_dataset))","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:09:52.156365Z","iopub.execute_input":"2022-02-20T13:09:52.156662Z","iopub.status.idle":"2022-02-20T13:09:52.16232Z","shell.execute_reply.started":"2022-02-20T13:09:52.156624Z","shell.execute_reply":"2022-02-20T13:09:52.161468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit([X_train, X_train_feature_eng], y_train_dataset,\n          epochs =100,\n          validation_data=([X_valid, X_valid_feature_eng],y_valid_dataset),\n                    callbacks=[custom_callback]\n         )","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:09:52.164358Z","iopub.execute_input":"2022-02-20T13:09:52.1651Z","iopub.status.idle":"2022-02-20T13:11:14.833228Z","shell.execute_reply.started":"2022-02-20T13:09:52.165062Z","shell.execute_reply":"2022-02-20T13:11:14.832331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(history.history).plot()\nplt.title(\"USE with basic Three features + Cosine distance features + 9 Meta features\")","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:11:14.834707Z","iopub.execute_input":"2022-02-20T13:11:14.835402Z","iopub.status.idle":"2022-02-20T13:11:15.115773Z","shell.execute_reply.started":"2022-02-20T13:11:14.835358Z","shell.execute_reply":"2022-02-20T13:11:15.115069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Observation:**\n* The model has acheived an spearman score of **0.371537** which was similar with above two USE after training for 50 epochs and even if we train for more epochs the score remains same.","metadata":{}},{"cell_type":"markdown","source":"# **USE + All distance features + 9 Meta features**","metadata":{"editable":false}},{"cell_type":"code","source":"X_train = np.hstack([item for k, item in embeddings_train.items()] + [cos_distance_features_train, sim_distance_features_train])\nX_valid = np.hstack([item for k, item in embeddings_valid.items()] + [cos_distance_features_valid, sim_distance_features_valid])\nX_test = np.hstack([item for k, item in embeddings_test.items()] + [cos_distance_features_test, sim_distance_features_test])","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:11:15.117505Z","iopub.execute_input":"2022-02-20T13:11:15.11771Z","iopub.status.idle":"2022-02-20T13:11:15.144028Z","shell.execute_reply.started":"2022-02-20T13:11:15.117684Z","shell.execute_reply":"2022-02-20T13:11:15.143251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:11:15.145203Z","iopub.execute_input":"2022-02-20T13:11:15.145974Z","iopub.status.idle":"2022-02-20T13:11:15.151284Z","shell.execute_reply.started":"2022-02-20T13:11:15.145936Z","shell.execute_reply":"2022-02-20T13:11:15.150628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Building the model\n\n\n######################## USE Embeddings #########################\ninput_1 = tf.keras.layers.Input(shape=(X_train.shape[1],), name='USE_Embeddings')\n\n####################### 9 Meta features #########################\ninput_2 = tf.keras.layers.Input(shape=(9,))\nx  = tf.keras.layers.Dense(68, activation='relu')(input_2)\nx = tf.keras.layers.Dense(64, activation='relu')(input_2)\nx = tf.keras.layers.Dropout(0.2)(x)\nflat_2 = tf.keras.layers.Dense(32, activation='relu')(x)\n\nconcat = tf.keras.layers.Concatenate()([input_1,flat_2])\n\n\nx = tf.keras.layers.Dense(512, activation='elu')(concat)\nx = tf.keras.layers.Dropout(0.2)(x)\nx = tf.keras.layers.Dense(218, activation='relu')(x)\nx = tf.keras.layers.BatchNormalization()(x)\noutput = tf.keras.layers.Dense(30, activation='sigmoid')(x)\n\n\nmodel = tf.keras.Model(inputs=[input_1,input_2], outputs=output)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:11:15.152753Z","iopub.execute_input":"2022-02-20T13:11:15.153195Z","iopub.status.idle":"2022-02-20T13:11:15.224479Z","shell.execute_reply.started":"2022-02-20T13:11:15.153154Z","shell.execute_reply":"2022-02-20T13:11:15.223762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer= tf.keras.optimizers.Adam(lr=1e-4),\n        loss=['binary_crossentropy']\n    )\n\nmodel.summary()","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:11:15.225895Z","iopub.execute_input":"2022-02-20T13:11:15.226151Z","iopub.status.idle":"2022-02-20T13:11:15.244947Z","shell.execute_reply.started":"2022-02-20T13:11:15.226118Z","shell.execute_reply":"2022-02-20T13:11:15.244282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_valid_dataset = np.array(y_valid_dataset)\ncustom_callback = SpearmanCallback(validation_data=([X_valid,X_valid_feature_eng] , y_valid_dataset))","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:11:15.246644Z","iopub.execute_input":"2022-02-20T13:11:15.247148Z","iopub.status.idle":"2022-02-20T13:11:15.257891Z","shell.execute_reply.started":"2022-02-20T13:11:15.247112Z","shell.execute_reply":"2022-02-20T13:11:15.25715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit([X_train, X_train_feature_eng], y_train_dataset,\n          epochs =100,\n          validation_data=([X_valid, X_valid_feature_eng],y_valid_dataset),\n                    callbacks=[custom_callback]\n         )","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:11:15.259354Z","iopub.execute_input":"2022-02-20T13:11:15.259681Z","iopub.status.idle":"2022-02-20T13:12:31.098072Z","shell.execute_reply.started":"2022-02-20T13:11:15.259644Z","shell.execute_reply":"2022-02-20T13:12:31.097257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(history.history).plot()\nplt.title(\"USE with basic Three features + Cosine distance features + 9 Meta features\")","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-02-20T13:12:31.099832Z","iopub.execute_input":"2022-02-20T13:12:31.100314Z","iopub.status.idle":"2022-02-20T13:12:31.407383Z","shell.execute_reply.started":"2022-02-20T13:12:31.100274Z","shell.execute_reply":"2022-02-20T13:12:31.406707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Observations:**\n\n* From the above experiment we have observe that USE model with 9 meta featues + all distance similarity features we were able to acheive the maximum score of **0.37149**. ","metadata":{}},{"cell_type":"code","source":"from prettytable import PrettyTable\n\n\nmyTable = PrettyTable([\"USE Model\", \"Features\", \"Spearman scroe\"])\n\n\nmyTable.add_row([\"USE\", \"Three basic features\", \"0.33029\"])\nmyTable.add_row([\"USE\", \"Three basic features + 9 Meta Features\", \"0.37133\"])\nmyTable.add_row([\"USE\", \"Three basic features + L2 distance feature + 9 meta Features\", \"0.36575\"])\nmyTable.add_row([\"USE\", \"Three basic features + cosine distance + 9 Meta features\", \"0.37153\"])\nmyTable.add_row([\"USE\", \"Three basic features + L2 distance +cosine distance + 9 Meta features\", \"0.37061\"])\n\n\nprint(myTable)","metadata":{"execution":{"iopub.status.busy":"2022-02-20T13:27:31.655773Z","iopub.execute_input":"2022-02-20T13:27:31.656042Z","iopub.status.idle":"2022-02-20T13:27:31.662779Z","shell.execute_reply.started":"2022-02-20T13:27:31.656006Z","shell.execute_reply":"2022-02-20T13:27:31.662113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **USE Model Observations:**\n* Training time for USE model is very less. \n* It take more epochs to reach the best score.\n* By performing all the above USE experiment model we are able to acheive the maximum spearman score of **0.3712** with both cosine + L2 distance + .\n","metadata":{}},{"cell_type":"markdown","source":"# Over all Experiments insights","metadata":{}},{"cell_type":"code","source":"from prettytable import PrettyTable\n\n\nmyTable = PrettyTable([\"USE Model\", \"Features\", \"Spearman scroe\"])\n\n\nmyTable.add_row([\"Bi-LSTM\", \"Three basic features\", \"0.27561\"])\nmyTable.add_row([\"Bi-LSTM\", \"Three basic features + 18 FE Features(meta, TF-IDF, Web scraping)\", \"0.00253\"])\nmyTable.add_row([\"Bi-LSTM\", \"Three basic features + 13 FE Features(meta, Web scraping)\", \"0.01255\"])\nmyTable.add_row([\"Bi-LSTM\", \"Three basic features + 9 FE Features(meta features)\", \"0.28656\"])\nmyTable.add_row([\"Bi-LSTM\", \"Three basic features + 13 FE features with 100 dim embeddings(meta, Web scraping)\", \"-0.0041\"])\nmyTable.add_row([\"USE\", \"Three basic features\", \"0.33029\"])\nmyTable.add_row([\"USE\", \"Three basic features + 9 Meta Features\", \"0.37133\"])\nmyTable.add_row([\"USE\", \"Three basic features + L2 distance feature + 9 meta Features\", \"0.36575\"])\nmyTable.add_row([\"USE\", \"Three basic features + cosine distance + 9 Meta features\", \"0.37153\"])\nmyTable.add_row([\"USE\", \"Three basic features + L2 distance +cosine distance + 9 Meta features\", \"0.37061\"])\n\n\nprint(myTable)","metadata":{"execution":{"iopub.status.busy":"2022-02-20T13:29:53.854232Z","iopub.execute_input":"2022-02-20T13:29:53.854487Z","iopub.status.idle":"2022-02-20T13:29:53.864398Z","shell.execute_reply.started":"2022-02-20T13:29:53.85446Z","shell.execute_reply":"2022-02-20T13:29:53.863531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"References use for USE: \n* https://www.kaggle.com/abazdyrev/use-features-oof\n* https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwiq-eGqsI72AhXFb94KHRmZDOAQFnoECAIQAQ&url=https%3A%2F%2Fmedium.com%2Fanalytics-vidhya%2Fgoogle-quest-challenge-q-a-labelling-9df4aff317d5&usg=AOvVaw2bN-Rqi5hBouX0fIdLHXVc\n* https://www.kaggle.com/abhishek/distilbert-use-features-oof","metadata":{}},{"cell_type":"markdown","source":"> # Note: Further experiments will be performed using transformer based model like bert, roberta, albert, XLnet etc ","metadata":{}}]}