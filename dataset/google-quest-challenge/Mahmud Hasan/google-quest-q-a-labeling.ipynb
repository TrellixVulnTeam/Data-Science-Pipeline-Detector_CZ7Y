{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\nimport sys\nimport glob\nimport torch\nimport transformers\nimport math\n\nimport re\nimport gc\nimport pickle  \nimport random\nimport string\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import stats\n\nfrom scipy.stats import spearmanr, rankdata\nfrom os.path import join as path_join\nfrom numpy.random import seed\nfrom urllib.parse import urlparse\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import KFold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seed(42)\nrandom.seed(42)\n\n\nimport nltk\nfrom nltk.corpus import stopwords","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import clone\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.preprocessing import StandardScaler, PowerTransformer, OneHotEncoder, RobustScaler, KBinsDiscretizer, QuantileTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import StratifiedKFold, GridSearchCV, KFold, GroupKFold\nfrom sklearn.multioutput import MultiOutputRegressor\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import make_scorer\n\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, HuberRegressor, RANSACRegressor\nfrom sklearn.svm import LinearSVR, SVR\nfrom sklearn.ensemble import ExtraTreesRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eng_stopwords = set(stopwords.words(\"english\"))\n\nimport tensorflow as tf\nimport tensorflow_hub as hub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#data_dir = 'E:/Prabhkirat/Python/google-quest-challenge/'\ndata_dir = '/kaggle/input/google-quest-challenge/'\nmetas_dir = ''\nsub_dir = ''\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RANDOM_STATE = 42\n\nimport datetime\ntodate = datetime.date.today().strftime(\"%m%d\")\n\nprint(todate)\nnfolds = 5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# count words\ndef word_count(xstring):\n    return xstring.split().str.len()\n\ndef spearman_corr(y_true, y_pred):\n        if np.ndim(y_pred) == 2:\n            corr = np.mean([stats.spearmanr(y_true[:, i], y_pred[:, i])[0] for i in range(y_true.shape[1])])\n        else:\n            corr = stats.spearmanr(y_true, y_pred)[0]\n        return corr\n    \ncustom_scorer = make_scorer(spearman_corr, greater_is_better=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def chunks(l, n):\n    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n    for i in range(0, len(l), n):\n        yield l[i:i + n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fetch_vectors(string_list, batch_size=64):\n    DEVICE = torch.device(\"cuda\")\n    tokenizer = transformers.DistilBertTokenizer.from_pretrained(\"../input/distilbertbaseuncased/\")\n    model = transformers.DistilBertModel.from_pretrained(\"../input/distilbertbaseuncased/\")\n    model.to(DEVICE)\n\n    fin_features = []\n    for data in chunks(string_list, batch_size):\n        tokenized = []\n        for x in data:\n            x = \" \".join(x.strip().split()[:300])\n            tok = tokenizer.encode(x, add_special_tokens=True)\n            tokenized.append(tok[:512])\n\n        max_len = 512\n        padded = np.array([i + [0] * (max_len - len(i)) for i in tokenized])\n        attention_mask = np.where(padded != 0, 1, 0)\n        input_ids = torch.tensor(padded).to(DEVICE)\n        attention_mask = torch.tensor(attention_mask).to(DEVICE)\n\n        with torch.no_grad():\n            last_hidden_states = model(input_ids, attention_mask=attention_mask)\n\n        features = last_hidden_states[0][:, 0, :].cpu().numpy()\n        fin_features.append(features)\n\n    fin_features = np.vstack(fin_features)\n    return fin_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nxtrain = pd.read_csv(data_dir + 'train.csv')\nxtest = pd.read_csv(data_dir + 'test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntarget_cols = ['question_asker_intent_understanding', 'question_body_critical', \n               'question_conversational', 'question_expect_short_answer', \n               'question_fact_seeking', 'question_has_commonly_accepted_answer', \n               'question_interestingness_others', 'question_interestingness_self', \n               'question_multi_intent', 'question_not_really_a_question', \n               'question_opinion_seeking', 'question_type_choice', \n               'question_type_compare', 'question_type_consequence', \n               'question_type_definition', 'question_type_entity', \n               'question_type_instructions', 'question_type_procedure', \n               'question_type_reason_explanation', 'question_type_spelling', \n               'question_well_written', 'answer_helpful', \n               'answer_level_of_information', 'answer_plausible', \n               'answer_relevance', 'answer_satisfaction', \n               'answer_type_instructions', 'answer_type_procedure', \n               'answer_type_reason_explanation', 'answer_well_written']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for colname in ['question_title', 'question_body', 'answer']:\n    newname = colname + '_word_len'\n    xtrain[newname] = xtrain[colname].str.split().str.len()\n    xtest[newname] = xtest[colname].str.split().str.len()\n    #print(xtrain[newname])\ndel newname, colname","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for colname in ['question', 'answer']:\n\n    # check for nonames, i.e. users with logins like user12389\n    xtrain['is_'+colname+'_no_name_user'] = xtrain[colname +'_user_name'].str.contains('^user\\d+$') + 0\n    xtest['is_'+colname+'_no_name_user'] = xtest[colname +'_user_name'].str.contains('^user\\d+$') + 0\n    \n\ncolname = 'answer'\n# check lexical diversity (unique words count vs total )\nxtrain[colname+'_div'] = xtrain[colname].apply(lambda s: len(set(s.split())) / len(s.split()) )\nxtest[colname+'_div'] = xtest[colname].apply(lambda s: len(set(s.split())) / len(s.split()) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## domain components\nxtrain['domcom'] = xtrain['question_user_page'].apply(lambda s: s.split('://')[1].split('/')[0].split('.'))\nxtest['domcom'] = xtest['question_user_page'].apply(lambda s: s.split('://')[1].split('/')[0].split('.'))\nprint(xtrain['domcom'])\n# count components\nxtrain['dom_cnt'] = xtrain['domcom'].apply(lambda s: len(s))\nxtest['dom_cnt'] = xtest['domcom'].apply(lambda s: len(s))\nprint(xtrain['dom_cnt'])\n# extend length\nxtrain['domcom'] = xtrain['domcom'].apply(lambda s: s + ['none', 'none'])\nxtest['domcom'] = xtest['domcom'].apply(lambda s: s + ['none', 'none'])\nprint(xtrain['domcom'])\n# components\nfor ii in range(0,4):\n    xtrain['dom_'+str(ii)] = xtrain['domcom'].apply(lambda s: s[ii])\n    xtest['dom_'+str(ii)] = xtest['domcom'].apply(lambda s: s[ii])\n    print(xtrain['dom_'+str(ii)])\n# clean up\nxtrain.drop('domcom', axis = 1, inplace = True)\nxtest.drop('domcom', axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# shared elements\nxtrain['q_words'] = xtrain['question_body'].apply(lambda s: [f for f in s.split() if f not in eng_stopwords] )\nprint(xtrain['q_words'])\nxtrain['a_words'] = xtrain['answer'].apply(lambda s: [f for f in s.split() if f not in eng_stopwords] )\nxtrain['qa_word_overlap'] = xtrain.apply(lambda s: len(np.intersect1d(s['q_words'], s['a_words'])), axis = 1)\nxtrain['qa_word_overlap_norm1'] = xtrain.apply(lambda s: s['qa_word_overlap']/(1 + len(s['a_words'])), axis = 1)\nxtrain['qa_word_overlap_norm2'] = xtrain.apply(lambda s: s['qa_word_overlap']/(1 + len(s['q_words'])), axis = 1)\nxtrain.drop(['q_words', 'a_words'], axis = 1, inplace = True)\n\nxtest['q_words'] = xtest['question_body'].apply(lambda s: [f for f in s.split() if f not in eng_stopwords] )\nxtest['a_words'] = xtest['answer'].apply(lambda s: [f for f in s.split() if f not in eng_stopwords] )\nxtest['qa_word_overlap'] = xtest.apply(lambda s: len(np.intersect1d(s['q_words'], s['a_words'])), axis = 1)\nxtest['qa_word_overlap_norm1'] = xtest.apply(lambda s: s['qa_word_overlap']/(1 + len(s['a_words'])), axis = 1)\nxtest['qa_word_overlap_norm2'] = xtest.apply(lambda s: s['qa_word_overlap']/(1 + len(s['q_words'])), axis = 1)\nxtest.drop(['q_words', 'a_words'], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Number of characters in the text ##\nxtrain[\"question_title_num_chars\"] = xtrain[\"question_title\"].apply(lambda x: len(str(x)))\nxtest[\"question_title_num_chars\"] = xtest[\"question_title\"].apply(lambda x: len(str(x)))\nxtrain[\"question_body_num_chars\"] = xtrain[\"question_body\"].apply(lambda x: len(str(x)))\nxtest[\"question_body_num_chars\"] = xtest[\"question_body\"].apply(lambda x: len(str(x)))\nxtrain[\"answer_num_chars\"] = xtrain[\"answer\"].apply(lambda x: len(str(x)))\nxtest[\"answer_num_chars\"] = xtest[\"answer\"].apply(lambda x: len(str(x)))\n\n## Number of stopwords in the text ##\nxtrain[\"question_title_num_stopwords\"] = xtrain[\"question_title\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\nxtest[\"question_title_num_stopwords\"] = xtest[\"question_title\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\nxtrain[\"question_body_num_stopwords\"] = xtrain[\"question_body\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\nxtest[\"question_body_num_stopwords\"] = xtest[\"question_body\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\nxtrain[\"answer_num_stopwords\"] = xtrain[\"answer\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\nxtest[\"answer_num_stopwords\"] = xtest[\"answer\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n\n## Number of punctuations in the text ##\nxtrain[\"question_title_num_punctuations\"] =xtrain['question_title'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\nxtest[\"question_title_num_punctuations\"] =xtest['question_title'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\nxtrain[\"question_body_num_punctuations\"] =xtrain['question_body'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\nxtest[\"question_body_num_punctuations\"] =xtest['question_body'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\nxtrain[\"answer_num_punctuations\"] =xtrain['answer'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\nxtest[\"answer_num_punctuations\"] =xtest['answer'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n\n## Number of title case words in the text ##\nxtrain[\"question_title_num_words_upper\"] = xtrain[\"question_title\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\nxtest[\"question_title_num_words_upper\"] = xtest[\"question_title\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\nxtrain[\"question_body_num_words_upper\"] = xtrain[\"question_body\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\nxtest[\"question_body_num_words_upper\"] = xtest[\"question_body\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\nxtrain[\"answer_num_words_upper\"] = xtrain[\"answer\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\nxtest[\"answer_num_words_upper\"] = xtest[\"answer\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embeddings_train = {}\nembeddings_test = {}\nfor text in ['question_title', 'question_body', 'answer']:\n    train_text = xtrain[text].str.replace('?', '.').str.replace('!', '.').tolist()\n    test_text = xtest[text].str.replace('?', '.').str.replace('!', '.').tolist()\n    \n    curr_train_emb = []\n    curr_test_emb = []\n    batch_size = 4\n    ind = 0\n    while ind*batch_size < len(train_text):\n        curr_train_emb.append(embed(train_text[ind*batch_size: (ind + 1)*batch_size])[\"outputs\"].numpy())\n        ind += 1\n        \n    ind = 0\n    while ind*batch_size < len(test_text):\n        curr_test_emb.append(embed(test_text[ind*batch_size: (ind + 1)*batch_size])[\"outputs\"].numpy())\n        ind += 1    \n        \n    embeddings_train[text + '_embedding'] = np.vstack(curr_train_emb)\n    embeddings_test[text + '_embedding'] = np.vstack(curr_test_emb)\n\n    print(text)\n    \ndel embed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"limit_char = 5000\nlimit_word = 25000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = xtrain.loc[:, ~xtrain.columns.isin(target_cols)].columns.tolist()\nX = xtrain[cols]\ny = xtrain[target_cols].values\nprint(X.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"title_col = 'question_title'\ntitle_transformer = Pipeline([\n    ('tfidf', TfidfVectorizer(lowercase = False, max_df = 0.3, min_df = 1,\n                             binary = False, use_idf = True, smooth_idf = False,\n                             ngram_range = (1,2), stop_words = 'english', \n                             token_pattern = '(?u)\\\\b\\\\w+\\\\b' , max_features = limit_word ))\n])\n        \ntitle_transformer2 = Pipeline([\n ('tfidf2',  TfidfVectorizer( sublinear_tf=True,\n    strip_accents='unicode', analyzer='char',\n    stop_words='english', ngram_range=(1, 4), max_features= limit_char))   \n])\n\n\nbody_col = 'question_body'\nbody_transformer = Pipeline([\n    ('tfidf',TfidfVectorizer(lowercase = False, max_df = 0.3, min_df = 1,\n                             binary = False, use_idf = True, smooth_idf = False,\n                             ngram_range = (1,2), stop_words = 'english', \n                             token_pattern = '(?u)\\\\b\\\\w+\\\\b' , max_features = limit_word ))\n])\n\n\nbody_transformer2 = Pipeline([\n ('tfidf2',  TfidfVectorizer( sublinear_tf=True,\n    strip_accents='unicode', analyzer='char',\n    stop_words='english', ngram_range=(1, 4), max_features= limit_char))   \n])\n\nanswer_col = 'answer'\n\nanswer_transformer = Pipeline([\n    ('tfidf', TfidfVectorizer(lowercase = False, max_df = 0.3, min_df = 1,\n                             binary = False, use_idf = True, smooth_idf = False,\n                             ngram_range = (1,2), stop_words = 'english', \n                             token_pattern = '(?u)\\\\b\\\\w+\\\\b' , max_features = limit_word ))\n])\n\nanswer_transformer2 = Pipeline([\n ('tfidf2',  TfidfVectorizer( sublinear_tf=True,\n    strip_accents='unicode', analyzer='char',\n    stop_words='english', ngram_range=(1, 4), max_features= limit_char))   \n])\n\nnum_cols = [\n    'question_title_word_len', 'question_body_word_len', 'answer_word_len', 'answer_div',\n    'question_title_num_chars','question_body_num_chars','answer_num_chars',\n    'question_title_num_stopwords','question_body_num_stopwords','answer_num_stopwords',\n    'question_title_num_punctuations','question_body_num_punctuations','answer_num_punctuations',\n    'question_title_num_words_upper','question_body_num_words_upper','answer_num_words_upper'\n]\n\nnum_transformer = Pipeline([\n    ('impute', SimpleImputer(strategy='constant', fill_value=0)),\n    ('scale', PowerTransformer(method='yeo-johnson'))\n])\n\n\ncat_cols = [\n    'dom_0', \n    'dom_1', \n    'dom_2', \n    'dom_3',     \n    'category', \n    'is_question_no_name_user',\n    'is_answer_no_name_user',\n    'dom_cnt'\n]\n\ncat_transformer = Pipeline([\n    ('impute', SimpleImputer(strategy='constant', fill_value='')),\n    ('encode', OneHotEncoder(handle_unknown='ignore'))\n])\n\n\npreprocessor = ColumnTransformer(\n    transformers = [\n        ('title', title_transformer, title_col),\n        ('title2', title_transformer2, title_col),\n        ('body', body_transformer, body_col),\n        ('body2', body_transformer2, body_col),\n        ('answer', answer_transformer, answer_col),\n        ('answer2', answer_transformer2, answer_col),\n        ('num', num_transformer, num_cols),\n        ('cat', cat_transformer, cat_cols)\n    ]\n)\n\npipeline = Pipeline([\n    ('preprocessor', preprocessor),\n    ('estimator',Ridge(random_state=RANDOM_STATE))\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vector_as = np.zeros((y.shape[1],1))\n\n\ncv = KFold(n_splits = nfolds, shuffle=True, random_state=RANDOM_STATE)\n\nparam_grid = {\n    'estimator': [\n        Ridge(random_state=RANDOM_STATE),\n        \n    ],\n    'estimator__alpha': [1, 5, 20],\n    \n    'preprocessor__title__tfidf__lowercase': [False],\n    'preprocessor__title__tfidf__max_df': [0.3],\n    'preprocessor__title__tfidf__min_df': [1],\n    'preprocessor__title__tfidf__binary': [False],\n    'preprocessor__title__tfidf__use_idf': [True],\n    'preprocessor__title__tfidf__smooth_idf': [False],\n    'preprocessor__title__tfidf__sublinear_tf': [False],\n    'preprocessor__title__tfidf__ngram_range': [(1, 2)], \n    'preprocessor__title__tfidf__stop_words': ['english'],\n    'preprocessor__title__tfidf__token_pattern': ['(?u)\\\\b\\\\w+\\\\b'],\n    \n    'preprocessor__body__tfidf__lowercase': [False],\n    'preprocessor__body__tfidf__max_df': [0.3],\n    'preprocessor__body__tfidf__min_df': [1],\n    'preprocessor__body__tfidf__binary': [False],\n    'preprocessor__body__tfidf__use_idf': [False],\n    'preprocessor__body__tfidf__smooth_idf': [False],\n    'preprocessor__body__tfidf__sublinear_tf': [False],\n    'preprocessor__body__tfidf__ngram_range': [(1, 2)],\n    'preprocessor__body__tfidf__stop_words': ['english'],\n    'preprocessor__body__tfidf__token_pattern': ['(?u)\\\\b\\\\w+\\\\b'],\n\n    'preprocessor__num__impute__strategy': ['constant'],\n    'preprocessor__num__scale': [PowerTransformer()],\n    \n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_list = []\n\nfor ii in range(0, y.shape[1]):\n    \n    grid_search = GridSearchCV(pipeline, param_grid, scoring=custom_scorer, \n                           cv=cv, n_jobs=-1, refit=True, return_train_score=True, verbose=2)\n\n    grid_search.fit(X, y[:,ii])\n    \n    \n    grid_search.best_score_, grid_search.best_params_, grid_search.cv_results_\n    best_estimator = clone(grid_search.best_estimator_)\n    model_list.append(best_estimator)\n    vector_as[ii] = model_list[ii].steps[1][1].alpha","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pda = pd.DataFrame(vector_as)\npda.columns = ['alpha']\npda.to_csv('E:/Prabhkirat/Python/alphas_vector.csv', index = False)\n\nid_train = xtrain['qa_id']\nytrain = xtrain[target_cols]\nxtrain.drop(target_cols + ['qa_id'], axis = 1, inplace = True)\n\n\nid_test = xtest['qa_id'] \nxtest.drop('qa_id', axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dropcols = [f for f in xtrain.columns if f not in cat_cols and f not in num_cols and f not in title_col and f not in body_col and f not in answer_col]\n\nxtrain.drop(dropcols, axis = 1, inplace = True)\nxtest.drop(dropcols, axis = 1, inplace = True)\n\n\nmvalid = np.zeros((xtrain.shape[0], len(target_cols)))\nmfull = np.zeros((xtest.shape[0], len(target_cols)))\n\nkf = KFold(n_splits = nfolds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for train_index, test_index in kf.split(xtrain):\n    \n    print('---')\n    # split\n    x0, x1 = xtrain.loc[train_index], xtrain.loc[test_index]\n    y0, y1 = ytrain.loc[train_index], ytrain.loc[test_index]\n\n    for ii in range(0, ytrain.shape[1]):\n\n        # fit model\n        be = model_list[ii]\n        be.fit(x0, np.array(y0)[:,ii])\n\n        # park forecast\n        mvalid[test_index, ii] = be.predict(x1)\n        mfull[:,ii] += be.predict(xtest)/kf.n_splits\n        # print(stats.spearmanr(np.array(y1)[:,ii], mvalid[test_index, ii])[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corvec = np.zeros((ytrain.shape[1],1))\nfor ii in range(0, ytrain.shape[1]):\n    mvalid[:,ii] = rankdata(mvalid[:,ii])/mvalid.shape[0]\n    mfull[:,ii] = rankdata(mfull[:,ii])/mfull.shape[0]\n    \n    corvec[ii] = stats.spearmanr(ytrain[ytrain.columns[ii]], mvalid[:,ii])[0]\n\n    \nprint(corvec.mean())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prval = pd.DataFrame(mvalid)\nprval.columns = ytrain.columns\nprval['qa_id'] = id_train\nprval = prval[['qa_id'] + list(prval.columns[:-1])]\nprval.to_csv('E:/Prabhkirat/Python/'+ metas_dir + 'prval_ridge_'+todate+ '.csv', index = False)\n\nprfull = pd.DataFrame(mfull)\nprfull.columns = ytrain.columns\nprfull['qa_id'] = id_test\nprfull = prfull[['qa_id'] + list(prfull.columns[:-1])]\nprfull.to_csv('E:/Prabhkirat/Python/'+ metas_dir + 'prfull_ridge_'+todate+ '.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prfull.to_csv('E:/Prabhkirat/Python/'+ 'submission.csv', index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}