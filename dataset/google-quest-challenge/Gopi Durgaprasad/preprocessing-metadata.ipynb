{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nfrom tqdm import tqdm_notebook\nimport re\nfrom bs4 import BeautifulSoup\nimport os\nimport re\nimport gc\nimport sys\nimport time\nimport json\nimport random\nimport unicodedata\nimport multiprocessing\nfrom functools import partial, lru_cache\n\nimport emoji\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.externals import joblib\nfrom tqdm import tqdm, tqdm_notebook\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom nltk import TweetTokenizer\nfrom nltk.stem import PorterStemmer, SnowballStemmer\nfrom nltk.stem.lancaster import LancasterStemmer\nimport html\nfrom sklearn.feature_extraction.text import TfidfVectorizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import GroupKFold\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm_notebook as tqdm\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport os\nfrom scipy.stats import spearmanr\nfrom math import floor, ceil\nfrom transformers import *\n\nnp.set_printoptions(suppress=True)\nprint(tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"GOOGLE_PATH = \"../input/google-quest-challenge/\"\nSTACK_PATH = \"../input/stackexchange123/StackexchangeExtract/\"\n\ntrain = pd.read_csv(GOOGLE_PATH+\"train.csv\")\ntest = pd.read_csv(GOOGLE_PATH+\"test.csv\")\nsample_submission = pd.read_csv(GOOGLE_PATH+\"sample_submission.csv\")\n\ntrain.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_columns = sample_submission.columns[1:].values\ntarget_columns.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def url_id_ex(url):\n    try:\n        ids = int(url.split(\"/\")[-2])\n    except:\n        ids = int(url.split(\"/\")[-1])\n    return ids\n\n# category_type\ntrain[\"category_type\"] = train[\"url\"].apply(lambda x : x.split(\".\")[0].split(\"/\")[-1])\ntest[\"category_type\"] = test[\"url\"].apply(lambda x : x.split(\".\")[0].split(\"/\")[-1])\n\ntrain[\"quser_id\"] = train[\"question_user_page\"].apply(lambda x: int(x.split(\"/\")[-1]))\ntrain[\"auser_id\"] = train[\"answer_user_page\"].apply(lambda x: int(x.split(\"/\")[-1]))\ntrain[\"url_id\"] = train[\"url\"].apply(url_id_ex)\n\ntest[\"quser_id\"] = test[\"question_user_page\"].apply(lambda x: int(x.split(\"/\")[-1]))\ntest[\"auser_id\"] = test[\"answer_user_page\"].apply(lambda x: int(x.split(\"/\")[-1]))\ntest[\"url_id\"] = test[\"url\"].apply(url_id_ex)\n\ntrain.category_type.replace(\"programmers\", \"softwareengineering\", inplace=True)\ntest.category_type.replace(\"programmers\", \"softwareengineering\", inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def final_dataframe(files_path, df):\n    listofdir = list(os.listdir(files_path))\n    listofdir.remove('dataset-metadata.json')\n    final_list = []\n    \n    posts_columns = None\n    q_users_columns = None\n    a_users_columns = None\n    \n    for file in tqdm_notebook(listofdir):\n        temp_df = df[df.category_type == file]\n        \n        temp_users = pd.read_csv(STACK_PATH+file+\"/user_df.csv\")\n        temp_posts = pd.read_csv(STACK_PATH+file+\"/posts_df.csv\")\n        temp_users_columns = temp_users.columns.values\n        \n        posts_columns = temp_posts.columns.values\n        temp_df = pd.merge(temp_df, temp_posts, left_on=\"url_id\", right_on=\"Id\", how=\"left\")\n        \n        del temp_posts\n        \n        temp_users = temp_users.add_prefix(\"q_\")\n        q_users_columns = temp_users.columns.values\n        temp_df = pd.merge(temp_df, temp_users, left_on=\"quser_id\", right_on=\"q_Id\", how=\"left\")\n        \n        temp_users.columns = temp_users_columns\n        \n        temp_users = temp_users.add_prefix(\"a_\")\n        a_users_columns = temp_users.columns.values\n        temp_df = pd.merge(temp_df, temp_users, left_on=\"auser_id\", right_on=\"a_Id\", how=\"left\")\n        del temp_users\n        \n        temp_df = temp_df.to_dict(\"records\")\n        final_list.extend(temp_df)\n        del temp_df\n        \n    total_columns_dic = {\n        \"posts_columns\":posts_columns,\n        \"q_users_columns\":q_users_columns,\n        \"a_users_columns\":a_users_columns\n    }\n    \n    final_df = pd.DataFrame(final_list)\n    del final_list\n        \n    return final_df, total_columns_dic","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!cat /proc/meminfo | grep Mem","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfiles_path = STACK_PATH\ntrain_final, total_columns_dic = final_dataframe(files_path, train)\ntest_final, total_columns_dic = final_dataframe(files_path, test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_final.shape, test_final.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stackof_train = train[train.category == \"STACKOVERFLOW\"].copy()\nstackof_test = test[test.category == \"STACKOVERFLOW\"].copy()\nstackof_train.shape , stackof_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_final = train_final.append(stackof_train)\ntest_final = test_final.append(stackof_test)\ntrain_final.shape, test_final.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_columns = test.columns.values.tolist()\ntrain_columns = train.columns.values.tolist()\ntrain_final = pd.merge(train, train_final, left_on=train_columns, right_on=train_columns, how=\"left\")\ntest_final = pd.merge(test, test_final, left_on=test_columns, right_on=test_columns, how=\"left\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_final.shape , test_final.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!cat /proc/meminfo | grep Mem","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas.api.types import is_datetime64_any_dtype as is_datetime\nfrom pandas.api.types import is_categorical_dtype\n\ndef reduce_mem_usage(df, use_float16=False):\n    \"\"\"\n    Iterate through all the columns of a dataframe and modify the data type to reduce memory usage.\n    \"\"\"\n\n    start_mem = df.memory_usage().sum() / 1024**2\n    print(\"Memory usage of dataframe is {:.2f} MB\".format(start_mem))\n\n    for col in df.columns:\n        if is_datetime(df[col]) or is_categorical_dtype(df[col]):\n            continue\n        col_type = df[col].dtype\n\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if use_float16 and c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            #df[col] = df[col].astype(\"category\")\n            pass\n        \n    end_mem = df.memory_usage().sum() / 1024**2\n    print(\"Memory usage after optimization is: {:.2f} MB\".format(end_mem))\n    print(\"Decreased by {:.1f}%\".format(100 * (start_mem - end_mem) / start_mem))\n\n    return df\n\nfrom bs4 import BeautifulSoup\ndef removing_html_tags(raw_html):\n    cleantext = BeautifulSoup(raw_html, \"lxml\").text\n    return cleantext\n\ndef replace_urls(text):\n    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', ' web ', text, flags=re.MULTILINE)\n    return text\n\ndef clean_AboutMe(text):\n    text = removing_html_tags(text)\n    text = replace_urls(text)\n    return text\n\ndef clean_Name(text):\n    text = str(text)\n    text = re.sub(r' ', '_', text, flags=re.MULTILINE)\n    text = re.sub(r',',' ', text, flags=re.MULTILINE)\n    return text\n\ndef clean_Class(text):\n    text = str(text)\n    text = re.sub(r',', ' ',text, flags=re.MULTILINE)\n    return text\n\ndef log_transform_apply(value):\n    if value == 0:\n        return value\n    elif value < 1:\n        value = np.log1p(abs(value))\n        return -value\n    else:\n        return np.log1p(value)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_final_targets = train_final[target_columns].copy()\ntrain_final_targets[\"qa_id\"] = train_final[\"qa_id\"]\ntrain_final.drop(columns=target_columns, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_final = reduce_mem_usage(train_final)\ntest_final = reduce_mem_usage(test_final)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# User AboutMe\ntrain_final[\"q_AboutMe_nan\"] = train_final[\"q_AboutMe\"].isnull().astype(int)\ntrain_final[\"a_AboutMe_nan\"] = train_final[\"a_AboutMe\"].isnull().astype(int)   \n\ntest_final[\"q_AboutMe_nan\"] = test_final[\"q_AboutMe\"].isnull().astype(int)\ntest_final[\"a_AboutMe_nan\"] = test_final[\"a_AboutMe\"].isnull().astype(int) \n\ntrain_final[\"q_AboutMe\"].fillna(\"nane\", inplace=True)\ntrain_final[\"a_AboutMe\"].fillna(\"nane\", inplace=True)\n\ntest_final[\"q_AboutMe\"].fillna(\"nane\", inplace=True)\ntest_final[\"a_AboutMe\"].fillna(\"nane\", inplace=True)\n\ntrain_final[\"q_AboutMe\"] = train_final[\"q_AboutMe\"].apply(clean_AboutMe)\ntrain_final[\"a_AboutMe\"] = train_final[\"a_AboutMe\"].apply(clean_AboutMe)\n\ntest_final[\"q_AboutMe\"] = test_final[\"q_AboutMe\"].apply(clean_AboutMe)\ntest_final[\"a_AboutMe\"] = test_final[\"a_AboutMe\"].apply(clean_AboutMe)\n\n# User TagBased\n\ntrain_final[\"q_TagBased_nan\"] = train_final[\"q_TagBased\"].isnull().astype(int)\ntrain_final[\"a_TagBased_nan\"] = train_final[\"a_TagBased\"].isnull().astype(int)   \n\ntest_final[\"q_TagBased_nan\"] = test_final[\"q_TagBased\"].isnull().astype(int)\ntest_final[\"a_TagBased_nan\"] = test_final[\"a_TagBased\"].isnull().astype(int) \n\n\ntrain_final[\"q_TagBased\"].fillna(\"nane\", inplace=True)\ntrain_final[\"a_TagBased\"].fillna(\"nane\", inplace=True)\n\ntest_final[\"q_TagBased\"].fillna(\"nane\", inplace=True)\ntest_final[\"a_TagBased\"].fillna(\"nane\", inplace=True)\n\ntrain_final[\"q_TagBased\"] = train_final[\"q_TagBased\"].apply(clean_Class)\ntrain_final[\"a_TagBased\"] = train_final[\"a_TagBased\"].apply(clean_Class)\n\ntest_final[\"q_TagBased\"] = test_final[\"q_TagBased\"].apply(clean_Class)\ntest_final[\"a_TagBased\"] = test_final[\"a_TagBased\"].apply(clean_Class)\n\n# User Name\n\ntrain_final[\"q_Name_nan\"] = train_final[\"q_Name\"].isnull().astype(int)\ntrain_final[\"a_Name_nan\"] = train_final[\"a_Name\"].isnull().astype(int)   \n\ntest_final[\"q_Name_nan\"] = test_final[\"q_Name\"].isnull().astype(int)\ntest_final[\"a_Name_nan\"] = test_final[\"a_Name\"].isnull().astype(int) \n\n\ntrain_final[\"q_Name\"].fillna(\"nane\", inplace=True)\ntrain_final[\"a_Name\"].fillna(\"nane\", inplace=True)\n\ntest_final[\"q_Name\"].fillna(\"nane\", inplace=True)\ntest_final[\"a_Name\"].fillna(\"nane\", inplace=True)\n\ntrain_final[\"q_Name\"] = train_final[\"q_Name\"].apply(clean_Name)\ntrain_final[\"a_Name\"] = train_final[\"a_Name\"].apply(clean_Name)\n\ntest_final[\"q_Name\"] = test_final[\"q_Name\"].apply(clean_Name)\ntest_final[\"a_Name\"] = test_final[\"a_Name\"].apply(clean_Name)\n\n# User Class\n\ntrain_final[\"q_Class\"].fillna(\"0\", inplace=True)\ntrain_final[\"a_Class\"].fillna(\"0\", inplace=True)\n\ntest_final[\"q_Class\"].fillna(\"0\", inplace=True)\ntest_final[\"a_Class\"].fillna(\"0\", inplace=True)\n\ntrain_final[\"q_Class\"] = train_final[\"q_Class\"].apply(clean_Class)\ntrain_final[\"a_Class\"] = train_final[\"a_Class\"].apply(clean_Class)\n\ntest_final[\"q_Class\"] = test_final[\"q_Class\"].apply(clean_Class)\ntest_final[\"a_Class\"] = test_final[\"a_Class\"].apply(clean_Class)\n\n# User Views\ntrain_final[\"q_Views_nan\"] = train_final[\"q_Views\"].isnull().astype(int)\ntrain_final[\"a_Views_nan\"] = train_final[\"a_Views\"].isnull().astype(int)   \n\ntest_final[\"q_Views_nan\"] = test_final[\"q_Views\"].isnull().astype(int)\ntest_final[\"a_Views_nan\"] = test_final[\"a_Views\"].isnull().astype(int) \n\ntrain_final[\"q_Views\"].fillna(0, inplace=True)\ntrain_final[\"a_Views\"].fillna(0, inplace=True)\n\ntest_final[\"q_Views\"].fillna(0, inplace=True)\ntest_final[\"a_Views\"].fillna(0, inplace=True)\n\n# User UpVotes\n#train_final[\"q_Views_nan\"] = train_final[\"q_Views\"].isnull().astype(int)\n#train_final[\"a_Views_nan\"] = train_final[\"a_Views\"].isnull().astype(int)   \n\n#test_final[\"q_Views_nan\"] = test_final[\"q_Views\"].isnull().astype(int)\n#test_final[\"a_Views_nan\"] = test_final[\"a_Views\"].isnull().astype(int)\n\n# User UpVotes\n\ntrain_final[\"q_UpVotes_nan\"] = train_final[\"q_UpVotes\"].isnull().astype(int)\ntrain_final[\"a_UpVotes_nan\"] = train_final[\"a_UpVotes\"].isnull().astype(int)   \n\ntest_final[\"q_UpVotes_nan\"] = test_final[\"q_UpVotes\"].isnull().astype(int)\ntest_final[\"a_UpVotes_nan\"] = test_final[\"a_UpVotes\"].isnull().astype(int) \n\ntrain_final[\"q_UpVotes\"].fillna(0, inplace=True)\ntrain_final[\"a_UpVotes\"].fillna(0, inplace=True)\n\ntest_final[\"q_UpVotes\"].fillna(0, inplace=True)\ntest_final[\"a_UpVotes\"].fillna(0, inplace=True)\n\n# User DownVotes\ntrain_final[\"q_DownVotes_nan\"] = train_final[\"q_DownVotes\"].isnull().astype(int)\ntrain_final[\"a_DownVotes_nan\"] = train_final[\"a_DownVotes\"].isnull().astype(int)   \n\ntest_final[\"q_DownVotes_nan\"] = test_final[\"q_DownVotes\"].isnull().astype(int)\ntest_final[\"a_DownVotes_nan\"] = test_final[\"a_DownVotes\"].isnull().astype(int) \n\ntrain_final[\"q_DownVotes\"].fillna(0, inplace=True)\ntrain_final[\"a_DownVotes\"].fillna(0, inplace=True)\n\ntest_final[\"q_DownVotes\"].fillna(0, inplace=True)\ntest_final[\"a_DownVotes\"].fillna(0, inplace=True)\n\nuser_drop_cols = [\"q_Id\", \"q_DisplayName\", \"q_UserId\", \"a_Id\", \"a_DisplayName\", \"a_UserId\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_Tags(text):\n    text = str(text)\n    text = re.sub(r'><', '> <', text, flags=re.MULTILINE)\n    text = re.sub(r'>', '', text, flags=re.MULTILINE)\n    text = re.sub(r'<', '', text, flags=re.MULTILINE)\n    text = ''.join([i for i in text if not i.isdigit()])\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Posts PostTypeId\ntrain_final[\"PostTypeId_nan\"] = train_final[\"PostTypeId\"].isnull().astype(int)\ntest_final[\"PostTypeId_nan\"] = test_final[\"PostTypeId\"].isnull().astype(int) \n\ntrain_final[\"PostTypeId\"].fillna(1.0, inplace=True)\ntest_final[\"PostTypeId\"].fillna(1.0, inplace=True)\n\n# Posts Score\ntrain_final[\"Score_nan\"] = train_final[\"Score\"].isnull().astype(int)\ntest_final[\"Score_nan\"] = test_final[\"Score\"].isnull().astype(int) \n\ntrain_final[\"Score\"].fillna(0, inplace=True)\ntest_final[\"Score\"].fillna(0, inplace=True)\n\ntrain_final[\"Score\"] = train_final[\"Score\"].apply(log_transform_apply)\ntest_final[\"Score\"] = test_final[\"Score\"].apply(log_transform_apply)\n\n# Posts ViewCount\ntrain_final[\"ViewCount_nan\"] = train_final[\"ViewCount\"].isnull().astype(int)\ntest_final[\"ViewCount_nan\"] = test_final[\"ViewCount\"].isnull().astype(int) \n\ntrain_final[\"ViewCount\"].fillna(0, inplace=True)\ntest_final[\"ViewCount\"].fillna(0, inplace=True)\n\ntrain_final[\"ViewCount\"] = np.log1p(abs(train_final[\"ViewCount\"]))\ntest_final[\"ViewCount\"] = np.log1p(abs(test_final[\"ViewCount\"]))\n\n# Posts Tags\ntrain_final[\"Tags_nan\"] = train_final[\"Tags\"].isnull().astype(int)\ntest_final[\"Tags_nan\"] = test_final[\"Tags\"].isnull().astype(int) \n\ntrain_final[\"Tags\"].fillna(\"<nanetag>\", inplace=True)\ntest_final[\"Tags\"].fillna(\"<nanetag>\", inplace=True)\n\ntrain_final[\"Tags\"] = train_final[\"Tags\"].apply(clean_Tags)\ntest_final[\"Tags\"] = test_final[\"Tags\"].apply(clean_Tags)\n\n\n# Posts AnswerCount\ntrain_final[\"AnswerCount_nan\"] = train_final[\"AnswerCount\"].isnull().astype(int)\ntest_final[\"AnswerCount_nan\"] = test_final[\"AnswerCount\"].isnull().astype(int) \n\ntrain_final[\"AnswerCount\"].fillna(1, inplace=True)\ntest_final[\"AnswerCount\"].fillna(1, inplace=True)\n\n\n# Posts CommentCount\ntrain_final[\"CommentCount_nan\"] = train_final[\"CommentCount\"].isnull().astype(int)\ntest_final[\"CommentCount_nan\"] = test_final[\"CommentCount\"].isnull().astype(int) \n\ntrain_final[\"CommentCount\"].fillna(0, inplace=True)\ntest_final[\"CommentCount\"].fillna(0, inplace=True)\n\n# Posts FavoriteCount\ntrain_final[\"FavoriteCount_nan\"] = train_final[\"FavoriteCount\"].isnull().astype(int)\ntest_final[\"FavoriteCount_nan\"] = test_final[\"FavoriteCount\"].isnull().astype(int) \n\ntrain_final[\"FavoriteCount\"].fillna(0, inplace=True)\ntest_final[\"FavoriteCount\"].fillna(0, inplace=True)\n\ntrain_final[\"FavoriteCount\"] = train_final[\"FavoriteCount\"].apply(log_transform_apply)\ntest_final[\"FavoriteCount\"] = test_final[\"FavoriteCount\"].apply(log_transform_apply)\n\n\nposts_drop_cols = ['Id', 'AcceptedAnswerId', 'OwnerUserId', 'LastActivityDate', 'ParentId', 'ClosedDate', 'LastEditorDisplayName', 'OwnerDisplayName', 'CommunityOwnedDate']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_code_html(text, body):\n    if text == np.nan:\n        body = str(body)\n        code_list = []\n        codes_list1 = re.findall(':\\n\\n.*?\\n\\n\\n',body, flags=re.DOTALL)\n        codes_list2 = re.findall('.\\n\\n(.*?)\\n\\n\\n',body, flags=re.DOTALL)\n        codes_list3 = re.findall('{(.*?)}',body, flags=re.DOTALL)\n        code_list.extend(codes_list1)\n        code_list.extend(codes_list2)\n        code_list.extend(codes_list3)\n        if len(codes_list) > 0:\n            code = '<#next#>'.join(map(str, codes_list))\n            return code\n        else:\n            return \"NONE\"\n    else:\n        text = str(text)\n        codes_list = re.findall('<code>(.*?)</code>',text, flags=re.DOTALL)\n        if len(codes_list) > 0:\n            code = '<#next#>'.join(map(str, codes_list))\n            return code\n        else:\n            return \"NONE\"\n    \ndef get_code_replace(text, code):\n    text = str(text)\n    code = str(code)\n    if code != \"NONE\":\n        codes_list = code.split(\"<#next#>\")\n        codes_list = sorted(codes_list, key=len,reverse=True)\n        for i in codes_list:\n            i = re.escape(i)\n            text = re.sub(f\"{i}\", ' [CODE] ', text, flags=re.DOTALL)\n        return text\n    else:\n        return text      \n    \ndef get_blockquote_html(text):\n    text = str(text)\n    codes_list = re.findall('<blockquote>(.*?)</blockquote>',text, flags=re.DOTALL)\n    if len(codes_list) > 0:\n        code = ' '.join(map(str, codes_list))\n        return code\n    else:\n        return \"NONE\"\n    \ndef get_slsldolel(text):\n    text = str(text)\n    codes_list = re.findall('\\\\\\\\\\$(.*?)\\\\\\\\\\$',text, flags=re.DOTALL)\n    if len(codes_list) > 0:\n        code = ' '.join(map(str, codes_list))\n        return code\n    else:\n        return \"NONE\"\n\ndef get_slsldolel_replace(text):\n    text = str(text)\n    codes_list = re.findall('\\\\\\\\\\$.*?\\\\\\\\\\$',text, flags=re.DOTALL)\n    if len(codes_list) > 0:\n        #code = ' '.join(map(str, codes_list))\n        for i in codes_list:\n            i = re.escape(i)\n            text = re.sub(f\"{i}\", ' [FORMULA] ', text)\n        return text\n    else:\n        return text\n\ndef get_doldol(text):\n    text = str(text)\n    codes_list = re.findall('\\$\\$(.*?)\\$\\$',text, flags=re.DOTALL)\n    if len(codes_list) > 0:\n        code = ' '.join(map(str, codes_list))\n        return code\n    else:\n        return \"NONE\"\n\ndef get_doldol_replace(text):\n    text = str(text)\n    codes_list = re.findall('\\$\\$.*?\\$\\$',text, flags=re.DOTALL)\n    if len(codes_list) > 0:\n        #code = ' '.join(map(str, codes_list))\n        for i in codes_list:\n            i = re.escape(i)\n            text = re.sub(f\"{i}\", ' [FORMULA] ', text)\n        return text\n    else:\n        return text\n\ndef get_spdol(text):\n    text = str(text)\n    codes_list = re.findall(' \\$(.*?) \\$',text, flags=re.DOTALL)\n    if len(codes_list) > 0:\n        code = ' '.join(map(str, codes_list))\n        return code\n    else:\n        return \"NONE\"\n    \ndef get_spdol_replace(text):\n    text = str(text)\n    codes_list = re.findall(' \\$.*? \\$',text, flags=re.DOTALL)\n    if len(codes_list) > 0:\n        #code = ' '.join(map(str, codes_list))\n        for i in codes_list:\n            i = re.escape(i)\n            text = re.sub(f\"{i}\", ' [FORMULA] ', text)\n        return text\n    else:\n        return text\n\ndef get_dol(text):\n    text = str(text)\n    codes_list = re.findall('\\$(.*?)\\$',text, flags=re.DOTALL)\n    if len(codes_list) > 0:\n        code = ' '.join(map(str, codes_list))\n        return code\n    else:\n        return \"NONE\"\n    \ndef get_dol_replace(text):\n    text = str(text)\n    codes_list = re.findall('\\$.*?\\$',text, flags=re.DOTALL)\n    if len(codes_list) > 0:\n        #code = ' '.join(map(str, codes_list))\n        for i in codes_list:\n            i = re.escape(i)\n            text = re.sub(f\"{i}\", ' [FORMULA] ', text)\n        return text\n    else:\n        return text\n\ndef get_code1(text):\n    text = str(text)\n    codes_list = re.findall(':\\n\\n(.*?)\\n\\n\\n',text, flags=re.DOTALL)\n    if len(codes_list) > 0:\n        code = ' . '.join(map(str, codes_list))\n        return code\n    else:\n        return \"NONE\"\n\ndef get_code1_replace(text):\n    text = str(text)\n    codes_list = re.findall(':\\n\\n.*?\\n\\n\\n',text, flags=re.DOTALL)\n    if len(codes_list) > 0:\n        #code = ' '.join(map(str, codes_list))\n        for i in codes_list:\n            i = re.escape(i)\n            text = re.sub(f\"{i}\", ' [CODE] ', text)\n        return text\n    else:\n        return text\n\ndef get_code2(text):\n    text = str(text)\n    codes_list = re.findall('.\\n\\n(.*?)\\n\\n\\n',text, flags=re.DOTALL)\n    if len(codes_list) > 0:\n        code = ' . '.join(map(str, codes_list))\n        return code\n    else:\n        return \"NONE\"\n\ndef get_code2_replace(text):\n    text = str(text)\n    codes_list = re.findall('.\\n\\n.*?\\n\\n\\n',text, flags=re.DOTALL)\n    if len(codes_list) > 0:\n        #code = ' '.join(map(str, codes_list))\n        for i in codes_list:\n            if i.count(\"\\n\") > 4:\n                i = re.escape(i)\n                text = re.sub(f\"{i}\", ' [CODE] ', text)\n            else:\n                pass\n        return text\n    else:\n        return text\n    \ndef get_code3(text):\n    text = str(text)\n    codes_list = re.findall('{(.*?)}',text, flags=re.DOTALL)\n    if len(codes_list) > 0:\n        code = ' . '.join(map(str, codes_list))\n        return code\n    else:\n        return \"NONE\"\n\ndef get_code3_replace(text):\n    text = str(text)\n    codes_list = re.findall('{.*?}',text, flags=re.DOTALL)\n    if len(codes_list) > 0:\n        #code = ' '.join(map(str, codes_list))\n        for i in codes_list:\n            if len(i) > 10:\n                i = re.escape(i)\n                text = re.sub(f\"{i}\", ' [CODE] ', text)\n            else:\n                pass\n        return text\n    else:\n        return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# question_body_code\ntrain_final[\"question_body_code\"] = train_final.apply(lambda x: get_code_html(x[\"Body\"], x[\"question_body\"]), axis=1)\ntrain_final[\"question_body_clean\"] = train_final.apply(lambda x: get_code_replace(x[\"question_body\"], x[\"question_body_code\"]), axis=1)\n\ntest_final[\"question_body_code\"] = test_final.apply(lambda x: get_code_html(x[\"Body\"], x[\"question_body\"]), axis=1)\ntest_final[\"question_body_clean\"] = test_final.apply(lambda x: get_code_replace(x[\"question_body\"], x[\"question_body_code\"]), axis=1)\n\n# question_body_slsldolel\ntrain_final[\"question_body_slsldolel\"] = train_final[\"question_body\"].apply(get_slsldolel)\ntrain_final[\"question_body_clean\"] = train_final[\"question_body_clean\"].apply(get_slsldolel_replace)\n\ntest_final[\"question_body_slsldolel\"] = test_final[\"question_body\"].apply(get_slsldolel)\ntest_final[\"question_body_clean\"] = test_final[\"question_body_clean\"].apply(get_slsldolel_replace)\n\n# question_body_doldol\ntrain_final[\"question_body_doldol\"] = train_final[\"question_body\"].apply(get_doldol)\ntrain_final[\"question_body_clean\"] = train_final[\"question_body_clean\"].apply(get_doldol_replace)\n\ntest_final[\"question_body_doldol\"] = test_final[\"question_body\"].apply(get_doldol)\ntest_final[\"question_body_clean\"] = test_final[\"question_body_clean\"].apply(get_doldol_replace)\n\n# question_body_spdol\ntrain_final[\"question_body_spdol\"] = train_final[\"question_body\"].apply(get_spdol)\ntrain_final[\"question_body_clean\"] = train_final[\"question_body_clean\"].apply(get_spdol_replace)\n\ntest_final[\"question_body_spdol\"] = test_final[\"question_body\"].apply(get_spdol)\ntest_final[\"question_body_clean\"] = test_final[\"question_body_clean\"].apply(get_spdol_replace)\n\n# question_body_dol\ntrain_final[\"question_body_dol\"] = train_final[\"question_body\"].apply(get_dol)\ntrain_final[\"question_body_clean\"] = train_final[\"question_body_clean\"].apply(get_dol_replace)\n\ntest_final[\"question_body_dol\"] = test_final[\"question_body\"].apply(get_dol)\ntest_final[\"question_body_clean\"] = test_final[\"question_body_clean\"].apply(get_dol_replace)\n\ntrain_final[\"question_body_all\"] = list(map(lambda a,b,c,d,e : str(a) + ' ' + str(b) + ' ' + str(c) +' '+ str(d) +' '+ str(e),train_final[\"question_body_code\"],train_final[\"question_body_slsldolel\"],train_final[\"question_body_doldol\"],train_final[\"question_body_spdol\"], train_final[\"question_body_dol\"]))\ntest_final[\"question_body_all\"] = list(map(lambda a,b,c,d,e : str(a) + ' ' + str(b) + ' ' + str(c) +' '+ str(d) +' '+ str(e),test_final[\"question_body_code\"],test_final[\"question_body_slsldolel\"],test_final[\"question_body_doldol\"],test_final[\"question_body_spdol\"], test_final[\"question_body_dol\"]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_code1(text):\n    text = str(text)\n    codes_list = re.findall(':\\n\\n(.*?)\\n\\n\\n',text, flags=re.DOTALL)\n    if len(codes_list) > 0:\n        code = ' . '.join(map(str, codes_list))\n        return code\n    else:\n        return \"NONE\"\n\ndef get_code1_replace(text):\n    text = str(text)\n    codes_list = re.findall(':\\n\\n.*?\\n\\n\\n',text, flags=re.DOTALL)\n    if len(codes_list) > 0:\n        #code = ' '.join(map(str, codes_list))\n        for i in codes_list:\n            i = re.escape(i)\n            text = re.sub(f\"{i}\", ' [CODE] ', text)\n        return text\n    else:\n        return text\n\ndef get_code2(text):\n    text = str(text)\n    codes_list = re.findall('.\\n\\n(.*?)\\n\\n\\n',text, flags=re.DOTALL)\n    if len(codes_list) > 0:\n        code = ' . '.join(map(str, codes_list))\n        return code\n    else:\n        return \"NONE\"\n\ndef get_code2_replace(text):\n    text = str(text)\n    codes_list = re.findall('.\\n\\n.*?\\n\\n\\n',text, flags=re.DOTALL)\n    if len(codes_list) > 0:\n        #code = ' '.join(map(str, codes_list))\n        for i in codes_list:\n            if i.count(\"\\n\") > 4:\n                i = re.escape(i)\n                text = re.sub(f\"{i}\", ' [CODE] ', text)\n            else:\n                pass\n        return text\n    else:\n        return text\n    \ndef get_code3(text):\n    text = str(text)\n    codes_list = re.findall('{(.*?)}',text, flags=re.DOTALL)\n    if len(codes_list) > 0:\n        code = ' . '.join(map(str, codes_list))\n        return code\n    else:\n        return \"NONE\"\n\ndef get_code3_replace(text):\n    text = str(text)\n    codes_list = re.findall('{.*?}',text, flags=re.DOTALL)\n    if len(codes_list) > 0:\n        #code = ' '.join(map(str, codes_list))\n        for i in codes_list:\n            if len(i) > 10:\n                i = re.escape(i)\n                text = re.sub(f\"{i}\", ' [CODE] ', text)\n            else:\n                pass\n        return text\n    else:\n        return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# answer_code1\n\ntrain_final[\"answer_code1\"] = train_final[\"answer\"].apply(get_code1)\ntrain_final[\"answer_clean\"] = train_final[\"answer\"].apply(get_code1_replace)\n\ntest_final[\"answer_code1\"] = test_final[\"answer\"].apply(get_code1)\ntest_final[\"answer_clean\"] = test_final[\"answer\"].apply(get_code1_replace)\n\n# answer_code2\n\ntrain_final[\"answer_code2\"] = train_final[\"answer\"].apply(get_code2)\ntrain_final[\"answer_clean\"] = train_final[\"answer_clean\"].apply(get_code2_replace)\n\ntest_final[\"answer_code2\"] = test_final[\"answer\"].apply(get_code2)\ntest_final[\"answer_clean\"] = test_final[\"answer_clean\"].apply(get_code2_replace)\n\n# answer_code3\n\ntrain_final[\"answer_code3\"] = train_final[\"answer\"].apply(get_code3)\ntrain_final[\"answer_clean\"] = train_final[\"answer_clean\"].apply(get_code3_replace)\n\ntest_final[\"answer_code3\"] = test_final[\"answer\"].apply(get_code3)\ntest_final[\"answer_clean\"] = test_final[\"answer_clean\"].apply(get_code3_replace)\n\ntrain_final[\"answer_code\"] = list(map(lambda a,b,c : str(a) + ' ' + str(b) + ' ' + str(c),train_final[\"answer_code1\"],train_final[\"answer_code2\"],train_final[\"answer_code3\"]))\ntest_final[\"answer_code\"] = list(map(lambda a,b,c : str(a) + ' ' + str(b) + ' ' + str(c),test_final[\"answer_code1\"],test_final[\"answer_code2\"],test_final[\"answer_code3\"]))\n\n# question_body_slsldolel\ntrain_final[\"answer_slsldolel\"] = train_final[\"answer\"].apply(get_slsldolel)\ntrain_final[\"answer_clean\"] = train_final[\"answer_clean\"].apply(get_slsldolel_replace)\n\ntest_final[\"answer_slsldolel\"] = test_final[\"answer\"].apply(get_slsldolel)\ntest_final[\"answer_clean\"] = test_final[\"answer_clean\"].apply(get_slsldolel_replace)\n\n# question_body_doldol\ntrain_final[\"answer_doldol\"] = train_final[\"answer\"].apply(get_doldol)\ntrain_final[\"answer_clean\"] = train_final[\"answer_clean\"].apply(get_doldol_replace)\n\ntest_final[\"answer_doldol\"] = test_final[\"answer\"].apply(get_doldol)\ntest_final[\"answer_clean\"] = test_final[\"answer_clean\"].apply(get_doldol_replace)\n\n# question_body_spdol\ntrain_final[\"answer_spdol\"] = train_final[\"answer\"].apply(get_spdol)\ntrain_final[\"answer_clean\"] = train_final[\"answer_clean\"].apply(get_spdol_replace)\n\ntest_final[\"answer_spdol\"] = test_final[\"answer\"].apply(get_spdol)\ntest_final[\"answer_clean\"] = test_final[\"answer_clean\"].apply(get_spdol_replace)\n\n# question_body_dol\ntrain_final[\"answer_dol\"] = train_final[\"answer\"].apply(get_dol)\ntrain_final[\"answer_clean\"] = train_final[\"answer_clean\"].apply(get_dol_replace)\n\ntest_final[\"answer_dol\"] = test_final[\"answer\"].apply(get_dol)\ntest_final[\"answer_clean\"] = test_final[\"answer_clean\"].apply(get_dol_replace)\n\ntrain_final[\"answer_all\"] = list(map(lambda a,b,c,d,e : str(a) + ' ' + str(b) + ' ' + str(c) +' '+ str(d)+' '+ str(e),train_final[\"answer_code\"],train_final[\"answer_slsldolel\"],train_final[\"answer_doldol\"],train_final[\"answer_spdol\"],train_final[\"answer_dol\"]))\ntest_final[\"answer_all\"] = list(map(lambda a,b,c,d,e : str(a) + ' ' + str(b) + ' ' + str(c) +' '+ str(d)+' '+ str(e),test_final[\"answer_code\"],test_final[\"answer_slsldolel\"],test_final[\"answer_doldol\"],test_final[\"answer_spdol\"], test_final[\"answer_dol\"]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"droped_columns = []\ndroped_columns.extend(user_drop_cols)\ndroped_columns.extend(posts_drop_cols)\nprint(len(droped_columns))\n\ntrain_final.drop(columns=droped_columns, inplace=True)\ntest_final.drop(columns=droped_columns, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_final.shape, test_final.shape, train_final_targets.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Text cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"def url_replace(text):\n    text = re.sub(\"(?:(?:https?|ftp):\\/\\/)?[\\w/\\-?=%.]+\\.[\\w/\\-?=%.]+\", '[URL]' ,text, flags=re.DOTALL)\n    return text\n\ndef url_count(text):\n    count = len(re.findall('(?:(?:https?|ftp):\\/\\/)?[\\w/\\-?=%.]+\\.[\\w/\\-?=%.]+',text))\n    return count","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"CUSTOM_TABLE = str.maketrans(\n    {\n        \"\\xad\": None,\n        \"\\x7f\": None,\n        \"\\ufeff\": None,\n        \"\\u200b\": None,\n        \"\\u200e\": None,\n        \"\\u202a\": None,\n        \"\\u202c\": None,\n        \"‘\": \"'\",\n        \"’\": \"'\",\n        \"`\": \"'\",\n        \"“\": '\"',\n        \"”\": '\"',\n        \"«\": '\"',\n        \"»\": '\"',\n        \"ɢ\": \"G\",\n        \"ɪ\": \"I\",\n        \"ɴ\": \"N\",\n        \"ʀ\": \"R\",\n        \"ʏ\": \"Y\",\n        \"ʙ\": \"B\",\n        \"ʜ\": \"H\",\n        \"ʟ\": \"L\",\n        \"ғ\": \"F\",\n        \"ᴀ\": \"A\",\n        \"ᴄ\": \"C\",\n        \"ᴅ\": \"D\",\n        \"ᴇ\": \"E\",\n        \"ᴊ\": \"J\",\n        \"ᴋ\": \"K\",\n        \"ᴍ\": \"M\",\n        \"Μ\": \"M\",\n        \"ᴏ\": \"O\",\n        \"ᴘ\": \"P\",\n        \"ᴛ\": \"T\",\n        \"ᴜ\": \"U\",\n        \"ᴡ\": \"W\",\n        \"ᴠ\": \"V\",\n        \"ĸ\": \"K\",\n        \"в\": \"B\",\n        \"м\": \"M\",\n        \"н\": \"H\",\n        \"т\": \"T\",\n        \"ѕ\": \"S\",\n        \"—\": \"-\",\n        \"–\": \"-\",\n    }\n)\n\nWORDS_REPLACER = [\n    (\"sh*t\", \"shit\"),\n    (\"s**t\", \"shit\"),\n    (\"f*ck\", \"fuck\"),\n    (\"fu*k\", \"fuck\"),\n    (\"f**k\", \"fuck\"),\n    (\"f*****g\", \"fucking\"),\n    (\"f***ing\", \"fucking\"),\n    (\"f**king\", \"fucking\"),\n    (\"p*ssy\", \"pussy\"),\n    (\"p***y\", \"pussy\"),\n    (\"pu**y\", \"pussy\"),\n    (\"p*ss\", \"piss\"),\n    (\"b*tch\", \"bitch\"),\n    (\"bit*h\", \"bitch\"),\n    (\"h*ll\", \"hell\"),\n    (\"h**l\", \"hell\"),\n    (\"cr*p\", \"crap\"),\n    (\"d*mn\", \"damn\"),\n    (\"stu*pid\", \"stupid\"),\n    (\"st*pid\", \"stupid\"),\n    (\"n*gger\", \"nigger\"),\n    (\"n***ga\", \"nigger\"),\n    (\"f*ggot\", \"faggot\"),\n    (\"scr*w\", \"screw\"),\n    (\"pr*ck\", \"prick\"),\n    (\"g*d\", \"god\"),\n    (\"s*x\", \"sex\"),\n    (\"a*s\", \"ass\"),\n    (\"a**hole\", \"asshole\"),\n    (\"a***ole\", \"asshole\"),\n    (\"a**\", \"ass\"),\n]\n\nWORDS_REPLACER_2 = [\n    (\"ain't\", 'is not'),\n     (\"aren't\", 'are not'),\n     (\"can't\", 'cannot'),\n     (\"'cause\", 'because'),\n     (\"could've\", 'could have'),\n     (\"couldn't\", 'could not'),\n     (\"didn't\", 'did not'),\n     (\"doesn't\", 'does not'),\n     (\"don't\", 'do not'),\n     (\"hadn't\", 'had not'),\n     (\"hasn't\", 'has not'),\n     (\"haven't\", 'have not'),\n     (\"he'd\", 'he would'),\n     (\"he'll\", 'he will'),\n     (\"he's\", 'he is'),\n     (\"how'd\", 'how did'),\n     (\"how'd'y\", 'how do you'),\n     (\"how'll\", 'how will'),\n     (\"how's\", 'how is'),\n     (\"i'd\", 'i would'),\n     (\"i'd've\", 'i would have'),\n     (\"i'll\", 'i will'),\n     (\"i'll've\", 'i will have'),\n     (\"i'm\", 'i am'),\n     (\"i've\", 'i have'),\n     (\"i'd\", 'i would'),\n     (\"i'd've\", 'i would have'),\n     (\"i'll\", 'i will'),\n     (\"i'll've\", 'i will have'),\n     (\"i'm\", 'i am'),\n     (\"i've\", 'i have'),\n     (\"isn't\", 'is not'),\n     (\"it'd\", 'it would'),\n     (\"it'd've\", 'it would have'),\n     (\"it'll\", 'it will'),\n     (\"it'll've\", 'it will have'),\n     (\"it's\", 'it is'),\n     (\"let's\", 'let us'),\n     (\"ma'am\", 'madam'),\n     (\"mayn't\", 'may not'),\n     (\"might've\", 'might have'),\n     (\"mightn't\", 'might not'),\n     (\"mightn't've\", 'might not have'),\n     (\"must've\", 'must have'),\n     (\"mustn't\", 'must not'),\n     (\"mustn't've\", 'must not have'),\n     (\"needn't\", 'need not'),\n     (\"needn't've\", 'need not have'),\n     (\"o'clock\", 'of the clock'),\n     (\"oughtn't\", 'ought not'),\n     (\"oughtn't've\", 'ought not have'),\n     (\"shan't\", 'shall not'),\n     (\"sha'n't\", 'shall not'),\n     (\"shan't've\", 'shall not have'),\n     (\"she'd\", 'she would'),\n     (\"she'd've\", 'she would have'),\n     (\"she'll\", 'she will'),\n     (\"she'll've\", 'she will have'),\n     (\"she's\", 'she is'),\n     (\"should've\", 'should have'),\n     (\"shouldn't\", 'should not'),\n     (\"shouldn't've\", 'should not have'),\n     (\"so've\", 'so have'),\n     (\"so's\", 'so as'),\n     (\"this's\", 'this is'),\n     (\"that'd\", 'that would'),\n     (\"that'd've\", 'that would have'),\n     (\"that's\", 'that is'),\n     (\"there'd\", 'there would'),\n     (\"there'd've\", 'there would have'),\n     (\"there's\", 'there is'),\n     (\"here's\", 'here is'),\n     (\"they'd\", 'they would'),\n     (\"they'd've\", 'they would have'),\n     (\"they'll\", 'they will'),\n     (\"they'll've\", 'they will have'),\n     (\"they're\", 'they are'),\n     (\"they've\", 'they have'),\n     (\"to've\", 'to have'),\n     (\"wasn't\", 'was not'),\n     (\"we'd\", 'we would'),\n     (\"we'd've\", 'we would have'),\n     (\"we'll\", 'we will'),\n     (\"we'll've\", 'we will have'),\n     (\"we're\", 'we are'),\n     (\"we've\", 'we have'),\n     (\"weren't\", 'were not'),\n     (\"what'll\", 'what will'),\n     (\"what'll've\", 'what will have'),\n     (\"what're\", 'what are'),\n     (\"what's\", 'what is'),\n     (\"what've\", 'what have'),\n     (\"when's\", 'when is'),\n     (\"when've\", 'when have'),\n     (\"where'd\", 'where did'),\n     (\"where's\", 'where is'),\n     (\"where've\", 'where have'),\n     (\"who'll\", 'who will'),\n     (\"who'll've\", 'who will have'),\n     (\"who's\", 'who is'),\n     (\"who've\", 'who have'),\n     (\"why's\", 'why is'),\n     (\"why've\", 'why have'),\n     (\"will've\", 'will have'),\n     (\"won't\", 'will not'),\n     (\"won't've\", 'will not have'),\n     (\"would've\", 'would have'),\n     (\"wouldn't\", 'would not'),\n     (\"wouldn't've\", 'would not have'),\n     (\"y'all\", 'you all'),\n     (\"y'all'd\", 'you all would'),\n     (\"y'all'd've\", 'you all would have'),\n     (\"y'all're\", 'you all are'),\n     (\"y'all've\", 'you all have'),\n     (\"you'd\", 'you would'),\n     (\"you'd've\", 'you would have'),\n     (\"you'll\", 'you will'),\n     (\"you'll've\", 'you will have'),\n     (\"you're\", 'you are'),\n     (\"you've\", 'you have'),\n     ('what”s', 'what is'),\n     ('what\"s', 'what is'),\n     ('its', 'it is'),\n     (\"what's\", 'what is'),\n     (\"'ll\", 'will'),\n     (\"n't\", 'not'),\n     (\"'re\", 'are'),\n     (\"ain't\", 'is not'),\n     (\"aren't\", 'are not'),\n     (\"can't\", 'cannot'),\n     (\"'cause\", 'because'),\n     (\"could've\", 'could have'),\n     (\"couldn't\", 'could not'),\n     (\"didn't\", 'did not'),\n     (\"doesn't\", 'does not'),\n     (\"don't\", 'do not'),\n     (\"hadn't\", 'had not'),\n     (\"hasn't\", 'has not'),\n     (\"haven't\", 'have not'),\n     (\"he'd\", 'he would'),\n     (\"he'll\", 'he will'),\n     (\"he's\", 'he is'),\n     (\"how'd\", 'how did'),\n     (\"how'd'y\", 'how do you'),\n     (\"how'll\", 'how will'),\n     (\"how's\", 'how is'),\n     (\"i'd\", 'i would'),\n     (\"i'd've\", 'i would have'),\n     (\"i'll\", 'i will'),\n     (\"i'll've\", 'i will have'),\n     (\"i'm\", 'i am'),\n     (\"i've\", 'i have'),\n     (\"i'd\", 'i would'),\n     (\"i'd've\", 'i would have'),\n     (\"i'll\", 'i will'),\n     (\"i'll've\", 'i will have'),\n     (\"i'm\", 'i am'),\n     (\"i've\", 'i have'),\n     (\"isn't\", 'is not'),\n     (\"it'd\", 'it would'),\n     (\"it'd've\", 'it would have'),\n     (\"it'll\", 'it will'),\n     (\"it'll've\", 'it will have'),\n     (\"it's\", 'it is'),\n     (\"let's\", 'let us'),\n     (\"ma'am\", 'madam'),\n     (\"mayn't\", 'may not'),\n     (\"might've\", 'might have'),\n     (\"mightn't\", 'might not'),\n     (\"mightn't've\", 'might not have'),\n     (\"must've\", 'must have'),\n     (\"mustn't\", 'must not'),\n     (\"mustn't've\", 'must not have'),\n     (\"needn't\", 'need not'),\n     (\"needn't've\", 'need not have'),\n     (\"o'clock\", 'of the clock'),\n     (\"oughtn't\", 'ought not'),\n     (\"oughtn't've\", 'ought not have'),\n     (\"shan't\", 'shall not'),\n     (\"sha'n't\", 'shall not'),\n     (\"shan't've\", 'shall not have'),\n     (\"she'd\", 'she would'),\n     (\"she'd've\", 'she would have'),\n     (\"she'll\", 'she will'),\n     (\"she'll've\", 'she will have'),\n     (\"she's\", 'she is'),\n     (\"should've\", 'should have'),\n     (\"shouldn't\", 'should not'),\n     (\"shouldn't've\", 'should not have'),\n     (\"so've\", 'so have'),\n     (\"so's\", 'so as'),\n     (\"this's\", 'this is'),\n     (\"that'd\", 'that would'),\n     (\"that'd've\", 'that would have'),\n     (\"that's\", 'that is'),\n     (\"there'd\", 'there would'),\n     (\"there'd've\", 'there would have'),\n     (\"there's\", 'there is'),\n     (\"here's\", 'here is'),\n     (\"they'd\", 'they would'),\n     (\"they'd've\", 'they would have'),\n     (\"'they're\", 'they are'),\n     (\"they'll\", 'they will'),\n     (\"they'll've\", 'they will have'),\n     (\"they're\", 'they are'),\n     (\"they've\", 'they have'),\n     (\"to've\", 'to have'),\n     (\"wasn't\", 'was not'),\n     (\"we'd\", 'we would'),\n     (\"we'd've\", 'we would have'),\n     (\"we'll\", 'we will'),\n     (\"we'll've\", 'we will have'),\n     (\"we're\", 'we are'),\n     (\"we've\", 'we have'),\n     (\"weren't\", 'were not'),\n     (\"what'll\", 'what will'),\n     (\"what'll've\", 'what will have'),\n     (\"what're\", 'what are'),\n     (\"what's\", 'what is'),\n     (\"what've\", 'what have'),\n     (\"when's\", 'when is'),\n     (\"when've\", 'when have'),\n     (\"where'd\", 'where did'),\n     (\"where's\", 'where is'),\n     (\"where've\", 'where have'),\n     (\"who'll\", 'who will'),\n     (\"who'll've\", 'who will have'),\n     (\"who's\", 'who is'),\n     (\"who've\", 'who have'),\n     (\"why's\", 'why is'),\n     (\"why've\", 'why have'),\n     (\"will've\", 'will have'),\n     (\"won't\", 'will not'),\n     (\"won't've\", 'will not have'),\n     (\"would've\", 'would have'),\n     (\"wouldn't\", 'would not'),\n     (\"wouldn't've\", 'would not have'),\n     (\"y'all\", 'you all'),\n     (\"y'all'd\", 'you all would'),\n     (\"y'all'd've\", 'you all would have'),\n     (\"y'all're\", 'you all are'),\n     (\"y'all've\", 'you all have'),\n     (\"you'd\", 'you would'),\n     (\"you'd've\", 'you would have'),\n     (\"you'll\", 'you will'),\n     (\"you'll've\", 'you will have'),\n     (\"you're\", 'you are'),\n     (\"you've\", 'you have')\n    ]\n\nmispell_dict = {\"aren't\" : \"are not\",\n                \"can't\" : \"cannot\",\n                \"couldn't\" : \"could not\",\n                \"couldnt\" : \"could not\",\n                \"didn't\" : \"did not\",\n                \"doesn't\" : \"does not\",\n                \"doesnt\" : \"does not\",\n                \"don't\" : \"do not\",\n                \"hadn't\" : \"had not\",\n                \"hasn't\" : \"has not\",\n                \"haven't\" : \"have not\",\n                \"havent\" : \"have not\",\n                \"he'd\" : \"he would\",\n                \"he'll\" : \"he will\",\n                \"he's\" : \"he is\",\n                \"i'd\" : \"I would\",\n                \"i'd\" : \"I had\",\n                \"i'll\" : \"I will\",\n                \"i'm\" : \"I am\",\n                \"isn't\" : \"is not\",\n                \"it's\" : \"it is\",\n                \"it'll\":\"it will\",\n                \"i've\" : \"I have\",\n                \"let's\" : \"let us\",\n                \"mightn't\" : \"might not\",\n                \"mustn't\" : \"must not\",\n                \"shan't\" : \"shall not\",\n                \"she'd\" : \"she would\",\n                \"she'll\" : \"she will\",\n                \"she's\" : \"she is\",\n                \"shouldn't\" : \"should not\",\n                \"shouldnt\" : \"should not\",\n                \"that's\" : \"that is\",\n                \"thats\" : \"that is\",\n                \"there's\" : \"there is\",\n                \"theres\" : \"there is\",\n                \"they'd\" : \"they would\",\n                \"they'll\" : \"they will\",\n                \"they're\" : \"they are\",\n                \"theyre\":  \"they are\",\n                \"they've\" : \"they have\",\n                \"we'd\" : \"we would\",\n                \"we're\" : \"we are\",\n                \"weren't\" : \"were not\",\n                \"we've\" : \"we have\",\n                \"what'll\" : \"what will\",\n                \"what're\" : \"what are\",\n                \"what's\" : \"what is\",\n                \"what've\" : \"what have\",\n                \"where's\" : \"where is\",\n                \"who'd\" : \"who would\",\n                \"who'll\" : \"who will\",\n                \"who're\" : \"who are\",\n                \"who's\" : \"who is\",\n                \"who've\" : \"who have\",\n                \"won't\" : \"will not\",\n                \"wouldn't\" : \"would not\",\n                \"you'd\" : \"you would\",\n                \"you'll\" : \"you will\",\n                \"you're\" : \"you are\",\n                \"you've\" : \"you have\",\n                \"'re\": \" are\",\n                \"wasn't\": \"was not\",\n                \"we'll\":\" will\",\n                \"didn't\": \"did not\",\n                \"tryin'\":\"trying\",\n                \"‘\": \"'\", \n                \"₹\": \"e\", \n                \"´\": \"'\",\n                \"°\": \"\", \n                \"€\": \"e\", \n                \"™\": \"tm\", \n                \"√\": \" sqrt \",\n                \"×\": \"x\", \n                \"²\": \"2\", \n                \"—\": \"-\",\n                \"–\": \"-\",\n                \"’\": \"'\",\n                \"_\": \"-\",\n                \"`\": \"'\", \n                '“': '\"',\n                '”': '\"',\n                '“': '\"',\n                \"£\": \"e\", \n                '∞': 'infinity',\n                'θ': 'theta',\n                '÷': '/',\n                'α': 'alpha', \n                '•': '.',\n                'à': 'a', \n                '−': '-', \n                'β': 'beta', \n                '∅': '', \n                '³': '3', \n                'π': 'pi',\n                '\\u200b': ' ', \n                '…': ' ... ', \n                '\\ufeff': '', \n                'करना': '', \n                'है': '',  \n                \n               }\n\n\nWORDS_REPLACER_3 = [(k, v) for k, v in mispell_dict.items()]\n\nWORDS_REPLACER_4 = [('automattic', 'automatic'),\n         ('sweetpotato', 'sweet potato'),\n         ('statuscode', 'status code'),\n         ('applylayer', 'apply layer'),\n         ('aligator', 'alligator'),\n         ('downloands', 'download'),\n         ('dowloand', 'download'),\n         ('thougths', 'thoughts'),\n         ('helecopter', 'helicopter'),\n         ('telugul', 'telugu'),\n         ('unconditionaly', 'unconditionally'),\n         ('coompanies', 'companies'),\n         ('lndigenous', 'indigenous'),\n         ('evluate', 'evaluate'),\n         ('suggstion', 'suggestion'),\n         ('thinkning', 'thinking'),\n         ('concatinate', 'concatenate'),\n         ('constitutionals', 'constitutional'),\n         ('moneyback', 'money back'),\n         ('civilazation', 'civilization'),\n         ('paranoria', 'paranoia'),\n         ('rightside', 'right side'),\n         ('methamatics', 'mathematics'),\n         ('natual', 'natural'),\n         ('brodcast', 'broadcast'),\n         ('pleasesuggest', 'please suggest'),\n         ('intitution', 'institution'),\n         ('experinces', 'experiences'),\n         ('reallyreally', 'really'),\n         ('testostreone', 'testosterone'),\n         ('musceles', 'muscle'),\n         ('bacause', 'because'),\n         ('peradox', 'paradox'),\n         ('probabity', 'probability'),\n         ('collges', 'college'),\n         ('diciplined', 'disciplined'),\n         ('completeted', 'completed'),\n         ('lunchshould', 'lunch should'),\n         ('battlenet', 'battle net'),\n         ('dissapoint', 'disappoint'),\n         ('resultsnew', 'results new'),\n         ('indcidents', 'incidents'),\n         ('figuire', 'figure'),\n         ('protonneutron', 'proton neutron'),\n         ('tecnical', 'technical'),\n         ('patern', 'pattern'),\n         ('unenroll', 'un enroll'),\n         ('proceedures', 'procedures'),\n         ('srategy', 'strategy'),\n         ('mordern', 'modern'),\n         ('prepartion', 'preparation'),\n         ('throuhout', 'throught'),\n         ('academey', 'academic'),\n         ('instituitions', 'institutions'),\n         ('abadon', 'abandon'),\n         ('compitetive', 'competitive'),\n         ('hypercondriac', 'hypochondriac'),\n         ('spiliting', 'splitting'),\n         ('physchic', 'psychic'),\n         ('flippingly', 'flipping'),\n         ('likelyhood', 'likelihood'),\n         ('armsindustry', 'arms industry'),\n         (' turorials', 'tutorials'),\n         ('photostats', 'photostat'),\n         ('sunconcious', 'subconscious'),\n         ('chemistryphysics', 'chemistry physics'),\n         ('secondlife', 'second life'),\n         ('histrorical', 'historical'),\n         ('disordes', 'disorders'),\n         ('differenturl', 'differential'),\n         ('councilling', ' counselling'),\n         ('sugarmill', 'sugar mill'),\n         ('relatiosnhip', 'relationship'),\n         ('fanpages', 'fan pages'),\n         ('agregator', 'aggregator'),\n         ('switc', 'switch'),\n         ('smatphones', 'smartphones'),\n         ('headsize', 'head size'),\n         ('pendrives', 'pen drives'),\n         ('biotecnology', 'biotechnology'),\n         ('borderlink', 'border link'),\n         ('furnance', 'furnace'),\n         ('competetion', 'competition'),\n         ('distibution', 'distribution'),\n         ('ananlysis', ' analysis'),\n         ('textile？', 'textile'),\n         ('howww', 'how'),\n         ('strategybusiness', 'strategy business'),\n         ('spectrun', 'spectrum'),\n         ('propasal', 'proposal'),\n         ('appilcable', 'applicable'),\n         ('accountwhat', ' account what'),\n         ('algorithems', ' algorithms'),\n         ('protuguese', ' Portuguese'),\n         ('exatly', 'exactly'),\n         ('disturbence', 'disturbance'),\n         ('govrnment', 'government'),\n         ('requiremnt', 'requirement'),\n         ('vargin', 'virgin'),\n         ('lonleley', 'lonely'),\n         ('unmateralistic', 'materialistic'),\n         ('dveloper', 'developer'),\n         ('dcuments', 'documents'),\n         ('techonologies', 'technologies'),\n         ('morining', 'morning'),\n         ('samsing', 'Samsung'),\n         ('engeeniring', 'engineering'),\n         ('racetrac', 'racetrack'),\n         ('physian', 'physician'),\n         ('theretell', 'there tell'),\n         ('tryto', 'try to'),\n         ('teamfight', 'team fight'),\n         ('recomend', 'recommend'),\n         ('spectables', 'spectacles'),\n         ('emtional', 'emotional'),\n         ('engeenerring', 'engineering'),\n         ('optionsgood', 'options good'),\n         ('primarykey', 'primary key'),\n         ('foreignkey', 'foreign key'),\n         ('concieved', 'conceived'),\n         ('leastexpensive', 'least expensive'),\n         ('foodtech', 'food tech'),\n         ('electronegetivity', 'electronegativity'),\n         ('polticians', 'politicians'),\n         ('distruptive', 'disruptive'),\n         ('currrent', 'current'),\n         ('hidraulogy', 'hydrology'),\n         ('californa', 'California'),\n         ('electrrical', 'electrical'),\n         ('navigationally', 'navigation'),\n         ('whwhat', 'what'),\n         ('bcos', 'because'),\n         ('vaccancies', 'vacancies'),\n         ('articels', 'articles'),\n         ('boilng', 'boiling'),\n         ('hyperintensity', 'hyper intensity'),\n         ('rascism', 'racism'),\n         ('messenging', 'messaging'),\n         ('cleaniness', 'cleanliness'),\n         ('vetenary', 'veterinary'),\n         ('investorswhat', 'investors what'),\n         ('chrestianity', 'Christianity'),\n         ('apporval', 'approval'),\n         ('repaire', 'repair'),\n         ('biggerchance', 'bigger chance'),\n         ('manufacturering', 'manufacturing'),\n         ('buildertrend', 'builder trend'),\n         ('allocatively', 'allocative'),\n         ('subliminals', 'subliminal'),\n         ('mechnically', 'mechanically'),\n         ('binaurial', 'binaural'),\n         ('naaked', 'naked'),\n         ('aantidepressant', 'antidepressant'),\n         ('geunine', 'genuine'),\n         ('quantitaive', 'quantitative'),\n         ('paticipated', 'participated'),\n         ('repliedjesus', 'replied Jesus'),\n         ('baised', 'biased'),\n         ('worldreport', 'world report'),\n         ('eecutives', 'executives'),\n         ('paitents', 'patients'),\n         ('telgu', 'Telugu'),\n         ('nomeniculature', 'nomenclature'),\n         ('crimimaly', 'criminally'),\n         ('resourse', 'resource'),\n         ('procurenent', 'procurement'),\n         ('improvemet', 'improvement'),\n         ('metamers', 'metamer'),\n         ('tautomers', 'tautomer'),\n         ('knowwhen', 'know when'),\n         ('whatdoes', 'what does'),\n         ('pletelets', 'platelets'),\n         ('pssesive', 'possessive'),\n         ('oxigen', 'oxygen'),\n         ('ethniticy', 'ethnicity'),\n         ('situatiation', 'situation'),\n         ('ecoplanet', 'eco planet'),\n         ('situatio', 'situation'),\n         ('dateing', 'dating'),\n         ('hostress', 'hostess'),\n         ('initialisation', 'initialization'),\n         ('hydrabd', 'Hyderabad'),\n         ('deppresed', 'depressed'),\n         ('dwnloadng', 'downloading'),\n         ('expirey', 'expiry'),\n         ('engeenering', 'engineering'),\n         ('hyderebad', 'Hyderabad'),\n         ('automatabl', 'automatable'),\n         ('architetureocasions', 'architectureoccasions'),\n         ('restaraunts', 'restaurants'),\n         ('recommedations', 'recommendations'),\n         ('intergrity', 'integrity'),\n         ('reletively', 'relatively'),\n         ('priceworthy', 'price worthy'),\n         ('princples', 'principles'),\n         ('reconigze', 'recognize'),\n         ('paticular', 'particular'),\n         ('musictheory', 'music theory'),\n         ('requied', 'required'),\n         ('netural', 'natural'),\n         ('fluoresent', 'fluorescent'),\n         ('girlfiend', 'girlfriend'),\n         ('develpment', 'development'),\n         ('eridicate', 'eradicate'),\n         ('techologys', 'technologies'),\n         ('hybridyzation', 'hybridization'),\n         ('ideaa', 'ideas'),\n         ('tchnology', 'technology'),\n         ('appropiate', 'appropriate'),\n         ('respone', 'response'),\n         ('celebreties', 'celebrities'),\n         ('exterion', 'exterior'),\n         ('uservoice', 'user voice'),\n         ('effeciently', 'efficiently'),\n         ('torquise', 'turquoise '),\n         ('governmentand', 'government and'),\n         ('eletricity', 'electricity'),\n         ('coulums', 'columns'),\n         ('nolonger', 'no longer'),\n         ('wheras', 'whereas'),\n         ('infnite', 'infinite'),\n         ('decolourised', 'no color'),\n         ('onepiece', 'one piece'),\n         ('assignements', 'assignments'),\n         ('celebarted', 'celebrated'),\n         ('pharmacistical', 'pharmaceutical'),\n         ('jainsingle', 'Jain single'),\n         ('asssistance', 'assistance'),\n         ('glases', 'glasses'),\n         ('polymorpism', 'polymorphism'),\n         ('amerians', 'Americans'),\n         ('masquitos', 'mosquitoes'),\n         ('interseted', 'interested'),\n         ('thehighest', 'the highest'),\n         ('etnicity', 'ethnicity'),\n         ('anopportunity', 'anopportunity'),\n         ('multidiscipline', 'multi discipline'),\n         ('smartchange', 'smart change'),\n         ('collegefest', 'college fest'),\n         ('disdvantages', 'disadvantages'),\n         ('successfcators', 'success factors'),\n         ('sustitute', 'substitute'),\n         ('caoching', 'coaching'),\n         ('bullyed', 'bullied'),\n         ('comunicate', 'communicate'),\n         ('prisioner', 'prisoner'),\n         ('tamilnaadu', 'Tamil Nadu'),\n         ('methodologyies', 'methodologies'),\n         ('tranfers', 'transfers'),\n         ('truenorth', 'true north'),\n         ('backdonation', 'back donation'),\n         ('oreals', 'ordeals'),\n         ('browsec', 'browser'),\n         ('solarwinds', 'solar winds'),\n         ('susten', 'sustain'),\n         ('carnegi', 'Carnegie'),\n         ('doesent', \"doesn't\"),\n         ('automtotive', 'automotive'),\n         ('nimuselide', 'nimesulide'),\n         ('subsciption', 'subscription'),\n         ('quatrone', 'Quattrone'),\n         ('qatalyst', 'catalyst'),\n         ('vardamana', 'Vardaman'),\n         ('suplements', 'supplements'),\n         ('repore', 'report'),\n         ('pikettys', 'Piketty'),\n         ('paramilltary', 'paramilitary'),\n         ('aboutlastnight', 'about last night'),\n         ('vidyapeth', 'Vidyapeeth'),\n         ('extraterrestial', 'extraterrestrial'),\n         ('powerloom', 'power loom'),\n         ('zonbie', 'zombie'),\n         ('cococola', 'Coca Cola'),\n         ('hameorrhage', 'hemorrhage'),\n         ('abhayanand', 'Abhay Anand'),\n         ('romedynow', 'remedy now'),\n         ('couster', 'counter'),\n         ('encouaged', 'encouraged'),\n         ('toprepare', 'to prepare'),\n         ('eveteasing', 'eve teasing'),\n         ('roulete', 'roulette'),\n         ('sorkar', 'Sarkar'),\n         ('waveboard', 'wave board'),\n         ('acclerate', 'accelerate'),\n         ('togrow', 'to grow'),\n         ('felatio', 'fellatio'),\n         ('baherain', 'Bahrain'),\n         ('teatment', 'treatment'),\n         ('iwitness', 'eye witness'),\n         ('autoplaying', 'autoplay'),\n         ('twise', 'twice'),\n         ('timeskip', 'time skip'),\n         ('disphosphorus', 'diphosphorus'),\n         ('implemnt', 'implement'),\n         ('proview', 'preview'),\n         ('pinshoppr', 'pin shoppe'),\n         ('protestng', 'protesting'),\n         ('chromatographymass', 'chromatography mass'),\n         ('ncache', 'cache'),\n         ('dowloands', 'downloads'),\n         ('biospecifics', 'bio specifics'),\n         ('conforim', 'conform'),\n         ('dreft', 'draft'),\n         ('sinhaleseand', 'Sinhalese'),\n         ('swivl', 'swivel'),\n         ('officerjms', 'officers'),\n         ('refrigrant', 'refrigerant'),\n         ('kendras', 'Kendra'),\n         ('alchoholism', 'alcoholism'),\n         ('dollor', 'dollar'),\n         ('jeyalalitha', 'Jayalalitha'),\n         ('bettner', 'better'),\n         ('itemstream', 'timestream'),\n         ('notetaking', 'note taking'),\n         ('cringworthy', 'cringeworthy'),\n         ('easyday', 'easy day'),\n         ('scenessex', 'scenes sex'),\n         ('vivavideo', 'via video'),\n         ('washboth', 'wash both'),\n         ('textout', 'text out'),\n         ('createwindow', 'create window'),\n         ('calsium', 'calcium'),\n         ('biofibre', 'bio fibre'),\n         ('emailbesides', 'email besides'),\n         ('kathhi', 'Kathi'),\n         ('cenre', 'center'),\n         ('polyarmory', 'polyamory'),\n         ('superforecasters', 'super forecasters'),\n         ('blogers', 'bloggers'),\n         ('medicalwhich', 'medical which'),\n         ('iiving', 'living'),\n         ('pronouciation', 'pronunciation'),\n         ('youor', 'you or'),\n         ('thuderbird', 'Thunderbird'),\n         ('oneside', 'one side'),\n         ('spearow', 'Spearow'),\n         ('aanythign', 'anything'),\n         ('inmaking', 'in making'),\n         ('datamining', 'data mining'),\n         ('greybus', 'grey bus'),\n         ('onmeter', 'on meter'),\n         ('biling', 'billing'),\n         ('fidlago', 'Fidalgo'),\n         ('edfice', 'edifice'),\n         ('microsolutions', 'micro solutions'),\n         ('easly', 'easily'),\n         ('eukarotic', 'eukaryotic'),\n         ('accedental', 'accidental'),\n         ('intercasts', 'interests'),\n         ('oppresive', 'oppressive'),\n         ('generalizably', 'generalizable'),\n         ('tacometer', 'tachometer'),\n         ('loking', 'looking'),\n         ('scrypt', 'script'),\n         ('usafter', 'us after'),\n         ('everyweek', 'every week'),\n         ('hopesthe', 'hopes the'),\n         ('openflow', 'OpenFlow'),\n         ('checkride', 'check ride'),\n         ('springdrive', 'spring drive'),\n         ('emobile', 'mobile'),\n         ('dermotology', 'dermatology'),\n         ('somatrophin', 'somatropin'),\n         ('saywe', 'say we'),\n         ('multistores', 'multistory'),\n         ('bolognaise', 'Bolognese'),\n         ('hardisk', 'harddisk'),\n         ('penisula', 'peninsula'),\n         ('refferring', 'referring'),\n         ('freshere', 'fresher'),\n         ('pokemkon', 'Pokemon'),\n         ('nuero', 'neuro'),\n         ('whosampled', 'who sampled'),\n         ('researchkit', 'research kit'),\n         ('speach', 'speech'),\n         ('acept', 'accept'),\n         ('indiashoppe', 'Indian shoppe'),\n         ('todescribe', 'to describe'),\n         ('hollywod', 'Hollywood'),\n         ('whastup', 'whassup'),\n         ('kjedahls', 'Kjeldahl'),\n         ('lancher', 'launcher'),\n         ('stalkees', 'stalkers'),\n         ('baclinks', 'backlinks'),\n         ('instutional', 'institutional'),\n         ('wassap', 'Wassup'),\n         ('methylethyl', 'methyl ethyl'),\n         ('fundbox', 'fund box'),\n         ('keypoints', 'key points'),\n         ('particually', 'particularly'),\n         ('loseit', 'lose it'),\n         ('gowipe', 'go wipe'),\n         ('autority', 'authority'),\n         ('prinicple', 'principle'),\n         ('complaince', 'compliance'),\n         ('itnormal', 'it normal'),\n         ('forpeople', 'for people'),\n         ('chaces', 'chances'),\n         ('yearhow', 'year how'),\n         ('fastcomet', 'fast comet'),\n         ('withadd', 'with add'),\n         ('omnicient', 'omniscient'),\n         ('tofeel', 'to feel'),\n         ('becauseof', 'because of'),\n         ('laungauage', 'language'),\n         ('combodia', 'Cambodia'),\n         ('bhuvneshwer', 'Bhubaneshwar'),\n         ('cognito', 'Cognito'),\n         ('thaelsemia', 'thalassemia'),\n         ('meritstore', 'merit store'),\n         ('masterbuate', 'masturbate'),\n         ('planethere', 'planet here'),\n         ('mostof', 'most of'),\n         ('shallowin', 'shallow in'),\n         ('wordwhen', 'word when'),\n         ('biodesalination', 'desalination'),\n         ('tendulkars', 'Tendulkar'),\n         ('kerja', 'Kerja'),\n         ('sertifikat', 'certificate'),\n         ('indegenous', 'indigenous'),\n         ('lowpage', 'low page'),\n         ('asend', 'ascend'),\n         ('leadreship', 'leadership'),\n         ('openlab', 'open lab'),\n         ('foldinghome', 'folding home'),\n         ('sachins', 'Sachin'),\n         ('pleatue', 'plateau'),\n         ('passwor', 'password'),\n         ('manisfestation', 'manifestation'),\n         ('valryian', 'valerian'),\n         ('chemotaxic', 'chemotaxis'),\n         ('condesending', 'condescending'),\n         ('spiltzvilla', 'splitsville'),\n         ('mammaliaforme', 'mammaliaform'),\n         ('instituteagra', 'institute agra'),\n         ('learningand', 'learning and'),\n         ('ramamurthynagar', 'Ramamurthy Nagar'),\n         ('glucoses', 'glucose'),\n         ('imitaion', 'imitation'),\n         ('awited', 'awaited'),\n         ('realvision', 'real vision'),\n         ('simslot', 'sim slot'),\n         ('yourr', 'your'),\n         ('pacjage', 'package'),\n         ('branchth', 'branch'),\n         ('magzin', 'magazine'),\n         ('frozon', 'frozen'),\n         ('codescomputational', 'code computational'),\n         ('tempratures', 'temperatures'),\n         ('neurophaphy', 'neuropathy'),\n         ('freezone', 'free zone'),\n         ('speices', 'species'),\n         ('compaitable', 'compatible'),\n         ('sensilization', 'sensitization'),\n         ('tuboscope', 'tube scope'),\n         ('gamechangers', 'game changer'),\n         ('windsheild', 'windshield'),\n         ('explorerie', 'explorer'),\n         ('cuccina', 'Cucina'),\n         ('earthstone', 'hearthstone'),\n         ('vocabs', 'vocab'),\n         ('previouse', 'previous'),\n         ('oneview', 'one view'),\n         ('relance', 'reliance'),\n         ('waterstop', 'water stop'),\n         ('imput', 'input'),\n         ('survivers', 'survivors'),\n         ('benedryl', 'Benadryl'),\n         ('requestparam', 'request param'),\n         ('typeadd', 'type add'),\n         ('autists', 'artists'),\n         ('forany', 'for any'),\n         ('inteview', 'interview'),\n         ('aphantasia', 'Phantasia'),\n         ('lisanna', 'Lisanne'),\n         ('civilengineering', 'civil engineering'),\n         ('austrailia', 'Australia'),\n         ('alchoholic', 'alcoholic'),\n         ('adaptersuch', 'adapter such'),\n         ('sphilosopher', 'philosopher'),\n         ('calenderisation', 'calendarization'),\n         ('smooking', 'smoking'),\n         ('pemdulum', 'pendulum'),\n         ('analsyis', 'analysis'),\n         ('psycholology', 'psychology'),\n         ('ubantu', 'ubuntu'),\n         ('emals', 'emails'),\n         ('questionth', 'questions'),\n         ('jawarlal', 'Jawaharlal'),\n         ('svaldbard', 'Svalbard'),\n         ('prabhudeva', 'Prabhudeva'),\n         ('robtics', 'robotics'),\n         ('umblock', 'unblock'),\n         ('professionaly', 'professionally'),\n         ('biovault', 'bio vault'),\n         ('bibal', 'bible'),\n         ('higherstudies', 'higher studies'),\n         ('lestoil', 'less oil'),\n         ('biteshow', 'bike show'),\n         ('humanslike', 'humans like'),\n         ('purpse', 'purpose'),\n         ('barazilian', 'Brazilian'),\n         ('gravitional', 'gravitational'),\n         ('cylinderical', 'cylindrical'),\n         ('peparing', 'preparing'),\n         ('healthequity', 'health equity'),\n         ('appcleaner', 'app cleaner'),\n         ('instantq', 'instant'),\n         ('abolisihed', 'abolished'),\n         ('kwench', 'quench'),\n         ('prisamatic', 'prismatic'),\n         ('bhubneshwar', 'Bhubaneshwar'),\n         ('liscense', 'license'),\n         ('cyberbase', 'cyber base'),\n         ('safezone', 'safe zone'),\n         ('deactivat', 'deactivate'),\n         ('salicyclic', 'salicylic'),\n         ('cocacola', 'coca cola'),\n         ('noice', 'noise'),\n         ('examinaton', 'examination'),\n         ('pharmavigilance', 'pharmacovigilance'),\n         ('sixthsense', 'sixth sense'),\n         ('musiclly', 'musically'),\n         ('khardushan', 'Kardashian'),\n         ('chandragupt', 'Chandragupta'),\n         ('bayesians', 'bayesian'),\n         ('engineeringbut', 'engineering but'),\n         ('caretrust', 'care trust'),\n         ('girlbut', 'girl but'),\n         ('aviations', 'aviation'),\n         ('joinee', 'joiner'),\n         ('tutior', 'tutor'),\n         ('tylenal', 'Tylenol'),\n         ('neccesity', 'necessity'),\n         ('kapsule', 'capsule'),\n         ('prayes', 'prayers'),\n         ('depositmobile', 'deposit mobile'),\n         ('settopbox', 'set top box'),\n         ('meotic', 'meiotic'),\n         ('accidentially', 'accidentally'),\n         ('offcloud', 'off cloud'),\n         ('keshavam', 'Keshava'),\n         ('domaincentral', 'domain central'),\n         ('onetaste', 'one taste'),\n         ('lumpsum', 'lump sum'),\n         ('medschool', 'med school'),\n         ('digicard', 'Digi card'),\n         ('abroadus', 'abroad'),\n         ('campusexcept', 'campus except'),\n         ('aptittude', 'aptitude'),\n         ('neutrions', 'neutrinos'),\n         ('onepaper', 'one paper'),\n         ('remidies', 'remedies'),\n         ('convinient', 'convenient'),\n         ('financaily', 'financially'),\n         ('postives', 'positives'),\n         ('nikefuel', 'Nike fuel'),\n         ('ingrediants', 'ingredients'),\n         ('aspireat', 'aspirate'),\n         ('firstand', 'first'),\n         ('mohammmad', 'Mohammad'),\n         ('mutliple', 'multiple'),\n         ('dimonatization', 'demonization'),\n         ('cente', 'center'),\n         ('marshmellow', 'marshmallow'),\n         ('citreon', 'Citroen'),\n         ('theirony', 'the irony'),\n         ('slienced', 'silenced'),\n         ('identifiy', 'identify'),\n         ('energ', 'energy'),\n         ('distribuiton', 'distribution'),\n         ('devoloping', 'developing'),\n         ('maharstra', 'Maharastra'),\n         ('siesmologist', 'seismologist'),\n         ('geckoos', 'geckos'),\n         ('placememnt', 'placement'),\n         ('introvercy', 'introvert'),\n         ('nuerosurgeon', 'neurosurgeon'),\n         ('realsense', 'real sense'),\n         ('congac', 'cognac'),\n         ('plaese', 'please'),\n         ('addicition', 'addiction'),\n         ('othet', 'other'),\n         ('howwill', 'how will'),\n         ('betablockers', 'beta blockers'),\n         ('phython', 'Python'),\n         ('concelling', 'counseling'),\n         ('einstine', 'Einstein'),\n         ('takinng', 'taking'),\n         ('birtday', 'birthday'),\n         ('prefessor', 'professor'),\n         ('dreamscreen', 'dream screen'),\n         ('satyabama', 'Satyabhama'),\n         ('faminism', 'feminism'),\n         ('noooooooooo', 'no'),\n         ('certifaction', 'certification'),\n         ('smalll', 'small'),\n         ('sterlization', 'sterilization'),\n         ('athelete', 'athlete'),\n         ('comppany', 'company'),\n         ('handlebreakup', 'handle a breakup'),\n         ('wellrounded', 'well rounded'),\n         ('breif', 'brief'),\n         ('engginering', 'engineering'),\n         ('genrally', 'generally'),\n         ('forgote', 'forgot'),\n         ('compuny', 'the company'),\n         ('wholeseller', 'wholesaler'),\n         ('conventioal', 'conventional'),\n         ('healther', 'healthier'),\n         ('realitic', 'realistic'),\n         ('israil', 'Israel'),\n         ('morghulis', 'Margulis'),\n         ('begineer', 'beginner'),\n         ('unwaiveringly', 'unwavering'),\n         ('writen', 'written'),\n         ('gastly', 'ghastly'),\n         ('obscurial', 'obscure'),\n         ('permanetly', 'permanently'),\n         ('bday', 'birthday'),\n         ('studing', 'studying'),\n         ('blackcore', 'black core'),\n         ('macbok', 'MacBook'),\n         ('realted', 'related'),\n         ('resoning', 'reasoning'),\n         ('servicenow', 'service now'),\n         ('medels', 'medals'),\n         ('hairloss', 'hair loss'),\n         ('messanger', 'messenger'),\n         ('masterbate', 'masturbate'),\n         ('oppurtunities', 'opportunities'),\n         ('newzealand', 'new zealand'),\n         ('offcampus', 'off campus'),\n         ('lonliness', 'loneliness'),\n         ('percentilers', 'percentiles'),\n         ('caccount', 'account'),\n         ('imrovement', 'improvement'),\n         ('cashbacks', 'cashback'),\n         ('inhand', 'in hand'),\n         ('baahubali', 'bahubali'),\n         ('diffrent', 'different'),\n         ('strategywho', 'strategy who'),\n         ('meetme', 'meet me'),\n         ('wealthfront', 'wealth front'),\n         ('masterbation', 'masturbation'),\n         ('successfull', 'successful'),\n         ('lenght', 'length'),\n         ('increse', 'increase'),\n         ('mastrubation', 'masturbation'),\n         ('intresting', 'interesting'),\n         ('quesitons', 'questions'),\n         ('fullstack', 'full stack'),\n         ('harambe', 'Harambee'),\n         ('criterias', 'criteria'),\n         ('rajyasabha', 'Rajya Sabha'),\n         ('techmahindra', 'tech Mahindra'),\n         ('messeges', 'messages'),\n         ('intership', 'internship'),\n         ('benifits', 'benefits'),\n         ('dowload', 'download'),\n         ('dellhi', 'Delhi'),\n         ('traval', 'travel'),\n         ('prepration', 'preparation'),\n         ('engineeringwhat', 'engineering what'),\n         ('habbit', 'habit'),\n         ('diference', 'difference'),\n         ('permantley', 'permanently'),\n         ('doesnot', 'does not'),\n         ('thebest', 'the best'),\n         ('addmision', 'admission'),\n         ('gramatically', 'grammatically'),\n         ('dayswhich', 'days which'),\n         ('intrest', 'interest'),\n         ('seperatists', 'separatists'),\n         ('plagarism', 'plagiarism'),\n         ('demonitize', 'demonetize'),\n         ('explaination', 'explanation'),\n         ('numericals', 'numerical'),\n         ('defination', 'definition'),\n         ('inmortal', 'immortal'),\n         ('elasticsearch', 'elastic search')\n    ]\n\n\n\nREGEX_REPLACER = [\n    (re.compile(pat.replace(\"*\", \"\\*\"), flags=re.IGNORECASE), repl)\n    for pat, repl in WORDS_REPLACER\n]\n\nREGEX_REPLACER_2 = [\n    (re.compile(pat.replace(\"*\", \"\\*\"), flags=re.IGNORECASE), repl)\n    for pat, repl in WORDS_REPLACER_2\n]\n\nREGEX_REPLACER_3 = [\n    (re.compile(pat.replace(\"*\", \"\\*\"), flags=re.IGNORECASE), repl)\n    for pat, repl in WORDS_REPLACER_3\n]\n\nREGEX_REPLACER_4 = [\n    (re.compile(pat.replace(\"*\", \"\\*\"), flags=re.IGNORECASE), repl)\n    for pat, repl in WORDS_REPLACER_4\n]\n\n\"\"\"\nWORDS_REPLACER_5 = [('[\"code\"]', '[\"CODE\"]'),\n                    ('[\"formula\"]', '[\"FORMULA\"]')\n                   ]\n\nREGEX_REPLACER_5 = [\n    (re.compile(pat.replace(\"*\", \"\\*\"), flags=re.IGNORECASE), repl)\n    for pat, repl in WORDS_REPLACER_5\n]\n\"\"\"\n\nRE_SPACE = re.compile(r\"\\s\")\nRE_MULTI_SPACE = re.compile(r\"\\s+\")\n\nsymbols_to_isolate = '.,?!-;*…:—()[]%#$&_/@＼・ω+=^–>\\\\°<~•≠™ˈʊɒ∞§{}·τα❤☺ɡ|¢→̶`❥━┣┫┗Ｏ►★©―ɪ✔®\\x96\\x92●£♥➤´¹☕≈÷♡◐║▬′ɔː€۩۞†μ✒➥═☆ˌ◄½ʻπδηλσερνʃ✬ＳＵＰＥＲＩＴ☻±♍µº¾✓◾؟．⬅℅»Вав❣⋅¿¬♫ＣＭβ█▓▒░⇒⭐›¡₂₃❧▰▔◞▀▂▃▄▅▆▇↙γ̄″☹➡«φ⅓„✋：¥̲̅́∙‛◇✏▷❓❗¶˚˙）сиʿ✨。ɑ\\x80◕！％¯−ﬂﬁ₁²ʌ¼⁴⁄₄⌠♭✘╪▶☭✭♪☔☠♂☃☎✈✌✰❆☙○‣⚓年∎ℒ▪▙☏⅛ｃａｓǀ℮¸ｗ‚∼‖ℳ❄←☼⋆ʒ⊂、⅔¨͡๏⚾⚽Φ×θ￦？（℃⏩☮⚠月✊❌⭕▸■⇌☐☑⚡☄ǫ╭∩╮，例＞ʕɐ̣Δ₀✞┈╱╲▏▕┃╰▊▋╯┳┊≥☒↑☝ɹ✅☛♩☞ＡＪＢ◔◡↓♀⬆̱ℏ\\x91⠀ˤ╚↺⇤∏✾◦♬³の｜／∵∴√Ω¤☜▲↳▫‿⬇✧ｏｖｍ－２０８＇‰≤∕ˆ⚜☁'\nsymbols_to_delete = '\\n🍕\\r🐵😑\\xa0\\ue014\\t\\uf818\\uf04a\\xad😢🐶️\\uf0e0😜😎👊\\u200b\\u200e😁عدويهصقأناخلىبمغر😍💖💵Е👎😀😂\\u202a\\u202c🔥😄🏻💥ᴍʏʀᴇɴᴅᴏᴀᴋʜᴜʟᴛᴄᴘʙғᴊᴡɢ😋👏שלוםבי😱‼\\x81エンジ故障\\u2009🚌ᴵ͞🌟😊😳😧🙀😐😕\\u200f👍😮😃😘אעכח💩💯⛽🚄🏼ஜ😖ᴠ🚲‐😟😈💪🙏🎯🌹😇💔😡\\x7f👌ἐὶήιὲκἀίῃἴξ🙄Ｈ😠\\ufeff\\u2028😉😤⛺🙂\\u3000تحكسة👮💙فزط😏🍾🎉😞\\u2008🏾😅😭👻😥😔😓🏽🎆🍻🍽🎶🌺🤔😪\\x08‑🐰🐇🐱🙆😨🙃💕𝘊𝘦𝘳𝘢𝘵𝘰𝘤𝘺𝘴𝘪𝘧𝘮𝘣💗💚地獄谷улкнПоАН🐾🐕😆ה🔗🚽歌舞伎🙈😴🏿🤗🇺🇸мυтѕ⤵🏆🎃😩\\u200a🌠🐟💫💰💎эпрд\\x95🖐🙅⛲🍰🤐👆🙌\\u2002💛🙁👀🙊🙉\\u2004ˢᵒʳʸᴼᴷᴺʷᵗʰᵉᵘ\\x13🚬🤓\\ue602😵άοόςέὸתמדףנרךצט😒͝🆕👅👥👄🔄🔤👉👤👶👲🔛🎓\\uf0b7\\uf04c\\x9f\\x10成都😣⏺😌🤑🌏😯ех😲Ἰᾶὁ💞🚓🔔📚🏀👐\\u202d💤🍇\\ue613小土豆🏡❔⁉\\u202f👠》कर्मा🇹🇼🌸蔡英文🌞🎲レクサス😛外国人关系Сб💋💀🎄💜🤢َِьыгя不是\\x9c\\x9d🗑\\u2005💃📣👿༼つ༽😰ḷЗз▱ц￼🤣卖温哥华议会下降你失去所有的钱加拿大坏税骗子🐝ツ🎅\\x85🍺آإشء🎵🌎͟ἔ油别克🤡🤥😬🤧й\\u2003🚀🤴ʲшчИОРФДЯМюж😝🖑ὐύύ特殊作戦群щ💨圆明园קℐ🏈😺🌍⏏ệ🍔🐮🍁🍆🍑🌮🌯🤦\\u200d𝓒𝓲𝓿𝓵안영하세요ЖљКћ🍀😫🤤ῦ我出生在了可以说普通话汉语好极🎼🕺🍸🥂🗽🎇🎊🆘🤠👩🖒🚪天一家⚲\\u2006⚭⚆⬭⬯⏖新✀╌🇫🇷🇩🇪🇮🇬🇧😷🇨🇦ХШ🌐\\x1f杀鸡给猴看ʁ𝗪𝗵𝗲𝗻𝘆𝗼𝘂𝗿𝗮𝗹𝗶𝘇𝗯𝘁𝗰𝘀𝘅𝗽𝘄𝗱📺ϖ\\u2000үսᴦᎥһͺ\\u2007հ\\u2001ɩｙｅ൦ｌƽｈ𝐓𝐡𝐞𝐫𝐮𝐝𝐚𝐃𝐜𝐩𝐭𝐢𝐨𝐧Ƅᴨןᑯ໐ΤᏧ௦Іᴑ܁𝐬𝐰𝐲𝐛𝐦𝐯𝐑𝐙𝐣𝐇𝐂𝐘𝟎ԜТᗞ౦〔Ꭻ𝐳𝐔𝐱𝟔𝟓𝐅🐋ﬃ💘💓ё𝘥𝘯𝘶💐🌋🌄🌅𝙬𝙖𝙨𝙤𝙣𝙡𝙮𝙘𝙠𝙚𝙙𝙜𝙧𝙥𝙩𝙪𝙗𝙞𝙝𝙛👺🐷ℋ𝐀𝐥𝐪🚶𝙢Ἱ🤘ͦ💸ج패티Ｗ𝙇ᵻ👂👃ɜ🎫\\uf0a7БУі🚢🚂ગુજરાતીῆ🏃𝓬𝓻𝓴𝓮𝓽𝓼☘﴾̯﴿₽\\ue807𝑻𝒆𝒍𝒕𝒉𝒓𝒖𝒂𝒏𝒅𝒔𝒎𝒗𝒊👽😙\\u200cЛ‒🎾👹⎌🏒⛸公寓养宠物吗🏄🐀🚑🤷操美𝒑𝒚𝒐𝑴🤙🐒欢迎来到阿拉斯ספ𝙫🐈𝒌𝙊𝙭𝙆𝙋𝙍𝘼𝙅ﷻ🦄巨收赢得白鬼愤怒要买额ẽ🚗🐳𝟏𝐟𝟖𝟑𝟕𝒄𝟗𝐠𝙄𝙃👇锟斤拷𝗢𝟳𝟱𝟬⦁マルハニチロ株式社⛷한국어ㄸㅓ니͜ʖ𝘿𝙔₵𝒩ℯ𝒾𝓁𝒶𝓉𝓇𝓊𝓃𝓈𝓅ℴ𝒻𝒽𝓀𝓌𝒸𝓎𝙏ζ𝙟𝘃𝗺𝟮𝟭𝟯𝟲👋🦊多伦🐽🎻🎹⛓🏹🍷🦆为和中友谊祝贺与其想象对法如直接问用自己猜本传教士没积唯认识基督徒曾经让相信耶稣复活死怪他但当们聊些政治题时候战胜因圣把全堂结婚孩恐惧且栗谓这样还♾🎸🤕🤒⛑🎁批判检讨🏝🦁🙋😶쥐스탱트뤼도석유가격인상이경제황을렵게만들지않록잘관리해야합다캐나에서대마초와화약금의품런성분갈때는반드시허된사용🔫👁凸ὰ💲🗯𝙈Ἄ𝒇𝒈𝒘𝒃𝑬𝑶𝕾𝖙𝖗𝖆𝖎𝖌𝖍𝖕𝖊𝖔𝖑𝖉𝖓𝖐𝖜𝖞𝖚𝖇𝕿𝖘𝖄𝖛𝖒𝖋𝖂𝕴𝖟𝖈𝕸👑🚿💡知彼百\\uf005𝙀𝒛𝑲𝑳𝑾𝒋𝟒😦𝙒𝘾𝘽🏐𝘩𝘨ὼṑ𝑱𝑹𝑫𝑵𝑪🇰🇵👾ᓇᒧᔭᐃᐧᐦᑳᐨᓃᓂᑲᐸᑭᑎᓀᐣ🐄🎈🔨🐎🤞🐸💟🎰🌝🛳点击查版🍭𝑥𝑦𝑧ＮＧ👣\\uf020っ🏉ф💭🎥Ξ🐴👨🤳🦍\\x0b🍩𝑯𝒒😗𝟐🏂👳🍗🕉🐲چی𝑮𝗕𝗴🍒ꜥⲣⲏ🐑⏰鉄リ事件ї💊「」\\uf203\\uf09a\\uf222\\ue608\\uf202\\uf099\\uf469\\ue607\\uf410\\ue600燻製シ虚偽屁理屈Г𝑩𝑰𝒀𝑺🌤𝗳𝗜𝗙𝗦𝗧🍊ὺἈἡχῖΛ⤏🇳𝒙ψՁմեռայինրւդձ冬至ὀ𝒁🔹🤚🍎𝑷🐂💅𝘬𝘱𝘸𝘷𝘐𝘭𝘓𝘖𝘹𝘲𝘫کΒώ💢ΜΟΝΑΕ🇱♲𝝈↴💒⊘Ȼ🚴🖕🖤🥘📍👈➕🚫🎨🌑🐻𝐎𝐍𝐊𝑭🤖🎎😼🕷ｇｒｎｔｉｄｕｆｂｋ𝟰🇴🇭🇻🇲𝗞𝗭𝗘𝗤👼📉🍟🍦🌈🔭《🐊🐍\\uf10aლڡ🐦\\U0001f92f\\U0001f92a🐡💳ἱ🙇𝗸𝗟𝗠𝗷🥜さようなら🔼'\nsymbols_to_delete2 = '\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~' + '“”’'\n\n\nisolate_dict = {ord(c):f' {c} ' for c in symbols_to_isolate}\nremove_dict = {ord(c):f'' for c in symbols_to_delete}\nremove_dict1 = {ord(c):f'' for c in symbols_to_delete2}\n\nNMS_TABLE = dict.fromkeys(\n    i for i in range(sys.maxunicode + 1) if unicodedata.category(chr(i)) == \"Mn\"\n)\n\nHEBREW_TABLE = {i: \"א\" for i in range(0x0590, 0x05FF)}\nARABIC_TABLE = {i: \"ا\" for i in range(0x0600, 0x06FF)}\nCHINESE_TABLE = {i: \"是\" for i in range(0x4E00, 0x9FFF)}\nKANJI_TABLE = {i: \"ッ\" for i in range(0x2E80, 0x2FD5)}\nHIRAGANA_TABLE = {i: \"ッ\" for i in range(0x3041, 0x3096)}\nKATAKANA_TABLE = {i: \"ッ\" for i in range(0x30A0, 0x30FF)}\n\nTABLE = dict()\nTABLE.update(CUSTOM_TABLE)\nTABLE.update(NMS_TABLE)\n# Non-english languages\nTABLE.update(CHINESE_TABLE)\nTABLE.update(HEBREW_TABLE)\nTABLE.update(ARABIC_TABLE)\nTABLE.update(HIRAGANA_TABLE)\nTABLE.update(KATAKANA_TABLE)\nTABLE.update(KANJI_TABLE)\n\n\n\nEMOJI_REGEXP = emoji.get_emoji_regexp()\n\nUNICODE_EMOJI_MY = {\n    k: f\" EMJ {v.strip(':').replace('_', ' ')} \"\n    for k, v in emoji.UNICODE_EMOJI_ALIAS.items()\n}\n\ndef my_demojize(string: str) -> str:\n    def replace(match):\n        return UNICODE_EMOJI_MY.get(match.group(0), match.group(0))\n    \n    return re.sub(\"\\ufe0f\", \"\", EMOJI_REGEXP.sub(replace, string))\n\ndef normalize(text: str) -> str:\n    \n    text = text.replace(\"[CODE]\", \" ACODEA \")\n    text = text.replace(\"[FORMULA]\", \" AFORMULAA \")\n    #text = text.replace(\"[]\", \" [URL] \")\n    \n    text = html.unescape(text)\n    \n    text = text.lower()\n    \n    text = my_demojize(text)\n    \n    # replacing urls with \"url\" string\n    text = re.sub('(?:(?:https?|ftp):\\/\\/)?[\\w/\\-?=%.]+\\.[\\w/\\-?=%.]+', '[WEB]' ,text)\n    text = text.replace(\"[WEB]\", \" AWEBA \")\n    \n    text = RE_SPACE.sub(\" \", text)\n    text = unicodedata.normalize(\"NFKD\", text)\n    text = text.translate(TABLE)\n    text = RE_MULTI_SPACE.sub(\" \", text).strip()\n    \n    # remove some unimportent symbles\n    text = text.translate(remove_dict)\n    \n    # remove some unimportent symbles\n    text = text.translate(remove_dict1)\n    \n    text = text.translate(isolate_dict)\n    \n\n    # Replacing and mispell\n    \n    for pattern, repl in REGEX_REPLACER:\n        text = pattern.sub(repl, text)\n    \n    for pattern, repl in REGEX_REPLACER_2:\n        text = pattern.sub(repl, text)\n            \n    for pattern, repl in REGEX_REPLACER_3:\n        text = pattern.sub(repl, text)\n        \n    # isolated_characters\n    #text = text.translate(isolate_dict)\n    \n    for pattern, repl in REGEX_REPLACER_4:\n        text = pattern.sub(repl, text)\n    \"\"\"\n    for pattern, repl in REGEX_REPLACER_5:\n        text = pattern.sub(repl, text)\n    \"\"\"\n    \n    \n        \n    text = RE_MULTI_SPACE.sub(\" \", text).strip()\n    \n    text = text.replace(\" acodea \", \" [CODE] \")\n    text = text.replace(\" aformulaa \", \" [FORMULA] \")\n    text = text.replace(\" AWEBA \", \" [WEB] \")\n    \n    text = RE_MULTI_SPACE.sub(\" \", text).strip()\n    \n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ntrain_final[\"question_body_clean1\"] = train_final.question_body_clean.apply(normalize)\ntrain_final[\"answer_clean1\"]  = train_final.answer_clean.apply(normalize)\ntrain_final[\"question_title_clean1\"] = train_final.question_title.apply(normalize)\n\ntest_final[\"question_body_clean1\"] = test_final.question_body_clean.apply(normalize)\ntest_final[\"answer_clean1\"]  = test_final.answer_clean.apply(normalize)\ntest_final[\"question_title_clean1\"] = test_final.question_title.apply(normalize)\n\ntrain_final[\"question_body_clean1\"].fillna(\"please see figure below\", inplace=True)\ntest_final[\"question_body_clean1\"].fillna(\"please see figure below\", inplace=True)\n\ntrain_final[\"answer_clean1\"].fillna(\"please see figure below\", inplace=True)\ntest_final[\"answer_clean1\"].fillna(\"please see figure below\", inplace=True)\n\ntrain_final[\"question_title_clean1\"].fillna(\"please see figure below\", inplace=True)\ntest_final[\"question_title_clean1\"].fillna(\"please see figure below\", inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ndef replace_specialtokens(text):\n    text = text.replace(\" [CODE] \", \" code \")\n    text = text.replace(\" [FORMULA] \", \" formula \")\n    text = text.replace(\" [WEB] \", \" web \")\n    return text\n\ntrain_final[\"question_body_clean2\"] = train_final.question_body_clean1.apply(replace_specialtokens)\ntrain_final[\"answer_clean2\"]  = train_final.answer_clean1.apply(replace_specialtokens)\ntrain_final[\"question_title_clean2\"] = train_final.question_title_clean1.apply(replace_specialtokens)\n\ntest_final[\"question_body_clean2\"] = test_final.question_body_clean1.apply(replace_specialtokens)\ntest_final[\"answer_clean2\"]  = test_final.answer_clean1.apply(replace_specialtokens)\ntest_final[\"question_title_clean2\"] = test_final.question_title_clean1.apply(replace_specialtokens)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_final[\"question_body_all_clean\"] = train_final.question_body_all.apply(normalize)\ntrain_final[\"answer_all_clean\"]  = train_final.answer_all.apply(normalize)\n\ntest_final[\"question_body_all_clean\"] = test_final.question_body_all.apply(normalize)\ntest_final[\"answer_all_clean\"]  = test_final.answer_all.apply(normalize)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## META DATA"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom scipy.sparse import vstack\nfrom sklearn.feature_extraction.text import CountVectorizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def text_metadata(train, test):\n    \n    a_AboutMe_text = train[\"a_AboutMe\"].apply(normalize)\n    q_AboutMe_text = train[\"q_AboutMe\"].apply(normalize)\n    all_text = pd.concat([a_AboutMe_text, q_AboutMe_text])\n\n    word_vectorizer = TfidfVectorizer(\n                sublinear_tf=True,\n                strip_accents='unicode',\n                analyzer='word',\n                token_pattern=r'\\w{1,}',\n                stop_words='english',\n                ngram_range=(1,1))\n    \n    word_vectorizer.fit(all_text)\n    \n    q_AboutMe_cols =  [f\"q_AboutMe_PCA_{i}\" for i in range(1,101)] \n    a_AboutMe_cols =  [f\"a_AboutMe_PCA_{i}\" for i in range(1,101)]\n    \n    q_AboutMe_text = word_vectorizer.transform(q_AboutMe_text)\n    a_AboutMe_text = word_vectorizer.transform(a_AboutMe_text)\n    \n    tq_AboutMe_text = word_vectorizer.transform(test[\"q_AboutMe\"].apply(normalize))\n    ta_AboutMe_text = word_vectorizer.transform(test[\"a_AboutMe\"].apply(normalize))\n    \n    new = vstack([q_AboutMe_text, a_AboutMe_text])\n    \n    pca = PCA(n_components=100)\n    pca.fit(new.toarray())\n    \n    q_AboutMe_text = pca.transform(q_AboutMe_text.toarray())\n    a_AboutMe_text = pca.transform(a_AboutMe_text.toarray())\n    \n    tq_AboutMe_text = pca.transform(tq_AboutMe_text.toarray())\n    ta_AboutMe_text = pca.transform(ta_AboutMe_text.toarray())\n    \n    \n    train[q_AboutMe_cols] = pd.DataFrame(q_AboutMe_text, columns=q_AboutMe_cols, index=train.index)\n    test[q_AboutMe_cols] = pd.DataFrame(tq_AboutMe_text, columns=q_AboutMe_cols, index=test.index)\n    \n    train[a_AboutMe_cols] = pd.DataFrame(a_AboutMe_text, columns=a_AboutMe_cols, index=train.index)\n    test[a_AboutMe_cols] = pd.DataFrame(ta_AboutMe_text, columns=a_AboutMe_cols, index=test.index)\n    \n    \n    # Tages\n    \n    tags_all_text = train[\"Tags\"]\n\n    word_vectorizer = CountVectorizer()\n    \n    word_vectorizer.fit(tags_all_text)\n    \n    tags_cols =  [\"Tags_\" + sub for sub in word_vectorizer.get_feature_names()] \n    \n    train[tags_cols] = pd.DataFrame(word_vectorizer.transform(train[\"Tags\"]).toarray(), columns=tags_cols, index=train.index)\n    test[tags_cols] = pd.DataFrame(word_vectorizer.transform(test[\"Tags\"]).toarray(), columns=tags_cols, index=test.index)\n    \n    \n    \n    return train, test, q_AboutMe_cols, a_AboutMe_cols, tags_cols\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ntrain_final, test_final, q_AboutMe_cols, a_AboutMe_cols, tags_cols = text_metadata(train_final, test_final)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_final.shape, test_final.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def linear_based_models(train, test, cat_cols, num_cols):\n    \n    # Column std MinMax Scalling\n    std = MinMaxScaler()\n    train[num_cols] = std.fit_transform(train[num_cols])\n    test[num_cols] = std.transform(test[num_cols])\n    \n    # One Hot Encoder\n    train = pd.get_dummies(train, columns=cat_cols, prefix=cat_cols)\n    test = pd.get_dummies(test, columns=cat_cols, prefix=cat_cols)\n    \n    rem = list(set(train.columns).intersection(set(test.columns)))\n    \n    train = train[rem]\n    test = test[rem]\n    \n\n    return train, test\n\nnum_cols = ['AnswerCount', 'CommentCount', 'FavoriteCount','PostTypeId', 'Score','ViewCount','a_DownVotes','a_UpVotes', 'a_Views','q_DownVotes', 'q_UpVotes', 'q_Views']\ncat_cols = ['category', 'category_type']\ntrain_final, test_final = linear_based_models(train_final, test_final, cat_cols, num_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_final.shape, test_final.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_final = pd.merge(train_final, train_final_targets, left_on=\"qa_id\", right_on=\"qa_id\", how=\"left\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_final.shape, test_final.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_final\ntest_df = test_final\n\ntarget_columns = train_df.columns.values.tolist()[-30:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"category_features = [col for col in train_df.columns if col.startswith('category_')]\ntags_cols = [col for col in train_df.columns if col.startswith('Tags_')]\na_AboutMe_cols = [col for col in train_df.columns if col.startswith('a_AboutMe_')]\nq_AboutMe_cols = [col for col in train_df.columns if col.startswith('q_AboutMe_')]\nnum_cols = ['AnswerCount', 'CommentCount', 'FavoriteCount','PostTypeId', 'Score','ViewCount','a_DownVotes','a_UpVotes', 'a_Views','q_DownVotes', 'q_UpVotes', 'q_Views','AnswerCount_nan', 'CommentCount_nan', 'FavoriteCount_nan', 'Score_nan','ViewCount_nan','a_DownVotes_nan','a_UpVotes_nan', 'a_Views_nan','q_DownVotes_nan', 'q_UpVotes_nan', 'q_Views_nan']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a_AboutMe_cols = ['a_AboutMe_PCA_64',\n 'a_AboutMe_PCA_19',\n 'a_AboutMe_PCA_40',\n 'a_AboutMe_PCA_6',\n 'a_AboutMe_PCA_77',\n 'a_AboutMe_PCA_35',\n 'a_AboutMe_PCA_100',\n 'a_AboutMe_PCA_57',\n 'a_AboutMe_PCA_76',\n 'a_AboutMe_PCA_16',\n 'a_AboutMe_PCA_67',\n 'a_AboutMe_PCA_36',\n 'a_AboutMe_PCA_25',\n 'a_AboutMe_PCA_26',\n 'a_AboutMe_PCA_7',\n 'a_AboutMe_PCA_50',\n 'a_AboutMe_PCA_32',\n 'a_AboutMe_PCA_60',\n 'a_AboutMe_PCA_54',\n 'a_AboutMe_PCA_84',\n 'a_AboutMe_PCA_66',\n 'a_AboutMe_PCA_88',\n 'a_AboutMe_PCA_61',\n 'a_AboutMe_PCA_23',\n 'a_AboutMe_PCA_37',\n 'a_AboutMe_PCA_1',\n 'a_AboutMe_PCA_21',\n 'a_AboutMe_PCA_20',\n 'a_AboutMe_PCA_55',\n 'a_AboutMe_PCA_86',\n 'a_AboutMe_PCA_2',\n 'a_AboutMe_PCA_3',\n 'a_AboutMe_PCA_99',\n 'a_AboutMe_PCA_18',\n 'a_AboutMe_PCA_78',\n 'a_AboutMe_PCA_51',\n 'a_AboutMe_PCA_53',\n 'a_AboutMe_PCA_96',\n 'a_AboutMe_PCA_15',\n 'a_AboutMe_PCA_11',\n 'a_AboutMe_PCA_89',\n 'a_AboutMe_PCA_82',\n 'a_AboutMe_PCA_13',\n 'a_AboutMe_PCA_44',\n 'a_AboutMe_PCA_28',\n 'a_AboutMe_PCA_41',\n 'a_AboutMe_PCA_68',\n 'a_AboutMe_PCA_42',\n 'a_AboutMe_PCA_27',\n 'a_AboutMe_PCA_73',\n 'a_AboutMe_PCA_95',\n 'a_AboutMe_PCA_85',\n 'a_AboutMe_PCA_49',\n 'a_AboutMe_PCA_33',\n 'a_AboutMe_PCA_48',\n 'a_AboutMe_PCA_59',\n 'a_AboutMe_PCA_46',\n 'a_AboutMe_PCA_65',\n 'a_AboutMe_PCA_75',\n 'a_AboutMe_PCA_63',\n 'a_AboutMe_PCA_4',\n 'a_AboutMe_PCA_52',\n 'a_AboutMe_PCA_5',\n 'a_AboutMe_PCA_17',\n 'a_AboutMe_PCA_92',\n 'a_AboutMe_PCA_47',\n 'a_AboutMe_PCA_80',\n 'a_AboutMe_PCA_14',\n 'a_AboutMe_PCA_98',\n 'a_AboutMe_PCA_34',\n 'a_AboutMe_PCA_83',\n 'a_AboutMe_PCA_58',\n 'a_AboutMe_PCA_94',\n 'a_AboutMe_PCA_69',\n 'a_AboutMe_PCA_45',\n 'a_AboutMe_PCA_31',\n 'a_AboutMe_PCA_91',\n 'a_AboutMe_PCA_12',\n 'a_AboutMe_PCA_70',\n 'a_AboutMe_PCA_8',\n 'a_AboutMe_PCA_39',\n 'a_AboutMe_PCA_74',\n 'a_AboutMe_PCA_43',\n 'a_AboutMe_PCA_62',\n 'a_AboutMe_PCA_10',\n 'a_AboutMe_PCA_9',\n 'a_AboutMe_PCA_22',\n 'a_AboutMe_PCA_30',\n 'a_AboutMe_PCA_24',\n 'a_AboutMe_PCA_87',\n 'a_AboutMe_PCA_79',\n 'a_AboutMe_PCA_81',\n 'a_AboutMe_PCA_90',\n 'a_AboutMe_PCA_93',\n 'a_AboutMe_PCA_38',\n 'a_AboutMe_PCA_72',\n 'a_AboutMe_PCA_29',\n 'a_AboutMe_PCA_56',\n 'a_AboutMe_PCA_97',\n 'a_AboutMe_PCA_71']\n\nq_AboutMe_cols = ['q_AboutMe_PCA_93',\n 'q_AboutMe_PCA_50',\n 'q_AboutMe_PCA_70',\n 'q_AboutMe_PCA_65',\n 'q_AboutMe_PCA_85',\n 'q_AboutMe_PCA_71',\n 'q_AboutMe_PCA_18',\n 'q_AboutMe_PCA_69',\n 'q_AboutMe_PCA_51',\n 'q_AboutMe_PCA_79',\n 'q_AboutMe_PCA_31',\n 'q_AboutMe_PCA_99',\n 'q_AboutMe_PCA_40',\n 'q_AboutMe_PCA_92',\n 'q_AboutMe_PCA_86',\n 'q_AboutMe_PCA_34',\n 'q_AboutMe_PCA_2',\n 'q_AboutMe_PCA_64',\n 'q_AboutMe_PCA_1',\n 'q_AboutMe_PCA_72',\n 'q_AboutMe_PCA_32',\n 'q_AboutMe_PCA_29',\n 'q_AboutMe_PCA_5',\n 'q_AboutMe_PCA_7',\n 'q_AboutMe_PCA_67',\n 'q_AboutMe_PCA_96',\n 'q_AboutMe_PCA_82',\n 'q_AboutMe_PCA_35',\n 'q_AboutMe_PCA_55',\n 'q_AboutMe_PCA_39',\n 'q_AboutMe_PCA_27',\n 'q_AboutMe_PCA_4',\n 'q_AboutMe_PCA_66',\n 'q_AboutMe_PCA_57',\n 'q_AboutMe_PCA_38',\n 'q_AboutMe_PCA_12',\n 'q_AboutMe_PCA_76',\n 'q_AboutMe_PCA_20',\n 'q_AboutMe_PCA_89',\n 'q_AboutMe_PCA_28',\n 'q_AboutMe_PCA_30',\n 'q_AboutMe_PCA_98',\n 'q_AboutMe_PCA_100',\n 'q_AboutMe_PCA_61',\n 'q_AboutMe_PCA_3',\n 'q_AboutMe_PCA_37',\n 'q_AboutMe_PCA_81',\n 'q_AboutMe_PCA_97',\n 'q_AboutMe_PCA_49',\n 'q_AboutMe_PCA_91',\n 'q_AboutMe_PCA_43',\n 'q_AboutMe_PCA_90',\n 'q_AboutMe_PCA_94',\n 'q_AboutMe_PCA_58',\n 'q_AboutMe_PCA_36',\n 'q_AboutMe_PCA_8',\n 'q_AboutMe_PCA_46',\n 'q_AboutMe_PCA_25',\n 'q_AboutMe_PCA_13',\n 'q_AboutMe_PCA_10',\n 'q_AboutMe_PCA_87',\n 'q_AboutMe_PCA_21',\n 'q_AboutMe_PCA_62',\n 'q_AboutMe_PCA_11',\n 'q_AboutMe_PCA_42',\n 'q_AboutMe_PCA_33',\n 'q_AboutMe_PCA_74',\n 'q_AboutMe_PCA_26',\n 'q_AboutMe_PCA_6',\n 'q_AboutMe_PCA_68',\n 'q_AboutMe_PCA_54',\n 'q_AboutMe_PCA_73',\n 'q_AboutMe_PCA_17',\n 'q_AboutMe_PCA_44',\n 'q_AboutMe_PCA_52',\n 'q_AboutMe_PCA_47',\n 'q_AboutMe_PCA_56',\n 'q_AboutMe_PCA_15',\n 'q_AboutMe_PCA_59',\n 'q_AboutMe_PCA_88',\n 'q_AboutMe_PCA_22',\n 'q_AboutMe_PCA_77',\n 'q_AboutMe_PCA_84',\n 'q_AboutMe_PCA_75',\n 'q_AboutMe_PCA_63',\n 'q_AboutMe_PCA_60',\n 'q_AboutMe_PCA_41',\n 'q_AboutMe_PCA_53',\n 'q_AboutMe_PCA_45',\n 'q_AboutMe_PCA_14',\n 'q_AboutMe_PCA_16',\n 'q_AboutMe_PCA_24',\n 'q_AboutMe_PCA_19',\n 'q_AboutMe_PCA_83',\n 'q_AboutMe_PCA_9',\n 'q_AboutMe_PCA_48',\n 'q_AboutMe_PCA_23',\n 'q_AboutMe_PCA_78',\n 'q_AboutMe_PCA_95',\n 'q_AboutMe_PCA_80']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"meta_cols = [*category_features, *num_cols]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(meta_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def _convert_to_transformer_inputs(title, question, answer, tokenizer, max_sequence_lenght):\n    \"\"\"Converts tokenized input to ids, masks and segments for transformer (including bert)\"\"\"\n\n    def return_id(str1, str2, lenght):\n\n        inputs = tokenizer.encode_plus(\n            str1, str2,\n            add_special_tokens = True,\n            max_length = lenght,\n            pad_to_max_length = True,\n            return_token_type_ids = True,\n            return_attention_mask = True,\n            truncation_strategy = \"longest_first\"\n        )\n\n        input_ids = inputs[\"input_ids\"]\n        attention_mask = inputs[\"attention_mask\"]\n        token_type_ids = inputs[\"token_type_ids\"]\n\n        return [input_ids, attention_mask, token_type_ids]\n\n\n    input_ids_q , attention_mask_q, token_type_ids_q = return_id(title, question, max_sequence_lenght)\n    input_ids_a , attention_mask_a, token_type_ids_a = return_id(answer,None, max_sequence_lenght)\n\n    return [\n            input_ids_q , attention_mask_q, token_type_ids_q,\n            input_ids_a , attention_mask_a, token_type_ids_a\n    ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_input_arrays(df, columns, meta_cols, q_AboutMe_cols, a_AboutMe_cols, tokenizer, max_sequence_length):\n    input_ids_q , attention_mask_q, token_type_ids_q = [], [], []\n    input_ids_a , attention_mask_a, token_type_ids_a = [], [], []\n    meta_features = []\n    q_AboutMe_features = []\n    a_AboutMe_features = []\n\n    total_cols = [*columns, *meta_cols, *q_AboutMe_cols, *a_AboutMe_cols]\n\n    #i = 0\n\n    for _, instance in tqdm(df[total_cols].iterrows()):\n\n    \n        t, q, a, qc, ac = instance.question_title_clean2, instance.question_body_clean2, instance.answer_clean2,instance.question_body_all_clean, instance.answer_all_clean\n        t = str(t) \n        q = str(q) + \" [SEP] \"+ str(qc)\n        a = str(a) + \" [SEP] \"+ str(ac)\n\n        ids_q, masks_q, segments_q, ids_a, masks_a, segments_a = _convert_to_transformer_inputs(t, q, a, tokenizer, max_sequence_length)\n\n        input_ids_q.append(ids_q)\n        attention_mask_q.append(masks_q)\n        token_type_ids_q.append(segments_q)\n\n        input_ids_a.append(ids_a)\n        attention_mask_a.append(masks_a)\n        token_type_ids_a.append(segments_a)\n\n        meta_data = instance[meta_cols].values.tolist()\n        q_AboutMe_data = instance[q_AboutMe_cols].values.tolist()\n        a_AboutMe_data = instance[a_AboutMe_cols].values.tolist()\n\n        meta_features.append(meta_data)\n        q_AboutMe_features.append(q_AboutMe_data)\n        a_AboutMe_features.append(a_AboutMe_data)\n        #i = i+1\n        #if i == 100:\n        #    break\n\n    return [np.asarray(input_ids_q, dtype=np.int32), \n            np.asarray(attention_mask_q, dtype=np.int32), \n            np.asarray(token_type_ids_q, dtype=np.int32),\n            np.asarray(input_ids_a, dtype=np.int32), \n            np.asarray(attention_mask_a, dtype=np.int32), \n            np.asarray(token_type_ids_a, dtype=np.int32),\n            np.asarray(meta_features, dtype=np.float32),\n            np.asarray(q_AboutMe_features, dtype=np.float32),\n            np.asarray(a_AboutMe_features, dtype=np.float32)\n            ]\n\ndef compute_output_arrays(df, columns):\n    return np.asarray(df[columns])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model_soft(BERT_PATH):\n    q_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    a_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    \n    q_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    a_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    \n    q_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    a_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n\n    meta_features_layer = tf.keras.layers.Input((len(meta_cols),), dtype=tf.float32)\n    q_AboutMe_features_layer = tf.keras.layers.Input((len(q_AboutMe_cols),), dtype=tf.float32)\n    a_AboutMe_features_layer = tf.keras.layers.Input((len(a_AboutMe_cols),), dtype=tf.float32)\n    \n    config = BertConfig() # print(config) to see settings\n    config.output_hidden_states = False # Set to True to obtain hidden states\n    # caution: when using e.g. XLNet, XLNetConfig() will automatically use xlnet-large config\n    \n    # normally \".from_pretrained('bert-base-uncased')\", but because of no internet, the \n    # pretrained model has been downloaded manually and uploaded to kaggle. \n    bert_model = TFBertModel.from_pretrained(BERT_PATH+'bert-base-uncased-tf_model.h5',config=config )\n    \n    # if config.output_hidden_states = True, obtain hidden states via bert_model(...)[-1]\n    q_embedding = bert_model(q_id, attention_mask=q_mask, token_type_ids=q_atn)[0]\n    a_embedding = bert_model(a_id, attention_mask=a_mask, token_type_ids=a_atn)[0]\n    \n    q = tf.keras.layers.GlobalAveragePooling1D()(q_embedding)\n    a = tf.keras.layers.GlobalAveragePooling1D()(a_embedding)\n    \n    x = tf.keras.layers.Concatenate()([q, q_AboutMe_features_layer, a, a_AboutMe_features_layer, meta_features_layer, ])\n    \n    x = tf.keras.layers.Dropout(0.2)(x)\n    \n    x = tf.keras.layers.Dense(21, activation='sigmoid')(x)\n\n    model = tf.keras.models.Model(inputs=[q_id, q_mask, q_atn, a_id, a_mask, a_atn, meta_features_layer, q_AboutMe_features_layer, a_AboutMe_features_layer], outputs=x)\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model_hard(BERT_PATH):\n    q_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    a_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    \n    q_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    a_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    \n    q_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    a_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n\n    meta_features_layer = tf.keras.layers.Input((len(meta_cols),), dtype=tf.float32)\n    q_AboutMe_features_layer = tf.keras.layers.Input((len(q_AboutMe_cols),), dtype=tf.float32)\n    a_AboutMe_features_layer = tf.keras.layers.Input((len(a_AboutMe_cols),), dtype=tf.float32)\n    \n    config = BertConfig() # print(config) to see settings\n    config.output_hidden_states = False # Set to True to obtain hidden states\n    # caution: when using e.g. XLNet, XLNetConfig() will automatically use xlnet-large config\n    \n    # normally \".from_pretrained('bert-base-uncased')\", but because of no internet, the \n    # pretrained model has been downloaded manually and uploaded to kaggle. \n    bert_model = TFBertModel.from_pretrained(BERT_PATH+'bert-base-uncased-tf_model.h5',config=config )\n    \n    # if config.output_hidden_states = True, obtain hidden states via bert_model(...)[-1]\n    q_embedding = bert_model(q_id, attention_mask=q_mask, token_type_ids=q_atn)[0]\n    a_embedding = bert_model(a_id, attention_mask=a_mask, token_type_ids=a_atn)[0]\n    \n    q = tf.keras.layers.GlobalAveragePooling1D()(q_embedding)\n    a = tf.keras.layers.GlobalAveragePooling1D()(a_embedding)\n    \n    x = tf.keras.layers.Concatenate()([q, q_AboutMe_features_layer, a, a_AboutMe_features_layer, meta_features_layer, ])\n    \n    x = tf.keras.layers.Dropout(0.2)(x)\n    \n    x = tf.keras.layers.Dense(8, activation='sigmoid')(x)\n\n    model = tf.keras.models.Model(inputs=[q_id, q_mask, q_atn, a_id, a_mask, a_atn, meta_features_layer, q_AboutMe_features_layer, a_AboutMe_features_layer], outputs=x)\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_categories = [\"question_title_clean2\", \"question_body_clean2\", \"answer_clean2\", \"question_body_all_clean\", \"answer_all_clean\"]\n\nMAX_SEQUENCE_LENGTH = 512","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BERT_PATH = '../input/bert-base-uncased-huggingface-transformer/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained(BERT_PATH+'bert-base-uncased-vocab.txt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#outputs = compute_output_arrays(train_df, target_columns)\n#inputs = compute_input_arrays(train_df, input_categories,meta_cols, q_AboutMe_cols, a_AboutMe_cols ,tokenizer, MAX_SEQUENCE_LENGTH)\ntest_inputs = compute_input_arrays(test_df, input_categories,meta_cols,q_AboutMe_cols, a_AboutMe_cols , tokenizer, MAX_SEQUENCE_LENGTH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_weights_path = '../input/quest-bert-soft-hard-models/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"soft_target_columns = [\n #'question_asker_intent_understanding',\n 'question_body_critical',\n 'question_conversational',\n 'question_expect_short_answer',\n 'question_fact_seeking',\n 'question_has_commonly_accepted_answer',\n 'question_interestingness_others',\n 'question_interestingness_self',\n 'question_multi_intent',\n #'question_not_really_a_question',\n 'question_opinion_seeking',\n 'question_type_choice',\n #'question_type_compare',\n #'question_type_consequence',\n 'question_type_definition',\n 'question_type_entity',\n 'question_type_instructions',\n 'question_type_procedure',\n 'question_type_reason_explanation',\n #'question_type_spelling',\n 'question_well_written',\n #'answer_helpful',\n 'answer_level_of_information',\n #'answer_plausible',\n #'answer_relevance',\n #'answer_satisfaction',\n 'answer_type_instructions',\n 'answer_type_procedure',\n 'answer_type_reason_explanation',\n 'answer_well_written']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"soft_test_predictions = []\n\nfor i in range(5):\n    j = 1\n    model_path = f'{model_weights_path}bert-{i}-{j}.h5'\n    model1 = create_model_soft(BERT_PATH)\n    model1.load_weights(model_path)\n    soft_test_predictions.append(model1.predict(test_inputs, batch_size=2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(soft_test_predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"soft_test_predictions[0].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"soft_test_predictions = np.mean(soft_test_predictions, axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"soft_test_predictions.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hard_target_columns  = [\n'question_asker_intent_understanding',\n#  'question_body_critical',\n#  'question_conversational',\n#  'question_expect_short_answer',\n#  'question_fact_seeking',\n#  'question_has_commonly_accepted_answer',\n#  'question_interestingness_others',\n#  'question_interestingness_self',\n#  'question_multi_intent',\n'question_not_really_a_question',\n#  'question_opinion_seeking',\n#  'question_type_choice',\n'question_type_compare',\n'question_type_consequence',\n#  'question_type_definition',\n#  'question_type_entity',\n#  'question_type_instructions',\n#  'question_type_procedure',\n#  'question_type_reason_explanation',\n# 'question_type_spelling',\n#  'question_well_written',\n'answer_helpful',\n#  'answer_level_of_information',\n'answer_plausible',\n'answer_relevance',\n'answer_satisfaction',\n#  'answer_type_instructions',\n#  'answer_type_procedure',\n#  'answer_type_reason_explanation',\n#  'answer_well_written'\n ]\n\nhard_test_predictions = []\n\nfor i in range(1,5):\n    print(i)\n    model_path = f'{model_weights_path}hard-bert-{i}.h5'\n    model1 = create_model_hard(BERT_PATH)\n    model1.load_weights(model_path)\n    hard_test_predictions.append(model1.predict(test_inputs, batch_size=2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hard_test_predictions[0].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hard_test_predictions = np.mean(hard_test_predictions, axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hard_test_predictions.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission[soft_target_columns] = soft_test_predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission[hard_target_columns] = hard_test_predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def question_type_spelling_hard(test):\n    if test[\"category_type_english\"] == 1 or test[\"category_type_ell\"] == 1:\n        \n        if test[\"Tags_pronunciation\"] == 1:\n            return 0.666667\n        elif test[\"Tags_spelling\"] == 1:\n            return 0.666667\n        else:\n            return 0.555555\n    else:\n        return 0.00000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission[\"question_type_spelling\"] = test_df.apply(question_type_spelling_hard, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def question_type_compare_hard(text):\n    if text == np.nan:\n        return 0.00000\n    else:\n        text = str(text)\n        ls = text.split(\" \")\n        if \"vs\" in ls:\n            return 1.000000\n        elif (\"between\" or \"difference\") in ls:\n            return 0.666667\n        elif (\"means\" or \"better\") in ls:\n            return 0.333333\n        else:\n            return 0.000000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sample_submission[\"question_type_compare\"] = test_df.question_title_clean2.apply(question_type_compare_hard)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.nunique(axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.isna().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.fillna(0.5, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.iloc[:,1:].max().max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.iloc[:,1:].min().min()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sample_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TARGET_COLUMNS = target_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TARGET_COLUMNS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n\nfrom sklearn.preprocessing import MinMaxScaler\n    \ndef postprocessing(oof_df):\n   \n    scaler = MinMaxScaler()\n    \n    # type 1 column [0, 0.333333, 0.5, 0.666667, 1]\n    # type 2 column [0, 0.333333, 0.666667]\n    # type 3 column [0.333333, 0.444444, 0.5, 0.555556, 0.666667, 0.777778, 0.8333333, 0.888889, 1]\n    # type 4 column [0.200000, 0.266667, 0.300000, 0.333333, 0.400000, \\\n    # 0.466667, 0.5, 0.533333, 0.600000, 0.666667, 0.700000, \\\n    # 0.733333, 0.800000, 0.866667, 0.900000, 0.933333, 1]\n    \n    # comment some columns based on oof result\n    \n    ################################################# handle type 1 columns\n    type_one_column_list = [\n       'question_conversational', \\\n       'question_has_commonly_accepted_answer', \\\n       'question_not_really_a_question', \\\n       'question_type_choice', \\\n       'question_type_compare', \\\n       'question_type_consequence', \\\n       'question_type_definition', \\\n       'question_type_entity', \\\n       'question_type_instructions', \n    ]\n    \n    oof_df[type_one_column_list] = scaler.fit_transform(oof_df[type_one_column_list])\n    \n    tmp = oof_df.copy(deep=True)\n    \n    for column in type_one_column_list:\n        \n        oof_df.loc[tmp[column] <= 0.16667, column] = 0\n        oof_df.loc[(tmp[column] > 0.16667) & (tmp[column] <= 0.41667), column] = 0.333333\n        oof_df.loc[(tmp[column] > 0.41667) & (tmp[column] <= 0.58333), column] = 0.500000\n        oof_df.loc[(tmp[column] > 0.58333) & (tmp[column] <= 0.73333), column] = 0.666667\n        oof_df.loc[(tmp[column] > 0.73333), column] = 1\n    \n    \n    \n    ################################################# handle type 2 columns      \n#     type_two_column_list = [\n#         'question_type_spelling'\n#     ]\n    \n#     for column in type_two_column_list:\n#         if sum(tmp[column] > 0.15)>0:\n#             oof_df.loc[tmp[column] <= 0.15, column] = 0\n#             oof_df.loc[(tmp[column] > 0.15) & (tmp[column] <= 0.45), column] = 0.333333\n#             oof_df.loc[(tmp[column] > 0.45), column] = 0.666667\n#         else:\n#             t1 = max(int(len(tmp[column])*0.0013),2)\n#             t2 = max(int(len(tmp[column])*0.0008),1)\n#             thred1 = sorted(list(tmp[column]))[-t1]\n#             thred2 = sorted(list(tmp[column]))[-t2]\n#             oof_df.loc[tmp[column] <= thred1, column] = 0\n#             oof_df.loc[(tmp[column] > thred1) & (tmp[column] <= thred2), column] = 0.333333\n#             oof_df.loc[(tmp[column] > thred2), column] = 0.666667\n    \n    \n    \n    ################################################# handle type 3 columns      \n    type_three_column_list = [\n       'question_interestingness_self', \n    ]\n    scaler = MinMaxScaler(feature_range=(0, 1))\n    oof_df[type_three_column_list] = scaler.fit_transform(oof_df[type_three_column_list])\n    tmp[type_three_column_list] = scaler.fit_transform(tmp[type_three_column_list])\n    \n    for column in type_three_column_list:\n        oof_df.loc[tmp[column] <= 0.385, column] = 0.333333\n        oof_df.loc[(tmp[column] > 0.385) & (tmp[column] <= 0.47), column] = 0.444444\n        oof_df.loc[(tmp[column] > 0.47) & (tmp[column] <= 0.525), column] = 0.5\n        oof_df.loc[(tmp[column] > 0.525) & (tmp[column] <= 0.605), column] = 0.555556\n        oof_df.loc[(tmp[column] > 0.605) & (tmp[column] <= 0.715), column] = 0.666667\n        oof_df.loc[(tmp[column] > 0.715) & (tmp[column] <= 0.8), column] = 0.833333\n        oof_df.loc[(tmp[column] > 0.8) & (tmp[column] <= 0.94), column] = 0.888889\n        oof_df.loc[(tmp[column] > 0.94), column] = 1\n        \n        \n        \n    ################################################# handle type 4 columns      \n    type_four_column_list = [\n        'answer_satisfaction'\n    ]\n    scaler = MinMaxScaler(feature_range=(0.2, 1))\n    oof_df[type_four_column_list] = scaler.fit_transform(oof_df[type_four_column_list])\n    tmp[type_four_column_list] = scaler.fit_transform(tmp[type_four_column_list])\n    \n    for column in type_four_column_list:\n        \n        oof_df.loc[tmp[column] <= 0.233, column] = 0.200000\n        oof_df.loc[(tmp[column] > 0.233) & (tmp[column] <= 0.283), column] = 0.266667\n        oof_df.loc[(tmp[column] > 0.283) & (tmp[column] <= 0.315), column] = 0.300000\n        oof_df.loc[(tmp[column] > 0.315) & (tmp[column] <= 0.365), column] = 0.333333\n        oof_df.loc[(tmp[column] > 0.365) & (tmp[column] <= 0.433), column] = 0.400000\n        oof_df.loc[(tmp[column] > 0.433) & (tmp[column] <= 0.483), column] = 0.466667\n        oof_df.loc[(tmp[column] > 0.483) & (tmp[column] <= 0.517), column] = 0.500000\n        oof_df.loc[(tmp[column] > 0.517) & (tmp[column] <= 0.567), column] = 0.533333\n        oof_df.loc[(tmp[column] > 0.567) & (tmp[column] <= 0.633), column] = 0.600000\n        oof_df.loc[(tmp[column] > 0.633) & (tmp[column] <= 0.683), column] = 0.666667\n        oof_df.loc[(tmp[column] > 0.683) & (tmp[column] <= 0.715), column] = 0.700000\n        oof_df.loc[(tmp[column] > 0.715) & (tmp[column] <= 0.767), column] = 0.733333\n        oof_df.loc[(tmp[column] > 0.767) & (tmp[column] <= 0.833), column] = 0.800000\n        oof_df.loc[(tmp[column] > 0.883) & (tmp[column] <= 0.915), column] = 0.900000\n        oof_df.loc[(tmp[column] > 0.915) & (tmp[column] <= 0.967), column] = 0.933333\n        oof_df.loc[(tmp[column] > 0.967), column] = 1\n    \n    \n    ################################################# round to i / 90 (i from 0 to 90)\n    oof_values = oof_df[TARGET_COLUMNS].values\n    DEGREE = len(oof_df)//45*9\n#     if degree:\n#         DEGREE = degree\n#     DEGREE = 90\n    oof_values = np.around(oof_values * DEGREE) / DEGREE  ### 90 To be changed\n    oof_df[TARGET_COLUMNS] = oof_values\n    \n    return oof_df\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sample_submission_post = postprocessing(sample_submission)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sample_submission_post.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#for column in TARGET_COLUMNS:\n#    print(sample_submission_post[column].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sample_submission_post","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sample_submission_post","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sample_submission_post.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"06480943a9224250b27e5100aaf3662a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1ce94d5e333f45a886a57328abb600d5","placeholder":"​","style":"IPY_MODEL_96066085cec445d19469e33b87dfe6a2","value":" 58/58 [06:19&lt;00:00,  6.54s/it]"}},"0a3b8ec1684b43e1aa0bae3210ddc519":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d645a9fdcbf74650a961c9404414b70d","placeholder":"​","style":"IPY_MODEL_422a1b9f3cef4e29a634bf38b1bd83bc","value":" 58/58 [03:06&lt;00:00,  3.22s/it]"}},"133f0929f6fa4bb9a7f7979fe6c6afb4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"100%","description_tooltip":null,"layout":"IPY_MODEL_ecba213ffdf3494a836d3f93a826f012","max":58,"min":0,"orientation":"horizontal","style":"IPY_MODEL_68dcabb3f52c45bb811957adda6621e2","value":58}},"1ce94d5e333f45a886a57328abb600d5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ad989597e444159972195d74fc1b41d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"3273ca2e84fc47dea38a6912f6aee299":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d53edb3acd66460d96e6a316ddb55395","placeholder":"​","style":"IPY_MODEL_b040fa515b0d456e80a99d9ecc86ab4d","value":" 476/? [00:20&lt;00:00, 23.11it/s]"}},"3d7cf7794c614cc481408fe870c041fb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_89370c2c86e24cacaf1162244d335dab","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_50e152e032d6470a820e3d95aeb3c83c","value":1}},"422a1b9f3cef4e29a634bf38b1bd83bc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"48ad408bfa2e4d77b23ced0b982b632e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3d7cf7794c614cc481408fe870c041fb","IPY_MODEL_3273ca2e84fc47dea38a6912f6aee299"],"layout":"IPY_MODEL_922d90b8c69141278598f4f2115024c5"}},"50e152e032d6470a820e3d95aeb3c83c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"5be462448a8e4e45a148a10c1e9ba5af":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"68dcabb3f52c45bb811957adda6621e2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"692d3b8fba0047d0a1ea9931153139fc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e3bef8a4fdd49879fb94e05bb7c47a6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"100%","description_tooltip":null,"layout":"IPY_MODEL_5be462448a8e4e45a148a10c1e9ba5af","max":58,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2ad989597e444159972195d74fc1b41d","value":58}},"89370c2c86e24cacaf1162244d335dab":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"922d90b8c69141278598f4f2115024c5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"96066085cec445d19469e33b87dfe6a2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a887c8d540974419bd40d06c68d23344":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b040fa515b0d456e80a99d9ecc86ab4d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d37ba2bae713453b88214147bb67ee7c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6e3bef8a4fdd49879fb94e05bb7c47a6","IPY_MODEL_06480943a9224250b27e5100aaf3662a"],"layout":"IPY_MODEL_a887c8d540974419bd40d06c68d23344"}},"d53edb3acd66460d96e6a316ddb55395":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d645a9fdcbf74650a961c9404414b70d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d996fb3f05d741cc81679c389cac4545":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_133f0929f6fa4bb9a7f7979fe6c6afb4","IPY_MODEL_0a3b8ec1684b43e1aa0bae3210ddc519"],"layout":"IPY_MODEL_692d3b8fba0047d0a1ea9931153139fc"}},"ecba213ffdf3494a836d3f93a826f012":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}},"version_major":2,"version_minor":0}}},"nbformat":4,"nbformat_minor":1}