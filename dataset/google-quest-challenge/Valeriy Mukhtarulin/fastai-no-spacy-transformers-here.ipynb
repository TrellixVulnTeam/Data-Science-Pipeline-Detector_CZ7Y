{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import gc\nfrom functools import partial\nfrom pathlib import Path\n\nfrom fastai.text import *\nfrom fastai.callbacks import *\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\n\n# pd.set_option('display.max_colwidth', 200)\n# pd.set_option('display.max_columns', None)\n# pd.set_option('display.min_rows', 100)\n# pd.set_option('display.max_rows', 100)\n\nhome = Path(\".\")\ninput_dir = Path(\"/kaggle/input/google-quest-challenge/\")\n\n!mkdir -p ~/.fastai/models/\n!cp -R ../input/fastai-wt103-models/* ~/.fastai/models/\n!cp ~/.fastai/models/wt103-fwd/itos_wt103.pkl ~/.fastai/models/wt103-bwd/itos_wt103.pkl","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"This notebook is to train fastai classifier with transfer learning, i.e. ULMFIT with default Spacy tokenizer.\n\nI am using AWD-LSTM. Most other notebooks use transformers like BERT or XL, which potentially yield better results, but are slower.\n\nSources\nhttps://www.kaggle.com/melissarajaram/roberta-fastai-huggingface-transformers"},{"metadata":{},"cell_type":"markdown","source":"**Changes**\n\nV52\nReturn of backwards model\n\nV51\n\nRound of categorical labels\n\nV48\n\nSpacy, only fwd\n\nV32\n\nreplace code blocks\n\nV31\n\nSentencePiece + backwards, test dataset uses the same tokenizer\n\nV30\n\nBackwards model\n\nV29\n\nSentencePiece tokenizer"},{"metadata":{},"cell_type":"markdown","source":"What didn't work:\n\n1. Rounding. Metrics is a correlation\n2. Codeblocks tokenizing (replacing entire blocks with a token)\n3. Removing columns separator"},{"metadata":{"trusted":true},"cell_type":"code","source":"# The metric used in the competition\nfrom scipy.stats import spearmanr\n\nclass AvgSpearman(Callback):\n    def on_epoch_begin(self, **kwargs):\n        self.preds = None\n        self.target = None\n    \n    def on_batch_end(self, last_output, last_target, **kwargs):\n        if self.preds is None or self.target is None:\n            self.preds = last_output.cpu()\n            self.target = last_target.cpu()\n        else:\n            self.preds = np.append(self.preds, last_output.cpu(), axis=0)\n            self.target = np.append(self.target, last_target.cpu(), axis=0)\n    \n    def on_epoch_end(self, last_metrics, **kwargs):\n        spearsum = 0\n        for col in range(self.preds.shape[1]):\n            spearsum += spearmanr(self.preds[:,col], self.target[:,col]).correlation\n        res = spearsum / (self.preds.shape[1] + 1)\n        return add_metrics(last_metrics, res)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocess"},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_test = pd.read_csv(input_dir/\"test.csv\"); raw_test.tail(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_train = pd.read_csv(input_dir/\"train.csv\"); raw_train.tail(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pd.get_dummies(raw_train, columns=class_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# just to be sane\n\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nseed_everything(42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = pd.read_csv(input_dir/\"sample_submission.csv\").columns[1:].to_list()\nassert len(labels) == 30\ntext_cols = [\"question_title\", \"question_body\", \"answer\"] + [\"host\", \"category\", \"question_user_name\", \"question_user_page\", \"answer_user_page\", \"answer_user_name\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = raw_train.iloc[np.random.permutation(len(raw_train))]\ntrain_lm_df = raw_train.append(raw_test, ignore_index=True, sort=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LM\n## Tokenize code"},{"metadata":{"trusted":true},"cell_type":"code","source":"m_code = re.compile(r\"(\\n(?:[a-z  ][\\s\\S]*?(?: = |{|\\()[\\s\\S]+?)+?\\n)\")\ncode = re.compile(r\"(  [\\s\\S]+?\\n){2,}\",)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_lm_df.loc[train_lm_df.category.isin([\"STACKOVERFLOW\", \"TECHNOLOGY\"]), [\"question_body\"]] = train_lm_df[train_lm_df.category.isin([\"STACKOVERFLOW\", \"TECHNOLOGY\"])][\"question_body\"].apply(lambda x: m_code.sub(\" xxcodeblock \", x))\n# train_lm_df.loc[train_lm_df.category.isin([\"STACKOVERFLOW\", \"TECHNOLOGY\"]), [\"answer\"]] = train_lm_df[train_lm_df.category.isin([\"STACKOVERFLOW\", \"TECHNOLOGY\"])][\"answer\"].apply(lambda x: m_code.sub(\" xxcodeblock \", x))\n\n# train_df.loc[train_df.category.isin([\"STACKOVERFLOW\", \"TECHNOLOGY\"]), [\"question_body\"]] = train_df[train_df.category.isin([\"STACKOVERFLOW\", \"TECHNOLOGY\"])][\"question_body\"].apply(lambda x: m_code.sub(\" xxcodeblock \", x))\n# train_df.loc[train_df.category.isin([\"STACKOVERFLOW\", \"TECHNOLOGY\"]), [\"answer\"]] = train_df[train_df.category.isin([\"STACKOVERFLOW\", \"TECHNOLOGY\"])][\"answer\"].apply(lambda x: m_code.sub(\" xxcodeblock \", x))\n\n# raw_test.loc[raw_test.category.isin([\"STACKOVERFLOW\", \"TECHNOLOGY\"]), [\"question_body\"]] = raw_test[raw_test.category.isin([\"STACKOVERFLOW\", \"TECHNOLOGY\"])][\"question_body\"].apply(lambda x: m_code.sub(\" xxcodeblock \", x))\n# raw_test.loc[raw_test.category.isin([\"STACKOVERFLOW\", \"TECHNOLOGY\"]), [\"answer\"]] = raw_test[raw_test.category.isin([\"STACKOVERFLOW\", \"TECHNOLOGY\"])][\"answer\"].apply(lambda x: m_code.sub(\" xxcodeblock \", x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BS = 256","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# data for the language models\ntokenizer = Tokenizer(SpacyTokenizer, 'en')\nprocessor = [TokenizeProcessor(tokenizer=tokenizer, mark_fields=True), NumericalizeProcessor()]\n\nlm_label_list = (TextList.from_df(train_lm_df, \".\", text_cols, processor=processor)\n                 .split_by_rand_pct(0.1, seed=42)\n                 .label_for_lm())\n\ndata_lm = lm_label_list.databunch(bs=BS)\ndata_lm_bwd = lm_label_list.databunch(bs=BS, backwards=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# data for classifiers\nvocab = data_lm.vocab\nBSC = 120\n\nclas_label_list = (TextList.from_df(train_df, \".\", text_cols, vocab=vocab, processor=processor)\n                   .split_by_rand_pct(0.2, seed=42)\n                   .label_from_df(cols=labels)\n                   .add_test(TextList.from_df(raw_test, \".\", text_cols, vocab=vocab, processor=processor)))\n\ndata_clas = clas_label_list.databunch(bs=BSC)\ndata_clas_bwd = clas_label_list.databunch(bs=BSC, backwards=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Fine tune LM"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = 1e-02\nlr *= BS/48\nmoms = (0.8, 0.7)\nwd=0.1\ndrop_mult = 0.5\n\ndef fit_lm(data, epochs=10, head_epochs=5, prefix=\"fwd\"):\n    learn = language_model_learner(data, AWD_LSTM, drop_mult=drop_mult,\n                                   metrics=[accuracy, Perplexity()],\n                               )\n    learn = learn.to_fp16()\n    learn.fit_one_cycle(head_epochs, slice(lr), moms=moms, wd=wd)\n    learn.unfreeze()\n    learn.save(f\"{prefix}_lm_learn_1\")\n    learn = learn.load(f\"{prefix}_lm_learn_1\")\n    learn.fit_one_cycle(epochs, slice(lr/100, lr/2), moms=moms, wd=wd,\n                        callbacks=[SaveModelCallback(learn, monitor=\"perplexity\", mode=\"min\", name=\"best_model\"),]\n                        )\n    learn.save_encoder(f\"{prefix}_enc\")\n    learn.save(f\"{prefix}_lm_model\")\n    return learn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# learn = language_model_learner(data_lm, AWD_LSTM, config, drop_mult=1.0,\n#                                metrics=[accuracy, Perplexity()],\n#                                )\n# learn.lr_find()\n# learn.recorder.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1/0\n# learn.purge();\n# gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn = fit_lm(data_lm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn = fit_lm(data=data_lm_bwd, prefix=\"bwd\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Train Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = 5e-02\nlr *= BSC/48  # Scale learning rate by batch size\nmoms = (0.8, 0.7)\nwd=0.1\n\ndef fit(data, prefix=\"fwd\", epochs=20, epochs_1=2):\n    learn = text_classifier_learner(data, AWD_LSTM,\n                                    pretrained=False,\n                                    metrics=[AvgSpearman()],\n                                    ).to_fp16()\n    learn.load_encoder(f\"{prefix}_enc\");\n    learn.fit_one_cycle(epochs_1, lr, moms=moms, wd=wd)\n\n    learn.freeze_to(-2)\n    learn.save(\"learn\")\n    learn = learn.load(\"learn\")\n    learn.fit_one_cycle(2, slice(lr/(2.6**4),lr), moms=moms, wd=wd)\n\n    learn.freeze_to(-3)\n    learn.save(\"learn\")\n    learn = learn.load(\"learn\")\n    learn.fit_one_cycle(2, slice(lr/2/(2.6**4),lr/2), moms=moms, wd=wd)\n\n    learn.unfreeze()\n    learn.save(f\"{prefix}_learn\")\n    learn = learn.load(f\"{prefix}_learn\")\n    learn.fit_one_cycle(epochs, slice(lr/10/(2.6**4),lr/10), moms=moms, wd=wd,\n                        callbacks=[SaveModelCallback(learn, monitor=\"avg_spearman\", mode=\"max\", name=\"best_model\")]\n                        )\n    learn.save(f\"{prefix}_learn_4\")\n    learn = learn.load(f\"{prefix}_learn_4\")\n    return learn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn = fit(data_clas)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_bwd = fit(data=data_clas_bwd, prefix=\"bwd\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission = pd.DataFrame(columns=[\"qa_id\"]+labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def spearm(preds, target):\n    spearsum = 0\n    for col in range(preds.shape[1]):\n        spearsum += spearmanr(preds[:,col], target[:,col]).correlation\n    return spearsum / (preds.shape[1] + 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds, target = learn.get_preds(DatasetType.Valid, ordered=True)\npreds_b, _ = learn_bwd.get_preds(DatasetType.Valid, ordered=True)\nspearm((preds+preds_b)/2, target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds, _ = learn.get_preds(DatasetType.Test, ordered=True)\ntest_preds_b, _ = learn_bwd.get_preds(DatasetType.Test, ordered=True)\npreds_avg = (test_preds+test_preds_b)/2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.loc[:, \"qa_id\"] = raw_test[\"qa_id\"]\n# sample_submission.loc[:, labels] = test_preds\nsample_submission.loc[:, labels] = preds_avg\nsample_submission.loc[:, labels] = np.clip(sample_submission.loc[:, 1:], 0.00001, 0.999999)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}