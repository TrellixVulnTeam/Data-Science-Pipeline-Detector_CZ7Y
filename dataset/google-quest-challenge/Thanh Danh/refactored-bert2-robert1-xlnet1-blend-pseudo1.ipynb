{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport torch\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n#import os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n!ls /kaggle/input/\n\n# Any results you write to the current directory are saved as output.","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2022-06-04T06:06:39.898245Z","iopub.execute_input":"2022-06-04T06:06:39.898601Z","iopub.status.idle":"2022-06-04T06:06:42.4753Z","shell.execute_reply.started":"2022-06-04T06:06:39.898542Z","shell.execute_reply":"2022-06-04T06:06:42.474553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls /kaggle/input/e127-roberta-answer\n!ls /kaggle/input/huggingface-model-configs\n!ls /kaggle/input/sub-e078-e083-bert2-roberta1-xlnet1","metadata":{"execution":{"iopub.status.busy":"2022-06-04T06:06:42.478625Z","iopub.execute_input":"2022-06-04T06:06:42.478959Z","iopub.status.idle":"2022-06-04T06:06:44.476202Z","shell.execute_reply.started":"2022-06-04T06:06:42.478903Z","shell.execute_reply":"2022-06-04T06:06:44.475477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv('/kaggle/input/google-quest-challenge/test.csv')","metadata":{"execution":{"iopub.status.busy":"2022-06-04T06:06:44.47913Z","iopub.execute_input":"2022-06-04T06:06:44.47945Z","iopub.status.idle":"2022-06-04T06:06:44.515588Z","shell.execute_reply.started":"2022-06-04T06:06:44.479384Z","shell.execute_reply":"2022-06-04T06:06:44.514827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## pre-settings","metadata":{}},{"cell_type":"code","source":"import os\n\ndef OSprint(string):\n    os.system(f'echo \\\"{string}\\\"')\n    print(string)","metadata":{"execution":{"iopub.status.busy":"2022-06-04T06:06:44.516807Z","iopub.execute_input":"2022-06-04T06:06:44.517071Z","iopub.status.idle":"2022-06-04T06:06:44.52354Z","shell.execute_reply.started":"2022-06-04T06:06:44.517027Z","shell.execute_reply":"2022-06-04T06:06:44.522637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls /kaggle/input/","metadata":{"execution":{"iopub.status.busy":"2022-06-04T06:06:44.527251Z","iopub.execute_input":"2022-06-04T06:06:44.527562Z","iopub.status.idle":"2022-06-04T06:06:45.197529Z","shell.execute_reply.started":"2022-06-04T06:06:44.52751Z","shell.execute_reply":"2022-06-04T06:06:45.196822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NUM_FOLDS = 5\nDEVICE = 'cuda'\n#DEVICE = 'cpu'\nBATCH_SIZE = 8\n\nBERT_BASE_MODEL_PATH = '/kaggle/input/huggingface-model-configs/bert-model-uncased-config.pkl'\nROBERTA_BASE_MODEL_PATH = '/kaggle/input/huggingface-model-configs/roberta-model-base-config.pkl'\nXLNET_BASE_MODEL_PATH = '/kaggle/input/huggingface-model-configs/xlnet-model-base-cased-config.pkl'\nBERT_Q_BASE_PATH = '/kaggle/input/e121-bert-question/'\nBERT_A_BASE_PATH = '/kaggle/input/e125-bert-answer/'\nROBERTA_Q_BASE_PATH = '/kaggle/input/e126-roberta-question/'\nROBERTA_A_BASE_PATH = '/kaggle/input/e127-roberta-answer/'\nXLNET_Q_BASE_PATH = '/kaggle/input/e128-xlnet-question/'\nXLNET_A_BASE_PATH = '/kaggle/input/e129-xlnet-answer/'","metadata":{"execution":{"iopub.status.busy":"2022-06-04T06:06:45.201235Z","iopub.execute_input":"2022-06-04T06:06:45.201501Z","iopub.status.idle":"2022-06-04T06:06:45.206997Z","shell.execute_reply.started":"2022-06-04T06:06:45.201453Z","shell.execute_reply":"2022-06-04T06:06:45.205772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Q_LABEL_COL = [\n    'question_asker_intent_understanding',\n    'question_body_critical',\n    'question_conversational',\n    'question_expect_short_answer',\n    'question_fact_seeking',\n    'question_has_commonly_accepted_answer',\n    'question_interestingness_others',\n    'question_interestingness_self',\n    'question_multi_intent',\n    'question_not_really_a_question',\n    'question_opinion_seeking',\n    'question_type_choice',\n    'question_type_compare',\n    'question_type_consequence',\n    'question_type_definition',\n    'question_type_entity',\n    'question_type_instructions',\n    'question_type_procedure',\n    'question_type_reason_explanation',\n    'question_type_spelling',\n    'question_well_written',\n]\n\nA_LABEL_COL = [\n    'answer_helpful',\n    'answer_level_of_information',\n    'answer_plausible',\n    'answer_relevance',\n    'answer_satisfaction',\n    'answer_type_instructions',\n    'answer_type_procedure',\n    'answer_type_reason_explanation',\n    'answer_well_written'\n]","metadata":{"execution":{"iopub.status.busy":"2022-06-04T06:06:45.208441Z","iopub.execute_input":"2022-06-04T06:06:45.208924Z","iopub.status.idle":"2022-06-04T06:06:45.217756Z","shell.execute_reply.started":"2022-06-04T06:06:45.208869Z","shell.execute_reply":"2022-06-04T06:06:45.216833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install /kaggle/input/sacremoses-master/sacremoses > /dev/null\n#!pip install /kaggle/input/transformers/transformers-master #> /dev/null\n!pip install --no-deps /kaggle/input/guchio-transformers/*.whl #> /dev/null","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.status.busy":"2022-06-04T06:06:45.219357Z","iopub.execute_input":"2022-06-04T06:06:45.220005Z","iopub.status.idle":"2022-06-04T06:07:46.671907Z","shell.execute_reply.started":"2022-06-04T06:06:45.219709Z","shell.execute_reply":"2022-06-04T06:07:46.671125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import transformers\nfrom transformers import BertModel, BertTokenizer, RobertaModel, RobertaTokenizer, XLNetModel, XLNetTokenizer\nimport random\nfrom math import ceil, floor\nimport pandas as pd\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset\nfrom tqdm.notebook import tqdm\nimport pickle\nimport gc","metadata":{"execution":{"iopub.status.busy":"2022-06-04T06:07:46.673459Z","iopub.execute_input":"2022-06-04T06:07:46.673774Z","iopub.status.idle":"2022-06-04T06:07:52.287889Z","shell.execute_reply.started":"2022-06-04T06:07:46.673692Z","shell.execute_reply":"2022-06-04T06:07:52.287193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"OSprint('start training!')","metadata":{"execution":{"iopub.status.busy":"2022-06-04T06:07:52.289337Z","iopub.execute_input":"2022-06-04T06:07:52.289603Z","iopub.status.idle":"2022-06-04T06:07:52.314075Z","shell.execute_reply.started":"2022-06-04T06:07:52.289559Z","shell.execute_reply":"2022-06-04T06:07:52.313281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## utils","metadata":{}},{"cell_type":"code","source":"class QUESTDataset(Dataset):\n    def __init__(self, df, mode, tokens, augment,\n                 tokenizer_type, pretrained_model_name_or_path, do_lower_case,\n                 LABEL_COL, t_max_len, q_max_len, a_max_len, tqa_mode,\n                 TBSEP, pos_id_type, MAX_SEQUENCE_LENGTH=None,\n                 use_category=True, logger=None):\n        self.mode = mode\n        self.augment = augment\n        self.len = len(df)\n        self.t_max_len = t_max_len\n        self.q_max_len = q_max_len\n        self.a_max_len = a_max_len\n        self.tqa_mode = tqa_mode\n        self.TBSEP = TBSEP\n        self.pos_id_type = pos_id_type\n        if MAX_SEQUENCE_LENGTH:\n            self.MAX_SEQUENCE_LENGTH = MAX_SEQUENCE_LENGTH\n        else:\n            raise NotImplementedError\n        self.use_category = use_category\n        self.logger = logger\n        self.cat_dict = {\n            'CAT_TECHNOLOGY'.casefold(): 0,\n            'CAT_STACKOVERFLOW'.casefold(): 1,\n            'CAT_CULTURE'.casefold(): 2,\n            'CAT_SCIENCE'.casefold(): 3,\n            'CAT_LIFE_ARTS'.casefold(): 4,\n        }\n\n        if mode == \"test\":\n            self.labels = pd.DataFrame([[-1] * len(LABEL_COL)] * len(df))\n        else:  # train or valid\n            self.labels = df[LABEL_COL]\n\n        self.tokenizer_type = tokenizer_type\n        if tokenizer_type == 'bert':\n            tokenizer = BertTokenizer\n        elif tokenizer_type == 'roberta':\n            tokenizer = RobertaTokenizer\n        elif tokenizer_type == 'xlnet':\n            tokenizer = XLNetTokenizer\n        else:\n            raise NotImplementedError\n        self.tokenizer = tokenizer.from_pretrained(\n            pretrained_model_name_or_path, do_lower_case=do_lower_case)\n        self.tokenizer.add_tokens([self.TBSEP])\n\n        tokens = [token.encode('ascii', 'replace').decode()\n                  for token in tokens if token != '']\n        added_num = self.tokenizer.add_tokens(tokens)\n        if logger:\n            logger.info(f'additional_tokens : {added_num}')\n        else:\n            print(f'additional_tokens : {added_num}')\n        # change online preprocess or off line preprocess\n        self.original_df = df\n\n    def __len__(self):\n        return self.len\n\n    def __getitem__(self, idx):\n        # change online preprocess or off line preprocess\n        idx_row = self.original_df.iloc[idx].copy()\n        idx_row = self.__preprocess_text_row(idx_row,\n                                             t_max_len=self.t_max_len,\n                                             q_max_len=self.q_max_len,\n                                             a_max_len=self.a_max_len)\n        input_ids = idx_row['input_ids'].squeeze()\n        if self.tokenizer_type == 'roberta':\n            token_type_ids = torch.zeros(self.MAX_SEQUENCE_LENGTH, dtype=torch.long)\n        else:\n            token_type_ids = idx_row['token_type_ids'].squeeze()\n        attention_mask = idx_row['attention_mask'].squeeze()\n        qa_id = idx_row['qa_id'].squeeze()\n\n        if self.pos_id_type == 'arange':\n            position_ids = torch.arange(self.MAX_SEQUENCE_LENGTH)\n        # elif self.pos_id_type == 'tq_a_sep':\n        #     position_ids = torch.cat([\n        #         torch.arange(self.t_max_len+self.q_max_len+3),\n        #         torch.arange(self.q_max_len+1)])\n        # elif self.pos_id_type == 't_q_sep':\n        #     position_ids = torch.cat([\n        #         torch.arange(self.t_max_len+self.q_max_len+3),\n        #         torch.arange(self.q_max_len+1)])\n        # elif self.pos_id_type == 't_a_sep':\n        #     position_ids = torch.cat(torch.arange(self.t_max_len+self.q_max_len+3)\n        #             self.MAX_SEQUENCE_LENGTH)\n        else:\n            raise NotImplementedError\n\n        labels = self.labels.iloc[idx].values\n        return qa_id, input_ids, attention_mask, \\\n            token_type_ids, position_ids, labels\n\n    def _trim_input(self, title, question, answer,\n                    t_max_len, q_max_len, a_max_len):\n\n        t_len = len(title)\n        q_len = len(question)\n        a_len = len(answer)\n\n        if (t_len + q_len + a_len + 4) > self.MAX_SEQUENCE_LENGTH:\n            if t_max_len > t_len:\n                t_new_len = t_len\n                a_max_len = a_max_len + floor((t_max_len - t_len) / 2)\n                q_max_len = q_max_len + ceil((t_max_len - t_len) / 2)\n            else:\n                t_new_len = t_max_len\n\n            if a_max_len > a_len:\n                a_new_len = a_len\n                q_new_len = q_max_len + (a_max_len - a_len)\n            elif q_max_len > q_len:\n                a_new_len = a_max_len + (q_max_len - q_len)\n                q_new_len = q_len\n            else:\n                a_new_len = a_max_len\n                q_new_len = q_max_len\n\n            if t_new_len + a_new_len + q_new_len + 4 != self.MAX_SEQUENCE_LENGTH:\n                raise ValueError(\"New sequence length should be %d, but is %d\"\n                                 % (self.MAX_SEQUENCE_LENGTH,\n                                     (t_new_len + a_new_len + q_new_len + 4)))\n            if len(title) > t_new_len:\n                title = title[:t_new_len // 2] + title[-t_new_len // 2:]\n            else:\n                title = title[:t_new_len]\n            if len(question) > q_new_len:\n                question = question[:q_new_len // 2] + \\\n                    question[-q_new_len // 2:]\n            else:\n                question = question[:q_new_len]\n            if len(answer) > a_new_len:\n                answer = answer[:a_new_len // 2] + answer[-a_new_len // 2:]\n            else:\n                answer = answer[:a_new_len]\n        return title, question, answer\n\n    def __preprocess_text_row(self, row, t_max_len, q_max_len, a_max_len):\n        qa_id = row.qa_id\n        title = self.tokenizer.tokenize(row.question_title)\n        body = self.tokenizer.tokenize(row.question_body)\n        answer = self.tokenizer.tokenize(row.answer)\n        category = ('CAT_' + row.category).casefold()\n\n        # category を text として入れてしまう !!!\n        if self.use_category:\n            title = [category] + title\n\n        title, body, answer = self._trim_input(title, body, answer,\n                                               t_max_len=t_max_len,\n                                               q_max_len=q_max_len,\n                                               a_max_len=a_max_len)\n\n        if len(title) == 0:\n            # print(f'NO TITLE, qa_id: {qa_id}')\n            title = ['_']\n        if len(body) == 0:\n            # print(f'NO BODY, qa_id: {qa_id}')\n            body = ['_']\n        if len(answer) == 0:\n            # print(f'NO ANSWER, qa_id: {qa_id}')\n            answer = ['_']\n\n        if self.tqa_mode == 'tq_a':\n            text = title + [self.TBSEP] + body\n            text_pair = answer\n        elif self.tqa_mode == 't_q':\n            text = title\n            text_pair = body\n        elif self.tqa_mode == 't_a':\n            text = title\n            text_pair = answer\n\n        encoded_texts_dict = self.tokenizer.encode_plus(\n            text=text,\n            text_pair=text_pair,\n            add_special_tokens=True,\n            max_length=self.MAX_SEQUENCE_LENGTH,\n            pad_to_max_length=True,\n            return_tensors='pt',\n            return_token_type_ids=True,\n            return_attention_mask=True,\n            return_overflowing_tokens=True,\n        )\n        encoded_texts_dict['qa_id'] = qa_id\n        return encoded_texts_dict\n\nclass BertModelForBinaryMultiLabelClassifier(nn.Module):\n    def __init__(self, num_labels, config_path, state_dict,\n                 token_size=None, MAX_SEQUENCE_LENGTH=512):\n        super(BertModelForBinaryMultiLabelClassifier, self).__init__()\n        with open(config_path, 'rb') as fin:\n            config = pickle.load(fin)\n        self.model = BertModel(config)\n        # self.model.load_state_dict(state_dict)\n        self.dropout = nn.Dropout(0.2)\n        self.classifier = nn.Linear(self.model.config.hidden_size, num_labels)\n\n        # resize\n        if token_size:\n            self.model.resize_token_embeddings(token_size)\n\n        # add modules\n        self.add_module('my_fc_output', self.classifier)\n\n    def forward(self, input_ids=None, input_cats=None, labels=None, attention_mask=None,\n                token_type_ids=None, position_ids=None, head_mask=None,\n                inputs_embeds=None, encoder_hidden_states=None,\n                encoder_attention_mask=None):\n\n        outputs = self.model(input_ids=input_ids,\n                             attention_mask=attention_mask,\n                             token_type_ids=token_type_ids,\n                             position_ids=position_ids,\n                             head_mask=head_mask,\n                             inputs_embeds=inputs_embeds,\n                             encoder_hidden_states=encoder_hidden_states,\n                             encoder_attention_mask=encoder_attention_mask)\n\n        pooled_output = torch.mean(outputs[0], dim=1)\n\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n\n        # add hidden states and attention if they are here\n        outputs = (logits,) + outputs[2:]\n\n        return outputs  # logits, (hidden_states), (attentions)\n\n    def resize_token_embeddings(self, token_num):\n        self.model.resize_token_embeddings(token_num)\n\n    def freeze_unfreeze_bert(self, freeze=True, logger=None):\n        if freeze:\n            print('FREEZE bert model !', logger)\n            # for name, child in self.model.module.named_children():\n            for name, child in self.model.named_children():\n                for param in child.parameters():\n                    param.requires_grad = False\n        else:\n            print('UNFREEZE bert model !', logger)\n            # for name, child in self.model.module.named_children():\n            for name, child in self.model.named_children():\n                for param in child.parameters():\n                    param.requires_grad = True\n\n    # def _resize_embeddings(self, old_embeddings, new_num_tokens):\n    #     old_num_tokens, old_embedding_dim = old_embeddings.weight.size()\n    #     if old_num_tokens == new_num_tokens:\n    #         return old_embeddings\n\n    #     # Build new embeddings\n    #     new_embeddings = nn.Embedding(new_num_tokens, old_embedding_dim)\n    #     new_embeddings.to(old_embeddings.weight.device)\n\n    #     # Copy word embeddings from the previous weights\n    #     num_tokens_to_copy = min(old_num_tokens, new_num_tokens)\n    #     new_embeddings.weight.data[:num_tokens_to_copy,\n    #                                :] = old_embeddings.weight.data[:num_tokens_to_copy, :]\n\n    #     return new_embeddings\n\n\nclass RobertaModelForBinaryMultiLabelClassifier(nn.Module):\n    def __init__(self, num_labels, config_path, state_dict,\n                 token_size=None, MAX_SEQUENCE_LENGTH=512):\n        super(RobertaModelForBinaryMultiLabelClassifier, self).__init__()\n        with open(config_path, 'rb') as fin:\n            config = pickle.load(fin)\n        self.model = RobertaModel(config)\n        # self.model.load_state_dict(state_dict)\n        # # only for roberta\n        # if self.model.state_dict()['embeddings.token_type_embeddings.weight'].shape[0] == 1:\n        #     self.model.load_state_dict(state_dict)\n        #     self.model.embeddings.token_type_embeddings = self._resize_embeddings(\n        #         self.model.embeddings.token_type_embeddings, 2)\n        # elif self.model.state_dict()['embeddings.token_type_embeddings.weight'].shape[0] == 2:\n        #     self.model.embeddings.token_type_embeddings = self._resize_embeddings(\n        #         self.model.embeddings.token_type_embeddings, 2)\n        #     self.model.load_state_dict(state_dict)\n        # else:\n        #     raise NotImplementedError\n        self.dropout = nn.Dropout(0.2)\n        self.classifier = nn.Linear(self.model.config.hidden_size, num_labels)\n\n        # resize\n        if token_size:\n            self.model.resize_token_embeddings(token_size)\n\n        # add modules\n        self.add_module('my_fc_output', self.classifier)\n\n    def forward(self, input_ids=None, input_cats=None, labels=None, attention_mask=None,\n                token_type_ids=None, position_ids=None, head_mask=None,\n                inputs_embeds=None, encoder_hidden_states=None,\n                encoder_attention_mask=None):\n\n        outputs = self.model(input_ids=input_ids,\n                             attention_mask=attention_mask,\n                             token_type_ids=token_type_ids,\n                             position_ids=position_ids,\n                             head_mask=head_mask,\n                             inputs_embeds=inputs_embeds,\n                             encoder_hidden_states=encoder_hidden_states,\n                             encoder_attention_mask=encoder_attention_mask)\n\n        pooled_output = torch.mean(outputs[0], dim=1)\n\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n\n        # add hidden states and attention if they are here\n        outputs = (logits,) + outputs[2:]\n\n        return outputs  # logits, (hidden_states), (attentions)\n\n    def resize_token_embeddings(self, token_num):\n        self.model.resize_token_embeddings(token_num)\n\n    def freeze_unfreeze_bert(self, freeze=True, logger=None):\n        if freeze:\n            print('FREEZE bert model !', logger)\n            # for name, child in self.model.module.named_children():\n            for name, child in self.model.named_children():\n                for param in child.parameters():\n                    param.requires_grad = False\n        else:\n            print('UNFREEZE bert model !', logger)\n            # for name, child in self.model.module.named_children():\n            for name, child in self.model.named_children():\n                for param in child.parameters():\n                    param.requires_grad = True\n\n    # def _resize_embeddings(self, old_embeddings, new_num_tokens):\n    #     old_num_tokens, old_embedding_dim = old_embeddings.weight.size()\n    #     if old_num_tokens == new_num_tokens:\n    #         return old_embeddings\n\n    #     # Build new embeddings\n    #     new_embeddings = nn.Embedding(new_num_tokens, old_embedding_dim)\n    #     new_embeddings.to(old_embeddings.weight.device)\n\n    #     # Copy word embeddings from the previous weights\n    #     num_tokens_to_copy = min(old_num_tokens, new_num_tokens)\n    #     new_embeddings.weight.data[:num_tokens_to_copy,\n    #                                :] = old_embeddings.weight.data[:num_tokens_to_copy, :]\n\n    #     return new_embeddings\n\n\nclass XLNetModelForBinaryMultiLabelClassifier(nn.Module):\n    def __init__(self, num_labels, config_path, state_dict,\n                 token_size=None, MAX_SEQUENCE_LENGTH=512):\n        super(XLNetModelForBinaryMultiLabelClassifier, self).__init__()\n        with open(config_path, 'rb') as fin:\n            config = pickle.load(fin)\n        self.model = XLNetModel(config)\n        # self.model.load_state_dict(state_dict)\n        self.dropout = nn.Dropout(0.2)\n        self.classifier = nn.Linear(self.model.config.d_model, num_labels)\n\n        # resize\n        if token_size:\n            self.model.resize_token_embeddings(token_size)\n\n        # add modules\n        self.add_module('my_fc_output', self.classifier)\n\n    def forward(self, input_ids=None, input_cats=None, labels=None, attention_mask=None,\n                token_type_ids=None, position_ids=None, head_mask=None,\n                inputs_embeds=None, encoder_hidden_states=None,\n                encoder_attention_mask=None):\n\n        outputs = self.model(input_ids=input_ids,\n                             attention_mask=attention_mask,\n                             token_type_ids=token_type_ids,\n                             # position_ids=position_ids,\n                             head_mask=head_mask,\n                             inputs_embeds=inputs_embeds,\n                             # encoder_hidden_states=encoder_hidden_states,\n                             # encoder_attention_mask=encoder_attention_mask\n                             )\n\n        pooled_output = torch.mean(outputs[0], dim=1)\n\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n\n        # add hidden states and attention if they are here\n        outputs = (logits,) + outputs[2:]\n\n        return outputs  # logits, (hidden_states), (attentions)\n\n    def resize_token_embeddings(self, token_num):\n        self.model.resize_token_embeddings(token_num)\n\n    def freeze_unfreeze_bert(self, freeze=True, logger=None):\n        if freeze:\n            print('FREEZE bert model !', logger)\n            # for name, child in self.model.module.named_children():\n            for name, child in self.model.named_children():\n                for param in child.parameters():\n                    param.requires_grad = False\n        else:\n            print('UNFREEZE bert model !', logger)\n            # for name, child in self.model.module.named_children():\n            for name, child in self.model.named_children():\n                for param in child.parameters():\n                    param.requires_grad = True\n\n    # def _resize_embeddings(self, old_embeddings, new_num_tokens):\n    #     old_num_tokens, old_embedding_dim = old_embeddings.weight.size()\n    #     if old_num_tokens == new_num_tokens:\n    #         return old_embeddings\n\n    #     # Build new embeddings\n    #     new_embeddings = nn.Embedding(new_num_tokens, old_embedding_dim)\n    #     new_embeddings.to(old_embeddings.weight.device)\n\n    #     # Copy word embeddings from the previous weights\n    #     num_tokens_to_copy = min(old_num_tokens, new_num_tokens)\n    #     new_embeddings.weight.data[:num_tokens_to_copy,\n    #                                :] = old_embeddings.weight.data[:num_tokens_to_copy, :]\n\n    #     return new_embeddings\n\ndef compute_spearmanr(trues, preds):\n    rhos = []\n    for col_trues, col_pred in zip(trues.T, preds.T):\n        rhos.append(\n            spearmanr(\n                col_trues,\n                col_pred +\n                np.random.normal(\n                    0,\n                    1e-7,\n                    col_pred.shape[0])).correlation)\n    return rhos\n\n\ndef train_one_epoch(model, fobj, optimizer, loader, DEVICE):\n    model.train()\n\n    running_loss = 0\n    for (qa_id, input_ids, attention_mask,\n         token_type_ids, position_ids, labels) in tqdm(loader):\n        # send them to DEVICE\n        input_ids = input_ids.to(DEVICE)\n        attention_mask = attention_mask.to(DEVICE)\n        token_type_ids = token_type_ids.to(DEVICE)\n        position_ids = position_ids.to(DEVICE)\n        labels = labels.to(DEVICE)\n\n        # forward\n        outputs = model(\n            input_ids=input_ids,\n            labels=labels,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids\n        )\n        loss = fobj(outputs[0], labels.float())\n\n        # backword and update\n        optimizer.zero_grad()\n        loss.backward()\n\n        optimizer.step()\n\n        # store loss to culc epoch mean\n        running_loss += loss\n\n    loss_mean = running_loss / len(loader)\n\n    return loss_mean\n\n\ndef test(model, fobj, loader, DEVICE, mode):\n    model.eval()\n\n    with torch.no_grad():\n        y_preds, y_trues, qa_ids = [], [], []\n\n        running_loss = 0\n        for (qa_id, input_ids, attention_mask,\n             token_type_ids, position_ids, labels) in tqdm(loader):\n            # send them to DEVICE\n            input_ids = input_ids.to(DEVICE)\n            attention_mask = attention_mask.to(DEVICE)\n            token_type_ids = token_type_ids.to(DEVICE)\n            position_ids = position_ids.to(DEVICE)\n            labels = labels.to(DEVICE)\n\n            # forward\n            outputs = model(\n                input_ids=input_ids,\n                labels=labels,\n                attention_mask=attention_mask,\n                token_type_ids=token_type_ids,\n                position_ids=position_ids\n            )\n            logits = outputs[0]\n            if mode != 'test':\n                loss = fobj(logits, labels.float())\n\n                running_loss += loss\n\n            y_preds.append(torch.sigmoid(logits))\n            y_trues.append(labels)\n            qa_ids.append(qa_id)\n\n        loss_mean = running_loss / len(loader)\n\n        y_preds = torch.cat(y_preds).to('cpu').numpy()\n        y_trues = torch.cat(y_trues).to('cpu').numpy()\n        qa_ids = torch.cat(qa_ids).to('cpu').numpy()\n\n        if mode == 'valid':\n            metric_raws = compute_spearmanr(y_trues, y_preds)\n            metric = np.mean(metric_raws)\n        elif mode != 'test':\n            raise NotImplementedError\n        else:\n            metric_raws = None\n            metric = None\n\n    return loss_mean, metric, metric_raws, y_preds, y_trues, qa_ids\n\n\ndef seed_everything(seed=71):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\n\nseed_everything()","metadata":{"execution":{"iopub.status.busy":"2022-06-04T06:07:52.315686Z","iopub.execute_input":"2022-06-04T06:07:52.315876Z","iopub.status.idle":"2022-06-04T06:07:52.530503Z","shell.execute_reply.started":"2022-06-04T06:07:52.315846Z","shell.execute_reply":"2022-06-04T06:07:52.529741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## BERT","metadata":{}},{"cell_type":"code","source":"#%debug\n# prediction_loop\nimport os\nimport gc\nimport pickle\n\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.sampler import RandomSampler\n\n\nFOLD_NUM = 5\nRANK_NUM = 1\n\nstate_dict = {}\n\n#############################\n#  BERT QUESTION\n#############################\nbert_question_fold_prediction_dict = {}\nfor fold in range(FOLD_NUM):\n    for rank in range(RANK_NUM):\n        OSprint(f'fold -- {fold}')\n        STATE_DICT_PATH = f'{BERT_Q_BASE_PATH}/state_dicts/fold_{fold}_rank_{rank}_state_dict.pkl'\n        with open(STATE_DICT_PATH, 'rb') as fin:\n            del state_dict\n            state_dict = pickle.load(fin)\n            gc.collect()\n\n        model = BertModelForBinaryMultiLabelClassifier(21, BERT_BASE_MODEL_PATH, state_dict, token_size=30528, )\n        model.load_state_dict(state_dict)\n        model.to(DEVICE)        \n        \n        test_dataset = QUESTDataset(\n            df=test_df,\n            mode='test',\n            tokens = [\n                'CAT_TECHNOLOGY'.casefold(),\n                'CAT_STACKOVERFLOW'.casefold(),\n                'CAT_CULTURE'.casefold(),\n                'CAT_SCIENCE'.casefold(),\n                'CAT_LIFE_ARTS'.casefold(),\n            ],\n            augment=[],\n            tokenizer_type='bert',\n            pretrained_model_name_or_path=BERT_Q_BASE_PATH,\n            do_lower_case=True,\n            LABEL_COL=Q_LABEL_COL,\n            t_max_len=30,\n            q_max_len=239 * 2,\n            a_max_len=239 * 0,\n            tqa_mode='tq_a',\n            TBSEP='[TBSEP]',\n            pos_id_type='arange',\n            MAX_SEQUENCE_LENGTH=512,\n        )\n        \n        test_sampler = RandomSampler(data_source=test_dataset)\n        test_loader = DataLoader(\n                test_dataset,\n                batch_size=BATCH_SIZE,\n                sampler=test_sampler,\n                num_workers=os.cpu_count(),\n                worker_init_fn=lambda x: np.random.seed(),\n                drop_last=False,\n                pin_memory=True\n            )\n\n        _, _, _, y_preds, _, qa_ids = test(model, None, test_loader, DEVICE, 'test')\n\n        fold_df = pd.DataFrame(y_preds)\n        fold_df['qa_id'] = qa_ids\n        fold_df = fold_df.set_index('qa_id')\n        if fold in bert_question_fold_prediction_dict:\n            bert_question_fold_prediction_dict[fold] += fold_df\n        else:\n            bert_question_fold_prediction_dict[fold] = fold_df\n        \n        del model, test_dataset, test_sampler, test_loader\n        gc.collect()\n    bert_question_fold_prediction_dict[fold] = bert_question_fold_prediction_dict[fold] / RANK_NUM #AVG            \n    \n    \n#############################\n#  BERT ANSWER\n#############################\nFOLD_NUM = 5\nRANK_NUM = 1\n\nbert_answer_fold_prediction_dict = {}\nfor fold in range(FOLD_NUM):\n    for rank in range(RANK_NUM):\n        OSprint(f'fold -- {fold}')\n        STATE_DICT_PATH = f'{BERT_A_BASE_PATH}/state_dicts/fold_{fold}_rank_{rank}_state_dict.pkl'\n        with open(STATE_DICT_PATH, 'rb') as fin:\n            del state_dict\n            state_dict = pickle.load(fin)\n            gc.collect()\n\n        model = BertModelForBinaryMultiLabelClassifier(9, BERT_BASE_MODEL_PATH, state_dict, token_size=30528, )\n        model.load_state_dict(state_dict)\n        model.to(DEVICE)\n        \n        test_dataset = QUESTDataset(\n            df=test_df,\n            mode='test',\n            tokens = [\n                'CAT_TECHNOLOGY'.casefold(),\n                'CAT_STACKOVERFLOW'.casefold(),\n                'CAT_CULTURE'.casefold(),\n                'CAT_SCIENCE'.casefold(),\n                'CAT_LIFE_ARTS'.casefold(),\n            ],\n            augment=[],\n            tokenizer_type='bert',\n            pretrained_model_name_or_path=BERT_A_BASE_PATH,\n            do_lower_case=True,\n            LABEL_COL=A_LABEL_COL,\n            t_max_len=30,\n            q_max_len=239 * 0,\n            a_max_len=239 * 2,\n            tqa_mode='tq_a',\n            TBSEP='[TBSEP]',\n            pos_id_type='arange',\n            MAX_SEQUENCE_LENGTH=512,\n        )\n        \n        test_sampler = RandomSampler(data_source=test_dataset)\n        test_loader = DataLoader(\n                test_dataset,\n                batch_size=BATCH_SIZE,\n                sampler=test_sampler,\n                num_workers=os.cpu_count(),\n                worker_init_fn=lambda x: np.random.seed(),\n                drop_last=False,\n                pin_memory=True\n            )\n\n        _, _, _, y_preds, _, qa_ids = test(model, None, test_loader, DEVICE, 'test')\n\n        fold_df = pd.DataFrame(y_preds)\n        fold_df['qa_id'] = qa_ids\n        fold_df = fold_df.set_index('qa_id')\n        if fold in bert_answer_fold_prediction_dict:\n            bert_answer_fold_prediction_dict[fold] += fold_df\n        else:\n            bert_answer_fold_prediction_dict[fold] = fold_df\n\n        del model, test_dataset, test_sampler, test_loader\n        gc.collect()\n    bert_answer_fold_prediction_dict[fold] = bert_answer_fold_prediction_dict[fold] / RANK_NUM #AVG\n\nbert_fold_prediction_dict = {}\nfor fold in bert_question_fold_prediction_dict:\n    bert_fold_prediction_dict[fold] = bert_question_fold_prediction_dict[fold].reset_index().merge(\n                                      bert_answer_fold_prediction_dict[fold].reset_index(), on='qa_id', how='left')","metadata":{"execution":{"iopub.status.busy":"2022-06-04T06:07:52.531997Z","iopub.execute_input":"2022-06-04T06:07:52.53338Z","iopub.status.idle":"2022-06-04T06:10:45.532485Z","shell.execute_reply.started":"2022-06-04T06:07:52.532238Z","shell.execute_reply":"2022-06-04T06:10:45.531669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_fold_prediction_dict[0]","metadata":{"execution":{"iopub.status.busy":"2022-06-04T06:10:45.533937Z","iopub.execute_input":"2022-06-04T06:10:45.534219Z","iopub.status.idle":"2022-06-04T06:10:45.573126Z","shell.execute_reply.started":"2022-06-04T06:10:45.534172Z","shell.execute_reply":"2022-06-04T06:10:45.572515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ROBERTA","metadata":{}},{"cell_type":"code","source":"# %debug\nFOLD_NUM = 5\nRANK_NUM = 2\n\nstate_dict = {}\n\n#############################\n#  ROBERTA QUESTION\n#############################\nroberta_question_fold_prediction_dict = {}\nfor fold in range(FOLD_NUM):\n    for rank in range(RANK_NUM):\n        OSprint(f'fold -- {fold}')\n        STATE_DICT_PATH = f'{ROBERTA_Q_BASE_PATH}/state_dicts/fold_{fold}_rank_{rank}_state_dict.pkl'\n        with open(STATE_DICT_PATH, 'rb') as fin:\n            del state_dict\n            state_dict = pickle.load(fin)\n            gc.collect()\n\n        model = RobertaModelForBinaryMultiLabelClassifier(21, ROBERTA_BASE_MODEL_PATH, state_dict, token_size=50271, )\n        model.load_state_dict(state_dict)\n        model.to(DEVICE)        \n        \n        test_dataset = QUESTDataset(\n            df=test_df,\n            mode='test',\n            tokens = [\n                'CAT_TECHNOLOGY'.casefold(),\n                'CAT_STACKOVERFLOW'.casefold(),\n                'CAT_CULTURE'.casefold(),\n                'CAT_SCIENCE'.casefold(),\n                'CAT_LIFE_ARTS'.casefold(),\n            ],\n            augment=[],\n            tokenizer_type='roberta',\n            pretrained_model_name_or_path=ROBERTA_Q_BASE_PATH,\n            do_lower_case=False,\n            LABEL_COL=Q_LABEL_COL,\n            t_max_len=30,\n            q_max_len=239 * 2,\n            a_max_len=239 * 0,\n            tqa_mode='tq_a',\n            TBSEP='[TBSEP]',\n            pos_id_type='arange',\n            MAX_SEQUENCE_LENGTH=512,\n        )\n        \n        test_sampler = RandomSampler(data_source=test_dataset)\n        test_loader = DataLoader(\n                test_dataset,\n                batch_size=BATCH_SIZE,\n                sampler=test_sampler,\n                num_workers=os.cpu_count(),\n                worker_init_fn=lambda x: np.random.seed(),\n                drop_last=False,\n                pin_memory=True\n            )\n\n        _, _, _, y_preds, _, qa_ids = test(model, None, test_loader, DEVICE, 'test')\n\n        fold_df = pd.DataFrame(y_preds)\n        fold_df['qa_id'] = qa_ids\n        fold_df = fold_df.set_index('qa_id')\n        if fold in roberta_question_fold_prediction_dict:\n            roberta_question_fold_prediction_dict[fold] += fold_df\n        else:\n            roberta_question_fold_prediction_dict[fold] = fold_df\n        \n        del model, test_dataset, test_sampler, test_loader\n        gc.collect()\n    roberta_question_fold_prediction_dict[fold] = roberta_question_fold_prediction_dict[fold] / RANK_NUM #AVG            \n    \n    \n#############################\n#  ROBERTA ANSWER\n#############################\nFOLD_NUM = 5\nRANK_NUM = 2\n\nroberta_answer_fold_prediction_dict = {}\nfor fold in range(FOLD_NUM):\n    for rank in range(RANK_NUM):\n        OSprint(f'fold -- {fold}')\n        STATE_DICT_PATH = f'{ROBERTA_A_BASE_PATH}/state_dicts/fold_{fold}_rank_{rank}_state_dict.pkl'\n        with open(STATE_DICT_PATH, 'rb') as fin:\n            del state_dict\n            state_dict = pickle.load(fin)\n            gc.collect()\n\n        model = RobertaModelForBinaryMultiLabelClassifier(9, ROBERTA_BASE_MODEL_PATH, state_dict, token_size=50271, )\n        model.load_state_dict(state_dict) \n        model.to(DEVICE)        \n        \n        test_dataset = QUESTDataset(\n            df=test_df,\n            mode='test',\n            tokens = [\n                'CAT_TECHNOLOGY'.casefold(),\n                'CAT_STACKOVERFLOW'.casefold(),\n                'CAT_CULTURE'.casefold(),\n                'CAT_SCIENCE'.casefold(),\n                'CAT_LIFE_ARTS'.casefold(),\n            ],\n            augment=[],\n            tokenizer_type='roberta',\n            pretrained_model_name_or_path=ROBERTA_A_BASE_PATH,\n            do_lower_case=False,\n            LABEL_COL=A_LABEL_COL,\n            t_max_len=30,\n            q_max_len=239 * 0,\n            a_max_len=239 * 2,\n            tqa_mode='tq_a',\n            TBSEP='[TBSEP]',\n            pos_id_type='arange',\n            MAX_SEQUENCE_LENGTH=512,\n        )\n        \n        test_sampler = RandomSampler(data_source=test_dataset)\n        test_loader = DataLoader(\n                test_dataset,\n                batch_size=BATCH_SIZE,\n                sampler=test_sampler,\n                num_workers=os.cpu_count(),\n                worker_init_fn=lambda x: np.random.seed(),\n                drop_last=False,\n                pin_memory=True\n            )\n\n        _, _, _, y_preds, _, qa_ids = test(model, None, test_loader, DEVICE, 'test')\n\n        fold_df = pd.DataFrame(y_preds)\n        fold_df['qa_id'] = qa_ids\n        fold_df = fold_df.set_index('qa_id')\n        if fold in roberta_answer_fold_prediction_dict:\n            roberta_answer_fold_prediction_dict[fold] += fold_df\n        else:\n            roberta_answer_fold_prediction_dict[fold] = fold_df\n\n        del model, test_dataset, test_sampler, test_loader\n        gc.collect()\n    roberta_answer_fold_prediction_dict[fold] = roberta_answer_fold_prediction_dict[fold] / RANK_NUM #AVG\n\nroberta_fold_prediction_dict = {}\nfor fold in roberta_question_fold_prediction_dict:\n    roberta_fold_prediction_dict[fold] = roberta_question_fold_prediction_dict[fold].reset_index().merge(\n                                      roberta_answer_fold_prediction_dict[fold].reset_index(), on='qa_id', how='left')","metadata":{"execution":{"iopub.status.busy":"2022-06-04T06:10:45.574584Z","iopub.execute_input":"2022-06-04T06:10:45.574858Z","iopub.status.idle":"2022-06-04T06:17:13.887514Z","shell.execute_reply.started":"2022-06-04T06:10:45.574812Z","shell.execute_reply":"2022-06-04T06:17:13.886656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"roberta_fold_prediction_dict[0]","metadata":{"execution":{"iopub.status.busy":"2022-06-04T06:17:13.888925Z","iopub.execute_input":"2022-06-04T06:17:13.889385Z","iopub.status.idle":"2022-06-04T06:17:13.923337Z","shell.execute_reply.started":"2022-06-04T06:17:13.889324Z","shell.execute_reply":"2022-06-04T06:17:13.922332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_fold_prediction_dict[0]","metadata":{"execution":{"iopub.status.busy":"2022-06-04T06:17:13.924803Z","iopub.execute_input":"2022-06-04T06:17:13.925221Z","iopub.status.idle":"2022-06-04T06:17:13.957639Z","shell.execute_reply.started":"2022-06-04T06:17:13.925034Z","shell.execute_reply":"2022-06-04T06:17:13.956939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## XLNet","metadata":{}},{"cell_type":"code","source":"# %debug\nFOLD_NUM = 5\nRANK_NUM = 1\n\nstate_dict = {}\n\n#############################\n#  XLNET QUESTION\n#############################\nxlnet_question_fold_prediction_dict = {}\nfor fold in range(FOLD_NUM):\n    for rank in range(RANK_NUM):\n        OSprint(f'fold -- {fold}')\n        STATE_DICT_PATH = f'{XLNET_Q_BASE_PATH}/state_dicts/fold_{fold}_rank_{rank}_state_dict.pkl'\n        with open(STATE_DICT_PATH, 'rb') as fin:\n            del state_dict\n            state_dict = pickle.load(fin)\n            gc.collect()\n\n        model = XLNetModelForBinaryMultiLabelClassifier(21, XLNET_BASE_MODEL_PATH, state_dict, token_size=32006, )\n        model.load_state_dict(state_dict)\n        model.to(DEVICE)        \n        \n        test_dataset = QUESTDataset(\n            df=test_df,\n            mode='test',\n            tokens = [\n                'CAT_TECHNOLOGY'.casefold(),\n                'CAT_STACKOVERFLOW'.casefold(),\n                'CAT_CULTURE'.casefold(),\n                'CAT_SCIENCE'.casefold(),\n                'CAT_LIFE_ARTS'.casefold(),\n            ],\n            augment=[],\n            tokenizer_type='xlnet',\n            pretrained_model_name_or_path=XLNET_Q_BASE_PATH,\n            do_lower_case=False,\n            LABEL_COL=Q_LABEL_COL,\n            t_max_len=30,\n            q_max_len=239 * 2,\n            a_max_len=239 * 0,\n            tqa_mode='tq_a',\n            TBSEP='[TBSEP]',\n            pos_id_type='arange',\n            MAX_SEQUENCE_LENGTH=512,\n        )\n        \n        test_sampler = RandomSampler(data_source=test_dataset)\n        test_loader = DataLoader(\n                test_dataset,\n                batch_size=BATCH_SIZE,\n                sampler=test_sampler,\n                num_workers=os.cpu_count(),\n                worker_init_fn=lambda x: np.random.seed(),\n                drop_last=False,\n                pin_memory=True\n            )\n\n        _, _, _, y_preds, _, qa_ids = test(model, None, test_loader, DEVICE, 'test')\n\n        fold_df = pd.DataFrame(y_preds)\n        fold_df['qa_id'] = qa_ids\n        fold_df = fold_df.set_index('qa_id')\n        if fold in xlnet_question_fold_prediction_dict:\n            xlnet_question_fold_prediction_dict[fold] += fold_df\n        else:\n            xlnet_question_fold_prediction_dict[fold] = fold_df\n        \n        del model, test_dataset, test_sampler, test_loader\n        gc.collect()\n    xlnet_question_fold_prediction_dict[fold] = xlnet_question_fold_prediction_dict[fold] / RANK_NUM #AVG            \n    \n    \n#############################\n#  XLNET ANSWER\n#############################\nFOLD_NUM = 5\nRANK_NUM = 1\n\nxlnet_answer_fold_prediction_dict = {}\nfor fold in range(FOLD_NUM):\n    for rank in range(RANK_NUM):\n        OSprint(f'fold -- {fold}')\n        STATE_DICT_PATH = f'{XLNET_A_BASE_PATH}/state_dicts/fold_{fold}_rank_{rank}_state_dict.pkl'\n        with open(STATE_DICT_PATH, 'rb') as fin:\n            del state_dict\n            state_dict = pickle.load(fin)\n            gc.collect()\n\n        model = XLNetModelForBinaryMultiLabelClassifier(9, XLNET_BASE_MODEL_PATH, state_dict, token_size=32006, )\n        model.load_state_dict(state_dict)\n        model.to(DEVICE)        \n        \n        test_dataset = QUESTDataset(\n            df=test_df,\n            mode='test',\n            tokens = [\n                'CAT_TECHNOLOGY'.casefold(),\n                'CAT_STACKOVERFLOW'.casefold(),\n                'CAT_CULTURE'.casefold(),\n                'CAT_SCIENCE'.casefold(),\n                'CAT_LIFE_ARTS'.casefold(),\n            ],\n            augment=[],\n            tokenizer_type='xlnet',\n            pretrained_model_name_or_path=XLNET_A_BASE_PATH,\n            do_lower_case=False,\n            LABEL_COL=A_LABEL_COL,\n            t_max_len=30,\n            q_max_len=239 * 0,\n            a_max_len=239 * 2,\n            tqa_mode='tq_a',\n            TBSEP='[TBSEP]',\n            pos_id_type='arange',\n            MAX_SEQUENCE_LENGTH=512,\n        )\n        \n        test_sampler = RandomSampler(data_source=test_dataset)\n        test_loader = DataLoader(\n                test_dataset,\n                batch_size=BATCH_SIZE,\n                sampler=test_sampler,\n                num_workers=os.cpu_count(),\n                worker_init_fn=lambda x: np.random.seed(),\n                drop_last=False,\n                pin_memory=True\n            )\n\n        _, _, _, y_preds, _, qa_ids = test(model, None, test_loader, DEVICE, 'test')\n\n        fold_df = pd.DataFrame(y_preds)\n        fold_df['qa_id'] = qa_ids\n        fold_df = fold_df.set_index('qa_id')\n        if fold in xlnet_answer_fold_prediction_dict:\n            xlnet_answer_fold_prediction_dict[fold] += fold_df\n        else:\n            xlnet_answer_fold_prediction_dict[fold] = fold_df\n\n        del model, test_dataset, test_sampler, test_loader\n        gc.collect()\n    xlnet_answer_fold_prediction_dict[fold] = xlnet_answer_fold_prediction_dict[fold] / RANK_NUM #AVG\n\nxlnet_fold_prediction_dict = {}\nfor fold in xlnet_question_fold_prediction_dict:\n    xlnet_fold_prediction_dict[fold] = xlnet_question_fold_prediction_dict[fold].reset_index().merge(\n                                      xlnet_answer_fold_prediction_dict[fold].reset_index(), on='qa_id', how='left')","metadata":{"execution":{"iopub.status.busy":"2022-06-04T06:17:13.95886Z","iopub.execute_input":"2022-06-04T06:17:13.959268Z","iopub.status.idle":"2022-06-04T06:22:36.22336Z","shell.execute_reply.started":"2022-06-04T06:17:13.959221Z","shell.execute_reply":"2022-06-04T06:22:36.220395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_fold_prediction_dict[0]","metadata":{"execution":{"iopub.status.busy":"2022-06-04T06:22:36.232392Z","iopub.execute_input":"2022-06-04T06:22:36.241724Z","iopub.status.idle":"2022-06-04T06:22:36.329383Z","shell.execute_reply.started":"2022-06-04T06:22:36.241656Z","shell.execute_reply":"2022-06-04T06:22:36.326403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"roberta_fold_prediction_dict[0]","metadata":{"execution":{"iopub.status.busy":"2022-06-04T06:22:36.331211Z","iopub.execute_input":"2022-06-04T06:22:36.331776Z","iopub.status.idle":"2022-06-04T06:22:36.434518Z","shell.execute_reply.started":"2022-06-04T06:22:36.331597Z","shell.execute_reply":"2022-06-04T06:22:36.433507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xlnet_fold_prediction_dict[0]","metadata":{"execution":{"iopub.status.busy":"2022-06-04T06:22:36.439183Z","iopub.execute_input":"2022-06-04T06:22:36.442108Z","iopub.status.idle":"2022-06-04T06:22:36.534075Z","shell.execute_reply.started":"2022-06-04T06:22:36.442046Z","shell.execute_reply":"2022-06-04T06:22:36.533437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### merge all fold prediction dicts","metadata":{}},{"cell_type":"code","source":"fold_prediction_dict = {}\n\ndicts = [bert_fold_prediction_dict, roberta_fold_prediction_dict, xlnet_fold_prediction_dict]\n\nfor _fold_prediction_dict in dicts:\n    for fold in _fold_prediction_dict:\n        if fold in fold_prediction_dict:\n            fold_prediction_dict[fold] += _fold_prediction_dict[fold].sort_values('qa_id').reset_index(drop=True)\n        else:\n            fold_prediction_dict[fold] = _fold_prediction_dict[fold].sort_values('qa_id').reset_index(drop=True)#.copy()\n\nfor fold in fold_prediction_dict:\n    fold_prediction_dict[fold] /= len(dicts)","metadata":{"execution":{"iopub.status.busy":"2022-06-04T06:22:36.537972Z","iopub.execute_input":"2022-06-04T06:22:36.540188Z","iopub.status.idle":"2022-06-04T06:22:36.892064Z","shell.execute_reply.started":"2022-06-04T06:22:36.540134Z","shell.execute_reply":"2022-06-04T06:22:36.891265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fold_prediction_dict[0]","metadata":{"execution":{"iopub.status.busy":"2022-06-04T06:22:36.896278Z","iopub.execute_input":"2022-06-04T06:22:36.897781Z","iopub.status.idle":"2022-06-04T06:22:36.943381Z","shell.execute_reply.started":"2022-06-04T06:22:36.896546Z","shell.execute_reply":"2022-06-04T06:22:36.94246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### ensemble","metadata":{}},{"cell_type":"code","source":"def convert_folds_prediction_to_sub(folds_prediction_dict):\n    for i, fold in enumerate(folds_prediction_dict):\n        if i == 0:\n            prediction = folds_prediction_dict[fold].sort_values('qa_id').set_index('qa_id').values\n        else:\n            prediction += folds_prediction_dict[fold].sort_values('qa_id').set_index('qa_id').values\n#    sub_df = pd.DataFrame(base_prediction)\n#    sub_df['qa_id'] = folds_prediction_dict[fold]['qa_id'].sort_values().values\n    prediction /= len(folds_prediction_dict)\n    return prediction","metadata":{"execution":{"iopub.status.busy":"2022-06-04T06:22:36.944688Z","iopub.execute_input":"2022-06-04T06:22:36.944969Z","iopub.status.idle":"2022-06-04T06:22:36.952729Z","shell.execute_reply.started":"2022-06-04T06:22:36.944924Z","shell.execute_reply":"2022-06-04T06:22:36.950896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction = convert_folds_prediction_to_sub(fold_prediction_dict)","metadata":{"execution":{"iopub.status.busy":"2022-06-04T06:22:36.954038Z","iopub.execute_input":"2022-06-04T06:22:36.954629Z","iopub.status.idle":"2022-06-04T06:22:36.9696Z","shell.execute_reply.started":"2022-06-04T06:22:36.954444Z","shell.execute_reply":"2022-06-04T06:22:36.968921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raw_prediction = prediction.copy()","metadata":{"execution":{"iopub.status.busy":"2022-06-04T06:22:36.972392Z","iopub.execute_input":"2022-06-04T06:22:36.972952Z","iopub.status.idle":"2022-06-04T06:22:36.97738Z","shell.execute_reply.started":"2022-06-04T06:22:36.972677Z","shell.execute_reply":"2022-06-04T06:22:36.976432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## opt","metadata":{}},{"cell_type":"code","source":"import pickle\nfrom functools import partial\nfrom glob import glob\n\nimport numpy as np\nimport pandas as pd\nimport scipy as sp\nimport torch\nfrom scipy.stats import spearmanr\nfrom tqdm import tqdm\n\nclass OptimizedRounder(object):\n    \"\"\"\n    An optimizer for rounding thresholds\n    to maximize Quadratic Weighted Kappa (QWK) score\n    # https://www.kaggle.com/naveenasaithambi/optimizedrounder-improved\n    \"\"\"\n\n    def __init__(self):\n        self.coef_ = 0\n\n    def _spearmanr_loss(self, coef, X, y, labels):\n        \"\"\"\n        Get loss according to\n        using current coefficients\n        :param coef: A list of coefficients that will be used for rounding\n        :param X: The raw predictions\n        :param y: The ground truth labels\n        \"\"\"\n        X_p = pd.cut(X, [-np.inf] + list(np.sort(coef)) +\n                     [np.inf], labels=labels)\n\n        # return -np.mean(spearmanr(y, X_p).correlation)\n        return -spearmanr(y, X_p).correlation\n\n    def fit(self, X, y, initial_coef):\n        \"\"\"\n        Optimize rounding thresholds\n        :param X: The raw predictions\n        :param y: The ground truth labels\n        \"\"\"\n        labels = self.labels\n        loss_partial = partial(self._spearmanr_loss, X=X, y=y, labels=labels)\n        self.coef_ = sp.optimize.minimize(\n            loss_partial, initial_coef, method='nelder-mead')\n\n    def predict(self, X, coef):\n        \"\"\"\n        Make predictions with specified thresholds\n        :param X: The raw predictions\n        :param coef: A list of coefficients that will be used for rounding\n        \"\"\"\n        labels = self.labels\n        return pd.cut(X, [-np.inf] + list(np.sort(coef)) +\n                      [np.inf], labels=labels)\n        # [np.inf], labels=[0, 1, 2, 3])\n\n    def coefficients(self):\n        \"\"\"\n        Return the optimized coefficients\n        \"\"\"\n        return self.coef_['x']\n\n    def set_labels(self, labels):\n        self.labels = labels","metadata":{"execution":{"iopub.status.busy":"2022-06-04T06:22:36.978789Z","iopub.execute_input":"2022-06-04T06:22:36.979366Z","iopub.status.idle":"2022-06-04T06:22:36.994427Z","shell.execute_reply.started":"2022-06-04T06:22:36.979043Z","shell.execute_reply":"2022-06-04T06:22:36.992767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#with open('/kaggle/input/e059-question-snapshot/optRs.pkl', 'rb') as fin:\n#    optRs = pickle.load(fin)\n#with open('/kaggle/input/e060-answer-snapshot/optRs.pkl', 'rb') as fin:\n#    optRs += pickle.load(fin)   \n\nwith open('/kaggle/input/sub-e121-e129-bert1-roberta2-xlnet1/optRs.pkl', 'rb') as fin:\n    optRs = pickle.load(fin)    ","metadata":{"execution":{"iopub.status.busy":"2022-06-04T06:22:36.995882Z","iopub.execute_input":"2022-06-04T06:22:36.996208Z","iopub.status.idle":"2022-06-04T06:22:37.014669Z","shell.execute_reply.started":"2022-06-04T06:22:36.996126Z","shell.execute_reply":"2022-06-04T06:22:37.014032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res_prediction = []\nfor i in tqdm(list(range(30))):\n    y_pred = prediction[:, i]\n    #if i not in [2,4,5,6,7,11,12,13,14,15,16,18,19,29]:\n    if i not in [2,4,5,6,7,8,9,11,12,13,14,15,16,19,23,25,]:\n        res_prediction.append(y_pred)\n        continue\n    \n    optR = optRs[i]\n    res = optR.predict(y_pred, optR.coefficients()).astype(float)\n\n    res_prediction.append(res)\n\nprediction = np.asarray(res_prediction).T","metadata":{"execution":{"iopub.status.busy":"2022-06-04T06:22:37.017128Z","iopub.execute_input":"2022-06-04T06:22:37.01759Z","iopub.status.idle":"2022-06-04T06:22:37.048116Z","shell.execute_reply.started":"2022-06-04T06:22:37.017543Z","shell.execute_reply":"2022-06-04T06:22:37.047459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raw_prediction[:, 0]","metadata":{"execution":{"iopub.status.busy":"2022-06-04T06:22:37.049191Z","iopub.execute_input":"2022-06-04T06:22:37.049485Z","iopub.status.idle":"2022-06-04T06:22:37.06487Z","shell.execute_reply.started":"2022-06-04T06:22:37.04944Z","shell.execute_reply":"2022-06-04T06:22:37.064276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optRs[0].coefficients()","metadata":{"execution":{"iopub.status.busy":"2022-06-04T06:22:37.066473Z","iopub.execute_input":"2022-06-04T06:22:37.066904Z","iopub.status.idle":"2022-06-04T06:22:37.074434Z","shell.execute_reply.started":"2022-06-04T06:22:37.066723Z","shell.execute_reply":"2022-06-04T06:22:37.072331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from matplotlib import pyplot as plt\n\nfor i in range(30):\n    plt.hist(prediction[:, i])\n    plt.title(f'{i}th label')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-04T06:22:37.075926Z","iopub.execute_input":"2022-06-04T06:22:37.076433Z","iopub.status.idle":"2022-06-04T06:22:44.308968Z","shell.execute_reply.started":"2022-06-04T06:22:37.076364Z","shell.execute_reply":"2022-06-04T06:22:44.308139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df = pd.read_csv('/kaggle/input/google-quest-challenge/sample_submission.csv')\nsub_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-04T06:22:44.313887Z","iopub.execute_input":"2022-06-04T06:22:44.314466Z","iopub.status.idle":"2022-06-04T06:22:44.380982Z","shell.execute_reply.started":"2022-06-04T06:22:44.314417Z","shell.execute_reply":"2022-06-04T06:22:44.380213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df.iloc[:, 1:] = prediction\nsub_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-04T06:22:44.382618Z","iopub.execute_input":"2022-06-04T06:22:44.383214Z","iopub.status.idle":"2022-06-04T06:22:44.421453Z","shell.execute_reply.started":"2022-06-04T06:22:44.383144Z","shell.execute_reply":"2022-06-04T06:22:44.420848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## avoid scoring error","metadata":{}},{"cell_type":"code","source":"pub_qa_id = [39, 46, 70, 132, 200, 245, 257, 267, 284, 292, 296, 312, 322, 327, 334, 340, 357, 374, 375, 387, 391, 395, 444, 482, 483, 513, 542, 579, 589, 625, 641, 683, 725, 727, 728, 740, 748, 765, 811, 830, 851, 856, 885, 905, 929, 938, 939, 962, 1082, 1091, 1101, 1119, 1153, 1226, 1230, 1238, 1247, 1249, 1266, 1282, 1297, 1331, 1359, 1398, 1423, 1477, 1502, 1544, 1567, 1654, 1676, 1700, 1701, 1727, 1764, 1794, 1795, 1807, 1812, 1816, 1833, 1847, 1868, 1877, 1885, 1934, 1959, 1983, 1990, 2005, 2018, 2027, 2042, 2066, 2070, 2075, 2094, 2128, 2163, 2180, 2203, 2230, 2244, 2257, 2277, 2278, 2286, 2303, 2335, 2374, 2387, 2395, 2455, 2465, 2474, 2487, 2493, 2534, 2569, 2573, 2580, 2592, 2607, 2621, 2655, 2666, 2669, 2670, 2676, 2691, 2748, 2763, 2774, 2789, 2793, 2795, 2797, 2806, 2844, 2868, 2895, 2912, 2922, 2931, 2968, 3034, 3062, 3087, 3107, 3173, 3207, 3229, 3290, 3336, 3378, 3399, 3437, 3461, 3463, 3502, 3504, 3524, 3526, 3527, 3532, 3543, 3544, 3560, 3592, 3671, 3682, 3696, 3720, 3787, 3854, 3871, 3876, 3881, 3901, 3941, 3943, 3949, 3961, 3962, 3967, 4006, 4039, 4057, 4070, 4099, 4139, 4157, 4160, 4176, 4182, 4183, 4211, 4213, 4250, 4263, 4269, 4281, 4284, 4285, 4286, 4346, 4387, 4388, 4401, 4416, 4441, 4497, 4546, 4547, 4564, 4569, 4575, 4598, 4607, 4663, 4679, 4743, 4751, 4778, 4792, 4835, 4881, 4901, 4954, 4973, 5001, 5003, 5019, 5035, 5050, 5057, 5076, 5091, 5095, 5141, 5152, 5253, 5367, 5373, 5403, 5417, 5422, 5499, 5503, 5530, 5541, 5542, 5660, 5663, 5697, 5753, 5783, 5790, 5835, 5847, 5862, 5878, 5891, 5904, 5907, 5914, 5936, 5947, 5958, 5972, 5979, 5998, 6016, 6042, 6058, 6079, 6087, 6111, 6126, 6132, 6159, 6204, 6212, 6226, 6258, 6271, 6280, 6285, 6301, 6319, 6325, 6331, 6332, 6336, 6346, 6378, 6379, 6420, 6445, 6481, 6494, 6495, 6502, 6560, 6580, 6583, 6621, 6643, 6646, 6688, 6694, 6715, 6723, 6737, 6744, 6745, 6766, 6770, 6774, 6821, 6832, 6838, 6856, 6888, 6889, 6955, 6957, 6964, 6988, 6994, 7012, 7018, 7036, 7064, 7072, 7114, 7116, 7123, 7150, 7165, 7176, 7194, 7201, 7216, 7247, 7254, 7272, 7278, 7281, 7293, 7302, 7326, 7404, 7410, 7438, 7477, 7481, 7485, 7519, 7520, 7525, 7531, 7544, 7546, 7589, 7595, 7614, 7640, 7654, 7672, 7711, 7727, 7739, 7758, 7766, 7815, 7838, 7852, 7869, 7878, 7899, 7935, 7939, 7970, 8021, 8032, 8045, 8070, 8089, 8115, 8143, 8146, 8191, 8197, 8206, 8212, 8242, 8245, 8250, 8258, 8271, 8273, 8339, 8350, 8355, 8376, 8395, 8412, 8427, 8437, 8464, 8496, 8516, 8517, 8551, 8564, 8591, 8621, 8626, 8629, 8672, 8684, 8685, 8690, 8738, 8755, 8756, 8760, 8771, 8773, 8778, 8823, 8834, 8842, 8846, 8875, 8916, 8921, 8932, 8934, 8938, 8973, 8987, 9001, 9006, 9018, 9033, 9065, 9140, 9141, 9174, 9213, 9225, 9228, 9237, 9240, 9256, 9259, 9263, 9298, 9324, 9350, 9391, 9400, 9439, 9454, 9476, 9478, 9497, 9545, 9567, 9569, 9590, 9597, 9623, 9640]\n# pub_qa_id = [70, 132, 200, 245, 257, 267, 284, 292, 296, 312, 322, 327, 334, 340, 357, 374, 375, 387, 391, 395, 444, 482, 483, 513, 542, 579, 589, 625, 641, 683, 725, 727, 728, 740, 748, 765, 811, 830, 851, 856, 885, 905, 929, 938, 939, 962, 1082, 1091, 1101, 1119, 1153, 1226, 1230, 1238, 1247, 1249, 1266, 1282, 1297, 1331, 1359, 1398, 1423, 1477, 1502, 1544, 1567, 1654, 1676, 1700, 1701, 1727, 1764, 1794, 1795, 1807, 1812, 1816, 1833, 1847, 1868, 1877, 1885, 1934, 1959, 1983, 1990, 2005, 2018, 2027, 2042, 2066, 2070, 2075, 2094, 2128, 2163, 2180, 2203, 2230, 2244, 2257, 2277, 2278, 2286, 2303, 2335, 2374, 2387, 2395, 2455, 2465, 2474, 2487, 2493, 2534, 2569, 2573, 2580, 2592, 2607, 2621, 2655, 2666, 2669, 2670, 2676, 2691, 2748, 2763, 2774, 2789, 2793, 2795, 2797, 2806, 2844, 2868, 2895, 2912, 2922, 2931, 2968, 3034, 3062, 3087, 3107, 3173, 3207, 3229, 3290, 3336, 3378, 3399, 3437, 3461, 3463, 3502, 3504, 3524, 3526, 3527, 3532, 3543, 3544, 3560, 3592, 3671, 3682, 3696, 3720, 3787, 3854, 3871, 3876, 3881, 3901, 3941, 3943, 3949, 3961, 3962, 3967, 4006, 4039, 4057, 4070, 4099, 4139, 4157, 4160, 4176, 4182, 4183, 4211, 4213, 4250, 4263, 4269, 4281, 4284, 4285, 4286, 4346, 4387, 4388, 4401, 4416, 4441, 4497, 4546, 4547, 4564, 4569, 4575, 4598, 4607, 4663, 4679, 4743, 4751, 4778, 4792, 4835, 4881, 4901, 4954, 4973, 5001, 5003, 5019, 5035, 5050, 5057, 5076, 5091, 5095, 5141, 5152, 5253, 5367, 5373, 5403, 5417, 5422, 5499, 5503, 5530, 5541, 5542, 5660, 5663, 5697, 5753, 5783, 5790, 5835, 5847, 5862, 5878, 5891, 5904, 5907, 5914, 5936, 5947, 5958, 5972, 5979, 5998, 6016, 6042, 6058, 6079, 6087, 6111, 6126, 6132, 6159, 6204, 6212, 6226, 6258, 6271, 6280, 6285, 6301, 6319, 6325, 6331, 6332, 6336, 6346, 6378, 6379, 6420, 6445, 6481, 6494, 6495, 6502, 6560, 6580, 6583, 6621, 6643, 6646, 6688, 6694, 6715, 6723, 6737, 6744, 6745, 6766, 6770, 6774, 6821, 6832, 6838, 6856, 6888, 6889, 6955, 6957, 6964, 6988, 6994, 7012, 7018, 7036, 7064, 7072, 7114, 7116, 7123, 7150, 7165, 7176, 7194, 7201, 7216, 7247, 7254, 7272, 7278, 7281, 7293, 7302, 7326, 7404, 7410, 7438, 7477, 7481, 7485, 7519, 7520, 7525, 7531, 7544, 7546, 7589, 7595, 7614, 7640, 7654, 7672, 7711, 7727, 7739, 7758, 7766, 7815, 7838, 7852, 7869, 7878, 7899, 7935, 7939, 7970, 8021, 8032, 8045, 8070, 8089, 8115, 8143, 8146, 8191, 8197, 8206, 8212, 8242, 8245, 8250, 8258, 8271, 8273, 8339, 8350, 8355, 8376, 8395, 8412, 8427, 8437, 8464, 8496, 8516, 8517, 8551, 8564, 8591, 8621, 8626, 8629, 8672, 8684, 8685, 8690, 8738, 8755, 8756, 8760, 8771, 8773, 8778, 8823, 8834, 8842, 8846, 8875, 8916, 8921, 8932, 8934, 8938, 8973, 8987, 9001, 9006, 9018, 9033, 9065, 9140, 9141, 9174, 9213, 9225, 9228, 9237, 9240, 9256, 9259, 9263, 9298, 9324, 9350, 9391, 9400, 9439, 9454, 9476, 9478, 9497, 9545, 9567, 9569, 9590, 9597, 9623, 9640]","metadata":{"execution":{"iopub.status.busy":"2022-06-04T06:22:44.422834Z","iopub.execute_input":"2022-06-04T06:22:44.423151Z","iopub.status.idle":"2022-06-04T06:22:44.455673Z","shell.execute_reply.started":"2022-06-04T06:22:44.423099Z","shell.execute_reply":"2022-06-04T06:22:44.454471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df_cols = sub_df.columns[1:]\nsub_df_cols","metadata":{"execution":{"iopub.status.busy":"2022-06-04T06:22:44.457325Z","iopub.execute_input":"2022-06-04T06:22:44.457924Z","iopub.status.idle":"2022-06-04T06:22:44.468793Z","shell.execute_reply.started":"2022-06-04T06:22:44.457694Z","shell.execute_reply":"2022-06-04T06:22:44.467824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(30):\n    sub_df_col = sub_df_cols[i]\n\n    pub_y_pred = raw_prediction[sub_df.qa_id.isin(pub_qa_id).values, i]\n    pub_y_res = sub_df.loc[sub_df.qa_id.isin(pub_qa_id).values, sub_df_col].values\n\n    pub_y_pred_argmax = np.argmax(pub_y_pred)\n    pub_y_pred_argmin = np.argmin(pub_y_pred)\n\n    if len(np.unique(pub_y_res)) == 1:\n        #if np.unique(pub_y_res)[0] == pub_y_pred[pub_y_pred_argmax]:\n        #    if np.unique(pub_y_res)[0] == pub_y_pred[pub_y_pred_argmin]:\n        #        if np.unique(pub_y_res)[0] > 0.5:\n        #            pub_y_res[pub_y_pred_argmin] = 0\n        #        else:\n        #            pub_y_res[pub_y_pred_argmax] = 1\n        #    else:\n        #        pub_y_res[pub_y_pred_argmin] = np.min(pub_y_pred)                \n        #else:\n        #    pub_y_res[pub_y_pred_argmax] = np.max(pub_y_pred)\n        pub_y_res = pub_y_pred\n    sub_df.loc[sub_df.qa_id.isin(pub_qa_id).values, sub_df_col] = pub_y_res\n\n\n    pri_y_pred = raw_prediction[~(sub_df.qa_id.isin(pub_qa_id).values), i]\n\n    # only for sub\n    if len(pri_y_pred) == 0:\n        continue    \n    pri_y_res = sub_df.loc[~(sub_df.qa_id.isin(pub_qa_id).values), sub_df_col].values  \n    \n    pri_y_pred_argmax = np.argmax(pri_y_pred)\n    pri_y_pred_argmin = np.argmin(pri_y_pred)    \n\n    if len(np.unique(pri_y_res)) == 1:\n        #if np.unique(pri_y_res)[0] == pri_y_pred[pri_y_pred_argmax]:\n        #    if np.unique(pri_y_res)[0] == pri_y_pred[pri_y_pred_argmin]:\n        #        if np.unique(pri_y_res)[0] > 0.5:\n        #            pri_y_res[pri_y_pred_argmin] = 0\n        #        else:\n        #            pri_y_res[pri_y_pred_argmax] = 1\n        #    else:\n        #        pri_y_res[pri_y_pred_argmin] = np.min(pri_y_pred)                \n        #else:\n        #    pri_y_res[pri_y_pred_argmax] = np.max(pri_y_pred)\n        pri_y_res = pri_y_pred\n        \n    sub_df.loc[~(sub_df.qa_id.isin(pub_qa_id).values), sub_df_col] = pri_y_res    ","metadata":{"execution":{"iopub.status.busy":"2022-06-04T06:22:44.470267Z","iopub.execute_input":"2022-06-04T06:22:44.470776Z","iopub.status.idle":"2022-06-04T06:22:44.580901Z","shell.execute_reply.started":"2022-06-04T06:22:44.470592Z","shell.execute_reply":"2022-06-04T06:22:44.580336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(30):\n    plt.hist(sub_df.iloc[:, i+1])\n    plt.title(f'{i}th : {sub_df.columns[i+1]}')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-04T06:22:44.583235Z","iopub.execute_input":"2022-06-04T06:22:44.583713Z","iopub.status.idle":"2022-06-04T06:22:52.294483Z","shell.execute_reply.started":"2022-06-04T06:22:44.583664Z","shell.execute_reply":"2022-06-04T06:22:52.293613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-04T06:22:52.295788Z","iopub.execute_input":"2022-06-04T06:22:52.29627Z","iopub.status.idle":"2022-06-04T06:22:52.470579Z","shell.execute_reply.started":"2022-06-04T06:22:52.296221Z","shell.execute_reply":"2022-06-04T06:22:52.469795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}