{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport gc\n\nfrom sklearn.model_selection import KFold\nfrom scipy.stats import spearmanr\nfrom tqdm import tqdm, tqdm_notebook\n\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\nfrom keras.models import Model\nfrom keras.models import Sequential\nfrom keras import optimizers\nfrom keras.layers import Embedding, Flatten, Dense,LeakyReLU, Input,concatenate, Dropout, GlobalMaxPooling1D,GlobalAveragePooling1D\nfrom keras.layers import Activation,LSTM, Bidirectional,ReLU\nfrom keras.callbacks import Callback\nfrom keras.callbacks.callbacks import EarlyStopping\n\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\nembeddings = pickle.load( open( \"/kaggle/input/pickled-crawl300d2m-for-kernel-competitions/crawl-300d-2M.pkl\", \"rb\" ) )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"dataPath = '/kaggle/input/google-quest-challenge/'\nraw_train_data = pd.read_csv(dataPath+'train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(raw_train_data.head(30))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission_df = pd.read_csv(dataPath+'sample_submission.csv')\nprint(sample_submission_df.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_labels = list(sample_submission_df.columns)\ntarget_labels.remove('qa_id')\nprint(target_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_data = raw_train_data[target_labels]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(['There are ' + str(target_data.shape[0]) + ' rows of data!'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tqdm_notebook().pandas()\nstop_words = set(stopwords.words('english'))\nraw_train_data['question_title'] = raw_train_data['question_title'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\nraw_train_data['question_body'] = raw_train_data['question_body'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\nraw_train_data['answer'] = raw_train_data['answer'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\nraw_train_data['category'] = raw_train_data['category'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \n                       \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \n                       \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \n                       \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \n                       \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \n                       \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\",\n                       \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \n                       \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \n                       \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \n                       \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \n                       \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\n                       \"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \n                       \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \n                       \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \n                       \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \n                       \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \n                       \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \n                       \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \n                       \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \n                       \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \n                       \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\n                       \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \n                       \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \n                       \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \n                       \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \n                       \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \n                       \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \n                       \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \n                       \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \n                       \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \n                       \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \n                       \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \n                       \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \n                       \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\n                       \"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \n                       \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n                       \"you're\": \"you are\", \"you've\": \"you have\" }\ndef clean_contractions(text, mapping):\n    specials = [\"’\", \"‘\", \"´\", \"`\"]\n    for s in specials:\n        text = text.replace(s, \"'\")\n    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n    return text\nraw_train_data['question_title'] = raw_train_data['question_title'].apply(lambda x: clean_contractions(x, contraction_mapping))\nraw_train_data['question_body'] = raw_train_data['question_body'].apply(lambda x: clean_contractions(x, contraction_mapping))\nraw_train_data['answer'] = raw_train_data['answer'].apply(lambda x: clean_contractions(x, contraction_mapping))\nraw_train_data['category'] = raw_train_data['category'].apply(lambda x: clean_contractions(x, contraction_mapping))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_train_data['question_title'] = raw_train_data.apply(lambda row: word_tokenize(row['question_title']), axis=1)\nraw_train_data['question_body'] = raw_train_data.apply(lambda row: word_tokenize(row['question_body']), axis=1)\nraw_train_data['answer'] = raw_train_data.apply(lambda row: word_tokenize(row['answer']), axis=1)\nraw_train_data['category'] = raw_train_data.apply(lambda row: word_tokenize(row['category']), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_test_data = pd.read_csv(dataPath+'test.csv')\nraw_test_data['question_title'] = raw_test_data['question_title'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\nraw_test_data['question_body'] = raw_test_data['question_body'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\nraw_test_data['answer'] = raw_test_data['answer'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\nraw_test_data['category'] = raw_test_data['category'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\nraw_test_data['question_title'] = raw_test_data['question_title'].apply(lambda x: clean_contractions(x, contraction_mapping))\nraw_test_data['question_body'] = raw_test_data['question_body'].apply(lambda x: clean_contractions(x, contraction_mapping))\nraw_test_data['answer'] = raw_test_data['answer'].apply(lambda x: clean_contractions(x, contraction_mapping))\nraw_test_data['category'] = raw_test_data['category'].apply(lambda x: clean_contractions(x, contraction_mapping))\nraw_test_data['question_title'] = raw_test_data.apply(lambda row: word_tokenize(row['question_title']), axis=1)\nraw_test_data['question_body'] = raw_test_data.apply(lambda row: word_tokenize(row['question_body']), axis=1)\nraw_test_data['answer'] = raw_test_data.apply(lambda row: word_tokenize(row['answer']), axis=1)\nraw_test_data['category'] = raw_test_data.apply(lambda row: word_tokenize(row['category']), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"maxAllowedSequenceLength = 65\ndef text_to_array(textVal):\n    emptyArr = np.zeros(300)    \n    textVal = textVal[:maxAllowedSequenceLength]\n    embed_text = [embeddings.get(text,emptyArr) for text in textVal]    \n    embed_text+= [emptyArr] * (maxAllowedSequenceLength - len(embed_text))  \n    return np.array(embed_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.DataFrame(columns = ['question_body','answer','question_title'])\ntest = pd.DataFrame(columns = ['question_body','answer','question_title'])\ntrain['question_body'] = raw_train_data['question_body']\ntrain['answer'] = raw_train_data['answer']\ntrain['question_title'] = raw_train_data['question_title']\ntrain['category'] = raw_train_data['category']\ntest['question_body'] = raw_test_data['question_body']\ntest['answer'] = raw_test_data['answer']\ntest['question_title'] = raw_test_data['question_title']\ntest['category'] = raw_test_data['category']\ntrain['question_body'] = train.apply(lambda row: np.array(text_to_array(row['question_body'])), axis=1)\ntrain['question_title'] = train.apply(lambda row: np.array(text_to_array(row['question_title'])), axis=1)\ntrain['answer'] = train.apply(lambda row: np.array(text_to_array(row['answer'])), axis=1)\ntrain['category'] = train.apply(lambda row: np.array(text_to_array(row['category'])), axis=1)\ntest['question_body'] = test.apply(lambda row: np.array(text_to_array(row['question_body'])), axis=1)\ntest['question_title'] = test.apply(lambda row: np.array(text_to_array(row['question_title'])), axis=1)\ntest['answer'] = test.apply(lambda row: np.array(text_to_array(row['answer'])), axis=1)\ntest['category'] = test.apply(lambda row: np.array(text_to_array(row['category'])), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del embeddings,raw_test_data,raw_train_data\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SpearmanRhoCallback(Callback):\n    def __init__(self, X_train,y_train,X_val,y_val,model_lstm):\n        self.x = X_train\n        self.y = y_train.values\n        self.x_val = X_val\n        self.y_val = y_val.values\n        self.model = model_lstm\n        \n    def on_train_begin(self, logs={}):\n        return\n\n    def on_train_end(self, logs={}):\n        return\n        \n    def on_epoch_begin(self, epoch,logs={}):\n        return\n\n    def on_epoch_end(self, epoch, logs={}):\n        rho_val = []\n        val_pred = self.model.predict(self.x_val)\n        shapeTarget = np.shape(self.y_val)\n        for i_col in range(0,shapeTarget[1]):\n            rho_val.append(spearmanr(self.y_val[:,i_col], val_pred[:,i_col]))\n        rho_val = np.mean(rho_val)        \n        return rho_val\n    \n    def on_batch_begin(self, batch, logs={}):\n        return\n\n    def on_batch_end(self, batch, logs={}):\n        return    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kf = KFold(n_splits=3, random_state=2019, shuffle=True)\ny_test = []\nfor ind, (tr, val) in enumerate(kf.split(train)):\n    X_train = train.loc[tr,:]\n    y_train = target_data.loc[tr]\n    X_val = train.loc[val,:]\n    y_val = target_data.loc[val]\n    inp1 = Input(shape=(maxAllowedSequenceLength,300))\n    inp2 = Input(shape=(maxAllowedSequenceLength,300))\n    inp3 = Input(shape=(maxAllowedSequenceLength,300))\n    inp4 = Input(shape=(maxAllowedSequenceLength,300))\n    inp = concatenate([inp1,inp2,inp3,inp4])\n    x = Dense(2048,activation = 'relu')(inp)\n    x = Dense(1024,activation = 'relu')(inp)\n    hidden = concatenate([GlobalMaxPooling1D()(x),GlobalAveragePooling1D()(x)])\n    hidden = Dense(256,activation = 'relu')(hidden)\n    output = Dense(30,activation = 'sigmoid')(hidden)\n    model_lstm = Model(inputs=[inp1,inp2,inp3,inp4], outputs=output)\n    es = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='auto', baseline=None, restore_best_weights=True)\n    model_lstm.compile(optimizer='adam',loss=['binary_crossentropy'])\n    model_lstm.summary()\n    model_lstm.fit([np.stack(X_train['question_title']),\n                    np.stack(X_train['question_body']),\n                    np.stack(X_train['answer']),\n                    np.stack(X_train['category'])], \n                   y_train, epochs=100, batch_size=32,validation_data=(\n                       [np.stack(X_val['question_title']),\n                        np.stack(X_val['question_body']),\n                        np.stack(X_val['answer']),\n                        np.stack(X_val['category'])], \n                       y_val), \n                   verbose=True,callbacks = [es])\n    y_test.append(model_lstm.predict([\n        np.stack(test['question_title']),\n        np.stack(test['question_body']),\n        np.stack(test['answer']),\n        np.stack(test['category'])\n    ]))\n    del model_lstm\n    gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_out = np.mean(y_test,axis = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission_df[target_labels] = np.squeeze(y_out)\nsample_submission_df.head()\nsample_submission_df.to_csv(\"submission.csv\", index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}