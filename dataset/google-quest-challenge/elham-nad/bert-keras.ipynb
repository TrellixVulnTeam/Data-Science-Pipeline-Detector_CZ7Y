{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport numpy as np\nfrom tqdm import tqdm\n%matplotlib inline  \nfrom sklearn.manifold import TSNE\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import colors\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nimport time \nimport multiprocessing \n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn import svm, datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom itertools import cycle\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import classification_report\n\nimport re\nfrom sklearn.svm import SVC\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.naive_bayes import MultinomialNB\n\n\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\nfrom wordcloud import WordCloud, STOPWORDS \nimport nltk \nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nfrom collections import defaultdict\nfrom gensim.models import LdaMulticore, TfidfModel, CoherenceModel\nfrom gensim.corpora import Dictionary\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nfrom keras.models import Sequential\nfrom keras.layers.recurrent import LSTM, GRU\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.utils import np_utils\nfrom keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\nfrom keras.preprocessing import sequence, text\nfrom keras.callbacks import EarlyStopping\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train = pd.read_csv('../input/dataclean/data.sv')\n#test = pd.read_csv('../input/spooky/test.csv')\ntraincleaned=pd.read_csv('../input/dataclean/data.csv')\n#traincleaned is the train data after cleaning, so I saved and load it here due to simplisity.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#traincleaned[:3].drop(traincleaned.columns[1], axis=1)\ntraincleaned.drop('Unnamed: 0', axis=1,inplace=True)  \ntraincleaned[:3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = traincleaned.clean_text\nY = traincleaned.author\n\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\nprint(X_train.shape)\nprint(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nlabel_encoder = LabelEncoder()\ninteger_encoded1 = label_encoder.fit_transform(y_train)\ninteger_encoded2 = label_encoder.fit_transform(y_test)\n\n\nle_name_mapping = dict(zip(label_encoder.transform(label_encoder.classes_),label_encoder.classes_))\nprint(\"Label Encoding Classes as \")\nprint(le_name_mapping)\n\n\n#Converts a class vector (integers) to binary class matrix.\ny_train=np_utils.to_categorical(integer_encoded1,num_classes=3)\ny_val=np_utils.to_categorical(integer_encoded2,num_classes=3)\nprint(\"One Hot Encoded class shape \")\nprint(y_train.shape)\nprint(y_train[0:3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train0 = X_train.values.astype(str)\n#y_train = df0_train['sentiment'].values\n\nX_val0 = X_test.values.astype(str)\n#y_val = df0_val['sentiment'].values\nprint('df_train shape: {}'.format(X_train0.shape))\nprint('df_val shape: {}'.format(X_val0.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First, we initialize the Tokenizer object which is imported from the Keras library as a token. Then fitting the tokenizer on the whole text where each word is assigned a unique number and every word is now represented by a number. The third line converts each sentence into a sequence of numbers that is assigned to in the previous line.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#from tensorflow.python.keras.preprocessing import sequence\n#from tensorflow.python.keras.preprocessing import text\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n# Vectorization parameters\n# Limit on the number of features. We use the top 20K features.\nTOP_K = 20000\n\n# Limit on the length of text sequences. Sequences longer than this\n# will be truncated.\nMAX_SEQUENCE_LENGTH = 500\n\n#def sequence_vectorize(train_texts, val_texts):\n\n    # Create vocabulary with training texts.\ntokenizer = text.Tokenizer(num_words=TOP_K)\ntokenizer.fit_on_texts(X_train0)\n\n    # Vectorize training and validation texts.\nx_train = tokenizer.texts_to_sequences(X_train0)\nx_val = tokenizer.texts_to_sequences(X_val0)\n\n    # Get max sequence length.\nmax_length = len(max(x_train, key=len))\nif max_length > MAX_SEQUENCE_LENGTH:\n    max_length = MAX_SEQUENCE_LENGTH\n\n    # Fix sequence length to max value. Sequences shorter than the length are\n    # padded in the beginning and sequences longer are truncated\n    # at the beginning.\nx_train = sequence.pad_sequences(x_train, maxlen=max_length)\nx_val = sequence.pad_sequences(x_val, maxlen=max_length)\n\n\nvocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n\n\nprint(x_train.shape)\nprint(x_val.shape)\n\n\nprint(max_length)\n\nprint(vocab_size)\nprint(x_train[0])\nprint(X_train0[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n\nvocab_size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_index = tokenizer.word_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets use glove word embeding:\n\nfrom numpy import array\nfrom numpy import asarray\nfrom numpy import zeros\n\nembeddings = dict()\nglove_file = open('../input/glove100/glove.6B.100d.txt', encoding=\"utf8\")\n#!wget http://nlp.stanford.edu/data/glove.6B.zip\n#!unzip -q glove.6B.zip\n# load the GloVe vectors in a dictionary:\nembeddings_index = {}\nf = open('../input/glove100/glove.6B.100d.txt', encoding='utf8')\nfor line in tqdm(f):\n    values = line.split()\n    word = ''.join(values[:-100])\n    coefs = np.asarray(values[-100:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))\n\nwords = ['sad']\nfor w in words:\n    if w in embeddings_index.keys():\n        print('Found the word {} in the dictionary'.format(w))\n        \n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create an embedding matrix for the words we have in our dataset\nembedding_matrix = np.zeros((len(tokenizer.word_index) + 1, 100))\nfor word, i in tqdm(word_index.items()):\n    embedding_vector = embeddings.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(embedding_matrix.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\n#we compute an index mapping words to \n#known embeddings, by parsing the data dump of pre-trained embeddings:\nNB_WORDS=17666 \nGLOVE_DIM=100\nemb_matrix = np.zeros((NB_WORDS, GLOVE_DIM))\n\nfor w, i in tokenizer.word_index.items():\n    # The word_index contains a token for all words of the training data so we need to limit that\n    if i < NB_WORDS:\n        vect = embeddings_index.get(w)\n        # Check if the word from the training data occurs in the GloVe word embeddings\n        # Otherwise the vector is kept with only zeros\n        if vect is not None:\n            emb_matrix[i] = vect\n    else:\n        break\n        \n        \n\nEMBEDDING_DIM = 100\nembedding_matrix = np.zeros((17666 , EMBEDDING_DIM))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector\n        \n'''  \n        \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM,Dense, Dropout,Bidirectional\nfrom tensorflow.keras.layers import SpatialDropout1D\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.preprocessing.text import Tokenizer\n\n\nembedding_vector_length = 100\n\nmodel1 = Sequential()\n\nmodel1.add(Embedding(len(tokenizer.word_index) + 1,\n                     embedding_vector_length,\n                     weights=[embedding_matrix],\n                     input_length=409,\n                     trainable=False) )\n\nmodel1.add(LSTM(64, dropout=0.4, recurrent_dropout=0.4))\nmodel1.add(Dropout(0.5))\nmodel1.add(Dense(3, activation='softmax'))\nmodel1.compile(loss='categorical_crossentropy',optimizer='adam', \n                           metrics=['accuracy'])\nprint(model1.summary())\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.callbacks import EarlyStopping\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\nhistory_1 = model1.fit(x_train, y_train,\n batch_size=100,\n epochs=100,\n validation_data=(x_val, y_val),\n callbacks=[es])\n#We save this model so that we can use in own web app\n\n\nmodel1.save('model_1_final.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\ndef plot_history(history):\n    acc = history.history['accuracy']\n    val_acc = history.history['val_accuracy']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    x = range(1, len(acc) + 1)\n\n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(x, acc, 'b', label='Training acc')\n    plt.plot(x, val_acc, 'r', label='Validation acc')\n    plt.title('Training and validation accuracy')\n    plt.legend()\n    plt.subplot(1, 2, 2)\n    plt.plot(x, loss, 'b', label='Training loss')\n    plt.plot(x, val_loss, 'r', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_history(history_1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM,Dense, Dropout,Bidirectional\nfrom tensorflow.keras.layers import SpatialDropout1D\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.preprocessing.text import Tokenizer\n\n\nembedding_vector_length = 100\n\nmodel_n = Sequential()\n\nmodel_n.add(Embedding(len(tokenizer.word_index) + 1,embedding_vector_length,     \n                                         input_length=409) )\n\nmodel_n.add(LSTM(64, dropout=0.4, recurrent_dropout=0.4))\nmodel_n.add(Dropout(0.5))\nmodel_n.add(Dense(3, activation='softmax'))\nmodel_n.compile(loss='categorical_crossentropy',optimizer='adam', \n                           metrics=['accuracy'])\nprint(model_n.summary())\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.callbacks import EarlyStopping\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\nhistory_n = model_n.fit(x_train, y_train,\n batch_size=100,\n epochs=100,\n validation_data=(x_val, y_val),\n callbacks=[es])\n#We save this model so that we can use in own web app\n\nplot_history(history__n)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Model\nfrom keras.layers import Input\nfrom keras.layers import Embedding\nfrom keras.layers.convolutional import Conv1D\nfrom keras.layers import GlobalMaxPooling1D, Dropout,MaxPool1D\nfrom keras.layers.core import Dense, Activation\nfrom keras.layers import concatenate\n\n\nnp.random.seed(42)\n#You needs to reshape your input data according to Conv1D layer input format - (batch_size, steps, input_dim). Try\n\n\n# set parameters of matrices and convolution\nembedding_dim = 100\nnb_filter = 64\nfilter_length = 3\nhidden_dims = 32\nstride_length = 1\n\nfrom keras.layers import Embedding\n\nembedding_layer = Embedding(len(tokenizer.word_index) + 1,\n                            embedding_dim,\n                            weights=[embedding_matrix],\n                            input_length=409,name=\"Embedding\")\ninp = Input(shape=(409,), dtype='int32')\nembeddings = embedding_layer(inp)\n\n\nconv1 = Conv1D(filters=512,  # Number of filters to use\n                    kernel_size=filter_length, # n-gram range of each filter.\n                    padding='same',  #valid: don't go off edge; same: use padding before applying filter\n                    activation='relu',\n                    name=\"CONV1\")(embeddings)\n\nconv2 = Conv1D(filters=512,  # Number of filters to use\n                    kernel_size=filter_length, # n-gram range of each filter.\n                    padding='same',  #valid: don't go off edge; same: use padding before applying filter\n                    activation='relu',\n                    name=\"CONV2\")(embeddings)\n\nconv3 = Conv1D(filters=512,  # Number of filters to use\n                    kernel_size=filter_length, # n-gram range of each filter.\n                    padding='same',  #valid: don't go off edge; same: use padding before applying filter\n                    activation='relu',\n                    name=\"CONV3\")(embeddings)\n\n\nmax1 = MaxPool1D(409, strides=1,name=\"MaxPool1D1\")(conv1)\nmax2 = MaxPool1D(409, strides=1,name=\"MaxPool1D2\")(conv2)\nmax3 = MaxPool1D(409, strides=1,name=\"MaxPool1D3\")(conv3)\n\nconc = concatenate([max1, max2, max3])\n\nflat = Flatten(name=\"FLATTEN\")(conc)\ndrop = Dropout(0.4,name=\"Dropout\")(flat)\nout =  Dense(3, activation='softmax', kernel_regularizer='l2',name=\"Dense\")(drop)\n#adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n\n\nmodel2 = Model(inputs=[inp], outputs=[out],name=\"CNN\")\nmodel2.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\nmodel2.summary()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We save this model\n\nfrom keras.callbacks import EarlyStopping\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\nhistory2=model2.fit(x_train, y_train,\n batch_size=100,\n epochs=30,\n validation_data=(x_val, y_val),\n callbacks=[es])\n\nmodel2.save('model_cnn_final.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_history(history2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_dim = 100\nnb_filter = 512\nfilter_length = 2\nhidden_dims = 32\nstride_length = 1\n\ninputs = Input((409, ), \n               name='word_IDs')\nembeddings = Embedding(len(tokenizer.word_index) + 1, \n                       embedding_dim,\n                       weights=[embedding_matrix], \n                       input_length=409)(inputs)\nconvolution = Conv1D(filters=nb_filter,  # Number of filters to use\n                    kernel_size=filter_length, # n-gram range of each filter.\n                    padding='same',  #valid: don't go off edge; same: use padding before applying filter\n                    activation='relu',\n                    strides=stride_length)(embeddings)\nconvolution2 = Activation(activation='tanh')(convolution)\npooling = GlobalMaxPooling1D()(convolution2)\ndropout1 = Dropout(0.2)(pooling)\ndense = Dense(hidden_dims, activation='relu')(dropout1)\ndropout2 = Dropout(0.2)(dense)\noutput = Dense(3, activation='softmax')(dropout2)\n\nmodel666 = Model(inputs=[inputs], outputs=[output])\n#adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n\n\nmodel666.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\nmodel666.summary()\n\n\n\nfrom keras.callbacks import EarlyStopping\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\nhistory2=model666.fit(x_train, y_train,\n batch_size=100,\n epochs=30,\n validation_data=(x_val, y_val),\n callbacks=[es])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nmax_len = 409\nmax_words = 17666\nemb_dim = 100\n\n\nmodel3 = Sequential(name=\"BI-LSTM\")\nmodel3.add(Embedding(max_words, emb_dim,weights=[embedding_matrix], input_length=max_len,name=\"Embedding\"))\nmodel3.add(Bidirectional(LSTM(64, return_sequences=False,dropout=0.4, recurrent_dropout=0.4, name=\"BI-LSTM\")))\nmodel3.add(Dropout(0.5,name= \"Dropout\"))\nmodel3.add(Dense(3, activation='softmax',name= \"Dense\"))\n\nmodel3.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\nmodel3.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.callbacks import EarlyStopping\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\nhistory3=model3.fit(x_train, y_train,\n batch_size=100,\n epochs=30,\n validation_data=(x_val, y_val),\n callbacks=[es])\nmodel3.save('bilstm_1_FINAL.h5')\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_history(history3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_score = model3.predict(x_val,verbose=1)\ny_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_val","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vv=np_utils.to_categorical(y_score,num_classes=3)\nvv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"probabilities = model3.predict_proba(x_val,verbose=1)\nprint(probabilities)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_score2 = model3.predict_classes(x_val,verbose=1)\ny_score2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    import itertools\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rr=label_encoder.inverse_transform(integer_encoded2)\n\nuu=label_encoder.inverse_transform(y_score2)\n\n\n\n\ncnf_matrix = confusion_matrix(rr, uu,labels=['EAP', 'HPL', 'MWS'])\nnp.set_printoptions(precision=2)\n\n# Plot non-normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=['EAP', 'HPL', 'MWS'],\n                      title='Confusion matrix')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.utils import to_categorical\n# data labels = [1, 2, 1...]\n#labels_index = { \"website\" : 0, \"money\" : 1 ....} \nprint(le_name_mapping)\n\ntt=np.vectorize(le_name_mapping.get)(y_score2)\ntt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction_model2 = model3.predict(x_val)\nprediction_model2 = pd.DataFrame(data={'author': y_test, 'new_predictions': label_encoder.inverse_transform(y_score2),'truth':y_test})\nprediction_model2[:4]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('ggplot')\n\n#ww = label_binarize(y1, classes=[0, 1, 2])\nn_classes = 3\nY_BIN=label_binarize(integer_encoded2, classes=[0, 1, 2])\n\n# Compute ROC curve and ROC area for each class\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(n_classes):\n    fpr[i], tpr[i], _ = roc_curve(Y_BIN[:, i], probabilities[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n# Compute micro-average ROC curve and ROC area\nfpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(Y_BIN.ravel(), probabilities.ravel())\nroc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n\ncolor = ['yellowgreen', 'maroon', 'lightblue']\n\n# Plot of a ROC curve for a specific class\nplt.figure()\n\nplt.plot(fpr[2], tpr[2], label='ROC curve (area = %0.2f)' % roc_auc[2],\n         color='turquoise', lw=1.5)\nplt.plot([0, 1], [0, 1], 'k--',color='navy')\nplt.xlim([-0.1, 1.1])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Micro-average ROC curve BI LSTM')\nplt.legend(loc=\"lower right\")\nplt.show()\n\n# Plot ROC curve\nplt.figure(facecolor=\"w\")\nplt.plot(fpr[\"micro\"], tpr[\"micro\"],color='turquoise',\n         label='micro-average ROC curve (area = {0:0.2f})'\n               ''.format(roc_auc[\"micro\"]))\n\n\nfor i in range(n_classes):\n    fpr[i], tpr[i], _ = roc_curve(Y_BIN[:, i], probabilities[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\ncolors = cycle(['lime', 'magenta', 'blue'])\nfor i, color in zip(range(n_classes), colors):\n    plt.plot(fpr[i], tpr[i], color=color, lw=1.5,\n             label='ROC curve of class {0} (area = {1:0.2f})'\n             ''.format(i, roc_auc[i]))\n    \nplt.plot([0, 1], [0, 1], 'r--', lw=2,color='navy')\nplt.xlim([-0.05, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC curve - Bi LSTM')\nplt.legend(loc=\"lower right\",fontsize=10)\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nmax_len = 409\nmax_words = 17666\nemb_dim = 100\n\n\nBB = Sequential()\nBB.add(Embedding(max_words, emb_dim,weights=[embedding_matrix], input_length=max_len))\nBB.add(Bidirectional(LSTM(64, return_sequences=True,dropout=0.4, recurrent_dropout=0.4)))\nBB.add(Bidirectional(LSTM(34, return_sequences=False,dropout=0.4, recurrent_dropout=0.4)))\n\nBB.add(Dropout(0.5))\nBB.add(Dense(3, activation='softmax'))\n\nBB.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\nBB.summary()\n\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.callbacks import EarlyStopping\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\nhistory_bilstm=BB.fit(x_train, y_train,\n batch_size=100,\n epochs=30,\n validation_data=(x_val, y_val),\n callbacks=[es])\n#model3.save('bilstm_1.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model3.save('bilstm_2_final.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_history(history_bilstm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras import backend as K\n##Keras has implemented some functions for getting or setting weights for every layer. \n#layer.get_weights(): returns the weights of the layer as a list of Numpy arrays.\n#for layer in model.layers:\n    #weights = layer.get_weights() \nmax_len = 409\nmax_words = 17666\nemb_dim = 100\n\nclass attention(Layer):\n    \n    def __init__(self, return_sequences=False):\n        self.return_sequences = return_sequences\n        super(attention,self).__init__()\n        \n    def build(self, input_shape):\n        \n        self.W=self.add_weight(name=\"att_weight\", shape=(input_shape[-1],1),\n                               initializer=\"normal\")\n        self.b=self.add_weight(name=\"att_bias\", shape=(input_shape[1],1),\n                               initializer=\"zeros\")\n        \n        super(attention,self).build(input_shape)\n        \n    def call(self, x):\n        \n        e = K.tanh(K.dot(x,self.W)+self.b)\n        a = K.softmax(e, axis=1)\n        output = x*a\n        \n        if self.return_sequences:\n            return output\n        \n        return K.sum(output, axis=1)\n    \n    def get_config(self):\n            cfg = super().get_config()\n            return cfg    \n\nmodel4 = Sequential()\nmodel4.add(Embedding(max_words, emb_dim,weights=[embedding_matrix], input_length=max_len))\nmodel4.add(Bidirectional(LSTM(64, return_sequences=True,dropout=0.4, recurrent_dropout=0.4)))\nmodel4.add(attention(return_sequences=False)) # return_sequences=False\nmodel4.add(Dropout(0.4))\nmodel4.add(Dense(3, activation='softmax'))\n\nmodel4.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\nmodel4.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.callbacks import EarlyStopping\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\nhistory4=model4.fit(x_train, y_train,\n batch_size=100,\n epochs=40,\n validation_data=(x_val, y_val),\n callbacks=[es])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_history(history4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model4.save('att.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nfrom keras.models import Sequential\nfrom keras.layers.recurrent import LSTM\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers import Bidirectional, SpatialDropout1D\nfrom keras.optimizers import SGD,Adam\nfrom keras.layers.core import Dense,Activation,Dropout\nfrom keras.utils import np_utils\nfrom keras.callbacks import EarlyStopping\nfrom keras.preprocessing import sequence,text\nfrom keras.callbacks import Callback,EarlyStopping,ReduceLROnPlateau\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nword_index = tokenizer.word_index\n\nmodel6 = Sequential()\nmodel6.add(Embedding(max_words, emb_dim,weights=[embedding_matrix], input_length=max_len))\nmodel6.add(SpatialDropout1D(0.3))\n\nmodel6.add(Bidirectional(LSTM(300, dropout=0.3, recurrent_dropout=0.3,return_sequences = True)))\nmodel6.add(Bidirectional(LSTM(300, dropout=0.3, recurrent_dropout=0.3)))\n\nmodel6.add(Dense(512, activation='relu'))\nmodel6.add(Dropout(0.5))\n\nmodel6.add(Dense(512, activation='relu'))\nmodel6.add(Dropout(0.5))\n\nmodel6.add(Dense(3))\nmodel6.add(Activation('softmax'))\nadam = Adam(lr=0.01, decay = 0.05)\nmodel6.compile(loss='categorical_crossentropy', optimizer=adam,metrics=['accuracy'])\nmodel6.summary()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\nhistory6=model6.fit(x_train, y_train,\n batch_size=64,\n epochs=40,\n validation_data=(x_val, y_val),\n callbacks=[es])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model6.save('bilstm2.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_history(history6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd \n  \ndata = [['LG', 0.75], ['NB', 0.75], ['LSTM', 0.82],['BI_LSTM', 0.82],['CNN', 0.80],['ATT', 0.80]] \n  \n# Create the pandas DataFrame \ndf = pd.DataFrame(data, columns = ['Classifier', 'Acutacy']) \n  \n# print dataframe. \ndf ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nnp.random.seed(1)\n\ncl = [\"LG\", \"NB\", \"LSTM\", \"BI_LSTM\", \"CNN\", \"ATT\"]\nacc = [0.75, 0.75, 0.82, 0.82, 0.80, 0.80]\n\nplt.scatter(cl, acc,  color='blue')\nplt.title(\"Diffrent model comparesion\",fontsize=20,linewidths=3)\nplt.xlabel(\"Models\",fontsize=15)\nplt.ylabel(\"Accuracy\",fontsize=15)\nplt.ylim([0, 0.99])\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}