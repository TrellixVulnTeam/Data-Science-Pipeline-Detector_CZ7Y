{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Introduction\n\nThis kernel shows how to use NBSVM (Naive Bayes - Support Vector Machine) on this dataset by training separate models for the question and answer targets, and then combining the predictions at submission.\n\nNBSVM was introduced by Sida Wang and Chris Manning in the paper [Baselines and Bigrams: Simple, Good Sentiment and Topic Classiﬁcation](https://nlp.stanford.edu/pubs/sidaw12_simple_sentiment.pdf)\n\nSpecial thanks to Jeremy Howard for this basis kernel from a previous [competition](https://www.kaggle.com/jhoward/nb-svm-strong-linear-baseline). "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pathlib import Path\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = Path(\"/kaggle/input/google-quest-challenge/train.csv\")\ntest =  Path(\"/kaggle/input/google-quest-challenge/test.csv\")\nsample_sub = Path(\"/kaggle/input/google-quest-challenge/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(train)\ntest_df = pd.read_csv(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.shape, test_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_targets = train_df.iloc[:, 11:]\ntrain_feats = train_df.iloc[:, 1:11]\n\ntest_targets = test_df.iloc[:, 11:]\ntest_feats = test_df.iloc[:, 1:11]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_targets.columns, train_feats.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Distinguish between features and targets about questions and answers"},{"metadata":{"trusted":true},"cell_type":"code","source":"question_target_cols = [col for col in train_targets.columns if col.split('_')[0] == 'question']\nanswer_target_cols = [col for col in train_targets.columns if col.split('_')[0] == 'answer']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"question_feat_cols = [col for col in train_feats.columns if col.split('_')[0] == 'question']\nanswer_feat_cols = [col for col in train_feats.columns if col.split('_')[0] == 'answer']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Sample question and answer"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Question:')\ntrain_df.iloc[0].question_title, train_df.iloc[0].question_body","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Answer:')\ntrain_df.iloc[0].answer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training the questions model"},{"metadata":{},"cell_type":"markdown","source":"Let's start with a simple implementation where the question features are purely the question title and body (truncated to median) and answer features are from the answer"},{"metadata":{"trusted":true},"cell_type":"code","source":"def trunc_text(text, n=102):\n    tokens = text.split()\n    return ' '.join(tokens[: n]) if len(tokens) > n else text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_question_text = train_feats.question_title + ' ' + train_feats.question_body\ntest_question_text = test_feats.question_title + ' ' + test_feats.question_body\n\ntrain_answer_text = train_feats.answer\ntest_answer_text = test_feats.answer\n\nlens = train_question_text.apply(lambda x: len(x.split()))\nprint(lens.describe())\n\ntrain_question_text = train_question_text.apply(lambda x: trunc_text(x))\ntest_question_text = test_question_text.apply(lambda x: trunc_text(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We'll create a list of all the labels to predict, and we'll also create a 'none' label so we can see how many comments have no labels. We can then summarize the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_targets['none_q'] = 1-train_targets[question_target_cols].max(axis=1)\ntrain_targets.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Filling NaNs"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_question_text.fillna(\"unknown\", inplace=True)\ntest_question_text.fillna(\"unknown\", inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Building the model\nWe'll start by creating a bag of words representation, as a term document matrix. We'll use ngrams, as suggested in the NBSVM paper."},{"metadata":{"trusted":true},"cell_type":"code","source":"import re, string\nre_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\ndef tokenize(s): return re_tok.sub(r' \\1 ', s).split()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n = train_df.shape[0]\nvec = TfidfVectorizer(ngram_range=(1,2), tokenizer=tokenize,\n               min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n               smooth_idf=1, sublinear_tf=1 )\ntrn_term_doc = vec.fit_transform(train_question_text)\ntest_term_doc = vec.transform(test_question_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vec_answers = TfidfVectorizer(ngram_range=(1,2), tokenizer=tokenize,\n               min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n               smooth_idf=1, sublinear_tf=1 )\ntrn_answers_term_doc = vec_answers.fit_transform(train_answer_text)\ntest_answers_term_doc = vec_answers.transform(test_answer_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trn_term_doc, test_term_doc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create category dummy features"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_cat_dummies = pd.get_dummies(train_feats['category']).values\ntest_cat_dummies = pd.get_dummies(test_feats['category']).values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Naive Bayes feature equation:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def pr(y_i, y):\n    p = x[y==y_i].sum(0)\n    return (p+1) / ((y==y_i).sum()+1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_mdl(y):\n    # Binarizing\n    y = y.gt(0.5).astype(int)\n    y = y.values\n    r = np.log(pr(1,y) / pr(0,y))\n    #m = LinearRegression()\n    m = LogisticRegression(C=4, dual=True)\n    x_nb = x.multiply(r)\n    x_nb_cat = np.concatenate([x_nb.toarray(), train_cat_dummies], axis=1)\n    return m.fit(x_nb_cat, y), r","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train and predict w/ question model"},{"metadata":{"trusted":true},"cell_type":"code","source":"x = trn_term_doc\ntest_x = test_term_doc\n\nquestion_preds = np.zeros((test_df.shape[0], len(question_target_cols)))\n\nfor i, j in enumerate(question_target_cols):\n    print('fitting', j)\n    m,r = get_mdl(train_targets[j])\n    test_x_nb_cat = np.concatenate([test_x.multiply(r).toarray(), test_cat_dummies], axis=1)\n    #question_preds[:,i] = np.clip(m.predict(), 0, 1)\n    print('predicting ...')\n    question_preds[:,i] = m.predict_proba(test_x_nb_cat)[:,1]\n    \nquestion_preds_df = pd.DataFrame(question_preds, columns=question_target_cols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train and predict w/ answer model"},{"metadata":{"trusted":true},"cell_type":"code","source":"x = trn_answers_term_doc\ntest_x = test_answers_term_doc\n\nanswer_preds = np.zeros((test_df.shape[0], len(answer_target_cols)))\n\nfor i, j in enumerate(answer_target_cols):\n    print('fit', j)\n    m,r = get_mdl(train_targets[j])\n    test_x_nb_cat = np.concatenate([test_x.multiply(r).toarray(), test_cat_dummies], axis=1)\n    #answer_preds[:,i] = np.clip(m.predict(test_x.multiply(r)), 0, 1)\n    print('predicting ...')\n    answer_preds[:,i] = m.predict_proba(test_x_nb_cat)[:,1]\n    \nanswer_preds_df = pd.DataFrame(answer_preds, columns=answer_target_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_df = pd.concat([question_preds_df, answer_preds_df], axis=1)\npreds_df['qa_id'] = test_df.qa_id","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submit predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df = pd.read_csv(sample_sub)\nsub_df_columns = sub_df.columns.values.tolist()\nsub_df = preds_df[sub_df_columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df.to_csv(\"submission.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}