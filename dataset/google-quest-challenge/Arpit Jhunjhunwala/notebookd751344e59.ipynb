{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-20T04:47:49.822464Z","iopub.execute_input":"2021-07-20T04:47:49.822811Z","iopub.status.idle":"2021-07-20T04:47:49.833709Z","shell.execute_reply.started":"2021-07-20T04:47:49.82278Z","shell.execute_reply":"2021-07-20T04:47:49.832629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(np.__version__)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T04:47:50.301289Z","iopub.execute_input":"2021-07-20T04:47:50.301605Z","iopub.status.idle":"2021-07-20T04:47:50.30591Z","shell.execute_reply.started":"2021-07-20T04:47:50.301578Z","shell.execute_reply":"2021-07-20T04:47:50.305072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport re\nimport numpy as np\nfrom pandas.core.frame import DataFrame\nfrom tensorflow.python.framework.indexed_slices import _LARGE_SPARSE_NUM_ELEMENTS\nfrom tensorflow.python.keras.backend import sigmoid\nimport tensorflow as tf\n\nfrom numpy.core.defchararray import title\nfrom pandas.core.arrays import categorical\nimport sklearn.preprocessing\nDEFAULT_FILE_PATH = \"/kaggle/input/filepython/glove.6B.50d.txt\"","metadata":{"execution":{"iopub.status.busy":"2021-07-20T04:47:53.150961Z","iopub.execute_input":"2021-07-20T04:47:53.151292Z","iopub.status.idle":"2021-07-20T04:47:57.999558Z","shell.execute_reply.started":"2021-07-20T04:47:53.151261Z","shell.execute_reply":"2021-07-20T04:47:57.998641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef df_process(df=None,train=False,model='rnn_glove'): # train arguement can be used somewhere\n    if df is None:\n        df = pd.read_csv(\"google-quest-challenge/train.csv\",index_col='qa_id')\n    if(model=='rnn_glove'):\n        #Extract id of user (maybe same person always get same kind of score) {for overlaying logic}\n        df['question_user_page']=df['question_user_page'].apply(lambda x: x.split(\"/\")[-1])\n        df['answer_user_page']=df['answer_user_page'].apply(lambda x: x.split(\"/\")[-1])\n        #print(df['category'].unique())\n        df['category']=df['category'].apply(lambda x:re.sub(\"[^a-zA-Z]\",\" \",x).lower())\n        #df['category']=df['category'].str.replace('life arts',)\n        df['answer_user_page']=df['answer_user_page'].apply(lambda x: x.split(\"/\")[-1])\n        #extract site name on which question posted\n        df['host']=df['host'].apply(lambda x: x.split(\".\")[0])\n        df.host=pd.Categorical(df.host)\n        df['host'] = df.host.cat.codes\n        df.category=pd.Categorical(df.category)\n        df['category'] = df.category.cat.codes\n        #print(df['host'].unique()) #remove host column, not needed\n        #dropping useless columns\n        df=df.drop(['question_user_name','answer_user_name','url'],axis=1)\n        #create new features: question title length, question length, answer length\n        df['q_len']=df['question_body'].str.len()\n        df['t_len']=df['question_title'].str.len()\n        df['ans_len']=df['answer'].str.len()\n       \n        return df\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-20T04:47:58.000964Z","iopub.execute_input":"2021-07-20T04:47:58.001332Z","iopub.status.idle":"2021-07-20T04:47:58.012023Z","shell.execute_reply.started":"2021-07-20T04:47:58.001296Z","shell.execute_reply":"2021-07-20T04:47:58.011061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def loadWordVectors(tokens, filepath=DEFAULT_FILE_PATH, dimensions=50): #tokens is the set of words in training data\n    \"\"\"Read pretrained GloVe vectors\"\"\"\n    wordVectors = np.random.randn(len(tokens), dimensions) #random normal distribution so words not in glove get random embeddings\n    with open(filepath) as ifs:\n        for line in ifs:\n            line = line.strip()\n            if not line:\n                continue\n            row = line.split()\n            token = row[0]\n            if token not in tokens:\n                continue\n            data = [float(x) for x in row[1:]]\n            if len(data) != dimensions:\n                raise RuntimeError(\"wrong number of dimensions\")\n            wordVectors[tokens[token]] = np.asarray(data)\n    return wordVectors","metadata":{"execution":{"iopub.status.busy":"2021-07-20T04:47:58.013662Z","iopub.execute_input":"2021-07-20T04:47:58.014132Z","iopub.status.idle":"2021-07-20T04:47:58.023213Z","shell.execute_reply.started":"2021-07-20T04:47:58.014097Z","shell.execute_reply":"2021-07-20T04:47:58.02246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class wordbank:\n    def __init__(self,df=None,train=True):\n        if df is None:\n            df = pd.read_csv(\"google-quest-challenge/train.csv\",index_col='qa_id')\n        self.df=df\n        self.train=train\n\n    def tokens(self):\n        if hasattr(self, \"_tokens\") and self._tokens: # if already attribute present \n            print(\"Token attribute present already\")\n            return self._tokens\n\n        tokens = dict()\n        tokenfreq = dict()\n        wordcount = 0\n        idx = 0\n        tokens[\"<pad>\"] = idx\n        tokenfreq[\"<pad>\"] = 0 \n        wordcount += 1\n        idx += 1\n        self._sentences=self.questions()+self.answers()+self.titles()\n        # create common token dict\n        for sentence in self._sentences:\n            for w in sentence:\n                wordcount += 1\n                if not w in tokens:\n                    tokens[w] = idx\n                    tokenfreq[w] = 1\n                    idx += 1\n                else:\n                    tokenfreq[w] += 1\n\n        \n\n        self._tokens = tokens\n        self._tokenfreq = tokenfreq\n        self._wordcount = wordcount\n        self._titles=[[tokens[word] for word in sent] for sent in self.titles()]\n        self._questions=[[tokens[word] for word in sent] for sent in self.questions()]\n        self._answers=[[tokens[word] for word in sent] for sent in self.answers()]\n        return self._tokens\n\n    '''Different functions if diffferent type of preprocessing required'''\n\n    def answers(self):\n        if hasattr(self, \"_answers\") and self._answers:\n            return self._answers\n\n        ans_df=self.df['answer']\n        self._use_answers = ans_df.tolist()\n        ans_df=ans_df.apply(lambda x: (re.sub(\"[^a-zA-Z]\",\" \",x)).strip().split())\n        ans_df=ans_df.apply(lambda x: [w.lower() for w in x])\n        self._answers = ans_df.tolist()\n        return self._answers\n    def titles(self):\n        if hasattr(self, \"_titles\") and self._titles:\n            return self._titles\n        \n        titles_df=self.df['question_title']\n        self._use_titles = titles_df.tolist()\n        titles_df=titles_df.apply(lambda x: (re.sub(\"[^a-zA-Z]\",\" \",x)).strip().split())\n        titles_df=titles_df.apply(lambda x: [w.lower() for w in x])\n        self._titles = titles_df.tolist()\n        return self._titles\n    def questions(self):\n        if hasattr(self, \"_questions\") and self._questions:\n            return self._questions\n\n        q_df=self.df['question_body']\n        self._use_questions = q_df.tolist()\n        q_df=q_df.apply(lambda x: (re.sub(\"[^a-zA-Z]\",\" \",x)).strip().split())\n        q_df=q_df.apply(lambda x: [w.lower() for w in x])\n        self._questions = q_df.tolist()\n        return self._questions\n\n    def numSentences(self):\n        if hasattr(self, \"_numSentences\") and self._numSentences:\n            return self._numSentences\n        else:\n            self._numSentences = len(self._answers())\n            return self._numSentences\n    def numQ(self): # numbe of unique questions or titles\n        if hasattr(self, \"_numQ\") and self._numQ:\n            return self._numQ\n        else:\n            self._numQ = self.df['question_title'].nunique()\n            return self._numQ\n    def test(self): # for test time\n        ind_unk=self._tokens['unknown']\n        self._titles=[[self._tokens.get(word,ind_unk) for word in sent] for sent in self.titles()]\n        self._questions=[[self._tokens.get(word,ind_unk) for word in sent] for sent in self.questions()]\n        self._answers=[[self._tokens.get(word,ind_unk) for word in sent] for sent in self.answers()]\n    \n        ","metadata":{"execution":{"iopub.status.busy":"2021-07-20T04:47:58.024661Z","iopub.execute_input":"2021-07-20T04:47:58.025013Z","iopub.status.idle":"2021-07-20T04:47:58.047505Z","shell.execute_reply.started":"2021-07-20T04:47:58.024978Z","shell.execute_reply":"2021-07-20T04:47:58.046671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Source: https://stackoverflow.com/questions/53404301/how-to-compute-spearman-correlation-in-tensorflow\nfrom scipy.stats import spearmanr\ndef get_spearman_rankcor(y_true, y_pred,flag=False):\n    if not flag:\n        \n        a=np.zeros(30)\n        for i in range(30):\n            a[i]=np.array((tf.py_function(spearmanr, [tf.cast(y_pred[:,i], tf.float32), \n                       tf.cast(y_true[:,i], tf.float32)], Tout = tf.float32)))\n        return a.mean()\n        \n            \n    return ( tf.py_function(spearmanr, [tf.cast(y_pred, tf.float32), \n                       tf.cast(y_true, tf.float32)], Tout = tf.float32) )","metadata":{"execution":{"iopub.status.busy":"2021-07-20T04:47:58.048675Z","iopub.execute_input":"2021-07-20T04:47:58.049032Z","iopub.status.idle":"2021-07-20T04:47:58.058745Z","shell.execute_reply.started":"2021-07-20T04:47:58.048997Z","shell.execute_reply":"2021-07-20T04:47:58.05799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''First Model'''\nclass NNGoogleQuest:\n    def __init__(self,data,vocab,embedding_matrix=None,max_length=[0,0,0],method='lstm'):\n        self._data=data\n        self._vocab=vocab\n        self._embedding_matrix=embedding_matrix\n        self._embedding_dim=self._embedding_matrix.shape[1]\n        self._num_vocab=len(self._vocab)\n        self._max_length=max_length\n        self._method=method\n    def embeddings(self,input,ind):\n        embedding_layer = tf.keras.layers.Embedding(\n            self._num_vocab,\n            self._embedding_dim,\n            embeddings_initializer=tf.keras.initializers.Constant(self._embedding_matrix),\n            trainable=True,mask_zero=True,input_length=self._max_length[ind])(input)\n        \n        #print(embedding_layer.shape)\n        if(self._method=='lstm'):\n            lstm_layer=tf.keras.layers.SimpleRNN(64)(embedding_layer)\n            #self._embedding_layer=embedding_layer\n            return lstm_layer\n        else:\n            cov_layer=tf.keras.layers.Conv1D(64, 3, activation='relu')(embedding_layer)\n            #print(cov_layer.shape)\n            \n            flat=tf.keras.layers.GlobalMaxPooling1D()(cov_layer)\n            #flat=tf.keras.layers.Flatten()(cov_layer)\n            return flat\n    def NN_model(self):\n        \n        title_input = tf.keras.Input(shape=(None,), name=\"title\")  # Variable-length sequence of ints\n        question_input = tf.keras.Input(shape=(None,), name=\"question\")  # Variable-length sequence of ints\n        answer_input=tf.keras.Input(shape=(None,), name=\"answer\") # Variable-length sequence of ints\n        category_input = tf.keras.Input(shape=(5,), name=\"category\")  # category binary input vector (5 categories)\n        #host_input=tf.keras.Input(shape=(59,), name=\"host\") #host category(59 total)\n        stats_input=tf.keras.Input(shape=(3,),name='stats') #new created features\n        if (self._method=='lstm'):\n            title_state_h = self.embeddings(title_input,0)\n\n            question_state_h = self.embeddings(question_input,1)\n            answer_state_h = self.embeddings(answer_input,2)\n        else:\n            title_state_h = self.embeddings(title_input,0)\n\n            question_state_h= self.embeddings(question_input,1)\n            answer_state_h= self.embeddings(answer_input,2)\n        \n        features= tf.keras.layers.concatenate([title_state_h, question_state_h, answer_state_h,category_input,stats_input])\n        hidden=tf.keras.layers.Dense(128,activation='relu',name='hidden')(features)\n        drop1=tf.keras.layers.Dropout(0.4)(hidden)\n        pred=tf.keras.layers.Dense(30,activation='sigmoid',name='prediction')(drop1)\n        model = tf.keras.Model(inputs=[title_input, question_input, answer_input,category_input,stats_input],outputs=pred)\n        self._model=model\n        if(self._method=='lstm'):\n            tf.keras.utils.plot_model(self._model, \"rnn_simple__model.png\", show_shapes=True)\n        else:\n            tf.keras.utils.plot_model(self._model, \"cnn_model.png\", show_shapes=True)\n        \n    \n    '''def answerEmbeddings(self):  \n    def questionEmbeddings(self):\n    def titleEmbeddings(self):\n    def categoryEmbeddings(self):'''\n    \n    def train(self):\n        self.NN_model()\n        self._model.compile(optimizer=tf.keras.optimizers.Adam(\n        learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,\n        name='Adam'\n        ),\n        loss='mean_absolute_error')\n        self._model.fit(\n        {\"title\": self._data['title'], \"question\": self._data['question'], \"answer\": self._data['answer'],\"category\":self._data['category'],\"stats\":self._data['stats']},\n        {\"prediction\": self._data['output']},\n        epochs=20,\n        batch_size=128,\n            validation_split=0.1\n        )\n        if(self._method=='lstm'):\n            self._model.save('rnn_glove_model')\n        else:\n            self._model.save('cnn_glove_model')\n        #self._model=tf.keras.models.load_model('rnn_glove_model')\n        #df = pd.DataFrame(np.array(self._model.predict({\"title\": self._data['title'], \"question\": self._data['question'], \"answer\": self._data['answer'],\"category\":self._data['category'],\"host\":self._data['host'],\"stats\":self._data['stats']},batch_size=32)))\n        #df.to_csv('submission.csv') ","metadata":{"execution":{"iopub.status.busy":"2021-07-20T05:06:00.867998Z","iopub.execute_input":"2021-07-20T05:06:00.868417Z","iopub.status.idle":"2021-07-20T05:06:00.887468Z","shell.execute_reply.started":"2021-07-20T05:06:00.868384Z","shell.execute_reply":"2021-07-20T05:06:00.886521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Train using LSTM and CNN models**","metadata":{}},{"cell_type":"code","source":"def multi_hot_enc(array,label=None):\n    label_binarizer = sklearn.preprocessing.LabelBinarizer()\n    label_binarizer.fit(range(max(train[label].to_numpy())+1)) # for test\n    array=label_binarizer.transform(array)\n    return array\ndef normalize(df):\n    x = df.values #returns a numpy array\n    min_max_scaler = sklearn.preprocessing.MinMaxScaler()\n    x_scaled = min_max_scaler.fit_transform(x)\n    return x_scaled\n\ntrain=pd.read_csv(\"/kaggle/input/google-quest-challenge/train.csv\",index_col='qa_id') #6079 rows\ntrain=df_process(train,True)\nGq=wordbank(train)\ntokens=Gq.tokens()\n\nword_vectors=loadWordVectors(tokens)\ntitles_vec=Gq._titles\n#title_vec = tf.keras.preprocessing.sequence.pad_sequences(titles_vec, padding=\"post\")\nquestions_vec=Gq._questions\nanswers_vec=Gq._answers\n'''if not any(Gq._titles+Gq._answers+Gq._questions):\n    print(\"Empty list!\")\n#word_vectors=tf.convert_to_tensor(word_vectors)'''\ndata={} # train data as dictionary\ndata['question']=tf.convert_to_tensor(tf.keras.preprocessing.sequence.pad_sequences(\n    questions_vec, padding=\"post\"))\ndata['answer']=tf.convert_to_tensor(tf.keras.preprocessing.sequence.pad_sequences(\n    answers_vec, padding=\"post\"))\ndata['title']=tf.convert_to_tensor(tf.keras.preprocessing.sequence.pad_sequences(\n    titles_vec, padding=\"post\"))\n#data['host']=tf.convert_to_tensor(multi_hot_enc(train['host'].to_numpy(),'host'))\ndata['category']=tf.convert_to_tensor(multi_hot_enc(train['category'].to_numpy(),'category'))\ndata['stats']=tf.convert_to_tensor(normalize(train[['q_len','t_len','ans_len']]))\ndata['output']=tf.convert_to_tensor(train.iloc[:,7:37].values)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-20T05:06:01.702328Z","iopub.execute_input":"2021-07-20T05:06:01.702656Z","iopub.status.idle":"2021-07-20T05:06:06.886617Z","shell.execute_reply.started":"2021-07-20T05:06:01.702626Z","shell.execute_reply":"2021-07-20T05:06:06.885682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Training using GRU model\\n')\nmodel=NNGoogleQuest(data,tokens,word_vectors,method='lstm',max_length=[data['title'].shape[1],data['question'].shape[1],data['answer'].shape[1]])\nmodel.train()\n","metadata":{"execution":{"iopub.status.busy":"2021-07-20T04:56:00.975583Z","iopub.execute_input":"2021-07-20T04:56:00.975835Z","iopub.status.idle":"2021-07-20T05:02:35.566121Z","shell.execute_reply.started":"2021-07-20T04:56:00.97581Z","shell.execute_reply":"2021-07-20T05:02:35.565246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Training using simple RNN model\\n')\nmodel=NNGoogleQuest(data,tokens,word_vectors,method='lstm',max_length=[data['title'].shape[1],data['question'].shape[1],data['answer'].shape[1]])\nmodel.train()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T05:06:17.216036Z","iopub.execute_input":"2021-07-20T05:06:17.216394Z","iopub.status.idle":"2021-07-20T06:21:13.192228Z","shell.execute_reply.started":"2021-07-20T05:06:17.216362Z","shell.execute_reply":"2021-07-20T06:21:13.191062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Loading Models**","metadata":{}},{"cell_type":"code","source":"'''rnn_model=tf.keras.models.load_model('../input/notebookd751344e59/rnn_glove_model',custom_objects={'get_spearman_rankcor':get_spearman_rankcor})'''\n","metadata":{"execution":{"iopub.status.busy":"2021-07-19T18:45:27.314535Z","iopub.execute_input":"2021-07-19T18:45:27.314866Z","iopub.status.idle":"2021-07-19T18:45:27.32048Z","shell.execute_reply.started":"2021-07-19T18:45:27.314837Z","shell.execute_reply":"2021-07-19T18:45:27.319684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''cnn_model=tf.keras.models.load_model('../input/notebookd751344e59/cnn_glove_model',custom_objects={'get_spearman_rankcor':get_spearman_rankcor})'''","metadata":{"execution":{"iopub.status.busy":"2021-07-19T18:45:27.523737Z","iopub.execute_input":"2021-07-19T18:45:27.524048Z","iopub.status.idle":"2021-07-19T18:45:27.530602Z","shell.execute_reply.started":"2021-07-19T18:45:27.524019Z","shell.execute_reply":"2021-07-19T18:45:27.529475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Test dataset**","metadata":{}},{"cell_type":"code","source":"test=pd.read_csv(\"/kaggle/input/google-quest-challenge/test.csv\",index_col='qa_id')\ntest2=test.copy()\ntest2=df_process(test2,False)\nGqTest=wordbank(test2,train=False)\nGqTest._tokens=tokens # train tokens\n\nGqTest.test()\ntitles_vecT=GqTest._titles\n#title_vec = tf.keras.preprocessing.sequence.pad_sequences(titles_vec, padding=\"post\")\nquestions_vecT=GqTest._questions\nanswers_vecT=GqTest._answers\ndataT={}\ndataT['question']=tf.convert_to_tensor(tf.keras.preprocessing.sequence.pad_sequences(\n    questions_vecT, padding=\"post\"))\ndataT['answer']=tf.convert_to_tensor(tf.keras.preprocessing.sequence.pad_sequences(\n    answers_vecT, padding=\"post\"))\ndataT['title']=tf.convert_to_tensor(tf.keras.preprocessing.sequence.pad_sequences(\n    titles_vecT, padding=\"post\"))\n#dataT['host']=tf.convert_to_tensor(multi_hot_enc(test2['host'].to_numpy(),'host'))\ndataT['category']=tf.convert_to_tensor(multi_hot_enc(test2['category'].to_numpy(),'category'))\ndataT['stats']=tf.convert_to_tensor(normalize(test2[['q_len','t_len','ans_len']]))\n'''df = pd.DataFrame(np.array(cnn_model.predict(\n    {\"title\": dataT['title'], \"question\": dataT['question'], \"answer\": dataT['answer'],\"category\":dataT['category'],\"stats\":dataT['stats']})))\ndf.index=test.index\ntest[list(train.iloc[:,7:37].columns)]=df.iloc[:,0:30]\ntest.to_csv('submission.csv') '''","metadata":{"execution":{"iopub.status.busy":"2021-07-19T18:45:48.16102Z","iopub.execute_input":"2021-07-19T18:45:48.16141Z","iopub.status.idle":"2021-07-19T18:45:48.385851Z","shell.execute_reply.started":"2021-07-19T18:45:48.16138Z","shell.execute_reply":"2021-07-19T18:45:48.385061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''test=test.drop(['question_title', 'question_body','question_user_name','question_user_page','answer','answer_user_name','answer_user_page','url','category','host'], axis = 1)\ntest.to_csv('submission.csv') '''","metadata":{"execution":{"iopub.status.busy":"2021-07-19T18:45:53.374479Z","iopub.execute_input":"2021-07-19T18:45:53.374799Z","iopub.status.idle":"2021-07-19T18:45:53.379733Z","shell.execute_reply.started":"2021-07-19T18:45:53.374773Z","shell.execute_reply":"2021-07-19T18:45:53.378927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Summary of RNN and LSTM models**","metadata":{}},{"cell_type":"code","source":"'''rnn_model.summary()'''","metadata":{"execution":{"iopub.status.busy":"2021-07-19T18:46:03.951248Z","iopub.execute_input":"2021-07-19T18:46:03.951566Z","iopub.status.idle":"2021-07-19T18:46:03.957017Z","shell.execute_reply.started":"2021-07-19T18:46:03.951539Z","shell.execute_reply":"2021-07-19T18:46:03.956034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''cnn_model.summary()'''","metadata":{"execution":{"iopub.status.busy":"2021-07-19T18:46:04.114578Z","iopub.execute_input":"2021-07-19T18:46:04.114857Z","iopub.status.idle":"2021-07-19T18:46:04.120263Z","shell.execute_reply.started":"2021-07-19T18:46:04.114831Z","shell.execute_reply":"2021-07-19T18:46:04.11919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#rnn_model.get_weights()","metadata":{"execution":{"iopub.status.busy":"2021-07-19T15:26:14.995246Z","iopub.execute_input":"2021-07-19T15:26:14.995598Z","iopub.status.idle":"2021-07-19T15:26:15.00138Z","shell.execute_reply.started":"2021-07-19T15:26:14.995566Z","shell.execute_reply":"2021-07-19T15:26:15.000569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#rnn_model.get_layer('lstm_18').get_weights()\n#cnn_model.input","metadata":{"execution":{"iopub.status.busy":"2021-07-19T15:26:15.003278Z","iopub.execute_input":"2021-07-19T15:26:15.003556Z","iopub.status.idle":"2021-07-19T15:26:15.009538Z","shell.execute_reply.started":"2021-07-19T15:26:15.003532Z","shell.execute_reply":"2021-07-19T15:26:15.008781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Extracting features from trained RNN-LSTM model and training using other models**","metadata":{}},{"cell_type":"code","source":"'''#extracting middle outputs from saved model\nrnn_embed=tf.keras.Model(inputs=rnn_model.input,outputs=rnn_model.get_layer('concatenate_6').output)\ntest_features=rnn_embed.predict({\"title\": dataT['title'], \"question\": dataT['question'], \"answer\": dataT['answer'],\"category\":dataT['category'],\"stats\":dataT['stats']})\ntrain_features=rnn_embed.predict({\"title\": data['title'], \"question\": data['question'], \"answer\": data['answer'],\"category\":data['category'],\"stats\":data['stats']})\nprint('Shape of middle features:',train_features.shape)'''","metadata":{"execution":{"iopub.status.busy":"2021-07-19T15:58:09.471559Z","iopub.status.idle":"2021-07-19T15:58:09.472115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\ntrain_y=data['output']\ntrain_X, val_X, train_y, val_y = train_test_split(np.array(train_features), np.array(train_y),\n                      test_size = 0.1, random_state = 123)\narr=np.zeros(30)\nfor i in range(30):\n    \n    regressor = DecisionTreeRegressor(random_state=0)\n    regressor.fit(train_X,train_y[:,i])\n    pred=regressor.predict(val_X)\n    print('Spearman score for predicting y',i,'=',get_spearman_rankcor(val_y[:,i],pred,True))\n    arr[i]=np.array(get_spearman_rankcor(val_y[:,i],pred,True))\nprint('Mean of Spearman score: ',arr.mean() )'''","metadata":{"execution":{"iopub.status.busy":"2021-07-19T18:46:15.702069Z","iopub.execute_input":"2021-07-19T18:46:15.702489Z","iopub.status.idle":"2021-07-19T18:46:15.707859Z","shell.execute_reply.started":"2021-07-19T18:46:15.70246Z","shell.execute_reply":"2021-07-19T18:46:15.707036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''import xgboost as xg\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error as MSE'''\n","metadata":{"execution":{"iopub.status.busy":"2021-07-19T18:46:15.891583Z","iopub.execute_input":"2021-07-19T18:46:15.891865Z","iopub.status.idle":"2021-07-19T18:46:15.89693Z","shell.execute_reply.started":"2021-07-19T18:46:15.89184Z","shell.execute_reply":"2021-07-19T18:46:15.89599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Extracting features from trained CNN model and training using other models**","metadata":{}},{"cell_type":"code","source":"'''#extracting middle outputs from saved model\ncnn_embed=tf.keras.Model(inputs=cnn_model.input,outputs=cnn_model.get_layer('concatenate_5').output)\ntest_features=cnn_embed.predict({\"title\": dataT['title'], \"question\": dataT['question'], \"answer\": dataT['answer'],\"category\":dataT['category'],\"stats\":dataT['stats']})\ntrain_features=cnn_embed.predict({\"title\": data['title'], \"question\": data['question'], \"answer\": data['answer'],\"category\":data['category'],\"stats\":data['stats']})\nprint('Shape of middle features:',train_features.shape)'''","metadata":{"execution":{"iopub.status.busy":"2021-07-19T18:46:20.288646Z","iopub.execute_input":"2021-07-19T18:46:20.288957Z","iopub.status.idle":"2021-07-19T18:46:20.294512Z","shell.execute_reply.started":"2021-07-19T18:46:20.288928Z","shell.execute_reply":"2021-07-19T18:46:20.293665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''train_y=data['output']\ntrain_X, val_X, train_y, val_y = train_test_split(np.array(train_features), np.array(train_y),\n                      test_size = 0.1, random_state = 123)\narr=np.zeros(30)\nfor i in range(30):\n    \n    regressor = DecisionTreeRegressor(random_state=0)\n    regressor.fit(train_X,train_y[:,i])\n    pred=regressor.predict(val_X)\n    print('Spearman score for predicting y',i,'=',get_spearman_rankcor(val_y[:,i],pred,True))\n    arr[i]=np.array(get_spearman_rankcor(val_y[:,i],pred,True))\nprint('Mean of Spearman score: ',arr.mean() )'''","metadata":{"execution":{"iopub.status.busy":"2021-07-19T18:46:24.847796Z","iopub.execute_input":"2021-07-19T18:46:24.848105Z","iopub.status.idle":"2021-07-19T18:46:24.85678Z","shell.execute_reply.started":"2021-07-19T18:46:24.848076Z","shell.execute_reply":"2021-07-19T18:46:24.855638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ref: https://www.tensorflow.org/hub/tutorials/semantic_similarity_with_tf_hub_universal_encoder\nimport tensorflow_hub as tfhub\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2021-07-19T18:46:39.013914Z","iopub.execute_input":"2021-07-19T18:46:39.014296Z","iopub.status.idle":"2021-07-19T18:46:39.51572Z","shell.execute_reply.started":"2021-07-19T18:46:39.014265Z","shell.execute_reply":"2021-07-19T18:46:39.514892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Universal Sentence Encoder**","metadata":{}},{"cell_type":"code","source":"\nmodule_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\nuse_model = tfhub.load(module_url)\ndef use_embed(input):\n    return use_model(input)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T18:46:41.206041Z","iopub.execute_input":"2021-07-19T18:46:41.206394Z","iopub.status.idle":"2021-07-19T18:46:59.34863Z","shell.execute_reply.started":"2021-07-19T18:46:41.206365Z","shell.execute_reply":"2021-07-19T18:46:59.347729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#preprocessing handled by USE automatically, mentioned in the docs\nuse_titles=use_embed(Gq._use_titles) # 512 dimesnional vector\nuse_questions=use_embed(Gq._use_questions)\nuse_answers=use_embed(Gq._use_answers)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T18:46:59.350259Z","iopub.execute_input":"2021-07-19T18:46:59.35058Z","iopub.status.idle":"2021-07-19T18:47:22.855034Z","shell.execute_reply.started":"2021-07-19T18:46:59.350546Z","shell.execute_reply":"2021-07-19T18:47:22.85416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"use_titles=tf.expand_dims(use_titles, axis=1)\nuse_questions=tf.expand_dims(use_questions, axis=1)\nuse_answers=tf.expand_dims(use_answers, axis=1)\nprint(use_questions.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T19:12:11.112609Z","iopub.execute_input":"2021-07-19T19:12:11.112937Z","iopub.status.idle":"2021-07-19T19:12:11.119673Z","shell.execute_reply.started":"2021-07-19T19:12:11.112909Z","shell.execute_reply":"2021-07-19T19:12:11.118801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"title_input = tf.keras.Input(shape=(1,512), name=\"title\")  # Variable-length sequence of ints\nquestion_input = tf.keras.Input(shape=(1,512), name=\"question\")  # Variable-length sequence of ints\nanswer_input=tf.keras.Input(shape=(1,512), name=\"answer\") # Variable-length sequence of ints\ncategory_input = tf.keras.Input(shape=(5,), name=\"category\")  # category binary input vector (5 categories)\n#host_input=tf.keras.Input(shape=(59,), name=\"host\") #host category(59 total)\nstats_input=tf.keras.Input(shape=(3,),name='stats') #new created features\nconcat=tf.keras.layers.concatenate([title_input, question_input,answer_input],axis=-2)\ncov_layer=tf.keras.layers.Conv1D(48, 3, activation='relu')(concat)\nflat=tf.keras.layers.Flatten()(cov_layer)\nconcat2=tf.keras.layers.concatenate([flat,category_input,stats_input],axis=-1)\ndrop=tf.keras.layers.Dropout(0.4)(concat2)\nhidden=tf.keras.layers.Dense(512,activation='relu',name='hidden')(drop)\ndrop2=tf.keras.layers.Dropout(0.4)(hidden)\npred=tf.keras.layers.Dense(30,activation='sigmoid',name='prediction')(drop2)\n'''model=tf.keras.Sequential()\nmodel.add(tf.keras.layers.Dense(128, input_shape=(512,),activation='relu'))\nmodel.add(tf.keras.layers.Dropout(0.4))\nmodel.add(tf.keras.layers.Dense(30,activation='sigmoid'))'''\n\nmodel = tf.keras.Model(inputs=[title_input, question_input, answer_input,category_input,stats_input],outputs=pred)\ntf.keras.utils.plot_model(model, \"use_model_logistic.png\", show_shapes=True)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-07-19T20:39:41.458256Z","iopub.execute_input":"2021-07-19T20:39:41.458583Z","iopub.status.idle":"2021-07-19T20:39:41.827072Z","shell.execute_reply.started":"2021-07-19T20:39:41.458557Z","shell.execute_reply":"2021-07-19T20:39:41.826135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nmodel.compile(optimizer=tf.keras.optimizers.Adam(\n        learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,\n        name='Adam'\n        ),\n        loss='mean_absolute_error')\nmodel.fit(\n        {\"title\": use_titles, \"question\": use_questions, \"answer\": use_answers,\"category\":data['category'],\"stats\":data['stats']},\n        {\"prediction\": data['output']},\n        epochs=100,\n        batch_size=48,\n            validation_split=0.1\n        )","metadata":{"execution":{"iopub.status.busy":"2021-07-19T20:39:41.830489Z","iopub.execute_input":"2021-07-19T20:39:41.830762Z","iopub.status.idle":"2021-07-19T20:40:20.410433Z","shell.execute_reply.started":"2021-07-19T20:39:41.830732Z","shell.execute_reply":"2021-07-19T20:40:20.409651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"pred=model.predict({\"title\": use_titles, \"question\": use_questions, \"answer\": use_answers,\"category\":data['category'],\"stats\":data['stats']})","metadata":{"execution":{"iopub.status.busy":"2021-07-19T20:31:04.308677Z","iopub.execute_input":"2021-07-19T20:31:04.309047Z","iopub.status.idle":"2021-07-19T20:31:04.693491Z","shell.execute_reply.started":"2021-07-19T20:31:04.309017Z","shell.execute_reply":"2021-07-19T20:31:04.692666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_spearman_rankcor(data['output'], pred)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T20:39:19.34397Z","iopub.execute_input":"2021-07-19T20:39:19.344334Z","iopub.status.idle":"2021-07-19T20:39:19.428957Z","shell.execute_reply.started":"2021-07-19T20:39:19.344303Z","shell.execute_reply":"2021-07-19T20:39:19.428202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**BERT**","metadata":{}},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2021-07-19T20:55:01.030151Z","iopub.execute_input":"2021-07-19T20:55:01.030471Z","iopub.status.idle":"2021-07-19T20:55:01.039957Z","shell.execute_reply.started":"2021-07-19T20:55:01.030442Z","shell.execute_reply":"2021-07-19T20:55:01.038909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}