{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers","metadata":{"execution":{"iopub.status.busy":"2022-02-28T09:46:10.962718Z","iopub.execute_input":"2022-02-28T09:46:10.962944Z","iopub.status.idle":"2022-02-28T09:46:37.220094Z","shell.execute_reply.started":"2022-02-28T09:46:10.9629Z","shell.execute_reply":"2022-02-28T09:46:37.21934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import transformers","metadata":{"execution":{"iopub.status.busy":"2022-02-28T09:46:37.22292Z","iopub.execute_input":"2022-02-28T09:46:37.223599Z","iopub.status.idle":"2022-02-28T09:46:38.763937Z","shell.execute_reply.started":"2022-02-28T09:46:37.223538Z","shell.execute_reply":"2022-02-28T09:46:38.763143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nimport glob\nimport torch\n\n\nimport os\nimport re\nimport gc\nimport pickle  \nimport random\nimport string\n\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\n\n# import transformers\nfrom transformers import DistilBertTokenizer,DistilBertModel\nimport math\n\n\nfrom scipy.stats import spearmanr, rankdata\nfrom os.path import join as path_join\nfrom numpy.random import seed\nfrom urllib.parse import urlparse\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import KFold\n\nseed(42)\nrandom.seed(42)\n\nimport nltk\nfrom nltk.corpus import stopwords\n\nfrom sklearn.base import clone\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.preprocessing import StandardScaler, PowerTransformer, OneHotEncoder, RobustScaler, KBinsDiscretizer, QuantileTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import StratifiedKFold, GridSearchCV, KFold, GroupKFold\nfrom sklearn.multioutput import MultiOutputRegressor\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import make_scorer\n\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, HuberRegressor, RANSACRegressor\nfrom sklearn.svm import LinearSVR, SVR\nfrom sklearn.ensemble import ExtraTreesRegressor\n\neng_stopwords = set(stopwords.words(\"english\"))\n\nimport tensorflow as tf\nimport tensorflow_hub as hub","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2022-02-28T09:46:38.76527Z","iopub.execute_input":"2022-02-28T09:46:38.76557Z","iopub.status.idle":"2022-02-28T09:46:41.994797Z","shell.execute_reply.started":"2022-02-28T09:46:38.765517Z","shell.execute_reply":"2022-02-28T09:46:41.993973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# settings\ndata_dir = '../input/google-quest-challenge/'\nmetas_dir = ''\nsub_dir = ''\n\nRANDOM_STATE = 42\n\nimport datetime\ntodate = datetime.date.today().strftime(\"%m%d\")\n","metadata":{"execution":{"iopub.status.busy":"2022-02-28T09:46:41.99732Z","iopub.execute_input":"2022-02-28T09:46:41.997796Z","iopub.status.idle":"2022-02-28T09:46:42.009931Z","shell.execute_reply.started":"2022-02-28T09:46:41.997744Z","shell.execute_reply":"2022-02-28T09:46:42.008821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Functions","metadata":{}},{"cell_type":"code","source":"# count words\ndef word_count(xstring):\n    return xstring.split().str.len()\n\n\ndef spearman_corr(y_true, y_pred):\n        if np.ndim(y_pred) == 2:\n            corr = np.mean([stats.spearmanr(y_true[:, i], y_pred[:, i])[0] for i in range(y_true.shape[1])])\n        else:\n            corr = stats.spearmanr(y_true, y_pred)[0]\n        return corr\n    \ncustom_scorer = make_scorer(spearman_corr, greater_is_better=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-28T09:46:42.014736Z","iopub.execute_input":"2022-02-28T09:46:42.015014Z","iopub.status.idle":"2022-02-28T09:46:42.023129Z","shell.execute_reply.started":"2022-02-28T09:46:42.014964Z","shell.execute_reply":"2022-02-28T09:46:42.022314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def chunks(l, n):\n\n    for i in range(0, len(l), n):\n        yield l[i:i + n]","metadata":{"execution":{"iopub.status.busy":"2022-02-28T09:46:42.026101Z","iopub.execute_input":"2022-02-28T09:46:42.026562Z","iopub.status.idle":"2022-02-28T09:46:42.033688Z","shell.execute_reply.started":"2022-02-28T09:46:42.026511Z","shell.execute_reply":"2022-02-28T09:46:42.032969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fetch_vectors(string_list, batch_size=64):\n    # inspired by https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/\n    DEVICE = torch.device(\"cuda\")\n    tokenizer = transformers.DistilBertTokenizer.from_pretrained(\"../input/distilbertbaseuncased/\")\n    model = transformers.DistilBertModel.from_pretrained(\"../input/distilbertbaseuncased/\")\n    model.to(DEVICE)\n\n    fin_features = []\n    for data in chunks(string_list, batch_size):\n        tokenized = []\n        for x in data:\n            x = \" \".join(x.strip().split()[:300])\n            tok = tokenizer.encode(x, add_special_tokens=True)\n            tokenized.append(tok[:512])\n\n        max_len = 512\n        padded = np.array([i + [0] * (max_len - len(i)) for i in tokenized])\n        attention_mask = np.where(padded != 0, 1, 0)\n        input_ids = torch.tensor(padded).to(DEVICE)\n        attention_mask = torch.tensor(attention_mask).to(DEVICE)\n\n        with torch.no_grad():\n            last_hidden_states = model(input_ids, attention_mask=attention_mask)\n\n        features = last_hidden_states[0][:, 0, :].cpu().numpy()\n        fin_features.append(features)\n\n    fin_features = np.vstack(fin_features)\n    return fin_features","metadata":{"execution":{"iopub.status.busy":"2022-02-28T09:46:42.036576Z","iopub.execute_input":"2022-02-28T09:46:42.03723Z","iopub.status.idle":"2022-02-28T09:46:42.048508Z","shell.execute_reply.started":"2022-02-28T09:46:42.036828Z","shell.execute_reply":"2022-02-28T09:46:42.047662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data","metadata":{}},{"cell_type":"code","source":"# load the data\n\nxtrain = pd.read_csv(data_dir + 'train.csv')\nxtest = pd.read_csv(data_dir + 'test.csv')\n","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.status.busy":"2022-02-28T09:46:42.050318Z","iopub.execute_input":"2022-02-28T09:46:42.050906Z","iopub.status.idle":"2022-02-28T09:46:42.392758Z","shell.execute_reply.started":"2022-02-28T09:46:42.050597Z","shell.execute_reply":"2022-02-28T09:46:42.392005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xtrain.head(4)","metadata":{"execution":{"iopub.status.busy":"2022-02-28T09:46:42.394393Z","iopub.execute_input":"2022-02-28T09:46:42.394737Z","iopub.status.idle":"2022-02-28T09:46:42.438144Z","shell.execute_reply.started":"2022-02-28T09:46:42.394689Z","shell.execute_reply":"2022-02-28T09:46:42.437273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_cols = ['question_asker_intent_understanding', 'question_body_critical', \n               'question_conversational', 'question_expect_short_answer', \n               'question_fact_seeking', 'question_has_commonly_accepted_answer', \n               'question_interestingness_others', 'question_interestingness_self', \n               'question_multi_intent', 'question_not_really_a_question', \n               'question_opinion_seeking', 'question_type_choice', \n               'question_type_compare', 'question_type_consequence', \n               'question_type_definition', 'question_type_entity', \n               'question_type_instructions', 'question_type_procedure', \n               'question_type_reason_explanation', 'question_type_spelling', \n               'question_well_written', 'answer_helpful', \n               'answer_level_of_information', 'answer_plausible', \n               'answer_relevance', 'answer_satisfaction', \n               'answer_type_instructions', 'answer_type_procedure', \n               'answer_type_reason_explanation', 'answer_well_written']","metadata":{"execution":{"iopub.status.busy":"2022-02-28T09:46:42.43941Z","iopub.execute_input":"2022-02-28T09:46:42.439842Z","iopub.status.idle":"2022-02-28T09:46:42.446681Z","shell.execute_reply.started":"2022-02-28T09:46:42.439653Z","shell.execute_reply":"2022-02-28T09:46:42.445996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA / FE","metadata":{}},{"cell_type":"markdown","source":"## Basic FE","metadata":{}},{"cell_type":"code","source":"# word count in title, body and answer\nfor colname in ['question_title', 'question_body', 'answer']:\n    newname = colname + '_word_len'\n    \n    xtrain[newname] = xtrain[colname].str.split().str.len()\n    xtest[newname] = xtest[colname].str.split().str.len()\n\n    \ndel newname, colname","metadata":{"execution":{"iopub.status.busy":"2022-02-28T09:46:42.448916Z","iopub.execute_input":"2022-02-28T09:46:42.449154Z","iopub.status.idle":"2022-02-28T09:46:42.78907Z","shell.execute_reply.started":"2022-02-28T09:46:42.449113Z","shell.execute_reply":"2022-02-28T09:46:42.788337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for colname in ['question', 'answer']:\n\n    # check for nonames, i.e. users with logins like user12389\n    xtrain['is_'+colname+'_no_name_user'] = xtrain[colname +'_user_name'].str.contains('^user\\d+$') + 0\n    xtest['is_'+colname+'_no_name_user'] = xtest[colname +'_user_name'].str.contains('^user\\d+$') + 0\n    \n\ncolname = 'answer'\n# check lexical diversity (unique words count vs total )\nxtrain[colname+'_div'] = xtrain[colname].apply(lambda s: len(set(s.split())) / len(s.split()) )\nxtest[colname+'_div'] = xtest[colname].apply(lambda s: len(set(s.split())) / len(s.split()) )\n","metadata":{"execution":{"iopub.status.busy":"2022-02-28T09:46:42.790687Z","iopub.execute_input":"2022-02-28T09:46:42.791126Z","iopub.status.idle":"2022-02-28T09:46:43.018235Z","shell.execute_reply.started":"2022-02-28T09:46:42.790947Z","shell.execute_reply":"2022-02-28T09:46:43.017566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## domain components\nfor df in [xtrain, xtest]:\n    \n    df['domcom'] = df['question_user_page'].apply(lambda s: s.split('://')[1].split('/')[0].split('.'))\n    # count components\n    df['dom_cnt'] = df['domcom'].apply(lambda s: len(s))\n    # pad the length in case some domains have fewer components in the name\n    df['domcom'] = df['domcom'].apply(lambda s: s + ['none', 'none'])\n\n    # components\n    for ii in range(0,4):\n        df['dom_'+str(ii)] = df['domcom'].apply(lambda s: s[ii])\n    \n# clean up\nxtrain.drop('domcom', axis = 1, inplace = True)\nxtest.drop('domcom', axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-02-28T09:46:43.019598Z","iopub.execute_input":"2022-02-28T09:46:43.019867Z","iopub.status.idle":"2022-02-28T09:46:43.078576Z","shell.execute_reply.started":"2022-02-28T09:46:43.019822Z","shell.execute_reply":"2022-02-28T09:46:43.077928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# shared elements\nfor df in [xtrain, xtest]:\n    df['q_words'] = df['question_body'].apply(lambda s: [f for f in s.split() if f not in eng_stopwords] )\n    df['a_words'] = df['answer'].apply(lambda s: [f for f in s.split() if f not in eng_stopwords] )\n    df['qa_word_overlap'] = df.apply(lambda s: len(np.intersect1d(s['q_words'], s['a_words'])), axis = 1)\n    df['qa_word_overlap_norm1'] = df.apply(lambda s: s['qa_word_overlap']/(1 + len(s['a_words'])), axis = 1)\n    df['qa_word_overlap_norm2'] = df.apply(lambda s: s['qa_word_overlap']/(1 + len(s['q_words'])), axis = 1)\n    df.drop(['q_words', 'a_words'], axis = 1, inplace = True)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-02-28T09:46:43.08122Z","iopub.execute_input":"2022-02-28T09:46:43.081733Z","iopub.status.idle":"2022-02-28T09:46:45.309873Z","shell.execute_reply.started":"2022-02-28T09:46:43.081682Z","shell.execute_reply":"2022-02-28T09:46:45.307485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for df in [xtrain, xtest]:\n    \n    ## Number of characters in the text ##\n    df[\"question_title_num_chars\"] = df[\"question_title\"].apply(lambda x: len(str(x)))\n    df[\"question_body_num_chars\"] = df[\"question_body\"].apply(lambda x: len(str(x)))\n    df[\"answer_num_chars\"] = df[\"answer\"].apply(lambda x: len(str(x)))\n\n    ## Number of stopwords in the text ##\n    df[\"question_title_num_stopwords\"] = df[\"question_title\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n    df[\"question_body_num_stopwords\"] = df[\"question_body\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n    df[\"answer_num_stopwords\"] = df[\"answer\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n\n    ## Number of punctuations in the text ##\n    df[\"question_title_num_punctuations\"] =df['question_title'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n    df[\"question_body_num_punctuations\"] =df['question_body'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n    df[\"answer_num_punctuations\"] =df['answer'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n\n    ## Number of title case words in the text ##\n    df[\"question_title_num_words_upper\"] = df[\"question_title\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n    df[\"question_body_num_words_upper\"] = df[\"question_body\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n    df[\"answer_num_words_upper\"] = df[\"answer\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n","metadata":{"execution":{"iopub.status.busy":"2022-02-28T09:46:45.311368Z","iopub.execute_input":"2022-02-28T09:46:45.311829Z","iopub.status.idle":"2022-02-28T09:46:47.051826Z","shell.execute_reply.started":"2022-02-28T09:46:45.311772Z","shell.execute_reply":"2022-02-28T09:46:47.051122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## FE - distance-based ","metadata":{}},{"cell_type":"code","source":"module_url = \"../input/universalsentenceencoderlarge4/\"\nembed = hub.load(module_url)","metadata":{"execution":{"iopub.status.busy":"2022-02-28T09:46:47.053126Z","iopub.execute_input":"2022-02-28T09:46:47.053427Z","iopub.status.idle":"2022-02-28T09:47:25.118136Z","shell.execute_reply.started":"2022-02-28T09:46:47.053382Z","shell.execute_reply":"2022-02-28T09:47:25.117249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embeddings_train = {}\nembeddings_test = {}\nfor text in ['question_title', 'question_body', 'answer']:\n    train_text = xtrain[text].str.replace('?', '.').str.replace('!', '.').tolist()\n    test_text = xtest[text].str.replace('?', '.').str.replace('!', '.').tolist()\n    \n    curr_train_emb = []\n    curr_test_emb = []\n    batch_size = 4\n    ind = 0\n    while ind*batch_size < len(train_text):\n        curr_train_emb.append(embed(train_text[ind*batch_size: (ind + 1)*batch_size])[\"outputs\"].numpy())\n        ind += 1\n        \n    ind = 0\n    while ind*batch_size < len(test_text):\n        curr_test_emb.append(embed(test_text[ind*batch_size: (ind + 1)*batch_size])[\"outputs\"].numpy())\n        ind += 1    \n        \n    embeddings_train[text + '_embedding'] = np.vstack(curr_train_emb)\n    embeddings_test[text + '_embedding'] = np.vstack(curr_test_emb)\n\n    print(text)\n    \ndel embed","metadata":{"execution":{"iopub.status.busy":"2022-02-28T09:47:25.121887Z","iopub.execute_input":"2022-02-28T09:47:25.122172Z","iopub.status.idle":"2022-02-28T09:50:02.047052Z","shell.execute_reply.started":"2022-02-28T09:47:25.122121Z","shell.execute_reply":"2022-02-28T09:50:02.046297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"l2_dist = lambda x, y: np.power(x - y, 2).sum(axis=1)\n\ncos_dist = lambda x, y: (x*y).sum(axis=1)\n\ndist_features_train = np.array([\n    l2_dist(embeddings_train['question_title_embedding'], embeddings_train['answer_embedding']),\n    l2_dist(embeddings_train['question_body_embedding'], embeddings_train['answer_embedding']),\n    l2_dist(embeddings_train['question_body_embedding'], embeddings_train['question_title_embedding']),\n    cos_dist(embeddings_train['question_title_embedding'], embeddings_train['answer_embedding']),\n    cos_dist(embeddings_train['question_body_embedding'], embeddings_train['answer_embedding']),\n    cos_dist(embeddings_train['question_body_embedding'], embeddings_train['question_title_embedding'])\n]).T\n\ndist_features_test = np.array([\n    l2_dist(embeddings_test['question_title_embedding'], embeddings_test['answer_embedding']),\n    l2_dist(embeddings_test['question_body_embedding'], embeddings_test['answer_embedding']),\n    l2_dist(embeddings_test['question_body_embedding'], embeddings_test['question_title_embedding']),\n    cos_dist(embeddings_test['question_title_embedding'], embeddings_test['answer_embedding']),\n    cos_dist(embeddings_test['question_body_embedding'], embeddings_test['answer_embedding']),\n    cos_dist(embeddings_test['question_body_embedding'], embeddings_test['question_title_embedding'])\n]).T\n\ndel embeddings_train, embeddings_test","metadata":{"execution":{"iopub.status.busy":"2022-02-28T09:50:02.049495Z","iopub.execute_input":"2022-02-28T09:50:02.04975Z","iopub.status.idle":"2022-02-28T09:50:02.224014Z","shell.execute_reply.started":"2022-02-28T09:50:02.049707Z","shell.execute_reply":"2022-02-28T09:50:02.223259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for ii in range(0,6):\n    xtrain['dist'+str(ii)] = dist_features_train[:,ii]\n    xtest['dist'+str(ii)] = dist_features_test[:,ii]\n    ","metadata":{"execution":{"iopub.status.busy":"2022-02-28T09:50:02.225511Z","iopub.execute_input":"2022-02-28T09:50:02.2258Z","iopub.status.idle":"2022-02-28T09:50:02.241737Z","shell.execute_reply.started":"2022-02-28T09:50:02.225753Z","shell.execute_reply":"2022-02-28T09:50:02.24092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"markdown","source":"## Pipeline buildup","metadata":{}},{"cell_type":"code","source":"limit_char = 5000\nlimit_word = 25000","metadata":{"execution":{"iopub.status.busy":"2022-02-28T09:50:02.243256Z","iopub.execute_input":"2022-02-28T09:50:02.243761Z","iopub.status.idle":"2022-02-28T09:50:02.248688Z","shell.execute_reply.started":"2022-02-28T09:50:02.24357Z","shell.execute_reply":"2022-02-28T09:50:02.247709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"title_col = 'question_title'\ntitle_transformer = Pipeline([\n    ('tfidf', TfidfVectorizer(lowercase = False, max_df = 0.3, min_df = 1,\n                             binary = False, use_idf = True, smooth_idf = False,\n                             ngram_range = (1,2), stop_words = 'english', \n                             token_pattern = '(?u)\\\\b\\\\w+\\\\b' , max_features = limit_word ))\n])\n\n        \ntitle_transformer2 = Pipeline([\n ('tfidf2',  TfidfVectorizer( sublinear_tf=True,\n    strip_accents='unicode', analyzer='char',\n    stop_words='english', ngram_range=(1, 4), max_features= limit_char))   \n])\n\n\nbody_col = 'question_body'\nbody_transformer = Pipeline([\n    ('tfidf',TfidfVectorizer(lowercase = False, max_df = 0.3, min_df = 1,\n                             binary = False, use_idf = True, smooth_idf = False,\n                             ngram_range = (1,2), stop_words = 'english', \n                             token_pattern = '(?u)\\\\b\\\\w+\\\\b' , max_features = limit_word ))\n])\n\n\nbody_transformer2 = Pipeline([\n ('tfidf2',  TfidfVectorizer( sublinear_tf=True,\n    strip_accents='unicode', analyzer='char',\n    stop_words='english', ngram_range=(1, 4), max_features= limit_char))   \n])\n\nanswer_col = 'answer'\n\nanswer_transformer = Pipeline([\n    ('tfidf', TfidfVectorizer(lowercase = False, max_df = 0.3, min_df = 1,\n                             binary = False, use_idf = True, smooth_idf = False,\n                             ngram_range = (1,2), stop_words = 'english', \n                             token_pattern = '(?u)\\\\b\\\\w+\\\\b' , max_features = limit_word ))\n])\n\nanswer_transformer2 = Pipeline([\n ('tfidf2',  TfidfVectorizer( sublinear_tf=True,\n    strip_accents='unicode', analyzer='char',\n    stop_words='english', ngram_range=(1, 4), max_features= limit_char))   \n])\n\nnum_cols = [\n    'question_title_word_len', 'question_body_word_len', 'answer_word_len', 'answer_div',\n    'question_title_num_chars','question_body_num_chars','answer_num_chars',\n    'question_title_num_stopwords','question_body_num_stopwords','answer_num_stopwords',\n    'question_title_num_punctuations','question_body_num_punctuations','answer_num_punctuations',\n    'question_title_num_words_upper','question_body_num_words_upper','answer_num_words_upper',\n    'dist0', 'dist1', 'dist2', 'dist3', 'dist4',       'dist5'\n]\n\nnum_transformer = Pipeline([\n    ('impute', SimpleImputer(strategy='constant', fill_value=0)),\n    ('scale', PowerTransformer(method='yeo-johnson'))\n])\n\n\ncat_cols = [\n    'dom_0', \n    'dom_1', \n    'dom_2', \n    'dom_3',     \n    'category', \n    'is_question_no_name_user',\n    'is_answer_no_name_user',\n    'dom_cnt'\n]\n\ncat_transformer = Pipeline([\n    ('impute', SimpleImputer(strategy='constant', fill_value='')),\n    ('encode', OneHotEncoder(handle_unknown='ignore'))\n])\n\n\npreprocessor = ColumnTransformer(\n    transformers = [\n        ('title', title_transformer, title_col),\n        ('title2', title_transformer2, title_col),\n        ('body', body_transformer, body_col),\n        ('body2', body_transformer2, body_col),\n        ('answer', answer_transformer, answer_col),\n        ('answer2', answer_transformer2, answer_col),\n        ('num', num_transformer, num_cols),\n        ('cat', cat_transformer, cat_cols)\n    ]\n)\n\npipeline = Pipeline([\n    ('preprocessor', preprocessor),\n    ('estimator',Ridge(random_state=RANDOM_STATE))\n])","metadata":{"execution":{"iopub.status.busy":"2022-02-28T09:50:02.25043Z","iopub.execute_input":"2022-02-28T09:50:02.250889Z","iopub.status.idle":"2022-02-28T09:50:02.274119Z","shell.execute_reply.started":"2022-02-28T09:50:02.250712Z","shell.execute_reply":"2022-02-28T09:50:02.273159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Find best parameters","metadata":{}},{"cell_type":"code","source":"# prep\nid_train = xtrain['qa_id']\nytrain = xtrain[target_cols]\nxtrain.drop(target_cols + ['qa_id'], axis = 1, inplace = True)\n\nid_test = xtest['qa_id'] \nxtest.drop('qa_id', axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-02-28T09:50:02.275872Z","iopub.execute_input":"2022-02-28T09:50:02.276525Z","iopub.status.idle":"2022-02-28T09:50:02.294947Z","shell.execute_reply.started":"2022-02-28T09:50:02.276169Z","shell.execute_reply":"2022-02-28T09:50:02.294392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dropcols = ['question_user_name', 'question_user_page',\n 'answer_user_name', 'answer_user_page','url','host']\n\nxtrain.drop(dropcols, axis = 1, inplace = True)\nxtest.drop(dropcols, axis = 1, inplace = True)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-28T09:50:02.2979Z","iopub.execute_input":"2022-02-28T09:50:02.298152Z","iopub.status.idle":"2022-02-28T09:50:02.30809Z","shell.execute_reply.started":"2022-02-28T09:50:02.298106Z","shell.execute_reply":"2022-02-28T09:50:02.307338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Folds","metadata":{}},{"cell_type":"code","source":"nfolds = 5\nmvalid = np.zeros((xtrain.shape[0], len(target_cols)))\nmfull = np.zeros((xtest.shape[0], len(target_cols)))\n\nkf = GroupKFold(n_splits= nfolds).split(X=xtrain.question_body, groups=xtrain.question_body)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-28T09:50:30.262882Z","iopub.execute_input":"2022-02-28T09:50:30.263192Z","iopub.status.idle":"2022-02-28T09:50:30.27023Z","shell.execute_reply.started":"2022-02-28T09:50:30.263142Z","shell.execute_reply":"2022-02-28T09:50:30.26947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" \nfor ind, (train_index, test_index) in enumerate(kf):\n    \n\n    # split\n    x0, x1 = xtrain.loc[train_index], xtrain.loc[test_index]\n    y0, y1 = ytrain.loc[train_index], ytrain.loc[test_index]\n\n    for ii in range(0, ytrain.shape[1]):\n\n        # fit model\n        be = clone(pipeline)\n#        be.steps[1][1].alpha = vector_as.loc[ii]\n        be.fit(x0, np.array(y0)[:,ii])\n\n        filename = 'ridge_f' + str(ind) + '_c' + str(ii) + '.pkl'\n        pickle.dump(be, open(filename, 'wb'))\n        \n        # park forecast\n        mvalid[test_index, ii] = be.predict(x1)\n        mfull[:,ii] += be.predict(xtest)/nfolds\n        \n    print('---')","metadata":{"execution":{"iopub.status.busy":"2022-02-28T09:50:47.634475Z","iopub.execute_input":"2022-02-28T09:50:47.634777Z","iopub.status.idle":"2022-02-28T09:53:15.18917Z","shell.execute_reply.started":"2022-02-28T09:50:47.634726Z","shell.execute_reply":"2022-02-28T09:53:15.18763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Performance","metadata":{}},{"cell_type":"code","source":"corvec = np.zeros((ytrain.shape[1],1))\nfor ii in range(0, ytrain.shape[1]):\n    mvalid[:,ii] = rankdata(mvalid[:,ii])/mvalid.shape[0]\n    mfull[:,ii] = rankdata(mfull[:,ii])/mfull.shape[0]\n    \n    corvec[ii] = stats.spearmanr(ytrain[ytrain.columns[ii]], mvalid[:,ii])[0]\n    \nprint(corvec.mean())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"prval = pd.DataFrame(mvalid)\nprval.columns = ytrain.columns\nprval['qa_id'] = id_train\nprval = prval[['qa_id'] + list(prval.columns[:-1])]\nprval.to_csv(metas_dir + 'prval_ridge_'+todate+ '.csv', index = False)\n\n\nprfull = pd.DataFrame(mfull)\nprfull.columns = ytrain.columns\nprfull['qa_id'] = id_test\nprfull = prfull[['qa_id'] + list(prfull.columns[:-1])]\nprfull.to_csv(metas_dir + 'prfull_ridge_'+todate+ '.csv', index = False)\n\nprfull.to_csv(sub_dir + 'submission.csv', index = False)","metadata":{},"execution_count":null,"outputs":[]}]}