{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport pickle as pkl\nimport datetime\nimport time\nimport networkx as nx\nimport scipy.sparse as sp\nfrom scipy.sparse.linalg.eigen.arpack import eigsh\nimport sys\nimport re\nfrom sklearn import metrics\nfrom nltk.corpus import stopwords\nimport nltk\nfrom nltk.wsd import lesk\nfrom nltk.corpus import wordnet as wn\nimport os\nimport random\nfrom math import log\nimport torch\nimport torch.nn as nn\n\n\nseed = random.randint(1, 200)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n\n\npath = '/kaggle/input/google-quest-challenge'\ntrain = pd.read_csv(f'{path}/train.csv')\ntest = pd.read_csv(f'{path}/test.csv')\nsubmission = pd.read_csv(f'{path}/sample_submission.csv')\n\n\nX_train = train['question_body'].fillna(\"fillna\").values\nX_test = test['question_body'].fillna(\"fillna\").values\n\n#y_train = train[submission.columns[1:]].values\ny_train = train[submission.columns[1]].values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"code","source":"a = np.append(X_train, X_test)\nprint(a.shape)\n\n\nwith open('mr_0' + '.txt', 'w') as f:\n    j = 0\n    for i in a:\n        f.write(i+'\\n')\n        j += 1\n        \n        \nx = []\nc = np.array(x)\nwith open('mr_0' + '.txt', 'r') as f:\n    lines = f.readlines()\n    i = -1\n    for line in lines:\n        i += 1\n        if i <= 5471:\n            line = str(i)+'\\t'+'train'+'\\t'+str(y_train[i])\n            c = np.append(c,line)\n\n        if 6078 >= i > 5471:\n            line = str(i)+'\\t'+'test'+'\\t'+str(y_train[i])\n            c = np.append(c,line)\n\nwith open('mr_1' + '.txt', 'w') as f:\n    for line in c:\n        f.write(line+'\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def parse_index_file(filename):\n    \"\"\"Parse index file.\"\"\"\n    index = []\n    for line in open(filename):\n        index.append(int(line.strip()))\n    return index\n\n\ndef sample_mask(idx, l):\n    \"\"\"Create mask.\"\"\"\n    mask = np.zeros(l)\n    mask[idx] = 1\n    return np.array(mask, dtype=np.bool)\n\n\ndef load_corpus(dataset_str):\n    \"\"\"\n    Loads input corpus from gcn/data directory\n    ind.dataset_str.x => the feature vectors of the training docs as scipy.sparse.csr.csr_matrix object;\n    ind.dataset_str.tx => the feature vectors of the test docs as scipy.sparse.csr.csr_matrix object;\n    ind.dataset_str.allx => the feature vectors of both labeled and unlabeled training docs/words\n        (a superset of ind.dataset_str.x) as scipy.sparse.csr.csr_matrix object;\n    ind.dataset_str.y => the one-hot labels of the labeled training docs as numpy.ndarray object;\n    ind.dataset_str.ty => the one-hot labels of the test docs as numpy.ndarray object;\n    ind.dataset_str.ally => the labels for instances in ind.dataset_str.allx as numpy.ndarray object;\n    ind.dataset_str.adj => adjacency matrix of word/doc nodes as scipy.sparse.csr.csr_matrix object;\n    ind.dataset_str.train.index => the indices of training docs in original doc list.\n    All objects above must be saved using python pickle module.\n    :param dataset_str: Dataset name\n    :return: All data input files loaded (as well the training/test data).\n    \"\"\"\n\n    names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'adj']\n    objects = []\n    for i in range(len(names)):\n        with open(\"ind.{}.{}\".format(dataset_str, names[i]), 'rb') as f:\n            if sys.version_info > (3, 0):\n                objects.append(pkl.load(f, encoding='latin1'))\n            else:\n                objects.append(pkl.load(f))\n\n    x, y, tx, ty, allx, ally, adj = tuple(objects)\n    #print(x.shape, y.shape, tx.shape, ty.shape, allx.shape, ally.shape)\n\n    features = sp.vstack((allx, tx)).tolil()\n    labels = np.vstack((ally, ty))\n    #print(len(labels))\n\n    train_idx_orig = parse_index_file(\n        \"{}.train.index\".format(dataset_str))\n    train_size = len(train_idx_orig)\n\n    val_size = train_size - x.shape[0]\n    test_size = tx.shape[0]\n\n    idx_train = range(len(y))\n    idx_val = range(len(y), len(y) + val_size)\n    idx_test = range(allx.shape[0], allx.shape[0] + test_size)\n\n    train_mask = sample_mask(idx_train, labels.shape[0])\n    val_mask = sample_mask(idx_val, labels.shape[0])\n    test_mask = sample_mask(idx_test, labels.shape[0])\n\n    y_train = np.zeros(labels.shape)\n    y_val = np.zeros(labels.shape)\n    y_test = np.zeros(labels.shape)\n    y_train[train_mask, :] = labels[train_mask, :]\n    y_val[val_mask, :] = labels[val_mask, :]\n    y_test[test_mask, :] = labels[test_mask, :]\n\n    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n\n    return adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask, train_size, test_size\n\n\ndef sparse_to_tuple(sparse_mx):\n    \"\"\"Convert sparse matrix to tuple representation.\"\"\"\n    def to_tuple(mx):\n        if not sp.isspmatrix_coo(mx):\n            mx = mx.tocoo()\n        coords = np.vstack((mx.row, mx.col)).transpose()\n        values = mx.data\n        shape = mx.shape\n        return coords, values, shape\n\n    if isinstance(sparse_mx, list):\n        for i in range(len(sparse_mx)):\n            sparse_mx[i] = to_tuple(sparse_mx[i])\n    else:\n        sparse_mx = to_tuple(sparse_mx)\n\n    return sparse_mx\n\n\ndef preprocess_features(features):\n    \"\"\"Row-normalize feature matrix and convert to tuple representation\"\"\"\n    rowsum = np.array(features.sum(1))\n    r_inv = np.power(rowsum, -1).flatten()\n    r_inv[np.isinf(r_inv)] = 0.\n    r_mat_inv = sp.diags(r_inv)\n    features = r_mat_inv.dot(features)\n    #return sparse_to_tuple(features)\n    return features.A\n\n\ndef normalize_adj(adj):\n    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n    adj = sp.coo_matrix(adj)\n    rowsum = np.array(adj.sum(1))\n    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n\n\ndef preprocess_adj(adj):\n    \"\"\"Preprocessing of adjacency matrix for simple GCN model and conversion to tuple representation.\"\"\"\n    adj_normalized = normalize_adj(adj + sp.eye(adj.shape[0]))\n    #return sparse_to_tuple(adj_normalized)\n    return adj_normalized.A\n\n\ndef construct_feed_dict(features, support, labels, labels_mask, placeholders):\n    \"\"\"Construct feed dictionary.\"\"\"\n    feed_dict = dict()\n    feed_dict.update({placeholders['labels']: labels})\n    feed_dict.update({placeholders['labels_mask']: labels_mask})\n    feed_dict.update({placeholders['features']: features})\n    feed_dict.update({placeholders['support'][i]: support[i]\n                      for i in range(len(support))})\n    feed_dict.update({placeholders['num_features_nonzero']: features[1].shape})\n    return feed_dict\n\n\ndef chebyshev_polynomials(adj, k):\n    \"\"\"Calculate Chebyshev polynomials up to order k. Return a list of sparse matrices (tuple representation).\"\"\"\n    print(\"Calculating Chebyshev polynomials up to order {}...\".format(k))\n\n    adj_normalized = normalize_adj(adj)\n    laplacian = sp.eye(adj.shape[0]) - adj_normalized\n    largest_eigval, _ = eigsh(laplacian, 1, which='LM')\n    scaled_laplacian = (\n        2. / largest_eigval[0]) * laplacian - sp.eye(adj.shape[0])\n\n    t_k = list()\n    t_k.append(sp.eye(adj.shape[0]))\n    t_k.append(scaled_laplacian)\n\n    def chebyshev_recurrence(t_k_minus_one, t_k_minus_two, scaled_lap):\n        s_lap = sp.csr_matrix(scaled_lap, copy=True)\n        return 2 * s_lap.dot(t_k_minus_one) - t_k_minus_two\n\n    for i in range(2, k+1):\n        t_k.append(chebyshev_recurrence(t_k[-1], t_k[-2], scaled_laplacian))\n\n    return sparse_to_tuple(t_k)\n\n\ndef loadWord2Vec(filename):\n    \"\"\"Read Word Vectors\"\"\"\n    vocab = []\n    embd = []\n    word_vector_map = {}\n    file = open(filename, 'r')\n    for line in file.readlines():\n        row = line.strip().split(' ')\n        if(len(row) > 2):\n            vocab.append(row[0])\n            vector = row[1:]\n            length = len(vector)\n            for i in range(length):\n                vector[i] = float(vector[i])\n            embd.append(vector)\n            word_vector_map[row[0]] = vector\n    print('Loaded Word Vectors!')\n    file.close()\n    return vocab, embd, word_vector_map\n\n\ndef clean_str(string):\n    \"\"\"\n    Tokenization/string cleaning for all datasets except for SST.\n    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n    \"\"\"\n    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n    string = re.sub(r\"\\'s\", \" \\'s\", string)\n    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n    string = re.sub(r\"\\'re\", \" \\'re\", string)\n    string = re.sub(r\"\\'d\", \" \\'d\", string)\n    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n    string = re.sub(r\",\", \" , \", string)\n    string = re.sub(r\"!\", \" ! \", string)\n    string = re.sub(r\"\\(\", \" \\( \", string)\n    string = re.sub(r\"\\)\", \" \\) \", string)\n    string = re.sub(r\"\\?\", \" \\? \", string)\n    string = re.sub(r\"\\s{2,}\", \" \", string)\n    return string.strip().lower()\n\n\ndef print_log(msg='', end='\\n'):\n    now = datetime.datetime.now()\n    t = str(now.year) + '/' + str(now.month) + '/' + str(now.day) + ' ' \\\n      + str(now.hour).zfill(2) + ':' + str(now.minute).zfill(2) + ':' + str(now.second).zfill(2)\n\n    if isinstance(msg, str):\n        lines = msg.split('\\n')\n    else:\n        lines = [msg]\n        \n    for line in lines:\n        if line == lines[-1]:\n            print('[' + t + '] ' + str(line), end=end)\n        else: \n            print('[' + t + '] ' + str(line))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = 'mr_0'\n\nnltk.download('stopwords')\nstop_words = set(stopwords.words('english'))\nprint(stop_words)\n\ndoc_content_list = []\nwith open('mr_0.txt', 'rb') as f:\n    for line in f.readlines():\n        doc_content_list.append(line.strip().decode('latin1'))\n        \nword_freq = {}\n\nfor doc_content in doc_content_list:\n    temp = clean_str(doc_content)\n    words = temp.split()\n    for word in words:\n        if word in word_freq:\n            word_freq[word] += 1\n        else:\n            word_freq[word] = 1    \n    \nclean_docs = []\nfor doc_content in doc_content_list:\n    temp = clean_str(doc_content)\n    words = temp.split()\n    doc_words = []\n    for word in words:\n        if dataset == 'mr_0':\n            doc_words.append(word)\n        elif word not in stop_words and word_freq[word] >= 5:\n            doc_words.append(word)\n\n    doc_str = ' '.join(doc_words).strip()\n    clean_docs.append(doc_str)\n\nclean_corpus_str = '\\n'.join(clean_docs)    \n    \n    \nwith open('mr_0' + '.clean.txt', 'w') as f:\n    f.write(clean_corpus_str)\n\nmin_len = 10000\naver_len = 0\nmax_len = 0 \n\nwith open('mr_0' + '.clean.txt', 'r') as f:\n    lines = f.readlines()\n    for line in lines:\n        line = line.strip()\n        temp = line.split()\n        aver_len = aver_len + len(temp)\n        if len(temp) < min_len:\n            min_len = len(temp)\n        if len(temp) > max_len:\n            max_len = len(temp)\n\naver_len = 1.0 * aver_len / len(lines)\nprint('Min_len : ' + str(min_len))\nprint('Max_len : ' + str(max_len))\nprint('Average_len : ' + str(aver_len))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_embeddings_dim = 300\nword_vector_map = {}\n\ndoc_name_list = []\ndoc_train_list = []\ndoc_test_list = []\n\nwith open('mr_1' + '.txt', 'r') as f:\n    lines = f.readlines()\n    for line in lines:\n        doc_name_list.append(line.strip())\n        temp = line.split(\"\\t\")\n        if temp[1].find('test') != -1:\n            doc_test_list.append(line.strip())\n        elif temp[1].find('train') != -1:\n            doc_train_list.append(line.strip())\n\ndoc_content_list = []\nwith open('mr_0' + '.clean.txt', 'r') as f:\n    lines = f.readlines()\n    for line in lines:\n        doc_content_list.append(line.strip())\n\ntrain_ids = []\nfor train_name in doc_train_list:\n    train_id = doc_name_list.index(train_name)\n    train_ids.append(train_id)\nrandom.shuffle(train_ids)\n\n\ntrain_ids_str = '\\n'.join(str(index) for index in train_ids)\nwith open('mr_0' + '.train.index', 'w') as f:\n    f.write(train_ids_str)\n\ntest_ids = []\nfor test_name in doc_test_list:\n    test_id = doc_name_list.index(test_name)\n    test_ids.append(test_id)\nrandom.shuffle(test_ids)\n\n\ntest_ids_str = '\\n'.join(str(index) for index in test_ids)\nwith open('mr_0' + '.test.index', 'w') as f:\n    f.write(test_ids_str)\n        \nids = train_ids + test_ids\n\nshuffle_doc_name_list = []\nshuffle_doc_words_list = []\nfor id in ids:\n    shuffle_doc_name_list.append(doc_name_list[int(id)])\n    shuffle_doc_words_list.append(doc_content_list[int(id)])\nshuffle_doc_name_str = '\\n'.join(shuffle_doc_name_list)\nshuffle_doc_words_str = '\\n'.join(shuffle_doc_words_list)\n\n\nwith open('mr_1' + '_shuffle.txt', 'w') as f:\n    f.write(shuffle_doc_name_str)\n\nwith open('mr_0' + '_shuffle.txt', 'w') as f:\n    f.write(shuffle_doc_words_str)    \n\n    \nword_freq = {}\nword_set = set()\nfor doc_words in shuffle_doc_words_list:\n    words = doc_words.split()\n    for word in words:\n        word_set.add(word)\n        if word in word_freq:\n            word_freq[word] += 1\n        else:\n            word_freq[word] = 1\n\nvocab = list(word_set)\nvocab_size = len(vocab)\n\nword_doc_list = {}\n\nfor i in range(len(shuffle_doc_words_list)):\n    doc_words = shuffle_doc_words_list[i]\n    words = doc_words.split()\n    appeared = set()\n    for word in words:\n        if word in appeared:\n            continue\n        if word in word_doc_list:\n            doc_list = word_doc_list[word]\n            doc_list.append(i)\n            word_doc_list[word] = doc_list\n        else:\n            word_doc_list[word] = [i]\n        appeared.add(word)\n\nword_doc_freq = {}\nfor word, doc_list in word_doc_list.items():\n    word_doc_freq[word] = len(doc_list)    \n    \n    \nword_id_map = {}\nfor i in range(vocab_size):\n    word_id_map[vocab[i]] = i\n\nvocab_str = '\\n'.join(vocab)\n\nwith open('mr_0' + '_vocab.txt', 'w') as f:\n    f.write(vocab_str)\n    \n    \nlabel_set = set()\nfor doc_meta in shuffle_doc_name_list:\n    temp = doc_meta.split('\\t')\n    label_set.add(temp[2])\nlabel_list = list(label_set)\n\nlabel_list_str = '\\n'.join(label_list)\nwith open('mr_0' + '_labels.txt', 'w') as f:\n    f.write(label_list_str)\n\n\ntrain_size = len(train_ids)\nval_size = int(0.1 * train_size)\nreal_train_size = train_size - val_size\n\nreal_train_doc_names = shuffle_doc_name_list[:real_train_size]\nreal_train_doc_names_str = '\\n'.join(real_train_doc_names)\n\nwith open('mr_1' + '.real_train.name', 'w') as f:\n    f.write(real_train_doc_names_str)\n    \n    \nrow_x = []\ncol_x = []\ndata_x = []\nfor i in range(real_train_size):\n    doc_vec = np.array([0.0 for k in range(word_embeddings_dim)])\n    doc_words = shuffle_doc_words_list[i]\n    words = doc_words.split()\n    doc_len = len(words)\n    for word in words:\n        if word in word_vector_map:\n            word_vector = word_vector_map[word]\n            doc_vec = doc_vec + np.array(word_vector)\n\n    for j in range(word_embeddings_dim):\n        row_x.append(i)\n        col_x.append(j)\n        data_x.append(doc_vec[j] / doc_len)\n\nx = sp.csr_matrix((data_x, (row_x, col_x)), shape=(\n    real_train_size, word_embeddings_dim))\n\ny = []\nfor i in range(real_train_size):\n    doc_meta = shuffle_doc_name_list[i]\n    temp = doc_meta.split('\\t')\n    label = temp[2]\n    one_hot = [0 for l in range(len(label_list))]\n    label_index = label_list.index(label)\n    one_hot[label_index] = 1\n    y.append(one_hot)\ny = np.array(y)   \n    \n    \ntest_size = len(test_ids)\n\nrow_tx = []\ncol_tx = []\ndata_tx = []\nfor i in range(test_size):\n    doc_vec = np.array([0.0 for k in range(word_embeddings_dim)])\n    doc_words = shuffle_doc_words_list[i + train_size]\n    words = doc_words.split()\n    doc_len = len(words)\n    for word in words:\n        if word in word_vector_map:\n            word_vector = word_vector_map[word]\n            doc_vec = doc_vec + np.array(word_vector)\n\n    for j in range(word_embeddings_dim):\n        row_tx.append(i)\n        col_tx.append(j)\n        data_tx.append(doc_vec[j] / doc_len)\n\ntx = sp.csr_matrix((data_tx, (row_tx, col_tx)),\n                   shape=(test_size, word_embeddings_dim))    \n\n\nty = []\nfor i in range(test_size):\n    doc_meta = shuffle_doc_name_list[i + train_size]\n    temp = doc_meta.split('\\t')\n    label = temp[2]\n    one_hot = [0 for l in range(len(label_list))]\n    label_index = label_list.index(label)\n    one_hot[label_index] = 1\n    ty.append(one_hot)\nty = np.array(ty)\n\n\nword_vectors = np.random.uniform(-0.01, 0.01,\n                                 (vocab_size, word_embeddings_dim))\n\nfor i in range(len(vocab)):\n    word = vocab[i]\n    if word in word_vector_map:\n        vector = word_vector_map[word]\n        word_vectors[i] = vector\n\nrow_allx = []\ncol_allx = []\ndata_allx = []\n\nfor i in range(train_size):\n    doc_vec = np.array([0.0 for k in range(word_embeddings_dim)])\n    doc_words = shuffle_doc_words_list[i]\n    words = doc_words.split()\n    doc_len = len(words)\n    for word in words:\n        if word in word_vector_map:\n            word_vector = word_vector_map[word]\n            doc_vec = doc_vec + np.array(word_vector)\n\n    for j in range(word_embeddings_dim):\n        row_allx.append(int(i))\n        col_allx.append(j)\n        data_allx.append(doc_vec[j] / doc_len)\nfor i in range(vocab_size):\n    for j in range(word_embeddings_dim):\n        row_allx.append(int(i + train_size))\n        col_allx.append(j)\n        data_allx.append(word_vectors.item((i, j)))\n\n\nrow_allx = np.array(row_allx)\ncol_allx = np.array(col_allx)\ndata_allx = np.array(data_allx)\n\nallx = sp.csr_matrix(\n    (data_allx, (row_allx, col_allx)), shape=(train_size + vocab_size, word_embeddings_dim))\n\nally = []\nfor i in range(train_size):\n    doc_meta = shuffle_doc_name_list[i]\n    temp = doc_meta.split('\\t')\n    label = temp[2]\n    one_hot = [0 for l in range(len(label_list))]\n    label_index = label_list.index(label)\n    one_hot[label_index] = 1\n    ally.append(one_hot)\n\nfor i in range(vocab_size):\n    one_hot = [0 for l in range(len(label_list))]\n    ally.append(one_hot)\n\nally = np.array(ally)\n\nprint(x.shape, y.shape, tx.shape, ty.shape, allx.shape, ally.shape)\n\n'''\nDoc word heterogeneous graph\n'''\n\n# Word co-occurence with context windows\nwindow_size = 20\nwindows = []\n\nfor doc_words in shuffle_doc_words_list:\n    words = doc_words.split()\n    length = len(words)\n    if length <= window_size:\n        windows.append(words)\n    else:\n        for j in range(length - window_size + 1):\n            window = words[j: j + window_size]\n            windows.append(window)\n\n\nword_window_freq = {}\nfor window in windows:\n    appeared = set()\n    for i in range(len(window)):\n        if window[i] in appeared:\n            continue\n        if window[i] in word_window_freq:\n            word_window_freq[window[i]] += 1\n        else:\n            word_window_freq[window[i]] = 1\n        appeared.add(window[i])\n\nword_pair_count = {}\nfor window in windows:\n    for i in range(1, len(window)):\n        for j in range(0, i):\n            word_i = window[i]\n            word_i_id = word_id_map[word_i]\n            word_j = window[j]\n            word_j_id = word_id_map[word_j]\n            if word_i_id == word_j_id:\n                continue\n            word_pair_str = str(word_i_id) + ',' + str(word_j_id)\n            if word_pair_str in word_pair_count:\n                word_pair_count[word_pair_str] += 1\n            else:\n                word_pair_count[word_pair_str] = 1\n            word_pair_str = str(word_j_id) + ',' + str(word_i_id)\n            if word_pair_str in word_pair_count:\n                word_pair_count[word_pair_str] += 1\n            else:\n                word_pair_count[word_pair_str] = 1\n\n                \nrow = []\ncol = []\nweight = []\n\nnum_window = len(windows)\n\nfor key in word_pair_count:\n    temp = key.split(',')\n    i = int(temp[0])\n    j = int(temp[1])\n    count = word_pair_count[key]\n    word_freq_i = word_window_freq[vocab[i]]\n    word_freq_j = word_window_freq[vocab[j]]\n    pmi = log((1.0 * count / num_window) /\n              (1.0 * word_freq_i * word_freq_j/(num_window * num_window)))\n    if pmi <= 0:\n        continue\n    row.append(train_size + i)\n    col.append(train_size + j)\n    weight.append(pmi)\n\n\ndoc_word_freq = {}\n\nfor doc_id in range(len(shuffle_doc_words_list)):\n    doc_words = shuffle_doc_words_list[doc_id]\n    words = doc_words.split()\n    for word in words:\n        word_id = word_id_map[word]\n        doc_word_str = str(doc_id) + ',' + str(word_id)\n        if doc_word_str in doc_word_freq:\n            doc_word_freq[doc_word_str] += 1\n        else:\n            doc_word_freq[doc_word_str] = 1\n\nfor i in range(len(shuffle_doc_words_list)):\n    doc_words = shuffle_doc_words_list[i]\n    words = doc_words.split()\n    doc_word_set = set()\n    for word in words:\n        if word in doc_word_set:\n            continue\n        j = word_id_map[word]\n        key = str(i) + ',' + str(j)\n        freq = doc_word_freq[key]\n        if i < train_size:\n            row.append(i)\n        else:\n            row.append(i + vocab_size)\n        col.append(train_size + j)\n        idf = log(1.0 * len(shuffle_doc_words_list) /\n                  word_doc_freq[vocab[j]])\n        weight.append(freq * idf)\n        doc_word_set.add(word)\n\nnode_size = train_size + vocab_size + test_size\nadj = sp.csr_matrix(\n    (weight, (row, col)), shape=(node_size, node_size))\n\n\nwith open(\"ind.{}.x\".format(dataset), 'wb') as f:\n    pkl.dump(x, f)\n\nwith open(\"ind.{}.y\".format(dataset), 'wb') as f:\n    pkl.dump(y, f)\n\nwith open(\"ind.{}.tx\".format(dataset), 'wb') as f:\n    pkl.dump(tx, f)\n\nwith open(\"ind.{}.ty\".format(dataset), 'wb') as f:\n    pkl.dump(ty, f)\n\nwith open(\"ind.{}.allx\".format(dataset), 'wb') as f:\n    pkl.dump(allx, f)\n\nwith open(\"ind.{}.ally\".format(dataset), 'wb') as f:\n    pkl.dump(ally, f)\n\nwith open(\"ind.{}.adj\".format(dataset), 'wb') as f:\n    pkl.dump(adj, f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class GraphConvolution(nn.Module):\n    def __init__( self, input_dim, \\\n                        output_dim, \\\n                        support, \\\n                        act_func = None, \\\n                        featureless = False, \\\n                        dropout_rate = 0., \\\n                        bias=False):\n        super(GraphConvolution, self).__init__()\n        self.support = support\n        self.featureless = featureless\n\n        for i in range(len(self.support)):\n            setattr(self, 'W{}'.format(i), nn.Parameter(torch.randn(input_dim, output_dim)))\n\n        if bias:\n            self.b = nn.Parameter(torch.zeros(1, output_dim))\n\n        self.act_func = act_func\n        self.dropout = nn.Dropout(dropout_rate)\n\n        \n    def forward(self, x):\n        x = self.dropout(x)\n\n        for i in range(len(self.support)):\n            if self.featureless:\n                pre_sup = getattr(self, 'W{}'.format(i))\n            else:\n                pre_sup = x.mm(getattr(self, 'W{}'.format(i)))\n            \n            if i == 0:\n                out = self.support[i].mm(pre_sup)\n            else:\n                out += self.support[i].mm(pre_sup)\n\n        if self.act_func is not None:\n            out = self.act_func(out)\n\n        return out\n\n\nclass GCN(nn.Module):\n    def __init__( self, input_dim, \\\n                        support,\\\n                        dropout_rate=0., \\\n                        num_classes=2):\n        super(GCN, self).__init__()\n        \n        self.layer1 = GraphConvolution(input_dim, 200, support, act_func=nn.ReLU(), featureless=True, dropout_rate=dropout_rate)\n        self.layer2 = GraphConvolution(200, num_classes, support, dropout_rate=dropout_rate)\n        \n    \n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CONFIG(object):\n    \"\"\"docstring for CONFIG\"\"\"\n    def __init__(self):\n        super(CONFIG, self).__init__()\n        \n        self.dataset = 'mr_0'\n        self.model = 'gcn'          # 'gcn', 'gcn_cheby', 'dense'\n        self.learning_rate = 0.02   # Initial learning rate.\n        self.epochs  = 200          # Number of epochs to train.\n        self.hidden1 = 200          # Number of units in hidden layer 1.\n        self.dropout = 0.5          # Dropout rate (1 - keep probability).\n        self.weight_decay = 0.      # Weight for L2 loss on embedding matrix.\n        self.early_stopping = 10    # Tolerance for early stopping (# of epochs).\n        self.max_degree = 3         # Maximum Chebyshev polynomial degree.\n        \ncfg = CONFIG()\ndataset = 'mr_0'\ncfg.dataset = dataset\n\n\n# Load data\nadj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask, train_size, test_size = load_corpus(\n    cfg.dataset)\n\nfeatures = sp.identity(features.shape[0])\n\n\n# Some preprocessing\nfeatures = preprocess_features(features)\nif cfg.model == 'gcn':\n    support = [preprocess_adj(adj)]\n    num_supports = 1\n    model_func = GCN\nelif cfg.model == 'gcn_cheby':\n    support = chebyshev_polynomials(adj, cfg.max_degree)\n    num_supports = 1 + cfg.max_degree\n    model_func = GCN\nelif cfg.model == 'dense':\n    support = [preprocess_adj(adj)]   \n    num_supports = 1\n    model_func = MLP\nelse:\n    raise ValueError('Invalid argument for model: ' + str(cfg.model))\n\n    \n# Define placeholders\nt_features = torch.from_numpy(features)\nt_y_train = torch.from_numpy(y_train)\nt_y_val = torch.from_numpy(y_val)\nt_y_test = torch.from_numpy(y_test)\nt_train_mask = torch.from_numpy(train_mask.astype(np.float32))\ntm_train_mask = torch.transpose(torch.unsqueeze(t_train_mask, 0), 1, 0).repeat(1, y_train.shape[1])\n\nt_support = []\nfor i in range(len(support)):\n    t_support.append(torch.Tensor(support[i]))\n\n\nmodel = model_func(input_dim=features.shape[0], support=t_support, num_classes=y_train.shape[1])\n\n\n# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=cfg.learning_rate)\n\n\n# Define model evaluation function\ndef evaluate(features, labels, mask):\n    t_test = time.time()\n    model.eval()\n    with torch.no_grad():\n        logits = model(features)\n        t_mask = torch.from_numpy(np.array(mask*1., dtype=np.float32))\n        tm_mask = torch.transpose(torch.unsqueeze(t_mask, 0), 1, 0).repeat(1, labels.shape[1])\n        loss = criterion(logits * tm_mask, torch.max(labels, 1)[1])\n        pred = torch.max(logits, 1)[1]\n        acc = ((pred == torch.max(labels, 1)[1]).float() * t_mask).sum().item() / t_mask.sum().item()\n        \n    return loss, acc, pred, labels.numpy(), (time.time() - t_test), logits\n\n\nval_losses = []\n\n# Train model\nfor epoch in range(cfg.epochs):\n\n    t = time.time()\n    \n    # Forward pass\n    logits = model(t_features)\n    loss = criterion(logits * tm_train_mask, torch.max(t_y_train, 1)[1])    \n    acc = ((torch.max(logits, 1)[1] == torch.max(t_y_train, 1)[1]).float() * t_train_mask).sum().item() / t_train_mask.sum().item()\n        \n    # Backward and optimize\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    \n    # Validation\n    val_loss, val_acc, pred, labels, duration, logits = evaluate(t_features, t_y_val, val_mask)\n    val_losses.append(val_loss)\n\n    print_log(\"Epoch: {:.0f}, train_loss= {:.5f}, train_acc= {:.5f}, val_loss= {:.5f}, val_acc= {:.5f}, time= {:.5f}\"\\\n                .format(epoch + 1, loss, acc, val_loss, val_acc, time.time() - t))\n\n    if epoch > cfg.early_stopping and val_losses[-1] > np.mean(val_losses[-(cfg.early_stopping+1):-1]):\n        print_log(\"Early stopping...\")\n        break\n\n\nprint_log(\"Optimization Finished!\")\n\n\n# Testing\ntest_loss, test_acc, pred, labels, test_duration, logits = evaluate(t_features, t_y_test, test_mask)\nprint_log(\"Test set results: \\n\\t loss= {:.5f}, accuracy= {:.5f}, time= {:.5f}\".format(test_loss, test_acc, test_duration))\n\ntest_pred = []\ntest_labels = []\nfor i in range(len(test_mask)):\n    if test_mask[i]:\n        test_pred.append(pred[i])\n        test_labels.append(labels[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}