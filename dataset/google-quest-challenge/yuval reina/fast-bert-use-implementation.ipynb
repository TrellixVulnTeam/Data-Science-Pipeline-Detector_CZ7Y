{"cells":[{"metadata":{"trusted":true},"cell_type":"markdown","source":"## How to make your inference faster"},{"metadata":{},"cell_type":"markdown","source":"This notebook shows how your inference can be faster.\n\nThe idea is to use the shortest max_len you can (instead of the standart 512)\n\nTo do it we re-order all sentences by their token length, each batch get the minimal max_len it can, and then sent to the model.\n\nLater we reorder the outputs to the original order.\n\nThis kernel is for demonstration only. It is not a full competition solution"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\ner=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install ../input/sacremoses/sacremoses-master/ > /dev/null","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import sys\nsys.path.insert(0, \"../input/transformers/transformers-master/\")\nimport warnings\nwarnings.filterwarnings(action='once')\nimport transformers\nfrom collections import defaultdict\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\nimport pickle\n%load_ext autoreload\n%autoreload 2\n%matplotlib inline\nfrom functools import partial\nfrom sklearn.model_selection import KFold\nimport torch\nimport torch.nn as nn\nimport torch.utils.data as D\nimport torch.nn.functional as F\nimport os\npd.set_option('max_columns', 1000)\nfrom tqdm import notebook\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The magic is in FastTokenIter it is an itaratore that is somewhat like DataLoader.\n\nJust remember to reorder everything at the end using .reorder attribute\n\n\"fetch_vectors_full\" is an example how to use it"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_model_device(model):\n    if not torch.cuda.is_available():\n        return torch.device('cpu')\n    else:\n        device_num = next(model.parameters()).get_device()\n        if device_num<0:\n            return torch.device('cpu')\n        else:\n            return torch.device(\"cuda:{}\".format(device_num))\n\nclass FastTokenIter(D.Dataset):\n    def __init__(self, ds,max_len=512, batch_size=16,shuffle = False,return_order=False):\n        self.ds = ds\n        self.max_len=max_len\n        self.batch_size=batch_size\n        self.num_items = ds.__len__()\n        self.len=int(np.ceil(float(self.num_items)/self.batch_size))\n        list_items=[ds.__getitem__(i) for i in notebook.tqdm(range(ds.__len__()) ,leave=False)]\n        self.items=[torch.cat([item[i][None] for item in list_items]) for i in range(len(list_items[0]))]\n        self.item_len=self.items[1].sum(1)\n        self.item_order = np.argsort(self.item_len.numpy())\n        self.reorder=np.argsort(self.item_order)\n        self.batch_order =np.arange(self.len)\n        self.len_tuple=len(self.items)\n        if shuffle:\n            np.rand.shuffle(self.batch_order)\n        self.return_order=return_order or shuffle\n        self.idx=0\n            \n    def __iter__(self):\n        self.idx = 0\n        return self\n    \n    def __next__(self):\n        if self.idx>=self.len:\n            raise StopIteration\n        sidx=self.batch_order[self.idx]\n        self.idx+=1\n        mlen=min(self.item_len[self.item_order[sidx*self.batch_size:(1+sidx)*self.batch_size]].max(),self.max_len)\n        ret =tuple([self.items[i][self.item_order[sidx*self.batch_size:(1+sidx)*self.batch_size]][:,:mlen] for i in range(self.len_tuple)])\n        return (self.item_order[sidx*self.batch_size:(1+sidx)*self.batch_size],)+ret if self.return_order else ret\n\ndef fetch_vectors_full(ds,model,batch_size=64,num_workers=8):\n    device = get_model_device(model)\n    fin_features=[]\n    dl = FastTokenIter(ds, batch_size=batch_size, shuffle=False)\n    _=model.eval()\n    with torch.no_grad():\n        for batch in notebook.tqdm(dl,total=dl.len,leave=False):\n            fin_features.append(model( input_ids=batch[0].to(device), attention_mask=batch[1].to(device))[0][:, 0, :].detach().cpu().numpy())\n    return np.vstack(fin_features)[dl.reorder]   \n\ndef fetch_vectors_full_slow(ds,model,batch_size=64,num_workers=8):\n    device = get_model_device(model)\n    fin_features=[]\n    dl = D.DataLoader(ds, batch_size=batch_size, shuffle=False)\n    _=model.eval()\n    with torch.no_grad():\n        for batch in notebook.tqdm(dl,leave=False):\n            fin_features.append(model( input_ids=batch[0].to(device), attention_mask=batch[1].to(device))[0][:, 0, :].detach().cpu().numpy())\n    return np.vstack(fin_features)  \n\nclass TextDataset(D.Dataset):\n    def __init__(self,text_list,tokenizer,max_len=512):\n        self.text_list=text_list\n        self.tokenizer = tokenizer\n        self.max_len=max_len\n    def __len__(self):\n        return len(self.text_list)\n    def __getitem__(self,idx):\n        token_ids=self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(self.text_list[idx]))[:self.max_len-2]\n        token_ids = [self.tokenizer.cls_token_id]+token_ids+[self.tokenizer.sep_token_id]\n        token_ids_tensor=torch.zeros(self.max_len,dtype=torch.long)\n        mask_tensor=torch.zeros(self.max_len,dtype=torch.long)\n        token_type_tensor=torch.zeros(self.max_len,dtype=torch.long)\n        token_ids[:len(token_ids)]=token_ids\n        mask_tensor[:len(token_ids)]=1\n        return tuple((token_type_tensor,mask_tensor,token_type_tensor))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device='cuda'\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('../input/google-quest-challenge/test.csv').fillna(' ')\nsample_submission = pd.read_csv('../input/google-quest-challenge/sample_submission.csv')\ner=2*er+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = transformers.DistilBertTokenizer.from_pretrained(\"../input/distilbertbaseuncased/\")\nmodel = transformers.DistilBertModel.from_pretrained(\"../input/distilbertbaseuncased/\")\nmodel.to(device)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Banchmark "},{"metadata":{"trusted":true},"cell_type":"code","source":"question_ds= TextDataset(test.question_body.to_list(),tokenizer,512)\n%time question_features=fetch_vectors_full(question_ds,model,batch_size=16)\n%time question_features=fetch_vectors_full_slow(question_ds,model,batch_size=16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}