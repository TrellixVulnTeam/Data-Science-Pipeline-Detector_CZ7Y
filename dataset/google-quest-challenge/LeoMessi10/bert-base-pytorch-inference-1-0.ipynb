{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ROOT = '../input/google-quest-challenge/'\n\ntest_df = pd.read_csv(ROOT+'test.csv')\ntrain_df = pd.read_csv(ROOT+'train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_cols = ['question_asker_intent_understanding', 'question_body_critical', \n               'question_conversational', 'question_expect_short_answer', \n               'question_fact_seeking', 'question_has_commonly_accepted_answer', \n               'question_interestingness_others', 'question_interestingness_self', \n               'question_multi_intent', 'question_not_really_a_question', \n               'question_opinion_seeking', 'question_type_choice', \n               'question_type_compare', 'question_type_consequence', \n               'question_type_definition', 'question_type_entity', \n               'question_type_instructions', 'question_type_procedure', \n               'question_type_reason_explanation', 'question_type_spelling', \n               'question_well_written', 'answer_helpful', \n               'answer_level_of_information', 'answer_plausible', \n               'answer_relevance', 'answer_satisfaction', \n               'answer_type_instructions', 'answer_type_procedure', \n               'answer_type_reason_explanation', 'answer_well_written']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nfrom torch.utils.data import TensorDataset\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import RandomSampler, SequentialSampler\nfrom pytorch_transformers import BertTokenizer\nfrom sklearn.preprocessing import MinMaxScaler","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom pathlib import Path\nimport pickle\n\ndef read_data(raw_data_path):\n    test = pd.read_csv(raw_data_path, encoding='utf-8')\n    targets = [-1] * len(test)\n    #targets = np.zeros(shape=(len(test),31))\n    sentence_a = test['question_title'] + test['question_body']\n    sentence_b = test['answer']\n\n    return targets, sentence_a, sentence_b\n\n\ndef save_pickle(data, file_path):\n    '''\n    :param data:\n    :param file_name:\n    :param pickle_path:\n    :return:\n    '''\n    if isinstance(file_path, Path):\n        file_path = str(file_path)\n    with open(file_path, 'wb') as f:\n        pickle.dump(data, f)\n        \ny, X_a, X_b  = read_data(ROOT+'test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = []\nfor step, (data_x_a, data_x_b, data_y) in enumerate(zip(X_a, X_b, y)):\n    data.append(([data_x_a, data_x_b], data_y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = BertTokenizer(\"/kaggle/input/bertpretrained/uncased_L-24_H-1024_A-16/uncased_L-24_H-1024_A-16/vocab.txt\", True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class InputExample(object):\n    def __init__(self, guid, text_a, text_b=None, label=None):\n        \"\"\"Constructs a InputExample.\n        Args:\n            guid: Unique id for the example.\n            text_a: string. The untokenized text of the first sequence. For single\n            sequence tasks, only this sequence must be specified.\n            text_b: (Optional) string. The untokenized text of the second sequence.\n            Only must be specified for sequence pair tasks.\n            label: (Optional) string. The label of the example. This should be\n            specified for train and dev examples, but not for test examples.\n        \"\"\"\n        self.guid = guid\n        self.text_a = text_a\n        self.text_b = text_b\n        self.label = label\n        \nclass InputFeature(object):\n    '''\n    A single set of features of data.\n    '''\n\n    def __init__(self, input_ids, input_mask, segment_ids, label_id, input_len):\n        self.input_ids = input_ids\n        self.input_mask = input_mask\n        self.segment_ids = segment_ids\n        self.label_id = label_id\n        self.input_len = input_len\n        \ndef create_examples(lines, example_type):\n    '''\n    Creates examples for data\n    '''\n    examples = []\n    for i, line in enumerate(lines):\n        guid = '%s-%d' % (example_type, i)\n        text_a = line[0][0]\n        text_b = line[0][1]\n        label = line[1]\n        example = InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label)\n        examples.append(example)\n    return examples\n\ndef truncate_seq_pair(tokens_a, tokens_b, max_length=512):\n    # This is a simple heuristic which will always truncate the longer sequence\n    # one token at a time. This makes more sense than truncating an equal percent\n    # of tokens from each, since if one sequence is very short then each token\n    # that's truncated likely contains more information than a longer sequence.\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_length:\n            break\n        if len(tokens_a) > len(tokens_b):\n            tokens_a.pop()\n        else:\n            tokens_b.pop()\n                \ndef create_features(examples, max_seq_len=512):\n    '''\n    # The convention in BERT is:\n    # (a) For sequence pairs:\n    #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n    #  type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1\n    # (b) For single sequences:\n    #  tokens:   [CLS] the dog is hairy . [SEP]\n    #  type_ids:   0   0   0   0  0     0   0\n    '''\n    features = []\n    for ex_id, example in enumerate(examples):\n        tokenizer = BertTokenizer(\"/kaggle/input/bertpretrained/uncased_L-24_H-1024_A-16/uncased_L-24_H-1024_A-16/vocab.txt\", True)\n        tokens_a = tokenizer.tokenize(example.text_a)\n        tokens_b = None\n        label_id = example.label\n\n        if example.text_b:\n            tokens_b = tokenizer.tokenize(example.text_b)\n            # Modifies `tokens_a` and `tokens_b` in place so that the total\n            # length is less than the specified length.\n            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n            truncate_seq_pair(tokens_a, tokens_b, max_length=max_seq_len - 3)\n        else:\n            # Account for [CLS] and [SEP] with '-2'\n            if len(tokens_a) > max_seq_len - 2:\n                tokens_a = tokens_a[:max_seq_len - 2]\n        tokens = ['[CLS]'] + tokens_a + ['[SEP]']\n        segment_ids = [0] * len(tokens)\n        if tokens_b:\n            tokens += tokens_b + ['[SEP]']\n            segment_ids += [1] * (len(tokens_b) + 1)\n\n        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n        input_mask = [1] * len(input_ids)\n        padding = [0] * (max_seq_len - len(input_ids))\n        input_len = len(input_ids)\n\n        input_ids += padding\n        input_mask += padding\n        segment_ids += padding\n\n        assert len(input_ids) == max_seq_len\n        assert len(input_mask) == max_seq_len\n        assert len(segment_ids) == max_seq_len\n\n        feature = InputFeature(input_ids=input_ids,\n                                input_mask=input_mask,\n                                segment_ids=segment_ids,\n                                label_id=label_id,\n                                input_len=input_len)\n        features.append(feature)\n    return features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_examples = create_examples(data, 'test')\ntest_features = create_features(test_examples)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_dataset(features, is_sorted=False):\n    # Convert to Tensors and build dataset\n    if is_sorted:\n        logger.info(\"sorted data by th length of input\")\n        features = sorted(features, key=lambda x: x.input_len, reverse=True)\n    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n    all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n    all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.float)\n    dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n    return dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dataset = create_dataset(test_features)\ntest_sampler = SequentialSampler(test_dataset)\ntest_dataloader = DataLoader(test_dataset,sampler=test_sampler,batch_size=32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn as nn\nfrom pytorch_transformers.modeling_bert import BertPreTrainedModel, BertModel\n\n\nclass BertForMultiClass(BertPreTrainedModel):\n    def __init__(self, config):\n        super(BertForMultiClass, self).__init__(config)\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        #self.dropout = nn.ModuleList([nn.Dropout(config.hidden_dropout_prob) for _ in range(2)])\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n        #self.classifier_m = nn.Linear(config.hidden_size, config.num_labels)\n        self.apply(self.init_weights)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, head_mask=None):\n        outputs = self.bert(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask,\n                            head_mask=head_mask)\n        pooled_output = outputs[1]\n        #for i, dropout in enumerate(self.dropout):\n        #    if i == 0:\n        #        logits = self.classifier(dropout(pooled_output))\n        #        logits_m = self.classifier_m(dropout(pooled_output))\n        #    else:\n        #        logits += self.classifier(dropout(pooled_output))\n        #        logits_m += self.classifier_m(dropout(pooled_output))\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n        #logits_m = self.classifier_m(pooled_output)\n        return logits#, logits_m #len(self.dropout)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def prepare_device(use_gpu=0):\n    \"\"\"\n    setup GPU device if available, move model into configured device\n    # 如果n_gpu_use为数字，则使用range生成list\n    # 如果输入的是一个list，则默认使用list[0]作为controller\n    Example:\n        use_gpu = '' : cpu\n        use_gpu = '0': cuda:0\n        use_gpu = '0,1' : cuda:0 and cuda:1\n     \"\"\"\n    n_gpu_use = [int(x) for x in use_gpu.split(\",\")]\n    if not use_gpu:\n        device_type = 'cpu'\n    else:\n        device_type = f\"cuda:{n_gpu_use[0]}\"\n    n_gpu = torch.cuda.device_count()\n    if len(n_gpu_use) > 0 and n_gpu == 0:\n        device_type = 'cpu'\n    if len(n_gpu_use) > n_gpu:\n        msg = f\"Warning: The number of GPU\\'s configured to use is {n_gpu}, but only {n_gpu} are available on this machine.\"\n        n_gpu_use = range(n_gpu)\n    device = torch.device(device_type)\n    list_ids = n_gpu_use\n    return device, list_ids\n\ndef model_device(n_gpu, model):\n    '''\n    :param n_gpu:\n    :param model:\n    :return:\n    '''\n    device, device_ids = prepare_device(n_gpu)\n    if len(device_ids) > 1:\n        model = torch.nn.DataParallel(model, device_ids=device_ids)\n    if len(device_ids) == 1:\n        os.environ['CUDA_VISIBLE_DEVICES'] = str(device_ids[0])\n    model = model.to(device)\n    return model, device\n\nclass Predictor(object):\n    def __init__(self, model, n_gpu):\n        self.model = model\n        self.model, self.device = model_device(n_gpu=n_gpu, model=self.model)\n\n    def predict(self, data):\n        all_logits = None\n        self.model.eval()\n        with torch.no_grad():\n            for step, batch in enumerate(data):\n                batch = tuple(t.to(self.device) for t in batch)\n                input_ids, input_mask, segment_ids, label_ids = batch\n                logits = self.model(input_ids, segment_ids, input_mask)\n                if all_logits is None:\n                    all_logits = torch.sigmoid(logits).detach().cpu().numpy()\n                else:\n                    all_logits = np.concatenate([all_logits, torch.sigmoid(logits).detach().cpu().numpy()], axis=0)\n        if 'cuda' in str(self.device):\n            torch.cuda.empty_cache()\n        return all_logits","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1 = BertForMultiClass.from_pretrained(\"/kaggle/input/bert-fold1-new/\", num_labels=30)\nmodel2 = BertForMultiClass.from_pretrained(\"/kaggle/input/bert-fold2/\", num_labels=30)\nmodel3 = BertForMultiClass.from_pretrained(\"/kaggle/input/bert-fold3-new/\", num_labels=30)\nmodel4 = BertForMultiClass.from_pretrained(\"/kaggle/input/bert-fold4/\", num_labels=30)\nmodel5 = BertForMultiClass.from_pretrained(\"/kaggle/input/bert-fold5/\", num_labels=30)\n\nresult = []\n\npredictor1 = Predictor(model=model1, n_gpu='0')\nresult1 = predictor1.predict(data=test_dataloader)\n\npredictor2 = Predictor(model=model2, n_gpu='0')\nresult2 = predictor2.predict(data=test_dataloader)\n\npredictor3 = Predictor(model=model3, n_gpu='0')\nresult3 = predictor3.predict(data=test_dataloader)\n\npredictor4 = Predictor(model=model4, n_gpu='0')\nresult4 = predictor4.predict(data=test_dataloader)\n\npredictor5 = Predictor(model=model5, n_gpu='0')\nresult5 = predictor5.predict(data=test_dataloader)\n\nresult.append([result1, result2, result3, result4, result5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_predictions = np.average(result[0], axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n=test_df['url'].apply(lambda x:('english.stackexchange.com' in x)).tolist()\nspelling=[]\nfor x in n:\n    if x:\n        spelling.append(0.5)\n    else:\n        spelling.append(0.)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# n=test_df['answer'].apply(lambda x:(('ʊ' in x) or \n#                                     ('ə' in x) or\n#                                     ('ɹ' in x) or\n#                                     ('ɪ' in x) or\n#                                     ('ʒ' in x) or\n#                                     ('ɑ' in x) or\n#                                     ('ʌ' in x) or\n#                                     ('æ' in x) or\n#                                     ('ː' in x) or\n#                                     ('ɔ' in x) or\n#                                     ('ɜ' in x) or\n#                                     ('adjective' in x) or\n#                                     ('pronounce' in x)\n#                                     )).tolist()\n# spelling=[]\n# for x in n:\n#     if x:\n#         spelling.append(0.5)\n#     else:\n#         spelling.append(0.)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"set(spelling)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds = test_predictions\ny_train = train_df[target_cols].values\n\nfor column_ind in range(30):\n    curr_column = y_train[:, column_ind]\n    values = np.unique(curr_column)\n    map_quantiles = []\n    for val in values:\n        occurrence = np.mean(curr_column == val)\n        cummulative = sum(el['occurrence'] for el in map_quantiles)\n        map_quantiles.append({'value': val, 'occurrence': occurrence, 'cummulative': cummulative})\n            \n    for quant in map_quantiles:\n        pred_col = test_preds[:, column_ind]\n        q1, q2 = np.quantile(pred_col, quant['cummulative']), np.quantile(pred_col, min(quant['cummulative'] + quant['occurrence'], 1))\n        pred_col[(pred_col >= q1) & (pred_col <= q2)] = quant['value']\n        test_preds[:, column_ind] = pred_col","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df = pd.read_csv(ROOT+'sample_submission.csv')\nsubmission_df[target_cols] = test_preds\nsubmission_df['question_type_spelling']=spelling\nsubmission_df['answer_relevance'] = submission_df['answer_relevance'].apply(lambda x : 0.33333334326744 if x < 0.7 else x)\nsubmission_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_file_name = 'submission.csv'\nsubmission_df.to_csv(sub_file_name, index=False)\n#if len(test_df) >= 1000:\n#    raise ValueError(\"We'll never see this message again\")\n#else:\n#    submission_df.to_csv(sub_file_name, index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}