{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!pip install ../input/sacremoses/sacremoses-master/ > /dev/null\nimport sys\nsys.path.insert(0, \"../input/transformers/transformers-master/\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\nimport torch\nimport torch.utils.data as D\nimport tqdm as notebook\nimport numpy as np\nimport transformers\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nimport keras.backend as K\nimport time\nimport gc\nimport random\nimport os\nimport torch\nimport warnings\nimport multiprocessing\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom contextlib import contextmanager\nfrom logging import getLogger, Formatter, StreamHandler, FileHandler, INFO\nfrom scipy.stats import spearmanr\nfrom sklearn.model_selection import GroupKFold\nfrom joblib import Parallel, delayed\nfrom tqdm import tqdm, tqdm_notebook\nfrom transformers import BertPreTrainedModel, BertModel, BertTokenizer\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom scipy.special import expit\nimport tensorflow_hub as hub\nfrom sklearn.preprocessing import OneHotEncoder\nimport re\nimport sys\nimport unicodedata\nfrom nltk.stem.lancaster import LancasterStemmer\nfrom nltk.stem import SnowballStemmer\nfrom nltk.stem import PorterStemmer\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TARGET = ['question_asker_intent_understanding', 'question_body_critical', 'question_conversational', 'question_expect_short_answer', 'question_fact_seeking',\n          'question_has_commonly_accepted_answer', 'question_interestingness_others', 'question_interestingness_self', 'question_multi_intent', 'question_not_really_a_question', \n          'question_opinion_seeking', 'question_type_choice', 'question_type_compare', 'question_type_consequence', 'question_type_definition', 'question_type_entity', 'question_type_instructions',\n          'question_type_procedure', 'question_type_reason_explanation', 'question_type_spelling', 'question_well_written', 'answer_helpful', 'answer_level_of_information', 'answer_plausible', 'answer_relevance',\n          'answer_satisfaction', 'answer_type_instructions', 'answer_type_procedure', 'answer_type_reason_explanation', 'answer_well_written']\nCOMPETITION_NAME = 'Google QUEST Q&A Labeling'\nMODEL_NAME = 'v001'\nlogger = getLogger(COMPETITION_NAME)\nLOGFORMAT = '%(asctime)s %(levelname)s %(message)s'\nPATH = '../input/google-quest-challenge/'\nN_SPLITS = 3\nGROUP = 'question_body'\nSEED = 396\ncpu_count = multiprocessing.cpu_count()\nDEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\nPRETRAINED = '../input/distilbertbaseuncased'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_model_device(model):\n    if not torch.cuda.is_available():\n        return torch.device('cpu')\n    else:\n        device_num = next(model.parameters()).get_device()\n        if device_num<0:\n            return torch.device('cpu')\n        else:\n            return torch.device(\"cuda:{}\".format(device_num))\n\nclass FastTokenIter(D.Dataset):\n    def __init__(self, ds,max_len=512, batch_size=8,shuffle = False,return_order=False):\n        self.ds = ds\n        self.max_len=max_len\n        self.batch_size=batch_size\n        self.num_items = ds.__len__()\n        self.len=int(np.ceil(float(self.num_items)/self.batch_size))\n        list_items=[ds.__getitem__(i) for i in notebook.tqdm(range(ds.__len__()) ,leave=False)]\n        self.items=[torch.cat([item[i][None] for item in list_items]) for i in range(len(list_items[0]))]\n        self.item_len=self.items[1].sum(1)\n        self.item_order = np.argsort(self.item_len.numpy())\n        self.reorder=np.argsort(self.item_order)\n        self.batch_order =np.arange(self.len)\n        self.len_tuple=len(self.items)\n        if shuffle:\n            np.rand.shuffle(self.batch_order)\n        self.return_order=return_order or shuffle\n        self.idx=0\n            \n    def __iter__(self):\n        self.idx = 0\n        return self\n    \n    def __next__(self):\n        if self.idx>=self.len:\n            raise StopIteration\n        sidx=self.batch_order[self.idx]\n        self.idx+=1\n        mlen=min(self.item_len[self.item_order[sidx*self.batch_size:(1+sidx)*self.batch_size]].max(),self.max_len)\n        ret =tuple([self.items[i][self.item_order[sidx*self.batch_size:(1+sidx)*self.batch_size]][:,:mlen] for i in range(self.len_tuple)])\n        return (self.item_order[sidx*self.batch_size:(1+sidx)*self.batch_size],)+ret if self.return_order else ret\n\ndef fetch_vectors_full(ds,model,batch_size=8,num_workers=8):\n    device = get_model_device(model)\n    fin_features=[]\n    dl = FastTokenIter(ds, batch_size=batch_size, shuffle=False)\n    _=model.eval()\n    with torch.no_grad():\n        for batch in notebook.tqdm(dl,total=dl.len,leave=False):\n            fin_features.append(model( input_ids=batch[0].to(device), attention_mask=batch[1].to(device))[0][:, 0, :].detach().cpu().numpy())\n    return np.vstack(fin_features)[dl.reorder]   \n\nclass TextDataset(D.Dataset):\n    def __init__(self,text_list,tokenizer,max_len=512):\n        self.text_list=text_list\n        self.tokenizer = tokenizer\n        self.max_len=max_len\n    def __len__(self):\n        return len(self.text_list)\n    def __getitem__(self,idx):\n        token_ids=self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(self.text_list[idx]))[:self.max_len-2]\n        token_ids = [self.tokenizer.cls_token_id]+token_ids+[self.tokenizer.sep_token_id]\n        token_ids_tensor=torch.zeros(self.max_len,dtype=torch.long)\n        mask_tensor=torch.zeros(self.max_len,dtype=torch.long)\n        token_type_tensor=torch.zeros(self.max_len,dtype=torch.long)\n        token_ids[:len(token_ids)]=token_ids\n        mask_tensor[:len(token_ids)]=1\n        return tuple((token_type_tensor,mask_tensor,token_type_tensor))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CUSTOM_TABLE = str.maketrans(\n    {\n        \"\\xad\": None,\n        \"\\x7f\": None,\n        \"\\ufeff\": None,\n        \"\\u200b\": None,\n        \"\\u200e\": None,\n        \"\\u202a\": None,\n        \"\\u202c\": None,\n        \"‘\": \"'\",\n        \"’\": \"'\",\n        \"`\": \"'\",\n        \"“\": '\"',\n        \"”\": '\"',\n        \"«\": '\"',\n        \"»\": '\"',\n        \"ɢ\": \"G\",\n        \"ɪ\": \"I\",\n        \"ɴ\": \"N\",\n        \"ʀ\": \"R\",\n        \"ʏ\": \"Y\",\n        \"ʙ\": \"B\",\n        \"ʜ\": \"H\",\n        \"ʟ\": \"L\",\n        \"ғ\": \"F\",\n        \"ᴀ\": \"A\",\n        \"ᴄ\": \"C\",\n        \"ᴅ\": \"D\",\n        \"ᴇ\": \"E\",\n        \"ᴊ\": \"J\",\n        \"ᴋ\": \"K\",\n        \"ᴍ\": \"M\",\n        \"Μ\": \"M\",\n        \"ᴏ\": \"O\",\n        \"ᴘ\": \"P\",\n        \"ᴛ\": \"T\",\n        \"ᴜ\": \"U\",\n        \"ᴡ\": \"W\",\n        \"ᴠ\": \"V\",\n        \"ĸ\": \"K\",\n        \"в\": \"B\",\n        \"м\": \"M\",\n        \"н\": \"H\",\n        \"т\": \"T\",\n        \"ѕ\": \"S\",\n        \"—\": \"-\",\n        \"–\": \"-\",\n    }\n)\n\nWORDS_REPLACER = [\n    (\"sh*t\", \"shit\"),\n    (\"s**t\", \"shit\"),\n    (\"f*ck\", \"fuck\"),\n    (\"fu*k\", \"fuck\"),\n    (\"f**k\", \"fuck\"),\n    (\"f*****g\", \"fucking\"),\n    (\"f***ing\", \"fucking\"),\n    (\"f**king\", \"fucking\"),\n    (\"p*ssy\", \"pussy\"),\n    (\"p***y\", \"pussy\"),\n    (\"pu**y\", \"pussy\"),\n    (\"p*ss\", \"piss\"),\n    (\"b*tch\", \"bitch\"),\n    (\"bit*h\", \"bitch\"),\n    (\"h*ll\", \"hell\"),\n    (\"h**l\", \"hell\"),\n    (\"cr*p\", \"crap\"),\n    (\"d*mn\", \"damn\"),\n    (\"stu*pid\", \"stupid\"),\n    (\"st*pid\", \"stupid\"),\n    (\"n*gger\", \"nigger\"),\n    (\"n***ga\", \"nigger\"),\n    (\"f*ggot\", \"faggot\"),\n    (\"scr*w\", \"screw\"),\n    (\"pr*ck\", \"prick\"),\n    (\"g*d\", \"god\"),\n    (\"s*x\", \"sex\"),\n    (\"a*s\", \"ass\"),\n    (\"a**hole\", \"asshole\"),\n    (\"a***ole\", \"asshole\"),\n    (\"a**\", \"ass\"),\n]\n\nREGEX_REPLACER = [\n    (re.compile(pat.replace(\"*\", \"\\*\"), flags=re.IGNORECASE), repl)\n    for pat, repl in WORDS_REPLACER\n]\n\nRE_SPACE = re.compile(r\"\\s\")\nRE_MULTI_SPACE = re.compile(r\"\\s+\")\n\nNMS_TABLE = dict.fromkeys(\n    i for i in range(sys.maxunicode + 1) if unicodedata.category(chr(i)) == \"Mn\"\n)\n\nHEBREW_TABLE = {i: \"א\" for i in range(0x0590, 0x05FF)}\nARABIC_TABLE = {i: \"ا\" for i in range(0x0600, 0x06FF)}\nCHINESE_TABLE = {i: \"是\" for i in range(0x4E00, 0x9FFF)}\nKANJI_TABLE = {i: \"ッ\" for i in range(0x2E80, 0x2FD5)}\nHIRAGANA_TABLE = {i: \"ッ\" for i in range(0x3041, 0x3096)}\nKATAKANA_TABLE = {i: \"ッ\" for i in range(0x30A0, 0x30FF)}\n\nTABLE = dict()\nTABLE.update(CUSTOM_TABLE)\nTABLE.update(NMS_TABLE)\n# Non-english languages\nTABLE.update(CHINESE_TABLE)\nTABLE.update(HEBREW_TABLE)\nTABLE.update(ARABIC_TABLE)\nTABLE.update(HIRAGANA_TABLE)\nTABLE.update(KATAKANA_TABLE)\nTABLE.update(KANJI_TABLE)\n\n\ndef fix_tokens(tokens):\n    \"\"\"\n    Expects a list of lower-cased tokens from TweeterTokenizer\n    \"\"\"\n    for token in tokens:\n        if token == \"gov't\" or token == \"govt\":\n            yield \"government\"\n        elif token == \"i'm\":\n            yield \"i\"\n            yield \"am\"\n        elif token.endswith(\"n't\"):\n            yield token[:-3]\n            yield \"not\"\n        elif token.endswith(\"'re\"):\n            yield token[:-3]\n            yield \"are\"\n        elif token.endswith(\"'ll\"):\n            yield token[:-3]\n            yield \"will\"\n        elif token.endswith(\"'ve\"):\n            yield token[:-3]\n            yield \"have\"\n        elif token.endswith(\"'s\"):\n            yield token[:-2]\n            yield \"'s\"\n        else:\n            yield token\n\n\ndef normalize(text: str) -> str:\n    text = RE_SPACE.sub(\" \", text)\n    text = unicodedata.normalize(\"NFKD\", text)\n    text = text.translate(TABLE)\n    text = RE_MULTI_SPACE.sub(\" \", text).strip()\n\n    for pattern, repl in REGEX_REPLACER:\n        text = pattern.sub(repl, text)\n\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embed = hub.load(\"../input/universalsentenceencoderlarge5\")\nK.clear_session()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/google-quest-challenge/' + 'train.csv')\ntest = pd.read_csv('../input/google-quest-challenge/' + 'test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in tqdm(['question_title','question_body','answer']):\n    train[i] = train[i].apply(lambda x : normalize(x))\n    test[i] = test[i].apply(lambda x : normalize(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"question_title = np.vstack([embed([i]).numpy() for i in tqdm_notebook(train.question_title.tolist())])\nK.clear_session()\nquestion_body = np.vstack([embed([i]).numpy() for i in tqdm_notebook(train.question_body.tolist())])\nK.clear_session()\nanswer = np.vstack([embed([i]).numpy() for i in tqdm_notebook(train.answer.tolist())])\nK.clear_session()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"question_title_ = np.vstack([embed([i]).numpy() for i in tqdm_notebook(test.question_title.tolist())])\nK.clear_session()\nquestion_body_ = np.vstack([embed([i]).numpy() for i in tqdm_notebook(test.question_body.tolist())])\nK.clear_session()\nanswer_ = np.vstack([embed([i]).numpy() for i in tqdm_notebook(test.answer.tolist())])\nK.clear_session()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del embed\nK.clear_session()\ntorch.cuda.empty_cache()\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from numba import cuda\ncuda.select_device(0)\ncuda.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cuda.select_device(0) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = transformers.DistilBertTokenizer.from_pretrained(PRETRAINED)\nmodel = transformers.DistilBertModel.from_pretrained(PRETRAINED)\nmodel.to('cuda:0')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!nvidia-smi","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"OHE = OneHotEncoder(sparse=False)\nOHE.fit(train[['category','host']].append(test[['category','host']]).values)\nDATA_ = [\n    question_title,\n    question_body,\n    answer,\n    OHE.transform(train[['category','host']]),\n    fetch_vectors_full(TextDataset(train.question_title.to_list(),tokenizer,512),model,batch_size=8),\n    fetch_vectors_full(TextDataset(train.question_body.to_list(),tokenizer,512),model,batch_size=8),\n    fetch_vectors_full(TextDataset(train.answer.to_list(),tokenizer,512),model,batch_size=8)\n]\nDATA_TEST = [\n    question_title_,\n    question_body_,\n    answer_,\n    OHE.transform(test[['category','host']]),\n    fetch_vectors_full(TextDataset(test.question_title.to_list(),tokenizer,512),model,batch_size=8),\n    fetch_vectors_full(TextDataset(test.question_body.to_list(),tokenizer,512),model,batch_size=8),\n    fetch_vectors_full(TextDataset(test.answer.to_list(),tokenizer,512),model,batch_size=8),\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!nvidia-smi","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN_ =  np.expand_dims(np.hstack(DATA_), 1)\nTEST_ = np.expand_dims(np.hstack(DATA_TEST), 1)\nTARGET_ = train[TARGET].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LSTM_UNITS = 1024\nDENSE_HIDDEN_UNITS =  4 * LSTM_UNITS\nINPUT_SHAPE = TRAIN_.shape[2]\n\n\nclass MODEL_v001(nn.Module):\n    def __init__(self, output=30):\n        super().__init__()\n        self.lstm1 = nn.GRU(\n        INPUT_SHAPE , LSTM_UNITS, bidirectional=True, batch_first=True\n        )\n        self.lstm2 = nn.GRU(\n            LSTM_UNITS * 2, LSTM_UNITS, bidirectional=True, batch_first=True\n        )\n        self.linear1 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n        self.linearnorm = nn.LayerNorm(DENSE_HIDDEN_UNITS)\n        self.linear_sub_out = nn.Linear(DENSE_HIDDEN_UNITS, output)\n\n    def forward(self, x, lengths=None):\n        h_lstm1, _ = self.lstm1(x)\n        h_lstm2, _ = self.lstm2(h_lstm1)\n\n        avg_pool1 = torch.mean(h_lstm1, 1)\n        avg_pool2 = torch.mean(h_lstm2, 1)\n\n        h_conc = torch.cat((avg_pool1, avg_pool2), 1)\n        h_conc_linear1 = self.linearnorm(F.relu(self.linear1(h_conc)))\n        hidden = h_conc + 2 * h_conc_linear1\n        out = self.linear_sub_out(hidden)\n        return out    \n    \ndef init_logger():\n    handler = StreamHandler()\n    handler.setLevel(INFO)\n    handler.setFormatter(Formatter(LOGFORMAT))\n    fh_handler = FileHandler('{}.log'.format(MODEL_NAME))\n    fh_handler.setFormatter(Formatter(LOGFORMAT))\n    logger.setLevel(INFO)\n    logger.addHandler(handler)\n    logger.addHandler(fh_handler)\n\n\n@contextmanager\ndef timer(name):\n    t0 = time.time()\n    yield\n    logger.info(f'[{name}] done in {time.time() - t0:.0f} s')\n\n\ndef seed_everything(seed=1234):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    tf.compat.v1.set_random_seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\n\ndef folds(data):\n    splits = list(GroupKFold(n_splits=N_SPLITS).split(X=data[GROUP], y=None, groups=data[GROUP]))\n    return splits\n\n\ndef _parallel(func, list_):\n    return Parallel(n_jobs=cpu_count)(delayed(func)(i) for i in tqdm(list_))\n\n\ndef evaluation(trues, preds):\n    rhos = []\n    for col_trues, col_pred in zip(trues.T, preds.T):\n        rhos.append(\n            spearmanr(col_trues, col_pred).correlation)\n    return np.mean(rhos)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 8 # how many samples to process at once\nn_epochs = 100 # how many times to iterate over all samples","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.from_numpy(TEST_)) ,batch_size=batch_size, shuffle=False, num_workers=cpu_count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!nvidia-smi","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class EarlyStopping:\n    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n    def __init__(self, patience=7, verbose=False, delta=0):\n        \"\"\"\n        Args:\n            patience (int): How long to wait after last time validation loss improved.\n                            Default: 7\n            verbose (bool): If True, prints a message for each validation loss improvement. \n                            Default: False\n            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n                            Default: 0\n        \"\"\"\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.val_loss_min = np.Inf\n        self.delta = delta\n\n    def __call__(self, val_loss, model):\n\n        score = -val_loss\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n        elif score < self.best_score + self.delta:\n            self.counter += 1\n            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n            self.counter = 0\n\n    def save_checkpoint(self, val_loss, model):\n        '''Saves model when validation loss decrease.'''\n        if self.verbose:\n            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n        torch.save(model.state_dict(), 'checkpoint.pt')\n        self.val_loss_min = val_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seed_everything(SEED)\nsplits = folds(train)\n# matrix for the out-of-fold predictions\ntrain_preds = np.zeros((len(train), len(TARGET)))\ntest_preds = np.zeros((len(test), len(TARGET)))\n# always call this before training for deterministic results\nseed_everything(SEED)\nscore = []\nfor i, (train_idx, valid_idx) in enumerate(splits):\n    seed_everything(SEED)\n    model = MODEL_v001()\n    model = model.cuda()\n    loss_fn = torch.nn.BCEWithLogitsLoss()\n    optimizer = torch.optim.Adam(model.parameters())\n    y_train_torch = torch.tensor(TARGET_[train_idx])\n    dataset = torch.utils.data.TensorDataset(\n        torch.from_numpy(TRAIN_[train_idx]), y_train_torch\n    )\n    train_loader = torch.utils.data.DataLoader(dataset ,batch_size=batch_size, shuffle=True, num_workers=cpu_count)\n    y_train_torch = torch.tensor(TARGET_[valid_idx])\n    dataset = torch.utils.data.TensorDataset(\n        torch.from_numpy(TRAIN_[valid_idx]), y_train_torch\n    )\n    valid_loader = torch.utils.data.DataLoader(dataset ,batch_size=batch_size, shuffle=False, num_workers=cpu_count)\n    early_stopping = EarlyStopping(patience=5, verbose=True)\n    print(f'Fold {i + 1}')\n    \n    for epoch in range(n_epochs):\n        # set train mode of the model. This enables operations which are only applied during training like dropout\n        start_time = time.time()\n        model.train()\n        avg_loss = 0.  \n        for x, y in tqdm_notebook(train_loader): \n            x, y = x.float().cuda(), y.cuda()\n            \n            y_pred = model(x)\n\n            # Compute and print loss.\n            loss = loss_fn(y_pred, y)\n            \n            optimizer.zero_grad()\n\n            # Backward pass: compute gradient of the loss with respect to model parameters\n            loss.backward()\n\n            # Calling the step function on an Optimizer makes an update to its parameters\n            optimizer.step()\n            avg_loss += loss.item() / len(train_loader)\n            \n        # set evaluation mode of the model. This disabled operations which are only applied during training like dropout\n        model.eval()\n        \n        valid_preds_fold = np.zeros((len(valid_idx), len(TARGET)))\n        test_preds_fold = np.zeros((len(test), len(TARGET)))\n        \n        avg_val_loss = 0.\n        for i, (x, y) in enumerate(valid_loader):\n            x, y = x.float().cuda(), y.cuda()\n            y_pred = model(x).detach()\n            \n            avg_val_loss += loss_fn(y_pred, y).item() / len(valid_loader)\n            valid_preds_fold[i * batch_size:(i+1) * batch_size] = y_pred.cpu().numpy()\n        \n        elapsed_time = time.time() - start_time \n        print('Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t time={:.2f}s'.format(\n            epoch + 1, n_epochs, avg_loss, avg_val_loss, elapsed_time))\n        \n        early_stopping(avg_val_loss, model)\n        if early_stopping.early_stop:\n            print(\"Early stopping\")\n            break\n        \n    valid_preds_fold = np.zeros((len(valid_idx), len(TARGET)))\n    test_preds_fold = np.zeros((len(test), len(TARGET)))\n\n    avg_val_loss = 0.\n    for i, (x, y) in enumerate(valid_loader):\n        x, y = x.float().cuda(), y.cuda()\n        y_pred = model(x).detach()\n\n        avg_val_loss += loss_fn(y_pred, y).item() / len(valid_loader)\n        valid_preds_fold[i * batch_size:(i+1) * batch_size] = y_pred.cpu().numpy()\n\n    elapsed_time = time.time() - start_time \n    print('Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t time={:.2f}s'.format(\n        epoch + 1, n_epochs, avg_loss, avg_val_loss, elapsed_time))\n    # load the last checkpoint with the best model\n    model.load_state_dict(torch.load('checkpoint.pt'))\n    # predict all samples in the test set batch per batch\n    for i, (x_batch,) in enumerate(test_loader):\n        x_batch = x_batch.float().cuda()\n        y_pred = model(x_batch).detach()\n\n        test_preds_fold[i * batch_size:(i+1) * batch_size] = y_pred.cpu().numpy()\n\n    train_preds[valid_idx] = valid_preds_fold\n    score.append(evaluation(train[TARGET].iloc[valid_idx].values, train_preds[valid_idx]))\n    print(score[-1])\n    del model\n    torch.cuda.empty_cache()\n    test_preds += test_preds_fold / len(splits)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.mean(score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame(test_preds, columns=TARGET).rank() / len(test_preds)\nsubmission['qa_id'] = test['qa_id'].values\nsubmission.to_csv(\"submission.csv\", index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}