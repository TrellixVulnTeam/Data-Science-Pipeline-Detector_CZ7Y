{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from collections import Counter, defaultdict\nimport copy\nfrom functools import partial\nimport itertools\nimport os\nfrom pathlib import Path\nimport random\nimport re\nimport string\nimport time\nfrom typing import Dict, List, Tuple\nimport warnings\n\nfrom gensim.models import Word2Vec\nimport joblib\nfrom numba import cuda\nimport numpy as np\nimport pandas as pd\nimport plotly_express as px\nimport scipy\n\nfrom nltk.stem import PorterStemmer, SnowballStemmer\nfrom nltk.stem.lancaster import LancasterStemmer\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.model_selection import GroupKFold, KFold\nfrom sklearn.utils import shuffle\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\n\nimport torch\nfrom torch import nn\nfrom torch.optim.optimizer import Optimizer\nfrom torch.utils.data import Dataset, Sampler, DataLoader","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"start_time = time.time()\n\nINPUT_DIR = Path('../input/')\nDATA_DIR = INPUT_DIR / 'google-quest-challenge'\nTRAIN_PATH = DATA_DIR / 'train.csv'\nTEST_PATH = DATA_DIR / 'test.csv'\nSAMPLE_SUBMISSION_PATH = DATA_DIR / 'sample_submission.csv'\n\nUSE_DIR = INPUT_DIR / 'universal-sentence-encoder/universal-sentence-encoder-qa/universal-sentence-encoder-qa/'\nEMBEDDING_PATH = INPUT_DIR / 'fasttext-vector/fasttext.pkl'\n\ntrain_df = pd.read_csv(TRAIN_PATH)\ntest_df = pd.read_csv(TEST_PATH)\n\nnum_targets = 30\ntarget_names = [\n    'question_asker_intent_understanding',\n    'question_body_critical',\n    'question_conversational',\n    'question_expect_short_answer',\n    'question_fact_seeking',\n    'question_has_commonly_accepted_answer',\n    'question_interestingness_others',\n    'question_interestingness_self',\n    'question_multi_intent',\n    'question_not_really_a_question',\n    'question_opinion_seeking',\n    'question_type_choice',\n    'question_type_compare',\n    'question_type_consequence',\n    'question_type_definition',\n    'question_type_entity',\n    'question_type_instructions',\n    'question_type_procedure',\n    'question_type_reason_explanation',\n    'question_type_spelling',\n    'question_well_written',\n    'answer_helpful',\n    'answer_level_of_information',\n    'answer_plausible',\n    'answer_relevance',\n    'answer_satisfaction',\n    'answer_type_instructions',\n    'answer_type_procedure',\n    'answer_type_reason_explanation',\n    'answer_well_written']\n\ntrain_df['question'] = train_df['question_title'] + ' ' + train_df['question_body']\ntest_df['question'] = test_df['question_title'] + ' ' + test_df['question_body']\n\nn_splits = 5\nn_epochs = 9\nbatch_size = 32\n\nmax_q_len = 512\nmax_a_len = 512\n\nupdates_per_epoch = 100\nmu = 0.9\n\nembed_size = 300\nmax_features = 60000\n\nseed = 1029\ndevice = torch.device('cuda')\n\nps = PorterStemmer()\nlc = LancasterStemmer()\nsb = SnowballStemmer('english')\n\nwarnings.filterwarnings('ignore')\nrandom.seed(seed)\nos.environ['PYTHONHASHSEED'] = str(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"misspell_dict = {\"aren't\": \"are not\", \"can't\": \"cannot\", \"couldn't\": \"could not\",\n                 \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\",\n                 \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n                 \"he'd\": \"he would\", \"he'll\": \"he will\", \"he's\": \"he is\",\n                 \"i'd\": \"I had\", \"i'll\": \"I will\", \"i'm\": \"I am\", \"isn't\": \"is not\",\n                 \"it's\": \"it is\", \"it'll\": \"it will\", \"i've\": \"I have\", \"let's\": \"let us\",\n                 \"mightn't\": \"might not\", \"mustn't\": \"must not\", \"shan't\": \"shall not\",\n                 \"she'd\": \"she would\", \"she'll\": \"she will\", \"she's\": \"she is\",\n                 \"shouldn't\": \"should not\", \"that's\": \"that is\", \"there's\": \"there is\",\n                 \"they'd\": \"they would\", \"they'll\": \"they will\", \"they're\": \"they are\",\n                 \"they've\": \"they have\", \"we'd\": \"we would\", \"we're\": \"we are\",\n                 \"weren't\": \"were not\", \"we've\": \"we have\", \"what'll\": \"what will\",\n                 \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\",\n                 \"where's\": \"where is\", \"who'd\": \"who would\", \"who'll\": \"who will\",\n                 \"who're\": \"who are\", \"who's\": \"who is\", \"who've\": \"who have\",\n                 \"won't\": \"will not\", \"wouldn't\": \"would not\", \"you'd\": \"you would\",\n                 \"you'll\": \"you will\", \"you're\": \"you are\", \"you've\": \"you have\",\n                 \"'re\": \" are\", \"wasn't\": \"was not\", \"we'll\": \" will\", \"tryin'\": \"trying\"}\n\n\ndef replace_typical_misspell(text: str) -> str:\n    misspell_re = re.compile('(%s)' % '|'.join(misspell_dict.keys()))\n\n    def replace(match):\n        return misspell_dict[match.group(0)]\n\n    return misspell_re.sub(replace, text)\n\n\npuncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']',\n          '>', '%', '=', '#', '*', '+', '\\\\', '•', '~', '@', '£', '·', '_', '{', '}', '©', '^',\n          '®', '`', '<', '→', '°', '€', '™', '›', '♥', '←', '×', '§', '″', '′', 'Â', '█',\n          '½', 'à', '…', '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶',\n          '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '▒', '：', '¼',\n          '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲',\n          'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '∙', '）', '↓', '、', '│', '（', '»', '，', '♪',\n          '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√']\n\n\ndef clean_text(text: str) -> str:\n    text = str(text)\n    for punct in puncts + list(string.punctuation):\n        if punct in text:\n            text = text.replace(punct, f' {punct} ')\n    return text\n\n\ndef clean_numbers(text: str) -> str:\n    return re.sub(r'\\d+', ' ', text)\n\n\ndef preprocess_text(text: str) -> str:\n    text = text.lower()\n    text = replace_typical_misspell(text)\n    text = clean_text(text)\n    text = clean_numbers(text)\n    text = text.strip()\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_columns = ['question', 'answer']\ntrain_texts = train_df[text_columns].applymap(preprocess_text).values\ntest_texts = test_df[text_columns].applymap(preprocess_text).values\nall_texts = list(itertools.chain(*train_texts, *test_texts))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_vocab(texts: List[str], max_features: int = 100000) -> Dict[str, Dict]:\n    counter = Counter()\n    for text in texts:\n        counter.update(text.split())\n\n    vocab = {}\n    vocab['token2id'] = {\n        token: _id + 1 for _id, (token, count) in\n        enumerate(counter.most_common(max_features))}\n    vocab['token2id']['<PAD>'] = 0\n    vocab['token2id']['<UNK>'] = len(vocab['token2id'])\n    vocab['id2token'] = {v: k for k, v in vocab['token2id'].items()}\n    vocab['word_freq'] = {\n        **{'<PAD>': 0, '<UNK>': 0},\n        **dict(counter.most_common(max_features)),\n    }\n    return vocab\n\n\ndef tokenize(texts: List[str],\n             token2id: Dict[str, int],\n             max_len: int = 200) -> List[List[int]]:\n    \n    def text2ids(text, token2id, max_len):\n        return [\n            token2id.get(token, len(token2id) - 1)\n            for token in text.split()[:max_len]]\n    \n    tokenized = [\n        text2ids(text, token2id, max_len)\n        for text in texts]\n    return tokenized","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_embedding(embedding_path: str, word_index: Dict[str, int]) -> np.ndarray:\n    embeddings_index = joblib.load(embedding_path)\n\n    # word_index = tokenizer.word_index\n    nb_words = min(max_features + 2, len(word_index))\n    embedding_matrix = np.zeros((nb_words, embed_size))\n\n    for key, i in word_index.items():\n        word = key\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n            continue\n        word = key.lower()\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n            continue\n        word = key.upper()\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n            continue\n        word = key.capitalize()\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n            continue\n        word = ps.stem(key)\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n            continue\n        word = lc.stem(key)\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n            continue\n        word = sb.stem(key)\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n            continue\n\n    return embedding_matrix\n\n\ndef w2v_fine_tune(all_texts: List[str], vocab: Dict, embedding_matrix: np.ndarray) -> np.ndarray:\n    model = Word2Vec(min_count=1, workers=1, iter=3, size=300)\n    model.build_vocab_from_freq(vocab['word_freq'])\n    idxmap = np.array(\n        [vocab['token2id'][w] for w in model.wv.index2entity])\n    model.wv.vectors[:] = embedding_matrix[idxmap]\n    model.trainables.syn1neg[:] = embedding_matrix[idxmap]\n    model.train(all_texts, total_examples=len(all_texts), epochs=model.epochs)\n    embedding_matrix = np.vstack([np.zeros((1, 300)), model.wv.vectors, np.zeros((1, 300))])\n    return embedding_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab = build_vocab(itertools.chain(*train_texts, *test_texts), max_features)\nembedding_matrix = load_embedding(EMBEDDING_PATH, vocab['token2id'])\nembedding_matrix = w2v_fine_tune(all_texts, vocab, embedding_matrix)\n\ntrain_q = tokenize(train_texts[:, 0], vocab['token2id'], max_q_len)\ntrain_a = tokenize(train_texts[:, 1], vocab['token2id'], max_a_len)\ntrain_x = np.array([train_q, train_a]).T\ntrain_y = train_df[target_names].values\ntrain_group = train_df['question_body'].values\n\ntest_q = tokenize(test_texts[:, 0], vocab['token2id'], max_q_len)\ntest_a = tokenize(test_texts[:, 1], vocab['token2id'], max_a_len)\ntest_x = np.array([test_q, test_a]).T\n\n# target scaling\nt_max = train_y.max(axis=0)[np.newaxis, :]\nt_min = train_y.min(axis=0)[np.newaxis, :]\ntrain_y = (train_y - t_min) / (t_max - t_min)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class UniversalSentenceEncoder(object):\n    \n    def __init__(self, model_dir: str, batch_size: int = 128):\n        self.module = hub.load(str(model_dir))\n        self.batch_size = batch_size\n\n    def __call__(self, texts: List[str], mode: str) -> torch.FloatTensor:\n        assert mode in ['question', 'answer']\n        embeddings = []\n        for i in range(0, len(texts), self.batch_size):\n            text = texts[i:(i + self.batch_size)]\n            if mode == 'question':\n                h_embedding = self.module.signatures['question_encoder'](\n                    tf.constant(text))['outputs']\n            else:\n                h_embedding = self.module.signatures['response_encoder'](\n                    input=tf.constant(text),\n                    context=tf.constant(text))['outputs']\n            h_embedding = torch.FloatTensor(h_embedding.numpy())\n            embeddings.append(h_embedding)\n        return torch.cat(embeddings, 0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"use = UniversalSentenceEncoder(USE_DIR, batch_size=32)\n\nq_texts = list(itertools.chain(train_df['question'].values, test_df['question'].values))\na_texts = list(itertools.chain(train_df['answer'].values, test_df['answer'].values))\nq_emb = use(q_texts, mode='question')\na_emb = use(a_texts, mode='answer')\n\ntrain_q_emb, test_q_emb = q_emb[:len(train_df)], q_emb[len(train_df):]\ntrain_a_emb, test_a_emb = a_emb[:len(train_df)], a_emb[len(train_df):]\n\ntrain_text_emb = torch.cat((train_q_emb, train_a_emb), 1)\ntest_text_emb = torch.cat((test_q_emb, test_a_emb), 1)\n\ndel use\ncuda.select_device(0)\ncuda.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TextDataset(Dataset):\n\n    def __init__(self, seqs, targets=None):\n        self.seqs = seqs\n        self.targets = targets\n        \n    def __len__(self):\n        return len(self.seqs)\n        \n    def get_keys(self):\n        return np.vectorize(len)(self.seqs).sum(axis=1)\n        \n    def __getitem__(self, index):\n        if self.targets is None:\n            return index, self.seqs[index]\n        return index, self.seqs[index], self.targets[index]\n\n\ndef collate_fn(data):\n\n    def _pad_sequences(seqs):\n        lens = [len(seq) for seq in seqs]\n        max_len = max(lens)\n\n        padded_seqs = torch.zeros(len(seqs), max_len).long()\n        for i, seq in enumerate(seqs):\n            start = max_len - lens[i]\n            padded_seqs[i, start:] = torch.LongTensor(seq)\n        return padded_seqs\n\n    transposed = list(zip(*data))\n    index = transposed[0]\n    q_seqs, a_seqs = zip(*transposed[1])\n    q_seqs = _pad_sequences(q_seqs)\n    a_seqs = _pad_sequences(a_seqs)\n    seqs = [q_seqs, a_seqs]\n    if len(transposed) == 2:  # targets == None\n        return index, seqs\n    return index, seqs, torch.FloatTensor(transposed[2])\n\n\nclass BucketSampler(Sampler):\n\n    def __init__(self, data_source, sort_keys, bucket_size=None, batch_size=1048, shuffle_data=True):\n        super().__init__(data_source)\n        self.shuffle = shuffle_data\n        self.batch_size = batch_size\n        self.sort_keys = sort_keys\n        self.bucket_size = bucket_size if bucket_size is not None else len(sort_keys)\n        self.weights = None\n\n        if not shuffle_data:\n            self.index = self.prepare_buckets()\n        else:\n            self.index = None\n\n    def set_weights(self, weights):\n        assert weights >= 0\n        total = np.sum(weights)\n        if total != 1:\n            weights = weights / total\n        self.weights = weights\n\n    def __iter__(self):\n        indices = None\n        if self.weights is not None:\n            total = len(self.sort_keys)\n            indices = np.random.choice(total, (total,), p=self.weights)\n        if self.shuffle:\n            self.index = self.prepare_buckets(indices)\n        return iter(self.index)\n\n    def get_reverse_indexes(self):\n        indexes = np.zeros((len(self.index),), dtype=np.int32)\n        for i, j in enumerate(self.index):\n            indexes[j] = i\n        return indexes\n\n    def __len__(self):\n        return len(self.sort_keys)\n        \n    def prepare_buckets(self, indices=None):\n        lens = - self.sort_keys\n        assert self.bucket_size % self.batch_size == 0 or self.bucket_size == len(lens)\n\n        if indices is None:\n            if self.shuffle:\n                indices = shuffle(np.arange(len(lens), dtype=np.int32))\n                lens = lens[indices]\n            else:\n                indices = np.arange(len(lens), dtype=np.int32)\n\n        #  bucket iterator\n        def divide_chunks(l, n):\n            if n == len(l):\n                yield np.arange(len(l), dtype=np.int32), l\n            else:\n                # looping till length l\n                for i in range(0, len(l), n):\n                    data = l[i:i + n]\n                    yield np.arange(i, i + len(data), dtype=np.int32), data\n    \n        new_indices = []\n        extra_batch = None\n        for chunk_index, chunk in divide_chunks(lens, self.bucket_size):\n            # sort indices in bucket by descending order of length\n            indices_sorted = chunk_index[np.argsort(chunk, axis=-1)]\n            batches = []\n            for _, batch in divide_chunks(indices_sorted, self.batch_size):\n                if len(batch) == self.batch_size:\n                    batches.append(batch.tolist())\n                else:\n                    assert extra_batch is None\n                    assert batch is not None\n                    extra_batch = batch\n    \n            # shuffling batches within buckets\n            if self.shuffle:\n                batches = shuffle(batches)\n            for batch in batches:\n                new_indices.extend(batch)\n    \n        if extra_batch is not None:\n            new_indices.extend(extra_batch)\n        return indices[new_indices]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dataset = TextDataset(test_x)\ntest_sampler = BucketSampler(test_dataset, test_dataset.get_keys(),\n                             batch_size=batch_size, shuffle_data=False)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, sampler=test_sampler,\n                         shuffle=False, num_workers=0, collate_fn=collate_fn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class LstmUnit(nn.Module):\n    \n    def __init__(self, embedding_matrix, lstm_hidden_size=120, gru_hidden_size=60):\n        super(LstmUnit, self).__init__()\n        self.embedding = nn.Embedding(*embedding_matrix.shape)\n        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n        self.embedding.weight.requires_grad = False\n        self.embedding_dropout = nn.Dropout2d(0.2)\n\n        self.lstm = nn.LSTM(embedding_matrix.shape[1], lstm_hidden_size, bidirectional=True, batch_first=True)\n        self.lstm2 = nn.LSTM(lstm_hidden_size * 2, gru_hidden_size, bidirectional=True, batch_first=True)\n        \n    def apply_spatial_dropout(self, h_embedding):\n        h_embedding = h_embedding.transpose(1, 2).unsqueeze(2)\n        h_embedding = self.embedding_dropout(h_embedding).squeeze(2).transpose(1, 2)\n        return h_embedding\n    \n    def flatten_parameters(self):\n        self.lstm.flatten_parameters()\n        self.lstm2.flatten_parameters()\n\n    def forward(self, x):\n        batch_size = x.size(0)\n        h_embedding = self.embedding(x)\n        h_embedding = self.apply_spatial_dropout(h_embedding)\n\n        h_lstm, _ = self.lstm(h_embedding)\n        h_lstm, _ = self.lstm2(h_lstm)\n\n        avg_pool = torch.mean(h_lstm, 1)\n        max_pool, _ = torch.max(h_lstm, 1)\n\n        out = torch.cat((avg_pool, max_pool), 1)\n        return out\n\n\nclass LstmModel(nn.Module):\n    \n    def __init__(self, embedding_matrix):\n        super(LstmModel, self).__init__()\n        q_lstm_size = 120\n        q_lstm2_size = 120\n        \n        a_lstm_size = 120\n        a_lstm2_size = 120\n        \n        self.q_lstm = LstmUnit(embedding_matrix, q_lstm_size, q_lstm2_size)\n        self.a_lstm = LstmUnit(embedding_matrix, a_lstm_size, a_lstm2_size)\n\n        self.linear = nn.Linear((q_lstm2_size + a_lstm2_size) * 4 + 512 * 2, 200)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.1)\n        self.out = nn.Linear(200, num_targets)\n        \n    def flatten_parameters(self):\n        self.q_lstm.flatten_parameters()\n        self.a_lstm.flatten_parameters()\n        \n    def forward(self, q_seqs, a_seqs, text_emb):\n        h_q = self.q_lstm(q_seqs)\n        h_a = self.a_lstm(a_seqs)\n\n        conc = torch.cat((h_q, h_a, text_emb), 1)\n        conc = self.relu(self.linear(conc))\n        conc = self.dropout(conc)\n        out = self.out(conc)\n\n        return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class EMA(object):\n\n    def __init__(self, model, mu, level='batch', n=1):\n        # self.ema_model = copy.deepcopy(model)\n        self.mu = mu\n        self.level = level\n        self.n = n\n        self.cnt = self.n\n        self.shadow = {}\n        for name, param in model.named_parameters():\n            if param.requires_grad:\n                self.shadow[name] = param.data\n\n    def _update(self, model):\n        for name, param in model.named_parameters():\n            if param.requires_grad:\n                new_average = (1 - self.mu) * param.data + self.mu * self.shadow[name]\n                self.shadow[name] = new_average.clone()\n\n    def set_weights(self, ema_model):\n        for name, param in ema_model.named_parameters():\n            if param.requires_grad:\n                param.data = self.shadow[name]\n\n    def on_batch_end(self, model):\n        if self.level is 'batch':\n            self.cnt -= 1\n            if self.cnt == 0:\n                self._update(model)\n                self.cnt = self.n\n                \n    def on_epoch_end(self, model):\n        if self.level is 'epoch':\n            self._update(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_scores(y_true, y_pred) -> Dict[str, float]:\n    # y_true, y_pred: np.ndarray with shape (sample_size, num_targets)\n    assert y_true.shape == y_pred.shape\n    assert y_true.shape[1] == num_targets\n    scores = {}\n    for target_name, i in zip(target_names, range(y_true.shape[1])):\n        scores[target_name] = scipy.stats.spearmanr(y_true[:, i], y_pred[:, i])[0]\n    return scores\n\n\ndef predict(model: nn.Module,\n            data_loader: DataLoader,\n            text_emb: torch.Tensor,\n            device: torch.device = torch.device('cuda')) -> np.ndarray:\n    model.eval()\n    preds_fold = np.zeros((len(data_loader.dataset), num_targets))\n\n    with torch.no_grad():\n        for index, x_batch in data_loader:\n            x_batch = (x.to(device) for x in x_batch)\n            emb_batch = text_emb[list(index)].to(device)\n            y_pred = model(*x_batch, emb_batch).detach()\n            preds_fold[list(index)] = torch.sigmoid(y_pred.cpu()).numpy()\n    return preds_fold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gkf = GroupKFold(n_splits=n_splits)\ncv_scores = []\nema_cv_scores = []\nfold_scores = []\ntrain_preds = np.zeros((len(train_x), num_targets))\ntest_preds = np.zeros((len(test_x), num_targets))\nema_train_preds = np.zeros((len(train_x), num_targets))\nema_test_preds = np.zeros((len(test_x), num_targets))\n\nfor i, (train_idx, valid_idx) in enumerate(list(gkf.split(train_x, train_y, train_group))):\n    print(f'fold {i + 1}')\n    train_fold_x, train_fold_y = train_x[train_idx], train_y[train_idx]\n    valid_fold_x, valid_fold_y = train_x[valid_idx], train_y[valid_idx]\n    train_fold_emb = train_text_emb[train_idx]\n    valid_fold_emb = train_text_emb[valid_idx]\n    \n    train_dataset = TextDataset(train_fold_x, train_fold_y)\n    valid_dataset = TextDataset(valid_fold_x)\n\n    train_sampler = BucketSampler(train_dataset, train_dataset.get_keys(),\n                                  bucket_size=batch_size * 20, batch_size=batch_size)\n    valid_sampler = BucketSampler(valid_dataset, valid_dataset.get_keys(),\n                                  batch_size=batch_size, shuffle_data=False)\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False,\n                              sampler=train_sampler, num_workers=0, collate_fn=collate_fn)\n    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False,\n                              sampler=valid_sampler, collate_fn=collate_fn)\n\n    model = LstmModel(embedding_matrix).to(device)\n    model.zero_grad()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    \n    ema_model = copy.deepcopy(model)\n    ema_model.eval()\n    \n    ema_n = int(len(train_loader.dataset) / (updates_per_epoch * batch_size))\n    ema = EMA(model, mu, n=ema_n)\n\n    for epoch in range(n_epochs):\n        epoch_start_time = time.time()\n        model.train()\n        for index, x_batch, y_batch in train_loader:\n            x_batch = (x.to(device) for x in x_batch)\n            y_batch = y_batch.to(device)\n            emb_batch = train_fold_emb[list(index)].to(device)\n            y_preds = model(*x_batch, emb_batch)\n            \n            loss = nn.BCEWithLogitsLoss()(y_preds, y_batch)\n            loss.backward()\n            optimizer.step()\n            model.zero_grad()\n            ema.on_batch_end(model)\n        \n        valid_preds = predict(model, valid_loader, valid_fold_emb, device=device)\n        scores = get_scores(valid_fold_y, valid_preds)\n        score = np.mean(list(scores.values()))\n        fold_scores.append({\n            'fold': i + 1,\n            'epoch': epoch + 1,\n            'score': score\n        })\n        elapsed_time = time.time() - epoch_start_time\n        print('Epoch {}/{} \\t score: {:.4f} \\t time: {:.2f}s'.format(\n            epoch + 1, n_epochs, score, elapsed_time))\n        ema.on_epoch_end(model)\n        \n    ema.set_weights(ema_model)\n    ema_model.flatten_parameters()\n    \n    train_preds[valid_idx] = valid_preds\n    ema_valid_preds = predict(ema_model, valid_loader, valid_fold_emb, device=device)\n    ema_train_preds[valid_idx] = ema_valid_preds\n\n    cv_scores.append(score)\n    ema_scores = get_scores(valid_fold_y, ema_valid_preds)\n    ema_score = np.mean(list(ema_scores.values()))\n    print(f'EMA score: {ema_score:.4f}')\n    ema_cv_scores.append(ema_scores)\n    \n    test_preds += predict(model, test_loader, test_text_emb, device=device) / n_splits\n    ema_test_preds += predict(ema_model, test_loader, test_text_emb, device=device) / n_splits","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.line(pd.DataFrame(fold_scores), x='epoch', y='score', color='fold')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ema_cv_scores = pd.DataFrame(ema_cv_scores).mean().reset_index()\nema_cv_scores.columns = ['target_name', 'score']\nema_cv_scores.sort_values('score', inplace=True)\nfig = px.bar(ema_cv_scores, x='score', y='target_name', orientation='h')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv(SAMPLE_SUBMISSION_PATH)\nsubmission[target_names] = ema_test_preds\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_score = np.mean(cv_scores)\nema_score = ema_cv_scores['score'].mean()\nprint(f'CV score: {cv_score:.4f}')\nprint(f'EMA score: {ema_score:.4f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class OptimizedRounder(BaseEstimator, TransformerMixin):\n    \n    def __init__(self):\n        self.threshold = [0., 1.]\n        self.ab_start = [(0., 0.2), (0.8, 1.)]\n    \n    def fit(self, train_labels, train_preds):\n        assert train_labels.shape == train_preds.shape\n        assert train_labels.ndim == 1\n        \n        self.best_score = self.score(train_labels, train_preds)\n        self._golden_section_search(train_labels, train_preds, 0)  # lower threshold\n        score = self.score(train_labels, train_preds)\n        if score > self.best_score + 1e-3:\n            self.best_score = score\n        else:\n            self.threshold[0] = 0.\n        \n        self._golden_section_search(train_labels, train_preds, 1)  # higher threshold\n        score = self.score(train_labels, train_preds)\n        if score > self.best_score + 1e-3:\n            self.best_score = score\n        else:\n            self.threshold[1] = 1.\n\n    def _golden_section_search(self, train_labels, train_preds, idx):\n        # idx == 0 -> lower threshold search\n        # idx == 1 -> higher threshold search\n        golden1 = 0.618\n        golden2 = 1 - golden1\n        for _ in range(10):\n            a, b = self.ab_start[idx]\n            # calc losses\n            self.threshold[idx] = a\n            la = -self.score(train_labels, train_preds)\n            self.threshold[idx] = b\n            lb = -self.score(train_labels, train_preds)\n            for _ in range(20):\n                # choose value\n                if la > lb:\n                    a = b - (b - a) * golden1\n                    self.threshold[idx] = a\n                    la = -self.score(train_labels, train_preds)\n                else:\n                    b = b - (b - a) * golden2\n                    self.threshold[idx] = b\n                    lb = -self.score(train_labels, train_preds)\n\n    def transform(self, preds):\n        transformed = np.clip(preds, *self.threshold)\n        if np.unique(transformed).size == 1:\n            return preds\n        return transformed\n        \n    def score(self, labels, preds):\n        p = self.transform(preds)\n        score = scipy.stats.spearmanr(labels, p)[0]\n        return score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kf = KFold(n_splits=3, shuffle=True, random_state=1029)\ntrain_scores = []\nvalid_scores = []\ntrain_optimized_scores = defaultdict(list)\nvalid_optimized_scores = defaultdict(list)\nthresholds = defaultdict(list)\n\nfor train_idx, valid_idx in kf.split(train_y):\n    train_fold_preds, train_fold_y = ema_train_preds[train_idx], train_y[train_idx]\n    valid_fold_preds, valid_fold_y = ema_train_preds[valid_idx], train_y[valid_idx]\n    train_scores.append(get_scores(train_fold_y, train_fold_preds))\n    valid_scores.append(get_scores(valid_fold_y, valid_fold_preds))\n    \n    for i, target_name in enumerate(target_names):\n        optimizer = OptimizedRounder()\n        optimizer.fit(train_y[train_idx, i], ema_train_preds[train_idx, i])\n        train_score = optimizer.score(train_y[train_idx, i], ema_train_preds[train_idx, i])\n        valid_score = optimizer.score(train_y[valid_idx, i], ema_train_preds[valid_idx, i])\n        train_optimized_scores[target_name].append(train_score)\n        valid_optimized_scores[target_name].append(valid_score)\n        thresholds[target_name].append(optimizer.threshold)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = pd.DataFrame(valid_optimized_scores).mean().loc[ema_cv_scores['target_name']].reset_index()\nscores.columns = ['target_name', 'score']\nfig = px.bar(scores, x='score', y='target_name', orientation='h')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_score = pd.DataFrame(train_scores).values.mean()\nvalid_score = pd.DataFrame(valid_scores).values.mean()\ntrain_optimized_score = np.mean(list(train_optimized_scores.values()))\nvalid_optimized_score = np.mean(list(valid_optimized_scores.values()))\nprint(f'train score: {train_score:.4f} -> {train_optimized_score:.4f}')\nprint(f'valid score: {valid_score:.4f} -> {valid_optimized_score:.4f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, target_name in enumerate(target_names):\n    optimizer = OptimizedRounder()\n    optimizer.threshold = np.mean(thresholds[target_name], axis=0)\n    ema_test_preds[:, i] = optimizer.transform(ema_test_preds[:, i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv(SAMPLE_SUBMISSION_PATH)\nsubmission[target_names] = ema_test_preds\nsubmission.to_csv('submission.csv', index=False)\nprint(f'all processes done in {(time.time() - start_time) / 60:.2f} min.')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}