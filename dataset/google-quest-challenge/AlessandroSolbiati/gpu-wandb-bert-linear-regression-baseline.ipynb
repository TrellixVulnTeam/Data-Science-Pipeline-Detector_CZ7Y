{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.environ[\"WANDB_API_KEY\"] = \"fd2bb4031eda4c72558d6d4b140e340de466e148\"\nimport wandb\nwandb.init(project=\"google-quest-q-a-labelling\", config={'BERT_features': 'last_hidden_CLS'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wandb.log({'test': 0})","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"TRAIN_PATH = \"/kaggle/input/google-quest-challenge/train.csv\"\n\nTEST_PATH = \"/kaggle/input/google-quest-challenge/test.csv\"\n\nSAMPLE_SUBMISSION_PATH = \"/kaggle/input/google-quest-challenge/sample_submission.csv\"\n\nSUBMISSION_PATH = \"submission.csv\"\n\nMODEL_TYPE = 'bert-base-uncased'\nBERT_BASE_UNCASED_LOCATION = \"/kaggle/input/bert-base-uncased\"\nBERT_BASE_UNCASED_TOKENIZER_LOCATION = \"/kaggle/input/bert-base-uncased/vocab.txt\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\ntest_data = pd.read_csv(TEST_PATH)\ntrain_data = pd.read_csv(TRAIN_PATH)\nsample_submission_data = pd.read_csv(SAMPLE_SUBMISSION_PATH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction_columns = [col for col in train_data.columns if col not in test_data]\ninput_columns = [col for col in test_data.columns]\nprint(prediction_columns)\nprint(input_columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X, y = train_data[input_columns], train_data[prediction_columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TODO: DELETE!\n# X = X.iloc[:101,:]\n# y = y.iloc[:101,:]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_full = pd.concat([train_data[input_columns], test_data[input_columns]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  BERT features\nfrom transformers import BertTokenizer, BertModel\n\nmodel = BertModel.from_pretrained(BERT_BASE_UNCASED_LOCATION)\nmodel.to('cuda')\ntokenizer = BertTokenizer.from_pretrained(BERT_BASE_UNCASED_LOCATION)\n\nBERT_columns = ['question_title', 'question_body', 'answer']\nBERT_max_sentence_size = 512\nBERT_embedding_size = 768\n\ndef extract_BERT_last_hidden_CLS(outputs):\n    return outputs[0][0][0].cpu().detach().numpy()\n\ndef BERT_tokens(string):\n    tokens = tokenizer(string, return_tensors=\"pt\")\n    for k in tokens.keys():\n        tokens[k] = tokens[k].to('cuda')\n    return tokens\n\n\ndef get_BERT_features(string):\n    inputs = BERT_tokens(string)\n    outputs = model(**inputs)\n    return extract_BERT_last_hidden_CLS(outputs)\n\ndef BERT_features_e2e(X, trained_pca=None):\n    from tqdm import tqdm\n    _BERT_features = []\n    for col in BERT_columns:\n        features = []\n        for idx in tqdm(range(len(X[col]))):\n            row = X[col][idx]\n            features.append(get_BERT_features(row[:BERT_max_sentence_size]))\n        _BERT_features.append(pd.DataFrame(features))\n    BERT_features = pd.concat(_BERT_features, axis=1)\n\n    _reduced_BERT_features = []\n\n    from sklearn.decomposition import PCA\n    pca = PCA(n_components=70)\n\n    for i, col in enumerate(BERT_columns):\n        pca_input_features = BERT_features.iloc[:, i*BERT_embedding_size:(i+1)*BERT_embedding_size]\n        if not trained_pca:\n            trained_pca = pca.fit(pca_input_features)\n        features = trained_pca.transform(pca_input_features)\n        _reduced_BERT_features.append(pd.DataFrame(features))\n\n    reduced_BERT_features = pd.concat(_reduced_BERT_features, axis=1)\n    return reduced_BERT_features, trained_pca\n\n# reduced_BERT_features = BERT_features_e2e(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#. non-BERT features\n\ndef non_BERT_features_e2e(X, X_full):\n\n    from sklearn import preprocessing\n\n    host_category_encoder = preprocessing.OneHotEncoder(drop='first').fit(X_full[['host', 'category']])\n\n    host_category_encoding = host_category_encoder.transform(X[['host', 'category']]).toarray()\n\n    assert host_category_encoding.shape[1] == len(X_full['host'].unique()) + len(X_full['category'].unique()) - 2\n    assert host_category_encoding.shape[0] == len(X['host'])\n\n    X_non_BERT_features = pd.concat([pd.DataFrame(host_category_encoding), X['question_title'].map(len), X['question_body'].map(len), X['answer'].map(len)], axis=1)\n\n    non_embedding_non_BERT_features = X_non_BERT_features.iloc[:,-3:]\n\n    return non_embedding_non_BERT_features\n\n# non_embedding_non_BERT_features = non_BERT_features_e2e(X, X_full)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def features_e2e(X, X_full, pca=None):\n    BERT_features, pca = BERT_features_e2e(X, pca)\n    return pd.concat([non_BERT_features_e2e(X, X_full), BERT_features], axis=1), pca","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train test split\n\nX, pca = features_e2e(X, X_full)\n\nassert X.shape[0] == y.shape[0]\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y)\n\nassert len(X) == len(X_train) + len(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model\n\nfrom sklearn.linear_model import LinearRegression\n\nlinear_regression = LinearRegression().fit(X_train, y_train)\n\ny_pred = linear_regression.predict(X_test)\ny_pred = pd.DataFrame(y_pred)\n\n\n\nassert len(y_test) == len(y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    wandb.sklearn.plot_regressor(linear_regression, X_train, X_test, y_train, y_test, 'LinearRegression')\nexcept ValueError:\n    pass\n#help(wandb.sklearn.plot_regressor)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# evaluation\nfrom matplotlib import pyplot as plt\nimport math\nfrom scipy import stats\n\ndef evaluate(y_test, y_pred):\n    assert y_test.shape == y_pred.shape\n    correlations = [stats.spearmanr(y_test.iloc[:, col_index], y_pred.iloc[:, col_index]).correlation for col_index in range(y_test.shape[1])]\n    score = sum([*filter(lambda x: not math.isnan(x), correlations)]) / len([*filter(lambda x: not math.isnan(x), correlations)])\n    return score, correlations\n\n\nvalidation_score, correlations = evaluate(y_test, y_pred)\nprint(\"score = {}\".format(validation_score))\nwandb.log({'validation_score': validation_score})\n\n\ndef visualise_correlations(correlations):\n    worst_columns = [prediction_columns[correlations.index(corr)] for corr in sorted(correlations)[:3]]\n    top_columns = [prediction_columns[correlations.index(corr)] for corr in sorted(correlations)[-3:]]\n    print(\"worst predicted columns are: \" + ', '.join(worst_columns))\n    print(\"top predicted columns are:  \" + ',  '.join(top_columns))\n    plt.plot(correlations, marker='o')\n    plt.ylim(0, 1)\n    plt.title(\"Correlation score for each column\", c=\"w\")\n    plt.show()\n    \nvisualise_correlations(correlations)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"#model.eval()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_X = test_data[input_columns]\n\n# DELETE\n# test_X = test_X.iloc[:101,:]\n\ntest_X, _ = features_e2e(test_X, X_full, pca)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predict\ny_pred = linear_regression.predict(test_X)\ny_pred = pd.DataFrame(y_pred)\n\ny_pred = y_pred.rename(columns={index: column_name for index, column_name in enumerate(sample_submission_data.columns[1:])})\n\ny_pred.insert(0, 'qa_id', test_data['qa_id'])\n\nfor col in y_pred.columns[1:]:\n    y_pred[col] = y_pred[col].apply(lambda x: round(x, 3))\n    y_pred[col] = y_pred[col].apply(lambda x: x if x < 1 else 0.999)\n    y_pred[col] = y_pred[col].apply(lambda x: x if x > 0 else 0.001)\n\n    \n\nassert y_pred.shape == sample_submission_data.shape\nassert len(y_pred.columns) == len(sample_submission_data.columns)\nassert all([y_pred.columns[i] == sample_submission_data.columns[i] for i, _ in enumerate(y_pred.columns)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred.to_csv(SUBMISSION_PATH, index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}