{"cells":[{"metadata":{},"cell_type":"markdown","source":"* # <a id='1'>1. Load libraries</a>","execution_count":null},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"import pandas as pd \nimport numpy as np \nimport matplotlib\nimport matplotlib.pyplot as plt \nimport seaborn as sns \ncolor = sns.color_palette()\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.offline as offline\noffline.init_notebook_mode()\nimport cufflinks as cf\ncf.go_offline()\n\nimport re\nimport nltk\nfrom nltk.probability import FreqDist\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nimport string\neng_stopwords = stopwords.words('english')\nimport gc\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"import os\nprint(os.listdir(\"../input/google-quest-challenge\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id='2-2'>2 Reading Data</a>","execution_count":null},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"print('Reading data...')\ntrain_data = pd.read_csv('../input/google-quest-challenge/train.csv')\ntest_data = pd.read_csv('../input/google-quest-challenge/test.csv')\nsample_submission = pd.read_csv('../input/google-quest-challenge/sample_submission.csv')\nprint('Reading data completed')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"print('Size of train_data', train_data.shape)\nprint('Size of test_data', test_data.shape)\nprint('Size of sample_submission', sample_submission.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id='3'>3. How Data Looks</a>","execution_count":null},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**train_data columns**","execution_count":null},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"train_data.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**test_data**","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"test_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**test_data columns**","execution_count":null},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"test_data.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**sample_submission**","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"sample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Target variables**","execution_count":null},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"targets = list(sample_submission.columns[1:])\ntargets","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ###  3.2 Summary of the target variables</a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data[targets].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id='4'> 4 Missing data</a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**checking missing data in train_data **","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# checking missing data\ntotal = train_data.isnull().sum().sort_values(ascending = False)\npercent = (train_data.isnull().sum()/train_data.isnull().count()*100).sort_values(ascending = False)\nmissing_train_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_train_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**checking missing data in test_data **","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# checking missing data\ntotal = test_data.isnull().sum().sort_values(ascending = False)\npercent = (test_data.isnull().sum()/test_data.isnull().count()*100).sort_values(ascending = False)\nmissing_test_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_test_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No missing data found","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# <a id='5'>5. Data Exploration and Insights</a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# <a id='5-1'>5.1 Distribution of websites from which Question & Answers collected)</a>","execution_count":null},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"temp = train_data[\"host\"].value_counts()\ndf = pd.DataFrame({'labels': temp.index,\n                   'values': temp.values\n                  })\ndf.iplot(kind='pie',labels='labels',values='values', title='Distribution of hosts in Training data')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"temp = test_data[\"host\"].value_counts()\ndf = pd.DataFrame({'labels': temp.index,\n                   'values': temp.values\n                  })\ndf.iplot(kind='pie',labels='labels',values='values', title='Distribution of hosts in test data')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id='5-2'>5.2 Distribution of categories</a>","execution_count":null},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"temp = train_data[\"category\"].value_counts()\n#print(\"Total number of states : \",len(temp))\ntrace = go.Bar(\n    x = temp.index,\n    y = (temp / temp.sum())*100,\n)\ndata = [trace]\nlayout = go.Layout(\n    title = \"Distribution of categories in training data in % \",\n    xaxis=dict(\n        title='category',\n        tickfont=dict(\n            size=14,\n            color='rgb(107, 107, 107)'\n        )\n    ),\n    yaxis=dict(\n        title='Count in %',\n        titlefont=dict(\n            size=16,\n            color='rgb(107, 107, 107)'\n        ),\n        tickfont=dict(\n            size=14,\n            color='rgb(107, 107, 107)'\n        )\n)\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='test')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"temp = test_data[\"category\"].value_counts()\n#print(\"Total number of states : \",len(temp))\ntrace = go.Bar(\n    x = temp.index,\n    y = (temp / temp.sum())*100,\n)\ndata = [trace]\nlayout = go.Layout(\n    title = \"Distribution of categories in test data in % \",\n    xaxis=dict(\n        title='category',\n        tickfont=dict(\n            size=14,\n            color='rgb(107, 107, 107)'\n        )\n    ),\n    yaxis=dict(\n        title='Count in %',\n        titlefont=dict(\n            size=16,\n            color='rgb(107, 107, 107)'\n        ),\n        tickfont=dict(\n            size=14,\n            color='rgb(107, 107, 107)'\n        )\n)\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='test')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id='5-3'>5.3 Distribution of Target variables</a>","execution_count":null},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(6, 5, figsize=(18, 15))\naxes = axes.ravel()\nbins = np.linspace(0, 1, 20)\n\nfor i, col in enumerate(targets):\n    ax = axes[i]\n    sns.distplot(train_data[col], label=col, kde=False, bins=bins, ax=ax)\n    # ax.set_title(col)\n    ax.set_xlim([0, 1])\n    ax.set_ylim([0, 6079])\nplt.tight_layout()\nplt.show()\nplt.close()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id='5-5'>5.4 Distribution for Question Title</a>","execution_count":null},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"train_question_title=train_data['question_title'].str.len()\ntest_question_title=test_data['question_title'].str.len()\nfig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,6))\nsns.distplot(train_question_title,ax=ax1,color='blue')\nsns.distplot(test_question_title,ax=ax2,color='green')\nax2.set_title('Distribution for Question Title in test data')\nax1.set_title('Distribution for Question Title in Training data')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id='5-6'>5.5 Distribution for Question body</a>","execution_count":null},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"train_question_title=train_data['question_body'].str.len()\ntest_question_title=test_data['question_body'].str.len()\nfig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,6))\nsns.distplot(train_question_title,ax=ax1,color='blue')\nsns.distplot(test_question_title,ax=ax2,color='green')\nax2.set_title('Distribution for Question Body in test data')\nax1.set_title('Distribution for Question Body in Training data')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id='5-7'>5.6 Distribution for Answers</a>","execution_count":null},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"train_question_title=train_data['answer'].str.len()\ntest_question_title=test_data['answer'].str.len()\nfig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,6))\nsns.distplot(train_question_title,ax=ax1,color='blue')\nsns.distplot(test_question_title,ax=ax2,color='green')\nax2.set_title('Distribution for Answers in test data')\nax1.set_title('Distribution for Answers in Training data')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Most popular questions**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## <a id='6-3'>5.7 Word Cloud Visualization</a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\n\n\ndef plot_wordcloud(text, ax, title=None):\n    wordcloud = WordCloud(max_font_size=None, background_color='white',\n                          width=1200, height=1000).generate(text_cat)\n    ax.imshow(wordcloud)\n    if title is not None:\n        ax.set_title(title)\n    ax.axis(\"off\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Training data Word Cloud')\n\nfig, axes = plt.subplots(1, 3, figsize=(16, 18))\n\ntext_cat = ' '.join(train_data['question_title'].values)\nplot_wordcloud(text_cat, axes[0], 'Question title')\n\ntext_cat = ' '.join(train_data['question_body'].values)\nplot_wordcloud(text_cat, axes[1], 'Question body')\n\ntext_cat = ' '.join(train_data['answer'].values)\nplot_wordcloud(text_cat, axes[2], 'Answer')\n\nplt.tight_layout()\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Test data Word Cloud')\n\nfig, axes = plt.subplots(1, 3, figsize=(16, 18))\n\ntext_cat = ' '.join(test_data['question_title'].values)\nplot_wordcloud(text_cat, axes[0], 'Question title')\n\ntext_cat = ' '.join(test_data['question_body'].values)\nplot_wordcloud(text_cat, axes[1], 'Question body')\n\ntext_cat = ' '.join(test_data['answer'].values)\nplot_wordcloud(text_cat, axes[2], 'Answer')\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id='6-3'>5.8 Correlation analysis</a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(18, 18))\nsns.heatmap(train_data[targets].corr(), ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id='6'>6. Data Preparation & Feature Engineering</a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## <a id='6-1'>6.1 Data cleaning</a>","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"#https://www.kaggle.com/urvishp80/quest-encoding-ensemble\nprint(\"Data cleaning started........\")\npuncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£',\n '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', '\\xa0', '\\t',\n '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '\\u3000', '\\u202f',\n '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '«',\n '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\nmispell_dict = {\"aren't\" : \"are not\",\n\"can't\" : \"cannot\",\n\"couldn't\" : \"could not\",\n\"couldnt\" : \"could not\",\n\"didn't\" : \"did not\",\n\"doesn't\" : \"does not\",\n\"doesnt\" : \"does not\",\n\"don't\" : \"do not\",\n\"hadn't\" : \"had not\",\n\"hasn't\" : \"has not\",\n\"haven't\" : \"have not\",\n\"havent\" : \"have not\",\n\"he'd\" : \"he would\",\n\"he'll\" : \"he will\",\n\"he's\" : \"he is\",\n\"i'd\" : \"I would\",\n\"i'd\" : \"I had\",\n\"i'll\" : \"I will\",\n\"i'm\" : \"I am\",\n\"isn't\" : \"is not\",\n\"it's\" : \"it is\",\n\"it'll\":\"it will\",\n\"i've\" : \"I have\",\n\"let's\" : \"let us\",\n\"mightn't\" : \"might not\",\n\"mustn't\" : \"must not\",\n\"shan't\" : \"shall not\",\n\"she'd\" : \"she would\",\n\"she'll\" : \"she will\",\n\"she's\" : \"she is\",\n\"shouldn't\" : \"should not\",\n\"shouldnt\" : \"should not\",\n\"that's\" : \"that is\",\n\"thats\" : \"that is\",\n\"there's\" : \"there is\",\n\"theres\" : \"there is\",\n\"they'd\" : \"they would\",\n\"they'll\" : \"they will\",\n\"they're\" : \"they are\",\n\"theyre\":  \"they are\",\n\"they've\" : \"they have\",\n\"we'd\" : \"we would\",\n\"we're\" : \"we are\",\n\"weren't\" : \"were not\",\n\"we've\" : \"we have\",\n\"what'll\" : \"what will\",\n\"what're\" : \"what are\",\n\"what's\" : \"what is\",\n\"what've\" : \"what have\",\n\"where's\" : \"where is\",\n\"who'd\" : \"who would\",\n\"who'll\" : \"who will\",\n\"who're\" : \"who are\",\n\"who's\" : \"who is\",\n\"who've\" : \"who have\",\n\"won't\" : \"will not\",\n\"wouldn't\" : \"would not\",\n\"you'd\" : \"you would\",\n\"you'll\" : \"you will\",\n\"you're\" : \"you are\",\n\"you've\" : \"you have\",\n\"'re\": \" are\",\n\"wasn't\": \"was not\",\n\"we'll\":\" will\",\n\"didn't\": \"did not\",\n\"tryin'\":\"trying\"}\n\n\ndef clean_text(text):\n    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n    text = text.lower().split()\n    stops = set(stopwords.words(\"english\"))\n    text = [w for w in text if not w in stops]    \n    text = \" \".join(text)\n    return(text)\n\ndef _get_mispell(mispell_dict):\n    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n    return mispell_dict, mispell_re\n\ndef replace_typical_misspell(text):\n    mispellings, mispellings_re = _get_mispell(mispell_dict)\n\n    def replace(match):\n        return mispellings[match.group(0)]\n\n    return mispellings_re.sub(replace, text)\n\ndef clean_data(df, columns: list):\n    for col in columns:\n        df[col] = df[col].apply(lambda x: clean_text(x.lower()))\n        df[col] = df[col].apply(lambda x: replace_typical_misspell(x))\n\n    return df","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"columns = ['question_title','question_body','answer']\ntrain_data = clean_data(train_data, columns)\ntest_data = clean_data(test_data, columns)\nprint(\"Data cleaning Done........\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id='6-2'>6.2 Word frequency</a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# training data\nfreq_dist = FreqDist([word for text in train_data['question_title'].str.replace('[^a-za-z0-9^,!.\\/+-=]',' ') for word in text.split()])\nplt.figure(figsize=(20, 7))\nplt.title('Word frequency on question title (Training Data)').set_fontsize(25)\nplt.xlabel('').set_fontsize(25)\nplt.ylabel('').set_fontsize(25)\nfreq_dist.plot(60,cumulative=False)\nplt.show()\n\n# test data\nfreq_dist = FreqDist([word for text in test_data['question_title'] for word in text.split()])\nplt.figure(figsize=(20, 7))\nplt.title('Word frequency on question title (Test Data)').set_fontsize(25)\nplt.xlabel('').set_fontsize(25)\nplt.ylabel('').set_fontsize(25)\nfreq_dist.plot(60,cumulative=False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# training data\nfreq_dist = FreqDist([word for text in train_data['question_body'].str.replace('[^a-za-z0-9^,!.\\/+-=]',' ') for word in text.split()])\nplt.figure(figsize=(20, 7))\nplt.title('Word frequency on question body (Training Data)').set_fontsize(25)\nplt.xlabel('').set_fontsize(25)\nplt.ylabel('').set_fontsize(25)\nfreq_dist.plot(60,cumulative=False)\nplt.show()\n\n# test data\nfreq_dist = FreqDist([word for text in test_data['question_body'] for word in text.split()])\nplt.figure(figsize=(20, 7))\nplt.title('Word frequency on question body (Test Data)').set_fontsize(25)\nplt.xlabel('').set_fontsize(25)\nplt.ylabel('').set_fontsize(25)\nfreq_dist.plot(60,cumulative=False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# training data\nfreq_dist = FreqDist([word for text in train_data['question_title'] for word in text.split()])\nplt.figure(figsize=(20, 7))\nplt.title('Word frequency on question title (Training Data)').set_fontsize(25)\nplt.xlabel('').set_fontsize(25)\nplt.ylabel('').set_fontsize(25)\nfreq_dist.plot(60,cumulative=False)\nplt.show()\n\n# test data\nfreq_dist = FreqDist([word for text in test_data['question_title'] for word in text.split()])\nplt.figure(figsize=(20, 7))\nplt.title('Word frequency on question title (Test Data)').set_fontsize(25)\nplt.xlabel('').set_fontsize(25)\nplt.ylabel('').set_fontsize(25)\nfreq_dist.plot(60,cumulative=False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id='6-3'>6.3 Feature Engineering</a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### <a id='6-3-1'>6.3.1 Text based features</a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Text based features are :\n * Number of characters in the question_title\n * Number of characters in the question_body\n * Number of characters in the answer\n * Number of words in the question_title\n * Number of words in the question_body\n * Number of words in the answer\n * Number of unique words in the question_title\n * Number of unique words in the question_body\n * Number of unique words in the answer","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"# Number of characters in the text\ntrain_data[\"question_title_num_chars\"] = train_data[\"question_title\"].apply(lambda x: len(str(x)))\ntrain_data[\"question_body_num_chars\"] = train_data[\"question_body\"].apply(lambda x: len(str(x)))\ntrain_data[\"answer_num_chars\"] = train_data[\"answer\"].apply(lambda x: len(str(x)))\n\ntest_data[\"question_title_num_chars\"] = test_data[\"question_title\"].apply(lambda x: len(str(x)))\ntest_data[\"question_body_num_chars\"] = test_data[\"question_body\"].apply(lambda x: len(str(x)))\ntest_data[\"answer_num_chars\"] = test_data[\"answer\"].apply(lambda x: len(str(x)))\n\n# Number of words in the text\ntrain_data[\"question_title_num_words\"] = train_data[\"question_title\"].apply(lambda x: len(str(x).split()))\ntrain_data[\"question_body_num_words\"] = train_data[\"question_body\"].apply(lambda x: len(str(x).split()))\ntrain_data[\"answer_num_words\"] = train_data[\"answer\"].apply(lambda x: len(str(x).split()))\n\ntest_data[\"question_title_num_words\"] = test_data[\"question_title\"].apply(lambda x: len(str(x).split()))\ntest_data[\"question_body_num_words\"] = test_data[\"question_body\"].apply(lambda x: len(str(x).split()))\ntest_data[\"answer_num_words\"] = test_data[\"answer\"].apply(lambda x: len(str(x).split()))\n\n# Number of unique words in the text\ntrain_data[\"question_title_num_unique_words\"] = train_data[\"question_title\"].apply(lambda x: len(set(str(x).split())))\ntrain_data[\"question_body_num_unique_words\"] = train_data[\"question_body\"].apply(lambda x: len(set(str(x).split())))\ntrain_data[\"answer_num_unique_words\"] = train_data[\"answer\"].apply(lambda x: len(set(str(x).split())))\n\ntest_data[\"question_title_num_unique_words\"] = test_data[\"question_title\"].apply(lambda x: len(set(str(x).split())))\ntest_data[\"question_body_num_unique_words\"] = test_data[\"question_body\"].apply(lambda x: len(set(str(x).split())))\ntest_data[\"answer_num_unique_words\"] = test_data[\"answer\"].apply(lambda x: len(set(str(x).split())))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <a id='6-3-2'>6.3.2 TF-IDF Features</a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### TF-IDF :\n  *  Term Frequency (TF) and Inverse Document Frequency (IDF)\n  *  TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)\n  *  IDF(t) = log_e(Total number of documents / Number of documents with term t in it)\n  \n**TF-IDF based features are :**\n\n* Character Level N-Gram TF-IDF of question_title\n* Character Level N-Gram TF-IDF of question_body\n* Character Level N-Gram TF-IDF of answer\n* Word Level N-Gram TF-IDF of question_title\n* Word Level N-Gram TF-IDF of question_body\n* Word Level N-Gram TF-IDF of answer","execution_count":null},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"tfidf = TfidfVectorizer(ngram_range=(1, 3))\ntsvd = TruncatedSVD(n_components = 128, n_iter=5)\ntfquestion_title = tfidf.fit_transform(train_data[\"question_title\"].values)\ntfquestion_title_test = tfidf.transform(test_data[\"question_title\"].values)\ntfquestion_title = tsvd.fit_transform(tfquestion_title)\ntfquestion_title_test = tsvd.transform(tfquestion_title_test)\n\ntfquestion_body = tfidf.fit_transform(train_data[\"question_body\"].values)\ntfquestion_body_test = tfidf.transform(test_data[\"question_body\"].values)\ntfquestion_body = tsvd.fit_transform(tfquestion_body)\ntfquestion_body_test = tsvd.transform(tfquestion_body_test)\n\ntfanswer = tfidf.fit_transform(train_data[\"answer\"].values)\ntfanswer_test = tfidf.transform(test_data[\"answer\"].values)\ntfanswer = tsvd.fit_transform(tfanswer)\ntfanswer_test = tsvd.transform(tfanswer_test)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"train_data[\"tfquestion_title\"] = list(tfquestion_title)\ntest_data[\"tfquestion_title_test\"] = list(tfquestion_title_test)\n\ntrain_data[\"tfquestion_body\"] = list(tfquestion_body)\ntest_data[\"tfquestion_body_test\"] = list(tfquestion_body_test)\n\ntrain_data[\"tfanswer\"] = list(tfanswer)\ntest_data[\"tfanswer_test\"] = list(tfanswer_test)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}