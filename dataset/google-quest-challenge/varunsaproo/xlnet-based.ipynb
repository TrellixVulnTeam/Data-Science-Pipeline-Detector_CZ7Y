{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport transformers\nimport tensorflow as tf\nfrom scipy.stats import spearmanr\nimport tensorflow_hub as hub\nimport re\nfrom sklearn.preprocessing import MinMaxScaler\nimport gc\nfrom sklearn.model_selection import GroupKFold,KFold\nfrom scipy.stats import spearmanr, rankdata\nfrom sklearn.linear_model import MultiTaskElasticNet\nimport string\nfrom collections import Counter\nfrom gensim.models import Word2Vec\nfrom transformers import XLNetConfig, TFXLNetModel, XLNetTokenizer, TFXLNetMainLayer, BertConfig, TFBertMainLayer, BertTokenizer, TFBertModel\nimport lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DIR = '/kaggle/input/google-quest-challenge'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xlnet_tokenizer = XLNetTokenizer.from_pretrained('/kaggle/input/xlnetbasecased/tokenizer')\nbert_tokenizer = BertTokenizer.from_pretrained('/kaggle/input/bertbaseuncasedcomplete/bert-model')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(DIR+'/train.csv')\ntest_df = pd.read_csv(DIR+'/test.csv')\ncols = train_df.columns[11:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokenize_input(tokenizer, s1, s2, tags, data_name, max_length, tokenizer_name = 'bert'):\n  if s2 is not None:\n    x = tokenizer.encode_plus(s1, s2, pad_to_max_length=False)\n    if len(x['input_ids']) > max_length:\n      segment_1 = int(0.25*max_length)\n      x['input_ids'] = x['input_ids'][:segment_1] + x['input_ids'][-(max_length-segment_1):]\n      x['attention_mask'] = x['attention_mask'][:segment_1] + x['attention_mask'][-(max_length-segment_1):]\n      x['token_type_ids'] = x['token_type_ids'][:segment_1] + x['token_type_ids'][-(max_length-segment_1):]\n    else:\n      diff = max_length - len(x['input_ids'])\n      if tokenizer_name == 'xlnet':\n        x['input_ids'] = [tokenizer.pad_token_id]*diff + x['input_ids']\n        x['attention_mask'] = [0]*diff + x['attention_mask']\n        x['token_type_ids'] = [tokenizer.pad_token_type_id]*diff + x['token_type_ids']\n      else:\n        x['input_ids'] = x['input_ids'] + [tokenizer.pad_token_id]*diff\n        x['attention_mask'] = x['attention_mask'] + [0]*diff\n        x['token_type_ids'] = x['token_type_ids'] + [0]*diff\n      \n  else:\n    x = tokenizer.encode_plus(s1)\n    if len(x['input_ids']) > max_length:\n      segment_1 = int(0.25*max_length)\n      x['input_ids'] = x['input_ids'][:segment_1] + x['input_ids'][-(max_length-segment_1):]\n      x['attention_mask'] = x['attention_mask'][:segment_1] + x['attention_mask'][-(max_length-segment_1):]\n      x['token_type_ids'] = x['token_type_ids'][:segment_1] + x['token_type_ids'][-(max_length-segment_1):]\n    else:\n      diff = max_length - len(x['input_ids'])\n      if tokenizer_name == 'xlnet':\n        x['input_ids'] = [tokenizer.pad_token_id]*diff + x['input_ids']\n        x['attention_mask'] = [0]*diff + x['attention_mask']\n        x['token_type_ids'] = [tokenizer.pad_token_type_id]*diff + x['token_type_ids']\n      else:\n        x['input_ids'] = x['input_ids'] + [tokenizer.pad_token_id]*diff\n        x['attention_mask'] = x['attention_mask'] + [0]*diff\n        x['token_type_ids'] = x['token_type_ids'] + [tokenizer.pad_token_type_id]*diff\n  \n  data[data_name][tags[0]].append(x['input_ids']) \n  data[data_name][tags[1]].append(x['token_type_ids'])\n  data[data_name][tags[2]].append(x['attention_mask']) \n\ndata = {}\n# ******************************************XLNET*************************************************************************************\ndata['xlnet_train_t_a'] = {}\ndata['xlnet_train_q_a'] = {}\ndata['xlnet_train_t_q'] = {}\ndata['xlnet_train_q'] = {}\ndata['xlnet_train_a'] = {}\n\ndata['xlnet_test_t_a'] = {}\ndata['xlnet_test_q_a'] = {}\ndata['xlnet_test_t_q'] = {}\ndata['xlnet_test_q'] = {}\ndata['xlnet_test_a'] = {}\n\ntags = ['input_ids', 'token_type_ids', 'attention_masks']\ndata['xlnet_train_t_a'][tags[0]], data['xlnet_train_t_a'][tags[1]], data['xlnet_train_t_a'][tags[2]] = [], [], []\ndata['xlnet_train_q_a'][tags[0]], data['xlnet_train_q_a'][tags[1]], data['xlnet_train_q_a'][tags[2]] = [], [], []\ndata['xlnet_train_t_q'][tags[0]], data['xlnet_train_t_q'][tags[1]], data['xlnet_train_t_q'][tags[2]] = [], [], []\ndata['xlnet_train_q'][tags[0]], data['xlnet_train_q'][tags[1]], data['xlnet_train_q'][tags[2]] = [], [], []\ndata['xlnet_train_a'][tags[0]], data['xlnet_train_a'][tags[1]], data['xlnet_train_a'][tags[2]] = [], [], []\n\n\ndata['xlnet_test_t_a'][tags[0]], data['xlnet_test_t_a'][tags[1]], data['xlnet_test_t_a'][tags[2]] = [], [], []\ndata['xlnet_test_q_a'][tags[0]], data['xlnet_test_q_a'][tags[1]], data['xlnet_test_q_a'][tags[2]] = [], [], []\ndata['xlnet_test_t_q'][tags[0]], data['xlnet_test_t_q'][tags[1]], data['xlnet_test_t_q'][tags[2]] = [], [], []\ndata['xlnet_test_q'][tags[0]], data['xlnet_test_q'][tags[1]], data['xlnet_test_q'][tags[2]] = [], [], []\ndata['xlnet_test_a'][tags[0]], data['xlnet_test_a'][tags[1]], data['xlnet_test_a'][tags[2]] = [], [], []\n\n\nfor i in range(train_df.shape[0]):\n  tokenize_input(xlnet_tokenizer, train_df.loc[i, 'question_title'], train_df.loc[i, 'answer'], tags, 'xlnet_train_t_a', 512, 'xlnet')\n  tokenize_input(xlnet_tokenizer, train_df.loc[i, 'question_body'], train_df.loc[i, 'answer'], tags, 'xlnet_train_q_a', 512, 'xlnet')\n  tokenize_input(xlnet_tokenizer, train_df.loc[i, 'question_title'], train_df.loc[i, 'question_body'], tags, 'xlnet_train_t_q', 512, 'xlnet')\n  tokenize_input(xlnet_tokenizer, train_df.loc[i, 'question_body'], None, tags, 'xlnet_train_q', 512, 'xlnet')\n  tokenize_input(xlnet_tokenizer, train_df.loc[i, 'answer'], None, tags, 'xlnet_train_a', 512, 'xlnet')\nfor i in range(test_df.shape[0]):\n  tokenize_input(xlnet_tokenizer, test_df.loc[i, 'question_title'], test_df.loc[i, 'answer'], tags, 'xlnet_test_t_a', 512, 'xlnet')\n  tokenize_input(xlnet_tokenizer, test_df.loc[i, 'question_body'], test_df.loc[i, 'answer'], tags, 'xlnet_test_q_a', 512, 'xlnet')\n  tokenize_input(xlnet_tokenizer, test_df.loc[i, 'question_title'], test_df.loc[i, 'question_body'], tags, 'xlnet_test_t_q', 512, 'xlnet')\n  tokenize_input(xlnet_tokenizer, test_df.loc[i, 'question_body'], None, tags, 'xlnet_test_q', 512, 'xlnet')\n  tokenize_input(xlnet_tokenizer, test_df.loc[i, 'answer'], None, tags, 'xlnet_test_a', 512, 'xlnet')\n\n# ******************************************BERT*************************************************************************************\n\ndata['bert_train_t_a'] = {}\ndata['bert_train_q_a'] = {}\ndata['bert_train_t_q'] = {}\ndata['bert_train_q'] = {}\ndata['bert_train_a'] = {}\n\ndata['bert_test_t_a'] = {}\ndata['bert_test_q_a'] = {}\ndata['bert_test_t_q'] = {}\ndata['bert_test_q'] = {}\ndata['bert_test_a'] = {}\n\ndata['bert_train_t_a'][tags[0]], data['bert_train_t_a'][tags[1]], data['bert_train_t_a'][tags[2]] = [], [], []\ndata['bert_train_q_a'][tags[0]], data['bert_train_q_a'][tags[1]], data['bert_train_q_a'][tags[2]] = [], [], []\ndata['bert_train_t_q'][tags[0]], data['bert_train_t_q'][tags[1]], data['bert_train_t_q'][tags[2]] = [], [], []\ndata['bert_train_q'][tags[0]], data['bert_train_q'][tags[1]], data['bert_train_q'][tags[2]] = [], [], []\ndata['bert_train_a'][tags[0]], data['bert_train_a'][tags[1]], data['bert_train_a'][tags[2]] = [], [], []\n\ndata['bert_test_t_a'][tags[0]], data['bert_test_t_a'][tags[1]], data['bert_test_t_a'][tags[2]] = [], [], []\ndata['bert_test_q_a'][tags[0]], data['bert_test_q_a'][tags[1]], data['bert_test_q_a'][tags[2]] = [], [], []\ndata['bert_test_t_q'][tags[0]], data['bert_test_t_q'][tags[1]], data['bert_test_t_q'][tags[2]] = [], [], []\ndata['bert_test_q'][tags[0]], data['bert_test_q'][tags[1]], data['bert_test_q'][tags[2]] = [], [], []\ndata['bert_test_a'][tags[0]], data['bert_test_a'][tags[1]], data['bert_test_a'][tags[2]] = [], [], []\n\nfor i in range(train_df.shape[0]):\n  tokenize_input(bert_tokenizer, train_df.loc[i, 'question_title'], train_df.loc[i, 'answer'], tags, 'bert_train_t_a', 512, 'bert')\n  tokenize_input(bert_tokenizer, train_df.loc[i, 'question_body'], train_df.loc[i, 'answer'], tags, 'bert_train_q_a', 512, 'bert')\n  tokenize_input(bert_tokenizer, train_df.loc[i, 'question_title'], train_df.loc[i, 'question_body'], tags, 'bert_train_t_q', 512, 'bert')\n  tokenize_input(bert_tokenizer, train_df.loc[i, 'question_body'], None, tags, 'bert_train_q', 512, 'bert')\n  tokenize_input(bert_tokenizer, train_df.loc[i, 'answer'], None, tags, 'bert_train_a', 512, 'bert')\nfor i in range(test_df.shape[0]):\n  tokenize_input(bert_tokenizer, test_df.loc[i, 'question_title'], test_df.loc[i, 'answer'], tags, 'bert_test_t_a', 512, 'bert')\n  tokenize_input(bert_tokenizer, test_df.loc[i, 'question_body'], test_df.loc[i, 'answer'], tags, 'bert_test_q_a', 512, 'bert')\n  tokenize_input(bert_tokenizer, test_df.loc[i, 'question_title'], test_df.loc[i, 'question_body'], tags, 'bert_test_t_q', 512, 'bert')\n  tokenize_input(bert_tokenizer, test_df.loc[i, 'question_body'], None, tags, 'bert_test_q', 512, 'bert')\n  tokenize_input(bert_tokenizer, test_df.loc[i, 'answer'], None, tags, 'bert_test_a', 512, 'bert')\n\nfor key, _ in data.items():\n  for k, _ in data[key].items():\n    data[key][k] = np.array(data[key][k])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def SpearmanCorrCoeff(A, B):\n  overall_score = 0\n  for index in range(A.shape[1]):\n      overall_score += spearmanr(A[:, index], B[:, index]).correlation\n  return overall_score/30\nclass PredictCallback(tf.keras.callbacks.Callback):\n  def __init__(self, data, labels):\n    self.data = data\n    self.labels = labels\n  def on_epoch_end(self, epoch, logs = {}):\n    predictions = self.model.predict(self.data)\n    print('\\n\\t Validation Score - ' + str(SpearmanCorrCoeff(self.labels, predictions)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class BERT(TFBertModel):\n  def __init__(self, config, *inputs, **kwrgs):\n    super(BERT, self).__init__(config, *inputs, **kwrgs)\n    self.bert = TFBertMainLayer(config, name = 'bert')\n    for i in range(1, 45):\n      self.bert.submodules[-i].trainable = False\n      \n  def call(self, inputs, **kwrgs):\n    outputs = self.bert(inputs)\n    hidden_states = outputs[2]\n    h12 = hidden_states[-1][:, 0, :]\n    h11 = hidden_states[-2][:, 0, :]\n    h10 = hidden_states[-3][:, 0, :]\n    h9 = hidden_states[-4][:, 0, :]\n    concat = tf.keras.layers.Concatenate(axis = -1)([h9, h10, h11, h12])\n    return concat\n\nclass XLNet(TFXLNetModel):\n  def __init__(self, config, *inputs, **kwrgs):\n    super(XLNet, self).__init__(config, *inputs, **kwrgs)\n    self.transformer = TFXLNetMainLayer(config, name = 'transformer')\n    for i in range(1, 3):\n      self.transformer.layer[-i].trainable = False\n  def call(self, inputs, **kwrgs):\n    outputs = self.transformer(inputs)\n    hidden_states = outputs[1]\n    h12 = hidden_states[-1][:, 0, :]\n    h11 = hidden_states[-2][:, 0, :]\n    h10 = hidden_states[-3][:, 0, :]\n    h9 = hidden_states[-4][:, 0, :]\n    concat = tf.keras.layers.Concatenate(axis = -1)([h9, h10, h11, h12])\n    return concat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model(name):\n  id_1 = tf.keras.Input(shape = (512), dtype = tf.int32)\n  id_2 = tf.keras.Input(shape = (512), dtype = tf.int32)\n\n  type_id_1 = tf.keras.Input(shape = (512), dtype = tf.int32)\n  type_id_2 = tf.keras.Input(shape = (512), dtype = tf.int32)\n    \n  a1 = tf.keras.Input(shape = (512), dtype = tf.int32)\n  a2 = tf.keras.Input(shape = (512), dtype = tf.int32)\n  if name == 'xlnet':\n    config = XLNetConfig.from_pretrained('/kaggle/input/xlnetbasecased/config-xlnet-base-cased', output_hidden_states = True)\n    transformer = XLNet.from_pretrained('/kaggle/input/xlnetbasecased/model-xlnet-base-cased', config = config)\n                                                \n  else:\n    config = BertConfig.from_pretrained('/kaggle/input/bertbaseuncasedcomplete/bert-model', output_hidden_states = True)\n    transformer = BERT.from_pretrained('/kaggle/input/bertbaseuncasedcomplete/bert-model', config = config)\n  \n  out_1 = transformer({'input_ids':id_1, 'attention_mask':a1, 'token_type_ids':type_id_1})\n  out_2 = transformer({'input_ids':id_2, 'attention_mask':a2, 'token_type_ids':type_id_2})\n  \n  concat = tf.keras.layers.Concatenate(axis = -1)([out_1, out_2])\n  drop = tf.keras.layers.Dropout(rate = 0.1)(concat)\n  dense = tf.keras.layers.Dense(30, activation = 'sigmoid')(drop)\n  return tf.keras.Model(inputs = [id_1, id_2, type_id_1, type_id_2, a1, a2], outputs = [dense])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"main_fold = np.random.randint(0, 5)\nprint(main_fold)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gkf = GroupKFold(n_splits = 5).split(X = train_df.url, groups = train_df.url)\nfor fold, (train_idx, valid_idx) in enumerate(gkf):\n  if fold != main_fold:\n    continue\n  final_outputs = train_df[cols].values.astype(np.float16)\n  tf.keras.backend.clear_session()\n  xlnet_train_inputs = (\n                    data['xlnet_train_a'][tags[0]][train_idx], data['xlnet_train_t_q'][tags[0]][train_idx], data['xlnet_train_a'][tags[1]][train_idx], data['xlnet_train_t_q'][tags[1]][train_idx],\n                   data['xlnet_train_a'][tags[2]][train_idx], data['xlnet_train_t_q'][tags[2]][train_idx]  \n                  \n                 )\n  xlnet_valid_inputs = (\n                    data['xlnet_train_a'][tags[0]][valid_idx], data['xlnet_train_t_q'][tags[0]][valid_idx], data['xlnet_train_a'][tags[1]][valid_idx], data['xlnet_train_t_q'][tags[1]][valid_idx],\n                   data['xlnet_train_a'][tags[2]][valid_idx], data['xlnet_train_t_q'][tags[2]][valid_idx]  \n                  \n                  )\n#   bert_train_inputs = (\n#                     data['bert_train_a'][tags[0]][train_idx], data['bert_train_t_q'][tags[0]][train_idx], data['bert_train_a'][tags[1]][train_idx], data['bert_train_t_q'][tags[1]][train_idx],\n#                    data['bert_train_a'][tags[2]][train_idx], data['bert_train_t_q'][tags[2]][train_idx]  \n                  \n#                  )\n#   bert_valid_inputs = (\n#                     data['bert_train_a'][tags[0]][valid_idx], data['bert_train_t_q'][tags[0]][valid_idx], data['bert_train_a'][tags[1]][valid_idx], data['bert_train_t_q'][tags[1]][valid_idx],\n#                    data['bert_train_a'][tags[2]][valid_idx], data['bert_train_t_q'][tags[2]][valid_idx]  \n                  \n#                   )\n\n  \n  xlnet_model = create_model('xlnet')\n  xlnet_model.compile(tf.keras.optimizers.Adam(learning_rate = 2*1e-5), loss = tf.keras.losses.BinaryCrossentropy())\n  xlnet_model.fit(x = xlnet_train_inputs, y = final_outputs[train_idx], epochs = 3, batch_size = 4, steps_per_epoch = train_idx.shape[0]//4)\n  tf.keras.backend.clear_session()\n  print(\"\\n####################################################################################################################\\n\")\n#   bert_model = create_model('bert')\n#   bert_model.compile(tf.keras.optimizers.Adam(learning_rate = 2.3*1e-5), loss = tf.keras.losses.BinaryCrossentropy())\n#   bert_model.fit(x = bert_train_inputs, y = final_outputs[train_idx], epochs = 2, batch_size = 4, steps_per_epoch = train_idx.shape[0]//4)\n\n  break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xlnet_inputs = (\n                    data['xlnet_train_a'][tags[0]], data['xlnet_train_t_q'][tags[0]], data['xlnet_train_a'][tags[1]], data['xlnet_train_t_q'][tags[1]],\n                   data['xlnet_train_a'][tags[2]], data['xlnet_train_t_q'][tags[2]]  \n                  \n                 )\npredictions = xlnet_model.predict(xlnet_inputs)\ntrain_y = final_outputs[train_idx]\nvalid_y = final_outputs[valid_idx]\ntrain_preds = predictions[train_idx]\nvalid_preds = predictions[valid_idx]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"estimators = []\npost_valid_preds = np.zeros_like(valid_preds)\nfor i in range(train_preds.shape[1]):\n  lgb_train = lgb.Dataset(train_preds[:, i:i+1], label=train_y[:, i])\n  params = {'objective': 'rmse', 'num_leaves': 3, 'learning_rate': 0.01, 'min_data_in_leaf': 20}\n  est = lgb.train(params, lgb_train, num_boost_round=50)\n  estimators.append(est)\n  post_valid_preds[:, i] = est.predict(valid_preds[:, i:i+1])\n  if (post_valid_preds[:, i].max() - post_valid_preds[:, i].min()) < 0.000001:\n    max_idx = np.argmax(post_valid_preds[:, i])\n    post_valid_preds[:, i][max_idx] = min(0.9999999, post_valid_preds[:, i][max_idx] + 0.001)\n    min_idx = np.argmin(post_valid_preds[:, i])\n    post_valid_preds[:, i][min_idx] = max(0.0000001, post_valid_preds[:, i][min_idx] - 0.001)\n    \nprint(f\"Validation Score (Before) {SpearmanCorrCoeff(valid_y, valid_preds)}\")\nprint(f\"Validation Score (After) {SpearmanCorrCoeff(valid_y, post_valid_preds)}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xlnet_test_inputs = inputs = (data['xlnet_test_a'][tags[0]], data['xlnet_test_t_q'][tags[0]], data['xlnet_test_a'][tags[1]], \n    data['xlnet_test_t_q'][tags[1]],data['xlnet_test_a'][tags[2]], data['xlnet_test_t_q'][tags[2]])\ntest_preds = xlnet_model.predict(xlnet_test_inputs)\npost_test_preds = np.zeros_like(test_preds)\n\nfor i in range(train_preds.shape[1]):\n    post_test_preds[:, i] = estimators[i].predict(test_preds[:, i:i+1])\n    if (post_valid_preds[:, i].max() - post_valid_preds[:, i].min()) < 0.000001:\n        max_idx = np.argmax(post_valid_preds[:, i])\n        post_valid_preds[:, i][max_idx] = min(0.9999999, post_valid_preds[:, i][max_idx] + 0.00001)\n        min_idx = np.argmin(post_valid_preds[:, i])\n        post_valid_preds[:, i][min_idx] = max(0.0000001, post_valid_preds[:, i][min_idx] - 0.00001)\nsubmission = pd.read_csv(DIR+'/sample_submission.csv')\nsubmission.iloc[:,1:] = post_test_preds\nsubmission.to_csv(\"submission.csv\", index = False)\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}