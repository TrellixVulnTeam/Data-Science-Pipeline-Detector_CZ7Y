{"cells":[{"metadata":{},"cell_type":"markdown","source":"# If this kernel helps in anyway, consider upvoting!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* Using ideas given in this kernel - https://www.kaggle.com/sakami/google-quest-single-lstm.\n* Written in TF2.0/Keras. \n* Will be updating if getting better scores :)","execution_count":null},{"metadata":{"_uuid":"69eac106-c7c6-45f9-812f-88c26538a52c","_cell_guid":"5fe72218-c14a-4bf0-b7d4-fa8f678ba246","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom gensim.models import Word2Vec, KeyedVectors\nimport string\nimport re\nfrom collections import Counter\nimport gensim.downloader as api\nfrom nltk.stem import PorterStemmer, SnowballStemmer\nfrom nltk.stem.lancaster import LancasterStemmer\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom sklearn.model_selection import GroupKFold\nfrom scipy.stats import spearmanr\nimport os\nimport gc\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vector_dim = 300\nencoder = hub.load(\"/kaggle/input/universalsentenceencoderqa\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ps = PorterStemmer()\nlc = LancasterStemmer()\nsb = SnowballStemmer('english')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"contractions = {\n\"ain't\": \"is not\",\n\"aren't\": \"are not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he will\",\n\"he'll've\": \"he he will have\",\n\"he's\": \"he is\",\n\"how'd\": \"how did\",\n\"how'd'y\": \"how do you\",\n\"how'll\": \"how will\",\n\"how's\": \"how is\",\n\"I'd\": \"I would\",\n\"i'd've\": \"I would have\",\n\"i'll\": \"I will\",\n\"i'll've\": \"I will have\",\n\"i'm\": \"I am\",\n\"i've\": \"I have\",\n\"i'd\": \"i would\",\n\"i'd've\": \"i would have\",\n\"i'll\": \"i will\",\n\"i'll've\": \"i will have\",\n\"i'm\": \"i am\",\n\"i've\": \"i have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it would\",\n\"it'd've\": \"it would have\",\n\"it'll\": \"it will\",\n\"it'll've\": \"it will have\",\n\"it's\": \"it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"mightn't've\": \"might not have\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"mustn't've\": \"must not have\",\n\"needn't\": \"need not\",\n\"needn't've\": \"need not have\",\n\"o'clock\": \"of the clock\",\n\"oughtn't\": \"ought not\",\n\"oughtn't've\": \"ought not have\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"shan't've\": \"shall not have\",\n\"she'd\": \"she would\",\n\"she'd've\": \"she would have\",\n\"she'll\": \"she will\",\n\"she'll've\": \"she will have\",\n\"she's\": \"she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"shouldn't've\": \"should not have\",\n\"so've\": \"so have\",\n\"so's\": \"so as\",\n\"that'd\": \"that would\",\n\"that'd've\": \"that would have\",\n\"that's\": \"that is\",\n\"there'd\": \"there would\",\n\"there'd've\": \"there would have\",\n\"there's\": \"there is\",\n\"they'd\": \"they would\",\n\"they'd've\": \"they would have\",\n\"they'll\": \"they will\",\n\"they'll've\": \"they will have\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"to've\": \"to have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we would\",\n\"we'd've\": \"we would have\",\n\"we'll\": \"we will\",\n\"we'll've\": \"we will have\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what will\",\n\"what'll've\": \"what will have\",\n\"what're\": \"what are\",\n\"what's\": \"what is\",\n\"what've\": \"what have\",\n\"when's\": \"when is\",\n\"when've\": \"when have\",\n\"where'd\": \"where did\",\n\"where's\": \"where is\",\n\"where've\": \"where have\",\n\"who'll\": \"who will\",\n\"who'll've\": \"who will have\",\n\"who's\": \"who is\",\n\"who've\": \"who have\",\n\"why's\": \"why is\",\n\"why've\": \"why have\",\n\"will've\": \"will have\",\n\"won't\": \"will not\",\n\"won't've\": \"will not have\",\n\"would've\": \"would have\",\n\"wouldn't\": \"would not\",\n\"wouldn't've\": \"would not have\",\n\"y'all\": \"you all\",\n\"y'all'd\": \"you all would\",\n\"y'all'd've\": \"you all would have\",\n\"y'all're\": \"you all are\",\n\"y'all've\": \"you all have\",\n\"you'd\": \"you would\",\n\"you'd've\": \"you would have\",\n\"you'll\": \"you will\",\n\"you'll've\": \"you will have\",\n\"you're\": \"you are\",\n\"you've\": \"you have\"\n}\n\nrules = {\n    \"'t\": \" not\",\n    \"'cause\": \" because\",\n    \"'ve\": \" have\",\n    \"'t\": \" not\",\n    \"'s\": \" is\",\n    \"'d\": \" had\"\n}\n\npunctuations = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']',\n          '>', '%', '=', '#', '*', '+', '\\\\', '•', '~', '@', '£', '·', '_', '{', '}', '©', '^',\n          '®', '`', '<', '→', '°', '€', '™', '›', '♥', '←', '×', '§', '″', '′', 'Â', '█',\n          '½', 'à', '…', '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶',\n          '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '▒', '：', '¼',\n          '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲',\n          'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '∙', '）', '↓', '、', '│', '（', '»', '，', '♪',\n          '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√'] + list(string.punctuation)\n\nspell_replace = {\n    'usepackage':'latex',\n    'orf19':'gene',\n    'documentclass':'latex',\n    'magento':'open-source e-commerce',\n    'appium':'web-app',\n    'tikz':'programming language',\n    'tikzpicture':'programming language',\n    'openvpn':'vpn',\n    'httpclient':'http client',\n    'arraylist':'array list',\n    'jsonobject': 'json',\n    'artifactid':'xml',\n    'hwnd':'os'\n    \n}\nspecial_chars = \",  .  \\\"  :  )  (  -  !  ?  |  ;  '  $  &  /  [  ]  >  %  =  #  *  +  \\  •  ~  @  £  ·  {  }  ©  ^  ®  <  →  °  €  ™  ›  ♥  ←  ×  §  ″  ′  Â  █  ½  à  …  “  ★  ”  –  ●  â  ►  −  ¢  ²  ¬  ░  ¶  ↑  ±  ¿  ═  ¦  ║  ―  ¥  ▓  —  ‹  ─  ▒  ：  ¼  ⊕  ▼  ▪  †  ■  ’  ▀  ¨  ▄  ♫  ☆  é  ¯  ♦  ¤  ▲  è  ¸  ¾  Ã  ⋅  ‘  ∞  ∙  ）  ↓  、  │  （  »  ，  ♪  ╩  ╚  ³  ・  ╦  ╣  ╔  ╗  ▬  ❤  ï  Ø  ¹  ≤  ‡  √  !  \\\"  $  %  &  '  (  )  *  +  ,  -  .  /  :  ;  <  =  >  ?  @  [  \\  ]  ^  {  |  }  ~ \".split()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/google-quest-challenge/train.csv')\ntest_df = pd.read_csv('/kaggle/input/google-quest-challenge/test.csv')\ncols = train_df.columns[11:]\ntrain_df['combined_t_q'] = train_df[['question_title', 'question_body']].apply(lambda x: x['question_title']+' '+x['question_body'], axis = 1)\ntest_df['combined_t_q'] = test_df[['question_title', 'question_body']].apply(lambda x: x['question_title']+' '+x['question_body'], axis = 1)\ntrain_df['combined_t_a'] = train_df[['question_title', 'answer']].apply(lambda x: x['question_title']+' '+x['answer'], axis = 1)\ntest_df['combined_t_a'] = test_df[['question_title', 'answer']].apply(lambda x: x['question_title']+' '+x['answer'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_outputs = train_df[cols].values\nunique_outputs = [np.sort(train_df[col].unique())[np.newaxis, :] for col in cols]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_text(s):\n    s = s.lower()\n    #Expand Contractions\n    for key, value in contractions.items():\n        s = s.replace(key, f' {value} ')\n    for key, value in rules.items():\n        s = s.replace(key, f' {value} ')\n    for punct in punctuations:\n        if punct in special_chars:\n            s = s.replace(punct, f' {punct} ')\n        else:\n            s = s.replace(punct, ' ')\n    for key, value in spell_replace.items():\n        s = s.replace(key, value)\n    s = re.sub('\\s+', ' ', s)\n    return s","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['clean_answer'] = train_df['answer'].apply(preprocess_text)\ntrain_df['clean_t_q'] = train_df['combined_t_q'].apply(preprocess_text)\n\ntest_df['clean_answer'] = test_df['answer'].apply(preprocess_text)\ntest_df['clean_t_q'] = test_df['combined_t_q'].apply(preprocess_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_texts = train_df.clean_answer.tolist() + train_df.clean_t_q.tolist() + test_df.clean_answer.tolist() + test_df.clean_t_q.tolist() \nall_texts = [text.split() for text in all_texts]\ncounter = Counter()\nfor text in all_texts:\n  counter.update(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab = {}\nvocab['token2id'] = {key:id+1 for id, (key, _) in enumerate(counter.items())}\nvocab['id2token'] = {value:key for key, value in vocab['token2id'].items()}\nvocab['word_freq'] = dict(counter)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_embedding_matrix(vocab, texts, embedd_size, model):\n  n = len(vocab['token2id'])+1\n  embedding_matrix = np.zeros((n, embedd_size))\n  for text in texts:\n    for key in text:\n      word = key\n      try:\n        embedding_matrix[vocab['token2id'][word]] = model.wv[word]\n        continue\n      except:\n        pass\n  return embedding_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Word2Vec(size=300, window = 5, min_count = 1)\nmodel.build_vocab(all_texts)\ntotal_examples = model.corpus_count\nmodel.intersect_word2vec_format('/kaggle/input/fasttext/fasttext-wiki-news-subwords-300', lockf=1.0)\nmodel.train(all_texts, total_examples=total_examples, epochs=5)\nembedding_matrix = build_embedding_matrix(vocab, all_texts, 300, model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_token_ids(texts, max_length):\n  tokens = []\n  for text in texts:\n    tmp_tokens = []\n    if len(text.split()) > max_length:\n      for each in (text.split()[:(max_length//2)] + text.split()[-(max_length//2):]):\n        tmp_tokens.append(vocab['token2id'][each])\n      tokens.append(tmp_tokens)\n    else:\n      for each in (text.split()[:max_length]):\n        tmp_tokens.append(vocab['token2id'][each])\n      tokens.append(tmp_tokens)\n  return tf.keras.preprocessing.sequence.pad_sequences(tokens, padding=\"post\", maxlen=max_length)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LENGTH = 250\ndata = {}\ndata['train_question_title'] = get_token_ids(train_df['clean_t_q'], MAX_LENGTH)\ndata['train_answer'] = get_token_ids(train_df['clean_answer'], MAX_LENGTH)\ndata['test_question_title'] = get_token_ids(test_df['clean_t_q'], MAX_LENGTH)\ndata['test_answer'] = get_token_ids(test_df['clean_answer'], MAX_LENGTH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['train_question_title_use'] = []\ndata['train_answer_use'] = []\n\ndata['test_question_title_use'] = []\ndata['test_answer_use'] = []\n\nBATCH_SIZE = 4\n\nfor i in range(0, train_df.shape[0], BATCH_SIZE):\n  data['train_question_title_use'] += [encoder.signatures['question_encoder'](tf.constant(train_df['clean_t_q'].iloc[i:i+BATCH_SIZE].tolist()))['outputs'].numpy().astype(np.float16)]\n  data['train_answer_use'] += [encoder.signatures['response_encoder'](input = tf.constant(train_df['clean_answer'].iloc[i:i+BATCH_SIZE].tolist()), \n                                                                context = tf.constant(train_df['clean_answer'].iloc[i:i+BATCH_SIZE].tolist()))['outputs'].numpy().astype(np.float16)]\n    \nfor i in range(0, test_df.shape[0], BATCH_SIZE):\n  data['test_question_title_use'] += [encoder.signatures['question_encoder'](tf.constant(test_df['clean_t_q'].iloc[i:i+BATCH_SIZE].tolist()))['outputs'].numpy().astype(np.float16)]\n  data['test_answer_use'] += [encoder.signatures['response_encoder'](input = tf.constant(test_df['clean_answer'].iloc[i:i+BATCH_SIZE].tolist()), \n                                                                context = tf.constant(test_df['clean_answer'].iloc[i:i+BATCH_SIZE].tolist()))['outputs'].numpy().astype(np.float16)]\n    \n\ndata['train_question_title_use'] = np.vstack(data['train_question_title_use'])\ndata['train_answer_use'] = np.vstack(data['train_answer_use'])\n\ndata['test_question_title_use'] = np.vstack(data['test_question_title_use'])\ndata['test_answer_use'] = np.vstack(data['test_answer_use'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_generator(X, batch_size = 32, training = True):\n    Y = X[1]\n    N = X[0][0].shape[0]\n    if training == True:\n        indexes = np.random.permutation(N)\n    else:\n        indexes = np.arange(N)\n    def generator():\n        for i in indexes:\n            yield {\"input_1\": X[0][0][i], \"input_2\": X[0][1][i],\"input_3\": X[0][2][i], \"input_4\": X[0][3][i]}, Y[i]\n    return tf.data.Dataset.from_generator(generator, \n    output_types = ({\"input_1\": tf.int32, \"input_2\": tf.int32,\"input_3\": tf.float16,\"input_4\": tf.float16}, tf.float16)).repeat().batch(batch_size)\n\ndef SpearmanCorrCoeff(A, B):\n  overall_score = 0\n  for index in range(A.shape[1]):\n      overall_score += spearmanr(A[:, index], B[:, index]).correlation\n  return np.round(overall_score/A.shape[1], 3)\n\nclass PredictCallback(tf.keras.callbacks.Callback):\n  def __init__(self, data, labels):\n    self.data = data\n    self.labels = labels\n  def on_epoch_end(self, epoch, logs = {}):\n    predictions = self.model.predict(self.data)\n    print('\\nValidation Score - ' + str(SpearmanCorrCoeff(self.labels, predictions)))\n    \n\n\nclass EWA(tf.keras.callbacks.Callback):\n  def on_train_batch_end(self, batch, logs = None):\n    global prev_weights\n    if prev_weights is None:\n      prev_weights = [tf.identity(x) for x in self.model.trainable_variables]\n    else:\n      beta = 0.1\n      for index, _ in enumerate(self.model.trainable_variables):\n        self.model.trainable_variables[index].assign(self.model.trainable_variables[index]*beta + (1-beta)*prev_weights[index])\n        prev_weights = [tf.identity(x) for x in self.model.trainable_variables]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model():\n  i1 = tf.keras.Input(shape = (MAX_LENGTH), dtype = tf.int32)\n  i2 = tf.keras.Input(shape = (MAX_LENGTH), dtype = tf.int32)\n  i3 = tf.keras.Input(shape = (512), dtype = tf.float16)\n  i4 = tf.keras.Input(shape = (512), dtype = tf.float16)\n\n  e1 = tf.keras.layers.Embedding(input_dim = embedding_matrix.shape[0], output_dim = embedding_matrix.shape[1], weights = [embedding_matrix], trainable = False)(i1)\n  e2 = tf.keras.layers.Embedding(input_dim = embedding_matrix.shape[0], output_dim = embedding_matrix.shape[1], weights = [embedding_matrix], trainable = False)(i2)\n  \n  sd_i1 = tf.keras.layers.SpatialDropout1D(0.2)(e1)\n  sd_i2 = tf.keras.layers.SpatialDropout1D(0.2)(e2)\n\n  lstm_q_1 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences = True))(sd_i1)\n  lstm_q_2 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences = True))(lstm_q_1)\n    \n  lstm_a_1 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences = True))(sd_i2)\n  lstm_a_2 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences = True))(lstm_a_1)\n\n  max_pool_1 = tf.keras.layers.GlobalMaxPooling1D()(lstm_q_2)\n  avg_pool_1 = tf.keras.layers.GlobalAveragePooling1D()(lstm_q_2)\n  max_pool_2 = tf.keras.layers.GlobalMaxPooling1D()(lstm_a_2)\n  avg_pool_2 = tf.keras.layers.GlobalAveragePooling1D()(lstm_a_2)\n\n  hidden = tf.keras.layers.Concatenate()([max_pool_1, max_pool_2, avg_pool_1, avg_pool_2, i3, i4])\n  dense = tf.keras.layers.Dense(256, activation = 'relu')(hidden)\n  drop = tf.keras.layers.Dropout(rate = 0.15)(dense)\n  \n  out = tf.keras.layers.Dense(30, activation = 'sigmoid')(drop)\n\n  model = tf.keras.Model(inputs = [i1, i2, i3, i4], outputs = [out])\n\n  return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"myfold = np.random.randint(0,5, size = 1000)\nmyfold_counter = Counter(myfold)\nprint(myfold_counter)\nmost_common = myfold_counter.most_common(1)[0][0]\nprint(most_common)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.keras.backend.clear_session()\nlr_sched = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-3* (0.83 ** ((epoch+1) / 2)))\n\ngkf = GroupKFold(n_splits = 5).split(X=train_df.url, groups = train_df.url)\n\nfor fold, (train_idx, valid_idx) in enumerate(gkf):\n  prev_weights = None\n  if fold != most_common:\n    continue\n  train_inputs = ((data['train_question_title'][train_idx], data['train_answer'][train_idx], data['train_question_title_use'][train_idx], \n                   data['train_answer_use'][train_idx]), (final_outputs[train_idx]))\n  valid_inputs = (\n                  (\n                      ([data['train_question_title'][valid_idx]], [data['train_answer'][valid_idx]], [data['train_question_title_use'][valid_idx]], [data['train_answer_use'][valid_idx]]), \n                      (None)\n                  )\n               )\n  BATCH_SIZE = 32\n  train_dataset = get_generator(train_inputs)\n  valid_dataset = tf.data.Dataset.from_tensor_slices(valid_inputs)\n\n  model = create_model()\n  optimizer = tf.keras.optimizers.Adam()\n  model.compile(loss='binary_crossentropy', optimizer=optimizer)\n\n\n  model.fit(train_dataset, epochs = 7, steps_per_epoch = train_idx.shape[0]//BATCH_SIZE,\n            callbacks=[PredictCallback(valid_dataset, final_outputs[valid_idx]), lr_sched, EWA()])\n  break\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Optimize:\n    def __init__(self):\n        self.clips = [[0, 1] for i in range(30)]\n        self.ab_ = [(0, 0.15), (0.85, 1)]\n        self.new_scores, self.scores = (None, None)\n    def fit(self, labels, preds):\n        self.scores = [SpearmanCorrCoeff(labels[:, i:i+1], preds[:, i:i+1]) for i in range(30)]\n        for i in range(30):\n            self.golden_section_search(labels[:, i:i+1], preds[:, i:i+1], i, 0)\n            self.golden_section_search(labels[:, i:i+1], preds[:, i:i+1], i, 1)\n        self.new_scores = [np.nan_to_num(SpearmanCorrCoeff(labels[:, i:i+1], np.clip(preds[:, i:i+1], self.clips[i][0], self.clips[i][1]))) for i in range(30)]\n        for i in range(30):\n            if self.scores[i] >= self.new_scores[i]:\n                self.clips[i] = [0, 1]\n    def golden_section_search(self, labels, preds, i, idx):\n        (a, b) = self.ab_[idx]\n        c = 0.618\n        x1 = b - c*(b-a)\n        x2 = (b-a)*c + a\n        \n        for epochs in range(10):\n            self.clips[i][idx] = x1\n            score_a = -self.score(labels, preds, i)\n            self.clips[i][idx] = x2\n            score_b = -self.score(labels, preds, i)\n            if np.isnan(score_a):\n                continue\n            elif np.isnan(score_b):\n                continue\n            elif score_a <= score_b:\n                b = x2\n                x2 = x1\n                x1 = b - c*(b-a)\n            else:\n                a = x1\n                x1 = x2\n                x2 = (b-a)*c + a\n        \n        self.clips[i][idx] = x1\n        score_x1 = self.score(labels, preds, i)\n        self.clips[i][idx] = x2\n        score_x2 = self.score(labels, preds, i)\n        if score_x1 > score_x2:\n            self.clips[i][idx] = x1\n        else:\n            self.clips[i][idx] = x2\n                    \n            \n    def score(self, labels, preds, i):\n        return SpearmanCorrCoeff(labels, np.clip(preds, self.clips[i][0], self.clips[i][1]))\n    def transform(self, preds):\n        temp = preds.copy()\n        for i in range(30):\n            clipped = np.clip(preds[:, i], self.clips[i][0], self.clips[i][1])\n            if np.unique(clipped).shape[0] > 1:\n                temp[:, i][:] = clipped\n        return temp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_inputs, valid_inputs, valid_dataset, train_dataset\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inputs = (\n              (\n                  ([data['train_question_title']], [data['train_answer']], [data['train_question_title_use']], [data['train_answer_use']]), \n                  (None)\n              )\n           )\ndataset = tf.data.Dataset.from_tensor_slices(inputs)\npredictions = model.predict(dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_y = final_outputs[train_idx]\nvalid_y = final_outputs[valid_idx]\ntrain_preds = predictions[train_idx]\nvalid_preds = predictions[valid_idx]\n\n\nopt = Optimize()\nopt.fit(train_y, train_preds)\npost_valid_preds = opt.transform(valid_preds)\nprint(f\"Validation Score (Before) {SpearmanCorrCoeff(valid_y, valid_preds)}\")\nprint(f\"Validation Score (After) {SpearmanCorrCoeff(valid_y, post_valid_preds)}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_inputs = (\n              (\n                  ([data['test_question_title']], [data['test_answer']], [data['test_question_title_use']], [data['test_answer_use']]), \n                  (None)\n              )\n           )\ntest_dataset = tf.data.Dataset.from_tensor_slices(test_inputs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds = model.predict(test_dataset)\npost_test_preds = opt.transform(test_preds)\nsubmission = pd.read_csv('/kaggle/input/google-quest-challenge/sample_submission.csv')\nsubmission.iloc[:,1:] = post_test_preds\nsubmission.to_csv(\"submission.csv\", index = False)\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}