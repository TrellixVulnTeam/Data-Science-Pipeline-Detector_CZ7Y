{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Google QUEST Q&A Labeling\nThis notebook performs the micro EDA of the Contest dataset of Google QUEST Q&A Labeling\nIn this notebook I have tried to perform a extremely basic Data Analysis of the provided data. If you have suggestion or willing to correct me anywhere in the comments.\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from pathlib import Path\nimport os\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"RANDOM_SEED = 123\nBASE=Path('../input/google-quest-challenge')\nfor i in os.walk(os.path.join(BASE)):\n    print(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df=pd.read_csv(BASE/'train.csv')\ntrain_df.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df=pd.read_csv(BASE/'test.csv')\ntest_df.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Viewing the value of first row of the training data\nfor i,j in train_df.iloc[0].items():\n#     print(i.ljust(30),j)\n    print('-'*80)\n    print(i)\n    print('-'*80)\n    print(j,'\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the channels where the Training queries comes from in the data\nfig = plt.figure(figsize=(16,8))\nax = fig.add_subplot(111)\nwidth = 0.4\ntrain_df.host.value_counts().plot(kind='bar', color='blue', ax=ax, width=width, position=1)\ntest_df.host.value_counts().plot(kind='bar', color='red', ax=ax, width=width, position=0)\nax.set_xlabel('Sites')\nax.set_ylabel('Question Counts')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the channels where the testing queries comes from in the data\nplt.figure(figsize=(16,5))\nwidth = 0.4\ntest_df.host.value_counts().plot(kind='bar', color='red', width=width)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the category occurance of the queries\nfig = plt.figure(figsize=(10,8))\nax = fig.add_subplot(111)\nwidth = 0.2\ntrain_df.category.value_counts().plot(kind='bar', color='blue', ax=ax, width=width, position=1, legend=True)\ntest_df.category.value_counts().plot(kind='bar', color='red', ax=ax, width=width, position=0, legend=True)\nax.set_xlabel('Sites')\nax.set_ylabel('Question Counts')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finding the size of longest query in the table\nsentence_len = train_df.answer.apply(lambda x: len(x))\nsentence_len.max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prints column with longest query\ntrain_df[sentence_len==22636]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I hope you find it useful. Please drop your comment and help me improve in my future kernels. Also if you find it informative, do **UPVOTE**."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}