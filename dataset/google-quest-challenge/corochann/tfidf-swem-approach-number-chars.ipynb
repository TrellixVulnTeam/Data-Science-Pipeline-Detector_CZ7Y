{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\nI will add some simple features I checked in my **data analysis kernel [Google QUEST: First data introduction](https://www.kaggle.com/corochann/google-quest-first-data-introduction)**.\n\nExcept that, most of the code is just copy from [https://www.kaggle.com/hukuda222/tfidf-swem-approach](https://www.kaggle.com/hukuda222/tfidf-swem-approach) by @hukuda222.\nThis kernel is based on TFIDF+NN model(https://www.kaggle.com/ryches/tfidf-benchmark ).\n\n> I will add new information to TFIDF+NN model(https://www.kaggle.com/ryches/tfidf-benchmark ).<br>\n> TFIDF can create features based on actual vocabulary, but it can't handle well when there is another word of close meaning.<br>\n> Therefore, I thought that adding SWEM(https://arxiv.org/abs/1805.09843) using learned word2vec as a feature value would increase the score.\n\nSince I only added some codes from forked kernel, **please upvote original kernel as well :)**."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport gensim\nfrom nltk.corpus import brown\nimport random\nfrom sklearn.model_selection import KFold\nimport lightgbm as lgb\nimport gc\nfrom keras.callbacks.callbacks import EarlyStopping\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom keras.callbacks.callbacks import EarlyStopping\nfrom scipy.stats import spearmanr\nfrom nltk.corpus import wordnet as wn\nimport tqdm\nfrom sklearn.model_selection import StratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/google-quest-challenge/train.csv\")\ntest = pd.read_csv(\"../input/google-quest-challenge/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_sub = pd.read_csv(\"../input/google-quest-challenge/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"sample_sub ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_cols = ['question_asker_intent_understanding',\n       'question_body_critical', 'question_conversational',\n       'question_expect_short_answer', 'question_fact_seeking',\n       'question_has_commonly_accepted_answer',\n       'question_interestingness_others', 'question_interestingness_self',\n       'question_multi_intent', 'question_not_really_a_question',\n       'question_opinion_seeking', 'question_type_choice',\n       'question_type_compare', 'question_type_consequence',\n       'question_type_definition', 'question_type_entity',\n       'question_type_instructions', 'question_type_procedure',\n       'question_type_reason_explanation', 'question_type_spelling',\n       'question_well_written', 'answer_helpful',\n       'answer_level_of_information', 'answer_plausible', 'answer_relevance',\n       'answer_satisfaction', 'answer_type_instructions',\n       'answer_type_procedure', 'answer_type_reason_explanation',\n       'answer_well_written']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Adding simple feature\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def char_count(s):\n    return len(s)\n\ndef word_count(s):\n    return s.count(' ')\n\ntrain['question_title_n_chars'] = train['question_title'].apply(char_count)\ntrain['question_title_n_words'] = train['question_title'].apply(word_count)\ntrain['question_body_n_chars'] = train['question_body'].apply(char_count)\ntrain['question_body_n_words'] = train['question_body'].apply(word_count)\ntrain['answer_n_chars'] = train['answer'].apply(char_count)\ntrain['answer_n_words'] = train['answer'].apply(word_count)\n\ntest['question_title_n_chars'] = test['question_title'].apply(char_count)\ntest['question_title_n_words'] = test['question_title'].apply(word_count)\ntest['question_body_n_chars'] = test['question_body'].apply(char_count)\ntest['question_body_n_words'] = test['question_body'].apply(word_count)\ntest['answer_n_chars'] = test['answer'].apply(char_count)\ntest['answer_n_words'] = test['answer'].apply(word_count)\n\ntrain['question_body_n_chars'].clip(0, 5000, inplace=True)\ntest['question_body_n_chars'].clip(0, 5000, inplace=True)\ntrain['question_body_n_words'].clip(0, 1000, inplace=True)\ntest['question_body_n_words'].clip(0, 1000, inplace=True)\n\ntrain['answer_n_chars'].clip(0, 5000, inplace=True)\ntest['answer_n_chars'].clip(0, 5000, inplace=True)\ntrain['answer_n_words'].clip(0, 1000, inplace=True)\ntest['answer_n_words'].clip(0, 1000, inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_question = train['question_user_name'].value_counts()\nnum_answer = train['answer_user_name'].value_counts()\n\ntrain['num_answer_user'] = train['answer_user_name'].map(num_answer)\ntrain['num_question_user'] = train['question_user_name'].map(num_question)\ntest['num_answer_user'] = test['answer_user_name'].map(num_answer)\ntest['num_question_user'] = test['question_user_name'].map(num_question)\n\n# map is done by train data, we need to fill value for user which does not appear in train data...\ntest['num_answer_user'].fillna(1, inplace=True)\ntest['num_question_user'].fillna(1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"simple_feature_cols = [\n    'question_title_n_chars', 'question_title_n_words', 'question_body_n_chars', 'question_body_n_words',\n    'answer_n_chars', 'answer_n_words', 'num_answer_user', 'num_question_user'\n]\nsimple_engineered_feature = train[simple_feature_cols].values\nsimple_engineered_feature_test = test[simple_feature_cols].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MaxAbsScaler\n\nscaler = MaxAbsScaler()\nsimple_engineered_feature = scaler.fit_transform(simple_engineered_feature)\nsimple_engineered_feature_test = scaler.transform(simple_engineered_feature_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# other feature engineering\n\nBelow is just a copy of https://www.kaggle.com/hukuda222/tfidf-swem-approach"},{"metadata":{"trusted":true},"cell_type":"code","source":"def simple_prepro(s):\n    return [w for w in s.replace(\"\\n\",\" \").replace(\",\",\" , \").replace(\"(\",\" ( \").replace(\")\",\" ) \").\n            replace(\".\",\" . \").replace(\"?\",\" ? \").replace(\":\",\" : \").replace(\"n't\",\" not\").\n            replace(\"'ve\",\" have\").replace(\"'re\",\" are\").replace(\"'s\",\" is\").split(\" \") if w != \"\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def simple_prepro_tfidf(s):\n    return \" \".join([w for w in s.lower().replace(\"\\n\",\" \").replace(\",\",\" , \").replace(\"(\",\" ( \").replace(\")\",\" ) \").\n            replace(\".\",\" . \").replace(\"?\",\" ? \").replace(\":\",\" : \").replace(\"n't\",\" not\").\n            replace(\"'ve\",\" have\").replace(\"'re\",\" are\").replace(\"'s\",\" is\").split(\" \") if w != \"\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is basic preprocessing. This time, symbols and words are attached, so they are separated here."},{"metadata":{"trusted":true},"cell_type":"code","source":"qt_max = max([len(simple_prepro(l)) for l in list(train[\"question_title\"].values)])\nqb_max = max([len(simple_prepro(l))  for l in list(train[\"question_body\"].values)])\nan_max = max([len(simple_prepro(l))  for l in list(train[\"answer\"].values)])\nprint(\"max lenght of question_title is\",qt_max)\nprint(\"max lenght of question_body is\",qb_max)\nprint(\"max lenght of question_answer is\",an_max)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The text is so long that it is difficult to apply RNN to all series."},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model = gensim.models.Word2Vec(brown.sents())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we use a trained word2vec model that is easily available with nltk.<br>\nWe used SWEM with max pooling.<br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_word_embeddings(text):\n    np.random.seed(abs(hash(text)) % (10 ** 8))\n    words = simple_prepro(text)\n    vectors = np.zeros((len(words),100))\n    if len(words)==0:\n        vectors = np.zeros((1,100))\n    for i,word in enumerate(simple_prepro(text)):\n        try:\n            vectors[i]=w2v_model[word]\n        except:\n            vectors[i]=np.random.uniform(-0.01, 0.01,100)\n            #np.array([len(text)/5000,len(words)/1000,text.count(\"\\n\")/10])]\n    return np.max(np.array(vectors), axis=0)\n                           ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"question_title = [get_word_embeddings(l) for l in tqdm.tqdm(train[\"question_title\"].values)]\nquestion_title_test = [get_word_embeddings(l) for l in tqdm.tqdm(test[\"question_title\"].values)]\n\nquestion_body = [get_word_embeddings(l) for l in tqdm.tqdm(train[\"question_body\"].values)]\nquestion_body_test = [get_word_embeddings(l) for l in tqdm.tqdm(test[\"question_body\"].values)]\n\nanswer = [get_word_embeddings(l) for l in tqdm.tqdm(train[\"answer\"].values)]\nanswer_test = [get_word_embeddings(l) for l in tqdm.tqdm(test[\"answer\"].values)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From here on, I'm quite referring to https://www.kaggle.com/ryches/tfidf-benchmark."},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()\ntfidf = TfidfVectorizer(ngram_range=(1, 3))\ntsvd = TruncatedSVD(n_components = 50)\ntfidf_question_title = tfidf.fit_transform([simple_prepro_tfidf(l) for l in tqdm.tqdm(train[\"question_title\"].values)])\ntfidf_question_title_test = tfidf.transform([simple_prepro_tfidf(l) for l in tqdm.tqdm(test[\"question_title\"].values)])\ntfidf_question_title = tsvd.fit_transform(tfidf_question_title)\ntfidf_question_title_test = tsvd.transform(tfidf_question_title_test)\n\ntfidf_question_body = tfidf.fit_transform([simple_prepro_tfidf(l) for l in tqdm.tqdm(train[\"question_body\"].values)])\ntfidf_question_body_test = tfidf.transform([simple_prepro_tfidf(l) for l in tqdm.tqdm(test[\"question_body\"].values)])\ntfidf_question_body = tsvd.fit_transform(tfidf_question_body)\ntfidf_question_body_test = tsvd.transform(tfidf_question_body_test)\n\ntfidf_answer = tfidf.fit_transform([simple_prepro_tfidf(l) for l in tqdm.tqdm(train[\"answer\"].values)])\ntfidf_answer_test = tfidf.transform([simple_prepro_tfidf(l) for l in tqdm.tqdm(test[\"answer\"].values)])\ntfidf_answer = tsvd.fit_transform(tfidf_answer)\ntfidf_answer_test = tsvd.transform(tfidf_answer_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type2int = {type:i for i,type in enumerate(list(set(train[\"category\"])))}\ncate = np.identity(5)[np.array(train[\"category\"].apply(lambda x:type2int[x]))].astype(np.float64)\ncate_test = np.identity(5)[np.array(test[\"category\"].apply(lambda x:type2int[x]))].astype(np.float64)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = np.concatenate([question_title, question_body, answer,\n                                 tfidf_question_title, tfidf_question_body, tfidf_answer, \n                                 cate, simple_engineered_feature\n                                ], axis=1)\ntest_features = np.concatenate([question_title_test, question_body_test, answer_test, \n                               tfidf_question_title_test, tfidf_question_body_test, tfidf_answer_test,\n                                cate_test, simple_engineered_feature_test\n                                ], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"num_folds = 10\nfold_scores = []\nkf = KFold(n_splits = num_folds, shuffle = True, random_state = 42)\ntest_preds = np.zeros((len(test_features), len(target_cols)))\nfor train_index, val_index in kf.split(train_features):\n    gc.collect()\n    train_X = train_features[train_index, :]\n    train_y = train[target_cols].iloc[train_index]\n    \n    val_X = train_features[val_index, :]\n    val_y = train[target_cols].iloc[val_index]\n    \n    model = Sequential([\n        Dense(512, input_shape=(train_features.shape[1],)),\n        Activation('relu'),\n        Dense(128),\n        Activation('relu'),\n        Dense(len(target_cols)),\n        Activation('sigmoid'),\n    ])\n    \n    es = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='auto', baseline=None, restore_best_weights=True)\n    model.compile(optimizer='adam',\n                  loss='binary_crossentropy')\n    \n    model.fit(train_X, train_y, epochs = 100, validation_data=(val_X, val_y), callbacks = [es])\n    preds = model.predict(val_X)\n    overall_score = 0\n    for col_index, col in enumerate(target_cols):\n        overall_score += spearmanr(preds[:, col_index], val_y[col].values).correlation/len(target_cols)\n        print(col, spearmanr(preds[:, col_index], val_y[col].values).correlation)\n    fold_scores.append(overall_score)\n    print(overall_score)\n\n    test_preds += model.predict(test_features)/num_folds\n    \nprint(fold_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv(\"../input/google-quest-challenge/sample_submission.csv\")\nfor col_index, col in enumerate(target_cols):\n    sub[col] = test_preds[:, col_index]\nsub.to_csv(\"submission.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"test_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"sub.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Check prediction\n\nCompare train ground truth and test prediction for the distribution."},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\n\nfig, axes = plt.subplots(6, 5, figsize=(18, 15))\naxes = axes.ravel()\nbins = np.linspace(0, 1, 20)\n\nfor i, col in enumerate(target_cols):\n    ax = axes[i]\n    sns.distplot(train[col], label=col, bins=bins, ax=ax, color='blue')\n    sns.distplot(sub[col], label=col, bins=bins, ax=ax, color='orange')\n    # ax.set_title(col)\n    ax.set_xlim([0, 1])\nplt.tight_layout()\nplt.show()\nplt.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}