{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\ntqdm.pandas()\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\nimport string\nimport re    #for regex\nimport nltk\nfrom nltk.corpus import stopwords\nimport spacy\nfrom nltk import pos_tag\nfrom nltk.stem.wordnet import WordNetLemmatizer \nfrom nltk.tokenize import word_tokenize\n# Tweet tokenizer does not split at apostophes which is what we want\nfrom nltk.tokenize import TweetTokenizer   \nimport time\nimport re\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is just for my reference and trial for this competition . I have learnt this from sensei @christofhenkel \n\nhttps://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings"},{"metadata":{},"cell_type":"markdown","source":"## Let us first see what I have done in my public Kernel .\n#### We will firt load our data and fasttext crawl embedding and apply the same preprocessing as the public kernel ."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"## Load data\ntrain = pd.read_csv('/kaggle/input/google-quest-challenge/train.csv')\ntest = pd.read_csv('/kaggle/input/google-quest-challenge/test.csv')\nprint(\"Train shape : \",train.shape)\nprint(\"Test shape : \",test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Building vocubulary from our Quest Data\ndef build_vocab(sentences, verbose =  True):\n    \"\"\"\n    :param sentences: list of list of words\n    :return: dictionary of words and their count\n    \"\"\"\n    vocab = {}\n    for sentence in tqdm(sentences, disable = (not verbose)):\n        for word in sentence:\n            try:\n                vocab[word] += 1\n            except KeyError:\n                vocab[word] = 1\n    return vocab\n\n##Apply the vocab function to get the words and the corresponding counts\nsentences = train[\"question_body\"].progress_apply(lambda x: x.split()).values\nvocab = build_vocab(sentences)\nprint({k: vocab[k] for k in list(vocab)[:20]})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Here I am Loading the fasttext crawl embedding using .vec file and gensim library . It takes around 10 minutes +"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time \nfrom gensim.models import KeyedVectors\n\nnews_path = '/kaggle/input/fasttext-crawl-300d-2m/crawl-300d-2M.vec'\nembeddings_index = KeyedVectors.load_word2vec_format(news_path, binary=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nimport operator \n## This is a common function to check coverage between our quest data and the word embedding\ndef check_coverage(vocab,embeddings_index):\n    a = {}\n    oov = {}\n    k = 0\n    i = 0\n    for word in tqdm(vocab):\n        try:\n            a[word] = embeddings_index[word]\n            k += vocab[word]\n        except:\n\n            oov[word] = vocab[word]\n            i += vocab[word]\n            pass\n\n    print('Found embeddings for {:.2%} of vocab'.format(len(a) / len(vocab)))\n    print('Found embeddings for  {:.2%} of all text'.format(k / (k + i)))\n    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n\n    return sorted_x","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Here we are checking base out of vocabulary word %"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\noov = check_coverage(vocab,embeddings_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## List 10 out of vocabulary word\noov[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now apply the Public Kernel preprocessing functions one by one and try to see the impact of OOV %"},{"metadata":{"trusted":true},"cell_type":"code","source":"def decontract(text):\n    text = re.sub(r\"(W|w)on(\\'|\\’)t \", \"will not \", text)\n    text = re.sub(r\"(C|c)an(\\'|\\’)t \", \"can not \", text)\n    text = re.sub(r\"(Y|y)(\\'|\\’)all \", \"you all \", text)\n    text = re.sub(r\"(Y|y)a(\\'|\\’)ll \", \"you all \", text)\n    text = re.sub(r\"(I|i)(\\'|\\’)m \", \"i am \", text)\n    text = re.sub(r\"(A|a)isn(\\'|\\’)t \", \"is not \", text)\n    text = re.sub(r\"n(\\'|\\’)t \", \" not \", text)\n    text = re.sub(r\"(\\'|\\’)re \", \" are \", text)\n    text = re.sub(r\"(\\'|\\’)d \", \" would \", text)\n    text = re.sub(r\"(\\'|\\’)ll \", \" will \", text)\n    text = re.sub(r\"(\\'|\\’)t \", \" not \", text)\n    text = re.sub(r\"(\\'|\\’)ve \", \" have \", text)\n    return text\ntrain[\"question_body\"] = train[\"question_body\"].progress_apply(lambda x: decontract(x))\nsentences = train[\"question_body\"].apply(lambda x: x.split())\nvocab = build_vocab(sentences)\noov = check_coverage(vocab,embeddings_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_apostrophes(x):\n    apostrophes = [\"’\", \"‘\", \"´\", \"`\"]\n    for s in apostrophes:\n        x = re.sub(s, \"'\", x)\n    return x\n\ntrain[\"question_body\"] = train[\"question_body\"].progress_apply(lambda x: clean_apostrophes(x))\nsentences = train[\"question_body\"].apply(lambda x: x.split())\nvocab = build_vocab(sentences)\noov = check_coverage(vocab,embeddings_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# clean weird / special characters\n\nletter_mapping = {'\\u200b':' ', 'ũ': \"u\", 'ẽ': 'e', 'é': \"e\", 'á': \"a\", 'ķ': 'k', \n                  'ï': 'i', 'Ź': 'Z', 'Ż': 'Z', 'Š': 'S', 'Π': ' pi ', 'Ö': 'O', \n                  'É': 'E', 'Ñ': 'N', 'Ž': 'Z', 'ệ': 'e', '²': '2', 'Å': 'A', 'Ā': 'A',\n                  'ế': 'e', 'ễ': 'e', 'ộ': 'o', '⧼': '<', '⧽': '>', 'Ü': 'U', 'Δ': 'delta',\n                  'ợ': 'o', 'İ': 'I', 'Я': 'R', 'О': 'O', 'Č': 'C', 'П': 'pi', 'В': 'B', 'Φ': \n                  'phi', 'ỵ': 'y', 'օ': 'o', 'Ľ': 'L', 'ả': 'a', 'Γ': 'theta', 'Ó': 'O', 'Í': 'I',\n                  'ấ': 'a', 'ụ': 'u', 'Ō': 'O', 'Ο': 'O', 'Σ': 'sigma', 'Â': 'A', 'Ã': 'A', 'ᗯ': 'w', \n                  'ᕼ': \"h\", \"ᗩ\": \"a\", \"ᖇ\": \"r\", \"ᗯ\": \"w\", \"O\": \"o\", \"ᗰ\": \"m\", \"ᑎ\": \"n\", \"ᐯ\": \"v\", \"н\": \n                  \"h\", \"м\": \"m\", \"o\": \"o\", \"т\": \"t\", \"в\": \"b\", \"υ\": \"u\",  \"ι\": \"i\",\"н\": \"h\", \"č\": \"c\", \"š\":\n                  \"s\", \"ḥ\": \"h\", \"ā\": \"a\", \"ī\": \"i\", \"à\": \"a\", \"ý\": \"y\", \"ò\": \"o\", \"è\": \"e\", \"ù\": \"u\", \"â\": \n                  \"a\", \"ğ\": \"g\", \"ó\": \"o\", \"ê\": \"e\", \"ạ\": \"a\", \"ü\": \"u\", \"ä\": \"a\", \"í\": \"i\", \"ō\": \"o\", \"ñ\": \"n\",\n                  \"ç\": \"c\", \"ã\": \"a\", \"ć\": \"c\", \"ô\": \"o\", \"с\": \"c\", \"ě\": \"e\", \"æ\": \"ae\", \"î\": \"i\", \"ő\": \"o\", \"å\": \n                  \"a\", \"Ä\": \"A\",\"&gt\":\" greater than\",\"&lt\" :\"lesser than\", \"(not\" : \"not\" , \"});\":\"\",\">\" :\"greater\",\"<\":\"lesser\" ,\"$\":\"dollar\",\"\\\\\\\\\":\" \",\"\\\\\": \" \"} \n\ndef clean_special_chars(text):\n    new_text = ''\n    for i in range(len(text)):\n        if i in letter_mapping:\n            c = letter_mapping[i]\n        else:\n            c = text[i]\n        new_text += c\n    return new_text\n\ntrain[\"question_body\"] = train[\"question_body\"].progress_apply(lambda x: clean_special_chars(x))\nsentences = train[\"question_body\"].apply(lambda x: x.split())\nvocab = build_vocab(sentences)\noov = check_coverage(vocab,embeddings_index)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove useless punctuations\n\nuseless_punct = ['च', '不', 'ঢ়', '平', 'ᠠ', '錯', '判', '∙',\n                 '言', 'ς', 'ل', '្', 'ジ', 'あ', '得', '水', 'ь', '◦', '创', \n                 '康', '華', 'ḵ', '☺', '支', '就', '„', '」', '어', '谈', '陈', '团', '腻', '权', \n                 '年', '业', 'マ', 'य', 'ا', '売', '甲', '拼', '˂', 'ὤ', '贯', '亚', 'ि', '放', 'ʻ', 'ទ', 'ʖ', \n                 '點', '્', '発', '青', '能', '木', 'д', '微', '藤', '̃', '僕', '妒', '͜', 'ន', 'ध', '이', '希', '特',\n                 'ड', '¢', '滢', 'ส', '나', '女', 'క', '没', '什', 'з', '天', '南', 'ʿ', 'ค', 'も', '凰', '步', '籍', '西',\n                 'ำ', '−', 'л', 'ڤ', 'ៃ', '號', 'ص', 'स', '®', 'ʋ', '批', 'រ', '치', '谢', '生', '道', '═', '下', '俄', 'ɖ',\n                 '觀', 'வ', '—', 'ی', '您', '♥', '一', 'や', '⊆', 'ʌ', '語', 'ี', '兴', '惶', '瀛', '狐', '⁴', 'प', '臣', 'ద',\n                 '―', 'ì', 'ऌ', 'ీ', '自', '信', '健', '受', 'ɨ', '시', 'י', 'ছ', '嬛', '湾', '吃', 'ち', 'ड़', '反', '红', '有',\n                 '配', 'ে', 'ឯ', '宮', 'つ', 'μ', '記', '口', '℅ι', 'ो', '狸', '奇', 'о', 'ट', '聖', '蘭', '読', 'ū', '標', '要', \n                 'ត', '识', 'で', '汤', 'ま', 'ʀ', '局', 'リ', '्', 'ไ', '呢', '工', 'ल', '沒', 'τ', 'ិ', 'ö', 'せ', '你', 'ん', 'ュ', \n                 '枚', '部', '大', '罗', 'হ', 'て', '表', '报', '攻', 'ĺ', 'ฉ', '∩', '宝', '对', '字', '文', '这', '∑', '髪', 'り', '่', '능',\n                 '罢', '내', '阻', '为', '菲', 'ي', 'न', 'ί', 'ɦ', '開', '†', '茹', '做', '東', 'ত', 'に', 'ت', '晓', '키', '悲', 'સ', \n                 '好', '›', '上', '存', '없', '하', '知', 'ធ', '斯', ' ', '授', 'ł', '傳', '兰', '封', 'ோ', 'و', 'х', 'だ', '人', '太', \n                 '品', '毒', 'ᡳ', '血', '席', '剔', 'п', '蛋', '王', '那', '梦', 'ី', '彩', '甄', 'и', '柏', 'ਨ', '和', '坊', '⌚', '广', \n                 '依', '∫', 'į', '故', 'ś', 'ऊ', '几', '日', 'ک', '音', '×', '”', '▾', 'ʊ', 'ज', 'ด', 'ठ', 'उ', 'る', '清', 'ग', 'ط',\n                 'δ', 'ʏ', '官', '∛', '়', '้', '男', '骂', '复', '∂', 'ー', '过', 'য', '以', '短', '翻', 'র', '教', '儀', 'ɛ', '‹', 'へ', \n                 '¾', '合', '学', 'ٌ', '학', '挑', 'ष', '比', '体', 'م', 'س', 'អ', 'ת', '訓', '∀', '迎', 'វ', 'ɔ', '٨', '▒', '化', 'చ', '‛', \n                 'প', 'º', 'น', '업', '说', 'ご', '¸', '₹', '儿', '︠', '게', '骨', 'ท', 'ऋ', 'ホ', '茶', '는', 'જ', 'ุ', '羡', '節', 'ਮ', \n                 'উ', '番', 'ড়', '讲', 'ㅜ', '등', '伟', 'จ', '我', 'ล', 'す', 'い', 'ញ', '看', 'ċ', '∧', 'भ', 'ઘ', 'ั', 'ម', '街', 'ય', \n                 '还', '鰹', 'ខ', 'ు', '訊', 'म', 'ю', '復', '杨', 'ق', 'त', '金', '味', 'ব', '风', '意', '몇', '佬', '爾', '精', '¶', \n                 'ం', '乱', 'χ', '교', 'ה', '始', 'ᠰ', '了', '个', '克', '্', 'ห', '已', 'ʃ', 'わ', '新', '译', '︡', '本', 'ง', 'б', 'け', \n                 'ి', '明', '¯', '過', 'ك', 'ῥ', 'ف', 'ß', '서', '进', 'ដ', '样', '乐', '寧', '€', 'ณ', 'ル', '乡', '子', 'ﬁ', 'ج', '慕',\n                 '–', 'ᡵ', 'Ø', '͡', '제', 'Ω', 'ប', '絕', '눈', 'फ', 'ম', 'గ', '他', 'α', 'ξ', '§', 'ஜ', '黎', 'ね', '복', 'π', 'ú', '鸡',\n                 '话', '会', 'ক', '八', '之', '북', 'ن', '¦', '가', 'ו', '恋', '地', 'ῆ', '許', '产', 'ॡ', 'ش', '़', '野', 'ή', 'ɒ', '啧',\n                 'យ', '᠌', 'ᠨ', 'ب', '皎', '老', '公', '☆', 'व', 'ি', 'ល', 'ر', 'គ', '행', 'ង', 'ο', '让', 'ំ', 'λ', 'خ', 'ἰ', '家',\n                 'ট', 'ब', '理', '是', 'め', 'र', '√', '기', 'ν', '玉', '한', '入', 'ד', '别', 'د', 'ะ', '电', 'ા', '♫', 'ع', 'ં', '堵',\n                 '嫉', '伊', 'う', '千', '관', '篇', 'क', '非', '荣', '粵', '瑜', '英', '를', '美', '条', '`', '宋', '←', '수', '後', '•',\n                 '³', 'ी', '고', '肉', '℃', 'し', '漢', '싱', 'ϵ', '送', 'ه', '落', 'న', 'ក', 'க', 'ℇ', 'た', 'ះ', '中', '射', '♪', '符',\n                 'ឃ', '谷', '分', '酱', 'び', 'থ', 'ة', 'г', 'σ', 'と', '楚', '胡', '饭', 'み', '禮', '主', '直', '÷', '夢', 'ɾ', 'চ', '⃗',\n                 '統', '高', '顺', '据', 'ら', '頭', 'よ', '最', 'ా', 'ੁ', '亲', 'ស', '花', '≡', '眼', '病', '…', 'の', '發', 'ா', '汝',\n                 '★', '氏', 'ร', '景', 'ᡠ', '读', '件', '仲', 'শ', 'お', 'っ', 'پ', 'ᡤ', 'ч', '♭', '悠', 'ं', '六', '也', 'ռ', 'য়', '恐', \n                 'ह', '可', '啊', '莫', '书', '总', 'ষ', 'ք', '̂', '간', 'な', '此', '愛', 'ర', 'ใ', '陳', 'Ἀ', 'ण', '望', 'द', '请', '油',\n                 '露', '니', 'ş', '宗', 'ʍ', '鳳', 'अ', '邋', '的', 'ព', '火', 'ा', 'ก', '約', 'ட', '章', '長', '商', '台', '勢', 'さ',\n                 '국', 'Î', '簡', 'ई', '∈', 'ṭ', '經', '族', 'ु', '孫', '身', '坑', 'স', '么', 'ε', '失', '殺', 'ž', 'ર', 'が', '手',\n                 'ា', '心', 'ਾ', '로', '朝', '们', '黒', '欢', '早', '️', 'া', 'आ', 'ɸ', '常', '快', '民', 'ﷺ', 'ូ', '遢', 'η', '国', \n                 '无', '江', 'ॠ', '「', 'ন', '™', 'ើ', 'ζ', '紫', 'ె', 'я', '“', '♨', '國', 'े', 'อ', '∞', \n                  '\\n', \"{\\n', '}\\n\", \"=&gt;\", '}\\n\\n', '-&gt;', '\\n\\ni', '&lt;','/&gt;\\n','{\\n\\n','\\\\','|','&','\\\\n\\\\n',\"\\\\appendix\"]\nuseless_punct.remove(' ')\n\ndef remove_useless_punct(text):\n    return re.sub(f'{\"|\".join(useless_punct)}', '', text)\ntrain[\"question_body\"] = train[\"question_body\"].progress_apply(lambda x: remove_useless_punct(x))\nsentences = train[\"question_body\"].apply(lambda x: x.split())\nvocab = build_vocab(sentences)\noov = check_coverage(vocab,embeddings_index)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let us now track the changes properly and see if we can get better coverage by systematic preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"## ReLoad data\ntrain = pd.read_csv('/kaggle/input/google-quest-challenge/train.csv')\ntest = pd.read_csv('/kaggle/input/google-quest-challenge/test.csv')\nprint(\"Train shape : \",train.shape)\nprint(\"Test shape : \",test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text(x):\n\n    x = str(x)\n    for punct in \"/-'\":\n        x = x.replace(punct, ' ')\n    for punct in '&':\n        x = x.replace(punct, f' {punct} ')\n    for punct in '?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~' + '“”’':\n        x = x.replace(punct, '')\n    return x\n\ntrain[\"question_body\"] = train[\"question_body\"].progress_apply(lambda x: clean_text(x))\nsentences = train[\"question_body\"].apply(lambda x: x.split())\nvocab = build_vocab(sentences)\n\noov = check_coverage(vocab,embeddings_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oov[:500]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Checking how the numbers are present in the crawl embedding .\n'1234567'in embeddings_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'14528' in embeddings_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\ndef clean_numbers(x):\n\n    x = re.sub('[0-9]{5,}', '12345', x)\n    x = re.sub('[0-9]{4}', '1234', x)\n    x = re.sub('[0-9]{3}', '123', x)\n    x = re.sub('[0-9]{2}', '12', x)\n    return x\n\ntrain[\"question_body\"] = train[\"question_body\"].progress_apply(lambda x: clean_numbers(x))\nsentences = train[\"question_body\"].apply(lambda x: x.split())\nvocab = build_vocab(sentences)\noov = check_coverage(vocab,embeddings_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oov[:20]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def _get_mispell(mispell_dict):\n    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n    return mispell_dict, mispell_re\n\n\nmispell_dict = {'colour':'color',\n                'centre':'center',\n                'didnt':'did not',\n                'doesnt':'does not',\n                'isnt':'is not',\n                'shouldnt':'should not',\n                'favourite':'favorite',\n                'travelling':'traveling',\n                'counselling':'counseling',\n                'theatre':'theater',\n                'cancelled':'canceled',\n                'labour':'labor',\n                'organisation':'organization',\n                'wwii':'world war 2',\n                'citicise':'criticize',\n                'instagram': 'social medium',\n                'whatsapp': 'social medium',\n                'snapchat': 'social medium'\n\n                }\nmispellings, mispellings_re = _get_mispell(mispell_dict)\n\ndef replace_typical_misspell(text):\n    def replace(match):\n        return mispellings[match.group(0)]\n\n    return mispellings_re.sub(replace, text)\n\ntrain[\"question_body\"] = train[\"question_body\"].progress_apply(lambda x: replace_typical_misspell(x))\nsentences = train[\"question_body\"].progress_apply(lambda x: x.split())\nto_remove = ['a','to','of','and']\nsentences = [[word for word in sentence if not word in to_remove] for sentence in tqdm(sentences)]\nvocab = build_vocab(sentences)\noov = check_coverage(vocab,embeddings_index)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## We can clearly see that the correct preprocessing has increased the coverage"},{"metadata":{},"cell_type":"markdown","source":"### Let us try GloVe preprocessing from pickle"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')\n\n\ndef load_embeddings(path):\n    with open(path,'rb') as f:\n        emb_arr = pickle.load(f)\n    return emb_arr\n\ndef build_matrix(word_index, path):\n    embedding_index = load_embeddings(path)\n    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n    unknown_words = []\n    \n    for word, i in word_index.items():\n        try:\n            embedding_matrix[i] = embedding_index[word]\n        except KeyError:\n            unknown_words.append(word)\n    return embedding_matrix, unknown_words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Reload the data \ntrain = pd.read_csv('/kaggle/input/google-quest-challenge/train.csv')\ntest = pd.read_csv('/kaggle/input/google-quest-challenge/test.csv')\nprint(\"Train shape : \",train.shape)\nprint(\"Test shape : \",test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GLOVE_EMBEDDING_PATH = '/kaggle/input/pickled-glove840b300d-for-10sec-loading/glove.840B.300d.pkl' ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import operator \n\ndef check_coverage(vocab,embeddings_index):\n    a = {}\n    oov = {}\n    k = 0\n    i = 0\n    for word in tqdm(vocab):\n        try:\n            a[word] = embeddings_index[word]\n            k += vocab[word]\n        except:\n\n            oov[word] = vocab[word]\n            i += vocab[word]\n            pass\n\n    print('Found embeddings for {:.2%} of vocab'.format(len(a) / len(vocab)))\n    print('Found embeddings for  {:.2%} of all text'.format(k / (k + i)))\n    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n\n    return sorted_x\n\ndef build_vocab(sentences, verbose =  True):\n    \"\"\"\n    :param sentences: list of list of words\n    :return: dictionary of words and their count\n    \"\"\"\n    vocab = {}\n    for sentence in tqdm(sentences, disable = (not verbose)):\n        for word in sentence:\n            try:\n                vocab[word] += 1\n            except KeyError:\n                vocab[word] = 1\n    return vocab","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tic = time.time()\nglove_embeddings = load_embeddings(GLOVE_EMBEDDING_PATH)\nprint(f'loaded {len(glove_embeddings)} word vectors in {time.time()-tic}s')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab = build_vocab(list(train['question_body'].apply(lambda x:x.split())))\noov = check_coverage(vocab,glove_embeddings)\noov[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def decontract(text):\n    text = re.sub(r\"(W|w)on(\\'|\\’)t \", \"will not \", text)\n    text = re.sub(r\"(C|c)an(\\'|\\’)t \", \"can not \", text)\n    text = re.sub(r\"(Y|y)(\\'|\\’)all \", \"you all \", text)\n    text = re.sub(r\"(Y|y)a(\\'|\\’)ll \", \"you all \", text)\n    text = re.sub(r\"(I|i)(\\'|\\’)m \", \"i am \", text)\n    text = re.sub(r\"(A|a)isn(\\'|\\’)t \", \"is not \", text)\n    text = re.sub(r\"n(\\'|\\’)t \", \" not \", text)\n    text = re.sub(r\"(\\'|\\’)re \", \" are \", text)\n    text = re.sub(r\"(\\'|\\’)d \", \" would \", text)\n    text = re.sub(r\"(\\'|\\’)ll \", \" will \", text)\n    text = re.sub(r\"(\\'|\\’)t \", \" not \", text)\n    text = re.sub(r\"(\\'|\\’)ve \", \" have \", text)\n    return text\n\ndef clean_text(x):\n\n    x = str(x)\n    for punct in \"/-'\":\n        x = x.replace(punct, ' ')\n    for punct in '&':\n        x = x.replace(punct, f' {punct} ')\n    for punct in '?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~' + '“”’':\n        x = x.replace(punct, '')\n    return x\n\ndef clean_numbers(x):\n\n    x = re.sub('[0-9]{5,}', '12345', x)\n    x = re.sub('[0-9]{4}', '1234', x)\n    x = re.sub('[0-9]{3}', '123', x)\n    x = re.sub('[0-9]{2}', '12', x)\n    return x\n\ndef preprocess(x):\n    x= decontract(x)\n    x=clean_text(x)\n    x=clean_number(x)\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"question_body\"] = train[\"question_body\"].progress_apply(lambda x: decontract(x))\nsentences = train[\"question_body\"].apply(lambda x: x.split())\nvocab = build_vocab(sentences)\noov = check_coverage(vocab,glove_embeddings)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"question_body\"] = train[\"question_body\"].progress_apply(lambda x: clean_text(x))\nsentences = train[\"question_body\"].apply(lambda x: x.split())\nvocab = build_vocab(sentences)\noov = check_coverage(vocab,glove_embeddings)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oov[:500]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'1234567' in glove_embeddings","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"question_body\"] = train[\"question_body\"].progress_apply(lambda x: clean_numbers(x))\nsentences = train[\"question_body\"].apply(lambda x: x.split())\nvocab = build_vocab(sentences)\noov = check_coverage(vocab,glove_embeddings)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oov[:20]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We are going to replace the public kernel preprocessing with this new preprocessing methods. Remember the order is important . If we remove the special characters and then try to do decontraction , you will not find a word to decontract, because I'll has already has become very very Ill."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}