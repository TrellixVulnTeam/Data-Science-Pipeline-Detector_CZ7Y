{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# Understanding the binary cross entropy"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"I have seen that most of the public kernels in this competition are using the **binary cross-entropy** as the loss function. Thus, I think it is very important that we fully understand how this loss works.\n\nThis is a loss function designed for **binary classification** problems, where the classifier can return the predicted probabilities of the positive class. The binary cross-entropy is defined as:\n$$\n\\sum_{i=1}^{N} l(y_i, p(x_i)) = \\sum_{i=1}^{N} - \\left( y_i \\cdot \\log(p(x_i)) + (1-y_i) \\cdot \\log(1-p(x_i))  \\right)\n$$\nwhere $y_i$ is the target variable (can only be $0$ or $1$), $x_i$ are the variables/features, and **$p$ the probability model of being of class $1$**. \n\nYou can see that with a sample of the class 1:\n$$\nl(y_i=1, p(x_i)) = - \\log(p(x_i))\n$$\nand thus the loss is $0$ when $p(x_i) = 1$ (the sample is of class 1, and the model outputs a probability 1 of being of class 1), and the loss is $\\infty$ when $p(x_i) = 0$ (the sample is of class 1, and the model outputs a probability 0 of being of class 1). \n\nOn the other hand, when the sample is of the class 0:\n$$\nl(y_i=0, p(x_i)) = - \\log(1-p(x_i))\n$$\nand thus the loss is $0$ when $p(x_i) = 0$ (the sample is of class 0, and the model outputs a probability 0 of being of class 1), and the loss is $\\infty$ when $p(x_i) = 1$ (the sample is of class 0, and the model outputs a probability 1 of being of class 1)."},{"metadata":{},"cell_type":"markdown","source":"***\n### Let's see it graphically"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I first define a function to compute $l(y_i, p(x_i))$ in a pairwise way:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def cross_entropy(yreal, ypred):\n    with np.errstate(divide='ignore', invalid='ignore'):\n        return -1*(yreal*np.log(ypred) + (1-yreal)*np.log(1-ypred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below you can see the values of $l(y_i, p(x_i))$ when all the samples are of class 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"yreal = np.ones(50)\nypred = np.linspace(0., 1., 50)\n\nplt.figure(figsize=(15,5))\nplt.plot(ypred, cross_entropy(yreal, ypred), \"o--\")\nplt.xlabel(\"prediction\", fontsize=23)\nplt.ylabel(\"loss\", fontsize=23)\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"and the values of $l(y_i, p(x_i))$ when all the samples are of class 0"},{"metadata":{"trusted":true},"cell_type":"code","source":"yreal = np.zeros(50)\nypred = np.linspace(0., 1., 50)\n\nplt.figure(figsize=(15,5))\nplt.plot(ypred, cross_entropy(yreal, ypred), \"o--\")\nplt.xlabel(\"prediction\", fontsize=23)\nplt.ylabel(\"loss\", fontsize=23)\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***\n### What does it happen when we use binary cross entropy and $y$ is not binary? \n\nIn this competition we are using binary cross-entropy, however, the target variable can take any value between 0 and 1. Let's see what happen with this loss function as we move the true target value between 0 and 1: "},{"metadata":{"trusted":true},"cell_type":"code","source":"for x in np.arange(0, 1.01, 0.1):\n    yreal = x*np.ones(50)\n    ypred = np.linspace(0., 1., 50)\n\n    plt.figure(figsize=(15,5))\n    plt.plot(ypred, cross_entropy(yreal, ypred), \"o--\")\n    plt.title(f\"true target: {x}\", fontsize=19)\n    plt.xlabel(\"prediction\", fontsize=23)\n    plt.ylabel(\"loss\", fontsize=23)\n    plt.grid()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems ok, right? The loss function takes its minimum when the prediction is equal to the true target. But there are two \"little\" problems:\n1. **The loss is not symmetric!**. For example, when the true target is 0.4, a sample with prediction 0 will get a higher penalization than a sample with prediction 0.8, but both are equidistant to the true target.\n2. And even more important, **the minimum value of the loss is not 0 when the true target is not 0 or 1** (see carefully the y-axis). \n\nFurthermore, the minimum value of the loss **depends on the true target value!**. If you do the math, you will see that $l(y_i, p(x_i))$ reaches its minimum when $p(x_i) = y_i$. \n\nThe figure in the cell below shows the minimal loss as a function of the true target value:"},{"metadata":{"trusted":true},"cell_type":"code","source":"true_target = np.linspace(0.001, 0.999, 50)\nmin_loss = [cross_entropy(x, x) for x in true_target]\n\nplt.figure(figsize=(15,5))\nplt.plot(true_target, min_loss, \"o--\")\nplt.xlabel(\"True target\", fontsize=23)\nplt.ylabel(\"Minimal loss\", fontsize=23)\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For these two reasons, I think that binary cross-entropy is **not an appropriate loss function for this problem**. \n\nWhat do you think?\n***"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}