{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ROOT = '../input/google-quest-challenge/'\n\n# test_df = pd.read_csv(ROOT+'test.csv')\n# print(test_df.shape)\n# train_df = pd.read_csv(ROOT+'train.csv')\n# print(train_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pytorch_transformers import AdamW","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_cols = ['question_asker_intent_understanding', 'question_body_critical', \n               'question_conversational', 'question_expect_short_answer', \n               'question_fact_seeking', 'question_has_commonly_accepted_answer', \n               'question_interestingness_others', 'question_interestingness_self', \n               'question_multi_intent', 'question_not_really_a_question', \n               'question_opinion_seeking', 'question_type_choice', \n               'question_type_compare', 'question_type_consequence', \n               'question_type_definition', 'question_type_entity', \n               'question_type_instructions', 'question_type_procedure', \n               'question_type_reason_explanation', 'question_type_spelling', \n               'question_well_written', 'answer_helpful', \n               'answer_level_of_information', 'answer_plausible', \n               'answer_relevance', 'answer_satisfaction', \n               'answer_type_instructions', 'answer_type_procedure', \n               'answer_type_reason_explanation', 'answer_well_written']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nfrom torch.utils.data import TensorDataset\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import RandomSampler, SequentialSampler\nfrom pytorch_transformers import BertTokenizer\nfrom sklearn.preprocessing import MinMaxScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nn_gpu = torch.cuda.device_count()\ntorch.cuda.get_device_name(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1\\get train data and test data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom pathlib import Path\nimport pickle\n\n\ndef read_data(train):\n#     train = pd.read_csv(raw_data_path, encoding='utf-8').sample(10)\n    targets = [-1] * len(train)\n    #targets = np.zeros(shape=(len(test),31))\n    sentence_a = train['question_title'] + train['question_body']\n    sentence_b = train['answer']\n\n    return targets, sentence_a, sentence_b\n\n# def save_pickle(data, file_path):\n#     '''\n#     :param data:\n#     :param file_name:\n#     :param pickle_path:\n#     :return:\n#     '''\n#     if isinstance(file_path, Path):\n#         file_path = str(file_path)\n#     with open(file_path, 'wb') as f:\n#         pickle.dump(data, f)\n\n\nROOT = '../input/google-quest-challenge/'\ntrain = pd.read_csv(ROOT+'train.csv')\n# print(train.shape())\ntest = pd.read_csv(ROOT+'test.csv')\n# train = pd.read_csv(ROOT, encoding='utf-8').sample(10)\ny, X_a, X_b  = read_data(train)\nyt, X_at, X_bt  = read_data(test)\n#get train data\ndata = []\nfor step, (data_x_a, data_x_b, data_y) in enumerate(zip(X_a, X_b, y)):\n    data.append(([data_x_a, data_x_b], data_y))\n#get test data\ndatat = []\nfor step, (data_x_a, data_x_b, data_y) in enumerate(zip(X_at, X_bt, yt)):\n    datat.append(([data_x_a, data_x_b], data_y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = BertTokenizer(\"/kaggle/input/bertpretrained/uncased_L-24_H-1024_A-16/uncased_L-24_H-1024_A-16/vocab.txt\", True)\nclass InputExample(object):\n    def __init__(self, guid, text_a, text_b=None, label=None):\n        \"\"\"Constructs a InputExample.\n        Args:\n            guid: Unique id for the example.\n            text_a: string. The untokenized text of the first sequence. For single\n            sequence tasks, only this sequence must be specified.\n            text_b: (Optional) string. The untokenized text of the second sequence.\n            Only must be specified for sequence pair tasks.\n            label: (Optional) string. The label of the example. This should be\n            specified for train and dev examples, but not for test examples.\n        \"\"\"\n        self.guid = guid\n        self.text_a = text_a\n        self.text_b = text_b\n        self.label = label\n        \nclass InputFeature(object):\n    '''\n    A single set of features of data.\n    '''\n\n    def __init__(self, input_ids, input_mask, segment_ids, label_id, input_len):\n        self.input_ids = input_ids\n        self.input_mask = input_mask\n        self.segment_ids = segment_ids\n        self.label_id = label_id\n        self.input_len = input_len\n        \ndef create_examples(lines, example_type):\n    '''\n    Creates examples for data\n    '''\n    examples = []\n    for i, line in enumerate(lines):\n        guid = '%s-%d' % (example_type, i)\n        text_a = line[0][0]\n        text_b = line[0][1]\n        label = line[1]\n        example = InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label)\n        examples.append(example)\n    return examples\n\ndef truncate_seq_pair(tokens_a, tokens_b, max_length=512):\n    # This is a simple heuristic which will always truncate the longer sequence\n    # one token at a time. This makes more sense than truncating an equal percent\n    # of tokens from each, since if one sequence is very short then each token\n    # that's truncated likely contains more information than a longer sequence.\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_length:\n            break\n        if len(tokens_a) > len(tokens_b):\n            tokens_a.pop()\n        else:\n            tokens_b.pop()\n                \ndef create_features(examples, i, max_seq_len=512):\n    '''\n    # The convention in BERT is:\n    # (a) For sequence pairs:\n    #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n    #  type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1\n    # (b) For single sequences:\n    #  tokens:   [CLS] the dog is hairy . [SEP]\n    #  type_ids:   0   0   0   0  0     0   0\n    '''\n    features = []\n    for ex_id, example in enumerate(examples):\n        tokenizer = BertTokenizer(\"/kaggle/input/bertpretrained/uncased_L-24_H-1024_A-16/uncased_L-24_H-1024_A-16/vocab.txt\", True)\n        tokens_a = tokenizer.tokenize(example.text_a)\n        tokens_b = None\n        label_id = example.label\n\n        if example.text_b:\n            tokens_b = tokenizer.tokenize(example.text_b)\n            # Modifies `tokens_a` and `tokens_b` in place so that the total\n            # length is less than the specified length.\n            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n            truncate_seq_pair(tokens_a, tokens_b, max_length=max_seq_len - 3)\n        else:\n            # Account for [CLS] and [SEP] with '-2'\n            if len(tokens_a) > max_seq_len - 2:\n                tokens_a = tokens_a[:max_seq_len - 2]\n        tokens = ['[CLS]'] + tokens_a + ['[SEP]']\n        segment_ids = [0] * len(tokens)\n        if tokens_b:\n            tokens += tokens_b + ['[SEP]']\n            segment_ids += [1] * (len(tokens_b) + 1)\n\n        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n        input_mask = [1] * len(input_ids)\n        padding = [0] * (max_seq_len - len(input_ids))\n        input_len = len(input_ids)\n\n        input_ids += padding\n        input_mask += padding\n        segment_ids += padding\n        i = i + 1\n        print(i)\n        assert len(input_ids) == max_seq_len\n        assert len(input_mask) == max_seq_len\n        assert len(segment_ids) == max_seq_len\n\n        feature = InputFeature(input_ids=input_ids,\n                                input_mask=input_mask,\n                                segment_ids=segment_ids,\n                                label_id=label_id,\n                                input_len=input_len)\n        features.append(feature)\n    return features\n\ntrain_examples = create_examples(data, 'train')\ntrain_features = create_features(train_examples,0)\n\ntest_examples = create_examples(datat,'test')\ntest_features = create_features(test_examples,0)\n\noutput_categories = target_cols\nOutput = np.asarray(train[output_categories], dtype = \"float64\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LABEL_WEIGHTS = torch.tensor(1.0 / train[output_categories].std().values, dtype=torch.float32)\nLABEL_WEIGHTS = LABEL_WEIGHTS / LABEL_WEIGHTS.sum() * 30\nfor name, weight in zip(output_categories, LABEL_WEIGHTS.cpu().numpy()):\n    print(name, \"\\t\", weight)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_dataset(features, Output, is_sorted=False):\n    # Convert to Tensors and build dataset\n    if is_sorted:\n        logger.info(\"sorted data by th length of input\")\n        features = sorted(features, key=lambda x: x.input_len, reverse=True)\n    output = torch.tensor(Output)\n    \n    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n    \n    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n    all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n    all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.float)\n    dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids,output)\n    return dataset\n\ndef create_dataset_for_predict(features, is_sorted=False):\n    # Convert to Tensors and build dataset\n    if is_sorted:\n        logger.info(\"sorted data by th length of input\")\n        features = sorted(features, key=lambda x: x.input_len, reverse=True)\n    \n    \n    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n    \n    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n    all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n    all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.float)\n    dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n    return dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = create_dataset(train_features,Output)\ntrain_sampler = SequentialSampler(train_dataset)\ntrain_dataloader = DataLoader(train_dataset,sampler=train_sampler,batch_size=3)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dataset = create_dataset_for_predict(test_features)\ntest_sampler = SequentialSampler(test_dataset)\ntest_dataloader = DataLoader(test_dataset,sampler=test_sampler,batch_size=16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn as nn\nfrom pytorch_transformers.modeling_bert import BertPreTrainedModel, BertModel\n\n\nclass BertForMultiClass(BertPreTrainedModel):\n    def __init__(self, config):\n        super(BertForMultiClass, self).__init__(config)\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        #self.dropout = nn.ModuleList([nn.Dropout(config.hidden_dropout_prob) for _ in range(2)])\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n        #self.classifier_m = nn.Linear(config.hidden_size, config.num_labels)\n        self.sig = torch.nn.Sigmoid()\n        self.apply(self.init_weights)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, head_mask=None):\n        outputs = self.bert(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask,\n                            head_mask=head_mask)\n        pooled_output = outputs[1]\n        #for i, dropout in enumerate(self.dropout):\n        #    if i == 0:\n        #        logits = self.classifier(dropout(pooled_output))\n        #        logits_m = self.classifier_m(dropout(pooled_output))\n        #    else:\n        #        logits += self.classifier(dropout(pooled_output))\n        #        logits_m += self.classifier_m(dropout(pooled_output))\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n        logits = self.sig(logits)\n        #logits_m = self.classifier_m(pooled_output)\n        return logits#, logits_m #len(self.dropout)\n    \n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm, trange","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def prepare_device(use_gpu=0):\n    \"\"\"\n    setup GPU device if available, move model into configured device\n    # 如果n_gpu_use为数字，则使用range生成list\n    # 如果输入的是一个list，则默认使用list[0]作为controller\n    Example:\n        use_gpu = '' : cpu\n        use_gpu = '0': cuda:0\n        use_gpu = '0,1' : cuda:0 and cuda:1\n     \"\"\"\n    n_gpu_use = [int(x) for x in use_gpu.split(\",\")]\n    if not use_gpu:\n        device_type = 'cpu'\n    else:\n        device_type = f\"cuda:{n_gpu_use[0]}\"\n    n_gpu = torch.cuda.device_count()\n    if len(n_gpu_use) > 0 and n_gpu == 0:\n        device_type = 'cpu'\n    if len(n_gpu_use) > n_gpu:\n        msg = f\"Warning: The number of GPU\\'s configured to use is {n_gpu}, but only {n_gpu} are available on this machine.\"\n        n_gpu_use = range(n_gpu)\n    device = torch.device(device_type)\n    list_ids = n_gpu_use\n    return device, list_ids\n\ndef model_device(n_gpu, model):\n    '''\n    :param n_gpu:\n    :param model:\n    :return:\n    '''\n    device, device_ids = prepare_device(n_gpu)\n    if len(device_ids) > 1:\n        model = torch.nn.DataParallel(model, device_ids=device_ids)\n    if len(device_ids) == 1:\n        os.environ['CUDA_VISIBLE_DEVICES'] = str(device_ids[0])\n    model = model.to(device)\n    return model, device\n\nclass Train(object):\n    def __init__(self, model, n_gpu):\n        self.model = model\n        self.model, self.device = model_device(n_gpu=n_gpu, model=self.model)\n\n    def train(self, data):\n        \n        tr_loss  = 0\n        nb_tr_steps  = 0\n        param_optimizer = list(self.model.named_parameters())\n        no_decay = ['bias', 'gamma', 'beta']\n        optimizer_grouped_parameters = [\n            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n             'weight_decay_rate': 0.01},\n            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n             'weight_decay_rate': 0.0}\n        ]\n\n        optimizer = AdamW(optimizer_grouped_parameters,lr=2e-5)\n        all_logits = None\n        \n        \n        self.model.train()\n        loss_func = torch.nn.L1Loss()\n        epochs = 5\n        for _ in trange(epochs, desc=\"Epoch\"):\n            for step, batch in enumerate(data):\n                batch = tuple(t.to(self.device) for t in batch)\n                input_ids, input_mask, segment_ids, label_ids, outputs = batch\n#                 print(type(outputs))\n                logits = self.model(input_ids, segment_ids, input_mask)\n#                 print(logits)\n#                 print(logits.size())\n    #             if all_logits is None:\n    #                 all_logits = torch.sigmoid(logits).detach().cpu().numpy()\n    #             else:\n    #                 all_logits = np.concatenate([all_logits, torch.sigmoid(logits).detach().cpu().numpy()], axis=0)\n    #         if 'cuda' in str(self.device):\n    #             torch.cuda.empty_cache()\n                loss = loss_func(logits, outputs.float()) \n#                 print(loss)\n        #         print(loss)\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n#                 print(loss.item())\n                tr_loss += loss.item()\n\n                nb_tr_steps += 1\n            print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n#         torch.save(model.state_dict(), \"model_.pkl\")\n        self.model.save_pretrained(\".\")\n        torch.save(self.model, 'bert_finetune1.pkl')\n        return logits\n\n# class Predictor(object):\n#     def __init__(self, model, n_gpu):\n#         self.model = model\n#         self.model, self.device = model_device(n_gpu=n_gpu, model=self.model)\n\n#     def predict(self, data):\n#         all_logits = None\n#         self.model.eval()\n#         with torch.no_grad():\n#             for step, batch in enumerate(data):\n#                 batch = tuple(t.to(self.device) for t in batch)\n#                 input_ids, input_mask, segment_ids, label_ids = batch\n#                 logits = self.model(input_ids, segment_ids, input_mask)\n#                 if all_logits is None:\n#                     all_logits = torch.sigmoid(logits).detach().cpu().numpy()\n#                 else:\n#                     all_logits = np.concatenate([all_logits, torch.sigmoid(logits).detach().cpu().numpy()], axis=0)\n#         if 'cuda' in str(self.device):\n#             torch.cuda.empty_cache()\n#         return all_logits\nclass Predictor(object):\n    def __init__(self, model, n_gpu):\n        self.model = model\n        self.model, self.device = model_device(n_gpu=n_gpu, model=self.model)\n\n    def predict(self, data):\n#         model.load_state_dict(torch.load(\"model_.pkl\"))\n        all_logits = None\n        predictions = []\n        self.model.eval()\n        with torch.no_grad():\n            for step, batch in enumerate(data):\n                batch = tuple(t.to(self.device) for t in batch)\n                input_ids, input_mask, segment_ids, label_ids = batch\n                logits = self.model(input_ids, segment_ids, input_mask)\n                logits = logits.cpu().numpy()\n#                 print(logits)\n#                 if all_logits is None:\n#                     all_logits = logits.detach().cpu().numpy()\n#                 else:\n#                     all_logits = np.concatenate([all_logits, torch.sigmoid(logits).detach().cpu().numpy()], axis=0)\n                for item in logits:\n                    predictions.append(item)\n#                     print(predictions)\n        if 'cuda' in str(self.device):\n            torch.cuda.empty_cache()\n        predictions =  np.array(predictions)\n#         print(predictions)\n        return predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1 = BertForMultiClass.from_pretrained(\"/kaggle/input/bert-fold6/bertmodel\", num_labels=30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nt1 = Train(model=model1, n_gpu='0')\nresult1 = t1.train(data=train_dataloader)\n\n\n\n\n\n\n\n\n# print(Output - result)\n# predictor2 = Predictor(model=model2, n_gpu='0')\n# result2 = predictor2.predict(data=test_dataloader)\n\n# predictor3 = Predictor(model=model3, n_gpu='0')\n# result3 = predictor3.predict(data=test_dataloader)\n\n# predictor4 = Predictor(model=model4, n_gpu='0')\n# result4 = predictor4.predict(data=test_dataloader)\n\n# predictor5 = Predictor(model=model5, n_gpu='0')\n# result5 = predictor5.predict(data=test_dataloader)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p1 = Predictor(model=model1, n_gpu='0')\npredictions = p1.predict(data=test_dataloader)\n#[0.65017515, 0.47032535, 0.264573  , 0.60513866, 0.5674752 ,\n#         0.559137  , 0.579473  , 0.6428144 , 0.40630287, 0.39215457,\n#         0.46353787, 0.42971882, 0.38580528, 0.45621312, 0.3852198 ,\n#         0.50983137, 0.5155569 , 0.4364345 , 0.48921314, 0.33544517,\n#         0.68783385, 0.6582898 , 0.6001512 , 0.567177  , 0.6301677 ,\n#         0.53974164, 0.46124914, 0.3399995 , 0.4909867 , 0.50540936],\n# print(result)\n# print(Output)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame.from_dict({\n    'qa_id': test['qa_id']\n})\nfor i in range(30):\n    submission[output_categories[i]] = predictions[:, i]\nsubmission.to_csv('submission.csv', index=False, float_format='%.20f')\nsubmission\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}