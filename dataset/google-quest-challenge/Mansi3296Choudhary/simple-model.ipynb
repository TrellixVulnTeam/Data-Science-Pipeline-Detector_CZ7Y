{"cells":[{"metadata":{"id":"t0kMQi_z73pD"},"cell_type":"markdown","source":"Overview\n*** Write an overview of the case study that you are working on. (MINIMUM 200 words) ***\n1. When it comes to questions and answers humans have a different way of\nunderstanding and answering the question asked. In this case study we are going\nto understand the same with the help of (Question-Answer) dataset from top\nwebsites.\n2. Humans have a unique way of answering any question asked. We first understand\nthe question, the intuition behind it, the significance of the question etc. With\nmachine it is different.\n3. In this problem, we will help the machine understand these aspects of question\nwith the help of a question-answer dataset from different websites.\n4. In our dataset we have 30 different aspects of question-answer like\n‘question_conversational’, ‘question_body_critical’, ‘answer_helpful’ etc.\n5. Our goal will be to find the scores for each of these aspects with the help of our\ntraining dataset. In our training dataset we have scores associated with each of\nthese labels in the range of [0,1]. We will make our model learn these aspects and\ntry to evaluate the same for our test dataset.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"id":"pUP6xyhR7FQm","outputId":"a399b7aa-e363-4b3c-94c0-04a4136dc5de","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import GroupKFold\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nimport tensorflow_hub as hub\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport gc\nimport os\nfrom scipy.stats import spearmanr\nfrom math import floor, ceil\nimport seaborn as sns\n\nnp.set_printoptions(suppress=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"nep0nFIyTNbI","trusted":true},"cell_type":"code","source":"path='/kaggle/input/google-quest-challenge/'","execution_count":null,"outputs":[]},{"metadata":{"id":"TiENfYRu8pRM"},"cell_type":"markdown","source":"**Reading Our Datsets**","execution_count":null},{"metadata":{"id":"UmkLLZ4q8s3u","outputId":"2327d32d-cc5e-4c5c-832e-76ed337b821a","trusted":true},"cell_type":"code","source":"#train dataset\ntrain_df=pd.read_csv(path+'train.csv')\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"XWO9JoSq9plM","outputId":"b02e47c1-74d1-4ca7-be41-ad4fe348a358","trusted":true},"cell_type":"code","source":"#test data\ntest_df=pd.read_csv(path+'test.csv')\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"Mh75JdFI91B2","outputId":"1ffb9c29-bd24-4db8-a944-8b554187b095","trusted":true},"cell_type":"code","source":"#submission file\nsubmission=pd.read_csv(path+'sample_submission.csv')\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"B6FclWrmY9cR","outputId":"81785c59-e183-4eaf-fd2a-a6fa34a3f5b9","trusted":true},"cell_type":"code","source":"#columns in our train_data\ntrain_columns=train_df.columns\ntest_columns=test_df.columns\nprint('-'*15+'Train Columns'+'-'*15)\nprint(train_columns)\nprint('-'*15+'Train data Shape'+'-'*15)\nprint(train_df.shape)\nprint('-'*15+'Test Columns'+'-'*15)\nprint(test_columns)\nprint('-'*15+'Test data Shape'+'-'*15)\nprint(test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"Nf-076XcZuT_"},"cell_type":"markdown","source":"We thus have 6079 training points and 476 test points/ We have 41 train columns and 11 test columns. For the 30 columns we have to predict value for the test dataset. So, let us have a look at these 30 values","execution_count":null},{"metadata":{"id":"C8GXO8gGaARB","outputId":"886b1060-ee78-4eb1-b2ef-638ed08fd17c","trusted":true},"cell_type":"code","source":"target_values=train_columns[11:]\nprint('Target columns are: ')\ntarget_values","execution_count":null,"outputs":[]},{"metadata":{"id":"tw5FNbYVae-z"},"cell_type":"markdown","source":"For these 30 columns we need to find value for each point in the test dataset in the range of [0,1]","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"jAOak8OhcNQk"},"cell_type":"markdown","source":"Now let us have a look at how these target_values are distributed across our training dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"GTVfVXlXawUf"},"cell_type":"markdown","source":"**Data Cleaning & Featurization**\n\nNow as we have explored our data it is time to text data in our dataset and obtain new features depending on our dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"6Q_Mb937bxMt"},"cell_type":"markdown","source":"**Remove Punctuations & Decontracted Words**","execution_count":null},{"metadata":{"id":"HmOgQ0BFbwp3","trusted":true},"cell_type":"code","source":"# https://stackoverflow.com/a/47091490/4084039\nimport re\n\ndef decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    phrase = re.sub(r\"couldn\\'t\", \" could not\", phrase)\n    phrase = re.sub(r\"couldnt\", \" could not\", phrase)\n    phrase = re.sub(r\"didn\\'t\", \" did not\", phrase)\n    phrase = re.sub(r\"doesn\\'t\", \" does not\", phrase)\n    phrase = re.sub(r\"don\\'t\", \" do not\", phrase)\n    phrase = re.sub(r\"hadn\\'t\", \" had not\", phrase)\n    phrase = re.sub(r\"hasn\\'t\", \" has not\", phrase)\n    phrase = re.sub(r\"haven\\'t\", \" have not\", phrase)\n    phrase = re.sub(r\"he\\'ll\", \" he will\", phrase)\n    phrase = re.sub(r\"he\\'d\", \" he would\", phrase)\n    phrase = re.sub(r\"didn\\'t\", \" did not\", phrase)\n    phrase = re.sub(r\"wasn\\'t\", \" was not\", phrase)\n    phrase = re.sub(r\"you\\'re\", \" you are\", phrase)\n    return phrase","execution_count":null,"outputs":[]},{"metadata":{"id":"5VNkovCEfAH6","trusted":true},"cell_type":"code","source":"#code reference: https://www.kaggle.com/codename007/start-from-here-quest-complete-eda-fe\n\n\ndef clean_text(text):\n    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n    text = text.lower().split()   \n    text = \" \".join(text)\n    return(text)","execution_count":null,"outputs":[]},{"metadata":{"id":"hnXaouT5fWkK","trusted":true},"cell_type":"code","source":"#cleaning of train data\ntrain_df['question_body']=train_df['question_body'].apply(clean_text)\ntrain_df['question_body']=train_df['question_body'].apply(decontracted)\ntrain_df['question_title']=train_df['question_body'].apply(clean_text)\ntrain_df['question_title']=train_df['question_body'].apply(decontracted)\ntrain_df['answer']=train_df['question_body'].apply(clean_text)\ntrain_df['answer']=train_df['question_body'].apply(decontracted)\n\n#cleaning of test data\ntest_df['question_body']=test_df['question_body'].apply(clean_text)\ntest_df['question_body']=test_df['question_body'].apply(decontracted)\ntest_df['question_title']=test_df['question_body'].apply(clean_text)\ntest_df['question_title']=test_df['question_body'].apply(decontracted)\ntest_df['answer']=test_df['question_body'].apply(clean_text)\ntest_df['answer']=test_df['question_body'].apply(decontracted)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"i2s4coY8jN0k"},"cell_type":"markdown","source":"**Printing Our Cleaned Data**","execution_count":null},{"metadata":{"id":"zmI8Wll8jQcx","outputId":"a50ac694-0746-4b4d-fc04-7ce78e6bd17b","trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"rcbgn2zmjWqF","outputId":"4c1bfe6e-3cc9-47c6-8bc2-44f383a318ba","trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"CrZveNXlrDFn"},"cell_type":"markdown","source":"#Training with a Baseline Model","execution_count":null},{"metadata":{"id":"aYI375t_Jev1"},"cell_type":"markdown","source":"Splitting our train data into train and test dataset for our validating our model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"random.seed(10)\nprint(random.sample([1,2,3,4,5,6,7,8,9,10],5))\nprint(random.sample([1,2,3,4,5,6,7,8,9,10],3))\n\nrandom.seed(10)\nprint(random.sample([1,2,3,4,5,6,7,8,9,10],5))\nprint(random.sample([1,2,3,4,5,6,7,8,9,10],3))\n\nrandom.seed(10)\nprint(random.sample([1,2,3,4,5,6,7,8,9,10],5))\nprint(random.sample([1,2,3,4,5,6,7,8,9,10],3))\n","execution_count":null,"outputs":[]},{"metadata":{"id":"NjXZpuFiJnQa","trusted":true},"cell_type":"code","source":"import random\n\nrandom.seed(42)\nvalid_n_test_indexes=random.sample(list(range(1,len(train_df))),int(len(train_df)*0.3))\ntrain_indexes = list(set(list(range(1,len(train_df))))-set(valid_n_test_indexes))\nvalid_indexes=random.sample(valid_n_test_indexes,int(len(train_df)*0.2))\ntest_indexes=list(set(valid_n_test_indexes)-set(valid_indexes))\n                        ","execution_count":null,"outputs":[]},{"metadata":{"id":"pfKTP1PqKOIf","outputId":"25f381e9-c9ce-4d77-e00f-70b4c5dc8f62","trusted":true},"cell_type":"code","source":"X_train=train_df.iloc[train_indexes]\nX_valid=train_df.iloc[valid_indexes]\nX_test=train_df.iloc[test_indexes]\n\nprint('Train data shape ='+str(X_train.shape))\nprint('Valid data shape ='+str(X_valid.shape))\nprint('Test data shape ='+str(X_test.shape))","execution_count":null,"outputs":[]},{"metadata":{"id":"ooCsNNIFGb13","trusted":true},"cell_type":"code","source":"all_words=X_train['question_title']+X_train['question_body']+X_train['answer']","execution_count":null,"outputs":[]},{"metadata":{"id":"eUMcSU_REdEg","trusted":true},"cell_type":"code","source":"tokenizer = tf.keras.preprocessing.text.Tokenizer()\ntokenizer.fit_on_texts(all_words)","execution_count":null,"outputs":[]},{"metadata":{"id":"nU43UfHmFaHO","trusted":true},"cell_type":"code","source":"#for train data\nquest_title = tokenizer.texts_to_sequences(X_train['question_title'])\nquest_body = tokenizer.texts_to_sequences(X_train['question_body'])\nanswer = tokenizer.texts_to_sequences(X_train['answer'])","execution_count":null,"outputs":[]},{"metadata":{"id":"mGHWwpxbfqj_","trusted":true},"cell_type":"code","source":"#for valid data\n\nquest_title_valid = tokenizer.texts_to_sequences(X_valid['question_title'])\nquest_body_valid = tokenizer.texts_to_sequences(X_valid['question_body'])\nanswer_valid = tokenizer.texts_to_sequences(X_valid['answer'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#for test data taken from train dataset\n\nquest_title_ts = tokenizer.texts_to_sequences(X_test['question_title'])\nquest_body_ts = tokenizer.texts_to_sequences(X_test['question_body'])\nanswer_ts = tokenizer.texts_to_sequences(X_test['answer'])","execution_count":null,"outputs":[]},{"metadata":{"id":"8XPB6H23heAd","trusted":true},"cell_type":"code","source":"#for test data\n\nquest_title_test = tokenizer.texts_to_sequences(test_df['question_title'])\nquest_body_test = tokenizer.texts_to_sequences(test_df['question_body'])\nanswer_test = tokenizer.texts_to_sequences(test_df['answer'])","execution_count":null,"outputs":[]},{"metadata":{"id":"DfmOq0ZbLq22","trusted":true},"cell_type":"code","source":"title_lens=[len(i) for i in quest_title]\nquest_lens=[len(i) for i in quest_body]\nans_lens=[len(i) for i in answer]","execution_count":null,"outputs":[]},{"metadata":{"id":"Zr6isTJkL5GO"},"cell_type":"markdown","source":"Now we will see the percentilte of lenghts to decide our max_tensor_len","execution_count":null},{"metadata":{"id":"pWxpuHG2L2-Q","outputId":"9bfbc270-b2db-4144-f61a-5ad9c52f6a75","trusted":true},"cell_type":"code","source":"#len for question body\npercentile_train_len = sorted(np.percentile((quest_lens),np.array(range(0,110,10))))\nk=0\nfor i in range(len(percentile_train_len)):\n  print(str(k)+' th percentile is '+str(percentile_train_len[i]))\n  k+=10","execution_count":null,"outputs":[]},{"metadata":{"id":"yAWYgopyNhBt","outputId":"25e9156e-569f-485b-a35f-3043d822e2ec","trusted":true},"cell_type":"code","source":"percentile_train_len = sorted(np.percentile((quest_lens),np.array(range(90,101))))\nk=90\nfor i in range(len(percentile_train_len)):\n  print(str(k)+' th percentile is '+str(percentile_train_len[i]))\n  k+=1","execution_count":null,"outputs":[]},{"metadata":{"id":"RL5t--fIODm-","outputId":"b844ddeb-aac1-4024-b458-f4288deb5a92","trusted":true},"cell_type":"code","source":"#percentiles for answers\npercentile_train_len = sorted(np.percentile((ans_lens),np.array(range(0,110,10))))\nk=0\nfor i in range(len(percentile_train_len)):\n  print(str(k)+' th percentile is '+str(percentile_train_len[i]))\n  k+=10","execution_count":null,"outputs":[]},{"metadata":{"id":"LngGn_AmOJJs","outputId":"276f937d-48fb-45d0-d991-9a2699037758","trusted":true},"cell_type":"code","source":"percentile_train_len = sorted(np.percentile((ans_lens),np.array(range(91,101))))\nk=0\nfor i in range(len(percentile_train_len)):\n  print(str(k)+' th percentile is '+str(percentile_train_len[i]))\n  k+=10","execution_count":null,"outputs":[]},{"metadata":{"id":"NOXPKTXSPWNq"},"cell_type":"markdown","source":"As we see that more than 99% values have lenght of question or answer not more than 750. We take it as our max len.","execution_count":null},{"metadata":{"id":"B6w3ichrHzGo","outputId":"760646f8-87be-4efb-c21c-263be995d25e","trusted":true},"cell_type":"code","source":"#for train data\n\npadded_question_title_train=np.array(tf.keras.preprocessing.sequence.pad_sequences(quest_title, maxlen=800, padding='post'))\npadded_question_body_train=np.array(tf.keras.preprocessing.sequence.pad_sequences(quest_body, maxlen=800, padding='post'))\npadded_question_answer_train=np.array(tf.keras.preprocessing.sequence.pad_sequences(answer, maxlen=800, padding='post'))\n\nprint('shape of question title: '+str(padded_question_title_train.shape))\nprint('shape of question body: '+str(padded_question_body_train.shape))\nprint('shape of answer: '+str(padded_question_answer_train.shape))","execution_count":null,"outputs":[]},{"metadata":{"id":"alFsHC5RfKWs","outputId":"bbb85bb1-a226-48d1-a7c0-259c2a6d104d","trusted":true},"cell_type":"code","source":"#for valid data\n\npadded_question_title_valid=np.array(tf.keras.preprocessing.sequence.pad_sequences(quest_title_valid, maxlen=800, padding='post'))\npadded_question_body_valid=np.array(tf.keras.preprocessing.sequence.pad_sequences(quest_body_valid, maxlen=800, padding='post'))\npadded_question_answer_valid=np.array(tf.keras.preprocessing.sequence.pad_sequences(answer_valid, maxlen=800, padding='post'))\n\nprint('shape of question title: '+str(padded_question_title_valid.shape))\nprint('shape of question body: '+str(padded_question_body_valid.shape))\nprint('shape of answer: '+str(padded_question_answer_valid.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#for test data taken from train_dataset\n\npadded_question_title_ts=np.array(tf.keras.preprocessing.sequence.pad_sequences(quest_title_ts, maxlen=800, padding='post'))\npadded_question_body_ts=np.array(tf.keras.preprocessing.sequence.pad_sequences(quest_body_ts, maxlen=800, padding='post'))\npadded_question_answer_ts=np.array(tf.keras.preprocessing.sequence.pad_sequences(answer_ts, maxlen=800, padding='post'))\n\nprint('shape of question title: '+str(padded_question_title_ts.shape))\nprint('shape of question body: '+str(padded_question_body_ts.shape))\nprint('shape of answer: '+str(padded_question_answer_ts.shape))","execution_count":null,"outputs":[]},{"metadata":{"id":"d6hcqXmuhwbZ","outputId":"0eb44ff6-3087-4350-854b-431edca4fa52","trusted":true},"cell_type":"code","source":"#for test data\n\npadded_question_title_test=np.array(tf.keras.preprocessing.sequence.pad_sequences(quest_title_test, maxlen=800, padding='post'))\npadded_question_body_test=np.array(tf.keras.preprocessing.sequence.pad_sequences(quest_body_test, maxlen=800, padding='post'))\npadded_question_answer_test=np.array(tf.keras.preprocessing.sequence.pad_sequences(answer_test, maxlen=800, padding='post'))\n\nprint('shape of question title: '+str(padded_question_title_test.shape))\nprint('shape of question body: '+str(padded_question_body_test.shape))\nprint('shape of answer: '+str(padded_question_answer_test.shape))","execution_count":null,"outputs":[]},{"metadata":{"id":"FOJGIxkrQG83","outputId":"13ed73f2-e502-4786-c2f5-c3256b181ef3","trusted":true},"cell_type":"code","source":"vocab_size=len(tokenizer.word_index)+1\nprint(vocab_size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Glove Vector Words**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\n\nwith open(r'/kaggle/input/glove-vectors/glove_vectors', 'rb') as f:\n    model = pickle.load(f)\n    glove_words =  set(model.keys())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix = np.zeros((vocab_size, 300))\nfor word, i in tokenizer.word_index.items():\n    embedding_vector = model.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"id":"ZNV5h050c9KG","trusted":true},"cell_type":"code","source":"import tensorflow.keras.backend as T\nfrom keras.callbacks import Callback\nfrom scipy.stats import spearmanr\n\ndef compute_spearmanr(trues, preds):\n    rhos = []\n \n    for col_trues, col_pred in zip(trues.T, preds.T):\n     \n        rhos.append(\n            spearmanr(col_trues, col_pred + np.random.normal(0, 1e-7, col_pred.shape[0])).correlation)\n    return np.mean(rhos)\n\n\nclass CustomCallback(tf.keras.callbacks.Callback):\n    \n    def __init__(self, valid_data, test_data, batch_size=16, fold=None):\n\n        self.valid_inputs = valid_data[0]\n        self.valid_outputs = valid_data[1]\n        self.test_inputs = test_data\n        \n        self.batch_size = batch_size\n        self.fold = fold\n        \n    def on_train_begin(self, logs={}):\n        self.valid_predictions = []\n        self.test_predictions = []\n        \n    def on_epoch_end(self, epoch, logs={}):\n        print(len(self.valid_predictions))\n        self.valid_predictions.append(\n            self.model.predict(self.valid_inputs, batch_size=self.batch_size))\n      \n        rho_val = compute_spearmanr(\n            self.valid_outputs, np.average(self.valid_predictions, axis=0))\n        \n        print(\"\\nvalidation rho: %.4f\" % rho_val)\n        \n        if self.fold is not None:\n            self.model.save_weights(f'bert-base-{fold}-{epoch}.h5py')\n        \n        self.test_predictions.append(\n            self.model.predict(self.test_inputs, batch_size=self.batch_size)\n        )","execution_count":null,"outputs":[]},{"metadata":{"id":"GyyKECd4JEZ0","outputId":"d534fb5a-71de-449c-d599-afb29d001124","trusted":true},"cell_type":"code","source":"from tensorflow.keras.layers import Input,LSTM,Embedding,Dense,TimeDistributed,Concatenate\n\n\n\ninput_quest=Input(shape=(800,))\nembedding_layer1=Embedding(vocab_size,output_dim=300,input_length=800)(input_quest)\nlstm_q=LSTM(100, activation='tanh', recurrent_activation='sigmoid', use_bias=True, kernel_initializer='glorot_uniform',return_sequences = True)(embedding_layer1)\nflatten_question=tf.keras.layers.Flatten()(lstm_q)\n\n\ninput_ans=Input(shape=(800,))\nembedding_layer2=Embedding(vocab_size,output_dim=300, weights = [embedding_matrix],input_length=800)(input_ans)\nlstm_a=LSTM(100, activation='tanh', recurrent_activation='sigmoid', use_bias=True, kernel_initializer='glorot_uniform',return_sequences = True)(embedding_layer2)\nflatten_answer=tf.keras.layers.Flatten()(lstm_a)\n\nconcatenate_layer = tf.keras.layers.Concatenate(axis=1)(inputs = [flatten_question, flatten_answer])\n\ndense_layer1 = Dense(512,activation='tanh',kernel_initializer=tf.keras.initializers.glorot_normal(seed=30))(concatenate_layer)\ndense_layer2 = Dense(256,activation='relu',kernel_initializer=tf.keras.initializers.glorot_normal(seed=30))(dense_layer1)\ndense_layer3 = Dense(128,activation='relu',kernel_initializer=tf.keras.initializers.glorot_normal(seed=30))(dense_layer2)\noutput =  tf.keras.layers.Dense(30,activation='sigmoid',kernel_initializer=tf.keras.initializers.glorot_normal(seed=0))(dense_layer3)\nmodel = tf.keras.Model(inputs=[input_quest,input_ans],outputs=output)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"id":"_dNzL51CdwE_","trusted":true},"cell_type":"code","source":"adam = tf.keras.optimizers.Adam(0.0003)\n#checkpoint,earlystop,\nmodel.compile(loss='mse', optimizer=adam)","execution_count":null,"outputs":[]},{"metadata":{"id":"K3_q25VTe-ZB","trusted":true},"cell_type":"code","source":"inputs_train=[padded_question_body_train,padded_question_answer_train]\ninputs_valid=[padded_question_body_valid,padded_question_answer_valid]\ninputs_test=[padded_question_body_test,padded_question_answer_test]\ntrain_output=np.array(X_train[target_values])\nvalid_output=np.array(X_valid[target_values])\n","execution_count":null,"outputs":[]},{"metadata":{"id":"fUUCgV5MgKjz","trusted":true},"cell_type":"code","source":"  custom_callback = CustomCallback(\n        valid_data=(inputs_valid,valid_output), \n        test_data=inputs_test,\n        batch_size=16,\n        fold=None)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"XPjoQCUVgvP-","outputId":"4a56b55a-1c17-4392-c188-68e3de838c77","trusted":true},"cell_type":"code","source":"model.fit(inputs_train,train_output,batch_size=16,epochs =30,validation_data=(inputs_valid,valid_output),callbacks=[custom_callback])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_output=model.predict(inputs_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(test_output[:,0]))\n\nfor i in range(len(target_values)):\n  submission[target_values[i]]=test_output[:,i]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir(path)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}