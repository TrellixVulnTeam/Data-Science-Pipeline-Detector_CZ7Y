{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nimport gc\nimport numpy as np\nnp.random.seed(420)\nimport pandas as pd\nimport sentencepiece as spm\nfrom gensim.models import KeyedVectors\nimport tensorflow\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom spacy.lang.en import English\nfrom tqdm import tqdm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAXLEN_TITLE = 200\nMAXLEN_QA = 30\nGLOVE_DIMS = 100\nnlp = English()\ntokenizer = nlp.Defaults.create_tokenizer(nlp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_glove():\n    glove = {}\n    with open(f'../input/glove-global-vectors-for-word-representation/glove.6B.{GLOVE_DIMS}d.txt','r') as f:\n        for line in f:\n            values = line.split()\n            vectors = np.asarray(values[1:],'float32')\n            glove[values[0]]=vectors\n    f.close()\n    return glove\n\ndef urls_to_label(urls):\n    return [x.split('//')[1].split('.')[0] for x in urls]\n\ndef desuffix(token, glove):\n    while token:\n        token = token[:-1]\n        if token in glove:\n            return token\n    return token\n\ndef embed(glove, tokenizer, texts, maxlen=0, padding='post'):\n    seqs = []\n    for text in tqdm(tokenizer.pipe(texts), total=len(texts)):\n        seq = []\n        for t in text:\n            t = t.text.lower()\n            if t not in glove:\n                t = desuffix(t, glove)\n                if t:\n                    seq.append(glove[t])\n                else:\n                    seq.append(np.zeros(GLOVE_DIMS))\n            else:\n                seq.append(glove[t])\n        if maxlen:\n            seqs.append(seq)\n        else:\n            try:\n                seqs.append(np.mean(seqs, axis=0))\n            except:\n                seqs.append(np.zeros(GLOVE_DIMS))\n    if maxlen:\n        return pad_sequences(seqs, maxlen=maxlen, padding=padding, dtype='float32')\n    else:\n        return seqs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"glove = get_glove()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/google-quest-challenge/train.csv\")\n# train = train.sample(20)\nval = train.sample(int((len(train) / 100) * 10))\ntrain = train.drop(val.index)\ntest = pd.read_csv(\"../input/google-quest-challenge/test.csv\")\n# test = test.sample(5)\ntarget_columns = train.columns[len(test.columns):]\ntarget = train[train.columns[-len(train.columns[len(test.columns):]):]]\ntarget_val = val[val.columns[-len(val.columns[len(test.columns):]):]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_q = train[[x for x in train.columns[-len(train.columns[len(test.columns):]):] if x.startswith('q')]]\ntarget_a = train[[x for x in train.columns[-len(train.columns[len(test.columns):]):] if x.startswith('a')]]\ntarget_q_val = val[[x for x in val.columns[-len(val.columns[len(test.columns):]):] if x.startswith('q')]]\ntarget_a_val = val[[x for x in val.columns[-len(val.columns[len(test.columns):]):] if x.startswith('a')]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SelfAttention(tensorflow.keras.layers.Layer):\n    \"\"\"\n\n    Lifted from https://github.com/uzaymacar/attention-mechanisms <3\n\n    Layer for implementing self-attention mechanism. Weight variables were preferred over Dense()\n    layers in implementation because they allow easier identification of shapes. Softmax activation\n    ensures that all weights sum up to 1.\n    @param (int) size: a.k.a attention length, number of hidden units to decode the attention before\n           the softmax activation and becoming annotation weights\n    @param (int) num_hops: number of hops of attention, or number of distinct components to be\n           extracted from each sentence.\n    @param (bool) use_penalization: set True to use penalization, otherwise set False\n    @param (int) penalty_coefficient: the weight of the extra loss\n    @param (str) model_api: specify to use TF's Sequential OR Functional API, note that attention\n           weights are not outputted with the former as it only accepts single-output layers\n    \"\"\"\n\n    def __init__(\n        self,\n        size,\n        num_hops=8,\n        use_penalization=False,\n        penalty_coefficient=0.1,\n        model_api='functional',\n        W1=None,\n        W2=None,\n        **kwargs,\n    ):\n        if model_api not in ['sequential', 'functional']:\n            raise ValueError(\"Argument for param @model_api is not recognized\")\n        self.size = size\n        self.num_hops = num_hops\n        self.use_penalization = use_penalization\n        self.penalty_coefficient = penalty_coefficient\n        self.model_api = model_api\n        super(SelfAttention, self).__init__(**kwargs)\n\n    def get_config(self):\n        base_config = super(SelfAttention, self).get_config()\n        base_config['size'] = self.size\n        base_config['num_hops'] = self.num_hops\n        base_config['use_penalization'] = self.use_penalization\n        base_config['penalty_coefficient'] = self.penalty_coefficient\n        base_config['model_api'] = self.model_api\n        return base_config\n\n    def build(self, input_shape):\n        self.W1 = self.add_weight(\n            name='W1',\n            shape=(self.size, input_shape[2]),\n            initializer='glorot_uniform',\n            trainable=True,\n        )\n        self.W2 = self.add_weight(\n            name='W2',\n            shape=(self.num_hops, self.size),\n            initializer='glorot_uniform',\n            trainable=True,\n        )\n        super(SelfAttention, self).build(input_shape)\n\n    def call(self, inputs):\n        # Expand weights to include batch size through implicit broadcasting\n        W1, W2 = self.W1[None, :, :], self.W2[None, :, :]\n        hidden_states_transposed = tensorflow.keras.layers.Permute(dims=(2, 1))(inputs)\n        attention_score = tensorflow.matmul(W1, hidden_states_transposed)\n        attention_score = tensorflow.keras.layers.Activation('tanh')(attention_score)\n        attention_weights = tensorflow.matmul(W2, attention_score)\n        attention_weights = tensorflow.keras.layers.Activation('softmax')(attention_weights)\n        embedding_matrix = tensorflow.matmul(attention_weights, inputs)\n        embedding_matrix_flattened = tensorflow.keras.layers.Flatten()(embedding_matrix)\n\n        if self.use_penalization:\n            attention_weights_transposed = tensorflow.keras.layers.Permute(dims=(2, 1))(attention_weights)\n            product = tensorflow.matmul(attention_weights, attention_weights_transposed)\n            identity = tensorflow.eye(self.num_hops, batch_shape=(inputs.shape[0],))\n            frobenius_norm = tensorflow.sqrt(tf.reduce_sum(tf.square(product - identity)))\n            self.add_loss(self.penalty_coefficient * frobenius_norm)\n\n        if self.model_api == 'functional':\n            return embedding_matrix_flattened, attention_weights\n        elif self.model_api == 'sequential':\n            return embedding_matrix_flattened\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start_time = time.time()\ntitle = embed(glove, tokenizer, train[\"question_title\"].values, maxlen=MAXLEN_TITLE)\ntitle_val = embed(glove, tokenizer, val[\"question_title\"].values, maxlen=MAXLEN_TITLE)\ntitle_test = embed(glove, tokenizer, test[\"question_title\"].values, maxlen=MAXLEN_TITLE)\nquestion = embed(glove, tokenizer, train[\"question_body\"].values, maxlen=MAXLEN_QA)\nquestion_val = embed(glove, tokenizer, val[\"question_body\"].values, maxlen=MAXLEN_QA)\nquestion_test = embed(glove, tokenizer, test[\"question_body\"].values, maxlen=MAXLEN_QA)\nanswer = embed(glove, tokenizer, train[\"answer\"].values, maxlen=MAXLEN_QA)\nanswer_val = embed(glove, tokenizer, val[\"answer\"].values, maxlen=MAXLEN_QA)\nanswer_test = embed(glove, tokenizer, test[\"answer\"].values, maxlen=MAXLEN_QA)\nelapsed = time.strftime('%H:%M:%S', time.gmtime(time.time() - start_time))\nprint('Done embedding in:', elapsed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Question model\n\ninput_title = tensorflow.keras.layers.Input(\n    shape=(MAXLEN_TITLE, GLOVE_DIMS), dtype='float32'\n)\ninput_question = tensorflow.keras.layers.Input(\n    shape=(MAXLEN_QA, GLOVE_DIMS), dtype='float32'\n)\ninput_answer = tensorflow.keras.layers.Input(\n    shape=(MAXLEN_QA, GLOVE_DIMS), dtype='float32'\n)\n\ndropped_title = tensorflow.keras.layers.Dropout(0.5)(input_title)\ndropped_question = tensorflow.keras.layers.Dropout(0.5)(input_question)\ndropped_answer = tensorflow.keras.layers.Dropout(0.5)(input_answer)\n\nattended_title, _ = SelfAttention(size=128, num_hops=MAXLEN_TITLE)(dropped_title)\nattended_question, _ = SelfAttention(size=128, num_hops=int(MAXLEN_QA))(dropped_question)\nattended_answer, _ = SelfAttention(size=128, num_hops=int(MAXLEN_QA))(dropped_answer)\n\nlstm_title, _, _ = tensorflow.keras.layers.LSTM(\n    128,\n    dropout=0.5,\n    recurrent_dropout=0.5,\n    return_sequences=True,\n    return_state=True,\n)(input_title)\nlstm_question, _, _ = tensorflow.keras.layers.LSTM(\n    256,\n    dropout=0.5,\n    recurrent_dropout=0.5,\n    return_sequences=True,\n    return_state=True,\n)(input_question)\nlstm_answer, _, _ = tensorflow.keras.layers.LSTM(\n    128,\n    dropout=0.5,\n    recurrent_dropout=0.5,\n    return_sequences=True,\n    return_state=True,\n)(input_answer)\n\nattended_lstm_title, _ = SelfAttention(size=128, num_hops=MAXLEN_TITLE)(lstm_title)\nattended_lstm_question, _ = SelfAttention(size=256, num_hops=int(MAXLEN_QA))(lstm_question)\nattended_lstm_answer, _ = SelfAttention(size=128, num_hops=int(MAXLEN_QA))(lstm_answer)\n\nconc = tensorflow.keras.layers.concatenate(\n    [attended_title, attended_question, attended_answer, attended_lstm_title, attended_lstm_question, attended_lstm_answer]\n)\noutput_layer = tensorflow.keras.layers.Dense(len(target_q.columns), activation='sigmoid')(conc)\nmodel = tensorflow.keras.models.Model(\n    inputs=[input_title, input_question, input_answer],\n    outputs=output_layer\n)\n\noptimizer = tensorflow.keras.optimizers.get('adam')\noptimizer.learning_rate = 1e-4\n\nmodel.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['mae'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start_time = time.time()\nmodel.fit(\n    [title, question, answer], [target_q],\n    validation_data=([title_val, question_val, answer_val], [target_q_val]),\n    batch_size=32,\n    epochs=50,\n    callbacks=[tensorflow.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)],\n    verbose=1,\n)\nelapsed = time.strftime('%H:%M:%S', time.gmtime(time.time() - start_time))\nprint('Done training in:', elapsed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_q = model.predict(\n    [title_test, question_test, answer_test]\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tensorflow.keras.backend.clear_session()\ndel model\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Answer model\n\ninput_title = tensorflow.keras.layers.Input(\n    shape=(MAXLEN_TITLE, GLOVE_DIMS), dtype='float32'\n)\ninput_question = tensorflow.keras.layers.Input(\n    shape=(MAXLEN_QA, GLOVE_DIMS), dtype='float32'\n)\ninput_answer = tensorflow.keras.layers.Input(\n    shape=(MAXLEN_QA, GLOVE_DIMS), dtype='float32'\n)\n\ndropped_title = tensorflow.keras.layers.Dropout(0.5)(input_title)\ndropped_question = tensorflow.keras.layers.Dropout(0.5)(input_question)\ndropped_answer = tensorflow.keras.layers.Dropout(0.5)(input_answer)\n\nattended_title, _ = SelfAttention(size=128, num_hops=MAXLEN_TITLE)(dropped_title)\nattended_question, _ = SelfAttention(size=128, num_hops=int(MAXLEN_QA))(dropped_question)\nattended_answer, _ = SelfAttention(size=128, num_hops=int(MAXLEN_QA))(dropped_answer)\n\nlstm_title, _, _ = tensorflow.keras.layers.LSTM(\n    128,\n    dropout=0.5,\n    recurrent_dropout=0.5,\n    return_sequences=True,\n    return_state=True,\n)(input_title)\nlstm_question, _, _ = tensorflow.keras.layers.LSTM(\n    128,\n    dropout=0.5,\n    recurrent_dropout=0.5,\n    return_sequences=True,\n    return_state=True,\n)(input_question)\nlstm_answer, _, _ = tensorflow.keras.layers.LSTM(\n    256,\n    dropout=0.5,\n    recurrent_dropout=0.5,\n    return_sequences=True,\n    return_state=True,\n)(input_answer)\n\nattended_lstm_title, _ = SelfAttention(size=128, num_hops=MAXLEN_TITLE)(lstm_title)\nattended_lstm_question, _ = SelfAttention(size=128, num_hops=int(MAXLEN_QA))(lstm_question)\nattended_lstm_answer, _ = SelfAttention(size=256, num_hops=int(MAXLEN_QA))(lstm_answer)\n\nconc = tensorflow.keras.layers.concatenate(\n    [attended_title, attended_question, attended_answer, attended_lstm_title, attended_lstm_question, attended_lstm_answer]\n)\noutput_layer = tensorflow.keras.layers.Dense(len(target_a.columns), activation='sigmoid')(conc)\nmodel = tensorflow.keras.models.Model(\n    inputs=[input_title, input_question, input_answer],\n    outputs=output_layer\n)\n\noptimizer = tensorflow.keras.optimizers.get('adam')\noptimizer.learning_rate = 1e-4\n\nmodel.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['mae'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start_time = time.time()\nmodel.fit(\n    [title, question, answer], [target_a],\n    validation_data=([title_val, question_val, answer_val], [target_a_val]),\n    batch_size=32,\n    epochs=50,\n    callbacks=[tensorflow.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)],\n    verbose=1,\n)\nelapsed = time.strftime('%H:%M:%S', time.gmtime(time.time() - start_time))\nprint('Done training in:', elapsed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_a = model.predict(\n    [title_test, question_test, answer_test]\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = np.concatenate([preds_q, preds_a], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# preds = model.predict(\n#     [title_test, question_test, answer_test]\n# )\nsubmission = pd.read_csv(\"/kaggle/input/google-quest-challenge/sample_submission.csv\")\nfor i, column in enumerate(target_columns):\n    submission[column] = preds[:, i]\nsubmission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}