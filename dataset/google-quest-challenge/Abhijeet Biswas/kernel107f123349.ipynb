{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packagesto load in \n!pip install ../input/sacremoses/sacremoses-master/ > /dev/null\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport sys\nimport torch\nimport re\nfrom sklearn.preprocessing import OneHotEncoder\nfrom urllib.parse import urlparse\nfrom scipy.stats import rankdata\n\nsys.path.insert(0, \"../input/transformers/transformers-master/\")\nimport transformers as ppb\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport keras.backend as K\n\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\npd.options.display.max_columns = 100\n\nimport os\nimport gc\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\n# from IPython.display import FileLink\n# FileLink('saved_vectors.npy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from IPython.display import FileLink\n# FileLink('use_vectors.npy')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/google-quest-challenge/train.csv')\ntest = pd.read_csv('/kaggle/input/google-quest-challenge/test.csv')\nsample = pd.read_csv('/kaggle/input/google-quest-challenge/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dtypes = train.dtypes\ntarget_columns= train_dtypes[train_dtypes==np.float64].index.tolist()\n\ndata = train[test.columns].append(test)\n\ntext_columns = ['question_title', 'question_body', 'answer']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, \n                                                    ppb.DistilBertTokenizer, \n                                                   \"../input/distilbertbaseuncased/\")\n\ntokenizer = tokenizer_class.from_pretrained(pretrained_weights)\nmodel = model_class.from_pretrained(pretrained_weights)\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nmodel.to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TextDataset(Dataset):\n    \n    def __init__(self, data ,col_name, max_len=512):\n    \n        self.col_name = col_name\n        self.data = data\n        self.max_len = max_len\n        \n    def __len__(self):\n        \n        return len(self.data)\n        \n    def __getitem__(self,index):\n        \n        tempData = self.data.iloc[index,:]\n        text = tempData[self.col_name]\n        sequence = tokenizer.encode(text, \n                add_special_tokens=True,\n                max_length = self.max_len)\n        \n        if len(sequence)<self.max_len:\n            sequence = sequence + [0]*(self.max_len-len(sequence))\n           \n            \n        attention_mask = torch.tensor(np.where(np.array(sequence)!=0,1,0))\n        \n        padded_sequence = torch.tensor(sequence)\n            \n        \n        return padded_sequence,attention_mask\n    \n\n    \n\n# def my_collate_func(batch):\n    \n#     sequence_tensor = torch.stack([i[0] for i in batch])\n#     mask_tensor = torch.stack([i[1] for i in batch])\n    \n#     return [sequence_tensor,mask_tensor]\n\n\n    \n    \ntext_dataset_dict = dict()\ntext_dataloader_dict = dict()\n\nBATCH_SIZE = 10\nfor col in text_columns:\n    text_dataset_dict[col] = TextDataset(data.copy(), col)\n    text_dataloader_dict[col] = DataLoader(text_dataset_dict[col],\n                                           batch_size= BATCH_SIZE)\n        \n        \n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_vectors(dataloader,dim=768, verbose=False):\n    \n    vectors = np.zeros((len(dataloader.dataset),dim))\n    \n    if verbose:\n        print(\"Running for column: %s\"%(dataloader.dataset.col_name))\n    j = 0\n       \n    for i,batch in enumerate(dataloader):\n        \n        \n        temp_input = batch[0].to(device)\n        temp_mask = batch[1].to(device)\n        if verbose:\n            print(\"Running Batch Number: %d\"%i)\n            \n        with torch.no_grad():\n            \n            train_hidden_state = model(temp_input, temp_mask)\n            train_hidden_state = train_hidden_state[0].cpu()\n        \n        vectors[j:j+len(temp_input),:] = train_hidden_state[:,0,:].numpy()\n        \n        j+=len(temp_input)\n        \n        \n    return vectors\n        \n                        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectors_dict=dict()\nfor col in text_columns:\n     vectors_dict[col] = get_vectors(text_dataloader_dict[col])\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bert_vectors = np.hstack(list(vectors_dict.values()))\n\ndel vector_dict\ngc.collect()\ntorch.cuda.empty_cache()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# bert_vectors = np.load(\"../input/distilbertembedding/saved_vectors.npy\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"module_url = \"../input/universalsentenceencoderlarge4/\"\nembed = hub.load(module_url)\nembed_dim = 512 \nembeddings ={}\nembed_verbose = False\n\nfor text in text_columns:\n    if embed_verbose:\n        print(\"Running for column: %s\"%text)\n    data_text = (data[text]\n                 .str.replace('?', '.')\n                 .str.replace('!', '.')\n                 .tolist())\n    \n    curr_data_emb = []\n    batch_size = 2\n   \n    text_embedding = np.zeros((len(data_text),embed_dim))\n    \n    ind = 0\n    \n    while True:\n        if embed_verbose:\n            print(\"Running Batch: %d\"%ind)\n        if (ind+1)*batch_size< len(data_text):\n            text_embedding[ind*batch_size:(ind+1)*batch_size,:]= (embed(data_text[ind*batch_size: (ind + 1)*batch_size])[\"outputs\"].numpy())\n        else:\n            \n            text_embedding[ind*batch_size:,:]= (embed(data_text[ind*batch_size:])[\"outputs\"].numpy())\n            break\n        ind += 1\n        \n        \n    embeddings[text+\"_USE\"] = text_embedding\n      \ndel embed\nK.clear_session()\ngc.collect()\ntorch.cuda.empty_cache()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"use_vectors = np.hstack(list(embeddings.values()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# use_vectors = np.load(\"../input/use-vectors/use_vectors.npy\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"l2_dist = lambda x, y: np.power(x - y, 2).sum(axis=1)\n\ncos_dist = lambda x, y: (x*y).sum(axis=1)\n\n\ndef create_dist_features(vectors,dim=None,stack_array=True):\n    \n    features= np.array(\n        [l2_dist(vectors[:,:dim],vectors[:,dim*2:]),\n         l2_dist(vectors[:,dim:dim*2],vectors[:,dim*2:]),\n         l2_dist(vectors[:,dim:dim*2],vectors[:,:dim]),\n         \n         cos_dist(vectors[:,:dim],vectors[:,dim*2:]),\n         cos_dist(vectors[:,dim:dim*2],vectors[:,dim*2:]),\n         cos_dist(vectors[:,dim:dim*2],vectors[:,:dim])\n         ]).T\n    \n    if stack_array:\n        \n        return np.hstack([vectors,features])\n    \n    return features\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bert_vectors = create_dist_features(bert_vectors, dim = 768)\nuse_vectors = create_dist_features(use_vectors, dim = 512)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"find = re.compile(r\"^[^.]*\")\ndata.loc[:,'netloc'] = data['url'].apply(lambda x: \n                                         re.findall(find, urlparse(x).netloc)[0])\n\nohe  = OneHotEncoder(sparse = False)\n\ntopic_features = ohe.fit_transform(data[['netloc','category']],)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf_vec_dict = {}\nfor col in text_columns:\n    tfidf = TfidfVectorizer(strip_accents = 'ascii', \n                        stop_words = 'english',\n                        ngram_range = (1,3),\n                        max_features = 300,\n                       )\n    vec_train = tfidf.fit_transform(train['question_title']).toarray()\n    vec_test = tfidf.transform(test['question_title']).toarray()\n    tfidf_vec_dict[col] = np.vstack([vec_train, vec_test])\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf_vectors = np.hstack(list(tfidf_vec_dict.values()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_vectors = np.hstack([bert_vectors, use_vectors, topic_features, tfidf_vectors])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# x_train, x_test, y_train, y_test = train_test_split(all_vectors[:len(train),:],\n#                                                     train[target_columns].values,\n#                                                     test_size = 0.20, random_state = 42)\n\n# x_train = torch.tensor(x_train).float()\n# x_test = torch.tensor(x_test).float()\n# y_train = torch.tensor(y_train).float()\n# y_test = torch.tensor(y_test).float()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class LogCoshLoss(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, y_t, y_prime_t):\n        ey_t = y_t - y_prime_t\n        return torch.mean(torch.log(torch.cosh(ey_t + 1e-12)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ncriterion = nn.BCELoss()\nlogCosh = LogCoshLoss()\n\nclass Classifier(nn.Module):\n    \n    def __init__(self,input_dim, output_dim, drop_out=0.2):\n        \n        super().__init__()\n        \n        self.drop_out = drop_out\n        #self.layer = nn.Linear(input_dim, output_dim)\n        self.layer1 = nn.Linear(input_dim, 512)\n        self.layer2 = nn.Linear(512, output_dim)\n        \n    def forward(self,x):\n        \n        x = F.gelu(self.layer1(x))\n        x = nn.Dropout(p=self.drop_out)(x)\n        x = self.layer2(x)\n        x = torch.sigmoid(x)\n        \n        return x\n        \n   \ndef SpearmanCorr(output, target):\n    \n    x = output\n    y = target\n    \n    vx = x - torch.mean(x,axis=0,keepdim=True)\n    vy = y - torch.mean(y, axis =0,keepdim=True)\n    \n    \n    cost = 1.0*torch.sum(vx * vy,axis=0,keepdim=True) / (torch.sqrt(torch.sum(vx ** 2,axis=0,keepdim=True)) * torch.sqrt(torch.sum(vy ** 2,axis=0,keepdim=True)))\n    \n    return cost\n\ndef train_model(x_train,y_train, x_test, y_test,num_epoch = 20, thresh = 1e-3):\n    \n    prev_val_cor = 0\n    model_state_dict = None\n    \n    for i in range(num_epoch):\n        \n        classifier.train()\n        \n        output = classifier(x_train)\n        \n        with torch.no_grad():\n            spearman_loss = -1*SpearmanCorr(output,y_train).mean()\n        \n        loss = criterion(output, y_train) + logCosh(y_train,output)\n        \n        print(\"Spearman Loss\",spearman_loss)\n#         print(\"BCE Loss\",bce_loss)\n        \n#         loss = (spearman_loss + bce_loss)\n        \n        print(\"Running Epoch: %d\"%i)\n        \n        print(\"Training Loss: %f\"%(loss.item()))\n        \n        optimizer.zero_grad()\n        \n        loss.backward()\n        \n        optimizer.step()\n        \n        with torch.no_grad():\n            classifier.eval()\n            val_cor = SpearmanCorr(classifier(x_test),\n                                    y_test).mean()\n            \n            if (val_cor-prev_val_cor> thresh):\n                prev_val_cor = val_cor\n                model_state_dict = classifier.state_dict()\n            \n            print(\"Val Corr: %f\"%val_cor.item()) \n            \n    return model_state_dict\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_fold = 5\n\nkf = KFold(n_fold, shuffle=True, random_state=42)\nX_train = all_vectors[:len(train),:]\nX_test = torch.tensor(all_vectors[len(train):,:]).float()\nY_train = train[target_columns].values\n\nfinal_predictions = np.zeros((n_fold,len(test),len(target_columns)))\n\ni_fold = 0\nfor train_index, test_index in kf.split(X_train):\n    \n    print(\"Running Fold: %d\"%(i_fold))\n    x_train, x_test = X_train[train_index], X_train[test_index]\n    y_train, y_test = Y_train[train_index], Y_train[test_index]\n    \n    x_train = torch.tensor(x_train).float()\n    x_test = torch.tensor(x_test).float()\n    y_train = torch.tensor(y_train).float()\n    y_test = torch.tensor(y_test).float()\n    \n    classifier = Classifier(all_vectors.shape[1],len(target_columns),\n                        drop_out =0.3)\n\n    optimizer = optim.Adam(classifier.parameters(), lr = 1e-3)\n\n    model_state_dict=train_model(x_train,\n                        y_train,\n                        x_test,\n                        y_test,\n                        num_epoch = 150,\n                        thresh = 1e-3)\n\n    final_classifier = Classifier(all_vectors.shape[1],len(target_columns),\n                            drop_out =0.3)\n\n    final_classifier.load_state_dict(model_state_dict)\n    final_classifier.eval()\n    \n    with torch.no_grad():\n        \n        Y_test = final_classifier(X_test)\n        \n    final_predictions[i_fold,:,:] = Y_test.numpy()\n    \n    i_fold+=1\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# classifier = Classifier(all_vectors.shape[1],len(target_columns),\n#                         drop_out =0.5)\n\n# optimizer = optim.Adam(classifier.parameters(), lr = 1e-3)\n\n# model_state_dict=train_model(x_train,\n#                     y_train,\n#                     x_test,\n#                     y_test,\n#                     num_epoch = 100)\n\n# final_classifier = Classifier(all_vectors.shape[1],len(target_columns),\n#                         drop_out =0.5)\n\n# final_classifier.load_state_dict(model_state_dict)\n# final_classifier.eval()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# preds = np.array([[rankdata(row) for row in fold] for fold in final_predictions]).mean(axis=0)\n# max_val = preds.max() + 1\n# final_preds = preds/max_val +1e-12","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample.loc[:,target_columns] = np.mean(final_predictions, axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample.to_csv(\"submission.csv\",index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}