{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install ../input/huggingface-transformers/sacremoses-master/sacremoses-master\n!pip install ../input/huggingface-transformers/transformers-master/transformers-master","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport matplotlib.pyplot as plt\nimport tensorflow.keras as keras\nimport gc\nfrom sklearn.model_selection import train_test_split, GroupKFold\n# import bert_tokenization as tokenization\nfrom scipy.stats import spearmanr\nfrom math import floor, ceil\nfrom transformers import TFPreTrainedModel, TFBertMainLayer, BertConfig, TFBertModel, BertTokenizer\nfrom tqdm import tqdm\ntqdm.pandas()\nimport pyprind \nimport warnings\nwarnings.filterwarnings('ignore')\nimport operator\nimport string\nimport json\nimport re\nimport gensim\nfrom gensim.models import KeyedVectors\nimport seaborn as sns\nimport time\nimport random\nimport pickle\nimport joblib\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\nimport operator\nfrom nltk.tokenize.treebank import TreebankWordTokenizer\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.corpus import wordnet\nimport nltk\nfrom spacy.lang.en import English\nimport re\nfrom keras.preprocessing import text\n# from tqdm import tqdm, tqdm_notebook\n# tqdm_notebook().pandas()\nimport bert_tokenization as tokenization\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"PATH = '../input/google-quest-challenge/'\n\nBERT_PATH = '../input/bert-base-from-tfhub/bert_en_uncased_L-12_H-768_A-12'\ntokenizer_google_qa = tokenization.FullTokenizer(BERT_PATH+'/assets/vocab.txt', True)\ntokenizer2 = BertTokenizer.from_pretrained(BERT_PATH+'/assets/vocab.txt', do_lower_case=True,)\nMAX_SEQUENCE_LENGTH = 512\n\ndf_train = pd.read_csv(PATH+'train.csv',header=0,encoding='utf-8')\ndf_test = pd.read_csv(PATH+'test.csv',header=0,encoding='utf-8')\ndf_sub = pd.read_csv(PATH+'sample_submission.csv')\ndf = pd.concat([df_train,df_test],axis=0,ignore_index=True)\nprint(f'''Train Shape: {df_train.shape}\nTest Shape: {df_test.shape}\nDf Shape:{df.shape}''')\n\noutput_categories = ['question_asker_intent_understanding', 'question_body_critical', 'question_conversational', 'question_expect_short_answer', 'question_fact_seeking', 'question_has_commonly_accepted_answer', 'question_interestingness_others', 'question_interestingness_self', 'question_multi_intent', 'question_not_really_a_question', 'question_opinion_seeking', 'question_type_choice', 'question_type_compare', 'question_type_consequence', 'question_type_definition', 'question_type_entity', 'question_type_instructions', 'question_type_procedure', 'question_type_reason_explanation', 'question_type_spelling', 'question_well_written', 'answer_helpful', 'answer_level_of_information', 'answer_plausible', 'answer_relevance', 'answer_satisfaction', 'answer_type_instructions', 'answer_type_procedure', 'answer_type_reason_explanation', 'answer_well_written']\ninput_categories = ['question_title', 'question_body', 'answer']\nprint('\\noutput categories:\\n\\t', output_categories)\nprint('\\ninput categories:\\n\\t', input_categories)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def get_wordnet_pos(treebank_tag):\n#     \"\"\"\n#     return WORDNET POS compliance to WORDENT lemmatization (a,n,r,v) \n#     \"\"\"\n#     if treebank_tag.startswith('J'):\n#         return wordnet.ADJ\n#     elif treebank_tag.startswith('V'):\n#         return wordnet.VERB\n#     elif treebank_tag.startswith('N'):\n#         return wordnet.NOUN\n#     elif treebank_tag.startswith('R'):\n#         return wordnet.ADV\n#     else:\n#         # As default pos in lemmatization is Noun\n#         return wordnet.NOUN","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tree_tokenizer = TreebankWordTokenizer()\n# wnl = WordNetLemmatizer()\ndef get_tree_tokens(x):\n    x = tree_tokenizer.tokenize(x)\n#     pos_tokens = [nltk.pos_tag(x)]\n#     l=[]\n#     for pos in pos_tokens: \n#         for word, tag in pos:\n#             l.append(wnl.lemmatize(word,get_wordnet_pos(tag)))\n    \n    x = ' '.join(x)\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in input_categories:\n    df_train[f'treated_{col}'] = df_train[col].progress_apply(lambda x: get_tree_tokens(x))\n    df_test[f'treated_{col}'] = df_test[col].progress_apply(lambda x: get_tree_tokens(x))\n    df[f'treated_{col}'] = df[col].progress_apply(lambda x: get_tree_tokens(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = text.Tokenizer(lower=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_question = df_train['question_body']\nX_train_title    = df_train['question_title']\nX_train_answer   = df_train['answer']\n\nX_test_question  = df_test['question_body']\nX_test_title     = df_test['question_title']\nX_test_answer    = df_test['answer']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer.fit_on_texts(list(X_train_title)+list(X_train_question)+list(X_train_answer)+list(X_test_title)+list(X_test_question)+list(X_test_answer))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp = English()\nsentencizer = nlp.create_pipe('sentencizer')\nnlp.add_pipe(sentencizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def split_document(texts):\n    all_sents = []\n    max_num_sentences = 0.0\n    for text in texts:\n        doc = nlp(text)\n#         print('Sents',doc.sents)\n        sents=[]\n        for i,sent in enumerate(doc.sents):\n            sents.append(sent.text)\n        all_sents.append(sents)\n    \n    return all_sents\n        \nX_train_question = split_document(X_train_question)\n# X_train_title = split_document(X_train_title)\nX_train_answer = split_document(X_train_answer)\n\nX_test_question = split_document(X_test_question)\nX_test_answer = split_document(X_test_answer)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_question_metadata_features(text):\n    doc=nlp(text)\n    indirect = 0\n    choice_words=0\n    reason_explanation_words = 0\n    question_count = 0\n    \n    for sent in doc.sents:\n        if '?' in sent.text and '?' == sent.text[-1]:\n            question_count += 1\n            for token in sent:\n                if token.text.lower()=='why':\n                    reason_explanation_words+=1\n                elif token.text.lower()=='or':\n                    choice_words+=1\n    if question_count==0:\n        indirect+=1\n    \n    return np.array([indirect, question_count, reason_explanation_words, choice_words])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ans_user_and_category=df_train[df_train[['answer_user_name', 'category']].duplicated()][['answer_user_name', 'category']].values\nans_user_and_category.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def question_answer_author_same(df):\n    q_username = df['question_user_name']\n    a_username = df['answer_user_name']\n    author_same=[]\n    \n    for i in range(len(df)):\n        if q_username[i] == a_username[i]:\n            author_same.append(int(1))\n        else:\n            author_same.append(int(0))\n    return author_same\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_external_features(df):\n    df['question_body'] = df['question_body'].progress_apply(lambda x: str(x))\n    df['question_body_num_words'] = df['question_body'].str.count('\\S+')\n    \n    df['answer'] = df['answer'].progress_apply(lambda x: str(x))\n    df['answer_num_words'] = df['answer'].str.count('\\S+')\n    \n    df['question_vs_answer_length'] = df['question_body_num_words']/df['answer_num_words']\n    \n    df['q_a_author_same'] = question_answer_author_same(df)\n    \n    answer_user_cat = []\n    for i in tqdm(df[['answer_user_name', 'category']].values):\n        if i in ans_user_and_category:\n            answer_user_cat.append(int(1))\n        else:\n            answer_user_cat.append(int(0))\n    df['answer_user_cat'] = answer_user_cat\n    \n    handmade_features=[]\n    for text in df['question_body'].values:\n        handmade_features.append(add_question_metadata_features(text))\n\n    \n    return df, np.array(handmade_features)\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train, handmade_features = add_external_features(df_train)\ndf_test, handmade_features_test = add_external_features(df_test)\n\ndf_train = pd.concat([df_train,pd.DataFrame(handmade_features, columns=['indirect', 'question_count', 'reason_explanation_words', 'choice_words'])],axis=1)\ndf_test = pd.concat([df_test,pd.DataFrame(handmade_features_test, columns=['indirect', 'question_count', 'reason_explanation_words', 'choice_words'])],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.shape, df_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_words_scaler = MinMaxScaler()\ndf_train[['question_body_num_words', 'answer_num_words']] = num_words_scaler.fit_transform(df_train[['question_body_num_words', 'answer_num_words']].values)\ndf_test[['question_body_num_words', 'answer_num_words']] = num_words_scaler.transform(df_test[['question_body_num_words', 'answer_num_words']].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# unique_categories = list(set(df_train['category'].unique().tolist() + df_test['category'].unique().tolist()))\n# category_dict = {i + 1: e for i, e in enumerate(unique_categories)}\n# category_dict_reverse = {v: k for k, v in category_dict.items()}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# le_cat = LabelEncoder()\n# df_train['category']=le_cat.fit_transform(df_train['category'])\n# df_test['category']=le_cat.transform(df_test['category'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# le_host = LabelEncoder()\n# le_host.fit(list(set(df_train['host'].values.tolist()+df_test['host'].values.tolist())))\n# df_train['host']=le_host.transform(df_train['host'])\n# df_test['host']=le_host.transform(df_test['host'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.spatial.distance import cdist\ncols = output_categories+['question_body_num_words',\n       'answer_num_words', 'question_vs_answer_length', 'q_a_author_same',\n       'answer_user_cat', 'indirect', 'question_count',\n       'reason_explanation_words', 'choice_words']\nfig, ax = plt.subplots(figsize=(25,25))\nsns.heatmap(df_train[cols].corr() , ax=ax, annot=True, cmap='vlag')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.shape, df_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.concat([df,pd.get_dummies(df['host'], drop_first=False, prefix='host')],axis=1)\ndf=pd.concat([df,pd.get_dummies(df['category'], drop_first=False, prefix='cat')],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(['qa_id']+[i for i in df.columns if i.startswith('host_') or i.startswith('cat_')])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train=pd.merge(df_train, df[['qa_id']+[i for i in df.columns if i.startswith('host_') or i.startswith('cat_')]], how='inner', on='qa_id')\ndf_test = pd.merge(df_test, df[['qa_id']+[i for i in df.columns if i.startswith('host_') or i.startswith('cat_')]], how='inner', on='qa_id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_train=pd.concat([df_train,pd.get_dummies(df_train['host'], drop_first=False, prefix='host')],axis=1)\n# df_train=pd.concat([df_train,pd.get_dummies(df_train['category'], drop_first=False, prefix='cat')],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_test=pd.concat([df_test,pd.get_dummies(df_test['host'], drop_first=False, prefix='host')],axis=1)\n# df_test=pd.concat([df_test,pd.get_dummies(df_test['category'], drop_first=False, prefix='cat')],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.shape, df_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.drop(['host', 'category'], inplace=True, axis=1)\ndf_test.drop(['host', 'category'], inplace=True, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def _get_masks(tokens, max_seq_length):\n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    return [1]*len(tokens)+[0]*(max_seq_length-len(tokens))\n\ndef _get_segments(tokens, max_seq_length):\n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    \n    segments=[]\n    first_sep=True\n    current_segment_id = 0\n    for token in tokens:\n        segments.append(current_segment_id)\n        if token == '[SEP]':\n#             if first_sep:\n#                 first_sep=False\n#             else:\n            current_segment_id=1\n    return segments+[0]*(max_seq_length-len(tokens))\n\ndef _trim_input_questions(title, question, max_sequence_length, t_max_len=30, q_max_len=479):\n\n    t = tokenizer2.tokenize(title)\n    q = tokenizer2.tokenize(question)\n#     a = tokenizer2.tokenize(answer)\n    \n    t_len = len(t)\n    q_len = len(q)\n    #a_len = len(a)\n\n    if (t_len+q_len+3) > max_sequence_length:\n        \n        if t_max_len > t_len:\n            t_new_len = t_len\n            #a_max_len = a_max_len + floor((t_max_len - t_len)/2)\n            q_max_len = q_max_len + ceil((t_max_len - t_len)/2)\n        else:\n            t_new_len = t_max_len\n      \n        #if a_max_len > a_len:\n        #    a_new_len = a_len \n        #    q_new_len = q_max_len + (a_max_len - a_len)\n        if q_max_len > q_len:\n        #    a_new_len = a_max_len + (q_max_len - q_len)\n            q_new_len = q_len\n        else:\n        #    a_new_len = a_max_len\n            q_new_len = q_max_len\n            \n            \n        if t_new_len+q_new_len+3 > max_sequence_length:\n            raise ValueError(\"New sequence length should be %d, but is %d\" \n                             % (max_sequence_length, (t_new_len+q_new_len+3)))\n        \n        t = t[:t_new_len]\n        q = q[:q_new_len]\n        #a = a[:a_new_len]\n    \n    return t, q\n    \ndef _get_ids(tokens, tokenizer, max_seq_length):\n    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n    input_ids = token_ids + [0]*(max_seq_length - len(token_ids))\n    return input_ids\n    \n    \ndef _convert_to_bert_inputs_questions(title, question,tokenizer, max_seq_length):\n    stoken = ['[CLS]']+title+['[SEP]']+question+['[SEP]']#+answer+['[SEP]']\n    input_ids = _get_ids(tokens=stoken, tokenizer=tokenizer,max_seq_length=max_seq_length)\n    input_masks = _get_masks(tokens=stoken, max_seq_length=max_seq_length)\n    input_segments = _get_segments(tokens=stoken, max_seq_length=max_seq_length)\n    return [input_ids, input_masks, input_segments]\n    \n    \ndef compute_input_array_questions(df, columns, tokenizer, max_sequence_length):\n    input_ids, input_masks, input_segments = [], [], []\n    for _, col in tqdm(df[columns].iterrows()):\n        t, q = col['treated_question_title'], col['treated_question_body']\n        \n        t,q = _trim_input_questions(t,q, max_sequence_length)\n        \n        ids, masks, segments = _convert_to_bert_inputs_questions(t,q, tokenizer, max_sequence_length)\n        input_ids.append(ids)\n        input_masks.append(masks)\n        input_segments.append(segments)\n        \n    return [np.array(input_ids, dtype=np.int32), np.array(input_masks, dtype=np.int32), \n            np.array(input_segments, dtype=np.int32)]\n\n\ndef _trim_input_answers(answer, max_sequence_length, a_max_len=510):\n\n    #t = tokenizer2.tokenize(title)\n    #q = tokenizer2.tokenize(question)\n    a = tokenizer2.tokenize(answer)\n    \n    #t_len = len(t)\n    #q_len = len(q)\n    a_len = len(a)\n\n    if (a_len+2) > max_sequence_length:\n        \n        #if t_max_len > t_len:\n        #    t_new_len = t_len\n        #    a_max_len = a_max_len + floor((t_max_len - t_len)/2)\n        #    q_max_len = q_max_len + ceil((t_max_len - t_len)/2)\n        #else:\n        #    t_new_len = t_max_len\n      \n        if a_max_len > a_len:\n            a_new_len = a_len \n        #    q_new_len = q_max_len + (a_max_len - a_len)\n        #elif q_max_len > q_len:\n        #    a_new_len = a_max_len + (q_max_len - q_len)\n        #    q_new_len = q_len\n        else:\n            a_new_len = a_max_len\n        #    q_new_len = q_max_len\n            \n            \n        if a_new_len+2 > max_sequence_length:\n            raise ValueError(\"New sequence length should be %d, but is %d\" \n                             % (max_sequence_length, (a_new_len+2)))\n        \n        #t = t[:t_new_len]\n        #q = q[:q_new_len]\n        a = a[:a_new_len]\n    \n    return a\n    \n    \n    \ndef _convert_to_bert_inputs_answers(answer, tokenizer, max_seq_length):\n    stoken = ['[CLS]']+answer+['[SEP]']\n    input_ids = _get_ids(tokens=stoken, tokenizer=tokenizer,max_seq_length=max_seq_length)\n    input_masks = _get_masks(tokens=stoken, max_seq_length=max_seq_length)\n    input_segments = _get_segments(tokens=stoken, max_seq_length=max_seq_length)\n    return [input_ids, input_masks, input_segments]\n    \n    \ndef compute_input_array_answers(df, columns, tokenizer, max_sequence_length):\n    input_ids, input_masks, input_segments = [], [], []\n    for _, col in tqdm(df[columns].iterrows()):\n        a = col['treated_answer']\n        a = _trim_input_answers(a, max_sequence_length)\n        ids, masks, segments = _convert_to_bert_inputs_answers(a, tokenizer, max_sequence_length)\n        input_ids.append(ids)\n        input_masks.append(masks)\n        input_segments.append(segments)\n        \n    return [np.array(input_ids, dtype=np.int32), np.array(input_masks, dtype=np.int32), \n            np.array(input_segments, dtype=np.int32)]   \n\ndef compute_output_arrays(df, columns):\n    return np.asarray(df[columns])\n\ndef compute_spearmanr(trues, preds):\n    rhos=[]\n    for t, p in zip(trues.T, preds.T):\n        rhos.append(\n            spearmanr(t,p).correlation\n        )\n    return np.nanmean(rhos)\n\ntest_predictions=[]\n\nclass CustomCallback(keras.callbacks.Callback):\n    def __init__(self, valid_data, test_data, test_predictions=test_predictions, batch_size=16, fold=None):\n        self.valid_inputs = valid_data[0]\n        self.valid_outputs = valid_data[1]\n        self.test_inputs = test_data\n        self.batch_size = batch_size\n        self.test_predictions = test_predictions\n        self.fold = fold\n        \n    def on_train_begin(self, logs={}):\n        self.valid_predictions=[]\n        #self.test_predictions=[]\n        \n    def on_epoch_end(self, epoch, logs={}):\n        self.valid_predictions.append(self.model.predict(self.valid_inputs, batch_size=self.batch_size))\n        \n        rho_val = compute_spearmanr(self.valid_outputs, np.average(self.valid_predictions, axis=0))\n        print(f\"\\nvalidation rho: {round(rho_val,4)}\")\n        \n#         if (epoch+1)%4==0:\n#             self.model.save_weights(f'/kaggle/working/bert-base-{fold}-{epoch}.hdf5')\n            \n        self.test_predictions.append(self.model.predict(self.test_inputs, batch_size=self.batch_size))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bert_config=BertConfig(unk_token=\"[QBODY]\", pad_token=\"[ANS]\").from_pretrained('../input/bert-tensorflow/bert-base-uncased-config.json',output_hidden_states=True)\ndef bertModel():\n    input_ids_q = keras.layers.Input((MAX_SEQUENCE_LENGTH), dtype = tf.int32, name = 'input_word_ids_q')\n    input_mask_q = keras.layers.Input((MAX_SEQUENCE_LENGTH), dtype = tf.int32, name = 'input_masks_q')\n    input_segments_q = keras.layers.Input((MAX_SEQUENCE_LENGTH), dtype = tf.int32, name = 'input_segments_q')\n#     input_categories = keras.layers.Input((len([i for i in df.columns if i.startswith('host_') or i.startswith('cat_')])),dtype=tf.float32, name='input_categorical')\n    input_new_features = keras.layers.Input((9,), dtype=tf.float32, name='input_new_features')\n    \n    input_ids_a = keras.layers.Input((MAX_SEQUENCE_LENGTH), dtype = tf.int32, name = 'input_word_ids_a')\n    input_mask_a = keras.layers.Input((MAX_SEQUENCE_LENGTH), dtype = tf.int32, name = 'input_masks_a')\n    input_segments_a = keras.layers.Input((MAX_SEQUENCE_LENGTH), dtype = tf.int32, name = 'input_segments_a')\n    \n    \n    bert_model_q = TFBertModel.from_pretrained(pretrained_model_name_or_path='../input/bert-tensorflow/bert-base-uncased-tf_model.h5',config = bert_config)\n    \n    bert_model_a = TFBertModel.from_pretrained(pretrained_model_name_or_path='../input/bert-tensorflow/bert-base-uncased-tf_model.h5',config = bert_config)\n#     bert_model.resize_token_embeddings(len(tokenizer2))\n    _, _, hidden_states_q = bert_model_q([input_ids_q,input_mask_q, input_segments_a])\n    _, _, hidden_states_a = bert_model_a([input_ids_a,input_mask_a, input_segments_a])\n    \n    h12_q = tf.reshape(hidden_states_q[-1][:,0],(-1,1,768))\n    h11_q = tf.reshape(hidden_states_q[-2][:,0],(-1,1,768))\n    h10_q = tf.reshape(hidden_states_q[-3][:,0],(-1,1,768))\n    h09_q = tf.reshape(hidden_states_q[-4][:,0],(-1,1,768))\n    concat_hidden_q = keras.layers.Concatenate(axis=2)([h12_q, h11_q, h10_q, h09_q])\n    \n    h12_a = tf.reshape(hidden_states_a[-1][:,0],(-1,1,768))\n    h11_a = tf.reshape(hidden_states_a[-2][:,0],(-1,1,768))\n    h10_a = tf.reshape(hidden_states_a[-3][:,0],(-1,1,768))\n    h09_a = tf.reshape(hidden_states_a[-4][:,0],(-1,1,768))\n    concat_hidden_a = keras.layers.Concatenate(axis=2)([h12_a, h11_a, h10_a, h09_a])\n\n    x = keras.layers.GlobalAveragePooling1D()(concat_hidden_q)\n    x_a = keras.layers.GlobalAveragePooling1D()(concat_hidden_a)\n    \n    x = keras.layers.Concatenate()([x, input_new_features])#, input_categories])\n    x_a = keras.layers.Concatenate()([x_a,x, input_new_features])#, input_categories])\n#     dense1 = keras.layers.Dense(768)(x)\n#     dense1 = keras.layers.LeakyReLU()(dense1)\n#     x = keras.layers.Add()([dense1,x])    \n#     x = keras.layers.Dropout(0.2)(x)\n#     x_a = keras.layers.Dropout(0.2)(x_a)\n    \n    out = keras.layers.Dense(len([i for i in output_categories if i.startswith('question')]), activation='sigmoid', name='final_dense_output_q')(x)\n    out_a = keras.layers.Dense(len([i for i in output_categories if i.startswith('answer')]), activation='sigmoid', name='final_dense_output_a')(x_a)\n    out = keras.layers.Concatenate()([out,out_a])\n    \n    model = keras.models.Model(inputs=[input_ids_q, input_mask_q, input_segments_q,#input_categories, \n                                       input_new_features, input_ids_a, input_mask_a, input_segments_a]\n                               , outputs=out)\n    model.compile(loss='binary_crossentropy', optimizer = keras.optimizers.Adam(lr=3e-5))\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model = bertModel()\n# model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gkf = GroupKFold(n_splits=10).split(X=df_train.question_body, groups=df_train.question_body)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# outputs_q = compute_output_arrays(df_train, [i for i in output_categories if i.startswith('question')])\n# outputs_a = compute_output_arrays(df_train, [i for i in output_categories if i.startswith('answer')])\noutputs = compute_output_arrays(df_train, output_categories)\ninputs_q = compute_input_array_questions(df_train, ['treated_question_title','treated_question_body'], tokenizer2, MAX_SEQUENCE_LENGTH)\ninputs_a = compute_input_array_answers(df_train, ['treated_answer'], tokenizer2, MAX_SEQUENCE_LENGTH)\ntest_inputs_q = compute_input_array_questions(df_test, ['treated_question_title','treated_question_body'], tokenizer2, MAX_SEQUENCE_LENGTH)\ntest_inputs_a = compute_input_array_answers(df_test, ['treated_answer'], tokenizer2, MAX_SEQUENCE_LENGTH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# for fold, (train_idx, valid_idx) in enumerate(gkf):\n    \n#     if fold<5:\n#         keras.backend.clear_session()\n#         model = bertModel()\n#         print(f'/kaggle/input/google-qa-bert-trained-tfbert-hiddenl-preprocess/bert_base_fold_{fold}.hdf5')\n#         model.load_weights(f'/kaggle/input/google-qa-bert-trained-tfbert-hiddenl-preprocess/bert_base_fold_{fold}.hdf5')\n#         preds = model.predict(test_inputs_q + #[df_test[[i for i in df.columns if i.startswith('host_') or i.startswith('cat_')]].values]+\n#                                                      [df_test[['question_body_num_words', 'answer_num_words','question_vs_answer_length', 'q_a_author_same', 'answer_user_cat','indirect', 'question_count', 'reason_explanation_words','choice_words']].values]+test_inputs_a,\n#                                  batch_size=8, verbose=1)\n#         test_predictions.append(preds)\n#         print(f'Fold:{fold} Completed')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"histories = []\n\nfor fold, (train_idx, valid_idx) in enumerate(gkf):\n    \n    if fold>8:\n        keras.backend.clear_session()\n        model = bertModel()        \n        \n        train_inputs_q = [inputs_q[i][train_idx] for i in range(3)] #+[df_train[[i for i in df_train.columns if i.startswith('cat_') or i.startswith('host_')]].iloc[train_idx,:].values]+[df_train[['question_body_num_words', 'answer_num_words','question_vs_answer_length', 'q_a_author_same', 'answer_user_cat','indirect', 'question_count', 'reason_explanation_words','choice_words']].iloc[train_idx,:].values]\n        train_inputs_a = [inputs_a[i][train_idx] for i in range(3)]\n        train_outputs = outputs[train_idx]\n    \n        valid_inputs_q = [inputs_q[i][valid_idx] for i in range(3)]\n        valid_inputs_a = [inputs_a[i][valid_idx] for i in range(3)]\n        valid_outputs = outputs[valid_idx]\n    \n        custom_callback = CustomCallback(valid_data=(valid_inputs_q + #[df_train[[i for i in df.columns if i.startswith('host_') or i.startswith('cat_')]].iloc[valid_idx,:].values]+\n                                                      [df_train[['question_body_num_words', 'answer_num_words','question_vs_answer_length', 'q_a_author_same', 'answer_user_cat','indirect', 'question_count', 'reason_explanation_words','choice_words']].iloc[valid_idx,:].values]+ valid_inputs_a,\n                                                      valid_outputs), \n                                         test_data=test_inputs_q + #[df_test[[i for i in df.columns if i.startswith('host_') or i.startswith('cat_')]].values]+\n                                                    [df_test[['question_body_num_words', 'answer_num_words','question_vs_answer_length', 'q_a_author_same', 'answer_user_cat','indirect', 'question_count', 'reason_explanation_words','choice_words']].values]+test_inputs_a,\n                                         batch_size=8, fold=fold)\n        H = model.fit(train_inputs_q+ \n                       #[df_train[[i for i in df.columns if i.startswith('host_') or i.startswith('cat_')]].iloc[train_idx,:].values]+ \n                       [df_train[['question_body_num_words', 'answer_num_words','question_vs_answer_length', 'q_a_author_same', 'answer_user_cat','indirect', 'question_count', 'reason_explanation_words','choice_words']].iloc[train_idx,:].values] + train_inputs_a,\n                      train_outputs, batch_size=4, epochs=4, callbacks=[custom_callback])\n        histories.append(H)\n        model.save_weights(f'/kaggle/working/bert_base_fold_{fold}.hdf5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sssddfgdfgfsdfsddfsdfsewefsdsdfghgfefsd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(test_predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds = [test_predictions[i] for i in range(len(test_predictions))]\ntest_preds = [np.average(test_preds[i], axis=0) for i in range(len(test_preds))]\ntest_preds = np.mean(test_predictions, axis=0)\n\n# df_sub.iloc[:, 1:] = test_preds\n\n# df_sub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def _get_masks_google_qa(tokens, max_seq_length):\n    \"\"\"Mask for padding\"\"\"\n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n\ndef _get_segments_google_qa(tokens, max_seq_length):\n    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    segments = []\n    first_sep = True\n    current_segment_id = 0\n    for token in tokens:\n        segments.append(current_segment_id)\n        if token == \"[SEP]\":\n            if first_sep:\n                first_sep = False \n            else:\n                current_segment_id = 1\n    return segments + [0] * (max_seq_length - len(tokens))\n\ndef _get_ids_google_qa(tokens, tokenizer, max_seq_length):\n    \"\"\"Token ids from Tokenizer vocab\"\"\"\n    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n    return input_ids\n\ndef _trim_input_google_qa(title, question, answer, max_sequence_length, \n                t_max_len=60, q_max_len=224, a_max_len=224):\n\n    t = tokenizer_google_qa.tokenize(title)\n    q = tokenizer_google_qa.tokenize(question)\n    a = tokenizer_google_qa.tokenize(answer)\n    \n    t_len = len(t)\n    q_len = len(q)\n    a_len = len(a)\n\n    if (t_len+q_len+a_len+4) > max_sequence_length:\n        \n        if t_max_len > t_len:\n            t_new_len = t_len\n            a_max_len = a_max_len + floor((t_max_len - t_len)/2)\n            q_max_len = q_max_len + ceil((t_max_len - t_len)/2)\n        else:\n            t_new_len = t_max_len\n      \n        if a_max_len > a_len:\n            a_new_len = a_len \n            q_new_len = q_max_len + (a_max_len - a_len)\n        elif q_max_len > q_len:\n            a_new_len = a_max_len + (q_max_len - q_len)\n            q_new_len = q_len\n        else:\n            a_new_len = a_max_len\n            q_new_len = q_max_len\n            \n            \n        if t_new_len+a_new_len+q_new_len+4 != max_sequence_length:\n            raise ValueError(\"New sequence length should be %d, but is %d\" \n                             % (max_sequence_length, (t_new_len+a_new_len+q_new_len+4)))\n        \n        t = t[:t_new_len]\n        q = q[:q_new_len]\n        a = a[:a_new_len]\n    \n    return t, q, a\n\ndef _convert_to_bert_inputs_google_qa(title, question, answer, tokenizer, max_sequence_length):\n    \"\"\"Converts tokenized input to ids, masks and segments for BERT\"\"\"\n    \n    stoken = [\"[CLS]\"] + title + [\"[SEP]\"] + question + [\"[SEP]\"] + answer + [\"[SEP]\"]\n\n    input_ids = _get_ids_google_qa(stoken, tokenizer, max_sequence_length)\n    input_masks = _get_masks_google_qa(stoken, max_sequence_length)\n    input_segments = _get_segments_google_qa(stoken, max_sequence_length)\n\n    return [input_ids, input_masks, input_segments]\n\ndef compute_input_arays_google_qa(df, columns, tokenizer, max_sequence_length):\n    input_ids, input_masks, input_segments = [], [], []\n    for _, instance in tqdm(df[columns].iterrows()):\n        t, q, a = instance['treated_question_title'], instance['treated_question_body'], instance['treated_answer']\n\n        t, q, a = _trim_input_google_qa(t, q, a, max_sequence_length)\n\n        ids, masks, segments = _convert_to_bert_inputs_google_qa(t, q, a, tokenizer, max_sequence_length)\n        input_ids.append(ids)\n        input_masks.append(masks)\n        input_segments.append(segments)\n        \n    return [np.asarray(input_ids, dtype=np.int32), \n            np.asarray(input_masks, dtype=np.int32), \n            np.asarray(input_segments, dtype=np.int32)\n           ]\n\n\ndef compute_output_arrays_google_qa(df, columns):\n    return np.asarray(df[columns])\ndef bert_model_google_qa(learning_rate=4e-5,loss_function='binary_crossentropy'):\n    \n    input_word_ids = tf.keras.layers.Input(\n        (MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_word_ids')\n    input_masks = tf.keras.layers.Input(\n        (MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_masks')\n    input_segments = tf.keras.layers.Input(\n        (MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_segments')\n        \n    bert_layer = hub.KerasLayer(BERT_PATH, trainable=True)\n    \n    _, sequence_output = bert_layer([input_word_ids, input_masks, input_segments])\n    \n    x = tf.keras.layers.GlobalAveragePooling1D()(sequence_output)\n    out = tf.keras.layers.Dense(30, activation=\"sigmoid\", name=\"dense_output\")(x)\n    \n    model = tf.keras.models.Model(\n        inputs=[[input_word_ids, input_masks, input_segments]], outputs=out)\n    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n    model.compile(loss=loss_function, optimizer=optimizer)\n    return model    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gkf_google_qa = GroupKFold(n_splits=10).split(X=df_train.question_body, groups=df_train.question_body)\noutputs_google_qa = compute_output_arrays_google_qa(df_train, output_categories)\ninputs_google_qa = compute_input_arays_google_qa(df_train, ['treated_question_title','treated_question_body','treated_answer'], tokenizer_google_qa, MAX_SEQUENCE_LENGTH)\ntest_inputs_google_qa = compute_input_arays_google_qa(df_test, ['treated_question_title','treated_question_body','treated_answer'],tokenizer_google_qa, MAX_SEQUENCE_LENGTH)\n# histories = []\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_predictions_google_qa=[]\nfor fold, (train_idx, valid_idx) in enumerate(gkf_google_qa):\n    \n    if fold<5:\n        keras.backend.clear_session()\n        model_qa = bert_model_google_qa()\n        \n        print(f'/kaggle/input/google-qa-bert-trained-tfbert-hiddenl-preprocess/bert_base_quest_kernel_{fold}.hdf5')\n        model_qa.load_weights(f'/kaggle/input/google-qa-bert-trained-tfbert-hiddenl-preprocess/bert_base_quest_kernel_{fold}.hdf5')\n        preds_qa = model_qa.predict(test_inputs_google_qa , batch_size=8, verbose=1)\n        test_predictions_google_qa.append(preds_qa)\n        print(f'Fold:{fold} Completed')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(test_predictions_google_qa)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds_google_qa = [test_predictions_google_qa[i] for i in range(len(test_predictions_google_qa))]\ntest_preds_google_qa = [np.average(test_preds_google_qa, axis=0) for i in range(len(test_preds_google_qa))]\ntest_preds_google_qa = np.mean(test_preds_google_qa, axis=0)\ntest_preds_google_qa.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_preds = np.average(np.array([test_preds, test_preds_google_qa]),axis=0)\nfinal_preds.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sub.iloc[:, 1:] = final_preds\n\ndf_sub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}