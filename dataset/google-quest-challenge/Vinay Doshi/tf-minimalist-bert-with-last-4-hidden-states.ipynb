{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install ../input/huggingface-transformers/sacremoses-master/sacremoses-master\n!pip install ../input/huggingface-transformers/transformers-master/transformers-master","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport torch\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport matplotlib.pyplot as plt\nimport tensorflow.keras as keras\nimport gc\nfrom sklearn.model_selection import train_test_split, GroupKFold\nimport bert_tokenization as tokenization\nfrom scipy.stats import spearmanr\nfrom math import floor, ceil\nfrom transformers import TFPreTrainedModel, TFBertMainLayer, BertConfig, TFBertModel, BertTokenizer\nfrom tqdm.notebook import tqdm\n# import pytorch_transformers\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n# tf.executing_eagerly()\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"PATH = '../input/google-quest-challenge/'\nBERT_PATH = '../input/bert-base-from-tfhub/bert_en_uncased_L-12_H-768_A-12'\ntokenizer = tokenization.FullTokenizer(BERT_PATH+'/assets/vocab.txt', True)\n# tokenizer2 = transformers.BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\nMAX_SEQUENCE_LENGTH = 512\n\ndf_train = pd.read_csv(PATH+'train.csv')\ndf_test = pd.read_csv(PATH+'test.csv')\ndf_sub = pd.read_csv(PATH+'sample_submission.csv')\nprint('train shape =', df_train.shape)\nprint('test shape =', df_test.shape)\n\n\noutput_categories = ['question_asker_intent_understanding', 'question_body_critical', 'question_conversational', 'question_expect_short_answer', 'question_fact_seeking', 'question_has_commonly_accepted_answer', 'question_interestingness_others', 'question_interestingness_self', 'question_multi_intent', 'question_not_really_a_question', 'question_opinion_seeking', 'question_type_choice', 'question_type_compare', 'question_type_consequence', 'question_type_definition', 'question_type_entity', 'question_type_instructions', 'question_type_procedure', 'question_type_reason_explanation', 'question_type_spelling', 'question_well_written', 'answer_helpful', 'answer_level_of_information', 'answer_plausible', 'answer_relevance', 'answer_satisfaction', 'answer_type_instructions', 'answer_type_procedure', 'answer_type_reason_explanation', 'answer_well_written']\ninput_categories = ['question_title', 'question_body', 'answer']\nprint('\\noutput categories:\\n\\t', output_categories)\nprint('\\ninput categories:\\n\\t', input_categories)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def _get_masks(tokens, max_seq_length):\n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    return [1]*len(tokens)+[0]*(max_seq_length-len(tokens))\n\ndef _get_segments(tokens, max_seq_length):\n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    \n    segments=[]\n    first_sep=True\n    current_segment_id = 0\n    for token in tokens:\n        segments.append(current_segment_id)\n        if token == '[SEP]':\n            if first_sep:\n                first_sep=False\n            else:\n                current_segment_id=1\n    return segments+[0]*(max_seq_length-len(tokens))\n\ndef _trim_input(title, question, answer, max_sequence_length, t_max_len=30, q_max_len=239, a_max_len=239):\n\n    t = tokenizer.tokenize(title)\n    q = tokenizer.tokenize(question)\n    a = tokenizer.tokenize(answer)\n    \n    t_len = len(t)\n    q_len = len(q)\n    a_len = len(a)\n\n    if (t_len+q_len+a_len+4) > max_sequence_length:\n        \n        if t_max_len > t_len:\n            t_new_len = t_len\n            a_max_len = a_max_len + floor((t_max_len - t_len)/2)\n            q_max_len = q_max_len + ceil((t_max_len - t_len)/2)\n        else:\n            t_new_len = t_max_len\n      \n        if a_max_len > a_len:\n            a_new_len = a_len \n            q_new_len = q_max_len + (a_max_len - a_len)\n        elif q_max_len > q_len:\n            a_new_len = a_max_len + (q_max_len - q_len)\n            q_new_len = q_len\n        else:\n            a_new_len = a_max_len\n            q_new_len = q_max_len\n            \n            \n        if t_new_len+a_new_len+q_new_len+4 != max_sequence_length:\n            raise ValueError(\"New sequence length should be %d, but is %d\" \n                             % (max_sequence_length, (t_new_len+a_new_len+q_new_len+4)))\n        \n        t = t[:t_new_len]\n        q = q[:q_new_len]\n        a = a[:a_new_len]\n    \n    return t, q, a\n\ndef _get_ids(tokens, tokenizer, max_seq_length):\n    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n    input_ids = token_ids + [0]*(max_seq_length - len(token_ids))\n    return input_ids\n\ndef _convert_to_bert_inputs(title, question, answer, tokenizer, max_seq_length):\n    stoken = ['[CLS]']+title+['[SEP]']+question+['[SEP]']+answer+['[SEP]']\n    input_ids = _get_ids(tokens=stoken, tokenizer=tokenizer,max_seq_length=max_seq_length)\n    input_masks = _get_masks(tokens=stoken, max_seq_length=max_seq_length)\n    input_segments = _get_segments(tokens=stoken, max_seq_length=max_seq_length)\n    \n    return [input_ids, input_masks, input_segments]\n\ndef compute_input_array(df, columns, tokenizer, max_sequence_length):\n    input_ids, input_masks, input_segments = [], [], []\n    for _, col in tqdm(df[columns].iterrows()):\n        t, q, a = col['question_title'], col['question_body'], col['answer']\n        t,q,a = _trim_input(t,q,a, max_sequence_length)\n        ids, masks, segments = _convert_to_bert_inputs(t,q,a, tokenizer, max_sequence_length)\n        input_ids.append(ids)\n        input_masks.append(masks)\n        input_segments.append(segments)\n        \n    return [np.array(input_ids, dtype=np.int32), np.array(input_masks, dtype=np.int32), \n            np.array(input_segments, dtype=np.int32)]\n\ndef compute_output_arrays(df, columns):\n    return np.asarray(df[columns])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_spearmanr(trues, preds):\n    rhos=[]\n    for t, p in zip(trues.T, preds.T):\n        rhos.append(\n            spearmanr(t,p).correlation\n        )\n    return np.nanmean(rhos)\n\ntest_predictions=[]\n\nclass CustomCallback(keras.callbacks.Callback):\n    def __init__(self, valid_data, test_data, test_predictions=test_predictions, batch_size=16, fold=None):\n        self.valid_inputs = valid_data[0]\n        self.valid_outputs = valid_data[1]\n        self.test_inputs = test_data\n        self.batch_size = batch_size\n        self.test_predictions = test_predictions\n        self.fold = fold\n        \n    def on_train_begin(self, logs={}):\n        self.valid_predictions=[]\n        #self.test_predictions=[]\n        \n    def on_epoch_end(self, epoch, logs={}):\n        self.valid_predictions.append(self.model.predict(self.valid_inputs, batch_size=self.batch_size))\n        \n        rho_val = compute_spearmanr(self.valid_outputs, np.average(self.valid_predictions, axis=0))\n        print(f\"\\nvalidation rho: {round(rho_val,4)}\")\n        \n        if self.fold is not None and int(epoch)==5:\n            self.model.save_weights(f'bert-base-{fold}-{epoch}.hdf5')\n            \n        self.test_predictions.append(self.model.predict(self.test_inputs, batch_size=self.batch_size))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bert_config=BertConfig.from_pretrained('../input/bert-tensorflow/bert-base-uncased-config.json',output_hidden_states=True)\ndef bertModel():\n    input_ids = keras.layers.Input((MAX_SEQUENCE_LENGTH), dtype = tf.int32, name = 'input_word_ids')\n    input_mask = keras.layers.Input((MAX_SEQUENCE_LENGTH), dtype = tf.int32, name = 'input_masks')\n    input_segments = keras.layers.Input((MAX_SEQUENCE_LENGTH), dtype = tf.int32, name = 'input_segments')\n    \n    bert_model = TFBertModel.from_pretrained(pretrained_model_name_or_path='../input/bert-tensorflow/bert-base-uncased-tf_model.h5',config = bert_config)\n\n    sequence_output, pooler_output, hidden_states = bert_model([input_ids,input_mask, input_segments])\n    \n    h12 = tf.reshape(hidden_states[-1][:,0],(-1,1,768))\n    h11 = tf.reshape(hidden_states[-2][:,0],(-1,1,768))\n    h10 = tf.reshape(hidden_states[-3][:,0],(-1,1,768))\n    h09 = tf.reshape(hidden_states[-4][:,0],(-1,1,768))\n    concat_hidden = keras.layers.Concatenate(axis=1)([h12, h11, h10, h09])\n    \n    x = keras.layers.GlobalAveragePooling1D()(concat_hidden)\n    x = keras.layers.Dropout(0.2)(x)\n    out = keras.layers.Dense(len(output_categories), activation='sigmoid', name='final_dense_output')(x)\n    \n    model = keras.models.Model(inputs=[input_ids, input_mask, input_segments], outputs=out)\n    model.compile(loss='binary_crossentropy', optimizer = keras.optimizers.Adam(lr=3e-5))\n    return model\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model = bertModel()\n# model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gkf = GroupKFold(n_splits=10).split(X=df_train.question_body, groups=df_train.question_body)\n\noutputs = compute_output_arrays(df_train, output_categories)\ninputs = compute_input_array(df_train, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)\ntest_inputs = compute_input_array(df_test, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"histories = []\n\nfor fold, (train_idx, valid_idx) in enumerate(gkf):\n    \n    if fold<3:\n        keras.backend.clear_session()\n        model = bertModel()\n        train_inputs = [inputs[i][train_idx] for i in range(3)]\n        train_outputs = outputs[train_idx]\n    \n        valid_inputs = [inputs[i][valid_idx] for i in range(3)]\n        valid_outputs = outputs[valid_idx]\n    \n        custom_callback = CustomCallback(valid_data=(valid_inputs, valid_outputs), test_data=test_inputs,\n                                         batch_size=8, fold=fold)\n        H = model.fit(train_inputs, train_outputs, batch_size=8, epochs=4, callbacks=[custom_callback])\n        histories.append(H)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(test_predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds = [test_predictions[i] for i in range(len(test_predictions))]\ntest_preds = [np.average(test_preds[i], axis=0) for i in range(len(test_preds))]\ntest_preds = np.mean(test_predictions, axis=0)\n\ndf_sub.iloc[:, 1:] = test_preds\n\ndf_sub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# del model\n# gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# zz = BertConfig.from_pretrained('../input/bert-tensorflow/bert-base-uncased-config.json',output_hidden_states=True)\n# class TFBertPreTrainedModel(TFPreTrainedModel):\n#     \"\"\" An abstract class to handle weights initialization and\n#         a simple interface for dowloading and loading pretrained models.\n#     \"\"\"\n\n#     config_class = zz\n#     pretrained_model_archive_map = {\"bert-base-uncased\": \"../input/bert-tensorflow/bert-base-uncased-tf_model.h5\"}\n#     base_model_prefix = \"bert\"\n\n\n# class TFBert_v2(TFBertPreTrainedModel):\n#     def __init__(self, config='../input/bert-tensorflow/bert-base-uncased-config.json', *inputs, **kwargs):\n#         self.config = config\n#         self.TF_BERT_PRETRAINED_MODEL_ARCHIVE_MAP = {\"bert-base-uncased\": \"../input/bert-tensorflow/bert-base-uncased-tf_model.h5\"}\n#         super(TFBert_v2, self).__init__(self.config, *inputs, **kwargs)\n#         self.bert = TFBertMainLayer(config, name=\"bert\")\n\n#     def call(self, inputs, **kwargs):\n#         outputs = self.bert(inputs, **kwargs)\n#         return outputs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# bert_config=BertConfig.from_pretrained('../input/bert-tensorflow/bert-base-uncased-config.json',output_hidden_states=True)\n# TFBertModel.from_pretrained(pretrained_model_name_or_path='../input/bert-tensorflow/bert-base-uncased-tf_model.h5', config = bert_config)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tok = BertTokenizer(vocab_file=BERT_PATH+'/assets/vocab.txt')#.from_pretrained('bert-base-uncased', do_lower_case=True)\n# input_ids=tf.constant(tok.encode(text=\"Hello, my dog is cute\", add_special_tokens=True))[None, :]  # Batch size 1\n# outputs = m(input_ids)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}