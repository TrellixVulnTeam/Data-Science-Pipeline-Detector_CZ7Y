{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \nfrom keras.preprocessing.text import Tokenizer\nfrom keras.engine.topology import Layer\nfrom keras.preprocessing.sequence import pad_sequences\nimport numpy as np # linear algebra\nfrom keras.optimizers import Adam, Nadam\nimport fasttext\nimport re\nfrom keras import initializers\nfrom nltk.tokenize import word_tokenize\nfrom keras.layers import Embedding\nfrom keras import backend as K\nfrom keras.layers import Dense, Input, Flatten\nfrom keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, LSTM, GRU, Bidirectional, TimeDistributed\nfrom keras.models import Model\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train=pd.read_csv(\"/kaggle/input/google-quest-challenge/train.csv\")\ntest=pd.read_csv(\"/kaggle/input/google-quest-challenge/test.csv\")\nsub=pd.read_csv(\"/kaggle/input/google-quest-challenge/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('[%s]' % ', '.join(map(str, train.columns.values.tolist())))\nprint(\"\\n\")\nprint('[%s]' % ', '.join(map(str, test.columns.values.tolist())))\nprint(\"\\n\")\nprint('[%s]' % ', '.join(map(str, sub.columns.values.tolist())))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_columns = sub.columns.values[1:].tolist()\ntarget_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xtrain=train[[\"question_title\", \"question_body\",\"answer\"]]\nxtest=test[[\"question_title\", \"question_body\",\"answer\"]]\nytrain=train[target_columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xtrain","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ytrain","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xtest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_text(sen):\n    # Remove punctuations and numbers\n    sentence = re.sub('[^a-zA-Z]', ' ', sen)\n\n    # Single character removal\n    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n\n    # Removing multiple spaces\n    sentence = re.sub(r'\\s+', ' ', sentence)\n    \n    sentence=sentence.lower()\n\n    return sentence","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(xtrain)):\n    xtrain[\"question_title\"][i]= preprocess_text(xtrain[\"question_title\"][i])\n    xtrain[\"question_body\"][i]= preprocess_text(xtrain[\"question_body\"][i])\n    xtrain[\"answer\"][i]= preprocess_text(xtrain[\"answer\"][i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(xtest)):\n    xtest[\"question_title\"][i]= preprocess_text(xtest[\"question_title\"][i])\n    xtest[\"question_body\"][i]= preprocess_text(xtest[\"question_body\"][i])\n    xtest[\"answer\"][i]= preprocess_text(xtest[\"answer\"][i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xtest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"emb_file = fasttext.load_model('../input/fasttext-pretrained-word-vectors-english/wiki.en.bin')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainl=len(xtrain)\ntestl=len(xtest)\nfulld=xtrain.append(xtest,ignore_index=True)\nfulld=fulld.stack().reset_index()[0]\nimport nltk\nfor i in range(len(fulld)):\n    k=len(nltk.word_tokenize(fulld[i]))\n    #lens.append(len(data[0][i].split(\" \")))\n    if(k<500):\n        pass\n    else:\n        te=nltk.word_tokenize(fulld[i])[:500]\n        fulld[i]=' '.join(te)\nmax_features = 50000 #number of words to keep. 1200 is the number of unique words in the corpus.\ntokenizer = Tokenizer(nb_words=max_features, split=' ')\ntokenizer.fit_on_texts(fulld.values)\nX = tokenizer.texts_to_sequences(fulld.values)\nX = pad_sequences(X, padding = 'post') #Zero padding at the end of the sequence\nword_index = tokenizer.word_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nb_words = len(word_index)+1\nembedding_dimension = 300\n\nembedding_matrix = np.zeros((nb_words, embedding_dimension))\nfor word, i in word_index.items():\n    embedding_matrix[i,:] = emb_file.get_word_vector(word).astype(np.float32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=np.split(X,trainl+testl)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=np.array(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xtrain1=X[0:trainl]\nxtest1=X[trainl:trainl+testl]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ytrain=train[target_columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class AttentionLayer(Layer):\n    \"\"\"\n    Hierarchial Attention Layer as described by Hierarchical Attention Networks for Document Classification(2016)\n    - Yang et. al.\n    Source: https://www.cs.cmu.edu/~hovy/papers/16HLT-hierarchical-attention-networks.pdf\n    Theano backend\n    \"\"\"\n    def __init__(self,attention_dim=100,return_coefficients=False,**kwargs):\n        # Initializer \n        self.supports_masking = True\n        self.return_coefficients = return_coefficients\n        self.init = initializers.get('glorot_uniform') # initializes values with uniform distribution\n        self.attention_dim = attention_dim\n        super(AttentionLayer, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        # Builds all weights\n        # W = Weight matrix, b = bias vector, u = context vector\n        assert len(input_shape) == 3\n        self.W = K.variable(self.init((input_shape[-1], self.attention_dim)),name='W')\n        self.b = K.variable(self.init((self.attention_dim, )),name='b')\n        self.u = K.variable(self.init((self.attention_dim, 1)),name='u')\n        self.trainable_weights = [self.W, self.b, self.u]\n\n        super(AttentionLayer, self).build(input_shape)\n\n    def compute_mask(self, input, input_mask=None):\n        return None\n\n    def call(self, hit, mask=None):\n        # Here, the actual calculation is done\n        uit = K.bias_add(K.dot(hit, self.W),self.b)\n        uit = K.tanh(uit)\n        \n        ait = K.dot(uit, self.u)\n        ait = K.squeeze(ait, -1)\n        ait = K.exp(ait)\n        \n        if mask is not None:\n            ait *= K.cast(mask, K.floatx())\n\n        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n        ait = K.expand_dims(ait)\n        weighted_input = hit * ait\n        \n        if self.return_coefficients:\n            return [K.sum(weighted_input, axis=1), ait]\n        else:\n            return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        if self.return_coefficients:\n            return [(input_shape[0], input_shape[-1]), (input_shape[0], input_shape[-1], 1)]\n        else:\n            return input_shape[0], input_shape[-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xtrain1.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_layer = Embedding(len(word_index) + 1,300,weights=[embedding_matrix],\n                            input_length=500,\n                            trainable=True,\n                            mask_zero=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Words level attention model\nword_input = Input(shape=(500,), dtype='int32',name='word_input')\nword_sequences = embedding_layer(word_input)\nword_gru = Bidirectional(LSTM(50, return_sequences=True),name='word_gru')(word_sequences)\nword_dense = Dense(100, activation='relu', name='word_dense')(word_gru) \nword_att,word_coeffs = AttentionLayer(300,True,name='word_attention')(word_dense)\nwordEncoder = Model(inputs = word_input,outputs = word_att)\n\n# Sentence level attention model\nsent_input = Input(shape=(3,500), dtype='int32',name='sent_input')\nsent_encoder = TimeDistributed(wordEncoder,name='sent_linking')(sent_input)\nsent_gru = Bidirectional(LSTM(50, return_sequences=True),name='sent_gru')(sent_encoder)\nsent_dense = Dense(100, activation='relu', name='sent_dense')(sent_gru) \nsent_att,sent_coeffs = AttentionLayer(300,return_coefficients=True,name='sent_attention')(sent_dense)\nsent_drop = Dropout(0.5,name='sent_dropout')(sent_att)\npreds = Dense(30, activation='sigmoid',name='output')(sent_drop)\n\n# Model compile\nmodel1 = Model(sent_input, preds)\nmodel1.compile(loss='binary_crossentropy', optimizer=Adam(lr=1e-3, clipnorm=4), metrics=['accuracy'])\nprint(wordEncoder.summary())\nprint(model1.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1.fit(xtrain1, ytrain,batch_size=50, epochs=3, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1.predict(xtest1).shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred = model1.predict(xtest1)\nprint(test_pred.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ids=test[\"qa_id\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tem=pd.DataFrame(test_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outs = tem.set_axis(target_columns, axis=1, inplace=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final=pd.concat([ids,outs], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}