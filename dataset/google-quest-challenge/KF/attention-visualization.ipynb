{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import functools\nimport importlib\nimport gc\nimport os\nimport sys\nfrom dataclasses import dataclass\nfrom multiprocessing import Pool\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport plotly\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport seaborn as sns\nimport torch\nimport transformers\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom tqdm.notebook import tqdm\nfrom transformers.modeling_bert import BertPreTrainedModel","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"@dataclass\nclass Config:\n    \n    nrows = None\n    inputdir = Path('/kaggle/input')\n    datadir = inputdir / 'google-quest-challenge'\n    codedir = inputdir / 'bert-base-random-code'\n    pretrained_modeldir = inputdir / 'bert-base-pretrained/stackx-base-cased'\n    finetune_modeldir = inputdir / 'bert-base-pseudo-noleak-random'\n    input_columns = [\"question_title\", \"question_body\", \"answer\"]\n    target_columns = [\n        \"question_asker_intent_understanding\",\n        \"question_body_critical\",\n        \"question_conversational\",\n        \"question_expect_short_answer\",\n        \"question_fact_seeking\",\n        \"question_has_commonly_accepted_answer\",\n        \"question_interestingness_others\",\n        \"question_interestingness_self\",\n        \"question_multi_intent\",\n        \"question_not_really_a_question\",\n        \"question_opinion_seeking\",\n        \"question_type_choice\",\n        \"question_type_compare\",\n        \"question_type_consequence\",\n        \"question_type_definition\",\n        \"question_type_entity\",\n        \"question_type_instructions\",\n        \"question_type_procedure\",\n        \"question_type_reason_explanation\",\n        \"question_type_spelling\",\n        \"question_well_written\",\n        \"answer_helpful\",\n        \"answer_level_of_information\",\n        \"answer_plausible\",\n        \"answer_relevance\",\n        \"answer_satisfaction\",\n        \"answer_type_instructions\",\n        \"answer_type_procedure\",\n        \"answer_type_reason_explanation\",\n        \"answer_well_written\",\n    ]\n    max_sequence_length = 500\n    max_title_length = 26\n    max_question_length = 260\n    max_answer_length = 210\n    batch_size = 8\n    head_tail = True\n    num_classes = 30\n\nconfig = Config()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_module(filename):\n    assert isinstance(filename, Path)\n    name = filename.stem\n    spec = importlib.util.spec_from_file_location(name, filename)\n    mod = importlib.util.module_from_spec(spec)\n    spec.loader.exec_module(mod)\n    sys.modules[mod.__name__] = mod\n    return mod","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"datasets = load_module(config.codedir / 'dataset.py')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv(config.datadir / 'test.csv', nrows=config.nrows)\n\ntokenizers = dict()\ntokenizers['origin'] = transformers.BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)\ntokenizers['pretrained'] = transformers.BertTokenizer.from_pretrained(str(config.pretrained_modeldir), do_lower_case=False)\n\ntest_datasets = dict()\ntest_datasets['origin'] = datasets.get_test_set(config, test_df, tokenizers['origin'])\ntest_datasets['pretrained'] = datasets.get_test_set(config, test_df, tokenizers['pretrained'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class BertAttentionExtractor(BertPreTrainedModel):\n\n    def __init__(self, config):\n        config.output_hidden_states = True\n        config.output_attentions = True\n        super().__init__(config)\n        self.num_labels = config.num_labels\n        self.bert = transformers.BertModel(config)\n        self.dropout = nn.Dropout(p=0.2)\n        self.high_dropout = nn.Dropout(p=0.5)\n\n        n_weights = config.num_hidden_layers + 1\n        weights_init = torch.zeros(n_weights).float()\n        weights_init.data[:-1] = -3\n        self.layer_weights = torch.nn.Parameter(weights_init)\n        self.classifier = torch.nn.Linear(config.hidden_size, self.config.num_labels)\n\n        self.init_weights()\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n    ):\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n        )\n        return outputs[-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# models = dict()\n# models['origin'] = BertAttentionExtractor.from_pretrained('bert-base-cased', num_labels=config.num_classes)\n# models['pretrained'] = BertAttentionExtractor.from_pretrained(str(config.pretrained_modeldir), num_labels=config.num_classes)\n# models['finetuned_fold0'] = BertAttentionExtractor.from_pretrained(str(config.pretrained_modeldir), num_labels=config.num_classes)\n# models['finetuned_fold0'] = torch.nn.DataParallel(models['finetuned_fold0'])\n# models['finetuned_fold0'].load_state_dict(torch.load(config.finetune_modeldir / 'fold0/best_model.pth', map_location=torch.device('cpu')))\n# models['finetuned_fold1'] = BertAttentionExtractor.from_pretrained(str(config.pretrained_modeldir), num_labels=config.num_classes)\n# models['finetuned_fold1'] = torch.nn.DataParallel(models['finetuned_fold1'])\n# models['finetuned_fold1'].load_state_dict(torch.load(config.finetune_modeldir / 'fold1/best_model.pth', map_location=torch.device('cpu')))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_heatmap(i, attention_maps, tokens):\n    row, col = i // n_cols, i % n_cols\n    heatmap = go.Heatmap(\n        z=attention_maps[row, col],\n        colorscale=['ivory', 'red', 'purple'],\n        zmin=0, zmax=0.1,\n#         customdata=tokens,\n        hovertemplate=\"\"\"\n        attention: %{z}\n        <br>x: %{x}, y: %{y}\n        <extra></extra>\n        \"\"\"\n#         <br>x: \"%{customdata[0]}\", y: \"%{customdata[1]}\"\n    )\n    return heatmap\n    \n\ndef plot_attention_heatmap(model, tokenizer, sample, title, n_rows=12, n_cols=12):\n    model.eval()\n    attention_maps = model(\n        input_ids=sample.input_ids[None],\n        attention_mask=sample.attention_mask[None],\n        token_type_ids=sample.token_type_ids[None],\n    )\n    attention_maps = torch.stack(attention_maps, axis=1).detach().cpu().numpy()[0]\n    \n    tokens = tokenizer.convert_ids_to_tokens(sample.input_ids)\n    tokens = np.stack([\n        np.array(tokens)[None, :].repeat(500, axis=0),\n        np.array(tokens)[:, None].repeat(500, axis=1),\n    ], axis=-1)\n    N = n_rows * n_cols\n    fig = plotly.subplots.make_subplots(n_rows, n_cols)\n    \n    f = functools.partial(\n        build_heatmap,\n        attention_maps=attention_maps,\n        tokens=tokens\n    )\n    with Pool(os.cpu_count()) as pool:\n        for i, heatmap in enumerate(tqdm(pool.imap(f, range(N)), total=N)):\n            row, col = i // n_cols, i % n_cols\n            fig.add_trace(trace=heatmap, row=row+1, col=col+1)\n\n    fig.update_layout(\n        width=200 * n_rows,\n        height=200 * n_cols,\n        title_text=f\"\"\"\n        Attention heatmap [{title}]\n        <br>question_title: {sample.question_title}\n        <br>question_body: {sample.question_body}\n        <br>answer: {sample.answer}\n        \"\"\".strip(),\n        font=dict(\n            size=7,\n        ),\n    )\n\n    return fig\n#     fig.write_html(f\"{key}_{i_data:04}.html\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(name):\n    if name == 'origin':\n        return BertAttentionExtractor.from_pretrained('bert-base-cased', num_labels=config.num_classes)\n    elif name == 'pretrained':\n        return BertAttentionExtractor.from_pretrained(str(config.pretrained_modeldir), num_labels=config.num_classes)\n    elif name == 'finetuned_fold0':\n        model = BertAttentionExtractor.from_pretrained(str(config.pretrained_modeldir), num_labels=config.num_classes)\n        model = torch.nn.DataParallel(model)\n        model.load_state_dict(torch.load(config.finetune_modeldir / 'fold0/best_model.pth', map_location=torch.device('cpu')))\n        return model\n    elif name == 'finetuned_fold1':\n        model = BertAttentionExtractor.from_pretrained(str(config.pretrained_modeldir), num_labels=config.num_classes)\n        model = torch.nn.DataParallel(model)\n        model.load_state_dict(torch.load(config.finetune_modeldir / 'fold1/best_model.pth', map_location=torch.device('cpu')))\n        return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"idx = 0\nname = 'origin'\nn_rows, n_cols = 2, 2\n\nmodel = build_model(name)\ndataset = test_datasets['origin'] if name == 'origin' else test_datasets['pretrained']\ntokenizer = tokenizers['origin'] if name == 'origin' else tokenizers['pretrained']\n\nsample = test_df.iloc[idx].copy()\nsample['input_ids'], sample['attention_mask'], sample['token_type_ids'], _ = dataset[idx]\nfig = plot_attention_heatmap(model, tokenizer, sample, title=name, n_rows=n_rows, n_cols=n_cols)\nfig\n#     fig.write_html(f\"{key}_{i_data:04}.html\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"idx = 1\nn_rows, n_cols = 12, 12\nmodels = [\n    'origin',\n    'pretrained',\n    'finetuned_fold0',\n    'finetuned_fold1',\n]\n\nfor name in models:\n    del model, fig\n    gc.collect()\n    model = build_model(name)\n    dataset = test_datasets['origin'] if name == 'origin' else test_datasets['pretrained']\n    tokenizer = tokenizers['origin'] if name == 'origin' else tokenizers['pretrained']\n\n    sample = test_df.iloc[idx].copy()\n    sample['input_ids'], sample['attention_mask'], sample['token_type_ids'], _ = dataset[idx]\n    fig = plot_attention_heatmap(model, tokenizer, sample, title=name, n_rows=n_rows, n_cols=n_cols)\n    fig.write_html(f\"{name}_data={idx:04}.html\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!df -h","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}