{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Load Datasets"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!ls ../input","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path_dir='/kaggle/input/google-quest-challenge/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df=pd.read_csv(path_dir+'test.csv',index_col='qa_id')\ntrain_df=pd.read_csv(path_dir+'train.csv',index_col='qa_id')\nsamp_sum_df=pd.read_csv(path_dir+'sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Train data ',train_df.shape)\nprint('Test data ',test_df.shape)\nprint('Sample submission data ',samp_sum_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Visualisation"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_category = train_df['category'].value_counts()\ntest_category = test_df['category'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(1, 2, figsize=(12, 6))\ntrain_category.plot(kind='bar', ax=axes[0])\naxes[0].set_title('Train')\ntest_category.plot(kind='bar', ax=axes[1])\naxes[1].set_title('Test')\nprint('Train/Test category distribution')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\n\n\ndef plot_wordcloud(text, ax, title=None):\n    wordcloud = WordCloud(max_font_size=None, background_color='white',\n                          width=1200, height=1000).generate(text_cat)\n    ax.imshow(wordcloud)\n    if title is not None:\n        ax.set_title(title)\n    ax.axis(\"off\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Training data Word Cloud')\n\nfig, axes = plt.subplots(1, 3, figsize=(16, 18))\n\ntext_cat = ' '.join(train_df['question_title'].values)\nplot_wordcloud(text_cat, axes[0], 'Question title')\n\ntext_cat = ' '.join(train_df['question_body'].values)\nplot_wordcloud(text_cat, axes[1], 'Question body')\n\ntext_cat = ' '.join(train_df['answer'].values)\nplot_wordcloud(text_cat, axes[2], 'Answer')\n\nplt.tight_layout()\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Test data Word Cloud')\n\nfig, axes = plt.subplots(1, 3, figsize=(16, 18))\n\ntext_cat = ' '.join(test_df['question_title'].values)\nplot_wordcloud(text_cat, axes[0], 'Question title')\n\ntext_cat = ' '.join(test_df['question_body'].values)\nplot_wordcloud(text_cat, axes[1], 'Question body')\n\ntext_cat = ' '.join(test_df['answer'].values)\nplot_wordcloud(text_cat, axes[2], 'Answer')\n\nplt.tight_layout()\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_columns = [\n    'question_asker_intent_understanding',\n    'question_body_critical',\n    'question_conversational',\n    'question_expect_short_answer',\n    'question_fact_seeking',\n    'question_has_commonly_accepted_answer',\n    'question_interestingness_others',\n    'question_interestingness_self',\n    'question_multi_intent',\n    'question_not_really_a_question',\n    'question_opinion_seeking',\n    'question_type_choice',\n    'question_type_compare',\n    'question_type_consequence',\n    'question_type_definition',\n    'question_type_entity',\n    'question_type_instructions',\n    'question_type_procedure',\n    'question_type_reason_explanation',\n    'question_type_spelling',\n    'question_well_written',\n    'answer_helpful',\n    'answer_level_of_information',\n    'answer_plausible',\n    'answer_relevance',\n    'answer_satisfaction',\n    'answer_type_instructions',\n    'answer_type_procedure',\n    'answer_type_reason_explanation',\n    'answer_well_written'\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = train_df[target_columns].copy()\nx_train = train_df.drop(target_columns, axis=1)\n#del train_df\n\nx_test = test_df.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_encoder = Pipeline([\n    ('Text-TF-IDF', TfidfVectorizer(ngram_range=(1, 3))),\n    ('Text-SVD', TruncatedSVD(n_components = 100))], verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nfrom urllib.parse import urlparse\nimport re\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import FunctionTransformer\nfrom category_encoders.one_hot import OneHotEncoder\n\n\n# gives part of string (URL) before '.'\nbefore_dot = re.compile('^[^.]*')\n\ndef transform_url(x):\n    return x.apply(lambda v: re.findall(before_dot, urlparse(v).netloc)[0])\n\nurl_encoder = Pipeline([\n    ('URL-transformer', FunctionTransformer(transform_url, validate=False)),\n    ('URL-OHE', OneHotEncoder(drop_invariant=True))], verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom category_encoders.one_hot import OneHotEncoder\n\n\nohe = OneHotEncoder(cols='category', drop_invariant=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\n\ndef count_words(data):\n    out = pd.DataFrame(index=data.index)\n    for column in data.columns:\n        out[column] = data[column].str.split().str.len()\n    return out\n\nword_counter = Pipeline([\n    ('WordCounter-transformer', FunctionTransformer(count_words, validate=False)),\n    ('WordCounter-std', StandardScaler())], verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocessor = ColumnTransformer([\n    ('Q-T', text_encoder, 'question_title'),\n    ('Q-B', text_encoder, 'question_body'),\n    ('A', text_encoder, 'answer'),\n    ('URL', url_encoder, 'url'),\n    ('Categoty', ohe, 'category'),\n    ('W-C', word_counter, ['question_body', 'answer'])], verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = preprocessor.fit_transform(x_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test = preprocessor.transform(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = y_train.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\n\nimport torch.nn as nn\n\nfrom torch.nn import Sequential\nfrom torch.nn import Linear\nfrom torch.nn import ReLU\nfrom torch.nn.utils.weight_norm import weight_norm\n\nfrom torch.nn import MSELoss\nfrom torch.optim import Adam\n\nimport random\n\n\nrandom.seed(42)\nnp.random.seed(42)\ntorch.manual_seed(42)\n\n\nclass PyTorch:\n    \n    def __init__(self, in_features, out_features, n_epochs, patience):\n        self.in_features = in_features\n        self.out_features = out_features\n        self.n_epochs = n_epochs\n        self.patience = patience\n    \n    \n    def init_model(self):\n        \n        # define a model\n        self.model = Sequential(\n            weight_norm(Linear(self.in_features, 128)),\n            ReLU(),\n            weight_norm(Linear(128, 128)),\n            ReLU(),\n            weight_norm(Linear(128, self.out_features)))\n        \n        # initialize model\n        for t in self.model:\n            if isinstance(t, Linear):\n                nn.init.kaiming_normal_(t.weight_v)\n                nn.init.kaiming_normal_(t.weight_g)\n                nn.init.constant_(t.bias, 0)\n        \n        # define loss function\n        self.loss_func = MSELoss()\n        \n        # define optimizer\n        self.optimizer = Adam(self.model.parameters(), lr=1e-3)\n    \n    \n    def fit(self, x_train, y_train, x_valid, y_valid):\n        \n        validate = (x_valid is not None) & (y_valid is not None)\n        \n        self.init_model()\n        \n        x_train_tensor = torch.as_tensor(x_train, dtype=torch.float32)\n        y_train_tensor = torch.as_tensor(y_train, dtype=torch.float32)\n        \n        if validate:\n            x_valid_tensor = torch.as_tensor(x_valid, dtype=torch.float32)\n            y_valid_tensor = torch.as_tensor(y_valid, dtype=torch.float32)\n        \n        min_loss = np.inf\n        counter = 0\n        \n        for epoch in range(self.n_epochs):\n            \n            self.model.train()\n            y_pred = self.model(x_train_tensor)\n            loss = self.loss_func(y_pred, y_train_tensor)\n            \n            loss.backward()\n            self.optimizer.step()\n            self.optimizer.zero_grad()\n            \n            current_loss = loss.item()\n            # print('Epoch %5d / %5d. Loss = %.5f' % (epoch + 1, self.n_epochs, current_loss))\n\n            if validate:\n                # calculate loss for validation set\n                self.model.eval()\n                with torch.no_grad():\n                    current_loss = self.loss_func(self.model(x_valid_tensor), y_valid_tensor).item()\n                # print('Epoch %5d / %5d. Validation loss = %.5f' % (epoch + 1, self.n_epochs, current_loss))\n            \n            # early stopping\n            if current_loss < min_loss:\n                min_loss = current_loss\n                counter = 0\n            else:\n                counter += 1\n                # print('Early stopping: %i / %i' % (counter, self.patience))\n                if counter >= self.patience:\n                    # print('Early stopping at epoch', epoch + 1)\n                    break\n    \n    \n    def predict(self, x):\n        x_tenson = torch.as_tensor(x, dtype=torch.float32)\n        self.model.eval()\n        with torch.no_grad():\n            return self.model(x_tenson).numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import spearmanr\n\n\ndef mean_spearmanr_correlation_score(y_true, y_pred):\n    return np.mean([spearmanr(y_pred[:, idx], y_true[:, idx]).correlation for idx in range(len(target_columns))])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pytorch_params = {\n    'in_features': x_train.shape[1],\n    'out_features': y_train.shape[1],\n    'n_epochs': 2500,\n    'patience': 5\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trained_estimators = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"estimator = PyTorch(**pytorch_params)\nestimator.fit(x_train, y_train, None, None)\ntrained_estimators.append(estimator)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\nimport math\n\n\nn_splits = 10\n\nscores = []\n\ncv = KFold(n_splits=n_splits, random_state=42)\nfor train_idx, valid_idx in cv.split(x_train, y_train):\n    \n    x_train_train = x_train[train_idx]\n    y_train_train = y_train[train_idx]\n    x_train_valid = x_train[valid_idx]\n    y_train_valid = y_train[valid_idx]\n    \n    estimator = PyTorch(**pytorch_params)\n    estimator.fit(x_train_train, y_train_train, x_train_valid, y_train_valid)\n    \n    oof_part = estimator.predict(x_train_valid)\n    score = mean_spearmanr_correlation_score(y_train_valid, oof_part)\n    print('Score:', score)\n    \n    if not math.isnan(score):\n        trained_estimators.append(estimator)\n        scores.append(score)\n\n\nprint('Mean score:', np.mean(scores))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = []\nfor estimator in trained_estimators:\n    y_pred.append(estimator.predict(x_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import rankdata\n\n\ndef blend_by_ranking(data, weights):\n    out = np.zeros(data.shape[0])\n    for idx,column in enumerate(data.columns):\n        out += weights[idx] * rankdata(data[column].values)\n    out /= np.max(out)\n    return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv(\"../input/google-quest-challenge/sample_submission.csv\", index_col='qa_id')\n\nout = pd.DataFrame(index=submission.index)\nfor column_idx,column in enumerate(target_columns):\n    \n    # collect all predictions for one column\n    column_data = pd.DataFrame(index=submission.index)\n    for prediction_idx,prediction in enumerate(y_pred):\n        column_data[str(prediction_idx)] = prediction[:, column_idx]\n    \n    out[column] = blend_by_ranking(column_data, np.ones(column_data.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"out.to_csv(\"submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_f=pd.read_csv('submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_f.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## If it is helpful to you appreciate it with your upvote."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}