{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import GroupKFold\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport os\nfrom scipy.stats import spearmanr\nfrom math import floor, ceil\nfrom transformers import *\n\nnp.set_printoptions(suppress=True)\nprint(tf.__version__)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-01-08T15:16:18.861971Z","iopub.execute_input":"2022-01-08T15:16:18.862321Z","iopub.status.idle":"2022-01-08T15:16:30.766439Z","shell.execute_reply.started":"2022-01-08T15:16:18.86228Z","shell.execute_reply":"2022-01-08T15:16:30.76424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 1. Read data and tokenizer\n\nRead tokenizer and data, as well as defining the maximum sequence length that will be used for the input to Bert (maximum is usually 512 tokens)","metadata":{}},{"cell_type":"code","source":"PATH = '../input/google-quest-challenge/'\n\nBERT_PATH = '../input/bert-base-uncased-huggingface-transformer/'\ntokenizer = BertTokenizer.from_pretrained(BERT_PATH+'bert-base-uncased-vocab.txt')\n\nMAX_SEQUENCE_LENGTH = 384\n\ndf_train = pd.read_csv(PATH+'train.csv')\ndf_test = pd.read_csv(PATH+'test.csv')\ndf_sub = pd.read_csv(PATH+'sample_submission.csv')\nprint('train shape =', df_train.shape)\nprint('test shape =', df_test.shape)\n\noutput_categories = list(df_train.columns[11:])\ninput_categories = list(df_train.columns[[1,2,5]])\nprint('\\noutput categories:\\n\\t', output_categories)\nprint('\\ninput categories:\\n\\t', input_categories)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:16:30.769516Z","iopub.execute_input":"2022-01-08T15:16:30.769856Z","iopub.status.idle":"2022-01-08T15:16:31.263533Z","shell.execute_reply.started":"2022-01-08T15:16:30.769818Z","shell.execute_reply":"2022-01-08T15:16:31.262306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_categories","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:16:31.264955Z","iopub.execute_input":"2022-01-08T15:16:31.265485Z","iopub.status.idle":"2022-01-08T15:16:31.277215Z","shell.execute_reply.started":"2022-01-08T15:16:31.265426Z","shell.execute_reply":"2022-01-08T15:16:31.275699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nlp = pipeline(\"sentiment-analysis\")\n\nprint(nlp(\"I hate you\"))\nprint(nlp(\"I love you\"))","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:16:31.280612Z","iopub.execute_input":"2022-01-08T15:16:31.281358Z","iopub.status.idle":"2022-01-08T15:16:49.027476Z","shell.execute_reply.started":"2022-01-08T15:16:31.281297Z","shell.execute_reply":"2022-01-08T15:16:49.025491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2. Preprocessing functions\n\nThese are some functions that will be used to preprocess the raw text data into useable Bert inputs.<br>\n\n*update 4:* credits to [Minh](https://www.kaggle.com/dathudeptrai) for this implementation. If I'm not mistaken, it could be used directly with other Huggingface transformers too! Note that due to the 2 x 512 input *(update 5: 2 x 384)*, it will require significantly more memory when finetuning BERT.","metadata":{}},{"cell_type":"code","source":"# 第7章 自然言語処理による感情分析（Transformer）\n\n\nimport glob\nimport os\nimport io\nimport string\nimport re\nimport random\nimport spacy\nimport torchtext\nfrom torchtext.vocab import Vectors\n\n#aclImdb_dir='./../data/aclImdb/'\naclImdb_dir='/content/data/aclImdb/'\n\ndef get_IMDb_DataLoaders_and_TEXT(max_length=256, batch_size=24):\n    \"\"\"IMDbのDataLoaderとTEXTオブジェクトを取得する。 \"\"\"\n\n    # 訓練データのtsvファイルを作成します\n    f = open('./data/IMDb_train.tsv', 'w')\n\n    path = aclImdb_dir+'train/pos/'\n    for fname in glob.glob(os.path.join(path, '*.txt')):\n        with io.open(fname, 'r', encoding=\"utf-8\") as ff:\n            text = ff.readline()\n\n            # タブがあれば消しておきます\n            text = text.replace('\\t', \" \")\n\n            text = text+'\\t'+'1'+'\\t'+'\\n'\n            f.write(text)\n\n    path = aclImdb_dir+'train/neg/'\n    for fname in glob.glob(os.path.join(path, '*.txt')):\n        with io.open(fname, 'r', encoding=\"utf-8\") as ff:\n            text = ff.readline()\n\n            # タブがあれば消しておきます\n            text = text.replace('\\t', \" \")\n\n            text = text+'\\t'+'0'+'\\t'+'\\n'\n            f.write(text)\n\n    f.close()\n\n   # テストデータの作成\n    f = open('./data/IMDb_test.tsv', 'w')\n\n    path = aclImdb_dir+'test/pos/'\n    for fname in glob.glob(os.path.join(path, '*.txt')):\n        with io.open(fname, 'r', encoding=\"utf-8\") as ff:\n            text = ff.readline()\n\n            # タブがあれば消しておきます\n            text = text.replace('\\t', \" \")\n\n            text = text+'\\t'+'1'+'\\t'+'\\n'\n            f.write(text)\n\n    path = aclImdb_dir+'test/neg/'\n    for fname in glob.glob(os.path.join(path, '*.txt')):\n        with io.open(fname, 'r', encoding=\"utf-8\") as ff:\n            text = ff.readline()\n\n            # タブがあれば消しておきます\n            text = text.replace('\\t', \" \")\n\n            text = text+'\\t'+'0'+'\\t'+'\\n'\n            f.write(text)\n    f.close()\n\n    def preprocessing_text(text):\n        # 改行コードを消去\n        text = re.sub('<br />', '', text)\n\n        # カンマ、ピリオド以外の記号をスペースに置換\n        for p in string.punctuation:\n            if (p == \".\") or (p == \",\"):\n                continue\n            else:\n                text = text.replace(p, \" \")\n\n        # ピリオドなどの前後にはスペースを入れておく\n        text = text.replace(\".\", \" . \")\n        text = text.replace(\",\", \" , \")\n        return text\n\n    # 分かち書き（今回はデータが英語で、簡易的にスペースで区切る）\n    def tokenizer_punctuation(text):\n        return text.strip().split()\n\n\n    # 前処理と分かち書きをまとめた関数を定義\n    def tokenizer_with_preprocessing(text):\n        text = preprocessing_text(text)\n        ret = tokenizer_punctuation(text)\n        return ret\n\n\n    # データを読み込んだときに、読み込んだ内容に対して行う処理を定義します\n    # max_length\n    TEXT = torchtext.legacy.data.Field(sequential=True, tokenize=tokenizer_with_preprocessing, use_vocab=True,\n                                lower=True, include_lengths=True, batch_first=True, fix_length=max_length, init_token=\"<cls>\", eos_token=\"<eos>\")\n    LABEL = torchtext.legacy.data.Field(sequential=False, use_vocab=False)\n\n    # フォルダ「data」から各tsvファイルを読み込みます\n    train_val_ds, test_ds = torchtext.legacy.data.TabularDataset.splits(\n        path='./data/', train='IMDb_train.tsv',\n        test='IMDb_test.tsv', format='tsv',\n        fields=[('Text', TEXT), ('Label', LABEL)])\n\n    # torchtext.legacy.data.Datasetのsplit関数で訓練データとvalidationデータを分ける\n    train_ds, val_ds = train_val_ds.split(\n        split_ratio=0.8, random_state=random.seed(1234))\n\n    # torchtextで単語ベクトルとして英語学習済みモデルを読み込みます\n    english_fasttext_vectors = Vectors(name='./../data/wiki-news-300d-1M.vec')\n\n    # ベクトル化したバージョンのボキャブラリーを作成します\n    TEXT.build_vocab(train_ds, vectors=english_fasttext_vectors, min_freq=10)\n\n    # DataLoaderを作成します（torchtextの文脈では単純にiteraterと呼ばれています）\n    train_dl = torchtext.legacy.data.Iterator(\n        train_ds, batch_size=batch_size, train=True)\n\n    val_dl = torchtext.legacy.data.Iterator(\n        val_ds, batch_size=batch_size, train=False, sort=False)\n\n    test_dl = torchtext.legacy.data.Iterator(\n        test_ds, batch_size=batch_size, train=False, sort=False)\n\n    return train_dl, val_dl, test_dl, TEXT\n","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:40:40.961349Z","iopub.execute_input":"2022-01-08T15:40:40.96168Z","iopub.status.idle":"2022-01-08T15:40:45.44813Z","shell.execute_reply.started":"2022-01-08T15:40:40.961611Z","shell.execute_reply":"2022-01-08T15:40:45.446952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 第7章 自然言語処理による感情分析（Transformer）\n# 実装参考に使用\n# https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec\n\n\n# 必要なパッケージのimport\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchtext\n\n\nclass Embedder(nn.Module):\n    '''idで示されている単語をベクトルに変換します'''\n\n    def __init__(self, text_embedding_vectors):\n        super(Embedder, self).__init__()\n\n        self.embeddings = nn.Embedding.from_pretrained(\n            embeddings=text_embedding_vectors, freeze=True)\n        # freeze=Trueによりバックプロパゲーションで更新されず変化しなくなります\n\n    def forward(self, x):\n        x_vec = self.embeddings(x)\n\n        return x_vec\n\n\nclass PositionalEncoder(nn.Module):\n    '''入力された単語の位置を示すベクトル情報を付加する'''\n\n    def __init__(self, d_model=300, max_seq_len=256):\n        super().__init__()\n\n        self.d_model = d_model  # 単語ベクトルの次元数\n\n        # 単語の順番（pos）と埋め込みベクトルの次元の位置（i）によって一意に定まる値の表をpeとして作成\n        pe = torch.zeros(max_seq_len, d_model)\n\n        # GPUが使える場合はGPUへ送る\n        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        pe = pe.to(device)\n\n        for pos in range(max_seq_len):\n            for i in range(0, d_model, 2):\n                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n\n                # 誤植_200510 #76\n                # pe[pos, i + 1] = math.cos(pos /\n                #                          (10000 ** ((2 * (i + 1))/d_model)))\n                pe[pos, i + 1] = math.cos(pos /\n                                          (10000 ** ((2 * i)/d_model)))\n\n        # 表peの先頭に、ミニバッチ次元となる次元を足す\n        self.pe = pe.unsqueeze(0)\n\n        # 勾配を計算しないようにする\n        self.pe.requires_grad = False\n\n    def forward(self, x):\n\n        # 入力xとPositonal Encodingを足し算する\n        # xがpeよりも小さいので、大きくする\n        ret = math.sqrt(self.d_model)*x + self.pe\n        return ret\n\n\nclass Attention(nn.Module):\n    '''Transformerは本当はマルチヘッドAttentionですが、\n    分かりやすさを優先しシングルAttentionで実装します'''\n\n    def __init__(self, d_model=300):\n        super().__init__()\n\n        # SAGANでは1dConvを使用したが、今回は全結合層で特徴量を変換する\n        self.q_linear = nn.Linear(d_model, d_model)\n        self.v_linear = nn.Linear(d_model, d_model)\n        self.k_linear = nn.Linear(d_model, d_model)\n\n        # 出力時に使用する全結合層\n        self.out = nn.Linear(d_model, d_model)\n\n        # Attentionの大きさ調整の変数\n        self.d_k = d_model\n\n    def forward(self, q, k, v, mask):\n        # 全結合層で特徴量を変換\n        k = self.k_linear(k)\n        q = self.q_linear(q)\n        v = self.v_linear(v)\n\n        # Attentionの値を計算する\n        # 各値を足し算すると大きくなりすぎるので、root(d_k)で割って調整\n        weights = torch.matmul(q, k.transpose(1, 2)) / math.sqrt(self.d_k)\n\n        # ここでmaskを計算\n        mask = mask.unsqueeze(1)\n        weights = weights.masked_fill(mask == 0, -1e9)\n\n        # softmaxで規格化をする\n        normlized_weights = F.softmax(weights, dim=-1)\n\n        # AttentionをValueとかけ算\n        output = torch.matmul(normlized_weights, v)\n\n        # 全結合層で特徴量を変換\n        output = self.out(output)\n\n        return output, normlized_weights\n\n\nclass FeedForward(nn.Module):\n    def __init__(self, d_model, d_ff=1024, dropout=0.1):\n        '''Attention層から出力を単純に全結合層2つで特徴量を変換するだけのユニットです'''\n        super().__init__()\n\n        self.linear_1 = nn.Linear(d_model, d_ff)\n        self.dropout = nn.Dropout(dropout)\n        self.linear_2 = nn.Linear(d_ff, d_model)\n\n    def forward(self, x):\n        x = self.linear_1(x)\n        x = self.dropout(F.relu(x))\n        x = self.linear_2(x)\n        return x\n\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, d_model, dropout=0.1):\n        super().__init__()\n\n        # LayerNormalization層\n        # https://pytorch.org/docs/stable/nn.html?highlight=layernorm\n        self.norm_1 = nn.LayerNorm(d_model)\n        self.norm_2 = nn.LayerNorm(d_model)\n\n        # Attention層\n        self.attn = Attention(d_model)\n\n        # Attentionのあとの全結合層2つ\n        self.ff = FeedForward(d_model)\n\n        # Dropout\n        self.dropout_1 = nn.Dropout(dropout)\n        self.dropout_2 = nn.Dropout(dropout)\n\n    def forward(self, x, mask):\n        # 正規化とAttention\n        x_normlized = self.norm_1(x)\n        output, normlized_weights = self.attn(\n            x_normlized, x_normlized, x_normlized, mask)\n\n        x2 = x + self.dropout_1(output)\n\n        # 正規化と全結合層\n        x_normlized2 = self.norm_2(x2)\n        output = x2 + self.dropout_2(self.ff(x_normlized2))\n\n        return output, normlized_weights\n\n\nclass ClassificationHead(nn.Module):\n    '''Transformer_Blockの出力を使用し、最後にクラス分類させる'''\n\n    def __init__(self, d_model=300, output_dim=2):\n        super().__init__()\n\n        # 全結合層\n        self.linear = nn.Linear(d_model, output_dim)  # output_dimはポジ・ネガの2つ\n\n        # 重み初期化処理\n        nn.init.normal_(self.linear.weight, std=0.02)\n        nn.init.normal_(self.linear.bias, 0)\n\n    def forward(self, x):\n        x0 = x[:, 0, :]  # 各ミニバッチの各文の先頭の単語の特徴量（300次元）を取り出す\n        out = self.linear(x0)\n\n        return out\n\n\n# 最終的なTransformerモデルのクラス\n\n\nclass TransformerClassification(nn.Module):\n    '''Transformerでクラス分類させる'''\n\n    def __init__(self, text_embedding_vectors, d_model=300, max_seq_len=256, output_dim=2):\n        super().__init__()\n\n        # モデル構築\n        self.net1 = Embedder(text_embedding_vectors)\n        self.net2 = PositionalEncoder(d_model=d_model, max_seq_len=max_seq_len)\n        self.net3_1 = TransformerBlock(d_model=d_model)\n        self.net3_2 = TransformerBlock(d_model=d_model)\n        self.net4 = ClassificationHead(output_dim=output_dim, d_model=d_model)\n\n    def forward(self, x, mask):\n        x1 = self.net1(x)  # 単語をベクトルに\n        x2 = self.net2(x1)  # Positon情報を足し算\n        x3_1, normlized_weights_1 = self.net3_1(\n            x2, mask)  # Self-Attentionで特徴量を変換\n        x3_2, normlized_weights_2 = self.net3_2(\n            x3_1, mask)  # Self-Attentionで特徴量を変換\n        x4 = self.net4(x3_2)  # 最終出力の0単語目を使用して、分類0-1のスカラーを出力\n        return x4, normlized_weights_1, normlized_weights_2","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:16:49.029112Z","iopub.execute_input":"2022-01-08T15:16:49.029668Z","iopub.status.idle":"2022-01-08T15:16:49.153109Z","shell.execute_reply.started":"2022-01-08T15:16:49.029585Z","shell.execute_reply":"2022-01-08T15:16:49.152224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# bertでもtransformerでも同じ\ndef _convert_to_transformer_inputs(title, question, answer, tokenizer, max_sequence_length):\n    \"\"\"Converts tokenized input to ids, masks and segments for transformer (including bert)\"\"\"\n    \n    def return_id(str1, str2, truncation_strategy, length):\n\n        inputs = tokenizer.encode_plus(str1, str2,\n            add_special_tokens=True,\n            max_length=length,\n            truncation_strategy=truncation_strategy)\n        \n        input_ids =  inputs[\"input_ids\"]\n        input_masks = [1] * len(input_ids)\n        input_segments = inputs[\"token_type_ids\"]\n        padding_length = length - len(input_ids)\n        padding_id = tokenizer.pad_token_id\n        input_ids = input_ids + ([padding_id] * padding_length)\n        input_masks = input_masks + ([0] * padding_length)\n        input_segments = input_segments + ([0] * padding_length)\n        \n        return [input_ids, input_masks, input_segments]\n    \n    input_ids_q, input_masks_q, input_segments_q = return_id(\n        title + ' ' + question, None, 'longest_first', max_sequence_length)\n    \n    input_ids_a, input_masks_a, input_segments_a = return_id(\n        answer, None, 'longest_first', max_sequence_length)\n    \n    return [input_ids_q, input_masks_q, input_segments_q,\n            input_ids_a, input_masks_a, input_segments_a]\n\ndef compute_input_arrays(df, columns, tokenizer, max_sequence_length):\n    input_ids_q, input_masks_q, input_segments_q = [], [], []\n    input_ids_a, input_masks_a, input_segments_a = [], [], []\n    for _, instance in tqdm(df[columns].iterrows()):\n        t, q, a = instance.question_title, instance.question_body, instance.answer\n\n        ids_q, masks_q, segments_q, ids_a, masks_a, segments_a = \\\n        _convert_to_transformer_inputs(t, q, a, tokenizer, max_sequence_length)\n        \n        input_ids_q.append(ids_q)\n        input_masks_q.append(masks_q)\n        input_segments_q.append(segments_q)\n\n        input_ids_a.append(ids_a)\n        input_masks_a.append(masks_a)\n        input_segments_a.append(segments_a)\n        \n    return [np.asarray(input_ids_q, dtype=np.int32), \n            np.asarray(input_masks_q, dtype=np.int32), \n            np.asarray(input_segments_q, dtype=np.int32),\n            np.asarray(input_ids_a, dtype=np.int32), \n            np.asarray(input_masks_a, dtype=np.int32), \n            np.asarray(input_segments_a, dtype=np.int32)]\n\ndef compute_output_arrays(df, columns):\n    return np.asarray(df[columns])","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:16:49.154938Z","iopub.execute_input":"2022-01-08T15:16:49.155421Z","iopub.status.idle":"2022-01-08T15:16:49.17244Z","shell.execute_reply.started":"2022-01-08T15:16:49.155364Z","shell.execute_reply":"2022-01-08T15:16:49.171428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 第7章 自然言語処理による感情分析（Transformer）\n# 実装参考に使用\n# https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec\n\n\n# 必要なパッケージのimport\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchtext\n\n\nclass Embedder(nn.Module):\n    '''idで示されている単語をベクトルに変換します'''\n\n    def __init__(self, text_embedding_vectors):\n        super(Embedder, self).__init__()\n\n        self.embeddings = nn.Embedding.from_pretrained(\n            embeddings=text_embedding_vectors, freeze=True)\n        # freeze=Trueによりバックプロパゲーションで更新されず変化しなくなります\n\n    def forward(self, x):\n        x_vec = self.embeddings(x)\n\n        return x_vec\n\n\nclass PositionalEncoder(nn.Module):\n    '''入力された単語の位置を示すベクトル情報を付加する'''\n\n    def __init__(self, d_model=300, max_seq_len=256):\n        super().__init__()\n\n        self.d_model = d_model  # 単語ベクトルの次元数\n\n        # 単語の順番（pos）と埋め込みベクトルの次元の位置（i）によって一意に定まる値の表をpeとして作成\n        pe = torch.zeros(max_seq_len, d_model)\n\n        # GPUが使える場合はGPUへ送る\n        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        pe = pe.to(device)\n\n        for pos in range(max_seq_len):\n            for i in range(0, d_model, 2):\n                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n\n                # 誤植_200510 #76\n                # pe[pos, i + 1] = math.cos(pos /\n                #                          (10000 ** ((2 * (i + 1))/d_model)))\n                pe[pos, i + 1] = math.cos(pos /\n                                          (10000 ** ((2 * i)/d_model)))\n\n        # 表peの先頭に、ミニバッチ次元となる次元を足す\n        self.pe = pe.unsqueeze(0)\n\n        # 勾配を計算しないようにする\n        self.pe.requires_grad = False\n\n    def forward(self, x):\n\n        # 入力xとPositonal Encodingを足し算する\n        # xがpeよりも小さいので、大きくする\n        ret = math.sqrt(self.d_model)*x + self.pe\n        return ret\n\n\nclass Attention(nn.Module):\n    '''Transformerは本当はマルチヘッドAttentionですが、\n    分かりやすさを優先しシングルAttentionで実装します'''\n\n    def __init__(self, d_model=300):\n        super().__init__()\n\n        # SAGANでは1dConvを使用したが、今回は全結合層で特徴量を変換する\n        self.q_linear = nn.Linear(d_model, d_model)\n        self.v_linear = nn.Linear(d_model, d_model)\n        self.k_linear = nn.Linear(d_model, d_model)\n\n        # 出力時に使用する全結合層\n        self.out = nn.Linear(d_model, d_model)\n\n        # Attentionの大きさ調整の変数\n        self.d_k = d_model\n\n    def forward(self, q, k, v, mask):\n        # 全結合層で特徴量を変換\n        k = self.k_linear(k)\n        q = self.q_linear(q)\n        v = self.v_linear(v)\n\n        # Attentionの値を計算する\n        # 各値を足し算すると大きくなりすぎるので、root(d_k)で割って調整\n        weights = torch.matmul(q, k.transpose(1, 2)) / math.sqrt(self.d_k)\n\n        # ここでmaskを計算\n        mask = mask.unsqueeze(1)\n        weights = weights.masked_fill(mask == 0, -1e9)\n\n        # softmaxで規格化をする\n        normlized_weights = F.softmax(weights, dim=-1)\n\n        # AttentionをValueとかけ算\n        output = torch.matmul(normlized_weights, v)\n\n        # 全結合層で特徴量を変換\n        output = self.out(output)\n\n        return output, normlized_weights\n\n\nclass FeedForward(nn.Module):\n    def __init__(self, d_model, d_ff=1024, dropout=0.1):\n        '''Attention層から出力を単純に全結合層2つで特徴量を変換するだけのユニットです'''\n        super().__init__()\n\n        self.linear_1 = nn.Linear(d_model, d_ff)\n        self.dropout = nn.Dropout(dropout)\n        self.linear_2 = nn.Linear(d_ff, d_model)\n\n    def forward(self, x):\n        x = self.linear_1(x)\n        x = self.dropout(F.relu(x))\n        x = self.linear_2(x)\n        return x\n\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, d_model, dropout=0.1):\n        super().__init__()\n\n        # LayerNormalization層\n        # https://pytorch.org/docs/stable/nn.html?highlight=layernorm\n        self.norm_1 = nn.LayerNorm(d_model)\n        self.norm_2 = nn.LayerNorm(d_model)\n\n        # Attention層\n        self.attn = Attention(d_model)\n\n        # Attentionのあとの全結合層2つ\n        self.ff = FeedForward(d_model)\n\n        # Dropout\n        self.dropout_1 = nn.Dropout(dropout)\n        self.dropout_2 = nn.Dropout(dropout)\n\n    def forward(self, x, mask):\n        # 正規化とAttention\n        x_normlized = self.norm_1(x)\n        output, normlized_weights = self.attn(\n            x_normlized, x_normlized, x_normlized, mask)\n\n        x2 = x + self.dropout_1(output)\n\n        # 正規化と全結合層\n        x_normlized2 = self.norm_2(x2)\n        output = x2 + self.dropout_2(self.ff(x_normlized2))\n\n        return output, normlized_weights\n\n\nclass ClassificationHead(nn.Module):\n    '''Transformer_Blockの出力を使用し、最後にクラス分類させる'''\n\n    def __init__(self, d_model=300, output_dim=2):\n        super().__init__()\n\n        # 全結合層\n        self.linear = nn.Linear(d_model, output_dim)  # output_dimはポジ・ネガの2つ\n\n        # 重み初期化処理\n        nn.init.normal_(self.linear.weight, std=0.02)\n        nn.init.normal_(self.linear.bias, 0)\n\n    def forward(self, x):\n        x0 = x[:, 0, :]  # 各ミニバッチの各文の先頭の単語の特徴量（300次元）を取り出す\n        out = self.linear(x0)\n\n        return out\n\n\n# 最終的なTransformerモデルのクラス\n\n\nclass TransformerClassification(nn.Module):\n    '''Transformerでクラス分類させる'''\n\n    def __init__(self, text_embedding_vectors, d_model=300, max_seq_len=256, output_dim=2):\n        super().__init__()\n\n        # モデル構築\n        self.net1 = Embedder(text_embedding_vectors)\n        self.net2 = PositionalEncoder(d_model=d_model, max_seq_len=max_seq_len)\n        self.net3_1 = TransformerBlock(d_model=d_model)\n        self.net3_2 = TransformerBlock(d_model=d_model)\n        self.net4 = ClassificationHead(output_dim=output_dim, d_model=d_model)\n\n    def forward(self, x, mask):\n        x1 = self.net1(x)  # 単語をベクトルに\n        x2 = self.net2(x1)  # Positon情報を足し算\n        x3_1, normlized_weights_1 = self.net3_1(\n            x2, mask)  # Self-Attentionで特徴量を変換\n        x3_2, normlized_weights_2 = self.net3_2(\n            x3_1, mask)  # Self-Attentionで特徴量を変換\n        x4 = self.net4(x3_2)  # 最終出力の0単語目を使用して、分類0-1のスカラーを出力\n        return x4, normlized_weights_1, normlized_weights_2\n","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:56:46.443886Z","iopub.execute_input":"2022-01-08T15:56:46.444192Z","iopub.status.idle":"2022-01-08T15:56:46.479899Z","shell.execute_reply.started":"2022-01-08T15:56:46.444162Z","shell.execute_reply":"2022-01-08T15:56:46.478862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3. Create model\n\n`compute_spearmanr()` is used to compute the competition metric for the validation set\n<br><br>\n`create_model()` contains the actual architecture that will be used to finetune BERT to our dataset.\n","metadata":{}},{"cell_type":"code","source":"# 読み込み\ntrain_dl, val_dl, test_dl, TEXT = get_IMDb_DataLoaders_and_TEXT(\n    max_length=256, batch_size=64)\n\n# 辞書オブジェクトにまとめる\ndataloaders_dict = {\"train\": train_dl, \"val\": val_dl}","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:57:36.635306Z","iopub.execute_input":"2022-01-08T15:57:36.6356Z","iopub.status.idle":"2022-01-08T15:57:36.685865Z","shell.execute_reply.started":"2022-01-08T15:57:36.635559Z","shell.execute_reply":"2022-01-08T15:57:36.684555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# モデル構築\nnet = TransformerClassification(\n    text_embedding_vectors=TEXT.vocab.vectors, d_model=300, max_seq_len=256, output_dim=2)\n\n# ネットワークの初期化を定義\n\n\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Linear') != -1:\n        # Liner層の初期化\n        nn.init.kaiming_normal_(m.weight)\n        if m.bias is not None:\n            nn.init.constant_(m.bias, 0.0)\n\n\n# 訓練モードに設定\nnet.train()\n\n# TransformerBlockモジュールを初期化実行\nnet.net3_1.apply(weights_init)\nnet.net3_2.apply(weights_init)\n\n\nprint('ネットワーク設定完了')","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:56:59.624551Z","iopub.execute_input":"2022-01-08T15:56:59.625165Z","iopub.status.idle":"2022-01-08T15:56:59.658978Z","shell.execute_reply.started":"2022-01-08T15:56:59.625133Z","shell.execute_reply":"2022-01-08T15:56:59.657617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 損失関数の設定\ncriterion = nn.CrossEntropyLoss()\n# nn.LogSoftmax()を計算してからnn.NLLLoss(negative log likelihood loss)を計算\n\n# 最適化手法の設定\nlearning_rate = 2e-5\noptimizer = optim.Adam(net.parameters(), lr=learning_rate)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:16:49.647121Z","iopub.status.idle":"2022-01-08T15:16:49.647627Z","shell.execute_reply.started":"2022-01-08T15:16:49.64734Z","shell.execute_reply":"2022-01-08T15:16:49.647368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_spearmanr_ignore_nan(trues, preds):\n    rhos = []\n    for tcol, pcol in zip(np.transpose(trues), np.transpose(preds)):\n        rhos.append(spearmanr(tcol, pcol).correlation)\n    return np.nanmean(rhos)\n\ndef create_model():\n    \"\"\"\n    q_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    a_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    \n    q_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    a_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    \n    q_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    a_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    \n    config = BertConfig() # print(config) to see settings\n    config.output_hidden_states = False # Set to True to obtain hidden states\n    # caution: when using e.g. XLNet, XLNetConfig() will automatically use xlnet-large config\n    \n    # normally \".from_pretrained('bert-base-uncased')\", but because of no internet, the \n    # pretrained model has been downloaded manually and uploaded to kaggle. \n    bert_model = TFBertModel.from_pretrained(\n        BERT_PATH+'bert-base-uncased-tf_model.h5', config=config)\n    \n    # if config.output_hidden_states = True, obtain hidden states via bert_model(...)[-1]\n    q_embedding = bert_model(q_id, attention_mask=q_mask, token_type_ids=q_atn)[0]\n    a_embedding = bert_model(a_id, attention_mask=a_mask, token_type_ids=a_atn)[0]\n    \n    q = tf.keras.layers.GlobalAveragePooling1D()(q_embedding)\n    a = tf.keras.layers.GlobalAveragePooling1D()(a_embedding)\n    \n    x = tf.keras.layers.Concatenate()([q, a])\n    \n    x = tf.keras.layers.Dropout(0.2)(x)\n    \n    x = tf.keras.layers.Dense(30, activation='sigmoid')(x)\n\n    model = tf.keras.models.Model(inputs=[q_id, q_mask, q_atn, a_id, a_mask, a_atn,], outputs=x)\n    \"\"\"\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:16:49.649512Z","iopub.status.idle":"2022-01-08T15:16:49.650105Z","shell.execute_reply.started":"2022-01-08T15:16:49.649819Z","shell.execute_reply":"2022-01-08T15:16:49.64985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 4. Obtain inputs and targets, as well as the indices of the train/validation splits","metadata":{}},{"cell_type":"code","source":"outputs = compute_output_arrays(df_train, output_categories)\ninputs = compute_input_arrays(df_train, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)\ntest_inputs = compute_input_arrays(df_test, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:16:49.651595Z","iopub.status.idle":"2022-01-08T15:16:49.652604Z","shell.execute_reply.started":"2022-01-08T15:16:49.652264Z","shell.execute_reply":"2022-01-08T15:16:49.652297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 5. Training, validation and testing\n\nLoops over the folds in gkf and trains each fold for 3 epochs --- with a learning rate of 3e-5 and batch_size of 6. A simple binary crossentropy is used as the objective-/loss-function. ","metadata":{}},{"cell_type":"code","source":"gkf = GroupKFold(n_splits=5).split(X=df_train.question_body, groups=df_train.question_body)\n\nvalid_preds = []\ntest_preds = []\nfor fold, (train_idx, valid_idx) in enumerate(gkf):\n    \n    # will actually only do 2 folds (out of 5) to manage < 2h\n    if fold in [0, 2]:\n\n        train_inputs = [inputs[i][train_idx] for i in range(len(inputs))]\n        train_outputs = outputs[train_idx]\n\n        valid_inputs = [inputs[i][valid_idx] for i in range(len(inputs))]\n        valid_outputs = outputs[valid_idx]\n        \n        K.clear_session()\n        model = create_model()\n        optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n        model.compile(loss='binary_crossentropy', optimizer=optimizer)\n        model.fit(train_inputs, train_outputs, epochs=3, batch_size=6)\n        # model.save_weights(f'bert-{fold}.h5')\n        valid_preds.append(model.predict(valid_inputs))\n        test_preds.append(model.predict(test_inputs))\n        \n        rho_val = compute_spearmanr_ignore_nan(valid_outputs, valid_preds[-1])\n        print('validation score = ', rho_val)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:16:49.654118Z","iopub.status.idle":"2022-01-08T15:16:49.655053Z","shell.execute_reply.started":"2022-01-08T15:16:49.654699Z","shell.execute_reply":"2022-01-08T15:16:49.654732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 6. Process and submit test predictions\n\nAverage fold predictions, then save as `submission.csv`","metadata":{}},{"cell_type":"code","source":"df_sub.iloc[:, 1:] = np.average(test_preds, axis=0) # for weighted average set weights=[...]\n\ndf_sub.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:16:49.657155Z","iopub.status.idle":"2022-01-08T15:16:49.657737Z","shell.execute_reply.started":"2022-01-08T15:16:49.657398Z","shell.execute_reply":"2022-01-08T15:16:49.657429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}