{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport json\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.models import Sequential, load_model,Model\nfrom keras.layers import Dense, LSTM, Dropout, Embedding, Input, Bidirectional\nfrom keras.layers import SimpleRNNCell, Concatenate, Add, RNN, BatchNormalization\nfrom keras.preprocessing.sequence import pad_sequences\nimport numpy as np\nfrom scipy.stats import spearmanr\nfrom keras.initializers import RandomNormal, RandomUniform\nfrom keras import regularizers\nimport time\nfrom pathlib import Path\nfrom tqdm import tqdm\n\ndef clean_texts(text):\n    \n    puncts = '.,?/\\\\!-:+#@()*%$\\'\"'\n    \n    stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you',\n                 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', \n                 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself',\n                 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which',\n                 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', \n                 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had',\n                 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and',\n                 'or', 'as', 'while', 'of', 'at', 'by', 'for', 'with', 'about',\n                 'into', 'through', 'above', 'to', 'from', 'up', 'in', 'out',\n                 'on', 'off', 'under', 'again', 'further', 'then', 'once', 'here',\n                 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both',\n                 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'only',\n                 'own', 'so', 'than', 'too', 'very', 'will', 'just', 'should']\n    \n    for word in stopwords:\n        if word in text:\n            text = text.replace(word, '')\n    for punct in puncts:\n        if punct in text:\n            text = text.replace(punct, '')\n    \n    return text\n\n\n\ndef cat_to_numeric(category):\n    if category=='LIFE_ARTS':\n        return 1\n    if category=='CULTURE':\n        return 2\n    if category=='SCIENCE':\n        return 3\n    if category=='STACKOVERFLOW':\n        return 4\n    if category=='TECHNOLOGY':\n        return 5\n\ndef prepare_data(frame):\n    \n    to_drop = []\n    for col in frame.columns:\n        if 'user_page' in col or 'host' in col or\\\n        'url' in col or 'user_name' in col or 'categ' in col:\n            to_drop.append(col)\n\n    data = frame.drop(to_drop, axis=1)\n    return data\n\n\ndef get_vars_and_targets(train_data, test_data):\n    \n    train_cols = test_data.columns\n    target_cols = set(train_data.columns).difference(set(test_data.columns))\n    train_cols = set(train_data.columns) - target_cols\n    target_cols = list(target_cols)\n    train_cols = list(train_cols)\n    return train_cols, target_cols\n\ndef get_text_cols(frame):\n    \n    text_cols = []\n    for col in frame.columns:\n        if 'title' in col or 'body' in col or col=='answer':\n            text_cols.append(col)\n            \n    return text_cols\n\n\ndef transform_texts(frame, tokenizer=None, test=False):\n    \n    text_cols = get_text_cols(frame)\n    for ind in frame.index:\n        for col in text_cols:\n            frame.loc[ind, col] = clean_texts(str(frame.loc[ind, col]))\n    if test==False:\n        tokenizer = Tokenizer(oov_token='OOV')\n        for col in text_cols:\n            tokenizer.fit_on_texts(frame[col])\n    else:\n        tokenizer=tokenizer\n    renamed_cols = []\n    for col in text_cols:\n        renamed_cols.append(col+'_tokenized')\n        frame[col+'_tokenized'] = tokenizer.texts_to_sequences(frame[col])\n        frame.drop([col], inplace=True, axis=1)\n    return [frame, renamed_cols, tokenizer]\n\n\n\n\nimport os\nimport json\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.models import Sequential, load_model,Model\nfrom keras.layers import Dense, LSTM, Dropout, Embedding, Input, Bidirectional\nfrom keras.layers import SimpleRNNCell, Concatenate, Add, RNN, BatchNormalization\nfrom keras.preprocessing.sequence import pad_sequences\nimport numpy as np\nfrom scipy.stats import spearmanr\nfrom keras.initializers import RandomNormal, RandomUniform\nfrom keras import regularizers\nimport time\nfrom pathlib import Path\nfrom tqdm import tqdm\n\ndef clean_texts(text):\n    \n    puncts = '.,?/\\\\!-:+#@()*%$\\'\"'\n    \n    stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you',\n                 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', \n                 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself',\n                 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which',\n                 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', \n                 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had',\n                 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and',\n                 'or', 'as', 'while', 'of', 'at', 'by', 'for', 'with', 'about',\n                 'into', 'through', 'above', 'to', 'from', 'up', 'in', 'out',\n                 'on', 'off', 'under', 'again', 'further', 'then', 'once', 'here',\n                 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both',\n                 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'only',\n                 'own', 'so', 'than', 'too', 'very', 'will', 'just', 'should']\n    \n    for word in stopwords:\n        if word in text:\n            text = text.replace(word, '')\n    for punct in puncts:\n        if punct in text:\n            text = text.replace(punct, '')\n    \n    return text\n\n\n\ndef cat_to_numeric(category):\n    if category=='LIFE_ARTS':\n        return 1\n    if category=='CULTURE':\n        return 2\n    if category=='SCIENCE':\n        return 3\n    if category=='STACKOVERFLOW':\n        return 4\n    if category=='TECHNOLOGY':\n        return 5\n\ndef prepare_data(frame):\n    \n    to_drop = []\n    for col in frame.columns:\n        if 'user_page' in col or 'host' in col or\\\n        'url' in col or 'user_name' in col or 'categ' in col:\n            to_drop.append(col)\n\n    data = frame.drop(to_drop, axis=1)\n    return data\n\n\ndef get_vars_and_targets(train_data, test_data):\n    \n    train_cols = test_data.columns\n    target_cols = set(train_data.columns).difference(set(test_data.columns))\n    train_cols = set(train_data.columns) - target_cols\n    target_cols = list(target_cols)\n    train_cols = list(train_cols)\n    return train_cols, target_cols\n\ndef get_text_cols(frame):\n    \n    text_cols = []\n    for col in frame.columns:\n        if 'title' in col or 'body' in col or col=='answer':\n            text_cols.append(col)\n            \n    return text_cols\n\n\ndef transform_texts(frame, tokenizer=None, test=False):\n    \n    text_cols = get_text_cols(frame)\n    for ind in frame.index:\n        for col in text_cols:\n            frame.loc[ind, col] = clean_texts(str(frame.loc[ind, col]))\n    if test==False:\n        tokenizer = Tokenizer(oov_token='OOV')\n        for col in text_cols:\n            tokenizer.fit_on_texts(frame[col])\n    else:\n        tokenizer=tokenizer\n    renamed_cols = []\n    for col in text_cols:\n        renamed_cols.append(col+'_tokenized')\n        frame[col+'_tokenized'] = tokenizer.texts_to_sequences(frame[col])\n        frame.drop([col], inplace=True, axis=1)\n    return [frame, renamed_cols, tokenizer]\n\n\ndef model_6_lstm(X_train_padded, tokenizer, maxlen=750):\n    \n    initializer = RandomUniform(seed=69)\n    #j = 0\n    embs = []\n    inputs = []\n    for each in X_train_padded:\n        input_layer = Input(shape=(maxlen, ))\n        emb_layer = Embedding(len(tokenizer.word_index)+1, output_dim=128)(input_layer)\n        embs.append(emb_layer)\n        inputs.append(input_layer)\n    concat = Concatenate(axis=1)(embs)\n    lstm_layer_1 = Bidirectional(LSTM(128, return_sequences=True, dropout=0.2))(concat)\n    lstm_layer_2 = Bidirectional(LSTM(128, dropout=0.1))(lstm_layer_1)\n    norm_layer = BatchNormalization()(lstm_layer_2)\n    dense_layer = Dense(256, activation='relu')(norm_layer)\n    \n    droput = Dropout(0.2)(dense_layer)\n    dense_1 = Dense(128, activation='relu')(droput)\n    dense_2 = Dense(64, activation='relu')(dense_1)\n    output = Dense(30)(dense_2)\n\n    model = Model(inputs=inputs, outputs=[output])\n    model.compile(optimizer='Adam', loss='binary_crossentropy', metrics=['accuracy'])\n    #print(model.summary())\n\n    return model    \n\ndef spears_score(y_test, predictions):\n    rhos = []\n    for tcol, pcol in zip(np.transpose(y_test), np.transpose(predictions)):\n        rhos.append(spearmanr(tcol, pcol).correlation)\n    return np.nanmean(rhos)\n\n\ndef main():\n    \n    train_data = pd.read_csv('../input/google-quest-challenge/train.csv')\n    test_data = pd.read_csv('../input/google-quest-challenge/test.csv')\n    train_data = prepare_data(train_data)\n    test_data = prepare_data(test_data)\n    train_cols, _ = get_vars_and_targets(train_data, test_data)\n    submission_data = pd.read_csv('../input/google-quest-challenge/sample_submission.csv', encoding='utf-8')\n    target_cols = list(submission_data.columns[1:].values)\n    \n    X_train, X_test, y_train, y_test = train_test_split(train_data.loc[:, train_cols], train_data.loc[:, target_cols], test_size=0.25)\n    X_train, text_cols, tokenizer = transform_texts(X_train.drop('qa_id', axis=1), test=False)\n    X_test, text_cols, _ = transform_texts(X_test.drop('qa_id', axis=1), tokenizer, test=True)\n    print(X_train.columns, X_test.columns)\n    #scaler = MinMaxScaler()\n    #scaler.fit(y_train.values)\n    #y_train, y_test = scaler.transform(y_train), scaler.transform(y_test.values)\n    X_train_features = []\n    X_test_features = []\n    for col in text_cols:\n        X_train_features.append('X_train_'+(('_').join(col.split('_')[:-1])))\n        X_test_features.append('X_test_'+(('_').join(col.split('_')[:-1])))\n    X_train_padded = []\n    X_test_padded = []\n    i = 0\n    for each in X_train_features:\n        X_train_padded.append(pad_sequences(X_train[text_cols[i]], maxlen=750, padding='pre'))\n        X_test_padded.append(pad_sequences(X_test[text_cols[i]], maxlen=750, padding='pre'))\n        i += 1\n    model = model_6_lstm(X_train_padded, tokenizer)\n    model.fit(X_train_padded, y_train.values, batch_size=16, epochs=2, validation_split=0.2)\n    print(model.evaluate(X_test_padded, y_test.values))\n    print('Spearmen score:', spears_score(y_test.values, model.predict(X_test_padded)))\n\n    test_data, text_cols, _ = transform_texts(test_data.drop(['qa_id'], axis=1), tokenizer=tokenizer, test=True)\n    test_features = []\n    for col in text_cols:\n        test_features.append(col)\n        test_padded = []\n    j = 0\n    for each in test_features:\n        test_padded.append(pad_sequences(test_data[each], maxlen=750, padding='pre'))\n        j += 1\n        \n    predictions = model.predict(test_padded)\n    print(predictions)\n    prediction = np.where(predictions<1, predictions, 0.99)\n        \n    submission_data[target_cols] = np.absolute(predictions)\n\n    print(submission_data.head())\n    submission_data.to_csv('submission.csv', index=False)\n    \n    \n\n    \nmain()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}