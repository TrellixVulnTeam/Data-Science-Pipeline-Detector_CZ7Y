{"cells":[{"metadata":{},"cell_type":"markdown","source":"# First of all, let me make things clear."},{"metadata":{},"cell_type":"markdown","source":"KAGGLE IS A GOOSE, NOT A CHICKEN!!"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from transformers import *","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nbert_model = TFBertForMaskedLM.from_pretrained('bert-base-uncased') \n\n# DEFINE SENTENCE\nstr = '[CLS] Data science is [MASK] . [SEP]'\nindices = tokenizer.encode(str, add_special_tokens=False, return_tensors='tf')\n\n# PREDICT MISSING WORDS\npred = bert_model(indices)\nmasked_indices = np.where(indices==103)[1]\n\n# DISPLAY MISSING WORDS\npredicted_words = np.argmax( np.asarray(pred[0][0])[masked_indices,:] ,axis=1)\nprint( tokenizer.decode(predicted_words) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"str = '[CLS] BERT is the [MASK] . [SEP]'\nindices = tokenizer.encode(str, add_special_tokens=False, return_tensors='tf')\n\n# PREDICT MISSING WORDS\npred = bert_model(indices)\nmasked_indices = np.where(indices==103)[1]\n\n# DISPLAY MISSING WORDS\npredicted_words = np.argmax( np.asarray(pred[0][0])[masked_indices,:] ,axis=1)\nprint( tokenizer.decode(predicted_words) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"str = '[CLS] Apple really [MASK] good those [MASK] . [SEP]'\nindices = tokenizer.encode(str, add_special_tokens=False, return_tensors='tf')\n\n# PREDICT MISSING WORDS\npred = bert_model(indices)\nmasked_indices = np.where(indices==103)[1]\n\n# DISPLAY MISSING WORDS\npredicted_words = np.argmax( np.asarray(pred[0][0])[masked_indices,:] ,axis=1)\nprint( tokenizer.decode(predicted_words) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"str = '[CLS] Kaggle is definitely not [MASK] . [SEP]'\nindices = tokenizer.encode(str, add_special_tokens=False, return_tensors='tf')\n\n# PREDICT MISSING WORDS\npred = bert_model(indices)\nmasked_indices = np.where(indices==103)[1]\n\n# DISPLAY MISSING WORDS\npredicted_words = np.argmax( np.asarray(pred[0][0])[masked_indices,:] ,axis=1)\nprint( tokenizer.decode(predicted_words) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"str = '[CLS] Google will dominate the [MASK] . [SEP]'\nindices = tokenizer.encode(str, add_special_tokens=False, return_tensors='tf')\n\n# PREDICT MISSING WORDS\npred = bert_model(indices)\nmasked_indices = np.where(indices==103)[1]\n\n# DISPLAY MISSING WORDS\npredicted_words = np.argmax( np.asarray(pred[0][0])[masked_indices,:] ,axis=1)\nprint( tokenizer.decode(predicted_words) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"str = '[CLS] Microsoft will languish in the [MASK] . [SEP]'\nindices = tokenizer.encode(str, add_special_tokens=False, return_tensors='tf')\n\n# PREDICT MISSING WORDS\npred = bert_model(indices)\nmasked_indices = np.where(indices==103)[1]\n\n# DISPLAY MISSING WORDS\npredicted_words = np.argmax( np.asarray(pred[0][0])[masked_indices,:] ,axis=1)\nprint( tokenizer.decode(predicted_words) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"str = '[CLS] The king of NLP will always [MASK] [MASK] . [SEP]'\nindices = tokenizer.encode(str, add_special_tokens=False, return_tensors='tf')\n\n# PREDICT MISSING WORDS\npred = bert_model(indices)\nmasked_indices = np.where(indices==103)[1]\n\n# DISPLAY MISSING WORDS\npredicted_words = np.argmax( np.asarray(pred[0][0])[masked_indices,:] ,axis=1)\nprint( tokenizer.decode(predicted_words) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"str = '[CLS] Kaggle is [MASK] . [SEP]'\nindices = tokenizer.encode(str, add_special_tokens=False, return_tensors='tf')\n\n# PREDICT MISSING WORDS\npred = bert_model(indices)\nmasked_indices = np.where(indices==103)[1]\n\n# DISPLAY MISSING WORDS\npredicted_words = np.argmax( np.asarray(pred[0][0])[masked_indices,:] ,axis=1)\nprint( tokenizer.decode(predicted_words) )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If BERT gets this one I'll give it a nice Coke."},{"metadata":{"trusted":true},"cell_type":"code","source":"str = '[CLS] @philippsinger is a [MASK] grandmaster on Kaggle . [SEP]'\nindices = tokenizer.encode(str, add_special_tokens=False, return_tensors='tf')\n\n# PREDICT MISSING WORDS\npred = bert_model(indices)\nmasked_indices = np.where(indices==103)[1]\n\n# DISPLAY MISSING WORDS\npredicted_words = np.argmax( np.asarray(pred[0][0])[masked_indices,:] ,axis=1)\nprint( tokenizer.decode(predicted_words) )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Credits\n\n* @cdeotte for providing the base with his wonderful discussion post."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}