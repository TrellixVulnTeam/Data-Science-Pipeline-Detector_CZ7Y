{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Overview\n\nThe purpose of this kernel is to take a look at the data, come up with some insights, and attempt to create a predictive model or two. This notebook is still **very** raw. I will work on it as my very limited time permits, and hope to expend it in the upcoming days and weeks.\n\n## Packages\n\nFirst, let's load a few useful Python packages. This section will keep growing in subsequent versions of this EDA."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport time\nfrom tqdm import tqdm\n\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\nfrom sklearn.linear_model import LogisticRegression, LinearRegression, Ridge\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom scipy.sparse import hstack\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import roc_auc_score, accuracy_score, log_loss\nfrom tqdm import tqdm_notebook, tqdm\nfrom scipy import stats\n\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nimport gc\n\nfrom scipy.sparse import hstack\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is a kernels-only competition, which means that we can only use tools and data that are available to us in a single Kaggle kernel. The Pyhon libraries that are available to us are the standard Kaggle kernels compute environment. So let's take a look at the data that's available to us: "},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\nprint(os.listdir(\"../input/google-quest-challenge\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's now load the datasets."},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/google-quest-challenge/train.csv').fillna(' ')\ntest = pd.read_csv('../input/google-quest-challenge/test.csv').fillna(' ')\nsample_submission = pd.read_csv('../input/google-quest-challenge/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The metric for this competitiomn is Spearman Correlation, and we will define it here for later use:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def spearman_corr(y_true, y_pred):\n        if np.ndim(y_pred) == 2:\n            corr = np.mean([stats.spearmanr(y_true[:, i], y_pred[:, i])[0] for i in range(y_true.shape[1])])\n        else:\n            corr = stats.spearmanr(y_true, y_pred)[0]\n        return corr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is a **very** small datset, especially for NLP competitions. Furthermore, we ahve 30 different target variables. It's very likely that many of those vriables will be hard to model with any high degree of accuracy.\n\nLet's not take a look at what kind of target valiables we have:"},{"metadata":{"trusted":true},"cell_type":"code","source":"targets = list(sample_submission.columns[1:])\ntargets","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most of these targets are fairly self-explanatory. let's look at tehir distributions in the train dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"train[targets].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that all targets are have values between 0 and 1. Other than that, their distributions vary - dramatically. However, there is only a handful of discrete valuas that each one of the target variables seems to attain. Let's take a closer look."},{"metadata":{"trusted":true},"cell_type":"code","source":"np.unique(train[targets].values, return_counts=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.unique(train[targets].values).shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So there are really only 25 discrete values that we have to deal with."},{"metadata":{},"cell_type":"markdown","source":"Let's take a look at some of these "},{"metadata":{"trusted":true},"cell_type":"code","source":"x= np.unique(train['question_asker_intent_understanding'].values, return_counts=True)[0]\ny= np.unique(train['question_asker_intent_understanding'].values, return_counts=True)[1]\nplt.bar(x, y, align='center', width=0.05)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x= np.unique(train['question_body_critical'].values, return_counts=True)[0]\ny= np.unique(train['question_body_critical'].values, return_counts=True)[1]\nplt.bar(x, y, align='center', width=0.05)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x= np.unique(train['question_not_really_a_question'].values, return_counts=True)[0]\ny= np.unique(train['question_not_really_a_question'].values, return_counts=True)[1]\nplt.bar(x, y, align='center', width=0.05)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x= np.unique(train['question_conversational'].values, return_counts=True)[0]\ny= np.unique(train['question_conversational'].values, return_counts=True)[1]\nplt.bar(x, y, align='center', width=0.05)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The metric for this competition is Spearman Correlation. It would be interesting to see how correlated various target columns are."},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = train[targets].corr()\ncorr.style.background_gradient(cmap='coolwarm')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So that is very interesting. Some of the targets are **extremely** correlated, such as 'question_type_instructions' and 'answer_type_instructions'. In that case this seems quite intutively obvious. "},{"metadata":{"trusted":true},"cell_type":"markdown","source":"For EDA and later modeling, it might be a good idea to create some metafeatures. This work is partly based on SRK's great EDAs, and [this one](http://www.kaggle.com/sudalairajkumar/simple-feature-engg-notebook-spooky-author) in particular. The metafeatures that we'll create are:\n\n\n* Number of words in the question_title\n* Number of words in the question_body\n* Number of words in the answer\n\n* Number of unique words in the question_title\n* Number of unique words in the question_body\n* Number of unique words in the answer\n\n* Number of characters in the question_title\n* Number of characters in the question_body\n* Number of characters in the answer\n\n* Number of stopwords in question_title\n* Number of stopwords in question_body\n* Number of stopwords in answer\n\n* Number of punctuations in question_title\n* Number of punctuations in question_body\n* Number of punctuations in answer\n\n* Number of upper case words in question_title\n* Number of upper case words in question_body\n* Number of upper case words in answer"},{"metadata":{"trusted":true},"cell_type":"code","source":"eng_stopwords = set(stopwords.words(\"english\"))\n\n\n## Number of words in the text ##\ntrain[\"question_title_num_words\"] = train[\"question_title\"].apply(lambda x: len(str(x).split()))\ntest[\"question_title_num_words\"] = test[\"question_title\"].apply(lambda x: len(str(x).split()))\ntrain[\"question_body_num_words\"] = train[\"question_body\"].apply(lambda x: len(str(x).split()))\ntest[\"question_body_num_words\"] = test[\"question_body\"].apply(lambda x: len(str(x).split()))\ntrain[\"answer_num_words\"] = train[\"answer\"].apply(lambda x: len(str(x).split()))\ntest[\"answer_num_words\"] = test[\"answer\"].apply(lambda x: len(str(x).split()))\n\n\n## Number of unique words in the text ##\ntrain[\"question_title_num_unique_words\"] = train[\"question_title\"].apply(lambda x: len(set(str(x).split())))\ntest[\"question_title_num_unique_words\"] = test[\"question_title\"].apply(lambda x: len(set(str(x).split())))\ntrain[\"question_body_num_unique_words\"] = train[\"question_body\"].apply(lambda x: len(set(str(x).split())))\ntest[\"question_body_num_unique_words\"] = test[\"question_body\"].apply(lambda x: len(set(str(x).split())))\ntrain[\"answer_num_unique_words\"] = train[\"answer\"].apply(lambda x: len(set(str(x).split())))\ntest[\"answer_num_unique_words\"] = test[\"answer\"].apply(lambda x: len(set(str(x).split())))\n\n## Number of characters in the text ##\ntrain[\"question_title_num_chars\"] = train[\"question_title\"].apply(lambda x: len(str(x)))\ntest[\"question_title_num_chars\"] = test[\"question_title\"].apply(lambda x: len(str(x)))\ntrain[\"question_body_num_chars\"] = train[\"question_body\"].apply(lambda x: len(str(x)))\ntest[\"question_body_num_chars\"] = test[\"question_body\"].apply(lambda x: len(str(x)))\ntrain[\"answer_num_chars\"] = train[\"answer\"].apply(lambda x: len(str(x)))\ntest[\"answer_num_chars\"] = test[\"answer\"].apply(lambda x: len(str(x)))\n\n## Number of stopwords in the text ##\ntrain[\"question_title_num_stopwords\"] = train[\"question_title\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\ntest[\"question_title_num_stopwords\"] = test[\"question_title\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\ntrain[\"question_body_num_stopwords\"] = train[\"question_body\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\ntest[\"question_body_num_stopwords\"] = test[\"question_body\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\ntrain[\"answer_num_stopwords\"] = train[\"answer\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\ntest[\"answer_num_stopwords\"] = test[\"answer\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n\n## Number of punctuations in the text ##\ntrain[\"question_title_num_punctuations\"] =train['question_title'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\ntest[\"question_title_num_punctuations\"] =test['question_title'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\ntrain[\"question_body_num_punctuations\"] =train['question_body'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\ntest[\"question_body_num_punctuations\"] =test['question_body'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\ntrain[\"answer_num_punctuations\"] =train['answer'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\ntest[\"answer_num_punctuations\"] =test['answer'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n\n## Number of title case words in the text ##\ntrain[\"question_title_num_words_upper\"] = train[\"question_title\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\ntest[\"question_title_num_words_upper\"] = test[\"question_title\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\ntrain[\"question_body_num_words_upper\"] = train[\"question_body\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\ntest[\"question_body_num_words_upper\"] = test[\"question_body\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\ntrain[\"answer_num_words_upper\"] = train[\"answer\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\ntest[\"answer_num_words_upper\"] = test[\"answer\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = ['question_title_num_words', 'question_body_num_words', 'answer_num_words', 'question_title_num_unique_words', 'question_body_num_unique_words', 'answer_num_unique_words',\n           'question_title_num_chars', 'question_body_num_chars', 'answer_num_chars', 'question_title_num_stopwords', 'question_body_num_stopwords', 'question_title_num_punctuations',\n           'question_body_num_punctuations', 'answer_num_punctuations', 'question_title_num_words_upper', 'question_body_num_words_upper', 'answer_num_words_upper']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.violinplot(data=train['question_body_num_words'])\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.violinplot(data=train['question_body_num_chars'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nplt.figure(figsize=(12,8))\nsns.violinplot(data=train['answer_num_chars'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = train[features].values\nX_test = test[features].values\nclass_names_2 = [class_name+'_2' for class_name in targets]\nfor class_name in targets:\n    train[class_name+'_2'] = (train[class_name].values >= 0.5)*1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nsubmission_1 = pd.DataFrame.from_dict({'qa_id': test['qa_id']})\n\nscores = []\nspearman_scores = []\n\nfor class_name in tqdm_notebook(targets):\n    print(class_name)\n    Y = train[class_name]\n    \n    n_splits = 3\n    kf = KFold(n_splits=n_splits, random_state=47)\n\n    train_oof = np.zeros((X_train.shape[0], ))\n    test_preds = 0\n    \n    score = 0\n\n    for jj, (train_index, val_index) in enumerate(kf.split(X_train)):\n        #print(\"Fitting fold\", jj+1)\n        train_features = X_train[train_index]\n        train_target = Y[train_index]\n\n        val_features = X_train[val_index]\n        val_target = Y[val_index]\n\n        model = Ridge()\n        model.fit(train_features, train_target)\n        val_pred = model.predict(val_features)\n        train_oof[val_index] = val_pred\n        #print(\"Fold auc:\", roc_auc_score(val_target, val_pred))\n        #score += roc_auc_score(val_target, val_pred)/n_splits\n\n        test_preds += model.predict(X_test)/n_splits\n        del train_features, train_target, val_features, val_target\n        gc.collect()\n        \n    model = Ridge()\n    model.fit(X_train, Y)\n    \n    preds = model.predict(X_test)\n    mms = MinMaxScaler(copy=True, feature_range=(0, 1))\n    preds = mms.fit_transform(preds.reshape(-1, 1)).flatten()\n    submission_1[class_name] = (preds+0.00005)/1.0001\n        \n    score = roc_auc_score(train[class_name+'_2'], train_oof) \n    \n    \n    spearman_score = spearman_corr(train[class_name], train_oof)\n    print(\"spearman_corr:\", spearman_score)\n    print(\"auc:\", score, \"\\n\")\n    spearman_scores.append(spearman_score)\n    \n    scores.append(score)\n    \nprint(\"Mean auc:\", np.mean(scores))\nprint(\"Mean spearman_scores\", np.mean(spearman_scores))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"HistGradientBoostingRegressor()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nsubmission_2 = pd.DataFrame.from_dict({'qa_id': test['qa_id']})\n\nscores = []\nspearman_scores = []\n\nfor class_name in tqdm_notebook(targets):\n    print(class_name)\n    Y = train[class_name]\n    \n    n_splits = 3\n    kf = KFold(n_splits=n_splits, random_state=47)\n\n    train_oof = np.zeros((X_train.shape[0], ))\n    test_preds = 0\n    \n    score = 0\n\n    for jj, (train_index, val_index) in enumerate(kf.split(X_train)):\n        #print(\"Fitting fold\", jj+1)\n        train_features = X_train[train_index]\n        train_target = Y[train_index]\n\n        val_features = X_train[val_index]\n        val_target = Y[val_index]\n\n        model = HistGradientBoostingRegressor(max_depth=5)\n        model.fit(train_features, train_target)\n        val_pred = model.predict(val_features)\n        train_oof[val_index] = val_pred\n        #print(\"Fold auc:\", roc_auc_score(val_target, val_pred))\n        #score += roc_auc_score(val_target, val_pred)/n_splits\n\n        test_preds += model.predict(X_test)/n_splits\n        del train_features, train_target, val_features, val_target\n        gc.collect()\n        \n    model = HistGradientBoostingRegressor(max_depth=5)\n    model.fit(X_train, Y)\n    \n    preds = model.predict(X_test)\n    mms = MinMaxScaler(copy=True, feature_range=(0, 1))\n    preds = mms.fit_transform(preds.reshape(-1, 1)).flatten()\n    submission_2[class_name] = (preds+0.00005)/1.0001\n        \n    score = roc_auc_score(train[class_name+'_2'], train_oof) \n    \n    \n    spearman_score = spearman_corr(train[class_name], train_oof)\n    print(\"spearman_corr:\", spearman_score)\n    print(\"auc:\", score, \"\\n\")\n    spearman_scores.append(spearman_score)\n    \n    scores.append(score)\n    \nprint(\"Mean auc:\", np.mean(scores))\nprint(\"Mean spearman_scores\", np.mean(spearman_scores))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = submission_1.copy()\nsubmission[targets] = 0.1*submission_1[targets].values + 0.9*submission_2[targets].values\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}