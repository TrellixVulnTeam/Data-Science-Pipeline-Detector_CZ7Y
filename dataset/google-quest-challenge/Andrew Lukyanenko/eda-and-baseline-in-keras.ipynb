{"cells":[{"metadata":{},"cell_type":"markdown","source":"## General information\n\nIn this kernel I work with data from Google QUEST Q&A Labeling competition.\n\nIn this challenge we work with... opinions. This could help Q&A systems, so let's try!"},{"metadata":{},"cell_type":"markdown","source":"## Importing libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport spacy\nimport gensim\nfrom scipy.stats import spearmanr\nfrom nltk.tokenize import TweetTokenizer\nimport datetime\nfrom scipy import stats\nfrom scipy.sparse import hstack, csr_matrix\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold\nfrom wordcloud import WordCloud\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression, LinearRegression, Ridge\nfrom sklearn.svm import LinearSVC\nfrom sklearn.multiclass import OneVsRestClassifier\nimport time\nimport re\npd.set_option('max_colwidth',400)\nfrom sklearn.multioutput import MultiOutputRegressor\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU, BatchNormalization\nfrom tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten, Masking, Reshape\nfrom tensorflow.keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D, Concatenate\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras import initializers, regularizers, constraints, optimizers, layers, callbacks\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.layers import InputSpec, Layer\nfrom tensorflow.keras.optimizers import Adam\nimport scipy as sp\nfrom tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping, ReduceLROnPlateau\nimport scipy as sp\nfrom keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping, ReduceLROnPlateau\nfrom sklearn.preprocessing import OneHotEncoder\nimport re\nimport torch\nimport os\nimport random\nimport numpy as np\n\nfrom nltk.stem import PorterStemmer\nps = PorterStemmer()\nfrom nltk.stem.lancaster import LancasterStemmer\nlc = LancasterStemmer()\nfrom nltk.stem import SnowballStemmer\nsb = SnowballStemmer(\"english\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Helper functions"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# added preprocessing from https://www.kaggle.com/wowfattie/3rd-place/data\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\n\npuncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£',\n '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', '\\n', '\\xa0', '\\t',\n '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '\\u3000', '\\u202f',\n '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '«',\n '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\nmispell_dict = {\"aren't\" : \"are not\",\n\"can't\" : \"cannot\",\n\"couldn't\" : \"could not\",\n\"couldnt\" : \"could not\",\n\"didn't\" : \"did not\",\n\"doesn't\" : \"does not\",\n\"doesnt\" : \"does not\",\n\"don't\" : \"do not\",\n\"hadn't\" : \"had not\",\n\"hasn't\" : \"has not\",\n\"haven't\" : \"have not\",\n\"havent\" : \"have not\",\n\"he'd\" : \"he would\",\n\"he'll\" : \"he will\",\n\"he's\" : \"he is\",\n\"i'd\" : \"I would\",\n\"i'd\" : \"I had\",\n\"i'll\" : \"I will\",\n\"i'm\" : \"I am\",\n\"isn't\" : \"is not\",\n\"it's\" : \"it is\",\n\"it'll\":\"it will\",\n\"i've\" : \"I have\",\n\"let's\" : \"let us\",\n\"mightn't\" : \"might not\",\n\"mustn't\" : \"must not\",\n\"shan't\" : \"shall not\",\n\"she'd\" : \"she would\",\n\"she'll\" : \"she will\",\n\"she's\" : \"she is\",\n\"shouldn't\" : \"should not\",\n\"shouldnt\" : \"should not\",\n\"that's\" : \"that is\",\n\"thats\" : \"that is\",\n\"there's\" : \"there is\",\n\"theres\" : \"there is\",\n\"they'd\" : \"they would\",\n\"they'll\" : \"they will\",\n\"they're\" : \"they are\",\n\"theyre\":  \"they are\",\n\"they've\" : \"they have\",\n\"we'd\" : \"we would\",\n\"we're\" : \"we are\",\n\"weren't\" : \"were not\",\n\"we've\" : \"we have\",\n\"what'll\" : \"what will\",\n\"what're\" : \"what are\",\n\"what's\" : \"what is\",\n\"what've\" : \"what have\",\n\"where's\" : \"where is\",\n\"who'd\" : \"who would\",\n\"who'll\" : \"who will\",\n\"who're\" : \"who are\",\n\"who's\" : \"who is\",\n\"who've\" : \"who have\",\n\"won't\" : \"will not\",\n\"wouldn't\" : \"would not\",\n\"you'd\" : \"you would\",\n\"you'll\" : \"you will\",\n\"you're\" : \"you are\",\n\"you've\" : \"you have\",\n\"'re\": \" are\",\n\"wasn't\": \"was not\",\n\"we'll\":\" will\",\n\"didn't\": \"did not\",\n\"tryin'\":\"trying\"}\n\n\ndef clean_text(x):\n    x = str(x)\n    for punct in puncts:\n        x = x.replace(punct, f' {punct} ')\n    return x\n\n\ndef clean_numbers(x):\n    x = re.sub('[0-9]{5,}', '#####', x)\n    x = re.sub('[0-9]{4}', '####', x)\n    x = re.sub('[0-9]{3}', '###', x)\n    x = re.sub('[0-9]{2}', '##', x)\n    return x\n\n\ndef _get_mispell(mispell_dict):\n    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n    return mispell_dict, mispell_re\n\n\ndef replace_typical_misspell(text):\n    mispellings, mispellings_re = _get_mispell(mispell_dict)\n\n    def replace(match):\n        return mispellings[match.group(0)]\n\n    return mispellings_re.sub(replace, text)\n\n\ndef clean_data(df, columns: list):\n    for col in columns:\n        df[col] = df[col].apply(lambda x: clean_numbers(x))\n        df[col] = df[col].apply(lambda x: clean_text(x.lower()))\n        df[col] = df[col].apply(lambda x: replace_typical_misspell(x))\n\n    return df\n\ndef get_coefs(word,*arr):\n    return word, np.asarray(arr, dtype='float32')\n\n\n\ndef build_matrix(embedding_path: str = '',\n                 embedding_path_spellcheck: str = r'f:\\embeddings\\wiki-news-300d-1M\\wiki-news-300d-1M.vec',\n                 word_dict: dict = None, lemma_dict: dict = None, max_features: int = 100000,\n                 embed_size: int= 300, ):\n    spell_model = gensim.models.KeyedVectors.load_word2vec_format(embedding_path_spellcheck)\n    words = spell_model.index2word\n    w_rank = {}\n    for i, word in enumerate(words):\n        w_rank[word] = i\n    WORDS = w_rank\n\n    def P(word):\n        \"Probability of `word`.\"\n        # use inverse of rank as proxy\n        # returns 0 if the word isn't in the dictionary\n        return - WORDS.get(word, 0)\n\n    def correction(word):\n        \"Most probable spelling correction for word.\"\n        return max(candidates(word), key=P)\n\n    def candidates(word):\n        \"Generate possible spelling corrections for word.\"\n        return (known([word]) or known(edits1(word)) or [word])\n\n    def known(words):\n        \"The subset of `words` that appear in the dictionary of WORDS.\"\n        return set(w for w in words if w in WORDS)\n\n    def edits1(word):\n        \"All edits that are one edit away from `word`.\"\n        letters = 'abcdefghijklmnopqrstuvwxyz'\n        splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n        deletes = [L + R[1:] for L, R in splits if R]\n        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n        replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n        inserts = [L + c + R for L, R in splits for c in letters]\n        return set(deletes + transposes + replaces + inserts)\n\n    def edits2(word):\n        \"All edits that are two edits away from `word`.\"\n        return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n\n    def singlify(word):\n        return \"\".join([letter for i, letter in enumerate(word) if i == 0 or letter != word[i - 1]])\n\n\n    embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path, encoding='utf-8'))\n\n    nb_words = min(max_features, len(word_dict))\n    embedding_matrix = np.zeros((nb_words + 1, embed_size))\n    unknown_words = []\n    for word, i in word_dict.items():\n        key = word\n        embedding_vector = embedding_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n            continue\n        embedding_vector = embedding_index.get(word.lower())\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n            continue\n        embedding_vector = embedding_index.get(word.upper())\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n            continue\n        embedding_vector = embedding_index.get(word.capitalize())\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n            continue\n        word = ps.stem(key)\n        embedding_vector = embedding_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[word_dict[key]] = embedding_vector\n            continue\n        word = lc.stem(key)\n        embedding_vector = embedding_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[word_dict[key]] = embedding_vector\n            continue\n        word = sb.stem(key)\n        embedding_vector = embedding_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[word_dict[key]] = embedding_vector\n            continue\n        word = lemma_dict[key]\n        embedding_vector = embedding_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[word_dict[key]] = embedding_vector\n            continue\n        if len(key) > 1:\n            word = correction(key)\n            embedding_vector = embedding_index.get(word)\n            if embedding_vector is not None:\n                embedding_matrix[word_dict[key]] = embedding_vector\n                continue\n        unknown_words.append(key)\n\n    print(f'{len(unknown_words) * 100 / len(word_dict):.4f}% words are not in embeddings')\n    return embedding_matrix, nb_words, unknown_words\n\n\ndef get_word_lemma_dict(full_text: list = None, ):\n    nlp = spacy.load('en_core_web_lg', disable=['parser','ner','tagger'])\n    nlp.vocab.add_flag(lambda s: s.lower() in spacy.lang.en.stop_words.STOP_WORDS, spacy.attrs.IS_STOP)\n    word_dict = {}\n    word_index = 1\n    lemma_dict = {}\n    docs = nlp.pipe(full_text, n_threads = os.cpu_count())\n    for doc in docs:\n        for token in doc:\n            if (token.text not in word_dict) and (token.pos_ is not \"PUNCT\"):\n                word_dict[token.text] = word_index\n                word_index += 1\n                lemma_dict[token.text] = token.lemma_\n\n    return lemma_dict, word_dict","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading and exploring the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('max_rows', 500)\npd.set_option('max_columns', 500)\npath = '/kaggle/input/google-quest-challenge'\nsample_submission = pd.read_csv(f'{path}/sample_submission.csv')\ntest = pd.read_csv(f'{path}/test.csv')\ntrain = pd.read_csv(f'{path}/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we have the following information:\n- question and its title;\n- answer;\n- questioner and answerer names and page links;\n- url of the question;\n- some kind of category and host;\n\nAlso we have 30 targets. They are very different and describe different things.\n\nIn this kernel I'll work with the texts themselves, without using additional information."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(train['question_body'].apply(lambda x: len(x.split())));\nplt.title('Distribution of number of words in questions');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(train['answer'].apply(lambda x: len(x.split())));\nplt.title('Distribution of number of words in answers');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is interesting that there are a lot of long questions/answers. And there are one which are exteremely wrong - we will need to be able to deal with it."},{"metadata":{"trusted":true},"cell_type":"code","source":"text = ' '.join(train['question_body'].values)\ntext_trigrams = [i for i in ngrams(text.split(), 3)]\nCounter(text_trigrams).most_common(30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As expected people describe what they are doing and what would they like to be able to do."},{"metadata":{"trusted":true},"cell_type":"code","source":"text = ' '.join(train['answer'].values)\ntext_trigrams = [i for i in ngrams(text.split(), 3)]\nCounter(text_trigrams).most_common(30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And in answers people explain what needs to be done."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"text = ' '.join(train['question_body'].values)\nwordcloud = WordCloud(max_font_size=None, background_color='white',\n                      width=1200, height=1000).generate(text)\nplt.figure(figsize=(12, 8))\nplt.imshow(wordcloud)\nplt.title('Top words in questions')\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"text = ' '.join(train['answer'].values)\nwordcloud = WordCloud(max_font_size=None, background_color='white',\n                      width=1200, height=1000).generate(text)\nplt.figure(figsize=(12, 8))\nplt.imshow(wordcloud)\nplt.title('Top words in answers')\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## A basic modelling\n\nAs a first attempt, let's try classical approaches: tf-idf with linear regression"},{"metadata":{},"cell_type":"markdown","source":"### preparing the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = clean_data(train, ['answer', 'question_body', 'question_title'])\ntest = clean_data(test, ['answer', 'question_body', 'question_title'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = TweetTokenizer()\ntrain_df, valid_df, y_train, y_valid = train_test_split(train, train[sample_submission.columns[1:]], test_size=0.1)\n\nvectorizer = TfidfVectorizer(ngram_range=(1, 2), tokenizer=tokenizer.tokenize)\nfull_text = list(train['question_body'].values) + list(test['question_body'].values)\nvectorizer.fit(full_text)\ntrain_vectorized = vectorizer.transform(train_df['question_body'])\nvalid_vectorized = vectorizer.transform(valid_df['question_body'])\ntest_vectorized = vectorizer.transform(test['question_body'])\n\nvectorizer = TfidfVectorizer(ngram_range=(1, 2), tokenizer=tokenizer.tokenize)\nfull_text = list(train['answer'].values) + list(test['answer'].values)\nvectorizer.fit(full_text)\ntrain_vectorized1 = vectorizer.transform(train_df['answer'])\nvalid_vectorized1 = vectorizer.transform(valid_df['answer'])\ntest_vectorized1 = vectorizer.transform(test['answer'])\n\nX_train = sp.sparse.hstack((train_vectorized, train_vectorized1))\nX_valid = sp.sparse.hstack((valid_vectorized, valid_vectorized1))\nX_test = sp.sparse.hstack((test_vectorized, test_vectorized1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I'm using `MultiOutputRegressor` which allows to model multiple continuous outputs."},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge = Ridge()\nmodel = MultiOutputRegressor(ridge, n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nmodel.fit(X_train, y_train);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_pred = model.predict(X_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"score = 0\nfor i, col in enumerate(train[sample_submission.columns[1:]]):\n    score += np.nan_to_num(spearmanr(y_valid[col], valid_pred[:, i]).correlation) / 30","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Mean spearman correlation: {score}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not bad, right?"},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = np.nan_to_num(model.predict(X_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## DL approach\n\nThere are several steps in preparing the data:\n- combine all the texts (title, question, answer) into one field and fit tokenizer of it;\n- generate word and lemma dictionaries with spacy for further preprocessing;\n- load embedding matrix.\n\n`build_matrix` function loads embeddings from a path, loads additional embeddings and tries to get embeddings for as many words as possible."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntrain['text'] = train['question_body'] + ' ' + train['answer'] + ' ' + train['question_title']\ntest['text'] = test['question_body'] + ' ' + test['answer'] + ' ' + test['question_title']\n\nfull_text = list(train['text'].values) + list(test['text'].values)\n\ntk = Tokenizer(lower = True, filters='')\ntk.fit_on_texts(full_text)\nprint(f'Words in dictionary: {len(tk.word_index)}')\n\nembed_size=300\nembedding_path = \"/kaggle/input/fasttext-crawl-300d-2m/crawl-300d-2M.vec\"\n\nlemma_dict, word_dict = get_word_lemma_dict(full_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time\nembedding_matrix, nb_words, unknown_words = build_matrix(embedding_path, '/kaggle/input/wikinews300d1mvec/wiki-news-300d-1M.vec', word_dict, lemma_dict,\n                                              100000, embed_size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we prepare the data itself:\n- texts are tokenized and padded;\n- host and category are encoded and will be used in the model;\n\nSome ideas for data processing and for models are from this kernel: https://www.kaggle.com/xhlulu/quest-bigru-starter-code"},{"metadata":{"trusted":true},"cell_type":"code","source":"tk.word_index = {k: v for k, v in tk.word_index.items() if k in word_dict.keys()}\ntrain['host'] = train['host'].apply(lambda x: x.split('.')[0] if len(x.split('.')) == 3 else '')\ntest['host'] = test['host'].apply(lambda x: x.split('.')[0] if len(x.split('.')) == 3 else '')\nunique_hosts = list(set(train['host'].unique().tolist() + test['host'].unique().tolist()))\nhost_dict = {i + 1: e for i, e in enumerate(unique_hosts)}\nhost_dict_reverse = {v: k for k, v in host_dict.items()}\n\nunique_categories = list(set(train['category'].unique().tolist() + test['category'].unique().tolist()))\ncategory_dict = {i + 1: e for i, e in enumerate(unique_categories)}\ncategory_dict_reverse = {v: k for k, v in category_dict.items()}\nmax_len = 500\nmax_len_title = 30\ntrain_question_tokenized = pad_sequences(tk.texts_to_sequences(train['question_body']), maxlen = max_len)\ntrain_answer_tokenized = pad_sequences(tk.texts_to_sequences(train['answer']), maxlen = max_len)\ntrain_title_tokenized = pad_sequences(tk.texts_to_sequences(train['question_title']), maxlen = max_len_title)\n\ntest_question_tokenized = pad_sequences(tk.texts_to_sequences(test['question_body']), maxlen = max_len)\ntest_answer_tokenized = pad_sequences(tk.texts_to_sequences(test['answer']), maxlen = max_len)\ntest_title_tokenized = pad_sequences(tk.texts_to_sequences(test['question_title']), maxlen = max_len_title)\ntrain_host = train['host'].apply(lambda x: host_dict_reverse[x]).values\ntrain_category = train['category'].apply(lambda x: category_dict_reverse[x]).values\n\ntest_host = test['host'].apply(lambda x: host_dict_reverse[x]).values\ntest_category = test['category'].apply(lambda x: category_dict_reverse[x]).values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"folds = KFold(n_splits=5, random_state=42)\nfor fold_n, (train_index, valid_index) in enumerate(folds.split(train)):\n    break\n    \nX_train = [train_question_tokenized[train_index], train_answer_tokenized[train_index], train_title_tokenized[train_index],\n           train_category[train_index], train_host[train_index]]\nX_valid = [train_question_tokenized[valid_index], train_answer_tokenized[valid_index], train_title_tokenized[valid_index],\n           train_category[valid_index], train_host[valid_index]]\nX_test = [test_question_tokenized, test_answer_tokenized, test_title_tokenized, test_category, test_host]\n\ny = train[sample_submission.columns[1:]].values\ny_train = y[train_index]\ny_valid = y[valid_index]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model architecture"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"class Attention(Layer):\n    def __init__(self, step_dim,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n        \"\"\"\n        Keras Layer that implements an Attention mechanism for temporal data.\n        Supports Masking.\n        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n        # Input shape\n            3D tensor with shape: `(samples, steps, features)`.\n        # Output shape\n            2D tensor with shape: `(samples, features)`.\n        :param kwargs:\n        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n        The dimensions are inferred based on the output shape of the RNN.\n        Example:\n            model.add(LSTM(64, return_sequences=True))\n            model.add(Attention())\n        \"\"\"\n        self.supports_masking = True\n        #self.init = initializations.get('glorot_uniform')\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        self.step_dim = step_dim\n        self.features_dim = 0\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight(shape=(input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        self.features_dim = input_shape[-1]\n\n        if self.bias:\n            self.b = self.add_weight(shape=(input_shape[1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n        else:\n            self.b = None\n\n        self.built = True\n\n    def compute_mask(self, input, input_mask=None):\n        # do not pass the mask to the next layers\n        return None\n\n    def call(self, x, mask=None):\n        # eij = K.dot(x, self.W) TF backend doesn't support it\n\n        # features_dim = self.W.shape[0]\n        # step_dim = x._keras_shape[1]\n\n        features_dim = self.features_dim\n        step_dim = self.step_dim\n\n        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n\n        if self.bias:\n            eij += self.b\n\n        eij = K.tanh(eij)\n\n        a = K.exp(eij)\n\n        # apply mask after the exp. will be re-normalized next\n        if mask is not None:\n            # Cast the mask to floatX to avoid float64 upcasting in theano\n            a *= K.cast(mask, K.floatx())\n\n        # in some cases especially in the early stages of training the sum may be almost zero\n        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n\n        a = K.expand_dims(a)\n        weighted_input = x * a\n    #print weigthted_input.shape\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        #return input_shape[0], input_shape[-1]\n        return input_shape[0],  self.features_dim","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def build_model_deep(embedding_matrix: np.array, max_len_title: int = 30, max_len: int = 500,\n                     gru_dim: int = 150, n_gru_layers: int = 2, units: int = 128, spatial_dr: int = 0.1,\n                     kernel_size1: int = 3, kernel_size2: int = 2, dense_units: int = 128, dr: int = 0.1,\n                     conv_size: int = 32, lr: int = 0.001, lr_d: int = 0.0):\n    text_embedding = Embedding(\n        *embedding_matrix.shape,\n        weights=[embedding_matrix],\n        trainable=False,\n        mask_zero=True,\n        name='text_embedding'\n    )\n    category_embedding = Embedding(\n        input_dim=6,\n        output_dim=3,\n        name='category_embedding'\n    )\n    host_embedding = Embedding(\n        input_dim=56,\n        output_dim=28,\n        name='host_embedding'\n    )\n\n    question_body_in = Input(shape=(max_len,))\n    answer_inp = Input(shape=(max_len,))\n    question_title_in = Input(shape=(max_len_title,))\n    category_inp = Input(shape=(1,))\n    host_inp = Input(shape=(1,))\n\n    # question\n    # x = Embedding(50001, embed_size, weights=[embedding_matrix], trainable=False)(question_body_in)\n    x = text_embedding(question_body_in)\n    x1 = SpatialDropout1D(spatial_dr)(x)\n\n    x_gru = Bidirectional(GRU(units, return_sequences=True))(x1)\n    x1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_gru)\n    avg_pool1_gru = GlobalAveragePooling1D()(x1)\n    max_pool1_gru = GlobalMaxPooling1D()(x1)\n\n    x3 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_gru)\n    avg_pool3_gru = GlobalAveragePooling1D()(x3)\n    max_pool3_gru = GlobalMaxPooling1D()(x3)\n\n    x_lstm = Bidirectional(LSTM(units, return_sequences=True))(x1)\n    x1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n    avg_pool1_lstm = GlobalAveragePooling1D()(x1)\n    max_pool1_lstm = GlobalMaxPooling1D()(x1)\n\n    x3 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n    avg_pool3_lstm = GlobalAveragePooling1D()(x3)\n    max_pool3_lstm = GlobalMaxPooling1D()(x3)\n\n    # answer\n    question_gru_layers = [\n        Bidirectional(GRU(gru_dim, return_sequences=True), name=f'BiGRU_q_{i}')\n        for i in range(1, n_gru_layers * 2)\n    ]\n    answer = gru_block(answer_inp, question_gru_layers, text_embedding, block_name='answer')\n\n    # title\n\n    title_gru_layers = [\n        Bidirectional(GRU(gru_dim, return_sequences=True), name=f'BiGRU_t_{i}')\n        for i in range(1, n_gru_layers)\n    ]\n    title = gru_block(question_title_in, title_gru_layers, text_embedding, block_name='title')\n\n    category = category_embedding(category_inp)\n    host = host_embedding(host_inp)\n\n    x = concatenate([avg_pool1_gru, max_pool1_gru, avg_pool3_gru, max_pool3_gru,\n                     avg_pool1_lstm, max_pool1_lstm, avg_pool3_lstm, max_pool3_lstm,\n                     answer, title, Flatten()(category), Flatten()(host)])\n    x = BatchNormalization()(x)\n    x = Dropout(dr)(Dense(dense_units, activation='relu')(x))\n    x = BatchNormalization()(x)\n    x = Dropout(dr)(Dense(int(dense_units / 2), activation='relu')(x))\n    x = Dense(30, activation=\"sigmoid\")(x)\n    model = Model(inputs=[question_body_in, answer_inp, question_title_in, category_inp, host_inp], outputs=x)\n    model.compile(loss=\"binary_crossentropy\", optimizer=Adam(lr=lr, decay=lr_d), metrics=['mse'])\n\n    return model\n\ndef build_model2(embedding_matrix, n_categories, n_hosts, output_shape,\n                gru_dim=150, n_gru_layers=3, n_fc_layers=2, hidden_dim=400):\n    text_embedding = Embedding(\n        *embedding_matrix.shape,\n        weights=[embedding_matrix],\n        trainable=False,\n        mask_zero=True,\n        name='text_embedding'\n    )\n    category_embedding = Embedding(\n        input_dim=n_categories + 1,\n        output_dim=3,\n        name='category_embedding'\n    )\n    host_embedding = Embedding(\n        input_dim=n_hosts + 1,\n        output_dim=28,\n        name='host_embedding'\n    )\n\n    # inputs\n    qb_in = Input(shape=(500,))\n    a_in = Input(shape=(500,))\n    qt_in = Input(shape=(30,))\n    c_in = Input(shape=(1,))\n    h_in = Input(shape=(1,))\n\n    # Embed the host and category labels\n    c = category_embedding(c_in)\n    h = host_embedding(h_in)\n    #c = Reshape((3, ), name='category_reshape')(c)\n    #h = Reshape((28, ), name='host_reshape')(h)\n\n    # Pass data through stacked bidirectional GRUs\n    gru_layers = [\n        Bidirectional(GRU(gru_dim, return_sequences=True), name=f'BiGRU{i}')\n        for i in range(1, n_gru_layers +1)\n    ]\n    qb = gru_block(qb_in, gru_layers, text_embedding, block_name='qbody')\n    a = gru_block(a_in, gru_layers, text_embedding, block_name='answer')\n    qt = gru_block(qt_in, gru_layers, text_embedding, block_name='qtitle')\n\n    # Concatenate pooled title/body/answer with embedded category/host\n    hidden = Concatenate(name='combine_all')([qt, qb, a, Flatten() (c), Flatten() (h)])\n\n    # Fully-connected layers\n    for _ in range(n_fc_layers):\n        hidden = Dense(hidden_dim, activation='relu')(hidden)\n        hidden = BatchNormalization()(hidden)\n        hidden = Dropout(0.1)(hidden)\n\n    out = Dense(output_shape, activation='sigmoid', name='output')(hidden)\n\n    model = Model(inputs=[qb_in, a_in, qt_in, c_in, h_in], outputs=[out])\n\n    model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.001, amsgrad=True), metrics=['mae'])\n\n    return model\n\n\ndef gru_block(x_in, gru_layers, embedding, block_name, hidden_dim=200):\n    x = embedding(x_in)\n    x = SpatialDropout1D(0.2, name=f'{block_name}_sp_drop')(x)\n\n    for gru in gru_layers:\n        x = gru(x)\n\n    x = GlobalMaxPooling1D(name=f'{block_name}_pool')(x)\n    x = Dense(hidden_dim, activation='relu', name=f'{block_name}_fc')(x)\n    x = BatchNormalization(name=f'{block_name}_bn')(x)\n    x = Dropout(0.5, name=f'{block_name}_drop')(x)\n\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1 = build_model2(\n    embedding_matrix, \n    n_categories=len(category_dict),\n    n_hosts=len(host_dict),\n    output_shape=30,\n    n_gru_layers=3\n)\nmodel1.summary()\nfile_path = \"best_model.hdf5\"\ncheck_point = ModelCheckpoint(file_path, monitor=\"val_loss\", verbose=1,\n                              save_best_only=True, mode=\"min\")\nearly_stop = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nhistory = model1.fit(X_train, y_train, batch_size=64, epochs=5, validation_data=(X_valid, y_valid),\n                    verbose=2, callbacks=[check_point, early_stop])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"valid_pred = model1.predict(X_valid, batch_size = 1024, verbose = 0)\nscore = 0\nfor i, col in enumerate(train[sample_submission.columns[1:]]):\n    score += np.nan_to_num(spearmanr(y_valid[:, i], valid_pred[:, i]).correlation) / 30","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction = np.nan_to_num(model1.predict(X_test, batch_size = 1024, verbose = 0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission[sample_submission.columns[1:]] = sigmoid(prediction)\nsample_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}