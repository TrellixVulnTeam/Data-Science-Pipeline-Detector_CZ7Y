{"cells":[{"metadata":{},"cell_type":"markdown","source":"## This is my first public kernel. \n\n### I try using some simple preprocessing and create LSTM neural network with pytorch \n\n### score : 0.339 in version 6\n\n\nThanks for Andrew Lukyanenko sharing, Some of the code can be seen in the kernel:\n\nhttps://www.kaggle.com/artgor/pytorch-approach/notebook\n\n### next step, I want to adding some additional feature, and try using BERT.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\nimport re\nimport os\nimport sys\nimport time\nimport pickle\nimport random\nimport unidecode\nfrom tqdm import tqdm\ntqdm.pandas()\nfrom scipy.stats import spearmanr\nfrom gensim.models import Word2Vec\nfrom flashtext import KeywordProcessor\nfrom keras.preprocessing import text, sequence\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom sklearn.model_selection import StratifiedKFold, KFold\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/google-quest-challenge/train.csv')\ntest = pd.read_csv('/kaggle/input/google-quest-challenge/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv('/kaggle/input/google-quest-challenge/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MODEL = pickle.load(open('/kaggle/input/pickled-glove840b300d-for-10sec-loading/glove.840B.300d.pkl', 'rb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PUNCTS = {\n            '》', '〞', '¢', '‹', '╦', '║', '♪', 'Ø', '╩', '\\\\', '★', '＋', 'ï', '<', '?', '％', '+', '„', 'α', '*', '〰', '｟', '¹', '●', '〗', ']', '▾', '■', '〙', '↓', '´', '【', 'ᴵ',\n            '\"', '）', '｀', '│', '¤', '²', '‡', '¿', '–', '」', '╔', '〾', '%', '¾', '←', '〔', '＿', '’', '-', ':', '‧', '｛', 'β', '（', '─', 'à', 'â', '､', '•', '；', '☆', '／', 'π',\n            'é', '╗', '＾', '▪', ',', '►', '/', '〚', '¶', '♦', '™', '}', '″', '＂', '『', '▬', '±', '«', '“', '÷', '×', '^', '!', '╣', '▲', '・', '░', '′', '〝', '‛', '√', ';', '】', '▼',\n            '.', '~', '`', '。', 'ə', '］', '，', '{', '～', '！', '†', '‘', '﹏', '═', '｣', '〕', '〜', '＼', '▒', '＄', '♥', '〛', '≤', '∞', '_', '[', '＆', '→', '»', '－', '＝', '§', '⋅', \n            '▓', '&', 'Â', '＞', '〃', '|', '¦', '—', '╚', '〖', '―', '¸', '³', '®', '｠', '¨', '‟', '＊', '£', '#', 'Ã', \"'\", '▀', '·', '？', '、', '█', '”', '＃', '⊕', '=', '〟', '½', '』',\n            '［', '$', ')', 'θ', '@', '›', '＠', '｝', '¬', '…', '¼', '：', '¥', '❤', '€', '−', '＜', '(', '〘', '▄', '＇', '>', '₤', '₹', '∅', 'è', '〿', '「', '©', '｢', '∙', '°', '｜', '¡', \n            '↑', 'º', '¯', '♫', '#'\n          }\n\n\nmispell_dict = {\"aren't\" : \"are not\", \"can't\" : \"cannot\", \"couldn't\" : \"could not\",\n\"couldnt\" : \"could not\", \"didn't\" : \"did not\", \"doesn't\" : \"does not\",\n\"doesnt\" : \"does not\", \"don't\" : \"do not\", \"hadn't\" : \"had not\", \"hasn't\" : \"has not\",\n\"haven't\" : \"have not\", \"havent\" : \"have not\", \"he'd\" : \"he would\", \"he'll\" : \"he will\", \"he's\" : \"he is\", \"i'd\" : \"I would\",\n\"i'd\" : \"I had\", \"i'll\" : \"I will\", \"i'm\" : \"I am\", \"isn't\" : \"is not\", \"it's\" : \"it is\",\n\"it'll\":\"it will\", \"i've\" : \"I have\", \"let's\" : \"let us\", \"mightn't\" : \"might not\", \"mustn't\" : \"must not\", \n\"shan't\" : \"shall not\", \"she'd\" : \"she would\", \"she'll\" : \"she will\", \"she's\" : \"she is\", \"shouldn't\" : \"should not\", \"shouldnt\" : \"should not\",\n\"that's\" : \"that is\", \"thats\" : \"that is\", \"there's\" : \"there is\", \"theres\" : \"there is\", \"they'd\" : \"they would\", \"they'll\" : \"they will\",\n\"they're\" : \"they are\", \"theyre\":  \"they are\", \"they've\" : \"they have\", \"we'd\" : \"we would\", \"we're\" : \"we are\", \"weren't\" : \"were not\",\n\"we've\" : \"we have\", \"what'll\" : \"what will\", \"what're\" : \"what are\", \"what's\" : \"what is\", \"what've\" : \"what have\", \"where's\" : \"where is\",\n\"who'd\" : \"who would\", \"who'll\" : \"who will\", \"who're\" : \"who are\", \"who's\" : \"who is\", \"who've\" : \"who have\", \"won't\" : \"will not\", \"wouldn't\" : \"would not\", \"you'd\" : \"you would\",\n\"you'll\" : \"you will\", \"you're\" : \"you are\", \"you've\" : \"you have\", \"'re\": \" are\", \"wasn't\": \"was not\", \"we'll\":\" will\", \"didn't\": \"did not\", \"tryin'\":\"trying\"}\n\n\ndef clean_punct(text):\n  text = str(text)\n  for punct in PUNCTS:\n    text = text.replace(punct, ' {} '.format(punct))\n  \n  return text\n\ndef _get_mispell(mispell_dict):\n    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n    return mispell_dict, mispell_re\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kp = KeywordProcessor(case_sensitive=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for k, v in mispell_dict.items():\n    kp.add_keyword(k, v)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocessing(text):\n    text = kp.replace_keywords(text)\n    text = clean_punct(text)\n    text = re.sub(r'\\n\\r', ' ', text)\n    text = re.sub(r'\\s{2,}', ' ', text)\n    \n    return text.split()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['clean_title'] = train['question_title'].apply(lambda x : preprocessing(x))\ntrain['clean_body'] = train['question_body'].apply(lambda x : preprocessing(x))\ntrain['clean_answer'] = train['answer'].apply(lambda x : preprocessing(x))\n\ntest['clean_title'] = test['question_title'].apply(lambda x : preprocessing(x))\ntest['clean_body'] = test['question_body'].apply(lambda x : preprocessing(x))\ntest['clean_answer'] = test['answer'].apply(lambda x : preprocessing(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_columns = ['question_asker_intent_understanding',\n       'question_body_critical', 'question_conversational',\n       'question_expect_short_answer', 'question_fact_seeking',\n       'question_has_commonly_accepted_answer',\n       'question_interestingness_others', 'question_interestingness_self',\n       'question_multi_intent', 'question_not_really_a_question',\n       'question_opinion_seeking', 'question_type_choice',\n       'question_type_compare', 'question_type_consequence',\n       'question_type_definition', 'question_type_entity',\n       'question_type_instructions', 'question_type_procedure',\n       'question_type_reason_explanation', 'question_type_spelling',\n       'question_well_written', 'answer_helpful',\n       'answer_level_of_information', 'answer_plausible', 'answer_relevance',\n       'answer_satisfaction', 'answer_type_instructions',\n       'answer_type_procedure', 'answer_type_reason_explanation',\n       'answer_well_written']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_matrix(word_index):\n\n    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n    \n    unknown_words = []\n    unk = {}\n    known_count = 0\n    unk_count = 0\n    for word, i in word_index.items():\n        if word in MODEL:\n            embedding_matrix[i] = MODEL[word]\n            known_count += 1\n            continue\n        if word.lower() in MODEL:\n            embedding_matrix[i] = MODEL[word.lower()]\n            known_count += 1\n            continue    \n        if word.upper() in MODEL:\n            embedding_matrix[i] = MODEL[word.upper()]\n            known_count += 1\n            continue\n        if word.capitalize() in MODEL:\n            embedding_matrix[i] = MODEL[word.capitalize()]\n            known_count += 1\n            continue\n        if unidecode.unidecode(word) in MODEL:\n            embedding_matrix[i] = MODEL[unidecode.unidecode(word)]\n            known_count += 1\n            continue\n        try:\n            unk[word] += 1 \n        except:\n            unk[word] = 1\n        \n        unk_count += 1\n    \n    \n    print('all token in embedding percentage : {:.2f}%'.format( known_count/(unk_count+known_count)  * 100))\n#     print('token in embedding percentage : {}'.format( known_count/(unk_count+known_count)))\n    return embedding_matrix, unk","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = text.Tokenizer(filters='', lower=False)\n\n\ntokenizer.fit_on_texts(list(train['clean_title']) + list(train['clean_body']) + list(train['clean_answer']) \\\n                        + list(test['clean_title']) + list(test['clean_body']) + list(test['clean_answer']))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TITLE_MAX_LEN = 50\nBODY_MAX_LEN = 500\nANSWER_MAX_LEN = 500\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['clean_title_len'] = train['clean_title'].apply(lambda x : len(x))\ntrain['clean_body_len'] = train['clean_body'].apply(lambda x : len(x))\ntrain['clean_answer_len'] = train['clean_answer'].apply(lambda x : len(x))\n\n\ntest['clean_title_len'] = test['clean_title'].apply(lambda x : len(x))\ntest['clean_body_len'] = test['clean_body'].apply(lambda x : len(x))\ntest['clean_answer_len'] = test['clean_answer'].apply(lambda x : len(x))\n\n# train title max 58 test 48\n\n# train body max 4924 test body max 1894\n\n# train answer max 8194 test max 2224","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train_title = tokenizer.texts_to_sequences(train['clean_title'])\nx_test_title = tokenizer.texts_to_sequences(test['clean_title'])\n\nx_train_body = tokenizer.texts_to_sequences(train['clean_body'])\nx_test_body = tokenizer.texts_to_sequences(test['clean_body'])\n\nx_train_answer = tokenizer.texts_to_sequences(train['clean_answer'])\nx_test_answer = tokenizer.texts_to_sequences(test['clean_answer'])\n\n\nx_train_title = sequence.pad_sequences(x_train_title, maxlen=TITLE_MAX_LEN,padding='post')\nx_test_title = sequence.pad_sequences(x_test_title, maxlen=TITLE_MAX_LEN,padding='post')\n\nx_train_body = sequence.pad_sequences(x_train_body, maxlen=BODY_MAX_LEN,padding='post')\nx_test_body = sequence.pad_sequences(x_test_body, maxlen=BODY_MAX_LEN,padding='post')\n\nx_train_answer = sequence.pad_sequences(x_train_answer, maxlen=ANSWER_MAX_LEN,padding='post')\nx_test_answer = sequence.pad_sequences(x_test_answer, maxlen=ANSWER_MAX_LEN,padding='post')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\n\nc = 'host'\nonehotencoder = OneHotEncoder(sparse=False, categories='auto').fit(np.concatenate((train[c].values.reshape(-1, 1).astype('str'), test[c].values.reshape(-1, 1).astype('str'))))\ntrain_trans = onehotencoder.transform(train[c].values.reshape(-1, 1).astype('str'))\ntest_trans = onehotencoder.transform(test[c].values.reshape(-1, 1).astype('str'))\nfor i in range(train_trans.shape[1]):\n    train['{}_{}'.format(c, i)] = train_trans[:, i]\n    test['{}_{}'.format(c, i)] = test_trans[:, i]\nprint('remove origin column : {}'.format(c))\ntrain = train.drop(columns=c)\ntest = test.drop(columns=c)\ngc.collect()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ## additional feature\n# def get_set_char_len(content):\n#     set_char = set()\n#     for char in ' '.join(content):\n#         set_char.add(char)\n#     return len(set_char)\n\n# train['title_set_char_len'] = train['clean_title'].apply(lambda x : get_set_char_len(x)) \n# train['body_set_char_len'] = train['clean_body'].apply(lambda x : get_set_char_len(x)) \n# train['answer_set_char_len'] = train['clean_answer'].apply(lambda x : get_set_char_len(x)) \n\n\n# test['title_set_char_len'] = test['clean_title'].apply(lambda x : get_set_char_len(x)) \n# test['body_set_char_len'] = test['clean_body'].apply(lambda x : get_set_char_len(x)) \n# test['answer_set_char_len'] = test['clean_answer'].apply(lambda x : get_set_char_len(x)) \n\n\n# train_title_len = train['clean_title_len'] / max(train['clean_title_len'])\n# train_body_len = train['clean_body_len'] / max(train['clean_body_len'])\n# train_answer_len = train['clean_answer_len'] / max(train['clean_answer_len'])\n\n# test_title_len = test['clean_title_len'] / max(test['clean_title_len'])\n# test_body_len = test['clean_body_len'] / max(test['clean_body_len'])\n# test_answer_len = test['clean_answer_len'] / max(test['clean_answer_len'])\n\n# train_title_set_char_len = train['title_set_char_len'] / max(train['title_set_char_len'])\n# train_body_set_char_len = train['body_set_char_len'] / max(train['body_set_char_len'])\n# train_answer_set_char_len = train['answer_set_char_len'] / max(train['answer_set_char_len'])\n\n# test_title_set_char_len = test['title_set_char_len'] / max(test['title_set_char_len'])\n# test_body_set_char_len = test['body_set_char_len'] / max(test['body_set_char_len'])\n# test_answer_set_char_len = test['answer_set_char_len'] / max(test['answer_set_char_len'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntrain_category = pd.get_dummies(train['category'].values).values\ntest_category = pd.get_dummies(test['category'].values).values\n\nhosts = ['host_{}'.format(i) for i in range(64)]\ntrain_host = train.loc[:, hosts].values\ntest_host = test.loc[:, hosts].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word2vec_matrix, unk = build_matrix(tokenizer.word_index)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Attention(nn.Module):\n    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):\n        super(Attention, self).__init__(**kwargs)\n\n        self.supports_masking = True\n        self.bias = bias\n        self.feature_dim = feature_dim\n        self.step_dim = step_dim\n        self.features_dim = 0\n\n        weight = torch.zeros(feature_dim, 1)\n        nn.init.xavier_uniform_(weight)\n        self.weight = nn.Parameter(weight)\n\n        if bias:\n            self.b = nn.Parameter(torch.zeros(1))\n\n    def forward(self, x, mask=None):\n\n        feature_dim = self.feature_dim\n        step_dim = self.step_dim\n\n        eij = torch.mm(\n            x.contiguous().view(-1, feature_dim), \n            self.weight\n        ).view(-1, step_dim)\n\n        if self.bias:\n            eij = eij + self.b\n\n        eij = torch.tanh(eij)\n        a = torch.exp(eij)\n\n        if mask is not None:\n            a = a * mask\n\n        a = a / torch.sum(a, 1, keepdim=True) + 1e-10\n\n        weighted_input = x * torch.unsqueeze(a, -1)\n        return torch.sum(weighted_input, 1)\n\nclass SpatialDropout(nn.Module):\n    def __init__(self,p):\n        super(SpatialDropout, self).__init__()\n        self.dropout = nn.Dropout2d(p)\n        \n    def forward(self, x):\n        x = x.permute(0, 2, 1)   # convert to [batch, feature, timestep]\n        x = self.dropout(x)\n        x = x.permute(0, 2, 1)   # back to [batch, timestep, feature]\n        return x\n\nclass LSTM_Model(nn.Module):\n    def __init__(self, embedding_matrix, hidden_unit, num_layer=1):\n        super(LSTM_Model, self).__init__()\n        self.max_feature = embedding_matrix.shape[0]\n        self.embedding_size = embedding_matrix.shape[1]\n      \n        self.embedding_body = nn.Embedding(self.max_feature, self.embedding_size)\n        self.embedding_body.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n        self.embedding_body.weight.required_grad = False\n        \n        self.embedding_answer = nn.Embedding(self.max_feature, self.embedding_size)\n        self.embedding_answer.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n        self.embedding_answer.weight.required_grad = False\n        \n        self.embedding_title = nn.Embedding(self.max_feature, self.embedding_size)\n        self.embedding_title.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n        self.embedding_title.weight.required_grad = False\n        \n        self.embedding_dropout = SpatialDropout(0.4)\n        \n        self.lstm1_body = nn.LSTM(self.embedding_size, hidden_unit, num_layers=num_layer, bidirectional=True, batch_first=True)\n        self.lstm2_body = nn.LSTM(hidden_unit*2, int(hidden_unit/2), num_layers=num_layer, bidirectional=True, batch_first=True)\n        \n        self.lstm1_answer = nn.LSTM(self.embedding_size, hidden_unit, num_layers=num_layer, bidirectional=True, batch_first=True)\n        self.lstm2_answer = nn.LSTM(hidden_unit*2, int(hidden_unit/2), num_layers=num_layer, bidirectional=True, batch_first=True)\n        \n        self.lstm1_title = nn.LSTM(self.embedding_size, hidden_unit, num_layers=num_layer, bidirectional=True, batch_first=True)\n        self.lstm2_title = nn.LSTM(hidden_unit*2, int(hidden_unit/2), num_layers=num_layer, bidirectional=True, batch_first=True)\n        \n        self.attention_body = Attention(hidden_unit, BODY_MAX_LEN)\n        self.attention_answer = Attention(hidden_unit, ANSWER_MAX_LEN)\n        self.attention_title = Attention(hidden_unit, TITLE_MAX_LEN)\n        \n#         self.category = nn.Embedding(5, 10)\n#         self.host = nn.Embedding(64, 128)\n        \n        self.linear_title = nn.Linear(hidden_unit*3, hidden_unit)\n        self.linear_body = nn.Linear(hidden_unit*3, hidden_unit)\n        self.linear_answer = nn.Linear(hidden_unit*3, hidden_unit)\n        \n#         self.linear_out = nn.Linear(hidden_unit, 30)\n        self.additional_category = nn.Linear(5, 5)\n        self.additional_host = nn.Linear(64, 32)\n        \n        self.linear_q = nn.Linear(hidden_unit*2+37, hidden_unit)\n        self.linear_a = nn.Linear(hidden_unit+37, hidden_unit)\n        self.linear_q_out = nn.Linear(hidden_unit, 21)\n        self.linear_a_out = nn.Linear(hidden_unit, 9)\n        \n    def forward(self, body, answer, title, category, host):\n        \n        x_body = self.embedding_dropout(self.embedding_body(body))\n        h_lstm1_body, _ = self.lstm1_body(x_body)\n        h_lstm2_body, _ = self.lstm2_body(h_lstm1_body)\n        \n        x_answer = self.embedding_dropout(self.embedding_answer(answer))\n        h_lstm1_answer, _ = self.lstm1_answer(x_answer)\n        h_lstm2_answer, _ = self.lstm2_answer(h_lstm1_answer)\n        \n        x_title = self.embedding_dropout(self.embedding_title(title))\n        h_lstm1_title, _ = self.lstm1_title(x_title)\n        h_lstm2_title, _ = self.lstm2_title(h_lstm1_title)\n        \n#         print(h_lstm2_body.size())\n        att_body = self.attention_body(h_lstm2_body)\n        att_answer = self.attention_answer(h_lstm2_answer)\n        att_title = self.attention_title(h_lstm2_title)\n        \n        avg_pool_body = torch.mean(h_lstm2_body, 1)\n        max_pool_body, _ = torch.max(h_lstm2_body, 1)\n        \n        avg_pool_answer = torch.mean(h_lstm2_answer, 1)\n        max_pool_answer, _ = torch.max(h_lstm2_answer, 1)\n        \n        avg_pool_title = torch.mean(h_lstm2_title, 1)\n        max_pool_title, _ = torch.max(h_lstm2_title, 1)\n        \n        body_cat = torch.cat((att_body, avg_pool_body, max_pool_body), 1)\n        answer_cat = torch.cat((att_answer, avg_pool_answer, max_pool_answer), 1)\n        title_cat = torch.cat((att_title, avg_pool_title, max_pool_title), 1)\n        \n#         additional_feature = self.addtional_linear()\n\n#         category = self.category(category)\n#         host = self.category(host)\n        \n        \n        body_cat = torch.relu(self.linear_body(body_cat))\n        answer_cat = torch.relu(self.linear_answer(answer_cat))\n        title_cat = torch.relu(self.linear_title(title_cat))\n\n        category = self.additional_category(category)\n        host = self.additional_host(host)\n        \n        hidden_q = self.linear_q(torch.cat((title_cat, body_cat, category, host), 1))\n        hidden_a = self.linear_a(torch.cat((answer_cat, category, host), 1))\n                                          \n        q_result = self.linear_q_out(hidden_q)\n        a_result = self.linear_a_out(hidden_a)\n        \n        out = torch.cat([q_result, a_result], 1)\n        return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED = 2020\nNFOLDS = 4\nBATCH_SIZE = 32\nEPOCHS = 6\nLR = 0.001\nhidden_unit = 256\nseed_everything(SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kf = list(KFold(n_splits=NFOLDS, shuffle=True, random_state=SEED).split(x_train_title))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TextDataset(torch.utils.data.TensorDataset):\n\n    def __init__(self, body_data, answer_data, title_data, category_data, host_data, idxs, targets=None):\n        self.body_data = body_data[idxs]\n        self.answer_data = answer_data[idxs]\n        self.title_data = title_data[idxs]\n        self.category_data = category_data[idxs]\n        self.host_data = host_data[idxs]\n        self.targets = targets[idxs] if targets is not None else np.zeros((self.body_data.shape[0], 30))\n\n    def __getitem__(self, idx):\n        body = self.body_data[idx]\n        answer = self.answer_data[idx]\n        title = self.title_data[idx]\n        category = self.category_data[idx]\n        host = self.host_data[idx]\n        target = self.targets[idx]\n\n        return body, answer, title, category, host, target\n\n    def __len__(self):\n        return len(self.body_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_loader = torch.utils.data.DataLoader(TextDataset(x_test_body, x_test_answer, x_test_title, test_category, test_host, test.index),\n                          batch_size=BATCH_SIZE, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train.loc[:, y_columns].values\n\noof = np.zeros((len(train), 30))\ntest_pred = np.zeros((len(test), 30))\n\n# del train, hosts, onehotencoder\n# gc.collect()\nfor i, (train_idx, valid_idx) in enumerate(kf):\n    print(f'fold {i+1}')\n    gc.collect()\n    train_loader = torch.utils.data.DataLoader(TextDataset(x_train_body, x_train_answer, x_train_title, train_category, train_host, train_idx, y),\n                          batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n    \n    val_loader = torch.utils.data.DataLoader(TextDataset(x_train_body, x_train_answer, x_train_title, train_category, train_host, valid_idx, y),\n                          batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)\n    \n    net = LSTM_Model(word2vec_matrix, hidden_unit)\n    net.cuda()\n    loss_fn = torch.nn.BCEWithLogitsLoss(reduction='mean').cuda()\n    optimizer = torch.optim.Adam(net.parameters(), lr=LR)\n\n    for epoch in range(EPOCHS):  \n        start_time = time.time()\n        avg_loss = 0.0\n        net.train()\n        for data in train_loader:\n\n            # get the inputs\n            body, answer, title, category, host, labels = data\n            pred = net(body.long().cuda(), answer.long().cuda(), title.long().cuda(), category.float().cuda(), host.float().cuda())\n\n            loss = loss_fn(pred, labels.cuda())\n            # Before the backward pass, use the optimizer object to zero all of the\n            # gradients for the Tensors it will update (which are the learnable weights\n            # of the model)\n            optimizer.zero_grad()\n\n            # Backward pass: compute gradient of the loss with respect to model parameters\n            loss.backward()\n            # Calling the step function on an Optimizer makes an update to its parameters\n            optimizer.step()\n\n            avg_loss += loss.item()\n        \n        avg_val_loss = 0.0\n        net.eval()\n\n        valid_preds = np.zeros((len(valid_idx), 30))\n        true_label = np.zeros((len(valid_idx), 30))\n        for j, data in enumerate(val_loader):\n\n            # get the inputs\n            body, answer, title, category, host, labels = data\n\n            ## forward + backward + optimize\n            pred = net(body.long().cuda(), answer.long().cuda(), title.long().cuda(), category.float().cuda(), host.float().cuda())\n\n            loss_val = loss_fn(pred, labels.cuda())\n            avg_val_loss += loss_val.item()\n\n            valid_preds[j * BATCH_SIZE:(j+1) * BATCH_SIZE] = torch.sigmoid(pred).cpu().detach().numpy()\n            true_label[j * BATCH_SIZE:(j+1) * BATCH_SIZE]  = labels\n            \n        score = 0\n        for i in range(30):\n            score += np.nan_to_num(\n                    spearmanr(true_label[:, i], valid_preds[:, i]).correlation / 30)\n        oof[valid_idx] = valid_preds\n        elapsed_time = time.time() - start_time \n        print('Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t spearman={:.2f} \\t time={:.2f}s'.format(\n            epoch + 1, EPOCHS, avg_loss / len(train_loader), avg_val_loss / len(val_loader), score, elapsed_time))\n        \n    test_pred_fold = np.zeros((len(test), 30))\n        \n    with torch.no_grad():\n        for q, data in enumerate(test_loader):\n            body, answer, title, category, host, _ = data\n            y_pred = net(body.long().cuda(), answer.long().cuda(), title.long().cuda(), category.float().cuda(), host.float().cuda())\n            test_pred_fold[q * BATCH_SIZE:(q+1) * BATCH_SIZE] = torch.sigmoid(y_pred).cpu().detach().numpy()\n    test_pred += test_pred_fold/NFOLDS\n        \n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.loc[:, y_columns] = test_pred\nsub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}