{"cells":[{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import GroupKFold\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nimport tensorflow_hub as hub\nimport tensorflow as tf\nimport bert_tokenization as tokenization\nimport tensorflow.keras.backend as K\nimport gc\nimport os\nfrom scipy.stats import spearmanr\nfrom math import floor, ceil\nfrom tensorflow.keras.models import load_model\n\nnp.set_printoptions(suppress=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Abuout this kernel\n\nIn this kernel, an example of adding features other than character strings is described with reference to \"l Bert-base TF2.0 (minimalistic) III\".\n\nReferenced kernel\nBert-base TF2.0 (minimalistic) III<br>\nhttps://www.kaggle.com/bibek777/bert-base-tf2-0-minimalistic-iii\n\nThank you."},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n#os.listdir('../input/')\n\n\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PATH = '../input/google-quest-challenge/'\n\n\nBERT_PATH = '../input/bert-base-from-tfhub/bert_en_uncased_L-12_H-768_A-12'\n\n#Large model memory error.\n#BERT_PATH = '../input/bert-base-from-tfhub-l24-h1024-a16/tf_bert_en_cased_L-24_H-1024_A-16'\n\n\ntokenizer = tokenization.FullTokenizer(BERT_PATH+'/assets/vocab.txt', True)\nMAX_SEQUENCE_LENGTH = 512\n\ndf_train = pd.read_csv(PATH+'train.csv')\ndf_test = pd.read_csv(PATH+'test.csv')\ndf_sub = pd.read_csv(PATH+'sample_submission.csv')\nprint('train shape =', df_train.shape)\nprint('test shape =', df_test.shape)\n\noutput_categories = list(df_train.columns[11:])\ninput_categories = list(df_train.columns[[1,2,5]])\nprint('\\noutput categories:\\n\\t', output_categories)\nprint('\\ninput categories:\\n\\t', input_categories)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test only\n#df_train = df_train[0:51]\n#df_test = df_train[0:41]\n#df_sub = df_sub[0:41]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#\ntargets = [\n        'question_asker_intent_understanding',\n        'question_body_critical',\n        'question_conversational',\n        'question_expect_short_answer',\n        'question_fact_seeking',\n        'question_has_commonly_accepted_answer',\n        'question_interestingness_others',\n        'question_interestingness_self',\n        'question_multi_intent',\n        'question_not_really_a_question',\n        'question_opinion_seeking',\n        'question_type_choice',\n        'question_type_compare',\n        'question_type_consequence',\n        'question_type_definition',\n        'question_type_entity',\n        'question_type_instructions',\n        'question_type_procedure',\n        'question_type_reason_explanation',\n        'question_type_spelling',\n        'question_well_written',\n        'answer_helpful',\n        'answer_level_of_information',\n        'answer_plausible',\n        'answer_relevance',\n        'answer_satisfaction',\n        'answer_type_instructions',\n        'answer_type_procedure',\n        'answer_type_reason_explanation',\n        'answer_well_written'    \n    ]\n\ninput_columns = ['question_title','question_body','answer']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#add features sample\n\n#Sentense count in each comment:\nimport string\nimport re\nfrom nltk.corpus import stopwords\nfrom sklearn.preprocessing import StandardScaler\n\neng_stopwords = set(stopwords.words(\"english\"))\n\n\ndef include_window_datas(df_q):\n    out_df = pd.DataFrame()\n    #  '\\n' can be used to count the number of sentences in each comment\n    out_df['count_sent']=df_q[\"question_body\"].apply(lambda x: len(re.findall(\"\\n\",str(x)))+1)\n    #Word count in each comment:\n    out_df['count_word']=df_q[\"question_body\"].apply(lambda x: len(str(x).split()))\n    #Unique word count\n    out_df['count_unique_word']=df_q[\"question_body\"].apply(lambda x: len(set(str(x).split())))\n    #Letter count\n    out_df['count_letters']=df_q[\"question_body\"].apply(lambda x: len(str(x)))\n    #punctuation count\n    out_df[\"count_punctuations\"] =df_q[\"question_body\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n    #upper case words count\n    out_df[\"count_words_upper\"] = df_q[\"question_body\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n    #title case words count\n    out_df[\"count_words_title\"] = df_q[\"question_body\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n    #Number of stopwords\n    out_df[\"count_stopwords\"] = df_q[\"question_body\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n    \n    #Average length of the words \n    #out_df[\"mean_word_len\"] = df_q[\"question_body\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n\n    #answer_count_word\n    out_df['answer_count_word']=df_q[\"answer\"].apply(lambda x: len(str(x).split()))\n    out_df['question_title_count_word']=df_q[\"question_title\"].apply(lambda x: len(str(x).split()))\n\n    \n\n    return out_df\n\n\n#add features \ndf_train_add_features = include_window_datas(df_train)\ndf_test_add_features = include_window_datas(df_test)\n\n#Get df len\nADD_FEATURES_COL_LEN = len(df_train_add_features.columns)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(df_train_add_features.head(5))\n\nprint(\"*\" * 100)\ndisplay(df_test_add_features.head(5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# StandardScaler\nsc = StandardScaler()\ntrain_add_features = sc.fit_transform(df_train_add_features)\ntest_add_features = sc.fit_transform(df_test_add_features)\n\ntrain_add_features[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def _get_masks(tokens, max_seq_length):\n    \"\"\"Mask for padding\"\"\"\n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n\ndef _get_segments(tokens, max_seq_length):\n    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    segments = []\n    first_sep = True\n    current_segment_id = 0\n    for token in tokens:\n        segments.append(current_segment_id)\n        if token == \"[SEP]\":\n            if first_sep:\n                first_sep = False \n            else:\n                current_segment_id = 1\n    return segments + [0] * (max_seq_length - len(tokens))\n\ndef _get_ids(tokens, tokenizer, max_seq_length):\n    \"\"\"Token ids from Tokenizer vocab\"\"\"\n    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n    return input_ids\n\ndef _trim_input(title, question, answer, max_sequence_length, \n                t_max_len=30, q_max_len=239, a_max_len=239):\n\n    t = tokenizer.tokenize(title)\n    q = tokenizer.tokenize(question)\n    a = tokenizer.tokenize(answer)\n    \n    t_len = len(t)\n    q_len = len(q)\n    a_len = len(a)\n\n    if (t_len+q_len+a_len+4) > max_sequence_length:\n        \n        if t_max_len > t_len:\n            t_new_len = t_len\n            a_max_len = a_max_len + floor((t_max_len - t_len)/2)\n            q_max_len = q_max_len + ceil((t_max_len - t_len)/2)\n        else:\n            t_new_len = t_max_len\n      \n        if a_max_len > a_len:\n            a_new_len = a_len \n            q_new_len = q_max_len + (a_max_len - a_len)\n        elif q_max_len > q_len:\n            a_new_len = a_max_len + (q_max_len - q_len)\n            q_new_len = q_len\n        else:\n            a_new_len = a_max_len\n            q_new_len = q_max_len\n            \n            \n        if t_new_len+a_new_len+q_new_len+4 != max_sequence_length:\n            raise ValueError(\"New sequence length should be %d, but is %d\" \n                             % (max_sequence_length, (t_new_len+a_new_len+q_new_len+4)))\n        \n        t = t[:t_new_len]\n        q = q[:q_new_len]\n        a = a[:a_new_len]\n    \n    return t, q, a\n\ndef _convert_to_bert_inputs(title, question, answer, tokenizer, max_sequence_length):\n    \"\"\"Converts tokenized input to ids, masks and segments for BERT\"\"\"\n    \n    stoken = [\"[CLS]\"] + title + [\"[SEP]\"] + question + [\"[SEP]\"] + answer + [\"[SEP]\"]\n\n    input_ids = _get_ids(stoken, tokenizer, max_sequence_length)\n    input_masks = _get_masks(stoken, max_sequence_length)\n    input_segments = _get_segments(stoken, max_sequence_length)\n\n    return [input_ids, input_masks, input_segments]\n\ndef compute_input_arays(df, columns,append_features, tokenizer, max_sequence_length):\n    input_ids, input_masks, input_segments = [], [], []\n    for _, instance in tqdm(df[columns].iterrows()):\n        t, q, a = instance.question_title, instance.question_body, instance.answer\n\n        t, q, a = _trim_input(t, q, a, max_sequence_length)\n\n        ids, masks, segments = _convert_to_bert_inputs(t, q, a, tokenizer, max_sequence_length)\n        input_ids.append(ids)\n        input_masks.append(masks)\n        input_segments.append(segments)\n        \n    #append\n        \n    return [np.asarray(input_ids, dtype=np.int32), \n            np.asarray(input_masks, dtype=np.int32), \n            np.asarray(input_segments, dtype=np.int32),\n            np.asarray(append_features) #append features \n           ]\n\n\ndef compute_output_arrays(df, columns):\n    return np.asarray(df[columns])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_spearmanr(trues, preds):\n    rhos = []\n    for col_trues, col_pred in zip(trues.T, preds.T):\n        rhos.append(\n            spearmanr(col_trues, col_pred + np.random.normal(0, 1e-7, col_pred.shape[0])).correlation)\n    return np.mean(rhos)\n\n\nclass CustomCallback(tf.keras.callbacks.Callback):\n    \n    def __init__(self, valid_data, test_data, batch_size=16, fold=None):\n\n        self.valid_inputs = valid_data[0]\n        self.valid_outputs = valid_data[1]\n        self.test_inputs = test_data\n        \n        self.batch_size = batch_size\n        self.fold = fold\n        \n    def on_train_begin(self, logs={}):\n        self.valid_predictions = []\n        self.test_predictions = []\n        \n    def on_epoch_end(self, epoch, logs={}):\n        self.valid_predictions.append(\n            self.model.predict(self.valid_inputs, batch_size=self.batch_size))\n        \n        rho_val = compute_spearmanr(\n            self.valid_outputs, np.average(self.valid_predictions, axis=0))\n        \n        print(\"\\nvalidation rho: %.4f\" % rho_val)\n        \n        if self.fold is not None:\n            self.model.save_weights(f'bert-base-{fold}-{epoch}.h5py')\n        \n        self.test_predictions.append(\n            self.model.predict(self.test_inputs, batch_size=self.batch_size)\n        )\n\ndef bert_model():\n    \n    input_word_ids = tf.keras.layers.Input(\n        (MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_word_ids')\n    input_masks = tf.keras.layers.Input(\n        (MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_masks')\n    input_segments = tf.keras.layers.Input(\n        (MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_segments')\n    \n    \n    #Add Features\n    inps = tf.keras.layers.Input(shape=(ADD_FEATURES_COL_LEN,),name='input_features')\n    x2 = tf.keras.layers.Dense(512, activation='elu', name='features_dense')(inps)\n    x2 = tf.keras.layers.Dropout(0.2, name='dropout_dense')(x2)\n    \n    \n    \n    bert_layer = hub.KerasLayer(BERT_PATH, trainable=True)\n    \n    _, x = bert_layer([input_word_ids, input_masks, input_segments])\n    \n    #x = tf.keras.layers.GlobalAveragePooling1D()(sequence_output)\n    x = tf.keras.layers.Lambda(lambda x: x[:, 0])(x)\n    x = tf.keras.layers.Dropout(0.2)(x)\n    \n    # bert + Features\n    combined = tf.keras.layers.concatenate([x, x2])    \n    \n    out = tf.keras.layers.Dense(30, activation=\"sigmoid\", name=\"dense_output\")(combined)\n    #out = tf.keras.layers.Dense(30, activation=\"sigmoid\", name=\"dense_output\")(x)\n\n    model = tf.keras.models.Model(\n        inputs=[input_word_ids, input_masks, input_segments, inps], outputs=out)\n    \n    return model    \n        \ndef train_and_predict(model, train_data, valid_data, test_data, \n                      learning_rate, epochs, batch_size, loss_function, fold):\n        \n    custom_callback = CustomCallback(\n        valid_data=(valid_data[0], valid_data[1]), \n        test_data=test_data,\n        batch_size=batch_size,\n        fold=None)\n\n    #print(train_data[0])\n    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n    model.compile(loss=loss_function, optimizer=optimizer)\n    model.fit(train_data[0], train_data[1],  epochs=epochs, \n              batch_size=batch_size, callbacks=[custom_callback])\n    \n    return custom_callback\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gkf = GroupKFold(n_splits=5).split(X=df_train.question_body, groups=df_train.question_body) ############## originaln_splits=5\n\noutputs = compute_output_arrays(df_train, output_categories)\ninputs = compute_input_arays(df_train, input_categories, train_add_features,tokenizer, MAX_SEQUENCE_LENGTH)\ntest_inputs = compute_input_arays(df_test, input_categories, test_add_features,tokenizer, MAX_SEQUENCE_LENGTH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"histories = []\ncount = 1\nfor fold, (train_idx, valid_idx) in enumerate(gkf):\n    #print(str(count) + \"回目のループ\")\n    #print(train_idx)\n  \n    # will actually only do 3 folds (out of 5) to manage < 2h\n    if fold < 3:\n        K.clear_session()\n        model = bert_model()\n\n\n        \n        train_inputs = [inputs[i][train_idx] for i in range(len(inputs))]\n   \n        train_outputs = outputs[train_idx]\n\n        valid_inputs = [inputs[i][valid_idx] for i in range(len(test_inputs))]\n        valid_outputs = outputs[valid_idx]\n\n        \n        \n        \n        model.summary()\n        name = \"model_\" + str(count) + \".png\"\n        tf.keras.utils.plot_model(model, to_file=name)\n        count +=1\n        \n        # history contains two lists of valid and test preds respectively:\n        #  [valid_predictions_{fold}, test_predictions_{fold}]\n        # Largeでbatch_sizeを8だとメモリ不足になるため6に調整 epochsを5から３にした→元の戻した\n        history = train_and_predict(model, \n                          train_data=(train_inputs, train_outputs), \n                          valid_data=(valid_inputs, valid_outputs),\n                          test_data=test_inputs, \n                          #learning_rateから「1e-5」→「 3e-5」\n                          learning_rate=3e-5, epochs=5, batch_size=8,\n                          loss_function='binary_crossentropy', fold=fold)\n        #epochs 5→1   batch_size 8→16\n        \n\n        histories.append(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_predictions = [histories[i].test_predictions for i in range(len(histories))]\ntest_predictions = [np.average(test_predictions[i], axis=0) for i in range(len(test_predictions))]\ntest_predictions = np.mean(test_predictions, axis=0)\n\ndf_sub.iloc[:, 1:] = test_predictions\n\ndf_sub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"nbformat":4,"nbformat_minor":1}