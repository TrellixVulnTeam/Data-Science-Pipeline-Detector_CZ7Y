{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np \nimport matplotlib\nimport matplotlib.pyplot as plt \nimport seaborn as sns\ncolor = sns.color_palette()\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.offline as offline\noffline.init_notebook_mode()\nimport cufflinks as cf\ncf.go_offline()\nfrom matplotlib_venn import venn2\nimport re\nimport nltk\nfrom nltk.probability import FreqDist\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nimport string\neng_stopwords = stopwords.words('english')\nimport gc\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\n\nimport os","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(os.listdir(\"../input/google-quest-challenge\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = pd.read_csv('../input/google-quest-challenge/train.csv')\ntest_data = pd.read_csv('../input/google-quest-challenge/test.csv')\nsample_submission = pd.read_csv('../input/google-quest-challenge/sample_submission.csv')\n\nprint('Size of train_data', train_data.shape)\nprint('Size of test_data', test_data.shape)\nprint('Size of sample_submission', sample_submission.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()\ntrain_data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.head()\ntest_data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Target variables\ntargets = list(sample_submission.columns[1:])\nprint(targets)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Statistical overview of the Data\ntrain_data[targets].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking missing data for train_data\ntotal = train_data.isnull().sum().sort_values(ascending=False)\npercent = (train_data.isnull().sum() / train_data.isnull().count()*100).sort_values(ascending=False)\nmissing_train_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking missing data for test_data\ntotal = test_data.isnull().sum().sort_values(ascending=False)\npercent = (test_data.isnull().sum() / test_data.isnull().count()*100).sort_values(ascending=False)\nmissing_test_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_test_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Exploration"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data[\"host\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of Host(from which website Question & Answers collected)\ntemp = train_data[\"host\"].value_counts()\ndf = pd.DataFrame({'labels': temp.index, 'values': temp.values})\ndf.iplot(kind='pie', labels='labels', values='values', title='Distribution of hosts in Training data')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = test_data[\"host\"].value_counts()\nprint(\"Total number of states : \",len(temp))\ndf = pd.DataFrame({'labels': temp.index,'values': temp.values})\ndf.iplot(kind='pie',labels='labels',values='values', title='Distribution of hosts in test data')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Distribution of categories"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train data\ntemp = train_data[\"category\"].value_counts()\ntrace = go.Bar(\n    x = temp.index,\n    y = (temp / temp.sum())*100,\n)\ndata = [trace]\nlayout = go.Layout(\n    title = \"Distribution of categories in training data in % \",\n    xaxis=dict(\n        title='category',\n        tickfont=dict(\n            size=14,\n            color='rgb(107, 107, 107)'\n        )\n    ),\n    yaxis=dict(\n        title='Count in %',\n        titlefont=dict(\n            size=16,\n            color='rgb(107, 107, 107)'\n        ),\n        tickfont=dict(\n            size=14,\n            color='rgb(107, 107, 107)'\n        )\n)\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='test')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test data\ntemp = test_data[\"category\"].value_counts()\ntrace = go.Bar(\n    x = temp.index,\n    y = (temp / temp.sum())*100,\n)\ndata = [trace]\nlayout = go.Layout(\n    title = \"Distribution of categories in test data in % \",\n    xaxis=dict(\n        title='category',\n        tickfont=dict(\n            size=14,\n            color='rgb(107, 107, 107)'\n        )\n    ),\n    yaxis=dict(\n        title='Count in %',\n        titlefont=dict(\n            size=16,\n            color='rgb(107, 107, 107)'\n        ),\n        tickfont=dict(\n            size=14,\n            color='rgb(107, 107, 107)'\n        )\n)\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='test')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of Target variables\nfig, axes = plt.subplots(6, 5, figsize=(18, 15))\naxes = axes.ravel()\nbins = np.linspace(0, 1, 20)\n\nfor i, col in enumerate(targets):\n\tax = axes[i]\n\tsns.distplot(train_data[col], label=col, kde=False, bins=bins, ax=ax)\n\tax.set_xlim([0, 1])\n\tax.set_ylim([0, 6079])\nplt.tight_layout()\nplt.show()\nplt.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Venn Diagram(Common Features values in training and test data)\nplt.figure(figsize=(23, 13))\nplt.subplot(321)\n\nvenn2([set(train_data.question_user_name.unique()), set(test_data.question_user_name.unique())], set_labels=('Train set', 'Test set'))\nplt.title(\"Common question_user_name in training and test data\", fontsize=15)\n\nplt.subplot(322)\nvenn2([set(train_data.answer_user_name.unique()), set(test_data.answer_user_name.unique())], set_labels=('Train set', 'Test set'))\nplt.title(\"Common answer_user_name in training and test data\", fontsize=15)\n\nplt.subplot(323)\nvenn2([set(train_data.question_title.unique()), set(test_data.question_title.unique())], set_labels = ('Train set', 'Test set') )\nplt.title(\"Common question_title in training and test data\", fontsize=15)\n\nplt.subplot(324)\nvenn2([set(train_data.question_user_name.unique()), set(train_data.answer_user_name.unique())], set_labels = ('Train set', 'Test set') )\nplt.title(\"Common users in both question & answer in train data\", fontsize=15)\n\nplt.subplot(325)\nvenn2([set(test_data.question_user_name.unique()), set(test_data.answer_user_name.unique())], set_labels = ('Train set', 'Test set') )\nplt.title(\"Common users in both question & answer in test data\", fontsize=15)\n\nplt.subplots_adjust(wspace=0.5, hspace=0.5, top=0.9)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution for Question Title\ntrain_question_title = train_data['question_title'].str.len()\ntest_question_title = test_data['question_title'].str.len()\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 6))\nsns.distplot(train_question_title, ax=ax1, color='blue')\nsns.distplot(test_question_title, ax=ax2, color='green')\nax2.set_title('Distribution for Question Title in test data')\nax1.set_title('Distribution for Question Title in Training data')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution for Question body\ntrain_question_title = train_data['question_body'].str.len()\ntest_question_title = test_data['question_body'].str.len()\n\nfig,(ax1,ax2) = plt.subplots(1, 2, figsize=(10,6))\nsns.distplot(train_question_title, ax=ax1, color='blue')\nsns.distplot(test_question_title, ax=ax2, color='green')\nax2.set_title('Distribution for Question Body in test data')\nax1.set_title('Distribution for Question Body in Training data')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution for Answers\ntrain_question_title = train_data['answer'].str.len()\ntest_question_title = test_data['answer'].str.len()\n\nfig,(ax1,ax2) = plt.subplots(1, 2, figsize=(10,6))\nsns.distplot(train_question_title, ax=ax1, color='blue')\nsns.distplot(test_question_title, ax=ax2, color='green')\nax2.set_title('Distribution for Answers in test data')\nax1.set_title('Distribution for Answers in Training data')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Duplicate Questions Title\nprint(\"Number of duplicate questions in descending order\")\nprint(\"------------------------------------------------------\")\ntrain_data.groupby('question_title').count()['qa_id'].sort_values(ascending = False).head(25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Most popular Questions\ntrain_data[train_data['question_title'] == 'What is the best introductory Bayesian statistics textbook?']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preparation & Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"puncts = [\n',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£',\n'·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', '\\xa0', '\\t',\n'“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '\\u3000', '\\u202f',\n'▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '«',\n'∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√'\n]\n\nmisspell_dict = {\n\"aren't\" : \"are not\",\n\"can't\" : \"cannot\",\n\"couldn't\" : \"could not\",\n\"couldnt\" : \"could not\",\n\"didn't\" : \"did not\",\n\"doesn't\" : \"does not\",\n\"doesnt\" : \"does not\",\n\"don't\" : \"do not\",\n\"hadn't\" : \"had not\",\n\"hasn't\" : \"has not\",\n\"haven't\" : \"have not\",\n\"havent\" : \"have not\",\n\"he'd\" : \"he would\",\n\"he'll\" : \"he will\",\n\"he's\" : \"he is\",\n\"i'd\" : \"I would\",\n\"i'd\" : \"I had\",\n\"i'll\" : \"I will\",\n\"i'm\" : \"I am\",\n\"isn't\" : \"is not\",\n\"it's\" : \"it is\",\n\"it'll\":\"it will\",\n\"i've\" : \"I have\",\n\"let's\" : \"let us\",\n\"mightn't\" : \"might not\",\n\"mustn't\" : \"must not\",\n\"shan't\" : \"shall not\",\n\"she'd\" : \"she would\",\n\"she'll\" : \"she will\",\n\"she's\" : \"she is\",\n\"shouldn't\" : \"should not\",\n\"shouldnt\" : \"should not\",\n\"that's\" : \"that is\",\n\"thats\" : \"that is\",\n\"there's\" : \"there is\",\n\"theres\" : \"there is\",\n\"they'd\" : \"they would\",\n\"they'll\" : \"they will\",\n\"they're\" : \"they are\",\n\"theyre\":  \"they are\",\n\"they've\" : \"they have\",\n\"we'd\" : \"we would\",\n\"we're\" : \"we are\",\n\"weren't\" : \"were not\",\n\"we've\" : \"we have\",\n\"what'll\" : \"what will\",\n\"what're\" : \"what are\",\n\"what's\" : \"what is\",\n\"what've\" : \"what have\",\n\"where's\" : \"where is\",\n\"who'd\" : \"who would\",\n\"who'll\" : \"who will\",\n\"who're\" : \"who are\",\n\"who's\" : \"who is\",\n\"who've\" : \"who have\",\n\"won't\" : \"will not\",\n\"wouldn't\" : \"would not\",\n\"you'd\" : \"you would\",\n\"you'll\" : \"you will\",\n\"you're\" : \"you are\",\n\"you've\" : \"you have\",\n\"'re\": \" are\",\n\"wasn't\": \"was not\",\n\"we'll\":\" will\",\n\"didn't\": \"did not\",\n\"tryin'\":\"trying\"\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text(text):\n\t# take the txt only no symbols[how are you?? => how are you]\n\ttext = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n\t# ['how', 'are', 'you']\n\ttext = text.lower().split()\n\tstops = set(stopwords.words(\"english\"))\n\ttext = [word for word in text if not word in stops]\n\ttext = \" \".join(text)\n\treturn(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def _get_misspell(misspell_dict):\n\t# target -> find all misspell_dict.keys() from a given txt\n\tmisspell_re = re.compile('(%s)' % '|'.join(misspell_dict.keys()))\n\t# misspell_re => re.compile(\"(she'd|shouldn't|haven't|shouldnt|theres|hadn't|what're|who's|it's|she'll|weren't|\n\t# \t\t\t\t you've|i'm|where's|that's|he'd|don't|they've|there's|what've|i'd|who'll|you're|can't|it'll|mustn't|he'll|who'd|i've|')  \n\treturn misspell_dict, misspell_re","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def replace_typical_misspell(text):\n    mispellings, mispellings_re = _get_misspell(misspell_dict)\n\n    def replace(match):\n        return mispellings[match.group(0)]\n\n    return mispellings_re.sub(replace, text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_data(df, columns: list):\n    for col in columns:\n        df[col] = df[col].apply(lambda x: clean_text(x.lower()))\n        df[col] = df[col].apply(lambda x: replace_typical_misspell(x))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = ['question_title','question_body','answer']\ntrain_data = clean_data(train_data, columns)\ntest_data = clean_data(test_data, columns)\nprint('Done cleaning done!!')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Word frequency"},{"metadata":{"trusted":true},"cell_type":"code","source":"# training data\nfreq_dist = FreqDist([word for text in train_data['question_title'] for word in text.split()])\n\nplt.figure(figsize=(20, 7))\nplt.title('Word frequency on question title (Training Data)').set_fontsize(25)\nplt.xlabel('').set_fontsize(25)\nplt.ylabel('').set_fontsize(25)\nfreq_dist.plot(60,cumulative=False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test data\nfreq_dist = FreqDist([word for text in test_data['question_title'] for word in text.split()])\n\nplt.figure(figsize=(20, 7))\nplt.title('Word frequency on question title (Test Data)').set_fontsize(25)\nplt.xlabel('').set_fontsize(25)\nplt.ylabel('').set_fontsize(25)\nfreq_dist.plot(60,cumulative=False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# training data\nfreq_dist = FreqDist([word for text in train_data['question_body'].str.replace('[^a-za-z0-9^,!.\\/+-=]',' ') for word in text.split()])\n\nplt.figure(figsize=(20, 7))\nplt.title('Word frequency on question body (Training Data)').set_fontsize(25)\nplt.xlabel('').set_fontsize(25)\nplt.ylabel('').set_fontsize(25)\nfreq_dist.plot(60,cumulative=False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test data\nfreq_dist = FreqDist([word for text in test_data['question_body'] for word in text.split()])\n\nplt.figure(figsize=(20, 7))\nplt.title('Word frequency on question body (Test Data)').set_fontsize(25)\nplt.xlabel('').set_fontsize(25)\nplt.ylabel('').set_fontsize(25)\nfreq_dist.plot(60,cumulative=False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of characters in the text\ntrain_data[\"question_title_num_chars\"] = train_data[\"question_title\"].apply(lambda x: len(str(x)))\ntrain_data[\"question_body_num_chars\"] = train_data[\"question_body\"].apply(lambda x: len(str(x)))\ntrain_data[\"answer_num_chars\"] = train_data[\"answer\"].apply(lambda x: len(str(x)))\n\ntest_data[\"question_title_num_chars\"] = test_data[\"question_title\"].apply(lambda x: len(str(x)))\ntest_data[\"question_body_num_chars\"] = test_data[\"question_body\"].apply(lambda x: len(str(x)))\ntest_data[\"answer_num_chars\"] = test_data[\"answer\"].apply(lambda x: len(str(x)))\n\n# Number of words in the text\ntrain_data[\"question_title_num_words\"] = train_data[\"question_title\"].apply(lambda x: len(str(x).split()))\ntrain_data[\"question_body_num_words\"] = train_data[\"question_body\"].apply(lambda x: len(str(x).split()))\ntrain_data[\"answer_num_words\"] = train_data[\"answer\"].apply(lambda x: len(str(x).split()))\n\ntest_data[\"question_title_num_words\"] = test_data[\"question_title\"].apply(lambda x: len(str(x).split()))\ntest_data[\"question_body_num_words\"] = test_data[\"question_body\"].apply(lambda x: len(str(x).split()))\ntest_data[\"answer_num_words\"] = test_data[\"answer\"].apply(lambda x: len(str(x).split()))\n\n# Number of unique words in the text\ntrain_data[\"question_title_num_unique_words\"] = train_data[\"question_title\"].apply(lambda x: len(set(str(x).split())))\ntrain_data[\"question_body_num_unique_words\"] = train_data[\"question_body\"].apply(lambda x: len(set(str(x).split())))\ntrain_data[\"answer_num_unique_words\"] = train_data[\"answer\"].apply(lambda x: len(set(str(x).split())))\n\ntest_data[\"question_title_num_unique_words\"] = test_data[\"question_title\"].apply(lambda x: len(set(str(x).split())))\ntest_data[\"question_body_num_unique_words\"] = test_data[\"question_body\"].apply(lambda x: len(set(str(x).split())))\ntest_data[\"answer_num_unique_words\"] = test_data[\"answer\"].apply(lambda x: len(set(str(x).split())))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TF-IDF Features\ntfidf = TfidfVectorizer(ngram_range=(1, 3))\ntsvd = TruncatedSVD(n_components=128, n_iter=5)\n\ntfquestion_title = tfidf.fit_transform(train_data[\"question_title\"].values)\ntfquestion_title_test = tfidf.transform(test_data[\"question_title\"].values)\n\ntfquestion_title = tsvd.fit_transform(tfquestion_title)\ntfquestion_title_test = tsvd.fit_transform(tfquestion_title_test)\n\ntfquestion_body = tfidf.fit_transform(train_data[\"question_body\"].values)\ntfquestion_body_test = tfidf.transform(test_data[\"question_body\"].values)\n\ntfquestion_body = tsvd.fit_transform(tfquestion_body)\ntfquestion_body_test = tsvd.transform(tfquestion_body_test)\n\ntfanswer = tfidf.fit_transform(train_data[\"answer\"].values)\ntfanswer_test = tfidf.transform(test_data[\"answer\"].values)\n\ntfanswer = tsvd.fit_transform(tfanswer)\ntfanswer_test = tsvd.transform(tfanswer_test)\n\ntrain_data[\"tfquestion_title\"] = list(tfquestion_title)\ntest_data[\"tfquestion_title_test\"] = list(tfquestion_title_test)\n\ntrain_data[\"tfquestion_body\"] = list(tfquestion_body)\ntest_data[\"tfquestion_body_test\"] = list(tfquestion_body_test)\n\ntrain_data[\"tfanswer\"] = list(tfanswer)\ntest_data[\"tfanswer_test\"] = list(tfanswer_test)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}