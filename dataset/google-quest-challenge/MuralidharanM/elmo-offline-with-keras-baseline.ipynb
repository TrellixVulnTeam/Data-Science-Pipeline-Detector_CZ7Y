{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# to be run the first time\n#from google.colab import drive\n#drive.mount('/content/drive')\n#!pip install --upgrade tensorflow\n\n#before restarting comment above\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\n#os.chdir('drive/My Drive/')\n#os.chdir('kaggle/google_quest')\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport datetime\nimport keras.backend as K\n\n#from tensorflow.keras.layers import Layer\n#from keras.models import \n#from keras.layers import Dense, Embedding, LSTM, Bidirectional\n\nimport h5py\n\nprint(\"Version: \", tf.__version__)\nprint(\"Eager mode: \", tf.executing_eagerly())\nprint(\"Hub version: \", hub.__version__)\nprint(\"GPU is\", \"available\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/google-quest-challenge/train.csv\")\ntest = pd.read_csv(\"../input/google-quest-challenge/test.csv\")\nsub = pd.read_csv(\"../input/google-quest-challenge/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_maxlen = 100\n\ndef gen_features(df,_maxlen):\n    \"\"\"\n  dataset contains question_body & answer columns\n  pads each of these columns to the maxlen provided\n  concatenates qn and ans into one sectence. qn marked with \"__qn__\", answer marked with \"__ans__\"\n  \"\"\"\n    qn = df['question_body'].tolist()\n    qn = [' '.join(t.split()[0:np.int(_maxlen/2)-1]) for t in qn]\n    #qn = tf.cast(qn,tf.string)#cast helps convert string into tensor string.\n    \n    ans = df['answer'].tolist()\n    ans = [' '.join(t.split()[0:np.int(_maxlen/2)-1]) for t in ans]\n    #qn = tf.cast(ans,tf.string)#cast helps convert string into tensor string.\n\n  \n  #[ \" __qn__ \"+ q + \" __answer__ \" + a for q,a in (zip(qn[:3],ans[:3]))] [\" __qn__ After playing around with macro photography \n  #X_list = tf.cast(X_list_,tf.string) #cast helps convert string into tensor string.\n    return [ \" __qn__ \"+ q + \" __answer__ \" + a for q,a in (zip(qn,ans))]\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls ../input/elmo-v3-fromtfhub/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"elmo_model = hub.load(\"../input/elmo-v3-fromtfhub\")\n\n#elmo_model([\"the cat is on the mat\", \"dogs are in the fog\"], signature= \"default\")\n# the above will throw an auto-trackable error. this is problem due to loading tf1 hub in tf 2. \n# notes that help to solve the problem: https://www.tensorflow.org/hub/common_issues\n\nelmo_model.signatures['default'](tf.cast([\"the cat is on the mat\", \"dogs are in the fog\"],tf.string))['elmo'].numpy().shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def gen_elmo_embedding(x,store=True):\n  # elmo is still available at tf 2.0, need to use hub.kerasLayer which works in tf 1.15\n  # ref: https://colab.research.google.com/gist/gowthamkpr/f01a548c4faa4088e476c727f693091b/untitled235.ipynb\n    elmo_model = hub.load(\"../input/elmo-v3-fromtfhub\")\n    #elmo_model = hub.KerasLayer(\"https://tfhub.dev/google/elmo/3\", trainable=True, signature=\"default\",output_key='elmo')\n  #print(elmo_model(tf.cast(qn[:2],tf.string)))\n    chunk_size=100\n    dim = 1024 #elmo vector dimension\n    \n    elmo_embedding= np.zeros((1, _maxlen,dim))\n    for i in np.arange(0,len(x),chunk_size):\n          #if i < 3:\n        e = elmo_model.signatures['default'](tf.cast(x[i:i+chunk_size],tf.string))['elmo'].numpy()\n        elmo_embedding = np.vstack((elmo_embedding,e))\n\n    if store:\n        print(\"embed layer stored\")\n        with h5py.File('../input/elmo_100_maxlen.h5', 'w') as hf:\n            hf.create_dataset(\"elmo_100_maxlen_npy\",  data=elmo_embedding)\n    return elmo_embedding[1:]\n  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Run this below cell only the first time. From second time, load directly from h5"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"X_list = gen_features(df=train,_maxlen=_maxlen)\ntrain_elmo = gen_elmo_embedding(X_list,store=True)\ny_list = np.asarray(train.iloc[:,11:]) #converting columns to numpy array"},{"metadata":{"trusted":true},"cell_type":"code","source":"with h5py.File('../input/elmo_100_maxlen.h5', 'r') as hf:\n    train_elmo = hf['elmo_100_maxlen_npy'][:]\ny_list = np.asarray(train.iloc[:,11:]) #converting columns to numpy array","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.layers import Dense, Dropout,Embedding, LSTM, Bidirectional, Input, Dropout\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.preprocessing import sequence\n\ndef build_model():\n    input_ = Input(shape=(_maxlen,1024),name = 'qn')\n    X= Bidirectional(LSTM(50, activation='relu'), name='bLSTM')(input_) #Bilstm seems to increasing the loss exponentially. lstm requires 3 dim vectors and it converts into 2 dim vectors\n    #X= LSTM(100, activation='relu',dropout=0.2, name='LSTM')(input_) #lstm requires 3 dim vectors and it converts into 2 dim vectors\n    X= Dense(100, activation='relu')(X) # the above matrix with 300 units will be sparse. Convert to dense matrix of 100 hidden units\n    output_= Dense(30, activation='sigmoid', name='output')(X)\n\n    model = Model(input_,output_)\n    model.summary()\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = build_model()\nmodel.compile(optimizer = \"adam\",loss = \"binary_crossentropy\")\nhistory = model.fit(train_elmo[1:],y_list,epochs=10,batch_size = 100,validation_split=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = gen_features(df=test,_maxlen=_maxlen)\ntest_elmo =  gen_elmo_embedding(X_test,store=False)\ntest_preds = model.predict(test_elmo)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.iloc[:, 1:] = test_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}