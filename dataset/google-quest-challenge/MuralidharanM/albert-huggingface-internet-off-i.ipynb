{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Acknowledgements\nDerived from kernel: https://www.kaggle.com/akensert/bert-base-tf2-0-minimalistic"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import sys\nsys.path.insert(0, \"../input/transformers/transformers-master/\")\n!pip install ../input/sacremoses/sacremoses-master/ > /dev/null\n\nfrom transformers import *\n\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_hub as hub\nprint(tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Method 1"},{"metadata":{},"cell_type":"markdown","source":"To make Huggingface models work in kaggle with Internet Off\n1. Go to Huggingface - get link from https://github.com/huggingface/transformers#model-architectures\n2. from albert link: https://github.com/google-research/ALBERT, go to tfhub link. Do not download the tar file. Its useless for our purpose\n3. download the tarfile from tfhub. in this case, 2.tar.gz\n4. load the dataset into kaggle. already loaded - feel free to use albert-base-v2\n5. for tokenizer, go to the https://github.com/google-research/ALBERT and download tokenization.py\n6. in kaggle notebook, add it to file-> add utility script. Now you can import them as it is.\n7. if the above does not work - add it to dataset and use sys.path.append('../input/tokenization') as shown below\n8. to make the hub layer work, since hub is using tf 1X version. Use hub.load. For more details look at https://www.tensorflow.org/hub/common_issues\n\nhttps://github.com/huggingface/transformers/blob/master/src/transformers/tokenization_albert.py"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#import sys\n#sys.path.append(\"../input/albert-tokenization/\")\n#import albert_tokenization\n\n#BERT_PATH = '../input/albert-base-v2'\n#vocab file has <unk> and <pad> while tokenizer outputs [Unk] and [PAD]\n#tokenizer = albert_tokenization.FullTokenizer(BERT_PATH+'/30k-clean-Unkfix.vocab', True)\n\n# the below will throw an auto-trackable error if we use hub.module. this is problem due to loading tf1 hub in tf 2. \n# notes that help to solve the problem: https://www.tensorflow.org/hub/common_issues\n#albert_model = hub.load(BERT_PATH+'/2')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Method 2"},{"metadata":{},"cell_type":"markdown","source":"1. Go to the github page of huggingface transformers - source - link -https://github.com/huggingface/transformers/tree/master/src/transformers  \n2. Open the relevant config, modeling or tokenization. In this case, for example it will be configuration_albert.py\n3. Under Config_Archive_Map list, you will find aws source its downloading the required files\n4. You will require, one config.json, spiece.model (if sentence piece is being used) or vocab.txt/vocab and tf_model.h5 (for keras)\n4. Voila ! download file and add it as a dataset to your kernel. \n\nYou can use the dataset - https://www.kaggle.com/stitch/albertlargev2huggingface"},{"metadata":{"trusted":true},"cell_type":"code","source":"albert_path = '../input/albertlargev2huggingface/'\ntokenizer = AlbertTokenizer.from_pretrained(albert_path, do_lower_case=True)\nalbert_model = TFAlbertModel.from_pretrained(albert_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/google-quest-challenge/train.csv\")\ntest = pd.read_csv(\"../input/google-quest-challenge/test.csv\")\nsub = pd.read_csv(\"../input/google-quest-challenge/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def _get_segments(sentences):\n    sentences_segments = []\n    for sent in sentences:\n      temp = []\n      i = 0\n      for token in sent.split(\" \"):\n        temp.append(i)\n        if token == \"[SEP]\":\n          i += 1\n      sentences_segments.append(temp)\n    return sentences_segments\n\ndef _get_inputs(df,_maxlen,tokenizer,use_keras_pad=False):\n\n    maxqnans = np.int((_maxlen-20)/2)\n    pattern = '[^\\w\\s]+|\\n' # remove everything including newline (|\\n) other than words (\\w) or spaces (\\s)\n    \n    sentences = [\"[CLS] \" + \" \".join(tokenizer.tokenize(qn)[:maxqnans]) +\" [SEP] \" \n              + \" \".join(tokenizer.tokenize(ans)[:maxqnans]) +\" [SEP] \" \n              + \" \".join(tokenizer.tokenize(title)[:10]) + \" [SEP] \"\n              + \" \".join(tokenizer.tokenize(cat)[:10]) +\" [SEP]\" \n                for (title,qn,ans,cat) \n                in \n              zip(df['question_title'].str.replace(pattern, '').values.tolist(),\n              df['question_body'].str.replace(pattern, '').values.tolist(),\n              df['answer'].str.replace(pattern, '').values.tolist(),\n              df['category'].str.replace(pattern, '').values.tolist())]\n              #train.head()[['question_title','question_body','answer','category']].values.tolist()]\n    \n\n    #generate masks\n    # bert requires a mask for the words which are padded. \n    # Say for example, maxlen is 100, sentence size is 90. then, [1]*90 + [0]*[100-90]\n    sentences_mask = [[1]*len(sent.split(\" \"))+[0]*(_maxlen - len(sent.split(\" \"))) for sent in sentences]\n \n    #generate input ids  \n    # if less than max length provided then the words are padded\n    if use_keras_pad:\n      sentences_padded = pad_sequences(sentences.split(\" \"), dtype=object, maxlen=10, value='[PAD]',padding='post')\n    else:\n      sentences_padded = [sent + \" [PAD]\"*(_maxlen-len(sent.split(\" \"))) if len(sent.split(\" \"))!=_maxlen else sent for sent in sentences ]\n    \n    #print([s.split(\" \") for s in sentences_padded])\n    sentences_converted = [tokenizer.convert_tokens_to_ids(s.split(\" \")) for s in sentences_padded]\n    \n    #generate segments\n    # for each separation [SEP], a new segment is converted\n    sentences_segment = _get_segments(sentences_padded)\n\n    genLength = set([len(sent.split(\" \")) for sent in sentences_padded])\n    if _maxlen < 20:\n      raise Exception(\"max length cannot be less than 20\")\n    elif len(genLength)!=1: \n      print(genLength)\n      raise Exception(\"sentences are not of same size\")\n\n\n    #convert list into tensor integer arrays and return it\n    #return sentences_converted,sentences_segment, sentences_mask\n    return [np.asarray(sentences_converted, dtype=np.int32), \n           np.asarray(sentences_mask, dtype=np.int32),\n        np.asarray(sentences_segment, dtype=np.int32)]\n    #return [tf.cast(sentences_converted,tf.int32), tf.cast(sentences_segment,tf.int32), tf.cast(sentences_mask,tf.int32)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"maxlen = 200\nXtr = _get_inputs(df=train,tokenizer=tokenizer,_maxlen=maxlen)\nytr = np.asarray(train.iloc[:,11:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xte = _get_inputs(df=test,_maxlen=maxlen, tokenizer = tokenizer )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.layers import Dense, Dropout,Embedding, LSTM, Bidirectional, Input, Dropout, GlobalAveragePooling1D, LeakyReLU\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.preprocessing import sequence\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras import regularizers\nfrom sklearn.model_selection import KFold\nfrom scipy.stats import spearmanr\nimport tensorflow.keras.backend as K\n\nimport warnings; warnings.simplefilter('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#method1: using tfhub - didnt work\n#albert_inputs = dict(\n#    input_ids=Xtr[0],\n#    input_mask=Xtr[1],\n#    segment_ids=Xtr[2])\n\n#albert_model.signatures['tokens']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model():\n    token_inputs = Input((maxlen), dtype=tf.int32, name='input_word_ids')\n\n    X = albert_model(token_inputs)[1] #pooled output\n    #X = GlobalAveragePooling1D()(X)\n    #X = Dense(100, activation='relu',kernel_regularizer=regularizers.l2(0.01))(pooled_output)\n    #X = LeakyReLU(alpha=0.01)(X)\n    X = Dropout(0.2)(X)\n    output_= Dense(30, activation='sigmoid', name='output')(X)\n\n    bert_model2 = Model(token_inputs,output_)\n    print(bert_model2.summary())\n    \n    bert_model2.compile(optimizer=Adam(learning_rate=0.000001), loss='binary_crossentropy')\n    \n    return bert_model2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Albert - all parameters in the model summary are shown to be trainable. So finetuning is possible.\n### Albert only requires input ids. Mask & Segments are not required.\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Rho_Calculator(tf.keras.callbacks.Callback):\n    \n    ## Not predicting on test data for each epoch. Its a bit of overkill and slows down the epoch completion\n    \n    def __init__(self, valid_data, batch_size=16, fold=None):\n\n        self.valid_inputs = valid_data[0]\n        self.valid_outputs = valid_data[1]\n       \n        self.batch_size = batch_size\n        self.fold = fold\n        \n    def on_train_begin(self, logs={}):\n        self.valid_predictions = []\n        \n    def on_epoch_end(self, epoch, logs={}):\n        pred_output = self.model.predict(self.valid_inputs, batch_size=self.batch_size)\n        \n        self.valid_predictions.append(pred_output)\n        \n        pred_ = pd.DataFrame(pred_output)\n        val_ = pd.DataFrame(self.valid_outputs)\n        # take each column at a time. carry out correlation. average correlation for all 30 columns ignoring nan values\n        rho_val = np.nanmean([spearmanr(val_.iloc[:,i].values,pred_.iloc[:,i].values).correlation for i in np.arange(len(pred_.columns))])\n        \n        #rho_val = compute_spearmanr( self.valid_outputs, np.average(self.valid_predictions, axis=0))\n        \n        print(\"\\nvalidation rho: %.4f\" % rho_val)\n        \n        if self.fold is not None:\n            self.model.save_weights(f'bert-base-{fold}-{epoch}.h5py')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"i=0\nnum_folds = 3\nkfold = KFold(n_splits=num_folds, shuffle=True, random_state=1)\nfold_score = []\n#test_preds = np.zeros((Xte[0].shape[0],ytr.shape[1])) # mimic rows shape of test data, columns shape from train since, test will have any column for outputs\ntest_preds = []\nfor train_index,val_index in kfold.split(ytr):\n    K.clear_session()\n    #print(train_index)\n    #print('\\n')\n    #print(val_index)\n    i= i+1\n    print('executing fold no: {}'.format(i))\n    \n    # train_index gets a random sample of rows for training\n    # Xtr is a list contains 3 np arrays - ids, masks, segments so, using list comprehension to get the splits\n    Xtr_fold = [arr[train_index] for arr in Xtr]\n    ytr_fold = ytr[train_index]\n    \n    Xtr_val = [arr[val_index] for arr in Xtr]\n    ytr_val = ytr[val_index]\n    \n    model = build_model()\n    rho = Rho_Calculator(valid_data=(Xtr_val[0], ytr_val),batch_size=8,fold=None)\n    es = EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=True)\n    model.fit(Xtr_fold[0],ytr_fold,epochs=10,batch_size = 8,validation_split=0.2,callbacks=[es,rho]) #,validation_data = (Xtr_val[0],ytr_val)\n    \n\n    # calcuate scores for test data\n    test_preds.append(model.predict(Xte[0][:],batch_size=8))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.iloc[:, 1:] = np.average(test_preds,axis=0)\nsub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}