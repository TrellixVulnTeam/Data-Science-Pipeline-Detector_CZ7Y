{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport gensim\nimport keras\nimport nltk.corpus\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"if '/usr/share/nltk_data' in nltk.data.path:\n    nltk.data.path.remove('/usr/share/nltk_data')\nnltk.data.path.append('../input/')\nnltk.data.path","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I will use the Brown Corpus to train a Doc2Vec model."},{"metadata":{"trusted":true},"cell_type":"code","source":"class BrownCorpus(object):\n    def __init__(self):\n        self.brown =  nltk.corpus.LazyCorpusLoader('brown', nltk.corpus.CategorizedTaggedCorpusReader, r'c[a-z]\\d\\d',\n                                                    cat_file='cats.csv', tagset='brown', encoding=\"ascii\",\n                                                    nltk_data_subdir='brown-corpus/brown')\n    def __iter__(self):\n        for (tag,doc) in enumerate(self.brown.paras()):\n            yield gensim.models.doc2vec.TaggedDocument(sum(doc,[]),[tag])\n            \nmodel = gensim.models.doc2vec.Doc2Vec(BrownCorpus(),\n                                      dm_concat=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.vector_size","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For each question/answer pair, I will infer a vector for each of question_title, question_body, and answer, and concatenate them."},{"metadata":{"trusted":true},"cell_type":"code","source":"def vectorize_cell(cell):\n    return model.infer_vector(list(gensim.utils.tokenize(cell)))\n\ndef vectorize(data):\n    return np.vstack([np.concatenate([vectorize_cell(row[cell])\n                                      for cell in ('question_title','question_body','answer')])\n                     for (i,row) in data.iterrows()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_data = pd.read_csv('../input/google-quest-challenge/train.csv',\n                           index_col='qa_id')\ntraining_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_vectors = vectorize(training_data)\ntraining_vectors","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now for a predictive model. I'll use a neural net with two dense hidden layers, each with 100 nodes and a softplus activation function. The output layer will have 30 nodes and a sigmoid activation function. Since I'm predicting continuous values, I'll use a Euclidean loss function."},{"metadata":{"trusted":true},"cell_type":"code","source":"predictor = keras.models.Sequential([keras.layers.Dense(100,\n                                                       input_shape=(300,),\n                                                       activation='softplus'),\n                                     keras.layers.Dense(100,\n                                                       activation='softplus'),\n                                     keras.layers.Dense(30,\n                                                       activation='sigmoid')])\n\npredictor.compile(optimizer='nadam',\n                  loss='mean_squared_error')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns_to_predict = pd.read_csv('../input/google-quest-challenge/sample_submission.csv',\n                                 index_col='qa_id').columns\npredictor.fit(training_vectors,\n              training_data.loc[:,columns_to_predict].values,\n              epochs=100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's try making some predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = pd.read_csv('../input/google-quest-challenge/test.csv',\n                        index_col='qa_id')\ntest_vectors = vectorize(test_data)\nresults = pd.DataFrame(predictor.predict(test_vectors),\n                       index = test_data.index,\n                       columns = columns_to_predict)\nresults","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results.to_csv('submission.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}