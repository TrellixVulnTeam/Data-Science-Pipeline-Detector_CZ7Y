{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nDATADIR = \"../input/LANL-Earthquake-Prediction\"\n\nprint(os.listdir(DATADIR))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# second cell\ndf = pd.read_csv(os.path.join(DATADIR, 'train.csv'),\n                 dtype={'acoustic_data': np.int16, 'time_to_failure': np.float64})","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import tensorflow as tf\n\ntf.set_random_seed(42)\nnp.random.seed(42)\n\nprint('GPU', tf.test.is_gpu_available())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DataTrainTestSplit(object):\n    \"\"\" Warning: The validation set should be statistically significant.\n    \n    There are 16 failures in the dataset.\n    \"\"\"\n    def __init__(self, n_blocks=16, split=0.2):\n        self.n_blocks = n_blocks\n        perm = np.random.permutation(n_blocks)\n        test_samples = int(np.floor(n_blocks * split))\n        self.train_slice = perm[:-test_samples]\n        self.test_slice = perm[-test_samples:]\n    def train(self):\n        return self.train_slice\n    def test(self):\n        return self.test_slice\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import linear_signal_py as linear_signal\n\nclass SignalFeatures(linear_signal.SignalFeatureGenerator):\n    SEQUENCE_LENGHT = 150_000\n\n    S_MEAN = 4\n    S_STD = 10\n\n    def __init__(self, normalize=True):\n        self.normalize = normalize\n\n    def shape(self):\n        return (self.SEQUENCE_LENGHT, 1)\n\n    def generate(self, df: pd.DataFrame, predict=False):\n        \"\"\" The performance of this function when vectorized is 10x of\n        an iterative loop.\n        \"\"\"\n        X = df['acoustic_data'].values[:, np.newaxis]\n        if self.normalize:\n            X = (X - self.S_MEAN) / self.S_STD\n        if predict:\n            return X\n        y = df['time_to_failure'].iloc[df.shape[0] - 1]\n        return X, np.array([y])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kFolds = [ DataTrainTestSplit(n_blocks=256, split=0.20) for _ in range(5) ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SEGMENT_SIZE = 150_000\nSTRIDES = 15_000\n\ndef get_generators(spliter):\n    ds_train = linear_signal.LinearDatasetAccessor(df, spliter.n_blocks, spliter.train())\n    ds_eval = linear_signal.LinearDatasetAccessor(df, spliter.n_blocks, spliter.test())\n\n    gen_train = linear_signal.LinearSignalGenerator(\n        ds_train, SEGMENT_SIZE, SignalFeatures(), strides=STRIDES, batch_size=64)\n    gen_eval = linear_signal.LinearSignalGenerator(\n        ds_eval, SEGMENT_SIZE, SignalFeatures(), strides=25_000)\n\n    return gen_train, gen_eval","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\n\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import Model, Sequential\nimport tensorflow.keras.backend as K\n\ndef make_layer(inp, index, filters, kernel_size, strides=1):\n    c = Conv1D(filters, kernel_size, strides=strides, name='layer_{0}_conv'.format(index), activation='relu')(inp)\n    p = MaxPooling1D(name='layer_{0}_pool'.format(index))(c)\n    d = Dropout(0.1)(p)\n    return d\n\nclass range_initializer(keras.initializers.Initializer):\n    def __init__(self, vmax, vmean):\n        self.vmax = vmax\n        self.vmean = vmean\n    def __call__(self, shape, dtype=None, partition_info=None):\n        if len(shape) != 2 or shape[1] != 1:\n            raise ValueError('Expected shape (N, 1), got ', shape)\n        return np.arange(shape[0])[:, np.newaxis] / shape[0] * self.vmax - self.vmean\n\ndef make_model():\n    inp = Input(shape=SignalFeatures().shape())\n    \n    params = [\n        (16, 16, 4),\n        (32, 16, 4),\n        (48, 16, 4),\n        (64, 8, 2),\n        (32, 8, 2),\n    ]\n\n    layer_in = inp\n    for i, param in enumerate(params):\n        output = make_layer(layer_in, i, *param)\n        layer_in = output\n\n    p = Permute((2, 1))(output)\n    s = Dense(8)(p)\n    summary = Flatten()(s)\n\n    last = Dense(24, activation='softmax')(summary)\n    out = Dense(1, kernel_initializer=range_initializer(16.0, 5.6),\n                bias_initializer=keras.initializers.Constant(5.6))(last)\n    model = Model(inp, out)\n    model.compile(optimizer=keras.optimizers.Adam(lr=0.0005), loss='mae')\n    return model\n\nK.clear_session()\nmodel = make_model()\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('signal-conv.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filepath=\"signal-conv.{0}.ckpt.hdf5\"\n\ncv_history = []\n\nfor i, spliter in enumerate(kFolds):\n    model = make_model()\n    gen_train, gen_val = get_generators(spliter)\n    cb_checkpoint = keras.callbacks.ModelCheckpoint(\n        filepath.format(i), monitor='val_loss', verbose=True,\n        save_best_only=True, mode='min')\n\n    history = model.fit_generator(\n        gen_train,\n        validation_data=gen_val,\n        callbacks=[cb_checkpoint],\n        epochs=30)\n\n    cv_history.append(history)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport matplotlib.pyplot as plt\n\nfor hist in cv_history:\n    plt.figure()\n    plt.plot(hist.history['loss'], label='loss')\n    plt.plot(hist.history['val_loss'], label='val_loss')\n    plt.legend()\n    plt.title('Model loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.plot()\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm_notebook\n\n# average the prediction from the multiple folds\n\nsubmission = pd.read_csv(os.path.join(DATADIR, 'sample_submission.csv'), index_col='seg_id', dtype={'time_to_failure': np.float32})\n\nfor fold in range(len(kFolds)):\n    model.load_weights('signal-conv.{0}.ckpt.hdf5'.format(fold))\n    for seg_id in tqdm_notebook(submission.index):\n        seg = pd.read_csv(os.path.join(DATADIR, 'test/' + seg_id + '.csv'))\n        X = SignalFeatures().generate(seg, predict=True)\n        y = model.predict(X[np.newaxis, :])\n        submission.loc[seg_id]['time_to_failure'] += y\n\nsubmission['time_to_failure'] /= len(kFolds)\nsubmission.to_csv('submission.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}