{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4caffc27fa23b56d6bf438caa4c81368f31b6781"},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom tqdm import tqdm_notebook as tqdm\nimport warnings\nimport sys\nimport gc\n\nfrom scipy.stats import kurtosis\nfrom scipy.stats import skew\nfrom scipy.stats import kstat\nfrom scipy.signal import hilbert\nfrom scipy.signal import hann\nfrom scipy.signal import convolve\nfrom scipy.signal import welch\n\nimport scipy.signal as sg\n\nfrom tsfresh.feature_extraction import feature_calculators\n\nimport nolds\nimport librosa\n\nfrom imblearn.over_sampling import SMOTE\n\nfrom catboost import CatBoostRegressor, Pool\nimport tensorflow\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, MaxAbsScaler\nfrom sklearn.svm import NuSVR\nfrom sklearn.svm import SVR\nfrom sklearn.manifold import TSNE\nfrom sklearn.metrics import mean_absolute_error, SCORERS\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LinearRegression, Lasso, MultiTaskLasso, ElasticNet, MultiTaskElasticNet, Ridge\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, BaggingRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"04daaad98e038bb3b3334ff38af52880e4758977"},"cell_type":"code","source":"%matplotlib inline\n%precision 4\nwarnings.filterwarnings('ignore')\nplt.style.use('ggplot')\nnp.set_printoptions(suppress=True)\npd.set_option(\"display.precision\", 15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2cbac275d36b2c467716c328bdf748b161f7de3a"},"cell_type":"code","source":"print('pandas: {}'.format(pd.__version__))\nprint('numpy: {}'.format(np.__version__))\nprint('Python: {}'.format(sys.version))\nprint('Tensorflow: {}'.format(tensorflow.__version__))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e0db87076445eea66a8e35239656b7f6254c34d4"},"cell_type":"markdown","source":"## Loading the data"},{"metadata":{"trusted":true,"_uuid":"f7568ca17c25655a2ec5092def6fb0d60da63464"},"cell_type":"code","source":"def load_data(nrows):\n    \"\"\"load the data for exploration\"\"\"\n    filename = \"../input/train.csv\"\n    \n    return pd.read_csv(\n        filename,\n        dtype={\n            'acoustic_data': np.int16,\n            'time_to_failure': np.float32\n        },\n        nrows=nrows,\n        skiprows=1,\n        names = ['acoustic_data', 'time_to_failure']\n    )\n\n\ndef get_data_iterator(nrows):\n    return pd.read_csv('../input/train.csv',\n        iterator=True,\n        dtype={\n            'acoustic_data': np.int16,\n            'time_to_failure': np.float32\n        },\n        nrows=nrows,\n        skiprows=1,\n        names = ['acoustic_data', 'time_to_failure']\n    )\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"%%time\nnrows = 600_000_000\ndata_df = load_data(nrows)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1bafa440c0be2cf44601d4e67df3329f681c489e"},"cell_type":"markdown","source":"## Data Exploration"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f187977939375c2a9dde9c35669ec01d71acc34d"},"cell_type":"code","source":"data_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5a4b1fa5aa275f23624b86d5c6eead507624fb7a"},"cell_type":"code","source":"acoustic_data_small = data_df['acoustic_data'].values[::50]\ntime_to_failure_small = data_df['time_to_failure'].values[::50]\n\nfig, ax1 = plt.subplots(figsize=(16, 8))\nplt.title(\"Trends of acoustic_data and time_to_failure. 2% of data (sampled)\")\nplt.plot(acoustic_data_small, color='b')\nax1.set_ylabel('acoustic_data', color='b')\nplt.legend(['acoustic_data'])\nax2 = ax1.twinx()\nplt.plot(time_to_failure_small, color='r')\nax2.set_ylabel('time_to_failure', color='r')\nplt.legend(['time_to_failure'], loc=(0.875, 0.9))\nplt.grid(False)\n\ndel acoustic_data_small\ndel time_to_failure_small\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acoustic_data_small = data_df['acoustic_data'].values[:150_000:50]\ntime_to_failure_small = data_df['time_to_failure'].values[:150_000:50]\n\nfig, ax1 = plt.subplots(figsize=(16, 8))\nplt.title(\"Trends of acoustic_data and time_to_failure. first 150_000, 2% of data (sampled)\")\nplt.plot(acoustic_data_small, color='b')\nax1.set_ylabel('acoustic_data', color='b')\nplt.legend(['acoustic_data'])\nax2 = ax1.twinx()\nplt.plot(time_to_failure_small, color='r')\nax2.set_ylabel('time_to_failure', color='r')\nplt.legend(['time_to_failure'], loc=(0.875, 0.9))\n#plt.grid(False)\n\ndel acoustic_data_small\ndel time_to_failure_small\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sig = data_df['acoustic_data'].values[:150_000]\nsig = sig - np.mean(sig)\nfft = np.fft.rfft(sig)\nplt.plot(fft)\nplt.title(\"FFT of first 150_000, without the mean\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(np.abs(fft))\nplt.title(\"FFT magnitude of first 150_000, without the mean\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(np.angle(fft))\nplt.title(\"FFT phase of first 150_000, without the mean\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hilbertSignal = hilbert(sig)\nplt.plot(hilbertSignal)\nplt.title(\"Hilbert signal\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(np.abs(hilbertSignal))\nplt.title(\"Hilbert signal, magnitude\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(np.angle(hilbertSignal))\nplt.title(\"Hilbert signal, phase\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del data_df\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"41fd58d7eb15dafcfaf107865b623dd9676119a3"},"cell_type":"markdown","source":"## Extracting features"},{"metadata":{},"cell_type":"markdown","source":"Code for FFT magnitude tajen from this Kernel: https://www.kaggle.com/vettejeep/masters-final-project-model-lb-1-392"},{"metadata":{"trusted":true},"cell_type":"code","source":"NY_FREQ_IDX = 75000  # the test signals are 150k samples long, Nyquist is thus 75k.\nCUTOFF = 18000\nMAX_FREQ_IDX = 20000\nFREQ_STEP = 2500","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def des_bw_filter_lp(cutoff=CUTOFF):  # low pass filter\n    b, a = sg.butter(4, Wn=cutoff/NY_FREQ_IDX)\n    return b, a\n\ndef des_bw_filter_hp(cutoff=CUTOFF):  # high pass filter\n    b, a = sg.butter(4, Wn=cutoff/NY_FREQ_IDX, btype='highpass')\n    return b, a\n\ndef des_bw_filter_bp(low, high):  # band pass filter\n    b, a = sg.butter(4, Wn=(low/NY_FREQ_IDX, high/NY_FREQ_IDX), btype='bandpass')\n    return b, a","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Taken from this Kernel https://www.kaggle.com/jsaguiar/baseline-with-multiple-models"},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_trend_feature(arr, abs_values=False):\n    arr = arr[::50]\n    idx = np.array(range(len(arr)))\n    if abs_values:\n        arr = np.abs(arr)\n    lr = LinearRegression()\n    lr.fit(idx.reshape(-1, 1), arr)\n    return lr.coef_[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def classic_sta_lta(x, length_sta, length_lta):\n    sta = np.cumsum(x ** 2)\n    # Convert to float\n    sta = np.require(sta, dtype=np.float)\n    # Copy for LTA\n    lta = sta.copy()\n    # Compute the STA and the LTA\n    sta[length_sta:] = sta[length_sta:] - sta[:-length_sta]\n    sta /= length_sta\n    lta[length_lta:] = lta[length_lta:] - lta[:-length_lta]\n    lta /= length_lta\n    # Pad zeros\n    sta[:length_lta - 1] = 0\n    # Avoid division by zero by setting zero values to tiny float\n    dtiny = np.finfo(0.0).tiny\n    idx = lta < dtiny\n    lta[idx] = dtiny\n    return sta / lta","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def calc_change_rate(x):\n    change = (np.diff(x) / x[:-1]).values\n    change = change[np.nonzero(change)[0]]\n    change = change[~np.isnan(change)]\n    change = change[change != -np.inf]\n    change = change[change != np.inf]\n    return np.mean(change)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Code for signal entropy"},{"metadata":{},"cell_type":"markdown","source":"Code for Signal Entropy, Fractal dimension and DFA from this Github [https://github.com/raphaelvallat/entropy/](https://github.com/raphaelvallat/entropy/)"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from numba import jit\nfrom math import log, floor\n\ndef _embed(x, order=3, delay=1):\n    \"\"\"Time-delay embedding.\n    Parameters\n    ----------\n    x : 1d-array, shape (n_times)\n        Time series\n    order : int\n        Embedding dimension (order)\n    delay : int\n        Delay.\n    Returns\n    -------\n    embedded : ndarray, shape (n_times - (order - 1) * delay, order)\n        Embedded time-series.\n    \"\"\"\n    N = len(x)\n    if order * delay > N:\n        raise ValueError(\"Error: order * delay should be lower than x.size\")\n    if delay < 1:\n        raise ValueError(\"Delay has to be at least 1.\")\n    if order < 2:\n        raise ValueError(\"Order has to be at least 2.\")\n    Y = np.zeros((order, N - (order - 1) * delay))\n    for i in range(order):\n        Y[i] = x[i * delay:i * delay + Y.shape[1]]\n    return Y.T\n\n\n@jit('UniTuple(float64, 2)(float64[:], float64[:])', nopython=True)\ndef _linear_regression(x, y):\n    \"\"\"Fast linear regression using Numba.\n    Parameters\n    ----------\n    x, y : ndarray, shape (n_times,)\n        Variables\n    Returns\n    -------\n    slope : float\n        Slope of 1D least-square regression.\n    intercept : float\n        Intercept\n    \"\"\"\n    n_times = x.size\n    sx2 = 0\n    sx = 0\n    sy = 0\n    sxy = 0\n    for j in range(n_times):\n        sx2 += x[j] ** 2\n        sx += x[j]\n        sxy += x[j] * y[j]\n        sy += y[j]\n    den = n_times * sx2 - (sx ** 2)\n    num = n_times * sxy - sx * sy\n    slope = num / den\n    intercept = np.mean(y) - slope * np.mean(x)\n    return slope, intercept\n\n\n@jit('i8[:](f8, f8, f8)', nopython=True)\ndef _log_n(min_n, max_n, factor):\n    \"\"\"\n    Creates a list of integer values by successively multiplying a minimum\n    value min_n by a factor > 1 until a maximum value max_n is reached.\n    Used for detrended fluctuation analysis (DFA).\n    Function taken from the nolds python package\n    (https://github.com/CSchoel/nolds) by Christopher Scholzel.\n    Parameters\n    ----------\n    min_n (float):\n        minimum value (must be < max_n)\n    max_n (float):\n        maximum value (must be > min_n)\n    factor (float):\n       factor used to increase min_n (must be > 1)\n    Returns\n    -------\n    list of integers:\n        min_n, min_n * factor, min_n * factor^2, ... min_n * factor^i < max_n\n        without duplicates\n    \"\"\"\n    max_i = int(floor(log(1.0 * max_n / min_n) / log(factor)))\n    ns = [min_n]\n    for i in range(max_i + 1):\n        n = int(floor(min_n * (factor ** i)))\n        if n > ns[-1]:\n            ns.append(n)\n    return np.array(ns, dtype=np.int64)\n\nfrom sklearn.neighbors import KDTree\nfrom scipy.signal import periodogram, welch\n\ndef perm_entropy(x, order=3, delay=1, normalize=False):\n    \"\"\"Permutation Entropy.\n    Parameters\n    ----------\n    x : list or np.array\n        One-dimensional time series of shape (n_times)\n    order : int\n        Order of permutation entropy\n    delay : int\n        Time delay\n    normalize : bool\n        If True, divide by log2(order!) to normalize the entropy between 0\n        and 1. Otherwise, return the permutation entropy in bit.\n    Returns\n    -------\n    pe : float\n        Permutation Entropy\n    Notes\n    -----\n    The permutation entropy is a complexity measure for time-series first\n    introduced by Bandt and Pompe in 2002 [1]_.\n    The permutation entropy of a signal :math:`x` is defined as:\n    .. math:: H = -\\\\sum p(\\\\pi)log_2(\\\\pi)\n    where the sum runs over all :math:`n!` permutations :math:`\\\\pi` of order\n    :math:`n`. This is the information contained in comparing :math:`n`\n    consecutive values of the time series. It is clear that\n    :math:`0 ≤ H (n) ≤ log_2(n!)` where the lower bound is attained for an\n    increasing or decreasing sequence of values, and the upper bound for a\n    completely random system where all :math:`n!` possible permutations appear\n    with the same probability.\n    The embedded matrix :math:`Y` is created by:\n    .. math:: y(i)=[x_i,x_{i+delay}, ...,x_{i+(order-1) * delay}]\n    .. math:: Y=[y(1),y(2),...,y(N-(order-1))*delay)]^T\n    References\n    ----------\n    .. [1] Bandt, Christoph, and Bernd Pompe. \"Permutation entropy: a\n           natural complexity measure for time series.\" Physical review letters\n           88.17 (2002): 174102.\n    Examples\n    --------\n    1. Permutation entropy with order 2\n        >>> from entropy import perm_entropy\n        >>> x = [4, 7, 9, 10, 6, 11, 3]\n        >>> # Return a value in bit between 0 and log2(factorial(order))\n        >>> print(perm_entropy(x, order=2))\n            0.918\n    2. Normalized permutation entropy with order 3\n        >>> from entropy import perm_entropy\n        >>> x = [4, 7, 9, 10, 6, 11, 3]\n        >>> # Return a value comprised between 0 and 1.\n        >>> print(perm_entropy(x, order=3, normalize=True))\n            0.589\n    \"\"\"\n    x = np.array(x)\n    ran_order = range(order)\n    hashmult = np.power(order, ran_order)\n    # Embed x and sort the order of permutations\n    sorted_idx = _embed(x, order=order, delay=delay).argsort(kind='quicksort')\n    # Associate unique integer to each permutations\n    hashval = (np.multiply(sorted_idx, hashmult)).sum(1)\n    # Return the counts\n    _, c = np.unique(hashval, return_counts=True)\n    # Use np.true_divide for Python 2 compatibility\n    p = np.true_divide(c, c.sum())\n    pe = -np.multiply(p, np.log2(p)).sum()\n    if normalize:\n        pe /= np.log2(factorial(order))\n    return pe\n\n\ndef spectral_entropy(x, sf, method='fft', nperseg=None, normalize=False):\n    \"\"\"Spectral Entropy.\n    Parameters\n    ----------\n    x : list or np.array\n        One-dimensional time series of shape (n_times)\n    sf : float\n        Sampling frequency\n    method : str\n        Spectral estimation method ::\n        'fft' : Fourier Transform (via scipy.signal.periodogram)\n        'welch' : Welch periodogram (via scipy.signal.welch)\n    nperseg : str or int\n        Length of each FFT segment for Welch method.\n        If None, uses scipy default of 256 samples.\n    normalize : bool\n        If True, divide by log2(psd.size) to normalize the spectral entropy\n        between 0 and 1. Otherwise, return the spectral entropy in bit.\n    Returns\n    -------\n    se : float\n        Spectral Entropy\n    Notes\n    -----\n    Spectral Entropy is defined to be the Shannon Entropy of the Power\n    Spectral Density (PSD) of the data:\n    .. math:: H(x, sf) =  -\\\\sum_{f=0}^{f_s/2} PSD(f) log_2[PSD(f)]\n    Where :math:`PSD` is the normalised PSD, and :math:`f_s` is the sampling\n    frequency.\n    References\n    ----------\n    .. [1] Inouye, T. et al. (1991). Quantification of EEG irregularity by\n       use of the entropy of the power spectrum. Electroencephalography\n       and clinical neurophysiology, 79(3), 204-210.\n    Examples\n    --------\n    1. Spectral entropy of a pure sine using FFT\n        >>> from entropy import spectral_entropy\n        >>> import numpy as np\n        >>> sf, f, dur = 100, 1, 4\n        >>> N = sf * duration # Total number of discrete samples\n        >>> t = np.arange(N) / sf # Time vector\n        >>> x = np.sin(2 * np.pi * f * t)\n        >>> print(np.round(spectral_entropy(x, sf, method='fft'), 2)\n            0.0\n    2. Spectral entropy of a random signal using Welch's method\n        >>> from entropy import spectral_entropy\n        >>> import numpy as np\n        >>> np.random.seed(42)\n        >>> x = np.random.rand(3000)\n        >>> print(spectral_entropy(x, sf=100, method='welch'))\n            9.939\n    3. Normalized spectral entropy\n        >>> print(spectral_entropy(x, sf=100, method='welch', normalize=True))\n            0.995\n    \"\"\"\n    x = np.array(x)\n    # Compute and normalize power spectrum\n    if method == 'fft':\n        _, psd = periodogram(x, sf)\n    elif method == 'welch':\n        _, psd = welch(x, sf, nperseg=nperseg)\n    psd_norm = np.divide(psd, psd.sum())\n    se = -np.multiply(psd_norm, np.log2(psd_norm)).sum()\n    if normalize:\n        se /= np.log2(psd_norm.size)\n    return se\n\n\ndef svd_entropy(x, order=3, delay=1, normalize=False):\n    \"\"\"Singular Value Decomposition entropy.\n    Parameters\n    ----------\n    x : list or np.array\n        One-dimensional time series of shape (n_times)\n    order : int\n        Order of permutation entropy\n    delay : int\n        Time delay\n    normalize : bool\n        If True, divide by log2(order!) to normalize the entropy between 0\n        and 1. Otherwise, return the permutation entropy in bit.\n    Returns\n    -------\n    svd_e : float\n        SVD Entropy\n    Notes\n    -----\n    SVD entropy is an indicator of the number of eigenvectors that are needed\n    for an adequate explanation of the data set. In other words, it measures\n    the dimensionality of the data.\n    The SVD entropy of a signal :math:`x` is defined as:\n    .. math::\n        H = -\\\\sum_{i=1}^{M} \\\\overline{\\\\sigma}_i log_2(\\\\overline{\\\\sigma}_i)\n    where :math:`M` is the number of singular values of the embedded matrix\n    :math:`Y` and :math:`\\\\sigma_1, \\\\sigma_2, ..., \\\\sigma_M` are the\n    normalized singular values of :math:`Y`.\n    The embedded matrix :math:`Y` is created by:\n    .. math:: y(i)=[x_i,x_{i+delay}, ...,x_{i+(order-1) * delay}]\n    .. math:: Y=[y(1),y(2),...,y(N-(order-1))*delay)]^T\n    Examples\n    --------\n    1. SVD entropy with order 2\n        >>> from entropy import svd_entropy\n        >>> x = [4, 7, 9, 10, 6, 11, 3]\n        >>> # Return a value in bit between 0 and log2(factorial(order))\n        >>> print(svd_entropy(x, order=2))\n            0.762\n    2. Normalized SVD entropy with order 3\n        >>> from entropy import svd_entropy\n        >>> x = [4, 7, 9, 10, 6, 11, 3]\n        >>> # Return a value comprised between 0 and 1.\n        >>> print(svd_entropy(x, order=3, normalize=True))\n            0.687\n    \"\"\"\n    x = np.array(x)\n    mat = _embed(x, order=order, delay=delay)\n    W = np.linalg.svd(mat, compute_uv=False)\n    # Normalize the singular values\n    W /= sum(W)\n    svd_e = -np.multiply(W, np.log2(W)).sum()\n    if normalize:\n        svd_e /= np.log2(order)\n    return svd_e","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Code for fractal dimension"},{"metadata":{"trusted":true},"cell_type":"code","source":"@jit('UniTuple(float64, 2)(float64[:], float64[:])', nopython=True)\ndef _linear_regression(x, y):\n    \"\"\"Fast linear regression using Numba.\n    Parameters\n    ----------\n    x, y : ndarray, shape (n_times,)\n        Variables\n    Returns\n    -------\n    slope : float\n        Slope of 1D least-square regression.\n    intercept : float\n        Intercept\n    \"\"\"\n    n_times = x.size\n    sx2 = 0\n    sx = 0\n    sy = 0\n    sxy = 0\n    for j in range(n_times):\n        sx2 += x[j] ** 2\n        sx += x[j]\n        sxy += x[j] * y[j]\n        sy += y[j]\n    den = n_times * sx2 - (sx ** 2)\n    num = n_times * sxy - sx * sy\n    slope = num / den\n    intercept = np.mean(y) - slope * np.mean(x)\n    return slope, intercept\n\ndef petrosian_fd(x):\n    \"\"\"Petrosian fractal dimension.\n    Parameters\n    ----------\n    x : list or np.array\n        One dimensional time series\n    Returns\n    -------\n    pfd : float\n        Petrosian fractal dimension\n    Notes\n    -----\n    The Petrosian algorithm can be used to provide a fast computation of\n    the FD of a signal by translating the series into a binary sequence.\n    The Petrosian fractal dimension of a time series :math:`x` is defined by:\n    .. math:: \\\\frac{log_{10}(N)}{log_{10}(N) +\n       log_{10}(\\\\frac{N}{N+0.4N_{\\\\Delta}})}\n    where :math:`N` is the length of the time series, and\n    :math:`N_{\\\\Delta}` is the number of sign changes in the binary sequence.\n    Original code from the pyrem package by Quentin Geissmann.\n    References\n    ----------\n    .. [1] A. Petrosian, Kolmogorov complexity of finite sequences and\n       recognition of different preictal EEG patterns, in , Proceedings of the\n       Eighth IEEE Symposium on Computer-Based Medical Systems, 1995,\n       pp. 212-217.\n    .. [2] Goh, Cindy, et al. \"Comparison of fractal dimension algorithms for\n       the computation of EEG biomarkers for dementia.\" 2nd International\n       Conference on Computational Intelligence in Medicine and Healthcare\n       (CIMED2005). 2005.\n    Examples\n    --------\n    Petrosian fractal dimension.\n        >>> import numpy as np\n        >>> from entropy import petrosian_fd\n        >>> np.random.seed(123)\n        >>> x = np.random.rand(100)\n        >>> print(petrosian_fd(x))\n            1.0505\n    \"\"\"\n    n = len(x)\n    # Number of sign changes in the first derivative of the signal\n    diff = np.ediff1d(x)\n    N_delta = (diff[1:-1] * diff[0:-2] < 0).sum()\n    return np.log10(n) / (np.log10(n) + np.log10(n / (n + 0.4 * N_delta)))\n\n\ndef katz_fd(x):\n    \"\"\"Katz Fractal Dimension.\n    Parameters\n    ----------\n    x : list or np.array\n        One dimensional time series\n    Returns\n    -------\n    kfd : float\n        Katz fractal dimension\n    Notes\n    -----\n    The Katz Fractal dimension is defined by:\n    .. math:: FD_{Katz} = \\\\frac{log_{10}(n)}{log_{10}(d/L)+log_{10}(n)}\n    where :math:`L` is the total length of the time series and :math:`d`\n    is the Euclidean distance between the first point in the\n    series and the point that provides the furthest distance\n    with respect to the first point.\n    Original code from the mne-features package by Jean-Baptiste Schiratti\n    and Alexandre Gramfort.\n    References\n    ----------\n    .. [1] Esteller, R. et al. (2001). A comparison of waveform fractal\n           dimension algorithms. IEEE Transactions on Circuits and Systems I:\n           Fundamental Theory and Applications, 48(2), 177-183.\n    .. [2] Goh, Cindy, et al. \"Comparison of fractal dimension algorithms for\n           the computation of EEG biomarkers for dementia.\" 2nd International\n           Conference on Computational Intelligence in Medicine and Healthcare\n           (CIMED2005). 2005.\n    Examples\n    --------\n    Katz fractal dimension.\n        >>> import numpy as np\n        >>> from entropy import katz_fd\n        >>> np.random.seed(123)\n        >>> x = np.random.rand(100)\n        >>> print(katz_fd(x))\n            5.1214\n    \"\"\"\n    x = np.array(x)\n    dists = np.abs(np.ediff1d(x))\n    ll = dists.sum()\n    ln = np.log10(np.divide(ll, dists.mean()))\n    aux_d = x - x[0]\n    d = np.max(np.abs(aux_d[1:]))\n    return np.divide(ln, np.add(ln, np.log10(np.divide(d, ll))))\n\n\n@jit('float64(float64[:], int32)')\ndef _higuchi_fd(x, kmax):\n    \"\"\"Utility function for `higuchi_fd`.\n    \"\"\"\n    n_times = x.size\n    lk = np.empty(kmax)\n    x_reg = np.empty(kmax)\n    y_reg = np.empty(kmax)\n    for k in range(1, kmax + 1):\n        lm = np.empty((k,))\n        for m in range(k):\n            ll = 0\n            n_max = floor((n_times - m - 1) / k)\n            n_max = int(n_max)\n            for j in range(1, n_max):\n                ll += abs(x[m + j * k] - x[m + (j - 1) * k])\n            ll /= k\n            ll *= (n_times - 1) / (k * n_max)\n            lm[m] = ll\n        # Mean of lm\n        m_lm = 0\n        for m in range(k):\n            m_lm += lm[m]\n        m_lm /= k\n        lk[k - 1] = m_lm\n        x_reg[k - 1] = log(1. / k)\n        y_reg[k - 1] = log(m_lm)\n    higuchi, _ = _linear_regression(x_reg, y_reg)\n    return higuchi\n\n\ndef higuchi_fd(x, kmax=10):\n    \"\"\"Higuchi Fractal Dimension.\n    Parameters\n    ----------\n    x : list or np.array\n        One dimensional time series\n    kmax : int\n        Maximum delay/offset (in number of samples).\n    Returns\n    -------\n    hfd : float\n        Higuchi Fractal Dimension\n    Notes\n    -----\n    Original code from the mne-features package by Jean-Baptiste Schiratti\n    and Alexandre Gramfort.\n    The `higuchi_fd` function uses Numba to speed up the computation.\n    References\n    ----------\n    .. [1] Higuchi, Tomoyuki. \"Approach to an irregular time series on the\n       basis of the fractal theory.\" Physica D: Nonlinear Phenomena 31.2\n       (1988): 277-283.\n    Examples\n    --------\n    Higuchi Fractal Dimension\n        >>> import numpy as np\n        >>> from entropy import higuchi_fd\n        >>> np.random.seed(123)\n        >>> x = np.random.rand(100)\n        >>> print(higuchi_fd(x))\n            2.051179\n    \"\"\"\n    x = np.asarray(x, dtype=np.float64)\n    kmax = int(kmax)\n    return _higuchi_fd(x, kmax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Code for detrended fluctuation analysis (DFA)"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n@jit('f8(f8[:])', nopython=True)\ndef _dfa(x):\n    \"\"\"\n    Utility function for detrended fluctuation analysis\n    \"\"\"\n    N = len(x)\n    nvals = _log_n(4, 0.1 * N, 1.2)\n    walk = np.cumsum(x - x.mean())\n    fluctuations = np.zeros(len(nvals))\n\n    for i_n, n in enumerate(nvals):\n        d = np.reshape(walk[:N - (N % n)], (N // n, n))\n        ran_n = np.array([float(na) for na in range(n)])\n        d_len = len(d)\n        slope = np.empty(d_len)\n        intercept = np.empty(d_len)\n        trend = np.empty((d_len, ran_n.size))\n        for i in range(d_len):\n            slope[i], intercept[i] = _linear_regression(ran_n, d[i])\n            y = np.zeros_like(ran_n)\n            # Equivalent to np.polyval function\n            for p in [slope[i], intercept[i]]:\n                y = y * ran_n + p\n            trend[i, :] = y\n        # calculate standard deviation (fluctuation) of walks in d around trend\n        flucs = np.sqrt(np.sum((d - trend) ** 2, axis=1) / n)\n        # calculate mean fluctuation over all subsequences\n        fluctuations[i_n] = flucs.sum() / flucs.size\n\n    # Filter zero\n    nonzero = np.nonzero(fluctuations)[0]\n    fluctuations = fluctuations[nonzero]\n    nvals = nvals[nonzero]\n    if len(fluctuations) == 0:\n        # all fluctuations are zero => we cannot fit a line\n        dfa = np.nan\n    else:\n        dfa, _ = _linear_regression(np.log(nvals), np.log(fluctuations))\n    return dfa\n\n\ndef detrended_fluctuation(x):\n    \"\"\"\n    Detrended fluctuation analysis (DFA).\n    Parameters\n    ----------\n    x : list or np.array\n        One-dimensional time-series.\n    Returns\n    -------\n    dfa : float\n        the estimate alpha for the Hurst parameter:\n        alpha < 1: stationary process similar to fractional Gaussian noise\n        with H = alpha\n        alpha > 1: non-stationary process similar to fractional Brownian\n        motion with H = alpha - 1\n    Notes\n    -----\n    Detrended fluctuation analysis (DFA) is used to find long-term statistical\n    dependencies in time series.\n    The idea behind DFA originates from the definition of self-affine\n    processes. A process :math:`X` is said to be self-affine if the standard\n    deviation of the values within a window of length n changes with the window\n    length factor L in a power law:\n    .. math:: \\\\text{std}(X, L * n) = L^H * \\\\text{std}(X, n)\n    where :math:`\\\\text{std}(X, k)` is the standard deviation of the process\n    :math:`X` calculated over windows of size :math:`k`. In this equation,\n    :math:`H` is called the Hurst parameter, which behaves indeed very similar\n    to the Hurst exponent.\n    For more details, please refer to the excellent documentation of the nolds\n    Python package by Christopher Scholzel, from which this function is taken:\n    https://cschoel.github.io/nolds/nolds.html#detrended-fluctuation-analysis\n    Note that the default subseries size is set to\n    entropy.utils._log_n(4, 0.1 * len(x), 1.2)). The current implementation\n    does not allow to manually specify the subseries size or use overlapping\n    windows.\n    The code is a faster (Numba) adaptation of the original code by Christopher\n    Scholzel.\n    References\n    ----------\n    .. [1] C.-K. Peng, S. V. Buldyrev, S. Havlin, M. Simons,\n           H. E. Stanley, and A. L. Goldberger, “Mosaic organization of\n           DNA nucleotides,” Physical Review E, vol. 49, no. 2, 1994.\n    .. [2] R. Hardstone, S.-S. Poil, G. Schiavone, R. Jansen,\n           V. V. Nikulin, H. D. Mansvelder, and K. Linkenkaer-Hansen,\n           “Detrended fluctuation analysis: A scale-free view on neuronal\n           oscillations,” Frontiers in Physiology, vol. 30, 2012.\n    Examples\n    --------\n        >>> import numpy as np\n        >>> from entropy import detrended_fluctuation\n        >>> np.random.seed(123)\n        >>> x = np.random.rand(100)\n        >>> print(detrended_fluctuation(x))\n            0.761647725305623\n    \"\"\"\n    x = np.asarray(x, dtype=np.float64)\n    return _dfa(x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Function to extract features from segment\nPut result in X_data."},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_features(X_data, segId, serie):\n    \"\"\"Extract some features from a chunk of 150_000, Fill X_data with features\"\"\"\n    \n    xcdm = serie - np.mean(serie) # signal without continuous component\n    \n    b, a = des_bw_filter_lp(cutoff=2500)\n    xc0 = sg.lfilter(b, a, xcdm)\n\n    b, a = des_bw_filter_bp(low=2500, high=5000)\n    xc1 = sg.lfilter(b, a, xcdm)\n\n    b, a = des_bw_filter_bp(low=5000, high=7500)\n    xc2 = sg.lfilter(b, a, xcdm)\n\n    b, a = des_bw_filter_bp(low=7500, high=10000)\n    xc3 = sg.lfilter(b, a, xcdm)\n\n    b, a = des_bw_filter_bp(low=10000, high=12500)\n    xc4 = sg.lfilter(b, a, xcdm)\n\n    b, a = des_bw_filter_bp(low=12500, high=15000) # important (mad)\n    xc5 = sg.lfilter(b, a, xcdm)\n\n    b, a = des_bw_filter_bp(low=15000, high=17500) # important (mad)\n    xc6 = sg.lfilter(b, a, xcdm)\n\n    b, a = des_bw_filter_bp(low=17500, high=20000)\n    xc7 = sg.lfilter(b, a, xcdm)\n\n    b, a = des_bw_filter_hp(cutoff=20000)\n    xc8 = sg.lfilter(b, a, xcdm)\n    \n    \n    sigs = [\n        serie,\n        pd.Series(xc0),\n        pd.Series(xc1),\n        pd.Series(xc2),\n        pd.Series(xc3),\n        pd.Series(xc4),\n        pd.Series(xc5),\n        pd.Series(xc6),\n        pd.Series(xc7),\n        pd.Series(xc8)\n    ]\n    \n    for i, sig in enumerate(sigs):\n        # TODO features for each filtered signals\n        #X_data.loc[segId, \"mean_all_%d\" % i] = sig.mean()\n        #X_data.loc[segId, \"std_all_%d\" % i] = sig.std() # correlated with mad\n\n        #X_data.loc[segId, \"mad_%d\" % i] = sig.mad() #important at 0,4,5 and (6,7)-> very important\n        #X_data.loc[segId, \"kurtosis_all_%d\" % i] = sig.kurtosis()\n        #X_data.loc[segId, \"skew_all_%d\" % i] = sig.skew()\n        \n        #X_data.loc[segId, \"fractal_dim_higuchi_%d\" % i] = higuchi_fd(sig) # always important\n        \n        X_data.loc[segId, \"hurst_exponent_%d\" % i] = nolds.hurst_rs(sig)\n        \n        \n        X_data.loc[segId, \"classic_sta_lta_mean_%d\" % i] = classic_sta_lta(sig, 100, 5000).mean() # important\n\n        #X_data.loc[segId, \"mean_change_rate_%d\" % i] = calc_change_rate(sig)\n        #X_data.loc[segId, \"iqr_%d\" % i] = np.subtract(*np.percentile(sig, [75, 25]))\n    \n    \n    X_data.loc[segId, \"fractal_dim_higuchi\"] = higuchi_fd(serie) # always important\n    \n    X_data.loc[segId, \"mean\"] = np.mean(serie)\n    X_data.loc[segId, \"mad_6\"] = pd.Series(xc5).mad() #important at 0,4,5 and (6,7)-> very important\n    X_data.loc[segId, \"mad_7\"] = pd.Series(xc6).mad() #important at 0,4,5 and (6,7)-> very important\n    \n    X_data.loc[segId, \"abs_q05\"] = np.quantile(np.abs(serie), 0.05)\n    \n    X_data.loc[segId,\"std_0_to_10\"]  = serie[(serie>=0) & (serie<=10)].std()\n    X_data.loc[segId,\"std_minus_10_to_10\"]  = serie[(serie>=-10) & (serie<=10)].std()\n    \n    for p in [5, 10]:\n         X_data.loc[segId, \"nb_peaks_%i\" % p] = feature_calculators.number_peaks(serie, p)\n    \n    X_data.loc[segId, \"num_crossings\"] = feature_calculators.number_crossing_m(serie, 0)\n    \n    \"\"\"\n    sigFloat = np.float32(serie)\n    X_data.loc[segId, \"spectral_centroid\"] = np.mean(librosa.feature.spectral_centroid(sigFloat))\n    X_data.loc[segId, \"spectral_bandwidth\"] = np.mean(librosa.feature.spectral_bandwidth(sigFloat))\n    X_data.loc[segId, \"spectral_rolloff\"] = np.mean(librosa.feature.spectral_rolloff(sigFloat))\n    X_data.loc[segId, \"spectral_flatness\"] = np.mean(librosa.feature.spectral_flatness(sigFloat))\n    X_data.loc[segId, \"spectral_contrast\"] = np.mean(librosa.feature.spectral_contrast(sigFloat))\n    \"\"\"\n    \n    x_roll_std_100 = serie.rolling(100).std().dropna().values[::100]\n    #X_data.loc[segId, \"q05_roll_std_100\"] = np.quantile(x_roll_std_100, 0.05) # important\n    #X_data.loc[segId, \"q15_roll_std_100\"] = np.quantile(x_roll_std_100, 0.15) # important \n    X_data.loc[segId, \"q30_roll_std_100\"] = np.quantile(x_roll_std_100, 0.30) # most important","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Extract features form data with iterator"},{"metadata":{"trusted":true},"cell_type":"code","source":"nrows = 615_000_000\nrows_by_chunk = 150_000 # number of rows by chunks\nsegments = nrows//rows_by_chunk\n\nX_data = pd.DataFrame(index=range(segments), dtype=np.float32)\ny_data = pd.DataFrame(index=range(segments), dtype=np.float32)\n\ndata_iterator = get_data_iterator(nrows)\n\nfor segId in tqdm(range(segments)):\n    seg = data_iterator.get_chunk(rows_by_chunk)\n    time_to_failure = seg['time_to_failure'].values[-1]\n    \n    extract_features(X_data, segId, seg['acoustic_data'])\n    y_data.loc[segId, \"time_to_failure\"] = time_to_failure # predict the last value of the chunk\n    \ny_data = np.array(y_data).flatten()\n\ngc.collect()\nX_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_data.fillna(value=0, inplace=True)\nfeature_list = np.array(X_data.columns)\nprint(\"Shape of data :\", X_data.shape)\nfeature_list","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Distribution of \"time_to_failure\"\nWe see here that there is a huge difference between smaller time to failure and biger ones (> 8).  \nThat explain why the predictions for those times are wrong.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(y_data, bins=17)\nplt.xlabel(\"time_of_failure\")\nplt.ylabel(\"occurences\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Correlation between features"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_corr(df,size=20):\n    '''Function plots a graphical correlation matrix for each pair of columns in the dataframe.\n\n    Input:\n        df: pandas DataFrame\n        size: vertical and horizontal size of the plot'''\n\n    corr = df.corr()\n    fig, ax = plt.subplots(figsize=(size, size))\n    mat = ax.matshow(corr, cmap=\"coolwarm\")\n    fig.colorbar(mat)\n    plt.xticks(range(len(corr.columns)), corr.columns, rotation='vertical');\n    plt.yticks(range(len(corr.columns)), corr.columns);\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_corr(X_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.abs(X_data.corrwith(pd.Series(y_data))).sort_values(ascending=False).head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_feature_vs_ttf(data, ttf, title=\"No Title\"):\n    fig, ax1 = plt.subplots(figsize=(16, 8))\n    plt.title(title)\n    plt.plot(data, color='b')\n    ax1.set_ylabel('acoustic_data', color='b')\n    plt.legend(['acoustic_data'])\n    ax2 = ax1.twinx()\n    plt.plot(ttf, color='r')\n    ax2.set_ylabel('time_to_failure', color='r')\n    plt.legend(['time_to_failure'], loc=(0.875, 0.9))\n    plt.grid(False)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for name, values in X_data.iteritems():\n    plot_feature_vs_ttf(values, y_data, name)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## T-SNE on features"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_TSNE(data, y_data):\n    X_embedded = TSNE(n_components=2).fit_transform(data)\n\n    plt.figure(figsize=(15,10))\n    plt.scatter(X_embedded[:,0], X_embedded[:,1], c=y_data, cmap='inferno')\n    plt.colorbar()\n    plt.title(\"Data tranformed by TSNE in two dimensions\")\n    plt.show()\n    \n    gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_TSNE(X_data, y_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## T-SNE on scaled data"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_TSNE(StandardScaler().fit_transform(X_data), y_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Separating between training and testing sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(\n    X_data, y_data,\n    test_size=0.2, \n    shuffle=True, random_state=42)\nX_train, X_valid, y_train, y_valid = train_test_split(\n    X_train, y_train,\n    test_size=0.2,\n    shuffle=True, random_state=42\n)\n\nprint(\"Training on\", X_train.shape[0], \"rows\")\nprint(\"Validating on\", X_valid.shape[0], \"rows\")\nprint(\"Testing on\", X_test.shape[0], \"rows\")\n\nf, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey=True)\nf.suptitle('Histogram of time_to_failure')\nax1.hist(y_train, bins=17)\nax1.set_title(\"train\")\nax2.hist(y_valid, bins=17)\nax2.set_title(\"test\")\nax3.hist(y_test, bins=17)\nax3.set_title(\"validation\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Augmenting the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"smt = SMOTE()\nX_train, y_train = smt.fit_sample(X_train, np.round(y_train))\n\nplt.hist(y_train, bins=17)\nplt.xlabel(\"time_of_failure\")\nplt.ylabel(\"occurences\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Scaling the data\nUsing the training set to fit the scaler.  \nScaling training, testing, and validation sets."},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\nX_valid = scaler.transform(X_valid)\nprint(\"Data scaled\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plotAndScore(y_pred, y_test, name):\n    \n    score = mean_absolute_error(y_test, y_pred)\n    \n    plt.figure(figsize=(6, 6))\n    plt.scatter(y_test, y_pred)\n    plt.xlim(0, 20)\n    plt.ylim(0, 20)\n    plt.xlabel('actual', fontsize=12)\n    plt.ylabel('predicted', fontsize=12)\n    plt.title(name + f' -> MAE: {score:0.3f}')\n    plt.plot([(0, 0), (20, 20)], [(0, 0), (20, 20)])\n    plt.show()\n    \n    return score\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Catboost\nTraining a Catboost model on the extracted features.  \nValidating on the run to avoid overfitting, select the model that overfit the least automatically."},{"metadata":{"trusted":true},"cell_type":"code","source":"dataPool = Pool(X_train, y_train) \nmodelCatBoost = CatBoostRegressor(iterations=1000,\n                        loss_function='MAE',\n                        learning_rate=0.1,\n                        depth=6,\n                        l2_leaf_reg=3,\n                        border_count=128,\n                        use_best_model=True,\n                        silent=True)\n\nmodelCatBoost.fit(dataPool, eval_set=Pool(X_valid, y_valid), plot=True)\n\n\ny_pred = modelCatBoost.predict(X_test)\n\nscore = plotAndScore(y_pred, y_test, \"Catboost\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"fea_imp = pd.DataFrame({'imp': modelCatBoost.feature_importances_, 'col': feature_list})\nfea_imp = fea_imp.sort_values(['imp', 'col'], ascending=[True, False])[-20:]\nfea_imp.plot(kind='barh', x='col', y='imp', figsize=(15, 10), legend=None)\nplt.title('CatBoost - Feature Importance')\nplt.ylabel('Features')\nplt.xlabel('Importance')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import shap\n\nshap.initjs()\n\nexplainer = shap.TreeExplainer(modelCatBoost)\nshap_values = explainer.shap_values(X_train)\n\n# visualize the first prediction's explanation (use matplotlib=True to avoid Javascript)\nshap.force_plot(explainer.expected_value, shap_values[0,:], feature_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.summary_plot(shap_values, X_train, feature_names=feature_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.summary_plot(shap_values, X_train, plot_type=\"bar\", feature_names=feature_list)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Cross Validation for scoring"},{"metadata":{},"cell_type":"markdown","source":"### With Catboost cv"},{"metadata":{"trusted":true},"cell_type":"code","source":"from catboost import cv\nparams = {}\nparams[\"iterations\"] = 500\nparams[\"loss_function\"] = 'MAE'\nparams[\"learning_rate\"] = 0.2\nparams[\"depth\"] = 6\nparams[\"verbose\"] = 0\n\ncv_data = cv(pool=Pool(StandardScaler().fit_transform(X_data), y_data), \n   params=params,\n   fold_count=5, \n   inverted=False,\n   shuffle=True, \n   stratified=False,\n   as_pandas=True,\n   plot=True,\n   verbose=False)\n\nmin_test_mean = cv_data[\"test-MAE-mean\"].min()\nprint(\"best MAE :\",min_test_mean)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Testing many kind of models"},{"metadata":{"trusted":true},"cell_type":"code","source":"models = {\n    \"CatBoostRegressor\" : CatBoostRegressor(iterations=300, loss_function='MAE', learning_rate=0.2, depth=6, verbose=0),\n    \"NuSVR\" : NuSVR(kernel=\"rbf\", C=1),\n    \"SVR\" : SVR(kernel=\"rbf\", C=1),\n    \"DecisionTreeRegressor\" : DecisionTreeRegressor(),\n    \"RandomForestRegressor\" : RandomForestRegressor(max_depth=6, n_estimators=200),\n    \"GradientBoostingRegressor\" : GradientBoostingRegressor(),\n    \"KNeighborsRegressor(n_neighbors=50)\" : KNeighborsRegressor(n_neighbors=50),\n    'LinearRegression': LinearRegression(),\n    'Ridge_1': Ridge(alpha=0.001),\n    'Lasso_1': Lasso(alpha=0.001),\n    'ElasticNet_1': ElasticNet(alpha=0.001, l1_ratio=0),\n    'ElasticNet_2': ElasticNet(alpha=0.001, l1_ratio=0.25),\n    'ElasticNet_3': ElasticNet(alpha=0.001, l1_ratio=0.33),\n    'ElasticNet_4': ElasticNet(alpha=0.001, l1_ratio=0.5),\n    'ElasticNet_5': ElasticNet(alpha=0.001, l1_ratio=0.66),\n    'ElasticNet_6': ElasticNet(alpha=0.001, l1_ratio=0.75),\n    'ElasticNet_7': ElasticNet(alpha=0.001, l1_ratio=1),\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_model_name = \"\"\nbest_score = 100\n\nfor name, model in tqdm(models.items()):\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    new_score = plotAndScore(y_pred, y_test, name)\n    if new_score < best_score:\n        best_score = new_score\n        best_model_name = name","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_model_name","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see here that catboost make the better predictions."},{"metadata":{},"cell_type":"markdown","source":"## Publishing the predictions for the competition\nNow done in another kernel to not compute data analysis everytime I commit...  \nThe better model is also trained with all the data, as it was proven effective here.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('../input/sample_submission.csv', index_col='seg_id')\nX_data_submission = pd.DataFrame(dtype=np.float64, index=submission.index)\n\nplot_data_submission = pd.Series()\n\nfor seg_id in tqdm(X_data_submission.index):\n    seg = pd.read_csv('../input/test/' + seg_id + '.csv')\n    \n    plot_data_submission.append(seg[::50])\n    \n    x = seg['acoustic_data']\n    extract_features(X_data_submission, seg_id, x)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_data_submission = scaler.transform(X_data_submission)\nsubmission['time_to_failure'] = [x*1.05 for x in modelCatBoost.predict(X_data_submission)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Display the prediction results"},{"metadata":{"trusted":true},"cell_type":"code","source":"# TODO\nacoustic_data_small = []\n\nfor seg_id in tqdm(submission.index):\n    seg = pd.read_csv('../input/test/' + seg_id + '.csv')\n    acoustic_data_small.append(seg['acoustic_data'][::50])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntime_to_failure_small = submission[\"time_to_failure\"].values\n\nfig, ax1 = plt.subplots(figsize=(16, 8))\nplt.title(\"Trends of acoustic_data and time_to_failure. 2% of data (sampled)\")\nplt.plot(acoustic_data_small, color='b')\nax1.set_ylabel('acoustic_data', color='b')\nplt.legend(['acoustic_data'])\nax2 = ax1.twinx()\nplt.plot(time_to_failure_small, color='r', alpha=0.5)\nax2.set_ylabel('time_to_failure', color='r')\nplt.legend(['time_to_failure'], loc=(0.875, 0.9))\nplt.grid(False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see here that the test segments are not in chronological order. LSTM is not really suited like that as a prediction model.  \nLSTM could be used on segments only, testing this currently in another kernel."},{"metadata":{"trusted":true},"cell_type":"code","source":"del acoustic_data_small\ndel time_to_failure_small\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv')\nprint(\"Submission data saved to csv.\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}