{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport gc","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"tr = pd.read_csv('../input/train.csv', dtype={'acoustic_data': 'int16', 'time_to_failure': 'float64'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"earthquake_list = [-1, 5656573, 50085877, 104677355, 138772452, 187641819, \n                   218652629, 245829584, 307838916, 338276286, 375377847, \n                   419368879, 461811622, 495800224, 528777114, 585568143, 621985672, 629145479]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"JUMP_AMOUNT = 40000\n\nlist_of_acoustics = []\nlist_of_earthquakes = []\nlist_of_ttfs = []\n\nfor i in tqdm(range(len(earthquake_list)-1)):\n    jumper = 0\n    # While you still have the ability to jump forward in your earthquake\n    while jumper < (earthquake_list[i+1] - earthquake_list[i] - 150000):\n        # Append numpy array of length 150,000 to your training set\n        list_of_acoustics.append(tr['acoustic_data'].iloc[(earthquake_list[i]+1+jumper):(earthquake_list[i]+1+jumper+150000)].values)\n        \n        # Append which earthquake you're recording\n        list_of_earthquakes.append(i)\n        \n        # Append the time to failure\n        list_of_ttfs.append(tr['time_to_failure'].iloc[earthquake_list[i]+1+jumper+150000])\n        \n        # Add the JUMP_AMOUNT\n        jumper += JUMP_AMOUNT\n        \n    # Now, jumper is too much, so let's manually add the last 150,000 before the time to failure\n    # To capture this special moment\n    list_of_acoustics.append(tr['acoustic_data'].iloc[(earthquake_list[i+1]+1-150000):(earthquake_list[i+1]+1)])\n    list_of_earthquakes.append(i)\n    list_of_ttfs.append(tr['time_to_failure'].iloc[earthquake_list[i+1]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del tr\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_set = np.vstack(list_of_acoustics)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_set.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_of_earthquakes = np.array(list_of_earthquakes)\nlist_of_ttfs = np.array(list_of_ttfs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_set = training_set.reshape(training_set.shape[0],training_set.shape[1],1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_of_ttfs = list_of_ttfs.reshape(list_of_ttfs.shape[0], 1, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Model\nfrom keras.layers import Input, Conv1D, Dense, Dropout, Lambda, concatenate, Flatten\nfrom keras.optimizers import Adam","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convolutional layer parameters\nn_filters = 4\nfilter_width = 10\ndilation_rates = [2**i for i in range(4)] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define an input history series and pass it through a stack of dilated causal convolutions. \nhistory_seq = Input(shape=(None, 1))\nx = history_seq","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for dilation_rate in dilation_rates:\n    x = Conv1D(filters=n_filters,\n               kernel_size=filter_width, \n               padding='causal',\n               dilation_rate=dilation_rate)(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = Dense(128, activation='relu')(x)\nx = Dropout(.2)(x)\nout = Dense(1)(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Model(history_seq, out)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"first_n_samples = 15000\nbatch_size = 16\nepochs = 10","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(Adam(), loss='mean_absolute_error')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(training_set[:first_n_samples], np.array(list_of_ttfs)[:first_n_samples],\n                    batch_size=batch_size,\n                    epochs=epochs,\n                    validation_data = (training_set[15000:], np.array(list_of_ttfs)[15000:]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}