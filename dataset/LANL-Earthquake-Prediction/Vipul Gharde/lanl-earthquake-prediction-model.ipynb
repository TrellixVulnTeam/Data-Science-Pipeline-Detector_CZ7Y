{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nfrom catboost import CatBoostRegressor\nfrom joblib import Parallel, delayed\nfrom tensorflow import keras\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler","execution_count":1,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"X_train_1 = pd.read_csv(\"../input/lanl-master-s-features-creating-0/train_X_features_865.csv\")\nX_train_2 = pd.read_csv(\"../input/lanl-master-s-features-creating-1/train_X_features_865.csv\")\ny_1 = pd.read_csv(\"../input/lanl-master-s-features-creating-0/train_y.csv\", index_col=False, header=None)\ny_2 = pd.read_csv(\"../input/lanl-master-s-features-creating-1/train_y.csv\", index_col=False, header=None)","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = pd.concat([X_train_1, X_train_2], axis=0)\nX_train = X_train.reset_index(drop=True)\nX_train.shape","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"(33000, 865)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"   FFT_Mag_01q0           ...            abs_max_roll_mean_1000\n0    139.890381           ...                             5.415\n1    140.598496           ...                             5.362\n2    123.395925           ...                             5.312\n3     92.046507           ...                             5.562\n4    103.474480           ...                             5.206\n\n[5 rows x 865 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>FFT_Mag_01q0</th>\n      <th>FFT_Mag_10q0</th>\n      <th>FFT_Mag_90q0</th>\n      <th>FFT_Mag_99q0</th>\n      <th>FFT_Mag_mean0</th>\n      <th>FFT_Mag_std0</th>\n      <th>FFT_Mag_max0</th>\n      <th>FFT_Phz_mean0</th>\n      <th>FFT_Phz_std0</th>\n      <th>FFT_Mag_01q2500</th>\n      <th>FFT_Mag_10q2500</th>\n      <th>FFT_Mag_90q2500</th>\n      <th>FFT_Mag_99q2500</th>\n      <th>FFT_Mag_mean2500</th>\n      <th>FFT_Mag_std2500</th>\n      <th>FFT_Mag_max2500</th>\n      <th>FFT_Phz_mean2500</th>\n      <th>FFT_Phz_std2500</th>\n      <th>FFT_Mag_01q5000</th>\n      <th>FFT_Mag_10q5000</th>\n      <th>FFT_Mag_90q5000</th>\n      <th>FFT_Mag_99q5000</th>\n      <th>FFT_Mag_mean5000</th>\n      <th>FFT_Mag_std5000</th>\n      <th>FFT_Mag_max5000</th>\n      <th>FFT_Phz_mean5000</th>\n      <th>FFT_Phz_std5000</th>\n      <th>FFT_Mag_01q7500</th>\n      <th>FFT_Mag_10q7500</th>\n      <th>FFT_Mag_90q7500</th>\n      <th>FFT_Mag_99q7500</th>\n      <th>FFT_Mag_mean7500</th>\n      <th>FFT_Mag_std7500</th>\n      <th>FFT_Mag_max7500</th>\n      <th>FFT_Phz_mean7500</th>\n      <th>FFT_Phz_std7500</th>\n      <th>FFT_Mag_01q10000</th>\n      <th>FFT_Mag_10q10000</th>\n      <th>FFT_Mag_90q10000</th>\n      <th>FFT_Mag_99q10000</th>\n      <th>...</th>\n      <th>q01_roll_std_100</th>\n      <th>q05_roll_std_100</th>\n      <th>q95_roll_std_100</th>\n      <th>q99_roll_std_100</th>\n      <th>av_change_abs_roll_std_100</th>\n      <th>av_change_rate_roll_std_100</th>\n      <th>abs_max_roll_std_100</th>\n      <th>ave_roll_mean_100</th>\n      <th>std_roll_mean_100</th>\n      <th>max_roll_mean_100</th>\n      <th>min_roll_mean_100</th>\n      <th>q01_roll_mean_100</th>\n      <th>q05_roll_mean_100</th>\n      <th>q95_roll_mean_100</th>\n      <th>q99_roll_mean_100</th>\n      <th>av_change_abs_roll_mean_100</th>\n      <th>av_change_rate_roll_mean_100</th>\n      <th>abs_max_roll_mean_100</th>\n      <th>ave_roll_std_1000</th>\n      <th>std_roll_std_1000</th>\n      <th>max_roll_std_1000</th>\n      <th>min_roll_std_1000</th>\n      <th>q01_roll_std_1000</th>\n      <th>q05_roll_std_1000</th>\n      <th>q95_roll_std_1000</th>\n      <th>q99_roll_std_1000</th>\n      <th>av_change_abs_roll_std_1000</th>\n      <th>av_change_rate_roll_std_1000</th>\n      <th>abs_max_roll_std_1000</th>\n      <th>ave_roll_mean_1000</th>\n      <th>std_roll_mean_1000</th>\n      <th>max_roll_mean_1000</th>\n      <th>min_roll_mean_1000</th>\n      <th>q01_roll_mean_1000</th>\n      <th>q05_roll_mean_1000</th>\n      <th>q95_roll_mean_1000</th>\n      <th>q99_roll_mean_1000</th>\n      <th>av_change_abs_roll_mean_1000</th>\n      <th>av_change_rate_roll_mean_1000</th>\n      <th>abs_max_roll_mean_1000</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>139.890381</td>\n      <td>430.720962</td>\n      <td>3841.665607</td>\n      <td>7039.062475</td>\n      <td>1747.987667</td>\n      <td>1505.807396</td>\n      <td>12964.190508</td>\n      <td>-0.019064</td>\n      <td>0.917755</td>\n      <td>580.989637</td>\n      <td>1788.141759</td>\n      <td>10385.447859</td>\n      <td>17303.410477</td>\n      <td>5576.975550</td>\n      <td>3670.078510</td>\n      <td>21851.886653</td>\n      <td>-0.018764</td>\n      <td>0.904588</td>\n      <td>576.378990</td>\n      <td>2244.232557</td>\n      <td>10573.085626</td>\n      <td>15464.766090</td>\n      <td>6118.346896</td>\n      <td>3304.632470</td>\n      <td>24716.397978</td>\n      <td>0.021492</td>\n      <td>0.912655</td>\n      <td>698.174578</td>\n      <td>2463.552257</td>\n      <td>13396.596838</td>\n      <td>21973.239590</td>\n      <td>7353.612733</td>\n      <td>4529.556526</td>\n      <td>32176.902122</td>\n      <td>-0.006021</td>\n      <td>0.909976</td>\n      <td>304.096958</td>\n      <td>992.766285</td>\n      <td>8185.997878</td>\n      <td>13830.325240</td>\n      <td>...</td>\n      <td>2.341954</td>\n      <td>2.528125</td>\n      <td>12.600701</td>\n      <td>29.647864</td>\n      <td>-4.876389e-05</td>\n      <td>74991.820387</td>\n      <td>64.916157</td>\n      <td>4.429363</td>\n      <td>0.546202</td>\n      <td>13.91</td>\n      <td>-6.31</td>\n      <td>3.12</td>\n      <td>3.73</td>\n      <td>5.12</td>\n      <td>5.66</td>\n      <td>0.000002</td>\n      <td>74986.645144</td>\n      <td>13.91</td>\n      <td>5.261040</td>\n      <td>4.761211</td>\n      <td>37.834871</td>\n      <td>2.542905</td>\n      <td>2.642355</td>\n      <td>2.753961</td>\n      <td>14.856489</td>\n      <td>31.176557</td>\n      <td>-1.579223e-05</td>\n      <td>74576.876580</td>\n      <td>37.834871</td>\n      <td>4.429468</td>\n      <td>0.237930</td>\n      <td>5.415</td>\n      <td>3.327</td>\n      <td>3.882</td>\n      <td>4.029</td>\n      <td>4.804</td>\n      <td>4.941</td>\n      <td>-7.516779e-07</td>\n      <td>74577.679964</td>\n      <td>5.415</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>140.598496</td>\n      <td>431.872538</td>\n      <td>3093.828306</td>\n      <td>5914.712982</td>\n      <td>1545.320042</td>\n      <td>1239.738638</td>\n      <td>14530.791553</td>\n      <td>0.002910</td>\n      <td>0.904066</td>\n      <td>371.779862</td>\n      <td>1247.165814</td>\n      <td>8126.085534</td>\n      <td>13429.208174</td>\n      <td>4307.232346</td>\n      <td>2810.568152</td>\n      <td>16980.104876</td>\n      <td>-0.000715</td>\n      <td>0.909822</td>\n      <td>758.474860</td>\n      <td>2403.552302</td>\n      <td>10859.395754</td>\n      <td>15352.256883</td>\n      <td>6500.555291</td>\n      <td>3294.129910</td>\n      <td>19523.499828</td>\n      <td>-0.012594</td>\n      <td>0.903179</td>\n      <td>549.766537</td>\n      <td>1893.113362</td>\n      <td>11412.308979</td>\n      <td>18897.650880</td>\n      <td>6049.687590</td>\n      <td>3897.710996</td>\n      <td>26050.265072</td>\n      <td>0.006173</td>\n      <td>0.900904</td>\n      <td>232.076227</td>\n      <td>898.383220</td>\n      <td>5984.967037</td>\n      <td>11639.375545</td>\n      <td>...</td>\n      <td>2.320288</td>\n      <td>2.486407</td>\n      <td>9.477645</td>\n      <td>22.710339</td>\n      <td>3.857706e-07</td>\n      <td>74941.498345</td>\n      <td>87.311820</td>\n      <td>4.241309</td>\n      <td>0.531692</td>\n      <td>20.71</td>\n      <td>-5.29</td>\n      <td>3.08</td>\n      <td>3.54</td>\n      <td>4.94</td>\n      <td>5.32</td>\n      <td>-0.000003</td>\n      <td>74937.945900</td>\n      <td>20.71</td>\n      <td>4.673184</td>\n      <td>4.222990</td>\n      <td>40.883127</td>\n      <td>2.512763</td>\n      <td>2.624581</td>\n      <td>2.734313</td>\n      <td>11.228717</td>\n      <td>24.813733</td>\n      <td>3.154961e-07</td>\n      <td>74486.700394</td>\n      <td>40.883127</td>\n      <td>4.241174</td>\n      <td>0.262043</td>\n      <td>5.362</td>\n      <td>3.139</td>\n      <td>3.636</td>\n      <td>3.792</td>\n      <td>4.681</td>\n      <td>4.865</td>\n      <td>-2.791946e-06</td>\n      <td>74484.368615</td>\n      <td>5.362</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>123.395925</td>\n      <td>364.114775</td>\n      <td>2052.051388</td>\n      <td>3594.307716</td>\n      <td>1157.415026</td>\n      <td>765.798779</td>\n      <td>11148.510210</td>\n      <td>-0.029014</td>\n      <td>0.904867</td>\n      <td>326.587430</td>\n      <td>974.239681</td>\n      <td>4765.050989</td>\n      <td>7051.493436</td>\n      <td>2682.743587</td>\n      <td>1499.339342</td>\n      <td>9153.525311</td>\n      <td>-0.023570</td>\n      <td>0.915614</td>\n      <td>391.627693</td>\n      <td>1548.206760</td>\n      <td>7269.258931</td>\n      <td>10307.151670</td>\n      <td>4170.463168</td>\n      <td>2252.933240</td>\n      <td>15298.295361</td>\n      <td>-0.018223</td>\n      <td>0.891113</td>\n      <td>581.302261</td>\n      <td>1843.330646</td>\n      <td>11490.306704</td>\n      <td>18905.302322</td>\n      <td>6056.587080</td>\n      <td>3985.532867</td>\n      <td>28425.965355</td>\n      <td>0.008742</td>\n      <td>0.898519</td>\n      <td>297.355997</td>\n      <td>852.686652</td>\n      <td>5616.317474</td>\n      <td>10041.900327</td>\n      <td>...</td>\n      <td>2.330388</td>\n      <td>2.500323</td>\n      <td>10.445704</td>\n      <td>17.818192</td>\n      <td>9.870522e-07</td>\n      <td>75071.732023</td>\n      <td>34.908566</td>\n      <td>4.616689</td>\n      <td>0.425430</td>\n      <td>8.29</td>\n      <td>1.64</td>\n      <td>3.61</td>\n      <td>3.93</td>\n      <td>5.29</td>\n      <td>5.65</td>\n      <td>0.000004</td>\n      <td>75068.641763</td>\n      <td>8.29</td>\n      <td>4.591089</td>\n      <td>2.652471</td>\n      <td>17.429133</td>\n      <td>2.524292</td>\n      <td>2.674261</td>\n      <td>2.731556</td>\n      <td>10.600250</td>\n      <td>15.267357</td>\n      <td>5.168715e-06</td>\n      <td>74654.822041</td>\n      <td>17.429133</td>\n      <td>4.615960</td>\n      <td>0.239739</td>\n      <td>5.312</td>\n      <td>3.932</td>\n      <td>4.029</td>\n      <td>4.180</td>\n      <td>5.004</td>\n      <td>5.102</td>\n      <td>2.221477e-06</td>\n      <td>74653.840860</td>\n      <td>5.312</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>92.046507</td>\n      <td>370.215407</td>\n      <td>2011.169915</td>\n      <td>3849.050613</td>\n      <td>1125.200572</td>\n      <td>817.604731</td>\n      <td>19980.904036</td>\n      <td>-0.002131</td>\n      <td>0.905245</td>\n      <td>216.375637</td>\n      <td>763.119727</td>\n      <td>4205.630709</td>\n      <td>5822.021994</td>\n      <td>2335.262484</td>\n      <td>1325.996108</td>\n      <td>7109.750499</td>\n      <td>-0.024701</td>\n      <td>0.911108</td>\n      <td>370.547784</td>\n      <td>1218.976374</td>\n      <td>6861.742884</td>\n      <td>10345.391762</td>\n      <td>3757.215191</td>\n      <td>2227.180238</td>\n      <td>13184.654197</td>\n      <td>0.001787</td>\n      <td>0.909935</td>\n      <td>488.192990</td>\n      <td>1661.204091</td>\n      <td>12983.887874</td>\n      <td>17999.987164</td>\n      <td>6501.345039</td>\n      <td>4378.756680</td>\n      <td>22646.978780</td>\n      <td>0.001345</td>\n      <td>0.898393</td>\n      <td>254.215327</td>\n      <td>786.916507</td>\n      <td>8551.993824</td>\n      <td>15609.258230</td>\n      <td>...</td>\n      <td>2.233265</td>\n      <td>2.360684</td>\n      <td>6.086216</td>\n      <td>19.444142</td>\n      <td>3.704347e-05</td>\n      <td>75102.812145</td>\n      <td>96.544908</td>\n      <td>4.702929</td>\n      <td>0.452306</td>\n      <td>14.33</td>\n      <td>-2.36</td>\n      <td>3.70</td>\n      <td>4.01</td>\n      <td>5.37</td>\n      <td>5.72</td>\n      <td>0.000002</td>\n      <td>75104.760683</td>\n      <td>14.33</td>\n      <td>3.806820</td>\n      <td>4.072783</td>\n      <td>46.829865</td>\n      <td>2.452629</td>\n      <td>2.537783</td>\n      <td>2.592750</td>\n      <td>8.418312</td>\n      <td>16.453753</td>\n      <td>5.558937e-06</td>\n      <td>74690.717526</td>\n      <td>46.829865</td>\n      <td>4.701168</td>\n      <td>0.284531</td>\n      <td>5.562</td>\n      <td>3.817</td>\n      <td>3.980</td>\n      <td>4.231</td>\n      <td>5.148</td>\n      <td>5.432</td>\n      <td>-3.892617e-07</td>\n      <td>74688.057388</td>\n      <td>5.562</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>103.474480</td>\n      <td>342.880969</td>\n      <td>1725.916000</td>\n      <td>2824.536178</td>\n      <td>990.653858</td>\n      <td>601.484954</td>\n      <td>5872.137835</td>\n      <td>-0.008527</td>\n      <td>0.914271</td>\n      <td>204.654237</td>\n      <td>694.918718</td>\n      <td>3593.658089</td>\n      <td>5253.741165</td>\n      <td>2040.447941</td>\n      <td>1142.267767</td>\n      <td>7832.216209</td>\n      <td>-0.047862</td>\n      <td>0.907809</td>\n      <td>285.839096</td>\n      <td>1056.727813</td>\n      <td>5113.042600</td>\n      <td>7870.400968</td>\n      <td>3000.993734</td>\n      <td>1647.532897</td>\n      <td>11748.378766</td>\n      <td>-0.013215</td>\n      <td>0.908274</td>\n      <td>420.394064</td>\n      <td>1199.422711</td>\n      <td>7219.815234</td>\n      <td>11711.825908</td>\n      <td>3878.826467</td>\n      <td>2498.221476</td>\n      <td>16038.562751</td>\n      <td>-0.004756</td>\n      <td>0.907722</td>\n      <td>198.981008</td>\n      <td>651.099046</td>\n      <td>3889.645858</td>\n      <td>6446.635167</td>\n      <td>...</td>\n      <td>2.232994</td>\n      <td>2.376144</td>\n      <td>6.290437</td>\n      <td>13.881533</td>\n      <td>-1.184534e-06</td>\n      <td>75112.066132</td>\n      <td>28.957998</td>\n      <td>4.580902</td>\n      <td>0.366311</td>\n      <td>8.01</td>\n      <td>1.71</td>\n      <td>3.69</td>\n      <td>3.99</td>\n      <td>5.16</td>\n      <td>5.40</td>\n      <td>-0.000001</td>\n      <td>75110.212557</td>\n      <td>8.01</td>\n      <td>3.502840</td>\n      <td>1.828850</td>\n      <td>14.633082</td>\n      <td>2.488788</td>\n      <td>2.546254</td>\n      <td>2.599299</td>\n      <td>7.229396</td>\n      <td>12.682185</td>\n      <td>-1.800481e-06</td>\n      <td>74730.790892</td>\n      <td>14.633082</td>\n      <td>4.580789</td>\n      <td>0.196801</td>\n      <td>5.206</td>\n      <td>3.762</td>\n      <td>4.071</td>\n      <td>4.237</td>\n      <td>4.882</td>\n      <td>5.026</td>\n      <td>-8.926174e-07</td>\n      <td>74730.013347</td>\n      <td>5.206</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = pd.concat([y_1, y_2], axis=0)\ny = y.reset_index(drop=True)\ny.shape","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"(33000, 1)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = pd.Series(y[0].values)","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = pd.read_csv(\"../input/lanl-master-s-features-creating-0/test_X_features_10.csv\")","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\ntrain_columns = X_train.columns\n\nX_train[train_columns] = scaler.fit_transform(X_train[train_columns])\nX_test[train_columns] = scaler.transform(X_test[train_columns])","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_columns = X_train.columns\nn_fold = 5","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfolds = KFold(n_splits=n_fold, shuffle=True, random_state=42)\n\noof = np.zeros(len(X_train))\ntrain_score = []\ncat_predictions = np.zeros(len(X_test))\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train, y_train.values)):\n    print(\"fold {}\".format(fold_))\n\n    X_tr, X_val = X_train[train_columns].iloc[trn_idx], X_train[train_columns].iloc[val_idx]\n    y_tr, y_val = y_train.iloc[trn_idx], y_train.iloc[val_idx]\n\n    model = CatBoostRegressor(n_estimators=25000, verbose=-1, objective=\"MAE\", loss_function=\"MAE\", boosting_type=\"Ordered\", task_type=\"GPU\")\n    model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], verbose=2500, early_stopping_rounds=500)\n    oof[val_idx] = model.predict(X_val)\n\n    cat_predictions += model.predict(X_test[train_columns]) / folds.n_splits\n    train_score.append(model.best_score_['learn'][\"MAE\"])\n\ncv_score = mean_absolute_error(y_train, oof)\nprint(f\"After {n_fold}: test_CV = {cv_score:.3f} | train_CV = {np.mean(train_score):.3f} | {cv_score-np.mean(train_score):.3f}\", end=\" \")","execution_count":null,"outputs":[{"output_type":"stream","text":"fold 0\n0:\tlearn: 5.6534239\ttest: 5.7103149\tbest: 5.7103149 (0)\ttotal: 69.5ms\tremaining: 28m 58s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model(input_dim=10):\n    model = keras.Sequential()\n    model.add(keras.layers.Dense(256, activation=\"relu\", input_dim=input_dim))\n    model.add(keras.layers.Dropout(0.3))\n    model.add(keras.layers.Dense(128, activation=\"relu\"))\n    model.add(keras.layers.Dropout(0.3))\n    model.add(keras.layers.Dense(96, activation=\"relu\"))\n    model.add(keras.layers.Dropout(0.3))\n    model.add(keras.layers.Dense(1, activation=\"linear\"))\n\n    optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n    model.compile(optimizer=optimizer, loss='mae')\n\n    return model\n\npatience = 50\ncall_ES = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=patience, verbose=1, mode='auto', baseline=None, restore_best_weights=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfolds = KFold(n_splits=n_fold, shuffle=True, random_state=42)\n\nNN_oof = np.zeros(len(X_train))\ntrain_score = []\nNN_predictions = np.zeros(len(X_test))\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train, y_train.values)):\n    print(\"fold {}\".format(fold_))\n    \n    X_tr, X_val = X_train[train_columns].iloc[trn_idx], X_train[train_columns].iloc[val_idx]\n    y_tr, y_val = y_train.iloc[trn_idx], y_train.iloc[val_idx]\n    \n    model = create_model(X_train.shape[-1])\n    model.fit(X_tr, y_tr, epochs=500, batch_size=32, verbose=0, callbacks=[call_ES,], validation_data=[X_val, y_val])\n    \n    NN_oof[val_idx] = model.predict(X_val)[:, 0]\n    \n    NN_predictions += model.predict(X_test[train_columns])[:, 0] / folds.n_splits\n    history = model.history.history\n    tr_loss = history[\"loss\"]\n    val_loss = history[\"val_loss\"]\n    print(f\"loss: {tr_loss[-patience]:.3f} | val_loss: {val_loss[-patience]:.3f} | diff: {val_loss[-patience]-tr_loss[-patience]:.3f}\")\n    train_score.append(tr_loss[-patience])\n    \ncv_score = mean_absolute_error(y_train, NN_oof)\nprint(f\"After {n_fold}: test_CV = {cv_score:.3f} | train_CV = {np.mean(train_score):.3f} | {cv_score-np.mean(train_score):.3f}\", end=\" \")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Scirpus_predictions = pd.read_csv(\"../input/andrews-new-script-plus-a-genetic-program-model/gpI.csv\")\nScirpus_predictions.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('../input/LANL-Earthquake-Prediction/sample_submission.csv')\n\nsubmission[\"time_to_failure\"] = (cat_predictions + NN_predictions + Scirpus_predictions.time_to_failure.values) / 3\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}