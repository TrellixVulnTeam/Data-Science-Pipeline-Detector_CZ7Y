{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Imports\nimport numpy as np\nimport pandas as pd\nimport os\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import NuSVR, SVR\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.linear_model import Ridge, RidgeCV\n\nimport lightgbm as lgb\nimport xgboost as xgb\nimport time\nimport datetime\npd.options.display.precision = 15\n\nimport psutil\nimport gc\nfrom catboost import CatBoostRegressor\nimport seaborn as sns\nfrom scipy import stats\n\nimport matplotlib.pyplot as plt\nget_ipython().run_line_magic('matplotlib', 'inline')\nfrom tqdm import tqdm_notebook","execution_count":4,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Read the csv file to take input into train dataframe using pandas, \n# 2 attributes from the training data: acoustic data: records the seismic activity and\n# time_to_failure: time left for the next laboratory earthquake\ntrain_dataset = pd.read_csv('../input/train.csv', dtype={'acoustic_data': np.int16, 'time_to_failure': np.float32})","execution_count":5,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2b369f9462919c0c6e29f8a5b6183cb3c57f057a","scrolled":true},"cell_type":"code","source":"# number of instances for each segment to be specified as 150000 because each test segment has 150000 observations.\ninstances = 150000\nsegments = int(np.floor(train_dataset.shape[0] / instances))\n\n# X_trainset is the training data: acoustic data\n# y_trainset is the value to be predicted, that is the time left for the next lab earthquake\nX_trainset = pd.DataFrame(index=range(segments), dtype=np.float64)\ny_trainset = pd.DataFrame(index=range(segments), dtype=np.float64,\n                       columns=['time_to_failure'])\n\n# for every segment id, do feature engineering\nfor segment in tqdm_notebook(range(segments)):\n\n#   creating segments of size 150000 starting from the row of segment id and upto segment id + instances\n    each_seg = train_dataset.iloc[segment * instances : segment * instances + instances]\n#     create x and y having acoustic_data and time_to _failure respectively\n    x_rawdata = each_seg['acoustic_data']\n    x = x_rawdata.values\n    y = each_seg['time_to_failure'].values[-1]\n    \n#     y train data is the time_to_failure for that segment instance\n    y_trainset.loc[segment, 'time_to_failure'] = y\n    X_trainset.loc[segment, 'average'] = x.mean()   # average of all acoustic data values for segment instance\n    X_trainset.loc[segment, 'standard_deviation'] = x.std()    # standard deviation\n    X_trainset.loc[segment, 'maximum'] = x.max()    # maximum value\n    X_trainset.loc[segment, 'minimum'] = x.min()    # minimum value\n    X_trainset.loc[segment, 'quantile_1_percentile'] = np.quantile(x,0.01)    # the value below which 1% of data appears in the acoustic_data attribute\n    X_trainset.loc[segment, 'quantile_5_percentile'] = np.quantile(x,0.05)    # the value below which 5%\n    X_trainset.loc[segment, 'quantile_95_percentile'] = np.quantile(x,0.95)    # the value below which 95%\n    X_trainset.loc[segment, 'quantile_99_percentile'] = np.quantile(x,0.99)    # the value below which 99%\n    X_trainset.loc[segment, 'median_absolute'] = np.median(np.abs(x))        # median of absolute values of acoustic_data\n    X_trainset.loc[segment, 'quantile_95_percentile_absolute'] = np.quantile(np.abs(x),0.95)    # the absolute value below which 95% of absolute acoustic_data data \n    X_trainset.loc[segment, 'quantile_99_percentile_absolute'] = np.quantile(np.abs(x),0.99)    # the absolute value below which 99% of absolute acoustic_data data \n    \n#     divide the data into group of 5; each of size 30000 and perform ANOVA tests to check\n#     if thesse groups have same population mean hence helping us determine the variance of the data.\n    X_trainset.loc[segment, 'F_test_measure'], X_trainset.loc[segment, 'p_test_measure'] = stats.f_oneway(x[:30000],x[30000:60000],x[60000:90000],x[90000:120000],x[120000:])\n\n#     .diff will give the change in x with respect to it's previous value; mean of all such changes.\n    X_trainset.loc[segment, 'average_change_absolute'] = np.mean(np.diff(x))      \n    \n#     take change values and divide by itself, then consider only those which come out to be non-zero\n    X_trainset.loc[segment, 'average_change_rate'] = np.mean(np.nonzero((np.diff(x) / x[:-1]))[0])\n    X_trainset.loc[segment, 'maximum_absolute'] = np.abs(x).max()     # max of absolute values in acoustic_data\n    \n    for windows in [10,100]:\n        x_roll_std = x_rawdata.rolling(windows).std().dropna().values\n        x_roll_mean = x_rawdata.rolling(windows).mean().dropna().values\n        \n        X_trainset.loc[segment, 'average_rolling_standard_deviation' + str(windows)] = x_roll_std.mean()\n        X_trainset.loc[segment, 'standard_deviation_rolling_standard_deviation' + str(windows)] = x_roll_std.std()\n        X_trainset.loc[segment, 'maximum_rolling_standard_deviation' + str(windows)] = x_roll_std.max()\n        X_trainset.loc[segment, 'minimum_rolling_standard_deviation' + str(windows)] = x_roll_std.min()\n        X_trainset.loc[segment, 'quantile_1_percentile_rolling_standard_deviation' + str(windows)] = np.quantile(x_roll_std,0.01)\n        X_trainset.loc[segment, 'quantile_5_percentile_rolling_standard_deviation' + str(windows)] = np.quantile(x_roll_std,0.05)\n        X_trainset.loc[segment, 'quantile_95_percentile_rolling_standard_deviation' + str(windows)] = np.quantile(x_roll_std,0.95)\n        X_trainset.loc[segment, 'quantile_99_percentile_rolling_standard_deviation' + str(windows)] = np.quantile(x_roll_std,0.99)\n        X_trainset.loc[segment, 'average_change_absolute_rolling_standard_deviation' + str(windows)] = np.mean(np.diff(x_roll_std))\n        X_trainset.loc[segment, 'average_change_rate_rolling_standard_deviation' + str(windows)] = np.mean(np.nonzero((np.diff(x_roll_std) / x_roll_std[:-1]))[0])\n        X_trainset.loc[segment, 'maximum_absolute_rolling_standard_deviation' + str(windows)] = np.abs(x_roll_std).max()\n        \n        X_trainset.loc[segment, 'average_absolute_rolling_mean' + str(windows)] = x_roll_mean.mean()\n        X_trainset.loc[segment, 'standard_deviation_rolling_mean' + str(windows)] = x_roll_mean.std()\n        X_trainset.loc[segment, 'maximum_rolling_mean' + str(windows)] = x_roll_mean.max()\n        X_trainset.loc[segment, 'minimum_rolling_mean' + str(windows)] = x_roll_mean.min()\n        X_trainset.loc[segment, 'quantile_1_percentile_rolling_mean' + str(windows)] = np.quantile(x_roll_mean,0.01)\n        X_trainset.loc[segment, 'quantile_5_percentile_rolling_mean' + str(windows)] = np.quantile(x_roll_mean,0.05)\n        X_trainset.loc[segment, 'quantile_95_percentile_rolling_mean' + str(windows)] = np.quantile(x_roll_mean,0.95)\n        X_trainset.loc[segment, 'quantile_99_percentile_rolling_mean' + str(windows)] = np.quantile(x_roll_mean,0.99)\n        X_trainset.loc[segment, 'average_change_absolute_rolling_mean' + str(windows)] = np.mean(np.diff(x_roll_mean))\n        X_trainset.loc[segment, 'average_change_rate_rolling_mean' + str(windows)] = np.mean(np.nonzero((np.diff(x_roll_mean) / x_roll_mean[:-1]))[0])\n        X_trainset.loc[segment, 'maximum_absolute_rolling_mean' + str(windows)] = np.abs(x_roll_mean).max()\n","execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=0, max=4194), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53e508c7f0c745b29d7fa9577ab4dc89"}},"metadata":{}},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:43: RuntimeWarning: divide by zero encountered in true_divide\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:43: RuntimeWarning: invalid value encountered in true_divide\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:71: RuntimeWarning: divide by zero encountered in true_divide\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:71: RuntimeWarning: invalid value encountered in true_divide\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:59: RuntimeWarning: divide by zero encountered in true_divide\n","name":"stderr"},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true,"_uuid":"8a8fefba8af693620aa3598ac3de8f2ad521bbf4"},"cell_type":"code","source":"submission = pd.read_csv('../input/sample_submission.csv', index_col='seg_id')\nX_testset = pd.DataFrame(columns=X_trainset.columns, dtype=np.float64, index=submission.index)\n\nfor i, seg_id in enumerate(tqdm_notebook(X_testset.index)):\n    each_seg = pd.read_csv('../input/test/' + seg_id + '.csv')\n    \n    x_rawdata = each_seg['acoustic_data']\n    x_roll = x_rawdata.rolling(windows).std().dropna().values\n    x = x_rawdata.values\n    \n    X_testset.loc[seg_id, 'average'] = x.mean()\n    X_testset.loc[seg_id, 'standard_deviation'] = x.std()\n    X_testset.loc[seg_id, 'maximum'] = x.max()\n    X_testset.loc[seg_id, 'minimum'] = x.min()\n    X_testset.loc[seg_id, 'quantile_1_percentile'] = np.quantile(x,0.01)\n    X_testset.loc[seg_id, 'quantile_5_percentile'] = np.quantile(x,0.05)\n    X_testset.loc[seg_id, 'quantile_95_percentile'] = np.quantile(x,0.95)\n    X_testset.loc[seg_id, 'quantile_99_percentile'] = np.quantile(x,0.99)\n    X_testset.loc[seg_id, 'median_absolute'] = np.median(np.abs(x))\n    X_testset.loc[seg_id, 'quantile_95_percentile_absolute'] = np.quantile(np.abs(x),0.95)\n    X_testset.loc[seg_id, 'quantile_99_percentile_absolute'] = np.quantile(np.abs(x),0.99)\n    X_testset.loc[seg_id, 'F_test_measure'], X_trainset.loc[segment, 'p_test_measure'] = stats.f_oneway(x[:30000],x[30000:60000],x[60000:90000],x[90000:120000],x[120000:])\n    X_testset.loc[seg_id, 'average_change_absolute'] = np.mean(np.diff(x))\n    X_testset.loc[seg_id, 'average_change_rate'] = np.mean(np.nonzero((np.diff(x) / x[:-1]))[0])\n    X_testset.loc[seg_id, 'maximum_absolute'] = np.abs(x).max()\n    \n    for windows in [10,100]:\n        x_roll_std = x_rawdata.rolling(windows).std().dropna().values\n        x_roll_mean = x_rawdata.rolling(windows).mean().dropna().values\n        \n        X_testset.loc[seg_id, 'average_rolling_standard_deviation' + str(windows)] = x_roll_std.mean()\n        X_testset.loc[seg_id, 'standard_deviation_rolling_standard_deviation' + str(windows)] = x_roll_std.std()\n        X_testset.loc[seg_id, 'maximum_rolling_standard_deviation' + str(windows)] = x_roll_std.max()\n        X_testset.loc[seg_id, 'minimum_rolling_standard_deviation' + str(windows)] = x_roll_std.min()\n        X_testset.loc[seg_id, 'quantile_1_percentile_rolling_standard_deviation' + str(windows)] = np.quantile(x_roll_std,0.01)\n        X_testset.loc[seg_id, 'quantile_5_percentile_rolling_standard_deviation' + str(windows)] = np.quantile(x_roll_std,0.05)\n        X_testset.loc[seg_id, 'quantile_95_percentile_rolling_standard_deviation' + str(windows)] = np.quantile(x_roll_std,0.95)\n        X_testset.loc[seg_id, 'quantile_99_percentile_rolling_standard_deviation' + str(windows)] = np.quantile(x_roll_std,0.99)\n        X_testset.loc[seg_id, 'average_change_absolute_rolling_standard_deviation' + str(windows)] = np.mean(np.diff(x_roll_std))\n        X_testset.loc[seg_id, 'average_change_rate_rolling_standard_deviation' + str(windows)] = np.mean(np.nonzero((np.diff(x_roll_std) / x_roll_std[:-1]))[0])\n        X_testset.loc[seg_id, 'maximum_absolute_rolling_standard_deviation' + str(windows)] = np.abs(x_roll_std).max()\n        \n        X_testset.loc[seg_id, 'average_absolute_rolling_mean' + str(windows)] = x_roll_mean.mean()\n        X_testset.loc[seg_id, 'standard_deviation_rolling_mean' + str(windows)] = x_roll_mean.std()\n        X_testset.loc[seg_id, 'maximum_rolling_mean' + str(windows)] = x_roll_mean.max()\n        X_testset.loc[seg_id, 'minimum_rolling_mean' + str(windows)] = x_roll_mean.min()\n        X_testset.loc[seg_id, 'quantile_1_percentile_rolling_mean' + str(windows)] = np.quantile(x_roll_mean,0.01)\n        X_testset.loc[seg_id, 'quantile_5_percentile_rolling_mean' + str(windows)] = np.quantile(x_roll_mean,0.05)\n        X_testset.loc[seg_id, 'quantile_95_percentile_rolling_mean' + str(windows)] = np.quantile(x_roll_mean,0.95)\n        X_testset.loc[seg_id, 'quantile_99_percentile_rolling_mean' + str(windows)] = np.quantile(x_roll_mean,0.99)\n        X_testset.loc[seg_id, 'average_change_absolute_rolling_mean' + str(windows)] = np.mean(np.diff(x_roll_mean))\n        X_testset.loc[seg_id, 'average_change_rate_rolling_mean' + str(windows)] = np.mean(np.nonzero((np.diff(x_roll_mean) / x_roll_mean[:-1]))[0])\n        X_testset.loc[seg_id, 'maximum_absolute_rolling_mean' + str(windows)] = np.abs(x_roll_mean).max()","execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=0, max=2624), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6eabb6795cdd480ab9a1a31e1bd113d9"}},"metadata":{}},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:25: RuntimeWarning: divide by zero encountered in true_divide\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:25: RuntimeWarning: invalid value encountered in true_divide\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:53: RuntimeWarning: divide by zero encountered in true_divide\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:53: RuntimeWarning: invalid value encountered in true_divide\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:41: RuntimeWarning: divide by zero encountered in true_divide\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:41: RuntimeWarning: invalid value encountered in true_divide\n","name":"stderr"},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1584x1152 with 0 Axes>"},"metadata":{}}]},{"metadata":{"trusted":true,"_uuid":"9172a3903b178e151d8aefbc68e01426a39e9e2e"},"cell_type":"code","source":"num_folds = 5\nk_folds = KFold(n_splits=num_folds, shuffle=True, random_state=11)","execution_count":39,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model_lgb(X=X_trainset, X_testset=X_testset, y=y_trainset, params=None, k_folds=k_folds, model=None):\n\n  x_values = np.zeros(len(X))\n  prediction = np.zeros(len(X_testset))\n  scores = []\n  feature_importance = pd.DataFrame()\n  for fold_n, (trainset_index, valid_set_index) in enumerate(k_folds.split(X)):\n    print('Fold', fold_n, 'started at', time.ctime())\n    X_train_per_fold, X_valid_per_fold = X.iloc[trainset_index], X.iloc[valid_set_index]\n    y_train_per_fold, y_valid_per_fold = y.iloc[trainset_index], y.iloc[valid_set_index]\n\n    model = lgb.LGBMRegressor(**params, n_estimators = 20000, nthread = 4, n_jobs = -1)\n    model.fit(X_train_per_fold, y_train_per_fold, \n           eval_set=[(X_train_per_fold, y_train_per_fold), (X_valid_per_fold, y_valid_per_fold)], eval_metric='mae',\n           verbose=1000, early_stopping_rounds=200)\n\n    y_pred_valid = model.predict(X_valid_per_fold)\n    y_pred = model.predict(X_testset, num_iteration=model.best_iteration_)\n\n    x_values[valid_set_index] = y_pred_valid.reshape(-1,)\n    scores.append(mean_absolute_error(y_valid_per_fold, y_pred_valid))\n\n    prediction += y_pred\n\n  prediction /= num_folds\n  print('CV mean score: {0:.4f}.'.format(mean_absolute_error(y, x_values)))\n  return x_values, prediction\n\ndef train_model_xgb(X=X_trainset, X_testset=X_testset, y=y_trainset, params=None, k_folds=k_folds, model=None):\n\n  x_value = np.zeros(len(X))\n  prediction = np.zeros(len(X_testset))\n  scores = []\n  feature_importance = pd.DataFrame()\n  for fold_n, (trainset_index, valid_set_index) in enumerate(k_folds.split(X)):\n    print('Fold', fold_n, 'started at', time.ctime())\n    X_train_per_fold, X_valid_per_fold = X.iloc[trainset_index], X.iloc[valid_set_index]\n    y_train_per_fold, y_valid_per_fold = y.iloc[trainset_index], y.iloc[valid_set_index]\n\n    train_data = xgb.DMatrix(data=X_train_per_fold, label=y_train_per_fold, feature_names=X_trainset.columns)\n    valid_data = xgb.DMatrix(data=X_valid_per_fold, label=y_valid_per_fold, feature_names=X_trainset.columns)\n\n    watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n    model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=500, params=params)\n    y_pred_valid = model.predict(xgb.DMatrix(X_valid_per_fold, feature_names=X_trainset.columns), ntree_limit=model.best_ntree_limit)\n    y_pred = model.predict(xgb.DMatrix(X_testset, feature_names=X_trainset.columns), ntree_limit=model.best_ntree_limit)\n\n    x_value[valid_set_index] = y_pred_valid.reshape(-1,)\n    scores.append(mean_absolute_error(y_valid_per_fold, y_pred_valid))\n\n    prediction += y_pred\n\n  prediction /= num_folds\n  print('CV mean score: {0:.4f}.'.format(mean_absolute_error(y, x_value)))\n  return x_value, prediction","execution_count":40,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c532c79cb6f3417c3a9eec579979aaea91a0aa10"},"cell_type":"code","source":"lgb_params = {'num_leaves': 64,\n         'min_data_in_leaf': 50,\n         'objective': 'mae',\n         'max_depth': -1,\n         'learning_rate': 0.001,\n         \"boosting\": \"gbdt\",\n          \"feature_fraction\": 0.5,\n         \"bagging_freq\": 2,\n         \"bagging_fraction\": 0.5,\n         \"bagging_seed\": 0,\n         \"metric\": 'mae',\n         \"verbosity\": -1,\n         'reg_alpha': 1.0,\n         'reg_lambda': 1.0,\n         }\nx_value_lgb, prediction_lgb = train_model_lgb(params = lgb_params)","execution_count":41,"outputs":[{"output_type":"stream","text":"Fold 0 started at Sat Apr 27 02:21:59 2019\nTraining until validation scores don't improve for 200 rounds.\n[1000]\ttraining's l1: 2.10627\tvalid_1's l1: 2.25745\n[2000]\ttraining's l1: 1.88766\tvalid_1's l1: 2.11879\n[3000]\ttraining's l1: 1.78665\tvalid_1's l1: 2.08893\n[4000]\ttraining's l1: 1.71547\tvalid_1's l1: 2.07937\n[5000]\ttraining's l1: 1.65694\tvalid_1's l1: 2.07576\n[6000]\ttraining's l1: 1.60834\tvalid_1's l1: 2.07395\nEarly stopping, best iteration is:\n[5852]\ttraining's l1: 1.61503\tvalid_1's l1: 2.07388\nFold 1 started at Sat Apr 27 02:22:17 2019\nTraining until validation scores don't improve for 200 rounds.\n[1000]\ttraining's l1: 2.11472\tvalid_1's l1: 2.21139\n[2000]\ttraining's l1: 1.89931\tvalid_1's l1: 2.06197\n[3000]\ttraining's l1: 1.79755\tvalid_1's l1: 2.03692\n[4000]\ttraining's l1: 1.72467\tvalid_1's l1: 2.0334\nEarly stopping, best iteration is:\n[3812]\ttraining's l1: 1.73688\tvalid_1's l1: 2.03326\nFold 2 started at Sat Apr 27 02:22:29 2019\nTraining until validation scores don't improve for 200 rounds.\n[1000]\ttraining's l1: 2.10286\tvalid_1's l1: 2.25594\n[2000]\ttraining's l1: 1.8772\tvalid_1's l1: 2.14852\n[3000]\ttraining's l1: 1.77242\tvalid_1's l1: 2.13587\nEarly stopping, best iteration is:\n[3652]\ttraining's l1: 1.72276\tvalid_1's l1: 2.13374\nFold 3 started at Sat Apr 27 02:22:40 2019\nTraining until validation scores don't improve for 200 rounds.\n[1000]\ttraining's l1: 2.14208\tvalid_1's l1: 2.05428\n[2000]\ttraining's l1: 1.92058\tvalid_1's l1: 1.92666\n[3000]\ttraining's l1: 1.81505\tvalid_1's l1: 1.91104\nEarly stopping, best iteration is:\n[3356]\ttraining's l1: 1.78596\tvalid_1's l1: 1.91045\nFold 4 started at Sat Apr 27 02:22:50 2019\nTraining until validation scores don't improve for 200 rounds.\n[1000]\ttraining's l1: 2.11469\tvalid_1's l1: 2.19865\n[2000]\ttraining's l1: 1.89391\tvalid_1's l1: 2.07435\n[3000]\ttraining's l1: 1.7902\tvalid_1's l1: 2.05436\n[4000]\ttraining's l1: 1.71574\tvalid_1's l1: 2.05\nEarly stopping, best iteration is:\n[4728]\ttraining's l1: 1.67093\tvalid_1's l1: 2.04882\nCV mean score: 2.0400.\n","name":"stdout"}]},{"metadata":{"trusted":true,"_uuid":"2d01a97d68c4031b3b2d11974a17b2e7053efc88","scrolled":false},"cell_type":"code","source":"xgb_params = {'eta': 0.01,\n              'max_depth': 6,\n              'subsample': 0.8,\n              'colsample_bytree': 0.8,\n              'colsample_bylevel': 0.8,\n              'colsample_bynode': 0.8,\n              'lambda': 0.1,\n              'alpha' : 0.1,\n              'objective': 'reg:linear',\n              'eval_metric': 'mae',\n              'silent': True,\n              'nthread': 4}\nx_value_xgb, prediction_xgb = train_model_xgb(params=xgb_params)","execution_count":32,"outputs":[{"output_type":"stream","text":"Fold 0 started at Sat Apr 27 02:18:34 2019\n[0]\ttrain-mae:5.13025\tvalid_data-mae:5.26969\nMultiple eval metrics have been passed: 'valid_data-mae' will be used for early stopping.\n\nWill train until valid_data-mae hasn't improved in 200 rounds.\nStopping. Best iteration:\n[242]\ttrain-mae:1.62971\tvalid_data-mae:2.1087\n\nFold 1 started at Sat Apr 27 02:18:37 2019\n[0]\ttrain-mae:5.15714\tvalid_data-mae:5.16176\nMultiple eval metrics have been passed: 'valid_data-mae' will be used for early stopping.\n\nWill train until valid_data-mae hasn't improved in 200 rounds.\nStopping. Best iteration:\n[278]\ttrain-mae:1.60137\tvalid_data-mae:2.03861\n\nFold 2 started at Sat Apr 27 02:18:41 2019\n[0]\ttrain-mae:5.15105\tvalid_data-mae:5.18889\nMultiple eval metrics have been passed: 'valid_data-mae' will be used for early stopping.\n\nWill train until valid_data-mae hasn't improved in 200 rounds.\n[500]\ttrain-mae:1.38039\tvalid_data-mae:2.14713\nStopping. Best iteration:\n[315]\ttrain-mae:1.53907\tvalid_data-mae:2.13704\n\nFold 3 started at Sat Apr 27 02:18:46 2019\n[0]\ttrain-mae:5.19952\tvalid_data-mae:4.99284\nMultiple eval metrics have been passed: 'valid_data-mae' will be used for early stopping.\n\nWill train until valid_data-mae hasn't improved in 200 rounds.\nStopping. Best iteration:\n[278]\ttrain-mae:1.61984\tvalid_data-mae:1.94291\n\nFold 4 started at Sat Apr 27 02:18:50 2019\n[0]\ttrain-mae:5.15295\tvalid_data-mae:5.18056\nMultiple eval metrics have been passed: 'valid_data-mae' will be used for early stopping.\n\nWill train until valid_data-mae hasn't improved in 200 rounds.\nStopping. Best iteration:\n[259]\ttrain-mae:1.62081\tvalid_data-mae:2.07509\n\nCV mean score: 2.0605.\n","name":"stdout"}]},{"metadata":{"trusted":true,"_uuid":"6ed44e58c888d3891ece9bb8d01a5d56ae682eb1","scrolled":false},"cell_type":"code","source":"print(mean_absolute_error(y_trainset, (x_value_lgb)))","execution_count":42,"outputs":[{"output_type":"stream","text":"2.040027143542845\n","name":"stdout"}]},{"metadata":{"trusted":true,"_uuid":"aed2c67cf5b7596b785bb85b0943ee9fef6e8187"},"cell_type":"code","source":"prediction_lgb[:10]","execution_count":43,"outputs":[{"output_type":"execute_result","execution_count":43,"data":{"text/plain":"array([3.21973922, 5.41623869, 5.276544  , 7.78150188, 6.81169566,\n       2.36501752, 7.000268  , 4.19822395, 4.78667879, 2.30686442])"},"metadata":{}}]},{"metadata":{"trusted":true,"_uuid":"acf417f3540ca69267b1088e199e4ae358b36439"},"cell_type":"code","source":"submission['time_to_failure'] = (prediction_lgb)\nprint(submission.head())\nsubmission.to_csv('submission_all.csv')","execution_count":44,"outputs":[{"output_type":"stream","text":"              time_to_failure\nseg_id                       \nseg_00030f  3.219739223113826\nseg_0012b5  5.416238693091099\nseg_00184e  5.276543999098358\nseg_003339  7.781501883220113\nseg_0042cc  6.811695656769492\n","name":"stdout"}]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}