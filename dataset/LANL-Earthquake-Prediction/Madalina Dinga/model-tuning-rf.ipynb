{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import gc\nimport os\nimport time\nimport logging\nimport datetime\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom scipy import stats\nfrom scipy.signal import hann\nfrom tqdm import tqdm_notebook\nimport matplotlib.pyplot as plt\nfrom scipy.signal import hilbert\nfrom scipy.signal import convolve\nfrom sklearn.svm import NuSVR, SVR\nfrom catboost import CatBoostRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import KFold,StratifiedKFold, RepeatedKFold\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV,RandomizedSearchCV\nwarnings.filterwarnings(\"ignore\")\nfrom typing import TypeVar, List, Dict, Tuple\nPandasDataFrame = TypeVar('pandas.core.frame.DataFrame')\n\n# Refs: https://www.kaggle.com/byfone/basic-feature-feat-catboost\n# Refs: https://www.kaggle.com/kernels/scriptcontent/13873316/download","execution_count":1,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_kg_hide-input":false,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"IS_LOCAL = False\nif(IS_LOCAL):\n    PATH=\"../input/LANL/\"\nelse:\n    PATH=\"../input/\"\nos.listdir(PATH)\n\ntrain_X = pd.read_csv(os.path.join(PATH,'../input/basic-features-randomized/scaled_train_X_rand_shuffle.csv'))\ntrain_X = train_X.drop(train_X.columns[0], axis=1)\n\ntrain_y = pd.read_csv(os.path.join(PATH,'../input/basic-features-randomized/train_y_rand2_shuffle.csv'))\ntrain_y = train_y.drop(train_y.columns[0], axis=1)\n\ntrain_X.head()","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"       mean           ...            abs_max_roll_mean_1000\n0 -0.866159           ...                         -0.209165\n1 -1.359823           ...                         -0.213951\n2 -0.910674           ...                         -0.205975\n3 -1.208440           ...                         -0.162375\n4 -0.091303           ...                         -0.013498\n\n[5 rows x 198 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean</th>\n      <th>std</th>\n      <th>max</th>\n      <th>min</th>\n      <th>Rmean</th>\n      <th>Rstd</th>\n      <th>Rmax</th>\n      <th>Rmin</th>\n      <th>Imean</th>\n      <th>Istd</th>\n      <th>Imax</th>\n      <th>Imin</th>\n      <th>Rmean_last_5000</th>\n      <th>Rstd__last_5000</th>\n      <th>Rmax_last_5000</th>\n      <th>Rmin_last_5000</th>\n      <th>Rmean_last_15000</th>\n      <th>Rstd_last_15000</th>\n      <th>Rmax_last_15000</th>\n      <th>Rmin_last_15000</th>\n      <th>mean_change_abs</th>\n      <th>mean_change_rate</th>\n      <th>abs_max</th>\n      <th>abs_min</th>\n      <th>std_first_50000</th>\n      <th>std_last_50000</th>\n      <th>std_first_10000</th>\n      <th>std_last_10000</th>\n      <th>avg_first_50000</th>\n      <th>avg_last_50000</th>\n      <th>avg_first_10000</th>\n      <th>avg_last_10000</th>\n      <th>min_first_50000</th>\n      <th>min_last_50000</th>\n      <th>min_first_10000</th>\n      <th>min_last_10000</th>\n      <th>max_first_50000</th>\n      <th>max_last_50000</th>\n      <th>max_first_10000</th>\n      <th>max_last_10000</th>\n      <th>...</th>\n      <th>q01_roll_std_759</th>\n      <th>q05_roll_std_759</th>\n      <th>q95_roll_std_759</th>\n      <th>q99_roll_std_759</th>\n      <th>av_change_abs_roll_std_759</th>\n      <th>av_change_rate_roll_std_759</th>\n      <th>abs_max_roll_std_759</th>\n      <th>ave_roll_mean_759</th>\n      <th>std_roll_mean_759</th>\n      <th>max_roll_mean_759</th>\n      <th>min_roll_mean_759</th>\n      <th>q01_roll_mean_759</th>\n      <th>q05_roll_mean_759</th>\n      <th>q95_roll_mean_759</th>\n      <th>q99_roll_mean_759</th>\n      <th>av_change_abs_roll_mean_759</th>\n      <th>av_change_rate_roll_mean_759</th>\n      <th>abs_max_roll_mean_759</th>\n      <th>ave_roll_std_1000</th>\n      <th>std_roll_std_1000</th>\n      <th>max_roll_std_1000</th>\n      <th>min_roll_std_1000</th>\n      <th>q01_roll_std_1000</th>\n      <th>q05_roll_std_1000</th>\n      <th>q95_roll_std_1000</th>\n      <th>q99_roll_std_1000</th>\n      <th>av_change_abs_roll_std_1000</th>\n      <th>av_change_rate_roll_std_1000</th>\n      <th>abs_max_roll_std_1000</th>\n      <th>ave_roll_mean_1000</th>\n      <th>std_roll_mean_1000</th>\n      <th>max_roll_mean_1000</th>\n      <th>min_roll_mean_1000</th>\n      <th>q01_roll_mean_1000</th>\n      <th>q05_roll_mean_1000</th>\n      <th>q95_roll_mean_1000</th>\n      <th>q99_roll_mean_1000</th>\n      <th>av_change_abs_roll_mean_1000</th>\n      <th>av_change_rate_roll_mean_1000</th>\n      <th>abs_max_roll_mean_1000</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.866159</td>\n      <td>-0.269957</td>\n      <td>-0.151219</td>\n      <td>0.106208</td>\n      <td>-0.690624</td>\n      <td>-0.242498</td>\n      <td>-0.866159</td>\n      <td>0.325043</td>\n      <td>-0.208961</td>\n      <td>-0.270338</td>\n      <td>-0.376071</td>\n      <td>0.376071</td>\n      <td>-0.236758</td>\n      <td>-0.267913</td>\n      <td>-0.331788</td>\n      <td>0.364167</td>\n      <td>-0.729032</td>\n      <td>-0.281551</td>\n      <td>-0.318025</td>\n      <td>0.325043</td>\n      <td>0.120638</td>\n      <td>0.107414</td>\n      <td>-0.158886</td>\n      <td>0.0</td>\n      <td>-0.292581</td>\n      <td>-0.129222</td>\n      <td>-0.636064</td>\n      <td>-0.269932</td>\n      <td>-1.191343</td>\n      <td>-0.398134</td>\n      <td>-1.358672</td>\n      <td>-0.259505</td>\n      <td>0.306030</td>\n      <td>-0.170096</td>\n      <td>0.753611</td>\n      <td>0.345998</td>\n      <td>-0.325824</td>\n      <td>0.097631</td>\n      <td>-0.673641</td>\n      <td>-0.375425</td>\n      <td>...</td>\n      <td>-1.370965</td>\n      <td>-1.288314</td>\n      <td>-0.895859</td>\n      <td>-0.186812</td>\n      <td>-0.035866</td>\n      <td>-0.212010</td>\n      <td>-0.132986</td>\n      <td>-0.864653</td>\n      <td>-0.191683</td>\n      <td>-0.195095</td>\n      <td>0.125786</td>\n      <td>-0.162078</td>\n      <td>-0.637024</td>\n      <td>-1.026392</td>\n      <td>-0.602866</td>\n      <td>0.224650</td>\n      <td>-0.202246</td>\n      <td>-0.193860</td>\n      <td>-0.582107</td>\n      <td>-0.207736</td>\n      <td>-0.134641</td>\n      <td>-1.349465</td>\n      <td>-1.220609</td>\n      <td>-1.265844</td>\n      <td>-0.845943</td>\n      <td>-0.186879</td>\n      <td>-0.032153</td>\n      <td>0.264048</td>\n      <td>-0.134641</td>\n      <td>-0.864453</td>\n      <td>-0.202793</td>\n      <td>-0.209165</td>\n      <td>0.114181</td>\n      <td>-0.182046</td>\n      <td>-0.669546</td>\n      <td>-0.991285</td>\n      <td>-0.693224</td>\n      <td>-0.159263</td>\n      <td>0.254931</td>\n      <td>-0.209165</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-1.359823</td>\n      <td>-0.149174</td>\n      <td>-0.141383</td>\n      <td>0.148828</td>\n      <td>-0.225349</td>\n      <td>-0.177262</td>\n      <td>-1.359823</td>\n      <td>0.004611</td>\n      <td>-0.224443</td>\n      <td>-0.154322</td>\n      <td>-0.125572</td>\n      <td>0.125572</td>\n      <td>0.011912</td>\n      <td>-0.189235</td>\n      <td>-0.173442</td>\n      <td>0.279324</td>\n      <td>0.189519</td>\n      <td>-0.142955</td>\n      <td>0.062292</td>\n      <td>0.004611</td>\n      <td>-0.027405</td>\n      <td>-1.374820</td>\n      <td>-0.149948</td>\n      <td>0.0</td>\n      <td>0.025787</td>\n      <td>-0.304260</td>\n      <td>-0.494023</td>\n      <td>-0.222812</td>\n      <td>-1.989700</td>\n      <td>-0.730608</td>\n      <td>-1.629616</td>\n      <td>-0.479359</td>\n      <td>-0.060290</td>\n      <td>0.398384</td>\n      <td>0.539884</td>\n      <td>0.301323</td>\n      <td>0.116376</td>\n      <td>-0.391122</td>\n      <td>-0.556455</td>\n      <td>-0.337728</td>\n      <td>...</td>\n      <td>-0.489757</td>\n      <td>-0.532032</td>\n      <td>-0.319132</td>\n      <td>-0.082173</td>\n      <td>-0.033981</td>\n      <td>-1.218251</td>\n      <td>-0.108121</td>\n      <td>-1.361888</td>\n      <td>0.070308</td>\n      <td>-0.218407</td>\n      <td>0.014229</td>\n      <td>-0.637405</td>\n      <td>-1.476834</td>\n      <td>-1.244975</td>\n      <td>-0.720710</td>\n      <td>-0.075411</td>\n      <td>-1.185362</td>\n      <td>-0.216948</td>\n      <td>-0.234896</td>\n      <td>-0.123798</td>\n      <td>-0.106817</td>\n      <td>-1.060932</td>\n      <td>-0.593264</td>\n      <td>-0.519711</td>\n      <td>-0.338419</td>\n      <td>-0.084915</td>\n      <td>-0.030550</td>\n      <td>-1.271445</td>\n      <td>-0.106817</td>\n      <td>-1.361585</td>\n      <td>0.109248</td>\n      <td>-0.213951</td>\n      <td>-0.070868</td>\n      <td>-0.700897</td>\n      <td>-1.533654</td>\n      <td>-1.262518</td>\n      <td>-0.781938</td>\n      <td>0.080959</td>\n      <td>-1.278397</td>\n      <td>-0.213951</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.910674</td>\n      <td>-0.096915</td>\n      <td>-0.220070</td>\n      <td>0.227510</td>\n      <td>0.395018</td>\n      <td>-0.131183</td>\n      <td>-0.910674</td>\n      <td>0.083700</td>\n      <td>0.255525</td>\n      <td>-0.097339</td>\n      <td>-0.068314</td>\n      <td>0.068314</td>\n      <td>0.193654</td>\n      <td>-0.128482</td>\n      <td>-0.202969</td>\n      <td>0.229070</td>\n      <td>0.281613</td>\n      <td>-0.093124</td>\n      <td>-0.027335</td>\n      <td>0.083700</td>\n      <td>-0.471534</td>\n      <td>1.581380</td>\n      <td>-0.221448</td>\n      <td>0.0</td>\n      <td>-0.174216</td>\n      <td>0.045298</td>\n      <td>-0.365792</td>\n      <td>0.193018</td>\n      <td>-0.243173</td>\n      <td>-1.348183</td>\n      <td>-0.820420</td>\n      <td>-0.141645</td>\n      <td>0.288305</td>\n      <td>0.044534</td>\n      <td>0.411648</td>\n      <td>-0.127553</td>\n      <td>-0.161416</td>\n      <td>-0.023119</td>\n      <td>-0.361146</td>\n      <td>0.076938</td>\n      <td>...</td>\n      <td>0.779945</td>\n      <td>0.684723</td>\n      <td>0.182893</td>\n      <td>-0.119915</td>\n      <td>0.043069</td>\n      <td>1.971829</td>\n      <td>-0.190323</td>\n      <td>-0.916170</td>\n      <td>0.001522</td>\n      <td>-0.213859</td>\n      <td>-0.020491</td>\n      <td>-0.427636</td>\n      <td>-0.907629</td>\n      <td>-0.712774</td>\n      <td>-0.500066</td>\n      <td>0.126192</td>\n      <td>1.950223</td>\n      <td>-0.212443</td>\n      <td>0.037385</td>\n      <td>-0.130342</td>\n      <td>-0.182806</td>\n      <td>0.644150</td>\n      <td>0.674383</td>\n      <td>0.624823</td>\n      <td>0.084665</td>\n      <td>-0.136731</td>\n      <td>0.078407</td>\n      <td>2.296023</td>\n      <td>-0.182806</td>\n      <td>-0.917983</td>\n      <td>0.027625</td>\n      <td>-0.205975</td>\n      <td>-0.056389</td>\n      <td>-0.483923</td>\n      <td>-0.925578</td>\n      <td>-0.712820</td>\n      <td>-0.534806</td>\n      <td>0.044926</td>\n      <td>2.298970</td>\n      <td>-0.205975</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-1.208440</td>\n      <td>-0.252248</td>\n      <td>-0.138104</td>\n      <td>0.198004</td>\n      <td>-0.535532</td>\n      <td>-0.244028</td>\n      <td>-1.208440</td>\n      <td>0.352303</td>\n      <td>0.332939</td>\n      <td>-0.250578</td>\n      <td>-0.280843</td>\n      <td>0.280843</td>\n      <td>-0.097595</td>\n      <td>-0.220148</td>\n      <td>-0.289157</td>\n      <td>0.282102</td>\n      <td>-0.139797</td>\n      <td>-0.265328</td>\n      <td>-0.166044</td>\n      <td>0.352303</td>\n      <td>0.416724</td>\n      <td>-0.111127</td>\n      <td>-0.146969</td>\n      <td>0.0</td>\n      <td>-0.066788</td>\n      <td>-0.367231</td>\n      <td>-0.557934</td>\n      <td>-0.211106</td>\n      <td>-1.454900</td>\n      <td>-1.028206</td>\n      <td>-1.601530</td>\n      <td>-1.206595</td>\n      <td>-0.007115</td>\n      <td>0.433189</td>\n      <td>0.561257</td>\n      <td>0.220909</td>\n      <td>0.122046</td>\n      <td>-0.316371</td>\n      <td>-0.517393</td>\n      <td>-0.026728</td>\n      <td>...</td>\n      <td>-1.062233</td>\n      <td>-1.044394</td>\n      <td>-0.697624</td>\n      <td>-0.143581</td>\n      <td>-0.032073</td>\n      <td>-1.120191</td>\n      <td>-0.199076</td>\n      <td>-1.206632</td>\n      <td>-0.166964</td>\n      <td>-0.182302</td>\n      <td>0.085279</td>\n      <td>-0.407552</td>\n      <td>-0.944954</td>\n      <td>-1.154691</td>\n      <td>-0.597852</td>\n      <td>-0.530191</td>\n      <td>-1.096603</td>\n      <td>-0.181189</td>\n      <td>-0.496260</td>\n      <td>-0.207264</td>\n      <td>-0.194952</td>\n      <td>-0.993794</td>\n      <td>-1.107091</td>\n      <td>-1.048331</td>\n      <td>-0.644325</td>\n      <td>-0.105810</td>\n      <td>-0.032380</td>\n      <td>-0.703565</td>\n      <td>-0.194952</td>\n      <td>-1.205603</td>\n      <td>-0.179014</td>\n      <td>-0.162375</td>\n      <td>0.060149</td>\n      <td>-0.465056</td>\n      <td>-0.939802</td>\n      <td>-1.139559</td>\n      <td>-0.589724</td>\n      <td>-0.375464</td>\n      <td>-0.708218</td>\n      <td>-0.162375</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.091303</td>\n      <td>-0.049959</td>\n      <td>0.199594</td>\n      <td>-0.395392</td>\n      <td>0.860293</td>\n      <td>-0.070145</td>\n      <td>-0.091303</td>\n      <td>-0.033622</td>\n      <td>-0.255409</td>\n      <td>-0.056957</td>\n      <td>-0.040162</td>\n      <td>0.040162</td>\n      <td>0.110735</td>\n      <td>-0.184604</td>\n      <td>-0.233829</td>\n      <td>0.212317</td>\n      <td>0.131113</td>\n      <td>-0.032995</td>\n      <td>-0.132706</td>\n      <td>-0.033622</td>\n      <td>-0.545555</td>\n      <td>-0.309300</td>\n      <td>0.296926</td>\n      <td>0.0</td>\n      <td>-0.088781</td>\n      <td>0.225578</td>\n      <td>0.151646</td>\n      <td>0.957975</td>\n      <td>-0.737176</td>\n      <td>0.368894</td>\n      <td>-0.932432</td>\n      <td>-0.054869</td>\n      <td>0.252854</td>\n      <td>-1.057621</td>\n      <td>-0.229532</td>\n      <td>-2.075363</td>\n      <td>-0.184093</td>\n      <td>0.712886</td>\n      <td>0.439623</td>\n      <td>1.660209</td>\n      <td>...</td>\n      <td>0.284801</td>\n      <td>0.017844</td>\n      <td>-0.040275</td>\n      <td>-0.181253</td>\n      <td>0.151932</td>\n      <td>-0.033113</td>\n      <td>0.086451</td>\n      <td>-0.093199</td>\n      <td>-0.151842</td>\n      <td>0.000216</td>\n      <td>0.019052</td>\n      <td>0.032070</td>\n      <td>-0.105144</td>\n      <td>-0.289865</td>\n      <td>-0.254350</td>\n      <td>0.072275</td>\n      <td>-0.017637</td>\n      <td>-0.000428</td>\n      <td>-0.073037</td>\n      <td>-0.019526</td>\n      <td>0.068833</td>\n      <td>1.023482</td>\n      <td>0.348283</td>\n      <td>-0.049371</td>\n      <td>-0.090806</td>\n      <td>-0.194214</td>\n      <td>-0.321057</td>\n      <td>-0.471790</td>\n      <td>0.068833</td>\n      <td>-0.093225</td>\n      <td>-0.161436</td>\n      <td>-0.013498</td>\n      <td>0.068978</td>\n      <td>0.002854</td>\n      <td>-0.097030</td>\n      <td>-0.253533</td>\n      <td>-0.275000</td>\n      <td>0.079243</td>\n      <td>-0.474514</td>\n      <td>-0.013498</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv(os.path.join(PATH,'LANL-Earthquake-Prediction/sample_submission.csv'), index_col='seg_id')\ntest_X = pd.read_csv(os.path.join(PATH,'lanlfeatures198/feature_extraction_ds_test.csv'), index_col='seg_id')\nsubmission.shape, test_X.shape","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"((2624, 1), (2624, 198))"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"Use cross-validation (e.g., KFold)\"\"\"\ndef fold_generator(x, y, groups=None, num_folds=10, shuffle=True, seed=2019):\n    folds = KFold(num_folds, shuffle=shuffle, random_state=seed)\n    for train_index, test_index in folds.split(x, y, groups):\n        yield train_index, test_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"Perform transforms and return final estimator\"\"\"\ndef make_pipeline(estimator):\n    pipeline = Pipeline([\n        # Each item is a tuple with a name and a transformer or estimator\n        ('scaler', StandardScaler()),\n        ('model', estimator)\n    ])\n    return pipeline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"Search the hyperparameters and return the estimator with the best parameters\"\"\"\ndef search_cv(x, y, pipeline, grid, max_iter=None, num_folds=10, shuffle=True):\n    t0 = time.time()\n    \n    cv = fold_generator(x, y, num_folds=num_folds)\n    if max_iter is None:\n        # Exhaustive search over specified parameter values for an estimator (pipeline)\n        search = GridSearchCV(pipeline, grid, cv=cv,\n                              scoring='neg_mean_absolute_error')\n    else:\n        # Randomized search on hyper parameters with \n        # The number of parameter settings that are tried is given by n_iter (not all parameter values are tried out)\n        search = RandomizedSearchCV(pipeline, grid, n_iter=max_iter, cv=cv,\n                                    scoring='neg_mean_absolute_error')\n    search.fit(x, y)\n    \n    t0 = time.time() - t0\n    print(\"Best CV score: {:.4f}, time: {:.1f}s\".format(-search.best_score_, t0))\n    print(search.best_params_)\n    return search.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"Train, make predictions & plot results\"\"\"\ndef make_predictions(x, y, pipeline, num_folds=10, shuffle=True, test=None, plot=True):\n    if test is not None:\n        sub_prediction = np.zeros(test.shape[0])\n        \n    oof_prediction = np.zeros(x.shape[0])\n    # use cross-validation (10-fold cross-validation)\n    for tr_idx, val_idx in fold_generator(x, y, num_folds=num_folds):\n        pipeline.fit(x.iloc[tr_idx], y.iloc[tr_idx])\n        oof_prediction[val_idx] = pipeline.predict(x.iloc[val_idx])\n\n        if test is not None:\n            sub_prediction += pipeline.predict(test) / num_folds\n    \n    if plot:\n        plot_predictions(y, oof_prediction)\n    if test is None:\n        return oof_prediction\n    else:\n        return oof_prediction, sub_prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_predictions(y, oof_predictions):\n    \"\"\"Plot out-of-fold predictions vs actual values.\"\"\"\n    fig, axis = plt.subplots(1, 2, figsize=(14, 6))\n    ax1, ax2 = axis\n    ax1.set_xlabel('actual')\n    ax1.set_ylabel('predicted')\n    ax1.set_ylim([-5, 20])\n    ax2.set_xlabel('train index')\n    ax2.set_ylabel('time to failure')\n    ax2.set_ylim([-2, 18])\n    ax1.scatter(y, oof_predictions, color='brown')\n    ax1.plot([(0, 0), (20, 20)], [(0, 0), (20, 20)], color='blue')\n    ax2.plot(y, color='blue', label='y_train')\n    ax2.plot(oof_predictions, color='orange')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid = {\n    'model__max_depth': [4, 6, 8, 10, 12],\n    'model__max_features': ['auto', 'sqrt', 'log2'],\n    'model__min_samples_leaf': [2, 4, 8, 12, 14, 16, 20],\n    'model__min_samples_split': [2, 4, 6, 8, 12, 16, 20],\n}\nrf_pipe = make_pipeline(RandomForestRegressor(criterion='mae', n_estimators=50))\nrf_pipe = search_cv(train_X, train_y, rf_pipe, grid, max_iter=10)\nrf_oof = make_predictions(train_X, train_y, rf_pipe)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_oof, rf_sub = make_predictions(train_X, train_y, rf_pipe,\n                                  test=test_X, plot=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.time_to_failure = rf_sub\nsubmission.to_csv('submission.csv',index=True)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.3"}},"nbformat":4,"nbformat_minor":1}