{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import gc\nimport os\nimport time\nimport logging\nimport datetime\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom scipy import stats\nfrom tqdm import tqdm_notebook\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import explained_variance_score\nrows = 150000\n\ntest_path = '../input/LANL-Earthquake-Prediction/test'\nsubmission_path = '../input/LANL-Earthquake-Prediction/sample_submission.csv'\ntrain_path = '../input/LANL-Earthquake-Prediction/train.csv'\n\ntrain_df = pd.read_csv(train_path,\n                       dtype={'acoustic_data': np.int16, 'time_to_failure': np.float64})\nsegments = int(np.floor(train_df.shape[0] / rows))\nprint(\"Số lượng segments là: \", segments)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-08T06:12:04.801163Z","iopub.execute_input":"2022-01-08T06:12:04.801533Z","iopub.status.idle":"2022-01-08T06:15:45.363659Z","shell.execute_reply.started":"2022-01-08T06:12:04.801438Z","shell.execute_reply":"2022-01-08T06:15:45.362154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nsample_test_df = pd.read_csv(test_path + \"/seg_00030f.csv\")\nsample_test_df\n\nfig, ax1 = plt.subplots(figsize=(16, 8))\n\nplt.title(\"Xu hướng của acoustic_data và time_to_failure trong 1% dữ liệu (mỗi điểm cách nhau 100 bước nhảy)\")\nplt.plot(train_df.acoustic_data.values[::100], color='b')\nax1.set_ylabel('acoustic_data', color='b')\nplt.legend(['acoustic_data'])\n\nax2 = ax1.twinx()\nplt.plot(train_df.time_to_failure.values[::100], color='r')\nax2.set_ylabel('time_to_failure', color='r')\nplt.legend(['time_to_failure'], loc=(0.875, 0.9))\n\nfig, ax = plt.subplots(2,1, figsize=(20,12))\nax[0].plot(train_df.index.values[:7500000], train_df.time_to_failure.values[:7500000], c=\"red\")\nax[0].set_title(\"Đồ thị của time_to_failure trong 7.5 triệu hàng đầu tiên\")\nax[0].set_ylabel(\"time_to_failure (ms)\")\n\nax[1].plot(train_df.index.values[:7500000], train_df.acoustic_data.values[:7500000], c=\"green\")\nax[1].set_title(\"Đồ thị của acoustic_data trong 7.5 triệu hàng đầu tiên\")\nax[1].set_ylabel(\"acoustic_data\")\n\nplt.subplots(figsize=(25,8))\n\nplt.plot(train_df.index.values[:150000], train_df.time_to_failure.values[:150000], c=\"red\")\nplt.ylabel(\"Time_to_failure (ms)\")\nplt.title(\"Sự thay đổi của time_to_failure trong 150000 điểm dữ liệu đầu\")\nplt.show()\n\nfig, ax = plt.subplots(2,1,figsize=(25,10))\n\nax[0].plot(train_df.index.values[0:149999], np.diff(train_df.time_to_failure.values[0:150000]), c=\"r\")\nax[0].set_ylabel(\"Độ lớn các bước giảm\")\nax[0].set_title(\"Chênh lệch độ lớn giữa các giá trị time_to_failure\");\n\nax[1].plot(train_df.index.values[0:4000], train_df.time_to_failure.values[0:4000], c=\"r\")\nax[1].set_ylabel(\"time_to_failure (ms)\")\nax[1].set_title(\"Xu hướng của time_to_failure trong một đường ngang\");\n\ntrain_7m5 = train_df[:7500000]\ntrain_7m5.describe()\n\ntrain_7m5['acoustic_data'].hist(bins=30, range = [-15,15], align='mid')\nplt.title(\"Mật độ của acoustic_data trong 7500000 giá trị đầu\")\nplt.xlabel('acoustic data')\nplt.ylabel('examples')\nplt.show()\n\nnameOfFileTest = ['seg_0012b5.csv', 'seg_00030f.csv', 'seg_00184e.csv']\nfor name in nameOfFileTest:\n    plt.subplots(figsize=(16, 8))\n    seg = pd.read_csv(test_path  + '/' + name)\n    plt.plot(seg.acoustic_data.values, c=\"green\")\n    plt.ylabel(\"acoustic_data\")\n    plt.title(\"Test \" + name);\n    plt.show()\n    \n    X_train = pd.DataFrame(index=range(segments), dtype=np.float64)\ny_train = pd.DataFrame(index=range(segments), dtype=np.float64, columns=['time_to_failure'])\n\nxc = pd.Series(seg['acoustic_data'].values)\ndef feature_generate(df, x, seg):\n    df.loc[seg, 'ave'] = x.mean() \n    df.loc[seg, 'std'] = x.std()  \n    df.loc[seg, 'max'] = x.max()\n    df.loc[seg, 'min'] = x.min()\n    df.loc[seg, 'sum'] = x.sum()\n    df.loc[seg, 'mad'] = x.mad()\n    df.loc[seg, 'skew'] = x.skew()\n    \n    df.loc[seg, 'abs_min'] = np.abs(x).min()\n    df.loc[seg, 'abs_max'] = np.abs(x).max()\n    df.loc[seg, 'abs_mean'] = np.abs(x).mean()\n    df.loc[seg, 'abs_std'] = np.abs(x).std()\n    \n    df.loc[seg, 'q01'] = np.quantile(x,0.01)\n    df.loc[seg, 'q05'] = np.quantile(x,0.05)\n    df.loc[seg, 'q95'] = np.quantile(x,0.95)\n    df.loc[seg, 'q99'] = np.quantile(x,0.99)\n    \n    df.loc[seg, 'abs_q01'] = np.quantile(np.abs(x), 0.01)\n    df.loc[seg, 'abs_q05'] = np.quantile(np.abs(x), 0.05)\n    df.loc[seg, 'abs_q95'] = np.quantile(np.abs(x), 0.95)\n    df.loc[seg, 'abs_q99'] = np.quantile(np.abs(x), 0.99)\n    \n    df.loc[seg, 'average_first_10000'] = x[:10000].mean()\n    df.loc[seg, 'average_last_10000']  =  x[-10000:].mean()\n    df.loc[seg, 'average_first_50000'] = x[:50000].mean()\n    df.loc[seg, 'average_last_50000'] = x[-50000:].mean()\n    \n    df.loc[seg, 'std_first_10000'] = x[:10000].std()\n    df.loc[seg, 'std_last_10000']  =  x[-10000:].std()\n    df.loc[seg, 'std_first_50000'] = x[:50000].std()\n    df.loc[seg, 'std_last_50000']  =  x[-50000:].std()\n    \n    df.loc[seg, 'std_first_10000'] = x[:10000].min()\n    df.loc[seg, 'std_last_10000']  =  x[-10000:].min()\n    df.loc[seg, 'std_first_50000'] = x[:50000].min()\n    df.loc[seg, 'std_last_50000']  =  x[-50000:].min()\n    \n    df.loc[seg, 'std_first_10000'] = x[:10000].max()\n    df.loc[seg, 'std_last_10000']  =  x[-10000:].max()\n    df.loc[seg, 'std_first_50000'] = x[:50000].max()\n    df.loc[seg, 'std_last_50000']  =  x[-50000:].max()\n    \n    df.loc[seg, '10q'] = np.percentile(x, 0.10)\n    df.loc[seg, '25q'] = np.percentile(x, 0.25)\n    df.loc[seg, '50q'] = np.percentile(x, 0.50)\n    df.loc[seg, '75q'] = np.percentile(x, 0.75)\n    df.loc[seg, '90q'] = np.percentile(x, 0.90)\n    \n    zc = np.fft.fft(xc)\n    realFFT = np.real(zc)\n    imagFFT = np.imag(zc)\n    df.loc[seg, 'Rmean'] = realFFT.mean()\n    df.loc[seg, 'Rstd'] = realFFT.std()\n    df.loc[seg, 'Rmax'] = realFFT.max()\n    df.loc[seg, 'Rmin'] = realFFT.min()\n    df.loc[seg, 'Imean'] = imagFFT.mean()\n    df.loc[seg, 'Istd'] = imagFFT.std()\n    df.loc[seg, 'Imax'] = imagFFT.max()\n    df.loc[seg, 'Imin'] = imagFFT.min()\n    df.loc[seg, 'Rmean_last_5000'] = realFFT[-5000:].mean()\n    df.loc[seg, 'Rstd__last_5000'] = realFFT[-5000:].std()\n    df.loc[seg, 'Rmax_last_5000'] = realFFT[-5000:].max()\n    df.loc[seg, 'Rmin_last_5000'] = realFFT[-5000:].min()\n    df.loc[seg, 'Rmean_last_15000'] = realFFT[-15000:].mean()\n    df.loc[seg, 'Rstd_last_15000'] = realFFT[-15000:].std()\n    df.loc[seg, 'Rmax_last_15000'] = realFFT[-15000:].max()\n    df.loc[seg, 'Rmin_last_15000'] = realFFT[-15000:].min()\n    \n    for w in [10, 100, 1000, 10000]:\n        x_roll_abs_mean = x.abs().rolling(w).mean().dropna().values\n        x_roll_mean = x.rolling(w).mean().dropna().values\n        x_roll_std = x.rolling(w).std().dropna().values\n        x_roll_min = x.rolling(w).min().dropna().values\n        x_roll_max = x.rolling(w).max().dropna().values\n        \n        df.loc[seg, 'ave_roll_std_' + str(w)] = x_roll_std.mean()\n        df.loc[seg, 'std_roll_std_' + str(w)] = x_roll_std.std()\n        df.loc[seg, 'max_roll_std_' + str(w)] = x_roll_std.max()\n        df.loc[seg, 'min_roll_std_' + str(w)] = x_roll_std.min()\n        df.loc[seg, 'q01_roll_std_' + str(w)] = np.quantile(x_roll_std, 0.01)\n        df.loc[seg, 'q05_roll_std_' + str(w)] = np.quantile(x_roll_std, 0.05)\n        df.loc[seg, 'q10_roll_std_' + str(w)] = np.quantile(x_roll_std, 0.10)\n        df.loc[seg, 'q95_roll_std_' + str(w)] = np.quantile(x_roll_std, 0.95)\n        df.loc[seg, 'q99_roll_std_' + str(w)] = np.quantile(x_roll_std, 0.99)\n        \n        df.loc[seg, 'ave_roll_mean_' + str(w)] = x_roll_mean.mean()\n        df.loc[seg, 'std_roll_mean_' + str(w)] = x_roll_mean.std()\n        df.loc[seg, 'max_roll_mean_' + str(w)] = x_roll_mean.max()\n        df.loc[seg, 'min_roll_mean_' + str(w)] = x_roll_mean.min()\n        df.loc[seg, 'q05_roll_mean_' + str(w)] = np.quantile(x_roll_mean, 0.05)\n        df.loc[seg, 'q95_roll_mean_' + str(w)] = np.quantile(x_roll_mean, 0.95)\n        \n        df.loc[seg, 'ave_roll_abs_mean_' + str(w)] = x_roll_abs_mean.mean()\n        df.loc[seg, 'std_roll_abs_mean_' + str(w)] = x_roll_abs_mean.std()\n        df.loc[seg, 'q05_roll_abs_mean_' + str(w)] = np.quantile(x_roll_abs_mean, 0.05)\n        df.loc[seg, 'q95_roll_abs_mean_' + str(w)] = np.quantile(x_roll_abs_mean, 0.95)\n        \n        df.loc[seg, 'std_roll_min_' + str(w)] = x_roll_min.std()\n        df.loc[seg, 'max_roll_min_' + str(w)] = x_roll_min.max()\n        df.loc[seg, 'q05_roll_min_' + str(w)] = np.quantile(x_roll_min, 0.05)\n        df.loc[seg, 'q95_roll_min_' + str(w)] = np.quantile(x_roll_min, 0.95)\n\n        df.loc[seg, 'std_roll_max_' + str(w)] = x_roll_max.std()\n        df.loc[seg, 'min_roll_max_' + str(w)] = x_roll_max.min()\n        df.loc[seg, 'q05_roll_max_' + str(w)] = np.quantile(x_roll_max, 0.05)\n        df.loc[seg, 'q95_roll_max_' + str(w)] = np.quantile(x_roll_max, 0.95)\n    return df\n\nfor s in range(segments):\n    seg = train_df.iloc[s*150000:s*150000+150000]\n    x = pd.Series(seg['acoustic_data'].values)\n    y = seg['time_to_failure'].values[-1]\n    y_train.loc[s, 'time_to_failure'] = y\n    X_train = feature_generate(X_train,x,s)\n    \n    def plot_feature(feature, X=X_train):\n        fig, ax = plt.subplots(figsize=(20, 8)) \n        ax.set_xlabel(feature)\n        ax.set_ylabel('time_to_failure')\n        plt.title('{} - time_to_falure correlation'.format(feature), color='r')\n        plt.scatter(x = X[feature], y = y_train)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T06:15:45.366169Z","iopub.execute_input":"2022-01-08T06:15:45.366516Z","iopub.status.idle":"2022-01-08T06:34:45.724144Z","shell.execute_reply.started":"2022-01-08T06:15:45.366479Z","shell.execute_reply":"2022-01-08T06:34:45.720443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for feature in X_train.columns:\n    plot_feature(feature)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-08T06:34:45.737534Z","iopub.execute_input":"2022-01-08T06:34:45.738122Z","iopub.status.idle":"2022-01-08T06:35:34.223474Z","shell.execute_reply.started":"2022-01-08T06:34:45.738069Z","shell.execute_reply":"2022-01-08T06:35:34.222531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = X_train.drop(columns=['abs_min', 'abs_q01', 'q05_roll_abs_mean_10000', 'q95_roll_mean_10000', 'q05_roll_mean_10000', 'ave_roll_mean_10000','q05_roll_abs_mean_1000', \n                                'ave_roll_mean_1000', 'q05_roll_abs_mean_100', 'ave_roll_mean_100', 'min_roll_std_100', 'ave_roll_mean_10', \n                                'average_first_50000', 'average_last_50000', 'average_first_10000', 'average_last_10000', 'sum', 'ave'])","metadata":{"execution":{"iopub.status.busy":"2022-01-08T06:35:34.224896Z","iopub.execute_input":"2022-01-08T06:35:34.225367Z","iopub.status.idle":"2022-01-08T06:35:34.242482Z","shell.execute_reply.started":"2022-01-08T06:35:34.225294Z","shell.execute_reply":"2022-01-08T06:35:34.241381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n    \n\n    \nscaler = StandardScaler()\nscaler.fit(X_train)\nscaled_X_train = pd.DataFrame(scaler.transform(X_train), columns=X_train.columns)\n\nsubmission = pd.read_csv(submission_path, index_col='seg_id')\n\nX_test = pd.DataFrame(columns=scaled_X_train.columns, dtype=np.float64, index=submission.index)\nfor s in X_test.index:\n    seg = pd.read_csv(test_path + '/' + s + '.csv')\n    x = pd.Series(seg['acoustic_data'].values)\n    X_test = feature_generate(X_test,x,s)\n    \nX_test = X_test.drop(columns=['abs_min', 'abs_q01', 'q05_roll_abs_mean_10000', 'q95_roll_mean_10000', 'q05_roll_mean_10000', 'ave_roll_mean_10000','q05_roll_abs_mean_1000', \n                            'ave_roll_mean_1000', 'q05_roll_abs_mean_100', 'ave_roll_mean_100', 'min_roll_std_100', 'ave_roll_mean_10', \n                            'average_first_50000', 'average_last_50000', 'average_first_10000', 'average_last_10000', 'sum', 'ave'])\n\nscaled_X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n    ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-08T06:35:34.244273Z","iopub.execute_input":"2022-01-08T06:35:34.244785Z","iopub.status.idle":"2022-01-08T06:48:46.342851Z","shell.execute_reply.started":"2022-01-08T06:35:34.244743Z","shell.execute_reply":"2022-01-08T06:48:46.341725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.svm import SVR\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import KFold\nimport re\nn_fold = 5\nfolds = KFold(n_splits=n_fold, shuffle=True, random_state=42)\nscaled_X_train = scaled_X_train.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n\npredictions_svr = np.zeros(len(scaled_X_test))\n\nfor fold_, (train_idx, val_idx) in enumerate(folds.split(scaled_X_train,y_train.values)):\n    strLog = \"fold **{}**\".format(fold_)\n    print(strLog)\n    \nX_tr, X_val = scaled_X_train.iloc[train_idx], scaled_X_train.iloc[val_idx]\ny_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n\nmodel = SVR(kernel='rbf', C=1.0, epsilon=0.1, gamma='scale')\n\nmodel.fit(X_tr, y_tr)\n\ny_pred = model.predict(X_val)\nmae = mean_absolute_error(y_val, y_pred)\nprint(\"MAE: \", mae)\npredictions_svr += model.predict(scaled_X_test) / folds.n_splits","metadata":{"execution":{"iopub.status.busy":"2022-01-08T06:48:46.344355Z","iopub.execute_input":"2022-01-08T06:48:46.344595Z","iopub.status.idle":"2022-01-08T06:48:51.331717Z","shell.execute_reply.started":"2022-01-08T06:48:46.344566Z","shell.execute_reply":"2022-01-08T06:48:51.330769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = {'num_leaves': 51,\n         'min_data_in_leaf': 10, \n         'objective':'regression',\n         'max_depth': -1,\n         'learning_rate': 0.001,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.9,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.9,\n         \"bagging_seed\": 42,\n         \"metric\": 'mae',\n         \"lambda_l1\": 0.1,\n         \"verbosity\": -1,\n         \"random_state\": 42}","metadata":{"execution":{"iopub.status.busy":"2022-01-08T06:48:51.333344Z","iopub.execute_input":"2022-01-08T06:48:51.333711Z","iopub.status.idle":"2022-01-08T06:48:51.341112Z","shell.execute_reply.started":"2022-01-08T06:48:51.333666Z","shell.execute_reply":"2022-01-08T06:48:51.340284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.svm import SVR\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import KFold\nimport re\nn_fold = 5\nfolds = KFold(n_splits=n_fold, shuffle=True, random_state=42)\nscaled_X_train = scaled_X_train.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n\npredictions_svr = np.zeros(len(scaled_X_test))\n\nfor fold_, (train_idx, val_idx) in enumerate(folds.split(scaled_X_train,y_train.values)):\n    strLog = \"fold {}\".format(fold_)\n    print(strLog)\n    \n    X_tr, X_val = scaled_X_train.iloc[train_idx], scaled_X_train.iloc[val_idx]\n    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n\n    model = SVR(kernel='rbf', C=1.0, epsilon=0.1, gamma='scale')\n    \n    model.fit(X_tr, y_tr)\n    \n    y_pred = model.predict(X_val)\n    mae = mean_absolute_error(y_val, y_pred)\n    print(\"MAE: \", mae)\n    predictions_svr += model.predict(scaled_X_test) / folds.n_splits","metadata":{"execution":{"iopub.status.busy":"2022-01-08T06:48:51.342541Z","iopub.execute_input":"2022-01-08T06:48:51.343123Z","iopub.status.idle":"2022-01-08T06:49:16.120876Z","shell.execute_reply.started":"2022-01-08T06:48:51.343085Z","shell.execute_reply":"2022-01-08T06:49:16.119911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.score(X_tr, y_tr)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T06:49:16.125051Z","iopub.execute_input":"2022-01-08T06:49:16.125467Z","iopub.status.idle":"2022-01-08T06:49:18.370157Z","shell.execute_reply.started":"2022-01-08T06:49:16.125417Z","shell.execute_reply":"2022-01-08T06:49:18.368981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = {'num_leaves': 51,\n         'min_data_in_leaf': 10, \n         'objective':'regression',\n         'max_depth': -1,\n         'learning_rate': 0.001,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.9,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.9,\n         \"bagging_seed\": 42,\n         \"metric\": 'mae',\n         \"lambda_l1\": 0.1,\n         \"verbosity\": -1,\n         \"random_state\": 42}","metadata":{"execution":{"iopub.status.busy":"2022-01-08T07:02:26.085818Z","iopub.execute_input":"2022-01-08T07:02:26.086537Z","iopub.status.idle":"2022-01-08T07:02:26.094655Z","shell.execute_reply.started":"2022-01-08T07:02:26.086491Z","shell.execute_reply":"2022-01-08T07:02:26.093418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import KFold\nimport re\nn_fold = 5\nfolds = KFold(n_splits=n_fold, shuffle=True, random_state=42)\nscaled_X_train = scaled_X_train.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n\noof = np.zeros(len(scaled_X_train))\npredictions = np.zeros(len(scaled_X_test))\n\nfor fold_, (train_idx, val_idx) in enumerate(folds.split(scaled_X_train,y_train.values)):\n    strLog = \"fold {}\".format(fold_)\n    print(strLog)\n    \n    X_tr, X_val = scaled_X_train.iloc[train_idx], scaled_X_train.iloc[val_idx]\n    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n\n    model = lgb.LGBMRegressor(**params, n_estimators = 10000, n_jobs = -1)\n    \n    model.fit(X_tr, y_tr, eval_set=[(X_tr, y_tr), (X_val, y_val)], eval_metric='mae',verbose=1000, early_stopping_rounds=500)\n    \n    oof[val_idx] = model.predict(X_val, num_iteration=model.best_iteration_)\n    predictions += model.predict(scaled_X_test, num_iteration=model.best_iteration_) / folds.n_splits","metadata":{"execution":{"iopub.status.busy":"2022-01-08T07:12:29.484518Z","iopub.execute_input":"2022-01-08T07:12:29.485094Z","iopub.status.idle":"2022-01-08T07:17:30.839928Z","shell.execute_reply.started":"2022-01-08T07:12:29.485046Z","shell.execute_reply":"2022-01-08T07:17:30.839151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}