{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\n\nfrom IPython.display import HTML\n\nimport os\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Reading the whole data\nNote: LARGE file, decrease the bit of float to 32 to avoid crush.\n(It took couple minutes to load the training file alone)"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv',dtype={'acoustic_data': np.int16, 'time_to_failure': np.float32})\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### A look at the whole data\n- There are only about 16 \"earthquakes\" happened in the training data. \n- Each \"earthquake\" seems to happend right after the unusually large activity in the acoustic data. "},{"metadata":{"trusted":true},"cell_type":"code","source":"measurement = train['acoustic_data'].values[::100]\nttf = train['time_to_failure'].values[::100]\nfig, ax1 = plt.subplots(figsize=(12, 8))\n# plt.title(title)\nplt.plot(measurement, color='black')\nax1.set_ylabel('acoustic data', color='black')\nplt.legend(['acoustic data'], loc=(0.01, 0.95))\nax2 = ax1.twinx()\nplt.plot(ttf, color='blue')\nax2.set_ylabel('time to failure', color='blue')\nplt.legend(['time to failure'], loc=(0.01, 0.9))\nplt.grid(True)\n\ndel measurement\ndel ttf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### First ~10% of training data"},{"metadata":{"trusted":true},"cell_type":"code","source":"measurement = train['acoustic_data'].values[:150000]\nttf = train['time_to_failure'].values[:150000]\nfig, ax1 = plt.subplots(figsize=(12, 8))\n# plt.title(title)\nplt.plot(measurement, color='black')\nax1.set_ylabel('acoustic data', color='black')\nplt.legend(['acoustic data'], loc=(0.01, 0.95))\nax2 = ax1.twinx()\nplt.plot(ttf, color='blue')\nax2.set_ylabel('time to failure', color='blue')\nplt.legend(['time to failure'], loc=(0.01, 0.9))\nplt.grid(True)\n\ndel measurement\ndel ttf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Basic Idea\n1. Divide the training data into segments the same length as test files (which all have 150,000 rows). \n2. Do feature engineering on both training segments and testing segments.\n3. The feature to predict for each segment is the \"time-to-failure\" at the last row of each segment."},{"metadata":{"trusted":true},"cell_type":"code","source":"tests = os.listdir(\"../input/test/\")\ntest1 = pd.read_csv(\"../input/test/\"+tests[2])\ntest1.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('There are %d test segments in the data.' %len(tests))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 4194 training segments if we divide the training data back-to-back. But there's no need to keep to a back-to-back dividing, so we can randomly select the starting point and generate more segments as needed. But keep in mind if we divide the training segments randomly, these segments are NOT independent."},{"metadata":{"trusted":true},"cell_type":"code","source":"np.floor(train.shape[0]/test1.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rows = 150000\nsegments = int(np.floor(train.shape[0]/rows))\ntrain_X = pd.DataFrame(dtype=np.float64)\n#train_y = pd.DataFrame(index=range(segments), dtype=np.float64, columns=['time_to_failure'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_features(seg_id,seg,train_X):\n    from scipy.stats import kurtosis\n    data = seg['acoustic_data']\n    features = {}\n    features['mean'] = seg['acoustic_data'].mean()\n    features['abs_mean']= seg['acoustic_data'].abs().mean()\n    features['std'] = seg['acoustic_data'].std()\n    features['min'] = seg['acoustic_data'].min()\n    features['max'] = seg['acoustic_data'].max()\n    features['range'] = features['max'] - features['min']\n    features['abs_min'] = seg['acoustic_data'].abs().min()\n    features['abs_max'] = seg['acoustic_data'].abs().max()\n    features['argmax'] = seg['acoustic_data'].argmax()\n    features['argmin'] = seg['acoustic_data'].argmin()\n    features['distance'] = np.abs(features['argmax'] - features['argmin'])\n    features['skew'] = seg['acoustic_data'].skew()\n    features['kurtosis'] = seg['acoustic_data'].kurt()\n    ffts = np.fft.fft(seg['acoustic_data'].values)\n    reals = np.array([x.real for x in ffts])\n    imags = np.array([x.imag for x in ffts])\n    features['real_fft_mean'] = np.mean(reals)\n    features['real_fft_abs_mean']= np.mean(np.abs(reals))\n    features['real_fft_std'] = np.std(reals)\n    features['real_fft_min'] = np.min(reals)\n    features['real_fft_max'] = np.max(reals)\n    features['real_fft_range'] = features['real_fft_max'] - features['real_fft_min']\n    features['real_fft_abs_min'] = np.min(np.abs(reals))\n    features['real_fft_abs_max'] = np.max(np.abs(reals))\n    features['real_fft_argmax'] = np.argmax(reals)\n    features['real_fft_argmin'] = np.argmin(reals)\n    features['real_fft_distance'] = np.abs(features['real_fft_argmax'] - features['real_fft_argmin'])\n    features['real_fft_skew'] = np.skew(reals)\n    features['real_fft_kurtosis'] = kurtosis(reals)\n    features['imag_fft_mean'] = np.mean(imags)\n    features['imag_fft_abs_mean']= np.mean(np.abs(imags))\n    features['imag_fft_std'] = np.std(imags)\n    features['imag_fft_min'] = np.min(imags)\n    features['imag_fft_max'] = np.max(imags)\n    features['imag_fft_range'] = features['imag_fft_max'] - features['imag_fft_min']\n    features['imag_fft_abs_min'] = np.min(np.abs(imags))\n    features['imag_fft_abs_max'] = np.max(np.abs(imags))\n    features['imag_fft_argmax'] = np.argmax(imags)\n    features['imag_fft_argmin'] = np.argmin(imags)\n    features['imag_fft_distance'] = np.abs(features['imag_fft_argmax'] - features['imag_fft_argmin'])\n    features['imag_fft_skew'] = np.skew(imags)\n    features['imag_fft_kurtosis'] = kurtosis(imags)\n    f = pd.DataFrame(list(features.values()))\n    f = f.T\n    f.columns = list(features.keys())\n    train_X = train_X.append(f)\n    return train_X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_y = []\nfor seg_id in range(segments):\n    seg = train.iloc[seg_id*rows:seg_id*rows+rows,:]\n    train_X = build_features(seg_id, seg, train_X)\n    train_y.append(seg['time_to_failure'].values[-1])\n\ntrain_X = train_X.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"i = 0\ntest_X = pd.DataFrame(dtype=np.float64)\nseg_ids = []\nfrom re import sub\nfor test_file in tests:\n    test_data = pd.read_csv(\"../input/test/\"+test_file)\n    seg_ids.append(sub('.csv','',test_file))\n    test_X = build_features(i, test_data, test_X)\n    i += 1\ntest_X = test_X.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GroupKFold, cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor\nmodel = RandomForestRegressor(n_estimators=500, criterion='mae', max_depth=20, bootstrap=True, n_jobs=-1)\nkfold = GroupKFold(n_splits=10).get_n_splits(train_X, train_y)\nresults = -1*cross_val_score(model, train_X, train_y, cv=kfold, n_jobs=-1,scoring='neg_mean_absolute_error')\nprint(\"Cross validation mae is: %.2f (%.2f)\" % (results.mean(), results.std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(train_X,train_y)\npredictions = model.predict(test_X)\nsubmission = pd.DataFrame({'seg_id':seg_ids,'time_to_failure':predictions})\nsubmission.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}