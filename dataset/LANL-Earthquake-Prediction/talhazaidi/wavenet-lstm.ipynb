{"cells":[{"metadata":{},"cell_type":"markdown","source":"# LSTM WORKING "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[{"output_type":"stream","text":"['sample_submission.csv', 'test', 'train.csv']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import NuSVR\nfrom sklearn.metrics import mean_absolute_error","execution_count":2,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv', dtype={'acoustic_data': np.float32, 'time_to_failure': np.float32}).values","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train [0:150000, 0 ] .mean(axis=0)","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"4.8841133"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pandas doesn't show us all the decimals\npd.options.display.precision = 15","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rows = 150_000\nsegments = int(np.floor(train.shape[0] / rows))\nprint('train.shape',train.shape)\nsegments\n","execution_count":6,"outputs":[{"output_type":"stream","text":"train.shape (629145480, 2)\n","name":"stdout"},{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"4194"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = pd.DataFrame(index=range(segments), dtype=np.float64,\n                       columns=['ave', 'std', 'max', 'min'])\ny_train = pd.DataFrame(index=range(segments), dtype=np.float64,\n                       columns=['time_to_failure'])\ny_train.shape\n\n\n\nsegments *rows+rows","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a training file with simple derived features\n \n# total train.shape (629145480, 2)\n\nrows = 150_000\nsegments = int(np.floor(train.shape[0] / rows))                         # 4194\n\nX_train = pd.DataFrame(index=range(segments), dtype=np.float64,\n                       columns=['ave', 'std', 'max', 'min'])             # 4194,4\ny_train = pd.DataFrame(index=range(segments), dtype=np.float64,\n                       columns=['time_to_failure'])                      # 4194,1\n\nfor segment in tqdm(range(segments)):                          # for loop from 1 to 4194 segment value\n    seg = train.iloc[segment*rows:segment*rows+rows]           # seg.shape (150000,2)  i-e first iteration (0-150000) then (150000-300000) then (300000-4500000) ...... (629100000-629250000)\n    x = seg['acoustic_data'].values                            # x.shape (150000,)\n    y = seg['time_to_failure'].values[-1]                      # i-e last time_to_failure value of every 150000 length segment. \n    \n    y_train.loc[segment, 'time_to_failure'] = y       # 4194,1      ie go on from (1,1) to (4194,1)\n    \n    X_train.loc[segment, 'ave'] = x.mean()         # 4194,4          ie go on from (1,4) to (4194,4)\n    X_train.loc[segment, 'std'] = x.std()          # 4194,4          ie go on from (1,4) to (4194,4)\n    X_train.loc[segment, 'max'] = x.max()          # 4194,4          ie go on from (1,4) to (4194,4)\n    X_train.loc[segment, 'min'] = x.min()          # 4194,4          ie go on from (1,4) to (4194,4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Extracting Features**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_features(z):\n     return np.c_[z.mean(axis=0), \n                  z.std(axis=0),\n                  z.max(axis=0),\n                  z.min(axis=0),\n                #  np.transpose(np.percentile(np.abs(z), q=[0, 50, 75, 100], axis=0)) .reshape(1,4)\n                 ]","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_X(x, last_index=None, n_steps=150, step_length=1000):\n    if last_index == None:\n        last_index=len(x)\n       \n    assert last_index - n_steps * step_length >= 0\n\n    # Reshaping and approximate standardization with mean 5 and std 3.\n    # ORIGINAL: I changed this becuase I got an No OpKernel was registered to support Op 'CudnnRNN' error\n    #temp = (x[(last_index - n_steps * step_length):last_index].reshape(n_steps, -1) - 5 ) / 3\n    # MY CHANGE: This doesn't fix things, I get the same errors\n    temp = (x[(int(last_index) - n_steps * step_length):int(last_index)].reshape(n_steps,step_length ).astype(np.float32) - 5 ) / 3\n    \n    # Extracts features of sequences of full length 1000, of the last 100 values and finally also \n    # of the last 10 observations. \n    return np.c_[extract_features(temp),\n                 extract_features(temp [ -n_steps // 10:]),\n                 extract_features(temp [ -n_steps // 100:])]\n\n","execution_count":8,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating Training Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Query \"create_X\" to figure out the number of features\nn_features = create_X(train [0:150000,0]).shape[1]\nprint(\"Our RNN is based on %i features\"% n_features)     # 18 features each row of segment ie 150x18 features of 150000 chunk input\n\nn_steps=150\nstep_length=1000\nmaxsize=train .shape[0]\nseg = int(np.floor(maxsize / (n_steps*step_length))) \nbatch_size = seg-1   # (4193,) \nxx=350\n\n\n##############################################################################################\nrows_initialize = np.zeros((seg), dtype=float)\nprint(rows_initialize.shape)\n\nfor seg1 in tqdm(range(1,seg)) :      # for loop from 1 to 4194 segment value\n    rows_initialize [seg1] = seg1 * (n_steps*step_length) \n\nrows=np.delete(rows_initialize,0)    # (4193,)\n\nprint(rows.shape)\n\n########################################################################################\nbatch_size=batch_size-xx    # training data\n#batch_size=xx              # validation data\nsplit_point=xx\nsecond_earthquake = rows[xx]\n\n\n\n##########################################################################################\n\nif batch_size < 1000  :    # validation set \n               rows_1 = rows[:split_point+1]    #  0:350 \n        \nif batch_size > 1000 :   # training set\n               rows_1 = rows[split_point+1 :]    # (351,) ie 351:4193    \n            \n\n       \n    # Initialize feature matrices and targets\nsamples_tr= np.zeros((rows_1.shape[0], step_length, n_features), dtype=float)   # (16, 150, 18)  for validation (350, 1, 24)  for training ( 3843, 1, 24) \ntargets_tr = np.zeros(rows_1.shape[0], )    # (16,)  for validation (350)    for training ( 3843)\n        \nfor j, row in enumerate(rows_1):             # 16 for validation (350)    for training ( 3843)\n    samples_tr[j] = create_X(train[:, 0], last_index=row, n_steps=n_steps, step_length=step_length)\n    targets_tr[j] = train[int(row - 1), 1]         \n    \n    \n################################################################################################\n\nprint('samples_tr shape', samples_tr.shape)\nprint('targets_tr shape', targets_tr.shape)\n\n\n","execution_count":9,"outputs":[{"output_type":"stream","text":"100%|██████████| 4193/4193 [00:00<00:00, 1529279.71it/s]","name":"stderr"},{"output_type":"stream","text":"Our RNN is based on 12 features\n(4194,)\n(4193,)\n","name":"stdout"},{"output_type":"stream","text":"\n","name":"stderr"},{"output_type":"stream","text":"samples_tr shape (3842, 1000, 12)\ntargets_tr shape (3842,)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Creating Validation data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#batch_size=batch_size-xx    # training data\nbatch_size=xx              # validation data\nsplit_point=xx\nsecond_earthquake = rows[xx]\n\n##########################################################################################\n\nif batch_size < 1000  :    # validation set \n               rows_1 = rows[:split_point+1]    #  0:350 \n        \nif batch_size > 1000 :   # training set\n               rows_1 = rows[split_point+1 :]    # (351,) ie 351:4193    \n            \n       \n    # Initialize feature matrices and targets\nsamples_vd= np.zeros((rows_1.shape[0], step_length, n_features), dtype=float)   #   for validation (350, 1, 24) \ntargets_vd = np.zeros(rows_1.shape[0], )    #  for validation (350)    \n        \nfor j, row in enumerate(rows_1):             # 16 for validation (350)    for training ( 3843)\n    samples_vd[j] = create_X(train[:, 0], last_index=row, n_steps=n_steps, step_length=step_length)\n    targets_vd[j] = train[int(row - 1), 1]  \n    \n    \nprint('samples_vd shape', samples_vd.shape)\nprint('targets_vd shape',targets_vd.shape)  \n#print('rows_1 shape',rows_1.shape[0])\n    ","execution_count":10,"outputs":[{"output_type":"stream","text":"samples_vd shape (351, 1000, 12)\ntargets_vd shape (351,)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Query \"create_X\" to figure out the number of features\nn_features = create_X(train [0:150000,0]).shape[1]\nprint(\"Our RNN is based on %i features\"% n_features)     # 18 features each row of segment ie 150x18 features of 150000 chunk input\n\nn_steps=150000\nstep_length=1\n\n# initialization for rows\nmaxsize=train .shape[0]\nseg = int(np.floor(maxsize / (n_steps*step_length)))    #4194\nsegment = (n_steps*step_length)* seg    # (150000* 4194) = 629100000\nrows = np.zeros(seg, )    # (4194,)          \n          \nfor seg1 in tqdm(range(1,seg)) :      # for loop from 1 to 4194 segment value\n    rows [seg1] = seg1 * (n_steps*step_length) \n\nrows=np.delete(rows,0)    # (4193,) \n\n# The generator endlessly selects \"batch_size\" ending positions of sub-time series. For each ending position,\n# the \"time_to_failure\" serves as target, while the features are created by the function \"create_X\".\ndef generator(data, min_index=0, max_index=None, batch_size=16, n_steps=150000, split_point=350, step_length=1):    \n    if max_index is None:\n        max_index = len(data) - 1     # data = float_data  len= 629145480-1\n     \n   # while True:\n       # # Pick indices of ending positions\n #####       rows_1 = np.random.randint(min_index + n_steps * step_length, max_index, size=batch_size)    # produce 16 random number bw min index and max index\n              \n        if batch_size < 1000  :    # validation set \n               rows_1 = rows[:split_point+1]    #  0:350 \n        \n        if batch_size > 1000 :   # training set\n               rows_1 = rows[split_point+1 :]    # (351,) ie 351:4193    \n            \n\n       \n    # Initialize feature matrices and targets\n        samples = np.zeros((batch_size, step_length, n_features), dtype=float)   # (16, 150, 18)  for validation (350, 1, 24)  for training ( 3843, 1, 24) \n        targets = np.zeros(batch_size, )    # (16,)  for validation (350)    for training ( 3843)\n        \n        for j, row in enumerate(rows_1):             # 16 for validation (350)    for training ( 3843)\n            samples[j] = create_X(data[:, 0], last_index=row, n_steps=n_steps, step_length=step_length)\n            targets[j] = data[int(row - 1), 1]\n        \n    return samples, targets\n        \n       \nbatch_size = seg-1   # (4193,) \n###### batch_size = 32\n\n# Position of second (of 16) earthquake. Used to have a clean split\n# between train and validation\nxx=350\nsecond_earthquake = rows[xx]\n\n#float_data[second_earthquake, 1] # time failure corresponding to this second earthquake indices\n\n# Initialize generators\n#train_gen = generator(float_data, batch_size=batch_size) # Use this for better score\n\ntrain_gen = generator(train , batch_size=batch_size-xx, split_point=xx, min_index=second_earthquake + 1)     # batch_size-xx =  3843\n#valid_gen = generator(train , batch_size=xx, max_index=second_earthquake)                    # xx =  350\n\n######train_gen = generator(float_data, batch_size=batch_size, split_point=xx, min_index=second_earthquake + 1) \n######valid_gen = generator(float_data, batch_size=batch_size, max_index=second_earthquake)\n\n\n#print(samples.shape)\n#print(targets.shape) \n","execution_count":11,"outputs":[{"output_type":"stream","text":"100%|██████████| 4193/4193 [00:00<00:00, 1047015.34it/s]","name":"stderr"},{"output_type":"stream","text":"Our RNN is based on 12 features\n","name":"stdout"},{"output_type":"stream","text":"\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"# Define the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, CuDNNGRU, SimpleRNN, LSTM\nfrom keras.optimizers import adam\nfrom keras.callbacks import ModelCheckpoint\n\ncb = [ModelCheckpoint(\"model.hdf5\", save_best_only=True, period=3)]\n\nmodel = Sequential()\nmodel.add(LSTM(48,dropout=0.2, recurrent_dropout=0.2, input_shape=(None, n_features), return_sequences=True))\nmodel.add(LSTM(20,dropout=0.2, recurrent_dropout=0.2, return_sequences=False))\n####model.add(CuDNNGRU(48, input_shape=(None, n_features)))\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dense(1, activation='linear'))\n\nmodel.summary()","execution_count":12,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"},{"output_type":"stream","text":"WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\nWARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nlstm_1 (LSTM)                (None, None, 48)          11712     \n_________________________________________________________________\nlstm_2 (LSTM)                (None, 20)                5520      \n_________________________________________________________________\ndense_1 (Dense)              (None, 100)               2100      \n_________________________________________________________________\ndense_2 (Dense)              (None, 1)                 101       \n=================================================================\nTotal params: 19,433\nTrainable params: 19,433\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Compile and fit model"},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nfrom keras.optimizers import RMSprop\nopt = keras.optimizers.adam(lr=.01)\n\nmodel.compile(loss=\"mae\",\n              optimizer=opt, metrics=['mean_absolute_error'])\n             # metrics=['accuracy'])\n\n\nbatch_size = 128 # mini-batch with 32 examples\nepochs = 10\nhistory = model.fit(\n    samples_tr, targets_tr,\n    batch_size=batch_size,\n    epochs=epochs,\n    verbose=1,\n    validation_data=(samples_vd  ,targets_vd ))","execution_count":15,"outputs":[{"output_type":"stream","text":"Train on 3842 samples, validate on 351 samples\nEpoch 1/10\n3842/3842 [==============================] - 78s 20ms/step - loss: 2.7765 - mean_absolute_error: 2.7765 - val_loss: 3.0701 - val_mean_absolute_error: 3.0701\nEpoch 2/10\n3842/3842 [==============================] - 77s 20ms/step - loss: 2.8524 - mean_absolute_error: 2.8524 - val_loss: 2.6600 - val_mean_absolute_error: 2.6600\nEpoch 3/10\n3842/3842 [==============================] - 77s 20ms/step - loss: 2.5537 - mean_absolute_error: 2.5537 - val_loss: 2.6698 - val_mean_absolute_error: 2.6698\nEpoch 4/10\n3842/3842 [==============================] - 76s 20ms/step - loss: 2.6146 - mean_absolute_error: 2.6146 - val_loss: 2.6289 - val_mean_absolute_error: 2.6289\nEpoch 5/10\n3842/3842 [==============================] - 76s 20ms/step - loss: 2.5442 - mean_absolute_error: 2.5442 - val_loss: 2.6386 - val_mean_absolute_error: 2.6386\nEpoch 6/10\n3842/3842 [==============================] - 76s 20ms/step - loss: 2.6095 - mean_absolute_error: 2.6095 - val_loss: 2.4945 - val_mean_absolute_error: 2.4945\nEpoch 7/10\n3842/3842 [==============================] - 76s 20ms/step - loss: 2.5758 - mean_absolute_error: 2.5758 - val_loss: 2.4873 - val_mean_absolute_error: 2.4873\nEpoch 8/10\n3842/3842 [==============================] - 76s 20ms/step - loss: 2.4936 - mean_absolute_error: 2.4936 - val_loss: 2.4135 - val_mean_absolute_error: 2.4135\nEpoch 9/10\n3842/3842 [==============================] - 76s 20ms/step - loss: 2.4535 - mean_absolute_error: 2.4535 - val_loss: 2.7134 - val_mean_absolute_error: 2.7134\nEpoch 10/10\n3842/3842 [==============================] - 76s 20ms/step - loss: 2.5470 - mean_absolute_error: 2.5470 - val_loss: 2.4740 - val_mean_absolute_error: 2.4740\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer=adam(lr=0.0005), loss=\"mae\", metrics=['accuracy'])\n\nhistory = model.fit_generator(train_gen,\n                              steps_per_epoch=1000,\n                              epochs=30,\n                              verbose=1,\n                              callbacks=cb,\n                              validation_data=valid_gen,\n                              validation_steps=351)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Load submission file\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('../input/sample_submission.csv', index_col='seg_id', dtype={\"time_to_failure\": np.float32})","execution_count":16,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prepare submission data\nLoad each test data, create the feature matrix, get numeric prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, seg_id in enumerate(tqdm(submission.index)):\n  #  print(i)\n    seg = pd.read_csv('../input/test/' + seg_id + '.csv')\n    x = seg['acoustic_data'].values\n    submission.time_to_failure[i] = model.predict(np.expand_dims(create_X(x), 0))\n\nsubmission.head()","execution_count":17,"outputs":[{"output_type":"stream","text":"100%|██████████| 2624/2624 [21:02<00:00,  2.06it/s]\n","name":"stderr"},{"output_type":"execute_result","execution_count":17,"data":{"text/plain":"              time_to_failure\nseg_id                       \nseg_00030f  5.253851890563965\nseg_0012b5  5.233308792114258\nseg_00184e  5.474508285522461\nseg_003339  8.704728126525879\nseg_0042cc  5.388502120971680","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>time_to_failure</th>\n    </tr>\n    <tr>\n      <th>seg_id</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>seg_00030f</th>\n      <td>5.253851890563965</td>\n    </tr>\n    <tr>\n      <th>seg_0012b5</th>\n      <td>5.233308792114258</td>\n    </tr>\n    <tr>\n      <th>seg_00184e</th>\n      <td>5.474508285522461</td>\n    </tr>\n    <tr>\n      <th>seg_003339</th>\n      <td>8.704728126525879</td>\n    </tr>\n    <tr>\n      <th>seg_0042cc</th>\n      <td>5.388502120971680</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Save submission file"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv')","execution_count":18,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# OTHER TECHNIQUES"},{"metadata":{"trusted":true},"cell_type":"code","source":"x.mean()  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\nscaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_scaled","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.values.flatten()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\n## Train the  Model\nmodel = RandomForestRegressor(n_estimators=200)\nmodel.fit(X_train_scaled, y_train.values.flatten())      # .fit used for training\ny_pred = model.predict(X_train_scaled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# number support vector regressor\n\n# svm = NuSVR()\n# svm.fit(X_train_scaled, y_train.values.flatten())\n# y_pred = svm.predict(X_train_scaled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(6, 6))\nplt.scatter(y_train.values.flatten(), y_pred)\nplt.xlim(0, 20)\nplt.ylim(0, 20)\nplt.xlabel('actual', fontsize=12)\nplt.ylabel('predicted', fontsize=12)\nplt.plot([(0, 0), (20, 20)], [(0, 0), (20, 20)])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = mean_absolute_error(y_train.values.flatten(), y_pred)\nprint(f'Score: {score:0.3f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('../input/sample_submission.csv', index_col='seg_id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = pd.DataFrame(columns=X_train.columns, dtype=np.float64, index=submission.index)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for seg_id in X_test.index:\n    seg = pd.read_csv('../input/test/' + seg_id + '.csv')\n    \n    x = seg['acoustic_data'].values\n    \n    X_test.loc[seg_id, 'ave'] = x.mean()\n    X_test.loc[seg_id, 'std'] = x.std()\n    X_test.loc[seg_id, 'max'] = x.max()\n    X_test.loc[seg_id, 'min'] = x.min()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_scaled = scaler.transform(X_test)\nsubmission['time_to_failure'] = svm.predict(X_test_scaled)\nsubmission.to_csv('submission.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}