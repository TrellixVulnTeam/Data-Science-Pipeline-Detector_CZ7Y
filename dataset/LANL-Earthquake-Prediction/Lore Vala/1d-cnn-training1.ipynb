{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Fix seeds\n#from numpy.random import seed\n#seed(1)\n#from tensorflow import set_random_seed\n#set_random_seed(2)\n\nimport numpy as np \nimport pandas as pd\nimport os\nfrom tqdm import tqdm\nimport time\nfrom IPython import display\nimport matplotlib.pyplot as plt\nimport matplotlib\n%matplotlib inline\n\nimport tensorflow as tf\nimport keras\nfrom keras import layers, models, activations\n\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Import data\ndata = pd.read_csv(\"../input/train.csv\", dtype={\"acoustic_data\": np.int16, \"time_to_failure\": np.float32}).values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#len(data)\ndata_len = 629145480\nsegment_len = 150000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get 1000 independent chunk of 150,000 sample as Validation Test: first and last 500\n\nval_id1 = np.arange(0,500)*segment_len\n\nval_id2 = np.sort(np.abs( (val_id1 - data_len) ) - segment_len)\n\nval_id = np.concatenate((val_id1, val_id2))\n\ntrain_id_m = val_id1[-1]+segment_len\ntrain_id_M = val_id2[0]-segment_len\n\nprint(\"Train id from\",train_id_m,\"to\",train_id_M)\nprint(\"Num of Validation segments :\",len(val_id))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# How many training segment?\nprint((train_id_M-train_id_m)/segment_len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train indexes\n\ntrain_id = np.arange(train_id_m, train_id_M-segment_len, segment_len, dtype=np.int)\nprint(train_id)\nprint(len(train_id))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# what's the margin to slide the indexes?\nmarg = train_id_M-train_id[-1]\nprint(marg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 19548 / 36\nslide = np.linspace(0, 19548, 544)\nprint(slide.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# creat a useful dictionary structures for data generation\n\npartition = {}\npartition['train'] = train_id\npartition['validation'] = val_id","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DataGenerator(keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, list_IDs, Slide, batch_size=32, dim=150000, n_channels=1,\n                 n_classes=1, shuffle=True, train=True):\n        'Initialization'\n        self.dim = dim\n        self.batch_size = batch_size\n        self.list_IDs = list_IDs\n        self.Slide = Slide\n        self.n_channels = n_channels\n        self.n_classes = n_classes\n        self.shuffle = shuffle\n        self.train = train\n        self.on_epoch_end()\n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return int(np.floor(len(self.list_IDs) / self.batch_size))\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        # Generate indexes of the batch\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n\n        # Find list of IDs\n        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n\n        # Generate data\n        X, y = self.__data_generation(list_IDs_temp)\n\n        return X, y\n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.indexes = np.arange(len(self.list_IDs))\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n\n    def __data_generation(self, list_IDs_temp):\n        'Generates data containing batch_size samples' \n        # Initialization\n        X = np.empty((self.batch_size, self.dim, self.n_channels), dtype=np.int16)\n        y = np.empty((self.batch_size, self.n_classes), dtype=np.float64)\n        off = 0\n        \n        if self.train:\n            off = np.random.choice(self.Slide) # generate a random offset\n        \n        # Generate data\n        for i, ID in enumerate(list_IDs_temp):\n            # Store sample\n            temp = data[ID+off :ID+off+150000 ,0]\n            X[i] = np.expand_dims(temp, axis=2)   \n           \n            # Store target labels\n            y[i] = data[ID+off+150000-1 ,1]\n\n        return X, y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_neural_network(data_size_in, n_classes):\n        \n    inputs = layers.Input(shape=data_size_in)\n    \n    x = layers.Conv1D(kernel_size=5000, filters=8, activation='relu')(inputs)\n    \n    #x = layers.Conv1D(kernel_size=5000, filters=8, activation='relu')(x)\n    \n    x = layers.normalization.BatchNormalization()(x)\n    \n    x = layers.AveragePooling1D(pool_size=3)(x)\n    \n    x = layers.Conv1D(kernel_size=1000, filters=8, activation='relu')(x)\n    \n    #x = layers.Conv1D(kernel_size=1000, filters=8, activation='relu')(x)\n    \n    x = layers.normalization.BatchNormalization()(x)\n    \n    x = layers.AveragePooling1D(pool_size=3)(x)\n    \n    x = layers.Conv1D(kernel_size=500, filters=16, activation='relu')(x)\n    \n    #x = layers.Conv1D(kernel_size=500, filters=16, activation='relu')(x)\n    \n    x = layers.normalization.BatchNormalization()(x)\n    \n    x = layers.AveragePooling1D(pool_size=3)(x)\n    \n    x = layers.Conv1D(kernel_size=500, filters=16, activation='relu')(x)\n    \n    #x = layers.Conv1D(kernel_size=500, filters=16, activation='relu')(x)\n    \n    x = layers.normalization.BatchNormalization()(x)\n    \n    x = layers.AveragePooling1D(pool_size=3)(x)\n    \n    x = layers.Conv1D(kernel_size=150, filters=32, activation='relu')(x)\n    \n    #x = layers.Conv1D(kernel_size=150, filters=32, activation='relu')(x)\n    \n    x = layers.normalization.BatchNormalization()(x)\n    \n    x = layers.AveragePooling1D(pool_size=3)(x)\n    \n    x = layers.Conv1D(kernel_size=150, filters=64, activation='relu')(x)\n    \n    #x = layers.Conv1D(kernel_size=150, filters=64, activation='relu')(x)\n    \n    x = layers.normalization.BatchNormalization()(x)\n    \n    x = layers.MaxPool1D(pool_size=3)(x)\n    \n    x = layers.Flatten()(x)\n        \n    x = layers.Dense(units=500, activation='sigmoid')(x)\n    \n    x = layers.Dropout(rate=0.5)(x)\n    \n    x = layers.Dense(units=25, activation='relu')(x)\n    \n    predictions = layers.Dense(units=n_classes, activation='linear')(x)\n    \n    \n    model = keras.models.Model(inputs=inputs, outputs=predictions)\n    \n    \n    print(model.summary())\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_size_in=(150000,1)\nn_classes=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Replace None with your code ###\nloss = 'mean_absolute_error'  \nlearning_rate = 0.01 \nsgd = keras.optimizers.SGD(lr=learning_rate, decay=1e-6, momentum=0.9, nesterov=True)\nmetrics = [keras.metrics.mae]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"network = build_neural_network(data_size_in, n_classes)\nnetwork.compile(loss=loss, optimizer=sgd, metrics=metrics)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_network(network, training_generator, validation_generator, n_epoch, network_filepath):\n\n\n    \n    # lists where we will be storing values during training, for visualization purposes\n    tra_losses = [] # list for training loss\n    tra_accs = [] # list for training accuracy\n    val_losses = [] # list for validation loss\n    val_accs = [] # list for validation accuracy\n\n    # we want to save the parameters that give the best performance on the validation set\n    # therefore, we store the best validation accuracy, and save the parameters to disk\n    best_validation_accuracy = 100000000 # best validation accuracy\n\n    for epoch in range(n_epoch):\n        st = time.time()\n\n        # Train your network\n        results = network.fit_generator(training_generator)\n\n        # Get training loss and accuracy\n        training_loss = results.history['loss']\n        training_accuracy = results.history['mean_absolute_error']\n\n        # Add to list\n        tra_losses.append(training_loss)\n        tra_accs.append(training_accuracy)\n\n        # Evaluate performance (loss and accuracy) on validation set\n        scores = network.evaluate_generator(validation_generator)     \n        validation_loss = scores[0]\n        validation_accuracy = scores[1]\n\n        # Add to list\n        val_losses.append(validation_loss)\n        val_accs.append(validation_accuracy)\n\n        # (Possibly) update best validation accuracy and save the network\n        if validation_accuracy < best_validation_accuracy:\n            best_validation_accuracy = validation_accuracy\n            network.save(network_filepath)\n\n        # Visualization of the learning curves\n        fig = plt.figure(figsize=(10, 5))\n        tra_loss_plt, = plt.plot(range(len(tra_losses)), tra_losses, 'b')\n        tra_accs_plt, = plt.plot(range(len(tra_accs)), tra_accs, 'c')\n        val_loss_plt, = plt.plot(range(len(val_losses)), val_losses, 'g')\n        val_acc_plt, = plt.plot(range(len(val_accs)), val_accs, 'r')\n        plt.xlabel('epoch')\n        plt.ylabel('loss')\n        plt.legend([tra_loss_plt, tra_accs_plt, val_loss_plt, val_acc_plt], \n                  ['training loss', 'training accuracy', 'validation loss', 'validation accuracy'],\n                  loc='center left', bbox_to_anchor=(1, 0.5))\n        plt.title('Best validation mae = {:.2f}'.format(best_validation_accuracy))\n        display.clear_output(wait=True)\n        display.display(plt.gcf())\n        time.sleep(.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define batch size.\nbatch_size = 32\nn_epoch = 50\nslide = np.linspace(0, 19548, 544, dtype=np.int)\n\n\n# Parameters\nparams = {'Slide' : slide,\n          'dim': 150000,\n          'batch_size': batch_size,\n          'n_classes': 1,\n          'shuffle': True}\n\n# Generators\ntraining_generator = DataGenerator(partition['train'], **params, train = True)\nvalidation_generator = DataGenerator(partition['validation'], **params, train = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"file_dir = '../working//Model'\nnetwork_filepath = os.path.join(file_dir, 'best_model.h5')\nos.mkdir(file_dir)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_network(network, training_generator, validation_generator, n_epoch, network_filepath)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Re-load the model found for the best accuracy; load_model function takes care of compiling again the function \n\n#best_network=keras.models.load_model(network_filepath)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test = np.empty((2624,150000,1), dtype=np.float)\n#for i, seg in enumerate(os.listdir(\"../input/test/\")):\n#    test[i] = pd.read_csv(\"../input/test/\"+seg).values\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#pred = best_network.predict_on_batch(test)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}