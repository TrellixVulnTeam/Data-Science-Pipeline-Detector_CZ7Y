{"cells":[{"metadata":{"_uuid":"08c0c8baf241a57ba91214df7f43329f106fe0de"},"cell_type":"markdown","source":"<h2>1. About this notebook</h2>\n\nIn this notebook I try a few different models to predict the time to failure during earthquake simulations. I'm using some new features like trend and absolute values with others from public kernels (e.g. quantiles and rolling means).\n\nUpdate 03/02: Fixed erros at lgbm; add feature importance and visualizations.\n\nFor more details about LANL competition you can check my [previous kernel](https://www.kaggle.com/jsaguiar/seismic-data-exploration)."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import os\nimport time\nimport random\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm_notebook as tqdm\n# Visualizations\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode, iplot\n# Sklearn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import GridSearchCV, KFold\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import NuSVR, SVR\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\nimport lightgbm as lgb\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=UserWarning)\nsns.set()\ninit_notebook_mode(connected=True)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data_type = {'acoustic_data': np.int16, 'time_to_failure': np.float64}\ntrain = pd.read_csv('../input/LANL-Earthquake-Prediction/train.csv', dtype=data_type)\ntrain.head(20)\ntrain.shape\n# train.describe()\n# print(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"b87152d065695414ea2581896bec5d8933642e26"},"cell_type":"code","source":"train_acoustic_data_small = train['acoustic_data'].values[::50]\ntrain_time_to_failure_small = train['time_to_failure'].values[::50]\n\nfig, ax1 = plt.subplots(figsize=(16, 8))\nplt.title(\"Trends of acoustic_data and time_to_failure. 2% of data (sampled)\")\nplt.plot(train_acoustic_data_small, color='b')\nax1.set_ylabel('acoustic_data', color='b')\nplt.legend(['acoustic_data'])\nax2 = ax1.twinx()\nplt.plot(train_time_to_failure_small, color='g')\nax2.set_ylabel('time_to_failure', color='g')\nplt.legend(['time_to_failure'], loc=(0.875, 0.9))\nplt.grid(False)\n\ndel train_acoustic_data_small\ndel train_time_to_failure_small\ndel train","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"22a9edd2ef3911b7de989cd22781b871416ecdd1"},"cell_type":"markdown","source":"<h2>2. Feature Engineering</h2>\n\nSimple trend feature: fit a linear regression and return the coefficient"},{"metadata":{"trusted":true,"_uuid":"e915924b13a10bb09a64e2c0d5a81e0a04905ec3"},"cell_type":"code","source":"def add_trend_feature(arr, abs_values=False):\n    idx = np.array(range(len(arr)))\n    if abs_values:\n        arr = np.abs(arr)\n    lr = LinearRegression()\n    lr.fit(idx.reshape(-1, 1), arr)\n    return lr.coef_[0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9a61d2b7b4650f23b6b0f02613ccd07f15c87d36"},"cell_type":"markdown","source":"cross zero rate feature"},{"metadata":{"trusted":true,"_uuid":"ec2fd4ca9a12a944f28d67de348858d58d501c7e"},"cell_type":"code","source":"cz = pd.Series(index=range(150000), dtype=np.float64)\ndef add_czr_feature(arrs):\n    lens=len(arrs)\n    return (arrs[lens-1]-arrs[0])/lens\n\ndef cpt_czr(arrs):\n    lens=len(arrs)    \n    cnt=0\n    for i in range(1,lens):\n        if((arrs[i]>0 and arrs[i-1]<0) or (arrs[i]<0 and arrs[i-1]>0)):\n            cnt=cnt+1\n        cz[i]=cnt\n    cz[0]=0","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ab25676aa4a93cb6213347bfd3dcd15d9c767fe7"},"cell_type":"markdown","source":"Group the training data in chunks of 150,000 examples and extract the following features:\n\n* Aggregations: min, max, mean and std\n* Absolute features: max, mean and std\n* Quantile features\n* Trend features\n* Rolling features\n* Ratios"},{"metadata":{"trusted":true,"_uuid":"1772c3a5aa5dbe3d672031eb64e6bac596206588","_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"\nrows = 150_000\nsegments = int(np.floor(629145480 / rows))\nX_train = pd.DataFrame(index=range(segments), dtype=np.float64)\ny_train = pd.DataFrame(index=range(segments), dtype=np.float64)\nfor segment in tqdm(range(segments)):\n    break\n    seg = train.iloc[segment*rows:segment*rows+rows]\n    x = seg['acoustic_data']   # pd series\n    y = seg['time_to_failure'].values[-1]  # single value\n    cpt_czr(x.values)\n    y_train.loc[segment, 'time_to_failure'] = y\n    X_train.loc[segment, 'ave'] = x.values.mean()\n    X_train.loc[segment, 'std'] = x.values.std()\n    X_train.loc[segment, 'max'] = x.values.max()\n    X_train.loc[segment, 'min'] = x.values.min()\n    X_train.loc[segment, 'abs_max'] = np.abs(x.values).max()\n    X_train.loc[segment, 'abs_mean'] = np.abs(x.values).mean()\n    X_train.loc[segment, 'abs_std'] = np.abs(x.values).std()\n    X_train.loc[segment, 'trend'] = add_trend_feature(x.values)\n    X_train.loc[segment, 'abs_trend'] = add_trend_feature(np.abs(x.values))\n    X_train.loc[segment, 'czr'] = add_czr_feature(cz.values)\n    # New features - rolling features\n    for w in [10, 100, 1000]:\n        x_roll_std = x.rolling(w).std().dropna().values\n        x_roll_mean = x.rolling(w).mean().dropna().values\n        x_roll_abs_mean = x.abs().rolling(w).mean().dropna().values\n        x_roll_abs_max = x.abs().rolling(w).max().dropna().values\n        x_roll_czr = cz.rolling(w).apply(add_czr_feature).dropna().values\n        \n        X_train.loc[segment, 'trent_roll_std_' + str(w)] = add_trend_feature(x_roll_std)\n        X_train.loc[segment, 'ave_roll_std_' + str(w)] = x_roll_std.mean()\n        X_train.loc[segment, 'std_roll_std_' + str(w)] = x_roll_std.std()\n        X_train.loc[segment, 'max_roll_std_' + str(w)] = x_roll_std.max()\n        X_train.loc[segment, 'min_roll_std_' + str(w)] = x_roll_std.min()\n        X_train.loc[segment, 'q01_roll_std_' + str(w)] = np.quantile(x_roll_std, 0.01)\n        X_train.loc[segment, 'q05_roll_std_' + str(w)] = np.quantile(x_roll_std, 0.05)\n        X_train.loc[segment, 'q10_roll_std_' + str(w)] = np.quantile(x_roll_std, 0.10)\n        X_train.loc[segment, 'q95_roll_std_' + str(w)] = np.quantile(x_roll_std, 0.95)\n        X_train.loc[segment, 'q99_roll_std_' + str(w)] = np.quantile(x_roll_std, 0.99)\n        \n        X_train.loc[segment, 'trent_roll_mean_' + str(w)] = add_trend_feature(x_roll_mean)\n        X_train.loc[segment, 'ave_roll_mean_' + str(w)] = x_roll_mean.mean()\n        X_train.loc[segment, 'std_roll_mean_' + str(w)] = x_roll_mean.std()\n        X_train.loc[segment, 'max_roll_mean_' + str(w)] = x_roll_mean.max()\n        X_train.loc[segment, 'min_roll_mean_' + str(w)] = x_roll_mean.min()\n        X_train.loc[segment, 'q01_roll_mean_' + str(w)] = np.quantile(x_roll_mean, 0.01)\n        X_train.loc[segment, 'q05_roll_mean_' + str(w)] = np.quantile(x_roll_mean, 0.05)\n        X_train.loc[segment, 'q95_roll_mean_' + str(w)] = np.quantile(x_roll_mean, 0.95)\n        X_train.loc[segment, 'q99_roll_mean_' + str(w)] = np.quantile(x_roll_mean, 0.99)\n        \n        X_train.loc[segment, 'trent_roll_abs_mean_' + str(w)] = add_trend_feature(x_roll_abs_mean)\n        X_train.loc[segment, 'ave_roll_abs_mean_' + str(w)] = x_roll_abs_mean.mean()\n        X_train.loc[segment, 'std_roll_abs_mean_' + str(w)] = x_roll_abs_mean.std()\n        X_train.loc[segment, 'max_roll_abs_mean_' + str(w)] = x_roll_abs_mean.max()\n        X_train.loc[segment, 'min_roll_abs_mean_' + str(w)] = x_roll_abs_mean.min()\n        X_train.loc[segment, 'q01_roll_abs_mean_' + str(w)] = np.quantile(x_roll_abs_mean, 0.01)\n        X_train.loc[segment, 'q05_roll_abs_mean_' + str(w)] = np.quantile(x_roll_abs_mean, 0.05)\n        X_train.loc[segment, 'q95_roll_abs_mean_' + str(w)] = np.quantile(x_roll_abs_mean, 0.95)\n        X_train.loc[segment, 'q99_roll_abs_mean_' + str(w)] = np.quantile(x_roll_abs_mean, 0.99)\n        \n        X_train.loc[segment, 'trent_roll_abs_max_' + str(w)] = add_trend_feature(x_roll_abs_max)\n    \n        X_train.loc[segment, 'trent_roll_czr_' + str(w)] = add_trend_feature(x_roll_czr)\n        X_train.loc[segment, 'ave_roll_czr_' + str(w)] = x_roll_czr.mean()\n        X_train.loc[segment, 'std_roll_czr_' + str(w)] = x_roll_czr.std()\n        X_train.loc[segment, 'max_roll_czr_' + str(w)] = x_roll_czr.max()\n        X_train.loc[segment, 'min_roll_czr_' + str(w)] = x_roll_czr.min()\n        X_train.loc[segment, 'q01_roll_czr_' + str(w)] = np.quantile(x_roll_czr, 0.01)\n        X_train.loc[segment, 'q05_roll_czr_' + str(w)] = np.quantile(x_roll_czr, 0.05)\n        X_train.loc[segment, 'q95_roll_czr_' + str(w)] = np.quantile(x_roll_czr, 0.95)\n        X_train.loc[segment, 'q99_roll_czr_' + str(w)] = np.quantile(x_roll_czr, 0.99)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2035e6b40f82c2e70259f476f59273283a595ff5"},"cell_type":"code","source":"X_train=pd.read_csv(\"../input/feature/X_train.csv\")\ny_train=pd.read_csv(\"../input/feature/y_train.csv\")\nX_train2=pd.read_csv(\"../input/feature2/X_train.csv\")\ny_train2=pd.read_csv(\"../input/feature2/y_train.csv\")\ndel X_train['Unnamed: 0']\ndel y_train['Unnamed: 0']\ndel X_train2['Unnamed: 0']\ndel y_train2['Unnamed: 0']\nX_train2=pd.concat([X_train,X_train2])\ny_train2=pd.concat([y_train,y_train2])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ff26be4db60a010ddf7e81554456bbd19e068f96"},"cell_type":"code","source":"print(\"Train shape:\", X_train.shape)\nX_train.head(3)\nprint(\"Train2 shape:\", X_train2.shape)\nX_train2.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"79675d6eca47c3039e729c937b1498eea8a54b60"},"cell_type":"markdown","source":"Scale features and helper functions:"},{"metadata":{"trusted":true,"_uuid":"c2f28f7269af8bd5ba9fe1bec70da271bc93d3fe","_kg_hide-input":true},"cell_type":"code","source":"scaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\ntarget = y_train.values.flatten()\nX_train_scaled2 = scaler.fit_transform(X_train2)\ntarget2 = y_train2.values.flatten()\nnum_folds = 5\n\ndef grid_search_cv(estimator, grid, features, target):\n    \"\"\"Return the best hyperparameters combination in grid.\"\"\"\n    t0 = time.time()\n    reg = GridSearchCV(estimator, grid, cv=num_folds, scoring='neg_mean_absolute_error')\n    reg.fit(features, target)\n    \n    t0 = time.time() - t0\n    print(\"Best CV score: {:.4f}, time: {:.1f}s\".format(-reg.best_score_, t0))\n    print(reg.best_params_)\n    return reg.best_params_\n\ndef make_predictions(estimator, features, target, test=None, plot=True, lgb=False):\n    \"\"\"Train the estimator and make predictions for oof and test data.\"\"\"\n    folds = KFold(num_folds, shuffle=True, random_state=2019)\n    oof_predictions = np.zeros(features.shape[0])\n    if test is not None:\n        sub_predictions = np.zeros(test.shape[0])\n    for (train_index, valid_index) in folds.split(features, target):\n        \n        if lgb:\n            estimator.fit(features[train_index], target[train_index],\n                          early_stopping_rounds=100, verbose=False,\n                          eval_set=[(features[train_index], target[train_index]),\n                                    (features[valid_index], target[valid_index])])\n        else:\n            estimator.fit(features[train_index], target[train_index])\n        oof_predictions[valid_index] = estimator.predict(features[valid_index]).flatten()\n        if test is not None:\n            sub_predictions += estimator.predict(test).flatten() / num_folds\n    \n    # Plot out-of-fold predictions vs actual values\n    if plot:\n        fig, axis = plt.subplots(2,1, figsize=(12,5))\n        ax1, ax2 = axis\n        ax1.set_xlabel('actual')\n        ax1.set_ylabel('predicted')\n        ax2.set_xlabel('train index')\n        ax2.set_ylabel('time to failure')\n        ax1.scatter(target[:4194], oof_predictions[:4194], color='brown')\n        ax1.plot([(0, 0), (20, 20)], [(0, 0), (20, 20)], color='blue')\n        \n        ax2.plot(oof_predictions[:4194], color='orange')\n        ax2.plot(target[:4194], color='blue', label='y_train')\n    if test is not None:\n        return oof_predictions, sub_predictions\n    else:\n        return oof_predictions","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d5255831db3f42ce8c31fcb18363ed7920f2d410"},"cell_type":"markdown","source":"<h2>**3. Models**</h2>\n\nLet's try a few different models and submit the one with the best validation score. The predicted values in the following plots are using a out-of-fold scheme.\n\n<h3>**Ridge Regression**</h3>\n\nThe first model will be a linear regression with L2 regularization.\n\n"},{"metadata":{"trusted":true,"_uuid":"3a66f7c8171b4fec957638f800a445d8e3d8a648"},"cell_type":"code","source":"print(X_train_scaled2.shape)\nprint(target2.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"38dd94339081930175c7ca60ed983fa802bd0e0a"},"cell_type":"code","source":"grid = [{'alpha': np.concatenate([np.linspace(0.001, 1, 100), np.linspace(1, 200, 1000)])}]\nrr_params = grid_search_cv(Ridge(), grid, X_train_scaled, target)\nridge_oof = make_predictions(Ridge(**rr_params), X_train_scaled, target)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f544d8928df1093611cbd2aa0b579e05cdf75cc2"},"cell_type":"markdown","source":"There are some huge negative values when using a linear model. We can try to change negative values for zeros:"},{"metadata":{"trusted":true,"_uuid":"638d7bef694b75cef0a7874c7a3c0bad7cb3c04b"},"cell_type":"code","source":"ridge_oof[ridge_oof < 0] = 0\nprint(\"Mean error: {:.4f}\".format(mean_absolute_error(target, ridge_oof)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"605934f5bb11e8d9a0d6ab257fba0da4a83183a4"},"cell_type":"markdown","source":"和上一个模型一样，只是数据是带有50%的重叠，下面的结果都是带有50%重叠的"},{"metadata":{"trusted":true,"_uuid":"e4ed906f1f40140fc5851b42a1a5810e917abdb9"},"cell_type":"code","source":"grid = [{'alpha': np.concatenate([np.linspace(0.001, 1, 100), np.linspace(1, 200, 1000)])}]\nrr_params2 = grid_search_cv(Ridge(), grid, X_train_scaled2, target2)\nridge_oof2 = make_predictions(Ridge(**rr_params2), X_train_scaled2, target2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"37fc4cff531998c5bfae84772795e8c01e607fbe"},"cell_type":"markdown","source":"There are some huge negative values when using a linear model. We can try to change negative values for zeros:"},{"metadata":{"trusted":true,"_uuid":"97bbc24d95919791d48766abe1f23f11eea78597"},"cell_type":"code","source":"ridge_oof2[ridge_oof2 < 0] = 0\nprint(\"Mean error: {:.4f}\".format(mean_absolute_error(target2, ridge_oof2)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7c9f936d7866fea46f5d83401b0e1882291c4d63"},"cell_type":"markdown","source":"<h3>**Kernel Ridge**</h3>\n\nThis model combines regularized linear regression with a given kernel (radial basis in this case)."},{"metadata":{"trusted":true,"_uuid":"f0b7e94dda285927518b226b716c2966e9def90f"},"cell_type":"code","source":"grid = [{'gamma': np.linspace(1e-8, 0.1, 10), 'alpha': [0.0005, 0.001, 0.02, 0.08, 0.1]}]\nkr_params2 = grid_search_cv(KernelRidge(kernel='rbf'), grid, X_train_scaled2, target2)\nkr_oof2 = make_predictions(KernelRidge(kernel='rbf', **kr_params2), X_train_scaled2, target2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2fc9dc56cc1220a8a07f69ab52dad35af7800572"},"cell_type":"markdown","source":"<h3>**Gradient Boosting**</h3>\n\nThe last model is a gradient boosting decision tree. It's not possible to use GridSearchCV with early stopping (lightgbm), so I am using a custom function for random search."},{"metadata":{"trusted":true,"_uuid":"e7e09e34f28daac2d4e7dca1f58f9882ef4f3e8a"},"cell_type":"code","source":"fixed_params = {\n    'objective': 'regression_l1',\n    'boosting': 'gbdt',\n    'verbosity': -1,\n    'random_seed': 19,\n    'n_estimators': 20000,\n}\n\nparam_grid = {\n    'learning_rate': [0.1, 0.05, 0.01, 0.005],\n    'num_leaves': list(range(8, 92, 4)),\n    'max_depth': [3, 4, 5, 6, 8, 12, 16, -1],\n    'feature_fraction': [0.8, 0.85, 0.9, 0.95, 1],\n    'subsample': [0.8, 0.85, 0.9, 0.95, 1],\n    'lambda_l1': [0, 0.1, 0.2, 0.4, 0.6, 0.9],\n    'lambda_l2': [0, 0.1, 0.2, 0.4, 0.6, 0.9],\n    'min_data_in_leaf': [10, 20, 40, 60, 100],\n    'min_gain_to_split': [0, 0.001, 0.01, 0.1],\n}\n\nbest_score = 999\ndataset = lgb.Dataset(X_train2, label=y_train2)  # no need to scale features\n\nfor i in range(20):\n    params = {k: random.choice(v) for k, v in param_grid.items()}\n    params.update(fixed_params)\n    result = lgb.cv(params, dataset, nfold=5, early_stopping_rounds=100,\n                    stratified=False)\n    \n    if result['l1-mean'][-1] < best_score:\n        best_score = result['l1-mean'][-1]\n        best_params = params\n        best_nrounds = len(result['l1-mean'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"08fc682c7ebff2403000a97cf23ea30cda98a65d"},"cell_type":"code","source":"print(\"Best mean score: {:.4f}, num rounds: {}\".format(best_score, best_nrounds))\nprint(best_params)\ngb_oof = make_predictions(lgb.LGBMRegressor(**best_params), X_train.values, target, lgb=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1b837edfd77f9eff4b45f452495186d6a2cc11ad"},"cell_type":"markdown","source":"Now let's have a look at the <b>feature importance</b>:"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"36c5fa96d7d353285006f7aef366d36eedc41be1","_kg_hide-output":true},"cell_type":"code","source":"def plot_feature_importance(features, target, columns):\n    folds = KFold(num_folds, shuffle=True, random_state=2019)\n    importance_frame = pd.DataFrame()\n    for (train_index, valid_index) in folds.split(features, target):\n        reg = lgb.LGBMRegressor(**best_params)\n        reg.fit(features[train_index], target[train_index],\n                early_stopping_rounds=100, verbose=False,\n                eval_set=[(features[train_index], target[train_index]),\n                          (features[valid_index], target[valid_index])])\n        fold_importance = pd.DataFrame()\n        fold_importance[\"feature\"] = columns\n        fold_importance[\"gain\"] = reg.booster_.feature_importance(importance_type='gain')\n        #fold_importance[\"split\"] = reg.booster_.feature_importance(importance_type='split')\n        importance_frame = pd.concat([importance_frame, fold_importance], axis=0)\n        \n    mean_importance = importance_frame.groupby('feature').mean().reset_index()\n    mean_importance.sort_values(by='gain', ascending=True, inplace=True)\n    trace = go.Bar(y=mean_importance.feature, x=mean_importance.gain,\n                   orientation='h', marker=dict(color='rgb(49,130,189)'))\n\n    layout = go.Layout(\n        title='Feature importance', height=1200, width=800,\n        showlegend=False,\n        xaxis=dict(\n            title='Importance by gain',\n            titlefont=dict(size=14, color='rgb(107, 107, 107)'),\n            domain=[0.25, 1]\n        ),\n    )\n\n    fig = go.Figure(data=[trace], layout=layout)\n    iplot(fig)\n    \nplot_feature_importance(X_train.values, target, X_train.columns)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}