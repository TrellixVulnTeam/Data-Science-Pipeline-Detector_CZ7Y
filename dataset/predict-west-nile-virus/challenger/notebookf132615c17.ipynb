{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0,"cells":[{"metadata":{"_cell_guid":"13a083b9-c1ba-682e-5f96-62ac2d8f585c","_active":false,"collapsed":false},"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re\nimport matplotlib.pyplot as plt\nimport gpxpy.geo\nfrom datetime import datetime\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.\n\n#TODO: add variable names for each category\n\n###################################### reading files\ndef readfile(filename,offset=0):\n    f = open(filename, 'r')\n    data =[]\n    for i,line in enumerate(f):\n        line = re.sub(r'(?!(([^\"]*\"){2})*[^\"]*$),', '',line)\n        if i == 0:\n            labels = line.strip().replace('\\\"','').split(\",\")\n        if i > 0: \n            data += [line.strip().replace('\\\"','').split(\",\")]\n    return labels[offset:], np.array(data)[:,offset:]\n\nsprayfile = \"../input/spray.csv\"\nweatherfile = \"../input/weather.csv\" \ntrainfile = \"../input/train.csv\"\ntestfile = \"../input/test.csv\"\n\nspraylabels,spray = readfile(sprayfile)\nweatherlabels,weather = readfile(weatherfile)\ntrainlabels,train = readfile(trainfile)\ntestlabels,test = readfile(testfile,1)\n\n\n########################################### create dictionaries for categorical vars\n# find indeces for aggregations - and build dictionaries {[key,indeces in train/test sets]}\ndef indeces(dataset):\n    addressIndeces = {}\n    speciesIndeces = {}\n    dateIndeces = {}\n    for i,line in enumerate(dataset):\n        address = line[1]\n        date = line[0].replace('/', '-')\n        species = line[2]\n        if address in addressIndeces: addressIndeces[address] += [i]\n        else: addressIndeces[address] = [i]\n        if species in speciesIndeces: speciesIndeces[species] += [i]\n        else: speciesIndeces[species] = [i]\n        if date in dateIndeces: dateIndeces[date] += [i]\n        else: dateIndeces[date] = [i]\n    return addressIndeces,speciesIndeces,dateIndeces\n    \nTrainaddressIndeces,TrainspeciesIndeces,TraindateIndeces = indeces(train)\nTestaddressIndeces,TestspeciesIndeces,TestdateIndeces = indeces(test)\n\n\n\n############################################################### do a stupid clustering on sites\n############################################################### TODO: make it smarter\nx = np.array(train[:,11],dtype=int) # Y variable\nN = len(train)\nNtest = len(test)\n\nNInf = sum(x)\nd = 10 # number of site cohorots\ndn = np.ceil(NInf/d-1) # #infections in each cohorot\n\nn = len(TrainaddressIndeces) # number of different addresses\naddressCohorot = np.zeros([n,2],dtype='int32') # for each address in TrainaddressIndeces find the sum(x) and put them in an array [sum(x),var_number]\naddressSums = {} # {(key in TrainaddressIndeces):sum(x)}\nfor i,key in enumerate(TrainaddressIndeces):\n    sumx = np.sum(x[TrainaddressIndeces[key]])\n    addressCohorot[i,0] = sumx \n    addressSums[key] = sumx\n\naddressCohorot = addressCohorot[(-addressCohorot[:,0]).argsort(),:] # sort addressCohorot in descending order by sum(x) over the site address\n\n## assign category number for each\ns = 0\ncategory = 0\nfor i in range(n):\n    s += addressCohorot[i,0]\n    if addressCohorot[i,0] >= dn:\n        addressCohorot[i,1] = category\n        category += 1\n        s = 0\n        continue\n    elif s >= dn:\n        category += 1\n        s = 0\n    addressCohorot[i,1] = category\n\n#################################################################### create categorical var for site clusters on train/test clusters\naddressCategory = {}\nXCohorot = np.zeros([N,d])\nXCohorottest = np.zeros([Ntest,d]) \nfor i,key in enumerate(TrainaddressIndeces):\n    # XCohorot at the indeces of this category and the column equal to the allocated cohorot should be 1\n    category = addressCohorot[np.where(addressCohorot[:,0]==addressSums[key])[0],1][0]\n    addressCategory[key] = category\n    XCohorot[TrainaddressIndeces[key],category] = 1\n    if TestaddressIndeces.get(key) != None:\n        XCohorottest[TestaddressIndeces[key],category] = 1\n\t\t\n\t\t\n############################################################ create var for each site with the distance to main two addresses\n\t\t\naddressCordinates = {} # dict with {address:array([latitude,longitude])} of all sites\nfor k, v in TrainaddressIndeces.items():\n\taddressCordinates[k] = np.array(train[v[0],[7,8]],dtype=float)\nfor k, v in TestaddressIndeces.items():\n\tif k not in addressCordinates:\n\t\taddressCordinates[k] = np.array(test[v[0],[7,8]],dtype=float)\n\t\n#extract the outlier addresses\noutbreaks = {} # dict with {address:array([latitude,longitude])} of outbreak sites\nfor k, v in addressSums.items(): \n\tif v > 30: # manual condition for outbreak. TODO: update if needed\n\t\toutbreaks[k] = addressCordinates[k]\n\noutbreaksDist = {}\nfor address, cordinates in addressCordinates.items():\n\td = []\n\tfor OBaddress, OBcordinates in outbreaks.items():\n\t\tif address == OBaddress:\n\t\t\td += [0]\n\t\telse:\n\t\t\td += [gpxpy.geo.haversine_distance(cordinates[0],cordinates[1],OBcordinates[0],OBcordinates[1])]\n\toutbreaksDist[address] = np.array(d,dtype=float)\n\n\nXoutbreaks = np.zeros([N,len(outbreaks)])\nXoutbreakstest = np.zeros([Ntest,len(outbreaks)])\nfor k, v in TrainaddressIndeces.items():\n\tXoutbreaks[v,:] = (outbreaksDist[k]/1000)**2\nfor k, v in TestaddressIndeces.items():\n\tXoutbreakstest[v,:] = (outbreaksDist[k]/1000)**2\n\n############################################################ create var for mosquito species\n\n##### classify species to 4 categories:\n#CULEX PIPIENS/RESTUANS 0\n#CULEX RESTUANS 1\n#CULEX PIPIENS 2\n#CULEX SALINARIUS 3\n#CULEX TERRITANS 3\n#CULEX TARSALIS 3\n#CULEX ERRATICUS 3\n\nXSpecis = np.zeros([N,4])\nfor k,v in TrainspeciesIndeces.items():\n\tif k == \"CULEX PIPIENS/RESTUANS\": XSpecis[v,0] = 1\n\telif k == \"CULEX RESTUANS\": XSpecis[v,1] = 1\n\telif k == \"CULEX PIPIENS\": XSpecis[v,2] = 1\n\telse:  XSpecis[v,3] = 1\n\nXSpecistest = np.zeros([Ntest,4])\nfor k,v in TestspeciesIndeces.items():\n\tif k == \"CULEX PIPIENS/RESTUANS\": XSpecistest[v,0] = 1\n\telif k == \"CULEX RESTUANS\": XSpecistest[v,1] = 1\n\telif k == \"CULEX PIPIENS\": XSpecistest[v,2] = 1\n\telse:  XSpecistest[v,3] = 1\n\t\n############################################################ climate variables - only based on station1\nweather0  = weather[weather[:,0]=='1'] # only station 1\ndates = weather0[:,1]\nvars = [4,6,7,10,11,17,19,21]\nfor v in vars:\n\tweather0[np.where(weather0[:,v]=='M')[0],v]='-1'\n\t\nsunset_minutes = np.array(weather0[:,[11]],dtype=float)\nsunrise_minutes = np.array(weather0[:,[10]],dtype=float)\nsunset_minutes = np.floor(sunset_minutes/100) * 60 + (sunset_minutes%100)\nsunrise_minutes = np.array(weather0[:,[10]],dtype=float)\nsunrise_minutes = np.floor(sunrise_minutes/100) * 60 + (sunrise_minutes%100)\ndaylight_minutes = sunset_minutes - sunrise_minutes # sunset - sunrise times\n\nwindows = [1,7,28]\nother_vars = np.array(weather0[:,[4,6,7,17,19,21]],dtype=float) \nXweatherRaw = np.hstack([other_vars,daylight_minutes])\nnw,dw = np.shape(XweatherRaw)\ndwMA = dw * len(windows)\nXweatherMA = np.zeros([nw,dwMA],dtype=float)\nXweatherMAcol = 0\nfor var in range(dw):\n\tfor line in range(nw):\n\t\tif XweatherRaw[line,var] == -1:\n\t\t\tif line == 0: XweatherRaw[line,var] = XweatherRaw[line+1,var]\n\t\t\telif line == nw-1: XweatherRaw[line,var] = XweatherRaw[line-1,var]\n\t\t\telse: XweatherRaw[line,var] = (XweatherRaw[line-1,var] + XweatherRaw[line+1,var])/2\n\tfor window in windows:\n\t\tXweatherMA[:,XweatherMAcol] = np.convolve(XweatherRaw[:,var], np.ones((window,))/window, mode='full')[:nw]\n\t\tXweatherMAcol += 1\n\n# move data in XweatherMA to X variables according to the dates\nXweatherNonLinear = np.zeros([N,dwMA],dtype=float)\nXweatherNonLineartest = np.zeros([Ntest,dwMA],dtype=float)\nfor i,d in enumerate(dates):\n\tif d in TraindateIndeces: \n\t\tXweatherNonLinear[TraindateIndeces[d],:] = XweatherMA[i,:]\n\tif d in TestdateIndeces: \n\t\tXweatherNonLineartest[TestdateIndeces[d],:] = XweatherMA[i,:]\n\n# produce categorical variables for linear models\nquantiles = range(10,100,10)\ndwMAInd = dwMA * (1+len(quantiles))\nXweatherMAInd = np.zeros([nw,dwMAInd],dtype=float) # dividing XweatherMA for 10 quantiles indicators per each line\nXweatherMAIndcol = 0\nfor var in range(dwMA):\n\tquant_prev = -float(\"inf\") #previous quantie in the loop\n\tfor q in quantiles:\n\t\tquant = np.percentile(XweatherMA[:,var],q)\n\t\tXweatherMAInd[ (XweatherMA[:,var]>quant_prev)*(XweatherMA[:,var]<quant) , XweatherMAIndcol] = 1\n\t\tXweatherMAIndcol += 1\n\t\tquant_prev = quant\n\tquant = float(\"inf\")\n\tXweatherMAInd[ np.where((XweatherMA[:,var]>quant_prev)*(XweatherMA[:,var]<quant))[0] , XweatherMAIndcol] = 1\n\tXweatherMAIndcol += 1\n\n# move data in XweatherMAIndcol to X variables according to the dates\nXweatherLinear = np.zeros([N,dwMAInd],dtype=float)\nXweatherLineartest = np.zeros([Ntest,dwMAInd],dtype=float)\nfor i,d in enumerate(dates):\n\tif d in TraindateIndeces: \n\t\tXweatherLinear[TraindateIndeces[d],:] = XweatherMAInd[i,:]\n\tif d in TestdateIndeces: \n\t\tXweatherLineartest[TestdateIndeces[d],:] = XweatherMAInd[i,:]\t\n\n\n############################################################ spray variables\n\ndists = []\nprevLine = None\nfor line in spray:\n\tif prevLine == None: \n\t\tprevLine = line\n\t\tcontinue\n\tif line[0] == prevLine[0]:\n\t\tdists += [gpxpy.geo.haversine_distance(float(line[2]),float(line[3]),float(prevLine[2]),float(prevLine[3]))]\n\t\tprevLine = line\n\nplt.hist(dists,bins = 100) \n# base on this, I choose 100meters as the radius of full effect, and after this it start decaying\n\nSprayEfforts = {} # a dict of {date:[all locations]}\nfor line in spray:\n\tdate = line[0]\n\tcoordinates = (float(line[2]),float(line[3]))\n\tif date in SprayEfforts: SprayEfforts[date] += [coordinates]\n\telse: SprayEfforts[date] = [coordinates]\n\naddressSprayEfforts = {} # a dict of dicts: {site:{Spraydate:minDist}} where the value refers to spraying in the area of at most 1KM to the site\nfor kaddress,vCordinate in addressCordinates.items():\n\tSprayEffortsPeraddress = {}\n\tfor SprayDate,SprayLocations in SprayEfforts.items():\n\t\tminDist = float(\"inf\")\n\t\tfor SprayCoordinates in SprayLocations:\n\t\t\tdist = gpxpy.geo.haversine_distance(float(vCordinate[0]),float(vCordinate[1]),float(SprayCoordinates[0]),float(SprayCoordinates[1]))\n\t\t\tif dist < min(minDist,1000):\n\t\t\t\tminDist = dist\n\t\t\t\tSprayEffortsPeraddress[SprayDate] = minDist\n\t\t\t\taddressSprayEfforts[kaddress] = SprayEffortsPeraddress\n","execution_count":null,"cell_type":"code","outputs":[],"execution_state":"idle"},{"metadata":{"_cell_guid":"4480b1df-6d6f-ba6b-ce86-938ecd4e3db4","_active":false,"collapsed":false},"source":"addressSprayEfforts","execution_count":null,"cell_type":"code","outputs":[],"execution_state":"idle"},{"metadata":{"_cell_guid":"d1ab58a7-4a3d-9160-ec3c-783988d8ba6c","_active":true,"collapsed":false},"source":null,"execution_count":null,"cell_type":"code","outputs":[],"execution_state":"idle"}]}