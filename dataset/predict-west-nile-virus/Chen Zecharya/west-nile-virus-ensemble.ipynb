{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom imblearn.over_sampling import SMOTE\nimport pandas as pd\nimport numpy as np\nimport joblib\nimport matplotlib.pyplot as plt \nimport pandas_profiling as pp\nimport seaborn as sns","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Read data","metadata":{}},{"cell_type":"code","source":"data_file_path = \"../input/predict-west-nile-virus/train.csv.zip\"\ndata = pd.read_csv(data_file_path)\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_year(data):\n    return data.split('-')[0]\n\ndef create_month(data):\n    return data.split('-')[1]\n\ndef create_day(data):\n    return data.split('-')[2]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Split date to day, month, year. Drop date column","metadata":{}},{"cell_type":"code","source":"data['day'] = data.Date.apply(create_day)\ndata['month'] = data.Date.apply(create_month)\ndata['year'] = data.Date.apply(create_year)\ndata = data.drop(['Date'], axis = 1)\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Drop address and adrressNumberAndStreet - won't use those","metadata":{}},{"cell_type":"code","source":"data = data.drop(['Address', 'AddressNumberAndStreet'], axis = 1)\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Assign numeric values to categorial features","metadata":{}},{"cell_type":"code","source":"lbl = LabelEncoder()\nlbl.fit(list(data['Species'].values))\ndata['Species'] = lbl.transform(data['Species'].values)\nlbl.fit(list(data['Street'].values))\ndata['Street'] = lbl.transform(data['Street'].values)\nlbl.fit(list(data['Trap'].values))\ndata['Trap'] = lbl.transform(data['Trap'].values)\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pp.ProfileReport(data, minimal = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# WNVpresent is our target, everything else is a feature","metadata":{}},{"cell_type":"code","source":"target = data.WnvPresent.values\nfeatures = data.drop(['WnvPresent'], axis = 1)\nsns.histplot(target)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SMOTE - Synthetic Minority Oversampling Technique\nAn algorithm to balance out the data","metadata":{}},{"cell_type":"code","source":"smote = SMOTE()\nx_smote ,y_smote = smote .fit_resample(features, target)\nsns.histplot(y_smote)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Split the data","metadata":{}},{"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(features, target, test_size = 0.2, random_state = 42, shuffle = True)\nxs_train ,xs_test ,ys_train ,ys_test = train_test_split(x_smote ,y_smote , test_size = 0.2 , random_state = 42 ,shuffle = True) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random forest, different criterions and #estimators\n## Using the original dataset","metadata":{}},{"cell_type":"code","source":"criterions = ['gini', 'entropy']\nbest_score = -1\nbest_pred = []\nbest_forest = None\nall_results = []\nfor criterion in criterions:\n    print(\"Using\", criterion)\n    for estimators in range(10, 201, 10):\n        print(\"\\t{} estimators\".format(estimators), end = \" \")\n        forest = RandomForestClassifier(n_estimators=estimators, criterion = criterion, random_state = 42)\n        forest.fit(x_train, y_train)\n        y_pred = forest.predict(x_test)\n        score = accuracy_score(y_test, y_pred)\n        all_results.append([forest, y_pred, score, estimators])\n        print(\"score = {}\".format(score))\n        if score > best_score:\n            best_pred = y_pred\n            best_score = score\n            best_forest = forest\nall_results = np.array(all_results, dtype=object)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Using data after oversampling","metadata":{}},{"cell_type":"code","source":"criterions = ['gini', 'entropy']\nbest_score_smote = -1\nbest_pred_smote = []\nbest_forest_smote = None\nall_results_smote = []\nfor criterion in criterions:\n    print(\"Using\", criterion)\n    for estimators in range(10, 201, 10):\n        print(\"\\t{} estimators\".format(estimators), end = \" \")\n        forest = RandomForestClassifier(n_estimators=estimators, criterion = criterion, random_state = 42)\n        forest.fit(xs_train, ys_train)\n        ys_pred = forest.predict(xs_test)\n        score = accuracy_score(ys_test, ys_pred)\n        all_results_smote.append([forest, ys_pred, score, estimators])\n\n        print(\"score = {}\".format(score))\n        if score > best_score_smote:\n            best_pred_smote = ys_pred\n            best_score_smote = score\n            best_forest_smote = forest\nall_results_smote = np.array(all_results_smote, dtype=object)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(all_results[:20][:, 3], all_results[:20][:, 2], label = \"Gini\")\nplt.plot(all_results[20:][:, 3], all_results[20:][:, 2], label = 'Entropy')\nplt.plot(all_results_smote[:20][:, 3], all_results_smote[:20][:, 2], label = \"Gini - smote\")\nplt.plot(all_results_smote[20:][:, 3], all_results_smote[20:][:, 2], label = 'Entropy - smote')\nplt.title(\"Scores on random forest by number of estimators(using smote)\")\nplt.ylabel(\"score\")\nplt.xlabel(\"number of estimators\")\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\\t\\tOriginal data\")\nprint(\"Best score using criterion={} with {} estimators\".format(best_forest.criterion, len(best_forest\n                                                                                         .estimators_)))\ncm = confusion_matrix(y_test,best_pred)\nprint(classification_report(y_test,best_pred))\nprint(\"Accuracy score\", accuracy_score(y_test, best_pred))\nsns.heatmap(cm/np.sum(cm), annot=True, fmt = '.2%', cmap = 'Blues')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\\t\\tafter smote\")\n\nprint(\"Best score using criterion={} with {} estimators\".format(best_forest_smote.criterion, len(best_forest_smote\n                                                                                         .estimators_)))\ncm = confusion_matrix(ys_test,best_pred_smote)\nprint(classification_report(ys_test,best_pred_smote))\nprint(\"Accuracy score\", accuracy_score(ys_test, best_pred_smote))\nsns.heatmap(cm/np.sum(cm), annot=True, fmt = '.2%', cmap = 'Blues')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# KNN - different K's\n## Original dataset","metadata":{}},{"cell_type":"code","source":"best_score = -1\nbest_pred = []\nbest_knn = None\nall_results = []\nfor p in [1, 2]:\n    print(\"Using l{}\".format(p))\n    for k in range(2, 31):\n        print(\"\\tUsing k={}\".format(k), end = ' ')\n        knn = KNeighborsClassifier(n_neighbors=k, p = p)\n        knn.fit(x_train, y_train)\n        y_pred = knn.predict(x_test)\n        score = accuracy_score(y_test, y_pred)\n        all_results.append([k, score])\n        print(\"score = {}\".format(score))\n        if score > best_score:\n            best_pred = y_pred\n            best_score = score\n            best_knn = knn\nall_results = np.array(all_results, dtype=object)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Using data after oversampling","metadata":{}},{"cell_type":"code","source":"best_score_smote = -1\nbest_pred_smote = []\nbest_knn_smote = None\nall_results_smote = []\nfor p in [1, 2]:\n    print(\"Using l{}\".format(p))\n    for k in range(2, 31):\n        print(\"\\tUsing k={}\".format(k), end = ' ')\n        knn = KNeighborsClassifier(n_neighbors=k, p = p)\n        knn.fit(xs_train, ys_train)\n        ys_pred = knn.predict(xs_test)\n        score = accuracy_score(ys_test, ys_pred)\n        all_results_smote.append([k, score])\n        print(\"score = {}\".format(score))\n        if score > best_score_smote:\n            best_pred_smote = ys_pred\n            best_score_smote = score\n            best_knn_smote = knn\nall_results_smote = np.array(all_results_smote, dtype=object)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(all_results[:29][:, 0], all_results[:29][:, 1], label = \"l1\")\nplt.plot(all_results[29:][:, 0], all_results[29:][:, 1], label = 'l2')\nplt.plot(all_results_smote[:29][:, 0], all_results_smote[:29][:, 1], label = 'l1 - smote')\nplt.plot(all_results_smote[29:][:, 0], all_results_smote[29:][:, 1], label = 'l2 - smote')\nplt.title(\"Scores on KNN by k value\")\nplt.ylabel(\"score\")\nplt.xticks(range(2, 31))\nplt.xlabel(\"k\")\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\\t\\tOriginal data\")\nprint(\"Best score using l{} with {} neighbors\".format(best_knn.p, best_knn.n_neighbors))\ncm = confusion_matrix(y_test,best_pred)\nprint(classification_report(y_test,best_pred))\nprint(\"Accuracy score\", accuracy_score(y_test, best_pred))\nsns.heatmap(cm/np.sum(cm), annot=True, fmt = '.2%', cmap = 'Blues')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\\t\\tAfter smote\")\nprint(\"Best score using l{} with {} neighbors\".format(best_knn_smote.p, best_knn_smote.n_neighbors))\ncm = confusion_matrix(ys_test,best_pred_smote)\nprint(classification_report(ys_test,best_pred_smote))\nprint(\"Accuracy score\", accuracy_score(ys_test, best_pred_smote))\nsns.heatmap(cm/np.sum(cm), annot=True, fmt = '.2%', cmap = 'Blues')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SVM\n## Original data","metadata":{}},{"cell_type":"code","source":"kernels = ['rbf', 'poly', 'sigmoid']\nbest_svm = None\nbest_score = -1\nall_results = []\nfor kernel in kernels:\n    print(\"Using kernel\", kernel)\n    for gamma in[0.0001, 0.001]: ## Bigger number makes the code take forever to run.\n        print(\"\\tUsing gamma = {}\".format(gamma), end = \" \")\n        svc =  SVC(kernel=kernel, gamma=gamma, probability = True)\n        svc.fit(x_train, y_train)\n        y_pred = svc.predict(x_test)\n        score = accuracy_score(y_test, y_pred)\n        all_results.append([gamma, score, kernel])\n        print(\"score = {}\".format(score))\n        if score > best_score:\n            best_pred = y_pred\n            best_score = score\n            best_svm = svc\nall_results = np.array(all_results, dtype=object)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Best score using kernel={} with gamma = {}\".format(best_svm.kernel, best_svm.gamma))\ncm = confusion_matrix(y_test,best_pred)\nprint(classification_report(y_test,best_pred))\nprint(\"Accuracy score\", accuracy_score(y_test, best_pred))\nsns.heatmap(cm/np.sum(cm), annot=True, fmt = '.2%', cmap = 'Blues')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Not using data after oversampling\nit takes forever to run + the results are really, really bad. ~0.65 ","metadata":{}},{"cell_type":"code","source":"# kernels = ['rbf', 'poly', 'sigmoid']\n# best_svm_smote = None\n# best_score_smote = 0\n# all_results = []\n# for kernel in kernels:\n#     print(\"Using kernel\", kernel)\n#     for gamma in[0.0001, 0.001, 0.01, 0.1]:\n#         print(\"\\tUsing gamma = {}\".format(gamma), end = \" \")\n#         svc =  SVC(kernel=kernel, gamma=gamma, probability = True)\n#         svc.fit(xs_train, ys_train)\n#         ys_pred = svc.predict(xs_test)\n#         score = accuracy_score(ys_test, ys_pred)\n#         all_results.append([gamma, kernel, score])\n#         print(\"score = {}\".format(score))\n#         if score > best_score_smote:\n#             best_pred_smote = y_pred\n#             best_score_smote = score\n#             best_svm_smote = svc\n# all_results = np.array(all_results, dtype=object)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# all_results[:3], all_results[3:6], all_results[6:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.plot(all_results[:3][:, 0], all_results[:3][:, 1], label = \"gamma(RBF)\")\n# plt.plot(all_results[3:6][:, 0], all_results[3:6][:, 1], label = \"gamma(Poly)\")\n# plt.plot(all_results[6:][:, 0], all_results[6:][:, 1], label = \"gamma(RBF)\")\n# plt.title(\"Scores on SVM by C value(smote only)\")\n# plt.ylabel(\"score\")\n# plt.xticks(range(1, 4))\n# plt.xlabel(\"C\")\n# plt.legend()\n# plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Logistic Regression\n\n## Original Data","metadata":{}},{"cell_type":"code","source":"for iterations in range(100, 1001, 100):\n    print(\"maximum {} iterations\".format(iterations), end = \" \")\n    lr = LogisticRegression(random_state = 42, max_iter = iterations)\n    lr.fit(x_train, y_train)\n    y_pred = lr.predict(x_test)\n    score = accuracy_score(y_test, y_pred)\n    print(\"score\", score)\ncm = confusion_matrix(y_test,y_pred)\nprint(classification_report(y_test,y_pred))\nsns.heatmap(cm/np.sum(cm), annot=True, fmt = '.2%', cmap = 'Blues')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## data after oversampling","metadata":{}},{"cell_type":"code","source":"for iterations in range(100, 1001, 100):\n    print(\"maximum {} iterations\".format(iterations), end = \" \")\n    lr = LogisticRegression(random_state = 42, max_iter = iterations)\n    lr.fit(xs_train, ys_train)\n    ys_pred = lr.predict(xs_test)\n    score = accuracy_score(ys_test, ys_pred)\n    print(\"score\", score)\nprint(len(ys_test), len(ys_pred))\ncm = confusion_matrix(ys_test,ys_pred)\nprint(classification_report(ys_test,ys_pred))\nsns.heatmap(cm/np.sum(cm), annot=True, fmt = '.2%', cmap = 'Blues')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ensemble all models","metadata":{}},{"cell_type":"code","source":"ensemble = [best_forest_smote, best_knn, lr, best_svm]\npred = []\nfor model in ensemble:\n    pred.append(model.predict_proba(x_test))\nprobs = sum(pred)/len(ensemble)\nfinal_pred = [0 if p[0] > p[1] else 1 for p in probs]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm = confusion_matrix(y_test,final_pred)\nprint(classification_report(y_test,final_pred))\nprint(accuracy_score(y_test, final_pred))\nsns.heatmap(cm/np.sum(cm), annot=True, fmt = '.2%', cmap = 'Blues')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}