{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport scikitplot as skplt\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.ensemble import RandomForestClassifier, BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\nimport xgboost as xgb\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/cat-in-the-dat-ii/train.csv')\ntest = pd.read_csv('/kaggle/input/cat-in-the-dat-ii/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def basic_eda(df):\n    print('----------Top5 records-----------')\n    print(df.head())\n    print('----------Information of data-----------')\n    print(df.info())\n    print('----------Shape of data-----------')\n    print(df.shape)\n    print('----------Columns of dataset-----------')\n    print(df.columns)\n    print('----------Statistics of dataset-----------')\n    print(df.describe())\n    print('----------Missing values-----------')\n    print(df.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#EDA of Training dataset\nbasic_eda(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#EDA of Test dataset\nbasic_eda(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking & Plotting missing values of training dataset on heatmap\nmissing_count_train = train.isnull().sum()\nmissing_prcnt_train = train.isnull().sum()/600000*100\nmissing_train = pd.DataFrame({'missing_count': missing_count_train, 'missing%': missing_prcnt_train}).sort_values(by='missing%', ascending=False)\nprint(missing_train)\n\nsns.heatmap(train.isnull(), cmap=\"viridis\", cbar=False, yticklabels=False)\nplt.title('Missing values in Training dataset')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking & Plotting missing values of test dataset on heatmap\nmissing_count_test = test.isnull().sum()\nmissing_prcnt_test = test.isnull().sum()/400000*100\nmissing_test = pd.DataFrame({'missing_count': missing_count_test, 'missing%': missing_prcnt_test}).sort_values(by='missing%', ascending=False)\nprint(missing_test)\n\nsns.heatmap(test.isnull(), cmap=\"viridis\", cbar=False, yticklabels=False)\nplt.title('Missing values in Test dataset')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data imputation in training dataset\n\nfor col in train.columns:\n    if train[col].isnull().sum() > 0:\n        train[col].fillna(train[col].mode()[0], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data imputation in test dataset\n\nfor col in test.columns:\n    if test[col].isnull().sum() > 0:\n        test[col].fillna(test[col].mode()[0], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking again if any missing value present in the datasets\nsns.heatmap(train.isnull(), cmap=\"viridis\", cbar=False, yticklabels=False)\nplt.title('Missing values in Training dataset')\nplt.show()\nsns.heatmap(test.isnull(), cmap=\"viridis\", cbar=False, yticklabels=False)\nplt.title('Missing values in Test dataset')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filter categorical columns in Training dataset & print\ncat_cols_train = train.columns[train.dtypes==object]\nprint(cat_cols_train)\n# Print categorical value counts\nfor i in cat_cols_train:\n    print(train[i].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe(include=['O']) # Categorical columns details of Training dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filter categorical columns in Test dataset & print\ncat_cols_test = test.columns[test.dtypes==object]\nprint(cat_cols_test)\n# Print categorical value counts\nfor i in cat_cols_test:\n    print(test[i].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.describe(include=['O']) # Categorical columns details of Test dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encoding binary objects (bin_3 & bin_4)\nbin_encoding = {'F':0, 'T':1, 'N':0, 'Y':1}\ntrain['bin_3'] = train['bin_3'].map(bin_encoding)\ntrain['bin_4'] = train['bin_4'].map(bin_encoding)\n\ntest['bin_3'] = test['bin_3'].map(bin_encoding)\ntest['bin_4'] = test['bin_4'].map(bin_encoding)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encoding nominal objects (nom_0,nom_1,nom_2,nom_3,nom_4,nom_5,nom_6,nom_7,nom_8,nom_9)\nfrom category_encoders.target_encoder import TargetEncoder\n\nce_target = TargetEncoder()\ntrain[['nom_0','nom_1', 'nom_2', 'nom_3', 'nom_4', 'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9']] = ce_target.fit_transform(train[['nom_0','nom_1', 'nom_2', 'nom_3', 'nom_4', 'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9']], train['target'])\ntest[['nom_0','nom_1', 'nom_2', 'nom_3', 'nom_4', 'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9']] = ce_target.transform(test[['nom_0','nom_1', 'nom_2', 'nom_3', 'nom_4', 'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encoding ordinal objects (ord_1,ord_2,ord_3,ord_4,ord_5)\nfrom sklearn.preprocessing import OrdinalEncoder\nenc = OrdinalEncoder()\ntrain[['ord_1','ord_2', 'ord_3', 'ord_4', 'ord_5']] = enc.fit_transform(train[['ord_1','ord_2', 'ord_3', 'ord_4', 'ord_5']], train['target'])\ntest[['ord_1','ord_2', 'ord_3', 'ord_4', 'ord_5']] = enc.transform(test[['ord_1','ord_2', 'ord_3', 'ord_4', 'ord_5']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.sample(10)  # Checking sample training dataset after encoding","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.sample(10)  # Checking sample test dataset after encoding","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature Selection using SelectKBest\n\nX_feat_sel = train.drop(columns=['target', 'id', 'day', 'month'], axis=1)\ny_feat_sel = train['target']\n\n#Applying SelectKBest class to extract top 10 best features\nfrom sklearn.feature_selection import SelectKBest, chi2\nbestfeatures = SelectKBest(score_func=chi2, k=10)\nfit = bestfeatures.fit(X_feat_sel,y_feat_sel)\nscore = fit.scores_\ncolumns = X_feat_sel.columns\nfeatureScores = pd.DataFrame({'Feature': columns, 'Score': score})\nprint(featureScores.nlargest(15,'Score'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature Selection using Yellobrick\n\n# One dimensional\nfrom yellowbrick.features import Rank1D\nvisualizer = Rank1D(algorithm='shapiro')\nvisualizer.fit(X_feat_sel,y_feat_sel)\nvisualizer.transform(X_feat_sel)\nvisualizer.show()\n\n# Two domensional\nfrom yellowbrick.features import Rank2D\nvisualizer2 = Rank2D(algorithm='pearson', colormap='RdBu_r')\nvisualizer2.fit(X_feat_sel,y_feat_sel)\nvisualizer2.transform(X_feat_sel)\nvisualizer2.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Selecting dependent and independent variables based on SelectKBest feature\nX = train[['ord_0','ord_1','ord_3','ord_4','ord_5', 'bin_0','bin_1','bin_2','bin_4', 'nom_8','nom_9']]\ny = train['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Splitting Training dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=22)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Choosing best parameters of Logistic regression using Grid search\n'''grid = {'penalty': ['l1', 'l2'],\n    'C': [0.001, 0.01, 0.1, 1, 10, 100]}\n\nCV_lr = GridSearchCV(estimator=LogisticRegression(), param_grid=grid, cv= 5)\nCV_lr.fit(X_train, y_train)\nprint(\"tuned hyperparameters :\",CV_lr.best_params_)\nprint(\"tuned parameter accuracy (best score):\",CV_lr.best_score_)'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using Logistic Regression with best parameters as per Grid search\n\nlr = LogisticRegression(penalty='l2', C=0.001, max_iter=100)\nacc_lr_cv = cross_val_score(estimator=lr,X=X_train,y=y_train,cv=10)  #K=10\nprint(\"Average accuracy of Logistic Regression using K-fold cross validation is :\",np.mean(acc_lr_cv))\n    \nlr.fit(X_train, y_train)\ny_pred_lr = lr.predict(X_test)\nacc_lr = metrics.accuracy_score(y_pred_lr, y_test)\nprint('Accuracy of Logistic Regression is: ', metrics.accuracy_score(y_pred_lr, y_test))\nprint('Classification report: ', classification_report(y_test, y_pred_lr))\nprint('Confusion matrix: ', confusion_matrix(y_test, y_pred_lr))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Choosing best parameters of Decision Tree using Grid search\n'''grid = {'criterion' : ['gini', 'entropy'],\n       'max_depth' : np.arange(1,10),\n       'min_samples_split' : np.arange(2,10),\n       'max_features' : ['auto', 'sqrt', 'log2']}\n\nCV_dtc = GridSearchCV(estimator=DecisionTreeClassifier(), param_grid=grid, cv=5)\nCV_dtc.fit(X_train, y_train)\nprint(\"tuned hyperparameters :\",CV_dtc.best_params_)\nprint(\"tuned parameter accuracy (best score):\",CV_dtc.best_score_)'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using Decision tree Classifier with Best parameter from Grid Search\n\ndtc = DecisionTreeClassifier(criterion='entropy', max_depth=8, min_samples_split=7, max_features='log2',min_samples_leaf=1)\nacc_dtc_cv = cross_val_score(estimator=dtc,X=X_train,y=y_train,cv=10)  #K=10\nprint(\"Average accuracy of Decision Classifier using K-fold cross validation is :\",np.mean(acc_dtc_cv))\n\ndtc.fit(X_train, y_train)\ny_pred_dtc = dtc.predict(X_test)\nacc_dtc = metrics.accuracy_score(y_pred_dtc, y_test)\nprint('Accuracy of test Decision Tree Classifier is: ', metrics.accuracy_score(y_pred_dtc, y_test))\nprint('Classification report: ', classification_report(y_test, y_pred_dtc))\nprint('Confusion matrix: ', confusion_matrix(y_test, y_pred_dtc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using Grid search to get best parameters for Random Forest classifier\n'''param_grid = { \n    'n_estimators': [10, 20, 30, 40, 50],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [2, 3, 4, 5, 6],\n    'bootstrap': [True, False],\n    'criterion' :['gini', 'entropy'],\n    'min_samples_leaf' : [5, 10, 15, 20]\n}\n\nCV_rfc = GridSearchCV(estimator=RandomForestClassifier(), param_grid=param_grid, cv= 5)\nCV_rfc.fit(X_train, y_train)\nprint(\"tuned hyperparameters :\",CV_rfc.best_params_)\nprint(\"tuned parameter accuracy (best score):\",CV_rfc.best_score_)'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using Random Forest Classifier with Gridsearch best parameters\n\nrfc = RandomForestClassifier(n_estimators=50, criterion='gini', max_depth=5, min_samples_split=2, min_samples_leaf=5, max_features='log2', bootstrap=False)\nacc_rfc_cv=cross_val_score(estimator=rfc,X=X_train,y=y_train,cv=10)  #K=10\nprint(\"Average accuracy of Random Forest Classifier using K-fold cross validation is :\",np.mean(acc_rfc_cv))\n\nrfc.fit(X_train, y_train)\ny_pred_rfc = rfc.predict(X_test)\nacc_rfc = metrics.accuracy_score(y_pred_rfc, y_test)\nprint('Accuracy of test Random Forest Classifier is: ', metrics.accuracy_score(y_pred_rfc, y_test))\nprint('Classification report: ', classification_report(y_test, y_pred_rfc))\nprint('Confusion matrix: ', confusion_matrix(y_test, y_pred_rfc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Choosing best parameters of Bagging classifier using Grid search\n'''grid = {'n_estimators' : np.arange(10,100),\n       'bootstrap' : ['True', 'False'],\n       'bootstrap_features' : ['True', 'False']}\n\nCV_bagclf = GridSearchCV(estimator=BaggingClassifier(), param_grid=grid, cv=5)\nCV_bagclf.fit(X_train, y_train)\nprint(\"tuned hyperparameters :\",CV_bagclf.best_params_)\nprint(\"tuned parameter accuracy (best score):\",CV_bagclf.best_score_)'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using Bagging classifier with best parameters from Grid search\n\nbagclf = BaggingClassifier(n_estimators=27, bootstrap=True, bootstrap_features=True)\nacc_bagclf_cv = cross_val_score(estimator=bagclf,X=X_train,y=y_train,cv=10)  #K=10\nprint(\"Average accuracy of Bagging classifier using K-fold cross validation is :\",np.mean(acc_bagclf_cv))\n\nbagclf.fit(X_train, y_train)\ny_pred_bagclf = bagclf.predict(X_test)\nacc_bagclf = accuracy_score(y_test, y_pred_bagclf)\nprint('Accuracy of Bagging classifier is: ', accuracy_score(y_test, y_pred_bagclf))\nprint('Classification report: ', classification_report(y_test, y_pred_bagclf))\nprint('Confusion matrix: ', confusion_matrix(y_test, y_pred_bagclf))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Choosing best parameters of AdaBoost classifier using Grid search\n'''grid = {'n_estimators' : np.arange(10,100)}\nCV_abc = GridSearchCV(estimator=AdaBoostClassifier(),param_grid=grid, cv=5)\nCV_abc.fit(X_train, y_train)\nprint(\"tuned hyperparameters :\",CV_abc.best_params_)\nprint(\"tuned parameter accuracy (best score):\",CV_abc.best_score_)'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using AdaBoost classifier with best parameters from Grid search\n\nabc = AdaBoostClassifier(n_estimators=18)\nacc_abc_cv = cross_val_score(estimator=abc,X=X_train,y=y_train,cv=10)  #K=10\nprint(\"Average accuracy of AdaBoost classifier using K-fold cross validation is :\",np.mean(acc_abc_cv))\n\nabc.fit(X_train, y_train)\ny_pred_abc = abc.predict(X_test)\nacc_abc = accuracy_score(y_test, y_pred_abc)\nprint('Accuracy of AdaBoost classifier is: ', accuracy_score(y_test, y_pred_abc))\nprint('Classification report: ', classification_report(y_test, y_pred_abc))\nprint('Confusion matrix: ', confusion_matrix(y_test, y_pred_abc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Choosing best parameters of Gradient Boosting classifier using Grid search\n'''grid = {'n_estimators' : np.arange(10,100),\n       'loss': ['deviance', 'exponential'],\n       'learning_rate' : [0.001, 0.01, 0.1]}\nCV_gbc = GridSearchCV(estimator=GradientBoostingClassifier(),param_grid=grid, cv=5)\nCV_gbc.fit(X_train, y_train)\nprint(\"tuned hyperparameters :\",CV_gbc.best_params_)\nprint(\"tuned parameter accuracy (best score):\",CV_gbc.best_score_)'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using Gradient Boosting classifier with best parameters from Grid search\n\ngbc = GradientBoostingClassifier(learning_rate=0.1, n_estimators=41, loss='exponential')\nacc_gbc_cv = cross_val_score(estimator=gbc,X=X_train,y=y_train,cv=10)  #K=10\nprint(\"Average accuracy of  Gradient Boosting classifier using K-fold cross validation is :\",np.mean(acc_gbc_cv))\n\ngbc.fit(X_train, y_train)\ny_pred_gbc = gbc.predict(X_test)\nacc_gbc = accuracy_score(y_test, y_pred_gbc)\nprint('Accuracy of Gradient Boosting classifier is: ', accuracy_score(y_test, y_pred_gbc))\nprint('Classification report: ', classification_report(y_test, y_pred_gbc))\nprint('Confusion matrix: ', confusion_matrix(y_test, y_pred_gbc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using XGBoost classifier\n\nxbc = xgb.XGBClassifier(random_state=1,learning_rate=0.01)\nacc_xbc_cv = cross_val_score(estimator=xbc,X=X_train,y=y_train,cv=10)  #K=10\nprint(\"Average accuracy of XGBoost classifier using K-fold cross validation is :\",np.mean(acc_xbc_cv))\n\nxbc.fit(X_train, y_train)\ny_pred_xbc = xbc.predict(X_test)\nacc_xbc = accuracy_score(y_test, y_pred_xbc)\nprint('Accuracy of XGBoost classifier is: ', accuracy_score(y_test, y_pred_xbc))\nprint('Classification report: ', classification_report(y_test, y_pred_xbc))\nprint('Confusion matrix: ', confusion_matrix(y_test, y_pred_xbc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Comparing Accuracy of each model\nmodels = pd.DataFrame({'Model' : ['RandomForest', 'DecisionTree', 'LogisticRegression', 'BaggingClassifier', 'AdaBoost', 'GradientBoost', 'XgBoost'], \n                      'Score' : [acc_rfc, acc_dtc, acc_lr, acc_bagclf, acc_abc, acc_gbc, acc_xbc]})\nmodels.sort_values(by='Score', ascending=False)\nfig, ax=plt.subplots(figsize=(10,6))\nsns.barplot(x='Model', y='Score', data=models)\nax.set_xlabel('Classifiers')\nax.set_ylabel('Accuracy Score')\nax.set_title('Classifiers Vs Accuracy score')\nax.set_ylim([0.7, 0.9])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from yellowbrick.classifier import ClassificationReport\n\n#Classification report of Logistic Regression classifier\nvisualizer = ClassificationReport(lr, classes=[0, 1],cmap=\"YlGn\", size=(600, 360))\nvisualizer.fit(X_train, y_train)\nvisualizer.score(X_test, y_test)\nvisualizer.show()\n\n#Classification report of Decision tree classifier\nvisualizer = ClassificationReport(dtc, classes=[0, 1],cmap=\"YlGn\", size=(600, 360))\nvisualizer.fit(X_train, y_train)\nvisualizer.score(X_test, y_test)\nvisualizer.show()\n\n#Classification report of Random Forest classifier\nvisualizer = ClassificationReport(rfc, classes=[0, 1],cmap=\"YlGn\", size=(600, 360))\nvisualizer.fit(X_train, y_train)\nvisualizer.score(X_test, y_test)\nvisualizer.show()\n\n#Classification report of Bagging classifier\nvisualizer = ClassificationReport(bagclf, classes=[0, 1],cmap=\"YlGn\", size=(600, 360))\nvisualizer.fit(X_train, y_train)\nvisualizer.score(X_test, y_test)\nvisualizer.show()\n\n#Classification report of AdaBoost classifier\nvisualizer = ClassificationReport(abc, classes=[0, 1],cmap=\"YlGn\", size=(600, 360))\nvisualizer.fit(X_train, y_train)\nvisualizer.score(X_test, y_test)\nvisualizer.show()\n\n#Classification report of GradientBoost classifier\nvisualizer = ClassificationReport(gbc, classes=[0, 1],cmap=\"YlGn\", size=(600, 360))\nvisualizer.fit(X_train, y_train)\nvisualizer.score(X_test, y_test)\nvisualizer.show()\n\n#Classification report of XGBoost classifier\nvisualizer = ClassificationReport(xbc, classes=[0, 1],cmap=\"YlGn\", size=(600, 360))\nvisualizer.fit(X_train, y_train)\nvisualizer.score(X_test, y_test)\nvisualizer.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from yellowbrick.classifier import ROCAUC\n\n# ROC-AUC curve  of Logistic Regression classifier\nvisualizer = ROCAUC(lr, classes=[0, 1])\nvisualizer.fit(X_train, y_train)\nvisualizer.score(X_test, y_test)\nvisualizer.show()\n\n# ROC-AUC curve  of Decision Tree classifier\nvisualizer = ROCAUC(dtc, classes=[0, 1])\nvisualizer.fit(X_train, y_train)\nvisualizer.score(X_test, y_test)\nvisualizer.show()\n\n# ROC-AUC curve  of Random Forest classifier\nvisualizer = ROCAUC(rfc, classes=[0, 1])\nvisualizer.fit(X_train, y_train)\nvisualizer.score(X_test, y_test)\nvisualizer.show()\n\n# ROC-AUC curve  of Bagging classifier\nvisualizer = ROCAUC(bagclf, classes=[0, 1])\nvisualizer.fit(X_train, y_train)\nvisualizer.score(X_test, y_test)\nvisualizer.show()\n\n# ROC-AUC curve  of AdaBoost classifier\nvisualizer = ROCAUC(abc, classes=[0, 1])\nvisualizer.fit(X_train, y_train)\nvisualizer.score(X_test, y_test)\nvisualizer.show()\n\n# ROC-AUC curve  of GradientBoost classifier\nvisualizer = ROCAUC(gbc, classes=[0, 1])\nvisualizer.fit(X_train, y_train)\nvisualizer.score(X_test, y_test)\nvisualizer.show()\n\n# ROC-AUC curve  of XGBoost classifier\nvisualizer = ROCAUC(xbc, classes=[0, 1])\nvisualizer.fit(X_train, y_train)\nvisualizer.score(X_test, y_test)\nvisualizer.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Submission of prediction file\n\npredictions = lr.predict(test[['ord_0','ord_1','ord_3','ord_4','ord_5', 'bin_0','bin_1','bin_2','bin_4', 'nom_8','nom_9']])\n\nid = test['id']\n\noutput = pd.DataFrame({'id': id, 'target': predictions})\noutput.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}