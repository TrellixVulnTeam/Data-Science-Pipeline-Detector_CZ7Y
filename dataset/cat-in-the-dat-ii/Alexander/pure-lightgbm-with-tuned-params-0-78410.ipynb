{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# LightGBM\n\nSimple, short kernel - just pure LightGBM ( no SKF, no feature engeneering),\nparams were found by small tunning. It is planned to make more extensive tunning - see updates\n\n**Version 9:\n**\n\nPublic score: 0.78410 (a little tunning been done).\n\nChanges: Max_depth changed from 3 to 2, retraining on the full train set.\n\nRemarks:\n(Other things - lr from 0.05, to 0.06, n_iter from 5000 to 15000 - improve local validation, but NOT the public score)\n\nOnly simple tuning has been done - just changing params one by one and choosing best result.\nMore extensive tunning planned to be done later.\n\n\n**Intial Version:**\n\nPublic score: 0.78261\n\nParams for lgb taken from Vincent Logut's kernel: \nhttps://www.kaggle.com/vincentlugat/skf-lightgbm-target-encoding\n\"SKF LightGBM - Target Encoding\"\n\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import datetime, time\n#from collections import Counter\n#import category_encoders as ce\n# from datetime import timedelta \n# from datetime import datetime\n#from scipy import interp\nimport pandas as pd\nimport numpy as np\n#import itertools\n#import warnings\n\n%matplotlib inline\n#import seaborn as sns\nimport matplotlib.pyplot as plt\n#from matplotlib import rcParams\n\nfrom sklearn.model_selection import train_test_split \n#from sklearn.metrics import precision_score, recall_score, confusion_matrix, roc_curve, precision_recall_curve\nfrom sklearn.metrics import roc_auc_score # , accuracy_score,  f1_score, auc\n#from sklearn.model_selection import StratifiedKFold\nimport lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"## 2. DATA"},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_data(file_path):\n    print('Loading datasets...')\n    train = pd.read_csv(file_path + 'train.csv', sep=',')\n    test = pd.read_csv(file_path + 'test.csv', sep=',')\n    print('Datasets loaded')\n    return train, test\n\nPATH = '../input/cat-in-the-dat-ii/'\ntrain, test = read_data(PATH)\nprint(train.shape, test.shape)\nprint(train.head(2))\nprint(test.head(2))\n\nX = train.drop(['id','target'], axis = 1)\ncategorical_features = [col for c, col in enumerate(X.columns) \\\n                        if not ( np.issubdtype(X.dtypes[c], np.number )  )  ]\ny = train['target']\n\nprint( len(categorical_features), X.shape, y.shape, y.mean()  )\nfor f in categorical_features:\n    X[f] = X[f].astype('category')\n\nX1,X2, y1,y2 = train_test_split(X,y, test_size = 0.2, random_state = 0, stratify = y )\nprint(X1.shape, X2.shape, y1.shape, y2.shape, y1.mean(), y2.mean(), y.mean() )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train on whole train set with max_depth = 2 "},{"metadata":{"trusted":true},"cell_type":"code","source":"model = lgb.LGBMClassifier(**{\n                'learning_rate': 0.05,\n                'feature_fraction': 0.1,\n                'min_data_in_leaf' : 12,\n                'max_depth': 2, # it was 3 \n                'reg_alpha': 1,\n                'reg_lambda': 1,\n                'objective': 'binary',\n                'metric': 'auc',\n                'n_jobs': -1,\n                'n_estimators' : 5000,\n                'feature_fraction_seed': 42,\n                'bagging_seed': 42,\n                'boosting_type': 'gbdt',\n                'verbose': 200,\n                'is_unbalance': True,\n                'boost_from_average': False})\n\nimport datetime\nprint('Start fit.', datetime.datetime.now() )\n\nmodel = model.fit(X, y,\n                  eval_set = [(X1, y1), \n                              (X2, y2)],\n                  verbose = 1000,\n                  eval_metric = 'auc',\n                  early_stopping_rounds = 1000)\n\nprint('End fit.', datetime.datetime.now() )\n\nX_test = test.drop('id',axis = 1 )\nfor f in categorical_features:\n    X_test[f] = X_test[f].astype('category')\npd.DataFrame({'id': test['id'], 'target': model.predict_proba(X_test)[:,1]}).to_csv('submission_max_depth2_trained_on_whole.csv', index=False)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}