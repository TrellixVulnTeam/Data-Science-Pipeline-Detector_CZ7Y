{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Listing all the imports\n! pip install imutils\n! mkdir /kaggle/working/trained_images/\n! rm -rf /kaggle/working/trained_images/\n! mkdir /kaggle/working/trained_images/\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport time\nimport imutils\nimport math","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! ls /kaggle/working\n! ls /kaggle/working/trained_images\n!ls -l /kaggle/working/trained_images/ | egrep -c '^-' ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# image height and image width ----> GLOBAL\nimg_ht = 256\nimg_wd = 256\n\ndef displayImage(display_name, image):\n    cv2.namedWindow(display_name,cv2.WINDOW_AUTOSIZE)\n    cv2.imshow(display_name, image)\n\ndef findContourEye(thresh_image):\n    cnts = cv2.findContours(thresh_image.copy(), cv2.RETR_EXTERNAL,\n\tcv2.CHAIN_APPROX_SIMPLE)\n    cnts = imutils.grab_contours(cnts)\n    cnts = max(cnts, key=cv2.contourArea)\n    return cnts\n\ndef findContourEyeExtreme(cnts):\n    # Locating extreme points on all 4 sides\n    leftmost = tuple(cnts[cnts[:,:,0].argmin()][0])\n    rightmost = tuple(cnts[cnts[:,:,0].argmax()][0])\n    topmost = tuple(cnts[cnts[:,:,1].argmin()][0])\n    bottommost = tuple(cnts[cnts[:,:,1].argmax()][0])\n    # Locating the top left and bottom right corner\n    x1 = leftmost[0]\n    y1 = topmost[1]\n    x2 = rightmost[0]\n    y2 = bottommost[1]\n    return x1,y1,x2,y2 \n\ndef findRadiusAndCentreOfContourEye(cnts):\n    M = cv2.moments(cnts)\n    if( M[\"m00\"]==0):\n        cX, cY = 0, 0\n    else:\n        cX = int(M[\"m10\"] / M[\"m00\"])\n        cY = int(M[\"m01\"] / M[\"m00\"])\n    if(cX < cY):\n        r = cX\n    else:\n        r = cY\n    return cX,cY,r\n\ndef drawCentreOnContourEye(image,cnts,cX,cY):\n    cv2.drawContours(image, [cnts], -1, (0, 255, 0), 2)\n    cv2.circle(image, (cX, cY), 7, (255, 255, 255), -1)\n    cv2.putText(image, \"center\", (cX - 20, cY - 20),\n    cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 2)\n    return image\n    \ndef Radius_Reduction(img,cX,cY,r):\n    h,w,c=img.shape\n    Frame=np.zeros((h,w,c),dtype=np.uint8)\n    cv2.circle(Frame,(int(cX),int(cY)),int(r), (255,255,255), -1)\n    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)\n    img1 =cv2.bitwise_and(img,img,mask=Frame1)\n    return img1\n\ndef imageResize(image, ht, wd):\n    # resized_image = imutils.resize(image, height = ht, width = wd)\n    resized_image = cv2.resize(image,(wd,ht))\n    return resized_image\n\ndef crop_black(image):\n    org = image.copy()\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n    thresh = cv2.threshold(blurred, 10, 255, cv2.THRESH_BINARY)[1]\n    # displayImage('thresh',thresh)\n    cnts = findContourEye(thresh)\n    x1,y1,x2,y2 = findContourEyeExtreme(cnts)\n    # print(x1,y1,x2,y2)\n    crop = org[y1:y2, x1:x2]\n    crop = imageResize(crop, img_ht, img_wd)\n    # displayImage(\"cr1\",crop)\n    return crop\n\ndef imageAugmentation(image):\n    x_flip = cv2.flip( image, 0 )\n    y_flip = cv2.flip( image, 1 )\n    xy_flip = cv2.flip(x_flip,1)\n    return x_flip, y_flip, xy_flip\n\ndef imageHistEqualization(image):\n    lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n    l, a, b = cv2.split(lab)\n    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n    cl = clahe.apply(l)\n    limg = cv2.merge((cl,a,b))\n    final = cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)\n    return final\n\ndef subtract_median_bg_image(im):\n    k = np.max(im.shape)//20*2+1\n    bg = cv2.medianBlur(im, k)\n    sub_med = cv2.addWeighted (im, 4, bg, -4, 100)\n    return sub_med\n\ndef colorEnhancement(image1,image2):\n    image_final = cv2.bitwise_and(image1,image2)\n    return image_final\n\ndef imageAugSave(path,img1,img2,img3,img4,img_ht,img_wd):\n    count = len(os.listdir(path))\n\n    img1 = imageResize(img1, img_ht, img_wd)\n    img2 = imageResize(img2, img_ht, img_wd)\n    img3 = imageResize(img3, img_ht, img_wd)\n    img4 = imageResize(img4, img_ht, img_wd)\n\n    cv2.imwrite(os.path.join(path , '%d.png'%(count+1)), img1)\n    cv2.imwrite(os.path.join(path , '%d.png'%(count+2)), img2)\n    cv2.imwrite(os.path.join(path , '%d.png'%(count+3)), img3)\n    cv2.imwrite(os.path.join(path , '%d.png'%(count+4)), img4)\n    return count+1,count+2,count+3,count+4\n\ndef processed_test_save(path,img,img_ht,img_wd):\n    count = len(os.listdir(path))\n    img = imageResize(img,img_ht,img_wd)\n    cv2.imwrite(os.path.join(path , '%d.png'%(count+1)), img)\n    return count+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img_ht = 256\nimg_wd = 256\npath_toCollect =  '/kaggle/input/aptos2019-blindness-detection/train_images'\npath_toSave = '/kaggle/working/trained_images'\ntrain_data = pd.read_csv('/kaggle/input/aptos2019-blindness-detection/train.csv')\nnewDataframe_cols = ['id_code','diagnosis'] \ntrained_data = pd.DataFrame(columns=newDataframe_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def feedToPipeline(image_name,diagnosis_type):\n    global path_toCollect\n    global path_toCollect\n    global img_ht,img_wd\n    global trained_data, train_data\n\n    try:\n        image_name = str(image_name) + '.png'\n        image = cv2.imread(os.path.join(path_toCollect,image_name))\n        image = imageResize(image, img_ht, img_wd)\n        org_copy = image.copy()\n        image_crop = crop_black(image)\n        image_clahe = imageHistEqualization(image_crop)\n        sub_med = subtract_median_bg_image(image_clahe)\n        image_final = colorEnhancement(sub_med, image_clahe)\n        aug1, aug2, aug3 = imageAugmentation(image_final)\n        count1,count2,count3,count4 = imageAugSave(path_toSave,image_final, aug1, aug2, aug3,img_ht,img_wd)\n        count1 = str(count1) + '.png'\n        count2 = str(count2) + '.png'\n        count3 = str(count3) + '.png'\n        count4 = str(count4) + '.png'\n        len_trained_data = len(trained_data)\n        trained_data.loc[len_trained_data]   = [count1,diagnosis_type] \n        trained_data.loc[len_trained_data+1] = [count2,diagnosis_type] \n        trained_data.loc[len_trained_data+2] = [count3,diagnosis_type] \n        trained_data.loc[len_trained_data+3] = [count4,diagnosis_type]\n#         print(\"Processed\")\n    except:\n        print(\"+========================+\")\n        pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.time()\n\n# # Vectorize approach took 846 seconds and the for loop took 905 seconds to process more than 3 thousand images\n# # \n# # np.vectorize(feedToPipeline)(train_data['id_code'],train_data['diagnosis'])\n# # \nfrom tqdm.notebook import tqdm\nfor i in tqdm(range(len(train_data))): \n#for i in tqdm(range(1)): \n#     print(i)\n    feedToPipeline(train_data['id_code'][i],train_data['diagnosis'][i])\n# # \ntrained_data.to_csv('/kaggle/working/final_trained.csv',index = False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls /kaggle/working/\n# !ls /kaggle/working/trained_images\n!ls -l /kaggle/working/trained_images/ | egrep -c '^-' ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\ngc.collect()\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Conv2D, BatchNormalization, LeakyReLU, Flatten, Activation, MaxPool2D\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.optimizers import Adam, RMSprop\nfrom keras.callbacks import EarlyStopping,ReduceLROnPlateau,LearningRateScheduler","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/working/final_trained.csv\")\ntrain_df[\"id_code\"]=train_df[\"id_code\"]\ntrain_df['diagnosis'] = train_df['diagnosis'].astype(str)\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Example of images \nimg_names = train_df['id_code'][:10]\nplt.figure(figsize=[15,15])\ni = 1\nfor img_name in img_names:\n    img = cv2.imread(\"/kaggle/working/trained_images/%s\" % img_name)[...,[2, 1, 0]]\n    ht,wd,ch = img.shape\n    print(ht,wd,ch)\n    plt.subplot(6, 5, i)\n    plt.imshow(img)\n    i += 1\nplt.show()\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nb_classes = 5\nlbls = list(map(str, range(nb_classes)))\nbatch_size = 32\nimg_size = 256\nnb_epochs = 5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ntrain_datagen=ImageDataGenerator(\n    rescale=1./255, \n    validation_split=0.20,\n#     horizontal_flip = True,   \n    rotation_range = 10,\n    zoom_range = 0.3,\n    width_shift_range = 0.3,\n    height_shift_range=0.3\n    )\n\ntrain_generator=train_datagen.flow_from_dataframe(\n    dataframe=train_df,\n    directory=\"/kaggle/working/trained_images\",\n    x_col=\"id_code\",\n    y_col=\"diagnosis\",\n    batch_size=batch_size,\n    shuffle=True,\n    class_mode=\"categorical\",\n    classes=lbls,\n    target_size=(img_size,img_size),\n    subset='training')\n\nvalid_generator=train_datagen.flow_from_dataframe(\n    dataframe=train_df,\n    directory=\"/kaggle/working/trained_images\",\n    x_col=\"id_code\",\n    y_col=\"diagnosis\",\n    batch_size=batch_size,\n    shuffle=True,\n    class_mode=\"categorical\", \n    classes=lbls,\n    target_size=(img_size,img_size),\n    subset='validation')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model():\n    \n    model = Sequential()\n    \n    \n    model.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(256,256,3)))\n    model.add(BatchNormalization())\n    model.add(Conv2D(64, kernel_size=3, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(MaxPool2D(pool_size=(2,2)))\n    \n    model.add(Conv2D(128, kernel_size=3, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(128, kernel_size=3, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(MaxPool2D(pool_size=(2,2)))\n    \n    model.add(Conv2D(256, kernel_size=3, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(256, kernel_size=3, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(MaxPool2D(pool_size=(2,2)))\n\n    model.add(Flatten())\n    model.add(Dense(1024, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(1024, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(5, activation='softmax'))\n\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"es = EarlyStopping(monitor='val_loss',\n                                      mode='min',\n                                      verbose=1,\n                                      patience=10)\n\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy',\n                                            patience=3,\n                                            verbose=1,\n                                            factor=0.25,\n                                            min_lr=0.000001)\n\noptimizer = Adam(learning_rate=0.5e-4, beta_1=0.9, beta_2=0.999, amsgrad=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = build_model()\nmodel.compile(\n    loss='binary_crossentropy',\n    optimizer=optimizer,\n    metrics=['accuracy']\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit_generator(\n    generator=train_generator, \n    steps_per_epoch  = 128, \n    validation_data  = valid_generator,\n    validation_steps = 128,\n#     epochs = 11, \n    epochs = 8, \n    use_multiprocessing=True,\n    verbose = 1,\n    callbacks = [es, learning_rate_reduction]\n)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save(\"aptos_first.h5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! rm -rf /kaggle/working/trained_images/\n! ls /kaggle/working","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}