{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    print(dirname, len(filenames))\n\n# You can write up to 20GB to the current directory /kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-02T05:40:01.807434Z","iopub.execute_input":"2021-08-02T05:40:01.807774Z","iopub.status.idle":"2021-08-02T05:40:07.753215Z","shell.execute_reply.started":"2021-08-02T05:40:01.807746Z","shell.execute_reply":"2021-08-02T05:40:07.752445Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim \nimport torch.nn.functional as F\nimport torchvision\n\nfrom torch.utils.data import  TensorDataset, DataLoader \nfrom torch.optim import lr_scheduler\nfrom torchvision import datasets, models, transforms\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(\"Using device\", device)\n\nfrom cv2 import cv2\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nimport time\nimport copy\nimport random\n\nfrom tqdm import tqdm\n\nrandom_seed = 42\n\ntorch.manual_seed(random_seed)\ntorch.cuda.manual_seed(random_seed)\ntorch.cuda.manual_seed_all(random_seed) # if use multi-GPU\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\nnp.random.seed(random_seed)\nrandom.seed(random_seed)\n\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2021-08-02T05:40:07.756056Z","iopub.execute_input":"2021-08-02T05:40:07.756323Z","iopub.status.idle":"2021-08-02T05:40:09.321983Z","shell.execute_reply.started":"2021-08-02T05:40:07.756297Z","shell.execute_reply":"2021-08-02T05:40:09.321085Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_path = \"/kaggle/input/aptos2019-blindness-detection/train_images/\"\ntest_path = \"/kaggle/input/aptos2019-blindness-detection/test_images/\"\ntrain_data = pd.read_csv(\"../input/aptos2019-blindness-detection/train.csv\")\ntest_data = pd.read_csv(\"../input/aptos2019-blindness-detection/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-08-02T05:40:09.323747Z","iopub.execute_input":"2021-08-02T05:40:09.324227Z","iopub.status.idle":"2021-08-02T05:40:09.376434Z","shell.execute_reply.started":"2021-08-02T05:40:09.324188Z","shell.execute_reply":"2021-08-02T05:40:09.375727Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_data.shape)\nprint(train_data.head(10))\nprint(test_data.shape)\nprint(test_data.head(10))\n\ntrain_data[\"diagnosis\"].value_counts().plot(kind=\"pie\")","metadata":{"execution":{"iopub.status.busy":"2021-08-02T05:40:09.379195Z","iopub.execute_input":"2021-08-02T05:40:09.379453Z","iopub.status.idle":"2021-08-02T05:40:09.541124Z","shell.execute_reply.started":"2021-08-02T05:40:09.379429Z","shell.execute_reply":"2021-08-02T05:40:09.540294Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# grayscale","metadata":{}},{"cell_type":"code","source":"def crop_image1(img,tol=7):\n    # img is image data\n    # tol  is tolerance        \n    mask = img>tol\n    return img[np.ix_(mask.any(1),mask.any(0))]\n\ndef crop_image(img,tol=7):\n    if img.ndim ==2:\n        mask = img>tol\n        return img[np.ix_(mask.any(1),mask.any(0))]\n    elif img.ndim==3:\n        h,w,_=img.shape\n#         print(h,w)\n        img1=cv2.resize(crop_image1(img[:,:,0]),(w,h))\n        img2=cv2.resize(crop_image1(img[:,:,1]),(w,h))\n        img3=cv2.resize(crop_image1(img[:,:,2]),(w,h))\n        \n#         print(img1.shape,img2.shape,img3.shape)\n        img[:,:,0]=img1\n        img[:,:,1]=img2\n        img[:,:,2]=img3\n        return img\n    \nfig = plt.figure(figsize=(25,16))\n\nfor class_id in sorted(train_data['diagnosis'].unique()):\n    for i, (idx, row) in enumerate(train_data.loc[train_data['diagnosis'] == class_id].sample(5).iterrows()):\n        ax = fig.add_subplot(5, 5, class_id * 5 + i + 1, xticks=[], yticks=[])\n        img = cv2.imread(train_path +'/' + row['id_code'] +'.png')\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n        img = crop_image(img)\n        img = cv2.resize(img,(300,300))\n        ax.imshow(img,cmap='gray')\n        ax.set_title('Label: %d-%d-%s' % (class_id, idx, row['id_code']) )","metadata":{"execution":{"iopub.status.busy":"2021-08-02T05:40:09.543661Z","iopub.execute_input":"2021-08-02T05:40:09.5439Z","iopub.status.idle":"2021-08-02T05:40:15.232035Z","shell.execute_reply.started":"2021-08-02T05:40:09.543877Z","shell.execute_reply":"2021-08-02T05:40:15.23121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ben color","metadata":{}},{"cell_type":"code","source":"def load_ben_color(path, sigmaX=10):\n    image = cv2.imread(path)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    image = crop_image(image)\n    image = cv2.resize(image, (300, 300))\n    image = cv2.addWeighted ( image,4, cv2.GaussianBlur( image , (0,0) , sigmaX) ,-4 ,128)        \n    return image\n\n\nfig = plt.figure(figsize=(25, 16))\nfor class_id in sorted(train_data['diagnosis'].unique()):\n    for i, (idx, row) in enumerate(train_data.loc[train_data['diagnosis'] == class_id].sample(5).iterrows()):\n        ax = fig.add_subplot(5,5, class_id * 5 + i + 1, xticks=[], yticks=[])\n        path=f\"../input/aptos2019-blindness-detection/train_images/{row['id_code']}.png\"\n        image = load_ben_color(path,sigmaX=30)\n        plt.imshow(image)\n        ax.set_title('%d-%d-%s' % (class_id, idx, row['id_code']))","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-08-02T05:40:15.233052Z","iopub.execute_input":"2021-08-02T05:40:15.233349Z","iopub.status.idle":"2021-08-02T05:40:25.742836Z","shell.execute_reply.started":"2021-08-02T05:40:15.233319Z","shell.execute_reply":"2021-08-02T05:40:25.734503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def crop_image1(img,tol=7):\n    # img is image data\n    # tol  is tolerance        \n    mask = img>tol\n    return img[np.ix_(mask.any(1),mask.any(0))]\n\ndef crop_image(img,tol=7):\n    if img.ndim ==2:\n        mask = img>tol\n        return img[np.ix_(mask.any(1),mask.any(0))]\n    elif img.ndim==3:\n        h,w,_=img.shape\n#         print(h,w)\n        img1=cv2.resize(crop_image1(img[:,:,0]),(w,h))\n        img2=cv2.resize(crop_image1(img[:,:,1]),(w,h))\n        img3=cv2.resize(crop_image1(img[:,:,2]),(w,h))\n        \n#         print(img1.shape,img2.shape,img3.shape)\n        img[:,:,0]=img1\n        img[:,:,1]=img2\n        img[:,:,2]=img3\n        return img","metadata":{"execution":{"iopub.status.busy":"2021-08-02T05:45:12.149695Z","iopub.status.idle":"2021-08-02T05:45:12.150281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def circle_crop(path):\n    img = cv2.imread(path)\n    img = crop_image(img)    \n\n    height, width, depth = img.shape    \n    \n    x = int(width/2)\n    y = int(height/2)\n    r = np.amin((x,y))\n    \n    circle_img = np.zeros((height, width), np.uint8)\n    cv2.circle(circle_img, (x,y), int(r), 1, thickness=-1)\n    img = cv2.bitwise_and(img, img, mask=circle_img)\n    img = crop_image(img)\n    return img\n\ndef change_ben_color(image, sigmaX=10):\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    image = cv2.addWeighted(image,4, cv2.GaussianBlur(image , (0,0),sigmaX) ,-4 ,128)        \n    image = cv2.addWeighted(image,-4, cv2.GaussianBlur(image , (0,0),sigmaX) ,4 ,128)        \n    return image\ndef change_gray_color(image, sigmaX=10):\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    image = cv2.addWeighted(image,4, cv2.GaussianBlur(image , (0,0),sigmaX) ,-4 ,128)        \n    image = cv2.addWeighted(image,-4, cv2.GaussianBlur(image , (0,0),sigmaX) ,4 ,128)        \n    return image\n\nfig = plt.figure(figsize=(25, 16))\nfor class_id in sorted(train_data['diagnosis'].unique()):\n    for i, (idx, row) in enumerate(train_data.loc[train_data['diagnosis'] == class_id].sample(5).iterrows()):\n        ax = fig.add_subplot(5,5, class_id * 5 + i + 1, xticks=[], yticks=[])\n        path=f\"../input/aptos2019-blindness-detection/train_images/{row['id_code']}.png\"\n        image = circle_crop(path)\n        #image = change_gray_color(image, 40)\n        #image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        image = change_ben_color(image, 30)\n        image = cv2.resize(image, (300, 300))\n        plt.imshow(image, cmap='gray')\n        \n        ax.set_title('%d-%d-%s' % (class_id, idx, row['id_code']))","metadata":{"execution":{"iopub.status.busy":"2021-08-02T05:40:25.756817Z","iopub.execute_input":"2021-08-02T05:40:25.75733Z","iopub.status.idle":"2021-08-02T05:41:18.026205Z","shell.execute_reply.started":"2021-08-02T05:40:25.757289Z","shell.execute_reply":"2021-08-02T05:41:18.025442Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_data['id_code'],val_set,train_data['diagnosis'],val_label = train_test_split(train_data['id_code'], train_data['diagnosis'],test_size=0.9, random_state=42)\ntrain_data=train_data.dropna()\ntrain_data=train_data.reset_index(drop=True)\ntrain_data.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T05:41:18.027943Z","iopub.execute_input":"2021-08-02T05:41:18.028404Z","iopub.status.idle":"2021-08-02T05:41:18.052786Z","shell.execute_reply.started":"2021-08-02T05:41:18.028355Z","shell.execute_reply":"2021-08-02T05:41:18.05211Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# augmentation 1 \n# transforms","metadata":{}},{"cell_type":"code","source":"transform = torchvision.transforms.Compose([\n    transforms.Resize((224,224)),\n    transforms.RandomHorizontalFlip(), #0.5\n    transforms.RandomRotation(10),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])","metadata":{"execution":{"iopub.status.busy":"2021-08-02T05:41:18.053879Z","iopub.execute_input":"2021-08-02T05:41:18.054313Z","iopub.status.idle":"2021-08-02T05:41:18.059993Z","shell.execute_reply.started":"2021-08-02T05:41:18.05428Z","shell.execute_reply":"2021-08-02T05:41:18.059094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. 색 변환 x","metadata":{}},{"cell_type":"code","source":"from PIL import Image\n\nclass Dataset1():\n    def __init__(self, data, root, transform):\n        self.files = list(root + data['id_code'] + '.png')\n        self.targets = data['diagnosis']\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.files)\n    \n    def circle_crop(self,path):\n        img = cv2.imread(path)    \n        height, width, depth = img.shape       \n        x = int(width/2)\n        y = int(height/2)\n        r = np.amin((x,y))\n        circle_img = np.zeros((height, width), np.uint8)\n        cv2.circle(circle_img, (x,y), int(r), 1, thickness=-1)\n        img = cv2.bitwise_and(img, img, mask=circle_img)\n        img = crop_image(img)\n        return img\n    \n    def __getitem__(self, idx):\n        img = self.circle_crop(self.files[idx])\n        img = Image.fromarray(img).convert('RGB')\n        x = self.transform(img)\n        y = torch.tensor(self.targets[idx]).unsqueeze(0).float()\n        return x, y","metadata":{"execution":{"iopub.status.busy":"2021-08-02T05:41:18.061227Z","iopub.execute_input":"2021-08-02T05:41:18.061833Z","iopub.status.idle":"2021-08-02T05:41:18.077588Z","shell.execute_reply.started":"2021-08-02T05:41:18.061738Z","shell.execute_reply":"2021-08-02T05:41:18.075408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = Dataset1(train_data, train_path, transform)\ndataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T05:41:18.07993Z","iopub.execute_input":"2021-08-02T05:41:18.081784Z","iopub.status.idle":"2021-08-02T05:41:18.095069Z","shell.execute_reply.started":"2021-08-02T05:41:18.081739Z","shell.execute_reply":"2021-08-02T05:41:18.093357Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x,y=train_dataset.__getitem__(0)\nprint(x.shape)\nprint(y.shape)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T05:41:18.101252Z","iopub.execute_input":"2021-08-02T05:41:18.10163Z","iopub.status.idle":"2021-08-02T05:41:18.620449Z","shell.execute_reply.started":"2021-08-02T05:41:18.101578Z","shell.execute_reply":"2021-08-02T05:41:18.619382Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images,labels=next(iter(dataloader))\nimages.shape,labels.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-02T05:41:18.624743Z","iopub.execute_input":"2021-08-02T05:41:18.62517Z","iopub.status.idle":"2021-08-02T05:41:31.021109Z","shell.execute_reply.started":"2021-08-02T05:41:18.625137Z","shell.execute_reply":"2021-08-02T05:41:31.020266Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_ft=models.resnet18(pretrained=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T05:41:31.022395Z","iopub.execute_input":"2021-08-02T05:41:31.022727Z","iopub.status.idle":"2021-08-02T05:41:34.556077Z","shell.execute_reply.started":"2021-08-02T05:41:31.02269Z","shell.execute_reply":"2021-08-02T05:41:34.555262Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params=list(model_ft.parameters())\nlen(params),params[0].size()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T05:41:34.557291Z","iopub.execute_input":"2021-08-02T05:41:34.557801Z","iopub.status.idle":"2021-08-02T05:41:34.565409Z","shell.execute_reply.started":"2021-08-02T05:41:34.557763Z","shell.execute_reply":"2021-08-02T05:41:34.564409Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_features=model_ft.fc.in_features\nmodel_ft.fc=nn.Linear(num_features,1)\n\nmodel_ft=model_ft.to(device)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T05:41:34.566867Z","iopub.execute_input":"2021-08-02T05:41:34.567237Z","iopub.status.idle":"2021-08-02T05:41:38.663825Z","shell.execute_reply.started":"2021-08-02T05:41:34.567202Z","shell.execute_reply":"2021-08-02T05:41:38.662997Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion =nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(lr=1e-4, params=model_ft.parameters())\nscheduler = lr_scheduler.StepLR(optimizer, step_size=10)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T05:41:38.665011Z","iopub.execute_input":"2021-08-02T05:41:38.665349Z","iopub.status.idle":"2021-08-02T05:41:38.670365Z","shell.execute_reply.started":"2021-08-02T05:41:38.665316Z","shell.execute_reply":"2021-08-02T05:41:38.669524Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"since = time.time()\ncriterion = torch.nn.MSELoss()\nnum_epochs = 15\nfor epoch in range(num_epochs):\n    print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n    print('-' * 10)\n    model_ft.train()\n    running_loss = 0.0\n    tk0 = tqdm(dataloader, total=int(len(dataloader)))\n    counter = 0\n    for bi, (d, t) in enumerate(tk0):\n        inputs = d\n        labels = t\n        inputs = inputs.to(device, dtype=torch.float)\n        labels = labels.to(device, dtype=torch.float)\n        optimizer.zero_grad()\n        with torch.set_grad_enabled(True):\n            outputs = model_ft(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n        running_loss += loss.item() * inputs.size(0)\n        counter += 1\n        tk0.set_postfix(loss=(running_loss / (counter * dataloader.batch_size)))\n    epoch_loss = running_loss / len(dataloader)\n    print('Training Loss: {:.4f}'.format(epoch_loss))\n    scheduler.step()\n\ntime_elapsed = time.time() - since\nprint('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\ntorch.save(model_ft.state_dict(), \"model1.bin\")","metadata":{"execution":{"iopub.status.busy":"2021-08-02T05:45:37.203352Z","iopub.execute_input":"2021-08-02T05:45:37.203762Z","iopub.status.idle":"2021-08-02T06:02:58.783955Z","shell.execute_reply.started":"2021-08-02T05:45:37.203731Z","shell.execute_reply":"2021-08-02T06:02:58.78074Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. change ben color","metadata":{}},{"cell_type":"code","source":"from PIL import Image\n\nclass Dataset2():\n    def __init__(self, data, root, transform):\n        self.files = list(root + data['id_code'] + '.png')\n        self.targets = data['diagnosis']\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.files)\n    \n    def circle_crop(self,path):\n        img = cv2.imread(path)    \n        height, width, depth = img.shape       \n        x = int(width/2)\n        y = int(height/2)\n        r = np.amin((x,y))\n        circle_img = np.zeros((height, width), np.uint8)\n        cv2.circle(circle_img, (x,y), int(r), 1, thickness=-1)\n        img = cv2.bitwise_and(img, img, mask=circle_img)\n        img = crop_image(img)\n        return img\n\n    def change_ben_color(self,image, sigmaX=30):\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        image = cv2.addWeighted(image,4, cv2.GaussianBlur(image , (0,0),sigmaX) ,-4 ,128)        \n        image = cv2.addWeighted(image,-4, cv2.GaussianBlur(image , (0,0),sigmaX) ,4 ,128)        \n        return image\n    \n    def change_gray_color(self,image, sigmaX=40):\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        image = cv2.addWeighted(image,4, cv2.GaussianBlur(image , (0,0),sigmaX) ,-4 ,128)        \n        image = cv2.addWeighted(image,-4, cv2.GaussianBlur(image , (0,0),sigmaX) ,4 ,128)        \n        return image\n    \n    def __getitem__(self, idx):\n        img = self.circle_crop(self.files[idx])\n        img = self.change_ben_color(img) \n        #img = self.change_gray_color(img) \n        img = Image.fromarray(img).convert('RGB')\n        x = self.transform(img)\n        y = torch.tensor(self.targets[idx]).unsqueeze(0).float()\n        return x, y","metadata":{"execution":{"iopub.status.busy":"2021-08-02T06:02:58.785552Z","iopub.execute_input":"2021-08-02T06:02:58.785959Z","iopub.status.idle":"2021-08-02T06:02:58.798891Z","shell.execute_reply.started":"2021-08-02T06:02:58.785919Z","shell.execute_reply":"2021-08-02T06:02:58.797973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = Dataset2(train_data, train_path, transform)\ndataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T06:02:58.800741Z","iopub.execute_input":"2021-08-02T06:02:58.801268Z","iopub.status.idle":"2021-08-02T06:02:58.818319Z","shell.execute_reply.started":"2021-08-02T06:02:58.801232Z","shell.execute_reply":"2021-08-02T06:02:58.817645Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model2=models.resnet18(pretrained=True)\n\nnum_features=model2.fc.in_features\nmodel2.fc=nn.Linear(num_features,1)\n\nmodel2=model2.to(device)\n\ncriterion =nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(lr=1e-4, params=model2.parameters())\nscheduler = lr_scheduler.StepLR(optimizer, step_size=10)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T06:02:58.820469Z","iopub.execute_input":"2021-08-02T06:02:58.820992Z","iopub.status.idle":"2021-08-02T06:02:59.153923Z","shell.execute_reply.started":"2021-08-02T06:02:58.82088Z","shell.execute_reply":"2021-08-02T06:02:59.153068Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"since = time.time()\ncriterion = torch.nn.MSELoss()\nnum_epochs = 15\nfor epoch in range(num_epochs):\n    print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n    print('-' * 10)\n    model2.train()\n    running_loss = 0.0\n    tk0 = tqdm(dataloader, total=int(len(dataloader)))\n    counter = 0\n    for bi, (d, t) in enumerate(tk0):\n        inputs = d\n        labels = t\n        inputs = inputs.to(device, dtype=torch.float)\n        labels = labels.to(device, dtype=torch.float)\n        optimizer.zero_grad()\n        with torch.set_grad_enabled(True):\n            outputs = model2(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n        running_loss += loss.item() * inputs.size(0)\n        counter += 1\n        tk0.set_postfix(loss=(running_loss / (counter * dataloader.batch_size)))\n    epoch_loss = running_loss / len(dataloader)\n    print('Training Loss: {:.4f}'.format(epoch_loss))\n    scheduler.step()\n\ntime_elapsed = time.time() - since\nprint('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\ntorch.save(model2.state_dict(), \"model2.bin\")","metadata":{"execution":{"iopub.status.busy":"2021-08-02T06:02:59.155238Z","iopub.execute_input":"2021-08-02T06:02:59.155617Z","iopub.status.idle":"2021-08-02T08:29:09.26286Z","shell.execute_reply.started":"2021-08-02T06:02:59.155581Z","shell.execute_reply":"2021-08-02T08:29:09.261508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. change gray color","metadata":{}},{"cell_type":"code","source":"from PIL import Image\n\nclass Dataset3():\n    def __init__(self, data, root, transform):\n        self.files = list(root + data['id_code'] + '.png')\n        self.targets = data['diagnosis']\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.files)\n    \n    def circle_crop(self,path):\n        img = cv2.imread(path)    \n        height, width, depth = img.shape       \n        x = int(width/2)\n        y = int(height/2)\n        r = np.amin((x,y))\n        circle_img = np.zeros((height, width), np.uint8)\n        cv2.circle(circle_img, (x,y), int(r), 1, thickness=-1)\n        img = cv2.bitwise_and(img, img, mask=circle_img)\n        img = crop_image(img)\n        return img\n\n    def change_ben_color(self,image, sigmaX=30):\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        image = cv2.addWeighted(image,4, cv2.GaussianBlur(image , (0,0),sigmaX) ,-4 ,128)        \n        image = cv2.addWeighted(image,-4, cv2.GaussianBlur(image , (0,0),sigmaX) ,4 ,128)        \n        return image\n    \n    def change_gray_color(self,image, sigmaX=40):\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        image = cv2.addWeighted(image,4, cv2.GaussianBlur(image , (0,0),sigmaX) ,-4 ,128)        \n        image = cv2.addWeighted(image,-4, cv2.GaussianBlur(image , (0,0),sigmaX) ,4 ,128)        \n        return image\n    \n    def __getitem__(self, idx):\n        img = self.circle_crop(self.files[idx])\n        #img = self.change_ben_color(img) \n        img = self.change_gray_color(img) \n        img = Image.fromarray(img).convert('RGB')\n        x = self.transform(img)\n        y = torch.tensor(self.targets[idx]).unsqueeze(0).float()\n        return x, y","metadata":{"execution":{"iopub.status.busy":"2021-08-02T08:29:09.264281Z","iopub.execute_input":"2021-08-02T08:29:09.264668Z","iopub.status.idle":"2021-08-02T08:29:09.277503Z","shell.execute_reply.started":"2021-08-02T08:29:09.264628Z","shell.execute_reply":"2021-08-02T08:29:09.276665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = Dataset3(train_data, train_path, transform)\ndataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-08-02T08:29:09.278827Z","iopub.execute_input":"2021-08-02T08:29:09.279465Z","iopub.status.idle":"2021-08-02T08:29:09.293427Z","shell.execute_reply.started":"2021-08-02T08:29:09.279425Z","shell.execute_reply":"2021-08-02T08:29:09.292546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model3=models.resnet18(pretrained=True)\n\nnum_features=model3.fc.in_features\nmodel3.fc=nn.Linear(num_features,1)\n\nmodel3=model3.to(device)\n\ncriterion =nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(lr=1e-4, params=model3.parameters())\nscheduler = lr_scheduler.StepLR(optimizer, step_size=10)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T08:29:09.296665Z","iopub.execute_input":"2021-08-02T08:29:09.296947Z","iopub.status.idle":"2021-08-02T08:29:09.59283Z","shell.execute_reply.started":"2021-08-02T08:29:09.296925Z","shell.execute_reply":"2021-08-02T08:29:09.591963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"since = time.time()\ncriterion = torch.nn.MSELoss()\nnum_epochs = 15\nfor epoch in range(num_epochs):\n    print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n    print('-' * 10)\n    model3.train()\n    running_loss = 0.0\n    tk0 = tqdm(dataloader, total=int(len(dataloader)))\n    counter = 0\n    for bi, (d, t) in enumerate(tk0):\n        inputs = d\n        labels = t\n        inputs = inputs.to(device, dtype=torch.float)\n        labels = labels.to(device, dtype=torch.float)\n        optimizer.zero_grad()\n        with torch.set_grad_enabled(True):\n            outputs = model3(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n        running_loss += loss.item() * inputs.size(0)\n        counter += 1\n        tk0.set_postfix(loss=(running_loss / (counter * dataloader.batch_size)))\n    epoch_loss = running_loss / len(dataloader)\n    print('Training Loss: {:.4f}'.format(epoch_loss))\n    scheduler.step()\n\ntime_elapsed = time.time() - since\nprint('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\ntorch.save(model3.state_dict(), \"model3.bin\")","metadata":{"execution":{"iopub.status.busy":"2021-08-02T08:29:09.594759Z","iopub.execute_input":"2021-08-02T08:29:09.595113Z","iopub.status.idle":"2021-08-02T10:32:51.283467Z","shell.execute_reply.started":"2021-08-02T08:29:09.595077Z","shell.execute_reply":"2021-08-02T10:32:51.280657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# augmentation 2 \n# 1. albumentation","metadata":{}},{"cell_type":"code","source":"import albumentations\nimport albumentations.pytorch","metadata":{"execution":{"iopub.status.busy":"2021-08-02T10:32:51.284904Z","iopub.execute_input":"2021-08-02T10:32:51.28565Z","iopub.status.idle":"2021-08-02T10:32:52.169787Z","shell.execute_reply.started":"2021-08-02T10:32:51.285603Z","shell.execute_reply":"2021-08-02T10:32:52.168603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\ntransform = torchvision.transforms.Compose([\n    transforms.Resize((224,224)),\n    transforms.RandomHorizontalFlip(), #0.5\n    transforms.RandomRotation(10),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n'''","metadata":{"execution":{"iopub.status.busy":"2021-08-02T10:32:52.171232Z","iopub.execute_input":"2021-08-02T10:32:52.171757Z","iopub.status.idle":"2021-08-02T10:32:52.178275Z","shell.execute_reply.started":"2021-08-02T10:32:52.171712Z","shell.execute_reply":"2021-08-02T10:32:52.177296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 이거 ~~!~~~~~~~~!!@!!!\n\nalbumentations_transform = albumentations.Compose([\n    albumentations.Resize(224,224), \n    albumentations.HorizontalFlip(), # Same with transforms.RandomHorizontalFlip()\n    albumentations.RandomRotate90(),\n    albumentations.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    albumentations.pytorch.transforms.ToTensorV2()\n])","metadata":{"execution":{"iopub.status.busy":"2021-08-02T10:32:52.179841Z","iopub.execute_input":"2021-08-02T10:32:52.180487Z","iopub.status.idle":"2021-08-02T10:32:52.18954Z","shell.execute_reply.started":"2021-08-02T10:32:52.180446Z","shell.execute_reply":"2021-08-02T10:32:52.18862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nclass Dataset4():\n    def __init__(self, data, root, transform):\n        self.files = list(root + data['id_code'] + '.png')\n        self.targets = data['diagnosis']\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.files)\n    \n    def circle_crop(self,path):\n        img = cv2.imread(path)    \n        height, width, depth = img.shape       \n        x = int(width/2)\n        y = int(height/2)\n        r = np.amin((x,y))\n        circle_img = np.zeros((height, width), np.uint8)\n        cv2.circle(circle_img, (x,y), int(r), 1, thickness=-1)\n        img = cv2.bitwise_and(img, img, mask=circle_img)\n        img = crop_image(img)\n        return img\n\n    def change_ben_color(self,image, sigmaX=30):\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        image = cv2.addWeighted(image,4, cv2.GaussianBlur(image , (0,0),sigmaX) ,-4 ,128)        \n        image = cv2.addWeighted(image,-4, cv2.GaussianBlur(image , (0,0),sigmaX) ,4 ,128)        \n        return image\n    \n    def change_gray_color(self,image, sigmaX=40):\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        image = cv2.addWeighted(image,4, cv2.GaussianBlur(image , (0,0),sigmaX) ,-4 ,128)        \n        image = cv2.addWeighted(image,-4, cv2.GaussianBlur(image , (0,0),sigmaX) ,4 ,128)        \n        return image\n    \n    def __getitem__(self, idx):\n        img = self.circle_crop(self.files[idx])\n        x = self.transform(image=img)['image'] #dictionary \n        y = torch.tensor(self.targets[idx]).unsqueeze(0).float()\n        return x, y","metadata":{"execution":{"iopub.status.busy":"2021-08-02T10:32:52.190747Z","iopub.execute_input":"2021-08-02T10:32:52.191103Z","iopub.status.idle":"2021-08-02T10:32:52.206201Z","shell.execute_reply.started":"2021-08-02T10:32:52.191058Z","shell.execute_reply":"2021-08-02T10:32:52.205152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = Dataset4(train_data, train_path, albumentations_transform)\ndataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T11:01:44.844714Z","iopub.execute_input":"2021-08-02T11:01:44.845064Z","iopub.status.idle":"2021-08-02T11:01:44.850961Z","shell.execute_reply.started":"2021-08-02T11:01:44.845035Z","shell.execute_reply":"2021-08-02T11:01:44.8499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model4=models.resnet18(pretrained=True)\n\nnum_features=model4.fc.in_features\nmodel4.fc=nn.Linear(num_features,1)\n\nmodel4=model3.to(device)\n\ncriterion =nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(lr=1e-4, params=model4.parameters())\nscheduler = lr_scheduler.StepLR(optimizer, step_size=10)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T11:01:45.8223Z","iopub.execute_input":"2021-08-02T11:01:45.822668Z","iopub.status.idle":"2021-08-02T11:01:46.155975Z","shell.execute_reply.started":"2021-08-02T11:01:45.822636Z","shell.execute_reply":"2021-08-02T11:01:46.154761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images,labels=next(iter(dataloader))\nprint(images.shape,labels.shape)\n\nparams=list(model4.parameters())\nlen(params),params[0].size()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T11:01:47.177771Z","iopub.execute_input":"2021-08-02T11:01:47.178238Z","iopub.status.idle":"2021-08-02T11:01:58.332158Z","shell.execute_reply.started":"2021-08-02T11:01:47.178203Z","shell.execute_reply":"2021-08-02T11:01:58.331331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"since = time.time()\ncriterion = torch.nn.MSELoss()\nnum_epochs = 15\nfor epoch in range(num_epochs):\n    print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n    print('-' * 10)\n    model4.train()\n    running_loss = 0.0\n    tk0 = tqdm(dataloader, total=int(len(dataloader)))\n    counter = 0\n    for bi, (d, t) in enumerate(tk0):\n        inputs = d\n        labels = t\n        inputs = inputs.to(device, dtype=torch.float)\n        labels = labels.to(device, dtype=torch.float)\n        optimizer.zero_grad()\n        with torch.set_grad_enabled(True):\n            outputs = model4(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n        running_loss += loss.item() * inputs.size(0)\n        counter += 1\n        tk0.set_postfix(loss=(running_loss / (counter * dataloader.batch_size)))\n    epoch_loss = running_loss / len(dataloader)\n    print('Training Loss: {:.4f}'.format(epoch_loss))\n    scheduler.step()\n\ntime_elapsed = time.time() - since\nprint('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\ntorch.save(model4.state_dict(), \"model4.bin\")","metadata":{"execution":{"iopub.status.busy":"2021-08-02T11:02:08.496687Z","iopub.execute_input":"2021-08-02T11:02:08.496998Z","iopub.status.idle":"2021-08-02T11:17:26.511998Z","shell.execute_reply.started":"2021-08-02T11:02:08.49697Z","shell.execute_reply":"2021-08-02T11:17:26.510985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. albumentation One of","metadata":{}},{"cell_type":"markdown","source":"1.ResizedRandomCrop","metadata":{}},{"cell_type":"code","source":"albumentations_transform_oneof = albumentations.Compose([\n    albumentations.Resize(256, 256), \n    albumentations.RandomCrop(224, 224),\n    albumentations.OneOf([\n                          albumentations.HorizontalFlip(p=1),\n                          albumentations.RandomRotate90(p=1),\n                          albumentations.VerticalFlip(p=1)            \n                ], p=1),\n    albumentations.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    albumentations.pytorch.transforms.ToTensorV2()\n])","metadata":{"execution":{"iopub.status.busy":"2021-08-02T11:17:26.513564Z","iopub.execute_input":"2021-08-02T11:17:26.513919Z","iopub.status.idle":"2021-08-02T11:17:26.520692Z","shell.execute_reply.started":"2021-08-02T11:17:26.513884Z","shell.execute_reply":"2021-08-02T11:17:26.519764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Dataset5():\n    def __init__(self, data, root, transform):\n        self.files = list(root + data['id_code'] + '.png')\n        self.targets = data['diagnosis']\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.files)\n    \n    def circle_crop(self,path):\n        img = cv2.imread(path)    \n        height, width, depth = img.shape       \n        x = int(width/2)\n        y = int(height/2)\n        r = np.amin((x,y))\n        circle_img = np.zeros((height, width), np.uint8)\n        cv2.circle(circle_img, (x,y), int(r), 1, thickness=-1)\n        img = cv2.bitwise_and(img, img, mask=circle_img)\n        img = crop_image(img)\n        return img\n\n    def change_ben_color(self,image, sigmaX=30):\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        image = cv2.addWeighted(image,4, cv2.GaussianBlur(image , (0,0),sigmaX) ,-4 ,128)        \n        image = cv2.addWeighted(image,-4, cv2.GaussianBlur(image , (0,0),sigmaX) ,4 ,128)        \n        return image\n    \n    def change_gray_color(self,image, sigmaX=40):\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        image = cv2.addWeighted(image,4, cv2.GaussianBlur(image , (0,0),sigmaX) ,-4 ,128)        \n        image = cv2.addWeighted(image,-4, cv2.GaussianBlur(image , (0,0),sigmaX) ,4 ,128)        \n        return image\n    \n    def __getitem__(self, idx):\n        img = self.circle_crop(self.files[idx])\n        #img = self.change_ben_color(img) \n        #img = self.change_gray_color(img) \n        x = self.transform(image=img)['image'] #dictionary \n        y = torch.tensor(self.targets[idx]).unsqueeze(0).float()\n        return x, y","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-08-02T11:17:26.52237Z","iopub.execute_input":"2021-08-02T11:17:26.522878Z","iopub.status.idle":"2021-08-02T11:17:26.536333Z","shell.execute_reply.started":"2021-08-02T11:17:26.522844Z","shell.execute_reply":"2021-08-02T11:17:26.535462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = Dataset5(train_data, train_path, albumentations_transform_oneof)\ndataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-08-02T11:17:26.539076Z","iopub.execute_input":"2021-08-02T11:17:26.539327Z","iopub.status.idle":"2021-08-02T11:17:26.551267Z","shell.execute_reply.started":"2021-08-02T11:17:26.539298Z","shell.execute_reply":"2021-08-02T11:17:26.550447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model5=models.resnet18(pretrained=True)\n\nnum_features=model5.fc.in_features\nmodel5.fc=nn.Linear(num_features,1)\n\nmodel5=model5.to(device)\n\ncriterion =nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(lr=1e-4, params=model5.parameters())\nscheduler = lr_scheduler.StepLR(optimizer, step_size=10)","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-08-02T11:17:26.552522Z","iopub.execute_input":"2021-08-02T11:17:26.55288Z","iopub.status.idle":"2021-08-02T11:17:26.87039Z","shell.execute_reply.started":"2021-08-02T11:17:26.552845Z","shell.execute_reply":"2021-08-02T11:17:26.869501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"since = time.time()\ncriterion = torch.nn.MSELoss()\nnum_epochs = 15\nfor epoch in range(num_epochs):\n    print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n    print('-' * 10)\n    model5.train()\n    running_loss = 0.0\n    tk0 = tqdm(dataloader, total=int(len(dataloader)))\n    counter = 0\n    for bi, (d, t) in enumerate(tk0):\n        inputs = d\n        labels = t\n        inputs = inputs.to(device, dtype=torch.float)\n        labels = labels.to(device, dtype=torch.float)\n        optimizer.zero_grad()\n        with torch.set_grad_enabled(True):\n            outputs = model5(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n        running_loss += loss.item() * inputs.size(0)\n        counter += 1\n        tk0.set_postfix(loss=(running_loss / (counter * dataloader.batch_size)))\n    epoch_loss = running_loss / len(dataloader)\n    print('Training Loss: {:.4f}'.format(epoch_loss))\n    scheduler.step()\n\ntime_elapsed = time.time() - since\nprint('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\ntorch.save(model5.state_dict(), \"model5.bin\")","metadata":{"execution":{"iopub.status.busy":"2021-08-02T11:17:26.871691Z","iopub.execute_input":"2021-08-02T11:17:26.872011Z","iopub.status.idle":"2021-08-02T11:32:53.430509Z","shell.execute_reply.started":"2021-08-02T11:17:26.871978Z","shell.execute_reply":"2021-08-02T11:32:53.429591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2. just resize","metadata":{}},{"cell_type":"code","source":"albumentations_transform_resize = albumentations.Compose([\n    albumentations.Resize(224, 224),\n    albumentations.OneOf([\n                          albumentations.HorizontalFlip(p=1),\n                          albumentations.RandomRotate90(p=1),\n                          albumentations.VerticalFlip(p=1)            \n                ]),\n    albumentations.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    albumentations.pytorch.transforms.ToTensorV2()\n])","metadata":{"execution":{"iopub.status.busy":"2021-08-02T11:32:53.431836Z","iopub.execute_input":"2021-08-02T11:32:53.432351Z","iopub.status.idle":"2021-08-02T11:32:53.438967Z","shell.execute_reply.started":"2021-08-02T11:32:53.432309Z","shell.execute_reply":"2021-08-02T11:32:53.438086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Dataset6():\n    def __init__(self, data, root, transform):\n        self.files = list(root + data['id_code'] + '.png')\n        self.targets = data['diagnosis']\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.files)\n    \n    def circle_crop(self,path):\n        img = cv2.imread(path)    \n        height, width, depth = img.shape       \n        x = int(width/2)\n        y = int(height/2)\n        r = np.amin((x,y))\n        circle_img = np.zeros((height, width), np.uint8)\n        cv2.circle(circle_img, (x,y), int(r), 1, thickness=-1)\n        img = cv2.bitwise_and(img, img, mask=circle_img)\n        img = crop_image(img)\n        return img\n\n    def __getitem__(self, idx):\n        img = self.circle_crop(self.files[idx])\n        x = self.transform(image=img)['image'] #dictionary \n        y = torch.tensor(self.targets[idx]).unsqueeze(0).float()\n        return x, y","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-08-02T11:32:53.441917Z","iopub.execute_input":"2021-08-02T11:32:53.442295Z","iopub.status.idle":"2021-08-02T11:32:53.45491Z","shell.execute_reply.started":"2021-08-02T11:32:53.442258Z","shell.execute_reply":"2021-08-02T11:32:53.454026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = Dataset6(train_data, train_path, albumentations_transform_resize)\ndataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T11:32:53.456858Z","iopub.execute_input":"2021-08-02T11:32:53.457204Z","iopub.status.idle":"2021-08-02T11:32:53.468631Z","shell.execute_reply.started":"2021-08-02T11:32:53.457168Z","shell.execute_reply":"2021-08-02T11:32:53.467845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model6=models.resnet18(pretrained=True)\n\nnum_features=model6.fc.in_features\nmodel6.fc=nn.Linear(num_features,1)\n\nmodel6=model6.to(device)\n\ncriterion =nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(lr=1e-4, params=model6.parameters())\nscheduler = lr_scheduler.StepLR(optimizer, step_size=10)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T11:32:53.470498Z","iopub.execute_input":"2021-08-02T11:32:53.470734Z","iopub.status.idle":"2021-08-02T11:32:53.770004Z","shell.execute_reply.started":"2021-08-02T11:32:53.470711Z","shell.execute_reply":"2021-08-02T11:32:53.769116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"since = time.time()\ncriterion = torch.nn.MSELoss()\nnum_epochs = 15\nfor epoch in range(num_epochs):\n    print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n    print('-' * 10)\n    model6.train()\n    running_loss = 0.0\n    tk0 = tqdm(dataloader, total=int(len(dataloader)))\n    counter = 0\n    for bi, (d, t) in enumerate(tk0):\n        inputs = d\n        labels = t\n        inputs = inputs.to(device, dtype=torch.float)\n        labels = labels.to(device, dtype=torch.float)\n        optimizer.zero_grad()\n        with torch.set_grad_enabled(True):\n            outputs = model6(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n        running_loss += loss.item() * inputs.size(0)\n        counter += 1\n        tk0.set_postfix(loss=(running_loss / (counter * dataloader.batch_size)))\n    epoch_loss = running_loss / len(dataloader)\n    print('Training Loss: {:.4f}'.format(epoch_loss))\n    scheduler.step()\n\ntime_elapsed = time.time() - since\nprint('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\ntorch.save(model6.state_dict(), \"model6.bin\")","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-08-02T11:32:53.771422Z","iopub.execute_input":"2021-08-02T11:32:53.771804Z","iopub.status.idle":"2021-08-02T11:48:40.127714Z","shell.execute_reply.started":"2021-08-02T11:32:53.771766Z","shell.execute_reply":"2021-08-02T11:48:40.126777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"3. p =1","metadata":{}},{"cell_type":"code","source":"albumentations_transform_ = albumentations.Compose([\n    albumentations.Resize(224, 224),\n    albumentations.OneOf([\n                          albumentations.HorizontalFlip(p=1),\n                          albumentations.RandomRotate90(p=1),\n                          albumentations.VerticalFlip(p=1)            \n                ],p=1),\n    albumentations.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    albumentations.pytorch.transforms.ToTensorV2()\n])","metadata":{"execution":{"iopub.status.busy":"2021-08-02T12:27:02.101119Z","iopub.execute_input":"2021-08-02T12:27:02.101537Z","iopub.status.idle":"2021-08-02T12:27:02.109188Z","shell.execute_reply.started":"2021-08-02T12:27:02.101494Z","shell.execute_reply":"2021-08-02T12:27:02.107812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Dataset7():\n    def __init__(self, data, root, transform):\n        self.files = list(root + data['id_code'] + '.png')\n        self.targets = data['diagnosis']\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.files)\n    \n    def circle_crop(self,path):\n        img = cv2.imread(path)    \n        height, width, depth = img.shape       \n        x = int(width/2)\n        y = int(height/2)\n        r = np.amin((x,y))\n        circle_img = np.zeros((height, width), np.uint8)\n        cv2.circle(circle_img, (x,y), int(r), 1, thickness=-1)\n        img = cv2.bitwise_and(img, img, mask=circle_img)\n        img = crop_image(img)\n        return img\n\n    def __getitem__(self, idx):\n        img = self.circle_crop(self.files[idx])\n        x = self.transform(image=img)['image'] #dictionary \n        y = torch.tensor(self.targets[idx]).unsqueeze(0).float()\n        return x, y","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-08-02T12:27:02.110765Z","iopub.execute_input":"2021-08-02T12:27:02.111167Z","iopub.status.idle":"2021-08-02T12:27:02.124137Z","shell.execute_reply.started":"2021-08-02T12:27:02.111127Z","shell.execute_reply":"2021-08-02T12:27:02.123174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = Dataset7(train_data, train_path, albumentations_transform_)\ndataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T12:27:02.127314Z","iopub.execute_input":"2021-08-02T12:27:02.127624Z","iopub.status.idle":"2021-08-02T12:27:02.139656Z","shell.execute_reply.started":"2021-08-02T12:27:02.127598Z","shell.execute_reply":"2021-08-02T12:27:02.138785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model7=models.resnet18(pretrained=True)\n\nnum_features=model7.fc.in_features\nmodel7.fc=nn.Linear(num_features,1)\n\nmodel7=model7.to(device)\n\ncriterion =nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(lr=1e-4, params=model7.parameters())\nscheduler = lr_scheduler.StepLR(optimizer, step_size=10)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T12:27:02.141093Z","iopub.execute_input":"2021-08-02T12:27:02.141564Z","iopub.status.idle":"2021-08-02T12:27:02.463885Z","shell.execute_reply.started":"2021-08-02T12:27:02.141528Z","shell.execute_reply":"2021-08-02T12:27:02.462904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"since = time.time()\ncriterion = torch.nn.MSELoss()\nnum_epochs = 15\nfor epoch in range(num_epochs):\n    print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n    print('-' * 10)\n    model7.train()\n    running_loss = 0.0\n    tk0 = tqdm(dataloader, total=int(len(dataloader)))\n    counter = 0\n    for bi, (d, t) in enumerate(tk0):\n        inputs = d\n        labels = t\n        inputs = inputs.to(device, dtype=torch.float)\n        labels = labels.to(device, dtype=torch.float)\n        optimizer.zero_grad()\n        with torch.set_grad_enabled(True):\n            outputs = model7(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n        running_loss += loss.item() * inputs.size(0)\n        counter += 1\n        tk0.set_postfix(loss=(running_loss / (counter * dataloader.batch_size)))\n    epoch_loss = running_loss / len(dataloader)\n    print('Training Loss: {:.4f}'.format(epoch_loss))\n    scheduler.step()\n\ntime_elapsed = time.time() - since\nprint('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\ntorch.save(model7.state_dict(), \"model7.bin\")","metadata":{"execution":{"iopub.status.busy":"2021-08-02T12:27:02.465176Z","iopub.execute_input":"2021-08-02T12:27:02.46551Z","iopub.status.idle":"2021-08-02T12:42:59.957672Z","shell.execute_reply.started":"2021-08-02T12:27:02.465477Z","shell.execute_reply":"2021-08-02T12:42:59.956681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"4. not augmented","metadata":{}},{"cell_type":"code","source":"albumentations_transform__ = albumentations.Compose([\n    albumentations.Resize(224, 224),\n    albumentations.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    albumentations.pytorch.transforms.ToTensorV2()\n])","metadata":{"execution":{"iopub.status.busy":"2021-08-02T12:42:59.959332Z","iopub.execute_input":"2021-08-02T12:42:59.959743Z","iopub.status.idle":"2021-08-02T12:42:59.965295Z","shell.execute_reply.started":"2021-08-02T12:42:59.959707Z","shell.execute_reply":"2021-08-02T12:42:59.964516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Dataset8():\n    def __init__(self, data, root, transform):\n        self.files = list(root + data['id_code'] + '.png')\n        self.targets = data['diagnosis']\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.files)\n    \n    def circle_crop(self,path):\n        img = cv2.imread(path)    \n        height, width, depth = img.shape       \n        x = int(width/2)\n        y = int(height/2)\n        r = np.amin((x,y))\n        circle_img = np.zeros((height, width), np.uint8)\n        cv2.circle(circle_img, (x,y), int(r), 1, thickness=-1)\n        img = cv2.bitwise_and(img, img, mask=circle_img)\n        img = crop_image(img)\n        return img\n\n    def __getitem__(self, idx):\n        img = self.circle_crop(self.files[idx])\n        x = self.transform(image=img)['image'] #dictionary \n        y = torch.tensor(self.targets[idx]).unsqueeze(0).float()\n        return x, y","metadata":{"execution":{"iopub.status.busy":"2021-08-02T12:42:59.966683Z","iopub.execute_input":"2021-08-02T12:42:59.967222Z","iopub.status.idle":"2021-08-02T12:42:59.978118Z","shell.execute_reply.started":"2021-08-02T12:42:59.967186Z","shell.execute_reply":"2021-08-02T12:42:59.977224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = Dataset8(train_data, train_path, albumentations_transform__)\ndataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T12:42:59.979452Z","iopub.execute_input":"2021-08-02T12:42:59.979861Z","iopub.status.idle":"2021-08-02T12:42:59.993Z","shell.execute_reply.started":"2021-08-02T12:42:59.979825Z","shell.execute_reply":"2021-08-02T12:42:59.992027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model8=models.resnet18(pretrained=True)\n\nnum_features=model8.fc.in_features\nmodel8.fc=nn.Linear(num_features,1)\n\nmodel8=model8.to(device)\n\ncriterion =nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(lr=1e-4, params=model8.parameters())\nscheduler = lr_scheduler.StepLR(optimizer, step_size=10)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T12:42:59.994424Z","iopub.execute_input":"2021-08-02T12:42:59.99492Z","iopub.status.idle":"2021-08-02T12:43:00.317812Z","shell.execute_reply.started":"2021-08-02T12:42:59.994882Z","shell.execute_reply":"2021-08-02T12:43:00.316842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"since = time.time()\ncriterion = torch.nn.MSELoss()\nnum_epochs = 15\nfor epoch in range(num_epochs):\n    print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n    print('-' * 10)\n    model8.train()\n    running_loss = 0.0\n    tk0 = tqdm(dataloader, total=int(len(dataloader)))\n    counter = 0\n    for bi, (d, t) in enumerate(tk0):\n        inputs = d\n        labels = t\n        inputs = inputs.to(device, dtype=torch.float)\n        labels = labels.to(device, dtype=torch.float)\n        optimizer.zero_grad()\n        with torch.set_grad_enabled(True):\n            outputs = model8(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n        running_loss += loss.item() * inputs.size(0)\n        counter += 1\n        tk0.set_postfix(loss=(running_loss / (counter * dataloader.batch_size)))\n    epoch_loss = running_loss / len(dataloader)\n    print('Training Loss: {:.4f}'.format(epoch_loss))\n    scheduler.step()\n\ntime_elapsed = time.time() - since\nprint('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\ntorch.save(model8.state_dict(), \"model8.bin\")","metadata":{"execution":{"iopub.status.busy":"2021-08-02T12:43:00.319151Z","iopub.execute_input":"2021-08-02T12:43:00.319532Z","iopub.status.idle":"2021-08-02T12:58:45.863152Z","shell.execute_reply.started":"2021-08-02T12:43:00.319495Z","shell.execute_reply":"2021-08-02T12:58:45.862173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# augmentation 3","metadata":{}},{"cell_type":"markdown","source":"# randaugment\n\n- batch를 추출할 때마다 여러 Augmentation 옵션들 중에서 random하게 추출해서 적용\n- 전체 transform 중에 몇 개씩 뽑을 지(N)와 Augmentation의 강도를 어느 정도로 줄지(M)이 hyper parameter","metadata":{}},{"cell_type":"code","source":"!pip install randaugment\nfrom randaugment import RandAugment","metadata":{"execution":{"iopub.status.busy":"2021-08-02T11:48:55.729342Z","iopub.execute_input":"2021-08-02T11:48:55.729747Z","iopub.status.idle":"2021-08-02T11:49:04.426158Z","shell.execute_reply.started":"2021-08-02T11:48:55.729716Z","shell.execute_reply":"2021-08-02T11:49:04.42487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntransform = torchvision.transforms.Compose([\n    transforms.Resize((224,224)),\n    transforms.RandomHorizontalFlip(), #0.5\n    RandAugment(),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])","metadata":{"execution":{"iopub.status.busy":"2021-08-02T11:49:04.431164Z","iopub.execute_input":"2021-08-02T11:49:04.433326Z","iopub.status.idle":"2021-08-02T11:49:04.452265Z","shell.execute_reply.started":"2021-08-02T11:49:04.433209Z","shell.execute_reply":"2021-08-02T11:49:04.451256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Dataset9():\n    def __init__(self, data, root, transform):\n        self.files = list(root + data['id_code'] + '.png')\n        self.targets = data['diagnosis']\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.files)\n    \n    def circle_crop(self,path):\n        img = cv2.imread(path)    \n        height, width, depth = img.shape       \n        x = int(width/2)\n        y = int(height/2)\n        r = np.amin((x,y))\n        circle_img = np.zeros((height, width), np.uint8)\n        cv2.circle(circle_img, (x,y), int(r), 1, thickness=-1)\n        img = cv2.bitwise_and(img, img, mask=circle_img)\n        img = crop_image(img)\n        return img\n\n    def __getitem__(self, idx):\n        img = self.circle_crop(self.files[idx])\n        img = Image.fromarray(img).convert('RGB')\n        x = self.transform(img)\n        y = torch.tensor(self.targets[idx]).unsqueeze(0).float()\n        return x, y","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-08-02T11:49:04.457562Z","iopub.execute_input":"2021-08-02T11:49:04.459785Z","iopub.status.idle":"2021-08-02T11:49:04.473756Z","shell.execute_reply.started":"2021-08-02T11:49:04.459717Z","shell.execute_reply":"2021-08-02T11:49:04.472662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = Dataset9(train_data, train_path, transform)\ndataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n\nmodel9=models.resnet18(pretrained=True)\n\nnum_features=model9.fc.in_features\nmodel9.fc=nn.Linear(num_features,1)\n\nmodel9=model9.to(device)\n\ncriterion =nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(lr=1e-4, params=model9.parameters())\nscheduler = lr_scheduler.StepLR(optimizer, step_size=10)","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-08-02T11:49:04.479209Z","iopub.execute_input":"2021-08-02T11:49:04.481712Z","iopub.status.idle":"2021-08-02T11:49:04.864265Z","shell.execute_reply.started":"2021-08-02T11:49:04.481674Z","shell.execute_reply":"2021-08-02T11:49:04.863369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"since = time.time()\ncriterion = torch.nn.MSELoss()\nnum_epochs = 15\nfor epoch in range(num_epochs):\n    print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n    print('-' * 10)\n    model9.train()\n    running_loss = 0.0\n    tk0 = tqdm(dataloader, total=int(len(dataloader)))\n    counter = 0\n    for bi, (d, t) in enumerate(tk0):\n        inputs = d\n        labels = t\n        inputs = inputs.to(device, dtype=torch.float)\n        labels = labels.to(device, dtype=torch.float)\n        optimizer.zero_grad()\n        with torch.set_grad_enabled(True):\n            outputs = model9(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n        running_loss += loss.item() * inputs.size(0)\n        counter += 1\n        tk0.set_postfix(loss=(running_loss / (counter * dataloader.batch_size)))\n    epoch_loss = running_loss / len(dataloader)\n    print('Training Loss: {:.4f}'.format(epoch_loss))\n    scheduler.step()\n\ntime_elapsed = time.time() - since\nprint('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\ntorch.save(model9.state_dict(), \"model9.bin\")","metadata":{"execution":{"iopub.status.busy":"2021-08-02T11:49:04.865612Z","iopub.execute_input":"2021-08-02T11:49:04.865965Z","iopub.status.idle":"2021-08-02T12:06:56.447722Z","shell.execute_reply.started":"2021-08-02T11:49:04.865927Z","shell.execute_reply":"2021-08-02T12:06:56.445895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# augmentation 4\n# uniform augmentation\n- search 없이 random하게 augmentation을 확률적으로 적용\n","metadata":{}},{"cell_type":"code","source":"!pip install git+https://github.com/tgilewicz/uniformaugment/\nfrom UniformAugment import UniformAugment\n\ntransform = transforms.Compose([\n    transforms.Resize((224,224)),\n    transforms.RandomHorizontalFlip(), \n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# Add UniformAugment with num_ops hyperparameter (num_ops=2 is optimal)\ntransform.transforms.insert(0, UniformAugment())","metadata":{"execution":{"iopub.status.busy":"2021-08-02T12:06:56.449293Z","iopub.execute_input":"2021-08-02T12:06:56.44969Z","iopub.status.idle":"2021-08-02T12:07:05.50985Z","shell.execute_reply.started":"2021-08-02T12:06:56.449653Z","shell.execute_reply":"2021-08-02T12:07:05.508855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Dataset10():\n    def __init__(self, data, root, transform):\n        self.files = list(root + data['id_code'] + '.png')\n        self.targets = data['diagnosis']\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.files)\n    \n    def circle_crop(self,path):\n        img = cv2.imread(path)    \n        height, width, depth = img.shape       \n        x = int(width/2)\n        y = int(height/2)\n        r = np.amin((x,y))\n        circle_img = np.zeros((height, width), np.uint8)\n        cv2.circle(circle_img, (x,y), int(r), 1, thickness=-1)\n        img = cv2.bitwise_and(img, img, mask=circle_img)\n        img = crop_image(img)\n        return img\n\n    def __getitem__(self, idx):\n        img = self.circle_crop(self.files[idx])\n        img = Image.fromarray(img).convert('RGB')\n        x = self.transform(img)\n        y = torch.tensor(self.targets[idx]).unsqueeze(0).float()\n        return x, y","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-08-02T12:07:05.511465Z","iopub.execute_input":"2021-08-02T12:07:05.511836Z","iopub.status.idle":"2021-08-02T12:07:05.524628Z","shell.execute_reply.started":"2021-08-02T12:07:05.511796Z","shell.execute_reply":"2021-08-02T12:07:05.523755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = Dataset10(train_data, train_path, transform)\ndataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n\nmodel10=models.resnet18(pretrained=True)\n\nnum_features=model10.fc.in_features\nmodel10.fc=nn.Linear(num_features,1)\n\nmodel10=model10.to(device)\n\ncriterion =nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(lr=1e-4, params=model10.parameters())\nscheduler = lr_scheduler.StepLR(optimizer, step_size=10)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T12:07:05.527983Z","iopub.execute_input":"2021-08-02T12:07:05.528754Z","iopub.status.idle":"2021-08-02T12:07:05.855136Z","shell.execute_reply.started":"2021-08-02T12:07:05.528721Z","shell.execute_reply":"2021-08-02T12:07:05.854228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"since = time.time()\ncriterion = torch.nn.MSELoss()\nnum_epochs = 15\nfor epoch in range(num_epochs):\n    print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n    print('-' * 10)\n    model10.train()\n    running_loss = 0.0\n    tk0 = tqdm(dataloader, total=int(len(dataloader)))\n    counter = 0\n    for bi, (d, t) in enumerate(tk0):\n        inputs = d\n        labels = t\n        inputs = inputs.to(device, dtype=torch.float)\n        labels = labels.to(device, dtype=torch.float)\n        optimizer.zero_grad()\n        with torch.set_grad_enabled(True):\n            outputs = model10(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n        running_loss += loss.item() * inputs.size(0)\n        counter += 1\n        tk0.set_postfix(loss=(running_loss / (counter * dataloader.batch_size)))\n    epoch_loss = running_loss / len(dataloader)\n    print('Training Loss: {:.4f}'.format(epoch_loss))\n    scheduler.step()\n\ntime_elapsed = time.time() - since\nprint('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\ntorch.save(model10.state_dict(), \"model10.bin\")","metadata":{"execution":{"iopub.status.busy":"2021-08-02T12:07:05.85683Z","iopub.execute_input":"2021-08-02T12:07:05.857236Z","iopub.status.idle":"2021-08-02T12:27:02.099348Z","shell.execute_reply.started":"2021-08-02T12:07:05.857192Z","shell.execute_reply":"2021-08-02T12:27:02.098237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}