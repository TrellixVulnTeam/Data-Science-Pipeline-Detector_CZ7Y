{"cells":[{"metadata":{},"cell_type":"markdown","source":"# APTOS - Inference #"},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=UserWarning)\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport time\nimport cv2\nimport gc\n\nfrom keras import backend as K\nfrom keras import losses\nfrom keras.applications.resnet50 import preprocess_input, ResNet50\n#from keras.applications.densenet import DenseNet121, preprocess_input\nfrom keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau, ProgbarLogger\nfrom keras.layers import Dense, GlobalAveragePooling2D, Dropout, Conv2D, BatchNormalization, Input, Flatten, LeakyReLU\nfrom keras.models import load_model, Model, Sequential\nfrom keras.optimizers import Adam, SGD, RMSprop\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.utils import to_categorical\n\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Constants ##"},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED = 575\nIMAGE_SIZE = 256\nNUM_CLASSES = 5\nBATCH_SIZE = 32\n\ntest_image_directory = '../input/aptos2019-blindness-detection/test_images/'\ntest_data_file = '../input/aptos2019-blindness-detection/test.csv'\n#weights_file = '../input/resnet50/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'\nbest_weight_file = '../input/weights6/aptos_best_weights(1).h5'\nsubmission_file = 'submission.csv'\nsample_file = '../input/aptos2019-blindness-detection/sample_submission.csv'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#os.listdir('../input')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def crop_image_from_gray(img,tol=7):\n    if img.ndim ==2:\n        mask = img>tol\n        return img[np.ix_(mask.any(1),mask.any(0))]\n    elif img.ndim==3:\n        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n        mask = gray_img>tol\n        \n        check_shape = img[:,:,0][np.ix_(mask.any(1),mask.any(0))].shape[0]\n        if (check_shape == 0): # image is too dark so that we crop out everything,\n            return img # return original image\n        else:\n            img1=img[:,:,0][np.ix_(mask.any(1),mask.any(0))]\n            img2=img[:,:,1][np.ix_(mask.any(1),mask.any(0))]\n            img3=img[:,:,2][np.ix_(mask.any(1),mask.any(0))]\n            img = np.stack([img1,img2,img3],axis=-1)\n        return img\n\ndef preprocess_image(img):\n    height, width = img.shape[0], img.shape[1]\n    ratio = height / width\n    #img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = crop_image_from_gray(img)\n    newheight, newwidth = int(IMAGE_SIZE * ratio), IMAGE_SIZE\n    #print('Resizing from ({},{}) to ({},{})'.format(width, height, newwidth, newheight))\n    img = cv2.addWeighted(img,4,cv2.GaussianBlur(img, (0,0), newheight/30) ,-4 ,128)\n    img = cv2.resize(img, (newwidth, newheight), interpolation=cv2.INTER_AREA)\n    img = preprocess_input(img)\n    return img","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model definition ##"},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model():\n    input_tensor = Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n    base_model = ResNet50(include_top=False, weights=None, input_tensor=input_tensor)\n    x = GlobalAveragePooling2D()(base_model.output)\n    x = Dropout(0.5)(x)\n    output_tensor = Dense(NUM_CLASSES, activation='softmax')(x)\n    model = Model(inputs=input_tensor, outputs=output_tensor)\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Run the training ##"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = build_model()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load best weights ##"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.load_weights(best_weight_file)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Run tests ##"},{"metadata":{},"cell_type":"markdown","source":"#### Test image generator ####"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_generator = ImageDataGenerator(preprocessing_function=preprocess_image)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Load test file ####"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = pd.read_csv(test_data_file)\ndf_sample = pd.read_csv(sample_file)\ndf_test['filename'] = df_test['id_code'].apply(lambda i : \"{}.png\".format(i))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_flow = test_generator.flow_from_dataframe(directory=test_image_directory, dataframe=df_test, x_col='filename', batch_size=BATCH_SIZE, max_queue_size=128, class_mode=None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Generate predictions ####"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict_generator(\n                    generator = test_flow,\n                    steps = (test_flow.n // test_flow.batch_size) + 1,\n                    verbose=1,\n                    max_queue_size = 75,\n                    workers=4\n                )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Display final predictions ####"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test['diagnosis'] = np.argmax(y_pred[0:len(df_test)], axis=-1).astype('uint8')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.groupby('diagnosis').count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Write submission file ####"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sample['id_code'] = df_test['id_code']\ndf_sample['diagnosis'] = df_test['diagnosis']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sample.to_csv(submission_file, index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sample.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(submission_file)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}