{"cells":[{"metadata":{},"cell_type":"markdown","source":"#### Hi Guys, this is a starter code for anyone looking to build an image pipeline in Tensorflow. \n#### We are going to use tf.data API to build an image based dataset that reads the training data file\n- In batches \n- Applies Image preprocessing - Like resizing & normalization \n\n#### Please give a upvote if you like the kernel !! Cheers"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\n#tf.enable_eager_execution()\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.getcwd()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport pathlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nfrom IPython.core.display import Image\nfrom IPython.display import display\n#print(tf.__version__)\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reading datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"##### Read data\ntrain_df = pd.read_csv('../input/train.csv')\ntest_df = pd.read_csv('../input/test.csv')\nsample_sub_df = pd.read_csv('../input/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_images_path =   \"../input/train_images\"\ntest_images_path =  \"../input/test_images\"\nprint(train_images_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['image_path'] = \"../input/train_images/\" + train_df['id_code'] + \".png\"\ntest_df['image_path'] = \"../input/test_images/\" + test_df['id_code'] + \".png\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[train_df.id_code == '5d024177e214']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classes_dist = pd.DataFrame(train_df['diagnosis'].value_counts()/train_df.shape[0]).reset_index()\n\n# barplot \nax = sns.barplot(x=\"index\", y=\"diagnosis\", data=classes_dist)\n\n# Imbalanced dataset with 49% - no DR, 8% proliferative - i.e most severe DR\n# Model Building - Need to do oversampling for minority classes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"root_path = pathlib.Path(train_images_path) # Returns POSIX path\n# for item in root_path.iterdir():\n#     print(item)\n#     break","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Display few images "},{"metadata":{"trusted":true},"cell_type":"code","source":"#!pip install ipython\n#!conda install -c anaconda ipython","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_paths = list(root_path.glob(\"*.png\"))\nall_paths[0:3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#all_paths = list(root_path.glob(\"*.png\"))\n## Display few images of each class - \nall_paths = [str(path) for path in all_paths]\nrandom.shuffle(all_paths)\n\n\n# Image('../input/train_images/5d024177e214.png',width=300,height=300)\nfor n in range(3):\n    image_path = random.choice(all_paths)\n    print(image_path)\n    display(Image(image_path,width=300,height=300))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Image preprocessing - Functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"# To decode datatype\ndef preprocess_image(image,labels):\n    image = tf.image.decode_png(image, channels=3)\n    image = tf.image.resize(image, [28,28])         # IMAGE RESIZING\n    image = tf.cast(image, tf.float32)                 \n    image /= 255.0  # normalize to [0,1] range         # IMAGE NORMALIZE\n    \n    \n    return(image,labels)\n\n# Read image, and process\ndef load_and_preprocess_image(path,labels):\n    image = tf.read_file(path)\n    return(preprocess_image(image,labels))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build Image Pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = tf.convert_to_tensor(np.array(train_df['diagnosis']), dtype=tf.int32)\nfilenames  = tf.convert_to_tensor(train_df['image_path'].tolist(), dtype=tf.string)\nfilenames[0:5]\nlabels[0:4]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filenames.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.one_hot(labels[0], 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_images(filenames,labels,batch_size):\n    \n    \n    dataset= tf.data.Dataset.from_tensor_slices((filenames,labels))\n    \n    \n    #labels =  tf.data.Dataset.from_tensor_slices(labels).map(lambda z: tf.one_hot(z, 5))\n    \n    #labels =  tf.data.Dataset.from_tensor_slices(labels)\n    #dataset = dataset.shuffle(len(labels))\n\n    # Image preprocessing\n    #dataset = dataset.map(load_and_preprocess_image,num_parallel_calls=4)\n    dataset = dataset.map(load_and_preprocess_image) \n    dataset.make_initializable_iterator()\n    \n    \n    #print(dataset)\n    # Get one batch\n    dataset = dataset.batch(batch_size,drop_remainder=False)\n    dataset = dataset.prefetch(1)\n    #dataset = dataset.shape\n    \n#     X,Y = tf.train.batch([dataset, labels], batch_size=batch_size,\n#                           capacity=batch_size * 8,\n#                           num_threads=4\n    #print(labels)\n    return(dataset,labels)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ## Create train & test datasets\n# filenames  = tf.convert_to_tensor(train['image_path'].tolist(), dtype=tf.string)\n# dx_train = tf.data.Dataset.from_tensor_slices(filenames)\n\n# labels = tf.convert_to_tensor(np.array(train['diagnosis']), dtype=tf.int32)\n# dy_train = tf.data.Dataset.from_tensor_slices(labels).map(lambda z: tf.one_hot(z, 5))\n\n# train_dataset = tf.data.Dataset.zip((dx_train, dy_train)).shuffle(500).repeat().batch(30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# filenames  = tf.convert_to_tensor(valid['image_path'].tolist(), dtype=tf.string)\n# dx_valid = tf.data.Dataset.from_tensor_slices(filenames)\n\n# labels = tf.convert_to_tensor(np.array(valid['diagnosis']), dtype=tf.int32)\n# dy_valid = tf.data.Dataset.from_tensor_slices(labels).map(lambda z: tf.one_hot(z, 5))\n\n# valid_dataset = tf.data.Dataset.zip((dx_test, dy_test)).shuffle(500).repeat().batch(30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n#                                                train_dataset.output_shapes)\n# next_element = iterator.get_next()\n\n# training_init_op = iterator.make_initializer(train_dataset)\n# validation_init_op = iterator.make_initializer(valid_dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cnn_model(in_data):\n    input_layer = tf.reshape(in_data, [-1, 28, 28, 3])\n    \n    input_layer = in_data\n    conv1 = tf.layers.conv2d(\n      inputs=input_layer,\n      filters=32,\n      kernel_size=[5, 5],\n      padding=\"same\",\n      activation=tf.nn.relu)\n\n     # Pooling Layer #1\n    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n\n    # Convolutional Layer #2 and Pooling Layer #2\n    conv2 = tf.layers.conv2d(\n      inputs=pool1,\n      filters=64,\n      kernel_size=[5, 5],\n      padding=\"same\",\n      activation=tf.nn.relu)\n\n    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n\n      # Dense Layer\n    pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n    #dense = tf.layers.dense(inputs=pool2_flat, units=128, activation=tf.nn.relu)\n#     dropout = tf.layers.dropout(\n#           inputs=dense, rate=0.4, training == tf.estimator.ModeKeys.TRAIN)\n\n      # Logits Layer\n    logits = tf.layers.dense(inputs=dense, units=5)\n\n    predictions = {\n      # Generate predictions (for PREDICT and EVAL mode)\n      \"classes\": tf.argmax(input=logits, axis=1),\n      # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n      # `logging_hook`.\n      \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n  }\n\n    return predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset,labels = read_images(filenames,labels,128)\niter = dataset.make_one_shot_iterator()\nx, y = iter.get_next()\n\npredictions = cnn_model(x)\npredictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions[\"classes\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions[\"probabilities\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#tf.disable_eager_execution()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS = 10\nBATCH_SIZE = 128\n# using two numpy arrays\n# features, labels = (np.array([np.random.sample((100,2))]), \n#                     np.array([np.random.sample((100,1))]))\n# dataset = tf.data.Dataset.from_tensor_slices((filenames,labels)).repeat().batch(BATCH_SIZE)\n# iter = dataset.make_one_shot_iterator()\n# x, y = iter.get_next()\n\ndataset,labels = read_images(filenames,labels,128)\niter = dataset.make_one_shot_iterator()\nx, y = iter.get_next()\n\n#tf.disable_eager_execution()\n# make a simple model\n# net = tf.layers.dense(x, 8, activation=tf.tanh) # pass the first value from iter.get_next() as input\n# net = tf.layers.dense(net, 8, activation=tf.tanh)\npredictions = cnn_model(x)\nprediction = predictions[\"classes\"]\nloss = tf.losses.mean_squared_error(prediction, y) # pass the second value from iter.get_net() as label\ntrain_op = tf.train.AdamOptimizer().minimize(loss)\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    for i in range(EPOCHS):\n        _, loss_value = sess.run([train_op, loss])\n        print(\"Iter: {}, Loss: {:.4f}\".format(i, loss_value))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Testing\n\n# with tf.Session() as sess:\n#     data,labels = read_images(filenames,labels,128)\n    \n#     iterator = data.make_initializable_iterator()\n#     image_batch,labels= iterator.get_next()\n    \n#     #image_batch,labels = next(iter(data))\n#     out = tf.layers.conv2d(image_batch,filters=32,kernel_size=(3,3))\n#     out = tf.nn.relu(out)\n#     out = tf.layers.max_pooling2d(out, 2, 2)\n#     out = tf.reshape(out, [-1, 32 * 32 * 16])\n#     # Now, logits is [batch_size, 6]\n#     logits = tf.layers.dense(out, 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predictions = tf.argmax(logits, 1)\n# predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n# optimizer = tf.train.AdamOptimizer(0.01)\n\n# # Create the training operation\n# train_op = optimizer.minimize(loss)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"# N_CLASSES = train_df.diagnosis.nunique()\n# N_CLASSES","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#tf.reset_default_graph() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build the data input\n#X, Y = read_images(filenames, labels, batch_size)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}