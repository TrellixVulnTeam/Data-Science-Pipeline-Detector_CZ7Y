{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"# README\n* Pre-trained EfficientNet-B6\n* Ben's Preprocessing + CenterCrop\n* 10 times TTA"},{"metadata":{},"cell_type":"markdown","source":"# 1. Setup"},{"metadata":{"trusted":true},"cell_type":"code","source":"import cv2\nimport matplotlib.pyplot as plt\nfrom os.path import isfile\nimport torch.nn.init as init\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport pandas as pd \nimport os\nfrom PIL import Image, ImageFilter\nprint(os.listdir(\"../input\"))\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\nimport torch.optim as optim\nimport time\nfrom tqdm import tqdm\nfrom torch.autograd import Variable\nimport torch.functional as F\nfrom tqdm import tqdm\nfrom sklearn import metrics\nimport urllib\nimport pickle\nimport cv2\nimport torch.nn.functional as F\nfrom torchvision import models\nimport seaborn as sns\nimport random\nimport sys\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Loading EfficientNet-B6"},{"metadata":{"trusted":true},"cell_type":"code","source":"package_path = '../input/efficientnet/efficientnet-pytorch/EfficientNet-PyTorch/'\nsys.path.append(package_path)\n\nfrom efficientnet_pytorch import EfficientNet\nmodel = EfficientNet.from_name('efficientnet-b6')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Seed setting"},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Option setting"},{"metadata":{"trusted":true},"cell_type":"code","source":"seed_everything(1234)\nnum_classes = 1\nIMG_SIZE = 256\nlr = 1e-3\nbatch_size = 32\nnum_TTA = 10","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# DataFrame for Train / Test set\ndf = pd.read_csv('../input/aptos2019-blindness-detection/train.csv')\ndf_test = pd.read_csv('../input/aptos2019-blindness-detection/test.csv')\ndf_sample = pd.read_csv('../input/aptos2019-blindness-detection/sample_submission.csv')\n\ny_train = df['diagnosis']\n\n# Image data\ntrain = '../input/aptos2019-blindness-detection/train_images/'\ntest = '../input/aptos2019-blindness-detection/test_images/'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Image Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"def expand_path(p):\n    p = str(p)\n    if isfile(train + p + \".png\"):\n        return train + (p + \".png\")\n    if isfile(test + p + \".png\"):\n        return test + (p + \".png\")\n    return p","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def crop_image_from_gray(img,tol=7):\n    if img.ndim ==2:\n        mask = img>tol\n        return img[np.ix_(mask.any(1),mask.any(0))]\n    elif img.ndim==3:\n        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n        mask = gray_img>tol\n        \n        check_shape = img[:,:,0][np.ix_(mask.any(1),mask.any(0))].shape[0]\n        if (check_shape == 0): # image is too dark so that we crop out everything,\n            return img # return original image\n        else:\n            img1=img[:,:,0][np.ix_(mask.any(1),mask.any(0))]\n            img2=img[:,:,1][np.ix_(mask.any(1),mask.any(0))]\n            img3=img[:,:,2][np.ix_(mask.any(1),mask.any(0))]\n    #         print(img1.shape,img2.shape,img3.shape)\n            img = np.stack([img1,img2,img3],axis=-1)\n    #         print(img.shape)\n        return img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from albumentations import *\nimport time\n\n# IMG_SIZE = (256, 256)\n\ndef albaugment(aug0, img):\n    return aug0(image=img)['image']\n\n# idx=8\n# image1 = x_test[idx]\n\n# # 1. Rotate or Flip\n# aug1 = OneOf([Rotate(p=0.99, limit=160, border_mode=0, value=0), Flip(p=0.5)], p=1)\n\n# # 2. Adjust Brightness or Contrast\n# aug2 = RandomBrightnessContrast(brightness_limit=0.45, contrast_limit=0.45, p=1)\n# h_min = np.round(IMG_SIZE*0.72).astype(int)\n# h_max = np.round(IMG_SIZE*0.9).astype(int)\n# # print(h_min, h_max)\n\n# # 3. Random Crop and then Resize\n# aug3 = RandomSizedCrop((h_min, h_max), IMG_SIZE, IMG_SIZE, w2h_ratio=IMG_SIZE/IMG_SIZE, p=1)\n\n# # 4. CutOut Augumentation\n# max_hole_size = int(IMG_SIZE/10)\n# aug4 = Cutout(p=1, max_h_size=max_hole_size, max_w_size=max_hole_size, num_holes=8)\n\n# # 5. SunFlare Augmentation\n# aug5 = RandomSunFlare(src_radius=max_hole_size, \n#                       num_flare_circles_lower=10,\n#                       num_flare_circles_upper=20, \n#                       p=1)\n\n# # 6. Ultimate Augmentation\n# final_aug = Compose([aug1, aug2, aug3, aug4, aug5], p=1)\n\n# 7. Center to zoom\naug6 = CenterCrop(height=180, width=180, p=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Dataset & DataLoader setting"},{"metadata":{"trusted":true},"cell_type":"code","source":"class MyDataset(Dataset):\n    \n    def __init__(self, dataframe, transform=None):\n        self.df = dataframe\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        label = self.df.diagnosis.values[idx]\n        label = np.expand_dims(label, -1)\n        p = self.df.id_code.values[idx]\n        p_path = expand_path(p)\n        # Ben's Preprocess\n        image = cv2.imread(p_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        image = crop_image_from_gray(image)\n        image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n        image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0,0), sigmaX=30), -4, 128)\n        # CenterCrop\n        image = albaugment(aug6, image)\n        image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n        image = transforms.ToPILImage()(image)\n        \n        if self.transform:\n            image = self.transform(image)\n            \n        return image, label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_transform = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation((-120, 120)),\n    transforms.ToTensor(),\n#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\ntestset = MyDataset(df_sample, transform=test_transform)\n\ntest_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Define Network"},{"metadata":{"trusted":true},"cell_type":"code","source":"in_features = model._fc.in_features\nmodel._fc = nn.Linear(in_features=in_features, out_features=num_classes)\nmodel.load_state_dict(torch.load('../input/efficientnetb6-weight-0831/efficientnet-b6_weight_best_0831.pt'))\nmodel.cuda()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Inference with TTA"},{"metadata":{"trusted":true},"cell_type":"code","source":"for param in model.parameters():\n    param.requires_grad = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ntest_pred = np.zeros((len(df_sample), 1))\n\nmodel.eval()\n\nfor i in range(num_TTA):\n    with torch.no_grad():\n        for i, data in enumerate(tqdm(test_loader)):\n            images, _ = data\n            images = images.cuda()\n            pred = model(images)\n            test_pred[i * batch_size:(i + 1) * batch_size] += pred.cpu().squeeze().numpy().reshape(-1, 1)\n\noutput = test_pred / num_TTA","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coef = [0.5, 1.5, 2.5, 3.5]\n\nfor i, pred in enumerate(output):\n    if pred < coef[0]:\n        output[i] = 0\n    elif pred >= coef[0] and pred < coef[1]:\n        output[i] = 1\n    elif pred >= coef[1] and pred < coef[2]:\n        output[i] = 2\n    elif pred >= coef[2] and pred < coef[3]:\n        output[i] = 3\n    else:\n        output[i] = 4","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6. Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({'id_code':df_sample.id_code.values,\n                           'diagnosis':np.squeeze(output).astype(int)})\nprint(submission.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)\nprint(os.listdir('./'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission['diagnosis'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}