{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport keras\nfrom keras.models import Sequential\nfrom keras import optimizers\nfrom keras.layers import Dense, Dropout,Activation, Flatten, Conv2D, MaxPooling2D, AveragePooling2D, GlobalMaxPooling2D, ZeroPadding2D, Input\nfrom keras.models import Model\nfrom keras.preprocessing import image\nfrom sklearn.model_selection import train_test_split\nimport cv2\nimport glob\nimport os\nimport PIL\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom keras.preprocessing import image\nfrom keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\nfrom keras.utils import to_categorical\nfrom keras.models import Sequential\nPATH ='https://www.kaggle.com/c/aptos2019-blindness-detection/data'\nfrom sklearn.linear_model import LinearRegression\nfrom  sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import Lasso, Ridge, ElasticNet","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/aptos2019-blindness-detection/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/aptos2019-blindness-detection/test.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_image_path = os.path.join(PATH, \"train.zip\")\ntest_image_path = os.path.join(PATH, \"test.zip\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_image = (\"/kaggle/input/aptos2019-blindness-detection/train_images\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_image","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_image =('/kaggle/input/aptos2019-blindness-detection/test_images')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_image","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_image)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(test_image)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pyunpack\n!pip install patool","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" #when padding = 'valid'\nmodel = Sequential()\nmodel.add(Conv2D(input_shape = (6, 6, 3), filters = 10, kernel_size = (3,3), strides = (1,1), padding = 'valid'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(model.output_shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# FIlter/kernels","metadata":{}},{"cell_type":"code","source":"# when padding = 'same'\nmodel = Sequential()\nmodel.add(Conv2D(input_shape = (10, 10, 3), filters = 10, kernel_size = (3,3), strides = (1,1), padding = 'same'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(model.output_shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# when filter size = 20\nmodel = Sequential()\nmodel.add(Conv2D(input_shape = (10, 10, 3), filters = 20, kernel_size = (3,3), strides = (1,1), padding = 'same'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#you could see that the depth of output = 20\nprint(model.output_shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pooling Usually, max pooling is applied for rectangular region\n\npooling size, padding type, and strides can be set similar to convolutional layer","metadata":{}},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Conv2D(input_shape = (10, 10, 3), filters = 10, kernel_size = (3,3), strides = (1,1), padding = 'same'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(model.output_shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# when 'strides' parameter is not defined, strides are equal to 'pool_size'\nmodel.add(MaxPooling2D(pool_size = (2,2), padding = 'valid'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(model.output_shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Conv2D(input_shape = (10, 10, 3), filters = 10, kernel_size = (3,3), strides = (1,1), padding = 'same'))\nmodel.add(MaxPooling2D(pool_size = (2,2), strides = (1,1), padding = 'valid'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(model.output_shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Conv2D(input_shape = (10, 10, 3), filters = 10, kernel_size = (3,3), strides = (1,1), padding = 'same'))\nmodel.add(AveragePooling2D(pool_size = (2,2), padding = 'valid'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(model.output_shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# globalmaxpooling performs maxpooling over whole channel with depth = 1\nmodel = Sequential()\nmodel.add(Conv2D(input_shape = (10, 10, 3), filters = 10, kernel_size = (3,3), strides = (1,1), padding = 'same'))\nmodel.add(GlobalMaxPooling2D())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# as the number of filters = 10, 10 values are returned as result of globalmaxpooling2D\nprint(model.output_shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Flattening\n\nTo be connected to fully connected layer (dense layer), convolutional/pooling\n\nlayer should be \"flattened\"\n\nResulting shape = (Number of instances, width X height X depth)","metadata":{}},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Conv2D(input_shape = (10, 10, 3), filters = 10, kernel_size = (3,3), strides = (1,1), padding = 'same'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(model.output_shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.add(Flatten())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(model.output_shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Fully Connected (Dense)\n\nAfter flattening layer, fully connected layer can be added\n\noutput shape (number of nodes) should be designated","metadata":{}},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Conv2D(input_shape = (10, 10, 3), filters = 10, kernel_size = (3,3), strides = (1,1), padding = 'same'))\nmodel.add(Flatten())\nmodel.add(Dense(50))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(model.output_shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = '/kaggle/input/aptos2019-blindness-detection/train_images'\ntrain_images=[]\ntrain_files=[]\nfor filename in glob.glob(path + '*.png'): #assuming gif \n    # the next line opens the image and convert it to grey scale\n    im=Image.open(filename).convert('L')\n    # the next line, Image.getdata returns the pixel values as sequence of flatten values\n    data = im.getdata()\n    nparray = np.array(data,'uint8')\n    # print(nparray)\n    train_images.append([nparray,os.path.basename(filename)])\n#    train_files.append(os.path.basename(filename))\n    im.close","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = '/kaggle/input/aptos2019-blindness-detection/test_images'\ntest_images=[]\ntest_files=[]\nfor filename in glob.glob(path + '*.png'): #assuming gif \n    # the next line opens the image and convert it to grey scale\n    im=Image.open(filename).convert('L')\n    # the next line, Image.getdata returns the pixel values as sequence of flatten values\n    data = im.getdata()\n    nparray = np.array(data,'uint8')\n    # print(nparray)\n    test_images.append([nparray,os.path.basename(filename)])\n#    train_files.append(os.path.basename(filename))\n    im.close","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = '/kaggle/input/aptos2019-blindness-detection/train_images'\ntrain_images=[]\ntrain_files=[]\nfor filename in glob.glob(path + '*.png'):\n    nparray = image.img_to_array(image.load_img(filename))\n    train_images.append(nparray)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = '/kaggle/input/aptos2019-blindness-detection/test_images'\ntest_images=[]\ntest_files=[]\nfor filename in glob.glob(path + '*.png'):\n    nparray = image.img_to_array(image.load_img(filename))\n    test_images.append(nparray)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_image)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(test_image)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\nimport math\nimport os\n\nimport cv2\nfrom PIL import Image\nimport numpy as np\nfrom keras import layers\nfrom keras.applications import DenseNet121\nfrom keras.callbacks import Callback, ModelCheckpoint\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import cohen_kappa_score, accuracy_score\nimport scipy\nfrom tqdm import tqdm\n\n%matplotlib inline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('../input/aptos2019-blindness-detection/train.csv')\ntest_df = pd.read_csv('../input/aptos2019-blindness-detection/test.csv')\nprint(train_df.shape)\nprint(test_df.shape)\ntest_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_pad_width(im, new_shape, is_rgb=True):\n    pad_diff = new_shape - im.shape[0], new_shape - im.shape[1]\n    t, b = math.floor(pad_diff[0]/2), math.ceil(pad_diff[0]/2)\n    l, r = math.floor(pad_diff[1]/2), math.ceil(pad_diff[1]/2)\n    if is_rgb:\n        pad_width = ((t,b), (l,r), (0, 0))\n    else:\n        pad_width = ((t,b), (l,r))\n    return pad_width\n\ndef preprocess_image(image_path, desired_size=224):\n    im = Image.open(image_path)\n    im = im.resize((desired_size, )*2, resample=Image.LANCZOS)\n#     im = im.resize((desired_size, )*2)\n    \n    return im","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N = test_df.shape[0]\nx_test = np.empty((N, 224, 224, 3), dtype=np.uint8)\n\nfor i, image_id in enumerate(tqdm(test_df['id_code'])):\n    x_test[i, :, :, :] = preprocess_image(\n        f'../input/aptos2019-blindness-detection/test_images/{image_id}.png'\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.summary()\ndef load_image_ben_orig(path,resize=True,crop=False,norm255=True,keras=False):\n    image = cv2.imread(path)\n    \n#     if crop:\n#         image = crop_image(image)\n    \n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n#     if resize:\n#         image = cv2.resize(image,(SIZE,SIZE))\n        \n    image=cv2.addWeighted( image,4, cv2.GaussianBlur( image , (0,0) ,  10) ,-4 ,128)\n#     image=cv2.addWeighted( image,4, cv2.medianBlur( image , 10) ,-4 ,128)\n    \n    # NOTE plt.imshow can accept both int (0-255) or float (0-1), but deep net requires (0-1)\n    if norm255:\n        return image/255\n    elif keras:\n        #see https://github.com/keras-team/keras-applications/blob/master/keras_applications/imagenet_utils.py for mode\n        #see https://github.com/keras-team/keras-applications/blob/master/keras_applications/xception.py for inception,xception mode\n        #the use of tf based preprocessing (- and / by 127 respectively) will results in [-1,1] so it will not visualize correctly (directly)\n        image = np.expand_dims(image, axis=0)\n        return preprocess_input(image)[0]\n    else:\n        return image.astype(np.int16)\n    \n    return image\n\ndef transform_image_ben(img,resize=True,crop=False,norm255=True,keras=False):  \n    image=cv2.addWeighted( img,4, cv2.GaussianBlur( img , (0,0) ,  10) ,-4 ,128)\n    \n    # NOTE plt.imshow can accept both int (0-255) or float (0-1), but deep net requires (0-1)\n    if norm255:\n        return image/255\n    elif keras:\n        image = np.expand_dims(image, axis=0)\n        return preprocess_input(image)[0]\n    else:\n        return image.astype(np.int16)\n    \n    return image","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display_samples(df, columns=5, rows=2, Ben=True):\n    fig=plt.figure(figsize=(5*columns, 4*rows))\n\n    for i in range(columns*rows):\n        image_path = df.loc[i,'id_code']\n#         image_id = df.loc[i,'diagnosis']\n        path = f'../input/aptos2019-blindness-detection/test_images/{image_path}.png'\n        if Ben:\n            img = load_image_ben_orig(path)\n        else:\n            img = cv2.imread(path)\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        fig.add_subplot(rows, columns, i+1)\n#         plt.title(image_id)\n        plt.imshow(img)\n    \n    plt.tight_layout()\n\ndisplay_samples(test_df, Ben=False)\ndisplay_samples(test_df, Ben=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras import layers\nfrom keras.models import Model\nimport keras.backend as K","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"K.clear_session()\ndensenet = DenseNet121(\n    weights=None,\n    include_top=False,\n    input_shape=(None,None,3)\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"GAP_layer = layers.GlobalAveragePooling2D()\ndrop_layer = layers.Dropout(0.5)\ndense_layer = layers.Dense(5, activation='sigmoid', name='final_output')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_model_sequential():\n    model = Sequential()\n    model.add(densenet)\n    model.add(GAP_layer)\n    model.add(drop_layer)\n    model.add(dense_layer)\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"modelA = build_model_sequential()\nmodelA.load_weights('../input/aptos-data/dense_xhlulu_731.h5')\n\nmodelA.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_model_functional():\n    base_model = densenet\n    \n    x = GAP_layer(base_model.layers[-1].output)\n    x = drop_layer(x)\n    final_output = dense_layer(x)\n    model = Model(base_model.layers[0].input, final_output)\n    \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = build_model_functional() # with pretrained weights, and layers we want\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# y_test = model.predict(x_test) > 0.5\n# y_test = y_test.astype(int).sum(axis=1) - 1\n\n# test_df['diagnosis'] = y_test\n# test_df.to_csv('submission.csv',index=False)\n# y_test.shape, x_test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import seaborn as sns\n# import cv2\n\n# SIZE=224\n# def create_pred_hist(pred_level_y,title='NoTitle'):\n#     results = pd.DataFrame({'diagnosis':pred_level_y})\n\n#     f, ax = plt.subplots(figsize=(7, 4))\n#     ax = sns.countplot(x=\"diagnosis\", data=results, palette=\"GnBu_d\")\n#     sns.despine()\n#     plt.title(title)\n#     plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def gen_heatmap_img(img, model0, layer_name='last_conv_layer',viz_img=None,orig_img=None):\n    preds_raw = model0.predict(img[np.newaxis])\n    preds = preds_raw > 0.5 # use the same threshold as @xhlulu original kernel\n    class_idx = (preds.astype(int).sum(axis=1) - 1)[0]\n#     print(class_idx, class_idx.shape)\n    class_output_tensor = model0.output[:, class_idx]\n    \n    viz_layer = model0.get_layer(layer_name)\n    grads = K.gradients(\n                        class_output_tensor ,\n                        viz_layer.output\n                        )[0] # gradients of viz_layer wrt output_tensor of predicted class\n    \n    pooled_grads=K.mean(grads,axis=(0,1,2))\n    iterate=K.function([model0.input],[pooled_grads, viz_layer.output[0]])\n    \n    pooled_grad_value, viz_layer_out_value = iterate([img[np.newaxis]])\n    \n    for i in range(pooled_grad_value.shape[0]):\n        viz_layer_out_value[:,:,i] *= pooled_grad_value[i]\n    \n    heatmap = np.mean(viz_layer_out_value, axis=-1)\n    heatmap = np.maximum(heatmap,0)\n    heatmap /= np.max(heatmap)\n\n    viz_img=cv2.resize(viz_img,(img.shape[1],img.shape[0]))\n    heatmap=cv2.resize(heatmap,(viz_img.shape[1],viz_img.shape[0]))\n    \n    heatmap_color = cv2.applyColorMap(np.uint8(heatmap*255), cv2.COLORMAP_SPRING)/255\n    heated_img = heatmap_color*0.5 + viz_img*0.5\n    \n    print('raw output from model : ')\n    print_pred(preds_raw)\n    \n    if orig_img is None:\n        show_Nimages([img,viz_img,heatmap_color,heated_img])\n    else:\n        show_Nimages([orig_img,img,viz_img,heatmap_color,heated_img])\n    \n    plt.show()\n    return heated_img","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_image(image,figsize=None,title=None):\n    \n    if figsize is not None:\n        fig = plt.figure(figsize=figsize)\n#     else: # crash!!\n#         fig = plt.figure()\n        \n    if image.ndim == 2:\n        plt.imshow(image,cmap='gray')\n    else:\n        plt.imshow(image)\n        \n    if title is not None:\n        plt.title(title)\n\ndef show_Nimages(imgs,scale=1):\n\n    N=len(imgs)\n    fig = plt.figure(figsize=(25/scale, 16/scale))\n    for i, img in enumerate(imgs):\n        ax = fig.add_subplot(1, N, i + 1, xticks=[], yticks=[])\n        show_image(img)\n        \ndef print_pred(array_of_classes):\n    xx = array_of_classes\n    s1,s2 = xx.shape\n    for i in range(s1):\n        for j in range(s2):\n            print('%.3f ' % xx[i,j],end='')\n        print('')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NUM_SAMP=10\nSEED=77\nlayer_name = 'relu' #'conv5_block16_concat'\nfor i, (idx, row) in enumerate(test_df[:NUM_SAMP].iterrows()):\n    path=f\"../input/aptos2019-blindness-detection/test_images/{row['id_code']}.png\"\n    ben_img = load_image_ben_orig(path)\n    input_img = np.empty((1,224, 224, 3), dtype=np.uint8)\n    input_img[0,:,:,:] = preprocess_image(path)\n        \n    print('test pic no.%d' % (i+1))\n    _ = gen_heatmap_img(input_img[0],\n                        model, layer_name=layer_name,viz_img=ben_img)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from albumentations import *\nimport time\n\nIMG_SIZE = (224,224)\n\n'''Use case from https://www.kaggle.com/alexanderliao/image-augmentation-demo-with-albumentation/'''\ndef albaugment(aug0, img):\n    return aug0(image=img)['image']\nidx=8\nimage1=x_test[idx]\n\n'''1. Rotate or Flip'''\naug1 = OneOf([\n    Rotate(p=0.99, limit=160, border_mode=0,value=0), # value=black\n    Flip(p=0.5)\n    ],p=1)\n\n'''2. Adjust Brightness or Contrast'''\naug2 = RandomBrightnessContrast(brightness_limit=0.45, contrast_limit=0.45,p=1)\nh_min=np.round(IMG_SIZE[1]*0.72).astype(int)\nh_max= np.round(IMG_SIZE[1]*0.9).astype(int)\nprint(h_min,h_max)\n\n'''3. Random Crop and then Resize'''\n#w2h_ratio = aspect ratio of cropping\naug3 = RandomSizedCrop((h_min, h_max),IMG_SIZE[1],IMG_SIZE[0], w2h_ratio=IMG_SIZE[0]/IMG_SIZE[1],p=1)\n\n'''4. CutOut Augmentation'''\nmax_hole_size = int(IMG_SIZE[1]/10)\naug4 = Cutout(p=1,max_h_size=max_hole_size,max_w_size=max_hole_size,num_holes=8 )#default num_holes=8\n\n'''5. SunFlare Augmentation'''\naug5 = RandomSunFlare(src_radius=max_hole_size,\n                      num_flare_circles_lower=10,\n                      num_flare_circles_upper=20,\n                      p=1)#default flare_roi=(0,0,1,0.5),\n\n'''6. Ultimate Augmentation -- combine everything'''\nfinal_aug = Compose([\n    aug1,aug2,aug3,aug4,aug5\n],p=1)\n\n\nimg1 = albaugment(aug1,image1)\nimg2 = albaugment(aug1,image1)\nprint('Rotate or Flip')\nshow_Nimages([image1,img1,img2],scale=2)\n# time.sleep(1)\n\nimg1 = albaugment(aug2,image1)\nimg2 = albaugment(aug2,image1)\nimg3 = albaugment(aug2,image1)\nprint('Brightness or Contrast')\nshow_Nimages([img3,img1,img2],scale=2)\n# time.sleep(1)\n\nimg1 = albaugment(aug3,image1)\nimg2 = albaugment(aug3,image1)\nimg3 = albaugment(aug3,image1)\nprint('Rotate and Resize')\nshow_Nimages([img3,img1,img2],scale=2)\nprint(img1.shape,img2.shape)\n# time.sleep(1)\n\nimg1 = albaugment(aug4,image1)\nimg2 = albaugment(aug4,image1)\nimg3 = albaugment(aug4,image1)\nprint('CutOut')\nshow_Nimages([img3,img1,img2],scale=2)\n# time.sleep(1)\n\nimg1 = albaugment(aug5,image1)\nimg2 = albaugment(aug5,image1)\nimg3 = albaugment(aug5,image1)\nprint('Sun Flare')\nshow_Nimages([img3,img1,img2],scale=2)\n# time.sleep(1)\n\nimg1 = albaugment(final_aug,image1)\nimg2 = albaugment(final_aug,image1)\nimg3 = albaugment(final_aug,image1)\nprint('All above combined')\nshow_Nimages([img3,img1,img2],scale=2)\nprint(img1.shape,img2.shape)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"aug_list = [aug5, aug2, aug3, aug4, aug1, final_aug]\naug_name = ['SunFlare', 'brightness or contrast', 'crop and resized', 'CutOut', 'rotate or flip', 'Everything Combined']\n\nidx=8\nlayer_name = 'relu' #'conv5_block16_concat'\nfor i in range(len(aug_list)):\n    path=f\"../input/aptos2019-blindness-detection/test_images/{test_df.iloc[idx]['id_code']}.png\"\n    input_img = np.empty((1,224, 224, 3), dtype=np.uint8)\n    input_img[0,:,:,:] = preprocess_image(path)\n    aug_img = albaugment(aug_list[i],input_img[0,:,:,:])\n    ben_img = transform_image_ben(aug_img)\n    \n    print('test pic no.%d -- augmentation: %s' % (i+1, aug_name[i]))\n    _ = gen_heatmap_img(aug_img,\n                        model, layer_name=layer_name,viz_img=ben_img,orig_img=input_img[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}