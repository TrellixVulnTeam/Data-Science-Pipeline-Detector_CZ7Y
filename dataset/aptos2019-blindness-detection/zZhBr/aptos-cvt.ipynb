{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install opencv-python\n!pip install pyyaml\n!pip install json_tricks\n!pip install yacs\n!pip install tensorwatch\n!pip install tensorboardX\n!pip install ptflops\n!pip install timm\n!pip install einops\n!pip install torch==1.8.1 torchvision","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-14T03:59:56.383421Z","iopub.execute_input":"2022-04-14T03:59:56.384312Z","iopub.status.idle":"2022-04-14T04:02:49.592823Z","shell.execute_reply.started":"2022-04-14T03:59:56.38418Z","shell.execute_reply":"2022-04-14T04:02:49.591399Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!lsb_release -a","metadata":{"execution":{"iopub.status.busy":"2022-04-14T04:02:49.595453Z","iopub.execute_input":"2022-04-14T04:02:49.596188Z","iopub.status.idle":"2022-04-14T04:02:50.416063Z","shell.execute_reply.started":"2022-04-14T04:02:49.596129Z","shell.execute_reply":"2022-04-14T04:02:50.414923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from functools import partial\nfrom itertools import repeat\nimport collections.abc as container_abcs\n\nimport logging\nimport os\nfrom collections import OrderedDict\n\nimport numpy as np\nimport scipy\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom einops import rearrange\nfrom einops.layers.torch import Rearrange\n\nfrom timm.models.layers import DropPath, trunc_normal_\n\n# From PyTorch internals\ndef _ntuple(n):\n    def parse(x):\n        if isinstance(x, container_abcs.Iterable):\n            return x\n        return tuple(repeat(x, n))\n\n    return parse\n\n\nto_1tuple = _ntuple(1)\nto_2tuple = _ntuple(2)\nto_3tuple = _ntuple(3)\nto_4tuple = _ntuple(4)\nto_ntuple = _ntuple\n\n\nclass LayerNorm(nn.LayerNorm):\n    \"\"\"Subclass torch's LayerNorm to handle fp16.\"\"\"\n\n    def forward(self, x: torch.Tensor):\n        orig_type = x.dtype\n        ret = super().forward(x.type(torch.float32))\n        return ret.type(orig_type)\n\n\nclass QuickGELU(nn.Module):\n    def forward(self, x: torch.Tensor):\n        return x * torch.sigmoid(1.702 * x)\n\n\nclass Mlp(nn.Module):\n    def __init__(self,\n                 in_features,\n                 hidden_features=None,\n                 out_features=None,\n                 act_layer=nn.GELU,\n                 drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\nclass Attention(nn.Module):\n    def __init__(self,\n                 dim_in,\n                 dim_out,\n                 num_heads,\n                 qkv_bias=False,\n                 attn_drop=0.,\n                 proj_drop=0.,\n                 method='dw_bn',\n                 kernel_size=3,\n                 stride_kv=1,\n                 stride_q=1,\n                 padding_kv=1,\n                 padding_q=1,\n                 with_cls_token=True,\n                 **kwargs\n                 ):\n        super().__init__()\n        self.stride_kv = stride_kv\n        self.stride_q = stride_q\n        self.dim = dim_out\n        self.num_heads = num_heads\n        # head_dim = self.qkv_dim // num_heads\n        self.scale = dim_out ** -0.5\n        self.with_cls_token = with_cls_token\n\n        self.conv_proj_q = self._build_projection(\n            dim_in, dim_out, kernel_size, padding_q,\n            stride_q, 'linear' if method == 'avg' else method\n        )\n        self.conv_proj_k = self._build_projection(\n            dim_in, dim_out, kernel_size, padding_kv,\n            stride_kv, method\n        )\n        self.conv_proj_v = self._build_projection(\n            dim_in, dim_out, kernel_size, padding_kv,\n            stride_kv, method\n        )\n\n        self.proj_q = nn.Linear(dim_in, dim_out, bias=qkv_bias)\n        self.proj_k = nn.Linear(dim_in, dim_out, bias=qkv_bias)\n        self.proj_v = nn.Linear(dim_in, dim_out, bias=qkv_bias)\n\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim_out, dim_out)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def _build_projection(self,\n                          dim_in,\n                          dim_out,\n                          kernel_size,\n                          padding,\n                          stride,\n                          method):\n        if method == 'dw_bn':\n            proj = nn.Sequential(OrderedDict([\n                ('conv', nn.Conv2d(\n                    dim_in,\n                    dim_in,\n                    kernel_size=kernel_size,\n                    padding=padding,\n                    stride=stride,\n                    bias=False,\n                    groups=dim_in\n                )),\n                ('bn', nn.BatchNorm2d(dim_in)),\n                ('rearrage', Rearrange('b c h w -> b (h w) c')),\n            ]))\n        elif method == 'avg':\n            proj = nn.Sequential(OrderedDict([\n                ('avg', nn.AvgPool2d(\n                    kernel_size=kernel_size,\n                    padding=padding,\n                    stride=stride,\n                    ceil_mode=True\n                )),\n                ('rearrage', Rearrange('b c h w -> b (h w) c')),\n            ]))\n        elif method == 'linear':\n            proj = None\n        else:\n            raise ValueError('Unknown method ({})'.format(method))\n\n        return proj\n\n    def forward_conv(self, x, h, w):\n        if self.with_cls_token:\n            cls_token, x = torch.split(x, [1, h*w], 1)\n\n        x = rearrange(x, 'b (h w) c -> b c h w', h=h, w=w)\n\n        if self.conv_proj_q is not None:\n            q = self.conv_proj_q(x)\n        else:\n            q = rearrange(x, 'b c h w -> b (h w) c')\n\n        if self.conv_proj_k is not None:\n            k = self.conv_proj_k(x)\n        else:\n            k = rearrange(x, 'b c h w -> b (h w) c')\n\n        if self.conv_proj_v is not None:\n            v = self.conv_proj_v(x)\n        else:\n            v = rearrange(x, 'b c h w -> b (h w) c')\n\n        if self.with_cls_token:\n            q = torch.cat((cls_token, q), dim=1)\n            k = torch.cat((cls_token, k), dim=1)\n            v = torch.cat((cls_token, v), dim=1)\n\n        return q, k, v\n\n    def forward(self, x, h, w):\n        if (\n            self.conv_proj_q is not None\n            or self.conv_proj_k is not None\n            or self.conv_proj_v is not None\n        ):\n            q, k, v = self.forward_conv(x, h, w)\n\n        q = rearrange(self.proj_q(q), 'b t (h d) -> b h t d', h=self.num_heads)\n        k = rearrange(self.proj_k(k), 'b t (h d) -> b h t d', h=self.num_heads)\n        v = rearrange(self.proj_v(v), 'b t (h d) -> b h t d', h=self.num_heads)\n\n        attn_score = torch.einsum('bhlk,bhtk->bhlt', [q, k]) * self.scale\n        attn = F.softmax(attn_score, dim=-1)\n        attn = self.attn_drop(attn)\n\n        x = torch.einsum('bhlt,bhtv->bhlv', [attn, v])\n        x = rearrange(x, 'b h t d -> b t (h d)')\n\n        x = self.proj(x)\n        x = self.proj_drop(x)\n\n        return x\n\n    @staticmethod\n    def compute_macs(module, input, output):\n        # T: num_token\n        # S: num_token\n        input = input[0]\n        flops = 0\n\n        _, T, C = input.shape\n        H = W = int(np.sqrt(T-1)) if module.with_cls_token else int(np.sqrt(T))\n\n        H_Q = H / module.stride_q\n        W_Q = H / module.stride_q\n        T_Q = H_Q * W_Q + 1 if module.with_cls_token else H_Q * W_Q\n\n        H_KV = H / module.stride_kv\n        W_KV = W / module.stride_kv\n        T_KV = H_KV * W_KV + 1 if module.with_cls_token else H_KV * W_KV\n\n        # C = module.dim\n        # S = T\n        # Scaled-dot-product macs\n        # [B x T x C] x [B x C x T] --> [B x T x S]\n        # multiplication-addition is counted as 1 because operations can be fused\n        flops += T_Q * T_KV * module.dim\n        # [B x T x S] x [B x S x C] --> [B x T x C]\n        flops += T_Q * module.dim * T_KV\n\n        if (\n            hasattr(module, 'conv_proj_q')\n            and hasattr(module.conv_proj_q, 'conv')\n        ):\n            params = sum(\n                [\n                    p.numel()\n                    for p in module.conv_proj_q.conv.parameters()\n                ]\n            )\n            flops += params * H_Q * W_Q\n\n        if (\n            hasattr(module, 'conv_proj_k')\n            and hasattr(module.conv_proj_k, 'conv')\n        ):\n            params = sum(\n                [\n                    p.numel()\n                    for p in module.conv_proj_k.conv.parameters()\n                ]\n            )\n            flops += params * H_KV * W_KV\n\n        if (\n            hasattr(module, 'conv_proj_v')\n            and hasattr(module.conv_proj_v, 'conv')\n        ):\n            params = sum(\n                [\n                    p.numel()\n                    for p in module.conv_proj_v.conv.parameters()\n                ]\n            )\n            flops += params * H_KV * W_KV\n\n        params = sum([p.numel() for p in module.proj_q.parameters()])\n        flops += params * T_Q\n        params = sum([p.numel() for p in module.proj_k.parameters()])\n        flops += params * T_KV\n        params = sum([p.numel() for p in module.proj_v.parameters()])\n        flops += params * T_KV\n        params = sum([p.numel() for p in module.proj.parameters()])\n        flops += params * T\n\n        module.__flops__ += flops\n\n\nclass Block(nn.Module):\n\n    def __init__(self,\n                 dim_in,\n                 dim_out,\n                 num_heads,\n                 mlp_ratio=4.,\n                 qkv_bias=False,\n                 drop=0.,\n                 attn_drop=0.,\n                 drop_path=0.,\n                 act_layer=nn.GELU,\n                 norm_layer=nn.LayerNorm,\n                 **kwargs):\n        super().__init__()\n\n        self.with_cls_token = kwargs['with_cls_token']\n\n        self.norm1 = norm_layer(dim_in)\n        self.attn = Attention(\n            dim_in, dim_out, num_heads, qkv_bias, attn_drop, drop,\n            **kwargs\n        )\n\n        self.drop_path = DropPath(drop_path) \\\n            if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim_out)\n\n        dim_mlp_hidden = int(dim_out * mlp_ratio)\n        self.mlp = Mlp(\n            in_features=dim_out,\n            hidden_features=dim_mlp_hidden,\n            act_layer=act_layer,\n            drop=drop\n        )\n\n    def forward(self, x, h, w):\n        res = x\n\n        x = self.norm1(x)\n        attn = self.attn(x, h, w)\n        x = res + self.drop_path(attn)\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n\n        return x\n\n\nclass ConvEmbed(nn.Module):\n    \"\"\" Image to Conv Embedding\n    \"\"\"\n\n    def __init__(self,\n                 patch_size=7,\n                 in_chans=3,\n                 embed_dim=64,\n                 stride=4,\n                 padding=2,\n                 norm_layer=None):\n        super().__init__()\n        patch_size = to_2tuple(patch_size)\n        self.patch_size = patch_size\n\n        self.proj = nn.Conv2d(\n            in_chans, embed_dim,\n            kernel_size=patch_size,\n            stride=stride,\n            padding=padding\n        )\n        self.norm = norm_layer(embed_dim) if norm_layer else None\n\n    def forward(self, x):\n        x = self.proj(x)\n\n        B, C, H, W = x.shape\n        x = rearrange(x, 'b c h w -> b (h w) c')\n        if self.norm:\n            x = self.norm(x)\n        x = rearrange(x, 'b (h w) c -> b c h w', h=H, w=W)\n\n        return x\n\n\nclass VisionTransformer(nn.Module):\n    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n    \"\"\"\n    def __init__(self,\n                 patch_size=16,\n                 patch_stride=16,\n                 patch_padding=0,\n                 in_chans=3,\n                 embed_dim=768,\n                 depth=12,\n                 num_heads=12,\n                 mlp_ratio=4.,\n                 qkv_bias=False,\n                 drop_rate=0.,\n                 attn_drop_rate=0.,\n                 drop_path_rate=0.,\n                 act_layer=nn.GELU,\n                 norm_layer=nn.LayerNorm,\n                 init='trunc_norm',\n                 **kwargs):\n        super().__init__()\n        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n\n        self.rearrage = None\n\n        self.patch_embed = ConvEmbed(\n            # img_size=img_size,\n            patch_size=patch_size,\n            in_chans=in_chans,\n            stride=patch_stride,\n            padding=patch_padding,\n            embed_dim=embed_dim,\n            norm_layer=norm_layer\n        )\n\n        with_cls_token = kwargs['with_cls_token']\n        if with_cls_token:\n            self.cls_token = nn.Parameter(\n                torch.zeros(1, 1, embed_dim)\n            )\n        else:\n            self.cls_token = None\n\n        self.pos_drop = nn.Dropout(p=drop_rate)\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n\n        blocks = []\n        for j in range(depth):\n            blocks.append(\n                Block(\n                    dim_in=embed_dim,\n                    dim_out=embed_dim,\n                    num_heads=num_heads,\n                    mlp_ratio=mlp_ratio,\n                    qkv_bias=qkv_bias,\n                    drop=drop_rate,\n                    attn_drop=attn_drop_rate,\n                    drop_path=dpr[j],\n                    act_layer=act_layer,\n                    norm_layer=norm_layer,\n                    **kwargs\n                )\n            )\n        self.blocks = nn.ModuleList(blocks)\n\n        if self.cls_token is not None:\n            trunc_normal_(self.cls_token, std=.02)\n\n        if init == 'xavier':\n            self.apply(self._init_weights_xavier)\n        else:\n            self.apply(self._init_weights_trunc_normal)\n\n    def _init_weights_trunc_normal(self, m):\n        if isinstance(m, nn.Linear):\n            logging.info('=> init weight of Linear from trunc norm')\n            trunc_normal_(m.weight, std=0.02)\n            if m.bias is not None:\n                logging.info('=> init bias of Linear to zeros')\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, (nn.LayerNorm, nn.BatchNorm2d)):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    def _init_weights_xavier(self, m):\n        if isinstance(m, nn.Linear):\n            logging.info('=> init weight of Linear from xavier uniform')\n            nn.init.xavier_uniform_(m.weight)\n            if m.bias is not None:\n                logging.info('=> init bias of Linear to zeros')\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, (nn.LayerNorm, nn.BatchNorm2d)):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    def forward(self, x):\n        x = self.patch_embed(x)\n        B, C, H, W = x.size()\n\n        x = rearrange(x, 'b c h w -> b (h w) c')\n\n        cls_tokens = None\n        if self.cls_token is not None:\n            # stole cls_tokens impl from Phil Wang, thanks\n            cls_tokens = self.cls_token.expand(B, -1, -1)\n            x = torch.cat((cls_tokens, x), dim=1)\n\n        x = self.pos_drop(x)\n\n        for i, blk in enumerate(self.blocks):\n            x = blk(x, H, W)\n\n        if self.cls_token is not None:\n            cls_tokens, x = torch.split(x, [1, H*W], 1)\n        x = rearrange(x, 'b (h w) c -> b c h w', h=H, w=W)\n\n        return x, cls_tokens\n\n\nclass ConvolutionalVisionTransformer(nn.Module):\n    def __init__(self,\n                 in_chans=3,\n                 num_classes=1000,\n                 act_layer=nn.GELU,\n                 norm_layer=nn.LayerNorm,\n                 init='trunc_norm',\n                 spec=None):\n        super().__init__()\n        self.num_classes = num_classes\n\n        self.num_stages = spec['NUM_STAGES']\n        for i in range(self.num_stages):\n            kwargs = {\n                'patch_size': spec['PATCH_SIZE'][i],\n                'patch_stride': spec['PATCH_STRIDE'][i],\n                'patch_padding': spec['PATCH_PADDING'][i],\n                'embed_dim': spec['DIM_EMBED'][i],\n                'depth': spec['DEPTH'][i],\n                'num_heads': spec['NUM_HEADS'][i],\n                'mlp_ratio': spec['MLP_RATIO'][i],\n                'qkv_bias': spec['QKV_BIAS'][i],\n                'drop_rate': spec['DROP_RATE'][i],\n                'attn_drop_rate': spec['ATTN_DROP_RATE'][i],\n                'drop_path_rate': spec['DROP_PATH_RATE'][i],\n                'with_cls_token': spec['CLS_TOKEN'][i],\n                'method': spec['QKV_PROJ_METHOD'][i],\n                'kernel_size': spec['KERNEL_QKV'][i],\n                'padding_q': spec['PADDING_Q'][i],\n                'padding_kv': spec['PADDING_KV'][i],\n                'stride_kv': spec['STRIDE_KV'][i],\n                'stride_q': spec['STRIDE_Q'][i],\n            }\n\n            stage = VisionTransformer(\n                in_chans=in_chans,\n                init=init,\n                act_layer=act_layer,\n                norm_layer=norm_layer,\n                **kwargs\n            )\n            setattr(self, f'stage{i}', stage)\n\n            in_chans = spec['DIM_EMBED'][i]\n\n        dim_embed = spec['DIM_EMBED'][-1]\n        self.norm = norm_layer(dim_embed)\n        self.cls_token = spec['CLS_TOKEN'][-1]\n\n        # Classifier head\n        self.head = nn.Linear(dim_embed, num_classes) if num_classes > 0 else nn.Identity()\n        trunc_normal_(self.head.weight, std=0.02)\n\n    def init_weights(self, pretrained='', pretrained_layers=[], verbose=True):\n        if os.path.isfile(pretrained):\n            pretrained_dict = torch.load(pretrained, map_location='cpu')\n            logging.info(f'=> loading pretrained model {pretrained}')\n            model_dict = self.state_dict()\n            pretrained_dict = {\n                k: v for k, v in pretrained_dict.items()\n                if k in model_dict.keys()\n            }\n            need_init_state_dict = {}\n            for k, v in pretrained_dict.items():\n                need_init = (\n                        k.split('.')[0] in pretrained_layers\n                        or pretrained_layers[0] is '*'\n                )\n                if need_init:\n                    if verbose:\n                        logging.info(f'=> init {k} from {pretrained}')\n                    if 'pos_embed' in k and v.size() != model_dict[k].size():\n                        size_pretrained = v.size()\n                        size_new = model_dict[k].size()\n                        logging.info(\n                            '=> load_pretrained: resized variant: {} to {}'\n                            .format(size_pretrained, size_new)\n                        )\n\n                        ntok_new = size_new[1]\n                        ntok_new -= 1\n\n                        posemb_tok, posemb_grid = v[:, :1], v[0, 1:]\n\n                        gs_old = int(np.sqrt(len(posemb_grid)))\n                        gs_new = int(np.sqrt(ntok_new))\n\n                        logging.info(\n                            '=> load_pretrained: grid-size from {} to {}'\n                            .format(gs_old, gs_new)\n                        )\n\n                        posemb_grid = posemb_grid.reshape(gs_old, gs_old, -1)\n                        zoom = (gs_new / gs_old, gs_new / gs_old, 1)\n                        posemb_grid = scipy.ndimage.zoom(\n                            posemb_grid, zoom, order=1\n                        )\n                        posemb_grid = posemb_grid.reshape(1, gs_new ** 2, -1)\n                        v = torch.tensor(\n                            np.concatenate([posemb_tok, posemb_grid], axis=1)\n                        )\n\n                    need_init_state_dict[k] = v\n            self.load_state_dict(need_init_state_dict, strict=False)\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        layers = set()\n        for i in range(self.num_stages):\n            layers.add(f'stage{i}.pos_embed')\n            layers.add(f'stage{i}.cls_token')\n\n        return layers\n\n    def forward_features(self, x):\n        for i in range(self.num_stages):\n            x, cls_tokens = getattr(self, f'stage{i}')(x)\n\n        if self.cls_token:\n            x = self.norm(cls_tokens)\n            x = torch.squeeze(x)\n        else:\n            x = rearrange(x, 'b c h w -> b (h w) c')\n            x = self.norm(x)\n            x = torch.mean(x, dim=1)\n\n        return x\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.head(x)\n\n        return x","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-04-14T04:02:50.421742Z","iopub.execute_input":"2022-04-14T04:02:50.422063Z","iopub.status.idle":"2022-04-14T04:02:52.447071Z","shell.execute_reply.started":"2022-04-14T04:02:50.422028Z","shell.execute_reply":"2022-04-14T04:02:52.44601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from yacs.config import CfgNode as CN\nimport yaml\n\n\n_C = CN()\n\n_C.BASE = ['']\n_C.NAME = ''\n_C.DATA_DIR = ''\n_C.DIST_BACKEND = 'nccl'\n_C.GPUS = (0,)\n# _C.LOG_DIR = ''\n_C.MULTIPROCESSING_DISTRIBUTED = True\n_C.OUTPUT_DIR = ''\n_C.PIN_MEMORY = True\n_C.PRINT_FREQ = 20\n_C.RANK = 0\n_C.VERBOSE = True\n_C.WORKERS = 4\n_C.MODEL_SUMMARY = False\n\n_C.AMP = CN()\n_C.AMP.ENABLED = False\n_C.AMP.MEMORY_FORMAT = 'nchw'\n\n# Cudnn related params\n_C.CUDNN = CN()\n_C.CUDNN.BENCHMARK = True\n_C.CUDNN.DETERMINISTIC = False\n_C.CUDNN.ENABLED = True\n\n# common params for NETWORK\n_C.MODEL = CN()\n_C.MODEL.NAME = 'cls_hrnet'\n_C.MODEL.INIT_WEIGHTS = True\n_C.MODEL.PRETRAINED = ''\n_C.MODEL.PRETRAINED_LAYERS = ['*']\n_C.MODEL.NUM_CLASSES = 1000\n_C.MODEL.SPEC = CN(new_allowed=True)\n\n_C.LOSS = CN(new_allowed=True)\n_C.LOSS.LABEL_SMOOTHING = 0.0\n_C.LOSS.LOSS = 'softmax'\n\n# DATASET related params\n_C.DATASET = CN()\n_C.DATASET.ROOT = ''\n_C.DATASET.DATASET = 'imagenet'\n_C.DATASET.TRAIN_SET = 'train'\n_C.DATASET.TEST_SET = 'val'\n_C.DATASET.DATA_FORMAT = 'jpg'\n_C.DATASET.LABELMAP = ''\n_C.DATASET.TRAIN_TSV_LIST = []\n_C.DATASET.TEST_TSV_LIST = []\n_C.DATASET.SAMPLER = 'default'\n\n_C.DATASET.TARGET_SIZE = -1\n\n# training data augmentation\n_C.INPUT = CN()\n_C.INPUT.MEAN = [0.485, 0.456, 0.406]\n_C.INPUT.STD = [0.229, 0.224, 0.225]\n\n# data augmentation\n_C.AUG = CN()\n_C.AUG.SCALE = (0.08, 1.0)\n_C.AUG.RATIO = (3.0/4.0, 4.0/3.0)\n_C.AUG.COLOR_JITTER = [0.4, 0.4, 0.4, 0.1, 0.0]\n_C.AUG.GRAY_SCALE = 0.0\n_C.AUG.GAUSSIAN_BLUR = 0.0\n_C.AUG.DROPBLOCK_LAYERS = [3, 4]\n_C.AUG.DROPBLOCK_KEEP_PROB = 1.0\n_C.AUG.DROPBLOCK_BLOCK_SIZE = 7\n_C.AUG.MIXUP_PROB = 0.0\n_C.AUG.MIXUP = 0.0\n_C.AUG.MIXCUT = 0.0\n_C.AUG.MIXCUT_MINMAX = []\n_C.AUG.MIXUP_SWITCH_PROB = 0.5\n_C.AUG.MIXUP_MODE = 'batch'\n_C.AUG.MIXCUT_AND_MIXUP = False\n_C.AUG.INTERPOLATION = 2\n_C.AUG.TIMM_AUG = CN(new_allowed=True)\n_C.AUG.TIMM_AUG.USE_LOADER = False\n_C.AUG.TIMM_AUG.USE_TRANSFORM = False\n\n# train\n_C.TRAIN = CN()\n\n_C.TRAIN.AUTO_RESUME = True\n_C.TRAIN.CHECKPOINT = ''\n_C.TRAIN.LR_SCHEDULER = CN(new_allowed=True)\n_C.TRAIN.SCALE_LR = True\n_C.TRAIN.LR = 0.001\n\n_C.TRAIN.OPTIMIZER = 'sgd'\n_C.TRAIN.OPTIMIZER_ARGS = CN(new_allowed=True)\n_C.TRAIN.MOMENTUM = 0.9\n_C.TRAIN.WD = 0.0001\n_C.TRAIN.WITHOUT_WD_LIST = []\n_C.TRAIN.NESTEROV = True\n# for adam\n_C.TRAIN.GAMMA1 = 0.99\n_C.TRAIN.GAMMA2 = 0.0\n\n_C.TRAIN.BEGIN_EPOCH = 0\n_C.TRAIN.END_EPOCH = 100\n\n_C.TRAIN.IMAGE_SIZE = [224, 224]  # width * height, ex: 192 * 256\n_C.TRAIN.BATCH_SIZE_PER_GPU = 32\n_C.TRAIN.SHUFFLE = True\n\n_C.TRAIN.EVAL_BEGIN_EPOCH = 0\n\n_C.TRAIN.DETECT_ANOMALY = False\n\n_C.TRAIN.CLIP_GRAD_NORM = 0.0\n_C.TRAIN.SAVE_ALL_MODELS = False\n\n# testing\n_C.TEST = CN()\n\n# size of images for each device\n_C.TEST.BATCH_SIZE_PER_GPU = 32\n_C.TEST.CENTER_CROP = True\n_C.TEST.IMAGE_SIZE = [224, 224]  # width * height, ex: 192 * 256\n_C.TEST.INTERPOLATION = 2\n_C.TEST.MODEL_FILE = ''\n_C.TEST.REAL_LABELS = False\n_C.TEST.VALID_LABELS = ''\n\n_C.FINETUNE = CN()\n_C.FINETUNE.FINETUNE = False\n_C.FINETUNE.USE_TRAIN_AUG = False\n_C.FINETUNE.BASE_LR = 0.003\n_C.FINETUNE.BATCH_SIZE = 512\n_C.FINETUNE.EVAL_EVERY = 3000\n_C.FINETUNE.TRAIN_MODE = True\n# _C.FINETUNE.MODEL_FILE = ''\n_C.FINETUNE.FROZEN_LAYERS = []\n_C.FINETUNE.LR_SCHEDULER = CN(new_allowed=True)\n_C.FINETUNE.LR_SCHEDULER.DECAY_TYPE = 'step'\n\n# debug\n_C.DEBUG = CN()\n_C.DEBUG.DEBUG = False\n\ndef _update_config_from_file(config, cfg_file):\n    config.defrost()\n    with open(cfg_file, 'r') as f:\n        yaml_cfg = yaml.load(f, Loader=yaml.FullLoader)\n\n    for cfg in yaml_cfg.setdefault('BASE', ['']):\n        if cfg:\n            _update_config_from_file(\n                config, op.join(op.dirname(cfg_file), cfg)\n            )\n    print('=> merge config from {}'.format(cfg_file))\n    config.merge_from_file(cfg_file)\n    config.freeze()\n\n\ndef update_config(config):\n    _update_config_from_file(config, '../input/cvt-pretrained-param/config.yaml')\n\n    config.defrost()\n#     config.merge_from_list(args.opts)\n#     if config.TRAIN.SCALE_LR:\n#         config.TRAIN.LR *= comm.world_size\n    file_name, _ = os.path.splitext(os.path.basename('../input/cvt-pretrained-param/config.yaml'))\n    config.NAME = file_name + config.NAME\n#     config.RANK = comm.rank\n\n    if 'timm' == config.TRAIN.LR_SCHEDULER.METHOD:\n        config.TRAIN.LR_SCHEDULER.ARGS.epochs = config.TRAIN.END_EPOCH\n\n    if 'timm' == config.TRAIN.OPTIMIZER:\n        config.TRAIN.OPTIMIZER_ARGS.lr = config.TRAIN.LR\n\n    aug = config.AUG\n    if aug.MIXUP > 0.0 or aug.MIXCUT > 0.0 or aug.MIXCUT_MINMAX:\n        aug.MIXUP_PROB = 1.0\n    config.freeze()\n\n","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-04-14T04:02:52.450644Z","iopub.execute_input":"2022-04-14T04:02:52.450994Z","iopub.status.idle":"2022-04-14T04:02:52.497057Z","shell.execute_reply.started":"2022-04-14T04:02:52.450949Z","shell.execute_reply":"2022-04-14T04:02:52.496046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nconfig = _C\nupdate_config(config)\nmsvit_spec = config.MODEL.SPEC\nmodel = ConvolutionalVisionTransformer(\n    in_chans=3,\n    num_classes=5,\n    act_layer=QuickGELU,\n    norm_layer=partial(LayerNorm, eps=1e-5),\n    init=getattr(msvit_spec, 'INIT', 'trunc_norm'),\n    spec=msvit_spec\n)\n\n# if config.MODEL.INIT_WEIGHTS:\n#     model.init_weights(\n#         config.MODEL.PRETRAINED,\n#         config.MODEL.PRETRAINED_LAYERS,\n#         config.VERBOSE\n#     )","metadata":{"execution":{"iopub.status.busy":"2022-04-14T04:02:52.498878Z","iopub.execute_input":"2022-04-14T04:02:52.499819Z","iopub.status.idle":"2022-04-14T04:02:53.228452Z","shell.execute_reply.started":"2022-04-14T04:02:52.499772Z","shell.execute_reply":"2022-04-14T04:02:53.227448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pretrained_dict = torch.load('../input/cvt-pretrained-param/CvT-21-224x224-IN-1k.pth')\nmodel_dict = model.state_dict()\n# for k, v in pretrained_dict.items():\n#     print(k)\n# print('-'*30)\n# for k, v in model_dict.items():\n#     print(k)\npretrained_dict = {k: v for k, v in pretrained_dict.items() if (k != 'head.weight' and k != 'head.bias')}\nmodel_dict.update(pretrained_dict)\nmodel.load_state_dict(model_dict)\n# model.load_state_dict(torch.load('../input/cvt-pretrained-param/CvT-21-224x224-IN-1k.pth'), False)\n# model = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T04:02:53.230397Z","iopub.execute_input":"2022-04-14T04:02:53.230769Z","iopub.status.idle":"2022-04-14T04:02:55.078109Z","shell.execute_reply.started":"2022-04-14T04:02:53.23072Z","shell.execute_reply":"2022-04-14T04:02:55.077237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport random\n\nimport time\nimport torchvision\nimport torch.nn as nn\nfrom tqdm import tqdm_notebook as tqdm\n\nfrom PIL import Image, ImageFile\nfrom torch.utils.data import Dataset\nimport torch\nimport torch.optim as optim\nfrom torchvision import transforms\nfrom torch.optim import lr_scheduler\nimport os\n\ndevice = torch.device(\"cuda:0\")\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\n\nos.environ['CUDA_VISIBLE_DEVICE'] = '0'\n\ntorch.cuda.set_device(0)\n# 设置全局参数\nmodellr = 1e-4\nBATCH_SIZE = 64\nEPOCHS = 100\nDEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2022-04-14T08:01:34.402264Z","iopub.execute_input":"2022-04-14T08:01:34.40324Z","iopub.status.idle":"2022-04-14T08:01:34.414178Z","shell.execute_reply.started":"2022-04-14T08:01:34.403186Z","shell.execute_reply":"2022-04-14T08:01:34.412659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def setup_seed(seed):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\nsetup_seed(2333)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T04:02:55.160697Z","iopub.execute_input":"2022-04-14T04:02:55.161381Z","iopub.status.idle":"2022-04-14T04:02:55.173903Z","shell.execute_reply.started":"2022-04-14T04:02:55.161327Z","shell.execute_reply":"2022-04-14T04:02:55.17261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RetinopathyDatasetTrain(Dataset):\n\n    def __init__(self, csv_file):\n\n        self.data = pd.read_csv(csv_file)\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        img_name = os.path.join('../input/aptos2019-blindness-detection/train_images', self.data.loc[idx, 'id_code'] + '.png')\n        image = Image.open(img_name)\n        image = image.resize((224, 224), resample=Image.BILINEAR)\n        label = torch.tensor(self.data.loc[idx, 'diagnosis'])\n        return {'image': transforms.ToTensor()(image),\n                'labels': label\n                }\n    \nclass RetinopathyDatasetTest(Dataset):\n\n    def __init__(self, csv_file):\n        self.data = pd.read_csv(csv_file)\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        img_name = os.path.join('../input/aptos2019-blindness-detection/test_images', self.data.loc[idx, 'id_code'] + '.png')\n        image = Image.open(img_name)\n        image = image.resize((224, 224), resample=Image.BILINEAR)\n        return transforms.ToTensor()(image)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T04:02:55.175657Z","iopub.execute_input":"2022-04-14T04:02:55.17649Z","iopub.status.idle":"2022-04-14T04:02:55.192472Z","shell.execute_reply.started":"2022-04-14T04:02:55.176356Z","shell.execute_reply":"2022-04-14T04:02:55.191168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\n\n# optimizer = optim.Adam(model.parameters(), lr=modellr)\noptimizer = optim.SGD(model.parameters(), lr=modellr, weight_decay=1e-4)\n\n# scheduler = optim.WarmupCosineSchedule(optimizer, warmup_steps=50, t_total=200)\n\ndef adjust_learning_rate(optimizer, epoch):\n    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n    modellrnew = modellr * (0.1 ** (epoch // 50))\n    print(\"lr:\", modellrnew)\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = modellrnew","metadata":{"execution":{"iopub.status.busy":"2022-04-14T08:01:38.044253Z","iopub.execute_input":"2022-04-14T08:01:38.044589Z","iopub.status.idle":"2022-04-14T08:01:38.056836Z","shell.execute_reply.started":"2022-04-14T08:01:38.044543Z","shell.execute_reply":"2022-04-14T08:01:38.0556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T04:02:55.214094Z","iopub.execute_input":"2022-04-14T04:02:55.21538Z","iopub.status.idle":"2022-04-14T04:02:58.243313Z","shell.execute_reply.started":"2022-04-14T04:02:55.215319Z","shell.execute_reply":"2022-04-14T04:02:58.242449Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = RetinopathyDatasetTrain(csv_file='../input/aptos2019-blindness-detection/train.csv')\ntrain_dataset, valid_dataset = torch.utils.data.random_split(dataset, [int(0.8 * len(dataset)), len(dataset) - int(0.8*len(dataset))])\ndata_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, drop_last=False)\nvalid_data_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, drop_last=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T04:02:58.245083Z","iopub.execute_input":"2022-04-14T04:02:58.245688Z","iopub.status.idle":"2022-04-14T04:02:58.270854Z","shell.execute_reply.started":"2022-04-14T04:02:58.245635Z","shell.execute_reply":"2022-04-14T04:02:58.269893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_acc = 0\ntrain_loss, valid_loss, valid_acc, train_acc = [], [], [], []\n# train_loss = np.load('../input/aptos-resnet-ckp/train_loss (2).npy')\n# valid_loss = np.load('../input/aptos-resnet-ckp/valid_loss (2).npy')\n# valid_acc = np.load('../input/aptos-resnet-ckp/valid_acc (2).npy')\n# valid_loss = valid_loss.tolist()\n# train_loss = train_loss.tolist()\n# valid_acc = valid_acc.tolist()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T04:02:58.272809Z","iopub.execute_input":"2022-04-14T04:02:58.273217Z","iopub.status.idle":"2022-04-14T04:02:58.279443Z","shell.execute_reply.started":"2022-04-14T04:02:58.273165Z","shell.execute_reply":"2022-04-14T04:02:58.277991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@torch.no_grad()\ndef Valid():\n    print('Valid')\n    tk1 = tqdm(valid_data_loader, total=int(len(valid_data_loader)))\n    acc_num, sum_num, loss_sum = 0, 0, 0\n    for bi, d in enumerate(tk1):\n        inputs = d[\"image\"]\n        labels = d[\"labels\"]\n        inputs = inputs.to(device, dtype=torch.float)\n        labels = labels.to(device, dtype=torch.long)\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        acc_num += (labels==outputs.argmax(-1)).sum()\n        sum_num += labels.size(0)\n        loss_sum += loss.item() * inputs.size(0)\n    acc = acc_num / sum_num\n    print('Valid Acc: {:.4f}'.format(acc))\n    global best_acc\n    if acc > best_acc:\n        best_acc = acc\n        torch.save(model.state_dict(), 'best_model.pt')\n        print('save best model with acc:{:.4f}'.format(acc))\n    valid_loss.append(loss_sum / len(valid_data_loader))\n    valid_acc.append(acc.cpu())","metadata":{"execution":{"iopub.status.busy":"2022-04-14T04:02:58.2819Z","iopub.execute_input":"2022-04-14T04:02:58.282702Z","iopub.status.idle":"2022-04-14T04:02:58.296556Z","shell.execute_reply.started":"2022-04-14T04:02:58.282638Z","shell.execute_reply":"2022-04-14T04:02:58.295546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\ndef plotTheCurve():\n    \n    plt.figure(figsize=(8,6))\n    plt.plot([i for i in range(len(train_loss))],train_loss,'',label=\"train_loss\")\n    plt.plot([i for i in range(len(valid_loss))],valid_loss,'',label=\"valid_loss\")\n    plt.title('loss')\n    plt.legend(loc='upper right')\n    plt.xlabel('epoch')\n    plt.ylabel('')\n    plt.grid(len(train_loss))\n    plt.show()\n    \n    plt.figure(figsize=(8,6))\n    plt.plot([i for i in range(len(valid_acc))],valid_acc,'',label=\"acc\")\n    plt.plot([i for i in range(len(valid_acc))],train_acc,'',label=\"acc\")\n    plt.title('acc')\n    plt.legend(loc='upper right')\n    plt.xlabel('epoch')\n    plt.ylabel('')\n    plt.grid(len(train_loss))\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T04:02:58.299008Z","iopub.execute_input":"2022-04-14T04:02:58.299665Z","iopub.status.idle":"2022-04-14T04:02:58.313182Z","shell.execute_reply.started":"2022-04-14T04:02:58.299614Z","shell.execute_reply":"2022-04-14T04:02:58.311959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nsince = time.time()\nfor epoch in range(0, EPOCHS):\n    print('Epoch {}/{}'.format(epoch, EPOCHS - 1))\n    print('-' * 30)\n    adjust_learning_rate(optimizer, epoch)\n#     print(\"lr:\", scheduler.get_lr()[0])\n    model.train()\n    running_loss = 0.0\n    tk0 = tqdm(data_loader, total=int(len(data_loader)))\n    counter = 0\n    acc_num, sum_num = 0, 0\n    for bi, d in enumerate(tk0):\n        inputs = d[\"image\"]\n        labels = d[\"labels\"]\n        inputs = inputs.to(device, dtype=torch.float)\n        labels = labels.to(device, dtype=torch.long)\n        optimizer.zero_grad()\n        with torch.set_grad_enabled(True):\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n#             scheduler.step()\n            optimizer.step()\n        running_loss += loss.item() * inputs.size(0)\n        acc_num += (labels==outputs.argmax(-1)).sum()\n        sum_num += labels.size(0)\n        counter += 1\n        tk0.set_postfix(loss=(running_loss / (counter * data_loader.batch_size)))\n    epoch_loss = running_loss / len(data_loader)\n    acc = acc_num / sum_num\n    print('Training Loss: {:.4f}, Training Acc: {:.4f}'.format(epoch_loss, acc))\n    Valid()\n    train_loss.append(epoch_loss)\n    train_acc.append(acc.cpu())\n    plotTheCurve()\n    \n\ntime_elapsed = time.time() - since\nprint('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\ntorch.save(model.state_dict(), \"model.bin\")","metadata":{"execution":{"iopub.status.busy":"2022-04-14T08:01:42.484357Z","iopub.execute_input":"2022-04-14T08:01:42.485058Z","iopub.status.idle":"2022-04-14T08:20:31.583791Z","shell.execute_reply.started":"2022-04-14T08:01:42.48501Z","shell.execute_reply":"2022-04-14T08:20:31.580357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntorch.save(model.state_dict(), './ckp.pt')","metadata":{"execution":{"iopub.status.busy":"2022-04-14T08:20:35.423629Z","iopub.execute_input":"2022-04-14T08:20:35.424467Z","iopub.status.idle":"2022-04-14T08:20:35.668709Z","shell.execute_reply.started":"2022-04-14T08:20:35.424428Z","shell.execute_reply":"2022-04-14T08:20:35.667939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(best_acc)\nprint(valid_acc[-1])","metadata":{"execution":{"iopub.status.busy":"2022-04-14T08:20:36.25211Z","iopub.execute_input":"2022-04-14T08:20:36.252685Z","iopub.status.idle":"2022-04-14T08:20:36.496494Z","shell.execute_reply.started":"2022-04-14T08:20:36.252647Z","shell.execute_reply":"2022-04-14T08:20:36.495604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(type(valid_acc[1])==float)\n# valid_acc = np.array(valid_acc)\nvalid_acc = [i if type(i)==float else i.cpu() for i in valid_acc]","metadata":{"execution":{"iopub.status.busy":"2022-04-14T08:20:37.435392Z","iopub.execute_input":"2022-04-14T08:20:37.436011Z","iopub.status.idle":"2022-04-14T08:20:37.441618Z","shell.execute_reply.started":"2022-04-14T08:20:37.435968Z","shell.execute_reply":"2022-04-14T08:20:37.440882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.figure(figsize=(8,6))\nplt.plot([i for i in range(len(train_loss))],train_loss,'',label=\"train_loss\")\nplt.plot([i for i in range(len(valid_loss))],valid_loss,'',label=\"valid_loss\")\nplt.title('loss')\nplt.legend(loc='upper right')\nplt.xlabel('epoch')\nplt.ylabel('')\nplt.grid(len(train_loss))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T08:20:38.517155Z","iopub.execute_input":"2022-04-14T08:20:38.517793Z","iopub.status.idle":"2022-04-14T08:20:39.003936Z","shell.execute_reply.started":"2022-04-14T08:20:38.517754Z","shell.execute_reply":"2022-04-14T08:20:39.003223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8,6))\nplt.plot([i for i in range(len(valid_acc))],valid_acc,'',label=\"acc\")\n\nplt.title('acc')\nplt.legend(loc='upper right')\nplt.xlabel('epoch')\nplt.ylabel('')\nplt.grid(len(train_loss))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T08:20:39.439631Z","iopub.execute_input":"2022-04-14T08:20:39.439893Z","iopub.status.idle":"2022-04-14T08:20:39.643107Z","shell.execute_reply.started":"2022-04-14T08:20:39.439863Z","shell.execute_reply":"2022-04-14T08:20:39.642405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.save('train_loss.npy', train_loss)\nnp.save('valid_loss.npy', valid_loss)\nnp.save('valid_acc.npy', valid_acc)\nnp.save('train_acc.npy', train_acc)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T08:21:43.510616Z","iopub.execute_input":"2022-04-14T08:21:43.511234Z","iopub.status.idle":"2022-04-14T08:21:43.518405Z","shell.execute_reply.started":"2022-04-14T08:21:43.511193Z","shell.execute_reply":"2022-04-14T08:21:43.517591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.chdir('/kaggle/working')\nprint(os.getcwd())\nprint(os.listdir(\"/kaggle/working\"))\nfrom IPython.display import FileLink\nfor i in os.listdir(\"/kaggle/working\"):\n    FileLink(i)\nFileLink('train_acc.npy')","metadata":{"execution":{"iopub.status.busy":"2022-04-14T08:21:54.098206Z","iopub.execute_input":"2022-04-14T08:21:54.09862Z","iopub.status.idle":"2022-04-14T08:21:54.108439Z","shell.execute_reply.started":"2022-04-14T08:21:54.09856Z","shell.execute_reply":"2022-04-14T08:21:54.107516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}