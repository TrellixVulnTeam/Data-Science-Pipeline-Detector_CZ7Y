{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"id":"osykOZRXY04t"},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport cv2\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n#from tensorflow.keras.utils import to_categorical\nfrom PIL import Image\nfrom PIL import ImageFilter\nimport os\nfrom random import sample\nfrom sklearn.linear_model import LogisticRegression\nfrom tensorflow.keras.layers import Dense,Input,Flatten\nfrom tensorflow.keras.models import Model # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.metrics import  confusion_matrix\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.applications.resnet50 import ResNet50\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"HPfbu0QWY043"},"cell_type":"code","source":"size=300\nb_size=16","execution_count":null,"outputs":[]},{"metadata":{"id":"cUQXGxTVY04-"},"cell_type":"markdown","source":"# Loading dataset"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"id":"KMhezvOuY04_","outputId":"9462ea05-9887-47fa-a426-c546d5c7bd2d"},"cell_type":"code","source":"train=pd.read_csv('../input/aptos2019-blindness-detection/train.csv',dtype=str)\ntest=pd.read_csv('../input/aptos2019-blindness-detection/test.csv',dtype=str)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"ln_NHi3CY05H"},"cell_type":"code","source":"train['id_code']=train['id_code'].map(lambda x: '../input/aptos2019-blindness-detection/train_images/'+x+'.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"UZ1MI7muY05N","outputId":"727ceade-80b7-4137-fe86-4ab11edb0872"},"cell_type":"code","source":"print(len(train),len(test))","execution_count":null,"outputs":[]},{"metadata":{"id":"T3SmSBX7Y05T"},"cell_type":"markdown","source":"# EDA of data"},{"metadata":{"trusted":true,"id":"f3anymzMY05U","outputId":"368b15ac-c7c9-4313-da4b-a35508d77646"},"cell_type":"code","source":"train['diagnosis'].hist(figsize=(10,5))","execution_count":null,"outputs":[]},{"metadata":{"id":"zZEVGhAtY05Z"},"cell_type":"markdown","source":"Ploting sample images for each class"},{"metadata":{"trusted":true,"id":"6YwGVD-0Y05Z","outputId":"eb365573-046b-47a6-c192-7ad78e8d074d"},"cell_type":"code","source":"fig = plt.figure(figsize=(25,10))\nfor id in range(5):\n    spl=train[train['diagnosis']==str(id)].sample(3)\n    for i,j in enumerate(spl['id_code']):\n        ax=fig.add_subplot(5,3, (id * 3) + i + 1,xticks=[], yticks=[])\n        img=cv2.imread(j)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        img = cv2.resize(img, (200, 200))\n        ax.set_title('Label:'+str(id))\n        plt.imshow(img)","execution_count":null,"outputs":[]},{"metadata":{"id":"Is--yQJBY05f"},"cell_type":"markdown","source":"Ploting sample images for each class"},{"metadata":{"id":"JbBz6PO7Y05f"},"cell_type":"markdown","source":"# Creating and saving augmented images"},{"metadata":{"trusted":true,"id":"lUvEImUoY05g"},"cell_type":"code","source":"train_0=train[train['diagnosis']!=str(0)]\ntrain_rest=train_0[train_0['diagnosis']!=str(2)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"GNt6dThmY05l"},"cell_type":"code","source":"#creating horizontal_flip and vertical_flip augmented images in order to deal with class imbalance \ndef save_augments(img):\n    \n    def horizontal_flip(img):\n        data_gen=ImageDataGenerator(horizontal_flip=True)\n        img=np.expand_dims(img, 0)\n        it = data_gen.flow(img, batch_size=1)\n        batch = it.next()\n        image = batch[0].astype('uint8')\n        im = Image.fromarray(image)\n        return im\n    def vertical_flip(img):\n        data_gen=ImageDataGenerator(vertical_flip=True)\n        img=np.expand_dims(img, 0)\n        it = data_gen.flow(img, batch_size=1)\n        batch = it.next()\n        image = batch[0].astype('uint8')\n        im = Image.fromarray(image)\n        return im \n    \n    \n    img1=horizontal_flip(img)\n    img2=vertical_flip(img)\n    return img1 ,img2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"IjDetD49Y05r"},"cell_type":"code","source":"a=[]\nfor i in range(len(train_rest)):\n    a.append(i)\ntemp=sample(a,700)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"0CnIVtN5Y05w"},"cell_type":"code","source":"#creating augmented data and saving it \nos.makedirs('./aug_image')\nfor i in temp:\n    img= cv2.imread(list(train_rest['id_code'])[i])\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img1,img2=save_augments(img)\n    img1.save('./aug_image/'+str(1)+list(train_rest['id_code'])[i].split('/')[-1])\n    img2.save('./aug_image/'+str(2)+list(train_rest['id_code'])[i].split('/')[-1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"YG54yzpJY052"},"cell_type":"code","source":"#loading the augmented data and storing the path in a dataframe\ntrain_final=pd.DataFrame(columns=['id_code','diagnosis'])\nlist1=list(train['id_code'])\nlist2=list(train['diagnosis'])\nfiles=os.listdir('../input/anwoy/aug_image')\nfor f in files:\n    list1.append('../input/anwoy/aug_image/'+f)\n    temp=train[train['id_code']=='../input/aptos2019-blindness-detection/train_images/'+f[1:]]\n    list2.append(list(temp['diagnosis'])[0])\ntrain_final['id_code']=list1\ntrain_final['diagnosis']=list2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"gz6_U3dcY056","outputId":"578024c5-cd1c-429e-8c79-76f67fa64a19"},"cell_type":"code","source":"train_final['diagnosis'].hist(figsize=(10,5))","execution_count":null,"outputs":[]},{"metadata":{"id":"pWgsl0_TY05_"},"cell_type":"markdown","source":"After augmentation we can see the class imbalance is reduced to some extent "},{"metadata":{"id":"q84EHQaWY05_"},"cell_type":"markdown","source":"# Image preprocessing"},{"metadata":{"trusted":true,"id":"Y_wODmCAY06A"},"cell_type":"code","source":"#https://www.kaggle.com/ratthachat/aptos-eye-preprocessing-in-diabetic-retinopathy\n# this function corp images by looking at the pixel intencity i.e considering those pixel whose values are greater than a given value  \ndef crop(img,maxi=7):\n    if img.ndim ==2:\n        mask = img>maxi\n        return img[np.ix_(mask.any(1),mask.any(0))]\n    elif img.ndim==3:\n        \n        img_ = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n        mask = img_>maxi\n        \n        shape_ = img[:,:,0][np.ix_(mask.any(1),mask.any(0))].shape[0]\n        if (shape_ == 0):\n            return img \n        else:\n            img_1=img[:,:,0][np.ix_(mask.any(1),mask.any(0))]\n            img_2=img[:,:,1][np.ix_(mask.any(1),mask.any(0))]\n            img_3=img[:,:,2][np.ix_(mask.any(1),mask.any(0))]\n            image = np.stack([img_1,img_2,img_3],axis=-1)\n        \n        return image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"PT0Lp_vsY06F"},"cell_type":"code","source":"# function to enhance the image pixel values in order to make more interpretable.\ndef preprocessing(img):\n    \n    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n    img = crop(img)\n    img = cv2.resize(img, (size, size))\n    img = cv2.addWeighted (img,4, cv2.GaussianBlur(img, (0,0) ,10), -4, 128)\n    \n    return img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"JifCTk99Y06I"},"cell_type":"code","source":"# canny edge detector helps in  finding imortant edges from a image by  removeing backgroung noise. \ndef edge_detector(img):\n    img = preprocessing(img)\n    edges=cv2.Canny(img,300,330)\n    return edges","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"YPa130IRY06M"},"cell_type":"code","source":"def threshold(img):\n    img=preprocessing(img)\n    #gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    #print(gray.shape)\n    blurred = cv2.GaussianBlur(img, (1,1), 0)\n    thresh = cv2.threshold(blurred, 135,255, cv2.THRESH_BINARY)[1]\n    #thresh=cv2.cvtColor(thresh, cv2.COLOR_GRAY2BGR)\n    \n    return thresh.astype('float32')/255","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"z-9jL3u2Y06R","outputId":"a5bcd805-ac5a-40c7-aab4-fa3c3bbc262a"},"cell_type":"code","source":"t=train[train['diagnosis']==str(4)].sample(1)\nlist(t['id_code'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"T93tmxy_Y06V","outputId":"73158396-685c-4a3b-d863-ee72e350d799"},"cell_type":"code","source":"img = cv2.imread('../input/aptos2019-blindness-detection/train_images/'+'247e98aba610.png')\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\npre=preprocessing(img)\n#plt.imshow(pre)\nplt.subplot(131),plt.imshow(pre)\nplt.title('Original Image'), plt.xticks([]), plt.yticks([])\nplt.subplot(132),plt.imshow(threshold(img))\nplt.title('Threshold image'), plt.xticks([]), plt.yticks([])\nplt.subplot(133),plt.imshow(edge_detector(img))\nplt.title('Edge Image'), plt.xticks([]), plt.yticks([])","execution_count":null,"outputs":[]},{"metadata":{"id":"e-iwJAc0Y06Y"},"cell_type":"markdown","source":"From the plot its clear that canny edge detector is not working properly hence we will use thresohlding image for training"},{"metadata":{"id":"kmaL0LD8Y06Z"},"cell_type":"markdown","source":"# loading data for training"},{"metadata":{"trusted":true,"id":"XCG_dW4IY06a"},"cell_type":"code","source":"train_set,valid_set=train_test_split(train_final,test_size=0.2,stratify=train_final[\"diagnosis\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set['diagnosis'].hist(figsize=(10,5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_set['diagnosis'].hist(figsize=(10,5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(train_set),len(valid_set))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"2pDn3XpwY06d"},"cell_type":"code","source":"def image_aug(df):\n    data_gen=ImageDataGenerator(preprocessing_function=threshold)\n    train_generator=data_gen.flow_from_dataframe(df, \n                                                x_col='id_code', \n                                                y_col='diagnosis',\n                                                #directory = '../input/aptos2019-blindness-detection/train_images',\n                                                target_size=(size,size),\n                                                batch_size=b_size,\n                                                class_mode=\"categorical\"\n                                                )\n    return train_generator","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"zeB9TrIQY06h"},"cell_type":"code","source":"train_generator=image_aug(train_set)\nvalid_generator=image_aug(valid_set)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"E9yk8DaeY06l"},"cell_type":"code","source":"import tensorflow.keras.backend as K\ndef auc2(y_true,y_pred):\n    try:\n        return roc_auc_score(K.eval(y_true), K.eval(y_pred))\n    except :\n        return 0\ndef auc(y_true,y_pred):\n    return tf.py_function(auc2,(y_true,y_pred),tf.double)","execution_count":null,"outputs":[]},{"metadata":{"id":"LaMZMJGGY06p"},"cell_type":"markdown","source":"# Modeling"},{"metadata":{"id":"SKb_mj9HY06q"},"cell_type":"markdown","source":"> >  ****Base model****"},{"metadata":{"trusted":true,"id":"cjN3tBJPY06s"},"cell_type":"code","source":"vgg = tf.keras.applications.VGG16(include_top=False, weights='imagenet',input_shape=(300, 300, 3))\nfor layer in vgg.layers:\n    layer.trainable = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"InDBOcI9Y06x"},"cell_type":"code","source":"tf.keras.backend.clear_session()\nInput_Layer = Input(shape=(size,size,3),name='Input_Layer')\nvgg_out=vgg(Input_Layer)\nflatten = Flatten(name='Flatten')(vgg_out)\nOut = Dense(units=5,activation='softmax',kernel_initializer=tf.keras.initializers.glorot_normal(seed=0),name='Output')(flatten)\nbase_model = Model(inputs=Input_Layer,outputs=Out)\nbase_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=1e-4), loss='categorical_crossentropy',metrics=[auc])\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n                              patience=2, min_lr=0.000005)\nbase_model.fit(train_generator,\n        steps_per_epoch=len(train_generator),\n        validation_data=valid_generator,\n        validation_steps=len(valid_generator),\n        callbacks=[reduce_lr],\n        epochs=7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"esoShTFhY063"},"cell_type":"code","source":"test_csv = pd.read_csv('../input/aptos2019-blindness-detection/sample_submission.csv')\nid_code = test_csv['id_code']\ntest_prediction = np.empty(len(id_code))\nfor i in range(len(id_code)):\n    image = cv2.imread('../input/aptos2019-blindness-detection/test_images/'+list(test_csv['id_code'])[i]+'.png')\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    image = threshold(image)\n    X = np.array([image])\n    pred = base_model.predict(X)\n    pred=np.argmax(pred,axis=1)\n    test_prediction[i]=pred[0]\n        \ntest_csv['diagnosis']=test_prediction\ntest_csv['diagnosis']=test_csv['diagnosis'].astype('int64')\ntest_csv.to_csv(\"submission.csv\", index=False)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"SLtwyZHUY066"},"cell_type":"code","source":"y_pred = np.empty(len(valid_set))\nfor i in range(len(valid_set)):\n    image = cv2.imread(list(valid_set['id_code'])[i])\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    image = threshold(image)\n    X = np.array([image])\n    pred = base_model.predict(X)\n    pred=np.argmax(pred,axis=1)\n    y_pred[i]=pred[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"jBDtO0U_Y07l","outputId":"97d7078a-a0ee-43ec-bbac-a2fc8a80f58e"},"cell_type":"code","source":"sns.heatmap(confusion_matrix(list(valid_set['diagnosis'].astype('int64')),y_pred),annot=True, fmt=\"d\")","execution_count":null,"outputs":[]},{"metadata":{"id":"w31WXA4-Y07B"},"cell_type":"markdown","source":"> > **CNN model**"},{"metadata":{"trusted":true,"id":"ZqGSgE6uY07C"},"cell_type":"code","source":"Input_Layer = Input(shape=(size,size,3),name='Input_Layer')\nx=Conv2D(filters=32,kernel_size=(1,1),strides=(3,3),kernel_initializer=tf.keras.initializers.he_normal(seed=0))(Input_Layer)\nx=BatchNormalization(axis=3)(x)\nx=Activation('relu')(x)\nx=Conv2D(filters=64,kernel_size=(1,1),strides=(1,1),kernel_initializer=tf.keras.initializers.he_normal(seed=0))(Input_Layer)\nx=BatchNormalization(axis=3)(x)\nx=Activation('relu')(x)\nx=Conv2D(filters=64,kernel_size=(1,1),strides=(3,3),padding='same',kernel_initializer=tf.keras.initializers.he_normal(seed=0))(Input_Layer)\nx=BatchNormalization(axis=3)(x)\nx=Activation('relu')(x)\nx=Flattern()(x)\nOut = Dense(units=5,activation='softmax',kernel_initializer=tf.keras.initializers.glorot_normal(seed=0),name='Output')(flatten)\ncnn_model = Model(inputs=Input_Layer,outputs=Out)\ncnn_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"qj79TGn9Y07F"},"cell_type":"code","source":"cnn_model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=1e-4), loss='categorical_crossentropy',metrics=[auc])\ncnn_model.fit(train_generator,\n        steps_per_epoch=len(train_generator),\n        validation_data=valid_generator,\n        validation_steps=len(valid_generator),\n        callbacks=[reduce_lr],\n        epochs=10)    \n             ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"mJBgPSm8Y07L"},"cell_type":"code","source":"test_csv = pd.read_csv('../input/aptos2019-blindness-detection/sample_submission.csv')\nid_code = test_csv['id_code']\ntest_prediction_1 = np.empty(len(id_code))\nfor i in range(len(id_code)):\n    image = cv2.imread('../input/aptos2019-blindness-detection/test_images/'+list(test_csv['id_code'])[i]+'.png')\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    image = threshold(image)\n    X = np.array([image])\n    pred = cnn_model.predict(X)\n    pred=np.argmax(pred,axis=1)\n    test_prediction_1[i]=pred[0]\n        \ntest_csv['diagnosis']=test_prediction_1\ntest_csv['diagnosis']=test_csv['diagnosis'].astype('int64')\ntest_csv.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"Yw_CTRMPY07O"},"cell_type":"code","source":"y_pred = np.empty(len(valid_set))\nfor i in range(len(valid_set)):\n    image = cv2.imread(list(valid_set['id_code'])[i])\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    image = threshold(image)\n    X = np.array([image])\n    pred = cnn_model.predict(X)\n    pred=np.argmax(pred,axis=1)\n    y_pred[i]=pred[0]\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"a7gnWd1tY07R","outputId":"3b4cd104-df59-4031-cf67-ba02ccf2e22d"},"cell_type":"code","source":"sns.heatmap(confusion_matrix(list(valid_set['diagnosis'].astype('int64')),y_pred),annot=True, fmt=\"d\")","execution_count":null,"outputs":[]},{"metadata":{"id":"xs-mVfIbY07V"},"cell_type":"markdown","source":"> > **Transfer learning**"},{"metadata":{"trusted":true,"id":"u3Yg9ND8Y07X"},"cell_type":"code","source":"Eff = tf.keras.applications.EfficientNetB3(weights='imagenet', include_top=False, input_shape=(size,size,3))\nx = Eff.output\nx = tf.keras.layers.GlobalAveragePooling2D()(x)\nx = Dropout(0.4)(x)\nx = Dense(5, activation='softmax', kernel_initializer='he_normal')(x)\nmodel = Model(inputs=Eff.input, outputs=x)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"C4Fasg5nY07a"},"cell_type":"code","source":"model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=1e-4), loss='categorical_crossentropy',metrics=[auc])\nmodel.fit(train_generator,\n        steps_per_epoch=len(train_generator),\n        validation_data=valid_generator,\n        validation_steps=len(valid_generator),\n        callbacks=[reduce_lr],\n        epochs=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"ZvdxfuhRY07d"},"cell_type":"code","source":"test_csv = pd.read_csv('../input/aptos2019-blindness-detection/sample_submission.csv')\nid_code = test_csv['id_code']\ntest_prediction_3 = np.empty(len(id_code))\nfor i in range(len(id_code)):\n    image = cv2.imread('../input/aptos2019-blindness-detection/test_images/'+list(test_csv['id_code'])[i]+'.png')\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    image = threshold(image)\n    X = np.array([image])\n    pred = model.predict(X)\n    pred=np.argmax(pred,axis=1)\n    test_prediction_3[i]=pred[0]\n        \ntest_csv['diagnosis']=test_prediction_3\ntest_csv['diagnosis']=test_csv['diagnosis'].astype('int64')\ntest_csv.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"r_V8b1lzY07h"},"cell_type":"code","source":"y_pred = np.empty(len(valid_set))\nfor i in range(len(valid_set)):\n    image = cv2.imread(list(valid_set['id_code'])[i])\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    image = threshold(image)\n    X = np.array([image])\n    pred = model.predict(X)\n    pred=np.argmax(pred,axis=1)\n    y_pred[i]=pred[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"psM6akfvY069","outputId":"8085b16a-ecb8-4faa-e9df-dc86109be6f6"},"cell_type":"code","source":"sns.heatmap(confusion_matrix(list(valid_set['diagnosis'].astype('int64')),y_pred),annot=True, fmt=\"d\")","execution_count":null,"outputs":[]},{"metadata":{"id":"nXM6qQDQY07q"},"cell_type":"markdown","source":"> > ****Ensemble****"},{"metadata":{"trusted":true,"id":"mpc3b38aY07q"},"cell_type":"code","source":"Resnet = ResNet50(weights='imagenet', include_top=False, input_shape=(size,size,3))\nx = Resnet.output\nx = tf.keras.layers.GlobalAveragePooling2D()(x)\nx = Dropout(0.4)(x)\nx = Dense(5, activation='softmax', kernel_initializer='he_normal')(x)\nmodel_1 = Model(inputs=Resnet.input, outputs=x)\nmodel_1.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"L1KX_a2VY074"},"cell_type":"code","source":"model_1.compile(optimizer=tf.keras.optimizers.RMSprop(lr=1e-4), loss='categorical_crossentropy',metrics=[auc])\nmodel_1.fit(train_generator,\n        steps_per_epoch=len(train_generator),\n        validation_data=valid_generator,\n        validation_steps=len(valid_generator),\n        callbacks=[reduce_lr],        \n        epochs=15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"hgL4KWKAY070"},"cell_type":"code","source":"vgg = tf.keras.applications.VGG16(include_top=False, weights='imagenet',input_shape=(300, 300, 3))\nx = vgg.output\nx = tf.keras.layers.GlobalAveragePooling2D()(x)\nx = Dropout(0.4)(x)\nx = Dense(5, activation='softmax', kernel_initializer='he_normal')(x)\nmodel_2 = Model(inputs=vgg.input, outputs=x)\nmodel_2.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"hxBqNmVKY07v"},"cell_type":"code","source":"model_2.compile(optimizer=tf.keras.optimizers.RMSprop(lr=1e-4), loss='categorical_crossentropy',metrics=[auc])\nmodel_2.fit(train_generator,\n        steps_per_epoch=len(train_generator),\n        validation_data=valid_generator,\n        validation_steps=len(valid_generator),\n        callbacks=[reduce_lr],\n        epochs=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"Rh2qX-qfY07_"},"cell_type":"code","source":"Eff = tf.keras.applications.EfficientNetB3(weights='imagenet', include_top=False, input_shape=(size,size,3))\nx = Eff.output\nx = tf.keras.layers.GlobalAveragePooling2D()(x)\nx = Dropout(0.4)(x)\nx = Dense(5, activation='softmax', kernel_initializer='he_normal')(x)\nmodel_3 = Model(inputs=Eff.input, outputs=x)\nmodel_3.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"XMnBF7eJY08D"},"cell_type":"code","source":"model_2.compile(optimizer=tf.keras.optimizers.RMSprop(lr=1e-4), loss='categorical_crossentropy',metrics=[auc])\nmodel_2.fit(train_generator,\n        steps_per_epoch=len(train_generator),\n        validation_data=valid_generator,\n        validation_steps=len(valid_generator),\n        callbacks=[reduce_lr],\n        epochs=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"u6hK4bvTY08H"},"cell_type":"code","source":"def majority(lst):\n    if lst[0]!=lst[1] and lst[0]!=lst[2] and lst[1]!=lst[2] and lst[1]!=lst[2]:\n        return np.rint(sum(lst)/3)\n    else:\n        ma=1\n        for i in lst:\n            if lst.count(i)>=ma:\n                ma=lst.count(i)\n                stor=i\n        return stor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"K-OyTXfjY08J"},"cell_type":"code","source":"test_csv = pd.read_csv('../input/aptos2019-blindness-detection/sample_submission.csv')\nid_code = test_csv['id_code']\npred_list=[]\ntest_prediction = np.empty(len(id_code))\nfor i in range(len(id_code)):\n    image = cv2.imread('../input/aptos2019-blindness-detection/test_images/'+list(test_csv['id_code'])[i]+'.png')\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    image = threshold(image)\n    X = np.array([image])\n    pred = model_1.predict(X)\n    pred=np.argmax(pred,axis=1)\n    pred_list.append(pred[0])\n    pred = model_2.predict(X)\n    pred=np.argmax(pred,axis=1)\n    pred_list.append(pred[0])\n    pred = model_3.predict(X)\n    pred=np.argmax(pred,axis=1)\n    pred_list.append(pred[0])\n    test_prediction[i]=majority(pred_list)\n        \ntest_csv['diagnosis']=test_prediction\ntest_csv['diagnosis']=test_csv['diagnosis'].astype('int64')\ntest_csv.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"uM2YwACIY08N"},"cell_type":"code","source":"temp=[]\ny_pred = np.empty(len(valid_set))\nfor i in range(len(valid_set)):\n    image = cv2.imread(list(valid_set['id_code'])[i])\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    image = threshold(image)\n    X = np.array([image])\n    pred = model_1.predict(X)\n    pred=np.argmax(pred,axis=1)\n    temp.append(pred[0])\n    pred = model_2.predict(X)\n    pred=np.argmax(pred,axis=1)\n    temp.append(pred[0])\n    pred = model_3.predict(X)\n    pred=np.argmax(pred,axis=1)\n    temp.append(pred[0])\n    y_pred[i]=majority(temp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"PO4yDSGhY08R","outputId":"af3a9786-d6d4-450f-e9b6-8719ce800d82"},"cell_type":"code","source":"sns.heatmap(confusion_matrix(list(valid_set['diagnosis'].astype('int64')),y_pred),annot=True, fmt=\"d\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}