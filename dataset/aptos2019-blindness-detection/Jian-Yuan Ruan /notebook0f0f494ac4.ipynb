{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nsys.path.extend(['../input/efficientnet/'])\nfrom timm import *\nfrom timm.models import *","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision\nimport csv\nimport time\nimport glob\nimport copy\nimport os\nimport json\nimport cv2\nimport numpy as np\nimport pickle\nimport torch.optim as optim\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn as nn\nfrom torch.optim import lr_scheduler\nfrom timm.models import *\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torchvision import transforms, utils, models, datasets\nfrom PIL import Image\nfrom PIL import Image, ImageEnhance, ImageOps\nfrom torchvision.models._utils import IntermediateLayerGetter\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection.backbone_utils import resnet_fpn_backbone\nfrom torchvision.ops.feature_pyramid_network import (\n    FeaturePyramidNetwork,\n    LastLevelMaxPool,\n)\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\nfrom torchvision.ops.misc import FrozenBatchNorm2d\nfrom torchvision.models.detection.rpn import AnchorGenerator\nfrom torchvision.ops import misc as misc_nn_ops\nfrom torch import Tensor, Size\nfrom torch.jit.annotations import List, Optional, Tuple\nfrom torch.nn.parameter import Parameter\n\n    \n\nclass Linear(nn.Linear):\n    def forward(self, input: torch.Tensor) -> torch.Tensor:\n        if torch.jit.is_scripting():\n            bias = self.bias.to(dtype=input.dtype) if self.bias is not None else None\n            return F.linear(input, self.weight.to(dtype=input.dtype), bias=bias)\n        else:\n            return F.linear(input, self.weight, self.bias)\n    \n    \n    \n    \nclass backboneNet_efficient(nn.Module):\n    def __init__(self):\n        super(backboneNet_efficient, self).__init__()\n#         norm_layer=FrozenBatchNorm2d\n        net = create_model('tf_efficientnet_b4_ns', pretrained=False)       \n#         layers_to_train = ['blocks']       \n#         for name, parameter in net.named_parameters():\n#             if all([not name.startswith(layer) for layer in layers_to_train]):\n#                 parameter.requires_grad_(False)\n        self.num_features = 1792\n        self.conv_stem = net.conv_stem\n        self.bn1 = net.bn1\n        self.act1 = net.act1       \n        self.block0 = net.blocks[0]\n        self.block1 = net.blocks[1]\n        self.block2 = net.blocks[2]\n        self.block3 = net.blocks[3]\n        self.block4 = net.blocks[4]\n        self.block5 = net.blocks[5]\n        self.block6 = net.blocks[6]       \n        self.conv_head = net.conv_head\n        self.bn2 = net.bn2\n        self.act2 = net.act2\n        self.global_pool =  net.global_pool\n        self.drop_rate = 0.4\n        \n        \n        self.rg_cls = Linear(self.num_features, 1, bias=True)\n        self.cls_cls = Linear(self.num_features, 5, bias=True)\n        self.ord_cls = Linear(self.num_features, 4, bias=True)\n    def forward(self, x):\n        \n        x1 = self.conv_stem(x)\n        x2 = self.bn1(x1)\n        x3 = self.act1(x2)\n        x4 = self.block0(x1)\n        x5 = self.block1(x4)\n        x6 = self.block2(x5)\n        x7 = self.block3(x6)\n        x8 = self.block4(x7)\n        x9 = self.block5(x8)\n        x10 = self.block6(x9)       \n        x11 = self.conv_head(x10)\n        x12 = self.bn2(x11)\n        x13 = self.act2(x12)\n        x14  = self.global_pool(x13)\n        if self.drop_rate > 0.:\n            x14 = F.dropout(x14, p=self.drop_rate, training=self.training)\n            \n#         x15 = self.dense1(x14)\n#         x15 = self.act2(x15)\n        \n#         x16 = self.dense1(x14)\n#         x16 = self.act2(x16)\n        \n#         x17 = self.dense1(x14)\n#         x17 = self.act2(x17)\n            \n        x15 = self.rg_cls(x14)\n        x16 = self.cls_cls(x14)\n        x17 = self.ord_cls(x14)\n\n        return x15, x16, x17\n    \n    \n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef gem(x, p=3, eps=1e-6):\n    return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1./p)\n\n\nclass GeM(nn.Module):\n    def __init__(self, p=3, eps=1e-6, flatten=False):\n        super(GeM,self).__init__()\n        self.p = Parameter(torch.ones(1)*p)\n        self.eps = eps\n        self.flatten = flatten\n    def forward(self, x):\n        x = gem(x, p=self.p, eps=self.eps)\n        if self.flatten:\n            x = x.flatten(1)\n        return x\n    def __repr__(self):\n        return self.__class__.__name__ + '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + ', ' + 'eps=' + str(self.eps) + ')'\n    \n    \nclass ThreeStage_Model(nn.Module):\n    def __init__(self, backbone=None):\n        super(ThreeStage_Model, self).__init__()\n        \n        self.backbone = tf_efficientnet_b4_ns(pretrained=False)\n        self.backbone.global_pool = GeM(flatten=True)\n\n        self.classifier = nn.Sequential(\n                nn.SiLU(),\n                nn.Linear(1000, 500),\n                nn.SiLU(),\n                nn.Linear(500, 5),\n            )\n        \n        self.regressor = nn.Sequential(\n                nn.SiLU(),\n                nn.Linear(1000, 500),\n                nn.SiLU(),\n                nn.Linear(500, 1),\n            )\n\n        self.ordinal = nn.Sequential(\n                nn.SiLU(),\n                nn.Linear(1000, 500),\n                nn.SiLU(),\n                nn.Linear(500, 4),\n            )\n        \n        self.final_regressor = nn.Sequential(\n                nn.SiLU(),\n                nn.Linear(10, 1),\n            )\n\n    def forward(self, x, final=False):\n        x = self.backbone(x)\n        \n        c_out = self.classifier(x)\n        r_out = self.regressor(x)\n        o_out = self.ordinal(x)\n        \n        if final:\n            out = torch.cat((c_out, r_out, o_out), 1)\n            out = self.final_regressor(out)\n            out = torch.sigmoid(out) * 4.5\n            return out\n        else:\n            r_out = torch.sigmoid(r_out) * 4.5\n            o_out = torch.sigmoid(o_out)\n            return c_out, r_out, o_out\n        \n        ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"scrolled":false},"cell_type":"code","source":"\n\nfrom torchvision import transforms, utils, models\nfrom torch.utils.data import Dataset, DataLoader, random_split\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport csv\nimport cv2\nfrom skimage import measure\nimport numpy as np\n\n\n\ndef crop_image_from_gray(img,tol=7):\n    if img.ndim ==2:\n        mask = img>tol\n        return img[np.ix_(mask.any(1),mask.any(0))]\n    elif img.ndim==3:\n        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n        mask = gray_img>tol\n        \n        check_shape = img[:,:,0][np.ix_(mask.any(1),mask.any(0))].shape[0]\n        if (check_shape == 0): # image is too dark so that we crop out everything,\n            return img # return original image\n        else:\n            img1=img[:,:,0][np.ix_(mask.any(1),mask.any(0))]\n            img2=img[:,:,1][np.ix_(mask.any(1),mask.any(0))]\n            img3=img[:,:,2][np.ix_(mask.any(1),mask.any(0))]\n    #         print(img1.shape,img2.shape,img3.shape)\n            img = np.stack([img1,img2,img3],axis=-1)\n    #         print(img.shape)\n        return img\n\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class cropTo4_3(object):\n    def __call__(self, image):\n        w, h = image.size\n        \n        if (w / h) >= (4 / 3):\n            new_h = h\n            new_w = int(h * 4 / 3)\n        else:\n            new_h = int(w * 3 / 4)\n            new_w = w\n            \n        left = (w - new_w)/2\n        top = (h - new_h)/2\n        right = left + new_w\n        bottom = top + new_h\n\n        return image.crop((left, top, right, bottom))\n\n    \nclass trim(object):\n    def __call__(self, image):\n        bg = Image.new(image.mode, image.size, image.getpixel((0,0)))\n        diff = ImageChops.difference(image, bg)\n        diff = ImageChops.add(diff, diff, 2.0, -10)\n        bbox = diff.getbbox()\n        if bbox:\n             return image.crop(bbox)\n\n\n\ndata_transforms = {\n    'train': transforms.Compose([\n        transforms.Resize((384, 384)),\n        transforms.RandomAffine(\n            degrees=(-180, 180),\n            scale=(0.8, 1.2),\n            shear=(0.8, 1.2),\n        ),\n        transforms.RandomHorizontalFlip(p=0.5),\n        transforms.RandomVerticalFlip(p=0.5),\n        transforms.ColorJitter(brightness=(0.2), \n                               contrast=(0.2),\n                               hue=(0.1),\n                               saturation=(0.2),\n                              ),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]), \n    ]),\n    'tta': transforms.Compose([\n        transforms.Resize((420,420)),\n        transforms.RandomAffine(\n            degrees=(-180, 180),\n        ),\n        transforms.RandomHorizontalFlip(p=0.5),\n        transforms.RandomVerticalFlip(p=0.5),\n        transforms.CenterCrop(384),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'test': transforms.Compose([\n        transforms.Resize((420,420)),\n        transforms.CenterCrop(384),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'test_Cpixel': transforms.Compose([\n        transforms.Resize((280,280)),\n        transforms.CenterCrop(256),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'tta_Cpixel': transforms.Compose([\n        transforms.Resize((288,288)),\n        transforms.RandomAffine(\n            degrees=(-180, 180),\n#             scale=(1.2, 1.2),\n        ),\n        transforms.RandomHorizontalFlip(p=0.5),\n        transforms.RandomVerticalFlip(p=0.5),\n        transforms.CenterCrop(256),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'bo_trans':transforms.Compose([\n                trim(),\n                cropTo4_3(),\n                transforms.Resize((384 * 3 // 4, 384)),\n                transforms.ToTensor(),\n                transforms.Normalize(mean=[0.384, 0.258, 0.174], std=[0.124, 0.089, 0.094]),\n   ]),\n}\n\n\ndef most_frequent(List):\n    counter = 0\n    num = List[0]\n\n    for i in List:\n        curr_frequency = List.count(i)\n        if curr_frequency > counter:\n            counter = curr_frequency\n            num = i\n            \n        if curr_frequency == counter:\n            num = (num + i) / 2\n            \n    num = round(num)\n    return num\n\ndef Average(lst): \n  \n    return  sum(lst) / len(lst)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport csv\nimport torch\nimport numpy as np\n\ncsv_name = 'submission.csv'\nnum_cv = 1\n\n\nmodel_ft = backboneNet_efficient()\nmodel_ft2 = backboneNet_efficient()\nmodel_ft3 = backboneNet_efficient()\nmodel_ft4 = backboneNet_efficient()\n\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n\nmodel_ft.load_state_dict(torch.load('../input/v22-pseudov2/rg_efficientnet_b4_v22_pseudov2_30',map_location='cuda:0'))\nmodel_ft = model_ft.to(device)\nmodel = model_ft\n\n# model_ft2.load_state_dict(torch.load('../input/v18-pseudo3/rg_efficientnet_b4_v19_256_41_pseudo_v3_19',map_location='cuda:0'))\n# model_ft2 = model_ft2.to(device)\n\n# model_ft3.load_state_dict(torch.load('../input/v20pseudo/rg_efficientnet_b4_v20_256_31_pseudo_20',map_location='cuda:0'))\n# model_ft3 = model_ft3.to(device)\n\n# model_ft4.load_state_dict(torch.load('../input/v20pseudo/rg_efficientnet_b4_v20_256_31_pseudo_20',map_location='cuda:0'))\n# model_ft4 = model_ft4.to(device)\n\n\n\n\nthrs = [0.5, 1.5, 2.5, 3.5]\n\nwith open(csv_name, 'w', newline='') as csvFile:\n    \n    field = ['id_code', 'diagnosis']\n    writer = csv.DictWriter(csvFile, field)\n    writer.writeheader()\n    \n    test_path = '../input/aptos2019-blindness-detection/test_images/'\n    allFileList = os.listdir(test_path)\n    \n    for file in allFileList:\n        result = []\n        for num in range(num_cv):\n#             for model in [model_ft,model_ft2,model_ft3]:\n                if os.path.isfile(test_path + file):\n                    path = test_path + file\n                    img = cv2.imread(path)\n                    img = crop_image_from_gray(img)\n                    img = Image.fromarray(cv2.cvtColor(img,cv2.COLOR_BGR2RGB))\n                    img = data_transforms['test_Cpixel'](img)\n    #                 if model != model_ft4:\n    #                     img = data_transforms['test_Cpixel'](img)\n    #                 elif model == model_ft4:\n    #                     img = data_transforms['bo_trans'](img)\n                    img = img.unsqueeze(0)\n                    with torch.no_grad():\n                        model.eval()\n                        img = img.to(device)\n                        outputs = model(img)\n                        rg_outputs = outputs[0]\n                        rg_outputs = torch.sigmoid(rg_outputs) * 4.5 \n\n\n                        cls_outputs = outputs[1]\n                        cls_softmax = torch.nn.Softmax(dim=1)\n                        cls_outputs = cls_softmax(cls_outputs)\n                        cls_preds = torch.max(cls_outputs, 1)\n                        cls_predict = cls_preds[1].cpu().numpy()\n\n                        ord_outputs = outputs[2]\n                        ord_outputs = torch.sigmoid(ord_outputs)\n\n                        rg_outputs = rg_outputs.unsqueeze(1)                    \n                        rg_outputs[rg_outputs < thrs[0]] = 0\n                        rg_outputs[(rg_outputs >= thrs[0]) & (rg_outputs < thrs[1])] = 1\n                        rg_outputs[(rg_outputs >= thrs[1]) & (rg_outputs < thrs[2])] = 2\n                        rg_outputs[(rg_outputs >= thrs[2]) & (rg_outputs < thrs[3])] = 3\n                        rg_outputs[rg_outputs >= thrs[3]] = 4                                           \n                        rg_preds = rg_outputs.tolist()\n                        rg_predict = rg_preds[0][0]\n\n\n                        ord_outputs[ord_outputs >= 0.5] = 1\n                        ord_outputs[ord_outputs < 0.5] = 0\n                        ord_predict = sum(ord_outputs[0].cpu().numpy())\n\n                        predict = [rg_predict[0],cls_predict[0],ord_predict]\n                        predict = [most_frequent(predict)]\n                        \n                        result.extend(predict)\n#         print(result)\n        final_predict = most_frequent(result)   \n#         final_predict = round(Average(result))\n#         print(final_predict)\n#         if final_predict < thrs[0]:\n#             final_predict = 0\n#         elif final_predict < thrs[1]:\n#             final_predict = 1\n#         elif final_predict < thrs[2]:\n#             final_predict = 2\n#         elif final_predict < thrs[3]:\n#             final_predict = 3\n#         else:\n#             final_predict = 4\n        writer.writerow({'id_code': file.split('.png')[0],'diagnosis':int(final_predict)})","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}