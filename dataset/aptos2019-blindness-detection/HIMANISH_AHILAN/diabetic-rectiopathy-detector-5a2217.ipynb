{"cells":[{"metadata":{},"cell_type":"markdown","source":"#                            DIABETIC RECTINOPATHY DETECTION\n\n---\n\n\n                         \n                         \n  Diabetic rectiopathy is an disease of eye caused to an person due to diabetes,it is the condition in which person's eye is damaged due to mellitus present in human eye.\n  \n\n\n\n\n> ![](https://afamilyoptician.co.uk/wp-content/uploads/2017/05/diabetic-retinopathy-v01.png)\n\n\n---\n\n\n\n# Diabetic rectiopathy has several stages\n\n---\n\n\n\n\n0 - No DR\n\n1 - Mild\n\n2 - Moderate\n\n3 - Severe\n\n4 - Proliferative DR\n\n\n---\n\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"# **IMPORT LIBRARIES**\n\n\n---\n\n\n\nFirst of all import the required libraries\nI am using fastai library for my project at this time as this an type of image classification problem I am using fatai.vision to solve the problem."},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nfiles = os.listdir(\"../input\")\nprint(files)\nprint('trainlabels.csv' in files)\nprint(len(files))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from fastai import *\nfrom fastai.vision import *\nimport matplotlib as plt\nimport pandas as pd\nfrom fastai.widgets import ClassConfusion\nfrom fastai.widgets import *","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Checking if an GPU is enabled or not\nMake sure that cuda is installed and is available."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Make sure cuda is installed:', torch.cuda.is_available())\nprint('Make sure cudnn is enabled:', torch.backends.cudnn.enabled)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **READ DATA**\n\n\n---\n\n\nRead the data which is given in an csv "},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df =  pd.read_csv(\"../input/aptos2019-blindness-detection/train.csv\")\nvalid_df =  pd.read_csv(\"../input/aptos2019-blindness-detection/test.csv\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Look at the data i.e dataframe of train images and also for the test images."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(train_df))\ntrain_df.head(10)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(valid_df))\nvalid_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Look through the training data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['diagnosis'].hist(figsize = (10, 5))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **TRANSFORMING IMAGES**\n\n\n---\n\n\n\nTransforming the images for bettor model training \n\n\nTransform the images of same type\nExample all the images are initially not of the same size and shape so we have to transform them in specific order for the better trainig the neural network.\nHere it rotate the image either vertical or flip or zoom it according to its need."},{"metadata":{"trusted":true},"cell_type":"code","source":"#tfms = get_tranforms(do_flip=True,)\ntfms=get_transforms(do_flip = True,flip_vert = True,max_rotate=360,max_zoom = 1.1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **CREATING DATABUNCH**\n\n\n---\n\n\n1. Now the data is ready to Bunch them together to fit the model for Trainig.\n2. Here I use ImageList to bunch them together.\n3. Am also splitting them by and random.\n4.  labbelling them according to the indexes.\n5.  and finnaly normalize them here normalization means arranging the things in specific order "},{"metadata":{"trusted":true},"cell_type":"code","source":"data = (ImageList.from_df(train_df,\"../input/aptos2019-blindness-detection/train_images\",suffix='.png').\n       split_by_rand_pct(0.1).\n       label_from_df(1).\n       transform(tfms,size=256).\n       databunch(bs = 20).\n       normalize(imagenet_stats))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.show_batch(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **CREATING LEANER**\n\n\n---\n\n\nmaking an Architecture\ncreate learn(A object for the neural network) here I am choosing Pretrained Model resnet101 for my problem to solve."},{"metadata":{"trusted":true},"cell_type":"code","source":"learn = cnn_learner(data, models.resnet101, metrics=accuracy,model_dir=\"/kaggle/working\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **LEARNING RATE ANALYSIS**\n\n\n---\n\n\nFor finding appropriate learning rate we go through the data once and plot the graph of it."},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.lr_find()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"plot the graph of learning rate finder."},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.recorder.plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **MODLE TRAINING**\n\n\n---\n\n\n\nHere learning rate is slice(2e-5,2e-3) which means first layer will have the learning rate as 2e-5 and last layer(not an output layer) will have learning rate of 2e-3 other all hidden layers will have learning rate of in between 2e-5 to 2e-3"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(5,slice(2e-5,2e-3),wd=0.1,moms=(0.8,0.9))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train a bit more"},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(2,max_lr=slice(2.5e-3),wd=0.1,moms=(0.8,0.9))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(2,slice(2.5e-5),wd=0.01,moms=(0.8,0.9))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(6,max_lr=slice(1e-3,1e-4),wd=0.1,moms=0.9)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **SAVE THE MODULE**\n\n\n---\n\n\nNow we got enough accurate results so save the Trained weights."},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.save('stage-1')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Loading the Trained Model named \"stage-1\""},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.load('stage-1')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Freeze the weights for not furthur modification"},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.freeze()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **PLOT THE LOSSES**\n\n\n---\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.recorder.plot_losses()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"show results will display the result of any batch in this case first batch."},{"metadata":{},"cell_type":"markdown","source":"# **CLASSIFICATION INTERPRETATION**\n\n\n---\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.show_results()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.get_preds()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"interp = ClassificationInterpretation.from_learner(learn)\n\nlosses,idxs = interp.top_losses()\n\nlen(data.valid_ds)==len(losses)==len(idxs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **CONFUSION MATRIX**\n\n\n---\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"interp.plot_confusion_matrix()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"interp.most_confused()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Transforming the images for prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"Tf = partial(Image.apply_tfms,tfms=get_transforms(do_flip=True, flip_vert = True)[0][1:]+get_transforms(do_flip=True, flip_vert = True)[1],size = 512)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv(\"../input/aptos2019-blindness-detection/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sub)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **SINGLE IMAGE TEST**\n\n\n---\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"img = open_image(\"../input/aptos2019-blindness-detection/test_images/01c31b10ab99.png\")\npre = learn.predict(img)\nx = pre[1]\nx = int(x)\nimg.show()\nprint('SERVITY_LEVEL:',(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Predicting the images and writting the prediction on submission.csv file for submission"},{"metadata":{},"cell_type":"markdown","source":"# **ALL TEST IMAGE RESULTS**\n\n\n---\n\n"},{"metadata":{"trusted":true,"_kg_hide-input":false,"_kg_hide-output":false},"cell_type":"code","source":"for i in range(len(sub.id_code)):\n    s=0\n    id = sub.id_code[i]\n    img=open_image(\"../input/aptos2019-blindness-detection/test_images/\"+sub.id_code[i]+\".png\")\n    \"\"\"\n    for i in range(10):\n            Img = Tf(img)\n            p = learn.predict(Img)\n            p = p[1]\n            p = int(p)\n            #print(p) \n            s=s+p\n    \"\"\"\n            \n    Img = Tf(img)\n    s = learn.predict(Img)\n    s = s[1]\n    s = int(s)\n    print(s)\n    sub.diagnosis[i]=s\n    print(sub.diagnosis[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" print(sub) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"**END PROCESSING**"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"7ced6a737d174d78b37621e48a485c70":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":"250px","justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"300px"}},"92dfcb3a243147748dfa3133b7aec878":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":"300px","justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":"hidden","overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"auto"}},"a771f7ff3d2d48e7a72b57b9d1d2a72c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"auto"}},"a8ae1f768faf43d6841efa7df096e2bb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a9e9f69021d84d109d537df267775ec7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"auto"}}},"version_major":2,"version_minor":0}}},"nbformat":4,"nbformat_minor":4}