{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.\n\nimport matplotlib.pyplot as plt\nimport cv2\n\nimport keras\nfrom keras.utils import np_utils\nfrom keras.models import Sequential\nfrom keras.layers.convolutional import Conv2D, MaxPooling2D\nfrom keras.layers.core import Dense, Dropout, Activation, Flatten\nfrom sklearn.model_selection import train_test_split\nfrom PIL import Image\nimport glob\nfrom keras.preprocessing.image import load_img, img_to_array\nfrom keras.initializers import TruncatedNormal, Constant\nfrom keras.layers import Input, Dropout, Flatten, Conv2D, MaxPooling2D, Dense, Activation, BatchNormalization\nfrom keras.optimizers import SGD","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# まずはデータを読み込む。画像のリストが用意されている。\ndf_train=pd.read_csv(\"../input/train.csv\",skiprows=[1,]) \ndf_test=pd.read_csv(\"../input/test.csv\",skiprows=[1,])\n\nprint(df_train)\nprint(df_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# データの内訳を見てみる\n# 軽度のヤツよりちょっとひどくなったヤツの方が多いらしい\n# 自覚症状が出て気づくんだろうな\nCLASSES={0:\"No DR\",1:\"Mild\",2:\"Moderate\",3:\"Severe\",4:\"Proliferative DR\"}\n\ndf_train['diagnosis'].value_counts().plot(kind='bar');\nplt.title('Samples Per Class');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# データを1つ見てみる\nimgPath=f\"../input/train_images/cd54d022e37d.png\"\nimg=cv2.imread(imgPath)\nprint(img.shape)\nplt.imshow(img)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# なんてこった！目玉が蒼井、いや、青い。\n# どうやらRGB記述になっているらしい\nprint(img.shape)\nimg = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\nplt.imshow(img)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ここで一つ、糖尿病性網膜症についてみてみる。\n\nhttp://dmic.ncgm.go.jp/general/about-dm/060/050/01.html\n\nによると、どうやら病気が進行すると、症状が斑点や出血として現れるようだ。\nつまり、周りより明るい、あるいは暗い部分が怪しいらしい。\nそれなら白黒画像で事足りるな。\nめんどくせぇから、そういうことにしようぜ。"},{"metadata":{"trusted":true},"cell_type":"code","source":"img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\nprint(img.shape)\nplt.imshow(img,cmap='gray')\nplt.show()\nprint(img)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"若干気になるのが、眼球以外の余白部分だ。\nこいつは学習とは関係ねぇ。\n除けちまおう。\n適当に黒っぽいところを除ければいいだろ。"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 画像の上下左右端を得る\ndef getEyeArea(image):\n    print(image.shape)\n    left = 0\n    right = image.shape[1] - 1\n    top = 0\n    bottom = image.shape[0] - 1\n    # 左端を求める\n    for i in range(0, image.shape[1], 1):\n        for j in range(0, image.shape[0], 1):\n            if image[j][i] > 10:\n                left = i\n                break\n        else:\n             continue\n        break\n    \n    #print(left)\n    \n    # 右端を求める\n    for i in range(image.shape[1]-1, 0, -1):\n        for j in range(0, image.shape[0], 1):\n            if image[j][i] > 10:\n                right = i\n                break\n        else:\n             continue\n        break\n    \n    #print(right)\n    \n    # 上端を求める\n    for j in range(0, image.shape[0], 1):\n        for i in range(0, image.shape[1], 1):\n            if image[j][i] > 10:\n                top = j\n                break\n        else:\n             continue\n        break\n    \n    #print(top)\n        \n    # 下端を求める\n    for j in range(image.shape[0]-1, 0, -1):\n        for i in range(0, image.shape[1], 1):\n            if image[j][i] > 10:\n                bottom = j\n                break\n        else:\n             continue\n        break\n    \n    #print(bottom)\n    \n    return left, right, top, bottom\n    \nleft, right, top, bottom = getEyeArea(img)\nimg_cut = img[top:bottom,left:right]\nprint(img_cut.shape)\nplt.imshow(img_cut,cmap='gray')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"画像がやや大きいらしい。\n大きすぎると入らない可能性がある。"},{"metadata":{"trusted":true},"cell_type":"code","source":"resize_num_test = 150\nimg_resize = cv2.resize(img_cut, dsize=(resize_num_test, resize_num_test))\nprint(img_resize.shape)\nplt.imshow(img_resize,cmap='gray')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"https://pchun.work/resnet%E3%82%92fine-tuning%E3%81%97%E3%81%A6%E8%87%AA%E5%88%86%E3%81%8C%E7%94%A8%E6%84%8F%E3%81%97%E3%81%9F%E7%94%BB%E5%83%8F%E3%82%92%E5%AD%A6%E7%BF%92%E3%81%95%E3%81%9B%E3%82%8B/"},{"metadata":{},"cell_type":"markdown","source":"とりあえず、適当にモデルを作って入れてみる。\n\nhttps://qiita.com/Phoeboooo/items/cfe8560fe8a285855340"},{"metadata":{},"cell_type":"markdown","source":"適当に画像を読み込んでリストにする。\nDataGeneratorとかそういう難しいのはよく理解してねぇ。\n\n↑精度がカスほども上がらない件。\n状況にあってない。"},{"metadata":{},"cell_type":"markdown","source":"画像をリサイズする。"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.image import load_img, img_to_array, array_to_img\nfrom keras.utils.np_utils import to_categorical\nimport csv\n\nimage_list = []\nlabel_list = []\n\nresize_num = 224\n\ntake = 0\n\n#print(df_train.values)\nfor i in range(0, len(df_train), 1):\n    #print(df_train.values[i,0])\n    imgPath=\"../input/train_images/\" + str(df_train.values[i,0]) + \".png\" \n    print(imgPath)\n    img = cv2.imread(str(imgPath))\n    #plt.imshow(img)\n    #plt.show()\n    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    left, right, top, bottom = getEyeArea(img)\n    img_cut = img[top:bottom,left:right]\n    img_resize = cv2.resize(img_cut, dsize=(resize_num, resize_num))\n    img_reshape = np.reshape(img_resize, (resize_num, resize_num, 1))\n    image_list.append(img_reshape)\n    label_list.append(str(df_train.values[i,1]))\n    take = take + 1\n    print(take)\n    \nimages = np.array(image_list)\nlabels = to_categorical(label_list)\n\n# imageの画素値をint型からfloat型にする\nimages = images.astype('float32')\n# 画素値を[0～255]⇒[0～1]とする\nimages = images / 255.0\n\nprint(labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"data frame形式からはvaluesで取らないと上手くいかないらしい。\n上手くいかなくてキレた。"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"やっとこさ学習かよ。\n\nhttps://qiita.com/hiroeorz@github/items/ecb39ed4042ebdc0a957"},{"metadata":{},"cell_type":"markdown","source":"alexnetに挑戦する\n\nhttp://tecsingularity.com/tensorflow/keras_alexnet/"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# GrayScaleのときに1、COLORのときに3にする\nCOLOR_CHANNEL = 1\n\n# 入力画像サイズ(画像サイズは正方形とする)\nINPUT_IMAGE_SIZE = 224\n\n# 訓練時のバッチサイズとエポック数\nBATCH_SIZE = 32\nEPOCH_NUM = 100\n\n# CLASS数を取得する\nCLASS_NUM = len(CLASSES)\nprint(\"クラス数 : \" + str(CLASS_NUM))\n\n\ndef conv2d(filters, kernel_size, strides=1, bias_init=1, **kwargs):\n    trunc = TruncatedNormal(mean=0.0, stddev=0.01)\n    cnst = Constant(value=bias_init)\n    return Conv2D(\n        filters,\n        kernel_size,\n        strides=strides,\n        padding='same',\n        activation='relu',\n        kernel_initializer=trunc,\n        bias_initializer=cnst,\n        **kwargs\n    )\n\ndef dense(units, **kwargs):\n    trunc = TruncatedNormal(mean=0.0, stddev=0.01)\n    cnst = Constant(value=1)\n    return Dense(\n        units,\n        activation='tanh',\n        kernel_initializer=trunc,\n        bias_initializer=cnst,\n        **kwargs\n    )\n\ndef AlexNet():\n    model = Sequential()\n\n    # 第1畳み込み層\n    model.add(conv2d(96, 11, strides=(4,4), bias_init=0, input_shape=(INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE, COLOR_CHANNEL)))\n    model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n    model.add(BatchNormalization())\n\n    # 第2畳み込み層\n    model.add(conv2d(256, 5, bias_init=1))\n    model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n    model.add(BatchNormalization())\n\n    # 第3~5畳み込み層\n    model.add(conv2d(384, 3, bias_init=0))\n    model.add(conv2d(384, 3, bias_init=1))\n    model.add(conv2d(256, 3, bias_init=1))\n    model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n    model.add(BatchNormalization())\n\n    # 全結合層\n    model.add(Flatten())\n    model.add(dense(4096))\n    model.add(Dropout(0.5))\n    model.add(dense(4096))\n    model.add(Dropout(0.5))\n\n    # 出力層\n    model.add(Dense(CLASS_NUM, activation='softmax'))\n\n    model.compile(optimizer=SGD(lr=0.01), loss='categorical_crossentropy', metrics=['accuracy'])\n    return model\n\n# コンパイル\nmodel = AlexNet()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(images, labels, nb_epoch=100, batch_size=100, validation_split=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_list_test = []\nlabel_list_test = []\nresults = []\n\npath = \"../submission.csv\"\n\nresults.append(\"id_code,diagnosis\")\n\ntake = 0\n\n#print(df_train.values)\nfor i in range(0, len(df_test), 1):\n    #print(df_train.values[i,0])\n    print(i)\n    imgPath=\"../input/test_images/\" + str(df_test.values[i,0]) + \".png\" \n    print(imgPath)\n    img = cv2.imread(str(imgPath))\n    #plt.imshow(img)\n    #plt.show()\n    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    left, right, top, bottom = getEyeArea(img)\n    img_cut = img[top:bottom,left:right]\n    img_resize = cv2.resize(img_cut, dsize=(resize_num, resize_num))\n    img_reshape = np.reshape(img_resize, (resize_num, resize_num, 1))\n    target = np.array(img_reshape, dtype=np.float32)\n    target = target / 255\n    target = target[None, ...]\n    #print(target_array.shape)\n    predict = model.predict(target, batch_size=1, verbose=0)\n    score = np.max(predict)\n    pred_label = np.argmax(predict)\n    image_list_test.append(str(df_test.values[i,0]))\n    label_list_test.append(str(pred_label))\n    result = str(df_test.values[i,0]) + \",\" + str(pred_label)\n    results.append(result)\n\nprint(results)\n    \nStackingSubmission = pd.DataFrame({ 'id_code': image_list_test,'diagnosis': label_list_test })\nStackingSubmission.to_csv(\"../submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.remove(\"../input/submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 結果のCSVを確認したい。\ndf_result=pd.read_csv(\"../submission.csv\",skiprows=[1,])\n\nprint(df_result)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}