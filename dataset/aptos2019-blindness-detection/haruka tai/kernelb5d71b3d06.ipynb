{"cells":[{"metadata":{},"cell_type":"markdown","source":"# import lib"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfrom keras.applications.xception import Xception\nfrom keras.models import Model\nfrom keras.layers import Input, Activation, Dropout, Flatten, Dense, GlobalAveragePooling2D\nfrom keras import optimizers\nfrom keras.callbacks import ModelCheckpoint\n\nfrom keras.utils import np_utils\n\nfrom keras.models import load_model\n\nimport cv2\n\nfrom keras import backend as K\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import rgb_to_hsv, hsv_to_rgb\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nimport gc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# load image and label"},{"metadata":{"trusted":true},"cell_type":"code","source":"img = np.loadtxt(\"../input/aptos2019-blindness-detection/train.csv\",       # 読み込みたいファイルのパス\n                  delimiter=\",\",    # ファイルの区切り文字\n                  skiprows=1,    # 先頭の何行を無視するか（指定した行数までは読み込まない）\n                  usecols=(0), # 読み込みたい列番号\n                  dtype = \"str\"\n                 )\nimg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label = np.loadtxt(\"../input/aptos2019-blindness-detection/train.csv\",       # 読み込みたいファイルのパス\n                  delimiter=\",\",    # ファイルの区切り文字\n                  skiprows=1,    # 先頭の何行を無視するか（指定した行数までは読み込まない）\n                  usecols=(1), # 読み込みたい列番号\n                  dtype = \"int\"\n                 )\nlabel","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# make unsamble dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"img_label_trains = []\nimg_label_validations = []\n\nfor i in range(4):\n    data_train, data_test, labels_train, labels_test = train_test_split(img, label, train_size=0.85,random_state=i*5,stratify=label)\n    \n    img_label_train = np.stack([data_train, labels_train],axis=1)\n    img_label_validation = np.stack([data_test, labels_test],axis=1)\n    \n    img_label_trains.append(img_label_train)\n    img_label_validations.append(img_label_validation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#confirm dataset count(train)\nprint(np.count_nonzero(img_label_trains[0][:,1] == \"0\"))\nprint(np.count_nonzero(img_label_trains[0][:,1] == \"1\"))\nprint(np.count_nonzero(img_label_trains[0][:,1] == \"2\"))\nprint(np.count_nonzero(img_label_trains[0][:,1] == \"3\"))\nprint(np.count_nonzero(img_label_trains[0][:,1] == \"4\"))\n#confirm dataset count\nprint(np.count_nonzero(img_label_validations[0][:,1] == \"0\"))\nprint(np.count_nonzero(img_label_validations[0][:,1] == \"1\"))\nprint(np.count_nonzero(img_label_validations[0][:,1] == \"2\"))\nprint(np.count_nonzero(img_label_validations[0][:,1] == \"3\"))\nprint(np.count_nonzero(img_label_validations[0][:,1] == \"4\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(img_label_trains[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(img_label_validations[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# data augmatation"},{"metadata":{"trusted":true},"cell_type":"code","source":"def vertical_flip(image, rate=0.5):\n    if np.random.rand() < rate:\n        image = image[::-1, :, :]\n    return image\n\ndef horizontal_flip(image):\n    image = image[:, ::-1, :]\n    return image\n\ndef image_translation(img):\n    params = np.random.randint(-50, 51)\n    if not isinstance(params, list):\n        params = [params, params]\n    rows, cols, ch = img.shape\n\n    M = np.float32([[1, 0, params[0]], [0, 1, params[1]]])\n    dst = cv2.warpAffine(img, M, (cols, rows))\n    return dst\n\ndef image_shear(img):\n    params = np.random.randint(-20, 21)*0.01\n    rows, cols, ch = img.shape\n    factor = params*(-1.0)\n    M = np.float32([[1, factor, 0], [0, 1, 0]])\n    dst = cv2.warpAffine(img, M, (cols, rows))\n    return dst\n\ndef image_rotation(img):\n    params = np.random.randint(-30, 31)\n    rows, cols, ch = img.shape\n    M = cv2.getRotationMatrix2D((cols/2, rows/2), params, 1)\n    dst = cv2.warpAffine(img, M, (cols, rows))\n    return dst\n\ndef image_contrast(img):\n    params = np.random.randint(7, 10)*0.1\n    alpha = params\n    new_img = cv2.multiply(img, np.array([alpha]))                    # mul_img = img*alpha\n    #new_img = cv2.add(mul_img, beta)                                  # new_img = img*alpha + beta\n  \n    return new_img\n\ndef image_brightness2(img):\n    params = np.random.randint(-21, 22)\n    beta = params\n    b, g, r = cv2.split(img)\n    b = cv2.add(b, beta)\n    g = cv2.add(g, beta)\n    r = cv2.add(r, beta)\n    new_img = cv2.merge((b, g, r))\n    return new_img\n\ndef pca_color_augmentation_modify(image_array_input):\n    assert image_array_input.ndim == 3 and image_array_input.shape[2] == 3\n    assert image_array_input.dtype == np.uint8\n\n    img = image_array_input.reshape(-1, 3).astype(np.float32)\n    # 分散を計算\n    ch_var = np.var(img, axis=0)\n    # 分散の合計が3になるようにスケーリング\n    scaling_factor = np.sqrt(3.0 / sum(ch_var))\n    # 平均で引いてスケーリング\n    img = (img - np.mean(img, axis=0)) * scaling_factor\n\n    cov = np.cov(img, rowvar=False)\n    lambd_eigen_value, p_eigen_vector = np.linalg.eig(cov)\n\n    rand = np.random.randn(3) * 0.1\n    delta = np.dot(p_eigen_vector, rand*lambd_eigen_value)\n    delta = (delta * 255.0).astype(np.int32)[np.newaxis, np.newaxis, :]\n\n    img_out = np.clip(image_array_input + delta, 0, 255).astype(np.uint8)\n    return img_out","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# data generator"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_random_data(image_lines_1, abs_path, img_width, img_height, data_aug):\n    image_file = abs_path + image_lines_1[0] + \".png\"\n    label = np.eye(5)[int(image_lines_1[1])]\n    \n    seed_image = cv2.imread(image_file)\n    seed_image = cv2.cvtColor(seed_image, cv2.COLOR_BGR2RGB)\n    seed_image = cv2.resize(seed_image, dsize=(img_width, img_height))\n    \n    if data_aug:\n        \n        r = np.random.rand()\n        \n        if r >= 0.5:\n    \n            seed_image = vertical_flip(seed_image)\n            seed_image = horizontal_flip(seed_image)\n            seed_image = image_shear(seed_image)\n            seed_image = image_rotation(seed_image)\n            seed_image = pca_color_augmentation_modify(seed_image)\n    \n    seed_image = seed_image / 255\n    \n    return seed_image, label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def data_generator(image_lines, batch_size, abs_path, img_width, img_height, data_aug):\n    '''data generator for fit_generator'''\n    n = len(image_lines)\n    i = 0\n    while True:\n        image_data = []\n        label_data = []\n        for b in range(batch_size):\n            if i==0:\n                np.random.shuffle(image_lines)\n            image, label = get_random_data(image_lines[i], abs_path, img_width, img_height, data_aug)\n            image_data.append(image)\n            label_data.append(label)\n            i = (i+1) % n\n        image_data = np.array(image_data)\n        label_data = np.array(label_data)\n        yield image_data, label_data\n\ndef data_generator_wrapper(image_lines, batch_size, abs_path, img_width, img_height, data_aug):\n    n = len(image_lines)\n    if n==0 or batch_size<=0: return None\n    return data_generator(image_lines, batch_size, abs_path, img_width, img_height, data_aug)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# make model"},{"metadata":{"trusted":true},"cell_type":"code","source":"img_width, img_height = 449, 449\nnum_train = len(img_label_trains[0])\nnum_val = len(img_label_validations[0])\nbatch_size = 4\nprint(num_train, num_val)\nabs_path = \"../input/aptos2019-blindness-detection/train_images/\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = []\n\nfor i in range(4):\n\n    input_tensor = Input(shape=(img_height, img_width, 3))\n\n    xception_model = Xception(include_top=False, weights=None, input_tensor=input_tensor)\n\n    xception_model.load_weights(\"../input/keras-pretrained-models/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\")\n\n    x = xception_model.output\n    x = GlobalAveragePooling2D()(x)\n    x = Dense(1024, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    outputs = Dense(5, activation='softmax')(x)\n\n    model = Model(inputs=xception_model.input, outputs=outputs)\n    \n    model.compile(optimizer=optimizers.SGD(lr=0.001,momentum=0.9),\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\n    model.summary()\n    \n    models.append(model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# train"},{"metadata":{"trusted":true},"cell_type":"code","source":"models[0].fit_generator(data_generator_wrapper(img_label_trains[0], batch_size, abs_path, img_width, img_height, True),\n        steps_per_epoch=max(1, num_train//batch_size),\n        validation_data=data_generator_wrapper(img_label_validations[0], batch_size, abs_path, img_width, img_height, True),\n        validation_steps=max(1, num_val//batch_size),\n        epochs=5,\n        initial_epoch=0,\n        class_weight=[1,4.8,1.8,9.1,6.3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models[0].fit_generator(data_generator_wrapper(img_label_trains[0], batch_size, abs_path, img_width, img_height, True),\n        steps_per_epoch=max(1, num_train//batch_size),\n        validation_data=data_generator_wrapper(img_label_validations[0], batch_size, abs_path, img_width, img_height, True),\n        validation_steps=max(1, num_val//batch_size),\n        epochs=5,\n        initial_epoch=0,\n        class_weight=[1,4.8,1.8,9.1,6.3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models[0].fit_generator(data_generator_wrapper(img_label_trains[0], batch_size, abs_path, img_width, img_height, True),\n        steps_per_epoch=max(1, num_train//batch_size),\n        validation_data=data_generator_wrapper(img_label_validations[0], batch_size, abs_path, img_width, img_height, True),\n        validation_steps=max(1, num_val//batch_size),\n        epochs=5,\n        initial_epoch=0,\n        class_weight=[1,4.8,1.8,9.1,6.3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models[0].fit_generator(data_generator_wrapper(img_label_trains[0], batch_size, abs_path, img_width, img_height, True),\n        steps_per_epoch=max(1, num_train//batch_size),\n        validation_data=data_generator_wrapper(img_label_validations[0], batch_size, abs_path, img_width, img_height, True),\n        validation_steps=max(1, num_val//batch_size),\n        epochs=5,\n        initial_epoch=0,\n        class_weight=[1,4.8,1.8,9.1,6.3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models[1].fit_generator(data_generator_wrapper(img_label_trains[1], batch_size, abs_path, img_width, img_height, True),\n        steps_per_epoch=max(1, num_train//batch_size),\n        validation_data=data_generator_wrapper(img_label_validations[1], batch_size, abs_path, img_width, img_height, True),\n        validation_steps=max(1, num_val//batch_size),\n        epochs=5,\n        initial_epoch=0,\n        class_weight=[1,4.8,1.8,9.1,6.3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models[1].fit_generator(data_generator_wrapper(img_label_trains[1], batch_size, abs_path, img_width, img_height, True),\n        steps_per_epoch=max(1, num_train//batch_size),\n        validation_data=data_generator_wrapper(img_label_validations[1], batch_size, abs_path, img_width, img_height, True),\n        validation_steps=max(1, num_val//batch_size),\n        epochs=5,\n        initial_epoch=0,\n        class_weight=[1,4.8,1.8,9.1,6.3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models[1].fit_generator(data_generator_wrapper(img_label_trains[1], batch_size, abs_path, img_width, img_height, True),\n        steps_per_epoch=max(1, num_train//batch_size),\n        validation_data=data_generator_wrapper(img_label_validations[1], batch_size, abs_path, img_width, img_height, True),\n        validation_steps=max(1, num_val//batch_size),\n        epochs=5,\n        initial_epoch=0,\n        class_weight=[1,4.8,1.8,9.1,6.3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models[1].fit_generator(data_generator_wrapper(img_label_trains[1], batch_size, abs_path, img_width, img_height, True),\n        steps_per_epoch=max(1, num_train//batch_size),\n        validation_data=data_generator_wrapper(img_label_validations[1], batch_size, abs_path, img_width, img_height, True),\n        validation_steps=max(1, num_val//batch_size),\n        epochs=5,\n        initial_epoch=0,\n        class_weight=[1,4.8,1.8,9.1,6.3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models[2].fit_generator(data_generator_wrapper(img_label_trains[2], batch_size, abs_path, img_width, img_height, True),\n        steps_per_epoch=max(1, num_train//batch_size),\n        validation_data=data_generator_wrapper(img_label_validations[2], batch_size, abs_path, img_width, img_height, True),\n        validation_steps=max(1, num_val//batch_size),\n        epochs=5,\n        initial_epoch=0,\n        class_weight=[1,4.8,1.8,9.1,6.3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models[2].fit_generator(data_generator_wrapper(img_label_trains[2], batch_size, abs_path, img_width, img_height, True),\n        steps_per_epoch=max(1, num_train//batch_size),\n        validation_data=data_generator_wrapper(img_label_validations[2], batch_size, abs_path, img_width, img_height, True),\n        validation_steps=max(1, num_val//batch_size),\n        epochs=5,\n        initial_epoch=0,\n        class_weight=[1,4.8,1.8,9.1,6.3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models[2].fit_generator(data_generator_wrapper(img_label_trains[2], batch_size, abs_path, img_width, img_height, True),\n        steps_per_epoch=max(1, num_train//batch_size),\n        validation_data=data_generator_wrapper(img_label_validations[2], batch_size, abs_path, img_width, img_height, True),\n        validation_steps=max(1, num_val//batch_size),\n        epochs=5,\n        initial_epoch=0,\n        class_weight=[1,4.8,1.8,9.1,6.3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models[2].fit_generator(data_generator_wrapper(img_label_trains[2], batch_size, abs_path, img_width, img_height, True),\n        steps_per_epoch=max(1, num_train//batch_size),\n        validation_data=data_generator_wrapper(img_label_validations[2], batch_size, abs_path, img_width, img_height, True),\n        validation_steps=max(1, num_val//batch_size),\n        epochs=5,\n        initial_epoch=0,\n        class_weight=[1,4.8,1.8,9.1,6.3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models[3].fit_generator(data_generator_wrapper(img_label_trains[2], batch_size, abs_path, img_width, img_height, True),\n        steps_per_epoch=max(1, num_train//batch_size),\n        validation_data=data_generator_wrapper(img_label_validations[2], batch_size, abs_path, img_width, img_height, True),\n        validation_steps=max(1, num_val//batch_size),\n        epochs=5,\n        initial_epoch=0,\n        class_weight=[1,4.8,1.8,9.1,6.3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models[3].fit_generator(data_generator_wrapper(img_label_trains[2], batch_size, abs_path, img_width, img_height, True),\n        steps_per_epoch=max(1, num_train//batch_size),\n        validation_data=data_generator_wrapper(img_label_validations[2], batch_size, abs_path, img_width, img_height, True),\n        validation_steps=max(1, num_val//batch_size),\n        epochs=5,\n        initial_epoch=0,\n        class_weight=[1,4.8,1.8,9.1,6.3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models[3].fit_generator(data_generator_wrapper(img_label_trains[2], batch_size, abs_path, img_width, img_height, True),\n        steps_per_epoch=max(1, num_train//batch_size),\n        validation_data=data_generator_wrapper(img_label_validations[2], batch_size, abs_path, img_width, img_height, True),\n        validation_steps=max(1, num_val//batch_size),\n        epochs=5,\n        initial_epoch=0,\n        class_weight=[1,4.8,1.8,9.1,6.3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models[3].fit_generator(data_generator_wrapper(img_label_trains[2], batch_size, abs_path, img_width, img_height, True),\n        steps_per_epoch=max(1, num_train//batch_size),\n        validation_data=data_generator_wrapper(img_label_validations[2], batch_size, abs_path, img_width, img_height, True),\n        validation_steps=max(1, num_val//batch_size),\n        epochs=5,\n        initial_epoch=0,\n        class_weight=[1,4.8,1.8,9.1,6.3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img_test = np.loadtxt(\"../input/aptos2019-blindness-detection/test.csv\",       # 読み込みたいファイルのパス\n                  delimiter=\",\",    # ファイルの区切り文字\n                  skiprows=1,    # 先頭の何行を無視するか（指定した行数までは読み込まない）\n#                  usecols=(1), # 読み込みたい列番号\n                  dtype = \"str\"\n                 )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_abs_path = \"../input/aptos2019-blindness-detection/test_images/\"\n\ndata = []\nfor i in range(len(img_test)):\n    image_file = test_abs_path + img_test[i] + \".png\"\n    seed_image = cv2.imread(image_file)\n    seed_image = cv2.cvtColor(seed_image, cv2.COLOR_BGR2RGB)\n    seed_image = cv2.resize(seed_image, dsize=(img_width, img_height))\n    seed_image = np.expand_dims(seed_image, axis=0)\n    seed_image = seed_image / 255\n    predict1 = models[0].predict(seed_image)\n    predict2 = models[1].predict(seed_image)\n    predict3 = models[2].predict(seed_image)\n    predict4 = models[3].predict(seed_image)\n    predict_mean = (predict1+predict2+predict3+predict4)/4\n    x = np.array([img_test[i], np.argmax(predict_mean)])\n    data.append(x)\n    \ndata = np.array(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = ['id_code', 'diagnosis']\nname = 'sample'\n\nd = pd.DataFrame(data=data, columns=columns, dtype='str')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d.to_csv(\"submission.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"submission.csv\")\nprint(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"}},"nbformat":4,"nbformat_minor":1}