{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# -*- coding: utf-8 -*-\nfrom __future__ import print_function, division, absolute_import\nimport torchvision.models as models\nimport torch.utils.model_zoo as model_zoo\nimport torch.nn.functional as F\nimport types\nimport re\n\n#################################################################\n# You can find the definitions of those models here:\n# https://github.com/pytorch/vision/blob/master/torchvision/models\n#\n# To fit the API, we usually added/redefined some methods and\n# renamed some attributs (see below for each models).\n#\n# However, you usually do not need to see the original model\n# definition from torchvision. Just use `print(model)` to see\n# the modules and see bellow the `model.features` and\n# `model.classifier` definitions.\n#################################################################\n\n__all__ = [\n    'alexnet',\n    'densenet121', 'densenet169', 'densenet201', 'densenet161',\n    'resnet18', 'resnet34', 'resnet50', 'resnet101', 'resnet152',\n    'inceptionv3',\n    'squeezenet1_0', 'squeezenet1_1',\n    'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn',\n    'vgg19_bn', 'vgg19'\n]\n\nmodel_urls = {\n    'alexnet': 'https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth',\n    'densenet121': 'http://data.lip6.fr/cadene/pretrainedmodels/densenet121-fbdb23505.pth',\n    'densenet169': 'http://data.lip6.fr/cadene/pretrainedmodels/densenet169-f470b90a4.pth',\n    'densenet201': 'http://data.lip6.fr/cadene/pretrainedmodels/densenet201-5750cbb1e.pth',\n    'densenet161': 'http://data.lip6.fr/cadene/pretrainedmodels/densenet161-347e6b360.pth',\n    'inceptionv3': 'https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth',\n    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n    'squeezenet1_0': 'https://download.pytorch.org/models/squeezenet1_0-a815701f.pth',\n    'squeezenet1_1': 'https://download.pytorch.org/models/squeezenet1_1-f364aa15.pth',\n    'vgg11': 'https://download.pytorch.org/models/vgg11-bbd30ac9.pth',\n    'vgg13': 'https://download.pytorch.org/models/vgg13-c768596a.pth',\n    'vgg16': 'https://download.pytorch.org/models/vgg16-397923af.pth',\n    'vgg19': 'https://download.pytorch.org/models/vgg19-dcbb9e9d.pth',\n    'vgg11_bn': 'https://download.pytorch.org/models/vgg11_bn-6002323d.pth',\n    'vgg13_bn': 'https://download.pytorch.org/models/vgg13_bn-abd245e5.pth',\n    'vgg16_bn': 'https://download.pytorch.org/models/vgg16_bn-6c64b313.pth',\n    'vgg19_bn': 'https://download.pytorch.org/models/vgg19_bn-c79401a0.pth',\n    # 'vgg16_caffe': 'https://s3-us-west-2.amazonaws.com/jcjohns-models/vgg16-00b39a1b.pth',\n    # 'vgg19_caffe': 'https://s3-us-west-2.amazonaws.com/jcjohns-models/vgg19-d01eb7cb.pth'\n}\n\ninput_sizes = {}\nmeans = {}\nstds = {}\n\nfor model_name in __all__:\n    input_sizes[model_name] = [3, 224, 224]\n    means[model_name] = [0.485, 0.456, 0.406]\n    stds[model_name] = [0.229, 0.224, 0.225]\n\nfor model_name in ['inceptionv3']:\n    input_sizes[model_name] = [3, 299, 299]\n    means[model_name] = [0.5, 0.5, 0.5]\n    stds[model_name] = [0.5, 0.5, 0.5]\n\npretrained_settings = {}\n\nfor model_name in __all__:\n    pretrained_settings[model_name] = {\n        'imagenet': {\n            'url': model_urls[model_name],\n            'input_space': 'RGB',\n            'input_size': input_sizes[model_name],\n            'input_range': [0, 1],\n            'mean': means[model_name],\n            'std': stds[model_name],\n            'num_classes': 1000\n        }\n    }\n\n# for model_name in ['vgg16', 'vgg19']:\n#     pretrained_settings[model_name]['imagenet_caffe'] = {\n#         'url': model_urls[model_name + '_caffe'],\n#         'input_space': 'BGR',\n#         'input_size': input_sizes[model_name],\n#         'input_range': [0, 255],\n#         'mean': [103.939, 116.779, 123.68],\n#         'std': [1., 1., 1.],\n#         'num_classes': 1000\n#     }\n\ndef update_state_dict(state_dict):\n    # '.'s are no longer allowed in module names, but pervious _DenseLayer\n    # has keys 'norm.1', 'relu.1', 'conv.1', 'norm.2', 'relu.2', 'conv.2'.\n    # They are also in the checkpoints in model_urls. This pattern is used\n    # to find such keys.\n    pattern = re.compile(\n        r'^(.*denselayer\\d+\\.(?:norm|relu|conv))\\.((?:[12])\\.(?:weight|bias|running_mean|running_var))$')\n    for key in list(state_dict.keys()):\n        res = pattern.match(key)\n        if res:\n            new_key = res.group(1) + res.group(2)\n            state_dict[new_key] = state_dict[key]\n            del state_dict[key]\n    return state_dict\n\ndef load_pretrained(model, num_classes, settings):\n    assert num_classes == settings['num_classes'], \\\n        \"num_classes should be {}, but is {}\".format(settings['num_classes'], num_classes)\n    state_dict = model_zoo.load_url(settings['url'])\n    state_dict = update_state_dict(state_dict)\n    model.load_state_dict(state_dict)\n    model.input_space = settings['input_space']\n    model.input_size = settings['input_size']\n    model.input_range = settings['input_range']\n    model.mean = settings['mean']\n    model.std = settings['std']\n    return model\n\n#################################################################\n# AlexNet\n\ndef modify_alexnet(model):\n    # Modify attributs\n    model._features = model.features\n    del model.features\n    model.dropout0 = model.classifier[0]\n    model.linear0 = model.classifier[1]\n    model.relu0 = model.classifier[2]\n    model.dropout1 = model.classifier[3]\n    model.linear1 = model.classifier[4]\n    model.relu1 = model.classifier[5]\n    model.last_linear = model.classifier[6]\n    del model.classifier\n\n    def features(self, input):\n        x = self._features(input)\n        x = x.view(x.size(0), 256 * 6 * 6)\n        x = self.dropout0(x)\n        x = self.linear0(x)\n        x = self.relu0(x)\n        x = self.dropout1(x)\n        x = self.linear1(x)\n        return x\n\n    def logits(self, features):\n        x = self.relu1(features)\n        x = self.last_linear(x)\n        return x\n\n    def forward(self, input):\n        x = self.features(input)\n        x = self.logits(x)\n        return x\n\n    # Modify methods\n    model.features = types.MethodType(features, model)\n    model.logits = types.MethodType(logits, model)\n    model.forward = types.MethodType(forward, model)\n    return model\n\ndef alexnet(num_classes=1000, pretrained='imagenet'):\n    r\"\"\"AlexNet model architecture from the\n    `\"One weird trick...\" <https://arxiv.org/abs/1404.5997>`_ paper.\n    \"\"\"\n    # https://github.com/pytorch/vision/blob/master/torchvision/models/alexnet.py\n    model = models.alexnet(pretrained=False)\n    if pretrained is not None:\n        settings = pretrained_settings['alexnet'][pretrained]\n        model = load_pretrained(model, num_classes, settings)\n    model = modify_alexnet(model)\n    return model\n\n###############################################################\n#Â DenseNets\n\ndef modify_densenets(model):\n    # Modify attributs\n    model.last_linear = model.classifier\n    del model.classifier\n\n    def logits(self, features):\n        x = F.relu(features, inplace=True)\n        x = F.avg_pool2d(x, kernel_size=7, stride=1)\n        x = x.view(x.size(0), -1)\n        x = self.last_linear(x)\n        return x\n\n    def forward(self, input):\n        x = self.features(input)\n        x = self.logits(x)\n        return x\n\n    # Modify methods\n    model.logits = types.MethodType(logits, model)\n    model.forward = types.MethodType(forward, model)\n    return model\n\ndef densenet121(num_classes=1000, pretrained='imagenet'):\n    r\"\"\"Densenet-121 model from\n    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`\n    \"\"\"\n    model = models.densenet121(pretrained=False)\n    if pretrained is not None:\n        settings = pretrained_settings['densenet121'][pretrained]\n        model = load_pretrained(model, num_classes, settings)\n    model = modify_densenets(model)\n    return model\n\ndef densenet169(num_classes=1000, pretrained='imagenet'):\n    r\"\"\"Densenet-169 model from\n    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`\n    \"\"\"\n    model = models.densenet169(pretrained=False)\n    if pretrained is not None:\n        settings = pretrained_settings['densenet169'][pretrained]\n        model = load_pretrained(model, num_classes, settings)\n    model = modify_densenets(model)\n    return model\n\ndef densenet201(num_classes=1000, pretrained='imagenet'):\n    r\"\"\"Densenet-201 model from\n    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`\n    \"\"\"\n    model = models.densenet201(pretrained=False)\n    if pretrained is not None:\n        settings = pretrained_settings['densenet201'][pretrained]\n        model = load_pretrained(model, num_classes, settings)\n    model = modify_densenets(model)\n    return model\n\ndef densenet161(num_classes=1000, pretrained='imagenet'):\n    r\"\"\"Densenet-161 model from\n    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`\n    \"\"\"\n    model = models.densenet161(pretrained=False)\n    if pretrained is not None:\n        settings = pretrained_settings['densenet161'][pretrained]\n        model = load_pretrained(model, num_classes, settings)\n    model = modify_densenets(model)\n    return model\n\n###############################################################\n#Â InceptionV3\n\ndef inceptionv3(num_classes=1000, pretrained='imagenet'):\n    r\"\"\"Inception v3 model architecture from\n    `\"Rethinking the Inception Architecture for Computer Vision\" <http://arxiv.org/abs/1512.00567>`_.\n    \"\"\"\n    model = models.inception_v3(pretrained=False)\n    if pretrained is not None:\n        settings = pretrained_settings['inceptionv3'][pretrained]\n        model = load_pretrained(model, num_classes, settings)\n\n    # Modify attributs\n    model.last_linear = model.fc\n    del model.fc\n\n    def features(self, input):\n        # 299 x 299 x 3\n        x = self.Conv2d_1a_3x3(input) # 149 x 149 x 32\n        x = self.Conv2d_2a_3x3(x) # 147 x 147 x 32\n        x = self.Conv2d_2b_3x3(x) # 147 x 147 x 64\n        x = F.max_pool2d(x, kernel_size=3, stride=2) # 73 x 73 x 64\n        x = self.Conv2d_3b_1x1(x) # 73 x 73 x 80\n        x = self.Conv2d_4a_3x3(x) # 71 x 71 x 192\n        x = F.max_pool2d(x, kernel_size=3, stride=2) # 35 x 35 x 192\n        x = self.Mixed_5b(x) # 35 x 35 x 256\n        x = self.Mixed_5c(x) # 35 x 35 x 288\n        x = self.Mixed_5d(x) # 35 x 35 x 288\n        x = self.Mixed_6a(x) # 17 x 17 x 768\n        x = self.Mixed_6b(x) # 17 x 17 x 768\n        x = self.Mixed_6c(x) # 17 x 17 x 768\n        x = self.Mixed_6d(x) # 17 x 17 x 768\n        x = self.Mixed_6e(x) # 17 x 17 x 768\n        if self.training and self.aux_logits:\n            self._out_aux = self.AuxLogits(x) # 17 x 17 x 768\n        x = self.Mixed_7a(x) # 8 x 8 x 1280\n        x = self.Mixed_7b(x) # 8 x 8 x 2048\n        x = self.Mixed_7c(x) # 8 x 8 x 2048\n        return x\n\n    def logits(self, features):\n        x = F.avg_pool2d(features, kernel_size=8) # 1 x 1 x 2048\n        x = F.dropout(x, training=self.training) # 1 x 1 x 2048\n        x = x.view(x.size(0), -1) # 2048\n        x = self.last_linear(x) # 1000 (num_classes)\n        if self.training and self.aux_logits:\n            aux = self._out_aux\n            self._out_aux = None\n            return x, aux\n        return x\n\n    def forward(self, input):\n        x = self.features(input)\n        x = self.logits(x)\n        return x\n\n    # Modify methods\n    model.features = types.MethodType(features, model)\n    model.logits = types.MethodType(logits, model)\n    model.forward = types.MethodType(forward, model)\n    return model\n\n###############################################################\n#Â ResNets\n\ndef modify_resnets(model):\n    # Modify attributs\n    model.last_linear = model.fc\n    model.fc = None\n\n    def features(self, input):\n        x = self.conv1(input)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        return x\n\n    def logits(self, features):\n        x = self.avgpool(features)\n        x = x.view(x.size(0), -1)\n        x = self.last_linear(x)\n        return x\n\n    def forward(self, input):\n        x = self.features(input)\n        x = self.logits(x)\n        return x\n\n    # Modify methods\n    model.features = types.MethodType(features, model)\n    model.logits = types.MethodType(logits, model)\n    model.forward = types.MethodType(forward, model)\n    return model\n\ndef resnet18(num_classes=1000, pretrained='imagenet'):\n    \"\"\"Constructs a ResNet-18 model.\n    \"\"\"\n    model = models.resnet18(pretrained=False)\n    if pretrained is not None:\n        settings = pretrained_settings['resnet18'][pretrained]\n        model = load_pretrained(model, num_classes, settings)\n    model = modify_resnets(model)\n    return model\n\ndef resnet34(num_classes=1000, pretrained='imagenet'):\n    \"\"\"Constructs a ResNet-34 model.\n    \"\"\"\n    model = models.resnet34(pretrained=False)\n    if pretrained is not None:\n        settings = pretrained_settings['resnet34'][pretrained]\n        model = load_pretrained(model, num_classes, settings)\n    model = modify_resnets(model)\n    return model\n\ndef resnet50(num_classes=1000, pretrained='imagenet'):\n    \"\"\"Constructs a ResNet-50 model.\n    \"\"\"\n    model = models.resnet50(pretrained=False)\n    if pretrained is not None:\n        settings = pretrained_settings['resnet50'][pretrained]\n        model = load_pretrained(model, num_classes, settings)\n    model = modify_resnets(model)\n    return model\n\ndef resnet101(num_classes=1000, pretrained='imagenet'):\n    \"\"\"Constructs a ResNet-101 model.\n    \"\"\"\n    model = models.resnet101(pretrained=False)\n    if pretrained is not None:\n        settings = pretrained_settings['resnet101'][pretrained]\n        model = load_pretrained(model, num_classes, settings)\n    model = modify_resnets(model)\n    return model\n\ndef resnet152(num_classes=1000, pretrained='imagenet'):\n    \"\"\"Constructs a ResNet-152 model.\n    \"\"\"\n    model = models.resnet152(pretrained=False)\n    if pretrained is not None:\n        settings = pretrained_settings['resnet152'][pretrained]\n        model = load_pretrained(model, num_classes, settings)\n    model = modify_resnets(model)\n    return model\n\n###############################################################\n#Â SqueezeNets\n\ndef modify_squeezenets(model):\n    # /!\\ Beware squeezenets do not have any last_linear module\n\n    # Modify attributs\n    model.dropout = model.classifier[0]\n    model.last_conv = model.classifier[1]\n    model.relu = model.classifier[2]\n    model.avgpool = model.classifier[3]\n    del model.classifier\n\n    def logits(self, features):\n        x = self.dropout(features)\n        x = self.last_conv(x)\n        x = self.relu(x)\n        x = self.avgpool(x)\n        return x\n\n    def forward(self, input):\n        x = self.features(input)\n        x = self.logits(x)\n        return x\n\n    # Modify methods\n    model.logits = types.MethodType(logits, model)\n    model.forward = types.MethodType(forward, model)\n    return model\n\ndef squeezenet1_0(num_classes=1000, pretrained='imagenet'):\n    r\"\"\"SqueezeNet model architecture from the `\"SqueezeNet: AlexNet-level\n    accuracy with 50x fewer parameters and <0.5MB model size\"\n    <https://arxiv.org/abs/1602.07360>`_ paper.\n    \"\"\"\n    model = models.squeezenet1_0(pretrained=False)\n    if pretrained is not None:\n        settings = pretrained_settings['squeezenet1_0'][pretrained]\n        model = load_pretrained(model, num_classes, settings)\n    model = modify_squeezenets(model)\n    return model\n\ndef squeezenet1_1(num_classes=1000, pretrained='imagenet'):\n    r\"\"\"SqueezeNet 1.1 model from the `official SqueezeNet repo\n    <https://github.com/DeepScale/SqueezeNet/tree/master/SqueezeNet_v1.1>`_.\n    SqueezeNet 1.1 has 2.4x less computation and slightly fewer parameters\n    than SqueezeNet 1.0, without sacrificing accuracy.\n    \"\"\"\n    model = models.squeezenet1_1(pretrained=False)\n    if pretrained is not None:\n        settings = pretrained_settings['squeezenet1_1'][pretrained]\n        model = load_pretrained(model, num_classes, settings)\n    model = modify_squeezenets(model)\n    return model\n\n###############################################################\n#Â VGGs\n\ndef modify_vggs(model):\n    # Modify attributs\n    model._features = model.features\n    del model.features\n    model.linear0 = model.classifier[0]\n    model.relu0 = model.classifier[1]\n    model.dropout0 = model.classifier[2]\n    model.linear1 = model.classifier[3]\n    model.relu1 = model.classifier[4]\n    model.dropout1 = model.classifier[5]\n    model.last_linear = model.classifier[6]\n    del model.classifier\n\n    def features(self, input):\n        x = self._features(input)\n        x = x.view(x.size(0), -1)\n        x = self.linear0(x)\n        x = self.relu0(x)\n        x = self.dropout0(x)\n        x = self.linear1(x)\n        return x\n\n    def logits(self, features):\n        x = self.relu1(features)\n        x = self.dropout1(x)\n        x = self.last_linear(x)\n        return x\n\n    def forward(self, input):\n        x = self.features(input)\n        x = self.logits(x)\n        return x\n\n    # Modify methods\n    model.features = types.MethodType(features, model)\n    model.logits = types.MethodType(logits, model)\n    model.forward = types.MethodType(forward, model)\n    return model\n\ndef vgg11(num_classes=1000, pretrained='imagenet'):\n    \"\"\"VGG 11-layer model (configuration \"A\")\n    \"\"\"\n    model = models.vgg11(pretrained=False)\n    if pretrained is not None:\n        settings = pretrained_settings['vgg11'][pretrained]\n        model = load_pretrained(model, num_classes, settings)\n    model = modify_vggs(model)\n    return model\n\ndef vgg11_bn(num_classes=1000, pretrained='imagenet'):\n    \"\"\"VGG 11-layer model (configuration \"A\") with batch normalization\n    \"\"\"\n    model = models.vgg11_bn(pretrained=False)\n    if pretrained is not None:\n        settings = pretrained_settings['vgg11_bn'][pretrained]\n        model = load_pretrained(model, num_classes, settings)\n    model = modify_vggs(model)\n    return model\n\ndef vgg13(num_classes=1000, pretrained='imagenet'):\n    \"\"\"VGG 13-layer model (configuration \"B\")\n    \"\"\"\n    model = models.vgg13(pretrained=False)\n    if pretrained is not None:\n        settings = pretrained_settings['vgg13'][pretrained]\n        model = load_pretrained(model, num_classes, settings)\n    model = modify_vggs(model)\n    return model\n\ndef vgg13_bn(num_classes=1000, pretrained='imagenet'):\n    \"\"\"VGG 13-layer model (configuration \"B\") with batch normalization\n    \"\"\"\n    model = models.vgg13_bn(pretrained=False)\n    if pretrained is not None:\n        settings = pretrained_settings['vgg13_bn'][pretrained]\n        model = load_pretrained(model, num_classes, settings)\n    model = modify_vggs(model)\n    return model\n\ndef vgg16(num_classes=1000, pretrained='imagenet'):\n    \"\"\"VGG 16-layer model (configuration \"D\")\n    \"\"\"\n    model = models.vgg16(pretrained=False)\n    if pretrained is not None:\n        settings = pretrained_settings['vgg16'][pretrained]\n        model = load_pretrained(model, num_classes, settings)\n    model = modify_vggs(model)\n    return model\n\ndef vgg16_bn(num_classes=1000, pretrained='imagenet'):\n    \"\"\"VGG 16-layer model (configuration \"D\") with batch normalization\n    \"\"\"\n    model = models.vgg16_bn(pretrained=False)\n    if pretrained is not None:\n        settings = pretrained_settings['vgg16_bn'][pretrained]\n        model = load_pretrained(model, num_classes, settings)\n    model = modify_vggs(model)\n    return model\n\ndef vgg19(num_classes=1000, pretrained='imagenet'):\n    \"\"\"VGG 19-layer model (configuration \"E\")\n    \"\"\"\n    model = models.vgg19(pretrained=False)\n    if pretrained is not None:\n        settings = pretrained_settings['vgg19'][pretrained]\n        model = load_pretrained(model, num_classes, settings)\n    model = modify_vggs(model)\n    return model\n\ndef vgg19_bn(num_classes=1000, pretrained='imagenet'):\n    \"\"\"VGG 19-layer model (configuration 'E') with batch normalization\n    \"\"\"\n    model = models.vgg19_bn(pretrained=False)\n    if pretrained is not None:\n        settings = pretrained_settings['vgg19_bn'][pretrained]\n        model = load_pretrained(model, num_classes, settings)\n    model = modify_vggs(model)\n    return model\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nResNet code gently borrowed from\nhttps://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n\"\"\"\nfrom __future__ import print_function, division, absolute_import\nfrom collections import OrderedDict\nimport math\n\nimport torch.nn as nn\nfrom torch.utils import model_zoo\n\n__all__ = ['SENet', 'senet154', 'se_resnet50', 'se_resnet101', 'se_resnet152',\n           'se_resnext50_32x4d', 'se_resnext101_32x4d']\n\npretrained_settings = {\n    'senet154': {\n        'imagenet': {\n            'url': 'http://data.lip6.fr/cadene/pretrainedmodels/senet154-c7b49a05.pth',\n            'input_space': 'RGB',\n            'input_size': [3, 224, 224],\n            'input_range': [0, 1],\n            'mean': [0.485, 0.456, 0.406],\n            'std': [0.229, 0.224, 0.225],\n            'num_classes': 1000\n        }\n    },\n    'se_resnet50': {\n        'imagenet': {\n            'url': 'http://data.lip6.fr/cadene/pretrainedmodels/se_resnet50-ce0d4300.pth',\n            'input_space': 'RGB',\n            'input_size': [3, 224, 224],\n            'input_range': [0, 1],\n            'mean': [0.485, 0.456, 0.406],\n            'std': [0.229, 0.224, 0.225],\n            'num_classes': 1000\n        }\n    },\n    'se_resnet101': {\n        'imagenet': {\n            'url': 'http://data.lip6.fr/cadene/pretrainedmodels/se_resnet101-7e38fcc6.pth',\n            'input_space': 'RGB',\n            'input_size': [3, 224, 224],\n            'input_range': [0, 1],\n            'mean': [0.485, 0.456, 0.406],\n            'std': [0.229, 0.224, 0.225],\n            'num_classes': 1000\n        }\n    },\n    'se_resnet152': {\n        'imagenet': {\n            'url': 'http://data.lip6.fr/cadene/pretrainedmodels/se_resnet152-d17c99b7.pth',\n            'input_space': 'RGB',\n            'input_size': [3, 224, 224],\n            'input_range': [0, 1],\n            'mean': [0.485, 0.456, 0.406],\n            'std': [0.229, 0.224, 0.225],\n            'num_classes': 1000\n        }\n    },\n    'se_resnext50_32x4d': {\n        'imagenet': {\n            'url': 'http://data.lip6.fr/cadene/pretrainedmodels/se_resnext50_32x4d-a260b3a4.pth',\n            'input_space': 'RGB',\n            'input_size': [3, 224, 224],\n            'input_range': [0, 1],\n            'mean': [0.485, 0.456, 0.406],\n            'std': [0.229, 0.224, 0.225],\n            'num_classes': 1000\n        }\n    },\n    'se_resnext101_32x4d': {\n        'imagenet': {\n            'url': 'http://data.lip6.fr/cadene/pretrainedmodels/se_resnext101_32x4d-3b2fe3d8.pth',\n            'input_space': 'RGB',\n            'input_size': [3, 224, 224],\n            'input_range': [0, 1],\n            'mean': [0.485, 0.456, 0.406],\n            'std': [0.229, 0.224, 0.225],\n            'num_classes': 1000\n        }\n    },\n}\n\n\nclass SEModule(nn.Module):\n\n    def __init__(self, channels, reduction):\n        super(SEModule, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc1 = nn.Conv2d(channels, channels // reduction, kernel_size=1,\n                             padding=0)\n        self.relu = nn.ReLU(inplace=True)\n        self.fc2 = nn.Conv2d(channels // reduction, channels, kernel_size=1,\n                             padding=0)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        module_input = x\n        x = self.avg_pool(x)\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.sigmoid(x)\n        return module_input * x\n\n\nclass Bottleneck(nn.Module):\n    \"\"\"\n    Base class for bottlenecks that implements `forward()` method.\n    \"\"\"\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out = self.se_module(out) + residual\n        out = self.relu(out)\n\n        return out\n\n\nclass SEBottleneck(Bottleneck):\n    \"\"\"\n    Bottleneck for SENet154.\n    \"\"\"\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n                 downsample=None):\n        super(SEBottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes * 2, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes * 2)\n        self.conv2 = nn.Conv2d(planes * 2, planes * 4, kernel_size=3,\n                               stride=stride, padding=1, groups=groups,\n                               bias=False)\n        self.bn2 = nn.BatchNorm2d(planes * 4)\n        self.conv3 = nn.Conv2d(planes * 4, planes * 4, kernel_size=1,\n                               bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SEResNetBottleneck(Bottleneck):\n    \"\"\"\n    ResNet bottleneck with a Squeeze-and-Excitation module. It follows Caffe\n    implementation and uses `stride=stride` in `conv1` and not in `conv2`\n    (the latter is used in the torchvision implementation of ResNet).\n    \"\"\"\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n                 downsample=None):\n        super(SEResNetBottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False,\n                               stride=stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1,\n                               groups=groups, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SEResNeXtBottleneck(Bottleneck):\n    \"\"\"\n    ResNeXt bottleneck type C with a Squeeze-and-Excitation module.\n    \"\"\"\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n                 downsample=None, base_width=4):\n        super(SEResNeXtBottleneck, self).__init__()\n        width = math.floor(planes * (base_width / 64)) * groups\n        self.conv1 = nn.Conv2d(inplanes, width, kernel_size=1, bias=False,\n                               stride=1)\n        self.bn1 = nn.BatchNorm2d(width)\n        self.conv2 = nn.Conv2d(width, width, kernel_size=3, stride=stride,\n                               padding=1, groups=groups, bias=False)\n        self.bn2 = nn.BatchNorm2d(width)\n        self.conv3 = nn.Conv2d(width, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SENet(nn.Module):\n\n    def __init__(self, block, layers, groups, reduction, dropout_p=0.2,\n                 inplanes=128, input_3x3=True, downsample_kernel_size=3,\n                 downsample_padding=1, num_classes=1000):\n        \"\"\"\n        Parameters\n        ----------\n        block (nn.Module): Bottleneck class.\n            - For SENet154: SEBottleneck\n            - For SE-ResNet models: SEResNetBottleneck\n            - For SE-ResNeXt models:  SEResNeXtBottleneck\n        layers (list of ints): Number of residual blocks for 4 layers of the\n            network (layer1...layer4).\n        groups (int): Number of groups for the 3x3 convolution in each\n            bottleneck block.\n            - For SENet154: 64\n            - For SE-ResNet models: 1\n            - For SE-ResNeXt models:  32\n        reduction (int): Reduction ratio for Squeeze-and-Excitation modules.\n            - For all models: 16\n        dropout_p (float or None): Drop probability for the Dropout layer.\n            If `None` the Dropout layer is not used.\n            - For SENet154: 0.2\n            - For SE-ResNet models: None\n            - For SE-ResNeXt models: None\n        inplanes (int):  Number of input channels for layer1.\n            - For SENet154: 128\n            - For SE-ResNet models: 64\n            - For SE-ResNeXt models: 64\n        input_3x3 (bool): If `True`, use three 3x3 convolutions instead of\n            a single 7x7 convolution in layer0.\n            - For SENet154: True\n            - For SE-ResNet models: False\n            - For SE-ResNeXt models: False\n        downsample_kernel_size (int): Kernel size for downsampling convolutions\n            in layer2, layer3 and layer4.\n            - For SENet154: 3\n            - For SE-ResNet models: 1\n            - For SE-ResNeXt models: 1\n        downsample_padding (int): Padding for downsampling convolutions in\n            layer2, layer3 and layer4.\n            - For SENet154: 1\n            - For SE-ResNet models: 0\n            - For SE-ResNeXt models: 0\n        num_classes (int): Number of outputs in `last_linear` layer.\n            - For all models: 1000\n        \"\"\"\n        super(SENet, self).__init__()\n        self.inplanes = inplanes\n        if input_3x3:\n            layer0_modules = [\n                ('conv1', nn.Conv2d(3, 64, 3, stride=2, padding=1,\n                                    bias=False)),\n                ('bn1', nn.BatchNorm2d(64)),\n                ('relu1', nn.ReLU(inplace=True)),\n                ('conv2', nn.Conv2d(64, 64, 3, stride=1, padding=1,\n                                    bias=False)),\n                ('bn2', nn.BatchNorm2d(64)),\n                ('relu2', nn.ReLU(inplace=True)),\n                ('conv3', nn.Conv2d(64, inplanes, 3, stride=1, padding=1,\n                                    bias=False)),\n                ('bn3', nn.BatchNorm2d(inplanes)),\n                ('relu3', nn.ReLU(inplace=True)),\n            ]\n        else:\n            layer0_modules = [\n                ('conv1', nn.Conv2d(3, inplanes, kernel_size=7, stride=2,\n                                    padding=3, bias=False)),\n                ('bn1', nn.BatchNorm2d(inplanes)),\n                ('relu1', nn.ReLU(inplace=True)),\n            ]\n        # To preserve compatibility with Caffe weights `ceil_mode=True`\n        # is used instead of `padding=1`.\n        layer0_modules.append(('pool', nn.MaxPool2d(3, stride=2,\n                                                    ceil_mode=True)))\n        self.layer0 = nn.Sequential(OrderedDict(layer0_modules))\n        self.layer1 = self._make_layer(\n            block,\n            planes=64,\n            blocks=layers[0],\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=1,\n            downsample_padding=0\n        )\n        self.layer2 = self._make_layer(\n            block,\n            planes=128,\n            blocks=layers[1],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.layer3 = self._make_layer(\n            block,\n            planes=256,\n            blocks=layers[2],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.layer4 = self._make_layer(\n            block,\n            planes=512,\n            blocks=layers[3],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.avg_pool = nn.AvgPool2d(7, stride=1)\n        self.dropout = nn.Dropout(dropout_p) if dropout_p is not None else None\n        self.last_linear = nn.Linear(512 * block.expansion, num_classes)\n\n    def _make_layer(self, block, planes, blocks, groups, reduction, stride=1,\n                    downsample_kernel_size=1, downsample_padding=0):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=downsample_kernel_size, stride=stride,\n                          padding=downsample_padding, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, groups, reduction, stride,\n                            downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, groups, reduction))\n\n        return nn.Sequential(*layers)\n\n    def features(self, x):\n        x = self.layer0(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        return x\n\n    def logits(self, x):\n        x = self.avg_pool(x)\n        if self.dropout is not None:\n            x = self.dropout(x)\n        x = x.view(x.size(0), -1)\n        x = self.last_linear(x)\n        return x\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.logits(x)\n        return x\n\n\ndef initialize_pretrained_model(model, num_classes, settings):\n    assert num_classes == settings['num_classes'], \\\n        'num_classes should be {}, but is {}'.format(\n            settings['num_classes'], num_classes)\n    model.load_state_dict(model_zoo.load_url(settings['url']))\n    model.input_space = settings['input_space']\n    model.input_size = settings['input_size']\n    model.input_range = settings['input_range']\n    model.mean = settings['mean']\n    model.std = settings['std']\n\n\ndef senet154(num_classes=1000, pretrained='imagenet'):\n    model = SENet(SEBottleneck, [3, 8, 36, 3], groups=64, reduction=16,\n                  dropout_p=0.2, num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings['senet154'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnet50(num_classes=1000, pretrained='imagenet'):\n    model = SENet(SEResNetBottleneck, [3, 4, 6, 3], groups=1, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings['se_resnet50'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnet101(num_classes=1000, pretrained='imagenet'):\n    model = SENet(SEResNetBottleneck, [3, 4, 23, 3], groups=1, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings['se_resnet101'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnet152(num_classes=1000, pretrained='imagenet'):\n    model = SENet(SEResNetBottleneck, [3, 8, 36, 3], groups=1, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings['se_resnet152'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnext50_32x4d(num_classes=1000, pretrained='imagenet'):\n    model = SENet(SEResNeXtBottleneck, [3, 4, 6, 3], groups=32, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings['se_resnext50_32x4d'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnext101_32x4d(num_classes=1000, pretrained='imagenet'):\n    model = SENet(SEResNeXtBottleneck, [3, 4, 23, 3], groups=32, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings['se_resnext101_32x4d'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nsys.path.append('/kaggle/working/')\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.parameter import Parameter\n\n\nclass GeM(nn.Module):\n    def __init__(self, p=3, eps=1e-6):\n        super(GeM,self).__init__()\n        self.p = Parameter(torch.ones(1)*p)\n        self.eps = eps\n    def forward(self, x):\n        return gem(x, p=self.p, eps=self.eps)       \n    def __repr__(self):\n        return self.__class__.__name__ + '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + ', ' + 'eps=' + str(self.eps) + ')'\n\n\ndef gem(x, p=3, eps=1e-6):\n    return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1./p)\n\n\ndef get_se_resnet50_gem(pretrain):\n    if pretrain == 'imagenet':\n        model = se_resnet50(num_classes=1000, pretrained='imagenet')\n    else:\n        model = se_resnet50(num_classes=1000, pretrained=None)\n    model.avg_pool = GeM()\n    model.last_linear = nn.Linear(2048, 1)\n    return model\n\ndef get_densenet121_gem(pretrain):\n    if pretrain == 'imagenet':\n        model = densenet121(num_classes=1000, pretrained='imagenet')\n    else:\n        model = densenet121(num_classes=1000, pretrained=None)\n    model.avg_pool = GeM()\n    model.last_linear = nn.Linear(1024, 1)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfrom glob import glob\n\nimport torch\nimport pandas as pd\nfrom PIL import Image, ImageFile\nfrom torchvision import transforms\n\nTEST_IMAGE_PATH = '/kaggle/input/aptos2019-blindness-detection/test_images'\ndevice = torch.device(\"cuda\")\ntest_images = glob(os.path.join(TEST_IMAGE_PATH, '*.png'))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_predictions(model, test_images, transforms, size=256, device=torch.device(\"cuda\")):\n    predictions = []\n    for i, im_path in enumerate(test_images):\n        image = Image.open(im_path)\n        image = image.resize((size, size), resample=Image.BILINEAR)\n\n\n        image = transforms(image).to(device)\n        output = model(image.unsqueeze(0))\n        # HFlip\n        output_flip = model(torch.flip(image.unsqueeze(0), dims=(3,)))\n\n        final_prediction = (output.item()+output_flip.item())/2\n\n        # Make submission\n        predictions.append((os.path.splitext(im_path.split('/')[-1])[0], final_prediction))\n    return predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Densenet 121\nMODEL_PATH = '../input/densenet121/model_densenet121_bs64_30.pth'\n# Load model\nmodel = get_densenet121_gem(pretrain=False)\nmodel.to(device)\nmodel.load_state_dict(torch.load(MODEL_PATH, map_location='cuda:0'))\nmodel.eval()\n\nnorm = transforms.Compose([transforms.ToTensor()])\n\n# predictions_densenet = make_predictions(model, test_images, norm, size=224, device=torch.device(\"cuda\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### SE Resnet50\n# MODEL_PATH = '/kaggle/input/seresnet50pretrain/fine_tune_256_model30.pth'\nMODEL_PATH = '../input/seresnet50testpseudo/model10.pth'  # pseudo label\n\n# Load model\nmodel = get_se_resnet50_gem(pretrain=False)\nmodel.to(device)\nmodel.load_state_dict(torch.load(MODEL_PATH, map_location='cuda:0'))\nmodel.eval()\n\nnorm = transforms.Compose([transforms.ToTensor(), \n                           transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n                         ])\n\n# predictions_seresnet = make_predictions(model, test_images, norm, size=256, device=torch.device(\"cuda\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### SE Resnet50 512\n# MODEL_PATH = '../input/seresnet50-512/model30_512.pth'\nMODEL_PATH = '../input/seresnet50pseudo-512/model30.pth'\n# Load model\nmodel = get_se_resnet50_gem(pretrain=False)\nmodel.to(device)\nmodel.load_state_dict(torch.load(MODEL_PATH, map_location='cuda:0'))\nmodel.eval()\n\nnorm = transforms.Compose([transforms.ToTensor(), \n                           transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n                         ])\n\npredictions_seresnet_512 = make_predictions(model, test_images, norm, size=512, device=torch.device(\"cuda\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merge predictions\nfinal_predictions = predictions_seresnet_512\n# final_predictions = []\n# for x, y in zip(predictions_densenet, predictions_seresnet):\n#     assert x[0] == y[0]\n#     final_predictions.append([x[0], (x[1]+y[1])/2])\n# for x, y, z in zip(predictions_densenet, predictions_seresnet, predictions_seresnet_512):\n#     assert x[0] == y[0]\n#     final_predictions.append([x[0], (x[1]+y[1]+z[1])/3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame(final_predictions)\nsubmission.columns = ['id_code','diagnosis']\n\n\nsubmission.loc[submission.diagnosis < 0.75, 'diagnosis'] = 0\nsubmission.loc[(0.75 <= submission.diagnosis) & (submission.diagnosis < 1.5), 'diagnosis'] = 1\nsubmission.loc[(1.5 <= submission.diagnosis) & (submission.diagnosis < 2.5), 'diagnosis'] = 2\nsubmission.loc[(2.5 <= submission.diagnosis) & (submission.diagnosis < 3.5), 'diagnosis'] = 3\nsubmission.loc[3.5 <= submission.diagnosis, 'diagnosis'] = 4\nsubmission['diagnosis'] = submission['diagnosis'].astype(int)\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n\n# # Load model\n# model = get_se_resnet50_gem(pretrain=False)\n# model.to(device)\n# model.load_state_dict(torch.load(MODEL_PATH, map_location='cuda:0'))\n# model.eval()\n\n\n# # Inference\n# test_images = glob(os.path.join(TEST_IMAGE_PATH, '*.png'))\n# predictions = []\n# for i, im_path in enumerate(test_images):\n# #     print(i, '/', len(test_images))\n#     image = Image.open(im_path)\n#     image = image.resize((256, 256), resample=Image.BILINEAR)\n#     norm = transforms.Compose([transforms.ToTensor(), \n#                                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n#                              ])\n\n#     image = norm(image).to(device)\n#     output = model(image.unsqueeze(0))\n#     # HFlip\n#     output_flip = model(torch.flip(image.unsqueeze(0), dims=(3,)))\n    \n#     final_prediction = (output.item()+output_flip.item())/2\n# #     final_prediction = output.item()\n\n#     # Make submission\n#     predictions.append((os.path.splitext(im_path.split('/')[-1])[0], final_prediction))\n\n#     # if i == 10:\n#     #     break\n\n\n# submission = pd.DataFrame(predictions)\n# submission.columns = ['id_code','diagnosis']\n\n\n# submission.loc[submission.diagnosis < 0.7, 'diagnosis'] = 0\n# submission.loc[(0.7 <= submission.diagnosis) & (submission.diagnosis < 1.5), 'diagnosis'] = 1\n# submission.loc[(1.5 <= submission.diagnosis) & (submission.diagnosis < 2.5), 'diagnosis'] = 2\n# submission.loc[(2.5 <= submission.diagnosis) & (submission.diagnosis < 3.5), 'diagnosis'] = 3\n# submission.loc[3.5 <= submission.diagnosis, 'diagnosis'] = 4\n# submission['diagnosis'] = submission['diagnosis'].astype(int)\n# submission.to_csv('submission.csv', index=False)\n# submission.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}