{"cells":[{"metadata":{"_cell_guid":"f5af45b1-871f-42a1-8a26-9edec081e46f","_uuid":"f251163d-f8ad-413b-a918-b64aa82c03b4","trusted":false},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils import to_categorical\n\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential, load_model\nfrom keras.layers import (Activation, Dropout, Flatten, Dense, GlobalMaxPooling2D,\n                          BatchNormalization, Input, Conv2D, GlobalAveragePooling2D,MaxPooling2D)\nfrom keras.callbacks import ModelCheckpoint\nfrom keras import metrics\nfrom keras.optimizers import Adam \nfrom keras import backend as K\nimport keras\nfrom keras.models import Model\nimport matplotlib.pyplot as plt \n\nfrom keras.preprocessing import image\nfrom keras.layers import merge, Input\nfrom keras.utils import np_utils\nimport os\nimport time\n\nimport cv2\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nSEED=2\n\ntrain=pd.read_csv(\"/kaggle/input/aptos2019-blindness-detection/train.csv\")\nsubmition=pd.read_csv(\"/kaggle/input/aptos2019-blindness-detection/test.csv\")\n\nx = train['id_code']\ny = train['diagnosis']\n\nx,y=shuffle(x,y)\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"842c2a75-55f6-4336-8201-8f5550f4c832","_uuid":"6b31f734-f996-4881-bee5-72bf97938fb0","trusted":false},"cell_type":"code","source":"df_X,X_test,df_y,y_test=train_test_split(x, y, test_size=0.15)\n\nX_train,X_valid,y_train,y_valid=train_test_split(df_X, df_y, test_size=0.15)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"80bd7528-ac1a-4135-891c-374e98c740f0","_uuid":"78408691-d6f2-4168-9e33-05fccf72cd9f"},"cell_type":"markdown","source":"# Number of samples in each class"},{"metadata":{"_cell_guid":"a715753d-198a-4c04-9050-23e744a012ef","_uuid":"ebd0df7a-c6f4-41e1-8744-09254542f73a","trusted":false},"cell_type":"code","source":"y_train.hist()\ny_test.hist()\ny_valid.hist()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"98957c73-7320-41e3-8fdd-ddfc493f38da","_uuid":"10f6a130-a518-4cf7-bcff-26331d00c45c"},"cell_type":"markdown","source":"- Train(Blue) \n- Test (Yellow)\n- Valid(Green)"},{"metadata":{"_cell_guid":"21ebac62-d34c-4b5b-96c5-1d39ab6cee5c","_uuid":"9ea24305-0b54-436d-beb6-c810a9a31404"},"cell_type":"markdown","source":"# Images befor pre processing"},{"metadata":{"_cell_guid":"e52b89aa-2a70-43de-bde5-57ba48b6d3cf","_uuid":"64f1eab1-7bdd-4e00-9f4b-6d0901d6aeb4","trusted":false},"cell_type":"code","source":"\n%time\nIMG_SIZE=224\n#IMG_SIZE=300\nfig = plt.figure(figsize=(25, 16))\nfor class_id in sorted(y_train.unique()):\n    for i, (idx, row) in enumerate(train.loc[train['diagnosis'] == class_id].sample(5, random_state=SEED).iterrows()):\n        ax = fig.add_subplot(5, 5, class_id * 5 + i + 1, xticks=[], yticks=[])\n        path=f\"../input/aptos2019-blindness-detection/train_images/{row['id_code']}.png\"\n        image = cv2.imread(path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        #image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n        #image=cv2.addWeighted ( image,4, cv2.GaussianBlur( image , (0,0) , IMG_SIZE/10) ,-4 ,128) # the trick is to add this line\n\n        plt.imshow(image)\n        ax.set_title('Label: %d-%d-%s' % (class_id, idx, row['id_code']) )","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"14741276-e38e-466e-b122-e9465b8258f4","_uuid":"8e063513-e9de-4dbc-8981-9b11870dfc07","trusted":false},"cell_type":"code","source":"def load_ben_color(path, sigmaX=20):\n    image = cv2.imread(path)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    image = crop_image_from_gray(image)\n    image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n    image=cv2.addWeighted ( image,4, cv2.GaussianBlur( image , (0,0) , sigmaX) ,-4 ,128)\n        \n    return image","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"31c776e6-8d09-4fa9-8944-77a6a86f1662","_uuid":"82620ad3-98b7-48f0-b92f-2a7235f6399c","trusted":false},"cell_type":"code","source":"def crop_image1(img,tol=7):\n    # img is image data\n    # tol  is tolerance\n        \n    mask = img>tol\n    return img[np.ix_(mask.any(1),mask.any(0))]\n\ndef crop_image_from_gray(img,tol=7):\n    if img.ndim ==2:\n        mask = img>tol\n        return img[np.ix_(mask.any(1),mask.any(0))]\n    elif img.ndim==3:\n        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n        mask = gray_img>tol\n        \n        check_shape = img[:,:,0][np.ix_(mask.any(1),mask.any(0))].shape[0]\n        if (check_shape == 0): # image is too dark so that we crop out everything,\n            return img # return original image\n        else:\n            img1=img[:,:,0][np.ix_(mask.any(1),mask.any(0))]\n            img2=img[:,:,1][np.ix_(mask.any(1),mask.any(0))]\n            img3=img[:,:,2][np.ix_(mask.any(1),mask.any(0))]\n    #         print(img1.shape,img2.shape,img3.shape)\n            img = np.stack([img1,img2,img3],axis=-1)\n    #         print(img.shape)\n        return img","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"32d5e1d0-60bf-40af-bc06-19aba80ae276","_uuid":"57b908a6-ff60-4e0c-9719-0739e795caee","trusted":false},"cell_type":"code","source":"\ntrain_images=[]\n\n%time\nfig = plt.figure(figsize=(25, 16))\n\nfor idx, row in enumerate(X_train):\n    \n    path=f\"../input/aptos2019-blindness-detection/train_images/{row}.png\"\n    image =load_ben_color(path)\n    train_images.append(image)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4ec2fe4c-8406-4d23-9867-301e604eb60d","_uuid":"5cda59de-ccc2-4479-8719-abab462d976a"},"cell_type":"markdown","source":"# After preProcessing"},{"metadata":{"_cell_guid":"a519d8f5-3ca7-4c45-b7a6-3faf6436dfe3","_uuid":"1208ee51-8958-4e6b-9cf3-e5234a8ed552","trusted":false},"cell_type":"code","source":"%time\nfig = plt.figure(figsize=(25, 16))\nfor i in range(0,20):\n    ax = fig.add_subplot(5, 5, 5 + i + 1, xticks=[], yticks=[])\n    #path=f\"../input/aptos2019-blindness-detection/train_images/{row['id_code']}.png\"\n    image = train_images[i]\n    #image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    #image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n    #image=cv2.addWeighted ( image,4, cv2.GaussianBlur( image , (0,0) , IMG_SIZE/10) ,-4 ,128) # the trick is to add this line\n\n    plt.imshow(image)\n    ax.set_title(i)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7c589935-5b4b-4af6-a76f-565869266e9e","_uuid":"c97d621c-ee2f-4103-833d-81e14cfd34dc","trusted":false},"cell_type":"code","source":"import matplotlib.pyplot as plt \nimport cv2\ntest_images=[]\n\n%time\nfig = plt.figure(figsize=(25, 16))\n\nfor idx, row in enumerate(X_test):\n    \n    path=f\"../input/aptos2019-blindness-detection/train_images/{row}.png\"\n    image =load_ben_color(path)\n    test_images.append(image)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"53ef54e5-e70d-4e77-a87f-ce737dae3772","_uuid":"dcf03e8e-f2f8-480d-b1c8-a874c8b46ffd","trusted":false},"cell_type":"code","source":"import matplotlib.pyplot as plt \nimport cv2\nvalid_images=[]\n\n%time\nfig = plt.figure(figsize=(25, 16))\n\nfor idx, row in enumerate(X_valid):\n    \n    path=f\"../input/aptos2019-blindness-detection/train_images/{row}.png\"\n    image =load_ben_color(path)\n    valid_images.append(image)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6b82e70e-d8a9-4af4-95c6-ad67e606acd1","_uuid":"35016cf6-c00b-43f4-a035-33fa26fbb165","trusted":false},"cell_type":"code","source":"from keras.utils import to_categorical\nNUM_CLASSES=5\ny_train_dummies=to_categorical(y_train,num_classes=NUM_CLASSES)\ny_test_dummies=to_categorical(y_test,num_classes=NUM_CLASSES)\ny_valid_dummies=to_categorical(y_valid,num_classes=NUM_CLASSES)\ny_train_dummies","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"197dd5a7-a1cf-4231-9bbb-3334f3540627","_uuid":"b388f9f9-99ad-4a2a-a504-7f470c888db6","trusted":false},"cell_type":"code","source":"num_Classes=5\n\ntrain=np.array(train_images)\ntest=np.array(test_images)\nvalid=np.array(valid_images)\n\nnum_of_samples=train.shape[0]\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c5509a59-766b-419a-ac12-87d5c646b87a","_uuid":"4d7b5c1e-f498-422b-8c65-ac5eda0260b4","trusted":false},"cell_type":"code","source":"#Importing the vgg16 model\n\nfrom keras.applications.vgg16 import VGG16, preprocess_input\n#Loading the vgg16 model with pre-trained ImageNet weights\npremodel = VGG16(weights=None, include_top=False, input_shape=(224, 224, 3))\npremodel.load_weights('../input/keras-pretrained-models/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4b43ce22-dbbd-49d1-8e6b-26d2e0073505","_uuid":"5579975e-026e-431b-8e05-fe931d37e332","trusted":false},"cell_type":"code","source":"#Reshaping the testing data\ntest=np.array(test_images)\n\n#Preprocessing the data, so that it can be fed to the pre-trained ResNet50 model.\nresnet_test_input = preprocess_input(test)\n\n#Creating bottleneck features for the testing data\ntest_features = premodel.predict(resnet_test_input)\n\n#Saving the bottleneck features\nnp.savez('../working/vgg16_features_test', features=test_features)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b275780b-1e54-4ac3-9a9e-dd33807b0b7a","_uuid":"716b709c-c552-41da-a328-ed196195c896","trusted":false},"cell_type":"code","source":"#Reshaping the testing data\ntrain=np.array(train_images)\n\n#Preprocessing the data, so that it can be fed to the pre-trained ResNet50 model.\nresnet_train_input = preprocess_input(train)\n\n#Creating bottleneck features for the testing data\ntrain_features = premodel.predict(resnet_train_input)\n\n#Saving the bottleneck features\nnp.savez('../working/vgg16_features_train', features=train_features)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"85159f29-552a-4d5a-bc97-f60d3059b3c6","_uuid":"56b57942-766e-42ba-b0c0-db39e558df5d","trusted":false},"cell_type":"code","source":"#Reshaping the testing data\nvalid=np.array(valid_images)\n\n#Preprocessing the data, so that it can be fed to the pre-trained ResNet50 model.\nresnet_valid_input = preprocess_input(valid)\n\n#Creating bottleneck features for the testing data\nvalid_features = premodel.predict(resnet_valid_input)\n\n#Saving the bottleneck features\nnp.savez('../working/vgg16_features_valid', features=valid_features)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3685d8ed-900b-4275-8924-fcaa93720a3f","_uuid":"76d41a07-6efd-4b35-adbf-078fb0b26998","trusted":false},"cell_type":"code","source":"\n\n# create and configure augmented image generator\ndatagen_train = ImageDataGenerator(\n    width_shift_range=0.1,  # randomly shift images horizontally (10% of total width)\n    height_shift_range=0.1,  # randomly shift images vertically (10% of total height)\n    horizontal_flip=True) # randomly flip images horizontally\n\n# create and configure augmented image generator\ndatagen_valid = ImageDataGenerator(\n    width_shift_range=0.1,  # randomly shift images horizontally (10% of total width)\n    height_shift_range=0.1,  # randomly shift images vertically (10% of total height)\n    horizontal_flip=True) # randomly flip images horizontally\n\n# fit augmented image generator on data\ndatagen_train.fit(train_features)\ndatagen_valid.fit(valid_features)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7bec15f0-3649-42b7-9502-4d884b5d4b48","_uuid":"9b56ed20-e0ce-4f09-a851-d5da79b44689","trusted":false},"cell_type":"code","source":"\nimport matplotlib.pyplot as plt\n\n# take subset of training data\nx_train_subset = train[:12]\nfig = plt.figure(figsize=(20,2))\nfor x_batch in datagen_train.flow(x_train_subset, batch_size=12):\n    for i in range(0, 12):\n        ax = fig.add_subplot(1, 12, i+1)\n        ax.imshow(x_batch[i])\n    fig.suptitle('Augmented Images', fontsize=20)\n    plt.show()\n    break;","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8d6fbc65-df44-41bb-be06-ff06c2e6b263","_uuid":"69e18361-7c5f-48af-849e-90031293afec","trusted":false},"cell_type":"code","source":"model = Sequential()\nmodel.add(GlobalAveragePooling2D(input_shape=train_features.shape[1:]))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(5, activation='softmax'))\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ce30c04a-5b7d-4f4c-9640-e004d62ce73a","_uuid":"158d7bf0-48e2-43b9-8bc7-93ecad657054","trusted":false},"cell_type":"code","source":"model.compile(loss=\"categorical_crossentropy\", optimizer='Adam',  metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2cc99c9d-5ea2-4a83-a2a4-3009613ff44c","_uuid":"660337af-1610-4bb5-ba86-fddf7cbfe1d1","trusted":false},"cell_type":"code","source":"batch_size=32\nt=time.time()\n\nmodel.fit_generator(datagen_train.flow(train_features, y_train_dummies, batch_size=batch_size),\n                    steps_per_epoch=train.shape[0]/32,\n                    epochs=2, verbose=2,\n                    validation_data=datagen_valid.flow(valid_features, y_valid_dummies, batch_size=batch_size),validation_steps=valid.shape[0]/32)\nprint(\"training time: %s\" %(t-time.time()))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ff6a6cfe-9668-4890-9b33-1b3226c36c09","_uuid":"8ce01ad9-cdc2-409e-9ac4-fdabcc4ab1a2","trusted":false},"cell_type":"code","source":"batch_size = 32\nepochs = 20\n\n\nt=time.time()\ncheckpointer = ModelCheckpoint(filepath='../working/weights.best.from_scratch.hdf5', verbose=1, save_best_only=True)\nmodel.fit_generator(datagen_train.flow(train_features, y_train_dummies, batch_size=batch_size),\n                    steps_per_epoch=train.shape[0]/16,\n                    epochs=epochs, verbose=2,\n                    callbacks=[checkpointer],\n                    validation_data=datagen_valid.flow(valid_features, y_valid_dummies, batch_size=batch_size),validation_steps=valid.shape[0]/16)\nprint(\"training time: %s\" %(t-time.time()))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"880918c7-9b5e-44aa-a694-b9433366fae7","_uuid":"c934639c-ed2f-401b-a497-8ed3253fb5ff","trusted":false},"cell_type":"code","source":"model.load_weights('../working/weights.best.from_scratch.hdf5')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"fe57fbf1-64fe-43bd-a6c0-31d429c7df1e","_uuid":"fe0e7618-d7de-4596-9389-96ace5e39d40","trusted":false},"cell_type":"code","source":"(loss, accuracy)=model.evaluate(test_features,y_test_dummies,verbose=1 ,batch_size=32)\nprint(loss)\nprint(accuracy*100)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"76818875-825d-4b13-b96d-58bd2e42812b","_uuid":"9834654c-b0f9-4030-a3cb-355f650d4a40","trusted":false},"cell_type":"code","source":"\nsubmit_images=[]\n\n%time\nfig = plt.figure(figsize=(25, 16))\n\nfor idx, row in enumerate(submition['id_code']):\n    \n    path=f\"../input/aptos2019-blindness-detection/test_images/\"+row+\".png\"\n   \n    image =load_ben_color(path)\n    submit_images.append(image)\nsubmit_image=np.array(submit_images)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"871d6b7f-868a-4f1c-92f8-b4a73f3a9224","_uuid":"6d14ee65-ca1e-4b38-b362-a6dcb6bd3499","trusted":false},"cell_type":"code","source":"\nsubmit=np.array(submit_image)\n\n#Preprocessing the data, so that it can be fed to the pre-trained ResNet50 model.\nresnet_submit_input = preprocess_input(submit)\n\n#Creating bottleneck features for the testing data\nsubmit_features = premodel.predict(resnet_submit_input)\n\n#Saving the bottleneck features\nnp.savez('../working/vgg16_features_submit', features=submit_features)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ede70db4-acd5-4d39-9cab-02ea794b0f6b","_uuid":"a37faa84-d247-42f7-86cc-f2fde35f6a38","trusted":false},"cell_type":"code","source":"predictions = [str(np.argmax(model.predict(np.expand_dims(tensor, axis=0)))) for tensor in submit_features]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1348991e-451a-4c5c-a599-f2eb533a2c4c","_uuid":"9a5493b0-d906-4cb3-ae22-1f6a6591c815","trusted":false},"cell_type":"code","source":"submition['diagnosis'] = predictions\nsubmition.to_csv('submission.csv', index=False)\nsubmition.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3ec15102-5acf-447c-a4e9-b2e0eba81cad","_uuid":"b5176edd-1b49-4f80-8059-709f140d5b10","trusted":false},"cell_type":"code","source":"%time\nIMG_SIZE=300\nfig = plt.figure(figsize=(25, 16))\nfor class_id in sorted(submition['diagnosis'].unique()):\n    for i, (idx, row) in enumerate(submition.loc[submition['diagnosis'] == class_id].sample(5, random_state=2).iterrows()):\n        ax = fig.add_subplot(5, 5, int(class_id) * 5 + i + 1, xticks=[], yticks=[])\n        path=f\"../input/aptos2019-blindness-detection/test_images/\"+row['id_code']+\".png\"\n        image = cv2.imread(path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        #image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n        #image=cv2.addWeighted ( image,4, cv2.GaussianBlur( image , (0,0) , IMG_SIZE/10) ,-4 ,128) # the trick is to add this line\n\n        plt.imshow(image)\n        ax.set_title('Label: %d-%d-%s' % (int(class_id), idx, row['id_code']) )","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}