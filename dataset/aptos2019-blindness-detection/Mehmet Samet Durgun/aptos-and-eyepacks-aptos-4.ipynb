{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport numpy as np # linear algebra\nimport pandas as pd # data processing,\nimport tensorflow as tf\nfrom keras.preprocessing.image import load_img,img_to_array,array_to_img\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom keras.models import Sequential # initialize neural network library\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom glob import glob\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport itertools\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D,BatchNormalization,Activation\nfrom keras.optimizers import RMSprop,Adam,SGD\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\nfrom tensorflow import keras\nimport os\nfrom keras.utils.np_utils import to_categorical \nfrom keras.models import Model\nimport cv2\nfrom tensorflow import Tensor\nfrom keras.layers import Input, Conv2D, ReLU, BatchNormalization,\\\n                                    Add, AveragePooling2D, Flatten, Dense\nfrom keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n\nfrom keras.applications import ResNet101,InceptionResNetV2,NASNetLarge,ResNet152,ResNet152V2,Xception,MobileNetV2\n\n\nfrom sklearn.utils import shuffle\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"scrolled":true,"execution":{"iopub.status.busy":"2021-06-02T06:29:08.084576Z","iopub.execute_input":"2021-06-02T06:29:08.085246Z","iopub.status.idle":"2021-06-02T06:29:28.044378Z","shell.execute_reply.started":"2021-06-02T06:29:08.085126Z","shell.execute_reply":"2021-06-02T06:29:28.019431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label=pd.read_csv(\"../input/aptos-and-eyepacs/train.csv\")\nlabel[\"path\"]=label[\"id_code\"].map(lambda x: os.path.join(\"../input/aptos-and-eyepacs/APTOS AND EYEPACS/APTOS AND EYEPACS\", \"{}.png\".format(x))) # paths of images\n\nlabel2=pd.read_csv(\"../input/aptos2019-blindness-detection/train.csv\")\nlabel2[\"path\"]=label2[\"id_code\"].map(lambda x: os.path.join(\"../input/aptos2019-blindness-detection/train_images\", \"{}.png\".format(x))) # paths of images\n\n#balanced_lab= label.groupby(['diagnosis']).apply(lambda x: x.sample(500, replace = True)).sort_index() \nbalanced_lab= label\n\nzeros=balanced_lab[balanced_lab[\"diagnosis\"]==0][:1500]\n#zeros_for_test=balanced_lab[balanced_lab[\"diagnosis\"]==0][470:500]\nzeros.reset_index(drop=True, inplace=True)\n#zeros_for_test.reset_index(drop=True, inplace=True)\n\n\nones=balanced_lab[balanced_lab[\"diagnosis\"]==1][:500]\n#ones_for_test=balanced_lab[balanced_lab[\"diagnosis\"]==1][340:370]\nones.reset_index(drop=True, inplace=True)\n#ones_for_test.reset_index(drop=True, inplace=True)\n\ntwo=balanced_lab[balanced_lab[\"diagnosis\"]==2][:500]\n#two_for_test=balanced_lab[balanced_lab[\"diagnosis\"]==2][470:500]\ntwo.reset_index(drop=True, inplace=True)\n#two_for_test.reset_index(drop=True, inplace=True)\n\ntree=balanced_lab[balanced_lab[\"diagnosis\"]==3][:300]\n#tree_for_test=balanced_lab[balanced_lab[\"diagnosis\"]==3][173:193]\ntree.reset_index(drop=True, inplace=True)\n#tree_for_test.reset_index(drop=True, inplace=True)\n\nfour=label2[label2[\"diagnosis\"]==4]\n#four_for_test=balanced_lab[balanced_lab[\"diagnosis\"]==4][265:295]\nfour.reset_index(drop=True, inplace=True)\n#four_for_test.reset_index(drop=True, inplace=True)\n\nframes=[zeros,ones,two,tree,four]\nrebalanced_lab=pd.concat(frames)\n\n#frames_for_test=[zeros_for_test,ones_for_test,two_for_test,tree_for_test,four_for_test]\n#rebalanced_lab_for_test=pd.concat(frames_for_test)\n\n#levels_for_test=rebalanced_lab_for_test[\"diagnosis\"]\n#levels_for_test=to_categorical(levels_for_test,num_classes=5)\n\nrebalanced_lab = shuffle(rebalanced_lab)\n\nlevels=rebalanced_lab[\"diagnosis\"]\nlevels=to_categorical(levels,num_classes=5)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T06:29:28.046352Z","iopub.execute_input":"2021-06-02T06:29:28.046845Z","iopub.status.idle":"2021-06-02T06:29:28.160016Z","shell.execute_reply.started":"2021-06-02T06:29:28.046789Z","shell.execute_reply":"2021-06-02T06:29:28.158941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rebalanced_lab.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T06:29:28.162336Z","iopub.execute_input":"2021-06-02T06:29:28.162848Z","iopub.status.idle":"2021-06-02T06:29:28.184111Z","shell.execute_reply.started":"2021-06-02T06:29:28.162792Z","shell.execute_reply":"2021-06-02T06:29:28.182186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rebalanced_lab[\"diagnosis\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T06:29:28.186774Z","iopub.execute_input":"2021-06-02T06:29:28.187252Z","iopub.status.idle":"2021-06-02T06:29:28.198656Z","shell.execute_reply.started":"2021-06-02T06:29:28.187207Z","shell.execute_reply":"2021-06-02T06:29:28.196905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_rows, img_cols = 256,256\nimmatrix=[]\nfor name in rebalanced_lab[\"path\"]:\n    img=cv2.imread(name)\n    img=cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    dim=(img_rows, img_cols)\n    resized=cv2.resize(img,dim)\n    \n    gray=cv2.cvtColor(resized, cv2.COLOR_BGR2GRAY) \n    thresh = cv2.threshold(gray, 10, 255, cv2.THRESH_BINARY)[1]\n    \n    # Find contour and sort by contour area\n    cnts = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n    cnts = sorted(cnts, key=cv2.contourArea, reverse=True)\n    \n    # Find bounding box and extract ROI\n    for c in cnts:\n        x,y,w,h = cv2.boundingRect(c)\n        ROI = resized[y:y+h, x:x+w]\n        break\n    crop_son=cv2.resize(ROI,dim)\n    \n    green_channel = crop_son[:,:,1]   \n\n    green_copy=green_channel.copy()\n    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n    clh = clahe.apply(green_copy)\n    \n    crop_son[:,:,1] = clh   \n    \n    arka_plan = cv2.medianBlur(crop_son, 59)\n    \n    maske = cv2.addWeighted(crop_son,1,arka_plan,-1,255)\n    \n    son_img = cv2.bitwise_and(maske,crop_son)\n\n    immatrix.append(son_img)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T06:29:28.200855Z","iopub.execute_input":"2021-06-02T06:29:28.201522Z","iopub.status.idle":"2021-06-02T06:36:25.996809Z","shell.execute_reply.started":"2021-06-02T06:29:28.201476Z","shell.execute_reply":"2021-06-02T06:36:25.995584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure()\nplt.imshow(resized)\n\nplt.figure()\nplt.imshow(thresh)\n\nplt.figure()\nplt.imshow(crop_son)\n\nplt.figure()\nplt.imshow(arka_plan)\n\nplt.figure()\nplt.imshow(maske)\n\nplt.figure()\nplt.imshow(son_img)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-02T06:36:25.998643Z","iopub.execute_input":"2021-06-02T06:36:25.999069Z","iopub.status.idle":"2021-06-02T06:36:27.328025Z","shell.execute_reply.started":"2021-06-02T06:36:25.999027Z","shell.execute_reply":"2021-06-02T06:36:27.326814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Normalizasyon işlemi\nimmatrix=np.array(immatrix)   \nimmatrix=immatrix/255 #normalization for grey form","metadata":{"execution":{"iopub.status.busy":"2021-06-02T06:36:27.329842Z","iopub.execute_input":"2021-06-02T06:36:27.330584Z","iopub.status.idle":"2021-06-02T06:36:31.020397Z","shell.execute_reply.started":"2021-06-02T06:36:27.330538Z","shell.execute_reply":"2021-06-02T06:36:31.019297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 3 boyutlu olan görsellerimizi 4 boyut'a çevirme. Sebebi sistemin öyle istemesi\nimmatrix=immatrix.reshape(-1,img_rows,img_cols,3) #1 for grey grey 3 for rgb","metadata":{"execution":{"iopub.status.busy":"2021-06-02T06:36:31.023511Z","iopub.execute_input":"2021-06-02T06:36:31.024261Z","iopub.status.idle":"2021-06-02T06:36:31.029804Z","shell.execute_reply.started":"2021-06-02T06:36:31.024196Z","shell.execute_reply":"2021-06-02T06:36:31.028272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Train test spliting\nx_train,x_val,y_train,y_val=train_test_split(immatrix,levels,random_state=42,test_size=0.1)\n\nimmatrix=None\nimmatrix_for_test=None\nlevels=None\nlevels_for_test=None","metadata":{"execution":{"iopub.status.busy":"2021-06-02T06:36:31.032049Z","iopub.execute_input":"2021-06-02T06:36:31.032551Z","iopub.status.idle":"2021-06-02T06:36:35.012292Z","shell.execute_reply.started":"2021-06-02T06:36:31.032504Z","shell.execute_reply":"2021-06-02T06:36:35.010923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"datagen = ImageDataGenerator(\n        horizontal_flip=True,\n        vertical_flip=True,\n        rotation_range=360\n        )\n      \nrn152=ResNet152(weights=\"imagenet\",include_top=False, input_shape=(img_rows, img_cols,3))    \n\n\ndatagen.fit(x_train)\n\ncheckopointer=tf.keras.callbacks.ModelCheckpoint(\n    filepath=\"./weihts.h5\",\n    monitor=\"val_accuracy\",\n    verbose=1,\n    save_best_only=True,\n    save_freq=\"epoch\"\n)\n\nmodel=Sequential()\nmodel.add(rn152)\nmodel.add(Dropout(0.5))\nmodel.add(Flatten())\nmodel.add(BatchNormalization())\nmodel.add(Dense(256,kernel_initializer='he_uniform'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(128,kernel_initializer='he_uniform'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(5,activation='softmax'))\nmodel.compile(loss='binary_crossentropy',optimizer=Adam(lr=0.0005),metrics=['accuracy'])\n\nmodel.summary()\n\nbatch_size=25\nepochs=200\nhistory=model.fit(datagen.flow(x_train, y_train, \n                               batch_size=batch_size),\n                               steps_per_epoch=len(x_train) / batch_size, \n                               epochs=epochs,\n                               validation_data=(x_val, y_val),\n                               callbacks=[checkopointer])","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-06-02T06:36:35.01489Z","iopub.execute_input":"2021-06-02T06:36:35.015729Z","iopub.status.idle":"2021-06-02T09:57:51.282161Z","shell.execute_reply.started":"2021-06-02T06:36:35.015674Z","shell.execute_reply":"2021-06-02T09:57:51.280987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max(history.history['val_accuracy'])\n\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T09:57:51.284415Z","iopub.execute_input":"2021-06-02T09:57:51.28492Z","iopub.status.idle":"2021-06-02T09:57:51.687631Z","shell.execute_reply.started":"2021-06-02T09:57:51.284846Z","shell.execute_reply":"2021-06-02T09:57:51.68662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(max((history.history['val_accuracy'])))","metadata":{"execution":{"iopub.status.busy":"2021-06-02T09:57:51.690128Z","iopub.execute_input":"2021-06-02T09:57:51.690625Z","iopub.status.idle":"2021-06-02T09:57:51.696982Z","shell.execute_reply.started":"2021-06-02T09:57:51.690566Z","shell.execute_reply":"2021-06-02T09:57:51.695647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#%%Corrolation table\n\nloaded_model = tf.keras.models.load_model('./weihts.h5')\n\n# confusion matrix\nimport seaborn as sns\n# Predict the values from the validation dataset\nY_pred = loaded_model.predict(x_val)\n# Convert predictions classes to one hot vectors \nY_pred_classes = np.argmax(Y_pred,axis = 1) \n# Convert validation observations to one hot vectors\nY_true = np.argmax(y_val,axis = 1) \n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n# plot the confusion matrix\nf,ax = plt.subplots(figsize=(8, 8))\nsns.heatmap(confusion_mtx, annot=True, linewidths=0.01,cmap=\"Greens\",linecolor=\"gray\", fmt= '.1f',ax=ax)\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.title(\"Confusion Matrix\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T09:57:51.698565Z","iopub.execute_input":"2021-06-02T09:57:51.699082Z","iopub.status.idle":"2021-06-02T09:58:07.837327Z","shell.execute_reply.started":"2021-06-02T09:57:51.699039Z","shell.execute_reply":"2021-06-02T09:58:07.836296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}