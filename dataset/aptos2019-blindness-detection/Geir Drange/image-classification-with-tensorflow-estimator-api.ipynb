{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Image classification with Tensorflow Estimator API\nThis kernel uses the TensorFlow high-level Estimator API. The model is built using transfer learning with a model from the TensorFlow Hub."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow_hub as hub","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data set\nThe training data set is described in a .csv file with image IDs and labels. There are 5 classes named 0 to 4."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_file='../input/aptos2019-blindness-detection/train.csv'\ndf=pd.read_csv(train_file)\ncat_cnt=df['diagnosis'].value_counts().sort_index()\ntrain_image_count=np.sum(cat_cnt)\nprint(\"Train data set count: \", train_image_count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(os.listdir(\"../input/aptos2019-blindness-detection\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Pre-trained model\nThe base model can be picked from TensorFlow Hub, and can be changed in one line below. A pre-trained model is usually made for a specific input image size, and thankfully we can get that parameter from the hub. Here we pick the NASNet-A model:"},{"metadata":{"trusted":true},"cell_type":"code","source":"USE_INTERNET = False\n\nif USE_INTERNET:\n    %env TFHUB_CACHE_DIR=./tfhub_cache/imagenet/feature_vector\n    mod_link=\"https://tfhub.dev/google/imagenet/nasnet_large/feature_vector/3\"\n    mod_name=mod_link.split('imagenet/')[1].split('/')[0]\n    input_img_size = hub.get_expected_image_size(hub.Module(mod_link))\nelse:\n    mod_link=\"../input/tfhub-cache/nasnet_large/nasnet_large\"\n    mod_name='nasnet'\n    input_img_size = [331, 331]\n\nprint(\"Expected input size for {} is:\".format(mod_name), input_img_size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data set is small (and heavily skewed), and the pre-trained model has never seen retina photos before - what could possibly go wrong? Let's find out..."},{"metadata":{},"cell_type":"markdown","source":"## Input function\nFirst step is to make an input function that will serve the model with data (TensorFlow dataset). Preprocessing steps are collected in the `_preproc_image()` function that is common for TRAIN, EVAL and PREDICT steps."},{"metadata":{"trusted":true},"cell_type":"code","source":"def _preproc_image(file, img_size):\n    image = tf.read_file(file)\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = tf.image.resize(image, img_size)\n    image /= 255.0  # normalize to [0,1] range\n    return image","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The input function below is used for TRAIN and EVAL. The AUTOTUNE option is used to let TensorFlow use available compute resources effectively. The images in this data set are generally large, and thus the preprocessing step could be the bottleneck rather than train step. The image files are shuffle and split into TRAIN and EVAL. Measures are taken here to ensure even distribution of the different categories in each mini-batch. "},{"metadata":{"trusted":true},"cell_type":"code","source":"SPLIT=0.804 # train/eval\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n\ndef imgs_input_fn(filename, batch_size=32, mode = tf.estimator.ModeKeys.TRAIN, total_images=1000, img_size=[200,200]):\n\n    def _agument_image(image):\n        image = tf.image.random_brightness(image, max_delta=0.2)\n        image = tf.image.random_contrast(image, 0.8, 1.2)\n        image = tf.image.random_flip_left_right(image)        \n        return image\n    \n    def _proc_image(file, label=None):\n        image = _preproc_image(tf.strings.join([tf.strings.join([\"../input/aptos2019-blindness-detection/train_images\",file], \"/\"), \"png\"], \".\"),\n                              img_size)\n        # Images is agumented in the TRAIN phase only\n        if mode == tf.estimator.ModeKeys.TRAIN:\n            image = _agument_image(image)\n        return { 'image': image}, label\n     \n    dataset_raw = tf.data.experimental.CsvDataset(filename, [tf.string, tf.string], header=True)\n    dataset_raw = dataset_raw.shuffle(total_images, seed=1)\n    train_cnt = int(SPLIT*total_images)\n    if mode == tf.estimator.ModeKeys.TRAIN:\n        dataset_raw = dataset_raw.take(train_cnt)\n        d0 = dataset_raw.filter(lambda file, label: tf.math.equal(label, '0')).repeat()\n        d1 = dataset_raw.filter(lambda file, label: tf.math.equal(label, '1')).repeat()\n        d2 = dataset_raw.filter(lambda file, label: tf.math.equal(label, '2')).repeat()\n        d3 = dataset_raw.filter(lambda file, label: tf.math.equal(label, '3')).repeat()\n        d4 = dataset_raw.filter(lambda file, label: tf.math.equal(label, '4')).repeat()\n        dataset = tf.data.experimental.sample_from_datasets([d0, d1, d2, d3, d4])\n    elif mode == tf.estimator.ModeKeys.EVAL:\n        dataset = dataset_raw.skip(train_cnt)\n        \n    dataset = dataset.map(_proc_image, num_parallel_calls=AUTOTUNE)\n    dataset = dataset.batch(batch_size).prefetch(buffer_size=AUTOTUNE)\n    return dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define model configuration\nModels are saved into separate `models` directories - useful when testing and comparing different pre-trained models."},{"metadata":{"trusted":true},"cell_type":"code","source":"model_dir = os.path.join(os.getcwd(), \"./models/{}\").format(mod_name)\nos.makedirs(model_dir, exist_ok=True)\n#%load_ext tensorboard\n#%tensorboard --logdir {model_dir} # can access from http://localhost:6006\nprint(\"model_dir: \",model_dir)\nconfig = tf.estimator.RunConfig(\n    model_dir=model_dir,\n    save_summary_steps=100,\n    save_checkpoints_steps=100,\n    keep_checkpoint_max=2,\n    log_step_count_steps=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define model function\nThe pre-trained model is used directly, with just a softmax layer with 5 classes on top. Additionally, the multi_class_head function is used that takes care of some details for us. The loss function it uses by default is `sparse_softmax_cross_entropy` loss. There are many useful discussions on using transfer learning for image classification, [for example this one](https://towardsdatascience.com/transfer-learning-from-pre-trained-models-f2393f124751)"},{"metadata":{"trusted":true},"cell_type":"code","source":"NUM_CLASSES = 5\n\ndef model_fn(features, labels, mode, params):\n    module = hub.Module(mod_link, trainable=False, name=mod_name)\n    feature_vec = module(features['image'])\n    #dropout = tf.layers.dropout(inputs=feature_vec, rate=0.1, training=mode == tf.estimator.ModeKeys.TRAIN)\n    logits = tf.layers.Dense(NUM_CLASSES)(feature_vec)\n    head = tf.contrib.estimator.multi_class_head(n_classes=NUM_CLASSES, label_vocabulary=['0','1','2','3','4'])\n    optimizer = tf.train.AdamOptimizer(learning_rate=0.003)\n    return head.create_estimator_spec(\n        features, mode, logits, labels, optimizer=optimizer\n    )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Setup the Estimator\nThe last thing remaining is to setup the estimator and then run training. The model is saved automatically in the model directory defined above."},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 40\nMAX_STEPS = 1250\n\ntrain_spec = tf.estimator.TrainSpec(input_fn=lambda: imgs_input_fn(train_file, \n                                                                   batch_size=BATCH_SIZE,\n                                                                   mode = tf.estimator.ModeKeys.TRAIN, \n                                                                   total_images=train_image_count,\n                                                                   img_size=input_img_size),\n                                   max_steps=MAX_STEPS)\neval_spec = tf.estimator.EvalSpec(input_fn=lambda: imgs_input_fn(train_file, \n                                                                   batch_size=BATCH_SIZE,\n                                                                   mode = tf.estimator.ModeKeys.EVAL, \n                                                                   total_images=train_image_count,\n                                                                   img_size=input_img_size))\nclassifier = tf.estimator.Estimator(\n        model_fn=model_fn,\n        model_dir=model_dir,\n        config=config,\n        params=[]\n    )\n\ntf.logging.set_verbosity(tf.logging.INFO)\ntf.estimator.train_and_evaluate(classifier, train_spec, eval_spec)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Making predictions\nThe test images for prediction is located in the `test_images` directory, and we simply make a list of all the files."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pathlib\nimport glob\ntest_set = [str(path) for path in list(pathlib.Path('../input/aptos2019-blindness-detection/test_images/').glob('*.png'))]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predict input function\nTo prevent cluttering up the input function used for training, a separate input function is defined for the predict step. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def pred_input_fn(filelist, batch_size=32, pred_images=100, img_size=[200,200]):\n    \n    def _proc_image(file):\n        image = _preproc_image(file, img_size)\n        return { 'image': image}\n     \n    dataset = tf.data.Dataset.from_tensor_slices(filelist)  \n    dataset = dataset.take(pred_images)\n    dataset = dataset.map(_proc_image, num_parallel_calls=AUTOTUNE)\n    dataset = dataset.batch(batch_size).prefetch(buffer_size=AUTOTUNE)\n    return dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predict\nPredictions is then just a matter of calling the .predict method."},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = classifier.predict(input_fn=lambda: pred_input_fn(test_set, \n                                                                batch_size=32,\n                                                                pred_images=len(test_set),\n                                                                img_size=input_img_size))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"NOTE! Nothing happens after the above line - need to iterate through `predictions` to get it going.\nEach prediction will be a dict with this format:\n\n`{'logits': array([ 0.5168798,  1.1608064,  3.1475224, -5.6981955,  0.5543511], dtype=float32), \n  'probabilities': array([5.6095276e-02, 1.0680217e-01, 7.7875328e-01, 1.1213811e-04, 5.8237117e-02], dtype=float32), \n  'class_ids': array([2], dtype=int64), \n  'classes': array([b'2'], dtype=object), \n  'all_class_ids': array([0, 1, 2, 3, 4]), \n  'all_classes': array([b'0', b'1', b'2', b'3', b'4'], dtype=object)}`\n  \nThe payload we are interested in can be found under `class_ids`. Let's create a Pandas dataframe while iterating through the predictions:"},{"metadata":{"trusted":true},"cell_type":"code","source":"fl=test_set.copy()\ndf=pd.DataFrame([(fl.pop(0).split(\"test_images/\")[1].split(\".\")[0], \n                p['class_ids'][0], p['probabilities'][p['class_ids'][0]]) for p in predictions], \n                columns=['id_code', 'diagnosis', 'probability'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Take a look at the first few predictions. The probability of each prediction is also included here for information."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Also look at predictions stats:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submit\nFinally, the result is submitted to a .csv file:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.to_csv('submission.csv', columns=['id_code', 'diagnosis'], index=None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Final comments\nThis kernel scored ~0.57. Some serious feature engineering is needed to properly adapt and train this model. The large image size in the data set really puts the CPU resources under pressure. On a desktop (i7-3930K + RTX2070), the whole notebook runs in ~20min. On Kaggle, the runtime is ~75min. Let's take a look at the predictions with a graphical view:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nsns.set()\ncp = sns.catplot(x=\"diagnosis\", y=\"probability\", kind=\"violin\", inner=None, data=df, palette=sns.color_palette(\"RdBu_r\", 5))\nsns.swarmplot(x=\"diagnosis\", y=\"probability\", color=\"k\", size=3, data=df, ax=cp.ax);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}