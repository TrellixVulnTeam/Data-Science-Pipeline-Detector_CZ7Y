{"cells":[{"metadata":{},"cell_type":"markdown","source":"## This model uses drhabib's L2 model and my L1 model. Predictions from both models are taken using TTA. The final output is obtained by taking the average of two model outputs and hand tuned class boundaries.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nos.listdir('../input/kaggle-public')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"%reload_ext autoreload\n%autoreload 2\n%matplotlib inline\nimport torch\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nimport seaborn as sns\n%matplotlib inline  \nfrom sklearn.model_selection import StratifiedKFold\nfrom joblib import load, dump\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.metrics import confusion_matrix\nfrom fastai import *\nfrom fastai.vision import *\nfrom fastai.callbacks import *\nfrom torchvision import models as md\nfrom torch import nn\nfrom torch.nn import functional as F\nimport re\nimport math\nimport collections\nfrom functools import partial\nfrom torch.utils import model_zoo\nfrom sklearn import metrics\nfrom collections import Counter\nimport json\n\nimport sys\npackage_path = '../input/efficientnet-pytorch/efficientnet-pytorch/EfficientNet-PyTorch-master'\nsys.path.append(package_path)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The following cell is needed to run drhabib's pretrained model"},{"metadata":{"trusted":true},"cell_type":"code","source":"#I could not figure out how to install package in local kernel so i just stole from github =)\n#code stolen from https://github.com/lukemelas/EfficientNet-PyTorch\n\n\n\"\"\"\nThis file contains helper functions for building the model and for loading model parameters.\nThese helper functions are built to mirror those in the official TensorFlow implementation.\n\"\"\"\n\n\n\n# Parameters for the entire model (stem, all blocks, and head)\nGlobalParams = collections.namedtuple('GlobalParams', [\n    'batch_norm_momentum', 'batch_norm_epsilon', 'dropout_rate',\n    'num_classes', 'width_coefficient', 'depth_coefficient',\n    'depth_divisor', 'min_depth', 'drop_connect_rate', 'image_size'])\n\n\n# Parameters for an individual model block\nBlockArgs = collections.namedtuple('BlockArgs', [\n    'kernel_size', 'num_repeat', 'input_filters', 'output_filters',\n    'expand_ratio', 'id_skip', 'stride', 'se_ratio'])\n\n\n# Change namedtuple defaults\nGlobalParams.__new__.__defaults__ = (None,) * len(GlobalParams._fields)\nBlockArgs.__new__.__defaults__ = (None,) * len(BlockArgs._fields)\n\n\ndef relu_fn(x):\n    \"\"\" Swish activation function \"\"\"\n    return x * torch.sigmoid(x)\n\n\ndef round_filters(filters, global_params):\n    \"\"\" Calculate and round number of filters based on depth multiplier. \"\"\"\n    multiplier = global_params.width_coefficient\n    if not multiplier:\n        return filters\n    divisor = global_params.depth_divisor\n    min_depth = global_params.min_depth\n    filters *= multiplier\n    min_depth = min_depth or divisor\n    new_filters = max(min_depth, int(filters + divisor / 2) // divisor * divisor)\n    if new_filters < 0.9 * filters:  # prevent rounding by more than 10%\n        new_filters += divisor\n    return int(new_filters)\n\n\ndef round_repeats(repeats, global_params):\n    \"\"\" Round number of filters based on depth multiplier. \"\"\"\n    multiplier = global_params.depth_coefficient\n    if not multiplier:\n        return repeats\n    return int(math.ceil(multiplier * repeats))\n\n\ndef drop_connect(inputs, p, training):\n    \"\"\" Drop connect. \"\"\"\n    if not training: return inputs\n    batch_size = inputs.shape[0]\n    keep_prob = 1 - p\n    random_tensor = keep_prob\n    random_tensor += torch.rand([batch_size, 1, 1, 1], dtype=inputs.dtype, device=inputs.device)\n    binary_tensor = torch.floor(random_tensor)\n    output = inputs / keep_prob * binary_tensor\n    return output\n\n\ndef get_same_padding_conv2d(image_size=None):\n    \"\"\" Chooses static padding if you have specified an image size, and dynamic padding otherwise.\n        Static padding is necessary for ONNX exporting of models. \"\"\"\n    if image_size is None:\n        return Conv2dDynamicSamePadding\n    else:\n        return partial(Conv2dStaticSamePadding, image_size=image_size)\n\nclass Conv2dDynamicSamePadding(nn.Conv2d):\n    \"\"\" 2D Convolutions like TensorFlow, for a dynamic image size \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dilation=1, groups=1, bias=True):\n        super().__init__(in_channels, out_channels, kernel_size, stride, 0, dilation, groups, bias)\n        self.stride = self.stride if len(self.stride) == 2 else [self.stride[0]]*2\n\n    def forward(self, x):\n        ih, iw = x.size()[-2:]\n        kh, kw = self.weight.size()[-2:]\n        sh, sw = self.stride\n        oh, ow = math.ceil(ih / sh), math.ceil(iw / sw)\n        pad_h = max((oh - 1) * self.stride[0] + (kh - 1) * self.dilation[0] + 1 - ih, 0)\n        pad_w = max((ow - 1) * self.stride[1] + (kw - 1) * self.dilation[1] + 1 - iw, 0)\n        if pad_h > 0 or pad_w > 0:\n            x = F.pad(x, [pad_w//2, pad_w - pad_w//2, pad_h//2, pad_h - pad_h//2])\n        return F.conv2d(x, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n\n\nclass Conv2dStaticSamePadding(nn.Conv2d):\n    \"\"\" 2D Convolutions like TensorFlow, for a fixed image size\"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, image_size=None, **kwargs):\n        super().__init__(in_channels, out_channels, kernel_size, **kwargs)\n        self.stride = self.stride if len(self.stride) == 2 else [self.stride[0]] * 2\n\n        # Calculate padding based on image size and save it\n        assert image_size is not None\n        ih, iw = image_size if type(image_size) == list else [image_size, image_size]\n        kh, kw = self.weight.size()[-2:]\n        sh, sw = self.stride\n        oh, ow = math.ceil(ih / sh), math.ceil(iw / sw)\n        pad_h = max((oh - 1) * self.stride[0] + (kh - 1) * self.dilation[0] + 1 - ih, 0)\n        pad_w = max((ow - 1) * self.stride[1] + (kw - 1) * self.dilation[1] + 1 - iw, 0)\n        if pad_h > 0 or pad_w > 0:\n            self.static_padding = nn.ZeroPad2d((pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2))\n        else:\n            self.static_padding = Identity()\n\n    def forward(self, x):\n        x = self.static_padding(x)\n        x = F.conv2d(x, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n        return x\n\n\nclass Identity(nn.Module):\n    def __init__(self,):\n        super(Identity, self).__init__()\n\n    def forward(self, input):\n        return input\n\n\n########################################################################\n############## HELPERS FUNCTIONS FOR LOADING MODEL PARAMS ##############\n########################################################################\n\n\ndef efficientnet_params(model_name):\n    \"\"\" Map EfficientNet model name to parameter coefficients. \"\"\"\n    params_dict = {\n        # Coefficients:   width,depth,res,dropout\n        'efficientnet-b0': (1.0, 1.0, 224, 0.2),\n        'efficientnet-b1': (1.0, 1.1, 240, 0.2),\n        'efficientnet-b2': (1.1, 1.2, 260, 0.3),\n        'efficientnet-b3': (1.2, 1.4, 300, 0.3),\n        'efficientnet-b4': (1.4, 1.8, 380, 0.4),\n        'efficientnet-b5': (1.6, 2.2, 456, 0.4),\n        'efficientnet-b6': (1.8, 2.6, 528, 0.5),\n        'efficientnet-b7': (2.0, 3.1, 600, 0.5),\n    }\n    return params_dict[model_name]\n\n\nclass BlockDecoder(object):\n    \"\"\" Block Decoder for readability, straight from the official TensorFlow repository \"\"\"\n\n    @staticmethod\n    def _decode_block_string(block_string):\n        \"\"\" Gets a block through a string notation of arguments. \"\"\"\n        assert isinstance(block_string, str)\n\n        ops = block_string.split('_')\n        options = {}\n        for op in ops:\n            splits = re.split(r'(\\d.*)', op)\n            if len(splits) >= 2:\n                key, value = splits[:2]\n                options[key] = value\n\n        # Check stride\n        assert (('s' in options and len(options['s']) == 1) or\n                (len(options['s']) == 2 and options['s'][0] == options['s'][1]))\n\n        return BlockArgs(\n            kernel_size=int(options['k']),\n            num_repeat=int(options['r']),\n            input_filters=int(options['i']),\n            output_filters=int(options['o']),\n            expand_ratio=int(options['e']),\n            id_skip=('noskip' not in block_string),\n            se_ratio=float(options['se']) if 'se' in options else None,\n            stride=[int(options['s'][0])])\n\n    @staticmethod\n    def _encode_block_string(block):\n        \"\"\"Encodes a block to a string.\"\"\"\n        args = [\n            'r%d' % block.num_repeat,\n            'k%d' % block.kernel_size,\n            's%d%d' % (block.strides[0], block.strides[1]),\n            'e%s' % block.expand_ratio,\n            'i%d' % block.input_filters,\n            'o%d' % block.output_filters\n        ]\n        if 0 < block.se_ratio <= 1:\n            args.append('se%s' % block.se_ratio)\n        if block.id_skip is False:\n            args.append('noskip')\n        return '_'.join(args)\n\n    @staticmethod\n    def decode(string_list):\n        \"\"\"\n        Decodes a list of string notations to specify blocks inside the network.\n\n        :param string_list: a list of strings, each string is a notation of block\n        :return: a list of BlockArgs namedtuples of block args\n        \"\"\"\n        assert isinstance(string_list, list)\n        blocks_args = []\n        for block_string in string_list:\n            blocks_args.append(BlockDecoder._decode_block_string(block_string))\n        return blocks_args\n\n    @staticmethod\n    def encode(blocks_args):\n        \"\"\"\n        Encodes a list of BlockArgs to a list of strings.\n\n        :param blocks_args: a list of BlockArgs namedtuples of block args\n        :return: a list of strings, each string is a notation of block\n        \"\"\"\n        block_strings = []\n        for block in blocks_args:\n            block_strings.append(BlockDecoder._encode_block_string(block))\n        return block_strings\n\n\ndef efficientnet(width_coefficient=None, depth_coefficient=None, dropout_rate=0.2,\n                 drop_connect_rate=0.2, image_size=None, num_classes=1000):\n    \"\"\" Creates a efficientnet model. \"\"\"\n\n    blocks_args = [\n        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',\n        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',\n        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',\n        'r1_k3_s11_e6_i192_o320_se0.25',\n    ]\n    blocks_args = BlockDecoder.decode(blocks_args)\n\n    global_params = GlobalParams(\n        batch_norm_momentum=0.99,\n        batch_norm_epsilon=1e-3,\n        dropout_rate=dropout_rate,\n        drop_connect_rate=drop_connect_rate,\n        # data_format='channels_last',  # removed, this is always true in PyTorch\n        num_classes=num_classes,\n        width_coefficient=width_coefficient,\n        depth_coefficient=depth_coefficient,\n        depth_divisor=8,\n        min_depth=None,\n        image_size=image_size,\n    )\n\n    return blocks_args, global_params\n\n\ndef get_model_params(model_name, override_params):\n    \"\"\" Get the block args and global params for a given model \"\"\"\n    if model_name.startswith('efficientnet'):\n        w, d, s, p = efficientnet_params(model_name)\n        # note: all models have drop connect rate = 0.2\n        blocks_args, global_params = efficientnet(\n            width_coefficient=w, depth_coefficient=d, dropout_rate=p, image_size=s)\n    else:\n        raise NotImplementedError('model name is not pre-defined: %s' % model_name)\n    if override_params:\n        # ValueError will be raised here if override_params has fields not included in global_params.\n        global_params = global_params._replace(**override_params)\n    return blocks_args, global_params\n\n\nurl_map = {\n    'efficientnet-b0': 'http://storage.googleapis.com/public-models/efficientnet-b0-08094119.pth',\n    'efficientnet-b1': 'http://storage.googleapis.com/public-models/efficientnet-b1-dbc7070a.pth',\n    'efficientnet-b2': 'http://storage.googleapis.com/public-models/efficientnet-b2-27687264.pth',\n    'efficientnet-b3': 'http://storage.googleapis.com/public-models/efficientnet-b3-c8376fa2.pth',\n    'efficientnet-b4': 'http://storage.googleapis.com/public-models/efficientnet-b4-e116e8b3.pth',\n    'efficientnet-b5': 'http://storage.googleapis.com/public-models/efficientnet-b5-586e6cc6.pth',\n}\n\ndef load_pretrained_weights(model, model_name, load_fc=True):\n    \"\"\" Loads pretrained weights, and downloads if loading for the first time. \"\"\"\n    state_dict = model_zoo.load_url(url_map[model_name])\n    if load_fc:\n        model.load_state_dict(state_dict)\n    else:\n        state_dict.pop('_fc.weight')\n        state_dict.pop('_fc.bias')\n        res = model.load_state_dict(state_dict, strict=False)\n        assert str(res.missing_keys) == str(['_fc.weight', '_fc.bias']), 'issue loading pretrained weights'\n    print('Loaded pretrained weights for {}'.format(model_name))\n    \n    \nclass MBConvBlock(nn.Module):\n    \"\"\"\n    Mobile Inverted Residual Bottleneck Block\n\n    Args:\n        block_args (namedtuple): BlockArgs, see above\n        global_params (namedtuple): GlobalParam, see above\n\n    Attributes:\n        has_se (bool): Whether the block contains a Squeeze and Excitation layer.\n    \"\"\"\n\n    def __init__(self, block_args, global_params):\n        super().__init__()\n        self._block_args = block_args\n        self._bn_mom = 1 - global_params.batch_norm_momentum\n        self._bn_eps = global_params.batch_norm_epsilon\n        self.has_se = (self._block_args.se_ratio is not None) and (0 < self._block_args.se_ratio <= 1)\n        self.id_skip = block_args.id_skip  # skip connection and drop connect\n\n        # Get static or dynamic convolution depending on image size\n        Conv2d = get_same_padding_conv2d(image_size=global_params.image_size)\n\n        # Expansion phase\n        inp = self._block_args.input_filters  # number of input channels\n        oup = self._block_args.input_filters * self._block_args.expand_ratio  # number of output channels\n        if self._block_args.expand_ratio != 1:\n            self._expand_conv = Conv2d(in_channels=inp, out_channels=oup, kernel_size=1, bias=False)\n            self._bn0 = nn.BatchNorm2d(num_features=oup, momentum=self._bn_mom, eps=self._bn_eps)\n\n        # Depthwise convolution phase\n        k = self._block_args.kernel_size\n        s = self._block_args.stride\n        self._depthwise_conv = Conv2d(\n            in_channels=oup, out_channels=oup, groups=oup,  # groups makes it depthwise\n            kernel_size=k, stride=s, bias=False)\n        self._bn1 = nn.BatchNorm2d(num_features=oup, momentum=self._bn_mom, eps=self._bn_eps)\n\n        # Squeeze and Excitation layer, if desired\n        if self.has_se:\n            num_squeezed_channels = max(1, int(self._block_args.input_filters * self._block_args.se_ratio))\n            self._se_reduce = Conv2d(in_channels=oup, out_channels=num_squeezed_channels, kernel_size=1)\n            self._se_expand = Conv2d(in_channels=num_squeezed_channels, out_channels=oup, kernel_size=1)\n\n        # Output phase\n        final_oup = self._block_args.output_filters\n        self._project_conv = Conv2d(in_channels=oup, out_channels=final_oup, kernel_size=1, bias=False)\n        self._bn2 = nn.BatchNorm2d(num_features=final_oup, momentum=self._bn_mom, eps=self._bn_eps)\n\n    def forward(self, inputs, drop_connect_rate=None):\n        \"\"\"\n        :param inputs: input tensor\n        :param drop_connect_rate: drop connect rate (float, between 0 and 1)\n        :return: output of block\n        \"\"\"\n\n        # Expansion and Depthwise Convolution\n        x = inputs\n        if self._block_args.expand_ratio != 1:\n            x = relu_fn(self._bn0(self._expand_conv(inputs)))\n        x = relu_fn(self._bn1(self._depthwise_conv(x)))\n\n        # Squeeze and Excitation\n        if self.has_se:\n            x_squeezed = F.adaptive_avg_pool2d(x, 1)\n            x_squeezed = self._se_expand(relu_fn(self._se_reduce(x_squeezed)))\n            x = torch.sigmoid(x_squeezed) * x\n\n        x = self._bn2(self._project_conv(x))\n\n        # Skip connection and drop connect\n        input_filters, output_filters = self._block_args.input_filters, self._block_args.output_filters\n        if self.id_skip and self._block_args.stride == 1 and input_filters == output_filters:\n            if drop_connect_rate:\n                x = drop_connect(x, p=drop_connect_rate, training=self.training)\n            x = x + inputs  # skip connection\n        return x\n\n\nclass EfficientNet(nn.Module):\n    \"\"\"\n    An EfficientNet model. Most easily loaded with the .from_name or .from_pretrained methods\n\n    Args:\n        blocks_args (list): A list of BlockArgs to construct blocks\n        global_params (namedtuple): A set of GlobalParams shared between blocks\n\n    Example:\n        model = EfficientNet.from_pretrained('efficientnet-b0')\n\n    \"\"\"\n\n    def __init__(self, blocks_args=None, global_params=None):\n        super().__init__()\n        assert isinstance(blocks_args, list), 'blocks_args should be a list'\n        assert len(blocks_args) > 0, 'block args must be greater than 0'\n        self._global_params = global_params\n        self._blocks_args = blocks_args\n\n        # Get static or dynamic convolution depending on image size\n        Conv2d = get_same_padding_conv2d(image_size=global_params.image_size)\n\n        # Batch norm parameters\n        bn_mom = 1 - self._global_params.batch_norm_momentum\n        bn_eps = self._global_params.batch_norm_epsilon\n\n        # Stem\n        in_channels = 3  # rgb\n        out_channels = round_filters(32, self._global_params)  # number of output channels\n        self._conv_stem = Conv2d(in_channels, out_channels, kernel_size=3, stride=2, bias=False)\n        self._bn0 = nn.BatchNorm2d(num_features=out_channels, momentum=bn_mom, eps=bn_eps)\n\n        # Build blocks\n        self._blocks = nn.ModuleList([])\n        for block_args in self._blocks_args:\n\n            # Update block input and output filters based on depth multiplier.\n            block_args = block_args._replace(\n                input_filters=round_filters(block_args.input_filters, self._global_params),\n                output_filters=round_filters(block_args.output_filters, self._global_params),\n                num_repeat=round_repeats(block_args.num_repeat, self._global_params)\n            )\n\n            # The first block needs to take care of stride and filter size increase.\n            self._blocks.append(MBConvBlock(block_args, self._global_params))\n            if block_args.num_repeat > 1:\n                block_args = block_args._replace(input_filters=block_args.output_filters, stride=1)\n            for _ in range(block_args.num_repeat - 1):\n                self._blocks.append(MBConvBlock(block_args, self._global_params))\n\n        # Head\n        in_channels = block_args.output_filters  # output of final block\n        out_channels = round_filters(1280, self._global_params)\n        self._conv_head = Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n        self._bn1 = nn.BatchNorm2d(num_features=out_channels, momentum=bn_mom, eps=bn_eps)\n\n        # Final linear layer\n        self._dropout = self._global_params.dropout_rate\n        self._fc = nn.Linear(out_channels, self._global_params.num_classes)\n\n    def extract_features(self, inputs):\n        \"\"\" Returns output of the final convolution layer \"\"\"\n\n        # Stem\n        x = relu_fn(self._bn0(self._conv_stem(inputs)))\n\n        # Blocks\n        for idx, block in enumerate(self._blocks):\n            drop_connect_rate = self._global_params.drop_connect_rate\n            if drop_connect_rate:\n                drop_connect_rate *= float(idx) / len(self._blocks)\n            x = block(x, drop_connect_rate=drop_connect_rate)\n\n        # Head\n        x = relu_fn(self._bn1(self._conv_head(x)))\n\n        return x\n\n    def forward(self, inputs):\n        \"\"\" Calls extract_features to extract features, applies final linear layer, and returns logits. \"\"\"\n\n        # Convolution layers\n        x = self.extract_features(inputs)\n\n        # Pooling and final linear layer\n        x = F.adaptive_avg_pool2d(x, 1).squeeze(-1).squeeze(-1)\n        if self._dropout:\n            x = F.dropout(x, p=self._dropout, training=self.training)\n        x = self._fc(x)\n        return x\n\n    @classmethod\n    def from_name(cls, model_name, override_params=None):\n        cls._check_model_name_is_valid(model_name)\n        blocks_args, global_params = get_model_params(model_name, override_params)\n        return EfficientNet(blocks_args, global_params)\n\n    @classmethod\n    def from_pretrained(cls, model_name, num_classes=1000):\n        model = EfficientNet.from_name(model_name, override_params={'num_classes': num_classes})\n        return model\n\n    @classmethod\n    def get_image_size(cls, model_name):\n        cls._check_model_name_is_valid(model_name)\n        _, _, res, _ = efficientnet_params(model_name)\n        return res\n\n    @classmethod\n    def _check_model_name_is_valid(cls, model_name, also_need_pretrained_weights=False):\n        \"\"\" Validates model name. None that pretrained weights are only available for\n        the first four models (efficientnet-b{i} for i in 0,1,2,3) at the moment. \"\"\"\n        num_models = 4 if also_need_pretrained_weights else 8\n        valid_models = ['efficientnet_b'+str(i) for i in range(num_models)]\n        if model_name.replace('-','_') not in valid_models:\n            raise ValueError('model_name should be one of: ' + ', '.join(valid_models))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#making model\nmd_ef = EfficientNet.from_pretrained('efficientnet-b5', num_classes=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#copying weighst to the local directory \n!mkdir models\n!cp '../input/kaggle-public/abcdef.pth' 'models'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import multiprocessing\nfrom tqdm import tqdm\nimport numpy as np\nimport glob\nimport cv2\nimport sys\nimport os\n\n\nclass ImagePreprocessor(object):\n    def __init__(self, root_dir: str, save_dir: str, img_size: int, tolerance: int = 10, remove_outer_pixels: float = 0.0):\n        \"\"\"\n        Preprocess images for kaggle competitions and general training purposes.\n\n        args:\n            root_dir  = absolute path to images folder\n            save_dir  = folder in which to store processed images\n            img_size  = final image dimensions, common values : 224, 512\n            tolerance = tolerance value for pitch_black_remover func\n            remove_outer_pixels = remove boundary pixels of image\n        \"\"\"\n        if remove_outer_pixels > 0.50:\n            print(\"ERROR: eroding more than 50% of image\")\n            raise InterruptedError\n\n        self.root_dir = root_dir\n        self.img_size = img_size\n        self.tolerance = tolerance\n        self.remove_outer_pixels = remove_outer_pixels\n\n        self.images = glob.glob(f\"{self.root_dir}/*.png\") + glob.glob(\n            f\"{self.root_dir}/*.jpeg\") + glob.glob(f\"{self.root_dir}/*.jpg\")\n        self.save_dir = save_dir\n        os.makedirs(self.save_dir, exist_ok=True)\n        self.total_count = len(self.images)\n\n    # counter decorator\n    @staticmethod\n    def _counter(func):\n        def wrapper(self, *args, **kwargs):\n            func(self, *args, **kwargs)\n        return wrapper\n\n    # different preprocessing methods\n\n    @staticmethod\n    def light_sensitivity_reducer(img: np.ndarray, alpha: int = 4, beta: int = -4, gamma: int = 128):\n        \"\"\"smooth image and apply ben's preprocessing\"\"\"\n        return cv2.addWeighted(img, alpha, cv2.GaussianBlur(img, (0, 0), 30), beta, gamma)\n\n    @staticmethod\n    def outer_pixels_remover(img: np.ndarray, scale: float):\n        \"\"\"remove outer/boundary pixels of image\"\"\"\n        scale_2 = scale / 2.0\n        miny = int(img.shape[0]*scale_2)\n        maxy = int(img.shape[0]-miny)\n        minx = int(img.shape[1]*scale_2)\n        maxx = int(img.shape[1]-minx)\n        return img[miny:maxy, minx:maxx]\n\n    @staticmethod\n    def scale_image(img: np.ndarray, img_size: int):\n        \"\"\"resize image based on given scale\"\"\"\n        return cv2.resize(img, (img_size, img_size))\n\n    @staticmethod\n    def pitch_black_remover(img: np.ndarray, tolerance: int = 10):\n        \"\"\"remove black pixels in image edges\"\"\"\n        if img.ndim == 2:\n            img_mask = img > tolerance\n            return img[np.ix_(img_mask.any(1), img_mask.any(0))]\n        elif img.ndim == 3:\n            greyed = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n            img_mask = greyed > tolerance\n            img_1 = img[:, :, 0][np.ix_(img_mask.any(1), img_mask.any(0))]\n            if img_1.shape[0] == 0:\n                return img\n            img_2 = img[:, :, 1][np.ix_(img_mask.any(1), img_mask.any(0))]\n            img_3 = img[:, :, 2][np.ix_(img_mask.any(1), img_mask.any(0))]\n            return np.stack([img_1, img_2, img_3], axis=-1)\n        else:\n            print(\"Image has more than 3 dimensions\")\n            raise InterruptedError\n\n    # collaging different methods together and preprocessing images\n\n    def replace_existing(self):\n        import shutil\n        shutil.rmtree(self.root_dir)\n        os.makedirs(self.root_dir)\n        processed_images = [i.path for i in os.scandir(\n            self.save_dir) if i.is_file()]\n        processor_pool = multiprocessing.Pool(64)\n        for counter, _ in enumerate(processor_pool.imap_unordered(lambda x: shutil.move(x, self.root_dir), processed_images), 1):\n            sys.stdout.write(\n                f\"\\rMoving : {(counter/self.total_count)*100:3.2f}% \\t[ {counter}/{self.total_count} ]\")\n        os.rmdir(self.save_dir)\n        sys.stdout.write(\"\\n\\n\")\n\n    def forward(self, image: str):\n        \"\"\"take a single image path, preprocesse image and store preprocessed image\"\"\"\n        img = cv2.imread(image)\n        img = self.pitch_black_remover(img, tolerance=self.tolerance)\n        img = self.scale_image(img, img_size=self.img_size)\n#         img = self.light_sensitivity_reducer(img)\n        if self.remove_outer_pixels > 0.0:\n            img = self.outer_pixels_remover(img, self.remove_outer_pixels)\n        cv2.imwrite(os.path.join(self.save_dir, image.split('/')[-1]), img)\n\n    def run(self, replace: bool = False):\n        \"\"\"process all images in root_dir in an iterative way\"\"\"\n        for image in tqdm(self.images):\n            # add logging if required\n            self.forward(image)\n        if replace:\n            self.replace_existing()\n\n    def parallel_run(self, workers: int = multiprocessing.cpu_count(), replace: bool = False):\n        \"\"\"process all images in root_dir parallely using python's multiprocessing library\"\"\"\n        # haven't figured out a stable way for logging in case of multiprocessing\n        processor_pool = multiprocessing.Pool(workers)\n\n        for counter, _ in enumerate(processor_pool.imap_unordered(self.forward, self.images), 1):\n            sys.stdout.write(\n                f\"\\rProgress : {(counter/self.total_count)*100:3.2f}% \\t[ {counter}/{self.total_count} ]\")\n        if replace:\n            self.replace_existing()\n        sys.stdout.write(\"\\n\\n\")\n        \npreprocessor = ImagePreprocessor(root_dir='../input/aptos2019-blindness-detection/test_images/', \n                                 save_dir='test_processed', \n                                 img_size=256,\n                                 tolerance=10, \n                                 remove_outer_pixels=0\n                                )\npreprocessor.parallel_run(multiprocessing.cpu_count(),False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_df():\n    base_image_dir = os.path.join('..', 'input/aptos2019-blindness-detection/')\n    train_dir = os.path.join(base_image_dir,'train_images/')\n    df = pd.read_csv(os.path.join(base_image_dir, 'train.csv'))\n    df['path'] = df['id_code'].map(lambda x: os.path.join(train_dir,'{}.png'.format(x)))\n    df = df.drop(columns=['id_code'])\n    df = df.sample(frac=1).reset_index(drop=True) #shuffle dataframe\n    test_df = pd.read_csv('../input/aptos2019-blindness-detection/sample_submission.csv')\n    return df, test_df\n\ndf, test_df = get_df()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#you can play around with tfms and image sizes\nbs = 64\nsz = 224\ntfms = get_transforms(do_flip=True,flip_vert=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = (ImageList.from_df(df=df,path='./',cols='path') \n        .split_by_rand_pct(0.2) \n        .label_from_df(cols='diagnosis',label_cls=FloatList) \n        .transform(tfms,size=sz,resize_method=ResizeMethod.SQUISH,padding_mode='zeros') \n        .databunch(bs=bs,num_workers=4) \n        .normalize(imagenet_stats)  \n       )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def qk(y_pred, y):\n    return torch.tensor(cohen_kappa_score(torch.round(y_pred), y, weights='quadratic'), device='cuda:0')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn = Learner(data, \n                md_ef, \n                metrics = [qk], \n                model_dir=\"models\")\nlearn.data.add_test(ImageList.from_df(test_df,\n                             'test_processed/',\n                             folder='',\n                             suffix='.png',\n                             ))\nlearn.load('abcdef')\npass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.to_fp32()\npass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class OptimizedRounder(object):\n    def __init__(self):\n        self.coef_ = 0\n\n    def _kappa_loss(self, coef, X, y):\n        X_p = np.copy(X)\n        for i, pred in enumerate(X_p):\n            if pred < coef[0]:\n                X_p[i] = 0\n            elif pred >= coef[0] and pred < coef[1]:\n                X_p[i] = 1\n            elif pred >= coef[1] and pred < coef[2]:\n                X_p[i] = 2\n            elif pred >=coef[2] and pred < coef[3]:\n                X_p[i] = 3\n            else:\n                X_p[i] = 4\n\n        ll = cohen_kappa_score(y, X_p, weights='quadratic')\n        return -ll\n\n    def fit(self, X, y):\n        y = y.cpu().apply_(lambda x: 0 if x==0 else 1 if x==0.25 else 2 if x==0.5 else 3 if x==0.75 else 4)\n        loss_partial = partial(self._kappa_loss, X=X, y=y)\n        initial_coef = [0.125,0.375,0.625,0.875]\n        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n        print(-loss_partial(self.coef_['x']))\n\n    def predict(self, X, coef):\n        X_p = np.copy(X)\n        for i, pred in enumerate(X_p):\n            if pred < coef[0]:\n                X_p[i] = 0\n            elif pred >= coef[0] and pred < coef[1]:\n                X_p[i] = 1\n            elif pred >= coef[1] and pred < coef[2]:\n                X_p[i] = 2\n            elif pred >= coef[2] and pred < coef[3]:\n                X_p[i] = 3\n            else:\n                X_p[i] = 4\n        return X_p\n\n    def coefficients(self):\n        return self.coef_['x']\n\nopt = OptimizedRounder()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"__all__ = []\n\ndef _tta_only(learn:Learner, ds_type:DatasetType=DatasetType.Valid, activ:nn.Module=None, scale:float=1.35) -> Iterator[List[Tensor]]:\n    \"Computes the outputs for several augmented inputs for TTA\"\n    dl = learn.dl(ds_type)\n    ds = dl.dataset\n    old = ds.tfms\n#     activ = ifnone(activ, _loss_func2activ(learn.loss_func))\n    augm_tfm = [o for o in learn.data.train_ds.tfms]\n    try:\n        pbar = master_bar(range(4))\n        for i in pbar:\n            row = 1 if i&1 else 0\n            col = 1 if i&2 else 0\n#             flip = i&4\n            d = {'row_pct':row, 'col_pct':col, 'is_random':False}\n            tfm = [*augm_tfm, zoom(scale=scale, **d), crop_pad(**d)]\n#             if flip: tfm.append(flip_lr(p=1.))\n            ds.tfms = tfm\n            yield get_preds(learn.model, dl, pbar=pbar, activ=activ)[0]\n    finally: ds.tfms = old\n\n# Learner.tta_only = _tta_only\n\ndef _TTA(learn:Learner, scale:float=1.2, ds_type:DatasetType=DatasetType.Valid, activ:nn.Module=None, with_loss:bool=False) -> Tensors:\n    \"Applies TTA to predict on `ds_type` dataset.\"\n    preds_normal,y = learn.get_preds(ds_type)\n    preds_normal_tta = list(_tta_only(learn,ds_type=ds_type, scale=scale))\n    preds_normal_tta_avg = torch.stack(preds_normal_tta).mean(0)\n    \n    learn.dl(ds_type).dataset.tfms = [flip_lr(p=1.)]\n    preds_flipped = get_preds(learn.model,learn.dl(ds_type))[0]\n    preds_flipped_tta = list(_tta_only(learn,ds_type=ds_type, scale=scale))\n    preds_flipped_tta_avg = torch.stack(preds_flipped_tta).mean(0)\n    \n#     if beta is None: return preds,avg_preds,y\n#     else:\n#         final_preds = preds*beta + avg_preds*(1-beta)\n#         if with_loss:\n#             with NoneReduceOnCPU(learn.loss_func) as lf: loss = lf(final_preds, y)\n#             return final_preds, y, loss\n#         return final_preds, y\n\n    return preds_normal,preds_normal_tta_avg,preds_flipped,preds_flipped_tta_avg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.dl(DatasetType.Test).dataset.tfms=[]\nop = _TTA(learn,ds_type=DatasetType.Test)\nop_1 = [2*p-4 for p in op]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!cp ../input/l1-aptos/L1.pkl export.pkl\nlearn = load_learner('',test=ImageList.from_df(test_df,\n                             'test_processed/',\n                             folder='',\n                             suffix='.png',\n                             ))\nlearn.dl(DatasetType.Test).dataset.tfms=[]\nop_2 = _TTA(learn,ds_type=DatasetType.Test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p = [0] * len(op_1)\nfor i in range(len(op_1)):\n    p[i] = (op_1[i]+op_2[i])/2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"opt = OptimizedRounder()\nop = 0.3*p[0]+0.2*p[1]+0.3*p[2]+0.2*p[3]\nop_preds = torch.Tensor(opt.predict(op,[-2.8031, -1.0877, 1.02 , 2.92]))\nprint(type(op_preds))\n# preds = torch.stack(op_preds).median(0).values \ntest_df.diagnosis = op_preds.numpy().astype(int)\ntest_df.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.stack(op_preds).median(0).values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.groupby('diagnosis').size()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!rm -r test_processed","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}