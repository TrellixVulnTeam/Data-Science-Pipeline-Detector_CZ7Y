{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport glob\nimport matplotlib.pyplot as plt\nimport gc\nimport scipy.io as sio\n#import cv2\n#import imutils\nfrom PIL import Image\nimport tensorflow as tf\nimport tensorflow.image\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"/kaggle/input\"))\n\npath = '/kaggle/input/'\n\n# Any results you write to the current directory are saved as output.\n%env JOBLIB_TEMP_FOLDER=/tmp\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf_train_lbl = pd.read_csv(path + 'train.csv')\ndf_test_lbl = pd.read_csv(path + 'test.csv')\n\nm_tr = np.shape(df_train_lbl)[0]\nm_te = np.shape(df_test_lbl)[0]\n\nprint(m_tr)\n\nno_dr_ratio = float((np.shape(df_train_lbl.loc[df_train_lbl['diagnosis']==0])[0])/m_tr)\nprint(no_dr_ratio)\n\nmild_dr_ratio = float((np.shape(df_train_lbl.loc[df_train_lbl['diagnosis']==1])[0])/m_tr)\nprint(mild_dr_ratio)\n\nmoderate_dr_ratio = float((np.shape(df_train_lbl.loc[df_train_lbl['diagnosis']==2])[0])/m_tr)\nprint(moderate_dr_ratio)\n\nsevere_dr_ratio = float((np.shape(df_train_lbl.loc[df_train_lbl['diagnosis']==3])[0])/m_tr)\nprint(severe_dr_ratio)\n\nproliferative_dr_ratio = float((np.shape(df_train_lbl.loc[df_train_lbl['diagnosis']==4])[0])/m_tr)\nprint(proliferative_dr_ratio)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Under-sampling of dataset\ntrain_path = path + 'train_images/'\ntest_path = path + 'test_images/'\n\nall_images = glob.glob(train_path + '*.png')\ni=1\n\nsampled_train_lbl = pd.DataFrame(columns = df_train_lbl.columns)\n\nif 'df_train_lbl' in globals():\n    for img_path in all_images[0:2]:\n        base_name = os.path.basename(img_path)\n        base_name = base_name.split('.')[0]\n\n        diagnosis_lbl = df_train_lbl.loc[df_train_lbl['id_code'] == base_name]['diagnosis']\n        diagnosis_lbl = diagnosis_lbl.values[0]\n        plt.figure(i)\n        img = plt.imread(img_path)\n        plt.imshow(img)\n        plt.title(str(diagnosis_lbl))\n        i+=1\n        \n        smallest_class_ratio = 0\n        if 'no_dr_ratio' in globals() and 'mild_dr_ratio' in globals() and 'moderate_dr_ratio' in globals() and 'severe_dr_ratio' in globals() and 'proliferative_dr_ratio' in globals():\n            smallest_class_ratio = severe_dr_ratio\n            class_size = int(m_tr * smallest_class_ratio)\n\n            print(smallest_class_ratio)\n            df_nodr = df_train_lbl[df_train_lbl.diagnosis == 0][np.random.rand(df_train_lbl[df_train_lbl.diagnosis== 0].index.size) < (smallest_class_ratio/2)]\n            sampled_train_lbl = pd.concat([sampled_train_lbl,df_nodr])\n            print('df_nodr size after sampling:' + str(np.shape(df_nodr)))\n\n            df_mild_dr = df_train_lbl[df_train_lbl.diagnosis == 1][np.random.rand(df_train_lbl[df_train_lbl.diagnosis== 1].index.size) < smallest_class_ratio]\n            sampled_train_lbl = pd.concat([sampled_train_lbl,df_mild_dr])\n            print('df_mild_dr size after sampling:' + str(np.shape(df_mild_dr)))\n\n            df_moderate_dr = df_train_lbl[df_train_lbl.diagnosis == 2][np.random.rand(df_train_lbl[df_train_lbl.diagnosis== 2].index.size) < smallest_class_ratio]\n            sampled_train_lbl = pd.concat([sampled_train_lbl,df_moderate_dr])\n            print('df_moderate_dr size after sampling:' + str(np.shape(df_moderate_dr)))\n\n            df_severe_dr = df_train_lbl[df_train_lbl.diagnosis == 3]#[np.random.rand(df_train_lbl[df_train_lbl.diagnosis== 3].index.size) < smallest_class_ratio]\n            sampled_train_lbl = pd.concat([sampled_train_lbl,df_severe_dr])\n            print('df_severe_dr size after sampling:' + str(np.shape(df_severe_dr)))\n\n            df_proliferative_dr = df_train_lbl[df_train_lbl.diagnosis == 4]#[np.random.rand(df_train_lbl[df_train_lbl.diagnosis== 4].index.size) < smallest_class_ratio]\n            sampled_train_lbl = pd.concat([sampled_train_lbl,df_proliferative_dr])\n            print('df_proliferative_dr size after sampling:' + str(np.shape(df_proliferative_dr)))\n\n            print(np.shape(sampled_train_lbl))\n            print(sampled_train_lbl)\n            sampled_m_tr = np.shape(sampled_train_lbl)[0]\n\n            no_dr_ratio = float((np.shape(sampled_train_lbl.loc[sampled_train_lbl['diagnosis']==0])[0])/sampled_m_tr)\n            print(no_dr_ratio)\n\n            mild_dr_ratio = float((np.shape(sampled_train_lbl.loc[sampled_train_lbl['diagnosis']==1])[0])/sampled_m_tr)\n            print(mild_dr_ratio)\n\n            moderate_dr_ratio = float((np.shape(sampled_train_lbl.loc[sampled_train_lbl['diagnosis']==2])[0])/sampled_m_tr)\n            print(moderate_dr_ratio)\n\n            severe_dr_ratio = float((np.shape(sampled_train_lbl.loc[sampled_train_lbl['diagnosis']==3])[0])/sampled_m_tr)\n            print(severe_dr_ratio)\n\n            proliferative_dr_ratio = float((np.shape(sampled_train_lbl.loc[sampled_train_lbl['diagnosis']==4])[0])/sampled_m_tr)\n            print(proliferative_dr_ratio)\n\n            print(np.shape(sampled_train_lbl))\n            \n            del df_nodr\n            del df_mild_dr\n            del df_moderate_dr\n            del df_severe_dr\n            del df_proliferative_dr\n            \n            gc.collect()\n\n        else:\n            print('variables not defined.. please run data_prep.py before running this file...')\n\ngc.collect()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def image_resize_tf(img_path):\n    filename = tf.placeholder(tf.string, name=\"inputFile\")\n    fileContent = tf.read_file(filename, name=\"loadFile\")\n    image = tf.image.decode_png(fileContent, name=\"decodePng\")\n    \n    resize_nearest_neighbor = tf.image.resize_images(image, size=[224,224], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n    \n    sess = tf.Session()\n    feed_dict={filename: img_path}\n    with sess.as_default():\n        actual_resize_nearest_neighbor = resize_nearest_neighbor.eval(feed_dict)\n        #plt.imshow(actual_resize_nearest_neighbor)\n    return actual_resize_nearest_neighbor","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"resized_img = image_resize_tf(\"../input/train_images/875d2ffcbf47.png\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# shuffling the data\nfrom random import shuffle\n\nidx_arr = [i for i in range(0,sampled_m_tr)]\nshuffle(idx_arr)\nm_train_validate = int(sampled_m_tr*0.7)\nm_validate = sampled_m_tr - m_train_validate\nidx_train = idx_arr[:m_train_validate]\nidx_validate = idx_arr[m_train_validate:]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# resizing training images\nimg_arr_train = np.ndarray(shape=(m_train_validate, 224, 224, 3))\nlbl_train = np.ndarray(shape=(m_train_validate, 5))\n#one_hot_targets = np.eye(nb_classes)[targets]\nidx = 0\nk = 0\nfor row in sampled_train_lbl.iterrows():\n    if idx in idx_train:\n        name = row[1]['id_code'] + '.png'\n        lbl_train[k,:] = np.eye(5)[int(row[1]['diagnosis'])].T\n        img = image_resize_tf(train_path + name)\n        #print(img)\n        img_arr_train[k,:,:,:] = img\n        k += 1\n    idx += 1\nprint(np.shape(img_arr_train))\nprint(np.shape(lbl_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# resizing validating images\nimg_arr_validate = np.ndarray(shape=(m_validate, 224, 224, 3))\nlbl_validate = np.ndarray(shape=(m_validate, 5))\nidx = 0\nk = 0\nfor row in sampled_train_lbl.iterrows():\n    if idx in idx_validate:\n        name = row[1]['id_code'] + '.png'\n        lbl_validate[k] = np.eye(5)[int(row[1]['diagnosis'])].T\n        img = image_resize_tf(train_path + name)\n        #print(img)\n        img_arr_validate[k,:,:,:] = img\n        k += 1\n    idx += 1\nprint(np.shape(img_arr_validate))\nprint(np.shape(lbl_validate))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# vgg19 code\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout, Flatten\nfrom keras.layers import Conv2D\nfrom keras.layers import MaxPooling2D\nfrom keras.optimizers import SGD\nfrom keras.optimizers import Adam\n\ninput_shape = (224, 224, 3)\n\n#Instantiate an empty model\nmodel = Sequential([\nConv2D(64, (3, 3), input_shape=input_shape, padding='same', activation='relu'),\nConv2D(64, (3, 3), activation='relu', padding='same'),\nMaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\nConv2D(128, (3, 3), activation='relu', padding='same'),\nConv2D(128, (3, 3), activation='relu', padding='same',),\nMaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\nConv2D(256, (3, 3), activation='relu', padding='same',),\nConv2D(256, (3, 3), activation='relu', padding='same',),\nConv2D(256, (3, 3), activation='relu', padding='same',),\nMaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\nConv2D(512, (3, 3), activation='relu', padding='same',),\nConv2D(512, (3, 3), activation='relu', padding='same',),\nConv2D(512, (3, 3), activation='relu', padding='same',),\nMaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\nConv2D(512, (3, 3), activation='relu', padding='same',),\nConv2D(512, (3, 3), activation='relu', padding='same',),\nConv2D(512, (3, 3), activation='relu', padding='same',),\nMaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\nFlatten(),\nDense(4096, activation='relu'),\nDense(4096, activation='relu'),\n#Dense(1000, activation='relu'),\nDense(5, activation='softmax'),    \n])\n\nmodel.summary()\n\n# Compile the model\n#model.compile(loss=keras.losses.categorical_crossentropy, optimizer='adam', metrics=[\"accuracy\"])\nsgd = SGD(lr=0.0001, momentum=0.9)\n#adm = Adam()\nmodel.compile(loss=keras.losses.categorical_crossentropy, optimizer='rmsprop', metrics=[\"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#img_arr_train = img_arr_train/255\n#img_arr_validate = img_arr_validate/255\n\n# contering the the image array for training\nfrom numpy import asarray\n\nk = 0\nfor k in range(0,np.size(img_arr_train,1)):\n    img_train = img_arr_train[k,:,:,:]\n    img_train_scaled = asarray(img_train)\n    mean1, std1 = img_train_scaled.mean(), img_train_scaled.std()\n    img_train_scaled = (img_train_scaled - mean1)/std1\n    img_arr_train[k,:,:,:] = img_train_scaled\n    \nprint(np.shape(img_arr_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# contering the the image array for validating\nfrom numpy import asarray\n\nk = 0\nfor k in range(0,np.size(img_arr_validate,1)):\n    img_validate = img_arr_validate[k,:,:,:]\n    img_validate_scaled = asarray(img_validate)\n    mean1, std1 = img_validate_scaled.mean(), img_validate_scaled.std()\n    img_validate_scaled = (img_validate_scaled - mean1)/std1\n    img_arr_validate[k,:,:,:] = img_validate_scaled\n    \nprint(np.shape(img_arr_validate))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"lbls = np.array(sampled_train_lbl['diagnosis']).reshape(sampled_m_tr,1)\nhistory = model.fit(x=img_arr_train,y=lbl_train,validation_data=(img_arr_validate, lbl_validate),batch_size=128,epochs=20,verbose=1) #batch_size=20","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#img_arr_train\ndel img_arr_train\ndel img_arr_validate\ndel df_train_lbl\ndel df_test_lbl\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test_images = glob.glob(test_path + '*.png')\ndf_sample_sub = pd.read_csv(path + 'sample_submission.csv')\nm_test = np.shape(df_sample_sub)[0]\ntest_images = np.ndarray(shape=(m_test, 224, 224, 3))\nk = 0\n\nfor row in df_sample_sub.iterrows():\n    name = row[1]['id_code'] + '.png'\n    img = image_resize_tf(test_path + name)\n    img_test_scaled = np.asarray(img)\n    mean1, std1 = img_test_scaled.mean(), img_test_scaled.std()\n    img_test_scaled = (img_test_scaled - mean1)/std1\n    test_images[k,:,:,:] = img_test_scaled\n    k += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"scores = model.predict_proba(test_images)\ny_test = np.argmax(scores,axis=1)\nprint(y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_sample_sub['diagnosis'] = y_test\ndf_sample_sub['diagnosis'].astype('int64')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.chdir(\"/kaggle/working/\")\ndf_sample_sub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}