{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"## This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n#from efficientnet_pytorch import EfficientNet\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom collections import OrderedDict\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport sys\npackage_path = '../input/efficientnetpytorch/'\nsys.path.append(package_path)\n\nfrom efficientnet_pytorch import EfficientNet\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        pass\n        #print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"![](http://)","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from __future__ import print_function, absolute_import\nimport os\nimport sys\nimport time\nimport datetime\nimport argparse\nimport os.path as osp\nimport numpy as np\nimport random\nfrom PIL import Image\nimport tqdm\nimport cv2\nimport csv\nimport math\nimport torchvision as tv\nimport torchvision\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nfrom sklearn.metrics import f1_score\nfrom torch.utils.data import DataLoader\nfrom torch.autograd import Variable\nfrom torch.optim import lr_scheduler\nfrom tqdm import tqdm\nfrom torch.utils.data import Dataset\nimport torchvision.transforms as transforms\nfrom tensorboardX import SummaryWriter","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"name_file='../input/aptos2019-blindness-detection/test.csv'\ncsv_file=csv.reader(open(name_file,'r'))\ncontent=[]\nfor line in csv_file:\n    #print(line[0]) #打印文件每一行的信息\n    content.append(line[0]+'.png')\n    content=content[1:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.nn.parameter import Parameter\n\ndef gem(x, p=3, eps=1e-6):\n    return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1./p)\nclass GeM(nn.Module):\n    def __init__(self, p=3, eps=1e-6):\n        super(GeM,self).__init__()\n        self.p = Parameter(torch.ones(1)*p)\n        self.eps = eps\n    def forward(self, x):\n        return gem(x, p=self.p, eps=self.eps)       \n    def __repr__(self):\n        return self.__class__.__name__ + '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + ', ' + 'eps=' + str(self.eps) + ')'\nclass Baseline_single_gem(nn.Module):\n    def __init__(self, num_classes, loss_type=\"single BCE\", **kwargs):\n        super(Baseline_single_gem, self).__init__()\n        self.loss_type = loss_type\n        #resnet50 = torchvision.models.densenet169(pretrained=False)\n        resnet50 = torchvision.models.densenet201(pretrained=False)\n        self.base = nn.Sequential(*list(resnet50.children())[:-1])\n        #self.feature_dim = 512*2\n        self.feature_dim = 1920        \n        #self.feature_dim=1664\n        # self.reduce_conv = nn.Conv2d(self.feature_dim * 4, self.feature_dim, 1)\n        if self.loss_type == \"single BCE\":\n            #self.ap = nn.AdaptiveAvgPool2d(1)\n            self.ap = GeM()\n            self.classifiers = nn.Linear(in_features=self.feature_dim, out_features=num_classes)\n            self.sigmoid = nn.Sigmoid()\n            self.dropout=nn.Dropout(0.5)\n            self.cal_score=nn.Linear(in_features=num_classes, out_features=1)\n        # for m in self.modules():\n        #     if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n        #         nn.init.kaiming_normal(m.weight, mode='fan_in', nonlinearity='relu')\n        #         # import scipy.stats as stats\n        #         # stddev = m.stddev if hasattr(m, 'stddev') else 0.1\n        #         # X = stats.truncnorm(-2, 2, scale=stddev)\n        #         # values = torch.Tensor(X.rvs(m.weight.data.numel()))\n        #         # values = values.view(m.weight.data.size())\n        #         # m.weight.data.copy_(values)\n        #     elif isinstance(m, nn.BatchNorm2d):\n        #         m.weight.data.fill_(1)\n        #         m.bias.data.zero_()\n    def freeze_base(self):\n        for p in self.base.parameters():\n            p.requires_grad = False\n\n    def unfreeze_all(self):\n        for p in self.parameters():\n            p.requires_grad = True\n    def forward(self, x1):\n        x = self.base(x1)\n        #print(x.shape)\n        map_feature=torch.tensor(x)\n        # x = self.reduce_conv(x)\n        if self.loss_type == \"single BCE\":\n            x = self.ap(x)\n            #print(x.shape)\n            x = self.dropout(x)\n            x = x.view(x.size(0), -1)\n            feat_m=torch.tensor(x)\n            ys = self.classifiers(x)\n            #y = self.sigmoid(y)\n        return ys\nclass Baseline_single(nn.Module):\n    def __init__(self, num_classes, loss_type=\"single BCE\", **kwargs):\n        super(Baseline_single, self).__init__()\n        self.loss_type = loss_type\n        #resnet50 = torchvision.models.densenet169(pretrained=False)\n        resnet50 = torchvision.models.densenet201(pretrained=False)\n        self.base = nn.Sequential(*list(resnet50.children())[:-1])\n        #self.feature_dim = 512*2\n        self.feature_dim = 1920        \n        #self.feature_dim=1664\n        # self.reduce_conv = nn.Conv2d(self.feature_dim * 4, self.feature_dim, 1)\n        if self.loss_type == \"single BCE\":\n            self.ap = nn.AdaptiveAvgPool2d(1)\n            #self.ap = GeM()\n            self.classifiers = nn.Linear(in_features=self.feature_dim, out_features=num_classes)\n            self.sigmoid = nn.Sigmoid()\n            self.dropout=nn.Dropout(0.5)\n            self.cal_score=nn.Linear(in_features=num_classes, out_features=1)\n        # for m in self.modules():\n        #     if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n        #         nn.init.kaiming_normal(m.weight, mode='fan_in', nonlinearity='relu')\n        #         # import scipy.stats as stats\n        #         # stddev = m.stddev if hasattr(m, 'stddev') else 0.1\n        #         # X = stats.truncnorm(-2, 2, scale=stddev)\n        #         # values = torch.Tensor(X.rvs(m.weight.data.numel()))\n        #         # values = values.view(m.weight.data.size())\n        #         # m.weight.data.copy_(values)\n        #     elif isinstance(m, nn.BatchNorm2d):\n        #         m.weight.data.fill_(1)\n        #         m.bias.data.zero_()\n    def freeze_base(self):\n        for p in self.base.parameters():\n            p.requires_grad = False\n\n    def unfreeze_all(self):\n        for p in self.parameters():\n            p.requires_grad = True\n    def forward(self, x1):\n        x = self.base(x1)\n        #print(x.shape)\n        map_feature=torch.tensor(x)\n        # x = self.reduce_conv(x)\n        if self.loss_type == \"single BCE\":\n            x = self.ap(x)\n            #print(x.shape)\n            x = self.dropout(x)\n            x = x.view(x.size(0), -1)\n            feat_m=torch.tensor(x)\n            ys = self.classifiers(x)\n            #y = self.sigmoid(y)\n        return ys","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cv_imread(file_path):\n\tcv_img=cv2.imdecode(np.fromfile(file_path,dtype=np.uint8),-1)\n\treturn cv_img \n\ndef change_size(image):\n\t#image=cv2.imread(read_file,1) #读取图片 image_name应该是变量\n\n\tb=cv2.threshold(image,15,255,cv2.THRESH_BINARY)          #调整裁剪效果\n\tbinary_image=b[1]               #二值图--具有三通道\n\tbinary_image=cv2.cvtColor(binary_image,cv2.COLOR_BGR2GRAY)\n\tprint(binary_image.shape)       #改为单通道\n\n\tx=binary_image.shape[0]\n\tprint(\"高度x=\",x)\n\ty=binary_image.shape[1]\n\tprint(\"宽度y=\",y)\n\tedges_x=[]\n\tedges_y=[]\n\n\tfor i in range(x):\n\n\t\tfor j in range(y):\n\n\t\t\tif binary_image[i][j]==255:\n\t\t\t # print(\"横坐标\",i)\n\t\t\t # print(\"纵坐标\",j)\n\t\t\t edges_x.append(i)\n\t\t\t edges_y.append(j)\n\n\tleft=min(edges_x)               #左边界\n\tright=max(edges_x)              #右边界\n\twidth=right-left                #宽度\n\n\tbottom=min(edges_y)             #底部\n\ttop=max(edges_y)                #顶部\n\theight=top-bottom               #高度\n\n\tpre1_picture=image[left:left+width,bottom:bottom+height]        #图片截取\n\n\treturn pre1_picture                                             #返回图片数据\n\n\ndef crop_image1(img,tol=7):\n\t# img is image data\n\t# tol  is tolerance\n\t\t\n\tmask = img>tol\n\treturn img[np.ix_(mask.any(1),mask.any(0))]\n\ndef crop_image_from_gray(img,tol=7):\n\tif img.ndim ==2:\n\t\tmask = img>tol\n\t\treturn img[np.ix_(mask.any(1),mask.any(0))]\n\telif img.ndim==3:\n\t\tgray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n\t\tmask = gray_img>tol\n\t\t\n\t\tcheck_shape = img[:,:,0][np.ix_(mask.any(1),mask.any(0))].shape[0]\n\t\tif (check_shape == 0): # image is too dark so that we crop out everything,\n\t\t\treturn img # return original image\n\t\telse:\n\t\t\timg1=img[:,:,0][np.ix_(mask.any(1),mask.any(0))]\n\t\t\timg2=img[:,:,1][np.ix_(mask.any(1),mask.any(0))]\n\t\t\timg3=img[:,:,2][np.ix_(mask.any(1),mask.any(0))]\n\t#         print(img1.shape,img2.shape,img3.shape)\n\t\t\timg = np.stack([img1,img2,img3],axis=-1)\n\t#         print(img.shape)\n\t\treturn img\ndef load_ben_color(image, sigmaX=10):\n\timage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\timage = crop_image_from_gray(image)\n\timage = cv2.resize(image, (492, 492))\n\timage=cv2.addWeighted ( image,4, cv2.GaussianBlur( image , (0,0) , sigmaX) ,-4 ,128)\n\t\t\n\treturn image\n\n\ndef findCircle(image):\n\t#print(image.shape)\n\thsv_img=cv2.cvtColor(image,cv2.COLOR_BGR2HSV)\n\th_img=image[:,:,0]\n\ts_img=image[:,:,1]\n\tv_img=image[:,:,2]\n\theight,width=v_img.shape\n\t#print(int(max(height,width)/16+1)*2)\n\tmask_v_a=cv2.adaptiveThreshold(v_img,255,cv2.ADAPTIVE_THRESH_MEAN_C,cv2.THRESH_BINARY_INV,int(max(height,width)/16)*2+1,1)\n\t#ret,mask_v_a = cv2.threshold(h_img,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n\n\t#cv2.imshow('msk_v_a',mask_v_a)\n\t#cv2.waitKey(0)\n\tratio=128/min(height,width)\n\tmsk=cv2.resize(mask_v_a,(int(width*ratio),int(height*ratio)),interpolation=cv2.INTER_CUBIC )\n\th,w=msk.shape\n\tmsk_expand=np.zeros((3*h,3*w),np.uint8)\n\tmsk_expand[h:2*h,w:2*w]=msk\n\t#cv2.imshow('msk_expand',msk_expand)\n\t#cv2.waitKey(0)\n\tlong_edge=max(h,w)\n\tr0=round(0.3*long_edge)\n\tr1=round(0.7*long_edge)\n\t#print(msk_expand,r0,r1)\n\tcircles=cv2.HoughCircles(msk_expand,cv2.HOUGH_GRADIENT,1,90,param1 = 50,param2 = 5,minRadius = r0,maxRadius = r1) \n\t#print(circles is None)\n\n\tif circles is  None:\n\t\tc_x=width/2\n\t\tc_y=height/2\n\t\tradius=0.55*max(height,width)\n\telse:\t\n\t\t#print(circles.shape)\n\t\tcircles = np.uint16(np.around(circles))\n\t\tc_x=(circles[0,0,0]-w)/ratio\n\t\tc_y=(circles[0,0,1]-h)/ratio\n\t\tradius=circles[0,0,2]/ratio\n\t'''\n\tc_x=int(c_x)\n\tc_y=int(c_y)\n\tradius=int(radius)\n\t#print(circles.shape)\n\t#for i in circles[0,:]:\n\tcv2.circle(image,(c_x,c_y),radius,(255,0,0),8) \n\tcv2.circle(image,(c_x,c_y),2,(0,0,255),10)\n\thsv_img=cv2.resize(image,(int(0.3*image.shape[1]),int(0.3*image.shape[0])),interpolation=cv2.INTER_CUBIC )\n\tcv2.imshow('circle',hsv_img)\n\tcv2.waitKey(0)\n\t'''\n\treturn c_x,c_y,radius\n\ndef circleCrop(c_x,c_y,radius,height,width):\n\tif math.floor(radius+c_y)>height:\n\t\ty0=max(math.ceil(c_y-radius),0);\n\t\ty1=height;\n\t\tif math.floor(radius+c_x)>width:\n\t\t\tx1=width\n\t\telse:\n\t\t\tx1=math.floor(radius+c_x)\n\t\tif math.floor(c_x-radius<0):\n\t\t\tx0=0\n\t\telse:\n\t\t\tx0=math.floor(c_x-radius)\n\telif math.ceil(c_y-radius)<0:\n\t\ty0=0\n\t\ty1=min(math.floor(c_y+radius),height)\n\t\tif math.floor(radius+c_x)>width:\n\t\t\tx1=width\n\t\telse:\n\t\t\tx1=math.floor(radius+c_x)\n\t\tif math.floor(c_x-radius<0):\n\t\t\tx0=0\n\t\telse:\n\t\t\tx0=math.floor(c_x-radius)\n\telse:\n\t\ty0=math.ceil(c_y-radius)\n\t\ty1=math.floor(c_y+radius)\n\t\tx0=math.ceil(c_x-radius)\n\t\tx1=math.floor(c_x+radius)\n\n\treturn x0,x1,y0,y1\n\t\t\t\n\ndef trimFundus(image):\n\tc_x,c_y,radius=findCircle(image)\n\theight=image.shape[0]\n\twidth=image.shape[1]\n\tx0,x1,y0,y1=circleCrop(c_x,c_y,radius,height,width)\n\t#print(x0,x1,y0,y1)\n\ttrimed=image[y0:y1,x0:x1,:]\n\treturn trimed\n\ndef load_ben_yuan(image,sigmaX=10):\n\timage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\timage = crop_image_from_gray(image)\n\timage = cv2.resize(image, (512, 512))\n\treturn image\n\n\nPARAM = 92\n\ndef Radius_Reduction(img,PARAM):\n    h,w,c=img.shape\n    Frame=np.zeros((h,w,c),dtype=np.uint8)\n    cv2.circle(Frame,(int(math.floor(w/2)),int(math.floor(h/2))),int(math.floor((h*PARAM)/float(2*100))), (255,255,255), -1)\n    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)\n    img1 =cv2.bitwise_and(img,img,mask=Frame1)\n    return img1\n\ndef info_image(im):\n    # Compute the center (cx, cy) and radius of the eye\n    cy = im.shape[0]//2\n    midline = im[cy,:]\n    midline = np.where(midline>midline.mean()/3)[0]\n    if len(midline)>im.shape[1]//2:\n        x_start, x_end = np.min(midline), np.max(midline)\n    else: # This actually rarely happens p~1/10000\n        x_start, x_end = im.shape[1]//10, 9*im.shape[1]//10\n    cx = (x_start + x_end)/2\n    r = (x_end - x_start)/2\n    return cx, cy, r\n\n\ndef resize_image(im, img_size, augmentation=False):\n    # Crops, resizes and potentially augments the image to IMG_SIZE\n    cx, cy, r = info_image(im)\n    scaling = img_size/(2*r)\n    rotation = 0\n    if augmentation:\n        scaling *= 1 + 0.3 * (np.random.rand()-0.5)\n        rotation = 360 * np.random.rand()\n    M = cv2.getRotationMatrix2D((cx,cy), rotation, scaling)\n    M[0,2] -= cx - img_size/2\n    M[1,2] -= cy - img_size/2\n    return cv2.warpAffine(im, M, (img_size, img_size)) # This is the most important line\n\n\ndef subtract_median_bg_image(im):\n    k = np.max(im.shape)//20*2+1\n    bg = cv2.medianBlur(im, k)\n    return cv2.addWeighted (im, 4, bg, -4, 128)\n\n\ndef subtract_gaussian_bg_image(im):\n    # k = np.max(im.shape)/10\n    bg = cv2.GaussianBlur(im ,(0,0) , 10)\n    return cv2.addWeighted (im, 4, bg, -4, 128)\n\ndef open_img(fn, size):\n    \"Open image in `fn`, subclass and overwrite for custom behavior.\"\n    image = cv2.imread(fn)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    image = resize_image(image, size)\n\n    # changing line here.\n    image = subtract_gaussian_bg_image(image)\n    image = Radius_Reduction(image, PARAM)\n    image = crop_image_from_gray(image)\n    image = cv2.resize(image,(512,512))\n    #image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n    return image\n    #return Image(pil2tensor(image, np.float32).div_(255))\n\ndef get_preds(arr):\n    mask = arr == 0\n    return np.clip(np.where(mask.any(1), mask.argmax(1), 5) - 1, 0, 4)\n\n\ncnt_t=0\nclass eye_dataset(Dataset):\n\t\"\"\"docstring for data\"\"\"\n\tdef __init__(self, txt_path,transform=None,transform2=None):\n\t\timgs = []\n\t\tfor img in txt_path:\n\t\t\timgs.append(img)\n\t\tself.imgs = imgs\n\t\tself.transform = transform\n\t\tself.transform2 = transform2        \n\tdef __getitem__(self, index):\n\t\tfn= self.imgs[index]\n\t\t#img = Image.open(fn).convert('RGB') \n\t\t#img= Image.open(fn).convert('L')\n\t\t\n\t\timg=cv2.imread('/kaggle/input/aptos2019-blindness-detection/test_images/'+fn)\n\t\t#img=load_ben_color(img)\n\t\timg_copy=img.copy()\n\t\timg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\t\timg_copy=img.copy()\n\t\ttry:\n\t\t\timg=trimFundus(img)\n\t\t\timg = cv2.resize(img, (512, 512))\n\t\texcept Exception as e:\n\t\t\t#raise e\n\t\t\tprint(e)\n\t\t\timg = crop_image_from_gray(img_copy)\n\t\t\timg = cv2.resize(img, (512, 512))\n\t\t#img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n\t\t#img=load_ben_yuan(img)\n\t\timg = Image.fromarray(img)\n\t\t#cv2.imwrite('./save_test/test_'+str(index)+'.jpg',img)\n\t\t#cnt_t+=1\n\t\tif self.transform is not None:\n\t\t\timg1 = self.transform(img)\n\t\t\timg2 = self.transform2(img)\n\t\t#img = img.unsqueeze(0) \n\t\treturn img1,img2, fn[:-4]\n\tdef __len__(self):\n\t\treturn len(self.imgs)\nclass eye_dataset_circle(Dataset):\n\t\"\"\"docstring for data\"\"\"\n\tdef __init__(self, txt_path,transform=None):\n\t\timgs = []\n\t\tfor img in txt_path:\n\t\t\timgs.append(img)\n\t\tself.imgs = imgs\n\t\tself.transform = transform\n\tdef __getitem__(self, index):\n\t\tfn= self.imgs[index]\n\t\t#img = Image.open(fn).convert('RGB') \n\t\t#img= Image.open(fn).convert('L')\n\t\t\n\t\t#img=cv2.imread('/kaggle/input/aptos2019-blindness-detection/test_images/'+fn)\n\t\timg=open_img('/kaggle/input/aptos2019-blindness-detection/test_images/'+fn,530)\n\t\timg = Image.fromarray(img)\n\t\t#cv2.imwrite('./save_test/test_'+str(index)+'.jpg',img)\n\t\t#cnt_t+=1\n\t\tif self.transform is not None:\n\t\t\timg = self.transform(img)\n\t\t#img = img.unsqueeze(0) \n\t\t\n\t\treturn img, fn[:-4]\n\tdef __len__(self):\n\t\treturn len(self.imgs)\n\nclass eye_dataset_orl(Dataset):\n\t\"\"\"docstring for data\"\"\"\n\tdef __init__(self, txt_path,transform=None,transform2=None,transform3=None,transform4=None):\n\t\timgs = []\n\t\tfor img in txt_path:\n\t\t\timgs.append(img)\n\t\tself.imgs = imgs\n\t\tself.transform = transform\n\t\tself.transform2 = transform2\n\t\tself.transform3 = transform3\n\t\tself.transform4 = transform4\n\tdef __getitem__(self, index):\n\t\tfn= self.imgs[index]\n\t\t#img = Image.open(fn).convert('RGB') \n\t\t#img= Image.open(fn).convert('L')\n\t\t\n\t\timg=cv2.imread('/kaggle/input/aptos2019-blindness-detection/test_images/'+fn)\n\t\timg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\t\timg=crop_image_from_gray(img)\n\t\timg3_=open_img('/kaggle/input/aptos2019-blindness-detection/test_images/'+fn,530)\n\t\timg4_=open_img('/kaggle/input/aptos2019-blindness-detection/test_images/'+fn,530)\n\t\timg3_ = Image.fromarray(img3_)\n\t\timg4_ = Image.fromarray(img4_)\n\t\timg=cv2.resize(img, (512, 512))     \n\t\timg = Image.fromarray(img)\n\t\t#cv2.imwrite('./save_test/test_'+str(index)+'.jpg',img)\n\t\t#cnt_t+=1\n\t\tif self.transform is not None:\n\t\t\timg1 = self.transform(img)\n\t\t\timg2 = self.transform2(img)\n\t\t\timg3 = self.transform3(img3_)\n\t\t\timg4 = self.transform4(img4_)\n\t\t\t#img5 = self.transform4(img4_)\n\t\t#img = img.unsqueeze(0) \n\t\t\n\t\treturn img1,img2,img3,img4, fn[:-4]\n\tdef __len__(self):\n\t\treturn len(self.imgs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_para_dict(model1):\n    state_dict_1=torch.load(model1)\n    new_state_dict = OrderedDict()\n    for k, v in state_dict_1.items():\n        if 'module' in k:\n            name = k[7:] # add `module.`\n        else:\n            name=k\n        new_state_dict[name] = v\n    return new_state_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if __name__ == '__main__':\n\n\tuse_gpu = torch.cuda.is_available()\n\tif use_gpu:\n\t\tcudnn.benchmark = True\n\t\ttorch.cuda.manual_seed_all(0)\n\telse:\n\t\tprint(\"Currently using CPU (GPU is highly recommended)\")\n\n\n\ttransform2 = transforms.Compose([\n\t\t\ttransforms.Resize((512, 512)),\n\t\t\t#transforms.ColorJitter(brightness=20,contrast=0.2,saturation=20,hue=0.1),\n\t\t\ttransforms.ToTensor(), # 转为Tensor\n\t\t\ttransforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n\t\t\t#transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), # 归一化\n\t\t\t\t\t\t\t ])\t\n\ttransform_eff = transforms.Compose([\n\t\t\ttransforms.Resize((512, 512)),\n\t\t\t#transforms.ColorJitter(brightness=20,contrast=0.2,saturation=20,hue=0.1),\n\t\t\ttransforms.ToTensor(), # 转为Tensor\n\t\t\t#transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n\t\t\t#transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), # 归一化\n\t\t\t\t\t\t\t ])\t\n\ttransform_eff_456 = transforms.Compose([\n\t\t\ttransforms.Resize((456, 456)),\n\t\t\t#transforms.ColorJitter(brightness=20,contrast=0.2,saturation=20,hue=0.1),\n\t\t\ttransforms.ToTensor(), # 转为Tensor\n\t\t\t#transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n\t\t\t#transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), # 归一化\n\t\t\t\t\t\t\t ])\t\n\ttransform_eff_380 = transforms.Compose([\n\t\t\ttransforms.Resize((380, 380)),\n\t\t\t#transforms.ColorJitter(brightness=20,contrast=0.2,saturation=20,hue=0.1),\n\t\t\ttransforms.ToTensor(), # 转为Tensor\n\t\t\t#transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n\t\t\t#transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), # 归一化\n\t\t\t\t\t\t\t ])\t\n\ttransform_eff_528 = transforms.Compose([\n\t\t\ttransforms.Resize((528, 528)),\n\t\t\t#transforms.ColorJitter(brightness=20,contrast=0.2,saturation=20,hue=0.1),\n\t\t\ttransforms.ToTensor(), # 转为Tensor\n\t\t\t#transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n\t\t\t#transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), # 归一化\n\t\t\t\t\t\t\t ])\t\n\ttransform_eff_512 = transforms.Compose([\n\t\t\ttransforms.Resize((512, 512)),\n\t\t\t#transforms.ColorJitter(brightness=20,contrast=0.2,saturation=20,hue=0.1),\n\t\t\ttransforms.ToTensor(), # 转为Tensor\n\t\t\t#transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n\t\t\t#transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), # 归一化\n\t\t\t\t\t\t\t ])\t\n\tname_file='../input/aptos2019-blindness-detection/test.csv'\n\tcsv_file=csv.reader(open(name_file,'r'))\n\tcontent=[]\n\tfor line in csv_file:\n\t\t#print(line) \n\t\tcontent.append(line[0]+'.png')\n\tcontent=content[1:]\n\n\ttest_data=eye_dataset_orl(content,transform2,transform_eff_512,transform_eff_512,transform_eff_512)\n\t#net=inception_resnet_v1()\n\t#net.load_state_dict(torch.load('model_dict.pkl'))\n\t#net=inception_v3()\n\tnet1=Baseline_single_gem(num_classes=5)\n\tnet1_2=Baseline_single(num_classes=5)\n\t#net = EfficientNet.from_pretrained(args.model)\n\tnet2 = EfficientNet.from_name('efficientnet-b6')\n\tfeature = net2._fc.in_features\n\tnet2._fc = nn.Linear(in_features=feature,out_features=5,bias=True)\n\tnet2._avg_pooling=GeM()\n\tnet3 = EfficientNet.from_name('efficientnet-b6')\n\tfeature = net3._fc.in_features\n\tnet3._fc = nn.Linear(in_features=feature,out_features=5,bias=True)\n\t#net3._avg_pooling=GeM()\n    \n\tnet3_2 = EfficientNet.from_name('efficientnet-b6')\n\tfeature = net3_2._fc.in_features\n\tnet3_2._fc = nn.Linear(in_features=feature,out_features=5,bias=True)\n\tnet3_2._avg_pooling=GeM()\n\n\tnet4 = EfficientNet.from_name('efficientnet-b5')\n\tfeature = net4._fc.in_features\n\tnet4._fc = nn.Linear(in_features=feature,out_features=5,bias=True)\n\tnet2_2 = EfficientNet.from_name('efficientnet-b6')\n\tfeature = net2_2._fc.in_features\n\tnet2_2._fc = nn.Linear(in_features=feature,out_features=5,bias=True)\n\tif use_gpu:\n\t\tnet1=net1.cuda()\n\t\tnet1_2=net1_2.cuda()\n\t\tnet2=net2.cuda()\n\t\tnet3=net3.cuda()\n\t\tnet3_2=net3_2.cuda()\n\t\tnet4=net4.cuda()\n\t\tnet2_2=net2_2.cuda()\n\tnet1.load_state_dict(load_para_dict('/kaggle/input/temp-file/model_yuan512_dense_00001_adam_combine_trim_bce_newest_gem_8001.pkl'))\n\tnet1_2.load_state_dict(load_para_dict('/kaggle/input/temp-file/model_yuan_dense201_00001_adam_combine_trim_bce_newest_5.pkl'))\n\tnet2.load_state_dict(load_para_dict('/kaggle/input/temp-file/model_yuan512_efficientnet-b6_00001_adam_combine_trim_bce_maxest_gem_8068.pkl'))\n\tnet2_2.load_state_dict(load_para_dict('/kaggle/input/temp-file/model_yuan456_efficientnet-b6_00001_adam_combine_trim_bce_maxest_07785.pkl'))\n\tnet3.load_state_dict(load_para_dict('/kaggle/input/temp-file/model_yuan512_efficientnet-b6_00001_adam_combine_circle_bce_maxest_slow_8139.pkl'))\n\t#net3_2.load_state_dict(load_para_dict('/kaggle/input/temp-file/model_yuan512_efficientnet-b6_00001_adam_combine_circle_bce_gem_maxest_slow_793.pkl'))\n\tnet4.load_state_dict(load_para_dict('/kaggle/input/temp-file/model_yuan456_efficientnet-b5_00001_adam_combine_circle_bce_newest_slow_7857.pkl'))\n\n\tcriterion = nn.CrossEntropyLoss()\n\t#criterion = nn.MSELoss()\n\t#optimizer=optim.RMSprop(net.parameters(),lr=args.lr,alpha=0.9)\n\t#optimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=args.momentum)\n\t#torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.5, last_epoch=1)\n\t#scheduler=torch.optim.lr_scheduler.MultiStepLR(optimizer,[3], gamma=0.1, last_epoch=-1)\n\tdataloader_test=DataLoader(\n\t\ttest_data,batch_size=1, shuffle = False, num_workers= 4)\n\tidx=0\n\tmax_correct=0\n\twith open('/kaggle/working/submission.csv',\"a+\", newline='')as f:\n\t\tf_csv = csv.writer(f)\n\t\tf_csv.writerow(['id_code','diagnosis'])\n\twith torch.no_grad():\n\t\tnet1.eval()\n\t\tnet1_2.eval()\n\t\tnet2.eval()\n\t\tnet2_2.eval()\n\t\tnet3.eval()\n\t\tnet3_2.eval()\n\t\tnet4.eval()\n\t\ttotal=0\n\t\ttotal_loss=0\n\t\tcorrect=0\n\t\tfor id,item in enumerate(dataloader_test):\n\t\t\tprint('id ',id)            \n\t\t\tdata1,data2,data3,data4,name=item\n\t\t\tif use_gpu:\n\t\t\t\tdata1=data1.cuda()\n\t\t\t\tdata2=data2.cuda()\n\t\t\t\tdata3=data3.cuda()\n\t\t\t\tdata4=data4.cuda() \n\t\t\t\t#data5=data5.cuda()  \n\t\t\t#print(data.size())\n\t\t\t#out,aux=net(data)\n\t\t\t#out,feat_map,feat=net(data)\n\t\t\tout1=net1(data1)\n\t\t\tout1_2=net1_2(data1)\n\t\t\tout2_2=net2_2(data2)\n\t\t\tout2=net2(data2)\n\t\t\tout3=net3(data3)\n\t\t\t#out3_2=net3_2(data3)\n\t\t\tout4=net4(data4)\n\t\t\t#out5=net5(data5)\n\t\t\tout=(0.1375*out1+0.1375*out1_2+0.1375*out2_2+0.1375*out2+0.2*out3+0.25*out4)\n\t\t\tprint(out)          \n\t\t\tpredicted=get_preds((torch.sigmoid(out) > 0.5).cpu().numpy())\n\t\t\t#_, predicted = torch.max(out, 1)\n\t\t\thh=[str(name[0]),str(predicted[0])]\n\t\t\tprint(hh)\n\t\t\twith open('/kaggle/working/submission.csv',\"a+\", newline='')as f:\n\t\t\t\tf_csv = csv.writer(f)\n\t\t\t\tf_csv.writerow(hh)\n\tprint(pd.read_csv('/kaggle/working/submission.csv').diagnosis.value_counts())\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}