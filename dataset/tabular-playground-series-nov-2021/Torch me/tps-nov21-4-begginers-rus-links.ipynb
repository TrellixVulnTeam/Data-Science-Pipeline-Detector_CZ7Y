{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Tabular Playground Series - Nov 2021\n## Part 1 - baseline \n__Content:__  \n1. EDA \n2. Data cleaning\n3. Data preparation \n4. Getting baseline model\n \n_This dataset is a binary classification task with nonlinear data, basically the problem will be modeling and stacking models, as well as testing neural networks (which do well in classification tasks)_\n\n__Links:__  \nTelegram: https://t.me/notedatascience\n\n__Some links for beginners__ [Rus lang]  \nЭтот ноутбук для тех, кто уже что то умеет в машинном обучение. Но если вы новичек, в этом всем легко разобраться.\nЯ оставлю пару полезных ссылок как для начинающих так же и для тех кто уже преисполнился в своем сознании.\n1. https://www.youtube.com/c/GlebMikhaylov/videos - Супер крутой чувак, заходите в плейлисты там куча всего\n2. https://www.youtube.com/channel/UCCbgOIWdmYncvYMbl3LjvBQ - Прикольно объясняет\n3. https://www.youtube.com/c/CompscicenterRu/videos - Гайды фундаментально, но иногда скучно\n4. https://www.youtube.com/channel/UCoTBTCesuoWxHFHQlhmdaiQ - Прикольный чел, но на троечку  \n__Курсы:__   \n5. https://www.youtube.com/watch?v=HLg4EpeqZP0&list=PLEwK9wdS5g0pAJyqWc9bKj1JjARM_jnzx - ФКН ВШЭ, крутой и понятный курс от Жени Соколова, отдельный респект  \n6. https://www.coursera.org/specializations/machine-learning-from-statistics-to-neural-networks? - Приятный курс, но немного багнутый, вряд ли вы получите сертификат, но многому научитесь. По крайней мере ТВИМС я выучил благодаря этому курсу","metadata":{}},{"cell_type":"code","source":"#Packages\nimport numpy as np \nimport pandas as pd \nfrom matplotlib import pyplot as plt \nimport seaborn as sns \nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import QuantileTransformer","metadata":{"execution":{"iopub.status.busy":"2021-11-09T09:29:06.564281Z","iopub.execute_input":"2021-11-09T09:29:06.565338Z","iopub.status.idle":"2021-11-09T09:29:07.529306Z","shell.execute_reply.started":"2021-11-09T09:29:06.565232Z","shell.execute_reply":"2021-11-09T09:29:07.528456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/tabular-playground-series-nov-2021/train.csv')\ntest = pd.read_csv('../input/tabular-playground-series-nov-2021/test.csv')\nssub = pd.read_csv('../input/tabular-playground-series-nov-2021/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-11-09T09:29:07.534696Z","iopub.execute_input":"2021-11-09T09:29:07.535361Z","iopub.status.idle":"2021-11-09T09:29:29.637858Z","shell.execute_reply.started":"2021-11-09T09:29:07.53531Z","shell.execute_reply":"2021-11-09T09:29:29.636825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-11-09T09:29:29.639199Z","iopub.execute_input":"2021-11-09T09:29:29.639438Z","iopub.status.idle":"2021-11-09T09:29:29.666148Z","shell.execute_reply.started":"2021-11-09T09:29:29.639411Z","shell.execute_reply":"2021-11-09T09:29:29.665534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'train shape: {train.shape}, test shape: {test.shape}') #Смотрим на размеры, на всякий случай, вдруг что то пойдет не так","metadata":{"execution":{"iopub.status.busy":"2021-11-09T09:29:29.667934Z","iopub.execute_input":"2021-11-09T09:29:29.668386Z","iopub.status.idle":"2021-11-09T09:29:29.672599Z","shell.execute_reply.started":"2021-11-09T09:29:29.668354Z","shell.execute_reply":"2021-11-09T09:29:29.671958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### EDA\nHere we look at our data and try to explore it.  \nWhat can we understand from our data:\n1. There is a lot of data, you will need to deal with it somehow, there will be problems with RAM\n2. The data is distributed non-linearly, we see some lognormal and bimodal distributions.\n3. The test and train data are practically not distorted\n\n__Miniconclusion:__  \nBinomial distributions are dangerous, they can become a problem for further learning. We may get stuck in a local minimum. We can get rid of it, but lose a little in quality. It is also not clear about the data with a large excess, I want to scale them somehow.\n\n__Ссылки:__   [Rus]  \nhttps://www.kaggle.com/ankitkalauni/tps-nov-21-logistic-regression-autoviz - автоматизированный EDA   \nhttps://www.kaggle.com/dwin183287 - Красивые графики  \nhttps://www.kaggle.com/subinium/all-you-need-is-pandas-benchmark-viz - Немного анализа","metadata":{}},{"cell_type":"code","source":"plt.rcParams['figure.dpi'] = 600\nfeatures = [col for col in train.columns if col not in ['id', 'target']]#['f0','f1','f2','f3', 'f4','f5','f6','f7', 'f8', 'f9']\nfeatures = features[:50]\nfig = plt.figure(figsize=(18, 20), facecolor='#f6f5f5')\ngs = fig.add_gridspec(10, 5)\ngs.update(wspace=0.3, hspace=0.3)\nbackground_color = \"#f6f5f5\"\nrun_no = 0\nfor row in range(0, 10):\n    for col in range(0, 5):\n        locals()[\"ax\"+str(run_no)] = fig.add_subplot(gs[row, col])\n        locals()[\"ax\"+str(run_no)].set_facecolor(background_color)\n        for s in [\"top\",\"right\"]:\n            locals()[\"ax\"+str(run_no)].spines[s].set_visible(False)\n        run_no += 1\n        \nax0.text(-4.5, 520500, 'First 50 train-test distributions', fontsize=16, fontweight='bold')\nax0.text(-4.5, 460000, 'feature_0 - feature_49', fontsize=10, fontweight='light')        \n        \nrun_no = 0\nfor col in features:\n    sns.set_palette(['#00A4CCFF'])\n    sns.histplot(x=train[col], ax=locals()[\"ax\"+str(run_no)], color='#ff5573',zorder=2, linewidth=0.6, alpha=0.9, label='Train data', bins=50)\n    sns.histplot(x=test[col], ax=locals()[\"ax\"+str(run_no)], label='Test data',zorder=2, linewidth=0.6, alpha=0.9, bins=50)\n    locals()[\"ax\"+str(run_no)].set_facecolor(background_color)\n    locals()[\"ax\"+str(run_no)].set_ylabel('')\n    locals()[\"ax\"+str(run_no)].set_xlabel(col, fontsize=8, fontweight='bold')\n    locals()[\"ax\"+str(run_no)].tick_params(labelsize=5, width=0.5)\n    locals()[\"ax\"+str(run_no)].xaxis.offsetText.set_fontsize(4)\n    locals()[\"ax\"+str(run_no)].yaxis.offsetText.set_fontsize(4)\n    run_no += 1","metadata":{"execution":{"iopub.status.busy":"2021-11-09T09:29:29.673761Z","iopub.execute_input":"2021-11-09T09:29:29.674194Z","iopub.status.idle":"2021-11-09T09:30:03.805339Z","shell.execute_reply.started":"2021-11-09T09:29:29.674162Z","shell.execute_reply":"2021-11-09T09:30:03.802428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.rcParams['figure.dpi'] = 600\nfeatures = [col for col in train.columns if col not in ['id', 'target']]#['f0','f1','f2','f3', 'f4','f5','f6','f7', 'f8', 'f9']\nfeatures = features[50:]\nfig = plt.figure(figsize=(18, 20), facecolor='#f6f5f5')\ngs = fig.add_gridspec(10, 5)\ngs.update(wspace=0.3, hspace=0.3)\n\nbackground_color = \"#f6f5f5\"\nrun_no = 0\nfor row in range(0, 10):\n    for col in range(0, 5):\n        locals()[\"ax\"+str(run_no)] = fig.add_subplot(gs[row, col])\n        locals()[\"ax\"+str(run_no)].set_facecolor(background_color)\n        for s in [\"top\",\"right\"]:\n            locals()[\"ax\"+str(run_no)].spines[s].set_visible(False)\n        run_no += 1\n        \nax0.text(-2, 30500, 'Second 50 train-test distributions', fontsize=16, fontweight='bold')\nax0.text(-2, 27000, 'feature_50 - feature_99', fontsize=10, fontweight='light')        \n        \nrun_no = 0\nfor col in features:\n    sns.set_palette(['#00A4CCFF'])\n    sns.histplot(x=train[col], ax=locals()[\"ax\"+str(run_no)], color='#ff5573',zorder=2, linewidth=0.6, alpha=0.9, label='Train data', bins=50)\n    sns.histplot(x=test[col], ax=locals()[\"ax\"+str(run_no)], label='Test data',zorder=2, linewidth=0.6, alpha=0.9, bins=50)\n    locals()[\"ax\"+str(run_no)].set_facecolor(background_color)\n    locals()[\"ax\"+str(run_no)].set_ylabel('')\n    locals()[\"ax\"+str(run_no)].set_xlabel(col, fontsize=8, fontweight='bold')\n    locals()[\"ax\"+str(run_no)].tick_params(labelsize=5, width=0.5)\n    locals()[\"ax\"+str(run_no)].xaxis.offsetText.set_fontsize(4)\n    locals()[\"ax\"+str(run_no)].yaxis.offsetText.set_fontsize(4)\n    run_no += 1\n    ","metadata":{"execution":{"iopub.status.busy":"2021-11-09T09:30:03.807231Z","iopub.execute_input":"2021-11-09T09:30:03.807612Z","iopub.status.idle":"2021-11-09T09:30:37.492273Z","shell.execute_reply.started":"2021-11-09T09:30:03.807573Z","shell.execute_reply":"2021-11-09T09:30:37.491137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Count of missing values in train: {train.isna().sum().sum()}')\nprint(f'Count of missing values in test: {test.isna().sum().sum()}')","metadata":{"execution":{"iopub.status.busy":"2021-11-09T09:30:37.493788Z","iopub.execute_input":"2021-11-09T09:30:37.494059Z","iopub.status.idle":"2021-11-09T09:30:37.734155Z","shell.execute_reply.started":"2021-11-09T09:30:37.494026Z","shell.execute_reply":"2021-11-09T09:30:37.732701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(12, 6), facecolor='#f6f5f5')\ngs = fig.add_gridspec(1, 2)\n\nax0 = fig.add_subplot(gs[0, 1])\nax0.set_facecolor(background_color)\nfor s in [\"top\",\"right\"]:\n            ax0.spines[s].set_visible(False)\nsns.barplot(ax=ax0, y=train['target'].value_counts(), x=train['target'].unique())\nax0.text(-0.5, 350000, 'Target values balance', fontsize=16, fontweight='bold')\n#ax0.text(-0.5, 335000, 'feature_50 - feature_99', fontsize=10, fontweight='light')        \n","metadata":{"execution":{"iopub.status.busy":"2021-11-09T09:30:37.735522Z","iopub.execute_input":"2021-11-09T09:30:37.736053Z","iopub.status.idle":"2021-11-09T09:30:38.595922Z","shell.execute_reply.started":"2021-11-09T09:30:37.736015Z","shell.execute_reply":"2021-11-09T09:30:38.594342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(18, 8), facecolor='#f6f5f5')\ngs = fig.add_gridspec(1, 2)\n\nax0 = fig.add_subplot(gs[0, 1])\nax0.set_facecolor(background_color)\nsns.heatmap(train.corr(), ax=ax0)\nax0.text(0, -4, 'Correlation matrix', fontsize=16, fontweight='bold')\n","metadata":{"execution":{"iopub.status.busy":"2021-11-09T09:30:38.59757Z","iopub.execute_input":"2021-11-09T09:30:38.597899Z","iopub.status.idle":"2021-11-09T09:30:58.487711Z","shell.execute_reply.started":"2021-11-09T09:30:38.597863Z","shell.execute_reply":"2021-11-09T09:30:58.486301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"correl = train.corr()['target']","metadata":{"execution":{"iopub.status.busy":"2021-11-09T09:30:58.489186Z","iopub.execute_input":"2021-11-09T09:30:58.48945Z","iopub.status.idle":"2021-11-09T09:31:16.197646Z","shell.execute_reply.started":"2021-11-09T09:30:58.489416Z","shell.execute_reply":"2021-11-09T09:31:16.196536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"abs(correl).sort_values(ascending=False).head(10)","metadata":{"execution":{"iopub.status.busy":"2021-11-09T09:31:16.198936Z","iopub.execute_input":"2021-11-09T09:31:16.19917Z","iopub.status.idle":"2021-11-09T09:31:16.208214Z","shell.execute_reply.started":"2021-11-09T09:31:16.199143Z","shell.execute_reply":"2021-11-09T09:31:16.206566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data preparation","metadata":{}},{"cell_type":"markdown","source":"And so we want to get rid of the bimodal distribution. To do this, I chose QuantileTransformer, but if it doesn't work, I will filter by StandartScaler as standard.\nQuantileTransformer - https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.QuantileTransformer.html  \nStandartScaler - https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n\nIn order for QuantileTransformer to work most efficiently, we will remove unnecessary columns, namely those that have the greatest variation (I check this in the first two lines)","metadata":{}},{"cell_type":"code","source":"train.var()\nhigh_var_col_list = ['f2', 'f35', 'f44', 'id', 'is_test']\n\ndef get_preparation(train, test, is_quntile=True):\n    \"\"\"\nDocstring:\nFunction to get data preparation. \n\nParameters\n----------\ntrain: :class:`pandas.DataFrame`\n        Input data structure, a train data that can be \n        transform or scalable\ntest: test df\n        Input data structure, a test data that can be \n        transform or scalable\nis_quntile: bool, default=True\n        Set to False to switch quantileTransformation \n        to StandardScale\n\nReturns\n-------\ntrain: :class:`pandas.DataFrame`\n    Inputed prepared train dataframe\ntest: :class:`pandas.DataFrame`\n    Inputed prepared test dataframe\n    \"\"\"\n    target = train['target']\n    train = train.drop(columns='target')\n    train.loc[:, 'is_test'] = 0\n    test.loc[:, 'is_test'] = 1\n    df = pd.concat([train, test])\n    if is_quntile == True:\n        colNames=[col for col in test.columns if col not in high_var_col_list]\n        for col in colNames:\n            qt = QuantileTransformer(random_state=21,n_quantiles=2000, output_distribution='normal')\n            df[col] = qt.fit_transform(df[[col]])\n            #test[col] = qt.transform(test[[col]])\n            #print(df[col])\n        test=df[df['is_test']==1]\n        train=df[df['is_test']==0]\n        train.loc[:, 'target'] = target\n    else:\n        scaler = StandardScaler()\n        colNames=[col for col in test.columns if col not in 'id']\n        train[cols] = scaler.fit_transform(train[cols])\n        test[cols] = scaler.transform(test[cols])\n    \n    return train, test","metadata":{"execution":{"iopub.status.busy":"2021-11-09T09:31:16.210055Z","iopub.execute_input":"2021-11-09T09:31:16.2104Z","iopub.status.idle":"2021-11-09T09:31:16.437976Z","shell.execute_reply.started":"2021-11-09T09:31:16.210355Z","shell.execute_reply":"2021-11-09T09:31:16.437148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train, test = get_preparation(train, test)","metadata":{"execution":{"iopub.status.busy":"2021-11-09T09:31:16.441494Z","iopub.execute_input":"2021-11-09T09:31:16.442031Z","iopub.status.idle":"2021-11-09T09:31:54.407038Z","shell.execute_reply.started":"2021-11-09T09:31:16.441983Z","shell.execute_reply":"2021-11-09T09:31:54.405955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df\n","metadata":{"execution":{"iopub.status.busy":"2021-11-09T09:31:54.408455Z","iopub.execute_input":"2021-11-09T09:31:54.408735Z","iopub.status.idle":"2021-11-09T09:31:54.423544Z","shell.execute_reply.started":"2021-11-09T09:31:54.408683Z","shell.execute_reply":"2021-11-09T09:31:54.422395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reduce_mem_usage(train)\nreduce_mem_usage(test)","metadata":{"execution":{"iopub.status.busy":"2021-11-09T09:31:54.427468Z","iopub.execute_input":"2021-11-09T09:31:54.428042Z","iopub.status.idle":"2021-11-09T09:32:09.817276Z","shell.execute_reply.started":"2021-11-09T09:31:54.427993Z","shell.execute_reply":"2021-11-09T09:32:09.816606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cogort analisys\n__Ссылки__:  \n\nhttps://www.kaggle.com/kavehshahhosseini/tps-nov-2021-pca-and-kmeans-feature-eng - PCA + kmeans  \nhttps://www.kaggle.com/lucasmorin/data-exploration-with-umap-hdbscan - UMAP + hdbscan  \nhttps://www.kaggle.com/lucamassaron/t-sne-and-umap-for-eda - t-SNE / UMAP  \n\nhttps://habr.com/ru/company/newprolab/blog/350584/ - Почитать что бы понять","metadata":{}},{"cell_type":"code","source":"#!pip install umap-learn\n#import umap\n#colNames=[col for col in test.columns if col not in 'id']\n#reducer = umap.UMAP(random_state=42, n_components=2, low_memory=True)\n#embedding = reducer.fit_transform(train)\n\n#Не хватает ОЗУ\n'''\nTo perform a dimension reduction using UMAP, you need from 16GB of RAM \n(This dataset may need more than 24)\nYou can use low_memory=True, but it also doesn't always help\n'''","metadata":{"execution":{"iopub.status.busy":"2021-11-09T09:32:09.818442Z","iopub.execute_input":"2021-11-09T09:32:09.819409Z","iopub.status.idle":"2021-11-09T09:32:09.825665Z","shell.execute_reply.started":"2021-11-09T09:32:09.819361Z","shell.execute_reply":"2021-11-09T09:32:09.824801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Models generation\n### baseline \nWe get the baseline model - this is the very first and standard model from which we will move on.  \nStandard - Logistic regression  \nWe will take StratifiedKFold - also standard, there is nothing to invent bicycles here.  \nAUC is the metric of this competition, so we will use it.  \n__Ссылки:__  \nhttps://habr.com/ru/company/io/blog/265007/  \nhttps://habr.com/ru/post/550978/ - Для понимания кросс-валидации  \nhttps://habr.com/ru/company/ods/blog/328372/ - Метрики  \nhttps://webiomed.ai/blog/osnovnye-metriki-zadach-klassifikatsii-v-mashinnom-obuchenii/ - Метрики  \n\nhttps://www.kaggle.com/motloch/nov21-mislabeled-25 - Про метрику на этом хакатоне и 25% лосс","metadata":{}},{"cell_type":"code","source":"colNames=[col for col in test.columns if col not in ['id', 'target']]\nX = train[colNames]\ny = train['target']\nfrom sklearn.model_selection import StratifiedKFold\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-11-09T09:32:09.827018Z","iopub.execute_input":"2021-11-09T09:32:09.827254Z","iopub.status.idle":"2021-11-09T09:32:10.089512Z","shell.execute_reply.started":"2021-11-09T09:32:09.827224Z","shell.execute_reply":"2021-11-09T09:32:10.088537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skf.get_n_splits(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-11-09T09:32:10.091024Z","iopub.execute_input":"2021-11-09T09:32:10.091294Z","iopub.status.idle":"2021-11-09T09:32:10.099046Z","shell.execute_reply.started":"2021-11-09T09:32:10.091261Z","shell.execute_reply":"2021-11-09T09:32:10.097699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\nfrom sklearn.linear_model import LogisticRegression\n#Дополнительная функция, взял у кого то в датасете потому что встроенная из sklearn криво работает\ndef get_auc(y_true, y_hat):\n    fpr, tpr, _ = roc_curve(y_true, y_hat)\n    score = auc(fpr, tpr)\n    return score\n\n#Кросс-валидациия, тоже супер стандартно. Однако если вы не делали модельинг, она будет ругаться, что вы застряли в лок. минимуме\npred = []\nfor fold, (idx1, idx2) in enumerate(skf.split(X, y)):\n    Xtrain, ytrain = X.iloc[idx1], y.iloc[idx1]\n    Xvalid, yvalid = X.iloc[idx2], y.iloc[idx2]\n    \n    lr = LogisticRegression(max_iter=20000,penalty='l2')\n    \n    lr.fit(Xtrain, ytrain)\n    \n    valid_pred = lr.predict(Xvalid)\n    test_pred = lr.predict(test[colNames])\n    \n    pred.append(test_pred)\n    \n    #acc_score = accuracy_score(yvalid, valid_pred)\n    \n    print(f'Fold: {fold} | roc auc score: {get_auc(yvalid, valid_pred)} | Model: Logisctic regression')","metadata":{"execution":{"iopub.status.busy":"2021-11-09T09:32:10.100841Z","iopub.execute_input":"2021-11-09T09:32:10.101128Z","iopub.status.idle":"2021-11-09T09:33:08.646306Z","shell.execute_reply.started":"2021-11-09T09:32:10.101094Z","shell.execute_reply":"2021-11-09T09:33:08.642954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_cv_preds = pd.DataFrame({f'fold {i}': pred for i, pred in zip(range(5), pred)}) # Делаем предикт для каждого фолда и записываем в датасет\nlr_cv_preds.shape #Проверочка на всякий случай\n","metadata":{"execution":{"iopub.status.busy":"2021-11-09T09:33:08.648811Z","iopub.execute_input":"2021-11-09T09:33:08.6516Z","iopub.status.idle":"2021-11-09T09:33:08.665099Z","shell.execute_reply.started":"2021-11-09T09:33:08.651536Z","shell.execute_reply":"2021-11-09T09:33:08.664146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ssub['target'] = lr_cv_preds['fold 4'] #Выбираем лучший по метрике из фолдов, а так же смотрим что наши фолды примерно одинаковы, значит данные хорошо почистили\nssub.set_index('id').to_csv('submit_lr_cv1.csv')","metadata":{"execution":{"iopub.status.busy":"2021-11-09T09:33:08.667535Z","iopub.execute_input":"2021-11-09T09:33:08.668666Z","iopub.status.idle":"2021-11-09T09:33:09.453Z","shell.execute_reply.started":"2021-11-09T09:33:08.668587Z","shell.execute_reply":"2021-11-09T09:33:09.450977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ssub['target']","metadata":{"execution":{"iopub.status.busy":"2021-11-09T09:33:09.45455Z","iopub.execute_input":"2021-11-09T09:33:09.454848Z","iopub.status.idle":"2021-11-09T09:33:09.466236Z","shell.execute_reply.started":"2021-11-09T09:33:09.454815Z","shell.execute_reply":"2021-11-09T09:33:09.464735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## To be continued...\nIn the second part, I want to analyze more complex models, feature selection and models generation. Also deal with Hyperparameters searching and model blending.","metadata":{}}]}