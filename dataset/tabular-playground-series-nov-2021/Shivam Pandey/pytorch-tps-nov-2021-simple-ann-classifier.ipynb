{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Simple Neural Network implementation on Pytorch","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2021-11-11T20:34:28.361235Z","iopub.execute_input":"2021-11-11T20:34:28.362064Z","iopub.status.idle":"2021-11-11T20:34:28.367221Z","shell.execute_reply.started":"2021-11-11T20:34:28.362007Z","shell.execute_reply":"2021-11-11T20:34:28.366287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Read the data\n- Using Pandas `read_csv` functionality to grab the data","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/tabular-playground-series-nov-2021/train.csv\", header=0, index_col=0)\ndf.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-11-11T20:34:28.36953Z","iopub.execute_input":"2021-11-11T20:34:28.369879Z","iopub.status.idle":"2021-11-11T20:34:46.401602Z","shell.execute_reply.started":"2021-11-11T20:34:28.369837Z","shell.execute_reply":"2021-11-11T20:34:46.400574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Separate out features and corresponding target values\n- In pandas one can simply provide a `list of columns` as an index to `pd.DataFrame` to get all the data associated to given columns as an index\n- `features_cols = [\"f0\", \"f1\", ...]`\n- `target_cols = \"target\"`","metadata":{}},{"cell_type":"code","source":"# Get the feature and target columns\nfeature_cols = df.columns[:-1]\ntarget_cols = df.columns[-1]\n\n# Get the data as a numpy matrix\nfeatures = df[feature_cols].to_numpy(dtype = np.float32)\ntarget = df[target_cols].to_numpy(dtype = np.float32)\nprint(f\"Data shape: features -> {features.shape}, and Target -> {target.shape}\") ","metadata":{"execution":{"iopub.status.busy":"2021-11-11T20:41:31.903705Z","iopub.execute_input":"2021-11-11T20:41:31.904167Z","iopub.status.idle":"2021-11-11T20:41:32.325108Z","shell.execute_reply.started":"2021-11-11T20:41:31.904134Z","shell.execute_reply":"2021-11-11T20:41:32.324155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Standardize the data\n- Bring all of the features to `0 mean`, and `standard deviation 1`\n- It's requred, other wise the model will be `ralatively over attentive` towards features with `larger scales`.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler","metadata":{"execution":{"iopub.status.busy":"2021-11-11T20:41:32.42775Z","iopub.execute_input":"2021-11-11T20:41:32.428377Z","iopub.status.idle":"2021-11-11T20:41:32.432953Z","shell.execute_reply.started":"2021-11-11T20:41:32.428327Z","shell.execute_reply":"2021-11-11T20:41:32.432324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the StandardScaler object\nscaler = StandardScaler()\n\n# Transform the features\nfeatures = scaler.fit_transform(features)","metadata":{"execution":{"iopub.status.busy":"2021-11-11T20:41:32.607757Z","iopub.execute_input":"2021-11-11T20:41:32.608347Z","iopub.status.idle":"2021-11-11T20:41:33.782347Z","shell.execute_reply.started":"2021-11-11T20:41:32.60831Z","shell.execute_reply":"2021-11-11T20:41:33.781162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Split the data into train and validation set\n- As we know that there isn't any **validation set** given.\n- We need to verify the performance of the model on **unseen dataset**.\n- Thus we need to make validation dataset from the given training data.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2021-11-11T20:41:33.78454Z","iopub.execute_input":"2021-11-11T20:41:33.78509Z","iopub.status.idle":"2021-11-11T20:41:33.790591Z","shell.execute_reply.started":"2021-11-11T20:41:33.784974Z","shell.execute_reply":"2021-11-11T20:41:33.788683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_val, Y_train, Y_val = train_test_split(features, target, train_size = 0.90, random_state = 42)\nprint(f\"Train data: {X_train.shape}, {Y_train.shape}, \\nValidation data: {X_val.shape}, {Y_val.shape}\")","metadata":{"execution":{"iopub.status.busy":"2021-11-11T20:41:33.792816Z","iopub.execute_input":"2021-11-11T20:41:33.793312Z","iopub.status.idle":"2021-11-11T20:41:34.687042Z","shell.execute_reply.started":"2021-11-11T20:41:33.79326Z","shell.execute_reply":"2021-11-11T20:41:34.686085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Lets work with pytorch and get our model ready","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\n","metadata":{"execution":{"iopub.status.busy":"2021-11-11T20:41:37.332408Z","iopub.execute_input":"2021-11-11T20:41:37.332759Z","iopub.status.idle":"2021-11-11T20:41:37.337029Z","shell.execute_reply.started":"2021-11-11T20:41:37.332725Z","shell.execute_reply":"2021-11-11T20:41:37.336095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create a Pytorch Dataset\n- Pytorch needs a dataset as a subclass of `torch.utils.data.Dataset`.\n- The subclass implements some functions to augument the `Dataset` class for custom datasets.\n- They are generally of 2 types \n    1. Iterable-style : Implements the methods `__iter__()` [Useful when we can't read the data randomly]\n    2. Map-style : Implements the method `__getitem__()`, and `__len__()` [Heavily used for dataset where we can access data through indexing.]\n- In this case `Map-style` dataset subclass is created, to give features, and target values at the query index.","metadata":{}},{"cell_type":"code","source":"class TabularDataset(Dataset):\n    def __init__(self, x, y, transform = None, target_transform = None):\n        self.x = x\n        self.y = y\n        self.transform = transform\n        self.target_transform = target_transform\n\n    def __getitem__(self, idx):\n        feature = self.x[idx, :]\n        target = self.y[idx]\n        target = np.array([target], dtype = np.float32) \n\n        if self.transform:\n            feature = self.transform(feature)\n        if self.target_transform:\n            target = self.target_transform(target)\n        \n        return torch.from_numpy(feature), torch.from_numpy(target)\n    \n    def __len__(self):\n        return len(self.x)","metadata":{"execution":{"iopub.status.busy":"2021-11-11T20:41:37.856622Z","iopub.execute_input":"2021-11-11T20:41:37.856984Z","iopub.status.idle":"2021-11-11T20:41:37.866743Z","shell.execute_reply.started":"2021-11-11T20:41:37.856953Z","shell.execute_reply":"2021-11-11T20:41:37.865667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Train_dataset = TabularDataset(X_train, Y_train)\nVal_dataset = TabularDataset(X_val, Y_val)","metadata":{"execution":{"iopub.status.busy":"2021-11-11T20:41:38.173339Z","iopub.execute_input":"2021-11-11T20:41:38.173681Z","iopub.status.idle":"2021-11-11T20:41:38.178574Z","shell.execute_reply.started":"2021-11-11T20:41:38.17365Z","shell.execute_reply":"2021-11-11T20:41:38.177627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Make the Dataloaders\n- Dataloader modalit is used in `PyTorch` to acces the bulk data with required additional operation on overall datasets like shuffle the dataset.\n- Dataloader provides iterable object, to acces the data while training or testing process.","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader","metadata":{"execution":{"iopub.status.busy":"2021-11-11T20:41:38.808555Z","iopub.execute_input":"2021-11-11T20:41:38.808841Z","iopub.status.idle":"2021-11-11T20:41:38.813705Z","shell.execute_reply.started":"2021-11-11T20:41:38.808811Z","shell.execute_reply":"2021-11-11T20:41:38.812555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's create the dataloader for train and test datasets\ntrain_dataloader = DataLoader(Train_dataset, batch_size = 128*5, shuffle = True)\nval_dataloader = DataLoader(Val_dataset, batch_size = 128*5, shuffle = True)","metadata":{"execution":{"iopub.status.busy":"2021-11-11T20:41:39.204672Z","iopub.execute_input":"2021-11-11T20:41:39.204976Z","iopub.status.idle":"2021-11-11T20:41:39.210887Z","shell.execute_reply.started":"2021-11-11T20:41:39.204944Z","shell.execute_reply":"2021-11-11T20:41:39.209963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's see what can be done with these iterators\ndataitr = iter(train_dataloader)\nfeatures, labels = dataitr.next()\nprint(f\"Features: {features.shape} \\nLabels: {labels.shape}\")","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-11-11T20:41:40.804944Z","iopub.execute_input":"2021-11-11T20:41:40.805494Z","iopub.status.idle":"2021-11-11T20:41:40.894795Z","shell.execute_reply.started":"2021-11-11T20:41:40.805414Z","shell.execute_reply":"2021-11-11T20:41:40.89334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create the Model\n- In `PyTorch` the model can be created either in `nn.Sequential` or as a subclass of `nn.Module` with implementation of `forward` function.\n- The former is easy to deal with, but the later provides **flexibility**.","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn","metadata":{"execution":{"iopub.status.busy":"2021-11-11T20:41:57.721211Z","iopub.execute_input":"2021-11-11T20:41:57.721576Z","iopub.status.idle":"2021-11-11T20:41:57.726589Z","shell.execute_reply.started":"2021-11-11T20:41:57.721531Z","shell.execute_reply":"2021-11-11T20:41:57.725659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create Model subclass to define the network\nclass Model(nn.Module):\n    def __init__(self, in_features = 100):\n        super().__init__()\n\n        # Define possible layers configuration\n        self.fc1 = nn.Linear(in_features, 150)\n        self.fc2 = nn.Linear(150, 90)\n        self.fc3 = nn.Linear(90, 70)\n        self.fc4 = nn.Linear(70, 50)\n        self.fc5 = nn.Linear(50, 30)\n        self.fc6 = nn.Linear(30, 20)\n        self.fc7 = nn.Linear(20, 10)\n        self.fc8 = nn.Linear(10, 5)\n        self.fc9 = nn.Linear(5, 1)\n        \n        # Define activations, classifier layer, \n        # and if required then regularizations\n        self.activation = nn.SELU() # Activations\n        self.classifier = nn.Sigmoid() # Classifier\n        self.dropout = nn.Dropout(p=0.1) # Regularization\n    \n    def forward(self, x):\n        \"\"\"\n        Function implements the `forward` pass of a network.\n        While training this will run with gradient enabled, to backprop,\n        otherwise while testing this is used with torch.no_grad() to infer on the query.\n        \"\"\"\n        x = self.activation(self.fc1(x))\n        x = self.activation(self.fc2(x))\n        x = self.activation(self.fc3(x))\n        x = self.activation(self.fc4(x))\n        x = self.activation(self.fc5(x))\n        x = self.activation(self.fc6(x))\n        x = self.activation(self.fc7(x))\n        x = self.activation(self.fc8(x))\n        x = self.classifier(self.fc9(x))\n        \n        return x","metadata":{"execution":{"iopub.status.busy":"2021-11-11T20:41:57.952623Z","iopub.execute_input":"2021-11-11T20:41:57.952919Z","iopub.status.idle":"2021-11-11T20:41:57.966543Z","shell.execute_reply.started":"2021-11-11T20:41:57.952891Z","shell.execute_reply":"2021-11-11T20:41:57.965612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We can specify the accelerator device for the model","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"{device} device will be used.\")","metadata":{"execution":{"iopub.status.busy":"2021-11-11T20:41:58.328428Z","iopub.execute_input":"2021-11-11T20:41:58.328759Z","iopub.status.idle":"2021-11-11T20:41:58.33479Z","shell.execute_reply.started":"2021-11-11T20:41:58.328727Z","shell.execute_reply":"2021-11-11T20:41:58.333889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Transfer the model parameters and properties to selected device\nmodel = Model().to(torch.device(device))","metadata":{"execution":{"iopub.status.busy":"2021-11-11T20:41:58.509793Z","iopub.execute_input":"2021-11-11T20:41:58.510475Z","iopub.status.idle":"2021-11-11T20:41:58.517925Z","shell.execute_reply.started":"2021-11-11T20:41:58.510431Z","shell.execute_reply":"2021-11-11T20:41:58.517096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Get the model description","metadata":{}},{"cell_type":"code","source":"try:\n    from torchsummary import summary\nexcept:\n    print(\"Installing Torchsummary..........\")\n    ! pip install torchsummary\n    from torchsummary import summary","metadata":{"execution":{"iopub.status.busy":"2021-11-11T20:41:58.848742Z","iopub.execute_input":"2021-11-11T20:41:58.849057Z","iopub.status.idle":"2021-11-11T20:41:58.85503Z","shell.execute_reply.started":"2021-11-11T20:41:58.849024Z","shell.execute_reply":"2021-11-11T20:41:58.854064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summary(model, (100,))","metadata":{"execution":{"iopub.status.busy":"2021-11-11T20:41:59.043597Z","iopub.execute_input":"2021-11-11T20:41:59.043913Z","iopub.status.idle":"2021-11-11T20:41:59.062879Z","shell.execute_reply.started":"2021-11-11T20:41:59.043873Z","shell.execute_reply":"2021-11-11T20:41:59.061277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_acc(acc_type = 'val'):\n    correct = 0\n    total = 0\n    # since we're not training, we don't need to calculate the gradients for our outputs\n    with torch.no_grad():\n        dl = val_dataloader if acc_type == 'val' else train_dataloader\n        for data in dl:\n            features, labels = data\n            features = features.to(device)\n            labels = labels.to(device)\n\n            # calculate outputs by running images through the network\n            outputs = model(features)\n\n            # the class with the highest energy is what we choose as prediction\n            pivot = torch.tensor([0.5]).to(device)\n            value = torch.tensor([0.0]).to(device)\n            predicted = torch.heaviside(outputs.data-pivot, value)\n            \n            # print(predicted, labels)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    \n    return 100*correct/total\n","metadata":{"execution":{"iopub.status.busy":"2021-11-11T20:41:59.193943Z","iopub.execute_input":"2021-11-11T20:41:59.194243Z","iopub.status.idle":"2021-11-11T20:41:59.203122Z","shell.execute_reply.started":"2021-11-11T20:41:59.194213Z","shell.execute_reply":"2021-11-11T20:41:59.202125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Initialize the model weights","metadata":{}},{"cell_type":"code","source":"def init_weights(layer):\n    if isinstance(layer, nn.Linear):\n        nn.init.xavier_normal_(layer.weight.data)\n\nmodel.apply(init_weights)","metadata":{"execution":{"iopub.status.busy":"2021-11-11T20:41:59.815996Z","iopub.execute_input":"2021-11-11T20:41:59.816296Z","iopub.status.idle":"2021-11-11T20:41:59.825229Z","shell.execute_reply.started":"2021-11-11T20:41:59.816266Z","shell.execute_reply":"2021-11-11T20:41:59.824341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## We can also load previously stored model\n- set `load_prev` to `True`","metadata":{}},{"cell_type":"code","source":"load_prev = False\nif load_prev:\n    model.load_state_dict(torch.load('./basemodel'))\n    print(model.eval())","metadata":{"execution":{"iopub.status.busy":"2021-11-11T20:42:00.309513Z","iopub.execute_input":"2021-11-11T20:42:00.309789Z","iopub.status.idle":"2021-11-11T20:42:00.315145Z","shell.execute_reply.started":"2021-11-11T20:42:00.30976Z","shell.execute_reply":"2021-11-11T20:42:00.314219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define the optimization algorithm, and loss function","metadata":{}},{"cell_type":"code","source":"import torch.optim as optim\n\ncriterion = nn.BCELoss() # Loss function\nparams_list = model.parameters() # model parameters\n\n## We can apply custom learning rate or any other perameters to each layer, use the following:\n# params_list = [\n#     {'params': model.fc1.parameters(), 'lr': 0.01},\n#     {'params': model.fc1_1.parameters(), 'lr': 0.01},\n#     {'params': model.fc2.parameters(), 'lr': 0.005},\n#     {'params': model.fc2_2.parameters(), 'lr': 0.005},\n#     {'params': model.fc3.parameters(), 'lr': 0.001},\n#     {'params': model.fc3_3.parameters(), 'lr': 0.001},\n#     {'params': model.fc4.parameters(), 'lr': 0.005},\n#     {'params': model.fc4_4.parameters(), 'lr': 0.005},\n#     {'params': model.fc5.parameters(), 'lr': 0.001},\n#     {'params': model.fc5_5.parameters(), 'lr': 0.001},\n# ]\noptimizer = optim.AdamW(params_list, lr=0.0007, weight_decay=0.01) # Optimizer","metadata":{"execution":{"iopub.status.busy":"2021-11-11T20:42:00.8396Z","iopub.execute_input":"2021-11-11T20:42:00.840326Z","iopub.status.idle":"2021-11-11T20:42:00.8489Z","shell.execute_reply.started":"2021-11-11T20:42:00.840284Z","shell.execute_reply":"2021-11-11T20:42:00.847924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Implement training loop","metadata":{}},{"cell_type":"code","source":"for epoch in range(700):  # loop over the dataset multiple times\n\n    running_loss = 0.0\n    for i, data in enumerate(train_dataloader, 0):\n        # get the inputs; data is a list of [inputs, labels]\n        inputs, labels = data\n        inputs = inputs.to(torch.device(device))\n        labels = labels.to(torch.device(device))\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        # print statistics\n        running_loss += loss.item()\n        if i % 400 == 399:    # print every 400 mini-batches\n            print('[%d, %5d] loss: %.3f, val accuracy: %.3f' %\n                  (epoch + 1, i + 1, running_loss / 400, get_acc('val')))\n            running_loss = 0.0\n\nprint('Finished Training')","metadata":{"scrolled":true,"tags":[],"execution":{"iopub.status.busy":"2021-11-11T20:42:01.416572Z","iopub.execute_input":"2021-11-11T20:42:01.41689Z","iopub.status.idle":"2021-11-11T20:45:22.521385Z","shell.execute_reply.started":"2021-11-11T20:42:01.416859Z","shell.execute_reply":"2021-11-11T20:45:22.520257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Check validation and train accuracy","metadata":{}},{"cell_type":"code","source":"print(' Validation accuracy of the network: %f %%' % (\n    get_acc('val')))\nprint(' Train accuracy of the network: %f %%' % (\n    get_acc('train')))","metadata":{"scrolled":true,"tags":[],"execution":{"iopub.status.busy":"2021-11-11T20:40:54.874576Z","iopub.execute_input":"2021-11-11T20:40:54.874928Z","iopub.status.idle":"2021-11-11T20:41:06.034164Z","shell.execute_reply.started":"2021-11-11T20:40:54.874892Z","shell.execute_reply":"2021-11-11T20:41:06.033441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Save the model parameters for future usage\n- This may be relaoded as further continuation of the training process","metadata":{}},{"cell_type":"code","source":"torch.save(model.state_dict(), \"./basemodel\")","metadata":{"execution":{"iopub.status.busy":"2021-11-11T19:52:12.836703Z","iopub.execute_input":"2021-11-11T19:52:12.837004Z","iopub.status.idle":"2021-11-11T19:52:12.84578Z","shell.execute_reply.started":"2021-11-11T19:52:12.83697Z","shell.execute_reply":"2021-11-11T19:52:12.844793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.state_dict()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-11-11T19:52:14.169527Z","iopub.execute_input":"2021-11-11T19:52:14.169837Z","iopub.status.idle":"2021-11-11T19:52:14.206582Z","shell.execute_reply.started":"2021-11-11T19:52:14.16979Z","shell.execute_reply":"2021-11-11T19:52:14.205802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let's infer on the test set and submit the predictions","metadata":{}},{"cell_type":"markdown","source":"### Load test data","metadata":{}},{"cell_type":"code","source":"df_test = pd.read_csv(\"../input/tabular-playground-series-nov-2021/test.csv\", header=0, index_col=0)\ndf_test.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-11-11T19:52:17.298477Z","iopub.execute_input":"2021-11-11T19:52:17.298937Z","iopub.status.idle":"2021-11-11T19:52:26.811845Z","shell.execute_reply.started":"2021-11-11T19:52:17.298896Z","shell.execute_reply":"2021-11-11T19:52:26.810934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Normalize it, and get the pytorch dataset, and dataloader","metadata":{}},{"cell_type":"code","source":"features = scaler.transform(np.float32(df_test.values))\ntest_dataset = TabularDataset(features, np.ones((len(features),)))\ntest_dataloader = DataLoader(test_dataset, batch_size = 128)","metadata":{"execution":{"iopub.status.busy":"2021-11-11T19:52:26.813585Z","iopub.execute_input":"2021-11-11T19:52:26.813972Z","iopub.status.idle":"2021-11-11T19:52:27.178526Z","shell.execute_reply.started":"2021-11-11T19:52:26.813932Z","shell.execute_reply":"2021-11-11T19:52:27.177769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Infer the results","metadata":{}},{"cell_type":"code","source":"result = []\nwith torch.no_grad():\n    for data in test_dataloader:\n        features = data[0].to(device)\n\n        # calculate outputs by running images through the network\n        outputs = model(features)\n\n        # the class with the highest energy is what we choose as prediction\n        _, predicted = torch.max(outputs.data, 1)\n        result.extend(predicted.cpu().detach().numpy())\n","metadata":{"execution":{"iopub.status.busy":"2021-11-11T19:52:27.17979Z","iopub.execute_input":"2021-11-11T19:52:27.180269Z","iopub.status.idle":"2021-11-11T19:52:34.570571Z","shell.execute_reply.started":"2021-11-11T19:52:27.180225Z","shell.execute_reply":"2021-11-11T19:52:34.569777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create results as a pandas DataFrame","metadata":{}},{"cell_type":"code","source":"df_result = pd.DataFrame(np.array([df_test.index.tolist(), result]).T, columns = ['id', 'target'])\ndf_result.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-11-11T19:52:34.572592Z","iopub.execute_input":"2021-11-11T19:52:34.572861Z","iopub.status.idle":"2021-11-11T19:52:34.809692Z","shell.execute_reply.started":"2021-11-11T19:52:34.572829Z","shell.execute_reply":"2021-11-11T19:52:34.808782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Save the dataframe as a csv file","metadata":{}},{"cell_type":"code","source":"df_result.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-11T19:52:34.811267Z","iopub.execute_input":"2021-11-11T19:52:34.811565Z","iopub.status.idle":"2021-11-11T19:52:35.754901Z","shell.execute_reply.started":"2021-11-11T19:52:34.811518Z","shell.execute_reply":"2021-11-11T19:52:35.754017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}