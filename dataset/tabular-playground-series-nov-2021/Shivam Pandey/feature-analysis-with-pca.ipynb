{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2021-11-08T06:12:17.315159Z","iopub.execute_input":"2021-11-08T06:12:17.316379Z","iopub.status.idle":"2021-11-08T06:12:17.351134Z","shell.execute_reply.started":"2021-11-08T06:12:17.316221Z","shell.execute_reply":"2021-11-08T06:12:17.350445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Read the dataset\n\n- Using `pd.read_csv` for reading the tabular data from csv files.\n- The methods returns a pandas `DataFrame` object, that can be explored in an interactive manner. For more details [follow](https://pandas.pydata.org/pandas-docs/dev/reference/api/pandas.DataFrame.html?highlight=dataframe#)\n- `read_csv` function returns an iterable object when provided with `chunksize: int` or `iterable: True`.\n- Using `next` functionality we can iterate through the object to get data wit `chunksize`","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/tabular-playground-series-nov-2021/train.csv\", header=0, index_col=0)\ndf.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-11-08T06:12:30.045071Z","iopub.execute_input":"2021-11-08T06:12:30.04539Z","iopub.status.idle":"2021-11-08T06:12:45.592602Z","shell.execute_reply.started":"2021-11-08T06:12:30.045341Z","shell.execute_reply":"2021-11-08T06:12:45.592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Huge chunk of features let's analyse which feature contains more rational information about data\n\n### Applying Principal Component Analysis(PCA)\n\n- PCA is applied to analyse that how much of information about data is contained in a certain feature\n- Basically PCA is Eigen Value Decomposition (eigen value for each feature defines the variance contained by that feature)\n- Sometimes EVD is replaced with Singular Value Decomposition (SVD). In this the Singular values acts similar to eigen values.","metadata":{}},{"cell_type":"code","source":"# Get the feature and target columns\nfeature_cols = df.columns[:-1]\ntarget_cols = df.columns[-1]\n\n# Get the data as a numpy matrix\nfeatures = df[feature_cols].to_numpy()\ntarget = df[target_cols].to_numpy()\nprint(f\"Data shape: features -> {features.shape}, and Target -> {target.shape}\") ","metadata":{"execution":{"iopub.status.busy":"2021-11-08T06:12:53.336738Z","iopub.execute_input":"2021-11-08T06:12:53.337021Z","iopub.status.idle":"2021-11-08T06:12:53.492969Z","shell.execute_reply.started":"2021-11-08T06:12:53.336992Z","shell.execute_reply":"2021-11-08T06:12:53.492067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Normalize the features\n\n- Normalization is something that is needed by PCA.\n- PCA is suceptible to variance in feature observations.\n- If some features have significantly different scales, then the features with lower scale can get supressed while decomposition.\n- We need to transform each feature to have standard deviation 1 unit, and mean to 0 unit.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler","metadata":{"execution":{"iopub.status.busy":"2021-11-08T06:12:55.429731Z","iopub.execute_input":"2021-11-08T06:12:55.43016Z","iopub.status.idle":"2021-11-08T06:12:56.319181Z","shell.execute_reply.started":"2021-11-08T06:12:55.430127Z","shell.execute_reply":"2021-11-08T06:12:56.318442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the StandardScaler object\nscaler = StandardScaler()\n\n# Transform the features\nfeatures = scaler.fit_transform(features)","metadata":{"execution":{"iopub.status.busy":"2021-11-08T06:12:56.320645Z","iopub.execute_input":"2021-11-08T06:12:56.320912Z","iopub.status.idle":"2021-11-08T06:12:57.623495Z","shell.execute_reply.started":"2021-11-08T06:12:56.32088Z","shell.execute_reply":"2021-11-08T06:12:57.622811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check if it worked or not\nfeatures_mean = np.mean(features, axis=0)\nfeatures_std = np.std(features, axis=0)\n\nprint(f\"Feature mean: {features_mean} \\n Features standard deviation: {features_std}\")","metadata":{"execution":{"iopub.status.busy":"2021-11-08T06:12:57.624564Z","iopub.execute_input":"2021-11-08T06:12:57.624961Z","iopub.status.idle":"2021-11-08T06:12:57.945035Z","shell.execute_reply.started":"2021-11-08T06:12:57.624932Z","shell.execute_reply":"2021-11-08T06:12:57.944293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Apply EVD","metadata":{}},{"cell_type":"code","source":"# Get the covariance matrix\nfeatures_cov = np.cov(features.T)\n\n# Apply eigen value decomposition\neigen_vals, eigen_vecs = np.linalg.eig(features_cov)\n\n# normalize the eigen values to compare in %\neigen_vals /= np.sum(eigen_vals)\neigen_vals *= 100","metadata":{"execution":{"iopub.status.busy":"2021-11-08T06:12:57.946485Z","iopub.execute_input":"2021-11-08T06:12:57.946843Z","iopub.status.idle":"2021-11-08T06:12:58.517663Z","shell.execute_reply.started":"2021-11-08T06:12:57.946814Z","shell.execute_reply":"2021-11-08T06:12:58.516851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualize the eigen values","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2021-11-08T06:12:58.519074Z","iopub.execute_input":"2021-11-08T06:12:58.523261Z","iopub.status.idle":"2021-11-08T06:12:58.704084Z","shell.execute_reply.started":"2021-11-08T06:12:58.523196Z","shell.execute_reply":"2021-11-08T06:12:58.703339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = [35, 7])\nep = sns.scatterplot(x = feature_cols, y = eigen_vals, hue = np.log(eigen_vals), legend = False)","metadata":{"execution":{"iopub.status.busy":"2021-11-08T06:12:58.705214Z","iopub.execute_input":"2021-11-08T06:12:58.705565Z","iopub.status.idle":"2021-11-08T06:13:00.210484Z","shell.execute_reply.started":"2021-11-08T06:12:58.705535Z","shell.execute_reply":"2021-11-08T06:13:00.209526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Sort eigen values and features for clear visualization","metadata":{}},{"cell_type":"code","source":"# Get the sort indexs\nidx = np.flip(np.argsort(eigen_vals))\n\n# plot the sorted eigen values and corresponsing features\nplt.figure(figsize = [35, 7])\nep = sns.scatterplot(x = feature_cols[idx], y = eigen_vals[idx], hue = np.log(eigen_vals[idx]), legend = False)","metadata":{"execution":{"iopub.status.busy":"2021-11-08T06:13:00.212501Z","iopub.execute_input":"2021-11-08T06:13:00.212759Z","iopub.status.idle":"2021-11-08T06:13:01.538797Z","shell.execute_reply.started":"2021-11-08T06:13:00.212728Z","shell.execute_reply":"2021-11-08T06:13:01.537954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Lets look at the cumulative behaviour of eigen values for each feature","metadata":{}},{"cell_type":"code","source":"eigen_vals_cumulative = np.cumsum(eigen_vals[idx])\nplt.figure(figsize = [35, 7])\ngraph = sns.scatterplot(x = feature_cols[idx], y = eigen_vals_cumulative, hue = np.log(eigen_vals_cumulative), legend = False)\ngraph.axhline(99)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-08T06:13:01.539987Z","iopub.execute_input":"2021-11-08T06:13:01.540256Z","iopub.status.idle":"2021-11-08T06:13:02.634246Z","shell.execute_reply.started":"2021-11-08T06:13:01.540225Z","shell.execute_reply":"2021-11-08T06:13:02.633629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> We can easily observe in the previous figure that the change in cumulative behaviour of eigen values is gradual through out the feature space(simply, the 99% data retention is only on the cost of a single feature). Thus the PCA based feature selection won't be useful and if applied will create problem of data loss, without significant advantage on feature dismissal.","metadata":{}},{"cell_type":"code","source":"sns.histplot(x = target)","metadata":{"execution":{"iopub.status.busy":"2021-11-08T06:13:02.635588Z","iopub.execute_input":"2021-11-08T06:13:02.635911Z","iopub.status.idle":"2021-11-08T06:13:03.024842Z","shell.execute_reply.started":"2021-11-08T06:13:02.635882Z","shell.execute_reply":"2021-11-08T06:13:03.023114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}