{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"The main part comes From : https://www.kaggle.com/ambrosm/tpsnov21-012-leaderboard-probing\nCongrats AmbrosM !\n\n2 main ideas to improve the main part:\n- give a better prediction for label = 1 => probability(Label==1) = 1 - probability(label == 0)\n- enlarge the differentiation between chunks\n\nDon't be afraid to upvote, no charge :-)\n\nOf course, some experimentations has to be performed to adjust some parameters according to your private file ","metadata":{}},{"cell_type":"markdown","source":"<h2> No change from AmbrosM solution :","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport pickle\nimport io\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import roc_auc_score, accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import LinearSVC","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-30T21:06:34.405584Z","iopub.execute_input":"2021-11-30T21:06:34.406204Z","iopub.status.idle":"2021-11-30T21:06:35.548066Z","shell.execute_reply.started":"2021-11-30T21:06:34.406096Z","shell.execute_reply":"2021-11-30T21:06:35.547151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read the data\ntrain_df = pd.read_csv('../input/tabular-playground-series-nov-2021/train.csv')\ntest_df = pd.read_csv('../input/tabular-playground-series-nov-2021/test.csv')\npure_df = pd.read_csv('../input/november21/train.csv')\ntest_df['chunk'] = test_df.id // 60000","metadata":{"execution":{"iopub.status.busy":"2021-11-30T21:06:35.549675Z","iopub.execute_input":"2021-11-30T21:06:35.549928Z","iopub.status.idle":"2021-11-30T21:07:20.162682Z","shell.execute_reply.started":"2021-11-30T21:06:35.549896Z","shell.execute_reply":"2021-11-30T21:07:20.161767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def postprocess_separate(submission_df, test_df=None, pure_df=None):\n    \"\"\"Update submission_df so that the predictions for the two sides of the hyperplane don't overlap.\n    \n    Parameters\n    ----------\n    submission_df : pandas DataFrame with columns 'id' and 'target'\n    test_df : the competition's test data\n    pure_df : the competition's original training data\n    \n    From https://www.kaggle.com/ambrosm/tpsnov21-007-postprocessing\n    \"\"\"\n    if pure_df is None: pure_df = pd.read_csv('../input/november21/train.csv')\n    if pure_df.shape != (600000, 102): raise ValueError(\"pure_df has the wrong shape\")\n    if test_df is None: test_df = pd.read_csv('../input/tabular-playground-series-nov-2021/test.csv')\n    if test_df.shape[0] != submission_df.shape[0] or test_df.shape[1] != 101: raise ValueError(\"test_df has the wrong shape\")\n\n    # Find the separating hyperplane for pure_df, step 1\n    # Use an SVM with almost no regularization\n    model1 = make_pipeline(StandardScaler(), LinearSVC(C=1e5, tol=1e-7, penalty='l2', dual=False, max_iter=2000, random_state=1))\n    model1.fit(pure_df.drop(columns=['id', 'target']), pure_df.target)\n    pure_pred = model1.predict(pure_df.drop(columns=['id', 'target']))\n    print((pure_pred != pure_df.target).sum(), (pure_pred == pure_df.target).sum()) # 1 599999\n    # model1 is not perfect: it predicts the wrong class for 1 of 600000 samples\n\n    # Find the separating hyperplane for pure_df, step 2\n    # Fit a second SVM to a subset of the points which contains the support vectors\n    pure_pred = model1.decision_function(pure_df.drop(columns=['id', 'target']))\n    subset_df = pure_df[(pure_pred > -5) & (pure_pred < 0.9)]\n    model2 = make_pipeline(StandardScaler(), LinearSVC(C=1e5, tol=1e-7, penalty='l2', dual=False, max_iter=2000, random_state=1))\n    model2.fit(subset_df.drop(columns=['id', 'target']), subset_df.target)\n    pure_pred = model2.predict(pure_df.drop(columns=['id', 'target']))\n    print((pure_pred != pure_df.target).sum(), (pure_pred == pure_df.target).sum()) # 0 600000\n    # model2 is perfect: it predicts the correct class for all 600000 training samples\n    \n    pure_test_pred = model2.predict(test_df.drop(columns=['id', 'target'], errors='ignore'))\n    lmax, rmin = submission_df[pure_test_pred == 0].target.max(), submission_df[pure_test_pred == 1].target.min()\n    if lmax < rmin:\n        print(\"There is no overlap. No postprocessing needed.\")\n        return\n    # There is overlap. Remove this overlap\n    submission_df.loc[pure_test_pred == 0, 'target'] -= lmax + 1\n    submission_df.loc[pure_test_pred == 1, 'target'] -= rmin - 1\n    print(submission_df[pure_test_pred == 0].target.min(), submission_df[pure_test_pred == 0].target.max(),\n          submission_df[pure_test_pred == 1].target.min(), submission_df[pure_test_pred == 1].target.max())\n","metadata":{"execution":{"iopub.status.busy":"2021-11-30T21:07:20.164158Z","iopub.execute_input":"2021-11-30T21:07:20.165213Z","iopub.status.idle":"2021-11-30T21:07:20.190372Z","shell.execute_reply.started":"2021-11-30T21:07:20.165159Z","shell.execute_reply":"2021-11-30T21:07:20.189281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a baseline submission which always predicts -1 or +1 and has an lb score of 0.74723\nbaseline = pd.DataFrame({'id': test_df.id, 'target': 0})\npostprocess_separate(baseline, test_df=test_df.drop(columns='chunk'), pure_df=pure_df)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-30T21:07:20.192964Z","iopub.execute_input":"2021-11-30T21:07:20.193715Z","iopub.status.idle":"2021-11-30T21:07:51.335082Z","shell.execute_reply.started":"2021-11-30T21:07:20.193669Z","shell.execute_reply":"2021-11-30T21:07:51.333936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2> Small updates here : Enlarge diferentiation between chunks","metadata":{}},{"cell_type":"code","source":"# previous chunk values from AmbrosM :\n\np_dict = {10: 0.26245756846719176,\n          17: 0.25772808586762075,\n          16: 0.25038670867946144,\n          13: 0.2498515790341643,\n          18: 0.24863555967320816,\n          11: 0.2476293839324911,\n          14: 0.2448713889988128,\n          12: 0.24464126228044064,\n          15: 0.2418890814558059}\n\nvalues = [val for val in p_dict.values()]\nMEAN = np.mean([val for val in values])\nnew_values =[]\n\nCOEF = 0.5 # You need to overfit the test file by several tries to get the best coef :-))\n\nfor val in values:\n    val = (val - MEAN) * COEF + val\n    new_values.append(val)\n    \ni=-1\nfor k in p_dict.keys() :\n    i+=1\n    p_dict[k] = new_values[i]\n\nprint(p_dict)","metadata":{"execution":{"iopub.status.busy":"2021-11-30T21:08:01.279121Z","iopub.execute_input":"2021-11-30T21:08:01.279403Z","iopub.status.idle":"2021-11-30T21:08:01.28888Z","shell.execute_reply.started":"2021-11-30T21:08:01.279373Z","shell.execute_reply":"2021-11-30T21:08:01.287871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2> Provide a better prediction for label 1 :","metadata":{}},{"cell_type":"code","source":"sub = baseline.copy()\n#sub.loc[baseline.target == 1, 'target'] = 0.75 => Now you replace it by (1-prob(label 0)) :\n\nfor chunk in range(10, 19):\n    sub.loc[(test_df.chunk == chunk) & (sub.target < 0), 'target'] = p_dict[chunk] \n    sub.loc[(test_df.chunk == chunk) & (baseline.target == 1), 'target'] = 1-p_dict[chunk]\n\nsub.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-11-30T21:09:07.517707Z","iopub.execute_input":"2021-11-30T21:09:07.518048Z","iopub.status.idle":"2021-11-30T21:09:07.626102Z","shell.execute_reply.started":"2021-11-30T21:09:07.518013Z","shell.execute_reply":"2021-11-30T21:09:07.625235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"other_submission = pd.read_csv('INSERT YOUR PRIVATE FILE')\nother_submission","metadata":{"execution":{"iopub.status.busy":"2021-11-25T07:56:25.202434Z","iopub.execute_input":"2021-11-25T07:56:25.203279Z","iopub.status.idle":"2021-11-25T07:56:25.478021Z","shell.execute_reply.started":"2021-11-25T07:56:25.203215Z","shell.execute_reply":"2021-11-25T07:56:25.477117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub['target'] += other_submission\nsub.to_csv(f'AmbrosM_&_Asterix_overfitting_solution.csv', index=False)\nsub.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-11-29T14:39:16.029243Z","iopub.execute_input":"2021-11-29T14:39:16.029552Z","iopub.status.idle":"2021-11-29T14:39:18.038567Z","shell.execute_reply.started":"2021-11-29T14:39:16.029518Z","shell.execute_reply":"2021-11-29T14:39:18.037734Z"},"trusted":true},"execution_count":null,"outputs":[]}]}