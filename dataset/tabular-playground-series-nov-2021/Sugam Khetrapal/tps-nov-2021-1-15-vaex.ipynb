{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Problem Statement\n\nIn this competition, we predict whether or not an email is spam.\n\nWe are going to cover the following steps:\n1. Install Vaex\n2. Adjust Matplotlib Parameters\n3. Load Data\n4. Shuffling\n5. Split into Train and Validation\n6. Sanity Checks\n7. Modeling (Part 1): Gradient Boosting Trees\n8. Performance on Training Set\n9. Performance on Validation Set\n10. Feature Importance\n11. Modeling (Part 2): Linear Models and Ensembles\n12. Ensemble\n13. References\n\nLet's get started.\n\n# Install Vaex","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"!pip install -I vaex","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-11-18T17:56:38.503695Z","iopub.execute_input":"2021-11-18T17:56:38.504202Z","iopub.status.idle":"2021-11-18T17:58:11.314743Z","shell.execute_reply.started":"2021-11-18T17:56:38.5041Z","shell.execute_reply":"2021-11-18T17:58:11.313664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load Libraries\nimport vaex\nvaex.multithreading.thread_count_default = 8\nimport vaex.ml\n\nimport numpy as np\nimport pylab as plt\nimport time\nfrom pathlib import Path\nimport pprint\nimport pandas\nfrom IPython.core.interactiveshell import InteractiveShell  # for printing all outputs of a cell \nInteractiveShell.ast_node_interactivity = \"all\" # to revert to original setting set InteractiveShell.ast_node_interactivity = \"last_expr\"\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2021-11-18T17:58:11.316901Z","iopub.execute_input":"2021-11-18T17:58:11.317141Z","iopub.status.idle":"2021-11-18T17:58:12.827755Z","shell.execute_reply.started":"2021-11-18T17:58:11.31711Z","shell.execute_reply":"2021-11-18T17:58:12.826765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Adjusting matplotlib parmeters\n\nLet's modify some of the matplotlib default settings, just to make the plots a bit more legible.","metadata":{}},{"cell_type":"code","source":"SMALL_SIZE = 12\nMEDIUM_SIZE = 14\nBIGGER_SIZE = 16\n\nplt.rc('font', size=SMALL_SIZE)          # controls default text sizes\nplt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\nplt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\nplt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\nplt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\nplt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\nplt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title","metadata":{"execution":{"iopub.status.busy":"2021-11-18T17:58:12.828962Z","iopub.execute_input":"2021-11-18T17:58:12.829299Z","iopub.status.idle":"2021-11-18T17:58:12.835989Z","shell.execute_reply.started":"2021-11-18T17:58:12.829252Z","shell.execute_reply":"2021-11-18T17:58:12.835265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"code","source":"# Load data using Vaex\nstart = time.time()\ndata_dir = Path('../input/tabular-playground-series-nov-2021/')\nvaex_train = vaex.read_csv(data_dir / \"train.csv\")\nvaex_test = vaex.read_csv(data_dir / \"test.csv\")\nend = time.time()\nprint(end - start)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T17:59:39.843141Z","iopub.execute_input":"2021-11-18T17:59:39.84357Z","iopub.status.idle":"2021-11-18T18:00:09.093183Z","shell.execute_reply.started":"2021-11-18T17:59:39.843526Z","shell.execute_reply":"2021-11-18T18:00:09.092533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# See the description\nvaex_train.info()","metadata":{"execution":{"iopub.status.busy":"2021-11-18T18:00:09.09666Z","iopub.execute_input":"2021-11-18T18:00:09.096894Z","iopub.status.idle":"2021-11-18T18:00:09.544222Z","shell.execute_reply.started":"2021-11-18T18:00:09.096866Z","shell.execute_reply":"2021-11-18T18:00:09.543306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Shuffling\n\nIf required, we can shuffle the dataset.","metadata":{}},{"cell_type":"code","source":"# let's shuffle\nvaex_train = vaex_train.shuffle(random_state=31)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T18:00:09.545756Z","iopub.execute_input":"2021-11-18T18:00:09.54606Z","iopub.status.idle":"2021-11-18T18:00:09.581528Z","shell.execute_reply.started":"2021-11-18T18:00:09.546017Z","shell.execute_reply":"2021-11-18T18:00:09.580779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Split into train and Validation\n\nOnce the data is shuffled, let’s split it into train and validation sets. The validation set will comprise 20% of the training data.","metadata":{}},{"cell_type":"code","source":"# Train and validation split, no shuffling occurs\ndf_train, df_validation = vaex_train.ml.train_test_split(test_size=0.2, verbose=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T18:00:09.582636Z","iopub.execute_input":"2021-11-18T18:00:09.583347Z","iopub.status.idle":"2021-11-18T18:00:09.632511Z","shell.execute_reply.started":"2021-11-18T18:00:09.583302Z","shell.execute_reply":"2021-11-18T18:00:09.631672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sanity Checks\n\nlet’s verify that our train and test sets are “similar” enough.\n\nLet us check the fraction of the target variable.","metadata":{}},{"cell_type":"code","source":"# Inspect the target variable\ntrain_spam_value_counts = df_train.target.value_counts()\nvalidation_spam_value_counts = df_validation.target.value_counts()\n\n\nplt.figure(figsize=(12, 4))\n\nplt.subplot(121)\ntrain_spam_value_counts.plot.bar()\ntrain_spam_ratio = train_spam_value_counts[1]/train_spam_value_counts[0]\nplt.title(f'Train set: spam ratio: {train_spam_ratio:.2f}')\nplt.ylabel('Number of Emails')\n\nplt.subplot(122)\nvalidation_spam_value_counts.plot.bar()\nvalidation_spam_ratio = validation_spam_value_counts[1]/validation_spam_value_counts[0]\nplt.title(f'Validation set: spam ratio: {validation_spam_ratio:.2f}')\nplt.ylabel('Number of Emails')\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-18T18:00:09.634653Z","iopub.execute_input":"2021-11-18T18:00:09.635092Z","iopub.status.idle":"2021-11-18T18:00:10.141039Z","shell.execute_reply.started":"2021-11-18T18:00:09.635059Z","shell.execute_reply":"2021-11-18T18:00:10.140471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling (part 1): gradient boosted trees","metadata":{}},{"cell_type":"code","source":"import xgboost\nimport vaex.ml.sklearn\n\nfeatures = vaex_train.column_names[1:-1] # because we want to exclude id and target columns from the training dataset\n\n# Instantiate the xgboost model normally, using the scikit-learn API\nxgb_model = xgboost.sklearn.XGBClassifier(\n#                                           max_depth=11,\n                                          learning_rate=0.1,\n#                                           n_estimators=500,\n#                                           subsample=0.75,\n#                                           colsample_bylevel=1,\n#                                           colsample_bytree=1,\n#                                           scale_pos_weight=1.5,\n                                          reg_lambda=1.5,\n                                          reg_alpha=5,\n#                                           n_jobs=8,\n                                          random_state=42,\n                                          use_label_encoder=False,\n                                          verbosity=0)\n\n# Make it work with vaex (for the automagic pipeline and lazy predictions)\nvaex_xgb_model = vaex.ml.sklearn.Predictor(features=features,\n                                           target='target',\n                                           model=xgb_model,\n                                           prediction_name='prediction_xgb')\n# Train the model\nvaex_xgb_model.fit(df_train)\n\n# Get the prediction of the model on the training data\ndf_train = vaex_xgb_model.transform(df_train)\n\n# Preview the resulting train dataframe that contans the predictions\ndf_train","metadata":{"execution":{"iopub.status.busy":"2021-11-18T18:35:44.322263Z","iopub.execute_input":"2021-11-18T18:35:44.322586Z","iopub.status.idle":"2021-11-18T18:46:07.197146Z","shell.execute_reply.started":"2021-11-18T18:35:44.322554Z","shell.execute_reply":"2021-11-18T18:46:07.195827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Performance on training set\n\nlet’s see what the performance is of the model on the training set. First let’s create a convenience function that will help us get multiple metrics at once.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\ndef binary_metrics(y_true, y_pred):\n    acc = accuracy_score(y_true=y_true, y_pred=y_pred)\n    f1 = f1_score(y_true=y_true, y_pred=y_pred)\n    roc = roc_auc_score(y_true=y_true, y_score=y_pred)\n    print(f'Accuracy: {acc:.3f}')\n    print(f'f1 score: {f1:.3f}')\n    print(f'roc-auc: {roc:.3f}')","metadata":{"execution":{"iopub.status.busy":"2021-11-18T18:46:07.198592Z","iopub.execute_input":"2021-11-18T18:46:07.198917Z","iopub.status.idle":"2021-11-18T18:46:07.207457Z","shell.execute_reply.started":"2021-11-18T18:46:07.198875Z","shell.execute_reply":"2021-11-18T18:46:07.206095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let’s check the performance of the model on the training set.","metadata":{}},{"cell_type":"code","source":"print('Metrics for the training set:')\nbinary_metrics(y_true=df_train.target.values, y_pred=df_train.prediction_xgb.values)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T18:46:07.208979Z","iopub.execute_input":"2021-11-18T18:46:07.209533Z","iopub.status.idle":"2021-11-18T18:46:09.221169Z","shell.execute_reply.started":"2021-11-18T18:46:07.209475Z","shell.execute_reply":"2021-11-18T18:46:09.220118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Performance on validation set\n\nLet's check the model performance on the validation set.","metadata":{}},{"cell_type":"code","source":"# Train the model\nvaex_xgb_model.fit(df_validation)\n\n# Get the prediction of the model on the validation data\ndf_validation = vaex_xgb_model.transform(df_validation)\n\n# Preview the resulting train dataframe that contans the predictions\ndf_validation","metadata":{"execution":{"iopub.status.busy":"2021-11-18T19:03:13.732941Z","iopub.execute_input":"2021-11-18T19:03:13.733861Z","iopub.status.idle":"2021-11-18T19:05:41.28278Z","shell.execute_reply.started":"2021-11-18T19:03:13.733812Z","shell.execute_reply":"2021-11-18T19:05:41.281703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Metrics for the validation set:')\nbinary_metrics(y_true=df_validation.target.values, y_pred=df_validation.prediction_xgb.values)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T19:05:41.284201Z","iopub.execute_input":"2021-11-18T19:05:41.284826Z","iopub.status.idle":"2021-11-18T19:05:41.799092Z","shell.execute_reply.started":"2021-11-18T19:05:41.284789Z","shell.execute_reply":"2021-11-18T19:05:41.798244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature importance\n\nLet’s now look at the feature importance of the xgboost model.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(6, 9))\n\nind = np.argsort(xgb_model.feature_importances_)[::-1]\nfeatures_sorted = np.array(features)[ind]\nimportances_sorted = xgb_model.feature_importances_[ind]\n\nplt.barh(y=range(len(features)), width=importances_sorted, height=0.2)\nplt.title('Gain')\nplt.yticks(ticks=range(len(features)), labels=features_sorted)\nplt.gca().invert_yaxis()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-18T19:05:41.800498Z","iopub.execute_input":"2021-11-18T19:05:41.800727Z","iopub.status.idle":"2021-11-18T19:05:43.427234Z","shell.execute_reply.started":"2021-11-18T19:05:41.800698Z","shell.execute_reply":"2021-11-18T19:05:43.426351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling (part 2): Linear models & Ensembles","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression","metadata":{"execution":{"iopub.status.busy":"2021-11-18T19:09:22.489864Z","iopub.execute_input":"2021-11-18T19:09:22.49018Z","iopub.status.idle":"2021-11-18T19:09:22.495184Z","shell.execute_reply.started":"2021-11-18T19:09:22.490147Z","shell.execute_reply":"2021-11-18T19:09:22.494347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The Support Vector Classifier\nvaex_svc = vaex.ml.sklearn.Predictor(features=features,\n                                     target='target',\n                                     model=SVC(max_iter=1000, random_state=42),\n                                     prediction_name='prediction_svc')\n\n# Logistic Regression\nvaex_logistic = vaex.ml.sklearn.Predictor(features=features,\n                                          target='target',\n                                          model=LogisticRegression(max_iter=1000, random_state=42, solver='liblinear'),\n                                          prediction_name='prediction_lr')\n\n# Train the new models and apply the transformation to the train dataframe\nfor model in [vaex_svc, vaex_logistic]:\n    model.fit(df_train)\n    df_train = model.transform(df_train)\n\n# Preview of the train DataFrame\ndf_train.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T19:19:52.384064Z","iopub.execute_input":"2021-11-18T19:19:52.384518Z","iopub.status.idle":"2021-11-18T19:25:09.21975Z","shell.execute_reply.started":"2021-11-18T19:19:52.384468Z","shell.execute_reply":"2021-11-18T19:25:09.218658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ensemble\n\nthe predictions from the SVC and the LogisticRegression classifiers are added as virtual columns in the training dataset. This is quite powerful, since now we can easily use them to create an ensemble! For example, let’s do a weighted mean.","metadata":{}},{"cell_type":"code","source":"# Weighed mean of the classes\nprediction_final = (df_train.prediction_xgb.astype('int') * 0.3 +\n                    df_train.prediction_svc.astype('int') * 0.5 +\n                    df_train.prediction_xgb.astype('int') * 0.2)\n# Get the predicted class\nprediction_final = (prediction_final >= 0.5)\n\n# Add the expression to the train DataFrame\ndf_train['prediction_final'] = prediction_final\n\n# Preview\ndf_train[df_train.get_column_names(regex='^predict')]","metadata":{"execution":{"iopub.status.busy":"2021-11-18T19:25:09.221096Z","iopub.execute_input":"2021-11-18T19:25:09.221409Z","iopub.status.idle":"2021-11-18T19:25:09.854828Z","shell.execute_reply.started":"2021-11-18T19:25:09.221368Z","shell.execute_reply":"2021-11-18T19:25:09.853898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"let’s check the performance of all the individual models as well as on the ensembler, on the validation set","metadata":{}},{"cell_type":"code","source":"pred_columns = df_validation.get_column_names(regex='^prediction_')\nfor i in pred_columns:\n    print(i)\n    binary_metrics(y_true=df_validation.target.values, y_pred=df_validation[i].values)\n    print(' ')","metadata":{"execution":{"iopub.status.busy":"2021-11-18T19:34:14.120208Z","iopub.execute_input":"2021-11-18T19:34:14.121092Z","iopub.status.idle":"2021-11-18T19:34:14.608629Z","shell.execute_reply.started":"2021-11-18T19:34:14.121047Z","shell.execute_reply":"2021-11-18T19:34:14.607504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# References\n\n1. Thank you to Vaex Documentation for showing [how to use Vaex](https://vaex.io/docs/example_ml_titanic.html#).\n2. This [Stack Overflow link](https://stackoverflow.com/questions/65682019/attributeerror-str-object-has-no-attribute-decode-in-fitting-logistic-regre) was used to resolve an error.","metadata":{}}]}