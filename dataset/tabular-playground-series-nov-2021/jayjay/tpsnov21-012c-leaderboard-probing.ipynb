{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Leaderboard Probing\n\nThis notebook is directly or indirectly based on work by @jmcslk, @criskiev, @grayjay, @javiervallejos, @adityasharma01, @sfktrkl, @motloch, @chaudharypriyanshu and others.\n\nWe first model the target distribution as a partition of 18 chunks, where each chunk has a fixed probability determined by [leaderboard probing](https://www.kaggle.com/ambrosm/tpsnov21-012-leaderboard-probing) and then blend the resulting model with the output of two [postprocessed](https://www.kaggle.com/ambrosm/tpsnov21-007-postprocessing) high-scoring public notebooks using the weights 91 : 8 : 1.\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport io\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import roc_auc_score, accuracy_score\nfrom sklearn.svm import LinearSVC","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-29T15:52:13.247793Z","iopub.execute_input":"2021-11-29T15:52:13.248358Z","iopub.status.idle":"2021-11-29T15:52:14.375075Z","shell.execute_reply.started":"2021-11-29T15:52:13.248198Z","shell.execute_reply":"2021-11-29T15:52:14.373995Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('../input/tabular-playground-series-nov-2021/train.csv')\ntest_df = pd.read_csv('../input/tabular-playground-series-nov-2021/test.csv')\npure_df = pd.read_csv('../input/november21/train.csv')\ntest_df['chunk'] = test_df.id // 60000","metadata":{"execution":{"iopub.status.busy":"2021-11-29T15:52:14.377246Z","iopub.execute_input":"2021-11-29T15:52:14.377491Z","iopub.status.idle":"2021-11-29T15:52:57.176261Z","shell.execute_reply.started":"2021-11-29T15:52:14.377463Z","shell.execute_reply":"2021-11-29T15:52:57.175374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def postprocess_separate(submission_df, test_df=None, pure_df=None):\n    \"\"\"Update submission_df so that the predictions for the two sides of the hyperplane don't overlap.\n    \n    Parameters\n    ----------\n    submission_df : pandas DataFrame with columns 'id' and 'target'\n    test_df : the competition's test data\n    pure_df : the competition's original training data\n    \n    From https://www.kaggle.com/ambrosm/tpsnov21-007-postprocessing\n    \"\"\"\n    if pure_df is None: pure_df = pd.read_csv('../input/november21/train.csv')\n    if pure_df.shape != (600000, 102): raise ValueError(\"pure_df has the wrong shape\")\n    if test_df is None: test_df = pd.read_csv('../input/tabular-playground-series-nov-2021/test.csv')\n    if test_df.shape[0] != submission_df.shape[0] or test_df.shape[1] != 101: raise ValueError(\"test_df has the wrong shape\")\n\n    # Find the separating hyperplane for pure_df, step 1\n    # Use an SVM with almost no regularization\n    model1 = make_pipeline(StandardScaler(), LinearSVC(C=1e5, tol=1e-7, penalty='l2', dual=False, max_iter=2000, random_state=1))\n    model1.fit(pure_df.drop(columns=['id', 'target']), pure_df.target)\n    pure_pred = model1.predict(pure_df.drop(columns=['id', 'target']))\n    #print((pure_pred != pure_df.target).sum(), (pure_pred == pure_df.target).sum()) # 1 599999\n    # model1 is not perfect: it predicts the wrong class for 1 of 600000 samples\n\n    # Find the separating hyperplane for pure_df, step 2\n    # Fit a second SVM to a subset of the points which contains the support vectors\n    pure_pred = model1.decision_function(pure_df.drop(columns=['id', 'target']))\n    subset_df = pure_df[(pure_pred > -5) & (pure_pred < 0.9)]\n    model2 = make_pipeline(StandardScaler(), LinearSVC(C=1e5, tol=1e-7, penalty='l2', dual=False, max_iter=2000, random_state=1))\n    model2.fit(subset_df.drop(columns=['id', 'target']), subset_df.target)\n    pure_pred = model2.predict(pure_df.drop(columns=['id', 'target']))\n    #print((pure_pred != pure_df.target).sum(), (pure_pred == pure_df.target).sum()) # 0 600000\n    # model2 is perfect: it predicts the correct class for all 600000 training samples\n    \n    pure_test_pred = model2.predict(test_df.drop(columns=['id', 'target'], errors='ignore'))\n    lmax, rmin = submission_df[pure_test_pred == 0].target.max(), submission_df[pure_test_pred == 1].target.min()\n    if lmax < rmin:\n        print(\"There is no overlap. No postprocessing needed.\")\n        return\n    # There is overlap. Remove this overlap\n    submission_df.loc[pure_test_pred == 0, 'target'] -= lmax + 1\n    submission_df.loc[pure_test_pred == 1, 'target'] -= rmin - 1\n    print(submission_df[pure_test_pred == 0].target.min(), submission_df[pure_test_pred == 0].target.max(),\n          submission_df[pure_test_pred == 1].target.min(), submission_df[pure_test_pred == 1].target.max())\n","metadata":{"execution":{"iopub.status.busy":"2021-11-29T15:52:57.177752Z","iopub.execute_input":"2021-11-29T15:52:57.178027Z","iopub.status.idle":"2021-11-29T15:52:57.191708Z","shell.execute_reply.started":"2021-11-29T15:52:57.177998Z","shell.execute_reply":"2021-11-29T15:52:57.19044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# name = name of chunk as in https://www.kaggle.com/ambrosm/tpsnov21-012-leaderboard-probing\n# len = number of samples in this chunk (len.sum() == 540000)\n# auc = public leaderboard score of this chunk\n# diff = difference of this chunk's auc score minus the baseline of 0.74723 = area of the added triangle\n# ratio = unused\n\nprobes_l = '''name\tlen\tauc\tdiff\tratio\n10H0\t30343\t74653\t-70\t -0.00231 \n17H0\t36335\t74671\t-52\t -0.00143 \n16H0\t41892\t74720\t-3\t -0.00007 \n13H0\t36383\t74724\t1\t 0.00003 \n18H0\t23746\t74729\t6\t 0.00025 \n11H0\t20501\t74732\t9\t 0.00044 \n14H0\t25270\t74747\t24\t 0.00095 \n12H0\t40308\t74763\t40\t 0.00099 \n15H0\t25965\t74762\t39\t 0.00150 \n'''\n\nprobes_r = '''\nname\tlen\tauc\tdiff\tratio\n10H1\t29657\t74695\t-28\t -0.00094 \n11H1\t39499\t74760\t37\t 0.00094 \n12H1\t19692\t74750\t27\t 0.00137 \n13H1\t23617\t74690\t-33\t -0.00140 \n14H1\t34730\t74735\t12\t 0.00035 \n15H1\t34035\t74815\t92\t 0.00270 \n16H1\t18108\t74682\t-41\t -0.00226 \n17H1\t23665\t74694\t-29\t -0.00123 \n18H1\t36254\t74683\t-40\t -0.00110 \n'''\n\nprobes_l_df = pd.read_csv(io.StringIO(probes_l), sep='\\t')\nprobes_r_df = pd.read_csv(io.StringIO(probes_r), sep='\\t')\n","metadata":{"execution":{"iopub.status.busy":"2021-11-29T15:52:57.192948Z","iopub.execute_input":"2021-11-29T15:52:57.193184Z","iopub.status.idle":"2021-11-29T15:52:57.231748Z","shell.execute_reply.started":"2021-11-29T15:52:57.193159Z","shell.execute_reply":"2021-11-29T15:52:57.230551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Left side\nl_dict = {}\nplt.figure(figsize=(10,10))\nfor row in probes_l_df.itertuples():\n    #print(row)\n    y0 = row.diff / 100000 * 8\n    plt.plot([0, 0.25], [y0, y0+0.75], color='r') # parallel for all points with this auc difference\n    plt.plot([0, row.len/270000], [row.len/270000, 0], color='g') # all points for this row.len\n    x = (row.len/270000 - y0) / 4\n    y = 3 * x + y0\n    plt.scatter([x], [y], color='k')\n    #print(f\"{row.name} {y/x:.5f} {x/(x+y):.5f}\")\n    l_dict[int(row.name[:2])] = x/(x+y)\n    print(f\"{row.name[:2]}: {x/(x+y):.5f},\")\nplt.plot([0, 0.25, 1], [0, 0.75, 1], color=\"y\", lw=1) # baseline roc curve (two segments)\nplt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\") # diagonal\nplt.gca().set_aspect('equal')\nif False:\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.0])\nelse:\n    plt.xlim([0.0, 0.2])\n    plt.ylim([0.0, 0.2])\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"Receiver operating characteristic\")\nplt.legend(loc=\"lower right\")\nplt.show()\nl_dict","metadata":{"execution":{"iopub.status.busy":"2021-11-29T15:52:57.234449Z","iopub.execute_input":"2021-11-29T15:52:57.235128Z","iopub.status.idle":"2021-11-29T15:52:57.622253Z","shell.execute_reply.started":"2021-11-29T15:52:57.235076Z","shell.execute_reply":"2021-11-29T15:52:57.621241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Right side\nr_dict = {}\nplt.figure(figsize=(10,10))\nfor row in probes_r_df.itertuples():\n    y0 = row.diff / 100000 * 8\n    plt.plot([0.25-y0, 1-y0], [0.75, 1], color='r') # parallel for all points with this auc difference\n    plt.plot([1-row.len/270000, 1], [1, 1-row.len/270000], color='g') # all points for this row.len\n    #x = (row.len/270000 - y0) / 4\n    #y = 3 * x + y0\n    x = (4 - 3*row.len/270000 - y0) / 4\n    y = 2 - row.len/270000 - x\n    plt.scatter([x], [y], color='k')\n    #print(f\"{row.name} {y/x:.5f} {x/(x+y):.5f}\")\n    r_dict[int(row.name[:2])] = (1-x)/(row.len/270000)\n    print(f\"{row.name[:2]}: {r_dict[int(row.name[:2])]:.5f},\")\nplt.plot([0, 0.25, 1], [0, 0.75, 1], color=\"y\", lw=1) # baseline roc curve (two segments)\nplt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\") # diagonal\nplt.gca().set_aspect('equal')\nif False:\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.0])\nelse:\n    plt.xlim([0.8, 1])\n    plt.ylim([0.8, 1])\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"Receiver operating characteristic\")\nplt.legend(loc=\"lower right\")\nplt.show()\nr_dict","metadata":{"execution":{"iopub.status.busy":"2021-11-29T15:52:57.623558Z","iopub.execute_input":"2021-11-29T15:52:57.623849Z","iopub.status.idle":"2021-11-29T15:52:57.944795Z","shell.execute_reply.started":"2021-11-29T15:52:57.623789Z","shell.execute_reply":"2021-11-29T15:52:57.943812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"baseline = pd.DataFrame({'id': test_df.id, 'target': 0})\npostprocess_separate(baseline, test_df=test_df.drop(columns='chunk'), pure_df=pure_df)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-29T15:53:36.373978Z","iopub.execute_input":"2021-11-29T15:53:36.374552Z","iopub.status.idle":"2021-11-29T15:54:06.447808Z","shell.execute_reply.started":"2021-11-29T15:53:36.374507Z","shell.execute_reply":"2021-11-29T15:54:06.446182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 18 probabilities for the 18 half-chunks -> lb 0.75110\nsub = baseline.copy()\nfor chunk in range(10, 19):\n    sub.loc[(test_df.chunk == chunk) & (baseline.target < 0), 'target'] = l_dict[chunk]\n    sub.loc[(test_df.chunk == chunk) & (baseline.target >= 0), 'target'] = r_dict[chunk]\n\nsub['target']=sub['target'].rank(pct=True)\nsub.to_csv(f'submission_probed.csv', index=False)\nsub.head(20)","metadata":{"execution":{"iopub.status.busy":"2021-11-29T15:54:06.449415Z","iopub.execute_input":"2021-11-29T15:54:06.450213Z","iopub.status.idle":"2021-11-29T15:54:07.829521Z","shell.execute_reply.started":"2021-11-29T15:54:06.450154Z","shell.execute_reply":"2021-11-29T15:54:07.828585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 8 % of @jmcslk's submission (which has lb 0.74996) -> lb 0.75209\njmcslk_submission = pd.read_csv('../input/tps-nov-2021-simple-single-nn-3/submission.csv')\npostprocess_separate(jmcslk_submission, test_df=test_df.drop(columns='chunk'), pure_df=pure_df)\nsub_8b = sub.copy()\n\nsub_8b['target'] += jmcslk_submission.target.rank(pct=True)\n\nsub_8b.to_csv(f'submission_probed_blended_8b.csv', index=False)\nsub_8b.head(20)","metadata":{"execution":{"iopub.status.busy":"2021-11-29T15:54:21.014542Z","iopub.execute_input":"2021-11-29T15:54:21.014832Z","iopub.status.idle":"2021-11-29T15:54:53.337482Z","shell.execute_reply.started":"2021-11-29T15:54:21.014804Z","shell.execute_reply":"2021-11-29T15:54:53.336735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 8 % of @sfktrkl's submission (which has lb 0.75002) -> lb 0.75204\nsfktrkl_submission = pd.read_csv('../input/tps-nov-2021-power-averaging/submission.csv')\npostprocess_separate(sfktrkl_submission, test_df=test_df.drop(columns='chunk'), pure_df=pure_df)\nsub_8c = sub.copy()\nsub_8c['target'] += sfktrkl_submission.target.rank(pct=True)\nsub_8c.to_csv(f'submission_probed_blended_8c.csv', index=False)\nsub_8c.head(20)\n","metadata":{},"execution_count":null,"outputs":[]}]}