{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Kaggle Tabular Playground Nov Data: Logistic regression","metadata":{}},{"cell_type":"markdown","source":"This work looks at Kaggle Tabular Playground Nov. Data and produces predictions for 'target' variable. \nThe data is synthetically generated by a GAN that was trained on a real-world dataset used to identify spam emails via various extracted features from the email.It contains 100 features with continuous values and one target variable (0,1).\nThe following steps were followed:\n\n- detailed exploratory data analysis\n- model construction and evaluation\n- hyperparameter tuning\n- producing predictions\n\n","metadata":{}},{"cell_type":"markdown","source":"## EDA","metadata":{}},{"cell_type":"markdown","source":"In this section train and test data sets are read, analysed and data checks are performed.","metadata":{}},{"cell_type":"code","source":"%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport glob\nimport os\nimport matplotlib.pyplot as plt\nfrom xgboost import XGBClassifier, plot_importance\nfrom sklearn.multioutput import MultiOutputRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import train_test_split, KFold\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfilename1 = r'/kaggle/input/tabular-playground-series-nov-2021/train.csv'\nfilename2 = r'/kaggle/input/tabular-playground-series-nov-2021/test.csv'\n\n\ndf_train = pd.read_csv(filename1, index_col=None, header=0)\ndf_test = pd.read_csv(filename2, index_col=None, header=0)\n#df=df.iloc[1:,:]\nprint(len(df_train), 'rows in training dataset')\nprint(df_train.head())\nprint(len(df_test), 'rows in test dataset')\nprint(df_test.head())","metadata":{"execution":{"iopub.status.busy":"2021-11-22T09:19:46.425362Z","iopub.execute_input":"2021-11-22T09:19:46.42599Z","iopub.status.idle":"2021-11-22T09:20:14.99579Z","shell.execute_reply.started":"2021-11-22T09:19:46.425948Z","shell.execute_reply":"2021-11-22T09:20:14.993966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.describe()","metadata":{"execution":{"iopub.status.busy":"2021-11-11T11:03:23.104074Z","iopub.execute_input":"2021-11-11T11:03:23.104419Z","iopub.status.idle":"2021-11-11T11:03:25.901923Z","shell.execute_reply.started":"2021-11-11T11:03:23.104385Z","shell.execute_reply":"2021-11-11T11:03:25.90108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.describe()","metadata":{"execution":{"iopub.status.busy":"2021-11-04T12:11:47.44501Z","iopub.execute_input":"2021-11-04T12:11:47.445257Z","iopub.status.idle":"2021-11-04T12:11:49.753461Z","shell.execute_reply.started":"2021-11-04T12:11:47.44523Z","shell.execute_reply":"2021-11-04T12:11:49.75259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is easy to see that data is continuous and is not normalised. Note that feature 'f2' gets significantly higher values compared to other features and depending on modelling technique adopted scaling could be required.","metadata":{}},{"cell_type":"markdown","source":"The next step is checking the columns for null values","metadata":{}},{"cell_type":"code","source":"df_train.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-11-04T12:16:24.67938Z","iopub.execute_input":"2021-11-04T12:16:24.679661Z","iopub.status.idle":"2021-11-04T12:16:24.983515Z","shell.execute_reply.started":"2021-11-04T12:16:24.67963Z","shell.execute_reply":"2021-11-04T12:16:24.982564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-11-04T12:16:30.14276Z","iopub.execute_input":"2021-11-04T12:16:30.143049Z","iopub.status.idle":"2021-11-04T12:16:30.272761Z","shell.execute_reply.started":"2021-11-04T12:16:30.143018Z","shell.execute_reply":"2021-11-04T12:16:30.271919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The preliminary checks detect no missing values hence the next step of analysis: distribution and feature correlation checks are performed.","metadata":{}},{"cell_type":"markdown","source":"### Distribution and correlation checks\n","metadata":{}},{"cell_type":"code","source":"Y=df_train['target']","metadata":{"execution":{"iopub.status.busy":"2021-11-04T13:29:07.604178Z","iopub.execute_input":"2021-11-04T13:29:07.604475Z","iopub.status.idle":"2021-11-04T13:29:07.609001Z","shell.execute_reply.started":"2021-11-04T13:29:07.604445Z","shell.execute_reply":"2021-11-04T13:29:07.608092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mask = df_train.dtypes == np.float\nfloat_cols = df_train.columns[mask]\nfig, axes = plt.subplots(len(float_cols) // 4, 4, figsize=(22, 40))\n#df[float_cols].hist(figsize=(20, 20), bins=50, xlabelsize=12, ylabelsize=12); \nfor col,axis in zip(float_cols,axes.reshape(-1)):\n    sns.histplot(df_train[col], ax=axis, kde=True,bins=100, label=f'train_{col}')\n    sns.histplot(df_test[col], color ='red' ,ax=axis, kde=True,bins=100, label=f'test_{col}')\n    axis.legend()\n    \n","metadata":{"execution":{"iopub.status.busy":"2021-11-04T13:29:54.033736Z","iopub.execute_input":"2021-11-04T13:29:54.034004Z","iopub.status.idle":"2021-11-04T13:40:37.533574Z","shell.execute_reply.started":"2021-11-04T13:29:54.033968Z","shell.execute_reply":"2021-11-04T13:40:37.532648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfcorr = df_train.corr()\nndf = dfcorr.loc[dfcorr.max(axis=1) > 0.50, dfcorr.max(axis=0) > 0.50]\n\nsns.heatmap(ndf)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-04T13:40:57.678999Z","iopub.execute_input":"2021-11-04T13:40:57.679737Z","iopub.status.idle":"2021-11-04T13:41:15.316239Z","shell.execute_reply.started":"2021-11-04T13:40:57.679701Z","shell.execute_reply":"2021-11-04T13:41:15.315322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that there is no strong correlation between features to be detected.","metadata":{}},{"cell_type":"code","source":"dfcorr['target']","metadata":{"execution":{"iopub.status.busy":"2021-11-04T13:41:43.710742Z","iopub.execute_input":"2021-11-04T13:41:43.710994Z","iopub.status.idle":"2021-11-04T13:41:43.720288Z","shell.execute_reply.started":"2021-11-04T13:41:43.710968Z","shell.execute_reply":"2021-11-04T13:41:43.719725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_train=df_train['target']\n","metadata":{"execution":{"iopub.status.busy":"2021-11-04T13:41:54.652955Z","iopub.execute_input":"2021-11-04T13:41:54.65323Z","iopub.status.idle":"2021-11-04T13:41:54.656843Z","shell.execute_reply.started":"2021-11-04T13:41:54.653201Z","shell.execute_reply":"2021-11-04T13:41:54.656245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Target Disribution check\n","metadata":{}},{"cell_type":"code","source":"## Target distibution\npie, ax = plt.subplots(figsize=[18,8])\ndf_train.groupby('target').size().plot(kind='pie',autopct='%.1f',colors=sns.color_palette('pastel')[0:2], x=ax,title='Target distibution')","metadata":{"execution":{"iopub.status.busy":"2021-11-04T13:43:05.864853Z","iopub.execute_input":"2021-11-04T13:43:05.865381Z","iopub.status.idle":"2021-11-04T13:43:06.002589Z","shell.execute_reply.started":"2021-11-04T13:43:05.865346Z","shell.execute_reply":"2021-11-04T13:43:06.001741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Skewness Check and Normalisation","metadata":{}},{"cell_type":"code","source":"# Create a list of float colums to check for skewing\nmask = df_train.dtypes == np.float\nfloat_cols = df_train.columns[mask]\nskew_limit = 0.75\nskew_vals = df_train[float_cols].skew()\n\nskew_cols = (skew_vals\n             .sort_values(ascending=False)\n             .to_frame()\n             .rename(columns={0:'Skew'})\n             .query('abs(Skew) > {0}'.format(skew_limit)))\n\nskew_cols = skew_cols.index.to_list()","metadata":{"execution":{"iopub.status.busy":"2021-11-22T09:20:49.613369Z","iopub.execute_input":"2021-11-22T09:20:49.614197Z","iopub.status.idle":"2021-11-22T09:20:50.715021Z","shell.execute_reply.started":"2021-11-22T09:20:49.614156Z","shell.execute_reply":"2021-11-22T09:20:50.7143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler\n\n# apply standard scaler to the data\nscaler = RobustScaler()\ndf_train[skew_cols] = scaler.fit_transform(df_train[skew_cols])\ndf_test[skew_cols] = scaler.transform(df_test[skew_cols])","metadata":{"execution":{"iopub.status.busy":"2021-11-22T09:20:54.903666Z","iopub.execute_input":"2021-11-22T09:20:54.90449Z","iopub.status.idle":"2021-11-22T09:20:56.751569Z","shell.execute_reply.started":"2021-11-22T09:20:54.904441Z","shell.execute_reply":"2021-11-22T09:20:56.749245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modelling and splitting the data into 5 KFolds","metadata":{}},{"cell_type":"code","source":"df_train=df_train.drop('id', axis=1)\ndf_test=df_test.drop('id', axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For modelling XGBoost was chosen as this is a powerful classifier to be used for logistic regression problems. The training set is split into 5 Folds and XGBClassifier is trained on that data. **Note** that tree based models do not require data scaling and normalising hence no data scaling/normalisation is performed.","metadata":{}},{"cell_type":"code","source":"xgb_classifier = XGBClassifier(booster='gbtree', \n                             max_depth = 4,\n                    objective = 'binary:logistic',         \n                    n_estimators=1000, \n                    learning_rate = .1)\n\nKFoldseed = 1\ncv = KFold(n_splits=5, shuffle=True, random_state=KFoldseed)","metadata":{"execution":{"iopub.status.busy":"2021-11-11T11:04:16.195039Z","iopub.execute_input":"2021-11-11T11:04:16.19533Z","iopub.status.idle":"2021-11-11T11:04:16.201917Z","shell.execute_reply.started":"2021-11-11T11:04:16.195299Z","shell.execute_reply":"2021-11-11T11:04:16.201053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#separate predictors and targets in data frame\n#remove id column as it is simply numbering the rows\nx_df = df_train[df_train.columns[:-1]]\ny_df = df_train[df_train.columns[-1:]]\nprint(x_df.shape)\nprint(y_df.shape)","metadata":{"execution":{"iopub.status.busy":"2021-11-22T09:51:29.124155Z","iopub.execute_input":"2021-11-22T09:51:29.124851Z","iopub.status.idle":"2021-11-22T09:51:29.269651Z","shell.execute_reply.started":"2021-11-22T09:51:29.124817Z","shell.execute_reply":"2021-11-22T09:51:29.268884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_df.columns\n","metadata":{"execution":{"iopub.status.busy":"2021-11-04T13:44:07.162157Z","iopub.execute_input":"2021-11-04T13:44:07.162422Z","iopub.status.idle":"2021-11-04T13:44:07.168898Z","shell.execute_reply.started":"2021-11-04T13:44:07.162396Z","shell.execute_reply":"2021-11-04T13:44:07.168086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\ncolumn='target'\nsigma_target_list = []\nsigma_target = np.std(y_df[column])\nsigma_target_list.append(sigma_target)\n\ncv_train_accuracy= []\ncv_test_accuracy = []\ncv_train_roc_auc= []\ncv_test_roc_auc = []\n\nfor train_idx, test_idx in cv.split(x_df):\n\n    x_train, x_test = x_df.iloc[train_idx], x_df.iloc[test_idx]\n    y_train, y_test = y_df[column].iloc[train_idx], y_df[column].iloc[test_idx]\n\n\n    fitted_model = xgb_classifier.fit(x_train, y_train, \n                                eval_set=[(x_train, y_train), (x_test, y_test)],\n                                eval_metric='auc',\n                                early_stopping_rounds=250, \n                                verbose=False)\n\n    pred_train = fitted_model.predict(x_train)\n    pred_train_prob = fitted_model.predict_proba(x_train)[:,1]\n\n    cv_train_accuracy.append(accuracy_score(y_train.values, pred_train))\n    cv_train_roc_auc.append(roc_auc_score(y_train, pred_train_prob))\n\n    pred_test = fitted_model.predict(x_test)\n    pred_test_prob = fitted_model.predict_proba(x_test)[:,1]\n    cv_test_accuracy.append(accuracy_score(y_test.values, pred_test))\n    cv_test_roc_auc.append(roc_auc_score(y_test.values, pred_test_prob))\n\n\n    ","metadata":{"execution":{"iopub.status.busy":"2021-11-04T13:44:12.275434Z","iopub.execute_input":"2021-11-04T13:44:12.276425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Other than results we are also interested in feature importance. This is important since if in the future hyperarameter optimization is performed and the goal is to improve the results elimination of some features can play an important role.","metadata":{}},{"cell_type":"code","source":"evals_result = fitted_model.evals_result()\nplt.plot(np.arange(len(evals_result['validation_0']['auc'])), evals_result['validation_0']['auc'], label='Training Set')\nplt.plot(np.arange(len(evals_result['validation_1']['auc'])), evals_result['validation_1']['auc'], label='Testing Set')\nplt.xlabel('Iteration')\nplt.ylabel('AUC')\nplt.title('Learning Curve for Target', fontweight='bold')\nplt.legend()\n\n#plot feature importance per target\nfeature_importances = pd.DataFrame(fitted_model.feature_importances_, index = x_df.columns, columns=['importance']).sort_values('importance')\nprint(feature_importances[feature_importances['importance']>0.02])\npos_importance = feature_importances[feature_importances['importance']>0.02]\npos_importance.plot(kind = 'barh',title=f'Target')\nplt.show()\nplt.clf()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Print model report:\nprint(\"\\nModel Report\")\nprint(\"XGBoost Mean Accuracy (Train) : %.4g\" % np.mean(cv_train_accuracy))\nprint(\"XGBoost Mean AUC Score (Train): %f\" % np.mean(cv_train_roc_auc))\n\nprint(\"XGBoost Mean Accuracy (Test) : %.4g\" % np.mean(cv_test_accuracy))\nprint(\"XGBoost Mean AUC Score (Test): %f\" % np.mean(cv_test_roc_auc))\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Confusion Matrix","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support\n# Creates a confusion matrix\ncm = confusion_matrix(y_test, pred_test) \n\n# Transform to df for easier plotting\ncm_df = pd.DataFrame(cm,\n                     index = ['0','1',], \n                     columns = ['0','1'])\n\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_df, annot=True,fmt='g')\nplt.title('XGBoost \\nAccuracy:{0:.3f}'.format(accuracy_score(y_test, pred_test)))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Improvements : LightGBM Model","metadata":{}},{"cell_type":"markdown","source":"XGBoost in this example takes a long time to run hence to LightGBM Classifier was tested to improve the execution times. Not only LightGBM rans faster but also it provides a slightly improved result both on training and testing sets.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\nimport lightgbm as lgb\ncolumn='target'\nsigma_target_list = []\nsigma_target = np.std(y_df[column])\nsigma_target_list.append(sigma_target)\n    \ncv_train_accuracy= []\ncv_test_accuracy = []\ncv_train_roc_auc= []\ncv_test_roc_auc = []\npreds = np.zeros(len(df_test))\nlgb = lgb.LGBMClassifier(learning_rate=0.04,max_depth=3, n_estimators=5000)\n\nfor train_idx, test_idx in cv.split(x_df):\n\n    x_train, x_test = x_df.iloc[train_idx], x_df.iloc[test_idx]\n    y_train, y_test = y_df[column].iloc[train_idx], y_df[column].iloc[test_idx]\n    \n    \n    fitted_model = lgb.fit(x_train, y_train, \n                                eval_set=[(x_train, y_train), (x_test, y_test)],\n                                eval_metric='auc',\n                                early_stopping_rounds=250, \n                                verbose=False)\n\n    pred_train = fitted_model.predict(x_train)\n    pred_train_prob = fitted_model.predict_proba(x_train)[:,1]\n   \n    cv_train_accuracy.append(accuracy_score(y_train.values, pred_train))\n    cv_train_roc_auc.append(roc_auc_score(y_train, pred_train_prob))\n   \n    pred_test = fitted_model.predict(x_test)\n    pred_test_prob = fitted_model.predict_proba(x_test)[:,1]\n    preds += fitted_model.predict_proba(df_test)[:,1]/5\n    cv_test_accuracy.append(accuracy_score(y_test.values, pred_test))\n    cv_test_roc_auc.append(roc_auc_score(y_test.values, pred_test_prob))","metadata":{"execution":{"iopub.status.busy":"2021-11-11T11:05:33.464904Z","iopub.execute_input":"2021-11-11T11:05:33.465233Z","iopub.status.idle":"2021-11-11T11:41:08.62521Z","shell.execute_reply.started":"2021-11-11T11:05:33.465201Z","shell.execute_reply":"2021-11-11T11:41:08.623863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Print model report:\nprint(\"\\nModel Report\")\nprint(\"LightGBM Mean Accuracy (Train) : %.4g\" % np.mean(cv_train_accuracy))\nprint(\"LightGBM Mean AUC Score (Train): %f\" % np.mean(cv_train_roc_auc))\n\nprint(\"LightGBM Mean Accuracy (Test) : %.4g\" % np.mean(cv_test_accuracy))\nprint(\"LightGBM Mean AUC Score (Test): %f\" % np.mean(cv_test_roc_auc))","metadata":{"execution":{"iopub.status.busy":"2021-11-11T11:48:24.181771Z","iopub.execute_input":"2021-11-11T11:48:24.182208Z","iopub.status.idle":"2021-11-11T11:48:24.193964Z","shell.execute_reply.started":"2021-11-11T11:48:24.182139Z","shell.execute_reply":"2021-11-11T11:48:24.192931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support\n# Creates a confusion matrix\ncm = confusion_matrix(y_test, pred_test) \n\n# Transform to df for easier plotting\ncm_df = pd.DataFrame(cm,\n                     index = ['0','1',], \n                     columns = ['0','1'])\n\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_df, annot=True,fmt='g')\nplt.title('LightGBM \\nAccuracy:{0:.3f}'.format(accuracy_score(y_test, pred_test)))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we see using LightGBM model has imporved the score and decreased the difference of AUC scores between the traning and testing sets.","metadata":{}},{"cell_type":"markdown","source":"## Improvements TabNet Model","metadata":{}},{"cell_type":"markdown","source":"Introduced in 2019 by Google TabNet (https://arxiv.org/pdf/1908.07442.pdf) is a Neural Network that was able to outperform the leading tree based models across a variety of benchmarks. It is also considered more explainable than boosted tree models and can be used without any feature preprocessing.  Hence it was interesting to try TabNet for this problem and see whether score can be improved. To test the labelled data set was split into 2 sets -training and test for TabNet to train.","metadata":{}},{"cell_type":"code","source":"!pip install pytorch-tabnet wget\n","metadata":{"execution":{"iopub.status.busy":"2021-11-22T09:19:20.073656Z","iopub.execute_input":"2021-11-22T09:19:20.073934Z","iopub.status.idle":"2021-11-22T09:19:31.031062Z","shell.execute_reply.started":"2021-11-22T09:19:20.073904Z","shell.execute_reply":"2021-11-22T09:19:31.030012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pytorch_tabnet\nfrom pytorch_tabnet.tab_model import TabNetClassifier\nimport numpy as np\nimport torch\nnp.random.seed(8)\ntabnetclass = TabNetClassifier(optimizer_fn=torch.optim.Adam,\n                       optimizer_params=dict(lr=2e-2),\n                       scheduler_params={\"step_size\":50, # how to use learning rate scheduler\n                                         \"gamma\":0.8},\n                       scheduler_fn=torch.optim.lr_scheduler.StepLR,\n                       mask_type='entmax' # \"sparsemax\"\n                      )\n\n","metadata":{"execution":{"iopub.status.busy":"2021-11-22T09:48:47.802522Z","iopub.execute_input":"2021-11-22T09:48:47.803094Z","iopub.status.idle":"2021-11-22T09:48:49.463225Z","shell.execute_reply.started":"2021-11-22T09:48:47.803052Z","shell.execute_reply":"2021-11-22T09:48:49.461508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nx_train, x_val, y_train, y_val = train_test_split(x_df, y_df, test_size=0.30, random_state=8)\nx_train_np= x_train.to_numpy()\ny_train_np= y_train.to_numpy().ravel()\nx_val_np = x_val.to_numpy()\ny_val_np = y_val.to_numpy().ravel()\n\ntabnetclass.fit(\n    x_train_np,y_train_np,\n    eval_set=[(x_train_np, y_train_np), (x_val_np, y_val_np)],\n    eval_name=['train', 'valid'],\n    eval_metric=['auc','accuracy'],\n    max_epochs=200 , patience=20,\n    batch_size=1024, virtual_batch_size=128,\n    num_workers=0,\n    weights=1,\n    drop_last=False\n)","metadata":{"execution":{"iopub.status.busy":"2021-11-22T09:52:33.920974Z","iopub.execute_input":"2021-11-22T09:52:33.921621Z","iopub.status.idle":"2021-11-22T10:06:53.569513Z","shell.execute_reply.started":"2021-11-22T09:52:33.921572Z","shell.execute_reply":"2021-11-22T10:06:53.568766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission","metadata":{}},{"cell_type":"code","source":"filename3 = r'/kaggle/input/tabular-playground-series-nov-2021/sample_submission.csv'\n\ndf3 = pd.read_csv(filename3, index_col=None, header=0)\n\ndf3['target']=preds\ndf3.to_csv('submission.csv', index=False)\ndf3\n","metadata":{"execution":{"iopub.status.busy":"2021-11-11T11:48:47.601422Z","iopub.execute_input":"2021-11-11T11:48:47.602089Z","iopub.status.idle":"2021-11-11T11:48:49.924671Z","shell.execute_reply.started":"2021-11-11T11:48:47.60203Z","shell.execute_reply":"2021-11-11T11:48:49.92367Z"},"trusted":true},"execution_count":null,"outputs":[]}]}