{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Introduction üìù\nüéØ Goal:Binary classification based on features\n\nüìñ Data:\n\ntrain.csv / test.csv - the training and testing set\n\nSubmissions are evaluated on area under the ROC curve between the predicted probability and the observed target. <br>\n______________________________________________________________________________________________________________________","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"### What is the purpose of the notebook?\n\nThis notebook looks at a simple nueral net model and identifies how we can interpret the results and insights from a nueral net model outputs","metadata":{}},{"cell_type":"code","source":"!pip install  tensorflow==2.5.0","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-13T09:31:49.047543Z","iopub.execute_input":"2021-11-13T09:31:49.047822Z","iopub.status.idle":"2021-11-13T09:31:49.051684Z","shell.execute_reply.started":"2021-11-13T09:31:49.047793Z","shell.execute_reply":"2021-11-13T09:31:49.050651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\nimport re\nimport time\nimport spacy\nimport gc\nimport shutil\nimport datatable as dt\nfrom pathlib import Path\nimport warnings\nimport os\nimport cupy as cp\nimport pandas as pd\nimport cudf\nimport dask_cudf\n","metadata":{"execution":{"iopub.status.busy":"2021-11-13T08:46:57.049237Z","iopub.execute_input":"2021-11-13T08:46:57.050077Z","iopub.status.idle":"2021-11-13T08:46:57.056587Z","shell.execute_reply.started":"2021-11-13T08:46:57.050026Z","shell.execute_reply":"2021-11-13T08:46:57.055479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Get the data and create train, test split","metadata":{}},{"cell_type":"code","source":"!pip freeze | grep 'tensorflow'","metadata":{"execution":{"iopub.status.busy":"2021-11-13T08:46:59.227586Z","iopub.execute_input":"2021-11-13T08:46:59.228237Z","iopub.status.idle":"2021-11-13T08:47:01.508331Z","shell.execute_reply.started":"2021-11-13T08:46:59.228196Z","shell.execute_reply":"2021-11-13T08:47:01.507437Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train=pd.read_csv(\"/kaggle/input/tabular-playground-series-nov-2021/train.csv\")\ntest=pd.read_csv(\"/kaggle/input/tabular-playground-series-nov-2021/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-11-13T08:47:14.090809Z","iopub.execute_input":"2021-11-13T08:47:14.09126Z","iopub.status.idle":"2021-11-13T08:47:32.188802Z","shell.execute_reply.started":"2021-11-13T08:47:14.09121Z","shell.execute_reply":"2021-11-13T08:47:32.188009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head(20)","metadata":{"execution":{"iopub.status.busy":"2021-11-13T08:47:32.190525Z","iopub.execute_input":"2021-11-13T08:47:32.190811Z","iopub.status.idle":"2021-11-13T08:47:32.227405Z","shell.execute_reply.started":"2021-11-13T08:47:32.190774Z","shell.execute_reply":"2021-11-13T08:47:32.226583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X=train.iloc[:,1:100]\nY=train['target']","metadata":{"execution":{"iopub.status.busy":"2021-11-13T08:47:32.228884Z","iopub.execute_input":"2021-11-13T08:47:32.229171Z","iopub.status.idle":"2021-11-13T08:47:32.371273Z","shell.execute_reply.started":"2021-11-13T08:47:32.229136Z","shell.execute_reply":"2021-11-13T08:47:32.370324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-11-13T08:47:32.37384Z","iopub.execute_input":"2021-11-13T08:47:32.374322Z","iopub.status.idle":"2021-11-13T08:47:32.673488Z","shell.execute_reply.started":"2021-11-13T08:47:32.374282Z","shell.execute_reply":"2021-11-13T08:47:32.67276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sklearn\nX_train, X_test, Y_train, Y_test = sklearn.model_selection.train_test_split(X, Y, test_size = 0.33, random_state = 5)\nprint('Y_train: ', Y_train.shape)\nprint('Y_test: ', Y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-11-13T08:47:32.675268Z","iopub.execute_input":"2021-11-13T08:47:32.676016Z","iopub.status.idle":"2021-11-13T08:47:33.187523Z","shell.execute_reply.started":"2021-11-13T08:47:32.675976Z","shell.execute_reply":"2021-11-13T08:47:33.18672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Create a simple DNN Model","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\ndef build_nn_model(input_size, output_size, activ_func='linear', dropout=0.2, loss='binary_crossentropy', optimizer='adam'):\n    model = keras.Sequential()\n    model.add(layers.Dense(512, input_dim=input_size, activation='relu'))\n    model.add(layers.Dropout(0.1))\n    model.add(layers.Dense(256, activation='relu'))\n    model.add(layers.Dropout(0.2))\n    model.add(layers.Dense(128, activation='relu'))\n    # model.add(Dropout(0.2))\n    model.add(layers.Dense(output_size, activation='sigmoid'))\n    # Compile model\n\n    model.compile(loss=loss, optimizer=optimizer, metrics=[ 'binary_crossentropy'])\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-11-13T09:00:00.355372Z","iopub.execute_input":"2021-11-13T09:00:00.355967Z","iopub.status.idle":"2021-11-13T09:00:00.364217Z","shell.execute_reply.started":"2021-11-13T09:00:00.355916Z","shell.execute_reply":"2021-11-13T09:00:00.363281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 5\nbatch_size = 500\nloss = 'binary_crossentropy'\ndropout = 0.2\noptimizer = 'adam'\nzero_base = True\noutput_size=1\ninput_size = 99","metadata":{"execution":{"iopub.status.busy":"2021-11-13T09:00:02.691841Z","iopub.execute_input":"2021-11-13T09:00:02.692602Z","iopub.status.idle":"2021-11-13T09:00:02.696887Z","shell.execute_reply.started":"2021-11-13T09:00:02.692566Z","shell.execute_reply":"2021-11-13T09:00:02.696084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping\nmodel = build_nn_model(\n    input_size, output_size=1, dropout=dropout, loss=loss,\n    optimizer=optimizer)\nhistory = model.fit(\n    X_train, Y_train, epochs=epochs, batch_size=batch_size, verbose=1, shuffle=True,\n    validation_data = (X_test,Y_test),\n    callbacks=[EarlyStopping(monitor='val_loss', patience=1,)])","metadata":{"execution":{"iopub.status.busy":"2021-11-13T09:00:04.160303Z","iopub.execute_input":"2021-11-13T09:00:04.160802Z","iopub.status.idle":"2021-11-13T09:01:03.238772Z","shell.execute_reply.started":"2021-11-13T09:00:04.160762Z","shell.execute_reply":"2021-11-13T09:01:03.237877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.predict(test.iloc[:,1:100][1:10])","metadata":{"execution":{"iopub.status.busy":"2021-11-13T09:01:05.631085Z","iopub.execute_input":"2021-11-13T09:01:05.631728Z","iopub.status.idle":"2021-11-13T09:01:05.866073Z","shell.execute_reply.started":"2021-11-13T09:01:05.631688Z","shell.execute_reply":"2021-11-13T09:01:05.865135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### SHAP\n\nyou can read more about SHAP here :https://github.com/slundberg/shap\n\nIt connects optimal credit allocation with local explanations using the classic Shapley values from game theory and their related extensions (see papers for details and citations).\n\nUsing DeepSHAP is pretty easy using shap python library. Just set it up as follows :\n\n","metadata":{}},{"cell_type":"code","source":"import shap\n\nshap.initjs()\n\n# Because our dataset is large we take a subset and use is to explain the model\nbackground = X_train.iloc[0:1000,:].values.astype('float')\nexplainer = shap.DeepExplainer(model, background)\nshap_values = explainer.shap_values(X=X_train.values[:500],\n                                      ranked_outputs=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-13T09:21:01.01967Z","iopub.execute_input":"2021-11-13T09:21:01.020263Z","iopub.status.idle":"2021-11-13T09:21:17.599968Z","shell.execute_reply.started":"2021-11-13T09:21:01.020221Z","shell.execute_reply":"2021-11-13T09:21:17.599193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shap.force_plot(explainer.expected_value.numpy(),\n                shap_values[0][0],\n                feature_names=X_train.columns)\n\nshap.force_plot(explainer.expected_value.numpy(),\n                shap_values[0][0][1],\n                X_train.values[:500][0],\n                feature_names=X_train.columns,)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-11-13T09:21:17.602385Z","iopub.execute_input":"2021-11-13T09:21:17.602934Z","iopub.status.idle":"2021-11-13T09:21:18.010801Z","shell.execute_reply.started":"2021-11-13T09:21:17.602892Z","shell.execute_reply":"2021-11-13T09:21:18.009952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Feature Importance","metadata":{}},{"cell_type":"code","source":"shap.summary_plot(shap_values[0], X_train[1:200], plot_type=\"bar\")","metadata":{"execution":{"iopub.status.busy":"2021-11-13T09:21:43.807901Z","iopub.execute_input":"2021-11-13T09:21:43.808195Z","iopub.status.idle":"2021-11-13T09:21:44.145453Z","shell.execute_reply.started":"2021-11-13T09:21:43.808165Z","shell.execute_reply":"2021-11-13T09:21:44.144775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-11-13T09:21:47.987532Z","iopub.execute_input":"2021-11-13T09:21:47.988155Z","iopub.status.idle":"2021-11-13T09:21:48.295517Z","shell.execute_reply.started":"2021-11-13T09:21:47.98811Z","shell.execute_reply":"2021-11-13T09:21:48.294737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### How to interpret the above?\n\nNotice there is this base value which is the expected value calculated by DeepSHAP which is just the value that would be predicted if you did not know any features. There is also this output value (i.e. the sumation of all feature contributions and base value) which is equal to the prediction of the actual model. SHAP values, then, just tells you how much contribution each feature adds in order to go from the base value to the output value.","metadata":{}},{"cell_type":"code","source":"record = 1 # this is just to pick one record in the dataset \nbase_value = explainer.expected_value\noutput= base_value + np.sum(shap_values[0][0][record])\nprint('base value: ',base_value)\nprint('output value: ',output)\n\n#sanity check that the ouput value is equal to the actual prediction\nprint(np.round(output,decimals=1) == np.round(model.predict(X_train.values)[record],decimals=1))\n\n\n# to get the shape values or each feature\nshap_df = pd.DataFrame(list(dict(zip(X_train.columns.values,shap_values[0][0][record])).items()),\n             columns=['features','shapvals']).sort_values(by='shapvals', ascending=True)\nshap_df","metadata":{"execution":{"iopub.status.busy":"2021-11-13T09:25:23.472713Z","iopub.execute_input":"2021-11-13T09:25:23.473403Z","iopub.status.idle":"2021-11-13T09:25:44.239055Z","shell.execute_reply.started":"2021-11-13T09:25:23.473368Z","shell.execute_reply":"2021-11-13T09:25:44.238336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above table shows how the NN derives the output for the particular sample from the base value. As you can see from the above there are positive and negative contributions. You will add them all to the base value to calculate the output that will happen for the given example.\n\nSo the total output is \n\n$basevalue +  + $$\\sum_{n=1}^{features} shapvalues_n$$","metadata":{}},{"cell_type":"markdown","source":"### Do upvote if you find the kernel useful","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}