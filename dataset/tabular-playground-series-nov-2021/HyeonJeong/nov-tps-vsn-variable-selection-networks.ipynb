{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.metrics import roc_auc_score\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    batch_size = strategy.num_replicas_in_sync * 64\n    print(\"Running on TPU:\", tpu.master())\n    print(f\"Batch Size: {batch_size}\")\n    \nexcept ValueError:\n    strategy = tf.distribute.get_strategy()\n    batch_size = 256\n    print(f\"Running on {strategy.num_replicas_in_sync} replicas\")\n    print(f\"Batch Size: {batch_size}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file_path = '/kaggle/input/tabular-playground-series-nov-2021/'\n\ntrain = pd.read_csv(file_path + 'train.csv')\ntest = pd.read_csv(file_path + 'test.csv')\nsub = pd.read_csv(file_path + 'sample_submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Train data shape: {train.shape}')\nprint(f'Test data shape: {test.shape}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_names = [f'f{i}' for i in range(100)]\ncsv_header = feature_names + ['target']\ntarget = train['target']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# feature scaling\nscaler = StandardScaler()\ntrain = scaler.fit_transform(train[feature_names])\ntest = scaler.transform(test[feature_names])\n\nscaler = MinMaxScaler()\ntrain = scaler.fit_transform(train)\ntest = scaler.transform(test)\n\ntrain = pd.DataFrame(train, columns=feature_names)\ntest = pd.DataFrame(test, columns=feature_names)\ntrain['target'] = target\ntest['target'] = target","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random_selection = np.random.rand(len(train.index)) <= 0.85\ntrain_data = train[random_selection]\nvalid_data = train[~random_selection]\ntest_data = test.copy()\n\ntrain_data.to_csv('train_data.csv', index=False, header=False)\nvalid_data.to_csv('valid_data.csv', index=False, header=False)\ntest_data.to_csv('test_data.csv', index=False, header=False)\n\nprint(f'Train data shape: {train_data.shape}')\nprint(f'Valid data shape: {valid_data.shape}')\nprint(f'Test data shape: {test.shape}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_dataset_from_csv(csv_file_path, shuffle=False, batch_size=128):\n    dataset = tf.data.experimental.make_csv_dataset(\n        csv_file_path,\n        batch_size=batch_size,\n        column_names=csv_header,\n        label_name='target',\n        num_epochs=1,\n        header=False,\n        shuffle=shuffle,\n    )\n    return dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_model_inputs():\n    inputs = {}\n    for feature_name in feature_names:\n        inputs[feature_name] = layers.Input(name=feature_name, shape=(), dtype=tf.float32)\n    return inputs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def encode_inputs(inputs, encoding_size):\n    encoded_features = []\n    for feature_name in inputs:\n        # Project the numeric feature to encoding_size using linear transformation.\n        encoded_feature = tf.expand_dims(inputs[feature_name], -1)\n        encoded_feature = layers.Dense(units=encoding_size)(encoded_feature)\n        encoded_features.append(encoded_feature)\n    return encoded_features","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class GatedLinearUnit(layers.Layer):\n    def __init__(self, units):\n        super(GatedLinearUnit, self).__init__()\n        self.linear = layers.Dense(units)\n        self.sigmoid = layers.Dense(units, activation=\"sigmoid\")\n\n    def call(self, inputs):\n        return self.linear(inputs) * self.sigmoid(inputs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class GatedResidualNetwork(layers.Layer):\n    def __init__(self, units, dropout_rate):\n        super(GatedResidualNetwork, self).__init__()\n        self.units = units\n        self.elu_dense = layers.Dense(units, activation=\"elu\")\n        self.linear_dense = layers.Dense(units)\n        self.dropout = layers.Dropout(dropout_rate)\n        self.gated_linear_unit = GatedLinearUnit(units)\n        self.layer_norm = layers.LayerNormalization()\n        self.project = layers.Dense(units)\n\n    def call(self, inputs):\n        x = self.elu_dense(inputs)\n        x = self.linear_dense(x)\n        x = self.dropout(x)\n        if inputs.shape[-1] != self.units:\n            inputs = self.project(inputs)\n        x = inputs + self.gated_linear_unit(x)\n        x = self.layer_norm(x)\n        return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class VariableSelection(layers.Layer):\n    def __init__(self, num_features, units, dropout_rate):\n        super(VariableSelection, self).__init__()\n        self.grns = list()\n        # Create a GRN for each feature independently\n        for idx in range(num_features):\n            grn = GatedResidualNetwork(units, dropout_rate)\n            self.grns.append(grn)\n        # Create a GRN for the concatenation of all the features\n        self.grn_concat = GatedResidualNetwork(units, dropout_rate)\n        self.softmax = layers.Dense(units=num_features, activation=\"softmax\")\n\n    def call(self, inputs):\n        v = layers.concatenate(inputs)\n        v = self.grn_concat(v)\n        v = tf.expand_dims(self.softmax(v), axis=-1)\n\n        x = []\n        for idx, inp in enumerate(inputs):\n            x.append(self.grns[idx](inp))\n        x = tf.stack(x, axis=1)\n\n        outputs = tf.squeeze(tf.matmul(v, x, transpose_a=True), axis=1)\n        return outputs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_model(encoding_size):\n    inputs = create_model_inputs()\n    feature_list = encode_inputs(inputs, encoding_size)\n    num_features = len(feature_list)\n\n    features = VariableSelection(num_features, encoding_size, dropout_rate)(feature_list)\n\n    outputs = layers.Dense(units=1, activation=\"sigmoid\")(features)\n    model = keras.Model(inputs=inputs, outputs=outputs)\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learning_rate = 0.001\ndropout_rate = 0.15\nbatch_size = 512\nnum_epochs = 15\nencoding_size = 512\n\nwith strategy.scope():\n    \n    model = create_model(encoding_size)\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n        loss=keras.losses.BinaryCrossentropy(),\n        metrics=[keras.metrics.AUC()],\n    )\n\n    es = tf.keras.callbacks.EarlyStopping(\n        monitor=\"val_loss\", patience=20, restore_best_weights=True\n    )\n\n    pl = tf.keras.callbacks.ReduceLROnPlateau(\n        monitor='val_loss', factor=0.1, patience=20, verbose=0,\n        mode='auto', min_delta=0.0001, cooldown=0, min_lr=0,\n    )\n    \n\n    print(\"Start training the model ...\")\n    train_dataset = get_dataset_from_csv('train_data.csv', shuffle=True, batch_size=batch_size)\n    valid_dataset = get_dataset_from_csv('valid_data.csv', batch_size=batch_size)\n    model.fit(\n        train_dataset,\n        epochs=num_epochs,\n        validation_data=valid_dataset,\n        callbacks=[es, pl],\n    )\n    print(\"Model training Completed.\")\n\n    \n    print(\"Predicting model performance ...\")\n    test_dataset = get_dataset_from_csv('test_data.csv', batch_size=batch_size)\n    preds = model.predict(test_dataset)\n    print(\"Prediction Completed.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub['target'] = preds\nsub.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}