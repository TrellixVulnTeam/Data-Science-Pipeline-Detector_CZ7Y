{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Learning with Mislabeled Data\n\nIn this notebook, we will explore the [cleanlab](https://github.com/cleanlab/cleanlab) library which provides functions for \"finding, quantifying, and learning with label errors in datasets.\" In particular, we will do the following:\n\n1. Use `get_noise_indices` to detect mislabeled training labels\n2. Use the `LearningWithNoisyLabels` wrapper with various scikit-learn compatible models to make predictions despite the mislabeled data.\n\n**Note:** We use the leaked training labels to test some of our cleanlab functions, however we won't use it to train any models.\n\n## -- Credits --\n\nThis notebook was inspired by the following discussions/notebooks:\n\n* [This discussion](https://www.kaggle.com/c/tabular-playground-series-nov-2021/discussion/285503) about the mislabeled training data and the [accompanying notebook](https://www.kaggle.com/motloch/nov21-mislabeled-25).\n* [This notebook](https://www.kaggle.com/criskiev/game-over-or-eda-of-the-leaked-train-csv) where the [original training labels](https://www.kaggle.com/criskiev/november21) were posted. \n* [This notebook](https://www.kaggle.com/kalaikumarr/comparing-22-different-classification-models) which gets baselines for various models.\n* [This notebook](https://www.kaggle.com/kaaveland/tps-nov-2021-some-models-that-work-ok) which tests various sklearn classifiers. I used this notebook to pick models (and sometimes parameters) to test with the `LearningWithNoisyLabels` wrapper.\n* [This notebook](https://www.kaggle.com/sugamkhetrapal/tps-nov-2021-1-14-xgboost-linear) which uses XGBoost with linear models (rather than trees as usual).\n\nPlease check these out (and upvote them!).","metadata":{}},{"cell_type":"code","source":"# Global variables for testing changes to this notebook quickly\nRANDOM_SEED = 0\nNUM_FOLDS = 8\n\n# Install cleanlab\n!pip install -q cleanlab","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generic imports\nimport numpy as np\nimport pandas as pd\nimport time\nimport gc\n\n# Hide warnings\nimport warnings\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \nwarnings.filterwarnings('ignore')\n\n# Plotting\nimport matplotlib.pyplot as plt\n\n# cleanlab\nimport cleanlab\nfrom cleanlab.pruning import get_noise_indices\nfrom cleanlab.classification import LearningWithNoisyLabels\n\n# Preprocessing\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n\n# Models & Evaluation\nfrom sklearn.metrics import roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score\nfrom sklearn.model_selection import StratifiedKFold, cross_val_predict\n\n# Models\nfrom sklearn.base import clone\nfrom sklearn.utils.extmath import softmax\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier\nfrom sklearn.neighbors import NearestCentroid\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.svm import LinearSVC\nfrom sklearn.neural_network import MLPClassifier\nfrom xgboost import XGBClassifier\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load data\noriginal_train = pd.read_csv('../input/november21/train.csv')\ntrain = pd.read_csv('../input/tabular-playground-series-nov-2021/train.csv')\ntest = pd.read_csv('../input/tabular-playground-series-nov-2021/test.csv')\nsubmission = pd.read_csv('../input/tabular-playground-series-nov-2021/sample_submission.csv')\n\n# Feature columns\nfeatures = [col for col in train.columns if col not in ['id', 'target']]\n\n# Check that the two train.csv are the same (except for the target)\nprint(train[features].equals(original_train[features]))\n\n# Save space\ny_actual = original_train['target'].copy()\ndel original_train\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Find Label Errors\n\nIn this section we use cleanlab functions to detect which labels are mislabeled. In particular, we do the following:\n\n1. Use logistic regression to estimate train label probabilities (from `predict_proba`)\n2.  `get_noisy_indices` to get the mislabled examples\n3. Compare with the actual mislabeled examples from the leaked training data.","metadata":{}},{"cell_type":"code","source":"# fix labels, assumes input is pandas dataframe/series\ndef fix_labels(X_train, y_train, y_actual):\n    \n    y_train = y_train.reset_index(drop = True)\n    y_actual = y_actual.reset_index(drop = True)\n    \n    # Logistic regression\n    pipeline = make_pipeline(\n        StandardScaler(),\n        LogisticRegression(\n            solver = 'saga', \n            random_state = RANDOM_SEED\n        ),\n    )\n\n    # Label probabilities\n    label_prob = cross_val_predict(\n        estimator = pipeline,\n        X = X_train,\n        y = y_train,\n        cv = StratifiedKFold(\n            n_splits = NUM_FOLDS, \n            shuffle = True, \n            random_state = RANDOM_SEED\n        ),\n        n_jobs = -1,\n        method = \"predict_proba\",\n    )\n\n    # Estimate label errors\n    pred_errors = get_noise_indices(\n        s = y_train,\n        psx = label_prob,\n        sorted_index_method='normalized_margin',\n     )\n\n    # Actual label errors\n    actual_errors = y_actual.index[y_train != y_actual].to_numpy()\n    \n    # Indicator vectors for label errors\n    y_true = y_actual.copy()\n    y_pred = y_train.copy()\n    \n    y_pred.values[:] = 0\n    y_pred.iloc[pred_errors] = 1\n    y_true.values[:] = 0\n    y_true.iloc[actual_errors] = 1\n\n    # Add \"fixed\" target labels\n    fixed = y_train.copy()\n    fixed.iloc[pred_errors] = (y_train.iloc[pred_errors] + 1) % 2\n    \n    return fixed, y_pred, y_true","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\npred_labels, pred_errors, true_errors = fix_labels(train[features], train['target'], y_actual)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Analysis\nprint(\"Total Rows:\", len(pred_labels))\nprint(\"Actual Errors:\", true_errors.sum())\nprint(\"Estimated Errors:\", pred_errors.sum())\nprint(\"\\nAccuracy:\", round(accuracy_score(true_errors, pred_errors), 3))\nprint(\"Precision:\", round(precision_score(true_errors, pred_errors), 3))\nprint(\"Recall:\", round(recall_score(true_errors, pred_errors), 3))\n\n# Confusion matrix\ncm = confusion_matrix(true_errors, pred_errors)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm)\ndisp.plot()\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Errors\")\nplt.ylabel(\"Actual Errors\")\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Testing Models with Noisy Data\n\nIn this section, we use a cleanlab function to make predictions on the partially mislabeled data using various scikit-learn compatibles models. We will do the following for each model:\n\n1. Get a baseline by training the vanilla model on the ~1/4 mislabeled training data\n2. Use `LearningWithNoisyLabels` to wrap the model and train on the same folds.\n\nWe check each of the following models:\n\n* Logistic Regression\n* Ridge Regression\n* Linear Discriminant Analysis\n* SGDClassifier\n* XGBoost\n* Multi-layer Perceptron Classifier\n\n**Note (1):** The wrapper expects a scikit-learn compatible estimators with `.fit()`, `.predict()` and `.predict_proba()` methods. Not all of these estimators have `.predict_proba()` methods so we have to extend them by defining our own (using the decision function and softmax).\n\n**Note (2):** The wrapper function attempts to fix the mislabeled data using cross-validation so instead of training one model per fold, we are actually training 5 models per fold. Hence, we should expect significantly longer training times.","metadata":{}},{"cell_type":"markdown","source":"## Scoring Functions\n\nThe following functions accept a scikit-learn compatible model or pipeline with fit, predict and predict_proba methods and return auc scores, out-of-fold predictions and test set predictions (averaged over each fold) for the vanilla models and the wrapped models, respectively.","metadata":{}},{"cell_type":"code","source":"# Scoring/Training Baseline Function\ndef train_model(sklearn_model):\n    \n    # Store the holdout predictions\n    oof_preds = np.zeros((train.shape[0],))\n    test_preds = np.zeros((test.shape[0],))\n    scores = np.zeros(NUM_FOLDS)\n    times = np.zeros(NUM_FOLDS)\n    print('')\n    \n    # Stratified k-fold cross-validation\n    skf = StratifiedKFold(n_splits = NUM_FOLDS, shuffle = True, random_state = RANDOM_SEED)\n    for fold, (train_idx, valid_idx) in enumerate(skf.split(train, train['target'])):\n        \n        # Training and Validation Sets\n        X_train, y_train = train[features].iloc[train_idx].to_numpy(), train['target'].iloc[train_idx].to_numpy()\n        X_valid, y_valid = train[features].iloc[valid_idx].to_numpy(), train['target'].iloc[valid_idx].to_numpy()\n        X_test = test[features]\n        \n        # Create model\n        model = clone(sklearn_model)\n            \n        start = time.time()\n\n        model.fit(X_train, y_train)\n        \n        end = time.time()\n        \n        # validation and test predictions\n        valid_preds = model.predict_proba(X_valid)[:, 1]\n        test_preds += model.predict_proba(X_test)[:, 1] / NUM_FOLDS\n        oof_preds[valid_idx] = valid_preds\n        \n        # fold auc score\n        fold_auc = roc_auc_score(y_valid, valid_preds)\n        end = time.time()\n        print(f'Fold {fold} (AUC): {round(fold_auc, 5)} in {round(end-start,2)}s.')\n        scores[fold] = fold_auc\n        times[fold] = end-start\n        \n        time.sleep(0.5)\n        \n    print(\"\\nAverage AUC:\", round(scores.mean(), 5))\n    print(f'Training Time: {round(times.sum(), 2)}s')\n    \n    return scores, test_preds, oof_preds","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scoring/Training function for LearningWithNoisyLabels\ndef train_noisy_model(sklearn_model):\n    \n    # Store the holdout predictions\n    oof_preds = np.zeros((train.shape[0],))\n    test_preds = np.zeros((test.shape[0],))\n    scores = np.zeros(NUM_FOLDS)\n    times = np.zeros(NUM_FOLDS)\n    print('')\n    \n    # Stratified k-fold cross-validation\n    skf = StratifiedKFold(n_splits = NUM_FOLDS, shuffle = True, random_state = RANDOM_SEED)\n    for fold, (train_idx, valid_idx) in enumerate(skf.split(train, train['target'])):\n        \n        # Training and Validation Sets\n        X_train, y_train = train[features].iloc[train_idx].to_numpy(), train['target'].iloc[train_idx].to_numpy()\n        X_valid, y_valid = train[features].iloc[valid_idx].to_numpy(), train['target'].iloc[valid_idx].to_numpy()\n        X_test = test[features]\n        \n        # Create model\n        model = LearningWithNoisyLabels(\n            clf = clone(sklearn_model)\n        )\n            \n        start = time.time()\n\n        model.fit(X_train, y_train)\n        \n        end = time.time()\n        \n        # validation and test predictions\n        valid_preds = model.predict_proba(X_valid)[:, 1]\n        test_preds += model.predict_proba(X_test)[:, 1] / NUM_FOLDS\n        oof_preds[valid_idx] = valid_preds\n        \n        # fold auc score\n        fold_auc = roc_auc_score(y_valid, valid_preds)\n        end = time.time()\n        print(f'Fold {fold} (AUC): {round(fold_auc, 5)} in {round(end-start,2)}s.')\n        scores[fold] = fold_auc\n        times[fold] = end-start\n        \n        time.sleep(0.5)\n        \n    print(\"\\nAverage AUC:\", round(scores.mean(), 5))\n    print(f'Training Time: {round(times.sum(), 2)}s')\n    \n    return scores, test_preds, oof_preds","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.1 Logistic Regression","metadata":{}},{"cell_type":"code","source":"# Logistic Regression\nlogit_pipeline = make_pipeline(\n    StandardScaler(),\n    LogisticRegression(\n        solver = 'saga',\n        random_state = RANDOM_SEED,\n        n_jobs = -1,\n    ),\n)","metadata":{"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Logistic Regression Baseline\nlogit_scores, logit_preds, logit_oof = train_model(logit_pipeline)\n\nsubmission['target'] = logit_preds\nsubmission.to_csv('logit_submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Logistic Regression w/ Wrapper\nnoisy_logit_scores, noisy_logit_preds, noisy_logit_oof = train_noisy_model(logit_pipeline)\n\nsubmission['target'] = noisy_logit_preds\nsubmission.to_csv('noisy_logit_submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.2 Ridge Regression\n\nThe wrapper function expects an estimator with a `predict_proba` method, so we create an equivalent using softmax:","metadata":{}},{"cell_type":"code","source":"# Class extending Ridge Regression\nclass ExtendedRidgeClassifier(RidgeClassifier):\n    def predict_proba(self, X):\n        temp = self.decision_function(X)\n        return softmax(np.c_[-temp, temp])\n    \n# Ridge Regression\nridge_pipeline = make_pipeline(\n    StandardScaler(),\n    ExtendedRidgeClassifier(random_state = RANDOM_SEED),\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Ridge Regression Baseline\nridge_scores, ridge_preds, ridge_oof = train_model(ridge_pipeline)\n\nsubmission['target'] = ridge_preds\nsubmission.to_csv('ridge_submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Ridge Regression w/ Wrapper\nnoisy_ridge_scores, noisy_ridge_preds, noisy_ridge_oof = train_noisy_model(ridge_pipeline)\n\nsubmission['target'] = noisy_ridge_preds\nsubmission.to_csv('noisy_ridge_submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.3 Linear Discriminant Analysis","metadata":{}},{"cell_type":"code","source":"# Linear Discriminant Analysis\nlda_pipeline = make_pipeline(\n    StandardScaler(),\n    LinearDiscriminantAnalysis(),\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lda_scores, lda_preds, lda_oof = train_model(lda_pipeline)\n\nsubmission['target'] = lda_preds\nsubmission.to_csv('lda_submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"noisy_lda_scores, noisy_lda_preds, noisy_lda_oof = train_noisy_model(lda_pipeline)\n\nsubmission['target'] = noisy_lda_preds\nsubmission.to_csv('noisy_lda_submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.4 SGDClassifier\n\nWe use the parameters borrowed from [this notebook](https://www.kaggle.com/kaaveland/tps-nov-2021-some-models-that-work-ok). Again, since the wrapper function expects an estimator with a `predict_proba` method, we create an equivalent using softmax:","metadata":{}},{"cell_type":"code","source":"# Extended SGDClassifier\nclass ExtendedSGDClassifier(SGDClassifier):\n    def predict_proba(self, X):\n        temp = self.decision_function(X)\n        return softmax(np.c_[-temp, temp])\n\n# SGDClassifier\nsgd_pipeline = make_pipeline(\n    RobustScaler(), \n    ExtendedSGDClassifier(\n        loss='hinge', \n        learning_rate='adaptive', \n        penalty='l2', \n        alpha=1e-3, \n        eta0=0.025,\n        random_state = RANDOM_SEED\n    )\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sgd_scores, sgd_preds, sgd_oof = train_model(sgd_pipeline)\n\nsubmission['target'] = sgd_preds\nsubmission.to_csv('sgd_submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"noisy_sgd_scores, noisy_sgd_preds, noisy_sgd_oof = train_noisy_model(sgd_pipeline)\n\nsubmission['target'] = noisy_sgd_preds\nsubmission.to_csv('noisy_sgd_submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.5 Naive Bayes Classifier","metadata":{}},{"cell_type":"code","source":"# Naive Bayes Classifier\nnb_pipeline = make_pipeline(\n    MinMaxScaler(),\n    MultinomialNB(),\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nb_scores, nb_preds, nb_oof = train_model(nb_pipeline)\n\nsubmission['target'] = nb_preds\nsubmission.to_csv('nb_submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"noisy_nb_scores, noisy_nb_preds, noisy_nb_oof = train_noisy_model(nb_pipeline)\n\nsubmission['target'] = noisy_nb_preds\nsubmission.to_csv('noisy_nb_submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.6 Multi-Layer Perceptron Classifier","metadata":{}},{"cell_type":"code","source":"# Multi-layer Perceptron Classifier\nmlp_pipeline = make_pipeline(\n    StandardScaler(),\n    MLPClassifier(\n        hidden_layer_sizes=(128, 64),\n        batch_size = 256, \n        early_stopping = True,\n        validation_fraction = 0.2,\n        n_iter_no_change = 5,\n        random_state = RANDOM_SEED\n    ),\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mlp_scores, mlp_preds, mlp_oof = train_model(mlp_pipeline)\n\nsubmission['target'] = mlp_preds\nsubmission.to_csv('mlp_submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"noisy_mlp_scores, noisy_mlp_preds, noisy_mlp_oof = train_noisy_model(mlp_pipeline)\n\nsubmission['target'] = noisy_mlp_preds\nsubmission.to_csv('noisy_mlp_submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.7 XGBoost with Linear Models","metadata":{}},{"cell_type":"code","source":"# XGBoost Classifier\nxgb_pipeline = make_pipeline(\n    StandardScaler(),\n    XGBClassifier(\n        booster = 'gblinear',\n        eval_metric = 'auc',\n        random_state = RANDOM_SEED\n    ),\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_scores, xgb_preds, xgb_oof = train_model(xgb_pipeline)\n\nsubmission['target'] = xgb_preds\nsubmission.to_csv('xgb_submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"noisy_xgb_scores, noisy_xgb_preds, noisy_xgb_oof = train_noisy_model(xgb_pipeline)\n\nsubmission['target'] = noisy_xgb_preds\nsubmission.to_csv('noisy_xgb_submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that the `LearningWithNoisyLabels` wrapper doesn't necessarily lead to better model performance. It may be worth exploring further, especially with better parameter tuning since we mostly only used default settings. However the training slow down may not be worthwhile.","metadata":{}}]}