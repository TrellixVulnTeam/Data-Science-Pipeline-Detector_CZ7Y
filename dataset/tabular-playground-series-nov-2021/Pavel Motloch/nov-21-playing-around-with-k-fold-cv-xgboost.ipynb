{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Summary","metadata":{}},{"cell_type":"markdown","source":"In this notebook our aim is focusing on K-fold cross validation and getting better intuition for it. We take a bunch of models (all XGBoost, with varying number of estimators) and see how well we can use AUC estimated from the out-of-fold part of the training set to predict AUC actually achieved on the test set.\n\nWe find that the results on the test set depend only very little on the K-fold cross validation we use. The out-of-fold AUC undershoot the test set AUC quite notably and the difference seems to be the largest for the 10-fold CV. This is a bit surprising, though it might be related to the large number of mislabeled samples.","metadata":{}},{"cell_type":"markdown","source":"# Import libraries","metadata":{}},{"cell_type":"code","source":"# import libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import roc_auc_score, roc_curve, accuracy_score\nfrom sklearn.model_selection import KFold\nnp.random.seed(42)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-09T00:26:47.135605Z","iopub.execute_input":"2021-11-09T00:26:47.136087Z","iopub.status.idle":"2021-11-09T00:26:48.179701Z","shell.execute_reply.started":"2021-11-09T00:26:47.136054Z","shell.execute_reply":"2021-11-09T00:26:48.178556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define various parameters of the K-fold CV and models. Plus the test set size.","metadata":{}},{"cell_type":"code","source":"# size of the test set (for our purposes we need to know the targets, \n# so this is different from the competition test set)\nTEST_SIZE = 0.9\n\n# number of folds to try out\nN_FOLDS = [3, 5, 7, 10]\n\n# define model parameters\nLEARNING_RATE = 0.1\nN_ESTIMATORS  = [10, 25, 50, 75, 100, 125, 150, 200] # this defines different models\nMAX_DEPTH     = 4\nN_JOBS        = 16\nTREE_METHOD   = 'hist'\nVERBOSITY     = 1","metadata":{"execution":{"iopub.status.busy":"2021-11-09T02:56:05.749119Z","iopub.execute_input":"2021-11-09T02:56:05.749454Z","iopub.status.idle":"2021-11-09T02:56:05.757088Z","shell.execute_reply.started":"2021-11-09T02:56:05.749421Z","shell.execute_reply":"2021-11-09T02:56:05.755942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load the data, separate training and test set","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/tabular-playground-series-nov-2021/train.csv')","metadata":{"execution":{"iopub.status.busy":"2021-11-09T00:26:49.058274Z","iopub.execute_input":"2021-11-09T00:26:49.059097Z","iopub.status.idle":"2021-11-09T00:27:08.132844Z","shell.execute_reply.started":"2021-11-09T00:26:49.059045Z","shell.execute_reply":"2021-11-09T00:27:08.131925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-09T00:27:08.134579Z","iopub.execute_input":"2021-11-09T00:27:08.134889Z","iopub.status.idle":"2021-11-09T00:27:08.173666Z","shell.execute_reply.started":"2021-11-09T00:27:08.134851Z","shell.execute_reply":"2021-11-09T00:27:08.172833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Separate train and test sets","metadata":{}},{"cell_type":"code","source":"ids = train['id'].values\ntrain_ids = np.random.choice(ids, replace=False, size=int((1 - TEST_SIZE) * len(train)))\ntest_ids = np.array([x for x in ids if x not in train_ids])","metadata":{"execution":{"iopub.status.busy":"2021-11-09T02:56:43.646229Z","iopub.execute_input":"2021-11-09T02:56:43.647234Z","iopub.status.idle":"2021-11-09T02:56:59.789505Z","shell.execute_reply.started":"2021-11-09T02:56:43.647189Z","shell.execute_reply":"2021-11-09T02:56:59.788489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = train[train['id'].isin(train_ids)]['target'].values\nX_train = train[train['id'].isin(train_ids)].drop(['id', 'target'], axis = 1).values\ny_test  = train[train['id'].isin(test_ids )]['target'].values\nX_test  = train[train['id'].isin(test_ids )].drop(['id', 'target'], axis = 1).values","metadata":{"execution":{"iopub.status.busy":"2021-11-09T02:56:59.791249Z","iopub.execute_input":"2021-11-09T02:56:59.791485Z","iopub.status.idle":"2021-11-09T02:57:01.17392Z","shell.execute_reply.started":"2021-11-09T02:56:59.791456Z","shell.execute_reply":"2021-11-09T02:57:01.172767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train various XGBoost classifiers using different number of K-folds","metadata":{}},{"cell_type":"markdown","source":"Here we train various XGBoost classifiers using K-fold cross validation. To simulate a set of models, at each number of K-folds we train several XGBoosts each with a different number of estimators.\n\nFor each model and K-fold CV we store average AUC on the test. For later analysis, for each fold we also store AUC for the out-of-fold part of the train set.","metadata":{}},{"cell_type":"code","source":"# Save final AUC for each K-fold and for each XGB model (defined by the number of estimators)\nauc = [[] for x in N_FOLDS]\n\n# Also save the results for out-of-fold part of the train set\n# Three indices: 1. how many K-folds (3, 5, ...) \n#                2. which model (how many estimators) \n#                3. which out-of-fold\noof_auc = [[] for x in N_FOLDS]\n    \n#Iterate over all possible K-fold cross-validation strategies\nfor i, nf in enumerate(N_FOLDS):\n    kf = KFold(n_splits=nf) \n    \n    print(f'\\nRunning {nf}-fold splitting')\n    print('-----')\n\n    # Iterate over XGB models (determined by the number of estimators)\n    for nest in N_ESTIMATORS:\n\n        print(f'Running {nest} estimators')\n\n        # define the model\n        xgb = XGBClassifier(learning_rate = LEARNING_RATE, n_estimators = nest, max_depth = MAX_DEPTH, \n                                n_jobs = N_JOBS, tree_method = TREE_METHOD, verbosity=VERBOSITY, \n                                eval_metric = 'logloss', use_label_encoder = False)\n\n\n        # predictions on the test set - we will average over the K-folds\n        y_test_pred = np.zeros(len(X_test))\n\n        # out-of-fold AUC for the current CV strategy/model\n        c_oof_auc = []\n\n        # iterate over the K-folds\n        for train_index, valid_index in kf.split(X_train):\n\n            # fit the model\ton the train set\n            model_xgb = xgb.fit(X_train[train_index],y_train[train_index])\n\n            # predict on the test set - average over the K-folds\n            y_test_pred += model_xgb.predict_proba(X_test)[:,1]/nf\n\n            # predict on the out-of-fold part of the train set\n            y_valid_pred = model_xgb.predict_proba(X_train[valid_index])[:,1]\n            c_oof_auc.append(roc_auc_score(y_train[valid_index], y_valid_pred))\n\n        # Save area under the curve for the final prediction \n        auc[i].append(roc_auc_score(y_test, y_test_pred))\n        # and also for the out-of-fold part of the training set\n        oof_auc[i].append(c_oof_auc)\n        \n        # Keep us informed about what is going on\n        print(f'     auc: {auc[i][-1]}')\n        print(f'     oof auc: {oof_auc[i][-1]}')","metadata":{"execution":{"iopub.status.busy":"2021-11-09T03:05:34.053556Z","iopub.execute_input":"2021-11-09T03:05:34.053966Z","iopub.status.idle":"2021-11-09T03:21:51.621315Z","shell.execute_reply.started":"2021-11-09T03:05:34.053934Z","shell.execute_reply":"2021-11-09T03:21:51.619874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Investigate the results","metadata":{}},{"cell_type":"markdown","source":"First let's compare the final results on the test set. We find that regardless of the cross-validation strategy, we get very similar results! The choice of model matters way more than the number of K-folds we choose.","metadata":{}},{"cell_type":"code","source":"cols = ['r', 'g', 'b', 'k']\n\nfor idx, col in zip(range(4), cols):\n    plt.scatter(N_ESTIMATORS, auc[idx], color = col, label = f'{N_FOLDS[idx]} folds')\n    \nplt.legend()\nplt.ylabel('AUC')\nplt.xlabel('Number of XGBoost estimators');","metadata":{"execution":{"iopub.status.busy":"2021-11-09T01:53:02.605064Z","iopub.execute_input":"2021-11-09T01:53:02.605352Z","iopub.status.idle":"2021-11-09T01:53:02.94405Z","shell.execute_reply.started":"2021-11-09T01:53:02.605318Z","shell.execute_reply":"2021-11-09T01:53:02.942933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we look at the differences. We subtract the result obtained with 10 K-folds and confirm that the differences are really tiny","metadata":{}},{"cell_type":"code","source":"for idx, col in zip(range(4), cols):\n    plt.scatter(N_ESTIMATORS, auc[idx] - np.array(auc[-1]), color = col, label = f'{N_FOLDS[idx]} folds')\n    # we convert to np.array as subtracting two lists is not defined\n    \nplt.legend()\nplt.ylabel(f'AUC relative to the {N_FOLDS[-1]}-fold result')\nplt.xlabel('Number of XGBoost estimators');","metadata":{"execution":{"iopub.status.busy":"2021-11-09T01:55:25.640769Z","iopub.execute_input":"2021-11-09T01:55:25.641201Z","iopub.status.idle":"2021-11-09T01:55:25.980034Z","shell.execute_reply.started":"2021-11-09T01:55:25.641169Z","shell.execute_reply":"2021-11-09T01:55:25.978858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we compare the results on the test set (which do not depend much on number of K-folds, so we just plot one with black points) with the out-of-fold results (color, showing +- one standard deviation).","metadata":{}},{"cell_type":"code","source":"cols = ['r', 'g', 'b', 'orange']\n\nfor i, ne in enumerate(N_ESTIMATORS):\n    for j, col in enumerate(cols):\n        if i == 0: #Avoid having too large a legend\n            plt.errorbar(\n                ne + 2*j - 3, \n                np.mean(oof_auc[j][i]), \n                yerr = np.std(oof_auc[j][i]), \n                color = col,\n                label = f'{N_FOLDS[j]} folds'\n            )\n        else:\n            plt.errorbar(\n                ne + 2*j - 3, \n                np.mean(oof_auc[j][i]), \n                yerr = np.std(oof_auc[j][i]), \n                color = col\n            )\n\nplt.scatter(N_ESTIMATORS, auc[-1], color = 'k')\nplt.legend();\nplt.ylabel('AUC')\nplt.xlabel('Number of XGBoost estimators');","metadata":{"execution":{"iopub.status.busy":"2021-11-09T03:26:36.297431Z","iopub.execute_input":"2021-11-09T03:26:36.297751Z","iopub.status.idle":"2021-11-09T03:26:36.709014Z","shell.execute_reply.started":"2021-11-09T03:26:36.297718Z","shell.execute_reply":"2021-11-09T03:26:36.707832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that the out-of-fold results undershoot the black dots, which represent the test set results. This is not that surprising as each was out-of-fold was trained on a subset of the data only and we expect the out-of-fold results to trail a bit. \n\nWhat is a bit surprising is the magnitude of the difference and the fact that the 10-fold CV leads to the worst out-of-fold result. It also has the largest error standard deviation. \n\nOverall we expected a somewhat tighter relation between the out-of-fold results and the results on the test set, but this might be caused by the large number of mislabelled data that [appears to be present](https://www.kaggle.com/motloch/nov21-mislabeled-25).","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}