{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction","metadata":{}},{"cell_type":"markdown","source":"As found out by [@grayjay](https://www.kaggle.com/grayjay) [here](https://www.kaggle.com/c/tabular-playground-series-nov-2021/discussion/286731), the train data appears to be in ten chunks with different average target value and differently distributed features. Here we check these statements with a few plots.","metadata":{}},{"cell_type":"markdown","source":"# Import the data","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-10T15:38:43.937112Z","iopub.execute_input":"2021-11-10T15:38:43.938139Z","iopub.status.idle":"2021-11-10T15:38:43.961687Z","shell.execute_reply.started":"2021-11-10T15:38:43.937997Z","shell.execute_reply":"2021-11-10T15:38:43.960934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/tabular-playground-series-nov-2021/train.csv')","metadata":{"execution":{"iopub.status.busy":"2021-11-10T15:38:43.963262Z","iopub.execute_input":"2021-11-10T15:38:43.963739Z","iopub.status.idle":"2021-11-10T15:39:01.493395Z","shell.execute_reply.started":"2021-11-10T15:38:43.963673Z","shell.execute_reply":"2021-11-10T15:39:01.492399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-10T15:39:01.494642Z","iopub.execute_input":"2021-11-10T15:39:01.494884Z","iopub.status.idle":"2021-11-10T15:39:01.5323Z","shell.execute_reply.started":"2021-11-10T15:39:01.494856Z","shell.execute_reply":"2021-11-10T15:39:01.531312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Alleged size of the chunk\nCS = 60000","metadata":{"execution":{"iopub.status.busy":"2021-11-10T15:39:01.534103Z","iopub.execute_input":"2021-11-10T15:39:01.534337Z","iopub.status.idle":"2021-11-10T15:39:01.538609Z","shell.execute_reply.started":"2021-11-10T15:39:01.534309Z","shell.execute_reply":"2021-11-10T15:39:01.537665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cumulative sum of target differs between chunks?","metadata":{}},{"cell_type":"markdown","source":"As suggested by @grayjay in the comments, it is easier to see the difference if we replace \ntarget zero with target -1","metadata":{}},{"cell_type":"code","source":"zero_target = train['target'] == 0\ntrain.loc[zero_target, 'target'] = -1","metadata":{"execution":{"iopub.status.busy":"2021-11-10T15:41:48.630399Z","iopub.execute_input":"2021-11-10T15:41:48.630738Z","iopub.status.idle":"2021-11-10T15:41:48.640112Z","shell.execute_reply.started":"2021-11-10T15:41:48.630704Z","shell.execute_reply":"2021-11-10T15:41:48.639033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is the variable we should be looking at","metadata":{}},{"cell_type":"code","source":"train['target_cumsum'] = train['target'].cumsum()","metadata":{"execution":{"iopub.status.busy":"2021-11-10T15:41:50.790316Z","iopub.execute_input":"2021-11-10T15:41:50.791064Z","iopub.status.idle":"2021-11-10T15:41:50.800329Z","shell.execute_reply.started":"2021-11-10T15:41:50.790991Z","shell.execute_reply":"2021-11-10T15:41:50.79939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The differences between chunks are clearly visible","metadata":{}},{"cell_type":"code","source":"plt.plot(train['target_cumsum'])\nfor i in range(10):\n    plt.axvline(60000*i,color = 'lightgray',alpha = 0.3)\nplt.ylabel('Cumulative sum of target')\nplt.xlabel('id');","metadata":{"execution":{"iopub.status.busy":"2021-11-10T15:39:57.56199Z","iopub.execute_input":"2021-11-10T15:39:57.5623Z","iopub.status.idle":"2021-11-10T15:39:57.982978Z","shell.execute_reply.started":"2021-11-10T15:39:57.56227Z","shell.execute_reply":"2021-11-10T15:39:57.982135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So indeed, the training data has been chunked in groups of 60k","metadata":{}},{"cell_type":"markdown","source":"Mean target in the individual chunks:","metadata":{}},{"cell_type":"code","source":"# Reverse the 0 to -1 replacement in the target\ntarget_m1 = train['target'] == -1\ntrain.loc[target_m1, 'target'] = 0\n\nfor i in range(10):\n    chunk = train['target'][i*CS:(i+1)*CS]\n    print(f'Mean in chunk {i} is {np.mean(chunk):.2f}')","metadata":{"execution":{"iopub.status.busy":"2021-11-10T15:43:21.694285Z","iopub.execute_input":"2021-11-10T15:43:21.695101Z","iopub.status.idle":"2021-11-10T15:43:21.717094Z","shell.execute_reply.started":"2021-11-10T15:43:21.695059Z","shell.execute_reply":"2021-11-10T15:43:21.716087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Wow, large differences!","metadata":{}},{"cell_type":"markdown","source":"# Difference in the features","metadata":{}},{"cell_type":"markdown","source":"Allegedly there are feature differences between the chunks, namely f27. Let's take a look at mean and std within the chunks","metadata":{}},{"cell_type":"code","source":"m = np.zeros(10)\ns = np.zeros(10)\nfor i in range(10):\n    print(f'Chunk {i}')\n\n    m[i] = np.mean(train['f27'][i*CS:(i+1)*CS])\n    s[i] = np.std(train['f27'][i*CS:(i+1)*CS])\n    \n    print(f'   Mean {m[i]:.4f}')\n    print(f'   Std  {s[i]:.4f}')","metadata":{"execution":{"iopub.status.busy":"2021-11-10T15:43:30.370152Z","iopub.execute_input":"2021-11-10T15:43:30.370491Z","iopub.status.idle":"2021-11-10T15:43:30.39329Z","shell.execute_reply.started":"2021-11-10T15:43:30.370458Z","shell.execute_reply":"2021-11-10T15:43:30.39229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So the standard deviation in the last chunk is indeed much smaller.","metadata":{}},{"cell_type":"markdown","source":"The feature histogram in the first and last chunk are clearly different:","metadata":{}},{"cell_type":"code","source":"plt.hist(train['f27'][:CS], bins = np.arange(-0.2, 0.3, 0.01));\nplt.hist(train['f27'][9*CS:], bins = np.arange(-0.2, 0.3, 0.01), alpha = 0.2);","metadata":{"execution":{"iopub.status.busy":"2021-11-10T15:43:34.79194Z","iopub.execute_input":"2021-11-10T15:43:34.792637Z","iopub.status.idle":"2021-11-10T15:43:35.220013Z","shell.execute_reply.started":"2021-11-10T15:43:34.792592Z","shell.execute_reply":"2021-11-10T15:43:35.219298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}