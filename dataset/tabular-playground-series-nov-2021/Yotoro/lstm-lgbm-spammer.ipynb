{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## <span style=\"background:red;padding:0.3em;width:100%;display:block;border-radius:0.1em;color:white;font-family:Monospace\">Work In progress</span>\n\n* **This is the summary of Learning through compitition Google ventilator pressure prediction**\n\n* **LSTM and RNN are not good for tabular data, this are very different flavour Neural network, although they outperforms LGBM,XGBOOST and feed forward NN in time series data**\n\n* **LGBM model** [Google brain GCVPP](https://www.kaggle.com/shivansh002/lgbm-lover-s)\n\n* **LSTM model** [Google brain GCVPP](https://www.kaggle.com/shivansh002/i-am-groot-tpu-war)\n\n","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datatable as dt\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-08T15:06:49.338005Z","iopub.execute_input":"2021-11-08T15:06:49.338625Z","iopub.status.idle":"2021-11-08T15:06:54.325893Z","shell.execute_reply.started":"2021-11-08T15:06:49.33854Z","shell.execute_reply":"2021-11-08T15:06:54.324977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <span style=\"background:red;padding:0.3em;width:100%;display:block;border-radius:0.1em;color:white;font-family:Monospace\">Recurrent Neural Networks</span>\n\n* **In Our beloved feed forward neural network information only flows in single direction**\n* **traditional neural network is not good for sequential data**\n* **data like time series and text.**\n* **decision at a node(single neural cell) is made only on current input, no data of past and no future scope**\n\n****\n![fnn](https://th.bing.com/th/id/OIP.W_Ff5HIQ15akx48m1PAe7AAAAA?pid=ImgDet&rs=1)\n\n****\n\n* **A recurrent neural network is a traditional neural network except that a memory-state is added to the neurons.**\n* **the output from node in not only sent to next layer and also to itself**\n\n\n![df](https://cdn.guru99.com/images/tensorflow/082918_1006_RNNRecurren1.png)\n\n****\n\n* **this gif makes concept clear the input \"NYU NLP Rocks !\" its for single RNN cell**\n\n\n![rnn](https://i.ibb.co/LdL7CM5/rnn-move-rnn-cell.gif)\n","metadata":{}},{"cell_type":"markdown","source":"## <span style=\"background:Red;padding:0.3em;width:100%;display:block;border-radius:0.1em;color:white;font-family:Monospace\">Data preparation</span>","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\ndef create_row_features(df):\n    df['row_sum'] = df.sum(axis=1)\n    df['row_mean'] = df.mean(axis=1)\n    df['row_std'] = df.std(axis=1)\n    \n    return df\n\n\n\ntrain1 = pd.read_csv('../input/tabular-playground-series-nov-2021/train.csv')\ntest = pd.read_csv('../input/tabular-playground-series-nov-2021/test.csv')\nsubmission = pd.read_csv('../input/tabular-playground-series-nov-2021/sample_submission.csv')\n\ntrain=create_row_features(train1)\ntest=create_row_features(test)\n\nX = train.drop(columns=['id','target'])\ny = train['target']\n\ntest = test.drop('id',axis=1)\n\nscaler = StandardScaler()\n\nX=scaler.fit_transform(X)\nX_test=scaler.transform(test)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-08T15:07:13.028342Z","iopub.execute_input":"2021-11-08T15:07:13.028802Z","iopub.status.idle":"2021-11-08T15:07:42.378527Z","shell.execute_reply.started":"2021-11-08T15:07:13.028767Z","shell.execute_reply":"2021-11-08T15:07:42.377811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <span style=\"background:RED;padding:0.3em;width:100%;display:block;border-radius:0.1em;color:white;font-family:Monospace\">LSTM </span>\n\n* **RNN have their own problems like vanishing gradients and exploding gradient**\n* **here is monstrous looking LSTM architecture but i'll try to explain in more philosophical way ;)**\n\n\n![lstm](https://miro.medium.com/max/700/1*qToyitOZkf7Nhvr1LwxWgQ.png)\n\n****\n# part1\n* **Forget gate**\n* **identifies only the relevent information from previous state**\n* **previous state: \"shyam likes ice-cream but raju like bread\"**\n* **current state: \"raju will meet alia in evening\"**\n* **in LSTM cell the info about raju is relevent because its being used in current state while the info of shyam is irrelevent.** \n* **the input from previous state is compared with current state, so that may be past data is significant for future data**\n\n# part2\n* **input gate**\n* **next part decide what new information to store in the cell state how much of past information should be added to current state**\n\n# part3\n* **output gate**\n* **Finally, we need to decide what weâ€™re going to output. This output will be based on our cell state, but will be a filtered version**","metadata":{}},{"cell_type":"code","source":"#train=np.array(X)\ntrain = X.reshape(-1, 60, X.shape[-1])\nX_test=X_test.reshape(-1, 60, X_test.shape[-1])\ny=y.to_numpy().reshape(-1,60)\ntrain.shape,X_test.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-08T15:07:42.380145Z","iopub.execute_input":"2021-11-08T15:07:42.380392Z","iopub.status.idle":"2021-11-08T15:07:42.387375Z","shell.execute_reply.started":"2021-11-08T15:07:42.380358Z","shell.execute_reply":"2021-11-08T15:07:42.386715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**LSTM expects inputs to be 3 Dimensional**","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras import Sequential\nfrom tensorflow.keras import layers\n\n\n\ndef Lstm_model():\n    model = Sequential([\n                layers.Input(shape = train.shape[-2:]),\n                layers.Bidirectional(layers.LSTM(98, return_sequences=True)),\n                layers.Dense(156, activation='selu'),\n                layers.Bidirectional(layers.LSTM(46, return_sequences=True)),\n                layers.Dense(64, activation='selu'),\n                layers.Dense(1,activation='sigmoid')])\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2021-11-08T15:07:42.388892Z","iopub.execute_input":"2021-11-08T15:07:42.389389Z","iopub.status.idle":"2021-11-08T15:07:43.382001Z","shell.execute_reply.started":"2021-11-08T15:07:42.389351Z","shell.execute_reply":"2021-11-08T15:07:43.381177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom tensorflow.keras import optimizers\n\n\ncv = KFold(n_splits=5, shuffle=True, random_state=1)\ni =0\npredictions = []\n\nfor fold, (idx_train, idx_valid) in enumerate(cv.split(train, y)):\n    X_train, y_train = train[idx_train], y[idx_train]\n    X_valid, y_valid = train[idx_valid], y[idx_valid]\n    \n    print(X_train.shape,y_train.shape)\n    model = Lstm_model()\n    \n    model.compile(\n        optimizer=optimizers.Adam(learning_rate=1e-5),\n        loss='binary_crossentropy',\n        metrics=['AUC',\n                  tf.keras.metrics.BinaryAccuracy(),\n                  tf.keras.metrics.FalseNegatives()])\n\n    \n    print(f\"training for Fold {i+1}\")\n    \n    LSTM = model.fit(\n        X_train, y_train,\n        validation_data=(X_valid, y_valid),\n        batch_size=512,\n        epochs=70)\n                 \n                 \n    prediction = model.predict(X_test)\n    predictions.append(prediction)","metadata":{"execution":{"iopub.status.busy":"2021-11-08T15:07:43.384143Z","iopub.execute_input":"2021-11-08T15:07:43.384373Z","iopub.status.idle":"2021-11-08T15:15:04.139705Z","shell.execute_reply.started":"2021-11-08T15:07:43.384339Z","shell.execute_reply":"2021-11-08T15:15:04.138934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict=[]\nfor l in range(5):\n    flat_ls = [item for sublist in predictions[l] for item in sublist]\n    predict.append(flat_ls)   \n\nsub=np.zeros(540000)\nprint(sub.shape)\nfor pred in predict:\n    pred=np.array(pred).reshape(540000)\n    sub+=pred\n\n    \nsub=sub/5\n\nsubmission['target'] = sub\nsubmission.to_csv('lstm.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-08T15:15:04.141293Z","iopub.execute_input":"2021-11-08T15:15:04.141551Z","iopub.status.idle":"2021-11-08T15:15:07.717321Z","shell.execute_reply.started":"2021-11-08T15:15:04.141501Z","shell.execute_reply":"2021-11-08T15:15:07.716503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LGBM","metadata":{}},{"cell_type":"code","source":"from lightgbm import LGBMClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score","metadata":{"execution":{"iopub.status.busy":"2021-11-08T15:23:19.988415Z","iopub.execute_input":"2021-11-08T15:23:19.988709Z","iopub.status.idle":"2021-11-08T15:23:21.366525Z","shell.execute_reply.started":"2021-11-08T15:23:19.988673Z","shell.execute_reply":"2021-11-08T15:23:21.365724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = test.drop('id',axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-11-08T15:38:55.038591Z","iopub.execute_input":"2021-11-08T15:38:55.03931Z","iopub.status.idle":"2021-11-08T15:38:55.169552Z","shell.execute_reply.started":"2021-11-08T15:38:55.039272Z","shell.execute_reply":"2021-11-08T15:38:55.168634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = train1.drop(columns=['id','target'])\ny = train1['target']\n\nskf=StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n\npred=[]\n\n\nfor fold,(idx1,idx2) in enumerate(skf.split(X,y)):\n    \n    Xtrain,ytrain=X.iloc[idx1],y.iloc[idx1]\n    Xvalid,yvalid=X.iloc[idx2],y.iloc[idx2]\n    \n    model=LGBMClassifier(n_estimators=1500,learning_rate=0.02,device='gpu',num_leaves=900,\n                         min_child_samples=60)\n    \n    model.fit(Xtrain,ytrain,eval_set=[(Xvalid,yvalid)],early_stopping_rounds=100,verbose=50)\n    \n    valid_pred = model.predict_proba(Xvalid)[:,1]\n    test_pred = model.predict_proba(test)[:,1]\n    \n    pred.append(test_pred)\n    \n    score = roc_auc_score(yvalid, valid_pred)\n    print(score)\n    ","metadata":{"execution":{"iopub.status.busy":"2021-11-08T15:38:59.004627Z","iopub.execute_input":"2021-11-08T15:38:59.005105Z","iopub.status.idle":"2021-11-08T16:15:58.266331Z","shell.execute_reply.started":"2021-11-08T15:38:59.005069Z","shell.execute_reply":"2021-11-08T16:15:58.265524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}