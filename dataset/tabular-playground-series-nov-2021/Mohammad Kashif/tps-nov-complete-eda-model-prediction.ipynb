{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport warnings\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib import ticker\nimport seaborn as sns\nimport gc\n\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\npd.set_option('float_format', '{:f}'.format)\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import missingno as msno\n%matplotlib inline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/tabular-playground-series-nov-2021/train.csv').drop('id', axis=1)\ntest  = pd.read_csv('../input/tabular-playground-series-nov-2021/test.csv').drop('id', axis=1)\nss    = pd.read_csv('../input/tabular-playground-series-nov-2021/sample_submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape, test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets check if there is any missing values or not.","metadata":{}},{"cell_type":"code","source":"msno.matrix(train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* We can clearly see that there is no missing values in the dataset.","metadata":{}},{"cell_type":"code","source":"train.loc[:, 'f0':'f99'].describe().T.style.bar(subset=['mean'], color='#206ff2')\\\n                            .background_gradient(subset=['std'], cmap='Reds')\\\n                            .background_gradient(subset=['50%'], cmap='coolwarm')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check the distribution of **target** in training dataset.","metadata":{}},{"cell_type":"code","source":"sns.countplot(train['target'], palette='Set3')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This plot shows that there is almost equal distribution of **Target** variable.","metadata":{}},{"cell_type":"markdown","source":"# Density plots of features\n\nHere we represent distribution of **train** and **test** in different color.","metadata":{}},{"cell_type":"code","source":"features = train.columns.values[0:100]\ni = 0\nsns.set_style('whitegrid')\nplt.figure()\nfig, ax = plt.subplots(10,10,figsize=(18,22))\n\nfor feature in features:\n    i += 1\n    plt.subplot(10,10,i)\n    sns.distplot(train[feature], hist=False,label='train')\n    sns.distplot(test[feature], hist=False,label='test')\n    plt.xlabel(feature, fontsize=9)\n    locs, labels = plt.xticks()\n    plt.tick_params(axis='x', which='major', labelsize=6, pad=-6)\n    plt.tick_params(axis='y', which='major', labelsize=6)\nplt.show();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Train and test seems to have almost same distribution.","metadata":{}},{"cell_type":"markdown","source":"# Distribution of mean and std\n\nLet's check the distribution of the mean values per row in the train and test set.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(16,6))\nfeatures = train.columns.values[0:100]\nplt.title(\"Distribution of mean values per row in the train and test set\")\nsns.distplot(train[features].mean(axis=1),color=\"green\", kde=True,bins=120, label='train')\nsns.distplot(test[features].mean(axis=1),color=\"blue\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Distribution of the **mean** values per **columns** in the train and test set.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of mean values per column in the train and test set\")\nsns.distplot(train[features].mean(axis=0),color=\"magenta\",kde=True,bins=120, label='train')\nsns.distplot(test[features].mean(axis=0),color=\"darkblue\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Distribution of **standard deviation** of values per **row** for train and test datasets.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of std values per row in the train and test set\")\nsns.distplot(train[features].std(axis=1),color=\"black\", kde=True,bins=120, label='train')\nsns.distplot(test[features].std(axis=1),color=\"red\", kde=True,bins=120, label='test')\nplt.legend();plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Distribution of the **standard deviation** of values per **columns** in the train and test datasets.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of std values per column in the train and test set\")\nsns.distplot(train[features].std(axis=0),color=\"blue\",kde=True,bins=120, label='train')\nsns.distplot(test[features].std(axis=0),color=\"green\", kde=True,bins=120, label='test')\nplt.legend(); plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Distribution of the **mean** value per **row** in the train dataset, grouped by value of **target**.","metadata":{}},{"cell_type":"code","source":"t0 = train.loc[train['target'] == 0]\nt1 = train.loc[train['target'] == 1]\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of mean values per row in the train set\")\nsns.distplot(t0[features].mean(axis=1),color=\"red\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].mean(axis=1),color=\"blue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Distribution of the **mean** value per **column** in the train dataset, grouped by value of **target**.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of mean values per column in the train set\")\nsns.distplot(t0[features].mean(axis=0),color=\"green\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].mean(axis=0),color=\"darkblue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Distribution of min and max\n\nLet's check the distribution of min per row in the train and test set.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(16,6))\nfeatures = train.columns.values[0:100]\nplt.title(\"Distribution of min values per row in the train and test set\")\nsns.distplot(train[features].min(axis=1),color=\"red\", kde=True,bins=120, label='train')\nsns.distplot(test[features].min(axis=1),color=\"orange\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Distribution of **min** per **column** in the train and test set.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(16,6))\nfeatures = train.columns.values[0:100]\nplt.title(\"Distribution of min values per column in the train and test set\")\nsns.distplot(train[features].min(axis=0),color=\"magenta\", kde=True,bins=120, label='train')\nsns.distplot(test[features].min(axis=0),color=\"darkblue\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Distribution of **max** values per **rows** for train and test set.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(16,6))\nfeatures = train.columns.values[0:100]\nplt.title(\"Distribution of max values per row in the train and test set\")\nsns.distplot(train[features].max(axis=1),color=\"brown\", kde=True,bins=120, label='train')\nsns.distplot(test[features].max(axis=1),color=\"yellow\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now showing the max distribution on columns for train and test set.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(16,6))\nfeatures = train.columns.values[0:100]\nplt.title(\"Distribution of max values per column in the train and test set\")\nsns.distplot(train[features].max(axis=0),color=\"blue\", kde=True,bins=120, label='train')\nsns.distplot(test[features].max(axis=0),color=\"red\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Distributions of **min** values per **row** in train set, grouped by value of **target**.","metadata":{}},{"cell_type":"code","source":"t0 = train.loc[train['target'] == 0]\nt1 = train.loc[train['target'] == 1]\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of min values per row in the train set\")\nsns.distplot(t0[features].min(axis=1),color=\"orange\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].min(axis=1),color=\"darkblue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Distribution of **min** values per **columns** in train set, grouped by **target**.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of min values per column in the train set\")\nsns.distplot(t0[features].min(axis=0),color=\"red\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].min(axis=0),color=\"blue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Distribution of **max** values per **row** in the train set, grouped by **target**.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of max values per row in the train set\")\nsns.distplot(t0[features].max(axis=1),color=\"gold\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].max(axis=1),color=\"darkblue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Distribution of **max** values per **columns** in the train set, grouped by **target**.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of max values per column in the train set\")\nsns.distplot(t0[features].max(axis=0),color=\"red\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].max(axis=0),color=\"blue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Distribution of skew and kurtosis\n\nDistribution of **skewness** calculated per **rows** in train and test sets.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of skew per row in the train and test set\")\nsns.distplot(train[features].skew(axis=1),color=\"red\", kde=True,bins=120, label='train')\nsns.distplot(test[features].skew(axis=1),color=\"orange\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Distribution of **skewness** calculated per **columns** in train and test sets.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of skew per column in the train and test set\")\nsns.distplot(train[features].skew(axis=0),color=\"magenta\", kde=True,bins=120, label='train')\nsns.distplot(test[features].skew(axis=0),color=\"darkblue\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Distribution of **kurtosis** calculated per **rows** in train and test sets.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of kurtosis per row in the train and test set\")\nsns.distplot(train[features].kurtosis(axis=1),color=\"darkblue\", kde=True,bins=120, label='train')\nsns.distplot(test[features].kurtosis(axis=1),color=\"yellow\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Distribution of **kurtosis** calculated per **columns** in train and test sets.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of kurtosis per column in the train and test set\")\nsns.distplot(train[features].kurtosis(axis=0),color=\"magenta\", kde=True,bins=120, label='train')\nsns.distplot(test[features].kurtosis(axis=0),color=\"green\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Distribution of **skewness** values per **row** in the train set, grouped by **target**.","metadata":{}},{"cell_type":"code","source":"t0 = train.loc[train['target'] == 0]\nt1 = train.loc[train['target'] == 1]\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of skew values per row in the train set\")\nsns.distplot(t0[features].skew(axis=1),color=\"red\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].skew(axis=1),color=\"blue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Distribution of **skewness** values per **column** in the train set, grouped by **target**.","metadata":{}},{"cell_type":"code","source":"t0 = train.loc[train['target'] == 0]\nt1 = train.loc[train['target'] == 1]\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of skew values per column in the train set\")\nsns.distplot(t0[features].skew(axis=0),color=\"red\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].skew(axis=0),color=\"blue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Distribution of **kurtosis** values per **row** in the train set, grouped by **target**.","metadata":{}},{"cell_type":"code","source":"t0 = train.loc[train['target'] == 0]\nt1 = train.loc[train['target'] == 1]\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of kurtosis values per row in the train set\")\nsns.distplot(t0[features].kurtosis(axis=1),color=\"red\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].kurtosis(axis=1),color=\"blue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Distribution of **kurtosis** values per **column** in the train set, grouped by **target**.","metadata":{}},{"cell_type":"code","source":"t0 = train.loc[train['target'] == 0]\nt1 = train.loc[train['target'] == 1]\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of kurtosis values per column in the train set\")\nsns.distplot(t0[features].kurtosis(axis=0),color=\"red\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].kurtosis(axis=0),color=\"blue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Correlation\n\nBelow is a heatmap plot of first 25 features(f0-f24) and target variable.","metadata":{}},{"cell_type":"code","source":"columns = train.columns[0:25].to_list()\ncolumns.append('target')\n\ncorr = train[columns].corr()\nf, ax = plt.subplots(figsize=(20,10))\nmask = np.triu(np.ones_like(corr, dtype=bool))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nsns.heatmap(corr, annot=True, mask = mask, cmap=cmap)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**f25-f49** and **target** variable.","metadata":{}},{"cell_type":"code","source":"columns = train.columns[25:50].to_list()\ncolumns.append('target')\n\ncorr = train[columns].corr()\nf, ax = plt.subplots(figsize=(20,10))\nmask = np.triu(np.ones_like(corr, dtype=bool))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nsns.heatmap(corr, annot=True, mask = mask, cmap=cmap)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**f50-f74** and **target** variable.","metadata":{}},{"cell_type":"code","source":"columns = train.columns[50:75].to_list()\ncolumns.append('target')\n\ncorr = train[columns].corr()\nf, ax = plt.subplots(figsize=(20,10))\nmask = np.triu(np.ones_like(corr, dtype=bool))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nsns.heatmap(corr, annot=True, mask = mask, cmap=cmap)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**f75-f99** and **target** variable.","metadata":{}},{"cell_type":"code","source":"columns = train.columns[75:100].to_list()\ncolumns.append('target')\n\ncorr = train[columns].corr()\nf, ax = plt.subplots(figsize=(20,10))\nmask = np.triu(np.ones_like(corr, dtype=bool))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nsns.heatmap(corr, annot=True, mask = mask, cmap=cmap)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems like there is common correlation coefficient(**0.11**) of some features with target variable.","metadata":{}},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{}},{"cell_type":"code","source":"X = train.drop('target', axis=1).copy()\ny = train['target']\nX_test = test.copy()\n\ndel train\ngc.collect\ndel test\ngc.collect","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def feature_eng(df):\n    df['sum'] = df.sum(axis=1)  \n    df['min'] = df.min(axis=1)\n    df['max'] = df.max(axis=1)\n    df['mean'] = df.mean(axis=1)\n    df['std'] = df.std(axis=1)\n    df['skew'] = df.skew(axis=1)\n    df['kurt'] = df.kurtosis(axis=1)\n    return df\n\nX = feature_eng(X)\nX_test = feature_eng(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(X.head())\ndisplay(X_test.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check the distribution of these new engineered features.","metadata":{}},{"cell_type":"code","source":"def plot_new_feature_distribution(df1, df2, label1, label2, features):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(2,4,figsize=(18,8))\n\n    for feature in features:\n        i += 1\n        plt.subplot(2,4,i)\n        sns.kdeplot(df1[feature], bw=0.5,label=label1)\n        sns.kdeplot(df2[feature], bw=0.5,label=label2)\n        plt.xlabel(feature, fontsize=11)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=8)\n        plt.tick_params(axis='y', which='major', labelsize=8)\n    plt.show();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = X.columns.values[100:107]\nplot_new_feature_distribution(X, X_test, 'train', 'test', features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_curve, auc","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = { \n          'objective': 'binary:logistic', \n          'gpu_id': 0, \n          'n_estimators': 10000, \n          'learning_rate': 0.01, \n          'gamma': 0.25, \n          'max_depth': 4, \n          'min_child_weight': 366, \n          'subsample': 0.64, \n          'colsample_bytree': 0.78, \n          'colsample_bylevel': 0.86, \n          'reg_lambda': 0, \n          'reg_alpha': 10\n          }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\npreds = []\nscores = []\nfeature_importance_df = pd.DataFrame()\n\nfor fold, (idx_train, idx_valid) in enumerate(kf.split(X, y)):\n    X_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n    X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid]\n    \n    model = XGBClassifier(**params,\n                            booster= 'gbtree',\n                            eval_metric = 'auc',\n                            tree_method= 'gpu_hist',\n                            predictor=\"gpu_predictor\",\n                            use_label_encoder=False)\n    \n    model.fit(X_train,y_train,\n              eval_set=[(X_valid,y_valid)],\n              early_stopping_rounds=100,\n              verbose=False)\n    \n    pred_valid = model.predict_proba(X_valid)[:,1]\n    fpr, tpr, _ = roc_curve(y_valid, pred_valid)\n    score = auc(fpr, tpr)\n    scores.append(score)\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"Feature\"] = X.columns\n    fold_importance_df[\"importance\"] = model.feature_importances_\n    fold_importance_df[\"fold\"] = fold + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    print(f\"Fold: {fold + 1} Score: {score}\")\n    print('||'*40)\n    \n    test_preds = model.predict_proba(X_test)[:,1]\n    preds.append(test_preds)\n    \nprint(f\"Overall Validation Score: {np.mean(scores)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n        .groupby(\"Feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:107].index)\nbest_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n\nplt.figure(figsize=(14,28))\nsns.barplot(x=\"importance\", y=\"Feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\nplt.title('Features importance (averaged/folds)')\nplt.tight_layout()\nplt.savefig('FI.png')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"predictions = np.mean(np.column_stack(preds),axis=1)\n\nss['target'] = predictions\nss.to_csv('./xgb.csv', index=False)\nss.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reference\n* https://www.kaggle.com/gpreda/santander-eda-and-prediction\n* https://www.kaggle.com/subinium/tps-oct-simple-eda","metadata":{}}]}