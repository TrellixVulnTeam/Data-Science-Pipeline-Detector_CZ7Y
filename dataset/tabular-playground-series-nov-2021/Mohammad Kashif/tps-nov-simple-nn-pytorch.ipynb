{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport gc\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.utils.data as data\nfrom sklearn.preprocessing import StandardScaler","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/tabular-playground-series-nov-2021/train.csv').drop('id', axis=1)\ntest = pd.read_csv('../input/tabular-playground-series-nov-2021/test.csv').drop('id', axis=1)\nss = pd.read_csv('../input/tabular-playground-series-nov-2021/sample_submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = train.drop(\"target\", axis=1).copy()\ny = train[\"target\"].copy()\nX_test = test.copy()\n\ndel train\ngc.collect()\ndel test\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = StandardScaler()\n\nX = pd.DataFrame(columns=X.columns, data=scaler.fit_transform(X))\nX_test = pd.DataFrame(columns=X_test.columns, data=scaler.transform(X_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomDataset:\n    \n    def __init__(self, data, target=None):\n        self.data = data\n        self.target = target\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        \n        if self.target is not None:\n            current_sample = self.data.values[idx]\n            current_target = self.target.values[idx]\n            \n            return torch.tensor(current_sample, dtype= torch.float), torch.tensor(current_target, dtype= torch.float)\n        else:\n            current_sample = self.data.values[idx]\n            return torch.tensor(current_sample, dtype= torch.float)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class NNModel(nn.Module):\n    \n    def __init__(self, features, activation = F.relu):\n        super(NNModel, self).__init__()\n        \n        \"\"\"Number of input is no of features(100)\"\"\"\n        self.layer_1 = nn.Linear(features, 264)\n        self.batchnorm1 = nn.BatchNorm1d(264)\n        self.layer_2 = nn.Linear(264, 128)\n        self.batchnorm2 = nn.BatchNorm1d(128)\n        self.layer_3 = nn.Linear(128, 64)\n        self.batchnorm3 = nn.BatchNorm1d(64)\n        self.layer_4 = nn.Linear(64,32)\n        self.batchnorm4 = nn.BatchNorm1d(32)\n        self.layer_out = nn.Linear(32,1)\n        self.flatten = nn.Flatten()\n        self.activation = activation\n        \n    def forward(self, x):\n        x = self.flatten(x)\n        x = self.batchnorm1(self.activation(self.layer_1(x)))\n        x = self.batchnorm2(self.activation(self.layer_2(x)))\n        x = self.batchnorm3(self.activation(self.layer_3(x)))\n        x = self.batchnorm4(self.activation(self.layer_4(x)))\n        x = torch.sigmoid(self.layer_out(x))\n        \n        return torch.squeeze(x, dim=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def initialize_weights(self):\n    for m in self.modules():\n        if isinstance(m, nn.Linear):\n            nn.init.kaiming_uniform_(m.weight.data, nonlinearity=\"relu\")\n            \n            if m.bias is not None:\n                nn.init.constant_(m.bias.data, 0)\n        \n        elif isinstance(m, nn.BatchNorm1d):\n            nn.init.constant_(m.weight.data, 1)\n            nn.init.constant_(m.bias.data, 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else 'cpu'\nBATCH_SIZE = 1024\nfeatures = X.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(dataloader, model, criterion, optimizer):\n    size = len(dataloader.dataset)\n    model.train()\n    batches = len(dataloader)\n    train_loss = 0\n    \n    for batch_idx, (X,y) in enumerate(dataloader):\n        X, y = X.to(device), y.to(device)\n        \n        scores = model(X)\n        loss = criterion(scores, y)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        loss = loss.item()\n        train_loss += loss\n        \n    train_loss_avg = train_loss/batches\n    print(f\"avg. train loss: {train_loss_avg}\")\n    return train_loss_avg","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def val_model(dataloader, model, criterion):\n    \n    size= len(dataloader.dataset)\n    batches= len(dataloader)\n    model.eval()\n    test_loss= 0\n\n    with torch.no_grad():\n        for X, y in (dataloader):\n            X, y= X.to(device), y.to(device)\n      \n            scores= model(X)\n            test_loss += criterion(scores, y)\n\n    test_loss /= batches\n    print(f\"avg test loss : {test_loss}\")\n    return test_loss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_model(dataloader, model):\n    model.eval()\n    y_pred= np.array([])\n    \n    with torch.no_grad():\n        for X in dataloader:\n            X = X.to(device)\n            \n            preds= model(X)\n            preds= preds.flatten().cpu().numpy()\n            \n            y_pred= np.concatenate((y_pred, preds))\n            \n    return y_pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\n\nKFold = StratifiedKFold(n_splits=5, random_state=2021, shuffle=True)\nEPOCHS = 100\ncv_scores = []\npredictions = np.zeros(X_test.shape[0])\n\nfor fold, (train_idx, val_idx) in enumerate(KFold.split(X, y)):\n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n    \n    train_dataset = CustomDataset(data=X_train, target=y_train)\n    val_dataset = CustomDataset(data=X_val, target=y_val)\n    \n    train_loader = data.DataLoader(train_dataset, batch_size = BATCH_SIZE)\n    val_loader = data.DataLoader(val_dataset, batch_size = BATCH_SIZE)\n    \n    model = NNModel(features=len(features), activation=F.hardswish).to(device)\n    model.apply(initialize_weights)\n    \n    criterion = nn.BCELoss()\n    criterion.to(device)\n    \n    optimizer= optim.Adam(model.parameters(), lr= 0.001)\n    scheduler= optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n                                                    factor= 0.5,\n                                                    patience= 10,\n                                                    verbose= True)\n    best_valid_loss = float('inf')\n    \n    avg_train_losses = []\n    avg_val_losses = []\n    \n    print(10*\"::\", f\"Fold={fold+1}\", 10*\"::\")\n    \n    for t in range(EPOCHS):\n        print(f\"Epoch: {t+1}\")\n        train_loss = train_model(train_loader, model, criterion, optimizer)\n        val_loss = val_model(val_loader, model, criterion)\n        \n        avg_train_losses.append(train_loss)\n        avg_val_losses.append(val_loss)\n        \n        if (val_loss < best_valid_loss):\n            best_valid_loss= val_loss\n            ofilename = 'TPS%d.pth' % fold\n            torch.save(model.state_dict(),  ofilename)\n        \n        scheduler.step(val_loss)\n        \n    cv_scores.append(best_valid_loss)\n    \n    test_dataset = CustomDataset(data = X_test, target = None)\n    test_loader = data.DataLoader(test_dataset, batch_size = BATCH_SIZE)\n                       \n    model.load_state_dict(torch.load('TPS%d.pth' % fold, map_location=device))\n    predictions += (predict_model(test_loader, model)/5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ss['target'] = predictions\nss.to_csv('submission.csv',index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ss.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reference\n\n* https://www.kaggle.com/sagarikajadon/simple-lstm-pytorch","metadata":{}}]}