{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Let's talk about Layers and activations and losses","metadata":{}},{"cell_type":"markdown","source":"### There are different types of Layers in a Neural Network\n\nThe most important and widely used are:\n\n* Linear layers :\n* Convolution layers :\n* Pooling Layers :\n* Dropout layers :\n\nWe will be using Linear layers and Dropout layers to build a simple shallow neural network\n\n### The different Activation functions\n\nThe activation functions we use depends on the task we perform:\n\n__Non-linear activation functions__:\n* nn.ReLU :\n* nn.Sigmoid :\n* nn.Tanh :\n\n__Linear activation fucntions are__:\n* nn.Sotfmax\n* nn.Softmin\n* nn.LogSoftmax\n\n### The different loss functions are:\n\nThe most widely used loss functions are:\n* nn.MSELoss : Mean Squared Error Loss function\n* nn.CrossEntropyLoss : This criterion computes the cross entropy loss between input and target.\n* nn.BCELoss : Creates a criterion that measures the Binary Cross Entropy between the target and the input probabilities\n* nn.BCEWithLogitsLoss : This loss combines a Sigmoid layer and the BCELoss in one single class.\n\nfor more information you can check this [link](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity).\n","metadata":{}},{"cell_type":"markdown","source":"# Lets start building a simple NN","metadata":{}},{"cell_type":"markdown","source":"We Will be building a simple NN from scratch so that later we can experiment it likewise :)\n\nLet's get started","metadata":{}},{"cell_type":"markdown","source":"# Please DO UPVOTE if you like :)","metadata":{}},{"cell_type":"markdown","source":"# Importing dependencies","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport datatable as dt\nfrom sklearn.model_selection import train_test_split\nfrom torch.optim import AdamW, lr_scheduler\nfrom tqdm.notebook import tqdm_notebook\nfrom tqdm import tqdm\nimport math\nfrom sklearn.preprocessing import StandardScaler\nfrom torch.utils.data import DataLoader\nimport time\nimport gc\nfrom sklearn.metrics import roc_auc_score\nimport pickle\nimport os\nimport math","metadata":{"execution":{"iopub.status.busy":"2021-11-13T13:32:37.804928Z","iopub.execute_input":"2021-11-13T13:32:37.805444Z","iopub.status.idle":"2021-11-13T13:32:37.812902Z","shell.execute_reply.started":"2021-11-13T13:32:37.805368Z","shell.execute_reply":"2021-11-13T13:32:37.811543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The sigmoid function for the outputs","metadata":{}},{"cell_type":"code","source":"def sigmoid(x):\n    return 1 / (1 + math.exp(-x))","metadata":{"execution":{"iopub.status.busy":"2021-11-13T13:32:37.815936Z","iopub.execute_input":"2021-11-13T13:32:37.816507Z","iopub.status.idle":"2021-11-13T13:32:37.826537Z","shell.execute_reply.started":"2021-11-13T13:32:37.816462Z","shell.execute_reply":"2021-11-13T13:32:37.825189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loss Function : Binary Cross Entropy Loss","metadata":{}},{"cell_type":"code","source":"def loss_fn(outputs, targets):\n    loss = nn.BCEWithLogitsLoss()(outputs, targets.view(-1,1))\n    return loss","metadata":{"execution":{"iopub.status.busy":"2021-11-13T13:32:37.828643Z","iopub.execute_input":"2021-11-13T13:32:37.829288Z","iopub.status.idle":"2021-11-13T13:32:37.83933Z","shell.execute_reply.started":"2021-11-13T13:32:37.829225Z","shell.execute_reply":"2021-11-13T13:32:37.837785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# AUC metric to measure model performance","metadata":{}},{"cell_type":"code","source":"def metrics(targets, outputs):\n    auc = roc_auc_score(targets, outputs)\n    return auc","metadata":{"execution":{"iopub.status.busy":"2021-11-13T13:32:37.840594Z","iopub.execute_input":"2021-11-13T13:32:37.841626Z","iopub.status.idle":"2021-11-13T13:32:37.848959Z","shell.execute_reply.started":"2021-11-13T13:32:37.841592Z","shell.execute_reply":"2021-11-13T13:32:37.847516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Config","metadata":{}},{"cell_type":"code","source":"class Config:\n    epochs = 10\n    scheduler = 'CosineAnnealingLR'\n    batch_size = 10240\n    early_stopping_epochs = 2\n    lr = 1e-5\n    weight_decay = 0.01","metadata":{"execution":{"iopub.status.busy":"2021-11-13T13:32:37.852655Z","iopub.execute_input":"2021-11-13T13:32:37.853569Z","iopub.status.idle":"2021-11-13T13:32:37.861145Z","shell.execute_reply.started":"2021-11-13T13:32:37.853518Z","shell.execute_reply":"2021-11-13T13:32:37.859899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train and Validation Dataset","metadata":{}},{"cell_type":"code","source":"class Dataset:\n    def __init__(self, X, y):\n        self.X = X\n        self.y = y.values\n        \n    def __len__(self):\n        return len(self.X)\n    \n    def __getitem__(self, idx):\n        return {\n            'X' : torch.tensor(self.X[idx], dtype=torch.float),\n            'targets' : torch.tensor(self.y[idx], dtype=torch.float)\n        }","metadata":{"execution":{"iopub.status.busy":"2021-11-13T13:32:37.863203Z","iopub.execute_input":"2021-11-13T13:32:37.864257Z","iopub.status.idle":"2021-11-13T13:32:37.875464Z","shell.execute_reply.started":"2021-11-13T13:32:37.864212Z","shell.execute_reply":"2021-11-13T13:32:37.874488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction Dataset","metadata":{}},{"cell_type":"markdown","source":"To learn more about Datasets and DataLoaders see this notebook : []()","metadata":{}},{"cell_type":"code","source":"class PredDataset:\n    def __init__(self, X):\n        self.X = X\n        \n    def __len__(self):\n        return len(self.X)\n    \n    def __getitem__(self, idx):\n        return {\n            'X' : torch.tensor(self.X[idx], dtype=torch.float)\n        }","metadata":{"execution":{"iopub.status.busy":"2021-11-13T13:32:37.877258Z","iopub.execute_input":"2021-11-13T13:32:37.877965Z","iopub.status.idle":"2021-11-13T13:32:37.885969Z","shell.execute_reply.started":"2021-11-13T13:32:37.877916Z","shell.execute_reply":"2021-11-13T13:32:37.88497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create your Model","metadata":{}},{"cell_type":"markdown","source":"### The model creation is simple...\nJust follow these 3 steps:\n1. Create the **Model** class and inherit from **nn.Module**\n2. Create the __init__ : define the layers you want to use\n3. Create the **forward** function for the forward propagation of the NN and return the output\n\n### Another interesting thing you can see:\n\nI have used **nn.LazyLinear** instead of **nn.Linear** as the input layer.\n\nThe reason: I am too lazy :)\n\n### So what does a LazyLinear layer do?\n\nEasy. Yeah, it makes our life so easy. **No need** to wander around calculating the number of input features for the model. O.o\n\nLazyLinear layer just takes in input only the number of **out_features**. :)\n\nDone. Now if you want to change the number of input features, excluding some columns... No need to worry :D","metadata":{}},{"cell_type":"code","source":"class TPSModel(nn.Module):\n    def __init__(self, args):\n        super(TPSModel, self).__init__()\n        self.args = args\n        self.linear = nn.Linear(128, 128)\n        self.lazylinear = nn.LazyLinear(128)\n        self.silu = nn.SiLU()\n        self.dropout = nn.Dropout(0.5)\n        self.output = nn.Linear(128, 1)\n    \n    # The forward Function\n    def forward(self, x):\n        x = self.lazylinear(x)\n        x = self.silu(x)\n        x = self.linear(x)\n        x = self.dropout(x)\n        x = self.output(x)\n        \n        return x","metadata":{"execution":{"iopub.status.busy":"2021-11-13T13:32:37.88891Z","iopub.execute_input":"2021-11-13T13:32:37.889593Z","iopub.status.idle":"2021-11-13T13:32:37.900107Z","shell.execute_reply.started":"2021-11-13T13:32:37.889544Z","shell.execute_reply":"2021-11-13T13:32:37.898788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating the training Epoch\n\nIn this step we give the model some inputs and get some outputs and return the loss for that epoch :)\n\nNothing else.\n\n\n\n\nLater we use this loss and backpropagate it into the NN and accordingly step the optimizer and the scheduler :)\n\nEasy right?","metadata":{}},{"cell_type":"code","source":"def train_epoch(args, dataloader, model, optimizer, scheduler, epoch):\n    \n    model.train()\n    \n    epoch_loss = 0.0\n    running_loss = 0.0\n    dataset_size=0\n    running_auc=0\n    batch_size = args.batch_size\n    \n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    for step, data in bar:\n        optimizer.zero_grad()\n        \n        X = data['X'].cuda()\n        targets = data['targets'].cuda()\n        outputs = model(X)\n        \n        loss = loss_fn(outputs.view(-1,1), targets)\n        loss.backward()\n        \n        optimizer.step()\n        if scheduler is not None:\n            scheduler.step()\n            \n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n        epoch_loss = running_loss / dataset_size\n        auc = metrics(targets.cpu().detach().numpy(), outputs.cpu().detach().numpy())\n        running_auc += auc * batch_size\n        epoch_auc = running_auc / dataset_size\n        bar.set_postfix(Epoch=epoch, Stage='Training', Train_Loss=epoch_loss,\n                        AUC=epoch_auc)\n    gc.collect()\n    return epoch_loss","metadata":{"execution":{"iopub.status.busy":"2021-11-13T13:32:37.90161Z","iopub.execute_input":"2021-11-13T13:32:37.902734Z","iopub.status.idle":"2021-11-13T13:32:37.915682Z","shell.execute_reply.started":"2021-11-13T13:32:37.902676Z","shell.execute_reply":"2021-11-13T13:32:37.914495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Validation Epoch\n\nWe just calculate the loss and return it :)","metadata":{}},{"cell_type":"code","source":"def validation(args, dataloader, model, epoch):\n    \n    model.eval()\n    \n    epoch_loss = 0.0\n    running_loss = 0.0\n    dataset_size=0\n    batch_size = args.batch_size\n    running_auc = 0\n    counter=0\n    with torch.no_grad():\n        bar = tqdm(enumerate(dataloader), total=len(dataloader))\n        for step, data in bar:\n\n            X = data['X'].cuda()\n            targets = data['targets'].cuda()\n            outputs = model(X)\n\n            loss = loss_fn(outputs.view(-1,1), targets)\n\n            running_loss += (loss.item() * batch_size)\n            dataset_size += batch_size\n            epoch_loss = running_loss / dataset_size\n            auc = metrics(targets.cpu().detach().numpy(), outputs.cpu().detach().numpy())\n            counter+=1\n            \n            running_auc += auc * batch_size\n            epoch_auc = running_auc / dataset_size\n            \n            bar.set_postfix(Epoch=epoch, AUC=epoch_auc, Train_Loss=epoch_loss, Stage='Validation')\n    gc.collect()\n    return epoch_loss, epoch_auc","metadata":{"execution":{"iopub.status.busy":"2021-11-13T13:32:37.917613Z","iopub.execute_input":"2021-11-13T13:32:37.918711Z","iopub.status.idle":"2021-11-13T13:32:37.932442Z","shell.execute_reply.started":"2021-11-13T13:32:37.918626Z","shell.execute_reply":"2021-11-13T13:32:37.931461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction Loop :)\nPredict the Output and returns it :D","metadata":{}},{"cell_type":"code","source":"def predict(args, dataloader, model):\n    print('-'*20,'Predicting for Submission','-'*20)\n    model.eval()\n    all_outputs=[]\n\n    with torch.no_grad():\n        bar = tqdm(enumerate(dataloader), total=len(dataloader))\n        for step, data in bar:\n\n            X = data['X'].cuda()\n            outputs = model(X)\n            outputs = outputs.cpu().detach().numpy()\n            all_outputs.append(outputs)\n            bar.set_postfix(Stage='Prediction')\n    gc.collect()\n    return np.vstack(all_outputs)","metadata":{"execution":{"iopub.status.busy":"2021-11-13T13:32:38.0535Z","iopub.execute_input":"2021-11-13T13:32:38.054031Z","iopub.status.idle":"2021-11-13T13:32:38.063865Z","shell.execute_reply.started":"2021-11-13T13:32:38.053985Z","shell.execute_reply":"2021-11-13T13:32:38.062479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Optimizer","metadata":{}},{"cell_type":"code","source":"def get_optimizer(args, params):\n    opt = AdamW(params, lr=args.lr, weight_decay=args.weight_decay)\n    return opt","metadata":{"execution":{"iopub.status.busy":"2021-11-13T13:32:38.066835Z","iopub.execute_input":"2021-11-13T13:32:38.068099Z","iopub.status.idle":"2021-11-13T13:32:38.074962Z","shell.execute_reply.started":"2021-11-13T13:32:38.068049Z","shell.execute_reply":"2021-11-13T13:32:38.073768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Scheduler","metadata":{}},{"cell_type":"code","source":"def get_scheduler(args, optimizer):\n    if args.scheduler == 'CosineAnnealingLR':\n        scheduler = lr_scheduler.CosineAnnealingLR(optimizer,T_max=500, \n                                                   eta_min=1e-6)\n    else:\n        schduler = None\n    return scheduler","metadata":{"execution":{"iopub.status.busy":"2021-11-13T13:32:38.076687Z","iopub.execute_input":"2021-11-13T13:32:38.077479Z","iopub.status.idle":"2021-11-13T13:32:38.085646Z","shell.execute_reply.started":"2021-11-13T13:32:38.077432Z","shell.execute_reply":"2021-11-13T13:32:38.084338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reading the DF","metadata":{}},{"cell_type":"code","source":"pred_df = dt.fread('../input/tabular-playground-series-nov-2021/test.csv').to_pandas()","metadata":{"execution":{"iopub.status.busy":"2021-11-13T13:32:38.087581Z","iopub.execute_input":"2021-11-13T13:32:38.088312Z","iopub.status.idle":"2021-11-13T13:32:39.906545Z","shell.execute_reply.started":"2021-11-13T13:32:38.088247Z","shell.execute_reply":"2021-11-13T13:32:39.905532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-13T13:32:39.911631Z","iopub.execute_input":"2021-11-13T13:32:39.911945Z","iopub.status.idle":"2021-11-13T13:32:39.944777Z","shell.execute_reply.started":"2021-11-13T13:32:39.911914Z","shell.execute_reply":"2021-11-13T13:32:39.943812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Processing the test DF","metadata":{}},{"cell_type":"code","source":"xpred = pred_df.drop(['id'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-11-13T13:32:39.946254Z","iopub.execute_input":"2021-11-13T13:32:39.947048Z","iopub.status.idle":"2021-11-13T13:32:40.075227Z","shell.execute_reply.started":"2021-11-13T13:32:39.946996Z","shell.execute_reply":"2021-11-13T13:32:40.074126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Main Training Loop\n\nIn this loop, for each fold the training is done. The Training epoch return the loss. It is then backpropagated in the NN. The validation loss and the metrics are also calculated. Finally the model is also saved. \n\nAnd the prediction is also done for the test set","metadata":{}},{"cell_type":"code","source":"def run(data, fold):\n    \n    if not os.path.isdir('standard_scaler'):\n        os.mkdir('standard_scaler')\n    if not os.path.isdir('models'):\n        os.mkdir('models')\n    \n    print('-'*50)\n    print(f'Fold : {fold}')\n    print('-'*50)\n    \n    args = Config()\n    start = time.time()\n    model = TPSModel(args)\n    model = model.cuda()\n    \n    optimizer = get_optimizer(args, model.parameters())\n    scheduler = get_scheduler(args, optimizer)\n    \n    train = data[data['kfold']!=fold]\n    valid = data[data['kfold']==fold]\n    \n    sc = StandardScaler()\n    \n    # We will be scaling down the inputs so that no feature is overlooked by another feature\n    xtrain = train.drop(['id', 'target', 'kfold'], axis=1)\n    ytrain = train['target']\n    xtrain = sc.fit_transform(xtrain)\n    \n    xtest = valid.drop(['id', 'target', 'kfold'], axis=1)\n    ytest = valid['target']\n    xtest = sc.transform(xtest)\n    \n    xpred_sc = sc.transform(xpred) \n    pred_dataset = PredDataset(xpred_sc)\n    pred_loader = DataLoader(pred_dataset, batch_size = 2*args.batch_size)\n    \n    with open(f'standard_scaler/sc_fold_{fold}.pickle', 'wb') as handle:\n        pickle.dump(sc, handle, protocol=pickle.HIGHEST_PROTOCOL)\n    \n    # Creating the datasets\n    train_dataset = Dataset(xtrain, ytrain)\n    valid_dataset = Dataset(xtest, ytest)\n    \n    # Creating the DataLoaders\n    train_loader = DataLoader(train_dataset, batch_size=args.batch_size)\n    valid_loader = DataLoader(valid_dataset, batch_size=2*args.batch_size)\n    \n    best_val_loss = np.inf\n    patience_counter = 0\n    best_auc = 0\n    \n    # Iterating through epochs\n    for epoch in range(args.epochs):\n        \n        # Trainign\n        train_loss = train_epoch(args, train_loader, model, optimizer, scheduler, epoch)\n        \n        # Validation\n        valid_loss, val_auc = validation(args, valid_loader, model, epoch)\n        \n        if val_auc >= best_auc:\n            patience_counter = 0\n            print(f\"Validation AUC improved from : ({best_auc} ---> {val_auc})\")\n            best_auc = val_auc\n\n            PATH = f\"models/model_fold_{fold}.bin\"\n            torch.save(model.state_dict(), PATH)\n            print(f\"----------Model Saved----------\")\n        \n        \n        # Early Stopping to prevent overfitting\n        else:\n            patience_counter += 1\n            print(f'Early stopping counter {patience_counter} of {args.early_stopping_epochs}')\n            if patience_counter == args.early_stopping_epochs:\n                print('*************** Early Stopping ***************')\n                break\n    \n    \n    end = time.time()\n    time_elapsed = end-start\n    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n        time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n    print(\"Best AUC: {:.4f}\".format(best_auc))\n    \n    \n    # Prediction\n    preds = predict(args, pred_loader, model)\n    \n    del model, train_loader, valid_loader\n    gc.collect()\n    \n    return preds","metadata":{"execution":{"iopub.status.busy":"2021-11-13T13:32:40.089495Z","iopub.execute_input":"2021-11-13T13:32:40.090397Z","iopub.status.idle":"2021-11-13T13:32:40.111248Z","shell.execute_reply.started":"2021-11-13T13:32:40.090346Z","shell.execute_reply":"2021-11-13T13:32:40.110181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = dt.fread('../input/fold-is-power/5fold.csv').to_pandas()","metadata":{"execution":{"iopub.status.busy":"2021-11-13T13:32:40.113225Z","iopub.execute_input":"2021-11-13T13:32:40.114003Z","iopub.status.idle":"2021-11-13T13:32:42.124442Z","shell.execute_reply.started":"2021-11-13T13:32:40.113954Z","shell.execute_reply":"2021-11-13T13:32:42.123399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-13T13:32:42.127079Z","iopub.execute_input":"2021-11-13T13:32:42.127776Z","iopub.status.idle":"2021-11-13T13:32:42.161642Z","shell.execute_reply.started":"2021-11-13T13:32:42.127726Z","shell.execute_reply":"2021-11-13T13:32:42.160604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Converting the targets from True-False to 1-0","metadata":{}},{"cell_type":"code","source":"df['target'] = pd.get_dummies(df['target'].values, drop_first=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-13T13:32:42.163591Z","iopub.execute_input":"2021-11-13T13:32:42.164327Z","iopub.status.idle":"2021-11-13T13:32:42.182445Z","shell.execute_reply.started":"2021-11-13T13:32:42.164279Z","shell.execute_reply":"2021-11-13T13:32:42.181496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Run Training, Validation and Prediction","metadata":{}},{"cell_type":"code","source":"all_preds = 0\nfor fold in range(5):\n    model_preds = run(df, fold=fold)\n    all_preds = all_preds + model_preds/5","metadata":{"execution":{"iopub.status.busy":"2021-11-13T13:36:17.965523Z","iopub.execute_input":"2021-11-13T13:36:17.965884Z","iopub.status.idle":"2021-11-13T13:38:26.637669Z","shell.execute_reply.started":"2021-11-13T13:36:17.965851Z","shell.execute_reply":"2021-11-13T13:38:26.63653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_preds = [sigmoid(x) for x in np.hstack(all_preds)]\npred_df['target'] = final_preds\npred_df[['id', 'target']].to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-13T13:34:51.786574Z","iopub.execute_input":"2021-11-13T13:34:51.786978Z","iopub.status.idle":"2021-11-13T13:34:53.930129Z","shell.execute_reply.started":"2021-11-13T13:34:51.786944Z","shell.execute_reply":"2021-11-13T13:34:53.929094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Please DO UPVOTE if you like","metadata":{}}]}