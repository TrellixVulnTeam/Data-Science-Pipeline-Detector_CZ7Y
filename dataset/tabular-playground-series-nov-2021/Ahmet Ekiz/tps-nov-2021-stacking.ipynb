{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a id=\"0\"></a> <br>\n# Table of Contents\n\n1. [Introduction to Tabular Playground Series - Nov 2021](#1)\n    1. [Variable Describtions](#2)\n1. [Load and Glance at the Data](#3)   \n1. [Feature Scaling](#4)\n1. [Helper Functions](#5)  \n1. [ML Models](#6)  \n1. [Read CSV files and Merge](#8)","metadata":{"execution":{"iopub.status.busy":"2021-11-04T17:34:40.692397Z","iopub.execute_input":"2021-11-04T17:34:40.692771Z","iopub.status.idle":"2021-11-04T17:34:40.711343Z","shell.execute_reply.started":"2021-11-04T17:34:40.692679Z","shell.execute_reply":"2021-11-04T17:34:40.710674Z"}}},{"cell_type":"markdown","source":"<a id=\"1\"></a> <br>\n# 1. Introduction to Tabular Playground Series - Nov 2021 \n\nTPS is a monthly competition prepared by Kaggle. The data is used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. More information can be found on the [Competition Overview Page](https://www.kaggle.com/c/tabular-playground-series-nov-2021/overview).\n\n**The goal** is **predicting probability** of the observed target 0 or 1. So it is **supervised learning** and **classification task**. Also **evaluation metric** is selected **area under the ROC curve**.\n\n[My first Notebook on this competition: TPS Nov 2021 Starter with XGBoost](https://www.kaggle.com/ahmetekiz/tps-nov-2021-starter-with-xgboost#7.-Selecting-Models)\n\n[back to the top](#0)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2\"></a> <br>\n## A. Variable Describtions:\n- **df_train** : Pandas data frame for training data set\n- **df_test** : Pandas data frame for test data set\n- **x_train** : Pandas data frame removed target columns from df_train\n- **y_train** : Pandas data frame from df_train","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3\"></a> <br>\n# 2. Load and Glance at the Data\nFirst things first, load and glance at the data.\n\n[back to the top](#0)","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-01T10:16:05.842781Z","iopub.execute_input":"2021-12-01T10:16:05.843123Z","iopub.status.idle":"2021-12-01T10:16:06.592568Z","shell.execute_reply.started":"2021-12-01T10:16:05.843017Z","shell.execute_reply":"2021-12-01T10:16:06.591794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv(\"/kaggle/input/tabular-playground-series-nov-2021/train.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/tabular-playground-series-nov-2021/test.csv\")\nsample_submission = pd.read_csv('/kaggle/input/tabular-playground-series-nov-2021/sample_submission.csv')\n\nrandom_state = 42","metadata":{"execution":{"iopub.status.busy":"2021-12-01T10:16:06.594261Z","iopub.execute_input":"2021-12-01T10:16:06.594515Z","iopub.status.idle":"2021-12-01T10:16:33.77037Z","shell.execute_reply.started":"2021-12-01T10:16:06.59448Z","shell.execute_reply":"2021-12-01T10:16:33.769477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T10:16:33.773711Z","iopub.execute_input":"2021-12-01T10:16:33.774025Z","iopub.status.idle":"2021-12-01T10:16:33.807646Z","shell.execute_reply.started":"2021-12-01T10:16:33.773988Z","shell.execute_reply":"2021-12-01T10:16:33.806775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[back to the top](#0)","metadata":{}},{"cell_type":"code","source":"%%time\ndf_train.info()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T10:16:33.809304Z","iopub.execute_input":"2021-12-01T10:16:33.810174Z","iopub.status.idle":"2021-12-01T10:16:33.830766Z","shell.execute_reply.started":"2021-12-01T10:16:33.810134Z","shell.execute_reply":"2021-12-01T10:16:33.829976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font color=green>All variables is numerical. So we will not strive with categorical data.</font>","metadata":{}},{"cell_type":"code","source":"df_train.describe()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T10:16:33.832381Z","iopub.execute_input":"2021-12-01T10:16:33.832953Z","iopub.status.idle":"2021-12-01T10:16:35.991289Z","shell.execute_reply.started":"2021-12-01T10:16:33.832908Z","shell.execute_reply":"2021-12-01T10:16:35.990601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[back to the top](#0)","metadata":{}},{"cell_type":"code","source":"columns = df_test.columns[1:]\nprint(columns)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T10:16:35.992654Z","iopub.execute_input":"2021-12-01T10:16:35.993144Z","iopub.status.idle":"2021-12-01T10:16:36.000969Z","shell.execute_reply.started":"2021-12-01T10:16:35.993103Z","shell.execute_reply":"2021-12-01T10:16:36.000332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train = df_train.drop(labels = \"target\", axis=1)\n# y_train = df_train[\"target\"].values\ny_train = df_train[\"target\"]\n\nx_test = df_test.copy()\nx_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T10:16:36.002338Z","iopub.execute_input":"2021-12-01T10:16:36.002678Z","iopub.status.idle":"2021-12-01T10:16:36.325302Z","shell.execute_reply.started":"2021-12-01T10:16:36.002636Z","shell.execute_reply":"2021-12-01T10:16:36.324554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4\"></a> <br>\n# 3. Feature Scaling\nIn order to, ML algorithms perform well, I will scale data with Standardization method.\n\n[Variable Describtions](#7)\n\n[back to the top](#0)","metadata":{}},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import ColumnTransformer","metadata":{"execution":{"iopub.status.busy":"2021-12-01T10:16:36.326689Z","iopub.execute_input":"2021-12-01T10:16:36.326945Z","iopub.status.idle":"2021-12-01T10:16:36.427685Z","shell.execute_reply.started":"2021-12-01T10:16:36.326911Z","shell.execute_reply":"2021-12-01T10:16:36.426957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create a Pipeline for Preparing Data to Training","metadata":{}},{"cell_type":"code","source":"std_scaler = StandardScaler()\n\nnum_pipeline = Pipeline([(('std_scaler'), StandardScaler()),])\nfull_pipeline = ColumnTransformer([('num', num_pipeline, columns),])\n\nx_train[columns] = full_pipeline.fit_transform(x_train)\nx_test[columns] = full_pipeline.transform(df_test)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T10:16:36.429881Z","iopub.execute_input":"2021-12-01T10:16:36.430315Z","iopub.status.idle":"2021-12-01T10:16:38.80558Z","shell.execute_reply.started":"2021-12-01T10:16:36.430277Z","shell.execute_reply":"2021-12-01T10:16:38.804827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[Variable Describtions](#7)\n\n[back to the top](#0)","metadata":{}},{"cell_type":"code","source":"# x_train.shape\nx_train.head()","metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-12-01T10:16:38.808553Z","iopub.execute_input":"2021-12-01T10:16:38.80889Z","iopub.status.idle":"2021-12-01T10:16:38.834595Z","shell.execute_reply.started":"2021-12-01T10:16:38.808852Z","shell.execute_reply":"2021-12-01T10:16:38.833713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[Variable Describtions](#7)\n[back to the top](#0)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"5\"></a> <br>\n# 4. Helper Functions\n[back to the top](#0)","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\n\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score, accuracy_score\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier","metadata":{"execution":{"iopub.status.busy":"2021-12-01T10:16:38.836114Z","iopub.execute_input":"2021-12-01T10:16:38.83639Z","iopub.status.idle":"2021-12-01T10:16:39.015344Z","shell.execute_reply.started":"2021-12-01T10:16:38.836352Z","shell.execute_reply":"2021-12-01T10:16:39.014613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_roc_curve(fpr, tpr, label=None, title=None):\n    \"\"\"\n    \"\"\"\n    plt.plot(fpr, tpr, linewidth=2, label=label)\n    plt.plot([0, 1], [0, 1], 'k--') \n    plt.legend(loc = 'lower right')\n    plt.title(title)\n    plt.show()\n\n    \ndef roc_auc_score_func(y_true, y_head, plot_roc=False):\n    \"\"\" evaluate roc auc score\n    Args:\n        y_true : a numpy array. True labels.\n        y_head : a numpy array. Predicted labels.\n        plot_roc : if you want to plot roc curve. (Default is False)\n        \n    \"\"\"\n    \n    if plot_roc:\n        fpr, tpr, thresholds = roc_curve(y_true, y_head)\n        plot_roc_curve(fpr, tpr, label=None, title=None)\n        \n        \n    # evaluate roc auc score\n    model_roc_auc_score = roc_auc_score(y_true, y_true)\n#     print(\"ROC AUC Score:\",roc_auc_score_train)\n    \n    return model_roc_auc_score\n\n\ndef train_and_predict(clf, clf_name, X, Y, x_test, n_splits=5):\n    \"\"\"train ml models with Stratified Kfol return auc score for probability of test data with ml model name\n    \n    Args:\n        clf : model classifier\n        clf_name : classifier name\n        x_train : a numpy.darray training data \n        y_train : a numpy.darray training labels\n        x_test: test data\n        n_splits : StratifiedKFold splits number\n        \n    Returns:\n        roc_auc_score_mean\n        accuracy_mean\n        valid_preds_dict : a dict that stores validation set ids and predictions probabilities from StratifiedKFold\n        test_preds: predictions probabilities of test set\n    \"\"\"\n    \n    valid_preds = []\n    valid_ids = []\n    valid_preds_dict = {}\n    \n    test_preds = []\n    \n    roc_auc_score_list = []  # roc auc score list\n    acc_score_list = [] # auc score list\n    \n#     test_preds = {}\n\n    skf = StratifiedKFold(n_splits=n_splits, random_state=random_state, shuffle=True)\n    \n    for i, (train_index, val_index) in enumerate(skf.split(X, Y)):\n        print(\"Fitting fold\", i+1)\n        X_train, X_val = X[train_index], X[val_index]\n        Y_train, Y_val = Y[train_index], Y[val_index]\n#         print(X_train.head())\n        \n        print(\"Model:\", clf_name)\n\n        # TRAINING\n        training_start_time = time.time()\n        local_time = time.ctime(training_start_time)\n        print(\"Local time:\", local_time)\n        \n        model = clf\n#         model.fit(X_train[columns], Y_train)\n        model.fit(X_train[:,1:], Y_train)  # except id column\n\n        training_end_time = time.time()\n        training_time = training_end_time - training_start_time\n        print(f\"Training elapsed seconds: {round(training_time,3)}\")\n\n\n        # EVALUATING\n        evaluating_start_time = time.time()\n        local_time = time.ctime(evaluating_start_time)\n        print(\"Local time:\", local_time)\n        \n        # create dictionary to save predictions of validation data\n#         valid_pred = model.predict_proba(X_val[:,1:])[:, 1]  # predict probabilty of validation set\n#         valid_preds.append(valid_pred)\n#         valid_ids.append(X[val_index,0])\n        valid_preds = model.predict_proba(X_val[:,1:])[:, 1]  # predict probabilty of validation set\n        valid_ids = X[val_index,0].tolist()\n        valid_preds_dict.update(dict(zip(valid_ids, valid_preds)))  \n    \n        \n        roc_auc_score_list.append(roc_auc_score(Y_val, valid_preds))\n        acc_score_list.append(accuracy_score(Y_val, model.predict(X_val[:,1:])))\n\n        evaluating_end_time = time.time()\n        evaluating_time = evaluating_end_time - evaluating_start_time\n        print(f\"evaluating scores elapsed seconds: {round(evaluating_time,3)}\")\n    \n    \n        # PREDICTION\n        prediction_start_time = time.time()\n        local_time = time.ctime(prediction_start_time)\n        print(\"Local time:\", local_time)\n        \n        test_pred = clf.predict_proba(x_test)[:, 1]\n        test_preds.append(test_pred)\n\n        prediction_end_time = time.time()\n        prediction_time = prediction_end_time - prediction_start_time\n        print(f\"predicting test probability scores elapsed seconds: {round(prediction_time,3)}\")\n    \n    roc_auc_score_mean = np.mean(roc_auc_score_list)\n    accuracy_mean = np.mean(acc_score_list) \n    \n    \n    print(f\"Mean accuracy: {round(accuracy_mean*100,3)}, Mean AUC Score: {round(roc_auc_score_mean*100,3)}\")\n    \n    return roc_auc_score_mean, accuracy_mean, valid_preds_dict, test_preds","metadata":{"execution":{"iopub.status.busy":"2021-12-01T10:16:39.016901Z","iopub.execute_input":"2021-12-01T10:16:39.017186Z","iopub.status.idle":"2021-12-01T10:16:39.03572Z","shell.execute_reply.started":"2021-12-01T10:16:39.017147Z","shell.execute_reply":"2021-12-01T10:16:39.034819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"One of my references here [abhishek's notebook](https://www.kaggle.com/abhishek/competition-part-6-stacking/notebookhttps://www.kaggle.com/abhishek/competition-part-6-stacking/notebook), especially mean of several kfold test predictions. \n\n[back to the top](#0)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"7\"></a> <br>\n## A. Split Train and Validation Data\n[back to the top](#0)","metadata":{}},{"cell_type":"code","source":"# # IT CONTROLS THE CODE WORKS BEFORE SUBMIT\n# from sklearn.model_selection import StratifiedShuffleSplit\n\n# split = StratifiedShuffleSplit(n_splits=1, test_size=0.001, random_state=42)\n\n# for train_index, val_index in split.split(x_train, y_train):\n#     x_train, y_train = x_train.loc[val_index], y_train.loc[val_index] \n# #     x_val, y_val = x_train.loc[val_index], y_train.loc[val_index]\n\n# # x_train_2 = x_train_2.values\n# # y_train_2 = y_train_2.values\n\n# # # x_train_2[:,0] # id","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-12-01T10:16:39.037466Z","iopub.execute_input":"2021-12-01T10:16:39.037748Z","iopub.status.idle":"2021-12-01T10:16:39.41164Z","shell.execute_reply.started":"2021-12-01T10:16:39.037709Z","shell.execute_reply":"2021-12-01T10:16:39.410938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"6\"></a> <br>\n# 5. ML Models\n[back to the top](#0)","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade xgboost\n\n# # xgb.__version__","metadata":{"execution":{"iopub.status.busy":"2021-12-01T10:16:39.414088Z","iopub.execute_input":"2021-12-01T10:16:39.414561Z","iopub.status.idle":"2021-12-01T10:16:58.87845Z","shell.execute_reply.started":"2021-12-01T10:16:39.41452Z","shell.execute_reply":"2021-12-01T10:16:58.87758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from xgboost import XGBClassifier","metadata":{"execution":{"iopub.status.busy":"2021-12-01T10:16:58.880319Z","iopub.execute_input":"2021-12-01T10:16:58.880609Z","iopub.status.idle":"2021-12-01T10:16:58.909225Z","shell.execute_reply.started":"2021-12-01T10:16:58.880573Z","shell.execute_reply":"2021-12-01T10:16:58.908561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# assert False","metadata":{"execution":{"iopub.status.busy":"2021-12-01T10:16:58.910322Z","iopub.execute_input":"2021-12-01T10:16:58.910578Z","iopub.status.idle":"2021-12-01T10:16:58.914346Z","shell.execute_reply.started":"2021-12-01T10:16:58.910541Z","shell.execute_reply":"2021-12-01T10:16:58.913395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[back to the top](#0)","metadata":{}},{"cell_type":"markdown","source":"I've chosen three classifiers according to my previous notebook's evaluation results.","metadata":{}},{"cell_type":"code","source":"x_train = x_train.values\ny_train = y_train.values","metadata":{"execution":{"iopub.status.busy":"2021-12-01T10:16:58.915751Z","iopub.execute_input":"2021-12-01T10:16:58.916288Z","iopub.status.idle":"2021-12-01T10:16:58.925302Z","shell.execute_reply.started":"2021-12-01T10:16:58.91624Z","shell.execute_reply":"2021-12-01T10:16:58.924594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classifiers = [LogisticRegression(solver='liblinear', random_state = random_state),\n               XGBClassifier(max_depth=8,\n                             learning_rate=0.01,\n                             n_estimators=10000,\n                             verbosity=1,\n                             silent=None,\n                             objective='binary:logistic',  \n                             tree_method = 'gpu_hist',\n                             booster='gbtree',\n                             n_jobs=-1,\n                             nthread=None,\n                             eval_metric='auc',\n                             gamma=0,\n                             min_child_weight=1,\n                             max_delta_step=0,\n                             subsample=0.7,\n                             colsample_bytree=1,\n                             colsample_bylevel=1,\n                             colsample_bynode=1,\n                             reg_alpha=0,\n                             reg_lambda=1,\n                             scale_pos_weight=1,\n                             base_score=0.5,\n                             random_state=random_state,\n                             seed=None)]\n\nclassifiers_names = [\"LogisticRegression\",\n                     \"XGB\"]","metadata":{"execution":{"iopub.status.busy":"2021-12-01T10:16:58.927693Z","iopub.execute_input":"2021-12-01T10:16:58.928404Z","iopub.status.idle":"2021-12-01T10:16:58.936368Z","shell.execute_reply.started":"2021-12-01T10:16:58.928364Z","shell.execute_reply":"2021-12-01T10:16:58.935588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nimport time\n\nclf_roc_auc_scores = [] \nclf_auc_scores = []\n\nscores_of_models = {}\n\n\n\nfor i, clf in enumerate(classifiers):\n    \n    \n    start_time = time.time()\n    clf_name = classifiers_names[i]\n    roc_auc_score_mean, accuracy_mean, valid_preds_dict, test_preds = train_and_predict(clf, \n                                                                                        clf_name, \n                                                                                        x_train, \n                                                                                        y_train,  \n                                                                                        x_test[columns],\n                                                                                        n_splits=2)\n    \n    clf_roc_auc_scores.append(roc_auc_score_mean)\n    clf_auc_scores.append(accuracy_mean)\n    \n    end_time = time.time()\n    \n    print('Elapsed seconds classifier training time:', round(end_time-start_time,2))\n    \n#     assert False\n\n    # save predictions of validation data to csv\n    valid_preds_dict = pd.DataFrame.from_dict(valid_preds_dict, orient=\"index\").reset_index()\n    valid_preds_dict.columns = [\"id\", f\"pred_{i}\"]\n    valid_preds_dict.to_csv(f\"valid_preds_{clf_name}.csv\", index=False)\n\n    # save predictions of test data to csv\n    sub = pd.read_csv('/kaggle/input/tabular-playground-series-nov-2021/sample_submission.csv')\n    \n    # it can be deleted.\n#     print(\"sub.shape\", sub.shape)\n#     print(test_preds)\n#     print(\"lenght test_preds\", len(test_preds))\n#     print(\"shape test_preds\", test_preds.shape())\n#     sub = sub[0:len(test_preds)]\n    \n    test_preds = np.mean(np.column_stack(test_preds), axis=1)  # mean of every kfold predictions. Before that it gives a list has two columns\n    \n    sub['target'] = test_preds\n    sub.columns = [\"id\", f\"pred_{i}\"]\n    sub.to_csv(f'test_preds_{clf_name}.csv', index=False)\n    \n\n# SAVE MODELS AUC SCORES \nclf_results = pd.DataFrame({\"ML Models\": classifiers_names, \"clf_roc_auc_scores\":clf_roc_auc_scores})\nclf_results.to_csv(\"classifiers_auc_scores.csv\", index=False)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-12-01T10:16:58.938812Z","iopub.execute_input":"2021-12-01T10:16:58.939733Z","iopub.status.idle":"2021-12-01T10:18:32.059428Z","shell.execute_reply.started":"2021-12-01T10:16:58.939556Z","shell.execute_reply":"2021-12-01T10:18:32.058599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[back to the top](#0)","metadata":{}},{"cell_type":"code","source":"# PLOT AUC Scores\n\nsns.set(style=\"whitegrid\", color_codes=True)\npal = sns.color_palette(\"Greens_d\", len(clf_results[\"ML Models\"]))\nrank = clf_results[\"ML Models\"].argsort().argsort() \ng = sns.barplot(\"ML Models\", \"clf_roc_auc_scores\", data = clf_results, palette=np.array(pal[::1])[rank])\ng.set_xlabel(\"Mean ROC AUC Score of Probability\")\ng.set_title(\"Stratified KFold\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T10:18:32.060742Z","iopub.execute_input":"2021-12-01T10:18:32.062334Z","iopub.status.idle":"2021-12-01T10:18:32.266607Z","shell.execute_reply.started":"2021-12-01T10:18:32.062292Z","shell.execute_reply":"2021-12-01T10:18:32.2659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"8\"></a> <br>\n# 6. Read CSV files and Merge\n[back to the top](#0)","metadata":{}},{"cell_type":"code","source":"df_valid_preds_0 = pd.read_csv(\"valid_preds_LogisticRegression.csv\")\ndf_valid_preds_1 = pd.read_csv(\"valid_preds_XGB.csv\")\n\ndf_test_preds_0 = pd.read_csv(\"test_preds_LogisticRegression.csv\")\ndf_test_preds_1 = pd.read_csv(\"test_preds_XGB.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-12-01T10:18:32.267905Z","iopub.execute_input":"2021-12-01T10:18:32.268642Z","iopub.status.idle":"2021-12-01T10:18:32.523129Z","shell.execute_reply.started":"2021-12-01T10:18:32.268602Z","shell.execute_reply":"2021-12-01T10:18:32.522333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_valid_preds_0.head()","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-12-01T10:18:32.524494Z","iopub.execute_input":"2021-12-01T10:18:32.524744Z","iopub.status.idle":"2021-12-01T10:18:32.53465Z","shell.execute_reply.started":"2021-12-01T10:18:32.52471Z","shell.execute_reply":"2021-12-01T10:18:32.533852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_valid_preds_1.head()","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-12-01T10:18:32.536036Z","iopub.execute_input":"2021-12-01T10:18:32.536475Z","iopub.status.idle":"2021-12-01T10:18:32.55164Z","shell.execute_reply.started":"2021-12-01T10:18:32.53643Z","shell.execute_reply":"2021-12-01T10:18:32.55093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test_preds_0.head()","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-12-01T10:18:32.553134Z","iopub.execute_input":"2021-12-01T10:18:32.553459Z","iopub.status.idle":"2021-12-01T10:18:32.562042Z","shell.execute_reply.started":"2021-12-01T10:18:32.55342Z","shell.execute_reply":"2021-12-01T10:18:32.561102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test_preds_1.head()","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-12-01T10:18:32.563829Z","iopub.execute_input":"2021-12-01T10:18:32.564329Z","iopub.status.idle":"2021-12-01T10:18:32.577696Z","shell.execute_reply.started":"2021-12-01T10:18:32.564282Z","shell.execute_reply":"2021-12-01T10:18:32.57624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv(\"/kaggle/input/tabular-playground-series-nov-2021/train.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/tabular-playground-series-nov-2021/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-12-01T10:18:32.579101Z","iopub.execute_input":"2021-12-01T10:18:32.579396Z","iopub.status.idle":"2021-12-01T10:18:51.336094Z","shell.execute_reply.started":"2021-12-01T10:18:32.579358Z","shell.execute_reply":"2021-12-01T10:18:51.335367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_valid_preds_0 = df_valid_preds_0.merge(df_train, on=\"id\", how=\"left\")\n# df_valid_preds_0 = df_valid_preds_0.merge(df_valid_preds_1, on=\"id\", how=\"left\")\n\n\ndf_train = df_train.merge(df_valid_preds_0, on=\"id\", how=\"left\")\ndf_train = df_train.merge(df_valid_preds_1, on=\"id\", how=\"left\")\n\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T10:18:51.337659Z","iopub.execute_input":"2021-12-01T10:18:51.337926Z","iopub.status.idle":"2021-12-01T10:18:52.01756Z","shell.execute_reply.started":"2021-12-01T10:18:51.33789Z","shell.execute_reply":"2021-12-01T10:18:52.016852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.shape","metadata":{"execution":{"iopub.status.busy":"2021-12-01T10:18:52.021098Z","iopub.execute_input":"2021-12-01T10:18:52.021683Z","iopub.status.idle":"2021-12-01T10:18:52.027592Z","shell.execute_reply.started":"2021-12-01T10:18:52.021641Z","shell.execute_reply":"2021-12-01T10:18:52.026735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# useful_features_train = [\"pred_0\", \"pred_1\", \"pred_2\", \"target\"]\nuseful_features_train = [\"pred_0\", \"pred_1\", \"target\"]\n\ndf_train[useful_features_train].head()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T10:18:52.028983Z","iopub.execute_input":"2021-12-01T10:18:52.029322Z","iopub.status.idle":"2021-12-01T10:18:52.356604Z","shell.execute_reply.started":"2021-12-01T10:18:52.029278Z","shell.execute_reply":"2021-12-01T10:18:52.355863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = df_test.merge(df_test_preds_0, on=\"id\", how=\"left\")\ndf_test = df_test.merge(df_test_preds_1, on=\"id\", how=\"left\")\n\ndf_test.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T10:18:52.357924Z","iopub.execute_input":"2021-12-01T10:18:52.358373Z","iopub.status.idle":"2021-12-01T10:18:52.878617Z","shell.execute_reply.started":"2021-12-01T10:18:52.358333Z","shell.execute_reply":"2021-12-01T10:18:52.877842Z"},"trusted":true},"execution_count":null,"outputs":[]}]}