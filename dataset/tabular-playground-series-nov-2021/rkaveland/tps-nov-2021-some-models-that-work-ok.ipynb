{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Simple models that do OK\n==\n\nTo my great joy, I quickly discovered that this time around, I won't be tuning boosters for weeks. :-)\n\nI found a number of other promising approaches that are quick to train, and don't have as many hyper parameters to tune -- which should free up time to work on feature engineering instead. In this notebook, I've set up simple demos of how that works using cross validation, but I'm not storing any output here. I've submitted a few of these models, and they do worse on the public LB than on these CVs, but they should still mostly be around AUC `.745`, and there's even one model achieving AUC `.747` here.\n\nThere's no EDA in this notebook, only things that are plug-and-play -- I had spent some time looking at features before I started testing these models, and got reasonable results with no feature engineering. It looks like the input data is probably generated based on some TfIdf-vectors, so I went looking for models that people would normally combine with that. Towards the end, I did some simple blending experiments.\n\nLet's get the rig out of the way first, here's my usual suspect imports, and also a scikit-learn upgrade to match what I have on my own machine:","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"%pip install -q -U scikit-learn\n\nimport random\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom multiprocessing import cpu_count\nfrom matplotlib import pyplot as plt\n\nfrom sklearn import (\n    metrics,\n    model_selection,\n    linear_model,\n    pipeline,\n    preprocessing,\n    base\n)","metadata":{"execution":{"iopub.status.busy":"2021-11-04T09:55:14.419736Z","iopub.execute_input":"2021-11-04T09:55:14.420163Z","iopub.status.idle":"2021-11-04T09:55:33.915394Z","shell.execute_reply.started":"2021-11-04T09:55:14.420061Z","shell.execute_reply":"2021-11-04T09:55:33.914242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Setting up folds\n==\n\nI reuse the same folds across all of the models here, in order to be able to compare results.\n\nI also set some variables I'll want to use later for checking metrics -- `y_true` is the out of fold order for my folds, I will check my model performance by calling `metrics.roc_auc_score(y_true, y_score)`.\n\nI'm using 5 folds here, somewhat arbitrarily chosen. ","metadata":{}},{"cell_type":"code","source":"np.random.seed(42)\nrandom.seed(42)\nn_jobs = cpu_count()\n\nsns.set(\n    style='whitegrid',\n    context='notebook',\n    rc={'figure.frameon': False, 'legend.frameon': False, 'figure.figsize': (12, 8)}\n)\n\ndata_root = '/kaggle/input/tabular-playground-series-nov-2021'\n\ndf = pd.read_csv(f'{data_root}/train.csv', dtype=np.float32).astype({'id': np.int32}).sample(frac=1) # shuffle\ndf_test = pd.read_csv(f'{data_root}/test.csv', dtype=np.float32).astype({'id': np.int32})\nX_test = df_test.drop(columns=['id']).to_numpy()\n\nids, X, y = df['id'].to_numpy(), df.drop(columns=['id', 'target']).to_numpy(), df.target.to_numpy()\n\nfolds_idx = list(model_selection.StratifiedKFold(n_splits=5).split(X, y))\nval_order = np.concatenate([val_idx for _, val_idx in folds_idx], axis=0)\ny_true = y[val_order]","metadata":{"execution":{"iopub.status.busy":"2021-11-04T09:55:33.917718Z","iopub.execute_input":"2021-11-04T09:55:33.918107Z","iopub.status.idle":"2021-11-04T09:56:13.82204Z","shell.execute_reply.started":"2021-11-04T09:55:33.918054Z","shell.execute_reply":"2021-11-04T09:56:13.821052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Making a rig to check CV score\n==\n\nThis is something you could just use `sklearn.model_selection.cross_val_score()` for. But this way of doing it is useful if you plan to do something with the estimators that are trained -- for example, store their test predictions and out of fold predictions (which I do, just not in this notebook).","metadata":{}},{"cell_type":"code","source":"def predict(model, X):\n    \"\"\"We'd prefer using predict_proba, but can fall back to decision_function or predict if need be.\"\"\"\n\n    try:\n        return model.predict_proba(X)[:, 1]\n    except (AttributeError, ValueError) as e:\n        # It doesn't have it, or it is SGD with loss='hinge', which has, but doesn't support it\n        if hasattr(model, 'decision_function'):\n            return model.decision_function(X)\n        else:\n            return model.predict(X)\n        \n\ndef score_model(model: base.BaseEstimator, n_jobs=n_jobs):\n    \"\"\"\n    Returns (auc, oof_predictions, test_predictions)\n    \n    auc is for oof_predictions, but it should not be *too* far off from test_predictions\n    \n    \"\"\"\n    report = model_selection.cross_validate(\n        model, X, y, \n        cv=folds_idx, # we always use the same folds\n        scoring='roc_auc',\n        return_estimator=True, # this gives us the trained estimators in the report\n        n_jobs=n_jobs,\n    )\n    oof, test = [], []\n    \n    for est, (_, val_idx) in zip(report['estimator'], folds_idx):\n        oof.append(predict(est, X[val_idx]))\n        test.append(predict(est, X_test))\n        \n    oof = np.concatenate(oof, axis=0)\n    test = np.c_[test].mean(axis=0)\n    return metrics.roc_auc_score(y_true, oof), oof, test","metadata":{"execution":{"iopub.status.busy":"2021-11-04T09:56:13.823475Z","iopub.execute_input":"2021-11-04T09:56:13.823724Z","iopub.status.idle":"2021-11-04T09:56:13.835886Z","shell.execute_reply.started":"2021-11-04T09:56:13.823694Z","shell.execute_reply":"2021-11-04T09:56:13.834439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"RidgeClassifier\n--\n\nRidge Regression is supposed to work well when features are highly correlated, which should be the case for TfIdf vectors, eg. multiple terms occuring together.","metadata":{}},{"cell_type":"code","source":"rdg = pipeline.make_pipeline(\n    preprocessing.RobustScaler(),\n    linear_model.RidgeClassifierCV()\n)\n\n%time rdg_auc, rdg_oof, rdg_preds = score_model(rdg)\nrdg_auc","metadata":{"execution":{"iopub.status.busy":"2021-11-04T09:59:00.923992Z","iopub.execute_input":"2021-11-04T09:59:00.924612Z","iopub.status.idle":"2021-11-04T09:59:57.362916Z","shell.execute_reply.started":"2021-11-04T09:59:00.924558Z","shell.execute_reply":"2021-11-04T09:59:57.361811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"That's a pretty good score for a model that's so fast to fit.\n\nSGD\n==\n\nNow we can check hinge-loss SGD, which is similar to a linear SVM. SVM were state-of-the-art for natural language processing at some point, and the synthetic data is generated based on an email dataset, so it's perhaps not surprising that it works well here.","metadata":{}},{"cell_type":"code","source":"sgd = pipeline.make_pipeline(\n    preprocessing.RobustScaler(), \n    linear_model.SGDClassifier(loss='hinge', learning_rate='adaptive', penalty='l2', alpha=1e-3, eta0=0.025)\n)\n\n%time sgd_auc, sgd_oof, sgd_preds = score_model(sgd)\nsgd_auc","metadata":{"execution":{"iopub.status.busy":"2021-11-04T09:59:57.365624Z","iopub.execute_input":"2021-11-04T09:59:57.366037Z","iopub.status.idle":"2021-11-04T10:00:58.270321Z","shell.execute_reply.started":"2021-11-04T09:59:57.365984Z","shell.execute_reply":"2021-11-04T10:00:58.269436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I tried several other approaches for SVM, but had issues getting them to work due to the number of samples we have.\n\nThese parameters weren't chosen at random, I did some hyper parameter search here for about 1 hour, based on a hunch that SGD would be great on this dataset. SGD is reasonably fast to experiment with.","metadata":{}},{"cell_type":"markdown","source":"Linear Discriminant Analysis\n==\n\nDoesn't get much simpler to tune than this one:","metadata":{}},{"cell_type":"code","source":"from sklearn import discriminant_analysis\n\nlda = pipeline.make_pipeline(\n    preprocessing.StandardScaler(), \n    discriminant_analysis.LinearDiscriminantAnalysis(),\n)\n\n%time lda_auc, lda_oof, lda_preds = score_model(lda)\nlda_auc","metadata":{"execution":{"iopub.status.busy":"2021-11-04T10:00:58.272088Z","iopub.execute_input":"2021-11-04T10:00:58.273162Z","iopub.status.idle":"2021-11-04T10:01:53.685427Z","shell.execute_reply.started":"2021-11-04T10:00:58.273104Z","shell.execute_reply":"2021-11-04T10:01:53.684452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I believe this is also a classic for NLP tasks with TfIdf vectors. I've used it for feature transformations in the past, but for this data set it works out to be a reasonably good classifier on its own, no tuning required.","metadata":{}},{"cell_type":"markdown","source":"Logistic Regression\n==\n\nThis one isn't too bad here either:","metadata":{}},{"cell_type":"code","source":"lreg = pipeline.make_pipeline(preprocessing.RobustScaler(), linear_model.LogisticRegression(C=0.0015, penalty='l2'))\n%time lreg_auc, lreg_oof, lreg_preds = score_model(lreg)\nlreg_auc","metadata":{"execution":{"iopub.status.busy":"2021-11-04T10:01:53.687664Z","iopub.execute_input":"2021-11-04T10:01:53.687902Z","iopub.status.idle":"2021-11-04T10:02:29.638541Z","shell.execute_reply.started":"2021-11-04T10:01:53.687873Z","shell.execute_reply":"2021-11-04T10:02:29.63782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I did a very quick hyper parameter search here, and got better scores using `RobustScaler` than `StandardScaler`.\n\nWith the very similar scores so far, it seems likely that these linear models are all finding the same solution.","metadata":{}},{"cell_type":"markdown","source":"Naive Bayes\n==\n\nThis would've been the classic for working with email (spam detection), but unfortunately it doesn't quite get there. Maybe with some tuning, it could?","metadata":{}},{"cell_type":"code","source":"from sklearn import naive_bayes\n\nnb = pipeline.make_pipeline(\n        preprocessing.MinMaxScaler(feature_range=(0, 1)),\n        naive_bayes.MultinomialNB()\n)\n\n%time nb_auc, nb_oof, nb_preds = score_model(nb)\nnb_auc","metadata":{"execution":{"iopub.status.busy":"2021-11-04T10:02:29.639657Z","iopub.execute_input":"2021-11-04T10:02:29.640495Z","iopub.status.idle":"2021-11-04T10:02:37.891738Z","shell.execute_reply.started":"2021-11-04T10:02:29.640446Z","shell.execute_reply":"2021-11-04T10:02:37.890828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`.725` is probably not good enough that we could include it in blending, even. It's possible that we could tune it, or get better if we tried some feature engineering?\n\nEither way, it's blazing fast, so if we were to experiment with it, we should be able to do it rapidly.","metadata":{}},{"cell_type":"markdown","source":"\nMLP\n==\n\nThis one, I would normally implement with torch or tensorflow and run it on the GPU, but for the sake of just showing that it works pretty well here, I've left the scikit-learn implementation running, since this isn't a particularly expensive NN to evaluate:","metadata":{}},{"cell_type":"code","source":"from sklearn import neural_network\n\nmlp = pipeline.make_pipeline(\n    preprocessing.StandardScaler(),\n    neural_network.MLPClassifier(hidden_layer_sizes=(100, 50, 10), batch_size=256, early_stopping=True)\n)\n%time mlp_auc, mlp_oof, mlp_preds = score_model(mlp)\n\nmlp_auc","metadata":{"execution":{"iopub.status.busy":"2021-11-04T10:02:37.893198Z","iopub.execute_input":"2021-11-04T10:02:37.893533Z","iopub.status.idle":"2021-11-04T10:05:41.16392Z","shell.execute_reply.started":"2021-11-04T10:02:37.893496Z","shell.execute_reply":"2021-11-04T10:05:41.163071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is the first set of parameters I attempted here, there's no reason to believe we couldn't do better if we tried some tuning, or maybe a different NN architecture than MLP.\n\nEnsembling / blending\n==\n\nSince we left all the out of fold predictions and test predictions in memory, we can easily start experimenting with ensembling in some way at this point. We have a reasonably diverse selection of models here, we know that the folds have been repeatedly happening in the same order, so it's easy to combine them to check what our best combination might be.\n\nWe'd do that by combining the out of fold predictions we kept, then score that against `y_true`. Here's a simple example of averaging everything except Naive Bayes:","metadata":{}},{"cell_type":"code","source":"scaler = preprocessing.MinMaxScaler()\noof_preds = np.c_[[sgd_oof, lda_oof, lreg_oof, mlp_oof, rdg_oof]].swapaxes(0, 1)\noof_blend = np.mean(scaler.fit_transform(oof_preds), axis=1)\nmetrics.roc_auc_score(y_true, oof_blend)","metadata":{"execution":{"iopub.status.busy":"2021-11-04T10:07:19.024515Z","iopub.execute_input":"2021-11-04T10:07:19.024993Z","iopub.status.idle":"2021-11-04T10:07:19.382233Z","shell.execute_reply.started":"2021-11-04T10:07:19.024935Z","shell.execute_reply":"2021-11-04T10:07:19.381382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We could also try to grid search over some simple estimators:","metadata":{}},{"cell_type":"code","source":"X_blend = np.c_[[sgd_oof, lda_oof, lreg_oof, mlp_oof, nb_oof, rdg_oof]].swapaxes(0, 1)\n\nblend = pipeline.Pipeline([\n    ('scaler', preprocessing.StandardScaler()),\n    ('predict', linear_model.LinearRegression())\n])\n\nparam_grid = {\n    'scaler': [preprocessing.RobustScaler(), preprocessing.StandardScaler(), preprocessing.MinMaxScaler()],\n    'predict': [\n        linear_model.RidgeClassifier(), linear_model.LogisticRegression(), \n        linear_model.BayesianRidge(), linear_model.LinearRegression()\n    ]\n}\n\ngrid_search = model_selection.GridSearchCV(blend, param_grid, n_jobs=-1, refit=True, scoring='roc_auc', cv=folds_idx)\n%time grid_search.fit(X_blend, y)\npd.DataFrame(grid_search.cv_results_)[['param_predict', 'param_scaler', 'mean_test_score']]","metadata":{"execution":{"iopub.status.busy":"2021-11-04T10:05:41.807067Z","iopub.execute_input":"2021-11-04T10:05:41.807304Z","iopub.status.idle":"2021-11-04T10:06:02.05789Z","shell.execute_reply.started":"2021-11-04T10:05:41.807274Z","shell.execute_reply":"2021-11-04T10:06:02.056847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_search.best_score_","metadata":{"execution":{"iopub.status.busy":"2021-11-04T10:06:02.065801Z","iopub.execute_input":"2021-11-04T10:06:02.066805Z","iopub.status.idle":"2021-11-04T10:06:02.074949Z","shell.execute_reply.started":"2021-11-04T10:06:02.066743Z","shell.execute_reply":"2021-11-04T10:06:02.073878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Or, since I also kept our estimators around, we could use the sklearn stacking ensemble, but this will retrain everything, so it'll take more time than the rest of the notebook up until this point:","metadata":{}},{"cell_type":"code","source":"from sklearn import ensemble\n\nstack = ensemble.StackingClassifier(\n    estimators=[('sgd', sgd), ('lda', lda), ('lreg', lreg), ('mlp', mlp), ('rdg', rdg), ('nb', nb)], \n    final_estimator=pipeline.make_pipeline(preprocessing.StandardScaler(), linear_model.LogisticRegression())\n)\n%time stack_auc, stack_oof, stack_preds = score_model(stack)\nstack_auc","metadata":{"execution":{"iopub.status.busy":"2021-11-04T10:06:02.077257Z","iopub.execute_input":"2021-11-04T10:06:02.077989Z","iopub.status.idle":"2021-11-04T10:06:12.601644Z","shell.execute_reply.started":"2021-11-04T10:06:02.077938Z","shell.execute_reply":"2021-11-04T10:06:12.600198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test[['id']].assign(\n    target=stack_preds\n).to_csv('/kaggle/working/simple_stacker.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I'm sending in the submission from that stack, just to associate a score with this notebook.\n\nDone\n==\n\nNote that this highly overestimates our LB score due to a target leak, since we're using our out of fold data to do things like early-stopping and choosing the best models, we can't also reuse the folds to estimate blend score. One way to get around that would be to split off some data from train before doing the folds, then use that data to measure the blends. It shouldn't be too hard to make that adaption.\n\nSo, where to go from here? One option is to train many models (possibly some of these), using a setup much like this, then find the best way of blending, or stacking them. Eventually, I think that's probably the direction I will be heading. There are also lots of NN architectures that could be worth a shot here, and there's almost certainly a way to tune a booster so that it could compete with these models we found. But for now, I think I'd like to try doing some work on our features. :-)","metadata":{}}]}