{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Mean target distribution for test chunks\n==\n\nThis notebook makes use of two observations:\n\n1. [The data is chunked](https://www.kaggle.com/c/tabular-playground-series-nov-2021/discussion/286731)\n2. [The original train labels are known](https://www.kaggle.com/criskiev/november21)\n\nWe make two assumptions:\n\n1. The initial test labels were \"too easy\"\n2. The original labels were flipped at random, with the same random chance across both train and test sets\n\nImports & setup\n--\n\nNothing interesting here.","metadata":{}},{"cell_type":"code","source":"import random\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport plotly.express as px\n\nfrom sklearn.model_selection import StratifiedKFold, cross_val_predict\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score, accuracy_score\n\nnp.random.seed(64)\nrandom.seed(64)\n\nfolds = StratifiedKFold(5, random_state=64, shuffle=True)\nsns.set(\n    style='darkgrid', context='notebook', rc={\n        'figure.frameon': False, 'figure.figsize': (12, 8), 'legend.frameon': False,\n    }\n)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-19T17:19:38.488311Z","iopub.execute_input":"2021-11-19T17:19:38.489069Z","iopub.status.idle":"2021-11-19T17:19:38.495503Z","shell.execute_reply.started":"2021-11-19T17:19:38.489024Z","shell.execute_reply":"2021-11-19T17:19:38.494929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Reading both old and new labels\n--\n\nWe're just immediately going to center both the train and test set, and read both the new and old train labels:","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv('../input/tabular-playground-series-nov-2021/train.csv', dtype=np.float32)\ndf_test = pd.read_csv('../input/tabular-playground-series-nov-2021/test.csv', dtype=np.float32)\nold_y = pd.read_csv('../input/november21/train.csv', usecols=['target'], dtype=np.float32).target\n\nfeatures = df_train.columns[df_train.columns.str.startswith('f')]\nscaler = StandardScaler().fit(pd.concat([df_train[features], df_test[features]], axis=0))\nX = scaler.transform(df_train[features])\nX_test = scaler.transform(df_test[features])\nnew_y = df_train.target","metadata":{"execution":{"iopub.status.busy":"2021-11-19T17:18:13.394294Z","iopub.execute_input":"2021-11-19T17:18:13.394824Z","iopub.status.idle":"2021-11-19T17:18:38.830193Z","shell.execute_reply.started":"2021-11-19T17:18:13.394785Z","shell.execute_reply":"2021-11-19T17:18:38.829431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Establish baselines for LogisticRegression\n==\n\nWe'll establish baselines for new and old labels using 5-fold split:","metadata":{}},{"cell_type":"code","source":"old_y_score = cross_val_predict(LogisticRegression(), X, old_y, method='decision_function', n_jobs=-1, cv=folds)\nold_auc, old_acc = roc_auc_score(old_y, old_y_score), accuracy_score(old_y, old_y_score > 0)\nnew_y_score = cross_val_predict(LogisticRegression(), X, new_y, method='decision_function', n_jobs=-1, cv=folds)\nnew_auc, new_acc = roc_auc_score(new_y, new_y_score), accuracy_score(new_y, new_y_score > 0)\nflip_chance = np.mean(new_y != old_y)\n\nprint(f'Old labels: auc={old_auc:.5f} acc={old_acc:.5f} => new labels auc={new_auc:.5} acc={new_acc:.5}, flip chance = {flip_chance:.5f}')","metadata":{"execution":{"iopub.status.busy":"2021-11-19T17:23:42.686859Z","iopub.execute_input":"2021-11-19T17:23:42.68772Z","iopub.status.idle":"2021-11-19T17:23:58.295507Z","shell.execute_reply.started":"2021-11-19T17:23:42.687664Z","shell.execute_reply":"2021-11-19T17:23:58.294471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If test isn't too dissimilar from train, it seems fair enough to assume that a simple model should score better than 99% accuracy. \n\nFlip chance = 25.12%\n--\n\nWith the new labels, the aggregate flip chance seems to be about 25.12%. Let's check whether that holds true across the train chunks:","metadata":{}},{"cell_type":"code","source":"pd.options.plotting.backend = \"plotly\"\n\npd.DataFrame({\n    'chunk': np.arange(len(new_y)) // 60000,\n    'flips': new_y != old_y\n}).groupby('chunk').flips.mean().plot.bar(y='flips', title='flip chance by chunk')","metadata":{"execution":{"iopub.status.busy":"2021-11-19T17:27:03.205599Z","iopub.execute_input":"2021-11-19T17:27:03.2063Z","iopub.status.idle":"2021-11-19T17:27:04.134519Z","shell.execute_reply.started":"2021-11-19T17:27:03.206251Z","shell.execute_reply":"2021-11-19T17:27:04.13357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is actually different from what I had expected -- I was assuming the flip would be the same across all chunks. But, clearly it's not. Maybe a chance was drawn from a distribution centered around .25 for each chunk, independently?\n\nAnyway, if the flip chance was .25, we expect new label distribution to be equal to `.25 + .5 * y` where `y` is the old one, ie before flips. Let's check how that holds up:","metadata":{}},{"cell_type":"code","source":"distribution = pd.DataFrame({\n    'chunk': np.arange(len(new_y)) // 60000,\n    'old_y': old_y,\n    'new_y': new_y,\n}).groupby('chunk').mean().assign(\n    expected_dist=lambda df: .25 + .5 * df.old_y \n)\n\ndistribution.plot.bar(barmode='group', title='mean(target) by new, old vs formula 25% flips')","metadata":{"execution":{"iopub.status.busy":"2021-11-19T17:33:58.983528Z","iopub.execute_input":"2021-11-19T17:33:58.984159Z","iopub.status.idle":"2021-11-19T17:33:59.081096Z","shell.execute_reply.started":"2021-11-19T17:33:58.984114Z","shell.execute_reply":"2021-11-19T17:33:59.080294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Given pre-flip labels, we can do good estimates of `mean(target)` by chunk.\n\nEstimating test chunks label distribution\n--\n\nThis is really just putting together all the steps above:\n\n1. Fit model on old train labels\n2. Predict on test set to obtain good approximation to old test labels\n3. Assuming 25% flip chance, it's just a simple calculation","metadata":{}},{"cell_type":"code","source":"clf = LogisticRegression().fit(X, old_y)\nold_y_test = clf.predict(X_test)\n\npd.DataFrame({\n    'chunk': np.arange(len(old_y_test)) // 60000 + 10,\n    'old_y': old_y_test\n}).groupby('chunk').old_y.mean().plot.bar(title='Test label distribution by chunk before flip')","metadata":{"execution":{"iopub.status.busy":"2021-11-19T17:39:20.568869Z","iopub.execute_input":"2021-11-19T17:39:20.569192Z","iopub.status.idle":"2021-11-19T17:39:25.536994Z","shell.execute_reply.started":"2021-11-19T17:39:20.569158Z","shell.execute_reply":"2021-11-19T17:39:25.536179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"But we can just put all of this together in the same DataFrame -- we'll populate one with all the old labels, then flip 25% of them at random, and that should end up fairly close to what this actually is!","metadata":{}},{"cell_type":"code","source":"old_labels = pd.DataFrame({\n    'target': np.concatenate([old_y, old_y_test])\n}).assign(\n    chunk=lambda df: np.arange(len(df)) // 60000\n)\n\nflip = np.random.uniform(size=len(old_labels)) < .25\ndf = old_labels.assign(\n    new_target=lambda df: df.target.where(~flip, 1 - df.target)\n)\n\ndf.groupby('chunk').mean().plot.bar(title='Label distributions by chunk', barmode='group')","metadata":{"execution":{"iopub.status.busy":"2021-11-19T17:43:28.505199Z","iopub.execute_input":"2021-11-19T17:43:28.506038Z","iopub.status.idle":"2021-11-19T17:43:28.648576Z","shell.execute_reply.started":"2021-11-19T17:43:28.505998Z","shell.execute_reply":"2021-11-19T17:43:28.647765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Of course, there's a good chance that we haven't got the right idea about how the samples to flip were selected, because our distribution of flip chance looks wrong, compared to what we plotted above:","metadata":{}},{"cell_type":"code","source":"df.assign(flips=df.new_target != df.target).groupby('chunk').flips.mean().plot.bar(title='flips by chunk')","metadata":{"execution":{"iopub.status.busy":"2021-11-19T17:46:50.172869Z","iopub.execute_input":"2021-11-19T17:46:50.173184Z","iopub.status.idle":"2021-11-19T17:46:50.268371Z","shell.execute_reply.started":"2021-11-19T17:46:50.173154Z","shell.execute_reply":"2021-11-19T17:46:50.267533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Notably, chunk 2 has only about 24% labels flipped in the real train set, which is incredibly unlikely to happen with the way I've been doing these flips. \n\nExpected label distributions in the test set\n==\n\nWe'll write this back out in case somebody wants to use it for anything. Here's the numbers for the train set -- these are known:","metadata":{}},{"cell_type":"code","source":"distribution","metadata":{"execution":{"iopub.status.busy":"2021-11-19T17:49:17.431058Z","iopub.execute_input":"2021-11-19T17:49:17.431793Z","iopub.status.idle":"2021-11-19T17:49:17.448103Z","shell.execute_reply.started":"2021-11-19T17:49:17.431753Z","shell.execute_reply":"2021-11-19T17:49:17.447544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here are the numbers for the test set, these are assumed:","metadata":{}},{"cell_type":"code","source":"projected = df.loc[df.chunk > 9].rename(columns={'target': 'old_y', 'new_target': 'new_y'})\nprojected = projected.groupby('chunk').mean().assign(\n    expected_dist=lambda df: .25 + .5 * df.old_y\n)\nprojected","metadata":{"execution":{"iopub.status.busy":"2021-11-19T17:52:27.722778Z","iopub.execute_input":"2021-11-19T17:52:27.723779Z","iopub.status.idle":"2021-11-19T17:52:27.766965Z","shell.execute_reply.started":"2021-11-19T17:52:27.723723Z","shell.execute_reply":"2021-11-19T17:52:27.766284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We'll write out both of these in case anybody wants to have a crack at using them for something:","metadata":{}},{"cell_type":"code","source":"pd.concat([distribution, projected], axis=0).reset_index().to_csv('label_distributions.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T17:53:35.527782Z","iopub.execute_input":"2021-11-19T17:53:35.528254Z","iopub.status.idle":"2021-11-19T17:53:35.540246Z","shell.execute_reply.started":"2021-11-19T17:53:35.528205Z","shell.execute_reply":"2021-11-19T17:53:35.53916Z"},"trusted":true},"execution_count":null,"outputs":[]}]}