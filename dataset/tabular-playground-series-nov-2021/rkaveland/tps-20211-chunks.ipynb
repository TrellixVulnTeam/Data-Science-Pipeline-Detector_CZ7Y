{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Chunk study\n==\n\nIn [this discussion](https://www.kaggle.com/c/tabular-playground-series-nov-2021/discussion/286731) thread,\nit's shown that the data is chunked -- there are 19 chunks, or sequences of samples have pretty sharp\nboundaries to the other samples.\n\nI've spent a few hours looking at it, and I can't find a way to exploit this information. But I\nmay have discovered something that somebody else could find a way to exploit, so why not make it public?\n\nFor this notebook, I'll be using a parquet dataset that I've made of the data, just to make it\nfaster to iterate. It contains both the train/test data in the same file, and they've both already\nbeen scaled with `sklearn.preprocessing.StandardScaler`. Let's load the data and start looking at\nit.","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"import numpy as np\nimport plotly.express as px\nimport pandas as pd\nimport seaborn as sns\nfrom tqdm.notebook import tqdm\nfrom sklearn import cluster, metrics, linear_model, model_selection, preprocessing\nimport xgboost\n\nsns.set(style='darkgrid', context='notebook', rc={\n    'figure.frameon': False, 'figure.figsize': (16, 12), 'legend.frameon': False\n})\n\npd.options.plotting.backend = 'plotly'\n\ndf = pd.read_parquet('../input/tps-nov-2021-parquet/data.pq').assign(\n    chunk=lambda df: df.id // 60000\n).drop(columns=['id']).sample(frac=1) # shuffle\n\ntrain = df.loc[df.target.notna()]\ntest = df.loc[df.target.isna()]\n\ndf.info()","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Significantly different target distribution\n==\n\nThis is mentioned in the discussion post already, let's quickly look:","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"train.groupby('chunk').target.mean().plot.bar(title='mean(y) by chunk')","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I think that if we gain anything from this finding at all, it's going to come from this\nfact. I still believe, from [this post](https://www.kaggle.com/c/tabular-playground-series-nov-2021/discussion/285503) that\naround 25% of the data must have had its label flipped. If you start out with `x = mean(target)`, then flip 25% of the\nlabels, you'd expect to end up with around `y = .25 + .5 * x`.\n\nIf the labels were flipped, but the features weren't, we should see some pretty massive differences between feature\ndistribution and label distribution in these chunks. If we're right about that, the original target distribution\nwould've been more like this:","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"y = train.groupby('chunk').target.mean()\n(2 * y - .5).plot.bar(title='mean(target) by chunk before label-flips')","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's compare:","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"pd.DataFrame({\n    'mean(target)_after': y,\n    'mean(target)_before': (2 * y - .5),\n}).plot.bar(barmode='group')","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I *believe* that there could be a way to use this to identify flipped labels in the training\ndata set -- and also possibly to say something about the target distribution in each test chunk.\n\nI've tried a number of approaches for this, but haven't really had any success.","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"markdown","source":"Significant differences in features\n==\n\nThe features differ significantly between chunks. This was already known in post that started the discussion thread linked to above, but here's one more way to easily see that.\n\nLet's grab two chunks, and make a classifier that can predict which chunk that a sample is from:","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"feats = df.columns[df.columns.str.startswith('f')]\n\ndef train_booster(left, right):\n    X, y = (\n        df.loc[df.chunk.isin({left, right}), feats],\n        df.loc[df.chunk.isin({left, right}), 'chunk'] == left\n    )\n\n    X_train, X_val, y_train, y_val = model_selection.train_test_split(X, y, test_size=.25, shuffle=True)\n\n    booster = xgboost.train(\n        params=dict(eta=.1, max_depth=4, objective='binary:logistic', eval_metric=['auc']),\n        dtrain=xgboost.DMatrix(X_train, label=y_train),\n        evals=[(xgboost.DMatrix(X_val, label=y_val), 'val')],\n        num_boost_round=200,\n        verbose_eval=100,\n    )\n\n    y_proba = booster.predict(xgboost.DMatrix(X_val))\n    auc = metrics.roc_auc_score(y_val, y_proba)\n    acc = metrics.accuracy_score(y_val, y_proba > .5)\n    print(f'Separate chunk {left} from chunk {right} with auc = {auc:.5f}, acc = {acc:.5}')\n    return booster, auc, acc\n\nbooster, auc, acc = train_booster(0, 1)","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The booster is pretty accurate here, although not perfect. This seems to work quite well\non all chunk boundaries, and also between chunks that are far away. Let's repeat:","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"booster_2, auc, acc = train_booster(3, 9)\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Also pretty solid. I've actually evaluated all chunk combinations and found that it's really\neasy to get around 95% accuracy for this kind of test, and we can get up to 98% or 99% in many\ncases, by tuning the classifier more.\n\nThese two boosters don't have the same idea of what the important features are:","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"fscore = booster.get_fscore()\nfscore = pd.DataFrame({'feature': fscore.keys(), 'value': fscore.values()})\nfscore_2 = booster_2.get_fscore()\nfscore_2 = pd.DataFrame({'feature': fscore_2.keys(), 'value': fscore_2.values()})\nfscore = pd.concat([fscore.assign(booster='first'), fscore_2.assign(booster='second')])\nfscore.plot.bar(x='feature', y='value', facet_row='booster')","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Linear models can't easily separate chunks\n==\n\nWe just saw XGBoost easily cruise through separating chunks with a binary label.\n\nHow about LogisticRegression?","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"left, right = 0, 1\n\nclf = linear_model.LogisticRegression()\nX, y = (\n    df.loc[df.chunk.isin({left, right}), feats],\n    df.loc[df.chunk.isin({left, right}), 'chunk'] == left\n)\n\nmodel_selection.cross_val_score(\n    clf, X, y, cv=model_selection.StratifiedKFold(5, shuffle=True)\n)","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It's not terrible, but clearly this is much easier for XGBoost. That's the opposite of what\nI've seen on the \"real problem\", where I've had a lot of success with linear models, and really\nstruggled to make the boosters work.\n\nFit on one chunk and predict another\n==\n\nThis one was pretty interesting to me. `LogisticRegression` does not care that much, which chunk it's\nfitted on:","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"left, right = 0, 1\n\ndef X(chunk):\n    return train.loc[train.chunk == chunk, feats]\n\ndef y(chunk):\n    return train.loc[train.chunk == chunk, 'target']\n\ndef Xy(chunk):\n    return X(chunk), y(chunk)\n\nclf = linear_model.LogisticRegression()\nclf.fit(*Xy(0)).score(*Xy(1)), clf.fit(*Xy(1)).score(*Xy(0))","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This seems to bed doing around 71% accuracy regardless. It does slightly better if it gets\nto see some data from both chunks:\n","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"model_selection.cross_val_score(\n    clf, pd.concat([X(0), X(1)]), pd.concat([y(0), y(1)]),\n    cv=model_selection.StratifiedKFold(2, shuffle=True)\n)","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"But it still does really well when predicting an unseen chunk.\n\nHere's one more example:","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"clf.fit(\n    pd.concat([X(0), X(1)]),\n    pd.concat([y(0), y(1)]),\n)\nclf.score(X(2), y(2))","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_selection.cross_val_score(\n    clf, pd.concat([X(0), X(1), X(2)]), pd.concat([y(0), y(1), y(2)]),\n    cv=model_selection.StratifiedKFold(3, shuffle=True)\n)","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"But we can use the features to separate the chunks from one another? Does that mean that\nwe're using... noise to do that?\n\nIs this why I'm not getting that much out of XGBoost in this competition? Maybe it's fitting\nthe noise that separates the chunks, to learn `mean(y)` for the different chunks? Because it\nseems to be much more able to fit that noise, than the other models I've tried here.\n\nAnalyzing label flips in chunks\n==\n\nRecall the distribution of labels in chunk 0 earlier:","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"train.loc[train.chunk == 0, 'target'].value_counts(normalize=True)","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I believe that if there was a label flip, the features should suggest there to be around\n35% positive samples, not 42.5%.","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"clf = linear_model.LogisticRegression()\nclf.fit(X(0), y(0))\npred = clf.predict(X(0))\nnp.mean(pred)","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check the next chunk:","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"train.loc[train.chunk == 1, 'target'].value_counts(normalize=True)","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Side note: This is basically the inverse chunk, I wonder if that's a coincidence?\n\nAnyway, I think these features should suggest around 66% positive samples:","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"clf = linear_model.LogisticRegression()\nclf.fit(X(1), y(1))\npred = clf.predict(X(1))\nnp.mean(pred)","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To me, this reinforces the notion that 25% of the labels would've been flipped. Because I\nthink that this LogisticRegression actually manages to fit almost all the data in the chunk.\n\nActually, let's do this exercise for each chunk and see how that lines up with our expectations\nthat the features distribute like `2 * mean(target) - .5`","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"y_mean = [\n    clf.fit(X(i), y(i)).predict(X(i)).mean()\n    for i in tqdm(range(train.chunk.nunique()))\n]\n\ny = train.groupby('chunk').target.mean()\n\npd.DataFrame({\n    'mean(target)_after': y,\n    'mean(target)_before': (2 * y - .5),\n    'mean(logistic_regression)': y_mean\n}).plot.bar(barmode='group')","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Is this some huge coincidence, or does this line up with what we'd expect to see if 25%\nof the labels had just been flipped?\n\nI *have* tried to flip the labels back, using a 74.83% out of fold accurate classifier,\nand the label distribution by chunk there looks almost like this.\n\nUnfortunately, I can still see no way to leverage any of this information to find out\nwhich labels in the test set that are affected.\n","metadata":{"pycharm":{"name":"#%% md\n"}}}]}