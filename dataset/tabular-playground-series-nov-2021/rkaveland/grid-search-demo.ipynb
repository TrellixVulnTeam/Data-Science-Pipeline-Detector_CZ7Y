{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Grid search\n==\n\nIt's the first half hour that you're looking a new problem. What's the first thing you should do?\n\nWell, that's exploratory data analysis, or EDA for short. To me, that involves doing a quick and dirty search for models that will work **while** I'm doing analysis. I'll do a quick check to look for missing data, if needed, I might impute or fill some of that. Then I'll start -- with the goal of getting a promising lead for a model I should pursue later.\n\nAt this point, I barely know the data at all. But it's better to start this job now, when I'm not waiting for it to finish, than later, when I have time to work, but don't know where to take it. So I think this is often a good idea for one of the first steps. Grid searches are not only for hyper parameter tuning, they're actually not bad for finding out where you should start.\n\nFirst, I'll install a more recent version of scikit-learn -- namely, I want one that has `HistGradientBoostingClassifier`. Just because that makes it super fast to set up an experiment that can check the classifiers that I'll most commonly want to try. So, here's that:","metadata":{}},{"cell_type":"code","source":"%pip install -U -q scikit-learn","metadata":{"execution":{"iopub.status.busy":"2021-11-11T22:02:06.876129Z","iopub.execute_input":"2021-11-11T22:02:06.876724Z","iopub.status.idle":"2021-11-11T22:02:23.804102Z","shell.execute_reply.started":"2021-11-11T22:02:06.876634Z","shell.execute_reply":"2021-11-11T22:02:23.802895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then, do lots of imports, set the most important seeds (for reproducible results), and read in the data. I also make sure to always use the same cross validation folds:","metadata":{}},{"cell_type":"code","source":"import random\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn import (\n    linear_model,\n    neural_network,\n    model_selection,\n    pipeline,\n    preprocessing,\n    svm,\n    ensemble,\n    metrics\n)\n\nrandom.seed(42)\nnp.random.seed(42)\nfolds = model_selection.StratifiedKFold(5, shuffle=True, random_state=42)\n\ndf = pd.read_csv(\n    '../input/tabular-playground-series-nov-2021/train.csv', dtype=np.float32\n).astype({'id': np.int32})","metadata":{"execution":{"iopub.status.busy":"2021-11-11T22:17:28.513954Z","iopub.execute_input":"2021-11-11T22:17:28.514264Z","iopub.status.idle":"2021-11-11T22:17:37.73255Z","shell.execute_reply.started":"2021-11-11T22:17:28.514219Z","shell.execute_reply":"2021-11-11T22:17:37.731705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The next thing I do, if I know the data has no glaring, huge, problems, is that I set up a `sklearn.pipeline.Pipeline` and verify that it works by running it in a `cross_val_score`","metadata":{}},{"cell_type":"code","source":"clf = pipeline.Pipeline([\n    ('scaler', preprocessing.StandardScaler()), # name of the step to the left\n    ('clf', linear_model.LogisticRegression()), # name of the step to the left\n])\n\nX = df.drop(columns=['id', 'target'])\ny = df.target\n\nmodel_selection.cross_val_score(clf, X, y, cv=folds)","metadata":{"execution":{"iopub.status.busy":"2021-11-11T22:18:00.053683Z","iopub.execute_input":"2021-11-11T22:18:00.054477Z","iopub.status.idle":"2021-11-11T22:18:12.128237Z","shell.execute_reply.started":"2021-11-11T22:18:00.054428Z","shell.execute_reply":"2021-11-11T22:18:12.127129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`LogisticRegression` with `StandardScaler` is very often a good first choice for tabular problems that are already numeric. It's fast to evaluate and can often get you decent models. With categorical features involved, I might try some kind of tree first, since it's fast to get started.\n\nSetting up a GridSearchCV\n==\n\nOnce you've a pipeline that works, it's super easy to have `sklearn.model_selection.GridSearchCV` try replacing parts of the pipeline for other parts, to find the combination of parts that work best.\n\nThis is fully automatic, and fully exhaustive. Eg. if your grid contains 3 options, with 4 choices each, that's 4 x 4 x 4 combinations that it'll search. So this could take a while. Which is a good reason to start the job early on, while you have other things to do than wait for it!\n\nUsually you should try a few different preprocessing steps. I tried to keep the clutter out of this demo, though.","metadata":{}},{"cell_type":"code","source":"grid = [ # A grid can be a single dict, or a list of dicts\n    dict(\n        # All combinations of options in this dict will be tried together\n        # 3 here -- 1 scaler times 3 different models\n        scaler=[preprocessing.StandardScaler()], # name of the pipeline step!\n        clf=[ # name of the pipeline step!\n            # These classifiers prefer centered input, so they work well with StandardScaler\n            svm.LinearSVC(), \n            linear_model.LogisticRegression(),\n            neural_network.MLPClassifier(early_stopping=True, hidden_layer_sizes=(32, 8)),\n        ]\n    ),\n    dict( \n        # You can pass as many dicts as you'd like, \n        # let's try using different scalers with SGDClassifier\n        scaler=[preprocessing.StandardScaler(), preprocessing.MinMaxScaler()],\n        clf=[linear_model.SGDClassifier()],\n        clf__loss=['hinge', 'log'], # run SGD twice, once with hinge-loss and once with logloss\n    ),\n    dict(\n        scaler=[None], # These two don't care about scaling, so we can run it without\n        clf=[ensemble.RandomForestClassifier(), ensemble.HistGradientBoostingClassifier()]\n    )\n]\ngrid","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-11T22:33:57.549679Z","iopub.execute_input":"2021-11-11T22:33:57.550344Z","iopub.status.idle":"2021-11-11T22:33:57.562416Z","shell.execute_reply.started":"2021-11-11T22:33:57.550301Z","shell.execute_reply":"2021-11-11T22:33:57.561608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can use the same mechanism to pass different kinds of options to the different classifiers -- for SGD, we're trying two different loss functions here. Now we can set up the GridSearchCV estimator:","metadata":{}},{"cell_type":"code","source":"gridsearch = model_selection.GridSearchCV(\n    clf, # the pipeline\n    grid, # the grid,\n    cv=folds, # the folds\n    scoring='roc_auc', # optionally the scoring\n    verbose=True, # If you want more output\n)","metadata":{"execution":{"iopub.status.busy":"2021-11-11T22:34:32.740879Z","iopub.execute_input":"2021-11-11T22:34:32.741753Z","iopub.status.idle":"2021-11-11T22:34:32.746647Z","shell.execute_reply.started":"2021-11-11T22:34:32.741711Z","shell.execute_reply":"2021-11-11T22:34:32.746076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And that's just a normal estimator, so we use it by calling `.fit()`. This will take a while, depending on how many models you asked it to check:","metadata":{}},{"cell_type":"code","source":"%time gridsearch.fit(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-11-11T22:35:56.654561Z","iopub.execute_input":"2021-11-11T22:35:56.655474Z","iopub.status.idle":"2021-11-11T22:40:41.364434Z","shell.execute_reply.started":"2021-11-11T22:35:56.655428Z","shell.execute_reply":"2021-11-11T22:40:41.363498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Once fitted, `GridSearchCV` has a convenient `cv_results_` object that can be turned into a `pd.DataFrame` and insepected:","metadata":{}},{"cell_type":"code","source":"pd.DataFrame(gridsearch.cv_results_)","metadata":{"execution":{"iopub.status.busy":"2021-11-11T22:40:41.366648Z","iopub.execute_input":"2021-11-11T22:40:41.36729Z","iopub.status.idle":"2021-11-11T22:40:41.444849Z","shell.execute_reply.started":"2021-11-11T22:40:41.367223Z","shell.execute_reply":"2021-11-11T22:40:41.443962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And you can easily retrieve the best estimator it found:","metadata":{}},{"cell_type":"code","source":"chosen = gridsearch.best_estimator_\nchosen, chosen.get_params()","metadata":{"execution":{"iopub.status.busy":"2021-11-11T22:45:12.71341Z","iopub.execute_input":"2021-11-11T22:45:12.713691Z","iopub.status.idle":"2021-11-11T22:45:12.72163Z","shell.execute_reply.started":"2021-11-11T22:45:12.713663Z","shell.execute_reply":"2021-11-11T22:45:12.72068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And that's how I ended up spending more energy on neural networks (MLPClassifier is a simple NN), than on gradient boosters this time around.","metadata":{}},{"cell_type":"code","source":"%time chosen.fit(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-11-11T22:46:49.903696Z","iopub.execute_input":"2021-11-11T22:46:49.904396Z","iopub.status.idle":"2021-11-11T22:46:52.182626Z","shell.execute_reply.started":"2021-11-11T22:46:49.904345Z","shell.execute_reply":"2021-11-11T22:46:52.18177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = pd.read_csv(\n    '../input/tabular-playground-series-nov-2021/test.csv', dtype=np.float32\n).astype({'id': np.int32})\nX_test = df_test.drop(columns=['id'])\ndf_test[['id']].assign(\n    target=chosen.predict_proba(X_test)[:, 1]\n).to_csv('submission.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]}]}