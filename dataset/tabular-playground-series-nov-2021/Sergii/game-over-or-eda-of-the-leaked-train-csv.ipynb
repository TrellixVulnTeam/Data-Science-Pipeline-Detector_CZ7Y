{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## How did I get this data?","metadata":{}},{"cell_type":"markdown","source":"By some random chance, I downloaded `train.csv.zip` in the first hour when competition just started. For several days I didn't check notebooks and forum not to spoil a pleasure of diving into the data.\n\nThis dataset was a bizzare one from the start. I was totally buffled and finally went to forum and checked notebooks of others to understand what's wrong. My code worked differently local and on Kaggle.\n\nAfter downloading data again and comparing it with my local version I realized that my `train.csv` was the original version without inversed labels.\n\nMost likely organizers made a mistake and uploaded `train.csv.zip` different from the one in the batch data file and didn't noticed the error at the beginning(the mistake was fixed later).\n\nWithout further ado, let's dive in.","metadata":{}},{"cell_type":"markdown","source":"## Loading Data","metadata":{}},{"cell_type":"markdown","source":"Let's load some basic libraries and both train sets. I uploaded original/pure version of train.csv into a dataset on Kaggle:","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2021-11-11T21:50:01.075027Z","iopub.execute_input":"2021-11-11T21:50:01.075578Z","iopub.status.idle":"2021-11-11T21:50:02.451628Z","shell.execute_reply.started":"2021-11-11T21:50:01.075455Z","shell.execute_reply":"2021-11-11T21:50:02.45026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATA_PATH = '../input/tabular-playground-series-nov-2021'\nPURE_DATA_PATH = '../input/november21'","metadata":{"execution":{"iopub.status.busy":"2021-11-11T21:51:06.403543Z","iopub.execute_input":"2021-11-11T21:51:06.404462Z","iopub.status.idle":"2021-11-11T21:51:06.409998Z","shell.execute_reply.started":"2021-11-11T21:51:06.404384Z","shell.execute_reply":"2021-11-11T21:51:06.408578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dtype = {f'f{i}': 'float32' for i in range(100)}\ntrain_dtype = {**test_dtype, 'target': 'int8'}","metadata":{"execution":{"iopub.status.busy":"2021-11-11T21:51:13.193736Z","iopub.execute_input":"2021-11-11T21:51:13.194043Z","iopub.status.idle":"2021-11-11T21:51:13.199449Z","shell.execute_reply.started":"2021-11-11T21:51:13.19401Z","shell.execute_reply":"2021-11-11T21:51:13.19868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_csv = pd.read_csv(f'{DATA_PATH}/train.csv', index_col='id', dtype=train_dtype)","metadata":{"execution":{"iopub.status.busy":"2021-11-11T21:51:22.053219Z","iopub.execute_input":"2021-11-11T21:51:22.053527Z","iopub.status.idle":"2021-11-11T21:51:39.041209Z","shell.execute_reply.started":"2021-11-11T21:51:22.053494Z","shell.execute_reply":"2021-11-11T21:51:39.040008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pure_csv = pd.read_csv(f'{PURE_DATA_PATH}/train.csv', index_col='id', dtype=train_dtype)","metadata":{"execution":{"iopub.status.busy":"2021-11-11T21:51:39.043629Z","iopub.execute_input":"2021-11-11T21:51:39.044014Z","iopub.status.idle":"2021-11-11T21:51:55.09155Z","shell.execute_reply.started":"2021-11-11T21:51:39.043969Z","shell.execute_reply":"2021-11-11T21:51:55.090092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Comparing Datasets","metadata":{}},{"cell_type":"markdown","source":"On the first glance both files look pretty similar:","metadata":{}},{"cell_type":"code","source":"train_csv.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-11T21:51:55.093914Z","iopub.execute_input":"2021-11-11T21:51:55.094286Z","iopub.status.idle":"2021-11-11T21:51:55.140879Z","shell.execute_reply.started":"2021-11-11T21:51:55.094239Z","shell.execute_reply":"2021-11-11T21:51:55.139537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pure_csv.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-11T21:51:55.143494Z","iopub.execute_input":"2021-11-11T21:51:55.144278Z","iopub.status.idle":"2021-11-11T21:51:55.175499Z","shell.execute_reply.started":"2021-11-11T21:51:55.14422Z","shell.execute_reply":"2021-11-11T21:51:55.174258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's confirm that they differ:","metadata":{}},{"cell_type":"code","source":"train_csv.equals(pure_csv)","metadata":{"execution":{"iopub.status.busy":"2021-11-11T21:51:59.830585Z","iopub.execute_input":"2021-11-11T21:51:59.830965Z","iopub.status.idle":"2021-11-11T21:52:00.090538Z","shell.execute_reply.started":"2021-11-11T21:51:59.830929Z","shell.execute_reply":"2021-11-11T21:52:00.089852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's verify that features are same:","metadata":{}},{"cell_type":"code","source":"train_csv.drop('target', axis=1).equals(pure_csv.drop('target', axis=1))","metadata":{"execution":{"iopub.status.busy":"2021-11-11T21:52:04.137809Z","iopub.execute_input":"2021-11-11T21:52:04.138833Z","iopub.status.idle":"2021-11-11T21:52:04.549125Z","shell.execute_reply.started":"2021-11-11T21:52:04.138771Z","shell.execute_reply":"2021-11-11T21:52:04.548035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"and labels differ:","metadata":{}},{"cell_type":"code","source":"y_train = train_csv['target']\ny_pure = pure_csv['target']\n\ny_train.equals(y_pure)","metadata":{"execution":{"iopub.status.busy":"2021-11-11T21:52:08.751594Z","iopub.execute_input":"2021-11-11T21:52:08.751945Z","iopub.status.idle":"2021-11-11T21:52:08.765637Z","shell.execute_reply.started":"2021-11-11T21:52:08.751912Z","shell.execute_reply":"2021-11-11T21:52:08.764799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And the difference is about `25%`:","metadata":{}},{"cell_type":"code","source":"(y_train != y_pure).sum() / y_train.shape[0]","metadata":{"execution":{"iopub.status.busy":"2021-11-11T21:52:11.986906Z","iopub.execute_input":"2021-11-11T21:52:11.987973Z","iopub.status.idle":"2021-11-11T21:52:11.998252Z","shell.execute_reply.started":"2021-11-11T21:52:11.987904Z","shell.execute_reply":"2021-11-11T21:52:11.997494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, all in all, looks like Kagglers already knew the truth.","metadata":{}},{"cell_type":"markdown","source":"## Baseline Models","metadata":{}},{"cell_type":"markdown","source":"Let's rescale our data to fit models on pure and train labels. We don't need to have `X_pure`, because features are same:","metadata":{}},{"cell_type":"code","source":"scaler = StandardScaler()\nX_train = scaler.fit_transform(train_csv.drop('target', axis=1))","metadata":{"execution":{"iopub.status.busy":"2021-11-11T21:52:31.141081Z","iopub.execute_input":"2021-11-11T21:52:31.142059Z","iopub.status.idle":"2021-11-11T21:52:32.556128Z","shell.execute_reply.started":"2021-11-11T21:52:31.141992Z","shell.execute_reply":"2021-11-11T21:52:32.555109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now the first surprise to me was this.","metadata":{}},{"cell_type":"code","source":"pure_model = LogisticRegression(random_state=83).fit(X_train, y_pure)\n\ny_pure_pred = pure_model.predict_proba(X_train)[:, 1]\nroc_auc_score(y_pure, y_pure_pred)","metadata":{"execution":{"iopub.status.busy":"2021-11-11T21:52:35.427255Z","iopub.execute_input":"2021-11-11T21:52:35.427563Z","iopub.status.idle":"2021-11-11T21:52:41.630907Z","shell.execute_reply.started":"2021-11-11T21:52:35.427531Z","shell.execute_reply":"2021-11-11T21:52:41.630141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"While we didn't use train/test split(we will get to it), there are `600_000` points with `100` features and a dumb `LogisticRegression` was able to successfully split this huge blob of data with `101` params. Really, not bad result!\n\nOk, let's see what would we get with the official version:","metadata":{}},{"cell_type":"code","source":"train_model = LogisticRegression(random_state=83).fit(X_train, y_train)\n\ny_train_pred = train_model.predict_proba(X_train)[:, 1]\nroc_auc_score(y_train, y_train_pred)","metadata":{"execution":{"iopub.status.busy":"2021-11-11T21:52:59.755781Z","iopub.execute_input":"2021-11-11T21:52:59.757023Z","iopub.status.idle":"2021-11-11T21:53:02.512723Z","shell.execute_reply.started":"2021-11-11T21:52:59.756965Z","shell.execute_reply":"2021-11-11T21:53:02.511577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Pretty close to a common score on the Leaderboard.\n\nLet's check what would be a score of pure-labels model on mixed labels:","metadata":{}},{"cell_type":"code","source":"roc_auc_score(y_train, pure_model.predict_proba(X_train)[:, 1])","metadata":{"execution":{"iopub.status.busy":"2021-11-11T21:53:18.292609Z","iopub.execute_input":"2021-11-11T21:53:18.292998Z","iopub.status.idle":"2021-11-11T21:53:18.982597Z","shell.execute_reply.started":"2021-11-11T21:53:18.292961Z","shell.execute_reply":"2021-11-11T21:53:18.981372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Not a big difference. Most likely due to mixed labels, `train_model` is less confident in its predictions as `pure_model`. Let's verify.","metadata":{}},{"cell_type":"code","source":"plt.hist(y_pure_pred, bins=100);","metadata":{"execution":{"iopub.status.busy":"2021-11-11T21:53:29.268361Z","iopub.execute_input":"2021-11-11T21:53:29.269385Z","iopub.status.idle":"2021-11-11T21:53:29.977089Z","shell.execute_reply.started":"2021-11-11T21:53:29.269334Z","shell.execute_reply":"2021-11-11T21:53:29.975857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.hist(y_train_pred, bins=100);","metadata":{"execution":{"iopub.status.busy":"2021-11-11T21:53:32.438898Z","iopub.execute_input":"2021-11-11T21:53:32.439256Z","iopub.status.idle":"2021-11-11T21:53:32.871263Z","shell.execute_reply.started":"2021-11-11T21:53:32.439222Z","shell.execute_reply":"2021-11-11T21:53:32.870194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we see, pure-data model is very confident, while mixed-data model is not at all.","metadata":{}},{"cell_type":"markdown","source":"## Train-Test Split","metadata":{}},{"cell_type":"markdown","source":"To be impeccable, let's do a simple train-test split to remove any chance for mistake. We will do `80/20` train/validation split. To make everything simpler, we will have a helper function to make all data lifting, so we can focus on the fun part.","metadata":{}},{"cell_type":"code","source":"def split_train_and_validate(name, X, y, test_size):\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=test_size, random_state=83)\n    \n    model = LogisticRegression(random_state=83).fit(X_train, y_train)\n    \n    train_score = roc_auc_score(y_train, model.predict_proba(X_train)[:, 1])\n    valid_score = roc_auc_score(y_valid, model.predict_proba(X_valid)[:, 1])\n\n    print(f'{name} Train size: {len(y_train)} - {((len(y_train) / len(y) * 100)):.2f}%')\n    print(f'{name} Valid size: {len(y_valid)} - {((len(y_valid) / len(y) * 100)):.2f}%\\n')\n    \n    print(f'{name} Train score: {train_score}')\n    print(f'{name} Valid score: {valid_score}')\n    \n    return train_score, valid_score","metadata":{"execution":{"iopub.status.busy":"2021-11-11T21:54:07.691638Z","iopub.execute_input":"2021-11-11T21:54:07.692025Z","iopub.status.idle":"2021-11-11T21:54:07.700752Z","shell.execute_reply.started":"2021-11-11T21:54:07.691984Z","shell.execute_reply":"2021-11-11T21:54:07.699498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ok, and now run it with pure labels:","metadata":{}},{"cell_type":"code","source":"split_train_and_validate('Pure', X_train, y_pure, test_size=0.2);","metadata":{"execution":{"iopub.status.busy":"2021-11-11T21:54:12.710438Z","iopub.execute_input":"2021-11-11T21:54:12.711414Z","iopub.status.idle":"2021-11-11T21:54:18.889277Z","shell.execute_reply.started":"2021-11-11T21:54:12.711357Z","shell.execute_reply":"2021-11-11T21:54:18.887946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"That's a bit buffling. We removed `20%` of the data but the score is still perfect.\n\nLet's remove half of the data, and verify again:","metadata":{}},{"cell_type":"code","source":"split_train_and_validate('Pure', X_train, y_pure, test_size=0.5);","metadata":{"execution":{"iopub.status.busy":"2021-11-11T21:54:29.36154Z","iopub.execute_input":"2021-11-11T21:54:29.361929Z","iopub.status.idle":"2021-11-11T21:54:33.848758Z","shell.execute_reply.started":"2021-11-11T21:54:29.361892Z","shell.execute_reply":"2021-11-11T21:54:33.847963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ok, still too good to be true. Maybe `20/80` will make it fail?","metadata":{}},{"cell_type":"code","source":"split_train_and_validate('Pure', X_train, y_pure, test_size=0.8);","metadata":{"execution":{"iopub.status.busy":"2021-11-11T21:54:37.589661Z","iopub.execute_input":"2021-11-11T21:54:37.590843Z","iopub.status.idle":"2021-11-11T21:54:40.406349Z","shell.execute_reply.started":"2021-11-11T21:54:37.590786Z","shell.execute_reply":"2021-11-11T21:54:40.405056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Hmm, what about `1/99`?","metadata":{}},{"cell_type":"code","source":"split_train_and_validate('Pure', X_train, y_pure, test_size=0.99);","metadata":{"execution":{"iopub.status.busy":"2021-11-11T21:54:44.388308Z","iopub.execute_input":"2021-11-11T21:54:44.388656Z","iopub.status.idle":"2021-11-11T21:54:46.274489Z","shell.execute_reply.started":"2021-11-11T21:54:44.388623Z","shell.execute_reply":"2021-11-11T21:54:46.273319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally on `600` training points we see some significant drop on the validation set:","metadata":{}},{"cell_type":"code","source":"split_train_and_validate('Pure', X_train, y_pure, test_size=0.999);","metadata":{"execution":{"iopub.status.busy":"2021-11-11T21:55:26.086841Z","iopub.execute_input":"2021-11-11T21:55:26.087292Z","iopub.status.idle":"2021-11-11T21:55:27.544386Z","shell.execute_reply.started":"2021-11-11T21:55:26.087242Z","shell.execute_reply":"2021-11-11T21:55:27.543151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"But even with `300` points the score is still very good:","metadata":{}},{"cell_type":"code","source":"split_train_and_validate('Pure', X_train, y_pure, test_size=0.9995);","metadata":{"execution":{"iopub.status.busy":"2021-11-11T21:55:29.220069Z","iopub.execute_input":"2021-11-11T21:55:29.220433Z","iopub.status.idle":"2021-11-11T21:55:30.697481Z","shell.execute_reply.started":"2021-11-11T21:55:29.220395Z","shell.execute_reply":"2021-11-11T21:55:30.69629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Mixed labels validation","metadata":{}},{"cell_type":"markdown","source":"To wrap up our experiment, let's check briefly mixed labels.","metadata":{}},{"cell_type":"code","source":"split_train_and_validate('Mixed', X_train, y_train, test_size=0.2); print(\"\\n\")\nsplit_train_and_validate('Mixed', X_train, y_train, test_size=0.99); print(\"\\n\")\nsplit_train_and_validate('Mixed', X_train, y_train, test_size=0.999); print(\"\\n\")\nsplit_train_and_validate('Mixed', X_train, y_train, test_size=0.9995);","metadata":{"execution":{"iopub.status.busy":"2021-11-11T21:55:37.629109Z","iopub.execute_input":"2021-11-11T21:55:37.629475Z","iopub.status.idle":"2021-11-11T21:55:46.442254Z","shell.execute_reply.started":"2021-11-11T21:55:37.629433Z","shell.execute_reply":"2021-11-11T21:55:46.441104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Mixed labels models are more sensitive to amount of data due to random mutations and fall short after `600` points gap.","metadata":{}},{"cell_type":"markdown","source":"## ROC Curves","metadata":{}},{"cell_type":"markdown","source":"To wrap our EDA part, let's check ROC curves for both labels.\n\nAt first pure-label model output:","metadata":{}},{"cell_type":"code","source":"fpr, tpr, thresholds = roc_curve(y_pure, y_pure_pred)\nplt.plot(fpr, tpr);","metadata":{"execution":{"iopub.status.busy":"2021-11-11T21:56:01.148771Z","iopub.execute_input":"2021-11-11T21:56:01.149389Z","iopub.status.idle":"2021-11-11T21:56:01.474252Z","shell.execute_reply.started":"2021-11-11T21:56:01.149348Z","shell.execute_reply":"2021-11-11T21:56:01.473016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"and mixed-label model:","metadata":{}},{"cell_type":"code","source":"fpr, tpr, thresholds = roc_curve(y_train, y_train_pred)\nplt.plot(fpr, tpr);","metadata":{"execution":{"iopub.status.busy":"2021-11-11T21:56:04.20183Z","iopub.execute_input":"2021-11-11T21:56:04.202169Z","iopub.status.idle":"2021-11-11T21:56:04.610498Z","shell.execute_reply.started":"2021-11-11T21:56:04.202134Z","shell.execute_reply":"2021-11-11T21:56:04.609616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Actually, the curve for mixed-label model looks a bit odd, usually it does not have such a linear shape. Next time, I'll check ROC curve shape, before trying to improve my model score.","metadata":{}},{"cell_type":"markdown","source":"## Conclusions or why Game Over?","metadata":{}},{"cell_type":"markdown","source":"Apparently, this dataset was generated in too good to be any close to real dataset thing. As a consequence, all tries to improve the score with neural networks, trees, ensembles and other more complex approaches do not make any sense.\n\nThe distribution is too simple and more complex models would not give any significant boost over basic approaches. It looks like a race to overfit better by chance.\n\nPure training set is quite odd, because it does not leave you any room for improvement on cross-validation. How can be improved something, when you have `99.99%` score with 1000 points?\n\nMixed training set is the same, it just makes you believe you can do better.","metadata":{}},{"cell_type":"markdown","source":"## What if Remastered over Synthetic?","metadata":{}},{"cell_type":"markdown","source":"TPS Kaggle series is a really cool thing to tinker with tabular data. But using GANs and other data generation techniques leaves us some unrealistic taste.\n\nBut Kaggle already has dozens of cool tabular datasets accrued through the long history. What if to take these old competitions data and start a new challenge? Yes, you can go and play with it by yourself. But it feels a bit lonely without live comments and new kernels.\n\nAlso, a lot of old competitions were run when we didn't have NNs, XGB and lightGMB. What if we can do better than the old Grand Masters?\n\nThere are some tiny details to address - like prohibiting submission in the old competitions for the time of remastered one, but this is a doable thing.","metadata":{}},{"cell_type":"markdown","source":"## PS - Let's try pure data model?","metadata":{}},{"cell_type":"markdown","source":"As a final step let's make a most stupid model submission and cross our fingers. What if this is the lucky winner?","metadata":{}},{"cell_type":"code","source":"test_csv = pd.read_csv(f'{DATA_PATH}/test.csv', index_col='id')\nsample_submission_csv = pd.read_csv(f'{DATA_PATH}/sample_submission.csv', index_col='id')","metadata":{"execution":{"iopub.status.busy":"2021-11-11T22:09:30.337399Z","iopub.execute_input":"2021-11-11T22:09:30.337805Z","iopub.status.idle":"2021-11-11T22:09:40.699287Z","shell.execute_reply.started":"2021-11-11T22:09:30.337761Z","shell.execute_reply":"2021-11-11T22:09:40.698117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test = scaler.transform(test_csv)\n\ny_test_pred = pure_model.predict_proba(X_test)[:, 1]","metadata":{"execution":{"iopub.status.busy":"2021-11-11T22:09:40.701344Z","iopub.execute_input":"2021-11-11T22:09:40.701867Z","iopub.status.idle":"2021-11-11T22:09:41.295089Z","shell.execute_reply.started":"2021-11-11T22:09:40.701827Z","shell.execute_reply":"2021-11-11T22:09:41.293932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission_csv['target'] = y_test_pred\nsample_submission_csv.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-11T22:09:41.296846Z","iopub.execute_input":"2021-11-11T22:09:41.298078Z","iopub.status.idle":"2021-11-11T22:09:41.32086Z","shell.execute_reply.started":"2021-11-11T22:09:41.298019Z","shell.execute_reply":"2021-11-11T22:09:41.319856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission_csv.to_csv(f'./submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-11-11T22:10:13.56873Z","iopub.execute_input":"2021-11-11T22:10:13.56911Z","iopub.status.idle":"2021-11-11T22:10:15.831039Z","shell.execute_reply.started":"2021-11-11T22:10:13.569071Z","shell.execute_reply":"2021-11-11T22:10:15.829904Z"},"trusted":true},"execution_count":null,"outputs":[]}]}