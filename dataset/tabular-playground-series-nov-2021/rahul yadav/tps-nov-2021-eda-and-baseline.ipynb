{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<b>Problem Statement:</b>  The dataset is used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting <b>identifying spam emails</b> via various extracted features from the email. We have to predict molecule response probability.\n\n<b>Problem type:</b> A binary classification problem.\n\n<b>Evaluation matrix:</b> Submissions are evaluated on area under the <b>ROC(receiver operating characteristic)</b> curve between the predicted probability and the observed target.","metadata":{"papermill":{"duration":0.054456,"end_time":"2021-10-17T05:47:08.684936","exception":false,"start_time":"2021-10-17T05:47:08.63048","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<h2 id=\"Approach\">Approach to the problem</h2>\nIdea is to develop a generalized approach for solving any binary classification problem\n<ol>\n    <li>Performing exploratory data analysis (EDA) and Data Preparation (DP).</li>\n    <ol>\n        <li><a href=\"#FeatureSummary\">Understanding Train and Test dataset features (EDA)</a></li>\n        <li><a href=\"#Downcasting\">Down Casting Training and Testing datasets (DP)</a></li>\n        <li><a href=\"#Target\">Understanding Target feature distribution (EDA)</a></li>\n        <li><a href=\"#Corr\">Correlation check (EDA)</a></li>\n        <li><a href=\"#TrainVisual\">Visualizing Training dataset (EDA)</a></li>\n        <li><a href=\"#Normalizing\">Normalizing dataset (DP)</a></li>\n    </ol>\n    <li>Feature Engineering.</li>\n    <ol>\n        <li><a href=\"#AggFeatures\">Creating Aggregated features</a></li>\n    </ol>\n    <li>Training Linear,Gradient Boost and Ensemble models.</li>\n    <ol>\n        <li><a href=\"#LogisticRegression\">Logistic Regression</a></li>\n        <li><a href=\"#Ridge\">Ridge Classifier</a></li>\n        <li><a href=\"#LGBM\">LGBM Classification</a></li>\n    </ol>\n    <li><a href=\"#Blending\">Blending</a></li>\n    <li><a href=\"#Stacking\">Stacking</a></li>\n   </ol>\n\n<h4>Observations</h4>\n<ul>\n    <li>Linear models performing better than gradient boost models. Best <b>Logistic Regression Public Score of 0.74553</b> submitted by this notebook </li>\n    <li>Can't find a blending score better than single model Linear Regression score.</li>\n    <li>Achieved best <b>Stacking Public Score of 0.74635</b> by Stacking Linear Regression and Ridge outputs to build Catboost model.</li>\n   \n</ul>","metadata":{"papermill":{"duration":0.051162,"end_time":"2021-10-17T05:47:08.790698","exception":false,"start_time":"2021-10-17T05:47:08.739536","status":"completed"},"tags":[]}},{"cell_type":"code","source":"#REQUIRED LIBRARIES\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport gc\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom catboost import CatBoostClassifier\nfrom sklearn.linear_model import LogisticRegression,RidgeClassifier,Lasso\n# from sklearn.model_selection import GridSearchCV\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import roc_auc_score,accuracy_score\nfrom sklearn.model_selection import KFold,StratifiedKFold\nfrom sklearn.cluster import KMeans\nfrom sklearn.ensemble import AdaBoostClassifier,RandomForestClassifier\nimport lightgbm as lgb\nfrom sklearn.utils.extmath import softmax\n# import pickle\n# from sklearn.externals import joblib\n\n# import pandas_profiling as pp\n\nwarnings.filterwarnings('ignore')\ngc.enable()\n%matplotlib inline","metadata":{"_kg_hide-input":true,"papermill":{"duration":2.915835,"end_time":"2021-10-17T05:47:11.767567","exception":false,"start_time":"2021-10-17T05:47:08.851732","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-08T04:52:19.038378Z","iopub.execute_input":"2021-11-08T04:52:19.039014Z","iopub.status.idle":"2021-11-08T04:52:21.492553Z","shell.execute_reply.started":"2021-11-08T04:52:19.038904Z","shell.execute_reply":"2021-11-08T04:52:21.491542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#CHECKING ALL AVAILABLE FILES\npath='/kaggle/input/tabular-playground-series-nov-2021/'\ndata_files=list(os.listdir(path))\ndf_files=pd.DataFrame(data_files,columns=['file_name'])\ndf_files['size_in_gb']=df_files.file_name.apply(lambda x: round(os.path.getsize(path+x)/(1024*1024*1024),4))\ndf_files['type']=df_files.file_name.apply(lambda x:'file' if os.path.isfile(path+x) else 'directory')\ndf_files['file_count']=df_files[['file_name','type']].apply(lambda x: 0 if x['type']=='file' else len(os.listdir(path+x['file_name'])),axis=1)\n\nprint('Following files are available under path:',path)\ndisplay(df_files)","metadata":{"papermill":{"duration":0.101594,"end_time":"2021-10-17T05:47:11.922077","exception":false,"start_time":"2021-10-17T05:47:11.820483","status":"completed"},"tags":[],"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-08T04:52:28.887361Z","iopub.execute_input":"2021-11-08T04:52:28.887703Z","iopub.status.idle":"2021-11-08T04:52:28.929962Z","shell.execute_reply.started":"2021-11-08T04:52:28.887669Z","shell.execute_reply":"2021-11-08T04:52:28.929353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ALL CUSTOM FUNCTIONS\n\n#FUNCTION FOR PROVIDING FEATURE SUMMARY\ndef feature_summary(df_fa):\n    print('DataFrame shape')\n    print('rows:',df_fa.shape[0])\n    print('cols:',df_fa.shape[1])\n    col_list=['null','unique_count','data_type','max/min','mean','median','mode','std','skewness','sample_values']\n    df=pd.DataFrame(index=df_fa.columns,columns=col_list)\n    df['null']=list([len(df_fa[col][df_fa[col].isnull()]) for i,col in enumerate(df_fa.columns)])\n    #df['%_Null']=list([len(df_fa[col][df_fa[col].isnull()])/df_fa.shape[0]*100 for i,col in enumerate(df_fa.columns)])\n    df['unique_count']=list([len(df_fa[col].unique()) for i,col in enumerate(df_fa.columns)])\n    df['data_type']=list([df_fa[col].dtype for i,col in enumerate(df_fa.columns)])\n    for i,col in enumerate(df_fa.columns):\n        if 'float' in str(df_fa[col].dtype) or 'int' in str(df_fa[col].dtype):\n            df.at[col,'max/min']=str(round(df_fa[col].max(),2))+'/'+str(round(df_fa[col].min(),2))\n            df.at[col,'mean']=round(df_fa[col].mean(),4)\n            df.at[col,'median']=round(df_fa[col].median(),4)\n            df.at[col,'mode']=round(df_fa[col].mode()[0],4)\n            df.at[col,'std']=round(df_fa[col].std(),4)\n            df.at[col,'skewness']=round(df_fa[col].skew(),4)\n        elif 'datetime64[ns]' in str(df_fa[col].dtype):\n            df.at[col,'max/min']=str(df_fa[col].max())+'/'+str(df_fa[col].min())\n        df.at[col,'sample_values']=list(df_fa[col].unique())\n    display(df_fa.head())      \n    return(df.fillna('-'))\n\n\ndef feature_compare(df_fa,df_ft):\n    print('Train DataFrame shape')\n    print('rows:',df_fa.shape[0])\n    print('cols:',df_fa.shape[1])\n    \n    print('Test DataFrame shape')\n    print('rows:',df_ft.shape[0])\n    print('cols:',df_ft.shape[1])\n    \n    col_list=['null','unique_count','data_type','max/min','mean','median','mode','std','skewness','sample_values']\n    df=pd.DataFrame(index=pd.MultiIndex.from_product([df_train.columns,['train','test']],names=['features','dataset']),columns=col_list)\n   \n    df.loc[(slice(None),['train']),'null']=list([len(df_fa[col][df_fa[col].isnull()]) for i,col in enumerate(df_fa.columns)])\n    df.loc[(slice(None),['test']),'null']=list([len(df_ft[col][df_ft[col].isnull()]) for i,col in enumerate(df_ft.columns)])+['-']\n    \n    \n    #df['%_Null']=list([len(df_fa[col][df_fa[col].isnull()])/df_fa.shape[0]*100 for i,col in enumerate(df_fa.columns)])\n    df.loc[(slice(None),['train']),'unique_count']=list([len(df_fa[col].unique()) for i,col in enumerate(df_fa.columns)])\n    df.loc[(slice(None),['test']),'unique_count']=list([len(df_ft[col].unique()) for i,col in enumerate(df_ft.columns)])+['-']\n    \n    df.loc[(slice(None),['train']),'data_type']=list([df_fa[col].dtype for i,col in enumerate(df_fa.columns)])\n    df.loc[(slice(None),['test']),'data_type']=list([df_ft[col].dtype for i,col in enumerate(df_ft.columns)])+['-']\n    \n    for i,col in enumerate(df_fa.columns):\n        if 'float' in str(df_fa[col].dtype) or 'int' in str(df_fa[col].dtype):\n            df.loc[([col],['train']),'max/min']=str(round(df_fa[col].max(),2))+'/'+str(round(df_fa[col].min(),2))\n            df.loc[([col],['train']),'mean']=round(df_fa[col].mean(),4)\n            df.loc[([col],['train']),'median']=round(df_fa[col].median(),4)\n            df.loc[([col],['train']),'mode']=round(df_fa[col].mode()[0],4)\n            df.loc[([col],['train']),'std']=round(df_fa[col].std(),4)\n            df.loc[([col],['train']),'skewness']=round(df_fa[col].skew(),4)\n        elif 'datetime64[ns]' in str(df_fa[col].dtype):\n            df.loc[([col],['train']),'max/min']=str(df_fa[col].max())+'/'+str(df_fa[col].min())\n        df.loc[([col],['train']),'sample_values']=str(list(df_fa[col].unique()))\n        \n        \n    for i,col in enumerate(df_ft.columns):            \n        if 'float' in str(df_fa[col].dtype) or 'int' in str(df_fa[col].dtype):\n            df.loc[([col],['test']),'max/min']=str(round(df_ft[col].max(),2))+'/'+str(round(df_ft[col].min(),2))\n            df.loc[([col],['test']),'mean']=round(df_ft[col].mean(),4)\n            df.loc[([col],['test']),'median']=round(df_ft[col].median(),4)\n            df.loc[([col],['test']),'mode']=round(df_ft[col].mode()[0],4)\n            df.loc[([col],['test']),'std']=round(df_ft[col].std(),4)\n            df.loc[([col],['test']),'skewness']=round(df_ft[col].skew(),4)\n        elif 'datetime64[ns]' in str(df_fa[col].dtype):\n            df.loc[([col],['test']),'max/min']=str(df_ft[col].max())+'/'+str(df_ft[col].min())\n        df.loc[([col],['test']),'sample_values']=str(list(df_ft[col].unique()))\n        \n    return(df.fillna('-'))\n\n#EXTENDING RIDGE CLASSIFIER WITH PREDICT PROBABILITY FUNCITON\n\nclass RidgeClassifierwithProba(RidgeClassifier):\n    def predict_proba(self, X):\n        d = self.decision_function(X)\n        d_2d = np.c_[-d, d]\n        return softmax(d_2d)\n\n    \n#PREDICTION FUNCTIONS\n\ndef response_predictor(X,y,test,iterations,model,model_name):  \n\n    df_preds=pd.DataFrame()\n    df_preds_x=pd.DataFrame()\n    k=1\n    splits=iterations\n    avg_score=0\n\n    #CREATING STRATIFIED FOLDS\n    skf = StratifiedKFold(n_splits=splits, shuffle=True, random_state=200)\n    print('\\nStarting KFold iterations...')\n    for train_index,test_index in skf.split(X,y):\n        df_X=X[train_index,:]\n        df_y=y[train_index]\n        val_X=X[test_index,:]\n        val_y=y[test_index]\n       \n\n    #FITTING MODEL\n        model.fit(df_X,df_y)\n\n    #PREDICTING ON VALIDATION DATA\n        col_name=model_name+'xpreds_'+str(k)\n        preds_x=pd.Series(model.predict_proba(val_X)[:,1])\n        df_preds_x[col_name]=pd.Series(model.predict_proba(X)[:,1])\n\n    #CALCULATING ACCURACY\n        acc=roc_auc_score(val_y,preds_x)\n        print('Iteration:',k,'  roc_auc_score:',acc)\n        if k==1:\n            score=acc\n            best_model=model\n            preds=pd.Series(model.predict_proba(test)[:,1])\n            col_name=model_name+'preds_'+str(k)\n            df_preds[col_name]=preds\n        else:\n            preds1=pd.Series(model.predict_proba(test)[:,1])\n            preds=preds+preds1\n            col_name=model_name+'preds_'+str(k)\n            df_preds[col_name]=preds1\n            if score<acc:\n                score=acc\n                best_model=model\n        avg_score=avg_score+acc        \n        k=k+1\n    print('\\n Best score:',score,' Avg Score:',avg_score/splits)\n    #TAKING AVERAGE OF PREDICTIONS\n    preds=preds/splits\n    \n    print('Saving test and train predictions per iteration...')\n    df_preds.to_csv(model_name+'.csv',index=False)\n    df_preds_x.to_csv(model_name+'_.csv',index=False)\n    x_preds=df_preds_x.mean(axis=1)\n    del df_preds,df_preds_x\n    gc.collect()\n    return preds,best_model,x_preds ","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.084289,"end_time":"2021-10-17T05:47:12.060859","exception":false,"start_time":"2021-10-17T05:47:11.97657","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-07T23:19:19.619798Z","iopub.execute_input":"2021-11-07T23:19:19.620137Z","iopub.status.idle":"2021-11-07T23:19:19.662846Z","shell.execute_reply.started":"2021-11-07T23:19:19.620103Z","shell.execute_reply":"2021-11-07T23:19:19.662054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n#READING TRAIN DATASET\n\ndf_train=pd.read_csv(path+'train.csv')\n\n#READING TEST DATASET AND SUBMISSION FILE\ndf_test=pd.read_csv(path+'test.csv')\ndf_submission=pd.read_csv(path+'sample_submission.csv')","metadata":{"papermill":{"duration":68.187602,"end_time":"2021-10-17T05:48:20.30271","exception":false,"start_time":"2021-10-17T05:47:12.115108","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-07T22:49:35.282693Z","iopub.execute_input":"2021-11-07T22:49:35.283336Z","iopub.status.idle":"2021-11-07T22:50:01.66736Z","shell.execute_reply.started":"2021-11-07T22:49:35.283295Z","shell.execute_reply":"2021-11-07T22:50:01.666305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 id=\"FeatureSummary\">Understanding Train and Test dataset features</h2>\nUnderstanding Train and Test dataset features in comparative view, using basic statistical measures.\n\n<h4>Observations</h4>\n<ul>\n    <li>No missing values in train or test dataset</li>\n</ul>\n\n<br><a href=\"#Approach\">back to main menu</a>","metadata":{"papermill":{"duration":0.053025,"end_time":"2021-10-17T05:48:20.409228","exception":false,"start_time":"2021-10-17T05:48:20.356203","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%%time\n#UNDERSTANDING TRAIN AND TEST DATASET USING FEATURE BY FEATURE COMPRISON\npd.set_option('display.max_rows', len(df_train.columns)*2)\nfeature_compare(df_train,df_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"papermill":{"duration":0.190304,"end_time":"2021-10-17T05:48:20.902506","exception":false,"start_time":"2021-10-17T05:48:20.712202","status":"completed"},"tags":[],"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#CREATING A FEATURE LIST EXCLUDING ID AND TARGET\nfeatures=[col for col in df_train.columns if col!='id' and col!='target']","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-07T22:50:32.790166Z","iopub.execute_input":"2021-11-07T22:50:32.79107Z","iopub.status.idle":"2021-11-07T22:50:32.796623Z","shell.execute_reply.started":"2021-11-07T22:50:32.791024Z","shell.execute_reply":"2021-11-07T22:50:32.795544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n#VISUALIZING TRAIN AND TEST FEATURE DISTRIBUTION\nplt.figure()\nfig, ax = plt.subplots(20, 5,figsize=(20,70))\n\nfor i,feature in enumerate(features):\n    plt.subplot(20, 5,i+1)\n    sns.kdeplot(data=df_train[feature],x=df_train[feature],color='red', label='train')\n    plt.axvline(x=df_train[feature].mean(),color='yellow',linestyle='--',label='train mean')\n    sns.kdeplot(df_test[feature],x=df_test[feature],color='grey',label='test')\n    plt.axvline(x=df_test[feature].mean(),color='orange',linestyle='--',label='test mean')\n    plt.xlabel(feature,color='blue')\n    if i%5!=0:\n        plt.ylabel('')\n        \n    else:\n        plt.ylabel('Density',color='blue')\n    plt.legend(loc=1,fontsize='x-small')\n    \n    \nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-11-06T14:20:37.140367Z","iopub.execute_input":"2021-11-06T14:20:37.141417Z","iopub.status.idle":"2021-11-06T14:22:07.018469Z","shell.execute_reply.started":"2021-11-06T14:20:37.141347Z","shell.execute_reply":"2021-11-06T14:22:07.017361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 id=\"Downcasting\">Down Casting Training and Testing datasets</h2>\nChecking possibility for down casting dataset datatypes. This will help in reducing overall dataset size.\n\n<h4>Observations</h4>\n<ul>\n    <li>We have only two data types in dataset float64 and int64</li>\n    <li>It is always a good idea to reduce overall dataset size by finding correct datatypes</li>\n    <li>With downcasting able to reduce training dataset size from 466.9 MB to 231.7 MB</li>\n    <li>With downcasting able to reduce training dataset size from 416.1 MB to 208.1 MB</li>\n</ul>\n\n<br><a href=\"#Approach\">back to main menu</a>","metadata":{"papermill":{"duration":0.054363,"end_time":"2021-10-17T05:48:21.011212","exception":false,"start_time":"2021-10-17T05:48:20.956849","status":"completed"},"tags":[]}},{"cell_type":"code","source":"#CHECKING TRAIN AND TEST DATASET MEMORY USAGE BEFORE DOWNCASTING\nprint('train dataset data usage information\\n')\ndf_train.info(memory_usage='deep')\nprint('\\ntest dataset data usage information\\n')\ndf_test.info(memory_usage='deep')","metadata":{"papermill":{"duration":0.076347,"end_time":"2021-10-17T05:48:21.143592","exception":false,"start_time":"2021-10-17T05:48:21.067245","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n#DOWNCASTING TRAIN DATASET\nfor column in df_train.columns:\n    if df_train[column].dtype == \"float64\":\n        df_train[column]=pd.to_numeric(df_train[column], downcast=\"float\")\n    if df_train[column].dtype == \"int64\":\n        df_train[column]=pd.to_numeric(df_train[column], downcast=\"integer\")\n        \n#DOWNCASTING TEST DATASET\nfor column in df_test.columns:\n    if df_test[column].dtype == \"float64\":\n        df_test[column]=pd.to_numeric(df_test[column], downcast=\"float\")\n    if df_test[column].dtype == \"int64\":\n        df_test[column]=pd.to_numeric(df_test[column], downcast=\"integer\")","metadata":{"papermill":{"duration":70.740266,"end_time":"2021-10-17T05:49:31.939389","exception":false,"start_time":"2021-10-17T05:48:21.199123","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-07T22:50:44.990267Z","iopub.execute_input":"2021-11-07T22:50:44.990835Z","iopub.status.idle":"2021-11-07T22:50:59.477704Z","shell.execute_reply.started":"2021-11-07T22:50:44.990794Z","shell.execute_reply":"2021-11-07T22:50:59.475776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#CHECKING TRAIN AND TEST DATASET MEMORY USAGE AFTER DOWNCASTING\nprint('train dataset data usage information\\n')\ndf_train.info(memory_usage='deep')\nprint('\\ntest dataset data usage information\\n')\ndf_test.info(memory_usage='deep')","metadata":{"papermill":{"duration":0.08758,"end_time":"2021-10-17T05:49:32.084058","exception":false,"start_time":"2021-10-17T05:49:31.996478","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"papermill":{"duration":0.193076,"end_time":"2021-10-17T05:49:32.333667","exception":false,"start_time":"2021-10-17T05:49:32.140591","status":"completed"},"tags":[],"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 id=\"Target\">Understanding Target feature distribution</h2>\nLets visualize Target feature.\n\n<h4>Observation</h4>\nAs observations have almost equal count of response and no response observations, this is a balanced dataset. \n\n<br><a href=\"#Approach\">back to main menu</a>","metadata":{"papermill":{"duration":0.056779,"end_time":"2021-10-17T05:50:43.027949","exception":false,"start_time":"2021-10-17T05:50:42.97117","status":"completed"},"tags":[]}},{"cell_type":"code","source":"#Understanding Target (claim) feature distribution\npie_labels=['Spam-'+str(df_train['target'][df_train.target==1].count()),'No Spam-'+\n            str(df_train['target'][df_train.target==0].count())]\npie_share=[df_train['target'][df_train.target==1].count()/df_train['target'].count(),\n           df_train['target'][df_train.target==0].count()/df_train['target'].count()]\nfigureObject, axesObject = plt.subplots(figsize=(6,6))\npie_colors=('yellow','lightgrey')\npie_explode=(.01,.01)\naxesObject.pie(pie_share,labels=pie_labels,explode=pie_explode,autopct='%.2f%%',colors=pie_colors,startangle=30,shadow=True)\naxesObject.axis('equal')\nplt.title('Percentage of Response - No Response Observations',color='blue',fontsize=12)\nplt.show()","metadata":{"papermill":{"duration":0.307246,"end_time":"2021-10-17T05:50:43.392809","exception":false,"start_time":"2021-10-17T05:50:43.085563","status":"completed"},"tags":[],"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 id=\"Corr\">Correlation Check</h2>\nLets check if there are any correlated features. If two features are highly correlated we can remove one of the feature.\nThis will help in dimentionality reduction.\n\n<h4>Observation</h4>\n<ul>\n    <li>No reasonable correlation is observed</li>\n   \n</ul>\n\n<br><a href=\"#Approach\">back to main menu</a>","metadata":{"papermill":{"duration":0.058648,"end_time":"2021-10-17T05:50:43.511134","exception":false,"start_time":"2021-10-17T05:50:43.452486","status":"completed"},"tags":[]}},{"cell_type":"code","source":"#CORRELATION CHECK CATEGORICAL FEATURES\ncorr = df_train[features+['target']].corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# Plotting correlation heatmap\nfig,ax=plt.subplots(figsize=(20,20))\nax.set_xticklabels(labels=corr.columns,fontsize=12)\nax.set_yticklabels(labels=corr.columns,fontsize=12)\n# plt.rcParams.update({'font.size': 12})\nsns.heatmap(corr,mask=mask,cmap='tab20c',linewidth=0.1)\nplt.title('Correlation Map',color='blue',fontsize=12)\nplt.show()","metadata":{"papermill":{"duration":7.908452,"end_time":"2021-10-17T05:50:51.478044","exception":false,"start_time":"2021-10-17T05:50:43.569592","status":"completed"},"tags":[],"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del corr\ngc.collect()","metadata":{"papermill":{"duration":0.196419,"end_time":"2021-10-17T05:50:51.736298","exception":false,"start_time":"2021-10-17T05:50:51.539879","status":"completed"},"tags":[],"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 id=\"TrainVisual\">Visualizating Training dataset</h2>\nWe are making use of PCA, dimentionality reduction technique to Visualize Training dataset.<br>\nVisualization is also helpful in understanding any grouping or patterns within dataset.\n<h4>Observation</h4>\nNo pattern or grouping observed in training dataset\n\n<br><a href=\"#Approach\">back to main menu</a>","metadata":{"papermill":{"duration":0.070833,"end_time":"2021-10-17T05:53:44.395631","exception":false,"start_time":"2021-10-17T05:53:44.324798","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%%time\nX=df_train.drop(['id','target'],axis=1)\n\npca = PCA(n_components=2,random_state=200)\nprincipalComponents = pca.fit_transform(X)\nprincipalDf = pd.DataFrame(data = principalComponents,columns = ['principal_component_1','principal_component_2'])\nprincipalDf['target']=df_train['target']\n\nfig = plt.figure(figsize=(15,15))\nsc=plt.scatter(x=principalDf['principal_component_1'], y=principalDf['principal_component_2'],c=principalDf['target'],cmap='Accent')\nplt.legend(*sc.legend_elements(),bbox_to_anchor=(1.05, 1), loc=2)\nplt.title('2D Visualization of train Dataset',color='blue',fontsize=12)\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nX=df_train.drop(['id','target'],axis=1)\n\npca = PCA(n_components=3,random_state=200)\nprincipalComponents = pca.fit_transform(X)\nprincipalDf = pd.DataFrame(data = principalComponents,columns = ['principal_component_1','principal_component_2','principal_component_3'])\nprincipalDf['target']=df_train['target']\n\nfig = plt.figure(figsize=(15,15))\nax = fig.add_subplot(111, projection = '3d')\n\nax.set_xlabel(\"principal_component_1\")\nax.set_ylabel(\"principal_component_2\")\nax.set_zlabel(\"principal_component_3\")\n\nsc=ax.scatter(xs=principalDf['principal_component_1'], ys=principalDf['principal_component_2'],\n              zs=principalDf['principal_component_3'],c=principalDf['target'],cmap='Accent')\nplt.legend(*sc.legend_elements(), bbox_to_anchor=(1.05, 1), loc=2)\nplt.title('3D Visualization of train Dataset',color='blue',fontsize=12)\nplt.show()","metadata":{"papermill":{"duration":29.324581,"end_time":"2021-10-17T05:54:13.791467","exception":false,"start_time":"2021-10-17T05:53:44.466886","status":"completed"},"tags":[],"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del X\ngc.collect()","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"papermill":{"duration":0.258936,"end_time":"2021-10-17T05:54:14.133587","exception":false,"start_time":"2021-10-17T05:54:13.874651","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 id=\"AggFeatures\">Creating Aggregated features</h2>\nCreating aggregated features\n<h4>Observation</h4>\n\n\n<br><a href=\"#Approach\">back to main menu</a>","metadata":{"papermill":{"duration":0.083008,"end_time":"2021-10-17T05:54:14.300801","exception":false,"start_time":"2021-10-17T05:54:14.217793","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%%time\n#SIMPLE FEATURE ENGINEERING, CREATING SOME AGGREGATION FEATURES\ndf_train['sum']=df_train[features].sum(axis=1)\ndf_test['sum']=df_test[features].sum(axis=1)\n\ndf_train['mean']=df_train[features].mean(axis=1)\ndf_test['mean']=df_test[features].mean(axis=1)\n\ndf_train['std'] = df_train[features].std(axis=1)\ndf_test['std'] = df_test[features].std(axis=1)\n\ndf_train['max'] = df_train[features].max(axis=1)\ndf_test['max'] = df_test[features].max(axis=1)\n\ndf_train['min'] = df_train[features].min(axis=1)\ndf_test['min'] = df_test[features].min(axis=1)\n\ndf_train['kurt'] = df_train[features].kurtosis(axis=1)\ndf_test['kurt'] = df_test[features].kurtosis(axis=1)\n\nagg_features= ['sum','mean','std','max','min','kurt']","metadata":{"papermill":{"duration":5.344709,"end_time":"2021-10-17T05:54:19.731254","exception":false,"start_time":"2021-10-17T05:54:14.386545","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-07T22:52:59.783081Z","iopub.execute_input":"2021-11-07T22:52:59.783402Z","iopub.status.idle":"2021-11-07T22:53:06.214685Z","shell.execute_reply.started":"2021-11-07T22:52:59.783368Z","shell.execute_reply":"2021-11-07T22:53:06.213802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.229276,"end_time":"2021-10-17T05:54:20.045173","exception":false,"start_time":"2021-10-17T05:54:19.815897","status":"completed"},"tags":[],"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-11-07T22:53:09.361873Z","iopub.execute_input":"2021-11-07T22:53:09.362298Z","iopub.status.idle":"2021-11-07T22:53:09.55645Z","shell.execute_reply.started":"2021-11-07T22:53:09.362245Z","shell.execute_reply":"2021-11-07T22:53:09.555586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 id=\"Normalization\">Normalizing dataset</h2>\nUsing Standard Scaler to normalize dataset\n\n<br><a href=\"#Approach\">back to main menu</a>\n","metadata":{"papermill":{"duration":0.084992,"end_time":"2021-10-17T05:54:20.214766","exception":false,"start_time":"2021-10-17T05:54:20.129774","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%%time\nscaler = StandardScaler()\nX = scaler.fit_transform(df_train[features+agg_features])\ntest = scaler.transform(df_test[features+agg_features])\ny=df_train['target'].values","metadata":{"papermill":{"duration":66.120432,"end_time":"2021-10-17T05:55:26.420186","exception":false,"start_time":"2021-10-17T05:54:20.299754","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-07T22:53:13.225206Z","iopub.execute_input":"2021-11-07T22:53:13.225671Z","iopub.status.idle":"2021-11-07T22:53:15.156405Z","shell.execute_reply.started":"2021-11-07T22:53:13.225631Z","shell.execute_reply":"2021-11-07T22:53:15.155576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"papermill":{"duration":0.26527,"end_time":"2021-10-17T05:55:30.391328","exception":false,"start_time":"2021-10-17T05:55:30.126058","status":"completed"},"tags":[],"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-11-07T22:53:17.257447Z","iopub.execute_input":"2021-11-07T22:53:17.257806Z","iopub.status.idle":"2021-11-07T22:53:17.445166Z","shell.execute_reply.started":"2021-11-07T22:53:17.257753Z","shell.execute_reply":"2021-11-07T22:53:17.444058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#FINAL DATASET SHAPES\nX.shape,y.shape,test.shape","metadata":{"papermill":{"duration":0.098544,"end_time":"2021-10-17T05:55:30.579426","exception":false,"start_time":"2021-10-17T05:55:30.480882","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-07T22:53:19.822338Z","iopub.execute_input":"2021-11-07T22:53:19.823421Z","iopub.status.idle":"2021-11-07T22:53:19.830457Z","shell.execute_reply.started":"2021-11-07T22:53:19.823346Z","shell.execute_reply":"2021-11-07T22:53:19.829533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 id=\"LogisticRegression\">LogisticRegression</h2>\nStarting with base Linear Model, with basic hyperparameter tuning.\n\n<h4>Observations</h4>\n\n<br><a href=\"#Approach\">back to main menu</a>","metadata":{"papermill":{"duration":0.09013,"end_time":"2021-10-17T05:55:30.757144","exception":false,"start_time":"2021-10-17T05:55:30.667014","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%%time\nlr_params={'C': 0.01, 'penalty': 'l2', 'solver': 'lbfgs'}\nmodel=LogisticRegression(**lr_params)\nprint('Logistic Regression parameters:\\n',model.get_params())\n\nlogistic_predictions,best_logistic_model,LRpreds=response_predictor(X,y,test,10,model,'LR')","metadata":{"papermill":{"duration":61.175901,"end_time":"2021-10-17T05:56:32.01869","exception":false,"start_time":"2021-10-17T05:55:30.842789","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"papermill":{"duration":0.240546,"end_time":"2021-10-17T05:56:32.34968","exception":false,"start_time":"2021-10-17T05:56:32.109134","status":"completed"},"tags":[],"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_submission['target']=logistic_predictions\n#SAVING LOGISTIC PREDICTIONS\ndf_submission.to_csv('logistic_submission.csv',index=False)\ndf_submission","metadata":{"papermill":{"duration":1.977476,"end_time":"2021-10-17T05:56:34.416041","exception":false,"start_time":"2021-10-17T05:56:32.438565","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"papermill":{"duration":0.266042,"end_time":"2021-10-17T05:56:34.771103","exception":false,"start_time":"2021-10-17T05:56:34.505061","status":"completed"},"tags":[],"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_feature_impt=pd.DataFrame()\ndf_feature_impt['features']=features+agg_features\ndf_feature_impt['importance']=best_logistic_model.coef_[0]\n\ndf_feature_impt.sort_values(by=['importance'],inplace=True,ascending=False)\nplt.figure(figsize = (15,50))\nax=sns.barplot(x=df_feature_impt['importance'],y=df_feature_impt['features'],data=df_feature_impt)\nplt.title('Feature importance Logistic Regression Model',color='blue',fontsize=12)\nax.bar_label(ax.containers[0]);","metadata":{"papermill":{"duration":7.648167,"end_time":"2021-10-17T05:56:42.518255","exception":false,"start_time":"2021-10-17T05:56:34.870088","status":"completed"},"tags":[],"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 id=\"Ridge\">Ridge Classifier</h2>\nStarting with base Ridge model, without any hyperparameter tuning.\n\n<br><a href=\"#Approach\">back to main menu</a>","metadata":{"papermill":{"duration":0.103855,"end_time":"2021-10-17T05:56:42.72675","exception":false,"start_time":"2021-10-17T05:56:42.622895","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%%time\nmodel=RidgeClassifierwithProba()\nprint('Ridge Classifier parameters:\\n',model.get_params())\n\nridge_predictions,best_ridge_model,ridge_preds=response_predictor(X,y,test,10,model,'RC')","metadata":{"papermill":{"duration":62.713035,"end_time":"2021-10-17T05:57:45.543819","exception":false,"start_time":"2021-10-17T05:56:42.830784","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"papermill":{"duration":0.277731,"end_time":"2021-10-17T05:57:45.930176","exception":false,"start_time":"2021-10-17T05:57:45.652445","status":"completed"},"tags":[],"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_submission['target']=ridge_predictions\n#SAVING LGBM PREDICTIONS\ndf_submission.to_csv('ridge_submission.csv',index=False)\ndf_submission","metadata":{"papermill":{"duration":2.065069,"end_time":"2021-10-17T05:57:48.10227","exception":false,"start_time":"2021-10-17T05:57:46.037201","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_feature_impt=pd.DataFrame()\ndf_feature_impt['features']=features+agg_features\ndf_feature_impt['importance']=best_ridge_model.coef_[0]\n\ndf_feature_impt.sort_values(by=['importance'],inplace=True,ascending=False)\nplt.figure(figsize = (15,50))\nax=sns.barplot(x=df_feature_impt['importance'],y=df_feature_impt['features'],data=df_feature_impt)\nplt.title('Feature importance Ridge Model',color='blue',fontsize=12)\nax.bar_label(ax.containers[0]);","metadata":{"papermill":{"duration":7.438323,"end_time":"2021-10-17T05:57:55.651995","exception":false,"start_time":"2021-10-17T05:57:48.213672","status":"completed"},"tags":[],"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"papermill":{"duration":0.350396,"end_time":"2021-10-17T05:57:56.124353","exception":false,"start_time":"2021-10-17T05:57:55.773957","status":"completed"},"tags":[],"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 id=\"LGBM\">LGBM Classification</h2>\n\nSimple LGBMClassifier without any hyperparameter tunning.\n\n<br><a href=\"#Approach\">back to main menu</a>","metadata":{"papermill":{"duration":0.134265,"end_time":"2021-10-17T06:06:22.820823","exception":false,"start_time":"2021-10-17T06:06:22.686558","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%%time\nlgbm_params = {\n    'metric' : 'auc',\n    'objective' : 'binary',\n   }\n\nmodel=lgb.LGBMClassifier(**lgbm_params)\nprint('LGBM parameters:\\n',model.get_params())\n\nlgb_predictions,best_lgb_model,LGBpreds=response_predictor(X,y,test,10,model,'LGB')","metadata":{"papermill":{"duration":2944.549502,"end_time":"2021-10-17T06:55:27.503634","exception":false,"start_time":"2021-10-17T06:06:22.954132","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"papermill":{"duration":0.317005,"end_time":"2021-10-17T06:55:27.956255","exception":false,"start_time":"2021-10-17T06:55:27.63925","status":"completed"},"tags":[],"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_submission['target']=lgb_predictions\n#SAVING LGBM PREDICTIONS\ndf_submission.to_csv('lgb_submission.csv',index=False)\ndf_submission","metadata":{"papermill":{"duration":2.060595,"end_time":"2021-10-17T06:55:30.15344","exception":false,"start_time":"2021-10-17T06:55:28.092845","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_feature_impt=pd.DataFrame()\ndf_feature_impt['features']=features+agg_features\ndf_feature_impt['importance']=best_lgb_model.feature_importances_\n\ndf_feature_impt.sort_values(by=['importance'],inplace=True,ascending=False)\nplt.figure(figsize = (15,50))\nax=sns.barplot(x=df_feature_impt['importance'],y=df_feature_impt['features'],data=df_feature_impt)\nplt.title('Feature importance LGB Model',color='blue',fontsize=12)\nax.bar_label(ax.containers[0]);","metadata":{"papermill":{"duration":7.485463,"end_time":"2021-10-17T06:55:37.77985","exception":false,"start_time":"2021-10-17T06:55:30.294387","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"papermill":{"duration":0.393038,"end_time":"2021-10-17T06:55:38.322385","exception":false,"start_time":"2021-10-17T06:55:37.929347","status":"completed"},"tags":[],"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 id=\"Blending\">Blending</h2>\n\nSimple Blending approach, based on observation\n\n<br><a href=\"#Approach\">back to main menu</a>","metadata":{}},{"cell_type":"code","source":"#BLENDING PREDICTIONS\ndf_submission['target']=logistic_predictions*0.6+ridge_predictions*0.2+lgb_predictions*0.2\n#CREATING SUMBISSION FILE\ndf_submission.to_csv('submission.csv',index=False)","metadata":{"papermill":{"duration":2.116911,"end_time":"2021-10-17T09:38:40.588487","exception":false,"start_time":"2021-10-17T09:38:38.471576","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_submission","metadata":{"papermill":{"duration":0.186564,"end_time":"2021-10-17T09:38:40.944864","exception":false,"start_time":"2021-10-17T09:38:40.7583","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 id=\"Stacking\">Stacking approach</h2>\n\n<ol>\n    <li>Stacking Linear Regression, Ridge and LGBM. Using Catboost on stacked dataset</li>\n    <li>Stacking Linear Regression and Ridge. Using LGBM on stacked dataset</li>\n    <li>Stacking Linear Regression and Ridge. Using Catboost on stacked dataset.</li>\n</ol>\n<h3>Observations</h3>\n<ul>\n    <li>Getting best <b>Public Score of 0.74635</b> using 3rd approach</li>\n</ul>\n<br><a href=\"#Approach\">back to main menu</a>","metadata":{}},{"cell_type":"code","source":"%%time\n#READING PREDICTED VALUES\ndf_LR_=pd.read_csv('./LR_.csv')\ndf_LR=pd.read_csv('./LR.csv')\ndf_RC_=pd.read_csv('./RC_.csv')\ndf_RC=pd.read_csv('./RC.csv')\ndf_LGB_=pd.read_csv('./LGB_.csv')\ndf_LGB=pd.read_csv('./LGB.csv')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nX=pd.concat([df_LR_,df_RC_,df_LGB_],axis=1).to_numpy()\ntest=pd.concat([df_LR,df_RC,df_LGB],axis=1).to_numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ncatb_params = {\n    'eval_metric' : 'AUC',\n    'verbose' : 0,\n     'learning_rate':0.01,\n    'n_estimators':200\n    }\nmodel=CatBoostClassifier(**catb_params)\nprint('CatBoost paramters:\\n',model.get_params())\n\ncatb_predictions,best_catb_model,CBpreds=response_predictor(X,y,test,10,model,'CBS')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_submission['target']=catb_predictions\n#SAVING CATBOOST PREDICTIONS\ndf_submission.to_csv('catbstack_mixed_submission.csv',index=False)\ndf_submission","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n#STACKING OUTCOMES FROM LINEAR MODELS\nX=pd.concat([df_LR_,df_RC_],axis=1).to_numpy()\ntest=pd.concat([df_LR,df_RC],axis=1).to_numpy()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nlgbm_params = {\n    'metric' : 'auc',\n    'objective' : 'binary',\n   }\n\nmodel=lgb.LGBMClassifier(**lgbm_params)\nprint('LGBM parameters:\\n',model.get_params())\n\nlgb_predictions,best_lgb_model,LGBpreds=response_predictor(X,y,test,10,model,'LGB')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_submission['target']=lgb_predictions\n#SAVING CATBOOST PREDICTIONS\ndf_submission.to_csv('lgbstack_linear_submission.csv',index=False)\ndf_submission","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ncatb_params = {\n    'eval_metric' : 'AUC',\n    'verbose' : 0,\n     'learning_rate':0.01,\n    'n_estimators':200\n    }\nmodel=CatBoostClassifier(**catb_params)\nprint('CatBoost paramters:\\n',model.get_params())\n\ncatb_predictions,best_catb_model,CBpreds=response_predictor(X,y,test,10,model,'CBS')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_submission['target']=catb_predictions\n#SAVING CATBOOST PREDICTIONS\ndf_submission.to_csv('catbstack_linear_submission.csv',index=False)\ndf_submission","metadata":{},"execution_count":null,"outputs":[]}]}