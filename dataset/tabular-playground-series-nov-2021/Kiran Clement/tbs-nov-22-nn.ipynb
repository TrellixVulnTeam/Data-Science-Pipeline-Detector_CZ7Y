{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-26T09:59:06.743145Z","iopub.execute_input":"2021-11-26T09:59:06.743893Z","iopub.status.idle":"2021-11-26T09:59:06.764932Z","shell.execute_reply.started":"2021-11-26T09:59:06.743802Z","shell.execute_reply":"2021-11-26T09:59:06.764264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Display Setting ","metadata":{}},{"cell_type":"markdown","source":"This we set 103 the maximum feature length so that all the features can be vissible by scrolling in the output window ","metadata":{}},{"cell_type":"code","source":"pd.set_option('display.max_columns',120) # setting the visulaization on columns\npd.set_option('display.max_rows',120)\npd.set_option(\"display.max_colwidth\",100)","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:59:06.766801Z","iopub.execute_input":"2021-11-26T09:59:06.767293Z","iopub.status.idle":"2021-11-26T09:59:06.771551Z","shell.execute_reply.started":"2021-11-26T09:59:06.767257Z","shell.execute_reply":"2021-11-26T09:59:06.77081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# IMPORTING LIBRARIES","metadata":{}},{"cell_type":"code","source":"import numpy as np # Linear algebra.\nimport scipy.stats as stat\nimport pandas as pd # Data processing, CSV file I/O (e.g. pd.read_csv).\nimport datatable as dt # Data processing, CSV file I/O (e.g. dt.fread).\n\n\n# Visualization.\nimport seaborn as sns \nimport matplotlib.pyplot as plt \n\n# Machine Learning \n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.svm import SVC\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import SGDClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.pipeline import make_pipeline\n\nfrom copy import deepcopy\nimport warnings\nwarnings.filterwarnings('ignore')\n","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:59:06.772693Z","iopub.execute_input":"2021-11-26T09:59:06.773367Z","iopub.status.idle":"2021-11-26T09:59:07.964924Z","shell.execute_reply.started":"2021-11-26T09:59:06.773332Z","shell.execute_reply":"2021-11-26T09:59:07.964202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LOAD DATA ","metadata":{}},{"cell_type":"code","source":"df_train =dt.fread('../input/tabular-playground-series-nov-2021/train.csv').to_pandas()\ndf_test = dt.fread('../input/tabular-playground-series-nov-2021/test.csv').to_pandas()\nsubmission = pd.read_csv('../input/tabular-playground-series-nov-2021/sample_submission.csv')\n\ndf_train['target'] = df_train['target'].astype('int32') # Datatable reads target as bool by default.\nprint(f'{df_train.info()}\\n\\n')","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:59:07.96724Z","iopub.execute_input":"2021-11-26T09:59:07.967652Z","iopub.status.idle":"2021-11-26T09:59:18.715938Z","shell.execute_reply.started":"2021-11-26T09:59:07.967615Z","shell.execute_reply":"2021-11-26T09:59:18.714017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Downcasting the data","metadata":{}},{"cell_type":"markdown","source":"Special thanks for  Sergie for some of the excelent ideas to reduce the load by downcasting and sampling. \nhttps://www.kaggle.com/sergiosaharovskiy/tps-nov-2021-a-complete-guide","metadata":{}},{"cell_type":"code","source":"memory_train = sum(df_train.memory_usage()) / 1e6\nprint(f'[INFO] Memory usage train_before: {memory_train:.2f} MB.')\n\nmemory_test = sum(df_test.memory_usage()) / 1e6\nprint(f'[INFO] Memory usage test_before: {memory_test:.2f} MB.\\n')\n\n# Downcasting the traind dataset.\nfor col in df_train.columns:\n    \n    if df_train[col].dtype == \"float64\":\n        df_train[col] = pd.to_numeric(df_train[col], downcast=\"float\")\n        \n    if df_train[col].dtype == \"int64\":\n        df_train[col] = pd.to_numeric(df_train[col], downcast=\"integer\")\n        \n# Downcasting the test dataset.\nfor col in df_test.columns:\n    \n    if df_test[col].dtype == \"float64\":\n        df_test[col] = pd.to_numeric(df_test[col], downcast=\"float\")\n        \n    if df_test[col].dtype == \"int64\":\n        df_test[col] = pd.to_numeric(df_test[col], downcast=\"integer\")\n        \nmemory_train = sum(df_train.memory_usage()) / 1e6\nprint(f'[INFO] Memory usage train: {memory_train:.2f} MB.')\n\nmemory_test = sum(df_test.memory_usage()) / 1e6\nprint(f'[INFO] Memory usage test: {memory_test:.2f} MB.')","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:59:18.719268Z","iopub.execute_input":"2021-11-26T09:59:18.71949Z","iopub.status.idle":"2021-11-26T09:59:31.390076Z","shell.execute_reply.started":"2021-11-26T09:59:18.719463Z","shell.execute_reply":"2021-11-26T09:59:31.389305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Cleaning and Handling NaN","metadata":{}},{"cell_type":"code","source":"#df_train.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:59:31.391587Z","iopub.execute_input":"2021-11-26T09:59:31.392093Z","iopub.status.idle":"2021-11-26T09:59:31.396199Z","shell.execute_reply.started":"2021-11-26T09:59:31.392053Z","shell.execute_reply":"2021-11-26T09:59:31.395429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df_test.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:59:31.397504Z","iopub.execute_input":"2021-11-26T09:59:31.39776Z","iopub.status.idle":"2021-11-26T09:59:31.40519Z","shell.execute_reply.started":"2021-11-26T09:59:31.397723Z","shell.execute_reply":"2021-11-26T09:59:31.404462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"No Null values in Train and Test Data","metadata":{}},{"cell_type":"markdown","source":"## Below codes to check the presence of Nan values in each features","metadata":{}},{"cell_type":"code","source":"# for i in df_train.columns:\n#     if df_train[i].isnull().sum() !=0:\n#         print(i, df_train[i].isnull().sum())\n#     else:\n#         print('No Null present in ',i)","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:59:31.406256Z","iopub.execute_input":"2021-11-26T09:59:31.406614Z","iopub.status.idle":"2021-11-26T09:59:31.41387Z","shell.execute_reply.started":"2021-11-26T09:59:31.406578Z","shell.execute_reply":"2021-11-26T09:59:31.412984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for i in df_test.columns:\n#     if df_test[i].isnull().sum() !=0:\n#         print(i, df_test[i].isnull().sum())\n#     else:\n#         print('No Null present in ',i)","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:59:31.415003Z","iopub.execute_input":"2021-11-26T09:59:31.415325Z","iopub.status.idle":"2021-11-26T09:59:31.422481Z","shell.execute_reply.started":"2021-11-26T09:59:31.41529Z","shell.execute_reply":"2021-11-26T09:59:31.421678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## *********","metadata":{}},{"cell_type":"code","source":"#df_train['target'].value_counts(normalize=True) #ehecking the ratio of target class","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:59:31.423804Z","iopub.execute_input":"2021-11-26T09:59:31.424283Z","iopub.status.idle":"2021-11-26T09:59:31.430683Z","shell.execute_reply.started":"2021-11-26T09:59:31.424246Z","shell.execute_reply":"2021-11-26T09:59:31.429881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The ratios of binary class is 50 :49.3 % , so this is a well balanced training dataset","metadata":{}},{"cell_type":"code","source":"#df_train.shape,df_test.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:59:31.432092Z","iopub.execute_input":"2021-11-26T09:59:31.434079Z","iopub.status.idle":"2021-11-26T09:59:31.439084Z","shell.execute_reply.started":"2021-11-26T09:59:31.434053Z","shell.execute_reply":"2021-11-26T09:59:31.43846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset= pd.concat(objs=[df_train,df_test],axis=0).reset_index(drop=True) #combining both test and train for easy preoprocessing and feature transformation\ndataset.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:59:31.440213Z","iopub.execute_input":"2021-11-26T09:59:31.440817Z","iopub.status.idle":"2021-11-26T09:59:32.224616Z","shell.execute_reply.started":"2021-11-26T09:59:31.440781Z","shell.execute_reply":"2021-11-26T09:59:32.223935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Checking the data distrubution","metadata":{}},{"cell_type":"code","source":"#dataset.iloc[:,1:].describe().T.sort_values(by='std',ascending=False).head(7)","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:59:32.225895Z","iopub.execute_input":"2021-11-26T09:59:32.226429Z","iopub.status.idle":"2021-11-26T09:59:32.231646Z","shell.execute_reply.started":"2021-11-26T09:59:32.22639Z","shell.execute_reply":"2021-11-26T09:59:32.230227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#dataset.iloc[:,1:].describe().T.sort_values(by='std',ascending=False).tail(5)","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:59:32.237135Z","iopub.execute_input":"2021-11-26T09:59:32.237587Z","iopub.status.idle":"2021-11-26T09:59:32.244365Z","shell.execute_reply.started":"2021-11-26T09:59:32.237552Z","shell.execute_reply":"2021-11-26T09:59:32.243593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we checked the standard deviation of the features , the max is 526.32 , and min std is 0.049.\nSo we have to address the highest std features f2,f35 ,f44","metadata":{}},{"cell_type":"markdown","source":"## Plot every features to check whether it is in  Gaussian distribution or Not","metadata":{}},{"cell_type":"code","source":"# fig, axes = plt.subplots(10,10, figsize=(25, 15))\n# axes = axes.flatten()\n\n# for idx, ax in enumerate(axes):\n    \n#     sns.kdeplot(\n#         data=df_train, ax=ax, hue='target', fill=True,\n#         x=f'f{idx}', palette=['#4DB6AC', 'red'], legend=idx==0\n#     )\n    \n#     ax.set_title(f'f{idx}', loc='right', weight='bold', fontsize=12)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:59:32.248077Z","iopub.execute_input":"2021-11-26T09:59:32.248338Z","iopub.status.idle":"2021-11-26T09:59:32.251969Z","shell.execute_reply.started":"2021-11-26T09:59:32.248308Z","shell.execute_reply":"2021-11-26T09:59:32.251099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can observe that many are non gaussian distributed data,eg: f0, f2 ,f4 etc....\nSo thees features need feature transformation to bring it into gaussian distrubution or very close to Gaussian distrubution.\n\nThere are many festure transoformations like log, square root,exponential, reciprocal and power transformers like Yeo-Johnson and Box-Cox Transformers.\n\nTo make things simple we first try log transformation for all the features, then to improve the model we can try different transformers for each features and check the distribution","metadata":{}},{"cell_type":"code","source":"#features required transformation since not in normal distrubution\nnon_gaus= ['f0','f2','f4','f9','f12','f16','f19','f20','f23','f24','f27',\n    'f28','f30','f31','f32','f33','f35','f39','f42','f44','f46','f48',\n    'f49','f51','f52','f53','f56','f58','f59','f60','f61','f62','f63',\n    'f64','f68','f69','f72','f73','f75','f76','f78','f79','f81','f83',\n    'f84','f87','f88','f89','f90','f92','f93','f94','f95','f98','f99' ]","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:59:32.253549Z","iopub.execute_input":"2021-11-26T09:59:32.254135Z","iopub.status.idle":"2021-11-26T09:59:32.263267Z","shell.execute_reply.started":"2021-11-26T09:59:32.254098Z","shell.execute_reply":"2021-11-26T09:59:32.262519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df_train[non_gaus].head(6)","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:59:32.264792Z","iopub.execute_input":"2021-11-26T09:59:32.265331Z","iopub.status.idle":"2021-11-26T09:59:32.271473Z","shell.execute_reply.started":"2021-11-26T09:59:32.265276Z","shell.execute_reply":"2021-11-26T09:59:32.270318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Log Transformation","metadata":{}},{"cell_type":"code","source":"# df_transform_log = df_train[non_gaus]\n\n# value_neg = (df_transform_log  < 0) #created a mask for negative values\n# value_pos = (df_transform_log  > 0) #created a mask for positive values\n\n# df_transform_log[value_neg]=np.log(np.abs(df_transform_log))  * (-1)\n# df_transform_log[value_pos]=np.log(df_transform_log)","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:59:32.27337Z","iopub.execute_input":"2021-11-26T09:59:32.273879Z","iopub.status.idle":"2021-11-26T09:59:32.278866Z","shell.execute_reply.started":"2021-11-26T09:59:32.273843Z","shell.execute_reply":"2021-11-26T09:59:32.278091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data distribution after log transformation","metadata":{}},{"cell_type":"code","source":"# df_transform_log['target'] = df_train['target']\n\n# fig, axes = plt.subplots(11,5, figsize=(20, 12))\n# axes = axes.flatten()\n\n# for col, ax in zip(non_gaus, axes):\n    \n#     sns.kdeplot(\n#         data=df_transform_log, ax=ax, hue='target', fill=True,\n#         x=col, palette=['#4DB6AC', 'red'], legend=idx==0\n#     )\n#     ax.set_title(f'f{idx}', loc='right', weight='bold', fontsize=12)","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:59:32.28059Z","iopub.execute_input":"2021-11-26T09:59:32.281615Z","iopub.status.idle":"2021-11-26T09:59:32.286849Z","shell.execute_reply.started":"2021-11-26T09:59:32.281553Z","shell.execute_reply":"2021-11-26T09:59:32.286053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Apply log transform to the whole dataset (train and test together)","metadata":{}},{"cell_type":"code","source":"print('Data before log transform')\ndataset.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:59:32.288506Z","iopub.execute_input":"2021-11-26T09:59:32.289223Z","iopub.status.idle":"2021-11-26T09:59:32.409375Z","shell.execute_reply.started":"2021-11-26T09:59:32.28911Z","shell.execute_reply":"2021-11-26T09:59:32.408691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#sns.distplot(dataset['f0'])","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:59:32.410598Z","iopub.execute_input":"2021-11-26T09:59:32.411058Z","iopub.status.idle":"2021-11-26T09:59:32.415103Z","shell.execute_reply.started":"2021-11-26T09:59:32.411023Z","shell.execute_reply":"2021-11-26T09:59:32.414318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Features from the dataset we need to give log transform\ndataset_transform_log =dataset[non_gaus]\nvalue_neg =dataset_transform_log  < 0\nvalue_pos = dataset_transform_log > 0\n\ndataset_transform_log[value_neg]=(np.log(np.abs(dataset[non_gaus])) * (-1))\ndataset_transform_log[value_pos]=(np.log(dataset[non_gaus]))\n\ndataset[non_gaus] = dataset_transform_log[non_gaus] #replace the original dataset[non_gaus] values with log transformed value","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:59:32.416864Z","iopub.execute_input":"2021-11-26T09:59:32.417484Z","iopub.status.idle":"2021-11-26T09:59:35.539294Z","shell.execute_reply.started":"2021-11-26T09:59:32.417446Z","shell.execute_reply":"2021-11-26T09:59:35.538295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#sns.distplot(dataset['f0'])","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:59:35.540823Z","iopub.execute_input":"2021-11-26T09:59:35.541071Z","iopub.status.idle":"2021-11-26T09:59:35.545303Z","shell.execute_reply.started":"2021-11-26T09:59:35.541037Z","shell.execute_reply":"2021-11-26T09:59:35.544235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Adding aggregate features ","metadata":{}},{"cell_type":"code","source":"features=dataset.columns[1:-1]\n#SIMPLE FEATURE ENGINEERING, CREATING SOME AGGREGATION FEATURES\ndataset['sum']=dataset[features].sum(axis=1)\n\ndataset['mean']=dataset[features].mean(axis=1)\n\n\ndataset['std'] = dataset[features].std(axis=1)\n\ndataset['max'] = dataset[features].max(axis=1)\n\ndataset['min'] = dataset[features].min(axis=1)\n\n\ndataset['kurt'] = dataset[features].kurtosis(axis=1)\n\nagg_features= ['sum','mean','std','max','min','kurt']","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:59:35.546825Z","iopub.execute_input":"2021-11-26T09:59:35.547516Z","iopub.status.idle":"2021-11-26T09:59:40.634224Z","shell.execute_reply.started":"2021-11-26T09:59:35.547475Z","shell.execute_reply":"2021-11-26T09:59:40.633268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Spliting the dataset back into train and test ","metadata":{}},{"cell_type":"code","source":"#Spliting train and test\n\ntrain=dataset.iloc[:len(df_train)]\ntest=dataset.iloc[len(df_train):]\ntest.drop(labels=[\"target\",\"id\"],axis = 1,inplace=True)\ntrain.drop(labels=[\"id\"],axis = 1,inplace=True)\n#train.drop(columns='id', inplace=True)\ntrain.shape,test.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:59:40.635301Z","iopub.execute_input":"2021-11-26T09:59:40.635563Z","iopub.status.idle":"2021-11-26T09:59:41.004773Z","shell.execute_reply.started":"2021-11-26T09:59:40.63553Z","shell.execute_reply":"2021-11-26T09:59:41.004064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Outlier detection and handling","metadata":{}},{"cell_type":"code","source":"outlier_feature=[]\nno_outlier_feature=[]\nfor i in train.columns:\n    IQR =train[i].quantile(.75) -train[i].quantile(.25)\n    upperlimit =train[i].quantile(.75) +(1.5*IQR)\n    lowerlimit =train[i].quantile(.25) -(1.5*IQR)\n    if (train[i].max() > upperlimit) | (train[i].min() < lowerlimit):\n        outlier_feature.append(i)\n    else:\n        no_outlier_feature.append(i)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:59:41.005869Z","iopub.execute_input":"2021-11-26T09:59:41.006711Z","iopub.status.idle":"2021-11-26T09:59:44.600745Z","shell.execute_reply.started":"2021-11-26T09:59:41.006672Z","shell.execute_reply":"2021-11-26T09:59:44.600013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.columns","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:59:44.60196Z","iopub.execute_input":"2021-11-26T09:59:44.602236Z","iopub.status.idle":"2021-11-26T09:59:44.609811Z","shell.execute_reply.started":"2021-11-26T09:59:44.602199Z","shell.execute_reply":"2021-11-26T09:59:44.609053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # ploting the outlier\n# for i in train.columns:\n#     sns.boxplot(x=train[i])\n#     plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:59:44.611248Z","iopub.execute_input":"2021-11-26T09:59:44.61151Z","iopub.status.idle":"2021-11-26T09:59:44.617667Z","shell.execute_reply.started":"2021-11-26T09:59:44.611475Z","shell.execute_reply":"2021-11-26T09:59:44.616969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Creation","metadata":{"execution":{"iopub.status.busy":"2021-11-23T21:14:47.689852Z","iopub.execute_input":"2021-11-23T21:14:47.690157Z","iopub.status.idle":"2021-11-23T21:14:47.694547Z","shell.execute_reply.started":"2021-11-23T21:14:47.690123Z","shell.execute_reply":"2021-11-23T21:14:47.693621Z"}}},{"cell_type":"code","source":"# Creating X_train and Y_train data\nX_train=train.drop(columns=['target'])\ny_train=train['target']","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:59:44.618907Z","iopub.execute_input":"2021-11-26T09:59:44.619203Z","iopub.status.idle":"2021-11-26T09:59:44.699379Z","shell.execute_reply.started":"2021-11-26T09:59:44.619149Z","shell.execute_reply":"2021-11-26T09:59:44.698419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.columns","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:59:44.700646Z","iopub.execute_input":"2021-11-26T09:59:44.701502Z","iopub.status.idle":"2021-11-26T09:59:44.711375Z","shell.execute_reply.started":"2021-11-26T09:59:44.70142Z","shell.execute_reply":"2021-11-26T09:59:44.71051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Base Line Model Creation","metadata":{}},{"cell_type":"markdown","source":"Below code is used for cross validation and model selection , and selected LinearDiscriminantAnalysis() , since it gives best score.\nAnd created a model \n\nTo make understanding simple comment out this process and directly used LinearDiscriminantAnalysis()  model for model creation","metadata":{}},{"cell_type":"code","source":"# # Cross validate model with Kfold stratified cross val\n# kfold = StratifiedKFold(n_splits=10)\n\n# # Modeling step Test differents algorithms \n# random_state = 2\n# classifiers = []\n# classifiers.append(SVC(random_state=random_state))\n# classifiers.append(DecisionTreeClassifier(random_state=random_state))\n# classifiers.append(AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state),random_state=random_state,learning_rate=0.1))\n# classifiers.append(RandomForestClassifier(random_state=random_state))\n# classifiers.append(ExtraTreesClassifier(random_state=random_state))\n# classifiers.append(GradientBoostingClassifier(random_state=random_state))\n# classifiers.append(MLPClassifier(random_state=random_state))\n# classifiers.append(KNeighborsClassifier())\n# classifiers.append(LogisticRegression(random_state = random_state),solver='liblinear')\n# classifiers.append(LinearDiscriminantAnalysis())\n\n\n# # result into a list \n\n# cv_results = []\n# for classifier in classifiers :\n#     cv_results.append(cross_val_score(classifier, X_train, y = y_train, scoring = \"accuracy\", cv = kfold, n_jobs=4))\n    \n# # mean and std on a list\n# cv_means = []\n# cv_std = []\n# for cv_result in cv_results:\n#     cv_means.append(cv_result.mean())\n#     cv_std.append(cv_result.std())\n    \n","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:59:44.713186Z","iopub.execute_input":"2021-11-26T09:59:44.713531Z","iopub.status.idle":"2021-11-26T09:59:44.718989Z","shell.execute_reply.started":"2021-11-26T09:59:44.713493Z","shell.execute_reply":"2021-11-26T09:59:44.718164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## NN- Model Creation","metadata":{"execution":{"iopub.status.busy":"2021-11-26T06:40:24.44284Z","iopub.execute_input":"2021-11-26T06:40:24.443315Z","iopub.status.idle":"2021-11-26T06:40:24.457286Z","shell.execute_reply.started":"2021-11-26T06:40:24.443285Z","shell.execute_reply":"2021-11-26T06:40:24.456553Z"}}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split \n\nx_train_nn, x_val_nn, y_train_nn, y_val_nn = train_test_split(X_train, y_train, train_size = 0.9, random_state = 0)","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:59:44.720686Z","iopub.execute_input":"2021-11-26T09:59:44.72094Z","iopub.status.idle":"2021-11-26T09:59:45.036235Z","shell.execute_reply.started":"2021-11-26T09:59:44.72091Z","shell.execute_reply":"2021-11-26T09:59:45.035407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import keras \nfrom tensorflow.keras import layers \nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n\nmodel = keras.Sequential([ \n    layers.BatchNormalization(),\n    layers.Dense(2048, activation='sigmoid', input_shape=[101]),\n    layers.Dropout(rate = 0.3), \n    layers.BatchNormalization(),    \n    layers.Dense(2048, activation='relu'), \n    layers.Dropout(rate = 0.3), \n    layers.BatchNormalization(),\n    layers.Dense(2048, activation='relu'), \n    layers.Dropout(rate = 0.3), \n    layers.Dense(1, activation = 'sigmoid'), \n]) \n\nearly_stopping = EarlyStopping(min_delta = 0.001, patience = 10, restore_best_weights = True)","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:59:45.03837Z","iopub.execute_input":"2021-11-26T09:59:45.038587Z","iopub.status.idle":"2021-11-26T09:59:51.941414Z","shell.execute_reply.started":"2021-11-26T09:59:45.038561Z","shell.execute_reply":"2021-11-26T09:59:51.939602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile( \n    optimizer='adam', \n    loss='binary_crossentropy', \n    metrics=['AUC'],\n    )","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:59:51.942706Z","iopub.execute_input":"2021-11-26T09:59:51.94295Z","iopub.status.idle":"2021-11-26T09:59:51.963041Z","shell.execute_reply.started":"2021-11-26T09:59:51.942915Z","shell.execute_reply":"2021-11-26T09:59:51.962158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction = model.fit( \n    x_train_nn, y_train_nn, \n    validation_data=(x_val_nn, y_val_nn), \n    batch_size=256, \n    epochs=100, \n    callbacks = [\n                early_stopping], \n    verbose = 2, \n)","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:59:51.964166Z","iopub.execute_input":"2021-11-26T09:59:51.964514Z","iopub.status.idle":"2021-11-26T10:03:59.684655Z","shell.execute_reply.started":"2021-11-26T09:59:51.964475Z","shell.execute_reply":"2021-11-26T10:03:59.683813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_NN = model.predict(test)","metadata":{"execution":{"iopub.status.busy":"2021-11-26T10:06:37.431478Z","iopub.execute_input":"2021-11-26T10:06:37.43202Z","iopub.status.idle":"2021-11-26T10:07:34.791348Z","shell.execute_reply.started":"2021-11-26T10:06:37.431981Z","shell.execute_reply":"2021-11-26T10:07:34.790573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('run finished')","metadata":{"execution":{"iopub.status.busy":"2021-11-26T10:07:34.792875Z","iopub.execute_input":"2021-11-26T10:07:34.793116Z","iopub.status.idle":"2021-11-26T10:07:34.797083Z","shell.execute_reply.started":"2021-11-26T10:07:34.793081Z","shell.execute_reply":"2021-11-26T10:07:34.796378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_NN[:,0]","metadata":{"execution":{"iopub.status.busy":"2021-11-26T10:10:38.738443Z","iopub.execute_input":"2021-11-26T10:10:38.738703Z","iopub.status.idle":"2021-11-26T10:10:38.745833Z","shell.execute_reply.started":"2021-11-26T10:10:38.738671Z","shell.execute_reply":"2021-11-26T10:10:38.744814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Creation","metadata":{}},{"cell_type":"code","source":"# clf_lda= LinearDiscriminantAnalysis()#n_estimator is the only main hp we optimize in RF,\n# #In practice, adjusting only one of these (e.g. max_depth) is enough to reduce overfitting\n# clf_lda.fit(X_train, y_train)\n\n# score_lda=clf_lda.score(X_train,y_train)\n\n# y_test_predict= clf_lda.predict(test)  # this is just to predict like a normal binary classification\n# y_test_predict_proba_lda= clf_lda.predict_proba(test)[:,1]\n# score_lda","metadata":{"execution":{"iopub.status.busy":"2021-11-25T21:29:47.255863Z","iopub.execute_input":"2021-11-25T21:29:47.256155Z","iopub.status.idle":"2021-11-25T21:30:02.896581Z","shell.execute_reply.started":"2021-11-25T21:29:47.256126Z","shell.execute_reply":"2021-11-25T21:30:02.895534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"below code is to submit the probability for LDA model only","metadata":{}},{"cell_type":"code","source":"# submission = pd.DataFrame({'id': df_test['id'], 'target': y_test_predict_proba_lda})\n# submission.to_csv('submission.csv', index=False)\n# submission.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-11-25T19:40:59.255752Z","iopub.status.idle":"2021-11-25T19:40:59.25642Z","shell.execute_reply.started":"2021-11-25T19:40:59.256116Z","shell.execute_reply":"2021-11-25T19:40:59.256145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Ensemble model with logistic and LDA","metadata":{}},{"cell_type":"code","source":"# random_state = 2\n# clf_log = LogisticRegression(random_state = random_state,solver='liblinear')\n# clf_log.fit(X_train,y_train)\n\n# score_logistic=clf_log.score(X_train,y_train)\n\n# #y_test_predict= clf_log.predict(test) \n# y_test_predict_proba_log= clf_log.predict_proba(test)[:,1]\n# score_logistic","metadata":{"execution":{"iopub.status.busy":"2021-11-25T21:30:02.903006Z","iopub.execute_input":"2021-11-25T21:30:02.903962Z","iopub.status.idle":"2021-11-25T21:31:21.503132Z","shell.execute_reply.started":"2021-11-25T21:30:02.903896Z","shell.execute_reply":"2021-11-25T21:31:21.502128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Ensemble model creation using Log and LDA","metadata":{}},{"cell_type":"code","source":"# ensemble = pd.DataFrame({'id': df_test['id'], 'target_log': y_test_predict_proba_log,'target_lda': y_test_predict_proba_lda})\n# targets=['target_lda','target_log']\n# ensemble.head(2)","metadata":{"execution":{"iopub.status.busy":"2021-11-25T19:40:59.263754Z","iopub.status.idle":"2021-11-25T19:40:59.26423Z","shell.execute_reply.started":"2021-11-25T19:40:59.263981Z","shell.execute_reply":"2021-11-25T19:40:59.264007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Submission","metadata":{}},{"cell_type":"code","source":"# #Submission\n# submission = pd.DataFrame({'id': df_test['id'], 'target': ensemble[targets].mean(axis=1)})\n# submission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-25T19:40:59.265705Z","iopub.status.idle":"2021-11-25T19:40:59.266563Z","shell.execute_reply.started":"2021-11-25T19:40:59.266265Z","shell.execute_reply":"2021-11-25T19:40:59.266295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Submission\nsubmission = pd.DataFrame({'id': df_test['id'], 'target':preds_NN[:,0]})\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-26T10:10:53.762691Z","iopub.execute_input":"2021-11-26T10:10:53.762944Z","iopub.status.idle":"2021-11-26T10:10:55.336356Z","shell.execute_reply.started":"2021-11-26T10:10:53.762916Z","shell.execute_reply":"2021-11-26T10:10:55.335421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import the modules we'll need\nfrom IPython.display import HTML\nimport pandas as pd\nimport numpy as np\nimport base64\n\n# function that takes in a dataframe and creates a text link to  \n# download it (will only work for files < 2MB or so)\ndef create_download_link(df, title = \"Download CSV file\", filename = \"data.csv\"):  \n    csv = df.to_csv()\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text/csv;base64,{payload}\" target=\"_blank\">{title}</a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n\n\n# create a link to download the dataframe\ncreate_download_link(submission)","metadata":{"execution":{"iopub.status.busy":"2021-11-26T10:11:55.817893Z","iopub.execute_input":"2021-11-26T10:11:55.818147Z","iopub.status.idle":"2021-11-26T10:11:57.752829Z","shell.execute_reply.started":"2021-11-26T10:11:55.818119Z","shell.execute_reply":"2021-11-26T10:11:57.751875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}