{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Leaderboard Probing\n\nFrom [that discussion](https://www.kaggle.com/c/tabular-playground-series-nov-2021/discussion/286731#1578128) we know that the competition's data consists of 19 chunks, 10 chunks in the training set and 9 in the test set. Every chunk looks homogeneous, but they have different characteristics.\n\nWe can think of these chunks as a categorical variable with 19 categories. Unfortunately, only 10 of these categories appear in the training set. For the other 9 categories, there is no labeled data.\n\nIn such a situation, we can use leaderboard probing. Leaderboard probing means that we submit predictions with the goal of getting information about the test labels. In this notebook, I'm creating 18 submission files for leaderboard probing. Every probe is derived from a baseline submission, but the predictions for one chunk are set to the minimum or maximum value of all predictions. The probe returns a lb auc score, which we can transform into a target probability per chunk by simple arithmetic.\n\nAfter the probing phase, we can submit the target probability for every chunk.\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport pickle\nimport io\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import roc_auc_score, accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import LinearSVC","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-23T06:17:49.532607Z","iopub.execute_input":"2021-11-23T06:17:49.533Z","iopub.status.idle":"2021-11-23T06:17:50.625795Z","shell.execute_reply.started":"2021-11-23T06:17:49.532903Z","shell.execute_reply":"2021-11-23T06:17:50.624973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read the data\ntrain_df = pd.read_csv('../input/tabular-playground-series-nov-2021/train.csv')\ntest_df = pd.read_csv('../input/tabular-playground-series-nov-2021/test.csv')\npure_df = pd.read_csv('../input/november21/train.csv')\ntest_df['chunk'] = test_df.id // 60000","metadata":{"execution":{"iopub.status.busy":"2021-11-23T06:17:50.629389Z","iopub.execute_input":"2021-11-23T06:17:50.629616Z","iopub.status.idle":"2021-11-23T06:18:29.668633Z","shell.execute_reply.started":"2021-11-23T06:17:50.629591Z","shell.execute_reply":"2021-11-23T06:18:29.667873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def postprocess_separate(submission_df, test_df=None, pure_df=None):\n    \"\"\"Update submission_df so that the predictions for the two sides of the hyperplane don't overlap.\n    \n    Parameters\n    ----------\n    submission_df : pandas DataFrame with columns 'id' and 'target'\n    test_df : the competition's test data\n    pure_df : the competition's original training data\n    \n    From https://www.kaggle.com/ambrosm/tpsnov21-007-postprocessing\n    \"\"\"\n    if pure_df is None: pure_df = pd.read_csv('../input/november21/train.csv')\n    if pure_df.shape != (600000, 102): raise ValueError(\"pure_df has the wrong shape\")\n    if test_df is None: test_df = pd.read_csv('../input/tabular-playground-series-nov-2021/test.csv')\n    if test_df.shape[0] != submission_df.shape[0] or test_df.shape[1] != 101: raise ValueError(\"test_df has the wrong shape\")\n\n    # Find the separating hyperplane for pure_df, step 1\n    # Use an SVM with almost no regularization\n    model1 = make_pipeline(StandardScaler(), LinearSVC(C=1e5, tol=1e-7, penalty='l2', dual=False, max_iter=2000, random_state=1))\n    model1.fit(pure_df.drop(columns=['id', 'target']), pure_df.target)\n    pure_pred = model1.predict(pure_df.drop(columns=['id', 'target']))\n    print((pure_pred != pure_df.target).sum(), (pure_pred == pure_df.target).sum()) # 1 599999\n    # model1 is not perfect: it predicts the wrong class for 1 of 600000 samples\n\n    # Find the separating hyperplane for pure_df, step 2\n    # Fit a second SVM to a subset of the points which contains the support vectors\n    pure_pred = model1.decision_function(pure_df.drop(columns=['id', 'target']))\n    subset_df = pure_df[(pure_pred > -5) & (pure_pred < 0.9)]\n    model2 = make_pipeline(StandardScaler(), LinearSVC(C=1e5, tol=1e-7, penalty='l2', dual=False, max_iter=2000, random_state=1))\n    model2.fit(subset_df.drop(columns=['id', 'target']), subset_df.target)\n    pure_pred = model2.predict(pure_df.drop(columns=['id', 'target']))\n    print((pure_pred != pure_df.target).sum(), (pure_pred == pure_df.target).sum()) # 0 600000\n    # model2 is perfect: it predicts the correct class for all 600000 training samples\n    \n    pure_test_pred = model2.predict(test_df.drop(columns=['id', 'target'], errors='ignore'))\n    lmax, rmin = submission_df[pure_test_pred == 0].target.max(), submission_df[pure_test_pred == 1].target.min()\n    if lmax < rmin:\n        print(\"There is no overlap. No postprocessing needed.\")\n        return\n    # There is overlap. Remove this overlap\n    submission_df.loc[pure_test_pred == 0, 'target'] -= lmax + 1\n    submission_df.loc[pure_test_pred == 1, 'target'] -= rmin - 1\n    print(submission_df[pure_test_pred == 0].target.min(), submission_df[pure_test_pred == 0].target.max(),\n          submission_df[pure_test_pred == 1].target.min(), submission_df[pure_test_pred == 1].target.max())\n","metadata":{"execution":{"iopub.status.busy":"2021-11-23T06:18:29.669969Z","iopub.execute_input":"2021-11-23T06:18:29.670206Z","iopub.status.idle":"2021-11-23T06:18:29.683269Z","shell.execute_reply.started":"2021-11-23T06:18:29.670178Z","shell.execute_reply":"2021-11-23T06:18:29.682331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a baseline submission which always predicts -1 or +1 and has an lb score of 0.74723\nbaseline = pd.DataFrame({'id': test_df.id, 'target': 0})\npostprocess_separate(baseline, test_df=test_df.drop(columns='chunk'), pure_df=pure_df)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-23T06:18:29.684902Z","iopub.execute_input":"2021-11-23T06:18:29.685133Z","iopub.status.idle":"2021-11-23T06:18:59.431375Z","shell.execute_reply.started":"2021-11-23T06:18:29.685104Z","shell.execute_reply":"2021-11-23T06:18:59.430622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create 18 probe submissions to gather information\nfor chunk in range(10, 19):\n    sub = baseline.copy()\n    sub.loc[(test_df.chunk == chunk) & (sub.target < 0), 'target'] = -10\n    sub.to_csv(f'submission_probe_{chunk}_H0.csv', index=False)\n    sub = baseline.copy()\n    sub.loc[(test_df.chunk == chunk) & (sub.target > 0), 'target'] = +10\n    sub.to_csv(f'submission_probe_{chunk}_H1.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-23T06:18:59.43352Z","iopub.execute_input":"2021-11-23T06:18:59.434016Z","iopub.status.idle":"2021-11-23T06:19:10.199749Z","shell.execute_reply.started":"2021-11-23T06:18:59.43397Z","shell.execute_reply":"2021-11-23T06:19:10.198788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I submitted 9 of these probe submissions and got the following probabilities:","metadata":{}},{"cell_type":"code","source":"p_dict = {10: 0.26245756846719176,\n          17: 0.25772808586762075,\n          16: 0.25038670867946144,\n          13: 0.2498515790341643,\n          18: 0.24863555967320816,\n          11: 0.2476293839324911,\n          14: 0.2448713889988128,\n          12: 0.24464126228044064,\n          15: 0.2418890814558059}","metadata":{"execution":{"iopub.status.busy":"2021-11-23T06:19:10.201043Z","iopub.execute_input":"2021-11-23T06:19:10.201292Z","iopub.status.idle":"2021-11-23T06:19:10.206284Z","shell.execute_reply.started":"2021-11-23T06:19:10.201264Z","shell.execute_reply":"2021-11-23T06:19:10.205211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we create a new submission from these probabilities. This submission has an lb score of 0.74896.","metadata":{}},{"cell_type":"code","source":"sub = baseline.copy()\nsub.loc[baseline.target == 1, 'target'] = 0.75\nfor chunk in range(10, 19):\n    sub.loc[(test_df.chunk == chunk) & (sub.target < 0), 'target'] = p_dict[chunk]\nsub.to_csv(f'submission_probed.csv', index=False)\nsub.head(20)","metadata":{"execution":{"iopub.status.busy":"2021-11-23T06:19:10.20777Z","iopub.execute_input":"2021-11-23T06:19:10.207996Z","iopub.status.idle":"2021-11-23T06:19:11.382601Z","shell.execute_reply.started":"2021-11-23T06:19:10.207971Z","shell.execute_reply":"2021-11-23T06:19:11.381593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can do better than that. f93 is the single feature which best predicts the ranking of samples within a chunk. We therefore subtract a tiny amount of f93 from the per-chunk predictions to get an lb score of 0.74902. Isn't this a nice example of feature selection?","metadata":{}},{"cell_type":"code","source":"sub = baseline.copy()\nsub.loc[baseline.target == 1, 'target'] = 0.75\nfor chunk in range(10, 19):\n    sub.loc[(test_df.chunk == chunk) & (sub.target < 0), 'target'] = p_dict[chunk] - test_df.f93.loc[(test_df.chunk == chunk) & (sub.target < 0)] * 1e-5\nsub.to_csv(f'submission_probed_f93.csv', index=False)\nsub.head(20)","metadata":{"execution":{"iopub.status.busy":"2021-11-23T06:53:31.733204Z","iopub.execute_input":"2021-11-23T06:53:31.734071Z","iopub.status.idle":"2021-11-23T06:53:32.929223Z","shell.execute_reply.started":"2021-11-23T06:53:31.734012Z","shell.execute_reply":"2021-11-23T06:53:32.928534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now I'm going to blend this result with the result of the notebook I published a few days ago to get the final predictions:","metadata":{}},{"cell_type":"code","source":"other_submission = pd.read_csv('../input/tpsnov21-007-postprocessing/submission_postprocessed.csv')\nsub['target'] += other_submission.target\nsub.to_csv(f'submission_probed_f93_blended.csv', index=False)\nsub.head(20)","metadata":{"execution":{"iopub.status.busy":"2021-11-23T06:53:48.07536Z","iopub.execute_input":"2021-11-23T06:53:48.07632Z","iopub.status.idle":"2021-11-23T06:53:49.411389Z","shell.execute_reply.started":"2021-11-23T06:53:48.076266Z","shell.execute_reply":"2021-11-23T06:53:49.410057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"What remains to be done? Probe the other nine probabilities - and you'll get a much higher score!","metadata":{}}]}