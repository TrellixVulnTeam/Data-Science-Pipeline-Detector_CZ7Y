{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importing Librabies and Loading datasets","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Plot\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Feature Selection\nfrom sklearn.feature_selection import SelectPercentile, f_classif\n\n# Modelling\nfrom sklearn.model_selection import train_test_split\n\n# Regression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import roc_auc_score\nfrom xgboost import XGBRegressor\n\n# Classification\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom xgboost import XGBClassifier\n\n# Cross-Validation\nfrom sklearn.model_selection import StratifiedKFold","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-17T00:45:47.196529Z","iopub.execute_input":"2021-11-17T00:45:47.1971Z","iopub.status.idle":"2021-11-17T00:45:48.447939Z","shell.execute_reply.started":"2021-11-17T00:45:47.197007Z","shell.execute_reply":"2021-11-17T00:45:48.447048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_data = pd.read_csv('../input/tabular-playground-series-nov-2021/sample_submission.csv')\ntrain_data = pd.read_csv('../input/tabular-playground-series-nov-2021/train.csv')\ntest_data = pd.read_csv('../input/tabular-playground-series-nov-2021/test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-11-17T00:45:48.452751Z","iopub.execute_input":"2021-11-17T00:45:48.453025Z","iopub.status.idle":"2021-11-17T00:46:16.330879Z","shell.execute_reply.started":"2021-11-17T00:45:48.452991Z","shell.execute_reply":"2021-11-17T00:46:16.330147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Explore Data","metadata":{}},{"cell_type":"code","source":"train_data.head()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-17T00:46:16.332107Z","iopub.execute_input":"2021-11-17T00:46:16.332387Z","iopub.status.idle":"2021-11-17T00:46:16.369263Z","shell.execute_reply.started":"2021-11-17T00:46:16.332351Z","shell.execute_reply":"2021-11-17T00:46:16.368441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.describe()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-17T00:46:16.371468Z","iopub.execute_input":"2021-11-17T00:46:16.371839Z","iopub.status.idle":"2021-11-17T00:46:18.465877Z","shell.execute_reply.started":"2021-11-17T00:46:16.371797Z","shell.execute_reply":"2021-11-17T00:46:18.465155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.columns\n# It appears there is a target column.","metadata":{"execution":{"iopub.status.busy":"2021-11-17T00:46:18.467438Z","iopub.execute_input":"2021-11-17T00:46:18.467959Z","iopub.status.idle":"2021-11-17T00:46:18.475984Z","shell.execute_reply.started":"2021-11-17T00:46:18.467919Z","shell.execute_reply":"2021-11-17T00:46:18.475173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Basic Data Check\n\nCredits to https://www.kaggle.com/raahulsaxena/tps-nov-21-data-check-feature-analysis\n\nAlthough this data doesn't contain any missing, duplicated, categorical variables etc. This notebook helped me a lot to understand the basics of data check.","metadata":{}},{"cell_type":"markdown","source":"## Handle missing values","metadata":{}},{"cell_type":"code","source":"# Missing values\nmissing_values_train = train_data.isna().any().sum()\nprint('Missing values in train data: {0}'.format(missing_values_train[missing_values_train > 0]))\n\nmissing_values_test = test_data.isna().any().sum()\nprint('Missing values in test data: {0}'.format(missing_values_test[missing_values_test > 0]))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-17T00:46:18.47702Z","iopub.execute_input":"2021-11-17T00:46:18.477224Z","iopub.status.idle":"2021-11-17T00:46:18.600785Z","shell.execute_reply.started":"2021-11-17T00:46:18.477198Z","shell.execute_reply":"2021-11-17T00:46:18.599919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Handle duplicates","metadata":{}},{"cell_type":"code","source":"# Duplicates\nduplicates_train = train_data.duplicated().sum()\nprint('Duplicates in train data: {0}'.format(duplicates_train))\n\nduplicates_test = test_data.duplicated().sum()\nprint('Duplicates in test data: {0}'.format(duplicates_test))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-17T00:46:18.602382Z","iopub.execute_input":"2021-11-17T00:46:18.602659Z","iopub.status.idle":"2021-11-17T00:46:28.878955Z","shell.execute_reply.started":"2021-11-17T00:46:18.60262Z","shell.execute_reply":"2021-11-17T00:46:28.878002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Categorical variables","metadata":{}},{"cell_type":"code","source":"categorical_train = train_data.nunique().sort_values(ascending=True)\nprint('Categorical variables in train data: \\n{0}'.format(categorical_train))\n\ncategorical_test = test_data.nunique().sort_values(ascending=True)\nprint('Categorical variables in train data: \\n{0}'.format(categorical_test))\n# No categorical variable other than the `target`.","metadata":{"execution":{"iopub.status.busy":"2021-11-17T00:46:28.880238Z","iopub.execute_input":"2021-11-17T00:46:28.880528Z","iopub.status.idle":"2021-11-17T00:46:33.639732Z","shell.execute_reply.started":"2021-11-17T00:46:28.880487Z","shell.execute_reply":"2021-11-17T00:46:33.638989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Correlated variables","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 2)\nsns.heatmap(train_data.corr(), ax=ax[0])\nsns.heatmap(test_data.corr(), ax=ax[1])\nfig.set_figheight(8)\nfig.set_figwidth(16)\nfig.show()\n# Variables are not correlated","metadata":{"execution":{"iopub.status.busy":"2021-11-17T00:46:33.641122Z","iopub.execute_input":"2021-11-17T00:46:33.641563Z","iopub.status.idle":"2021-11-17T00:47:03.192321Z","shell.execute_reply.started":"2021-11-17T00:46:33.641524Z","shell.execute_reply":"2021-11-17T00:47:03.191641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Features","metadata":{}},{"cell_type":"code","source":"# Get train data without the target and ids\ndata = train_data.iloc[:, 1:-1].copy()\n# Get the target\ny = train_data.target.copy()\n\n# It takes time to handle all of the data.\n# So, I am using a smaller portion of the data\n# while debugging/testing.\n#data = train_data.iloc[0:50000, 1:-1].copy()\n#y = train_data.target[0:50000].copy()","metadata":{"execution":{"iopub.status.busy":"2021-11-17T00:47:03.195122Z","iopub.execute_input":"2021-11-17T00:47:03.195956Z","iopub.status.idle":"2021-11-17T00:47:03.515813Z","shell.execute_reply.started":"2021-11-17T00:47:03.195918Z","shell.execute_reply":"2021-11-17T00:47:03.515007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I am actually planing to make a feature analysis but for now keep it simple.\n\nCredits to https://www.kaggle.com/markosthabit/tbs-november-naive-bayes","metadata":{}},{"cell_type":"code","source":"# Select features\nprint(f\"Data shape before selection: {data.shape}\")\nFeatureSelection = SelectPercentile(score_func=f_classif, percentile=20)\nselected = FeatureSelection.fit_transform(data, y)\nprint(f\"Data shape after selection: {selected.shape}\")\n\n# Get the list of the selected features\nselected_features = np.where(FeatureSelection.get_support())\nprint(f\"Selected Features: {selected_features}\") \n\nselected_features = [f'f{feature}' for feature in selected_features[0]]","metadata":{"execution":{"iopub.status.busy":"2021-11-17T00:47:03.517313Z","iopub.execute_input":"2021-11-17T00:47:03.517562Z","iopub.status.idle":"2021-11-17T00:47:04.462391Z","shell.execute_reply.started":"2021-11-17T00:47:03.517529Z","shell.execute_reply":"2021-11-17T00:47:04.461599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Since I am not sure about my selected features.\n# Sometimes it is better not even using them.\ndef get_X(use_selected_features=True):\n    if use_selected_features:\n        return data[selected_features]\n    return data","metadata":{"execution":{"iopub.status.busy":"2021-11-17T00:47:04.463784Z","iopub.execute_input":"2021-11-17T00:47:04.464232Z","iopub.status.idle":"2021-11-17T00:47:04.46885Z","shell.execute_reply.started":"2021-11-17T00:47:04.464194Z","shell.execute_reply":"2021-11-17T00:47:04.467894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modelling","metadata":{}},{"cell_type":"markdown","source":"I am not sure which modelling approach will give the best results. So, why not try many of them?\n\nAlso, different models will generate different predictions. Since, we are also allowed to submit probabilities, both predictions which are binary (0, 1) in classification models and probabilities in regression models should be okay to submit. It is also possible using `predict_proba` instead of `predict` in classification models.\n\n**I am trying what I have learned so far, so please comment if I am doing something wrong or weird :).**\n\n> It actually takes hours to run all those models, so instead of running them everytime I will directly give the outputs from my previous runs.","metadata":{}},{"cell_type":"code","source":"# Break data into two pieces or normalize\n# Gaussian Naive Bayes and Logistic Regression works with normalized data\ndef split_data(X, y, normalize=False):\n    if normalize:\n        scaler = StandardScaler()\n        normalized = scaler.fit_transform(X.copy())\n        return train_test_split(normalized, y, random_state=1)\n    return train_test_split(X, y, random_state=1)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-17T00:47:04.470341Z","iopub.execute_input":"2021-11-17T00:47:04.47059Z","iopub.status.idle":"2021-11-17T00:47:04.480026Z","shell.execute_reply.started":"2021-11-17T00:47:04.470557Z","shell.execute_reply":"2021-11-17T00:47:04.479253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Regression","metadata":{}},{"cell_type":"code","source":"def run_regression_algorithm(X, y, model, n, text, early_stopping_rounds = None):\n    # It actually takes hours to run all those models, \n    # so instead of running them everytime I will directly\n    # give the outputs from my previous runs.\n    return\n\n    # Split data\n    train_X, val_X, train_y, val_y = split_data(X, y)\n    # Fit model\n    if early_stopping_rounds: # For XGBRegressor\n        model.fit(train_X, train_y, early_stopping_rounds=early_stopping_rounds,\n                  eval_set=[(val_X, val_y)], verbose=False)\n    else:\n        model.fit(train_X, train_y)\n    # Make predictions\n    predictions = model.predict(val_X)\n    # Get AUC\n    auc = roc_auc_score(val_y, predictions)\n    # Print the error\n    print('{0}{1}AUC:  {2}'.format(text, n, auc))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-17T00:47:04.4814Z","iopub.execute_input":"2021-11-17T00:47:04.481665Z","iopub.status.idle":"2021-11-17T00:47:04.4904Z","shell.execute_reply.started":"2021-11-17T00:47:04.481628Z","shell.execute_reply":"2021-11-17T00:47:04.489692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Decision Tree Regressor","metadata":{}},{"cell_type":"code","source":"# Compare with different values of max_leaf_nodes\nfor max_leaf_nodes in [5, 50, 500, 5000]:\n    run_regression_algorithm(get_X(), y,\n                             DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0),\n                             '{0}  \\t\\t '.format(max_leaf_nodes), 'Max leaf nodes: ')","metadata":{"execution":{"iopub.status.busy":"2021-11-17T00:47:04.491658Z","iopub.execute_input":"2021-11-17T00:47:04.491928Z","iopub.status.idle":"2021-11-17T00:47:04.62177Z","shell.execute_reply.started":"2021-11-17T00:47:04.491891Z","shell.execute_reply":"2021-11-17T00:47:04.620745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"```\nMax leaf nodes: 5           AUC:  0.58807\nMax leaf nodes: 50          AUC:  0.62875\nMax leaf nodes: 500         AUC:  0.64594\nMax leaf nodes: 5000        AUC:  0.63100\n```","metadata":{}},{"cell_type":"markdown","source":"### Random Forest Regressor","metadata":{}},{"cell_type":"code","source":"model = RandomForestRegressor(random_state=1)\nrun_regression_algorithm(get_X(), y, model, '', '')","metadata":{"execution":{"iopub.status.busy":"2021-11-17T00:47:04.623144Z","iopub.execute_input":"2021-11-17T00:47:04.624117Z","iopub.status.idle":"2021-11-17T00:47:04.661858Z","shell.execute_reply.started":"2021-11-17T00:47:04.624073Z","shell.execute_reply":"2021-11-17T00:47:04.66094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"```\nAUC:  0.70127\n```","metadata":{}},{"cell_type":"markdown","source":"### XGBoost XGBRegressor","metadata":{}},{"cell_type":"code","source":"# Compare with different values of n_estimators\nfor n_estimators in range(100, 1000, 100):\n    run_regression_algorithm(get_X(), y,\n                             XGBRegressor(n_estimators=n_estimators, learning_rate=0.05, n_jobs=4),\n                             '{0}  \\t\\t '.format(n_estimators), 'N estimators: ', 5)","metadata":{"execution":{"iopub.status.busy":"2021-11-17T00:47:04.66368Z","iopub.execute_input":"2021-11-17T00:47:04.66401Z","iopub.status.idle":"2021-11-17T00:47:04.92583Z","shell.execute_reply.started":"2021-11-17T00:47:04.66397Z","shell.execute_reply":"2021-11-17T00:47:04.925077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"```\nN estimators: 100  \t\t AUC:  0.71377\nN estimators: 200  \t\t AUC:  0.71857\nN estimators: 300  \t\t AUC:  0.71919\nN estimators: 400  \t\t AUC:  0.71921\nN estimators: 500  \t\t AUC:  0.71921\nN estimators: 600  \t\t AUC:  0.71921\nN estimators: 700  \t\t AUC:  0.71921\nN estimators: 800  \t\t AUC:  0.71921\nN estimators: 900  \t\t AUC:  0.71921\n```","metadata":{}},{"cell_type":"markdown","source":"## Regression with Cross-Validation","metadata":{}},{"cell_type":"code","source":"# Cross-validation, https://www.kaggle.com/hamzaghanmi/make-it-simple/notebook\ndef run_regression_algoritm_with_cross_validation(X, y, model, early_stopping_rounds = None):\n    # It actually takes hours to run all those models, \n    # so instead of running them everytime I will directly\n    # give the outputs from my previous runs.\n    return\n\n    fold = 1\n    skf = StratifiedKFold(n_splits=15, random_state=48, shuffle=True)\n    for train_idx, test_idx in skf.split(X, y):\n        train_X, val_X = X.iloc[train_idx], X.iloc[test_idx]\n        train_y, val_y = y.iloc[train_idx], y.iloc[test_idx]\n\n        # Fit model\n        if early_stopping_rounds: # For XGBRegressor\n            model.fit(train_X, train_y, early_stopping_rounds=early_stopping_rounds,\n                      eval_set=[(val_X, val_y)], verbose=False)\n        else:\n            model.fit(train_X, train_y)\n        # Make predictions\n        predictions = model.predict(val_X)\n        # Get AUC\n        auc = roc_auc_score(val_y, predictions)\n        # Print the AUC\n        print(\"Fold: %d  \\t\\t AUC:  %f\" %(fold, auc))\n        fold += 1","metadata":{"execution":{"iopub.status.busy":"2021-11-17T00:47:04.926995Z","iopub.execute_input":"2021-11-17T00:47:04.927251Z","iopub.status.idle":"2021-11-17T00:47:04.935386Z","shell.execute_reply.started":"2021-11-17T00:47:04.927216Z","shell.execute_reply":"2021-11-17T00:47:04.934692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Decision Tree Regressor","metadata":{}},{"cell_type":"code","source":"model = DecisionTreeRegressor(max_leaf_nodes=500, random_state=0)\nrun_regression_algoritm_with_cross_validation(get_X(), y, model)","metadata":{"execution":{"iopub.status.busy":"2021-11-17T00:47:04.936678Z","iopub.execute_input":"2021-11-17T00:47:04.9371Z","iopub.status.idle":"2021-11-17T00:47:50.156683Z","shell.execute_reply.started":"2021-11-17T00:47:04.937064Z","shell.execute_reply":"2021-11-17T00:47:50.155132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"```\nFold: 1  \t\t AUC:  0.648110\nFold: 2  \t\t AUC:  0.645001\nFold: 3  \t\t AUC:  0.646716\nFold: 4  \t\t AUC:  0.651258\nFold: 5  \t\t AUC:  0.650062\nFold: 6  \t\t AUC:  0.650459\nFold: 7  \t\t AUC:  0.646524\nFold: 8  \t\t AUC:  0.652755\nFold: 9  \t\t AUC:  0.652514\nFold: 10 \t\t AUC:  0.647052\nFold: 11 \t\t AUC:  0.650992\nFold: 12 \t\t AUC:  0.646726\nFold: 13 \t\t AUC:  0.644246\nFold: 14 \t\t AUC:  0.648659\nFold: 15 \t\t AUC:  0.645532\n```","metadata":{}},{"cell_type":"markdown","source":"### Random Forest Regressor","metadata":{}},{"cell_type":"code","source":"model = RandomForestRegressor(random_state=1)\nrun_regression_algoritm_with_cross_validation(get_X(), y, model)","metadata":{"execution":{"iopub.status.busy":"2021-11-17T00:47:50.15774Z","iopub.status.idle":"2021-11-17T00:47:50.158217Z","shell.execute_reply.started":"2021-11-17T00:47:50.157981Z","shell.execute_reply":"2021-11-17T00:47:50.158005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"```\nFold: 1  \t\t AUC:  0.698372\nFold: 2  \t\t AUC:  0.699659\nFold: 3  \t\t AUC:  0.701081\nFold: 4  \t\t AUC:  0.705221\nFold: 5  \t\t AUC:  0.703162\nFold: 6  \t\t AUC:  0.702732\nFold: 7  \t\t AUC:  0.698388\nFold: 8  \t\t AUC:  0.704422\nFold: 9  \t\t AUC:  0.702508\nFold: 10 \t\t AUC:  0.699164\nFold: 11 \t\t AUC:  0.703322\nFold: 12 \t\t AUC:  0.699437\nFold: 13 \t\t AUC:  0.698628\nFold: 14 \t\t AUC:  0.703577\nFold: 15 \t\t AUC:  0.697842\n```","metadata":{}},{"cell_type":"markdown","source":"### XGBoost XGBRegressor","metadata":{}},{"cell_type":"code","source":"model = XGBRegressor(n_estimators=500, learning_rate=0.05, n_jobs=4)\nrun_regression_algoritm_with_cross_validation(get_X(), y, model, 5)","metadata":{"execution":{"iopub.status.busy":"2021-11-17T00:47:54.727704Z","iopub.execute_input":"2021-11-17T00:47:54.728046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"```\nFold: 1  \t\t AUC:  0.715399\nFold: 2  \t\t AUC:  0.716105\nFold: 3  \t\t AUC:  0.714442\nFold: 4  \t\t AUC:  0.723253\nFold: 5  \t\t AUC:  0.719537\nFold: 6  \t\t AUC:  0.720576\nFold: 7  \t\t AUC:  0.716693\nFold: 8  \t\t AUC:  0.722241\nFold: 9  \t\t AUC:  0.720729\nFold: 10 \t\t AUC:  0.719843\nFold: 11 \t\t AUC:  0.721102\nFold: 12 \t\t AUC:  0.716606\nFold: 13 \t\t AUC:  0.718105\nFold: 14 \t\t AUC:  0.721361\nFold: 15 \t\t AUC:  0.716609\n```","metadata":{}},{"cell_type":"markdown","source":"## Classification","metadata":{}},{"cell_type":"code","source":"def run_classification_algoritm(X, y, model, normalize):\n    # It actually takes hours to run all those models, \n    # so instead of running them everytime I will directly\n    # give the outputs from my previous runs.\n    return\n    \n    # Split data\n    train_X, val_X, train_y, val_y = split_data(X, y, normalize)\n    # Fit model\n    model.fit(train_X, train_y)\n    # Make predictions\n    predictions = model.predict(val_X)\n    # Get the accuracy score\n    score = accuracy_score(val_y, predictions)\n    # Print the accuracy\n    print(\"Accuracy score:  %f\" %(score))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-17T00:47:50.161424Z","iopub.status.idle":"2021-11-17T00:47:50.16207Z","shell.execute_reply.started":"2021-11-17T00:47:50.16183Z","shell.execute_reply":"2021-11-17T00:47:50.161855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Gaussian Naive Bayes","metadata":{}},{"cell_type":"code","source":"# Credits to https://www.kaggle.com/markosthabit/tbs-november-naive-bayes\n# https://iq.opengenus.org/gaussian-naive-bayes/\n#     Gaussian Naive Bayes is a variant of Naive Bayes that follows\n#     Gaussian normal distribution and supports continuous data.\n# So, normalize the data and actually try both ways to get the difference.  \nmodel = GaussianNB()\nrun_classification_algoritm(get_X(), y, model, True)\nrun_classification_algoritm(get_X(), y, model, False)","metadata":{"execution":{"iopub.status.busy":"2021-11-17T00:47:50.163496Z","iopub.status.idle":"2021-11-17T00:47:50.163899Z","shell.execute_reply.started":"2021-11-17T00:47:50.163681Z","shell.execute_reply":"2021-11-17T00:47:50.163703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"```\nAccuracy score:  0.676300\nAccuracy score:  0.676300\n```","metadata":{}},{"cell_type":"markdown","source":"### XGBoost XGBClassifier","metadata":{}},{"cell_type":"code","source":"# Credits to https://www.kaggle.com/sugamkhetrapal/tps-nov-2021-1-06-xgboost/notebook\nmodel = XGBClassifier(max_depth=1, subsample=0.5, colsample_bytree=0.5, eval_metric='error', use_label_encoder=False, random_state=1)\nrun_classification_algoritm(get_X(), y, model, False)","metadata":{"execution":{"iopub.status.busy":"2021-11-17T00:47:50.165222Z","iopub.status.idle":"2021-11-17T00:47:50.165797Z","shell.execute_reply.started":"2021-11-17T00:47:50.165564Z","shell.execute_reply":"2021-11-17T00:47:50.165588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"```\nAccuracy score:  0.672873\n```","metadata":{}},{"cell_type":"markdown","source":"### Logistic Regression","metadata":{}},{"cell_type":"code","source":"# Credits to https://www.kaggle.com/hamzaghanmi/make-it-simple\n# https://kambria.io/blog/logistic-regression-for-machine-learning/\nmodel = LogisticRegression(solver='liblinear')\nrun_classification_algoritm(get_X(False), y, model, True)\nrun_classification_algoritm(get_X(False), y, model, False)","metadata":{"execution":{"iopub.status.busy":"2021-11-17T00:47:50.166839Z","iopub.status.idle":"2021-11-17T00:47:50.167731Z","shell.execute_reply.started":"2021-11-17T00:47:50.167497Z","shell.execute_reply":"2021-11-17T00:47:50.167521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"```\nAccuracy score:  0.736847\nAccuracy score:  0.736813\n```","metadata":{}},{"cell_type":"markdown","source":"## Classification with Cross-Validation","metadata":{}},{"cell_type":"code","source":"# Cross-validation, https://www.kaggle.com/hamzaghanmi/make-it-simple/notebook\ndef run_classification_algoritm_with_cross_validation(X, y, model, normalize):\n    # It actually takes hours to run all those models, \n    # so instead of running them everytime I will directly\n    # give the outputs from my previous runs.\n    return\n\n    X = np.array(X)\n    # Apply standard scaler\n    if normalize:\n        scaler = StandardScaler()\n        X = scaler.fit_transform(X)\n\n    fold = 1\n    skf = StratifiedKFold(n_splits=15, random_state=48, shuffle=True)\n    for train_idx, test_idx in skf.split(X, y):\n        train_X, val_X = X[train_idx], X[test_idx]\n        train_y, val_y = y.iloc[train_idx], y.iloc[test_idx]\n\n        # Fit the model\n        model.fit(train_X, train_y)\n        # Make predictions\n        predictions = model.predict_proba(val_X)[:,1]\n        # Get AUC\n        auc = roc_auc_score(val_y, predictions)\n        # Print the AUC\n        print(\"Fold: %d  \\t\\t AUC:  %f\" %(fold, auc))\n        fold += 1","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-17T00:47:50.168882Z","iopub.status.idle":"2021-11-17T00:47:50.169309Z","shell.execute_reply.started":"2021-11-17T00:47:50.169065Z","shell.execute_reply":"2021-11-17T00:47:50.169087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Gaussian Naive Bayes","metadata":{}},{"cell_type":"code","source":"model = GaussianNB()\nrun_classification_algoritm_with_cross_validation(get_X(), y, model, True)","metadata":{"execution":{"iopub.status.busy":"2021-11-17T00:47:50.170587Z","iopub.status.idle":"2021-11-17T00:47:50.171023Z","shell.execute_reply.started":"2021-11-17T00:47:50.170804Z","shell.execute_reply":"2021-11-17T00:47:50.170826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"```\nFold: 1  \t\t AUC:  0.709428\nFold: 2  \t\t AUC:  0.709001\nFold: 3  \t\t AUC:  0.710660\nFold: 4  \t\t AUC:  0.714958\nFold: 5  \t\t AUC:  0.712237\nFold: 6  \t\t AUC:  0.715526\nFold: 7  \t\t AUC:  0.708318\nFold: 8  \t\t AUC:  0.714901\nFold: 9  \t\t AUC:  0.714512\nFold: 10 \t\t AUC:  0.711692\nFold: 11 \t\t AUC:  0.715017\nFold: 12 \t\t AUC:  0.708338\nFold: 13 \t\t AUC:  0.710486\nFold: 14 \t\t AUC:  0.713657\nFold: 15 \t\t AUC:  0.712419\n```","metadata":{}},{"cell_type":"markdown","source":"### XGBoost XGBClassifier","metadata":{}},{"cell_type":"code","source":"model = XGBClassifier(max_depth=1, subsample=0.5, colsample_bytree=0.5, eval_metric='error', use_label_encoder=False, random_state=1)\nrun_classification_algoritm_with_cross_validation(get_X(), y, model, False)","metadata":{"execution":{"iopub.status.busy":"2021-11-17T00:47:50.173045Z","iopub.status.idle":"2021-11-17T00:47:50.173599Z","shell.execute_reply.started":"2021-11-17T00:47:50.173359Z","shell.execute_reply":"2021-11-17T00:47:50.173384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"```\nFold: 1  \t\t AUC:  0.710758\nFold: 2  \t\t AUC:  0.711012\nFold: 3  \t\t AUC:  0.710687\nFold: 4  \t\t AUC:  0.717570\nFold: 5  \t\t AUC:  0.714278\nFold: 6  \t\t AUC:  0.715372\nFold: 7  \t\t AUC:  0.712060\nFold: 8  \t\t AUC:  0.717004\nFold: 9  \t\t AUC:  0.716060\nFold: 10 \t\t AUC:  0.714406\nFold: 11 \t\t AUC:  0.716232\nFold: 12 \t\t AUC:  0.710764\nFold: 13 \t\t AUC:  0.711267\nFold: 14 \t\t AUC:  0.715551\nFold: 15 \t\t AUC:  0.710048\n```","metadata":{}},{"cell_type":"markdown","source":"### Logistic Regression","metadata":{}},{"cell_type":"code","source":"model = LogisticRegression(solver='liblinear')\nrun_classification_algoritm_with_cross_validation(get_X(False), y, model, True)","metadata":{"execution":{"iopub.status.busy":"2021-11-17T00:47:50.174997Z","iopub.status.idle":"2021-11-17T00:47:50.175749Z","shell.execute_reply.started":"2021-11-17T00:47:50.175513Z","shell.execute_reply":"2021-11-17T00:47:50.175538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"```\nFold: 1  \t\t AUC:  0.748223\nFold: 2  \t\t AUC:  0.747558\nFold: 3  \t\t AUC:  0.744476\nFold: 4  \t\t AUC:  0.751129\nFold: 5  \t\t AUC:  0.750191\nFold: 6  \t\t AUC:  0.750874\nFold: 7  \t\t AUC:  0.746259\nFold: 8  \t\t AUC:  0.750019\nFold: 9  \t\t AUC:  0.751094\nFold: 10 \t\t AUC:  0.749236\nFold: 11 \t\t AUC:  0.750499\nFold: 12 \t\t AUC:  0.746850\nFold: 13 \t\t AUC:  0.747999\nFold: 14 \t\t AUC:  0.753662\nFold: 15 \t\t AUC:  0.747682\n```","metadata":{}},{"cell_type":"markdown","source":"## Final model\n\nSo far so good simplest solution gives the best results. So I have picked the `LogisticRegression` as the final model.\n\nCredits to https://www.kaggle.com/hamzaghanmi/make-it-simple/notebook","metadata":{}},{"cell_type":"code","source":"# Create X, do not use features for now.\nX = get_X(False)\n\n# Create test X, drop ids.\n# For now create it without selected features\n# It gives better results this way.\ntest_X = test_data.iloc[:, 1:]\n\n# Apply standard scaler\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\ntest_X = scaler.transform(test_X)\n\n# Create the model\nmodel = LogisticRegression(solver='liblinear')\n\n# Cross-validation, https://www.kaggle.com/hamzaghanmi/make-it-simple/notebook\nfold = 1\ntest_predictions = np.zeros(test_X.shape[0])\nskf = StratifiedKFold(n_splits=15, random_state=48, shuffle=True)\nfor train_idx, test_idx in skf.split(X, y):\n    train_X, val_X = X[train_idx], X[test_idx]\n    train_y, val_y = y.iloc[train_idx], y.iloc[test_idx]\n\n    # Fit the model\n    model.fit(train_X, train_y)\n    # Make predictions\n    predictions = model.predict_proba(val_X)[:,1]\n    # Get AUC\n    auc = roc_auc_score(val_y, predictions)\n    # Print the error\n    print(\"Fold: %d  \\t\\t AUC:  %f\" %(fold, auc))\n\n    # Make predictions, use probability\n    test_predictions += model.predict_proba(test_X)[:,1] / skf.n_splits\n    fold += 1","metadata":{"execution":{"iopub.status.busy":"2021-11-17T00:47:50.177028Z","iopub.status.idle":"2021-11-17T00:47:50.177959Z","shell.execute_reply.started":"2021-11-17T00:47:50.17771Z","shell.execute_reply":"2021-11-17T00:47:50.177734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"# Run the code to save predictions in the format used for competition scoring\noutput = pd.DataFrame({'id': test_data.id, 'target': test_predictions})\noutput.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-17T00:47:50.179039Z","iopub.status.idle":"2021-11-17T00:47:50.179879Z","shell.execute_reply.started":"2021-11-17T00:47:50.179641Z","shell.execute_reply":"2021-11-17T00:47:50.179665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output","metadata":{"execution":{"iopub.status.busy":"2021-11-17T00:47:50.180943Z","iopub.status.idle":"2021-11-17T00:47:50.181641Z","shell.execute_reply.started":"2021-11-17T00:47:50.181394Z","shell.execute_reply":"2021-11-17T00:47:50.181419Z"},"trusted":true},"execution_count":null,"outputs":[]}]}