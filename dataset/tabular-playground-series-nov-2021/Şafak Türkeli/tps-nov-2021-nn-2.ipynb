{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Deep Learning with Feature Engineering\n\n## Credits\nThis notebook is copied from https://www.kaggle.com/adityasharma01/simple-nn-tps-nov-21  \nThe original is https://www.kaggle.com/javiervallejos/simple-nn-with-good-results-tps-nov-21  \n\n## What I am doing in this notebook?\nFirst of all, I am currently learning neural networks, so don't expect too much from me.  \n\nI was actually following [the deep learning courses](https://www.kaggle.com/learn/intro-to-deep-learning), so I created a model after I have learned couple things. You can find that notebook [here.](https://www.kaggle.com/sfktrkl/tps-nov-2021-nn)  \nI wanted to create a model which produces good results but appearently it was not a perfect model :).\n\nThen, I have also started to [feature engineering courses](https://www.kaggle.com/learn/feature-engineering).  \nI tried couple things in [this notebook](https://www.kaggle.com/sfktrkl/tps-nov-2021-nn-with-feature-engineering) but it didn't also produce very good results because my model wasn't a good one. Still, I see some improvements. So, I wanted to try my changes in a better model. This is the reason why I have copied this notebook.\n\nSince my aim is to apply some changes to the feature engineering part, I am not touching the model at all.  \nOriginally in this notebook, dataset was splitted according to the distribution of each column and some columns were added (mea, std, var, mean, etc).  \nWhat I have added is the application of the [mutual information](https://www.kaggle.com/ryanholbrook/mutual-information) before creating those additional columns.","metadata":{}},{"cell_type":"markdown","source":"# Importing Libraries and Loading datasets","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.feature_selection import mutual_info_classif\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import callbacks","metadata":{"papermill":{"duration":28.22398,"end_time":"2021-11-10T03:26:44.193802","exception":false,"start_time":"2021-11-10T03:26:15.969822","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-21T00:08:55.370577Z","iopub.execute_input":"2021-11-21T00:08:55.37092Z","iopub.status.idle":"2021-11-21T00:09:21.941746Z","shell.execute_reply.started":"2021-11-21T00:08:55.37084Z","shell.execute_reply":"2021-11-21T00:09:21.940997Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reading the dataset\nraw_train = pd.read_csv(\"../input/tabular-playground-series-nov-2021/train.csv\")\nraw_test = pd.read_csv(\"../input/tabular-playground-series-nov-2021/test.csv\")\n\ntrain = raw_train.drop(['id','target'], axis = 1)\ntest = raw_test.drop('id', axis = 1)\n\ntarget = raw_train.target\nid_train = raw_train.id\nid_test = raw_test.id","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Engineering\n\nSplit the dataset by distribution of each column and add some basic columns (mea, std, var, mean, etc).","metadata":{}},{"cell_type":"markdown","source":"## Mutual information\n\nBefore creating those additional columns, select some features so that instead of having new columns which uses all of the data, only use selected features' columns. ","metadata":{}},{"cell_type":"code","source":"def make_mi_scores(mi_scores, X, y):\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\")\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\ndef plot_mi_scores(scores):\n    scores = scores.sort_values(ascending=True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    plt.barh(width, scores)\n    plt.yticks(width, ticks)\n    plt.title(\"Mutual Information Scores\")","metadata":{"execution":{"iopub.status.busy":"2021-11-21T00:10:23.307673Z","iopub.execute_input":"2021-11-21T00:10:23.307941Z","iopub.status.idle":"2021-11-21T00:10:23.314092Z","shell.execute_reply.started":"2021-11-21T00:10:23.307913Z","shell.execute_reply":"2021-11-21T00:10:23.313302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mi_scores = mutual_info_classif(train, target, random_state=1)\nmi_scores_classif = make_mi_scores(mi_scores, train, target)","metadata":{"execution":{"iopub.status.busy":"2021-11-21T00:10:46.111603Z","iopub.execute_input":"2021-11-21T00:10:46.112066Z","iopub.status.idle":"2021-11-21T00:26:43.969742Z","shell.execute_reply.started":"2021-11-21T00:10:46.112028Z","shell.execute_reply":"2021-11-21T00:26:43.968913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(dpi=100, figsize=(20, 16))\nplot_mi_scores(mi_scores_classif[mi_scores_classif > 1e-4])","metadata":{"execution":{"iopub.status.busy":"2021-11-21T00:26:43.971177Z","iopub.execute_input":"2021-11-21T00:26:43.971429Z","iopub.status.idle":"2021-11-21T00:26:44.935908Z","shell.execute_reply.started":"2021-11-21T00:26:43.971396Z","shell.execute_reply":"2021-11-21T00:26:44.935192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"selected_features = mi_scores_classif[mi_scores_classif > 1e-4].index.tolist()\nselected_features = [f'f{feature}' for feature in selected_features]\nprint(f\"Selected Features: {selected_features}\")","metadata":{"execution":{"iopub.status.busy":"2021-11-21T00:26:44.946167Z","iopub.execute_input":"2021-11-21T00:26:44.946617Z","iopub.status.idle":"2021-11-21T00:26:44.955336Z","shell.execute_reply.started":"2021-11-21T00:26:44.946579Z","shell.execute_reply":"2021-11-21T00:26:44.954127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Split the dataset and add new columns","metadata":{}},{"cell_type":"code","source":"# The number 2 is just a threshold to split\ndata = train[selected_features].copy()\nh_skew = data.loc[:,data.skew() >= 2].columns  # with Skewed \nl_skew = data.loc[:,data.skew() < 2].columns   # Bimodal\n\n# Skewed distrubutions\ntrain['median_h'] = train[h_skew].median(axis=1)\ntest['median_h'] = test[h_skew].median(axis=1)\n\ntrain['var_h'] = train[h_skew].var(axis=1)\ntest['var_h'] = test[h_skew].var(axis=1)\n\n# Bimodal distributions\ntrain['mean_l'] = train[l_skew].mean(axis=1)\ntest['mean_l'] = test[l_skew].mean(axis=1)\n\ntrain['std_l'] = train[l_skew].std(axis=1)\ntest['std_l'] = test[l_skew].std(axis=1)\n\ntrain['median_l'] = train[l_skew].median(axis=1)\ntest['median_l'] = test[l_skew].median(axis=1)\n\ntrain['skew_l'] = train[l_skew].skew(axis=1)\ntest['skew_l'] = test[l_skew].skew(axis=1)\n\ntrain['max_l'] = train[l_skew].max(axis=1)\ntest['max_l'] = test[l_skew].max(axis=1)\n\ntrain['var_l'] = train[l_skew].var(axis=1)\ntest['var_l'] = test[l_skew].var(axis=1)\n\nraw_train = train.copy()\nraw_test = test.copy()","metadata":{"papermill":{"duration":13.455351,"end_time":"2021-11-10T03:26:57.659068","exception":false,"start_time":"2021-11-10T03:26:44.203717","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-21T00:30:20.360193Z","iopub.execute_input":"2021-11-21T00:30:20.360659Z","iopub.status.idle":"2021-11-21T00:30:30.482294Z","shell.execute_reply.started":"2021-11-21T00:30:20.360624Z","shell.execute_reply":"2021-11-21T00:30:30.481603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scaling and Nomalization\ntransformer_high_skew = make_pipeline(\n    StandardScaler(), \n    MinMaxScaler(feature_range=(0, 1))\n)\n\ntransformer_low_skew = make_pipeline(\n    StandardScaler(),\n    MinMaxScaler(feature_range=(0, 1))\n)\n\nnew_cols = train.columns[-8:]\nh_skew = train.iloc[:,:100].loc[:, train.skew() >= 2].columns\nl_skew = train.iloc[:,:100].loc[:, train.skew() < 2].columns\n\ntransformer_new_cols = make_pipeline(\n    StandardScaler(),\n    MinMaxScaler(feature_range=(0, 1))\n)\n\npreprocessor = make_column_transformer(\n    (transformer_high_skew, l_skew),\n    (transformer_low_skew, h_skew),\n    (transformer_new_cols, new_cols),\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Neural Network","metadata":{}},{"cell_type":"code","source":"# Some parameters to config \nEPOCHS = 840\nBATCH_SIZE = 2048 \nACTIVATION = 'swish'\nLEARNING_RATE = 0.000265713\nFOLDS = 5","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Seed \nmy_seed = 42\ndef seedAll(seed):\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    \nseedAll(my_seed)\n\n# -----------------------------------------------------------------\ndef load_model(name:str):\n    early_stopping = callbacks.EarlyStopping(\n        patience=20,\n        min_delta=0,\n        monitor='val_loss',\n        restore_best_weights=True,\n        verbose=0,\n        mode='min', \n        baseline=None,\n    )\n\n    plateau = callbacks.ReduceLROnPlateau(\n            monitor='val_loss', \n            factor=0.2, \n            patience=7, \n            verbose=0,\n            mode='min')\n\n    model = keras.Sequential([\n        layers.Dense(108, activation = ACTIVATION, input_shape = [train.shape[1]]),      \n        layers.Dense(64, activation =ACTIVATION), \n        layers.Dense(32, activation =ACTIVATION),\n        layers.Dense(1, activation='sigmoid'),\n    ])\n\n    model.compile(\n        optimizer= keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n        loss='binary_crossentropy',\n        metrics=['AUC'],\n    )\n    \n    return model, early_stopping, plateau","metadata":{"papermill":{"duration":4.804345,"end_time":"2021-11-10T03:27:02.470705","exception":false,"start_time":"2021-11-10T03:26:57.66636","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-10T23:07:44.513243Z","iopub.execute_input":"2021-11-10T23:07:44.513802Z","iopub.status.idle":"2021-11-10T23:07:49.005981Z","shell.execute_reply.started":"2021-11-10T23:07:44.51376Z","shell.execute_reply":"2021-11-10T23:07:49.005104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"preds_valid_f = {}\npreds_test = []\ntotal_auc = []\nf_scores = []\n\nkf = StratifiedKFold(n_splits=FOLDS,random_state=0,shuffle=True)\nfor fold,(train_index, valid_index) in enumerate(kf.split(train,target)):\n    X_train,X_valid = train.loc[train_index], train.loc[valid_index]\n    y_train,y_valid = target.loc[train_index], target.loc[valid_index]\n\n    # Preprocessing\n    index_valid  = X_valid.index.tolist()\n    test  = raw_test.copy()\n    \n    X_train = preprocessor.fit_transform(X_train)\n    X_valid = preprocessor.transform(X_valid)\n    test = preprocessor.transform(test)\n      \n    # Model\n    model, early_stopping, plateau  = load_model('version1')\n    history = model.fit(  X_train, y_train,\n                validation_data = (X_valid, y_valid),\n                batch_size = BATCH_SIZE, \n                epochs = EPOCHS,\n                callbacks = [early_stopping, plateau],\n                shuffle = True,\n                verbose = 0\n              )\n    preds_valid = model.predict(X_valid).reshape(1,-1)[0] \n    preds_test.append(model.predict(test).reshape(1,-1)[0])\n    \n    #  Saving  scores to plot the end  \n    scores = pd.DataFrame(history.history)\n    scores['folds'] = fold\n    if fold == 0:\n        f_scores = scores \n    else: \n        f_scores = pd.concat([f_scores, scores], axis  = 0)\n        \n    # Concatenating valid preds\n    preds_valid_f.update(dict(zip(index_valid, preds_valid)))\n\n    # Getting score for a fold model\n    fold_auc = roc_auc_score(y_valid, preds_valid)\n    print(f\"Fold {fold} roc_auc_score: {fold_auc}\")\n\n    # Total auc\n    total_auc.append(fold_auc)\n\nprint(f\"mean roc_auc_score: {np.mean(total_auc)}, std: {np.std(total_auc)}\")","metadata":{"papermill":{"duration":904.734462,"end_time":"2021-11-10T03:42:07.212492","exception":false,"start_time":"2021-11-10T03:27:02.47803","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-10T23:07:49.008212Z","iopub.execute_input":"2021-11-10T23:07:49.008739Z","iopub.status.idle":"2021-11-10T23:15:36.242666Z","shell.execute_reply.started":"2021-11-10T23:07:49.008699Z","shell.execute_reply":"2021-11-10T23:15:36.241857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Outcomes","metadata":{}},{"cell_type":"code","source":"for fold in range(f_scores['folds'].nunique()):\n    history_f = f_scores[f_scores['folds'] == fold]\n\n    fig, ax = plt.subplots(1, 2, tight_layout=True, figsize=(14,4))\n    fig.suptitle('Fold : '+str(fold), fontsize=14)\n        \n    plt.subplot(1,2,1)\n    plt.plot(history_f.loc[:, ['loss', 'val_loss']], label= ['loss', 'val_loss'])\n    plt.legend(fontsize=15)\n    plt.grid()\n    \n    plt.subplot(1,2,2)\n    plt.plot(history_f.loc[:, ['auc', 'val_auc']],label= ['auc', 'val_auc'])\n    plt.legend(fontsize=15)\n    plt.grid()\n    \n    print(\"Validation Loss: {:0.4f}\".format(history_f['val_loss'].min()));","metadata":{"papermill":{"duration":2.963485,"end_time":"2021-11-10T03:42:10.185167","exception":false,"start_time":"2021-11-10T03:42:07.221682","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-10T23:15:36.243992Z","iopub.execute_input":"2021-11-10T23:15:36.244289Z","iopub.status.idle":"2021-11-10T23:15:38.985284Z","shell.execute_reply.started":"2021-11-10T23:15:36.24422Z","shell.execute_reply":"2021-11-10T23:15:38.984461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.read_csv(\"../input/tabular-playground-series-nov-2021/sample_submission.csv\")\nsub['target'] = np.mean(preds_test, axis = 0)\nsub.to_csv('submission.csv', index=False)\nsub.head()","metadata":{"papermill":{"duration":1.933432,"end_time":"2021-11-10T03:42:12.172879","exception":false,"start_time":"2021-11-10T03:42:10.239447","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-10T23:15:38.986776Z","iopub.execute_input":"2021-11-10T23:15:38.987038Z","iopub.status.idle":"2021-11-10T23:15:40.50139Z","shell.execute_reply.started":"2021-11-10T23:15:38.987002Z","shell.execute_reply":"2021-11-10T23:15:40.500528Z"},"trusted":true},"execution_count":null,"outputs":[]}]}