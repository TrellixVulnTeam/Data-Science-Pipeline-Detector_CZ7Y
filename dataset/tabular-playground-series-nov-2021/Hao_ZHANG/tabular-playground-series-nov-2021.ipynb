{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np              # linear algebra\nimport pandas as pd             # data processing, CSV file I/O (e.g. pd.read_csv)\n                                \nimport matplotlib.pyplot as plt # data visualization\nimport seaborn as sns           # data visualization\n                                \nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.model_selection import train_test_split\n\nimport optuna\nimport lightgbm as lgb\nfrom sklearn.metrics import auc, roc_curve","metadata":{"execution":{"iopub.status.busy":"2021-11-30T22:23:56.884839Z","iopub.execute_input":"2021-11-30T22:23:56.885763Z","iopub.status.idle":"2021-11-30T22:23:56.891382Z","shell.execute_reply.started":"2021-11-30T22:23:56.885718Z","shell.execute_reply":"2021-11-30T22:23:56.890429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preparation\n\n---\n\n## Data Extraction","metadata":{}},{"cell_type":"code","source":"sample_submission = pd.read_csv(\"/kaggle/input/tabular-playground-series-nov-2021/sample_submission.csv\")\ntrain = pd.read_csv(\"/kaggle/input/tabular-playground-series-nov-2021/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/tabular-playground-series-nov-2021/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-11-30T22:23:56.892697Z","iopub.execute_input":"2021-11-30T22:23:56.896178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Concatenation","metadata":{}},{"cell_type":"code","source":"data = pd.concat([train, test], sort = False)\ndata.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Null Check","metadata":{}},{"cell_type":"code","source":"null_cols = [col for col in data.iloc[: , : -1].columns if data[col].isnull().sum() != 0]\nnull_cols","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Visualization\n\n---\n\n## Features Distribution","metadata":{}},{"cell_type":"code","source":"float_cols = [col for col in train.iloc[: , 1 : -1].columns if train[col].dtype == \"float64\"]\nlen(float_cols)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CHUNMEIHONG = '#f1939c'\nQIUBOLAN = '#8abcd1'\nXIANGYABAI = '#fffef8'\nZHENZHUHUI = '#e4dfd7'\n\nfig, axes = plt.subplots(20, 5, figsize = (16, 48))\naxes = axes.flatten()\n\ndef features_distribution(axes):\n    for idx, ax in enumerate(axes):\n        sns.kdeplot(\n            data = train[float_cols + ['target']],\n            ax = ax,\n            hue = 'target',\n            fill = True,\n            x = f'f{idx}',\n            palette = [f'{CHUNMEIHONG}', f'{QIUBOLAN}'],\n            legend = idx == 0,\n            alpha = .5,\n            linewidth = 2.5,\n        )\n        \n        ax.grid(\n            color = XIANGYABAI,\n            linestyle = \":\",\n            linewidth = 1.25,\n            alpha = 0.3,\n        )\n        ax.set_facecolor(ZHENZHUHUI)\n        #ax.set_xticks([])\n        #ax.set_yticks([])\n        ax.spines['left'].set_visible(False)\n        ax.spines['top'].set_visible(False)\n        ax.yaxis.tick_right()\n        ax.yaxis.set_label_position(\"left\")\n        ax.set_title(\n            f'f{idx}',\n            loc = 'right',\n            weight = 'bold',\n            fontsize = 10,\n        )\n        #ax.set_xticks([])\n        #ax.set_yticks([])\n        ax.set_xlabel('')\n        #ax.set_ylabel('')\n        if idx % 5 != 0:\n            ax.set_ylabel('')\n\nfeatures_distribution(axes)\n\nfig.supxlabel('Probability', ha = 'center', fontweight = 'bold', fontsize = 16, y = -0.005,)\nfig.supylabel('Density', ha = 'center', fontweight = 'bold', fontsize = 16, x = -0.005,)\nfig.suptitle('Features Distribution', ha = 'center', fontweight = 'heavy', fontsize = 20, y = 1,)\nfig.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Engineering","metadata":{}},{"cell_type":"code","source":"df_train = train.copy()\ndf_test = test.copy()\n\npeaks = ['f0','f2','f4','f9','f12','f16','f19','f20','f23','f24','f27',\n    'f28','f30','f31','f32','f33','f35','f39','f42','f44','f46','f48',\n    'f49','f51','f52','f53','f56','f58','f59','f60','f61','f62','f63',\n    'f64','f68','f69','f72','f73','f75','f76','f78','f79','f81','f83',\n    'f84','f87','f88','f89','f90','f92','f93','f94','f95','f98','f99']\n\nno_peaks = [feats for feats in df_test.columns if feats not in peaks]\n\ndf_train['median_peaks'] = df_train[peaks].median(axis = 1)\ndf_train['median_no_peaks'] = df_train[no_peaks].median(axis = 1)\ndf_test['median_peaks'] = df_test[peaks].median(axis = 1)\ndf_test['median_no_peaks'] = df_test[no_peaks].median(axis = 1)\n\ndf_train['mean_peaks'] = df_train[peaks].mean(axis = 1)\ndf_train['mean_no_peaks'] = df_train[no_peaks].mean(axis = 1)\ndf_test['mean_peaks'] = df_test[peaks].mean(axis = 1)\ndf_test['mean_no_peaks'] = df_test[no_peaks].mean(axis = 1)\n\ndf_train['std_peaks'] = df_train[peaks].std(axis = 1)\ndf_train['std_no_peaks'] = df_train[no_peaks].std(axis = 1)\ndf_test['std_peaks'] = df_test[peaks].std(axis = 1)\ndf_test['std_no_peaks'] = df_test[no_peaks].std(axis = 1)\n\ndf_train['sum_peaks'] = df_train[peaks].sum(axis = 1)\ndf_train['sum_no_peaks'] = df_train[no_peaks].sum(axis = 1)\ndf_test['sum_peaks'] = df_test[peaks].sum(axis = 1)\ndf_test['sum_no_peaks'] = df_test[no_peaks].sum(axis = 1)\n\ndf_train['min_peaks'] = df_train[peaks].min(axis = 1)\ndf_train['min_no_peaks'] = df_train[no_peaks].min(axis = 1)\ndf_test['min_peaks'] = df_test[peaks].min(axis = 1)\ndf_test['min_no_peaks'] = df_test[no_peaks].min(axis = 1)\n\ndf_train['max_peaks'] = df_train[peaks].max(axis = 1)\ndf_train['max_no_peaks'] = df_train[no_peaks].max(axis = 1)\ndf_test['max_peaks'] = df_test[peaks].max(axis = 1)\ndf_test['max_no_peaks'] = df_test[no_peaks].max(axis = 1)\n\ndf_train['skew_peaks'] = df_train[peaks].skew(axis = 1)\ndf_train['skew_no_peaks'] = df_train[no_peaks].skew(axis = 1)\ndf_test['skew_peaks'] = df_test[peaks].skew(axis = 1)\ndf_test['skew_no_peaks'] = df_test[no_peaks].skew(axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = StandardScaler()\n\nfloat_columns = [feats for feats in df_train.select_dtypes('float')]\n\ndf_train[float_columns] = scaler.fit_transform(df_train[float_columns])\ndf_train = df_train.drop('id', axis = 1)\ndf_test = pd.DataFrame(scaler.transform(df_test[float_columns]), columns = df_test[float_columns].columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df_train.copy()\ny = X.pop('target')\nX_test = df_test.copy()\n\ndel train, test\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.3,\n                                                      random_state =0, stratify = y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgb_train = lgb.Dataset(X_train, y_train)\nlgb_eval = lgb.Dataset(X_valid, y_valid, reference = lgb_train)\n    \n\ndef objective(trial):\n    params = {\n        'metric': 'auc',\n        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),\n        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n        'feature_pre_filter': False,\n        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.02),\n        'max_bin': trial.suggest_int('max_bin', 64, 255),\n        'num_leaves': trial.suggest_int('num_leaves', 8, 32),\n        'device': 'gpu',\n    }\n    \n    lgb_train = lgb.Dataset(X_train, y_train)\n    lgb_eval = lgb.Dataset(X_valid, y_valid, reference = lgb_train)\n    \n    model = lgb.train(params,\n                      lgb_train,\n                      valid_sets = [lgb_train, lgb_eval],\n                      verbose_eval = 10,\n                      num_boost_round = 1000,\n                      early_stopping_rounds = 10)\n    \n    y_pred_valid = model.predict(X_valid, num_iteration = model.best_iteration)\n    \n    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_valid, y_pred_valid)\n    score = auc(false_positive_rate, true_positive_rate)\n    return score\n\nstudy = optuna.create_study(direction = 'maximize', sampler = optuna.samplers.RandomSampler(seed = 0))\nstudy.optimize(objective, n_trials = 30)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study.best_params","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = {\n    'metric': 'auc',\n    'lambda_l1': study.best_params['lambda_l1'],\n    'lambda_l2': study.best_params['lambda_l2'],\n    'feature_fraction': study.best_params['feature_fraction'],\n    'bagging_fraction': study.best_params['bagging_fraction'],\n    'bagging_freq': study.best_params['bagging_freq'],\n    'min_child_samples': study.best_params['min_child_samples'],\n    'feature_pre_filter': False,\n    'learning_rate': study.best_params['learning_rate'],\n    'num_leaves': study.best_params['num_leaves'],\n    'max_bin': study.best_params['max_bin'],\n    'device': 'gpu',\n}\n\n\nlgb_train = lgb.Dataset(X_train, y_train,)\nlgb_eval = lgb.Dataset(X_valid, y_valid, reference = lgb_train,)\n\nmodel = lgb.train(params,\n                  lgb_train,\n                  valid_sets = [lgb_train, lgb_eval],\n                  verbose_eval = 10,\n                  num_boost_round = 1000,\n                  early_stopping_rounds = 10)\n\n\ny_pred = model.predict(X_test, num_iteration = model.best_iteration)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.read_csv(\"/kaggle/input/tabular-playground-series-nov-2021/sample_submission.csv\")\nsub['target'] = y_pred\nsub.to_csv('submission_1st_trial.csv', index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}