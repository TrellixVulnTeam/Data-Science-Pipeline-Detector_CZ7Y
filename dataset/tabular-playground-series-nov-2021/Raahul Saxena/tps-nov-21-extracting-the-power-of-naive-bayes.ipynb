{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"background-color:rgba(110, 129, 21, 0.5);\">\n    <h1><center>Understand the Models You Love</center></h1>\n</div>","metadata":{}},{"cell_type":"markdown","source":"In this month's TPS, I am checking out some new algorithms I came across. I am choosing Naive Bayes right now. The purpose of this exercise is to become better at extracting maximum power from it and see if non-NN models can be used too.\n\nI would make changes to the important parameters and mention their impact. **Please note that these parameter observations are made independent of each other and only for the current data we have**. For speed, I am choosing a simple test split of 30% size on 10000 samples. I have shared references towards the end.\n\nI did a similar experiment last month with Random Forest - https://www.kaggle.com/raahulsaxena/tps-oct-21-understand-random-forest-parameters\n\n**Feel free to run your own experiments and upvote if you find this code useful :)**","metadata":{}},{"cell_type":"markdown","source":"# About Naive Bayes","metadata":{}},{"cell_type":"markdown","source":"It is a classification technique based on Bayesâ€™ Theorem with an **assumption of independence among predictors**. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature. Since our dataset has weakly correlated variables, NB might work well here.\n\n![](http://www.analyticsvidhya.com/wp-content/uploads/2015/09/Bayes_rule-300x172-300x172.png)\n\nIt perform wells in case of categorical input variables compared to numerical variable(s). **For numerical variable, normal distribution is assumed** (bell curve, which is a strong assumption), hence we will do variable transformation first.\n\nFollowing are the types of Naive Bayes algorithms-\n1. **Gaussian** - Assumes that features follow a normal distribution.\n2. **Multinomial** - Used for discrete counts\n3. **Bernoulli** - Used when features are binary in nature (0s and 1s)\n\nWe will **use Gaussian NB** after transforming our features.","metadata":{}},{"cell_type":"markdown","source":"# Importing Packages and Sample Data","metadata":{}},{"cell_type":"code","source":"import random\nrandom.seed(123)\n\nimport pandas as pd\nimport numpy as np\nimport datatable as dt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# importing evaluation and data split packages\n\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.metrics import roc_auc_score\n\n# importing modelling packages\n\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB","metadata":{"execution":{"iopub.status.busy":"2021-11-13T07:51:48.542677Z","iopub.execute_input":"2021-11-13T07:51:48.542975Z","iopub.status.idle":"2021-11-13T07:51:48.54941Z","shell.execute_reply.started":"2021-11-13T07:51:48.542945Z","shell.execute_reply":"2021-11-13T07:51:48.548486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# taking only 10000 rows as sample\n\ntrain = pd.read_csv(r'../input/tabular-playground-series-nov-2021/train.csv',nrows=10000)","metadata":{"execution":{"iopub.status.busy":"2021-11-13T07:51:54.40203Z","iopub.execute_input":"2021-11-13T07:51:54.402927Z","iopub.status.idle":"2021-11-13T07:51:54.642076Z","shell.execute_reply.started":"2021-11-13T07:51:54.402868Z","shell.execute_reply":"2021-11-13T07:51:54.640654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Splitting Data","metadata":{}},{"cell_type":"code","source":"X = train.drop(['id','target'],axis=1)\ny = train['target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=2,stratify=y)\n\nfeatures = X_train.columns","metadata":{"execution":{"iopub.status.busy":"2021-11-13T07:52:02.3057Z","iopub.execute_input":"2021-11-13T07:52:02.306619Z","iopub.status.idle":"2021-11-13T07:52:02.344463Z","shell.execute_reply.started":"2021-11-13T07:52:02.306568Z","shell.execute_reply":"2021-11-13T07:52:02.343492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Making Pipelines with Transformers","metadata":{}},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import RobustScaler, MinMaxScaler, QuantileTransformer, StandardScaler\n\npipe_1 = Pipeline([('scaler', StandardScaler()), ('nb', GaussianNB())])\npipe_2 = Pipeline([('scaler', MinMaxScaler()), ('nb', GaussianNB())])\npipe_3 = Pipeline([('scaler', RobustScaler()), ('nb', GaussianNB())])\npipe_4 = Pipeline([('scaler', QuantileTransformer()), ('nb', GaussianNB())])","metadata":{"execution":{"iopub.status.busy":"2021-11-13T07:52:09.586Z","iopub.execute_input":"2021-11-13T07:52:09.586632Z","iopub.status.idle":"2021-11-13T07:52:09.59507Z","shell.execute_reply.started":"2021-11-13T07:52:09.586586Z","shell.execute_reply":"2021-11-13T07:52:09.593702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:rgba(110, 129, 21, 0.5);\">\n    <h1><center>Gaussian Naive Bayes</center></h1>\n</div>","metadata":{}},{"cell_type":"markdown","source":"Methods of Improvement -\n\n![](http://www.baeldung.com/wp-content/ql-cache/quicklatex.com-8ebe947b0538431322197ecd5324bade_l3.svg)","metadata":{}},{"cell_type":"markdown","source":"# Analysing the X_train data (Experimenting)","metadata":{}},{"cell_type":"code","source":"print('No. of rows with all zeroes: ',((X_train == 0).sum(axis=1)==100).sum())","metadata":{"execution":{"iopub.status.busy":"2021-11-13T07:56:28.677654Z","iopub.execute_input":"2021-11-13T07:56:28.677948Z","iopub.status.idle":"2021-11-13T07:56:28.685939Z","shell.execute_reply.started":"2021-11-13T07:56:28.677917Z","shell.execute_reply":"2021-11-13T07:56:28.684735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fit a probability distribution to a univariate data sample\n\nimport scipy.stats as stats\ndef fit_distribution(data):\n    # estimate parameters\n    mu = np.mean(data)\n    sigma = np.std(data)\n    #fit distribution\n    dist = stats.norm.pdf(mu, sigma)\n    return dist","metadata":{"execution":{"iopub.status.busy":"2021-11-13T08:20:26.046465Z","iopub.execute_input":"2021-11-13T08:20:26.047409Z","iopub.status.idle":"2021-11-13T08:20:26.052787Z","shell.execute_reply.started":"2021-11-13T08:20:26.047366Z","shell.execute_reply":"2021-11-13T08:20:26.051709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sort data into classes\n\nXy0 = train[train['target'] == 0]\nXy1 = train[train['target'] == 1]\nXy0.drop(['id','target'],axis=1,inplace=True)\nXy1.drop(['id','target'],axis=1,inplace=True)\nprint(Xy0.shape, Xy1.shape)\n\n# calculate priors\n\npriory0 = len(Xy0) / len(X)\npriory1 = len(Xy1) / len(X)\nprint(priory0, priory1)","metadata":{"execution":{"iopub.status.busy":"2021-11-13T08:20:29.606068Z","iopub.execute_input":"2021-11-13T08:20:29.606537Z","iopub.status.idle":"2021-11-13T08:20:29.62692Z","shell.execute_reply.started":"2021-11-13T08:20:29.6065Z","shell.execute_reply":"2021-11-13T08:20:29.625157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create PDFs\n\nXy0_pdf = Xy0.apply(lambda x:fit_distribution(x))\nXy1_pdf = Xy1.apply(lambda x:fit_distribution(x))","metadata":{"execution":{"iopub.status.busy":"2021-11-13T08:20:33.476967Z","iopub.execute_input":"2021-11-13T08:20:33.47774Z","iopub.status.idle":"2021-11-13T08:20:33.555561Z","shell.execute_reply.started":"2021-11-13T08:20:33.477701Z","shell.execute_reply":"2021-11-13T08:20:33.554752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Best Variable Transformation\nQuantile Transformer works best because it helps variables assume normal distribution - useful for NB","metadata":{}},{"cell_type":"code","source":"print('Default Parameters: ',GaussianNB().get_params())\nmodel = GaussianNB()\nmodel.fit(X_train,y_train)\nprint('ROC score with No Scaler: ',roc_auc_score(y_test,model.predict_proba(X_test)[:,1]))\npipe_1.fit(X_train,y_train)\nprint('ROC score with StandardScaler: ',roc_auc_score(y_test,pipe_1.predict_proba(X_test)[:,1]))\npipe_2.fit(X_train,y_train)\nprint('ROC score with MinMaxScaler: ',roc_auc_score(y_test,pipe_2.predict_proba(X_test)[:,1]))\npipe_3.fit(X_train,y_train)\nprint('ROC score with RobustScaler: ',roc_auc_score(y_test,pipe_3.predict_proba(X_test)[:,1]))\npipe_4.fit(X_train,y_train)\nprint('ROC score with QuantileTransformer: ',roc_auc_score(y_test,pipe_4.predict_proba(X_test)[:,1]))","metadata":{"execution":{"iopub.status.busy":"2021-11-13T07:42:33.953068Z","iopub.execute_input":"2021-11-13T07:42:33.953541Z","iopub.status.idle":"2021-11-13T07:42:34.636788Z","shell.execute_reply.started":"2021-11-13T07:42:33.953492Z","shell.execute_reply":"2021-11-13T07:42:34.635735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Var Smoothing\nPortion of the largest variance of all features that is added to variances for calculation stability.\nIn statistics, Laplace Smoothing is a technique to smooth categorical data. Laplace Smoothing is introduced to solve the problem of zero probability.","metadata":{}},{"cell_type":"code","source":"var_smooth = [0.1,0.01,0.001,0.0001,0.00001,0.000001,0.0000001,0.00000001,0.000000001]\n\nfor var in var_smooth:\n    pipe = Pipeline([('scaler', QuantileTransformer()), ('nb', GaussianNB(var_smoothing=var))])\n    pipe.fit(X_train,y_train)\n    print('Var Smoothing: ',var,\" \",'AUC: ',roc_auc_score(y_test,pipe.predict_proba(X_test)[:,1]))","metadata":{"execution":{"iopub.status.busy":"2021-11-13T07:24:03.56581Z","iopub.execute_input":"2021-11-13T07:24:03.566086Z","iopub.status.idle":"2021-11-13T07:24:06.467563Z","shell.execute_reply.started":"2021-11-13T07:24:03.566058Z","shell.execute_reply":"2021-11-13T07:24:06.466566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Permutation Importance\n\nThe permutation feature importance is defined to be the **decrease in a model score when a single feature value is randomly shuffled**. This procedure breaks the relationship between the feature and the target, thus the drop in the model score is indicative of how much the model depends on the feature. ","metadata":{}},{"cell_type":"code","source":"from sklearn.inspection import permutation_importance\n\nmodel = GaussianNB()\nmodel.fit(X_train, y_train)\nimps = permutation_importance(model, X_test, y_test)\nimportances = imps.importances_mean\nstd = imps.importances_std\nindices = np.argsort(importances)[::-1]\n\n# Print the feature ranking\n#print(\"Feature ranking:\")\n#for f in range(X_test.shape[1]):\n #   print(\"%d. %s (%f)\" % (f + 1, features[indices[f]], importances[indices[f]]))","metadata":{"execution":{"iopub.status.busy":"2021-11-13T07:24:10.869236Z","iopub.execute_input":"2021-11-13T07:24:10.869794Z","iopub.status.idle":"2021-11-13T07:24:14.027082Z","shell.execute_reply.started":"2021-11-13T07:24:10.869757Z","shell.execute_reply":"2021-11-13T07:24:14.02599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fitting our pipeline with a subset of the data with these variables\n# first 39 variables have positive importance, hence ignoring them\n\nimportant_features = features[indices[39:]]\nprint('Important Features: ',important_features)\n\nX_train_new = X_train[important_features]\nX_test_new = X_test[important_features]\n\npipe_4.fit(X_train_new,y_train)\nprint('ROC score with QuantileTransformer and Selected Features: ',\n      roc_auc_score(y_test,pipe_4.predict_proba(X_test_new)[:,1]))","metadata":{"execution":{"iopub.status.busy":"2021-11-13T07:24:21.125387Z","iopub.execute_input":"2021-11-13T07:24:21.125674Z","iopub.status.idle":"2021-11-13T07:24:21.336418Z","shell.execute_reply.started":"2021-11-13T07:24:21.125638Z","shell.execute_reply":"2021-11-13T07:24:21.335374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Converting to Discrete\n\n6 bins, without any scaling, give good results, even better than simple quantile transformer scaling on continuous variables.","metadata":{}},{"cell_type":"code","source":"bins = [2,3,4,5,6,7,8,9,10]\nprint('GAUSSIAN WITH DISCRETE, CUT')\nmodel = GaussianNB()\nfor bin in bins:\n    X_train_binned = X_train.apply(lambda x:pd.cut(x,bins=bin,labels=False))\n    X_test_binned = X_test.apply(lambda x:pd.cut(x,bins=bin,labels=False))\n    model.fit(X_train_binned,y_train)\n    print('No. of Bins: ',bin,\" \",'AUC: ',\n          roc_auc_score(y_test,pipe.predict_proba(X_test_binned)[:,1]))\nprint('GAUSSIAN WITH DISCRETE, QCUT')\nmodel = GaussianNB()\nquantiles = [2,3,4,5,6,7,8,9,10]\nfor quantile in quantiles:\n    X_train_binned = X_train.apply(lambda x:pd.qcut(x,q=quantile,labels=False,precision=0))\n    X_test_binned = X_test.apply(lambda x:pd.qcut(x,q=quantile,labels=False,precision=0))\n    model.fit(X_train_binned,y_train)\n    print('No. of Quantiles: ',quantile,\" \",'AUC: ',\n          roc_auc_score(y_test,pipe.predict_proba(X_test_binned)[:,1]))","metadata":{"execution":{"iopub.status.busy":"2021-11-13T07:24:31.010369Z","iopub.execute_input":"2021-11-13T07:24:31.01068Z","iopub.status.idle":"2021-11-13T07:24:35.522898Z","shell.execute_reply.started":"2021-11-13T07:24:31.010635Z","shell.execute_reply":"2021-11-13T07:24:35.522067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy import stats\ndef percentiler(col):\n    ranked = stats.rankdata(col)\n    data_percentile = ranked/len(col)*100\n    bins_percentile = np.linspace(0,100,6)\n    data_binned_indices = np.digitize(data_percentile, bins_percentile, right=True)\n    return data_binned_indices\n\nprint('Percentiled Bins as per columns')\nmodel = GaussianNB()\nX_train_binned = X_train.apply(lambda x:percentiler(x))\nX_test_binned = X_test.apply(lambda x:percentiler(x))\nmodel.fit(X_train_binned,y_train)\nprint(roc_auc_score(y_test,pipe.predict_proba(X_test_binned)[:,1]))","metadata":{"execution":{"iopub.status.busy":"2021-11-13T07:24:43.405257Z","iopub.execute_input":"2021-11-13T07:24:43.405585Z","iopub.status.idle":"2021-11-13T07:24:43.630783Z","shell.execute_reply.started":"2021-11-13T07:24:43.405548Z","shell.execute_reply":"2021-11-13T07:24:43.629794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# References\n\nLinks - \n1. https://www.geeksforgeeks.org/naive-bayes-classifiers/\n2. https://towardsdatascience.com/introduction-to-na%C3%AFve-bayes-classifier-fa59e3e24aaf\n3. https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/\n4. https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html\n5. https://machinelearningmastery.com/better-naive-bayes/\n6. https://inblog.in/Feature-Importance-in-Naive-Bayes-Classifiers-5qob5d5sFW\n\nNotebooks -\n1. https://www.kaggle.com/rayhanlahdji/tps-1121-naive-bayes-for-naive-souls by Rayhan Lahdji\n2. https://www.kaggle.com/markosthabit/tbs-november-naive-bayes by Markos Thabit\n3. https://www.kaggle.com/prashant111/naive-bayes-classifier-in-python by Prashant Banerjee\n\nVideos -\n1. https://www.youtube.com/watch?v=H3EjCKtlVog on Gaussian Naive Bayes\n2. https://www.youtube.com/watch?v=O2L2Uv9pdDA on understanding NB","metadata":{}}]}