{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"I create this notebook in response to https://www.kaggle.com/zhangcheche/work-well-on-trainset-bad-on-testset\n\nI think reason why his validation-score (0.82) is **far higher** that the LB-score (0.74) is because his data is leaking between models.\n\nTo summarize, if you want to split the training-set apart from validation-set, make sure you only doing it **once** at the beginning. Ensure all of your models are being trained/fit on the same training-set, and being validated on the same validation-set.\n\nCalling `train_test_split` each time you want to train the base models is a bad idea, because `train_test_split` will shuffle the data by default. Validation-set for the 1st model may become training-set for 2nd model, etc, hence data-leak occurs.\n\nI put a lot of comment in the code here, hence make sure to read the code too.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom xgboost import XGBClassifier","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-09T12:32:50.199642Z","iopub.execute_input":"2021-11-09T12:32:50.202023Z","iopub.status.idle":"2021-11-09T12:32:50.212732Z","shell.execute_reply.started":"2021-11-09T12:32:50.201975Z","shell.execute_reply":"2021-11-09T12:32:50.211857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = pd.read_csv('/kaggle/input/tabular-playground-series-nov-2021/train.csv', index_col=1)\ny = x.pop('target')\nx = StandardScaler().fit_transform(x)","metadata":{"execution":{"iopub.status.busy":"2021-11-09T12:39:03.038333Z","iopub.execute_input":"2021-11-09T12:39:03.038629Z","iopub.status.idle":"2021-11-09T12:39:16.602376Z","shell.execute_reply.started":"2021-11-09T12:39:03.038578Z","shell.execute_reply":"2021-11-09T12:39:16.60147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1st model","metadata":{}},{"cell_type":"code","source":"train_x, val_x, train_y, val_y = train_test_split(x, y)\nest = LogisticRegression()\nest.fit(train_x, train_y)\nval_pred = est.predict_proba(val_x)[:,1]\nlogreg_pred = est.predict_proba(x)[:,1]\nroc_auc_score(val_y, val_pred)","metadata":{"execution":{"iopub.status.busy":"2021-11-09T12:20:43.760555Z","iopub.execute_input":"2021-11-09T12:20:43.761567Z","iopub.status.idle":"2021-11-09T12:20:47.730368Z","shell.execute_reply.started":"2021-11-09T12:20:43.761511Z","shell.execute_reply":"2021-11-09T12:20:47.729265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2nd model","metadata":{}},{"cell_type":"code","source":"train_x, val_x, train_y, val_y = train_test_split(x, y) # BAD - DON'T SPLIT AGAIN - DATA LEAKING\nest = LinearSVC(dual=False)\nest.fit(train_x, train_y) # 2nd-model can see the 1st-model's validation-set, due to 'split again'\nval_pred = est.decision_function(val_x)\nlsvc_pred = est.decision_function(x)\nroc_auc_score(val_y, val_pred)","metadata":{"execution":{"iopub.status.busy":"2021-11-09T12:24:45.195365Z","iopub.execute_input":"2021-11-09T12:24:45.195648Z","iopub.status.idle":"2021-11-09T12:25:00.435828Z","shell.execute_reply.started":"2021-11-09T12:24:45.195618Z","shell.execute_reply":"2021-11-09T12:25:00.434781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3rd model","metadata":{}},{"cell_type":"code","source":"train_x, val_x, train_y, val_y = train_test_split(x, y) # BAD - DON'T SPLIT AGAIN - DATA LEAKING\nxgb_model = XGBClassifier(use_label_encoder=False, tree_method='hist')\nxgb_model.fit(train_x, train_y) # 3rd-model can see the 2nd-model's validation-set, due to 'split again'\nval_pred = xgb_model.predict_proba(val_x)[:, 1]\nxgb_pred = xgb_model.predict_proba(x)[:, 1]\nroc_auc_score(val_y, val_pred)","metadata":{"execution":{"iopub.status.busy":"2021-11-09T12:39:16.604062Z","iopub.execute_input":"2021-11-09T12:39:16.605829Z","iopub.status.idle":"2021-11-09T12:39:37.123167Z","shell.execute_reply.started":"2021-11-09T12:39:16.605776Z","shell.execute_reply":"2021-11-09T12:39:37.122281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Stacking/Ensembling - Feeding the Prediction from 1st+2nd+3rd Models into the 4th Final Model\n\nRemember that the 2nd-model could see the 1st-model's validation-set,\n\nthe 3rd-model could see the 1st+2nd model's validation-set, etc.\n\nThe final-model will be able to see what 1st+2nd+3rd model saw in the training-set.\nDue to leak, Hence the final-model **almost can see everything** in the whole complete-data, including the label/answer from its validation-set.","metadata":{}},{"cell_type":"code","source":"x_new = pd.DataFrame(x)\nx_new['logreg'] = logreg_pred # 1st-model saw some-part of 2nd+3rd validation-set\nx_new['lsvc'] = lsvc_pred # 2nd-model saw some-part of 1st+3rd validation-set\nx_new['xgb'] = xgb_pred # 3rd-model saw some-part of 1st+2rd validation-set\nx_new.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-09T13:07:54.630302Z","iopub.execute_input":"2021-11-09T13:07:54.63065Z","iopub.status.idle":"2021-11-09T13:07:54.643017Z","shell.execute_reply.started":"2021-11-09T13:07:54.630616Z","shell.execute_reply":"2021-11-09T13:07:54.642153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_x, val_x, train_y, val_y = train_test_split(x_new, y) # BAD - DON'T SPLIT AGAIN\nfinal_model = XGBClassifier(use_label_encoder=False, tree_method='hist')\nfinal_model.fit(train_x, train_y)\nval_pred = final_model.predict_proba(val_x)[:, 1]\nroc_auc_score(val_y, val_pred)","metadata":{"execution":{"iopub.status.busy":"2021-11-09T13:08:40.031286Z","iopub.execute_input":"2021-11-09T13:08:40.031557Z","iopub.status.idle":"2021-11-09T13:09:02.498639Z","shell.execute_reply.started":"2021-11-09T13:08:40.031527Z","shell.execute_reply":"2021-11-09T13:09:02.497663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Boom, spot the high validation-score from the final-model.","metadata":{}},{"cell_type":"markdown","source":"# How to Fix\n\nAll model should be trained on the same training-set, and being validated on the same validation-set. It should be easier to do this by calling `train_test_split` only once at the beginning of your notebook/kernel.","metadata":{}},{"cell_type":"code","source":"train_x, val_x, train_y, val_y = train_test_split(x, y) # only once at the beginning\n\nest = LogisticRegression()\nest.fit(train_x, train_y)\nlogreg_train_pred = est.predict_proba(train_x)[:,1]\nlogreg_val_pred = est.predict_proba(val_x)[:,1]\nprint('lr', roc_auc_score(val_y, logreg_val_pred))\n\nest = LinearSVC(dual=False)\nest.fit(train_x, train_y)\nlsvc_train_pred = est.decision_function(train_x)\nlsvc_val_pred = est.decision_function(val_x)\nprint('lsvc', roc_auc_score(val_y, lsvc_val_pred))\n\nxgb_model = XGBClassifier(use_label_encoder=False, tree_method='hist')\nxgb_model.fit(train_x, train_y)\nxgb_train_pred = xgb_model.predict_proba(train_x)[:, 1]\nxgb_val_pred = xgb_model.predict_proba(val_x)[:, 1]\nprint('xgb', roc_auc_score(val_y, xgb_val_pred))\n\ntrain_x_new = pd.DataFrame(train_x)\ntrain_x_new['logreg'] = logreg_train_pred\ntrain_x_new['lsvc'] = lsvc_train_pred\ntrain_x_new['xgb'] = xgb_train_pred\nval_x_new = pd.DataFrame(val_x)\nval_x_new['logreg'] = logreg_val_pred\nval_x_new['lsvc'] = lsvc_val_pred\nval_x_new['xgb'] = xgb_val_pred\n\nfinal_model = XGBClassifier(use_label_encoder=False, tree_method='hist')\nfinal_model.fit(train_x_new, train_y)\nfinal_val_pred = final_model.predict_proba(val_x_new)[:, 1]\nprint('final', roc_auc_score(val_y, final_val_pred))","metadata":{"execution":{"iopub.status.busy":"2021-11-09T13:28:47.839781Z","iopub.execute_input":"2021-11-09T13:28:47.840493Z","iopub.status.idle":"2021-11-09T13:29:34.866996Z","shell.execute_reply.started":"2021-11-09T13:28:47.840451Z","shell.execute_reply":"2021-11-09T13:29:34.866066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"roc_auc_score(val_y, final_val_pred)","metadata":{"execution":{"iopub.status.busy":"2021-11-09T13:29:34.869092Z","iopub.execute_input":"2021-11-09T13:29:34.869608Z","iopub.status.idle":"2021-11-09T13:29:34.933271Z","shell.execute_reply.started":"2021-11-09T13:29:34.869563Z","shell.execute_reply":"2021-11-09T13:29:34.932117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Validation-score from the `final` model seems more make sense now :-)","metadata":{}}]}