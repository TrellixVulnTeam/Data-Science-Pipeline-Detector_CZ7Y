{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1 style=\"font-size:80px;color:#6166B3;text-align:center\"><strong>Deep Learning</strong> <strong style=\"color:black\">P3</strong></h1>\n\n![Neuron gif](https://scx2.b-cdn.net/gfx/news/2016/studyfindson.gif)\n\n<p style=\"font-size:120%\"> In the <strong>Tabular Playground Series - November 2021</strong>, we have seen a lot of Neural Network models. As a beginner, this motivated me to explore Deep Learning. I will be producing more and more notebooks on Deep Learning, as and when I get confident with my concepts and learning. Hope this kernel is helpful.</p>\n\n<h1 style=\"font-size:50px;color:#6166B3\"><strong>Assumptions of </strong> <strong style=\"color:black\">Neural Networks</strong></h1>\n\n![NN Diagram](https://miro.medium.com/max/1400/0*_SH7tsNDTkGXWtZb.png)\n<p style=\"font-size:120%;text-align:center\">Figure 1: Neural Network Architecture</p>\n\n<p style=\"font-size:120%\">Neural Network Architectures follow some simple Assumptions:</p>\n\n<ol>\n    <li style=\"font-size:120%\">Neurons are arranged in layers and the <strong>layers are arranged sequentially.</strong></li>\n    <li style=\"font-size:120%\">Neurons within the <strong>same layer do not interact with each other</strong>.</li>\n    <li style=\"font-size:120%\">All the inputs enter the network through the <strong>input layer</strong> and all the outputs go out of the network through the <strong>output layer.</strong></li>\n    <li style=\"font-size:120%\">Neurons in consecutive layers are <strong>densely connected</strong>. i.e, all the Neurons in layer <i>h<sub>1</sub></i> are connected to all the other neurons in layer <i>h<sub>2</sub></i>.</li>\n    <li style=\"font-size:120%\"><strong>Every interconnection</strong> in the neural network has a <strong>weight</strong> associated with it, and every neuron has a <strong>bias</strong> associated with it.</li>\n    <li style=\"font-size:120%\">All neurons in all layers use the <strong>same activation function</strong>.</li>\n</ol>\n\n<h1 style=\"font-size:50px;color:#6166B3\"><strong>Inputs to </strong> <strong style=\"color:black\">Neural Networks</strong></h1>\n\n<p style=\"font-size:120%\">There can be several inputs to a Neural Network, for example:</p>\n\n<ul>\n    <li style=\"font-size:120%\">Text</li>\n    <li style=\"font-size:120%\">Speech</li>\n    <li style=\"font-size:120%\">Images and many more...</li>\n</ul>\n\n<p style=\"font-size:120%\">But <strong>Remember!</strong> Whatever the input might be, <mark>make sure they are <strong>numerical</strong> in nature!</mark></p>\n\n<p style=\"font-size:120%\">Let's go ahead and explore one example.</p>\n\n![Image](https://cdn.upgrad.com/UpGrad/temp/e1cbb5a6-b4ae-45f6-aba1-8e7fce796b98/Screen+Shot+2018-08-22+at+11.54.11+AM.png)\n<p style=\"font-size:120%;text-align:center\">Figure 2: Numerical Representation of a Grey-Scale Image</p>\n\n<p style=\"font-size:120%\">Refer the image above. It is a grey scale image. A grey scale image contains pixels in the range 0 to 255. 0 is the darkest or black and 255 is the brightest or white. The varying intensity of these pixel within the range of 0 to 255 constitutes an image.</p>\n\n<p style=\"font-size:120%\">Therefore, for our Neural Network to understand, <mark>we are required to give numerical values as inputs</mark></p>\n\n<h1 style=\"font-size:50px;color:#6166B3\"><strong>Hyperparameters of </strong> <strong style=\"color:black\">Neural Network</strong></h1>\n\n<p style=\"font-size:120%\">These days, before we perform Hyper-Parameter tuning to the Neural Networks, we initially set our minds on an Architecture.</p>\n\n<p style=\"font-size:120%\">Now, what do we mean, when we say \"Deciding the Architecture\"? It means, we decide:</p>\n\n<ol>\n    <li style=\"font-size:120%\">How many Inputs there will be.</li>\n    <li style=\"font-size:120%\">How many Outputs.</li>\n    <li style=\"font-size:120%\">Fixing the Topology (Hidden Layers and Neurons in them).</li>\n    <li style=\"font-size:120%\">Deciding the Activation functions in each layer.</li>\n</ol>\n\n<p style=\"font-size:120%\">Now that these things are decided, we are left with <strong>Weights and Biases</strong> of the Neural Networks. Therefore, <mark>Weights and Biases are our Hyper-Parametes!</mark></p>\n\n<p style=\"font-size:120%\">That's a lot to take in as of now. So I will stop here. If you want to look at the previous concepts that have been covered in this series, refer the \"Quick Links\" section.</p>\n\n<p style=\"font-size:120%\">Next, we will learn about how a Neural Network learns. It's a familier topic... Gradient Decent.</p>\n\n<h1 style=\"font-size:50px;color:#6166B3\">Quick <strong style=\"color:black\">Links:</strong></h1>\n\n<a href=\"https://www.kaggle.com/vivek468/dl-part1-introduction-to-neural-network\" style=\"font-size:180%;text-decoration:none\">DL Part1: Introduction to Neural Network</a>\n\n<a href=\"https://www.kaggle.com/vivek468/dl-part2-perceptrons\" style=\"font-size:180%;text-decoration:none\">DL Part2: Perceptrons</a>\n\n<h1 style=\"font-size:20px;color:#6166B3;text-align:center\">If you like/fork my work, do give this an upvote. Appreciate your support and happy learning! Follow me for more on Deep Learning. Coming Soon.</h1>","metadata":{}}]}