{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1 style=\"font-size:80px;color:#6166B3;text-align:center\"><strong>Deep Learning</strong> <strong style=\"color:black\">P2</strong></h1>\n\n![Neuron gif](https://scx2.b-cdn.net/gfx/news/2016/studyfindson.gif)\n\n<p style=\"font-size:120%\"> In the <strong>Tabular Playground Series - November 2021</strong>, we have seen a lot of Neural Network models. As a beginner, this motivated me to explore Deep Learning. I will be producing more and more notebooks on Deep Learning, as and when I get confident with my concepts and learning. Hope this kernel is helpful.</p>\n\n<h1 style=\"font-size:50px;color:#6166B3\"><strong>Understanding</strong> <strong style=\"color:black\">Perceptrons</strong></h1>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-size:180%\"><strong>Consider the Scenario:</strong></p>\n\n<p style=\"font-size:120%\">Imagine this, you and five of your friends are deciding to go out for a brunch on a Sunday afternoon. For that, you guys decide to plan out ahead, so that everything goes well. Most of your friends suggest that, if it rains on that day, they'd cancel the plan. A few of them also point out that if the total cost of brunch is more than the decided budget of $100, they will look for another restaurant. You guys also mutually agree that atleast 3 people should be ready to accompany, else the plan will be re-scheduled.</p>\n\n<p style=\"font-size:120%\">With the above information at hand, isn't it fascinating how we are able to decide if we are going or not. Whether the restaurant is fine for our brunch.</p>\n\n<p style=\"font-size:120%\">With that being said, Let me introduce you all to <strong>Threshold Logic Unit</strong>. They make up the perceptrons. Let's see how.</p>\n\n![TLU](https://www.researchgate.net/profile/Pritam-Bhattacharya/publication/264080882/figure/fig10/AS:296615974850577@1447730167878/Threshold-Logic-Unit-Source-Wikimedia-Commons.png)\n<p style=\"font-size:120%;text-align:center\">Figure 1: Threshold Logic Unit</p>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-size:120%\">Look at the figure above. It's okay if you do not understand everything at first.</p>\n\n<p style=\"font-size:120%\">Recall the scenario where we are trying to plan out our brunch. Let's try to relate that with the figure above. Let's say that the <mark>inputs in our case are- <strong>no of friends, weather and budget</strong>. Consider them as <strong>I<sub>1</sub>, I<sub>2</sub> and I<sub>3</sub></strong>.</mark></p>\n\n<p style=\"font-size:120%\">Now depending on the priority which you decide to give to each of these input, we will decide on a weight. <mark>For example: It shouldn't rain on that day. If we give this the highest priority, then we will assign a higher weight to it. Similarly, changing the restaurant based on the budget of the brunch is given lower priority, it will have a lower weight associated with it.</mark></p>\n\n<p style=\"font-size:120%\">Next, we take the weighted sum of all the inputs! <strong>i.e. Weighted Sum = W<sub>1</sub>I<sub>1</sub> + W<sub>2</sub>I<sub>2</sub> + W<sub>3</sub>I<sub>3</sub></strong> (where <strong>W<sub>1</sub>, W<sub>2</sub> and W<sub>3</sub></strong> are weights associated with Inputs <strong>I<sub>1</sub>, I<sub>2</sub> and I<sub>3</sub></strong> respectively!)</p>\n\n<p style=\"font-size:120%\">Let's say if the sum is very low, then we decide to drop the plan, if it is average, we decide to change the restaurant and if it is high, we decide to go. But how do we decide these thresholds? This is where the concept of <strong>Activation Function</strong> comes into picture.</p>\n\n![Step Function](https://miro.medium.com/max/1400/1*pmFsxtfiFoo09rXLaKyrOg.png)\n<p style=\"font-size:120%;text-align:center\">Figure 2: A Binary Step Function</p>\n\n<p style=\"font-size:120%\">An Activation Function helps the Threshold Logic Unit to decide the outcome. For example, <mark>if we take a step function as our Activation Function, we can say that: <strong>If our weighted sum of inputs is less than 0, then we cancel the plan else we go for brunch!</strong></mark> It is as simple as that!</p>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-size:180%\"><strong>Let's begin to understand Perceptrons:</strong></p>\n\n<p style=\"font-size:120%\"><strong>A Perceptron is simply composed of a single layer of Threshold Logic Units (TLUs), with each TLU connected to all the inputs.</strong> <mark>When all the neurons in a layer are connected to every neuron in the previous layer (i.e., its input neurons), it is called a fully connected layer or a dense layer.</mark> To represent the fact that each input is sent to every TLU, it is common to draw special passthrough neurons called input neurons: they just output whatever input they are fed. All the input neurons form the input layer. <mark>Moreover, an extra <strong>bias feature</strong> is generally added (b = 1): it is typically represented using a special type of neuron called a bias neuron, which just outputs 1 all the time.</mark></p>\n\n![Perceptron](https://www.oreilly.com/library/view/neural-networks-and/9781492037354/assets/mlst_1005.png)\n\n","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-size:180%\"><strong>The General Equation of Perceptron Output:</strong></p>\n\n<p style=\"font-size:300%;text-align:center\">h<sub>W,b</sub> = &phi;(Wx + b)</p>\n\n<p style=\"font-size:120%\">h = Perceptron Output</p>\n<p style=\"font-size:120%\">W = Weights associated with each Inputs</p>\n<p style=\"font-size:120%\">x = Inputs</p>\n<p style=\"font-size:120%\">b = Bias term of the layer</p>\n<p style=\"font-size:120%\">&phi; = Activation Function</p>\n\n<p style=\"font-size:120%\">Therefore, the output of the perceptron is nothing but the weighted sum of the inputs plus the bias. When this is passed through the activation function, an output is generated.</p>","metadata":{}},{"cell_type":"markdown","source":"<h1 style=\"font-size:50px;color:#6166B3\"><strong>Activation</strong> <strong style=\"color:black\">Functions</strong></h1>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-size:120%\">As we learnt in the above section, Activation Functions help decide the output of a Perceptron or Neural Networks in general. Therefore, they should follow some rules of best practices:</p>\n\n<ul>\n    <li style=\"font-size:120%\">Activation functions should be <strong>smooth</strong> i.e. they should have no abrupt changes when plotted. Why, you ask? It is because, almost no real life decision is made where we say: if cholestrol is 5 then the person will have heart attack else if the cholestrol is 1 then he won't have any heart attack. Instead, the output of whether a person will experience a heart attack depends on the probability of how likely an event will occur based on the cholestrol level of that person.</li>\n    <li style=\"font-size:120%\">They should also <strong>make the inputs and outputs non-linear with respect to each other to some extent</strong>. This is because non-linearity helps in making neural networks more compact. Let us understand this further. If an activation function is linear in nature, we would require more neurons and activation functions to reach a solution. However, if the activation function injects a certain degree of non-linearity, we will need very less number of neurons to do the same job.</li>","metadata":{}},{"cell_type":"markdown","source":"# Some Popular Activation Functions:\n\n<p style=\"font-size:180%\"><strong>Logistic/Sigmoid Function:</strong></p>\n\n![Step Function](https://cdn.upgrad.com/UpGrad/temp/8cc0bbd4-f18b-478e-90bf-e19677d5963c/Sigmoid_fn.png)\n\n<p style=\"font-size:200%;text-align:center\">Output = <span>&#402;</span>(x) = 1 / 1 + e<sup>-x</sup></p>\n\n<p style=\"font-size:180%\"><strong>Tanh Function:</strong></p>\n\n![Tanh](https://cdn.upgrad.com/UpGrad/temp/78d7c753-da15-4369-ba82-82ae1e969aff/tanh.PNG)\n\n<p style=\"font-size:200%;text-align:center\">Output = <span>&#402;</span>(x) = e<sup>x</sup> - e<sup>-x</sup>/ e<sup>x</sup> + e<sup>-x</sup></p>\n\n<p style=\"font-size:180%\"><strong>Rectilinear Unit(Relu):</strong></p>\n\n![Relu](https://cdn.upgrad.com/UpGrad/temp/3cf2117d-2739-470e-9b76-ca721c929df0/ReLU.PNG)\n\n<p style=\"font-size:200%;text-align:center\">Output = <span>&#402;</span>(x) = x for x>=0 else 0 otherwise</p>\n\n<p style=\"font-size:180%\"><strong>Useful Resource:</strong></p>\n\n<iframe width=\"956\" height=\"538\" src=\"https://www.youtube.com/embed/aircAruvnKk\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n\n<p style=\"font-size:120%\"><strong>With this, I end my kernel here. In the next Kernel, we will look at the different hyperparameters of a Neural Network.</strong></p>","metadata":{}},{"cell_type":"markdown","source":"<h1 style=\"font-size:50px;color:#6166B3\">To <strong style=\"color:black\">Conclude:</strong></h1>\n\n<ol>\n    <li style=\"font-size:120%\">Perceptrons are made up of several Threshold Logic Units(TLUs)</li>\n    <li style=\"font-size:120%\">The Neurons in the perceptron accept inputs and assign each of them some weight depending on the importance and priority.</li>\n    <li style=\"font-size:120%\">Various signals are amplified or suppresed by these neurons based on the weights and the bias.</li>\n    <li style=\"font-size:120%\">Final decisions are made once these signals of weighted sum of inputs are passed to the Activation Function. Based on the output given by the Function, the Neural Network makes it's decisions.</li> \n</ol>\n\n<h1 style=\"font-size:50px;color:#6166B3\">Quick <strong style=\"color:black\">Links:</strong></h1>\n\n<a href=\"https://www.kaggle.com/vivek468/dl-part1-introduction-to-neural-network\" style=\"font-size:200%;text-decoration:none\">DL Part1: Introduction to Neural Network</a>\n\n<a href=\"https://www.kaggle.com/vivek468/dl-part3-assumptions-inputs-and-hyperparameters\" style=\"font-size:200%;text-decoration:none\">DL Part3: Assumptions, Inputs and HyperParameters</a>","metadata":{}},{"cell_type":"markdown","source":"<h1 style=\"font-size:20px;color:#6166B3;text-align:center\">If you like/fork my work, do give this an upvote. Appreciate your support and happy learning! Follow me for more on Deep Learning. Coming Soon.</h1>","metadata":{}}]}