{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-23T22:12:36.254808Z","iopub.execute_input":"2021-11-23T22:12:36.255244Z","iopub.status.idle":"2021-11-23T22:12:36.297595Z","shell.execute_reply.started":"2021-11-23T22:12:36.255093Z","shell.execute_reply":"2021-11-23T22:12:36.296361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sklearn\nsklearn.__version__","metadata":{"execution":{"iopub.status.busy":"2021-11-23T22:12:36.299475Z","iopub.execute_input":"2021-11-23T22:12:36.300081Z","iopub.status.idle":"2021-11-23T22:12:37.211683Z","shell.execute_reply.started":"2021-11-23T22:12:36.300036Z","shell.execute_reply":"2021-11-23T22:12:37.210842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import accuracy_score, roc_auc_score","metadata":{"execution":{"iopub.status.busy":"2021-11-23T22:12:37.213075Z","iopub.execute_input":"2021-11-23T22:12:37.213396Z","iopub.status.idle":"2021-11-23T22:12:37.395077Z","shell.execute_reply.started":"2021-11-23T22:12:37.213354Z","shell.execute_reply":"2021-11-23T22:12:37.393937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Loading data as usual, special thanks to @criskiev for providing the training labels before the infamous \"flip\".","metadata":{}},{"cell_type":"code","source":"train_data =  pd.read_csv('../input/tabular-playground-series-nov-2021/train.csv')\ntrain_data_0 = pd.read_csv('../input/november21/train.csv')\ntest_data =  pd.read_csv('../input/tabular-playground-series-nov-2021/test.csv')\nX = train_data.drop('target',axis=1).set_index('id')\ny = train_data.target\ny0 = train_data_0.target\nX_test = test_data.set_index('id')","metadata":{"execution":{"iopub.status.busy":"2021-11-23T22:12:37.39759Z","iopub.execute_input":"2021-11-23T22:12:37.398238Z","iopub.status.idle":"2021-11-23T22:13:23.362029Z","shell.execute_reply.started":"2021-11-23T22:12:37.398059Z","shell.execute_reply":"2021-11-23T22:13:23.360667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del train_data, train_data_0, test_data\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-11-23T22:13:23.364222Z","iopub.execute_input":"2021-11-23T22:13:23.364509Z","iopub.status.idle":"2021-11-23T22:13:23.563301Z","shell.execute_reply.started":"2021-11-23T22:13:23.364466Z","shell.execute_reply":"2021-11-23T22:13:23.562436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ss = StandardScaler().fit(X)\nX = pd.DataFrame(ss.transform(X),index=X.index,columns=X.columns)\nX_test = pd.DataFrame(ss.transform(X_test),index=X_test.index,columns=X_test.columns)","metadata":{"execution":{"iopub.status.busy":"2021-11-23T22:13:23.564944Z","iopub.execute_input":"2021-11-23T22:13:23.565467Z","iopub.status.idle":"2021-11-23T22:13:25.180467Z","shell.execute_reply.started":"2021-11-23T22:13:23.565416Z","shell.execute_reply":"2021-11-23T22:13:25.179557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If we fitted a hard margin SVM with all the samples, there would be too many (600000) constraints for the cvxopt solver. Instead, we build a soft margin SVM first as a first approximation of the decision boundary, and take only 3% of the samples around this approximate boundary.","metadata":{}},{"cell_type":"code","source":"clf = LinearSVC(C=1e6, dual=False, tol=1e-6, max_iter=100000,random_state=42).fit(X,y0)\naccuracy_score(y0,clf.predict(X))","metadata":{"execution":{"iopub.status.busy":"2021-11-23T22:13:25.18195Z","iopub.execute_input":"2021-11-23T22:13:25.182291Z","iopub.status.idle":"2021-11-23T22:13:38.526015Z","shell.execute_reply.started":"2021-11-23T22:13:25.18224Z","shell.execute_reply":"2021-11-23T22:13:38.525107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores = clf.decision_function(X)\npc = 3.0\nlo=np.percentile(scores[scores<0],100-pc/2)\nhi=np.percentile(scores[scores>=0],pc/2)\nmask = np.logical_and(scores>lo,scores<hi)\nX_b, y0_b = X[mask], y0[mask]","metadata":{"execution":{"iopub.status.busy":"2021-11-23T22:13:38.527714Z","iopub.execute_input":"2021-11-23T22:13:38.528264Z","iopub.status.idle":"2021-11-23T22:13:38.658602Z","shell.execute_reply.started":"2021-11-23T22:13:38.528209Z","shell.execute_reply":"2021-11-23T22:13:38.657613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install cvxopt","metadata":{"execution":{"iopub.status.busy":"2021-11-23T22:13:38.660825Z","iopub.execute_input":"2021-11-23T22:13:38.661566Z","iopub.status.idle":"2021-11-23T22:13:50.296483Z","shell.execute_reply.started":"2021-11-23T22:13:38.66151Z","shell.execute_reply":"2021-11-23T22:13:50.295282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cvxopt","metadata":{"execution":{"iopub.status.busy":"2021-11-23T22:13:50.299892Z","iopub.execute_input":"2021-11-23T22:13:50.300308Z","iopub.status.idle":"2021-11-23T22:13:50.322149Z","shell.execute_reply.started":"2021-11-23T22:13:50.300273Z","shell.execute_reply":"2021-11-23T22:13:50.321112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we solve the dual form of the hard margin SVM optimization problem. The following function is heavily based on the implementation of Xavier Bourret Sicotte.\n\n[https://xavierbourretsicotte.github.io/SVM_implementation.html](https://xavierbourretsicotte.github.io/SVM_implementation.html)","metadata":{}},{"cell_type":"code","source":"def hard_margin_svm(X,y):\n    X = X.to_numpy()\n    y = y.to_numpy().astype(np.float64)\n    \n    y = (2*y - 1).reshape(-1,1) # convert to +/- 1 target representation\n    \n    m,n = X.shape\n    X_1 = y * X\n    H = np.dot(X_1 , X_1.T) \n\n\n    P = cvxopt.matrix(H)\n    q = cvxopt.matrix(-np.ones((m, 1)))\n    G = cvxopt.matrix(-np.eye(m))\n    h = cvxopt.matrix(np.zeros((m,1)))\n    A = cvxopt.matrix(y.reshape(1, -1))\n    b = cvxopt.matrix(np.zeros(1))\n\n    cvxopt.solvers.options['show_progress'] = True\n    cvxopt.solvers.options['abstol'] = 1e-8\n    cvxopt.solvers.options['reltol'] = 1e-8\n    cvxopt.solvers.options['feastol'] = 1e-8\n\n    solution = cvxopt.solvers.qp(P, q, G, h, A, b)\n    alpha = np.array(solution['x']) \n    \n    w = ((y * alpha).T @ X).reshape(-1,1)\n    S = (alpha > 1e-4).flatten()\n    b = np.mean(y[S] - np.dot(X[S], w))\n    \n    return alpha, w, b\n    ","metadata":{"execution":{"iopub.status.busy":"2021-11-23T22:13:50.328661Z","iopub.execute_input":"2021-11-23T22:13:50.331558Z","iopub.status.idle":"2021-11-23T22:13:50.351733Z","shell.execute_reply.started":"2021-11-23T22:13:50.331488Z","shell.execute_reply":"2021-11-23T22:13:50.350625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now just run it already!","metadata":{}},{"cell_type":"code","source":"alpha, w, b = hard_margin_svm(X_b,y0_b)","metadata":{"execution":{"iopub.status.busy":"2021-11-23T22:13:50.35832Z","iopub.execute_input":"2021-11-23T22:13:50.361959Z","iopub.status.idle":"2021-11-23T22:22:55.749639Z","shell.execute_reply.started":"2021-11-23T22:13:50.361836Z","shell.execute_reply":"2021-11-23T22:22:55.748878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Convergence sooner than I thought! Now we need to write a little function to emulate the function with the same name in LinearSVC. This is not probability, but for AUC scoring purpose, this would suffice as a ranking function.","metadata":{}},{"cell_type":"code","source":"def decision_function(w,b,X):\n    return (X.to_numpy()@w+b).reshape(-1,) ","metadata":{"execution":{"iopub.status.busy":"2021-11-23T22:22:55.753531Z","iopub.execute_input":"2021-11-23T22:22:55.753989Z","iopub.status.idle":"2021-11-23T22:22:55.759661Z","shell.execute_reply.started":"2021-11-23T22:22:55.753949Z","shell.execute_reply":"2021-11-23T22:22:55.758674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Something we need to check before moving on. We only fitted the classifier with 3% of the data around the approximate decision boundary. How do we know that the hard margin constraints, viz., $$\\tilde{y}_i(x_i\\cdot w+b)\\ge1$$ are satisfied for the rest of the samples? We need to check this manually.","metadata":{}},{"cell_type":"code","source":"min(decision_function(w,b,X)*(2*y0.to_numpy()-1))","metadata":{"execution":{"iopub.status.busy":"2021-11-23T22:22:55.76158Z","iopub.execute_input":"2021-11-23T22:22:55.762266Z","iopub.status.idle":"2021-11-23T22:22:56.003475Z","shell.execute_reply.started":"2021-11-23T22:22:55.762217Z","shell.execute_reply":"2021-11-23T22:22:56.002657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Okay, this is practically 1. We have indeed found the hard margin SVM. And the margin width is given by $\\frac{2}{||w||}$.","metadata":{}},{"cell_type":"code","source":"2/np.linalg.norm(w)","metadata":{"execution":{"iopub.status.busy":"2021-11-23T22:22:56.005109Z","iopub.execute_input":"2021-11-23T22:22:56.00541Z","iopub.status.idle":"2021-11-23T22:22:56.011457Z","shell.execute_reply.started":"2021-11-23T22:22:56.005369Z","shell.execute_reply":"2021-11-23T22:22:56.010861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For perspective, all the features have been normalized to stdev 1. So the two classes can be linearly separated, but barely -- the margin is tiny.\n\nNext, what is the training accuracy on the original, unaltered labels? We know it should be 1 because the hard margin SVM separates the two classes by definition. Let's double check that.","metadata":{}},{"cell_type":"code","source":"accuracy_score(y0,decision_function(w,b,X)>0)","metadata":{"execution":{"iopub.status.busy":"2021-11-23T22:22:56.012386Z","iopub.execute_input":"2021-11-23T22:22:56.013177Z","iopub.status.idle":"2021-11-23T22:22:56.168678Z","shell.execute_reply.started":"2021-11-23T22:22:56.013126Z","shell.execute_reply":"2021-11-23T22:22:56.167825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"How does the training AUC score fare on the altered labels?","metadata":{}},{"cell_type":"code","source":"roc_auc_score(y,decision_function(w,b,X))","metadata":{"execution":{"iopub.status.busy":"2021-11-23T22:22:56.16995Z","iopub.execute_input":"2021-11-23T22:22:56.170697Z","iopub.status.idle":"2021-11-23T22:22:56.503667Z","shell.execute_reply.started":"2021-11-23T22:22:56.170637Z","shell.execute_reply":"2021-11-23T22:22:56.502776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nFinally, let's make a submission.","metadata":{}},{"cell_type":"code","source":"scores = decision_function(w,b,X_test)","metadata":{"execution":{"iopub.status.busy":"2021-11-23T22:22:56.504807Z","iopub.execute_input":"2021-11-23T22:22:56.505005Z","iopub.status.idle":"2021-11-23T22:22:56.531681Z","shell.execute_reply.started":"2021-11-23T22:22:56.50498Z","shell.execute_reply":"2021-11-23T22:22:56.530728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame({'id': X_test.index, 'target': scores}).to_csv('submission.csv', index=False)\nprint(\"Submission saved!\")","metadata":{"execution":{"iopub.status.busy":"2021-11-23T22:22:56.533223Z","iopub.execute_input":"2021-11-23T22:22:56.533855Z","iopub.status.idle":"2021-11-23T22:22:58.573213Z","shell.execute_reply.started":"2021-11-23T22:22:56.533805Z","shell.execute_reply":"2021-11-23T22:22:58.572467Z"},"trusted":true},"execution_count":null,"outputs":[]}]}