{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom pathlib import Path\n\nimport optuna\nimport matplotlib\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\n\ndata_dir = Path('../input/tabular-playground-series-nov-2021/')\n\ndf_train = pd.read_csv(\n    data_dir / \"train.csv\",\n    index_col='id',\n    #nrows=540000,# comment this row to use the full dataset\n)\n\ndf_train.head(20)","metadata":{"execution":{"iopub.status.busy":"2021-11-30T09:54:31.872448Z","iopub.execute_input":"2021-11-30T09:54:31.87344Z","iopub.status.idle":"2021-11-30T09:54:43.727582Z","shell.execute_reply.started":"2021-11-30T09:54:31.873319Z","shell.execute_reply":"2021-11-30T09:54:43.726626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Downcasting the traind dataset.\nfor col in df_train.columns:\n    \n    if df_train[col].dtype == \"float64\":\n        df_train[col] = pd.to_numeric(df_train[col], downcast=\"float\")\n        \n    if df_train[col].dtype == \"int64\":\n        df_train[col] = pd.to_numeric(df_train[col], downcast=\"integer\")","metadata":{"execution":{"iopub.status.busy":"2021-11-30T09:54:45.663337Z","iopub.execute_input":"2021-11-30T09:54:45.663775Z","iopub.status.idle":"2021-11-30T09:54:54.703234Z","shell.execute_reply.started":"2021-11-30T09:54:45.663723Z","shell.execute_reply":"2021-11-30T09:54:54.702171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Heatmap to View Missing Values by Variable\nplt.figure(figsize = (14,6))\np = sns.heatmap(df_train.isnull(), yticklabels = False, cbar = False, cmap = 'viridis')\np.axes.set_title(\"Valores Ausentes\", fontsize = 20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the unique values\n{df_train[col].nunique():col for col in df_train.columns if df_train[col].nunique() > 0}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory data analysis","metadata":{}},{"cell_type":"code","source":"df_train.describe().T.style.background_gradient(cmap='Blues')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr = df_train.corr()\ncorr[['target']].sort_values(by = 'target',ascending = False).style.background_gradient()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# View Dataset Class Distribution\n\nsns.set(style=\"whitegrid\")\n\n# Using a bar chart to show the distribution of classes\nbp = sns.countplot(x=df_train['target'])\nplt.title(\"Dataset Class Distribution\")\nbp.set_xticklabels([\"0\",\"1\"])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function for applying recursive scaling\ndef recursive_scaler(x, n_scaler, scaler):\n    for e in range(1,n_scaler):\n        x = scaler.transform(x)\n    return x","metadata":{"execution":{"iopub.status.busy":"2021-11-30T09:55:02.991777Z","iopub.execute_input":"2021-11-30T09:55:02.992123Z","iopub.status.idle":"2021-11-30T09:55:02.997662Z","shell.execute_reply.started":"2021-11-30T09:55:02.99209Z","shell.execute_reply":"2021-11-30T09:55:02.996396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop Cols\ndropcols = ['f2','f35','target']","metadata":{"execution":{"iopub.status.busy":"2021-11-30T09:55:06.17507Z","iopub.execute_input":"2021-11-30T09:55:06.175685Z","iopub.status.idle":"2021-11-30T09:55:06.180724Z","shell.execute_reply.started":"2021-11-30T09:55:06.175648Z","shell.execute_reply":"2021-11-30T09:55:06.17917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Features and target\nFEATURES = df_train.drop(dropcols, axis = 1)\nTARGET = df_train['target'].astype(int).astype(str)","metadata":{"execution":{"iopub.status.busy":"2021-11-30T09:55:19.261547Z","iopub.execute_input":"2021-11-30T09:55:19.261859Z","iopub.status.idle":"2021-11-30T09:55:20.289458Z","shell.execute_reply.started":"2021-11-30T09:55:19.261827Z","shell.execute_reply":"2021-11-30T09:55:20.288347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = FEATURES.columns","metadata":{"execution":{"iopub.status.busy":"2021-11-30T09:55:26.972602Z","iopub.execute_input":"2021-11-30T09:55:26.972927Z","iopub.status.idle":"2021-11-30T09:55:26.978114Z","shell.execute_reply.started":"2021-11-30T09:55:26.972896Z","shell.execute_reply":"2021-11-30T09:55:26.977056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2021-11-30T09:55:29.869258Z","iopub.execute_input":"2021-11-30T09:55:29.869623Z","iopub.status.idle":"2021-11-30T09:55:29.878986Z","shell.execute_reply.started":"2021-11-30T09:55:29.869581Z","shell.execute_reply":"2021-11-30T09:55:29.87795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Break off validation set from training data\nX_train, X_valid, y_train, y_valid = train_test_split(FEATURES, TARGET, \n                                                      train_size=0.67, test_size=0.33, random_state=42, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-30T09:55:32.621575Z","iopub.execute_input":"2021-11-30T09:55:32.62188Z","iopub.status.idle":"2021-11-30T09:55:33.075493Z","shell.execute_reply.started":"2021-11-30T09:55:32.621848Z","shell.execute_reply":"2021-11-30T09:55:33.074445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\nfrom catboost import CatBoostClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom xgboost import XGBRegressor, XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\nimport gc","metadata":{"execution":{"iopub.status.busy":"2021-11-30T09:55:34.816653Z","iopub.execute_input":"2021-11-30T09:55:34.817374Z","iopub.status.idle":"2021-11-30T09:55:35.766269Z","shell.execute_reply.started":"2021-11-30T09:55:34.817336Z","shell.execute_reply":"2021-11-30T09:55:35.765258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nfrom sklearn.model_selection import RepeatedKFold\nfrom optuna import create_study\nfrom optuna.samplers import TPESampler\nfrom optuna.integration import XGBoostPruningCallback\nfrom catboost import Pool\nfrom sklearn.utils import resample\nimport multiprocessing\nfrom sklearn.metrics import *\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, QuantileTransformer, MaxAbsScaler","metadata":{"execution":{"iopub.status.busy":"2021-11-30T09:55:37.203089Z","iopub.execute_input":"2021-11-30T09:55:37.203444Z","iopub.status.idle":"2021-11-30T09:55:37.212039Z","shell.execute_reply.started":"2021-11-30T09:55:37.203396Z","shell.execute_reply":"2021-11-30T09:55:37.210888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#%%time\n#n_trials = int(9)\n#SEED = 143","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#%%time\n#Function to seed everything\n#def seed_everything(seed):\n#    random.seed(seed)\n#    np.random.seed(seed)\n#    os.environ['PYTHONHASHSEED'] = str(seed)\n#seed_everything(SEED)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#n_scaler = 4\n\n#X = np.array(X_train)\n#y = np.array(y_train)\n\n#scaler = MinMaxScaler()\n#scaler.fit(X)\n#X = scaler.transform(X)\n\n#X  = recursive_scaler(X, n_scaler, scaler)\n\n#X = pd.DataFrame(X, columns = features)\n\n#X['sum']  = X[features].sum(axis=1)\n#X['mean'] = X[features].mean(axis=1)\n#X['std']  = X[features].std(axis=1)\n#X['max']  = X[features].max(axis=1)\n#X['min']  = X[features].min(axis=1)\n#X['kurt'] = X[features].kurtosis(axis=1)\n\n#X = np.array(X)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#def objective(trial):\n    # Parameters\n#    train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.33, random_state=int(SEED), shuffle=True)\n#    train_pool = Pool(train_x, train_y)\n#    test_pool = Pool(test_x, test_y)    \n#    params = {'iterations' : trial.suggest_int('iterations', 50, 1600),                         \n#            'depth' : trial.suggest_int('depth', 2, 10),                                       \n#            'random_strength' :trial.suggest_int('random_strength', 0, 100),                       \n#            'bagging_temperature' :trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n#            'learning_rate' :trial.suggest_loguniform('learning_rate', 1e-3, 1e-1),\n#            'od_type': trial.suggest_categorical('od_type', ['IncToDec', 'Iter'])\n#        }\n    # Learning\n#    model = CatBoostClassifier(\n#            loss_function=\"Logloss\",\n#            eval_metric=\"AUC\",\n#            task_type=\"GPU\",\n#            l2_leaf_reg=0.2,\n#            random_seed=SEED,\n#            border_count=64,\n#            **params\n#    )        \n#    model.fit(train_pool)\n    # Predict\n#    preds = model.predict_proba(test_x)\n   # Evaluation\n#    ROC_AUC_Score = roc_auc_score(test_y, preds[:, 1])\n#    print('ROC AUC Score of CatBoost =', ROC_AUC_Score)\n#    return ROC_AUC_Score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#%%time\n#study = optuna.create_study(direction = \"maximize\", sampler = TPESampler(seed=int(SEED)))\n#study.optimize(objective, n_trials = n_trials)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This is nice handy constant to turn on and off the GPU. When `False`\n# the notebook will ignore the GPU even when present.\n#GPU_ENABLED = True","metadata":{"execution":{"iopub.status.busy":"2021-11-30T09:55:42.272979Z","iopub.execute_input":"2021-11-30T09:55:42.27378Z","iopub.status.idle":"2021-11-30T09:55:42.278866Z","shell.execute_reply.started":"2021-11-30T09:55:42.273736Z","shell.execute_reply":"2021-11-30T09:55:42.277368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#X = np.array(X_train)\n#y = np.array(y_train)\n\n#scaler = QuantileTransformer()\n#scaler.fit(X)\n#X  = recursive_scaler(X, 2, scaler)\n#X = scaler.transform(X)\n\n#X = pd.DataFrame(X, columns = features)\n\n#X['sum']  = X[features].sum(axis=1)\n#X['mean'] = X[features].mean(axis=1)\n#X['std']  = X[features].std(axis=1)\n#X['max']  = X[features].max(axis=1)\n#X['min']  = X[features].min(axis=1)\n#X['kurt'] = X[features].kurtosis(axis=1)\n\n#X = np.array(X)","metadata":{"execution":{"iopub.status.busy":"2021-11-30T09:55:44.522867Z","iopub.execute_input":"2021-11-30T09:55:44.523589Z","iopub.status.idle":"2021-11-30T09:56:01.156518Z","shell.execute_reply.started":"2021-11-30T09:55:44.523543Z","shell.execute_reply":"2021-11-30T09:56:01.15546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#def train_model_for_study(X, y, model):\n#    X_train, X_valid, y_train, y_valid = train_test_split(\n#        X, \n#        y, \n#        test_size=0.33, \n#        random_state=42,\n#        shuffle=True\n#    )\n\n\n#    model.fit(\n#       X_train, \n#       y_train,\n     #  sample_weight=classes_weights,\n#        early_stopping_rounds=15,\n#        eval_set=[(X_train, y_train), (X_valid, y_valid)],\n#        eval_metric=\"auc\",\n#        verbose=True\n#    )\n\n#    yhat = model.predict_proba(X_valid)\n#    score = roc_auc_score(y_valid, yhat[:, 1])\n#    return score ","metadata":{"execution":{"iopub.status.busy":"2021-11-30T09:56:02.976582Z","iopub.execute_input":"2021-11-30T09:56:02.976986Z","iopub.status.idle":"2021-11-30T09:56:02.987848Z","shell.execute_reply.started":"2021-11-30T09:56:02.976941Z","shell.execute_reply":"2021-11-30T09:56:02.986146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#def objective_xgb(trial):\n#   \"\"\"\n#    Objective function to tune an `XGBRegressor` model.\n#    \"\"\"\n\n#    params = {\n#        'n_estimators': trial.suggest_int(\"n_estimators\", 1000, 10000),\n#        'reg_alpha': trial.suggest_loguniform(\"reg_alpha\", 1e-8, 100.0),\n#        'reg_lambda': trial.suggest_loguniform(\"reg_lambda\", 1e-8, 100.0),\n#        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0, step=0.1),\n#        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 2.0, log=True),\n#        'max_depth': trial.suggest_int(\"max_depth\", 2, 9),\n#        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 1.0),\n#    }\n\n#    if GPU_ENABLED:\n#        params[\"tree_method\"] = \"gpu_hist\"\n#        params[\"predictor\"] = \"gpu_predictor\"\n\n#    model = XGBClassifier(\n#        booster=\"gbtree\",\n#        objective=\"binary:logistic\",\n#        n_jobs=-1, \n#        random_state=42,\n#        **params\n#    )\n\n#    return train_model_for_study(X, y, model)","metadata":{"execution":{"iopub.status.busy":"2021-11-30T09:56:09.740256Z","iopub.execute_input":"2021-11-30T09:56:09.740565Z","iopub.status.idle":"2021-11-30T09:56:09.750682Z","shell.execute_reply.started":"2021-11-30T09:56:09.740535Z","shell.execute_reply":"2021-11-30T09:56:09.749478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#study_xgb = optuna.create_study(direction=\"maximize\")\n#study_xgb.optimize(objective_xgb, n_trials=9)\n#study_xgb.best_params","metadata":{"execution":{"iopub.status.busy":"2021-11-30T09:56:15.893655Z","iopub.execute_input":"2021-11-30T09:56:15.893974Z","iopub.status.idle":"2021-11-30T09:58:38.749818Z","shell.execute_reply.started":"2021-11-30T09:56:15.89394Z","shell.execute_reply":"2021-11-30T09:58:38.748763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#%%time\n#study = optuna.create_study()\n#study.optimize(objective, n_trials = 100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#%%time\n# Save\n#pickle.dump(study.best_trial.params, open('CatBoost_Hyperparameter.pickle', 'wb'))\n#print('CatBoost Hyperparameter:', study.best_trial.params)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# CastBoost model configuration\nctb = CatBoostClassifier(iterations=1462, \n                        learning_rate = 0.08775145655977466, \n                        random_strength = 15, \n                        l2_leaf_reg = 0.2, \n                        depth = 3,\n                        bagging_temperature=1.0190751501900164,\n                        od_type='IncToDec',\n                        loss_function=\"Logloss\",\n                        eval_metric='AUC:type=Ranking',\n                        random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-11-30T10:05:26.269004Z","iopub.execute_input":"2021-11-30T10:05:26.269955Z","iopub.status.idle":"2021-11-30T10:05:26.281847Z","shell.execute_reply.started":"2021-11-30T10:05:26.269911Z","shell.execute_reply":"2021-11-30T10:05:26.280526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# XGboost model configuration\nxgb = XGBClassifier(\n        learning_rate= 0.02003527792413422,\n        reg_alpha = 0.004448966694735556,\n        reg_lambda = 2.610959038520974,\n        max_depth=3,\n        subsample=0.8,\n        colsample_bytree=0.6183087610995104,\n        objective='binary:logistic',\n        n_estimators=4884,\n        eval_metric='auc',\n        n_jobs=-1,\n        tree_method='gpu_hist',\n        predictor = \"gpu_predictor\",\n        # Uncomment if you want to use GPU. Recommended for whole training set.\n        #tree_method='gpu_hist',\n        random_state=42,\n        )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Applying cross-validation using two models in parallel and storing the best results of each model.","metadata":{}},{"cell_type":"code","source":"kf = StratifiedShuffleSplit(n_splits=9, test_size=0.33, random_state=42)\n#kf = StratifiedKFold(n_splits=6, shuffle=False)\n#cv_score = []\ncv_score = {}\ni=1\nn_scaler1 = 4\nn_scaler2 = 2\n\nX_1 = np.array(X_train)\nX_2 = np.array(X_train)\ny = np.array(y_train)\n\nscaler1 = MinMaxScaler()\nscaler1.fit(X_1)\n\nscaler2 = QuantileTransformer()\nscaler2.fit(X_2)\n\nX_1  = recursive_scaler(X_1, n_scaler1, scaler1)\nX_2  = recursive_scaler(X_2, n_scaler2, scaler2)\n#X_2  = scaler2.transform(X_2)\n\nX_1 = pd.DataFrame(X_1, columns = features)\n\nX_1['sum']  = X_1[features].sum(axis=1)\nX_1['mean'] = X_1[features].mean(axis=1)\nX_1['std']  = X_1[features].std(axis=1)\nX_1['max']  = X_1[features].max(axis=1)\nX_1['min']  = X_1[features].min(axis=1)\nX_1['kurt'] = X_1[features].kurtosis(axis=1)\n\nX_1 = np.array(X_1)\n\nX_2 = pd.DataFrame(X_2, columns = features)\n\nX_2['sum']  = X_2[features].sum(axis=1)\nX_2['mean'] = X_2[features].mean(axis=1)\nX_2['std']  = X_2[features].std(axis=1)\nX_2['max']  = X_2[features].max(axis=1)\nX_2['min']  = X_2[features].min(axis=1)\nX_2['kurt'] = X_2[features].kurtosis(axis=1)\n\nX_2 = np.array(X_2)\n\nX_valid_1 = recursive_scaler(X_valid, n_scaler1, scaler1)\nX_valid_2 = recursive_scaler(X_valid, n_scaler2, scaler2)\n#X_valid_2  = scaler2.transform(X_valid)\n\nX_valid_1 = pd.DataFrame(X_valid_1, columns = features)\nX_valid_2 = pd.DataFrame(X_valid_2, columns = features)\n\n\nX_valid_1['sum']  = X_valid_1[features].sum(axis=1)\nX_valid_1['mean'] = X_valid_1[features].mean(axis=1)\nX_valid_1['std']  = X_valid_1[features].std(axis=1)\nX_valid_1['max']  = X_valid_1[features].max(axis=1)\nX_valid_1['min']  = X_valid_1[features].min(axis=1)\nX_valid_1['kurt'] = X_valid_1[features].kurtosis(axis=1)\n\nX_valid_1 = np.array(X_valid_1)\n\nX_valid_2['sum']  = X_valid_2[features].sum(axis=1)\nX_valid_2['mean'] = X_valid_2[features].mean(axis=1)\nX_valid_2['std']  = X_valid_2[features].std(axis=1)\nX_valid_2['max']  = X_valid_2[features].max(axis=1)\nX_valid_2['min']  = X_valid_2[features].min(axis=1)\nX_valid_2['kurt'] = X_valid_2[features].kurtosis(axis=1)\n\nX_valid_2 = np.array(X_valid_2)\n\nfor train_index, test_index in kf.split(X_1, y):\n    print(train_index)  \n    print('{} of KFold {}'.format(i,kf.n_splits)) \n    xtr_1,xvl_1 = X_1[train_index],X_1[test_index]\n    xtr_2,xvl_2 = X_2[train_index],X_2[test_index]\n    ytr,yvl = y[train_index],y[test_index]\n\n    #model\n    eval_set_1 = [(xtr_1,ytr), (xvl_1,yvl)]\n    eval_set_2 = [(xtr_2,ytr), (xvl_2,yvl)]\n    \n    \n    ctb.fit(xtr_1,ytr, early_stopping_rounds=15, eval_set=eval_set_1, verbose=False)\n    xgb.fit(xtr_2,ytr, early_stopping_rounds=15, eval_set=eval_set_2, verbose=False)\n    score1 = roc_auc_score(np.array(y_valid),ctb.predict_proba(X_valid_1)[:, 1])\n    score2 = roc_auc_score(np.array(y_valid),xgb.predict_proba(X_valid_2)[:, 1])\n    print('ROC AUC score1:',score1)\n    print('ROC AUC score2:',score2)\n    \n    print(train_index.shape)\n    print(test_index.shape)\n    if score1 > score2:\n        #cv_score.append(score1) \n        cv_score.update({'ctb'+str(i): score1})\n        pickle.dump(ctb, open(\"model\"+str(i)+\".pickle.dat\", \"wb\"))\n    else:\n        #cv_score.append(score2)\n        cv_score.update({'xgb'+str(i): score2})\n        pickle.dump(xgb, open(\"model\"+str(i)+\".pickle.dat\", \"wb\"))\n    i+=1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check Scores\ncv_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load model from file\nmdl1 = pickle.load(open(\"model1.pickle.dat\", \"rb\"))\nmdl2 = pickle.load(open(\"model2.pickle.dat\", \"rb\"))\nmdl3 = pickle.load(open(\"model3.pickle.dat\", \"rb\"))\nmdl4 = pickle.load(open(\"model4.pickle.dat\", \"rb\"))\nmdl5 = pickle.load(open(\"model5.pickle.dat\", \"rb\"))\nmdl6 = pickle.load(open(\"model6.pickle.dat\", \"rb\"))\nmdl7 = pickle.load(open(\"model7.pickle.dat\", \"rb\"))\nmdl8 = pickle.load(open(\"model8.pickle.dat\", \"rb\"))\nmdl9 = pickle.load(open(\"model9.pickle.dat\", \"rb\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#y_pred1 = ctb1.predict_proba(X_valid_1)\n#y_pred2 = ctb2.predict_proba(X_valid_1)\n#y_pred3 = ctb3.predict_proba(X_valid_1)\n#y_pred4 = ctb4.predict_proba(X_valid_1)\n#y_pred5 = ctb5.predict_proba(X_valid_1)\n#y_pred6 = ctb6.predict_proba(X_valid_1)\n#y_pred7 = ctb7.predict_proba(X_valid_1)\n#y_pred8 = ctb8.predict_proba(X_valid_1)\n#y_pred9 = ctb9.predict_proba(X_valid_1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#y_pred_1 = np.mean([y_pred1, y_pred2, y_pred3, y_pred4, y_pred5, y_pred6, y_pred7, y_pred8, y_pred9], axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Applying the models with the best scores to the validation data then I extract the average of the prediction results.","metadata":{}},{"cell_type":"code","source":"modelapply = [ mdl1, mdl2, mdl3, mdl4, mdl5, mdl6, mdl7, mdl8, mdl9 ]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"modelapply2 = [[], []]\ni = 0\nfor item in cv_score.keys():\n    if 'ctb' in item:\n        modelapply2[0].append('X_valid_1')\n    else:\n        modelapply2[0].append('X_valid_2')\n    modelapply2[1].append(modelapply[i])\n    i = i + 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_valid_1.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#def predict_subset(X_1, X_2, X1_name, ModelApply):\n#    y_pred = [[],[],[],[],[],[],[],[],[]]\n#    start = 0\n#    for i in range(0,9):\n#        print(ModelApply[0][i])\n#        model = ModelApply[1][i]\n\n#        if ModelApply[0][i] == 'X_valid_1':\n#            chunk_size = int(X_1.shape[0] / 9)\n#            subset = X_1[start:start + chunk_size]\n#            y_pred[i] = model.predict_proba(subset)\n#        else:\n#            chunk_size = int(X_2.shape[0] / 9)\n#            subset = X_2[start:start + chunk_size]\n#            y_pred[i] = model.predict_proba(subset)\n        #start = start + chunk_size\n#    return y_pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_model(X_1, X_2, X1_name, ModelApply):\n    y_pred = [[],[],[],[],[],[],[],[],[]]\n    for i in range(0,9):\n        print(ModelApply[0][i])\n        model = ModelApply[1][i]\n\n        if ModelApply[0][i] == 'X_valid_1':\n            y_pred[i] = model.predict_proba(X_1)\n        else:\n            y_pred[i] = model.predict_proba(X_2)\n    return y_pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = predict_model(X_valid_1, X_valid_2, 'X_valid_1', modelapply2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_1 = np.mean([y_pred[0], y_pred[1], y_pred[2], y_pred[3], y_pred[4], y_pred[5], y_pred[6], y_pred[7], y_pred[8]], axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#flatlist=[element.tolist() for sublist in y_pred for element in sublist]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#y_pred_1 = np.array(flatlist)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# retrieve just the probabilities for the positive class\npos_probs = y_pred_1[:, 1]\n# plot no skill roc curve\nplt.plot([0, 1], [0, 1], linestyle='--', label='No Skill')\n# calculate roc curve for model\nfpr, tpr, _ = roc_curve(y_valid.astype(str).astype(int), pos_probs)\n# plot model roc curve\nplt.plot(fpr, tpr, marker='.', label='Logistic')\n# axis labels\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\n# show the legend\nplt.legend()\n# show the plot\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"precisions, recalls, thresholds = precision_recall_curve(y_valid.astype(str).astype(int), y_pred_1[:,1])\n\ndef plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n    plt.xlabel(\"Threshold\")\n    plt.legend(loc=\"upper left\")\n    plt.ylim([0, 1])\n    \nplot_precision_recall_vs_threshold(precisions, recalls, thresholds)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import scikitplot as skplt\nskplt.metrics.plot_roc(y_valid.astype(str).astype(int), y_pred_1, figsize=(10, 8))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# reading test data\nX_test = pd.read_csv(data_dir / \"test.csv\", index_col='id')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Applying the models with the best scores to the test data then I extract the average of the prediction results.","metadata":{}},{"cell_type":"code","source":"# Downcasting the test dataset.\nfor col in X_test.columns:\n    \n    if X_test[col].dtype == \"float64\":\n        X_test[col] = pd.to_numeric(X_test[col], downcast=\"float\")\n        \n    if X_test[col].dtype == \"int64\":\n        X_test[col] = pd.to_numeric(X_test[col], downcast=\"integer\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test.describe().T.style.background_gradient(cmap='Blues')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get predictions\ndropcols = ['f2','f35']\nXt = X_test.drop(dropcols, axis = 1)\ncolumns = Xt.columns\n\nXt1  = recursive_scaler(Xt, n_scaler1, scaler1)\nXt2  = recursive_scaler(Xt, n_scaler2, scaler2)\n#Xt2  = scaler2.transform(Xt)\n\nXt1 = pd.DataFrame(Xt1, columns = features, )\nXt2 = pd.DataFrame(Xt2, columns = features)\n\nXt1['sum']  = Xt1[features].sum(axis=1)\nXt1['mean'] = Xt1[features].mean(axis=1)\nXt1['std']  = Xt1[features].std(axis=1)\nXt1['max']  = Xt1[features].max(axis=1)\nXt1['min']  = Xt1[features].min(axis=1)\nXt1['kurt'] = Xt1[features].kurtosis(axis=1)\n\nXt1 = np.array(Xt1)\n\nXt2['sum']  = Xt2[features].sum(axis=1)\nXt2['mean'] = Xt2[features].mean(axis=1)\nXt2['std']  = Xt2[features].std(axis=1)\nXt2['max']  = Xt2[features].max(axis=1)\nXt2['min']  = Xt2[features].min(axis=1)\nXt2['kurt'] = Xt2[features].kurtosis(axis=1)\n\nXt2 = np.array(Xt2)\n#y_pred = ctb.predict_proba(np.array(Xt))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test = predict_model(Xt1, Xt2, 'Xt1', modelapply2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test_1 = np.mean([y_test[0], y_test[1], y_test[2], y_test[3], y_test[4], y_test[5], y_test[6], y_test[7], y_test[8]], axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#y_test = predict_subset(Xt1, Xt2, 'Xt1', modelapply2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#flatlist=[element.tolist() for sublist in y_test for element in sublist]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#y_test_1 = np.array(flatlist)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#y_pred1 = ctb1.predict_proba(Xt1)\n#y_pred2 = ctb2.predict_proba(Xt1)\n#y_pred3 = ctb3.predict_proba(Xt1)\n#y_pred4 = ctb4.predict_proba(Xt1)\n#y_pred5 = ctb5.predict_proba(Xt1)\n#y_pred6 = ctb6.predict_proba(Xt1)\n#y_pred7 = ctb7.predict_proba(Xt1)\n#y_pred8 = ctb8.predict_proba(Xt1)\n#y_pred9 = ctb9.predict_proba(Xt1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#y_test_1 = np.mean([y_pred1, y_pred2, y_pred3, y_pred4, y_pred5, y_pred6, y_pred7, y_pred8, y_pred9], axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_test = pd.Series(\n    y_test_1[:, 1],\n    index=X_test.index,\n    name='target',\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create submission file\ny_pred_test.to_csv(\"submission.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}