{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Keras / Optuna Starter\n\nThe intention of this notebooks is to provide a simple example of how a basic Keras structure can be trained with Optuna. With a bit of experimentation to the architecture it should be fairly easy to get a nice score for this point in the competition. Hopefully this provides a more accessible point for those wishing to experiment with NNs for this project.\n## Grab our data","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nDATA_DIR = '../input/tabular-playground-series-nov-2021/'\n\ntrain = pd.read_csv(DATA_DIR + 'train.csv').set_index('id')\ny = train.pop('target').values\n\ntest = pd.read_csv(DATA_DIR + 'test.csv').set_index('id')\nsample = pd.read_csv(DATA_DIR + 'sample_submission.csv').set_index('id')\n\nno_features = test.shape[1]","metadata":{"execution":{"iopub.status.busy":"2021-11-07T07:46:08.873906Z","iopub.execute_input":"2021-11-07T07:46:08.874153Z","iopub.status.idle":"2021-11-07T07:46:36.976355Z","shell.execute_reply.started":"2021-11-07T07:46:08.874082Z","shell.execute_reply":"2021-11-07T07:46:36.975604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Now lets go generate some normalisations for us to play with\nI have taken out a few examples here and commented more. Robust and Z score seem to provide the best results for me.\n\nNote: I am cheating here and using the test set to help scale the training set. This is one of those things that's cool in a Kaggle competition but probably not advisable in the wild.","metadata":{"execution":{"iopub.status.busy":"2021-11-05T16:26:34.498768Z","iopub.execute_input":"2021-11-05T16:26:34.499093Z","iopub.status.idle":"2021-11-05T16:26:34.503882Z","shell.execute_reply.started":"2021-11-05T16:26:34.49905Z","shell.execute_reply":"2021-11-05T16:26:34.502785Z"}}},{"cell_type":"code","source":"from sklearn.preprocessing import QuantileTransformer, StandardScaler, RobustScaler, MinMaxScaler\nfrom gc import collect\n\ndata = {}\n\n# This was no help at all\n#print('Fitting quantiles transformer: Normal')\nnorm = QuantileTransformer(output_distribution='normal', n_quantiles=1000)\nnorm.fit(pd.concat([train, test]))\ndata['norm'] = {\n    'train': norm.transform(train),\n    'test': norm.transform(test),\n}\n\nprint('Fitting standard scaler')\nnorm = StandardScaler()\nnorm.fit(pd.concat([train, test]))\ndata['z'] = {\n    'train': norm.transform(train),\n    'test': norm.transform(test),\n}\n\nprint('Fitting robust scaler')\nnorm = RobustScaler()\nnorm.fit(pd.concat([train, test]))\ndata['robust'] = {\n    'train': norm.transform(train),\n    'test': norm.transform(test),\n}\n\n# This was no help at all\nprint('Fitting min-max')\nnorm = MinMaxScaler(feature_range=(-1, 1))\nnorm.fit(pd.concat([train, test]))\ndata['min-max'] = {\n    'train': norm.transform(train),\n    'test': norm.transform(test),\n}\n\ndel(train)\ncollect();","metadata":{"execution":{"iopub.status.busy":"2021-11-07T07:46:36.978156Z","iopub.execute_input":"2021-11-07T07:46:36.978552Z","iopub.status.idle":"2021-11-07T07:47:15.264482Z","shell.execute_reply.started":"2021-11-07T07:46:36.978515Z","shell.execute_reply":"2021-11-07T07:47:15.26377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model\nThis is our basic model structure. A few additions that worked for me...\n- Adding an attention layer between the raw and noised features\n- Using add to incorporate residuals","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.layers import BatchNormalization, Dense, Dropout, GaussianNoise, Input, Attention, Add, Concatenate\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.regularizers import l1, l2\nfrom tensorflow.keras.optimizers import Adam\n\ndef get_model(depth, noise, lr):\n    tf.random.set_seed(42)\n    np.random.seed(42)\n    \n    features = 2 ** (depth + 2)\n    \n    # Input\n    inputs = Input(no_features)\n\n    # Add noise to the continuous\n    x = GaussianNoise(noise)(inputs)\n    \n    # Build out some blocks\n    for _ in range(0, depth):\n        x = Dense(features, activation='swish')(x)\n        if features > 8:\n            features /= 8\n    \n    out = Dense(1, activation='sigmoid')(x)\n    \n    # Build\n    model = Model(inputs=inputs, outputs=out, name='perceptomanic')\n\n    # Optimiser\n    opt = Adam(learning_rate=lr)\n\n    # Metrics\n    auc = tf.keras.metrics.AUC(name='auc')\n    \n    # Compile\n    model.compile(loss='binary_crossentropy', \n                  optimizer=opt,\n                  metrics=[auc])\n    \n    return model\n","metadata":{"execution":{"iopub.status.busy":"2021-11-07T07:47:15.265898Z","iopub.execute_input":"2021-11-07T07:47:15.266317Z","iopub.status.idle":"2021-11-07T07:47:20.629832Z","shell.execute_reply.started":"2021-11-07T07:47:15.266276Z","shell.execute_reply":"2021-11-07T07:47:20.628966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Optuna Search","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nimport optuna\n\ndef nn(trial):\n    '''\n    '''\n    params = {\n        'noise': trial.suggest_float('noise', 0.0, 0.06),\n        'lr': trial.suggest_float('lr', 0.0001, 0.1),\n        'depth': trial.suggest_int('depth', 1, 8),\n    }\n    norm_method = trial.suggest_categorical('norm_method', ['robust'])\n    \n    kf = StratifiedKFold(10, shuffle=True, random_state=42)\n    estimates = []\n    y_hat_validation = np.zeros(len(y)) + np.NaN\n    \n    # callbacks\n    callback_early_stopping = EarlyStopping(monitor='val_loss', patience=20, verbose=False, mode='min', restore_best_weights=True)\n    callback_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, verbose=False, mode='min')\n    \n    for train_index, test_index in kf.split(data[norm_method]['train'], y):\n        # Train model\n        model = get_model(**params)\n\n        # Split out test and train\n        X_train = data[norm_method]['train'][train_index]\n        y_train = y[train_index]\n\n        X_test = data[norm_method]['train'][test_index]\n        y_test = y[test_index]\n\n        # Fit\n        model.fit(x=X_train, \n                  y=y_train, \n                  batch_size=4096, \n                  epochs=1000, \n                  validation_data=(X_test, y_test), \n                  verbose=False, \n                  callbacks=[callback_early_stopping, callback_lr])\n\n        # Predict\n        y_hat_validation[test_index] = model.predict(X_test).squeeze()\n        \n        # Check for early stop\n        not_null = ~np.isnan(y_hat_validation)\n        score = roc_auc_score(y[not_null], y_hat_validation[not_null])\n        if score < 0.735:\n            print('Early stopping')\n            return score\n        \n        del(model)\n        \n    score = roc_auc_score(y, y_hat_validation)\n    \n    return score\n","metadata":{"execution":{"iopub.status.busy":"2021-11-07T07:47:20.632157Z","iopub.execute_input":"2021-11-07T07:47:20.632444Z","iopub.status.idle":"2021-11-07T07:47:21.331265Z","shell.execute_reply.started":"2021-11-07T07:47:20.632406Z","shell.execute_reply":"2021-11-07T07:47:21.330398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study = optuna.create_study(study_name='Find me some params dude', direction='maximize')","metadata":{"execution":{"iopub.status.busy":"2021-11-07T07:47:21.332793Z","iopub.execute_input":"2021-11-07T07:47:21.333101Z","iopub.status.idle":"2021-11-07T07:47:21.344275Z","shell.execute_reply.started":"2021-11-07T07:47:21.333064Z","shell.execute_reply":"2021-11-07T07:47:21.343341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study.optimize(nn, timeout=60*60*6, gc_after_trial=True, show_progress_bar=True)\n\nprint('Number of finished trials: {}'.format(len(study.trials)))\n\nprint('Best trial:')\ntrial = study.best_trial\n\nprint('  Value: {}'.format(trial.value))\n\nprint('  Params: ')\nfor key, value in trial.params.items():\n    print('    {}: {}'.format(key, value))","metadata":{"execution":{"iopub.status.busy":"2021-11-07T07:47:21.346474Z","iopub.execute_input":"2021-11-07T07:47:21.347958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Now run it","metadata":{}},{"cell_type":"code","source":"def nn_run(params):\n    '''\n    '''\n    \n    norm_method = params.pop('norm_method')\n    \n    kf = StratifiedKFold(10, shuffle=True, random_state=42)\n    estimates = []\n    y_hat_validation = np.zeros(len(y)) + np.NaN\n    \n    # callbacks\n    callback_early_stopping = EarlyStopping(monitor='val_loss', patience=20, verbose=False, mode='min', restore_best_weights=True)\n    callback_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, verbose=False, mode='min')\n    \n    for train_index, test_index in kf.split(data[norm_method]['train'], y):\n        # Train model\n        model = get_model(**params)\n\n        # Split out test and train\n        X_train = data[norm_method]['train'][train_index]\n        y_train = y[train_index]\n\n        X_test = data[norm_method]['train'][test_index]\n        y_test = y[test_index]\n\n        # Fit\n        model.fit(x=X_train, \n                  y=y_train, \n                  batch_size=4096, \n                  epochs=1000, \n                  validation_data=(X_test, y_test), \n                  verbose=True, \n                  callbacks=[callback_early_stopping, callback_lr])\n\n        # Predict\n        y_hat_validation[test_index] = model.predict(X_test).squeeze()\n        estimates.append(model.predict(data[norm_method]['test']).squeeze())\n        \n        print('\\n')\n        \n        del(model)\n        \n    print(roc_auc_score(y, y_hat_validation))\n    \n    return estimates\n\nestimates = nn_run(study.best_trial.params)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample['target'] = np.vstack(estimates).T.mean(axis=1)\nsample.to_csv('simple_af_nn.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}