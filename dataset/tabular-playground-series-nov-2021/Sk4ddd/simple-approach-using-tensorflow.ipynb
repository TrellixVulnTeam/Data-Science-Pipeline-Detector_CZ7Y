{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Librairies","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport random\n\nimport datetime\nimport os\n\nimport bokeh\n\nfrom bokeh.io import output_notebook\nfrom bokeh.plotting import figure, output_file, show\nfrom bokeh.models import ColumnDataSource, FactorRange, HoverTool, BasicTicker, ColorBar, LinearColorMapper\nfrom bokeh.palettes import Spectral11, colorblind, Inferno, BuGn, brewer, Category20, Viridis256\nfrom bokeh.layouts import row, column, grid\n\n\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\noutput_notebook()","metadata":{"execution":{"iopub.status.busy":"2021-11-29T13:45:43.524873Z","iopub.execute_input":"2021-11-29T13:45:43.525262Z","iopub.status.idle":"2021-11-29T13:45:48.380795Z","shell.execute_reply.started":"2021-11-29T13:45:43.525228Z","shell.execute_reply":"2021-11-29T13:45:48.380087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For this month, let's explore the data and create a basic classifier using Tensorflow.\n\n- ***First Question : Among these 100 features, are they all useful? How can we use them?***\n- ***Second Question : How to create simple data pipeline and model using Tensorflow?***\n\n\nLet's check this","metadata":{}},{"cell_type":"markdown","source":"## Load data","metadata":{}},{"cell_type":"code","source":"files = []\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        files.append(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n\ntrain_df = pd.read_csv(files[1], index_col = 'id')\n\ntest_df = pd.read_csv(files[2], index_col = 'id')\n\n\ntrain_target = train_df.pop('target')","metadata":{"execution":{"iopub.status.busy":"2021-11-29T13:45:50.156257Z","iopub.execute_input":"2021-11-29T13:45:50.156912Z","iopub.status.idle":"2021-11-29T13:46:17.968502Z","shell.execute_reply.started":"2021-11-29T13:45:50.156872Z","shell.execute_reply":"2021-11-29T13:46:17.967698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-11-29T13:46:17.970132Z","iopub.execute_input":"2021-11-29T13:46:17.970365Z","iopub.status.idle":"2021-11-29T13:46:18.00272Z","shell.execute_reply.started":"2021-11-29T13:46:17.970331Z","shell.execute_reply":"2021-11-29T13:46:18.001832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_df.isnull().sum())\n\n# we don't need to fill empty values","metadata":{"execution":{"iopub.status.busy":"2021-11-29T13:46:18.004048Z","iopub.execute_input":"2021-11-29T13:46:18.004386Z","iopub.status.idle":"2021-11-29T13:46:18.122514Z","shell.execute_reply.started":"2021-11-29T13:46:18.004346Z","shell.execute_reply":"2021-11-29T13:46:18.121796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's explore the data distribution for each features","metadata":{}},{"cell_type":"code","source":"def plot_extremum(dataframe):\n\n    list_x = dataframe.max().to_list()\n    list_y = dataframe.min().to_list()\n\n    desc = dataframe.columns\n\n    source  = ColumnDataSource(data = dict( x= list_x, y= list_y, desc = desc))\n\n    hover = HoverTool(tooltips = [\n        (\"(x,y)\" , \"(@x, @y)\"),\n        ('desc', '@desc')\n    ])\n\n    p = figure(width = 800, height = 800, tools = [hover], title = 'Exploring possible outilers for each features', toolbar_location = 'right')\n    p.circle( 'x', 'y', size = 15, alpha = 0.4 ,source = source, fill_color = 'navy')\n    \n    return p\n\np = plot_extremum(train_df)\n\nshow(p)","metadata":{"execution":{"iopub.status.busy":"2021-11-29T13:46:18.124582Z","iopub.execute_input":"2021-11-29T13:46:18.124999Z","iopub.status.idle":"2021-11-29T13:46:18.337371Z","shell.execute_reply.started":"2021-11-29T13:46:18.124961Z","shell.execute_reply":"2021-11-29T13:46:18.336743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It clearly appears that features : `f35`, `f2` and `f44` might have some outilers, let's expect them closely.\n\n\nFor more information, we built a scatter plot mapping (min,max) for each features, <br>\nEven though, we can't conclude anything with **ONLY** this plot <br>\nIt might help when handling a large number of features, to rapidly gather some information","metadata":{}},{"cell_type":"code","source":"f2_val = train_df['f2'].sample(10000).to_list()\nf35_val = train_df['f35'].sample(10000).to_list()\nf44_val = train_df['f44'].sample(10000).to_list()\n\nzeros = list(np.zeros(len(f2_val)))\n\nsource_1 = ColumnDataSource(data =dict(x = f2_val, y = zeros))    \n\n\nsource_2 = ColumnDataSource(data =dict(x = f35_val, y = zeros))  \n\nsource_3 = ColumnDataSource(data =dict(x = f44_val, y = zeros))  \n\n\n\n\np= figure(width = 500, height = 400, title = 'Distribution of f2 values', toolbar_location = 'right')\np1= figure(width = 500, height = 400, title = 'Distribution of f35 values', toolbar_location = 'right')\np2= figure(width = 500, height = 400, title = 'Distribution of f44 values', toolbar_location = 'right')\n\n\np.circle('x', 'y' , source = source_1, size =6,  fill_color = 'black', line_color = 'grey')\np1.circle('x', 'y' , source = source_2, size =6,  fill_color = 'black', line_color = 'grey')\np2.circle('x', 'y' , source = source_3, size =6,  fill_color = 'black', line_color = 'grey')\n\n\nshow(row([p,p1,p2]))","metadata":{"execution":{"iopub.status.busy":"2021-11-29T13:46:18.338664Z","iopub.execute_input":"2021-11-29T13:46:18.339068Z","iopub.status.idle":"2021-11-29T13:46:18.651729Z","shell.execute_reply.started":"2021-11-29T13:46:18.339031Z","shell.execute_reply":"2021-11-29T13:46:18.651114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally,  these three features are just not on the same scale as others, so we must scale them down. <br>\n\nHowever before doing so, we will analyse the other features","metadata":{}},{"cell_type":"code","source":"list_x = train_df.drop(columns = ['f2','f35', 'f44']).max().to_list()\nlist_y = train_df.drop(columns = ['f2','f35', 'f44']).min().to_list()\n\ndesc = train_df.drop(columns = ['f2','f35', 'f44']).columns\n\nsource  = ColumnDataSource(data = dict( x= list_x, y= list_y, desc = desc))\n\n\n\nhover = HoverTool(tooltips = [\n    (\"(x,y)\" , \"(@x, @y)\"),\n    ('desc', '@desc')\n])\n\n\np = figure(width = 600, height = 600, tools = [hover], title = 'Exploring possible outilers for each features', toolbar_location = 'right')\np.circle( 'x', 'y', size = 15, alpha = 0.4 ,source = source, fill_color = 'navy', line_color = 'white')\n\nshow(p)","metadata":{"execution":{"iopub.status.busy":"2021-11-29T13:46:18.652608Z","iopub.execute_input":"2021-11-29T13:46:18.652836Z","iopub.status.idle":"2021-11-29T13:46:19.274345Z","shell.execute_reply.started":"2021-11-29T13:46:18.652802Z","shell.execute_reply":"2021-11-29T13:46:19.273512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Most of our features contains values between `[-5,10]` <br>\nTo go further we could analyse each feature not in this range, to make sure they are not outliers <br>\n\nWe will skip this part here","metadata":{}},{"cell_type":"markdown","source":"## A close look at distributions\n\n\nLet's analyse the group of features between `[-5,10]` using violin plot! <br>\n- For visiblity we are going to plot only 10 features randomly choosen","metadata":{}},{"cell_type":"code","source":"def plot_violin(dataframe,n = 10):\n\n    select_feat = dataframe.max().loc[lambda val : val <= 10]\n\n    feat_index = select_feat.index.to_list()\n\n    rand_feat = random.sample(feat_index,n)\n\n    distrib_df = dataframe[rand_feat].sample(10000).melt(var_name ='Column', value_name = 'Raw')\n\n\n    plt.figure(figsize=(16,8))\n    sns.violinplot(x = 'Column', y = 'Raw', data = distrib_df)\n    \n    plt.show()\n    \nplot_violin(train_df)","metadata":{"execution":{"iopub.status.busy":"2021-11-29T13:46:19.27581Z","iopub.execute_input":"2021-11-29T13:46:19.27614Z","iopub.status.idle":"2021-11-29T13:46:20.083269Z","shell.execute_reply.started":"2021-11-29T13:46:19.276088Z","shell.execute_reply":"2021-11-29T13:46:20.082589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We cannot learn much with this graph, values are too spread, mostly because we have a lot of values, <br>\nSo having outliers is most likely to occur <br>\n\n\n\nAs expected in these kind of problems, we are going to normalise\n\n- *We could also standardize our data* ","metadata":{}},{"cell_type":"code","source":"means_df = train_df.mean()\n\nstd_df = train_df.std()\n\n\nnormalised_df = (train_df - means_df) /std_df","metadata":{"execution":{"iopub.status.busy":"2021-11-29T13:46:20.08464Z","iopub.execute_input":"2021-11-29T13:46:20.085099Z","iopub.status.idle":"2021-11-29T13:46:20.635321Z","shell.execute_reply.started":"2021-11-29T13:46:20.085061Z","shell.execute_reply":"2021-11-29T13:46:20.6345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's quickly re-run our previous visualisations ","metadata":{}},{"cell_type":"code","source":"p5 = plot_extremum(normalised_df)\n\n\nshow(p5)","metadata":{"execution":{"iopub.status.busy":"2021-11-29T13:46:20.636889Z","iopub.execute_input":"2021-11-29T13:46:20.637164Z","iopub.status.idle":"2021-11-29T13:46:20.887444Z","shell.execute_reply.started":"2021-11-29T13:46:20.637126Z","shell.execute_reply":"2021-11-29T13:46:20.886806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_violin(normalised_df,10)","metadata":{"execution":{"iopub.status.busy":"2021-11-29T13:46:20.889701Z","iopub.execute_input":"2021-11-29T13:46:20.890123Z","iopub.status.idle":"2021-11-29T13:46:21.639268Z","shell.execute_reply.started":"2021-11-29T13:46:20.890076Z","shell.execute_reply":"2021-11-29T13:46:21.638604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Even if several features have some huge outliers, let's keep it like this for the moment\n\n___\n\n## Building a simple Neural Net","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2021-11-29T14:03:12.030161Z","iopub.execute_input":"2021-11-29T14:03:12.030418Z","iopub.status.idle":"2021-11-29T14:03:12.157935Z","shell.execute_reply.started":"2021-11-29T14:03:12.030388Z","shell.execute_reply":"2021-11-29T14:03:12.157228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 256\n\nEPOCHS = 30\n\n\nNUMBER_FEATURE = 100","metadata":{"execution":{"iopub.status.busy":"2021-11-29T15:18:31.232568Z","iopub.execute_input":"2021-11-29T15:18:31.232846Z","iopub.status.idle":"2021-11-29T15:18:31.238475Z","shell.execute_reply.started":"2021-11-29T15:18:31.232813Z","shell.execute_reply":"2021-11-29T15:18:31.237619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train, x_val, y_train, y_val = train_test_split(np.asarray(normalised_df),\n                                                  np.asarray(train_target),\n                                                  test_size = 0.1)\n\n\ndef build_dataset(features_matrix, target):\n    \n    dataset = tf.data.Dataset.from_tensor_slices((features_matrix, target))\n    \n    dataset = dataset.shuffle(buffer_size = 1000).batch(BATCH_SIZE, drop_remainder = True).prefetch(1)\n    \n    return dataset\n\n\n\n\ntrain_dataset = build_dataset(x_train, y_train)\n\nval_dataset = build_dataset(x_val,y_val)","metadata":{"execution":{"iopub.status.busy":"2021-11-29T15:18:31.605853Z","iopub.execute_input":"2021-11-29T15:18:31.606373Z","iopub.status.idle":"2021-11-29T15:18:33.176535Z","shell.execute_reply.started":"2021-11-29T15:18:31.606331Z","shell.execute_reply":"2021-11-29T15:18:33.175799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1. We use `tf.data.dataset` to build our datasets, thus we can apply functions on them much faster\n2. the **`batch_size`** and **`buffer_size`**  are chosen arbitrarily since we are working on kaggle's notebook\n\n\n\n___\n\n\n- Regarding the model, *Dropout layers* are used to simply the model and thus reduce overfitting, \n- 4 hidden layers were used, be you can obtain more or less the same accuracy with only 3\n- The Dropout rate wasn't really tweaked here.\n- I didn't tried adam optimizer, you maybe can achieve better performances with\n- LROnPlateau is usually usefull the gain some performances, but to obtain even better result creating your own learning_rate schedule is the best solution","metadata":{}},{"cell_type":"code","source":"def build_model(train_dataset, val_dataset, input_size):\n    \n    model = tf.keras.Sequential([\n        keras.layers.InputLayer(input_shape = (input_size,)),\n        keras.layers.Dense(units = 128, activation = 'relu'),\n        keras.layers.Dropout(0.4),\n        keras.layers.Dense(units = 64, activation = 'relu'),\n        keras.layers.Dropout(0.4),\n        keras.layers.Dense(units = 32, activation = 'relu'),\n        keras.layers.Dropout(0.4),\n        keras.layers.Dense(units = 16, activation = 'relu'),\n        keras.layers.Dense(units = 1, activation = 'sigmoid')\n\n    ])\n    \n    \n    \n    model.compile(optimizer = keras.optimizers.SGD(learning_rate = 4e-3,momentum = 0.9, nesterov = True),\n             loss = 'binary_crossentropy',\n             metrics = ['accuracy'])\n\n    reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor ='val_loss', patience = 3, factor = 0.1, verbose = 1)\n\n    history = model.fit(train_dataset,\n                        batch_size = 256,\n                        epochs = EPOCHS,\n                        validation_data = val_dataset,\n                        callbacks = [reduce_lr])\n    \n    return history, model\n\n\n\nhistory, mymodel = build_model(train_dataset = train_dataset,\n                              val_dataset = val_dataset,\n                              input_size = NUMBER_FEATURE)","metadata":{"execution":{"iopub.status.busy":"2021-11-29T15:18:33.179788Z","iopub.execute_input":"2021-11-29T15:18:33.180329Z","iopub.status.idle":"2021-11-29T15:23:29.75729Z","shell.execute_reply.started":"2021-11-29T15:18:33.180288Z","shell.execute_reply":"2021-11-29T15:23:29.756562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_metrics(history,epochs):\n    \n    titles = ['Training loss', 'Validation loss', 'Training Accuracy','Validation Accuracy']\n    metrics = ['loss', 'val_loss', 'accuracy','val_accuracy']\n    palette = Inferno[4]\n    figures = []\n    \n    for k in range(4):\n        \n        p = figure( width = 600, height = 400, title = titles[k])\n        \n        p.line(np.arange(epochs), history.history[metrics[k]], line_width = 4, color = palette[k%2+1])   \n        \n        figures.append(p)\n    \n    show(grid([figures[:2], figures[2:]]))\n\nplot_metrics(history,EPOCHS)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-29T15:24:20.255221Z","iopub.execute_input":"2021-11-29T15:24:20.255487Z","iopub.status.idle":"2021-11-29T15:24:20.615778Z","shell.execute_reply.started":"2021-11-29T15:24:20.255456Z","shell.execute_reply":"2021-11-29T15:24:20.614981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- By increasing the batch_size, validation loss is much less volatile","metadata":{}},{"cell_type":"code","source":"p = figure( width = 600, height = 400, title = 'Learning Rate Evolution when using ReduceLROnPlateau')\n        \np.line(np.arange(EPOCHS), history.history['lr'], line_width = 4, color = Inferno[4][2])   \n\nshow(p)","metadata":{"execution":{"iopub.status.busy":"2021-11-29T15:48:51.66384Z","iopub.execute_input":"2021-11-29T15:48:51.664384Z","iopub.status.idle":"2021-11-29T15:48:51.943597Z","shell.execute_reply.started":"2021-11-29T15:48:51.664346Z","shell.execute_reply":"2021-11-29T15:48:51.942694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Correlation and feature importance\n\n---\n\n\nChecking features impact is always good, and might help to remove some useless features","metadata":{}},{"cell_type":"code","source":"normalised_df['target'] = train_target\n\n\ncorr_df = normalised_df.corr()\n","metadata":{"execution":{"iopub.status.busy":"2021-11-29T15:50:17.03843Z","iopub.execute_input":"2021-11-29T15:50:17.038685Z","iopub.status.idle":"2021-11-29T15:50:32.207241Z","shell.execute_reply.started":"2021-11-29T15:50:17.038653Z","shell.execute_reply":"2021-11-29T15:50:32.206387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ncorr_df.index.name = 'Features1'\ncorr_df.columns.name = 'Features2'\n\n\ncorr_matrix = pd.DataFrame(corr_df.stack(), columns=['correlation']).reset_index()\n\n\nmapper = LinearColorMapper(palette=Viridis256,\n                           low=corr_matrix['correlation'].min(),\n                           high=corr_matrix['correlation'].max())\n\n\n\nTOOLS = \"hover,save,pan,box_zoom,reset,wheel_zoom\"\n\np = figure(title=\"Correlation Matrix\",\n           x_range=corr_matrix['Features1'].drop_duplicates().to_list(),\n           y_range=corr_matrix['Features1'].drop_duplicates().to_list(),\n           x_axis_location=\"below\",\n           width=1200,\n           height=1200,\n           tools=TOOLS,\n           toolbar_location='left')\n\np.grid.grid_line_color = None\np.axis.axis_line_color = None\np.axis.major_tick_line_color = None\np.axis.major_label_text_font_size = \"7px\"\np.axis.major_label_standoff = 0\np.xaxis.major_label_orientation = np.pi / 3\n\np.rect(x='Features1', y=\"Features2\", width=1, height=1,\n       source=corr_matrix,\n       fill_color={'field': 'correlation', 'transform': mapper},\n       line_color=None)\n\ncolor_bar = ColorBar(color_mapper=mapper,\n                     major_label_text_font_size=\"7px\",\n                     ticker=BasicTicker(desired_num_ticks=256),\n                     border_line_color=None)\np.add_layout(color_bar, 'right')\n\nshow(p)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-29T15:50:32.208697Z","iopub.execute_input":"2021-11-29T15:50:32.208964Z","iopub.status.idle":"2021-11-29T15:50:32.568105Z","shell.execute_reply.started":"2021-11-29T15:50:32.208928Z","shell.execute_reply":"2021-11-29T15:50:32.567377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#removing target\nnormalised_df.drop(columns = ['target'],inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-11-29T15:25:07.681469Z","iopub.execute_input":"2021-11-29T15:25:07.681741Z","iopub.status.idle":"2021-11-29T15:25:07.826331Z","shell.execute_reply.started":"2021-11-29T15:25:07.681707Z","shell.execute_reply":"2021-11-29T15:25:07.82526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Apparently features are not correlated. We just need to find the most useful features, to simplify our model (not done yet)","metadata":{}},{"cell_type":"code","source":"normalised_test_df = (test_df - means_df) /std_df\n\n#we use means and standard deviation of training set, so the distributions are matching with the ones used during training\n\n\nids = normalised_test_df.index","metadata":{"execution":{"iopub.status.busy":"2021-11-29T15:51:37.742227Z","iopub.execute_input":"2021-11-29T15:51:37.742796Z","iopub.status.idle":"2021-11-29T15:51:37.974782Z","shell.execute_reply.started":"2021-11-29T15:51:37.742736Z","shell.execute_reply":"2021-11-29T15:51:37.974033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = mymodel.predict(np.asarray(normalised_test_df), \n               batch_size = 128,\n               verbose = 1)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-11-29T15:25:24.083835Z","iopub.execute_input":"2021-11-29T15:25:24.084527Z","iopub.status.idle":"2021-11-29T15:25:29.124528Z","shell.execute_reply.started":"2021-11-29T15:25:24.084488Z","shell.execute_reply":"2021-11-29T15:25:29.123865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df = pd.DataFrame(data = {'id' : ids, 'target' : predictions.round().reshape(-1,)}).set_index('id')\n\nsubmission_df.to_csv('submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-11-29T15:25:38.663198Z","iopub.execute_input":"2021-11-29T15:25:38.663684Z","iopub.status.idle":"2021-11-29T15:25:39.928666Z","shell.execute_reply.started":"2021-11-29T15:25:38.663646Z","shell.execute_reply":"2021-11-29T15:25:39.927795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}