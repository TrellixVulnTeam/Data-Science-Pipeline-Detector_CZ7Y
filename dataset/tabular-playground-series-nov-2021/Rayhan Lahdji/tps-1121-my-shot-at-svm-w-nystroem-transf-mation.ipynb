{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Prologue\n\nChallenge of using SVM with dataset this big is twofold:\n\n- long runtime\n- large memory consumption especially with kernels other than 'linear'.\n\nThis notebook is my shot at using SVM. I work around the above challenges by using Nystroem transformation and LinearSVC in scikit-learn.\n\n**Why/what Nystroem transformation?**\n\nKernel tricks usually require a large matrix of size $n \\times n$ to be made. When n is very large, like in this competition, the matrix is easily beyond the size of available memory.\n\nRather than using all data, Nystroem transformation approximates the kernel information using sample of the data. This way, it becomes possible to use kernel tricks without overflowing our memory when n is very large.\n\n**Why/what LinearSVC?**\n\nLinearSVC is a variant of SVC available in sklearn that performs quite well when data is large. However, it only uses linear i.e. no kernel trick. Doing Nystroem transformation before feeding data into LinearSVC is approximately the same as doing SVC with kernel tricks, but working around the abovementioned challenges.","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade scikit-learn scikit-learn-intelex --progress-bar off >> pip.log","metadata":{"execution":{"iopub.status.busy":"2021-11-13T07:33:58.99243Z","iopub.execute_input":"2021-11-13T07:33:58.993067Z","iopub.status.idle":"2021-11-13T07:34:33.155642Z","shell.execute_reply.started":"2021-11-13T07:33:58.992942Z","shell.execute_reply":"2021-11-13T07:34:33.154902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearnex import patch_sklearn\npatch_sklearn()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-13T07:34:33.157035Z","iopub.execute_input":"2021-11-13T07:34:33.157261Z","iopub.status.idle":"2021-11-13T07:34:34.721818Z","shell.execute_reply.started":"2021-11-13T07:34:33.157231Z","shell.execute_reply":"2021-11-13T07:34:34.720476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('../input/tabular-playground-series-nov-2021/train.csv')\ntest_df = pd.read_csv('../input/tabular-playground-series-nov-2021/test.csv')\nss = pd.read_csv('../input/tabular-playground-series-nov-2021/sample_submission.csv')\n\nX = train_df.drop(['target', 'id'], axis = 1).values\ny = train_df['target'].values\nX_test = test_df.drop('id', axis = 1).values\n\ndel train_df, test_df","metadata":{"execution":{"iopub.status.busy":"2021-11-13T07:34:34.723104Z","iopub.execute_input":"2021-11-13T07:34:34.723337Z","iopub.status.idle":"2021-11-13T07:35:00.832969Z","shell.execute_reply.started":"2021-11-13T07:34:34.723309Z","shell.execute_reply":"2021-11-13T07:35:00.832073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tune hyperparams and fit model\n\nI use Bayesian optimization as provided in package skopt rather than grid search because:\n1. Bayesian optimization can find better parameters with the same number of iterations, so it **saves time**.\n2. I only need to define sane possible ranges for the hyperparameters, which is **much easier** than pre-specifying candidate hyperparameters in grid search.","metadata":{}},{"cell_type":"code","source":"from warnings import simplefilter\nfrom sklearn.exceptions import ConvergenceWarning\nsimplefilter(\"ignore\", category=ConvergenceWarning)\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import LinearSVC\nfrom sklearn.kernel_approximation import Nystroem\nfrom skopt import BayesSearchCV, space, plots\n\nsvm_pipeline = make_pipeline(StandardScaler(),\n                             Nystroem(kernel = 'rbf', n_components = 1_000),\n                             LinearSVC(max_iter = 100))\n\nparams = {\n    \"linearsvc__C\": space.Real(1e-3, 1e3, prior = 'log-uniform'),\n    \"nystroem__gamma\": space.Real(1e-3, 1e3, prior = 'log-uniform')\n}\n\nbs = BayesSearchCV(svm_pipeline, params, n_iter = 50, cv = 3, scoring = 'roc_auc',\n                   verbose = 3, refit = False)\nbs.fit(X, y)\n\nsvm_pipeline.set_params(**bs.best_params_)\nsvm_pipeline.fit(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-11-13T07:35:00.834883Z","iopub.execute_input":"2021-11-13T07:35:00.835312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Variations in loss w.r.t hyperparameters\nplots.plot_objective(bs.optimizer_results_[0],\n                     dimensions=[\"linearsvc__C\", 'nystroem__gamma'],\n                     n_minimum_search=int(1e8))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Make SVM able to output predictions\n\nsklearn's SVC by default does not give probabilities. There are ways around this, the easiest being setting argument `probabilities = True`. But I choose to use CalibratedClassifier instead, as follows.","metadata":{}},{"cell_type":"code","source":"from sklearn.calibration import CalibratedClassifierCV\n\n# cv = 'prefit' because my model is already fitted and I don't want to\n# recalibrate using cross-validation\ncalib = CalibratedClassifierCV(svm_pipeline, method = 'isotonic', cv = 'prefit')\n\nsimple_submit = calib.fit(X, y).predict_proba(X_test)[:, 1]\n\nss['target'] = simple_submit\nss.to_csv('submission.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2021-11-13T06:27:07.397133Z","iopub.status.idle":"2021-11-13T06:27:07.397468Z","shell.execute_reply.started":"2021-11-13T06:27:07.39731Z","shell.execute_reply":"2021-11-13T06:27:07.397327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Epilogue\n\nThere you go! I hope you learn stuffs. Now you know how to workaround to use kernels other than linear in SVM with this competition's dataset. Enjoy!\n\nFeel free to fork and upvote. Keep learning and happy data-sciencing!","metadata":{}}]}