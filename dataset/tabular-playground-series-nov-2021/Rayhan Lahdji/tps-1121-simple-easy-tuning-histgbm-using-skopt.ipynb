{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\nIn this notebook, we will train and tune sklearn's HistGBM classifier using Bayesian optimization package skopt to find the best hyperparameters.\n\nFor beginners who has used GridSearchCV and RandomizedSearchCV all the time, skopt's BayesSearchCV provides a Bayesian hyperparameter optimization with very small necessary changes (since it's derived from the same sklearn base class).\n\nFeel free to upvote and fork if you feel like it. Enjoy!","metadata":{}},{"cell_type":"code","source":"!pip install -U scikit-learn --progress-bar off >> z_pip.log","metadata":{"execution":{"iopub.status.busy":"2021-11-23T18:22:46.426994Z","iopub.execute_input":"2021-11-23T18:22:46.427546Z","iopub.status.idle":"2021-11-23T18:23:03.883663Z","shell.execute_reply.started":"2021-11-23T18:22:46.427452Z","shell.execute_reply":"2021-11-23T18:23:03.882862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as pd\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-23T18:55:51.297843Z","iopub.execute_input":"2021-11-23T18:55:51.298595Z","iopub.status.idle":"2021-11-23T18:55:51.441963Z","shell.execute_reply.started":"2021-11-23T18:55:51.298543Z","shell.execute_reply":"2021-11-23T18:55:51.441168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('../input/tabular-playground-series-nov-2021/train.csv')\ntest_df = pd.read_csv('../input/tabular-playground-series-nov-2021/test.csv')\nss = pd.read_csv('../input/tabular-playground-series-nov-2021/sample_submission.csv')\n\nX = train_df.drop(['target', 'id'], axis = 1).values\ny = train_df['target'].values\nX_test = test_df.drop('id', axis = 1).values\n\ndel train_df, test_df","metadata":{"execution":{"iopub.status.busy":"2021-11-23T18:23:03.891254Z","iopub.execute_input":"2021-11-23T18:23:03.891468Z","iopub.status.idle":"2021-11-23T18:23:30.23096Z","shell.execute_reply.started":"2021-11-23T18:23:03.891439Z","shell.execute_reply":"2021-11-23T18:23:30.229939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Find best hyperparameter\n\nNote: for the purposes of demonstration, I will tune most of the hyperparameters, including turning off early stopping and instead trying to tune max_iter. Whether this is a good approach in practice, I'm not sure.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import HistGradientBoostingClassifier\nfrom skopt import BayesSearchCV, plots, space\n\n# Intentionally turn early stopping off, tune max_iter instead\nmodel = HistGradientBoostingClassifier(early_stopping = False)\n\n# Define parameter spaces using classes provided in skopt.space\nparams = {\n    'learning_rate': space.Real(1e-3, 1, prior = 'log-uniform'),\n    'max_iter': space.Integer(25, 1_000),\n    \n    'max_leaf_nodes': space.Integer(4, 64),\n    'max_depth': space.Integer(3, 15),\n    'min_samples_leaf': space.Integer(2, 60_000, prior = 'log-uniform'),\n    \n    'l2_regularization': space.Real(1e-3, 1e3, prior = 'log-uniform'),\n    'max_bins': space.Integer(31, 255)\n}\n\nbs = BayesSearchCV(model, params, n_iter = 100, cv = 3, scoring = 'roc_auc',\n                   refit = False)\n\n# Fit the search, i.e. begin finding the best hyperparameters\nbs.fit(X, y)\n\n# Set the best hyperparameters onto our model\nmodel.set_params(**bs.best_params_)","metadata":{"execution":{"iopub.status.busy":"2021-11-23T18:45:00.941106Z","iopub.execute_input":"2021-11-23T18:45:00.941744Z","iopub.status.idle":"2021-11-23T18:53:07.061589Z","shell.execute_reply.started":"2021-11-23T18:45:00.941693Z","shell.execute_reply":"2021-11-23T18:53:07.060269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualize\n\nThe following plot will tell us the variation of loss w.r.t. choices of hyperparameters. It also shows the existence of interaction (or lack thereof) between hyperparameters.","metadata":{}},{"cell_type":"code","source":"plots.plot_objective(bs.optimizer_results_[0],\n                     n_minimum_search=int(1e8))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-23T18:55:15.30446Z","iopub.execute_input":"2021-11-23T18:55:15.304806Z","iopub.status.idle":"2021-11-23T18:55:29.517652Z","shell.execute_reply.started":"2021-11-23T18:55:15.304767Z","shell.execute_reply":"2021-11-23T18:55:29.516376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fit and submit","metadata":{}},{"cell_type":"code","source":"submit = model.fit(X, y).predict_proba(X_test)[:, 1]\n\nss['target'] = submit\nss.to_csv('submission.csv', index = False)","metadata":{},"execution_count":null,"outputs":[]}]}