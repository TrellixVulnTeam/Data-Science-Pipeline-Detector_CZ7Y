{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# TPS - NOV 2021","metadata":{}},{"cell_type":"markdown","source":"The main objective of this notebook is to learn for myself. I'm implementing different techniques that I learned in the previous TPS. But I cannot guarantee a high scoring notebook! Read on if you like this might help a few.","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport cupy as cp\nimport pandas as pd\nimport cudf\nimport dask_cudf\n\nimport gc #to manage ram \nimport subprocess\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nfrom catboost import CatBoostClassifier\nimport optuna","metadata":{"execution":{"iopub.status.busy":"2021-11-06T03:26:23.77828Z","iopub.execute_input":"2021-11-06T03:26:23.778993Z","iopub.status.idle":"2021-11-06T03:26:25.457006Z","shell.execute_reply.started":"2021-11-06T03:26:23.778913Z","shell.execute_reply":"2021-11-06T03:26:25.456223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrain = pd.read_csv('../input/tabular-playground-series-nov-2021/train.csv')\ntest = pd.read_csv('../input/tabular-playground-series-nov-2021/test.csv')\n\nprint(train.shape)\nprint(test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-11-06T03:26:45.074174Z","iopub.execute_input":"2021-11-06T03:26:45.074819Z","iopub.status.idle":"2021-11-06T03:27:05.091947Z","shell.execute_reply.started":"2021-11-06T03:26:45.074772Z","shell.execute_reply":"2021-11-06T03:27:05.09002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.describe()","metadata":{"execution":{"iopub.status.busy":"2021-11-06T03:27:09.774063Z","iopub.execute_input":"2021-11-06T03:27:09.774577Z","iopub.status.idle":"2021-11-06T03:27:11.940066Z","shell.execute_reply.started":"2021-11-06T03:27:09.774535Z","shell.execute_reply":"2021-11-06T03:27:11.939372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.describe()","metadata":{"execution":{"iopub.status.busy":"2021-11-06T03:27:14.993831Z","iopub.execute_input":"2021-11-06T03:27:14.994366Z","iopub.status.idle":"2021-11-06T03:27:16.890948Z","shell.execute_reply.started":"2021-11-06T03:27:14.994325Z","shell.execute_reply":"2021-11-06T03:27:16.890244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-11-06T03:27:21.073805Z","iopub.execute_input":"2021-11-06T03:27:21.074364Z","iopub.status.idle":"2021-11-06T03:27:21.19795Z","shell.execute_reply.started":"2021-11-06T03:27:21.074324Z","shell.execute_reply":"2021-11-06T03:27:21.197245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-11-06T03:27:24.684504Z","iopub.execute_input":"2021-11-06T03:27:24.684882Z","iopub.status.idle":"2021-11-06T03:27:24.811798Z","shell.execute_reply.started":"2021-11-06T03:27:24.684833Z","shell.execute_reply":"2021-11-06T03:27:24.810975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Number of missing values in training data: {train.isna().sum().sum()}')\nprint(f'Number of missing values in testing data: {test.isna().sum().sum()}')","metadata":{"execution":{"iopub.status.busy":"2021-11-06T03:27:28.154715Z","iopub.execute_input":"2021-11-06T03:27:28.154985Z","iopub.status.idle":"2021-11-06T03:27:28.366681Z","shell.execute_reply.started":"2021-11-06T03:27:28.154956Z","shell.execute_reply":"2021-11-06T03:27:28.365903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"No missing Values! ","metadata":{}},{"cell_type":"code","source":"Features = [col for col in train.columns if col not in ['id', 'target']]","metadata":{"execution":{"iopub.status.busy":"2021-11-06T03:27:32.759165Z","iopub.execute_input":"2021-11-06T03:27:32.759701Z","iopub.status.idle":"2021-11-06T03:27:32.764896Z","shell.execute_reply.started":"2021-11-06T03:27:32.759655Z","shell.execute_reply":"2021-11-06T03:27:32.763668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By initial impressions the data seems to have no categorical features. Let's check if that is the case.","metadata":{}},{"cell_type":"code","source":"train.drop('id',axis=1,inplace=True)\ntest.drop('id',axis=1,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-06T03:29:09.599705Z","iopub.execute_input":"2021-11-06T03:29:09.600287Z","iopub.status.idle":"2021-11-06T03:29:09.87124Z","shell.execute_reply.started":"2021-11-06T03:29:09.600247Z","shell.execute_reply":"2021-11-06T03:29:09.870433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.concat([train[Features], test[Features]], axis=0)\n\ncat_features = [col for col in Features if df[col].nunique() < 25]\ncont_features = [col for col in Features if df[col].nunique() >= 25]\n\nprint(f'Total number of features: {len(Features)}')\nprint(f'Number of categorical features: {len(cat_features)}')\nprint(f'Number of continuos features: {len(cont_features)}')\n\nplt.pie([len(cat_features), len(cont_features)], \n        labels=['Categorical', 'Continuos'],\n        textprops={'fontsize': 13},\n        autopct='%1.1f%%')\nplt.show()\n\ndel df","metadata":{"execution":{"iopub.status.busy":"2021-11-06T03:27:39.41411Z","iopub.execute_input":"2021-11-06T03:27:39.414373Z","iopub.status.idle":"2021-11-06T03:27:51.151262Z","shell.execute_reply.started":"2021-11-06T03:27:39.414344Z","shell.execute_reply":"2021-11-06T03:27:51.150468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Yes! We don't have any categorical fearures.","metadata":{}},{"cell_type":"code","source":"print(train['target'].value_counts())\nsns.countplot(x = train['target'],data = train);","metadata":{"execution":{"iopub.status.busy":"2021-11-06T03:29:13.82415Z","iopub.execute_input":"2021-11-06T03:29:13.824523Z","iopub.status.idle":"2021-11-06T03:29:14.242468Z","shell.execute_reply.started":"2021-11-06T03:29:13.824486Z","shell.execute_reply":"2021-11-06T03:29:14.241625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Distribution of the target value is balanced :)","metadata":{}},{"cell_type":"code","source":"#creating a random temperory dataframe to get an idea of how the data is distributed \n#For plotting distributions\n\nnp.random.seed(2110)\ntmp_train = train.sample(10000)\ntmp_test = test.sample(10000)","metadata":{"execution":{"iopub.status.busy":"2021-11-06T03:29:18.204318Z","iopub.execute_input":"2021-11-06T03:29:18.204956Z","iopub.status.idle":"2021-11-06T03:29:18.263087Z","shell.execute_reply.started":"2021-11-06T03:29:18.204917Z","shell.execute_reply":"2021-11-06T03:29:18.2624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Feature distribution of features: \")\nncols = 5\nnrows = 20\n\nfig, axes = plt.subplots(nrows, ncols, figsize=(18, 50), facecolor='#EAEAF2')\n\nfor r in range(nrows):\n    for c in range(ncols):\n        col = Features[r*ncols+c]\n        sns.kdeplot(x=tmp_train[col], ax=axes[r, c], label='Train data')\n        sns.kdeplot(x=tmp_test[col], ax=axes[r, c], color=\"orange\", label='Test data')\n        axes[r, c].set_ylabel('')\n        axes[r, c].set_xlabel(col, fontsize=8, fontweight='bold')\n        axes[r, c].tick_params(labelsize=5, width=0.5)\n        axes[r, c].xaxis.offsetText.set_fontsize(4)\n        axes[r, c].yaxis.offsetText.set_fontsize(4)\nplt.show()\n\ndel tmp_train\ndel tmp_test\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-11-06T03:29:24.695288Z","iopub.execute_input":"2021-11-06T03:29:24.696129Z","iopub.status.idle":"2021-11-06T03:29:46.570177Z","shell.execute_reply.started":"2021-11-06T03:29:24.696077Z","shell.execute_reply":"2021-11-06T03:29:46.56954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns = 10\nrows = 10\nf=0\nfig, ax_array = plt.subplots(rows, columns, squeeze=False)\nfor i,ax_row in enumerate(ax_array):\n    for j,axes in enumerate(ax_row):\n        axes.set_title('f'+str(f))\n        axes.set_yticklabels([])\n        axes.set_xticklabels([])\n        col = 'f'+str(f)\n        sns.set(rc = {'figure.figsize':(30,20)})\n        g2 = sns.boxplot(train[col],ax=axes)\n        g2.set(ylabel=None)\n        g2.set(xticklabels=[])\n        g2.set(yticklabels=[])\n        f=f+1\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-06T03:31:02.785448Z","iopub.execute_input":"2021-11-06T03:31:02.78642Z","iopub.status.idle":"2021-11-06T03:31:19.646926Z","shell.execute_reply.started":"2021-11-06T03:31:02.786346Z","shell.execute_reply":"2021-11-06T03:31:19.64625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[\"mean\"] = train[Features].mean(axis=1)\ntrain[\"std\"] = train[Features].std(axis=1)\ntrain[\"min\"] = train[Features].min(axis=1)\ntrain[\"max\"] = train[Features].max(axis=1)\ntrain[\"sum\"] = train[Features].sum(axis=1)\n\ntest[\"mean\"] = test[Features].mean(axis=1)\ntest[\"std\"] = test[Features].std(axis=1)\ntest[\"min\"] = test[Features].min(axis=1)\ntest[\"max\"] = test[Features].max(axis=1)\ntest[\"sum\"] = test[Features].sum(axis=1)\n\nFeatures.extend(['mean', 'std', 'min', 'max', 'sum'])\n\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-11-06T03:31:41.174979Z","iopub.execute_input":"2021-11-06T03:31:41.175256Z","iopub.status.idle":"2021-11-06T03:31:47.318074Z","shell.execute_reply.started":"2021-11-06T03:31:41.175227Z","shell.execute_reply":"2021-11-06T03:31:47.317308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train.shape)\nprint(test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-11-06T03:31:51.353813Z","iopub.execute_input":"2021-11-06T03:31:51.354088Z","iopub.status.idle":"2021-11-06T03:31:51.358903Z","shell.execute_reply.started":"2021-11-06T03:31:51.35406Z","shell.execute_reply":"2021-11-06T03:31:51.358145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr = train[Features+['target']].corr()\n\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\nfig,ax=plt.subplots(figsize=(20,20))\nax.set_xticklabels(labels=corr.columns,fontsize=12)\nax.set_yticklabels(labels=corr.columns,fontsize=12)\nsns.heatmap(corr,mask=mask,cmap='tab20c',linewidth=0.1)\nplt.title('Correlation Map',color='blue',fontsize=12)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-06T03:31:55.803915Z","iopub.execute_input":"2021-11-06T03:31:55.804505Z","iopub.status.idle":"2021-11-06T03:32:14.49099Z","shell.execute_reply.started":"2021-11-06T03:31:55.804464Z","shell.execute_reply":"2021-11-06T03:32:14.490273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = train['target']\ntrain = train.drop(['target'], axis=1)\n\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-11-06T03:32:48.259745Z","iopub.execute_input":"2021-11-06T03:32:48.260067Z","iopub.status.idle":"2021-11-06T03:32:48.6619Z","shell.execute_reply.started":"2021-11-06T03:32:48.260032Z","shell.execute_reply":"2021-11-06T03:32:48.660977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Adding these features improved the score. We could add more features by performing clustering and PCA(next time, maybe).\n\nScaling is important!","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import RobustScaler\nscaler = RobustScaler()\ntrain[Features] = scaler.fit_transform(train[Features])\ntest[Features] = scaler.transform(test[Features])","metadata":{"execution":{"iopub.status.busy":"2021-11-06T03:32:52.519083Z","iopub.execute_input":"2021-11-06T03:32:52.519853Z","iopub.status.idle":"2021-11-06T03:32:56.093159Z","shell.execute_reply.started":"2021-11-06T03:32:52.51981Z","shell.execute_reply":"2021-11-06T03:32:56.092397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_params = {    \n    \"objective\": \"CrossEntropy\",\n    \"eval_metric\" : \"AUC\",\n    \"task_type\": \"GPU\",\n    \"grow_policy\": \"SymmetricTree\",\n    \"learning_rate\": 0.08,\n    \"n_estimators\":  10_000,\n    \"random_strength\" : 1.0,\n    \"max_bin\": 128,\n    \"l2_leaf_reg\": 0.002550319996478972,\n    \"max_depth\": 4,\n    \"min_data_in_leaf\": 193,\n    'verbose': 0\n}","metadata":{"execution":{"iopub.status.busy":"2021-11-06T03:33:00.889029Z","iopub.execute_input":"2021-11-06T03:33:00.889768Z","iopub.status.idle":"2021-11-06T03:33:00.897666Z","shell.execute_reply.started":"2021-11-06T03:33:00.889727Z","shell.execute_reply":"2021-11-06T03:33:00.896804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above hyper-parameters used in training are borrowed from the catboost model from the previous TPS(tuned using Optuna).","metadata":{}},{"cell_type":"markdown","source":"Let's train our Catboost model.","metadata":{}},{"cell_type":"code","source":"folds = KFold(n_splits = 5, random_state = 2021, shuffle = True)\n\npredictions = np.zeros(len(test))\n\nfor fold, (trn_idx, val_idx) in enumerate(folds.split(train)):\n    \n    print(f\"Fold: {fold}\")\n    \n    X_train, X_test = train.iloc[trn_idx], train.iloc[val_idx]\n    y_train, y_test = y.iloc[trn_idx], y.iloc[val_idx]\n\n    model = CatBoostClassifier(**cat_params)\n    \n    model.fit(X_train, y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=400, verbose=False)\n    #model.fit(X_train, y_train, eval_set=[(X_test, y_test)], eval_metric = 'auc', verbose = 500, early_stopping_rounds = 200)\n    \n    pred = model.predict_proba(X_test)[:,1]\n    roc = roc_auc_score(y_test, pred)\n    print(f\" roc_auc_score: {roc}\")\n    print(\"-\"*50)","metadata":{"execution":{"iopub.status.busy":"2021-11-06T03:34:26.902977Z","iopub.execute_input":"2021-11-06T03:34:26.903247Z","iopub.status.idle":"2021-11-06T03:39:22.126134Z","shell.execute_reply.started":"2021-11-06T03:34:26.903219Z","shell.execute_reply":"2021-11-06T03:39:22.125422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lightGBM didn't work \n\"\"\"import lightgbm as lgb\nlgb_params = {\n    'objective': 'binary',\n    'n_estimators': 20000,\n    'random_state': 42,\n    'learning_rate': 8e-3,\n    'subsample': 0.6,\n    'subsample_freq': 1,\n    'colsample_bytree': 0.4,\n    'reg_alpha': 10.0,\n    'reg_lambda': 1e-1,\n    'min_child_weight': 256,\n    'min_child_samples': 20,\n    'device': 'gpu',\n}used these parameters to get an initial score\nparams = {\n        'objective': 'binary',\n        'metric': 'AUC',\n        'boosting_type': 'dart', # To improve AUC\n        'lambda_l1': trial.suggest_loguniform('lambda_l1', 0.1, 0.9),\n        'lambda_l2': trial.suggest_loguniform('lambda_l2', 0.1, 0.9),\n        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),\n        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n        'device': 'gpu',\n        'gpu_platform_id': 0,\n        'gpu_device_id': 0,\n        'verbose': 0\n    }range for optuna to search\n\ndef objective(trial):\n    params = {\n        'objective': 'binary',\n        'metric': 'AUC',\n        'boosting_type': 'dart', # To improve AUC\n        'lambda_l1': trial.suggest_loguniform('lambda_l1', 0.1, 0.9),\n        'lambda_l2': trial.suggest_loguniform('lambda_l2', 0.1, 0.9),\n        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),\n        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n        'device': 'gpu',\n        'gpu_platform_id': 0,\n        'gpu_device_id': 0,\n        'verbose': 0\n    }\n    # Learning\n    gbm = LightGBM(params, lgb_train, lgb_valid, num_boost_round, verbose_eval, FEATS)\n    # Prediction\n    y_pred = gbm.predict(valid[FEATS], num_iteration=gbm.best_iteration)\n    accuracy = roc_auc_score(y_va, y_pred, labels='ROC curve', average='weighted')\n    print('ROC curve:', accuracy)\n    ROC_curve(y_va, y_pred)\n    # Finish\n    print(\"Operation completed.\")\n    # Output\n    return accuracy\"\"\"","metadata":{"_kg_hide-input":true,"_kg_hide-output":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using Optuna for hyperparameter tuning for the first time. I am attaching this article to get better insight into Optuna implementation.\nhttps://towardsdatascience.com/hyper-parameter-optimization-with-optuna-4920d5732edf. For Optuna we are required to define an Objective function with the loss function or evaluation metric to optimize. The below code is mostly reusable.","metadata":{}},{"cell_type":"code","source":"def fit_cat(trial, x_train, y_train, x_test, y_test):\n    \n    params = {'iterations':trial.suggest_int(\"iterations\", 1000, 100000),\n              'od_wait':trial.suggest_int('od_wait', 500, 5000),\n              'task_type':\"GPU\",\n              'learning_rate' : trial.suggest_uniform('learning_rate', 0.02 , 0.06),\n              'reg_lambda': trial.suggest_loguniform('reg_lambda', 0.30 , 0.33),\n              'subsample': trial.suggest_uniform('subsample',0.8,1.0),\n              'random_strength': trial.suggest_uniform('random_strength',10,50),\n              'depth': trial.suggest_int('depth',1,15),\n              'min_data_in_leaf': trial.suggest_int('min_data_in_leaf',1,50),\n              'leaf_estimation_iterations': trial.suggest_int('leaf_estimation_iterations',1,15),\n              'bootstrap_type':'Poisson'\n               }\n    \n    \n    model = CatBoostClassifier(**params)\n    model.fit(x_train, y_train,eval_set=[(x_test,y_test)], early_stopping_rounds=150, verbose=False)\n    \n    y_train_pred = model.predict_proba(x_train)[:,1]\n    \n    y_test_pred = model.predict_proba(x_test)[:,1]\n    y_train_pred = np.clip(y_train_pred, 0.1, None)\n    y_test_pred = np.clip(y_test_pred, 0.1, None)\n    \n    log = {\n        \"train roc_auc\": roc_auc_score(y_train, y_train_pred),\n        \"valid roc_auc\": roc_auc_score(y_test, y_test_pred)\n    }\n    \n    return model, log","metadata":{"execution":{"iopub.status.busy":"2021-11-06T04:15:08.596281Z","iopub.execute_input":"2021-11-06T04:15:08.596982Z","iopub.status.idle":"2021-11-06T04:15:08.606161Z","shell.execute_reply.started":"2021-11-06T04:15:08.596943Z","shell.execute_reply":"2021-11-06T04:15:08.605321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def objective(trial):\n    roc_auc = 0\n    x_train, x_test, y_train, y_test = train_test_split(train, y, test_size=0.30)\n    model, log = fit_cat(trial, x_train, y_train, x_test, y_test)\n    roc_auc += log['valid roc_auc']\n        \n    return roc_auc","metadata":{"execution":{"iopub.status.busy":"2021-11-06T04:15:04.911315Z","iopub.execute_input":"2021-11-06T04:15:04.912027Z","iopub.status.idle":"2021-11-06T04:15:04.917032Z","shell.execute_reply.started":"2021-11-06T04:15:04.911989Z","shell.execute_reply":"2021-11-06T04:15:04.916162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=10)\n\nprint(\"Number of completed trials: {}\".format(len(study.trials)))","metadata":{"execution":{"iopub.status.busy":"2021-11-06T04:15:13.021248Z","iopub.execute_input":"2021-11-06T04:15:13.021895Z","iopub.status.idle":"2021-11-06T04:27:05.684526Z","shell.execute_reply.started":"2021-11-06T04:15:13.021843Z","shell.execute_reply":"2021-11-06T04:27:05.681373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Best trial:\")\ntrial = study.best_trial\nprint(trial)","metadata":{"execution":{"iopub.status.busy":"2021-11-06T04:44:06.145215Z","iopub.execute_input":"2021-11-06T04:44:06.145496Z","iopub.status.idle":"2021-11-06T04:44:06.151849Z","shell.execute_reply.started":"2021-11-06T04:44:06.145465Z","shell.execute_reply":"2021-11-06T04:44:06.150844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As the trails are randomly initialised different runs result in different parameters but generally they are fairly close.","metadata":{}},{"cell_type":"code","source":"optuna.visualization.plot_param_importances(study)","metadata":{"execution":{"iopub.status.busy":"2021-11-06T04:44:06.153502Z","iopub.execute_input":"2021-11-06T04:44:06.154202Z","iopub.status.idle":"2021-11-06T04:44:06.574131Z","shell.execute_reply.started":"2021-11-06T04:44:06.154167Z","shell.execute_reply":"2021-11-06T04:44:06.573437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Depth is most important feature followed by learning rate and subsample respectively.","metadata":{}},{"cell_type":"code","source":"optuna.visualization.plot_optimization_history(study)","metadata":{"execution":{"iopub.status.busy":"2021-11-06T04:44:06.575738Z","iopub.execute_input":"2021-11-06T04:44:06.575927Z","iopub.status.idle":"2021-11-06T04:44:06.590958Z","shell.execute_reply.started":"2021-11-06T04:44:06.575902Z","shell.execute_reply":"2021-11-06T04:44:06.590126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optuna.visualization.plot_slice(study, params=['depth', 'learning_rate', 'subsample'])","metadata":{"execution":{"iopub.status.busy":"2021-11-06T04:44:06.59249Z","iopub.execute_input":"2021-11-06T04:44:06.593026Z","iopub.status.idle":"2021-11-06T04:44:06.659652Z","shell.execute_reply.started":"2021-11-06T04:44:06.592964Z","shell.execute_reply":"2021-11-06T04:44:06.658889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lots of plots at our disposal. We can see how score is changing with respect to each parameter. Try tuning a little bit more, it might improve training time if not the final score. There are many more graphs and we can get better search space and we can use pruning to improve training time.","metadata":{}},{"cell_type":"markdown","source":"Let's our model with new hyperparameters.","metadata":{}},{"cell_type":"code","source":"new_params = {\n     'iterations': 80203,\n     'od_wait': 1765,\n     'learning_rate': 0.02010888271017379,\n     'reg_lambda': 0.3051769003766273,\n     'subsample': 0.9155353016941578,\n     'random_strength': 31.905377503941313,\n     'depth': 6,\n     'min_data_in_leaf': 14,\n     'leaf_estimation_iterations': 7,\n     'task_type':\"GPU\",\n     'bootstrap_type':'Poisson',\n}","metadata":{"execution":{"iopub.status.busy":"2021-11-06T04:44:06.662086Z","iopub.execute_input":"2021-11-06T04:44:06.662348Z","iopub.status.idle":"2021-11-06T04:44:06.66699Z","shell.execute_reply.started":"2021-11-06T04:44:06.662316Z","shell.execute_reply":"2021-11-06T04:44:06.666245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"folds = KFold(n_splits = 5, random_state = 2021, shuffle = True)\n\npredictions = np.zeros(len(test))\n\nfor fold, (trn_idx, val_idx) in enumerate(folds.split(train)):\n    \n    print(f\"Fold: {fold}\")\n    \n    X_train, X_test = train.iloc[trn_idx], train.iloc[val_idx]\n    y_train, y_test = y.iloc[trn_idx], y.iloc[val_idx]\n\n    model = CatBoostClassifier(**new_params)\n   \n    model.fit(X_train, y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=400, verbose=False)\n    \n    pred = model.predict_proba(X_test)[:,1]\n    roc = roc_auc_score(y_test, pred)\n    print(f\" roc_auc_score: {roc}\")\n    print(\"-\"*50)\n    \n    predictions += model.predict_proba(test[Features])[:,1] / folds.n_splits ","metadata":{"execution":{"iopub.status.busy":"2021-11-06T04:44:06.668524Z","iopub.execute_input":"2021-11-06T04:44:06.669101Z","iopub.status.idle":"2021-11-06T04:58:44.611589Z","shell.execute_reply.started":"2021-11-06T04:44:06.669062Z","shell.execute_reply":"2021-11-06T04:58:44.610827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.read_csv('../input/tabular-playground-series-nov-2021/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-11-06T04:58:44.612968Z","iopub.execute_input":"2021-11-06T04:58:44.613233Z","iopub.status.idle":"2021-11-06T04:58:44.794256Z","shell.execute_reply.started":"2021-11-06T04:58:44.6132Z","shell.execute_reply":"2021-11-06T04:58:44.793455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub['target'] = predictions\nsub.to_csv('submission.csv', index = 0)\nsub","metadata":{"execution":{"iopub.status.busy":"2021-11-06T04:58:44.797678Z","iopub.execute_input":"2021-11-06T04:58:44.797883Z","iopub.status.idle":"2021-11-06T04:58:46.505086Z","shell.execute_reply.started":"2021-11-06T04:58:44.797858Z","shell.execute_reply":"2021-11-06T04:58:46.504352Z"},"trusted":true},"execution_count":null,"outputs":[]}]}