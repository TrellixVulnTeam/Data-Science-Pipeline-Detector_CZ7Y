{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-22T18:51:06.575398Z","iopub.execute_input":"2021-11-22T18:51:06.575681Z","iopub.status.idle":"2021-11-22T18:51:06.584755Z","shell.execute_reply.started":"2021-11-22T18:51:06.575644Z","shell.execute_reply":"2021-11-22T18:51:06.58381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# to read cvs faster (uses GPU)\nimport cudf\n\n# to split dataset\nfrom sklearn.model_selection import train_test_split\n# model\nfrom xgboost import XGBClassifier\n# to split dataset in folds for cross-validation preserving the percentage of samples for each class\nfrom sklearn.model_selection import StratifiedKFold\n# to perform a randomized search for cross-validation\nfrom sklearn.model_selection import RandomizedSearchCV\n# to calculate the score\nfrom sklearn.metrics import roc_auc_score\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\n\nfrom sklearn.feature_selection import mutual_info_classif\n\n# to perform a hyperparameter scan using Bayesian Optimization\nfrom skopt import BayesSearchCV\n# parameter ranges are specified by one of below\nfrom skopt.space import Real, Categorical, Integer\n\n# garbage collector: to free-up memory when needed\nimport gc\n\n# to keep track of time\nimport time\n\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import KernelPCA","metadata":{"execution":{"iopub.status.busy":"2021-11-22T18:51:06.586518Z","iopub.execute_input":"2021-11-22T18:51:06.587226Z","iopub.status.idle":"2021-11-22T18:51:06.595197Z","shell.execute_reply.started":"2021-11-22T18:51:06.587184Z","shell.execute_reply":"2021-11-22T18:51:06.594421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Loading data sets using cudf (faster) and coverts to pandas (DataFrame)\ntrain = cudf.read_csv('../input/tabular-playground-series-nov-2021/train.csv', index_col = 'id').to_pandas()\ntrain","metadata":{"execution":{"iopub.status.busy":"2021-11-22T18:53:16.736685Z","iopub.execute_input":"2021-11-22T18:53:16.737237Z","iopub.status.idle":"2021-11-22T18:53:25.434866Z","shell.execute_reply.started":"2021-11-22T18:53:16.7372Z","shell.execute_reply":"2021-11-22T18:53:25.434004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check how many entries are missing per column, how many unique entries per column\ntrain.isnull().sum().values, train.dtypes.values, train.nunique().values","metadata":{"execution":{"iopub.status.busy":"2021-11-22T18:24:26.114881Z","iopub.execute_input":"2021-11-22T18:24:26.115205Z","iopub.status.idle":"2021-11-22T18:24:29.282313Z","shell.execute_reply.started":"2021-11-22T18:24:26.115174Z","shell.execute_reply":"2021-11-22T18:24:29.28126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.min().values <= -5, train.max().values>=5","metadata":{"execution":{"iopub.status.busy":"2021-11-19T14:56:41.549905Z","iopub.execute_input":"2021-11-19T14:56:41.550444Z","iopub.status.idle":"2021-11-19T14:56:41.721035Z","shell.execute_reply.started":"2021-11-19T14:56:41.550406Z","shell.execute_reply":"2021-11-19T14:56:41.720233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# All features are continuous, and some seem to need standardisation\n# First let us separate X from y and divide the sets into search set - to perform a randomized search -\n# and opt set - to be fitted by the model with the best \"optimized hyperparameter set\" from the search.\nX = train.drop(columns='target')\ny = train.target\nX_search, X_opt, y_search, y_opt = train_test_split(X, y, train_size = 0.5, random_state=7)\n\n# since the datasets below won't be used here, we free up memory space by removing them\ndel train\ndel X\ndel y\ndel X_opt\ndel y_opt\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-11-22T18:53:30.053536Z","iopub.execute_input":"2021-11-22T18:53:30.054065Z","iopub.status.idle":"2021-11-22T18:53:30.835324Z","shell.execute_reply.started":"2021-11-22T18:53:30.054025Z","shell.execute_reply":"2021-11-22T18:53:30.834632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Mutual information... selecting the features with higher MI scores...\ndiscrete_features = X_search.dtypes == int\n\nmi_scores = pd.Series(mutual_info_classif(X_search, y_search, discrete_features=discrete_features), index=X_search.columns).sort_values(ascending=False)\n\nmi_cols=list(mi_scores[mi_scores.values>0.0001].index)\n\nX_search= X_search[mi_cols]\nX_search","metadata":{"execution":{"iopub.status.busy":"2021-11-22T18:53:35.795988Z","iopub.execute_input":"2021-11-22T18:53:35.796419Z","iopub.status.idle":"2021-11-22T19:00:13.500235Z","shell.execute_reply.started":"2021-11-22T18:53:35.796381Z","shell.execute_reply":"2021-11-22T19:00:13.499407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sc = StandardScaler()\n\nprep = ColumnTransformer([('sc', sc, X_search.columns)], remainder='passthrough')\n# Defining the model: 'gpu_hist' is important to run it faster with GPU\nmodel = XGBClassifier(tree_method='gpu_hist', use_label_encoder=False, eval_metric='auc', random_state=7)\n\npipe = Pipeline(steps=[('preprocessing', prep), ('model', model)])","metadata":{"execution":{"iopub.status.busy":"2021-11-22T19:00:40.014146Z","iopub.execute_input":"2021-11-22T19:00:40.014414Z","iopub.status.idle":"2021-11-22T19:00:40.021544Z","shell.execute_reply.started":"2021-11-22T19:00:40.014386Z","shell.execute_reply":"2021-11-22T19:00:40.01903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Benchmark, adapted from TPC Oct 21 (randomized search, no scaling)\n# Best: 0.740070 using {'subsample': 0.30000000000000004, 'n_estimators': 800, 'max_depth': 3, 'learning_rate': 0.07600000000000001}\n# Seconds to run the scan: 1208.465334\n\n\n# Let us vary thorough the XGBoost paramenters to see which setup gives the best result (score)\n\nstart = time.time()\n\n# define the hyperparameters and the ranges to perform the scan\n# params_rnd = {'n_estimators':np.arange(100, 1000, 100),'learning_rate':np.arange(0.01, 0.31, 0.01),\n#           'max_depth':np.arange(3, 12, 1), 'subsample':np.arange(0.1, 1, 0.1), 'colsample_bytree':np.arange(0.1, 1.1, 0.1),\n#          'colsample_bylevel':np.arange(0.1, 1.1, 0.1), 'min_child_weight':np.arange(0,10,1), 'reg_alpha':np.arange(0,15,1), \n#               'reg_lambda':np.arange(0,30,1)}\n\n\nparams_rnd = {'model__n_estimators':np.arange(300, 1000, 50),'model__learning_rate':np.arange(0.006, 0.21, 0.01),\n          'model__max_depth':np.arange(3, 12, 1), 'model__subsample':np.arange(0.1, 1, 0.1)}\n\n# for cross validation with 5 splits, using StratifiedKFold to keep the same percentage of sample per each class\nskfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=7)\n\n# defining the random scan, n_iter=7 (picks 7 random scenarios from params_rnd), using 'roc_auc' and the scoring method\nrnd_search = RandomizedSearchCV(pipe, params_rnd, n_iter=20, scoring='roc_auc', cv=skfold, random_state=7)\n\nX_search=X_search[mi_cols]\n# the model should fit the search set\nrnd_result = rnd_search.fit(X_search, y_search)\n\n# print the best score in the search and the corresponding best parameters\nprint(\"Best: %f using %s\" % (rnd_result.best_score_, rnd_result.best_params_))\n\n# print the total time...\nelapsed = time.time() - start\nprint(\"Seconds to run the scan: %f\" % (elapsed))\n\n# Search datasets won't be used anymore, so we remove them from memory\ndel X_search\ndel y_search\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-11-22T19:00:42.171115Z","iopub.execute_input":"2021-11-22T19:00:42.171372Z","iopub.status.idle":"2021-11-22T19:01:22.750015Z","shell.execute_reply.started":"2021-11-22T19:00:42.171344Z","shell.execute_reply":"2021-11-22T19:01:22.747691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# In order to use rnd_result.best_params_ in the next model, we need to remove \"model__\" from the keys\nbest_parameters_rnd = dict(rnd_result.best_params_.copy())\n\nfor k in best_parameters_rnd.keys():\n    best_parameters_rnd[k.replace(\"model__\",\"\")] = best_parameters_rnd.pop(k)\n\n# for some reason, the loop is not replacing two model__ instances, so we do it one by one\nbest_parameters_rnd['learning_rate'] = best_parameters_rnd.pop('model__learning_rate')\nbest_parameters_rnd['max_depth'] = best_parameters_rnd.pop('model__max_depth')\n\nbest_parameters_rnd","metadata":{"execution":{"iopub.status.busy":"2021-11-04T17:23:37.635619Z","iopub.status.idle":"2021-11-04T17:23:37.636341Z","shell.execute_reply.started":"2021-11-04T17:23:37.636082Z","shell.execute_reply":"2021-11-04T17:23:37.636108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# now, using the tuned hyperparameters, we fit and test the model on the \"opt\" set \n# Defining the model\nmodel_opt = XGBClassifier(**best_parameters_rnd, tree_method='gpu_hist', use_label_encoder=False, \n                          eval_metric='auc', random_state=7)\n\npipe_opt = Pipeline(steps=[('preprocessing', prep), ('model', model_opt)])\n\ntrain = cudf.read_csv('../input/tabular-playground-series-nov-2021/train.csv', index_col = 'id').to_pandas()\nX = train.drop(columns='target')\ny = train.target\nX_search, X_opt, y_search, y_opt = train_test_split(X, y, train_size = 0.3, random_state=7)\n\n# Search datasets won't be used anymore, so we remove them from memory\ndel X\ndel y\ndel X_search\ndel y_search\ngc.collect()\n\npipe_opt.fit(X_opt[mi_cols], y_opt)\n\ndel X_opt\ndel y_opt","metadata":{"execution":{"iopub.status.busy":"2021-11-04T17:23:37.637896Z","iopub.status.idle":"2021-11-04T17:23:37.638312Z","shell.execute_reply.started":"2021-11-04T17:23:37.638083Z","shell.execute_reply":"2021-11-04T17:23:37.638106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### We calculate and store the probability of the positive prediction\n\nX_test = cudf.read_csv('../input/tabular-playground-series-nov-2021/test.csv', index_col = 'id').to_pandas()\n\npred_test = pipe_opt.predict_proba(X_test[mi_cols])[:,1]\n\n\noutput = pd.DataFrame({'id': X_test.index,\n                       'target': pred_test})\noutput.to_csv('submission_TPSNov21.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-04T17:23:37.639774Z","iopub.status.idle":"2021-11-04T17:23:37.640185Z","shell.execute_reply.started":"2021-11-04T17:23:37.639963Z","shell.execute_reply":"2021-11-04T17:23:37.639986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Let us vary thorough the XGBoost paramenters to see which setup gives the best result (score)\n\n\n# # define the hyperparameters and the ranges to perform the scan\n# # search_spaces = {'model__n_estimators': Integer(60, 500),'model__learning_rate': Real(0.001, 0.2, 'log-uniform'), 'model__max_depth': Integer(2, 12), \n# #                  'model__subsample': Real(0.1, 1, 'log-uniform'), 'model__colsample_bytree': Real(0.1, 1, 'log-uniform'),\n# #                  'model__colsample_bylevel':Real(0.1, 1, 'log-uniform'), 'model__min_child_weight': Integer(0, 10), 'model__reg_alpha': Integer(0, 15), \n# #                  'model__reg_lambda': Integer(0, 50)}\n\n# search_spaces = {'model__n_estimators': Integer(400, 1200),'model__learning_rate': Real(0.006, 0.21, 'log-uniform'), 'model__max_depth': Integer(3, 12), \n#                  'model__subsample': Real(0.1, 1, 'log-uniform')}\n\n# # for cross validation with 5 splits, using StratifiedKFold to keep the same percentage of sample per each class\n# skfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=7)\n\n# # defining a Bayes scan, n_iter=50 (picks 50 scenarios), using 'accuracy' and the scoring method\n# search_bay = BayesSearchCV(pipe, search_spaces, n_iter=30, scoring='accuracy', cv=skfold, random_state=7)","metadata":{"execution":{"iopub.status.busy":"2021-11-04T17:23:37.641385Z","iopub.status.idle":"2021-11-04T17:23:37.642251Z","shell.execute_reply.started":"2021-11-04T17:23:37.642002Z","shell.execute_reply":"2021-11-04T17:23:37.642028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # start counting how long the fitting takes\n# start = time.time()\n# # fit with the whole dataset\n# result_bay = search_bay.fit(X_search, y_search)\n\n# # print the best score and parameters found during the scan\n# print(\"(Bayes) Best: %f using %s\" % (result_bay.best_score_, result_bay.best_params_))\n# elapsed = time.time() - start\n# print(\"Time to run the scan: %f\" % (elapsed))","metadata":{"execution":{"iopub.status.busy":"2021-11-04T17:23:37.643385Z","iopub.status.idle":"2021-11-04T17:23:37.644225Z","shell.execute_reply.started":"2021-11-04T17:23:37.643982Z","shell.execute_reply":"2021-11-04T17:23:37.644006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # In order to use b_result.best_params_ in the next model, we need to remove \"model__\" from the keys\n# best_parameters_bay = dict(result_bay.best_params_.copy())\n\n# for k in best_parameters_bay.keys():\n#     best_parameters_bay[k.replace(\"model__\",\"\")] = best_parameters_bay.pop(k)\n\n# # for some reason, the loop is not replacing two model__ instances, so we do it one by one\n# best_parameters_bay['n_estimators'] = best_parameters_bay.pop('model__n_estimators')\n# best_parameters_bay['subsample'] = best_parameters_bay.pop('model__subsample')\n\n# best_parameters_bay","metadata":{"execution":{"iopub.status.busy":"2021-11-04T17:23:37.645377Z","iopub.status.idle":"2021-11-04T17:23:37.646206Z","shell.execute_reply.started":"2021-11-04T17:23:37.645964Z","shell.execute_reply":"2021-11-04T17:23:37.645988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # now, we fit using the role train data and the best parameters in the scan\n\n# model_opt_B = XGBClassifier(**best_parameters_bay, eval_metric='error', use_label_encoder=False, tree_method='gpu_hist')\n\n# # Defining the pipeline with the same preprocessing as before, but with the tuned model\n# pipe_opt_B = Pipeline(steps=[('preprocessing', prep), ('model', model_opt_B)])\n\n# train = cudf.read_csv('../input/tabular-playground-series-nov-2021/train.csv', index_col = 'id').to_pandas()\n# X = train.drop(columns='target')\n# y = train.target\n# X_search, X_opt, y_search, y_opt = train_test_split(X, y, train_size = 0.5, random_state=7)\n\n# # Search datasets won't be used anymore, so we remove them from memory\n# del X\n# del y\n# del X_search\n# del y_search\n# gc.collect()\n\n\n# # Fitting the whole dataset\n# pipe_opt_B.fit(X_opt, y_opt)\n\n# ### We calculate and store the probability of the positive prediction\n# X_test = cudf.read_csv('../input/tabular-playground-series-nov-2021/test.csv', index_col = 'id').to_pandas()\n\n# pred_test_B = pipe_opt_B.predict_proba(X_test)[:,1]\n\n# outputB = pd.DataFrame({'id': X_test.index,\n#                        'target': pred_test_B})\n# outputB.to_csv('submission_TPSNov21_B.csv', index=False)\n\n\n# outputB","metadata":{"execution":{"iopub.status.busy":"2021-11-04T17:23:37.647364Z","iopub.status.idle":"2021-11-04T17:23:37.647988Z","shell.execute_reply.started":"2021-11-04T17:23:37.647749Z","shell.execute_reply":"2021-11-04T17:23:37.647773Z"},"trusted":true},"execution_count":null,"outputs":[]}]}