{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"reference model : https://www.kaggle.com/hadeux/tps-nov-xgboost-baseline","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\n\nimport seaborn as sns\nimport sys\nimport csv\nimport datetime\nimport operator\nimport joblib\nimport warnings","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-12T01:24:08.604999Z","iopub.execute_input":"2021-11-12T01:24:08.605702Z","iopub.status.idle":"2021-11-12T01:24:08.634524Z","shell.execute_reply.started":"2021-11-12T01:24:08.605543Z","shell.execute_reply":"2021-11-12T01:24:08.632048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import Data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"../input/tabular-playground-series-nov-2021/train.csv\")\ntest = pd.read_csv(\"../input/tabular-playground-series-nov-2021/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-11-12T01:28:28.983013Z","iopub.execute_input":"2021-11-12T01:28:28.983335Z","iopub.status.idle":"2021-11-12T01:28:49.561872Z","shell.execute_reply.started":"2021-11-12T01:28:28.983299Z","shell.execute_reply":"2021-11-12T01:28:49.560669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.isnull().sum()[train.isnull().sum() != 0]","metadata":{"execution":{"iopub.status.busy":"2021-11-12T01:29:07.222374Z","iopub.execute_input":"2021-11-12T01:29:07.222727Z","iopub.status.idle":"2021-11-12T01:29:07.474846Z","shell.execute_reply.started":"2021-11-12T01:29:07.22269Z","shell.execute_reply":"2021-11-12T01:29:07.473959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = train.drop(['id'], axis = 1)\ntest_df = test.drop(['id'], axis = 1)","metadata":{"execution":{"iopub.status.busy":"2021-11-12T01:29:08.434862Z","iopub.execute_input":"2021-11-12T01:29:08.435198Z","iopub.status.idle":"2021-11-12T01:29:08.85637Z","shell.execute_reply.started":"2021-11-12T01:29:08.435162Z","shell.execute_reply":"2021-11-12T01:29:08.855524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**XGBoost hyperparameter**","metadata":{}},{"cell_type":"markdown","source":"\n\n    booster='gbtree'  1. 트리,회귀(gblinear) 트리가 항상  \n                         Tree, a gblinear tree is always\n                      2. 더 좋은 성능을 내기 때문에 수정할 필요없다고한다. \n                         It is said that there is no need to modify it because it gives better performance.\n    \n    silent=True       1. running message output X.\n                      2. 모델이 적합되는 과정을 이해하기위해선 False으로한다.\n                      To understand how the model is fitted, set it to False.\n    \n    min_child_weight=10    1. 값이 높아지면 under-fitting 되는 경우가 있다. CV를 통해 튜닝되어야 한다.\n                              Higher values ​​may lead to under-fitting. It should be tuned via CV.\n    \n    max_depth=8      1. 트리의 최대 깊이를 정의함.\n                        Defines the maximum depth of the tree.\n    \n                     2. 루트에서 가장 긴 노드의 거리.\n                        Distance of longest node from root.\n                     3. 8이면 중요변수에서 결론까지 변수가 9개거친다.\n                        If the value is 8, 9 variables pass from the important variable to the conclusion.\n                     4. Typical Value is 3-10. \n    \n    gamma =0         1. 노드가 split 되기 위한 loss function의 값이 감소하는 최소값을 정의한다. \n                        gamma 값이 높아질 수록 알고리즘은 보수적으로 변하고, loss function의 정의에 \n                        따라 적정값이 달라지기때문에 반드시 튜닝.\n                        Defines the minimum value at which the value of the loss function for the node to split is decreased. As the gamma value increases, the algorithm changes conservatively, and the appropriate value changes according to the definition of the loss function, \n                        so it must be tunning.\n    \n    nthread =4       1. XGBoost를 실행하기 위한 병렬처리(쓰레드) 갯수. 'n_jobs' 를 사용해라.\n                        The number of parallel processing (threads) to execute XGBoost. Use 'n_jobs' .\n    \n    colsample_bytree=0.8    1. 트리를 생성할때 훈련 데이터에서 변수를 샘플링해주는 비율. 보통 0.6~0.9\n                               The rate at which variables are sampled from the training data when creating the tree. Usually 0.6-0.9\n    \n    colsample_bylevel=0.9   1. 트리의 레벨별로 훈련 데이터의 변수를 샘플링해주는 비율. 보통0.6~0.9\n                               The rate at which the variables in the training data are sampled for each level of the tree. Usually 0.6~0.9\n    \n    n_estimators =(int)     1. 부스트트리의 양 amount of boost tree\n                            2. 트리의 갯수. number of trees.\n     \n    objective = 'reg:linear','binary:logistic','multi:softmax','multi:softprob'\n                 1.  regression case 'reg',\n                 2.  In case of binary classification 'binary',\n                 3.  Multiple classification case 'multi',\n                 4.  When returning a classified class 'softmax',\n                 5.  When returning the probability of belonging to each class 'softprob'*\n    \n    random_state =   1. random number seed.\n                     2. like seed.\n\n","metadata":{}},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"markdown","source":"# XGBoost Modeling","metadata":{}},{"cell_type":"code","source":"# data segmentation\nX = train_df.drop('target', axis=1)\ny = train_df['target']\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=0) # train, valid 8:2 분할","metadata":{"execution":{"iopub.status.busy":"2021-11-12T01:28:00.108693Z","iopub.execute_input":"2021-11-12T01:28:00.109039Z","iopub.status.idle":"2021-11-12T01:28:00.215755Z","shell.execute_reply.started":"2021-11-12T01:28:00.109003Z","shell.execute_reply":"2021-11-12T01:28:00.214432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgboost_model= xgb.XGBClassifier(max_depth = 8,\n                                 learning_rate = 0.005,\n                                 n_estimators = 10000,\n                                 objective = 'binary:logistic',\n                                 tree_method = 'gpu_hist',\n                                 booster = 'gbtree',\n                                 gamma = 0.64,\n                                 max_delta_step = 3,\n                                 min_child_weight = 7,\n                                 subsample = 0.7,\n                                 colsample_bytree = 0.8,\n                                 n_jobs = -1\n                                 )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start = datetime.datetime.now()\nxgb = xgboost_model.fit(X_train,\n                       y_train,\n                       eval_set = [(X_train, y_train), (X_valid, y_valid)], \n                       eval_metric = 'auc',\n                       early_stopping_rounds = 15,\n                       verbose = True)\nend = datetime.datetime.now()\nend-start","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = xgboost_model.predict(X_valid)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import scale\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import cohen_kappa_score\nfrom collections import OrderedDict\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom scipy.stats import norm, skew, probplot\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom category_encoders.target_encoder import TargetEncoder\nfrom sklearn.model_selection import StratifiedKFold\n\ndef classifier_eval(y_valid , y_pred) :\n  print('정확도(accuracy_score) : ', accuracy_score(y_valid, y_pred))\n  print('정밀도(precision_score) : ', precision_score(y_valid, y_pred))\n  print('재현율(recall_score) : ', recall_score(y_valid, y_pred))\n  print('F1 : ', f1_score(y_valid, y_pred))\n  print('AUC : ', roc_auc_score(y_valid, y_pred))\n\nclassifier_eval(y_valid, y_pred)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = np.array([accuracy_score(y_valid, y_pred),\n              precision_score(y_valid, y_pred),\n              recall_score(y_valid, y_pred),\n              f1_score(y_valid, y_pred),\n              roc_auc_score(y_valid, y_pred)])\n\nx","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**result visualization**","metadata":{}},{"cell_type":"code","source":"label = ['accuracy', 'precision', 'recall_score', 'f1_score', 'roc_auc']\n\nindex = np.arange(len(label))\n\n\nplt.bar(index, x, width=0.5)\nplt.title('evaluation index', fontsize=20)\nplt.ylabel('%', fontsize=18)\nplt.xticks(index, label, fontsize=15,rotation=90)    # X축의 범위: [xmin, xmax]\nplt.ylim([0, 1])     # Y축의 범위: [ymin, ymax]\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Features importance**","metadata":{}},{"cell_type":"code","source":"fi_vals = xgb.get_booster().get_score(importance_type = 'weight')\nfi_dict = {X_train.columns[i]:float(fi_vals.get('f'+str(i),0.)) for i in range(len(X_train.columns))}\nfeature_importance_ = sorted(fi_dict.items(), key=operator.itemgetter(1), reverse=True)\nfeature_importance_result = OrderedDict(feature_importance_)\n\nimportance = pd.DataFrame(feature_importance_)\nimportance.columns = ['feature','weight']\nimportance.head(10)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"xgboost_prediction = xgboost_model.predict(test_df)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_XGBoost = pd.DataFrame({'id':test['id'], 'target':xgboost_prediction})\nsubmission_XGBoost.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_XGBoost.to_csv('./submission.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]}]}