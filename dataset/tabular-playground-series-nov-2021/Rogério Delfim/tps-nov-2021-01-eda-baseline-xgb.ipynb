{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1 div class='alert alert-success'><center> Ponto de partida (EDA, linha de base)</center></h1>\n\n![](https://storage.googleapis.com/kaggle-competitions/kaggle/26480/logos/header.png?t=2021-04-09-00-57-05)","metadata":{"execution":{"iopub.status.busy":"2021-11-01T02:06:42.97338Z","iopub.execute_input":"2021-11-01T02:06:42.97373Z","iopub.status.idle":"2021-11-01T02:06:43.001573Z","shell.execute_reply.started":"2021-11-01T02:06:42.973646Z","shell.execute_reply":"2021-11-01T02:06:43.000581Z"}}},{"cell_type":"markdown","source":"# Descrição de dados\n\nPara esta competição, você vai prever se um cliente fez uma reclamação sobre uma apólice de seguro. A verdade fundamental claimtem valor binário, mas uma previsão pode ser qualquer número de 0.0 para 1.0, representando a probabilidade de uma reclamação. Os recursos neste conjunto de dados foram tornados anônimos e podem conter valores ausentes.\narquivos\n\n- `train.csv`: os dados de treinamento com o alvo claimcoluna\n- `test.csv`: o conjunto de teste; você estará prevendo o claimpara cada linha neste arquivo\n- `sample_submission.csv`:  um arquivo de envio de amostra no formato correto","metadata":{}},{"cell_type":"markdown","source":"# <div class=\"alert alert-success\">  1. IMPORTAÇÕES </div> ","metadata":{"execution":{"iopub.status.busy":"2021-11-01T02:18:02.191576Z","iopub.execute_input":"2021-11-01T02:18:02.192194Z","iopub.status.idle":"2021-11-01T02:18:02.196577Z","shell.execute_reply.started":"2021-11-01T02:18:02.19214Z","shell.execute_reply":"2021-11-01T02:18:02.195702Z"}}},{"cell_type":"markdown","source":"## 1.1. Bibliotecas ","metadata":{}},{"cell_type":"code","source":"import warnings\nimport random\nimport os\nimport gc\nimport torch","metadata":{"execution":{"iopub.status.busy":"2021-11-01T19:59:54.815365Z","iopub.execute_input":"2021-11-01T19:59:54.815901Z","iopub.status.idle":"2021-11-01T19:59:56.204233Z","shell.execute_reply.started":"2021-11-01T19:59:54.815862Z","shell.execute_reply":"2021-11-01T19:59:56.20344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas            as pd\nimport numpy             as np\nimport matplotlib.pyplot as plt \nimport seaborn           as sns\nimport joblib            as jb\nimport scikitplot        as skplt","metadata":{"execution":{"iopub.status.busy":"2021-11-01T19:59:56.205825Z","iopub.execute_input":"2021-11-01T19:59:56.206073Z","iopub.status.idle":"2021-11-01T19:59:56.211031Z","shell.execute_reply.started":"2021-11-01T19:59:56.206036Z","shell.execute_reply":"2021-11-01T19:59:56.210269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split, KFold, GridSearchCV\nfrom sklearn.preprocessing   import StandardScaler, MinMaxScaler, RobustScaler, MaxAbsScaler, QuantileTransformer\nfrom sklearn.impute          import SimpleImputer\nfrom sklearn                 import metrics","metadata":{"execution":{"iopub.status.busy":"2021-11-01T19:59:57.183824Z","iopub.execute_input":"2021-11-01T19:59:57.184093Z","iopub.status.idle":"2021-11-01T19:59:57.188893Z","shell.execute_reply.started":"2021-11-01T19:59:57.184062Z","shell.execute_reply":"2021-11-01T19:59:57.188145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import xgboost               as xgb","metadata":{"execution":{"iopub.status.busy":"2021-11-01T19:59:57.926592Z","iopub.execute_input":"2021-11-01T19:59:57.926875Z","iopub.status.idle":"2021-11-01T19:59:57.931032Z","shell.execute_reply.started":"2021-11-01T19:59:57.926843Z","shell.execute_reply":"2021-11-01T19:59:57.929952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.2. Funções\nAqui centralizamos todas as funções desenvolvidas durante o projeto para melhor organização do código.","metadata":{"execution":{"iopub.status.busy":"2021-11-01T02:09:14.919493Z","iopub.execute_input":"2021-11-01T02:09:14.920212Z","iopub.status.idle":"2021-11-01T02:09:14.92466Z","shell.execute_reply.started":"2021-11-01T02:09:14.920173Z","shell.execute_reply":"2021-11-01T02:09:14.923656Z"}}},{"cell_type":"code","source":"def jupyter_setting():\n    \n    %matplotlib inline\n      \n    #os.environ[\"WANDB_SILENT\"] = \"true\" \n    #plt.style.use('bmh') \n    #plt.rcParams['figure.figsize'] = [20,15]\n    #plt.rcParams['font.size']      = 13\n     \n    pd.options.display.max_columns = None\n    #pd.set_option('display.expand_frame_repr', False)\n\n    warnings.filterwarnings(action='ignore')\n    warnings.simplefilter('ignore')\n    warnings.filterwarnings('ignore')\n    #warnings.filterwarnings(category=UserWarning)\n\n    warnings.filterwarnings('ignore', category=DeprecationWarning)\n    warnings.filterwarnings('ignore', category=FutureWarning)\n    warnings.filterwarnings('ignore', category=RuntimeWarning)\n    warnings.filterwarnings('ignore', category=UserWarning)\n    #warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n\n    pd.set_option('display.max_rows', 150)\n    pd.set_option('display.max_columns', 500)\n    pd.set_option('display.max_colwidth', None)\n\n    icecream = [\"#00008b\", \"#960018\",\"#008b00\", \"#00468b\", \"#8b4500\", \"#582c00\"]\n    #sns.palplot(sns.color_palette(icecream))\n    \n    return icecream\n\nicecream = jupyter_setting()\n\n# Colors\ndark_red = \"#b20710\"\nblack    = \"#221f1f\"\ngreen    = \"#009473\"\nmyred    = '#CD5C5C'\nmyblue   = '#6495ED'\nmygreen  = '#90EE90'\n\ncols= [myred, myblue,mygreen]","metadata":{"execution":{"iopub.status.busy":"2021-11-01T20:00:34.82855Z","iopub.execute_input":"2021-11-01T20:00:34.829375Z","iopub.status.idle":"2021-11-01T20:00:34.843285Z","shell.execute_reply.started":"2021-11-01T20:00:34.829333Z","shell.execute_reply":"2021-11-01T20:00:34.842398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"colors = [\"lightcoral\", \"sandybrown\", \"darkorange\", \"mediumseagreen\",\n          \"lightseagreen\", \"cornflowerblue\", \"mediumpurple\", \"palevioletred\",\n          \"lightskyblue\", \"sandybrown\", \"yellowgreen\", \"indianred\",\n          \"lightsteelblue\", \"mediumorchid\", \"deepskyblue\"]","metadata":{"execution":{"iopub.status.busy":"2021-11-01T20:00:34.844483Z","iopub.execute_input":"2021-11-01T20:00:34.844837Z","iopub.status.idle":"2021-11-01T20:00:34.856945Z","shell.execute_reply.started":"2021-11-01T20:00:34.844799Z","shell.execute_reply":"2021-11-01T20:00:34.856082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def missing_zero_values_table(df):\n        mis_val         = df.isnull().sum()\n        mis_val_percent = round(df.isnull().mean().mul(100), 2)\n        mz_table        = pd.concat([mis_val, mis_val_percent], axis=1)\n        mz_table        = mz_table.rename(columns = {df.index.name:'col_name', \n                                                     0 : 'Valores ausentes', \n                                                     1 : '% de valores totais'})\n        \n        mz_table['Tipo de dados'] = df.dtypes\n        mz_table                  = mz_table[mz_table.iloc[:,1] != 0 ]. \\\n                                     sort_values('% de valores totais', ascending=False)\n        \n        msg = \"Seu dataframe selecionado tem {} colunas e {} \" + \\\n              \"linhas. \\nExistem {} colunas com valores ausentes.\"\n            \n        print (msg.format(df.shape[1], df.shape[0], mz_table.shape[0]))\n        \n        return mz_table.reset_index()","metadata":{"execution":{"iopub.status.busy":"2021-11-01T20:00:34.858055Z","iopub.execute_input":"2021-11-01T20:00:34.860248Z","iopub.status.idle":"2021-11-01T20:00:34.869133Z","shell.execute_reply.started":"2021-11-01T20:00:34.860207Z","shell.execute_reply":"2021-11-01T20:00:34.868349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def describe(df):\n    var = df.columns\n\n    # Medidas de tendência central, média e mediana \n    ct1 = pd.DataFrame(df[var].apply(np.mean)).T\n    ct2 = pd.DataFrame(df[var].apply(np.median)).T\n\n    # Dispensão - str, min , max range skew, kurtosis\n    d1 = pd.DataFrame(df[var].apply(np.std)).T\n    d2 = pd.DataFrame(df[var].apply(min)).T\n    d3 = pd.DataFrame(df[var].apply(max)).T\n    d4 = pd.DataFrame(df[var].apply(lambda x: x.max() - x.min())).T\n    d5 = pd.DataFrame(df[var].apply(lambda x: x.skew())).T\n    d6 = pd.DataFrame(df[var].apply(lambda x: x.kurtosis())).T\n    d7 = pd.DataFrame(df[var].apply(lambda x: (3 *( np.mean(x) - np.median(x)) / np.std(x) ))).T\n\n    # concatenete \n    m = pd.concat([d2, d3, d4, ct1, ct2, d1, d5, d6, d7]).T.reset_index()\n    m.columns = ['attrobutes', 'min', 'max', 'range', 'mean', 'median', 'std','skew', 'kurtosis','coef_as']\n    \n    return m","metadata":{"execution":{"iopub.status.busy":"2021-11-01T20:00:34.870366Z","iopub.execute_input":"2021-11-01T20:00:34.872221Z","iopub.status.idle":"2021-11-01T20:00:34.883811Z","shell.execute_reply.started":"2021-11-01T20:00:34.87218Z","shell.execute_reply":"2021-11-01T20:00:34.882889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reduce_memory_usage(df, verbose=True):\n    \n    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n    start_mem = df.memory_usage().sum() / 1024 ** 2\n    \n    for col in df.columns:\n        \n        col_type = df[col].dtypes\n        \n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            \n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if (\n                    c_min > np.finfo(np.float16).min\n                    and c_max < np.finfo(np.float16).max\n                ):\n                    df[col] = df[col].astype(np.float16)\n                elif (\n                    c_min > np.finfo(np.float32).min\n                    and c_max < np.finfo(np.float32).max\n                ):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() / 1024 ** 2\n    if verbose:\n        print(\n            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n                end_mem, 100 * (start_mem - end_mem) / start_mem\n            )\n        )\n        \n    return df","metadata":{"execution":{"iopub.status.busy":"2021-11-01T20:00:34.886252Z","iopub.execute_input":"2021-11-01T20:00:34.886625Z","iopub.status.idle":"2021-11-01T20:00:34.901279Z","shell.execute_reply.started":"2021-11-01T20:00:34.886585Z","shell.execute_reply":"2021-11-01T20:00:34.900401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_roc_curve(fpr, tpr, label=None):\n    fig, ax = plt.subplots()\n    ax.plot(fpr, tpr, \"r-\", label=label)\n    ax.plot([0, 1], [0, 1], transform=ax.transAxes, ls=\"--\", c=\".3\")\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.0])\n    plt.rcParams['font.size'] = 12\n    plt.title('ROC curve for TPS 09')\n    plt.xlabel('False Positive Rate (1 - Specificity)')\n    plt.ylabel('True Positive Rate (Sensitivity)')\n    plt.legend(loc=\"lower right\")\n    plt.grid(True)","metadata":{"execution":{"iopub.status.busy":"2021-11-01T20:00:34.902339Z","iopub.execute_input":"2021-11-01T20:00:34.902612Z","iopub.status.idle":"2021-11-01T20:00:34.914019Z","shell.execute_reply.started":"2021-11-01T20:00:34.902573Z","shell.execute_reply":"2021-11-01T20:00:34.913153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def confusion_plot(matrix, labels = None, title = None):\n        \n    labels = labels if labels else ['Negative (0)', 'Positive (1)']    \n    \n    fig, ax = plt.subplots(nrows=1, ncols=1)\n    \n    sns.heatmap(data        = matrix, \n                cmap        = 'Blues', \n                annot       = True, \n                fmt         = 'd',\n                xticklabels = labels, \n                yticklabels = labels, \n                ax          = ax);\n    \n    ax.set_xlabel('\\n PREVISTO', fontsize=15)\n    ax.set_ylabel('REAL \\n', fontsize=15)\n    ax.set_title(title)\n    \n    plt.close();\n    \n    return fig;","metadata":{"execution":{"iopub.status.busy":"2021-11-01T20:00:34.91684Z","iopub.execute_input":"2021-11-01T20:00:34.917277Z","iopub.status.idle":"2021-11-01T20:00:34.925942Z","shell.execute_reply.started":"2021-11-01T20:00:34.917234Z","shell.execute_reply":"2021-11-01T20:00:34.925218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir img\n!mkdir Data\n!mkdir Data/pkl\n!mkdir Data/submission\n\n!mkdir model\n!mkdir model/preds\n!mkdir model/optuna\n\n!mkdir model/preds/test\n!mkdir model/preds/test/n1\n!mkdir model/preds/test/n2\n!mkdir model/preds/test/n3\n\n!mkdir model/preds/train\n!mkdir model/preds/train/n1\n!mkdir model/preds/train/n2\n!mkdir model/preds/train/n3\n!mkdir model/preds/param","metadata":{"execution":{"iopub.status.busy":"2021-11-01T20:00:05.542751Z","iopub.execute_input":"2021-11-01T20:00:05.543006Z","iopub.status.idle":"2021-11-01T20:00:16.36119Z","shell.execute_reply.started":"2021-11-01T20:00:05.542978Z","shell.execute_reply":"2021-11-01T20:00:16.360182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.4. Carregar Dados\nSão dois arquivos que vamos utilizar para análise e treinanmento dos modelos, e um arquivo para submissão na competição.\n\n- `train.csv`: arquivo com dados de treinamento;  \n- `test.csv`: arquivo que será utilizado para previsão; \n- `sample_submission.csv`: arquivo utlizado para envio das previsões.  \n","metadata":{}},{"cell_type":"code","source":"path = '../input/tabular-playground-series-nov-2021/'","metadata":{"execution":{"iopub.status.busy":"2021-11-01T20:00:16.363619Z","iopub.execute_input":"2021-11-01T20:00:16.36419Z","iopub.status.idle":"2021-11-01T20:00:16.369033Z","shell.execute_reply.started":"2021-11-01T20:00:16.364147Z","shell.execute_reply":"2021-11-01T20:00:16.368233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\ndf1_train     = pd.read_csv(path + 'train.csv')\ndf1_test      = pd.read_csv(path + 'test.csv')\ndf_submission = pd.read_csv(path + 'sample_submission.csv')\n\ndf1_train.shape, df1_test.shape, df_submission.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-01T20:00:16.370887Z","iopub.execute_input":"2021-11-01T20:00:16.37181Z","iopub.status.idle":"2021-11-01T20:00:34.826496Z","shell.execute_reply.started":"2021-11-01T20:00:16.371743Z","shell.execute_reply":"2021-11-01T20:00:34.825758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-01T19:58:41.659944Z","iopub.execute_input":"2021-11-01T19:58:41.660232Z","iopub.status.idle":"2021-11-01T19:58:41.744809Z","shell.execute_reply.started":"2021-11-01T19:58:41.6602Z","shell.execute_reply":"2021-11-01T19:58:41.744088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1_test.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-01T19:58:42.849968Z","iopub.execute_input":"2021-11-01T19:58:42.850558Z","iopub.status.idle":"2021-11-01T19:58:42.929179Z","shell.execute_reply.started":"2021-11-01T19:58:42.850516Z","shell.execute_reply":"2021-11-01T19:58:42.928376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <div class=\"alert alert-success\">  1.0. Análise Exploratória de Dados (EDA)  </div> ","metadata":{}},{"cell_type":"markdown","source":"### 1.1.2. Dimensão do DataSet","metadata":{}},{"cell_type":"code","source":"print('TREINO')\nprint('Number of Rows: {}'.format(df1_train.shape[0]))\nprint('Number of Columns: {}'.format(df1_train.shape[1]), end='\\n\\n')\n\nprint('TESTE')\nprint('Number of Rows: {}'.format(df1_test.shape[0]))\nprint('Number of Columns: {}'.format(df1_test.shape[1]))","metadata":{"execution":{"iopub.status.busy":"2021-11-01T19:58:47.872429Z","iopub.execute_input":"2021-11-01T19:58:47.872698Z","iopub.status.idle":"2021-11-01T19:58:47.880204Z","shell.execute_reply.started":"2021-11-01T19:58:47.872668Z","shell.execute_reply":"2021-11-01T19:58:47.879065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.1.3. Tipo de Dados","metadata":{}},{"cell_type":"code","source":"df1_train.info()","metadata":{"execution":{"iopub.status.busy":"2021-11-01T02:19:30.641593Z","iopub.execute_input":"2021-11-01T02:19:30.641859Z","iopub.status.idle":"2021-11-01T02:19:30.665426Z","shell.execute_reply.started":"2021-11-01T02:19:30.641829Z","shell.execute_reply":"2021-11-01T02:19:30.664504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1_test.info()","metadata":{"execution":{"iopub.status.busy":"2021-11-01T02:20:34.957714Z","iopub.execute_input":"2021-11-01T02:20:34.95801Z","iopub.status.idle":"2021-11-01T02:20:34.972032Z","shell.execute_reply.started":"2021-11-01T02:20:34.957978Z","shell.execute_reply":"2021-11-01T02:20:34.971338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'{3*\"=\"} For Pandas {10*\"=\"}\\n{(df1_train.dtypes).value_counts()}')\nprint(f'\\n{3*\"=\"} For Datatable {7*\"=\"}\\n{(df1_test.dtypes).value_counts()}')","metadata":{"execution":{"iopub.status.busy":"2021-11-01T02:25:06.073257Z","iopub.execute_input":"2021-11-01T02:25:06.074106Z","iopub.status.idle":"2021-11-01T02:25:06.082079Z","shell.execute_reply.started":"2021-11-01T02:25:06.074058Z","shell.execute_reply":"2021-11-01T02:25:06.081112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-info\" role=\"alert\"> \n \n**`NOTA:`** <BR>\n    \n- O dataset de treiro tem 466.9 MB com 600000 de registros e 102 columas; \n- O dataset de teste tem 416.1 com 540000 de registros e 101 columas\n   \n    \nVamos fazer uma redução desses dataset nas próximas etapas, primeiro vamos identificar os tipos de dados que temos nos datasets.\n\n</div>","metadata":{"execution":{"iopub.status.busy":"2021-11-01T02:24:43.082088Z","iopub.execute_input":"2021-11-01T02:24:43.082669Z","iopub.status.idle":"2021-11-01T02:24:43.090804Z","shell.execute_reply.started":"2021-11-01T02:24:43.082628Z","shell.execute_reply":"2021-11-01T02:24:43.08939Z"}}},{"cell_type":"markdown","source":"### 1.1.4. Idenficar Variáveis Ausentes (NA)\nVamos verificar os valores ausentes em cada variável conjunto de treinono e teste.","metadata":{}},{"cell_type":"code","source":"missing = missing_zero_values_table(df1_train)\nmissing[:].style.background_gradient(cmap='Reds')","metadata":{"execution":{"iopub.status.busy":"2021-11-01T02:26:37.158064Z","iopub.execute_input":"2021-11-01T02:26:37.158564Z","iopub.status.idle":"2021-11-01T02:26:37.412122Z","shell.execute_reply.started":"2021-11-01T02:26:37.158523Z","shell.execute_reply":"2021-11-01T02:26:37.411415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing = missing_zero_values_table(df1_test)\nmissing[:].style.background_gradient(cmap='Reds')","metadata":{"execution":{"iopub.status.busy":"2021-11-01T02:26:48.543705Z","iopub.execute_input":"2021-11-01T02:26:48.543984Z","iopub.status.idle":"2021-11-01T02:26:48.731626Z","shell.execute_reply.started":"2021-11-01T02:26:48.543951Z","shell.execute_reply":"2021-11-01T02:26:48.73083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-info\" role=\"alert\"> \n    \n**`NOTA:`** <br>\n\nNão temos dados faltantes.\n    \n</div>","metadata":{"execution":{"iopub.status.busy":"2021-11-01T02:27:03.167605Z","iopub.execute_input":"2021-11-01T02:27:03.167891Z","iopub.status.idle":"2021-11-01T02:27:03.173405Z","shell.execute_reply.started":"2021-11-01T02:27:03.167858Z","shell.execute_reply":"2021-11-01T02:27:03.172205Z"}}},{"cell_type":"markdown","source":"### 1.1.6. Estatística Descritiva\nAbaixo estão as estatísticas básicas para cada variável que contém informações sobre contagem, média, desvio padrão, mínimo, 1º quartil, mediana, 3º quartil e máximo.","metadata":{}},{"cell_type":"code","source":"feature_cat   = df1_test.select_dtypes(object).columns.to_list()\nfeature_float = df1_test.select_dtypes(np.number).columns.to_list()","metadata":{"execution":{"iopub.status.busy":"2021-11-01T02:28:28.351928Z","iopub.execute_input":"2021-11-01T02:28:28.352209Z","iopub.status.idle":"2021-11-01T02:28:28.483879Z","shell.execute_reply.started":"2021-11-01T02:28:28.352158Z","shell.execute_reply":"2021-11-01T02:28:28.482881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Temos {} variávies numéricas e {} categóricas.'.format(len(feature_float), len(feature_cat)))","metadata":{"execution":{"iopub.status.busy":"2021-11-01T02:28:45.2798Z","iopub.execute_input":"2021-11-01T02:28:45.280067Z","iopub.status.idle":"2021-11-01T02:28:45.285068Z","shell.execute_reply.started":"2021-11-01T02:28:45.280036Z","shell.execute_reply":"2021-11-01T02:28:45.284069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 1.1.6.1. Atributos Numéricos","metadata":{}},{"cell_type":"markdown","source":"- Train","metadata":{"execution":{"iopub.status.busy":"2021-11-01T02:29:11.974443Z","iopub.execute_input":"2021-11-01T02:29:11.97472Z","iopub.status.idle":"2021-11-01T02:29:11.994878Z","shell.execute_reply.started":"2021-11-01T02:29:11.974691Z","shell.execute_reply":"2021-11-01T02:29:11.994065Z"}}},{"cell_type":"code","source":"df1_train[feature_float].describe().style.background_gradient(cmap='YlOrRd')","metadata":{"execution":{"iopub.status.busy":"2021-11-01T02:29:26.598785Z","iopub.execute_input":"2021-11-01T02:29:26.599057Z","iopub.status.idle":"2021-11-01T02:29:28.940658Z","shell.execute_reply.started":"2021-11-01T02:29:26.599027Z","shell.execute_reply":"2021-11-01T02:29:28.939868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Test","metadata":{}},{"cell_type":"code","source":"df1_test[feature_float].describe().style.background_gradient(cmap='YlOrRd')","metadata":{"execution":{"iopub.status.busy":"2021-11-01T02:33:01.312855Z","iopub.execute_input":"2021-11-01T02:33:01.313504Z","iopub.status.idle":"2021-11-01T02:33:03.449679Z","shell.execute_reply.started":"2021-11-01T02:33:01.313457Z","shell.execute_reply":"2021-11-01T02:33:03.449038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 1.1.6.1. Atributos Categóricos","metadata":{}},{"cell_type":"code","source":"df1_train[feature_cat].columns","metadata":{"execution":{"iopub.status.busy":"2021-11-01T02:33:35.066803Z","iopub.execute_input":"2021-11-01T02:33:35.067094Z","iopub.status.idle":"2021-11-01T02:33:35.076371Z","shell.execute_reply.started":"2021-11-01T02:33:35.067062Z","shell.execute_reply":"2021-11-01T02:33:35.075454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.2. Análise Gráfica","metadata":{"execution":{"iopub.status.busy":"2021-11-01T02:33:51.11361Z","iopub.execute_input":"2021-11-01T02:33:51.113876Z","iopub.status.idle":"2021-11-01T02:33:51.117614Z","shell.execute_reply.started":"2021-11-01T02:33:51.113846Z","shell.execute_reply":"2021-11-01T02:33:51.116815Z"}}},{"cell_type":"markdown","source":"### 1.2.1. Correlação\nVamos examinar a correlação entre as variáveis.","metadata":{"execution":{"iopub.status.busy":"2021-11-01T02:34:06.896733Z","iopub.execute_input":"2021-11-01T02:34:06.89702Z","iopub.status.idle":"2021-11-01T02:34:06.902831Z","shell.execute_reply.started":"2021-11-01T02:34:06.896987Z","shell.execute_reply":"2021-11-01T02:34:06.901596Z"}}},{"cell_type":"code","source":"df = df1_train[feature_float].corr().round(5)\n\n# Máscara para ocultar a parte superior direita do gráfico, pois é uma duplicata\nmask = np.zeros_like(df)\nmask[np.triu_indices_from(mask)] = True\n\n# Making a plot\nplt.figure(figsize=(16,16))\nax = sns.heatmap(df, annot=False, mask=mask, cmap=\"RdBu\", annot_kws={\"weight\": \"bold\", \"fontsize\":13})\n\nax.set_title(\"Mapa de calor de correlação das variável\", fontsize=17)\n\nplt.setp(ax.get_xticklabels(), \n         rotation      = 90, \n         ha            = \"right\",\n         rotation_mode = \"anchor\", \n         weight        = \"normal\")\n\nplt.setp(ax.get_yticklabels(), \n         weight        = \"normal\",\n         rotation_mode = \"anchor\", \n         rotation      = 0, \n         ha            = \"right\");","metadata":{"execution":{"iopub.status.busy":"2021-11-01T02:34:29.877605Z","iopub.execute_input":"2021-11-01T02:34:29.878023Z","iopub.status.idle":"2021-11-01T02:34:45.930965Z","shell.execute_reply.started":"2021-11-01T02:34:29.877979Z","shell.execute_reply":"2021-11-01T02:34:45.930313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-info\" role=\"alert\"> \n \n**`NOTA:`** <br>\n    \nComo podemos observar, a correlação está entre -0.075 e 0.1, o que é muito pequeno, portanto, as variáveis são fracamente correlacionados.\n    \n</div>","metadata":{}},{"cell_type":"markdown","source":"### 1.2.2. Distribuição","metadata":{"execution":{"iopub.status.busy":"2021-11-01T02:37:42.801088Z","iopub.execute_input":"2021-11-01T02:37:42.801785Z","iopub.status.idle":"2021-11-01T02:37:43.298923Z","shell.execute_reply.started":"2021-11-01T02:37:42.801742Z","shell.execute_reply":"2021-11-01T02:37:43.298137Z"}}},{"cell_type":"markdown","source":"#### 1.2.2.1. Train / Test","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(5, 5))\n\npie = ax.pie([len(df1_train), len(df1_test)],\n             labels   = [\"Train dataset\", \"Test dataset\"],\n             colors   = [\"salmon\", \"teal\"],\n             textprops= {\"fontsize\": 15},\n             autopct  = '%1.1f%%')\n\nax.axis(\"equal\")\nax.set_title(\"Comparação de comprimento do conjunto de dados \\n\", fontsize=18)\nfig.set_facecolor('white')\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-11-01T02:38:24.843258Z","iopub.execute_input":"2021-11-01T02:38:24.844039Z","iopub.status.idle":"2021-11-01T02:38:24.944822Z","shell.execute_reply.started":"2021-11-01T02:38:24.843999Z","shell.execute_reply":"2021-11-01T02:38:24.944022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 1.2.2.2. Proporção das variáveis","metadata":{"execution":{"iopub.status.busy":"2021-11-01T02:38:47.457613Z","iopub.execute_input":"2021-11-01T02:38:47.458293Z","iopub.status.idle":"2021-11-01T02:38:47.462419Z","shell.execute_reply.started":"2021-11-01T02:38:47.458258Z","shell.execute_reply":"2021-11-01T02:38:47.461616Z"}}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(5, 5))\n\nplt.pie([len(feature_cat), len(feature_float)], \n        labels=['Categorical', 'Continuos'],\n        textprops={'fontsize': 13},\n        autopct='%1.1f%%')\n\n#ax.axis(\"equal\")\nax.set_title(\"Comparação variáveis continuas/categóricas \\n Dataset Treino/Teste\", fontsize=18)\nfig.set_facecolor('white')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-01T02:38:59.998002Z","iopub.execute_input":"2021-11-01T02:38:59.998283Z","iopub.status.idle":"2021-11-01T02:39:00.268838Z","shell.execute_reply.started":"2021-11-01T02:38:59.998251Z","shell.execute_reply":"2021-11-01T02:39:00.268019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 1.2.2.1. Target\nVamos ver as ocorrências de números individuais do conjunto de dados de treino.","metadata":{}},{"cell_type":"code","source":"%%time \n\ncol = [(1,51), (51,100),  ]\n       \nfor x in col:\n       \n    L    = len(df1_train[feature_float].columns[x[0]:x[1]])\n    nrow = int(np.ceil(L/6))\n    ncol = 6\n    i    = 1\n\n    remove_last = (nrow * ncol) - L\n    fig, ax     = plt.subplots(nrow, ncol,figsize=(24, 30))\n    \n    fig.subplots_adjust(top=0.95)\n    \n    for feature in df1_train[feature_float].columns[x[0]:x[1]]:\n\n        plt.subplot(nrow, ncol, i)\n\n        ax = sns.kdeplot(df1_train[feature], shade=True, color='salmon',  alpha=0.5, label='train')\n        ax = sns.kdeplot(df1_test[feature], shade=True, color='teal',  alpha=0.5, label='test')\n        plt.xlabel(feature, fontsize=9)\n        plt.legend()\n\n        i += 1\n        \n        gc.collect()\n    \n    plt.suptitle('DistPlot: train & test data de {} à {}'.format(x[0],x[1]), fontsize=20)\n    plt.show()\n    \n    gc.collect()\n    \ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-11-01T02:40:00.909012Z","iopub.execute_input":"2021-11-01T02:40:00.90956Z","iopub.status.idle":"2021-11-01T02:48:21.727472Z","shell.execute_reply.started":"2021-11-01T02:40:00.90952Z","shell.execute_reply":"2021-11-01T02:48:21.726813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-info\" role=\"alert\"> \n \n**`NOTA:`** <br>\n\n- Os conjuntos de treinamento e teste têm aproximadamente as mesma distribuição em termos de variáveis; <br>\n- Temos poucas variáveis com distribuição normal; <br>\n- A maioria das variáveis tem distribuições distorcidas. <br>\n\n</div>","metadata":{}},{"cell_type":"markdown","source":"#### 1.2.2.1. Detecção de Outlier","metadata":{}},{"cell_type":"markdown","source":"##### 1.2.2.1.1. Data Train ","metadata":{}},{"cell_type":"code","source":"col = [(1,21), (21,41), (41,61), (61,81), (81,101)]\n\ndf_plot = ((df1_train[feature_float] - df1_train[feature_float].min())/\n           (df1_train[feature_float].max() - df1_train[feature_float].min()))\n\nfig, ax = plt.subplots(len(col), 1, figsize=(25,30))\n\nfor i, (x) in enumerate(col): \n    sns.boxplot(data = df_plot.iloc[:, x[0]:x[1] ], ax = ax[i]);\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-11-01T02:50:44.284386Z","iopub.execute_input":"2021-11-01T02:50:44.284748Z","iopub.status.idle":"2021-11-01T02:51:00.953708Z","shell.execute_reply.started":"2021-11-01T02:50:44.284712Z","shell.execute_reply":"2021-11-01T02:51:00.9517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 1.2.2.1.2. Data Test","metadata":{}},{"cell_type":"code","source":"col = [(1,21), (21,41), (41,61), (61,81), (81,101)]\n\ndf_plot = ((df1_test[feature_float] - df1_test[feature_float].min())/\n           (df1_test[feature_float].max() - df1_test[feature_float].min()))\n\nfig, ax = plt.subplots(len(col), 1, figsize=(25,30))\n\nfor i, (x) in enumerate(col): \n    sns.boxplot(data = df_plot.iloc[:, x[0]:x[1] ], ax = ax[i]);\n    gc.collect()    \n    ","metadata":{"execution":{"iopub.status.busy":"2021-11-01T02:50:29.358756Z","iopub.execute_input":"2021-11-01T02:50:29.359038Z","iopub.status.idle":"2021-11-01T02:50:44.282834Z","shell.execute_reply.started":"2021-11-01T02:50:29.359007Z","shell.execute_reply":"2021-11-01T02:50:44.282148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-info\" role=\"alert\"> \n \n**`NOTA:`** <br>\n    \nAcima observamos que temos muitos outliers em ambos conjunto de dados, na etapa de processamento vamos fazer o tramento ou remoção dos outliers para ajudar na previsão dos modelos. \n\n</div>","metadata":{}},{"cell_type":"markdown","source":"#### 1.2.2.3. Target\nA variável alvo tem os valores 0 e 1, vamos verificar a distribuição da variável `target` que é o nosso alvo de previsão.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(8, 6))\nax = sns.countplot(x=df1_train['target'], palette='viridis')\nax.set_title('Distribuição da variável Target', fontsize=20, y=1.05)\n\nsns.despine(right=True)\nsns.despine(offset=10, trim=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-01T02:51:48.043405Z","iopub.execute_input":"2021-11-01T02:51:48.044131Z","iopub.status.idle":"2021-11-01T02:51:48.255807Z","shell.execute_reply.started":"2021-11-01T02:51:48.04408Z","shell.execute_reply":"2021-11-01T02:51:48.255034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-info\" role=\"alert\"> \n \n**`NOTA:`** <br>\n    \nPodemos observar no gráfico acima que não temos desbalanceamento nos dados, temos mais dados na classe possitiva. \n    \n</div>","metadata":{"execution":{"iopub.status.busy":"2021-11-01T02:52:56.508604Z","iopub.execute_input":"2021-11-01T02:52:56.509536Z","iopub.status.idle":"2021-11-01T02:52:56.515424Z","shell.execute_reply.started":"2021-11-01T02:52:56.509486Z","shell.execute_reply":"2021-11-01T02:52:56.51443Z"}}},{"cell_type":"markdown","source":"#### 1.2.2.4. Variáveis preditoras  vs Target.","metadata":{}},{"cell_type":"code","source":"%%time \n\ncol = [(1,21), (21,41), (41,61), (61,81), (81,100) ]\n       \nfor x in col:\n       \n    L    = len(df1_train[feature_float].columns[x[0]:x[1]])\n    nrow = int(np.ceil(L/6))\n    ncol = 6\n    i    = 1\n\n    remove_last = (nrow * ncol) - L\n    fig, ax     = plt.subplots(nrow, ncol,figsize=(30, 20))\n    \n    fig.subplots_adjust(top=0.95)\n    \n    for feature in df1_train[feature_float].columns[x[0]:x[1]]:\n\n        plt.subplot(nrow, ncol, i)\n\n        ax = sns.kdeplot(df1_train[feature], \n                     shade    = True, \n                     palette  = 'viridis',  \n                     alpha    = 0.5, \n                     hue      = df1_train['target'], \n                     multiple = \"stack\")\n        \n        plt.xlabel(feature, fontsize=9)\n       \n        i += 1\n        \n        gc.collect()\n    \n    plt.suptitle('DistPlot: Variável de treino vs target {} à {}'.format(x[0],x[1]), fontsize=20)\n    \n    plt.show()\n       \n    gc.collect()\n    \ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-11-01T03:06:51.73273Z","iopub.execute_input":"2021-11-01T03:06:51.733553Z","iopub.status.idle":"2021-11-01T03:11:53.129103Z","shell.execute_reply.started":"2021-11-01T03:06:51.733505Z","shell.execute_reply":"2021-11-01T03:11:53.128239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <div class=\"alert alert-success\">  2. Split Train/Test </div> <br>\n\nAntes de fazer a divisão dos dados dataset, vamos fazer a redução dos mesmo com a utlização de uma função que modifica os tipos de variáveis dos dataset, ao fazermos isso ganhamos espaço.  ","metadata":{}},{"cell_type":"code","source":"%%time\ndf1_train = reduce_memory_usage(df1_train)\ndf1_test  = reduce_memory_usage(df1_test)","metadata":{"execution":{"iopub.status.busy":"2021-11-01T20:01:03.749738Z","iopub.execute_input":"2021-11-01T20:01:03.750365Z","iopub.status.idle":"2021-11-01T20:01:17.747651Z","shell.execute_reply.started":"2021-11-01T20:01:03.750324Z","shell.execute_reply":"2021-11-01T20:01:17.746895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#jb.dump(df1_train, path + 'pkl/df1_nb_01_train.pkl.z')\n#jb.dump(df1_test,  path + 'pkl/df1_nb_01_test.pkl.z')","metadata":{"execution":{"iopub.status.busy":"2021-11-01T03:03:52.937359Z","iopub.execute_input":"2021-11-01T03:03:52.937914Z","iopub.status.idle":"2021-11-01T03:03:52.941654Z","shell.execute_reply.started":"2021-11-01T03:03:52.937875Z","shell.execute_reply":"2021-11-01T03:03:52.940729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-info\" role=\"alert\"> \n \n**`NOTA:`** <br>\n    \nTivemos uma redução em ambos datasets de 74.9%, com isso o dataset de treino passou de 466.9 MB para 117.3 Mb e no dataset de teste passou de 416.1 MB para 105.06 Mb.\n    \n</div>","metadata":{}},{"cell_type":"code","source":"gc.collect()\nX      = df1_train.drop(['target', 'id'], axis=1)\ny      = df1_train['target']\nX_test = df1_test.drop(['id'], axis=1)\ncols   = X_test.columns\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, \n                                                      test_size    = 0.2,\n                                                      shuffle      = True, \n                                                      stratify     = y,\n                                                      random_state = 12359)\n\ndel df1_train,df1_test\n\nX_train.shape, y_train.shape, X_valid.shape, y_valid.shape , X_test.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-01T20:01:24.537887Z","iopub.execute_input":"2021-11-01T20:01:24.538177Z","iopub.status.idle":"2021-11-01T20:01:25.95173Z","shell.execute_reply.started":"2021-11-01T20:01:24.538144Z","shell.execute_reply":"2021-11-01T20:01:25.951008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-01T20:01:27.128292Z","iopub.execute_input":"2021-11-01T20:01:27.128739Z","iopub.status.idle":"2021-11-01T20:01:27.210526Z","shell.execute_reply.started":"2021-11-01T20:01:27.128703Z","shell.execute_reply":"2021-11-01T20:01:27.209651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <div class=\"alert alert-success\">  2. Modelo Baseline XGB </div> <br>\n\nNesta etapa do processo, vamos utilizar o **XGBClassifier** como linha de base, em relação ao tratamento dos dados vamos fazer apenas o scaler neste momento.","metadata":{"execution":{"iopub.status.busy":"2021-11-01T03:04:12.223447Z","iopub.execute_input":"2021-11-01T03:04:12.223724Z","iopub.status.idle":"2021-11-01T03:04:12.228778Z","shell.execute_reply.started":"2021-11-01T03:04:12.223692Z","shell.execute_reply":"2021-11-01T03:04:12.227738Z"}}},{"cell_type":"code","source":"seed   = 12359\nparams = {'objective'     : 'binary:logistic',    \n          'eval_metric'   : 'auc',\n          'random_state'  : seed}\n\nif torch.cuda.is_available():           \n    params.update({'predictor'  : 'gpu_predictor', \n                   'tree_method': 'gpu_hist', \n                   'gpu_id'     :  0})\n\nparams","metadata":{"execution":{"iopub.status.busy":"2021-11-01T20:20:48.40792Z","iopub.execute_input":"2021-11-01T20:20:48.408212Z","iopub.status.idle":"2021-11-01T20:20:48.417924Z","shell.execute_reply.started":"2021-11-01T20:20:48.408179Z","shell.execute_reply":"2021-11-01T20:20:48.414459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time \n\nmodel_baseline = xgb.XGBClassifier(**params)\n\nscalers = [None, \n           StandardScaler(), \n           RobustScaler(), \n           MinMaxScaler(), \n           MaxAbsScaler(), \n           QuantileTransformer(output_distribution='normal', random_state=0)]\n\nfor scaler in scalers: \n    \n    if scaler!=None:\n        X_train_s = scaler.fit_transform(X_train)\n        X_valid_s = scaler.fit_transform(X_valid)\n    else:\n        X_train_s = X_train\n        X_valid_s = X_valid\n                \n    model_baseline.fit(X_train_s, y_train, verbose = False)\n    y_hat = model_baseline.predict_proba(X_valid_s)[:, 1]    \n    auc   = metrics.roc_auc_score(y_valid, y_hat) \n    \n    print('Validaçao AUC: {:2.5f} => {}'.format(auc, scaler))\n\n    gc.collect()\n\nprint()   ","metadata":{"execution":{"iopub.status.busy":"2021-11-01T03:18:03.455002Z","iopub.execute_input":"2021-11-01T03:18:03.455874Z","iopub.status.idle":"2021-11-01T03:18:58.13908Z","shell.execute_reply.started":"2021-11-01T03:18:03.455835Z","shell.execute_reply":"2021-11-01T03:18:58.138321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-info\" role=\"alert\"> \n \n**`NOTA:`** <br>\n    \nCom scaler QuantileTransformer obtivemos uma AUC de 0.73057, como estamos fazer apenas uma validação simples, neste caso a pontuação do score pode ser afetada por aleatoriedade dos dados, sendo assim, vamos fazer uma validação cruzada para termos uma estimativa robusta.  <br>\n\nPara o treinamento do modelo foi criado uma função,que tem a finalidade de treinar um conjunto de scalers para um determinado modelo, durante o treinamento serão exibidos os resultados e no final será retornado o melhor modelo.\n    \n</div>","metadata":{}},{"cell_type":"code","source":"path='Data/'\ndef cross_val_model(model, X_train_, y_train_, X_test_,  scalers, name_model, \n                    FOLDS=5, verbose=False, seed=12359, use_ntree_limit=False): \n    \n    mdl_train   = []\n    feature_imp = 0 \n    auc_best    = 0\n    \n    for scaler in scalers: \n        \n        gc.collect()\n\n        df_submission.claim = 0           \n        feature_imp_best    = 0       \n        auc                 = []\n        lloss               = []\n        f1                  = []\n        ntree               = []\n        n_estimators        = model.get_params()['n_estimators'] \n        kfold               = KFold(n_splits=FOLDS, random_state=seed, shuffle=True)\n\n        if scaler!=None:\n            X_ts = scaler.fit_transform(X_test_.copy())\n        else:\n            X_ts = X_test_.copy()\n\n        print('='*80)\n        print('Scaler: {} - n_estimators: {}'.format(scaler,n_estimators))\n        print('='*80)\n\n        for i, (train_idx, test_idx) in enumerate(kfold.split(X_train_)):\n\n            i+=1\n\n            X_tr, y_tr = X_train_.iloc[train_idx], y_train_.iloc[train_idx]\n            X_vl, y_vl = X_train_.iloc[test_idx], y_train_.iloc[test_idx]\n\n            # Scaler\n            if scaler!=None:    \n                X_tr = scaler.fit_transform(X_tr)\n                X_vl = scaler.fit_transform(X_vl)                \n\n            model.fit(X_tr, y_tr, \n                      eval_set              = [(X_tr,y_tr), (X_vl,y_vl)],\n                      early_stopping_rounds = int(n_estimators*.1), \n                      verbose               = verbose\n                     )\n            \n            if use_ntree_limit:\n                y_hat_prob  = model.predict_proba(X_vl, ntree_limit=model.best_ntree_limit)[:, 1] # \n                best_ntree_ = model.best_ntree_limit\n            else: \n                y_hat_prob  = model.predict_proba(X_vl)[:, 1] # \n                best_ntree_ = n_estimators\n                            \n            y_hat         = (y_hat_prob >.5).astype(int) \n            log_loss_     = metrics.log_loss(y_vl, y_hat_prob)                \n            f1_score_     = metrics.f1_score(y_vl, y_hat)                    \n            auc_          = metrics.roc_auc_score(y_vl, y_hat_prob)\n\n            stop = '*' if n_estimators > best_ntree_ else ' '\n            msg  = '[Fold {}] AUC: {:.5f} - F1: {:.5f} - L. LOSS: {:.5f} {} {}'\n            print(msg.format(i, auc_, f1_score_,log_loss_, stop, best_ntree_))\n\n            # Getting mean feature importances (i.e. devided by number of splits)\n            feature_imp  += model.feature_importances_ / FOLDS\n            \n            df_submission['target'] += model.predict_proba(X_ts)[:, 1] / FOLDS\n\n            f1.append(f1_score_)\n            lloss.append(log_loss_)\n            auc.append(auc_)\n            ntree.append(best_ntree_)\n            \n            gc.collect()\n                        \n        auc_mean   = np.mean(auc)\n        auc_std    = np.std(auc)\n        lloss_mean = np.mean(lloss)\n        f1_mean    = np.mean(f1)\n        ntree_mean = np.mean(ntree)\n        \n        if auc_mean > auc_best: \n            auc_best          = auc_mean\n            f1_best           = f1_mean\n            lloss_best        = lloss_mean\n            model_best        = model\n            feature_imp_best  = feature_imp\n            scaler_best       = scaler\n                                    \n        print('-'*80)\n        msg = '[Mean Fold] AUC: {:.5f}(Std:{:.5f}) - F1: {:.5f} - L. LOSS: {:.5f} - {} '\n        print(msg.format(auc_mean,auc_std, f1_mean, lloss_mean, ntree_mean))\n        print('='*80)\n        print('')\n\n        # Gerar o arquivo de submissão \n        name_file_sub = 'submission/' + name_model + '_' + str(scaler).lower()[:4] + '.csv'\n        df_submission.to_csv(path + name_file_sub.format(auc_mean), index = False)\n\n        gc.collect()\n     \n    mdl_name_best = 'model/' + name_model.format(auc_mean)\n    \n    jb.dump(model_best, mdl_name_best)\n    \n    print()\n    print('='*80)\n    print('Scaler Best: {}'.format(scaler_best))\n    print('AUC        : {:2.5f}'.format(auc_best))\n    print('F1-Score   : {:2.5f}'.format(f1_best))\n    print('L. Loss    : {:2.5f}'.format(lloss_best))\n    print('='*80)\n    print()\n            \n    gc.collect()  \n    \n    return model_best","metadata":{"execution":{"iopub.status.busy":"2021-11-01T20:20:52.538212Z","iopub.execute_input":"2021-11-01T20:20:52.538545Z","iopub.status.idle":"2021-11-01T20:20:52.559982Z","shell.execute_reply.started":"2021-11-01T20:20:52.538497Z","shell.execute_reply":"2021-11-01T20:20:52.559123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\ngc.collect()\n\nparams.update({'n_estimators': 100})\n\nscalers = [None, \n           StandardScaler(), \n           RobustScaler(), \n           MinMaxScaler(), \n           MaxAbsScaler(), \n           QuantileTransformer(output_distribution='normal', random_state=0)]\n\nmodel_best = cross_val_model(model                = xgb.XGBClassifier(**params), \n                             X_train_             = X, \n                             y_train_             = y,\n                             X_test_              = X_test,                                            \n                             scalers              = scalers, \n                             name_model           = 'xgb_001_bl_{:2.5f}', \n                             FOLDS                = 5, \n                             seed                 = seed, \n                             use_ntree_limit      = False\n                             ) \n\ngc.collect()\nprint(params)","metadata":{"execution":{"iopub.status.busy":"2021-11-01T03:21:41.187746Z","iopub.execute_input":"2021-11-01T03:21:41.188008Z","iopub.status.idle":"2021-11-01T03:27:56.492892Z","shell.execute_reply.started":"2021-11-01T03:21:41.187979Z","shell.execute_reply":"2021-11-01T03:27:56.492184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-info\" role=\"alert\">\n\n**`NOTA:`** <br>\n    \nNa validação cruzada obtivemos uma `AUC` de `0.73104` com um desvio padrão de `0.00113` sem fazer o scaler, na submissão obtivemos os seguintes resultados: <br>\n\n- None: 0.73562    \n- StandarScaler: 0.73706    \n- QuartileTransforme: `0.73780`\n    \n\n<br>\n    \nCom a submissão dos arquivo na competição, observamos que QuartileTransforme resulta no melhor, **agora temos uma baseline que AUC 0.73780**.\n    \n \nVamos treinar novamente, sendo que vamos acrescentar o parametros  `n_estimators` que indica o número de arvores para o treinamento do modelo.\n    \n</div>","metadata":{}},{"cell_type":"code","source":"%%time\n\ngc.collect()\n\nparams.update({'n_estimators': 1000})\n\nscalers = [None, \n           StandardScaler(), \n           RobustScaler(),           \n           QuantileTransformer(output_distribution='normal', random_state=0)]\n\nmodel_best = cross_val_model(model           = xgb.XGBClassifier(**params), \n                             X_train_        = X, \n                             y_train_        = y,\n                             X_test_         = X_test,                                            \n                             scalers         = scalers, \n                             name_model      = 'xgb_002_bl_n_estimators_1000_{:2.5f}', \n                             FOLDS           = 5, \n                             seed            = seed, \n                             use_ntree_limit = False\n                             ) \n\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-11-01T20:23:39.620827Z","iopub.execute_input":"2021-11-01T20:23:39.621087Z","iopub.status.idle":"2021-11-01T20:29:14.171537Z","shell.execute_reply.started":"2021-11-01T20:23:39.621058Z","shell.execute_reply":"2021-11-01T20:29:14.170815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-info\" role=\"alert\"> \n \n**`NOTA:`** <br>\nNo treinamento do modelo, podemos observar uma melhar na AUC de `0.73104`  sem fazer o scaler para `0.73136`, no resultado das submissões dos arquivos obtivemos os seguintes resultados: \n\n\n- None: 0.73794\n- QuantileTransforme:  `0.73812`\n    \n    \nMais uma vez o scaler QuantileTransforme se mostrou melhor na submissão e bateu a baseline de `0.73136` para `0.73812`, como podemos observar o aumento de estimadores (arvores) melhorou o score, será que o número de 1000 arvores para o XBG é o ideal? O XGB tem um atributo chamado `best_ntree_limit` que retorna o número aproximado de arvores, que pode ser utilizado na previsão do modelo através do parametro `ntree_limit` do método `predict`ou pridict_proba.  \n    \n</div>","metadata":{}},{"cell_type":"code","source":"%%time\n\ngc.collect()\n\nparams.update({'n_estimators': 1000})\n\nscalers = [None, \n           StandardScaler(), \n           RobustScaler(),           \n           QuantileTransformer(output_distribution='normal', random_state=0)]\n\nmodel_best = cross_val_model(model           = xgb.XGBClassifier(**params), \n                             X_train_        = X, \n                             y_train_        = y,\n                             X_test_         = X_test,                                            \n                             scalers         = scalers, \n                             name_model      = 'xgb_003_bl_n_estimators_1000_limit_{:2.5f}', \n                             FOLDS           = 5, \n                             seed            = seed, \n                             use_ntree_limit = True\n                             ) \n\ngc.collect()\n\nprint(params)\nprint()","metadata":{"execution":{"iopub.status.busy":"2021-11-01T20:35:44.438289Z","iopub.execute_input":"2021-11-01T20:35:44.439013Z","iopub.status.idle":"2021-11-01T20:41:14.991806Z","shell.execute_reply.started":"2021-11-01T20:35:44.438975Z","shell.execute_reply":"2021-11-01T20:41:14.991037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-info\" role=\"alert\"> \n \n**`NOTA:`** <br>\n  \n\nComo podemos observar os resultados não melhou na validação, porém ao fazer as submissões dos dois melhores resultados obtivemos os seguites resultados com uma pequena melhoria em ambos os scalres: \n  \n- None: 0.73816 \n- QuantileTransforme: `0.73818`\n  \n<br> \n    \nO QuantileTransforme novamente se mostrou o melhor scaler para o XGB e outro ponto importante é o número de estimadores que fica em média de 100 à 120 estimadores, vamos fazer uma pequena análise do modelo com esses parametros e scaler quantileTransforme. \n    \n</div>","metadata":{}},{"cell_type":"markdown","source":"## 2.1. Análise do Modelo \nVamos fazer o treinamento novamente do modelo com 1000 estimadores e fazer a previsão utilizando o limite ideal sugerido pelo atributo `best_ntree_limit` do XGB, sendo que vamos treinar em 80% dos dados e fazer a previsão em 20% dos dados que o medelo não viu, para termos uma idéia da rubustez do modelo em dados não visto no treinamento.  ","metadata":{}},{"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, \n                                                      test_size    = 0.2,\n                                                      shuffle      = True, \n                                                      stratify     = y,\n                                                      random_state = 12359)\n\nX_train.shape, y_train.shape, X_valid.shape, y_valid.shape , X_test.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-01T21:55:38.820913Z","iopub.execute_input":"2021-11-01T21:55:38.821313Z","iopub.status.idle":"2021-11-01T21:55:39.552751Z","shell.execute_reply.started":"2021-11-01T21:55:38.821263Z","shell.execute_reply":"2021-11-01T21:55:39.551983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nparams.update({'n_estimators': 1000})\n\nscaler     = QuantileTransformer(output_distribution='normal', random_state=0) \nmodel_best = cross_val_model(model           = xgb.XGBClassifier(**params), \n                             X_train_        = X_train, \n                             y_train_        = y_train,\n                             X_test_         = X_test,                                            \n                             scalers         = [scaler], \n                             name_model      = 'xgb_004_bl_n_estimators_1000_{:2.5f}', \n                             FOLDS           = 5, \n                             seed            = seed, \n                             use_ntree_limit = False\n                             ) \n\ngc.collect()\n\nprint(params)","metadata":{"execution":{"iopub.status.busy":"2021-11-01T21:56:14.415245Z","iopub.execute_input":"2021-11-01T21:56:14.41585Z","iopub.status.idle":"2021-11-01T21:58:38.343205Z","shell.execute_reply.started":"2021-11-01T21:56:14.415813Z","shell.execute_reply":"2021-11-01T21:58:38.34241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.1.1. Número de Estimadores","metadata":{}},{"cell_type":"code","source":"model_best.get_params()","metadata":{"execution":{"iopub.status.busy":"2021-11-01T21:59:56.286956Z","iopub.execute_input":"2021-11-01T21:59:56.287497Z","iopub.status.idle":"2021-11-01T21:59:56.296043Z","shell.execute_reply.started":"2021-11-01T21:59:56.287457Z","shell.execute_reply":"2021-11-01T21:59:56.295227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results     = model_best.evals_result()\nntree_limit = model_best.best_ntree_limit\n\nplt.figure(figsize=(7,5))\nplt.plot(results[\"validation_0\"][\"auc\"], label=\"Treinamento\")\nplt.plot(results[\"validation_1\"][\"auc\"], label=\"Validação\")\n\n\nplt.axvline(ntree_limit, \n            color=\"gray\", \n            label=\"N. de árvore ideal {}\".format(ntree_limit))\n\nplt.xlabel(\"Número de árvores\")\nplt.ylabel(\"AUC\")\nplt.legend();","metadata":{"execution":{"iopub.status.busy":"2021-11-01T21:59:28.754896Z","iopub.execute_input":"2021-11-01T21:59:28.75522Z","iopub.status.idle":"2021-11-01T21:59:29.011537Z","shell.execute_reply.started":"2021-11-01T21:59:28.755186Z","shell.execute_reply":"2021-11-01T21:59:29.010842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-info\" role=\"alert\"> \n    \n**`NOTA:`** <br>\nAcima recuperamos as informações de treinamento do nosso modelo, podemos observar que o número de 1000 estimadores é mais que suficiente para o treinamento do modelo, o ideal é que fique em entre de 110 à 120 para esses dados e com a utilização dos parametros padrões que devem ser ajustados para o `XGB`. <br>\n    \n    \nVamos agora utilizar o modelo treinado que foi retornado pela função e vamos fazer a previsão para novos dados que o modelo não viu no treinamento, para termos uma ideia da generalização do modelo, lembrando que o modelo que foi treinado utiliza 1000 estimadores (arvores), sendo assim, vamos utilizar na previssão 114 estimadores utilizando o parametro `ntree_limit` ao fazermos as previsões.\n\n</div>","metadata":{}},{"cell_type":"code","source":"X_valid_sc  = scaler.fit_transform(X_valid)","metadata":{"execution":{"iopub.status.busy":"2021-11-01T22:02:04.689535Z","iopub.execute_input":"2021-11-01T22:02:04.689812Z","iopub.status.idle":"2021-11-01T22:02:09.913974Z","shell.execute_reply.started":"2021-11-01T22:02:04.689781Z","shell.execute_reply":"2021-11-01T22:02:09.91322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nthreshold   =.5\ny_pred_prob = model_best.predict_proba(X_valid_sc,  ntree_limit=ntree_limit)[:, 1] \ny_pred      = (y_pred_prob > threshold).astype(int)\n\nf1_    = metrics.f1_score(y_valid, y_pred)\nauc_   = metrics.roc_auc_score(y_valid, y_pred_prob)\nlloss_ = metrics.log_loss(y_valid, y_pred_prob) \n    \nprint('AUC     : {:2.5f}'.format(auc_))\nprint('F1-Score: {:2.5f}'.format(f1_))\nprint('L. Loss : {:2.5f}'.format(lloss_))\nprint()","metadata":{"execution":{"iopub.status.busy":"2021-11-01T22:03:18.05721Z","iopub.execute_input":"2021-11-01T22:03:18.057879Z","iopub.status.idle":"2021-11-01T22:03:18.468091Z","shell.execute_reply.started":"2021-11-01T22:03:18.057838Z","shell.execute_reply":"2021-11-01T22:03:18.46708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-info\" role=\"alert\"> \n    \n**`NOTA:`** <br>\n\nNo treinamento obtivemos uma AUC de 0.72913 e no dados de validação obtivemos uma AUC de 0.72866, temos uma pequena diferença, o que nos indica que o modelo consegue generalizar em dados não visto no treinamento.  \n    \n</div>","metadata":{}},{"cell_type":"markdown","source":"### 2.1.2. Curva ROC","metadata":{}},{"cell_type":"code","source":"fpr, tpr, thresholds = metrics.roc_curve(y_valid, y_pred_prob)\n\nplot_roc_curve(fpr, tpr, label=\"XGB\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-01T22:11:34.322371Z","iopub.execute_input":"2021-11-01T22:11:34.322639Z","iopub.status.idle":"2021-11-01T22:11:34.552419Z","shell.execute_reply.started":"2021-11-01T22:11:34.322603Z","shell.execute_reply":"2021-11-01T22:11:34.551703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-info\" role=\"alert\"> \n    \n**`NOTA:`** <br>\n    \nObservando a Curva ROC acima, podemos fazer alguns testes em relação ao ponto de corte, pois a AUC de 0.72866 tem um ponto de corte de 0.5 que é padrão, o que nos dar um F1-score de 0.69587, vamos fazer um gráfico de Matriz de Confusão para termos uma ideia melhor das previsões do modelo.  \n    \n</div>","metadata":{}},{"cell_type":"markdown","source":"### 2.1.3. Matriz de Confusão","metadata":{}},{"cell_type":"markdown","source":"#### 2.1.3.1. Com ponto de corte padrão","metadata":{}},{"cell_type":"code","source":"threshold    = .5\ny_pred_valid = (y_pred_prob > threshold).astype(int)\nf1_          = metrics.f1_score (y_valid, y_pred_valid)\nauc_         = metrics.roc_auc_score(y_valid, y_pred_prob)\n\nprint(metrics.classification_report(y_valid, y_pred_valid))\nprint('')\nprint('AUC     : {:2.5f}'.format(auc_))\nprint('F1-score: {:2.5f}'.format(f1_))\n\nconfusion_plot(metrics.confusion_matrix(y_valid, y_pred) )","metadata":{"execution":{"iopub.status.busy":"2021-11-01T22:15:35.051899Z","iopub.execute_input":"2021-11-01T22:15:35.052182Z","iopub.status.idle":"2021-11-01T22:15:36.181606Z","shell.execute_reply.started":"2021-11-01T22:15:35.052151Z","shell.execute_reply":"2021-11-01T22:15:36.180943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.1.3.2. Com ponto de corte","metadata":{}},{"cell_type":"code","source":"threshold    = .43\ny_pred_valid = (y_pred_prob > threshold).astype(int)\nf1_          = metrics.f1_score (y_valid, y_pred_valid)\nauc_         = metrics.roc_auc_score(y_valid, y_pred_prob)\n\nprint(metrics.classification_report(y_valid, y_pred_valid))\nprint('')\nprint('AUC     : {:2.5f}'.format(auc_))\nprint('F1-score: {:2.5f}'.format(f1_))\n\nconfusion_plot(metrics.confusion_matrix(y_valid, y_pred_valid) )","metadata":{"execution":{"iopub.status.busy":"2021-11-01T22:19:46.168834Z","iopub.execute_input":"2021-11-01T22:19:46.169584Z","iopub.status.idle":"2021-11-01T22:19:47.079627Z","shell.execute_reply.started":"2021-11-01T22:19:46.169534Z","shell.execute_reply":"2021-11-01T22:19:47.078963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-info\" role=\"alert\"> \n    \n**`NOTA:`** <br>\nCom um ponto de corte de 0.43 temos um F1-score de 0.71225, passamos a acerta mais os falso positivos. \n    \n</div>\n","metadata":{}},{"cell_type":"markdown","source":"### 2.1.3. Feature Importances  ","metadata":{}},{"cell_type":"code","source":"feature_imp_     = model_best.feature_importances_\n\ndf               = pd.DataFrame()\ndf[\"Feature\"]    = X.columns\ndf[\"Importance\"] = feature_imp_ / feature_imp_.sum()\n\ndf.sort_values(\"Importance\", axis=0, ascending=False, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-01T22:25:57.8413Z","iopub.execute_input":"2021-11-01T22:25:57.841562Z","iopub.status.idle":"2021-11-01T22:25:58.051903Z","shell.execute_reply.started":"2021-11-01T22:25:57.841532Z","shell.execute_reply":"2021-11-01T22:25:58.05107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(13, 70))\nbars    = ax.barh(df[\"Feature\"], \n                  df[\"Importance\"], \n                  height    = 0.4,\n                  color     = \"mediumorchid\", \n                  edgecolor = \"black\")\n\nax.set_title(\"Feature importances\", fontsize=30, pad=15)\nax.set_ylabel(\"Feature name\", fontsize=20, labelpad=15)\n#ax.set_xlabel(\"Feature importance\", fontsize=20, labelpad=15)\nax.set_yticks(df[\"Feature\"])\nax.set_yticklabels(df[\"Feature\"], fontsize=13)\nax.tick_params(axis=\"x\", labelsize=15)\nax.grid(axis=\"x\")\n\n# Adicionando rótulos na parte superior\nax2 = ax.secondary_xaxis('top')\n#ax2.set_xlabel(\"Feature importance\", fontsize=20, labelpad=13)\nax2.tick_params(axis=\"x\", labelsize=15)\nax.margins(0.05, 0.01)\n\n# Inverter a direção do eixo y \nplt.gca().invert_yaxis()","metadata":{"execution":{"iopub.status.busy":"2021-11-01T22:26:15.861853Z","iopub.execute_input":"2021-11-01T22:26:15.862238Z","iopub.status.idle":"2021-11-01T22:26:17.630477Z","shell.execute_reply.started":"2021-11-01T22:26:15.862092Z","shell.execute_reply":"2021-11-01T22:26:17.629775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <div class=\"alert alert-success\">  3. Conclusão </div> <br>","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-info\" role=\"alert\"> \n\nCom este notebook fizemos uma pequena análise dos dados e identificamos que os dados disponíveis para a competição não segue uma distribuição normal, tem muitos outliers que precisam de um tratamento, nesta etapa utilizamos o XGB como modelo de linha de base e testamos diversos tipos de padronização de dados, o `QuatileTransforme` se mostrou o melhor padronizador com o XGB com a maioria dos parametros padrão.   \n\n<br>     \nNo próximo notebook vamos criar novas variáveis para ajuda os modelos a identificar novos padrões nos dados e melhor AUC.  \n    \n    \n</div>\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}