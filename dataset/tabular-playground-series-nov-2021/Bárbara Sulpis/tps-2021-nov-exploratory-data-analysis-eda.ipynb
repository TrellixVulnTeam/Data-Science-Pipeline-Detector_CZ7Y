{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Exploratory Data Analisys for Tabular Playground Series (Nov 2021)","metadata":{}},{"cell_type":"code","source":"# =======================================================\n# TPS Nov 2021 - EDA\n# =======================================================\n# Name: BÃ¡rbara Sulpis\n# Date: 1-nov-2021\n# Description: I will analyze TPS data to have an idea of following steps...\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport scipy.stats as st # statistical functions\nimport os\n\nfrom sklearn.model_selection import train_test_split\n\n#Lgbm\nimport lightgbm as lgb\n\n# roc\nimport sklearn.metrics as metrics   # Para la curva ROC\nimport matplotlib.pyplot as plt     # Para la curva ROC\n\n# for hystograms\nimport seaborn as sns\n\n\n# ---------------------------\n# Input data:\n# Go to file -> add or upload data -> \"Competition\" data tab and select the commpetition which you want to add the csv data data \"\n# files are available in the read-only \"../input/\" directory\n# ---------------------------\n\nlist =  os. getcwd()\nprint(list) # shoud be in \"kaggle\" directory\n\n# I left this commented if you want to check that the files are there\n# i = 0\n# for subdir, dirs, files in os.walk('./'):\n#     for file in files:\n#         print(file)\n#         i+= 1\n#         if i>20: \n#             break\n\n\ndata = pd.read_csv(\"../input/tabular-playground-series-nov-2021/train.csv\")        \nsubm = pd.read_csv(\"../input/tabular-playground-series-nov-2021/test.csv\")  \n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-01T02:47:19.606473Z","iopub.execute_input":"2021-11-01T02:47:19.607155Z","iopub.status.idle":"2021-11-01T02:47:49.641518Z","shell.execute_reply.started":"2021-11-01T02:47:19.607011Z","shell.execute_reply":"2021-11-01T02:47:49.640661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Size of the dataset\ndata.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-01T02:48:13.757532Z","iopub.execute_input":"2021-11-01T02:48:13.758375Z","iopub.status.idle":"2021-11-01T02:48:13.764123Z","shell.execute_reply.started":"2021-11-01T02:48:13.75834Z","shell.execute_reply":"2021-11-01T02:48:13.763273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# With this setting we can see all rows of the dataset\npd.set_option(\"display.max_columns\", 300)\n# We have a look to the data\ndata","metadata":{"execution":{"iopub.status.busy":"2021-11-01T02:48:21.237792Z","iopub.execute_input":"2021-11-01T02:48:21.238402Z","iopub.status.idle":"2021-11-01T02:48:21.35386Z","shell.execute_reply.started":"2021-11-01T02:48:21.238365Z","shell.execute_reply":"2021-11-01T02:48:21.352855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Before working with the data, we reduce the use of memory, so we can improve performance\n# REFERENCE: https://www.kaggle.com/smiles28/tps10-optuna-xgb-catb-lgbm-stacking\n\n# What the function does is to deduce the data types and cast each column to its most performant type\n\ndef reduce_mem_usage(props):\n    start_mem_usg = props.memory_usage().sum() / 1024**2 \n    print(\"Memory usage of properties dataframe is :\",start_mem_usg,\" MB\")\n    for col in props.columns:\n        if props[col].dtype != object:  # Exclude strings\n            # make variables for Int, max and min\n            IsInt = False\n            mx = props[col].max()\n            mn = props[col].min()\n    \n            # test if column can be converted to an integer\n            asint = props[col].astype(np.int64)\n            result = (props[col] - asint)\n            result = result.sum()\n            if result > -0.01 and result < 0.01:\n                IsInt = True\n\n            \n            # Make Integer/unsigned Integer datatypes\n            if IsInt:\n                if mn >= 0:\n                    if mx < 255:\n                        props[col] = props[col].astype(np.uint8)\n                    elif mx < 65535:\n                        props[col] = props[col].astype(np.uint16)\n                    elif mx < 4294967295:\n                        props[col] = props[col].astype(np.uint32)\n                    else:\n                        props[col] = props[col].astype(np.uint64)\n                else:\n                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n                        props[col] = props[col].astype(np.int8)\n                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n                        props[col] = props[col].astype(np.int16)\n                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n                        props[col] = props[col].astype(np.int32)\n                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n                        props[col] = props[col].astype(np.int64)    \n            \n            # Make float datatypes 32 bit\n            else:\n                props[col] = props[col].astype(np.float32)\n    \n    # Print final result\n    print(\"___MEMORY USAGE AFTER COMPLETION:___\")\n    mem_usg = props.memory_usage().sum() / 1024**2 \n    print(\"Memory usage is: \",mem_usg,\" MB\")\n    print(\"This is \",100*mem_usg/start_mem_usg,\"% of the initial size\")\n    return props\n","metadata":{"execution":{"iopub.status.busy":"2021-11-01T02:48:34.532952Z","iopub.execute_input":"2021-11-01T02:48:34.533296Z","iopub.status.idle":"2021-11-01T02:48:34.549698Z","shell.execute_reply.started":"2021-11-01T02:48:34.533261Z","shell.execute_reply":"2021-11-01T02:48:34.548822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = reduce_mem_usage(data)","metadata":{"execution":{"iopub.status.busy":"2021-11-01T02:48:39.088914Z","iopub.execute_input":"2021-11-01T02:48:39.089249Z","iopub.status.idle":"2021-11-01T02:48:47.045112Z","shell.execute_reply.started":"2021-11-01T02:48:39.089216Z","shell.execute_reply":"2021-11-01T02:48:47.04423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"subm = reduce_mem_usage(subm)","metadata":{"execution":{"iopub.status.busy":"2021-11-01T02:48:56.418202Z","iopub.execute_input":"2021-11-01T02:48:56.418509Z","iopub.status.idle":"2021-11-01T02:49:03.649447Z","shell.execute_reply.started":"2021-11-01T02:48:56.418477Z","shell.execute_reply":"2021-11-01T02:49:03.648673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ------------------------------------------------------------\n#   Search for MISSING values\n# ------------------------------------------------------------\n# First we make a dataframe with the number of not-null values for each column\ncount = pd.DataFrame(data.count(), columns=['count'])\n\n# Then we get the fields that has a number smaller than 600k (the number of rows in train set)\ncount.query(\"count < 600000\")\n\n# As we can see there are not null values in the dataset. ","metadata":{"execution":{"iopub.status.busy":"2021-11-01T02:49:45.614784Z","iopub.execute_input":"2021-11-01T02:49:45.615158Z","iopub.status.idle":"2021-11-01T02:49:45.79117Z","shell.execute_reply.started":"2021-11-01T02:49:45.61512Z","shell.execute_reply":"2021-11-01T02:49:45.790104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"subm.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-01T02:52:03.653162Z","iopub.execute_input":"2021-11-01T02:52:03.653524Z","iopub.status.idle":"2021-11-01T02:52:03.658663Z","shell.execute_reply.started":"2021-11-01T02:52:03.653488Z","shell.execute_reply":"2021-11-01T02:52:03.658036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We can make the same check for the submission dataset (\"test.csv dataset\")\ncount = pd.DataFrame(subm.count(), columns=['count'])\ncount.query(\"count < 540000\")\n\n# As expected, there are not null values in test dataset neither.","metadata":{"execution":{"iopub.status.busy":"2021-11-01T02:52:27.522305Z","iopub.execute_input":"2021-11-01T02:52:27.522618Z","iopub.status.idle":"2021-11-01T02:52:27.657635Z","shell.execute_reply.started":"2021-11-01T02:52:27.522584Z","shell.execute_reply":"2021-11-01T02:52:27.656668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ------------------------------------------------------------\n#   Variable CORRELATION\n# ------------------------------------------------------------\n# Correlation matrix\n# --------------------\n# We make a correlation matrix to check if there are relations between the different fields.\ncorrmat = data.corr()","metadata":{"execution":{"iopub.status.busy":"2021-11-01T02:52:46.418956Z","iopub.execute_input":"2021-11-01T02:52:46.419261Z","iopub.status.idle":"2021-11-01T02:53:03.797727Z","shell.execute_reply.started":"2021-11-01T02:52:46.419229Z","shell.execute_reply":"2021-11-01T02:53:03.796865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's draw the corrmat\nf, ax = plt.subplots(figsize =(40, 40))\nsns.heatmap(corrmat, ax = ax, cmap =\"YlGnBu\", linewidths = 0.1)\n# Explanation of the graph: The blue diagonal \"line\" represents a 100% of correlation between each feature and itself\n#   the other points, as the right vertical correlation rule indicates, seems not to have correlation with other features except of itself. ","metadata":{"execution":{"iopub.status.busy":"2021-11-01T02:54:16.729462Z","iopub.execute_input":"2021-11-01T02:54:16.72983Z","iopub.status.idle":"2021-11-01T02:54:20.34762Z","shell.execute_reply.started":"2021-11-01T02:54:16.729776Z","shell.execute_reply":"2021-11-01T02:54:20.346644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Distribution of the target:\ndata.groupby('target').count()\n# 296394\n# 303606\n# The data is quite perfectly balanced","metadata":{"execution":{"iopub.status.busy":"2021-11-01T02:58:34.957882Z","iopub.execute_input":"2021-11-01T02:58:34.958526Z","iopub.status.idle":"2021-11-01T02:58:35.395962Z","shell.execute_reply.started":"2021-11-01T02:58:34.958481Z","shell.execute_reply":"2021-11-01T02:58:35.395096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ------------------------------------------------------------\n#   Variable DISTRIBUTIONS\n# ------------------------------------------------------------\n# I will draw the hystograms for all variables\ndata.hist(grid = False, figsize=(25,80), layout=(29, 10), bins=50)","metadata":{"execution":{"iopub.status.busy":"2021-11-01T02:59:26.554773Z","iopub.execute_input":"2021-11-01T02:59:26.555278Z","iopub.status.idle":"2021-11-01T02:59:56.309101Z","shell.execute_reply.started":"2021-11-01T02:59:26.555244Z","shell.execute_reply":"2021-11-01T02:59:56.308236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ------------------------------------------------------------------------------\n#  CARDINALITY OF VARIABLES\n# ------------------------------------------------------------------------------\n# After watching the output we can appreciate that there are many features that seems to have few different values\n# So, let's see theyr cardinality\npd.set_option(\"display.max_rows\", 300)\n\ndata.nunique()","metadata":{"execution":{"iopub.status.busy":"2021-11-01T03:03:34.530711Z","iopub.execute_input":"2021-11-01T03:03:34.531132Z","iopub.status.idle":"2021-11-01T03:03:37.138638Z","shell.execute_reply.started":"2021-11-01T03:03:34.531096Z","shell.execute_reply":"2021-11-01T03:03:37.137505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# What we can see below is that there are NO variables with few different values. (low cardinality)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We can find handy this other histogram plot, that makes two plots overlapped\n# Superposition of the two graphs: target==1 and target==0\n# We will only plot the first 5 features\n\n# REFERENCE: https://stackoverflow.com/questions/37911731/seaborn-histogram-with-4-panels-2-x-2-in-python\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndata_hist = pd.melt(data[['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'target']], \"target\", var_name=\"target distributions\")\ng = sns.FacetGrid(data_hist, hue=\"target\", col=\"target distributions\", col_wrap=5, sharex=False, sharey=False)\ng.map(plt.hist, \"value\", bins=20, alpha=.4)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-01T03:21:11.838613Z","iopub.execute_input":"2021-11-01T03:21:11.839135Z","iopub.status.idle":"2021-11-01T03:21:18.939794Z","shell.execute_reply.started":"2021-11-01T03:21:11.839099Z","shell.execute_reply":"2021-11-01T03:21:18.938772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ------------------------------------------------------------\n#  Checking SKEWNESS for continuous data\n# ------------------------------------------------------------\n# Last of all, the following code is to calculate the skewed data. In this example left skewed data.\n# This could be used to correct skewness with log or exponential transformations \n\ndata_skewed = pd.concat([pd.DataFrame(data.columns), pd.DataFrame(st.skew(data))], axis=1)\ndata_skewed.columns = ['names', 'skewness']\n# I only get fields that has a skewness bigger than 3\nskewed = data_skewed.query('skewness > 3')['names']\nskewed","metadata":{"execution":{"iopub.status.busy":"2021-11-01T03:22:08.301377Z","iopub.execute_input":"2021-11-01T03:22:08.301677Z","iopub.status.idle":"2021-11-01T03:22:09.730789Z","shell.execute_reply.started":"2021-11-01T03:22:08.301647Z","shell.execute_reply":"2021-11-01T03:22:09.730183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ------------------------------------------------------------\n#  Best performing algorithms\n# ------------------------------------------------------------\n# As part of the EDA I can add the output of the LazyPredict (TPS Oct 2021) used in other of my notebooks.\n# REFERENCE: https://www.kaggle.com/brbarasulpis/tps-2021-oct-automl-lazypredict-lazyclassifier","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CONCLUSIONS\nAfter this exploratory data analisys we now know that:\n* There are no missing values in the dataset\n* The target is balanced (nearly half values in 1 and half in 0)\n* There is no correlations between the variables\n* There are no categorical features\n* Many continuous features are left skewed\n* Many continuous features seems to have a compound distribution","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}