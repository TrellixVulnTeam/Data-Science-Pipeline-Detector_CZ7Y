{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Building upon the very good notebook by https://www.kaggle.com/mlanhenke (https://www.kaggle.com/mlanhenke/tps-11-nn-baseline-keras) which presents a very clean and easy to understand structure, I prepared some a few variations to help furthermore the participants of this competition.","metadata":{}},{"cell_type":"markdown","source":"The improvements upsofar are:\n\n* deterministic random seeding\n* enabling TPU usage just by switching accelerator type\n* adding mish and gelu activations\n* model summary and plot\n","metadata":{}},{"cell_type":"markdown","source":"In this notebook, I will create a variation inspired by the work of Chris Deotte (https://www.kaggle.com/cdeotte/lstm-feature-importance) in the recent Google Brain Ventilator Pressure competition. The idea is to use permutation feature importance (see: https://christophm.github.io/interpretable-ml-book/feature-importance.html#feature-importance) in order to figure the role of a feature in the prediction. ","metadata":{}},{"cell_type":"markdown","source":"When the first fold is created, and you have a baseline performance on the out of fold, you can try permuting each feature at once and see if the result degrades (highlighting the feature is important for the model to produce good predictions), stays the same (implying the feature is irrelevant) or improves (hinting that the feature is harmful).","metadata":{}},{"cell_type":"markdown","source":"The procedure doesn't require much computational power because it involves only prediction and not re-training. However, there are caveats. The procedure doesn't take into account multi-collinearity of the features. Consequently, if you remove one feature and then reprocess the importance (as in recursive feature elimination, see:https://scikit-learn.org/stable/auto_examples/feature_selection/plot_rfe_with_cross_validation.html) you are surely removing one unimportant feature. Instead, if you remove bunch of features at once, it may occur that you are removeing highly correlated features that hide each other importance in respect of the prediction (in fact, when you shuffle one, the other high correlated ones do work in its place). In this case you may see the performances unexpectantly decrese instead of improving or staying the same.","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport random\nfrom tqdm import tqdm\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\n\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'","metadata":{"execution":{"iopub.status.busy":"2021-11-06T18:34:28.201669Z","iopub.execute_input":"2021-11-06T18:34:28.202188Z","iopub.status.idle":"2021-11-06T18:34:29.032422Z","shell.execute_reply.started":"2021-11-06T18:34:28.202094Z","shell.execute_reply":"2021-11-06T18:34:29.031699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import Sequential, Model\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.layers import Dense, Flatten, Input, Concatenate, Dropout\nfrom tensorflow.keras.utils import plot_model","metadata":{"execution":{"iopub.status.busy":"2021-11-06T18:34:29.035734Z","iopub.execute_input":"2021-11-06T18:34:29.035934Z","iopub.status.idle":"2021-11-06T18:34:33.470658Z","shell.execute_reply.started":"2021-11-06T18:34:29.03591Z","shell.execute_reply":"2021-11-06T18:34:33.469908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed):\n    \"\"\"\n    Seeds basic parameters for reproductibility of results\n    \"\"\"\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    \nSEED = 3024","metadata":{"execution":{"iopub.status.busy":"2021-11-06T18:34:33.472047Z","iopub.execute_input":"2021-11-06T18:34:33.472298Z","iopub.status.idle":"2021-11-06T18:34:33.480235Z","shell.execute_reply.started":"2021-11-06T18:34:33.472264Z","shell.execute_reply":"2021-11-06T18:34:33.479167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def gelu(x):\n    return 0.5 * x * (1 + tf.tanh(tf.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3))))\n\nclass Mish(keras.layers.Activation):\n    '''\n    Mish Activation Function.\n    see: https://github.com/digantamisra98/Mish/blob/master/Mish/TFKeras/mish.py\n    .. math::\n        mish(x) = x * tanh(softplus(x)) = x * tanh(ln(1 + e^{x}))\n    Shape:\n        - Input: Arbitrary. Use the keyword argument `input_shape`\n        (tuple of integers, does not include the samples axis)\n        when using this layer as the first layer in a model.\n        - Output: Same shape as the input.\n    Examples:\n        >>> X = Activation('Mish', name=\"conv1_act\")(X_input)\n    '''\n\n    def __init__(self, activation, **kwargs):\n        super(Mish, self).__init__(activation, **kwargs)\n        self.__name__ = 'Mish'\n\ndef mish(inputs):\n    return inputs * tf.math.tanh(tf.math.softplus(inputs))\n\nkeras.utils.get_custom_objects().update({'gelu': keras.layers.Activation(gelu)})\nkeras.utils.get_custom_objects().update({'mish': Mish(mish)})","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-06T18:34:33.482113Z","iopub.execute_input":"2021-11-06T18:34:33.482352Z","iopub.status.idle":"2021-11-06T18:34:33.515599Z","shell.execute_reply.started":"2021-11-06T18:34:33.48232Z","shell.execute_reply":"2021-11-06T18:34:33.514969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_history(history, start=0, fold=0):\n    epochs = np.arange(len(history.history['loss']))\n    plt.figure(figsize=(15,5))\n    plt.plot(epochs[start:], history.history['loss'][start:], \n             label='train', color='blue')\n    plt.plot(epochs[start:], history.history['val_loss'][start:], \n             label='validation', color='red')\n    plt.ylabel('Loss',size=14)\n    plt.title(f\"Fold: {fold+1}\")\n    plt.legend()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-06T18:34:33.518323Z","iopub.execute_input":"2021-11-06T18:34:33.518517Z","iopub.status.idle":"2021-11-06T18:34:33.525171Z","shell.execute_reply.started":"2021-11-06T18:34:33.518483Z","shell.execute_reply":"2021-11-06T18:34:33.524396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading data\ndf_train = pd.read_csv('../input/tabular-playground-series-nov-2021/train.csv')\ndf_test = pd.read_csv('../input/tabular-playground-series-nov-2021/test.csv')\n\nsample_submission = pd.read_csv('../input/tabular-playground-series-nov-2021/sample_submission.csv')\n\n# Preparing training data\nX = df_train.drop(columns=['id','target']).copy()\ny = df_train['target'].copy()\n\n# Preparing test data\nX_test = df_test.drop(columns='id').copy()\n\n# Data Standardization\nscaler = MinMaxScaler(feature_range=(0, 1))\n \nX = pd.DataFrame(columns=X.columns, data=scaler.fit_transform(X))\nX_test = pd.DataFrame(columns=X_test.columns, data=scaler.transform(X_test))","metadata":{"execution":{"iopub.status.busy":"2021-11-06T18:34:33.526745Z","iopub.execute_input":"2021-11-06T18:34:33.527001Z","iopub.status.idle":"2021-11-06T18:35:02.65796Z","shell.execute_reply.started":"2021-11-06T18:34:33.526967Z","shell.execute_reply":"2021-11-06T18:35:02.657228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### define callbacks\nearly_stopping = EarlyStopping(\n    monitor='val_loss', \n    min_delta=0, \n    patience=20, \n    verbose=0,\n    mode='min', \n    baseline=None, \n    restore_best_weights=True\n)\n\nreduce_lr = ReduceLROnPlateau(\n    monitor='val_loss', \n    factor=0.2,\n    patience=5,\n    mode='min'\n)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-06T18:35:02.659316Z","iopub.execute_input":"2021-11-06T18:35:02.659814Z","iopub.status.idle":"2021-11-06T18:35:02.665623Z","shell.execute_reply.started":"2021-11-06T18:35:02.659777Z","shell.execute_reply":"2021-11-06T18:35:02.664973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### create baseline-model\ndef get_model(name:str):\n    \n    inputs_sequence = Input(shape=(X.shape[1]))\n    x = Flatten()(inputs_sequence)\n\n    skips = list()\n    layers = [128, 64, 32]\n    for layer, nodes in enumerate(layers):\n        x = Dense(128, activation='swish')(x)\n        x = Dropout(0.2)(x)\n        if layer != (len(layers) - 1):\n            skips.append(x)\n    \n    x = Concatenate(axis=1)([x] + skips)\n    output_class = Dense(1, activation='sigmoid')(x)\n\n    model = Model(inputs=inputs_sequence, outputs=output_class, name=name)\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2021-11-06T18:35:02.666803Z","iopub.execute_input":"2021-11-06T18:35:02.66712Z","iopub.status.idle":"2021-11-06T18:35:02.678293Z","shell.execute_reply.started":"2021-11-06T18:35:02.667082Z","shell.execute_reply":"2021-11-06T18:35:02.677389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = get_model(name='Baseline')\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-11-06T18:35:02.679583Z","iopub.execute_input":"2021-11-06T18:35:02.680031Z","iopub.status.idle":"2021-11-06T18:35:04.811491Z","shell.execute_reply.started":"2021-11-06T18:35:02.679995Z","shell.execute_reply":"2021-11-06T18:35:04.810779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model(\n    model, \n    to_file='baseline.png', \n    show_shapes=True,\n    show_layer_names=True\n)","metadata":{"execution":{"iopub.status.busy":"2021-11-06T18:35:04.814618Z","iopub.execute_input":"2021-11-06T18:35:04.815681Z","iopub.status.idle":"2021-11-06T18:35:05.647813Z","shell.execute_reply.started":"2021-11-06T18:35:04.815635Z","shell.execute_reply":"2021-11-06T18:35:05.644352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    # detect and init the TPU\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n    # instantiate a distribution strategy\n    tf_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    print(\"Running on TPU:\", tpu.master())\nexcept:\n    tf_strategy = tf.distribute.get_strategy()\n    print(f\"Running on {tf_strategy.num_replicas_in_sync} replicas\")\n    print(\"Number of GPUs Available: \", len(tf.config.list_physical_devices('GPU')))","metadata":{"execution":{"iopub.status.busy":"2021-11-06T18:35:05.649229Z","iopub.execute_input":"2021-11-06T18:35:05.649713Z","iopub.status.idle":"2021-11-06T18:35:05.656793Z","shell.execute_reply.started":"2021-11-06T18:35:05.649679Z","shell.execute_reply":"2021-11-06T18:35:05.656079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N_FOLDS = 10\nFEATURE_IMPORTANCE = True\n\n### cross-validation \ncv = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=1)\n\nscores = {fold:None for fold in range(cv.n_splits)}\npredictions = []\noof = np.zeros(len(X))\n\nwith tf_strategy.scope():\n    for fold, (idx_train, idx_valid) in enumerate(cv.split(X, y)):\n        X_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n        X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid]\n\n        model = get_model(name='Baseline')\n\n        model.compile(\n            keras.optimizers.Adam(learning_rate=0.001),\n            loss='binary_crossentropy',\n            metrics=['AUC']\n        )\n\n        print('**'*60)\n        print(f\"Fold {fold+1} | Now training ...\", end=\" \")\n\n        seed_everything(SEED+fold)\n\n        history = model.fit(\n            X_train, y_train,\n            validation_data=(X_valid, y_valid),\n            batch_size=1024,\n            epochs=1000,\n            verbose=0,\n            shuffle=True,\n            callbacks=[\n                early_stopping,\n                reduce_lr\n            ]\n        )\n        \n        plot_history(history, start=0, fold=fold)\n\n        scores[fold] = (history.history)\n        \n        print(f\"Best training AUC: {np.min(scores[fold]['auc']):0.5f}\")\n        print(f\"Best validation AUC: {np.min(scores[fold]['val_auc']):0.5f}\")\n        \n        oof[idx_valid] = np.ravel(model.predict(X_valid))\n        predictions.append(np.ravel(model.predict(X_test)))\n        \n        if (FEATURE_IMPORTANCE is True) and (fold==0):\n            # Feature importance\n            results = []\n            print('Computing DNN feature importance...')\n\n            # Compute baseline (no shuffle)\n            oof_preds = oof[idx_valid]\n            baseline = roc_auc_score(y_true=y_valid, y_score=oof_preds)\n            results.append({'feature':'BASELINE','roc_auc':baseline})           \n\n            cols = list(X.columns)\n            for k in range(len(cols)):\n\n                # Shuffle feature k\n                saved_col = X_valid.iloc[:, k].copy()\n                np.random.shuffle(X_valid.iloc[:,k].values)\n\n                # Computing OOF ROC-AUC with shuffled feature k\n                imp_oof_preds = model.predict(X_valid, batch_size=1024, verbose=0)\n                imp_roc_auc = roc_auc_score(y_true=y_valid, y_score=imp_oof_preds)\n                results.append({'feature':cols[k],'roc_auc':imp_roc_auc})\n\n                # Putting everything back as before\n                X_valid.iloc[:, k] = saved_col\n\n            # DISPLAY LSTM FEATURE IMPORTANCE\n            print()\n            df = pd.DataFrame(results)\n            df = df.sort_values('roc_auc', ascending=False)\n            plt.figure(figsize=(10, 20))\n            plt.barh(np.arange(len(cols)+1),df.roc_auc)\n            plt.yticks(np.arange(len(cols)+1),df.feature.values)\n            plt.title('DNN Feature Importance',size=16)\n            plt.ylim((-1, len(cols)+1))\n            plt.plot([baseline, baseline],[-1, len(cols)+1], '--', color='orange',\n                     label=f'Baseline OOF\\nROC-AUC={baseline:.3f}')\n            plt.xlabel(f'Fold {fold+1} OOF ROC-AUC with feature permuted',size=14)\n            plt.ylabel('Feature',size=14)\n            plt.legend()\n            plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-06T18:35:05.658187Z","iopub.execute_input":"2021-11-06T18:35:05.65866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### average predictions over each fold and create submission file\nsample_submission['target'] = np.mean(np.column_stack(predictions), axis=1)\nsample_submission.to_csv('./nn_baseline.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}