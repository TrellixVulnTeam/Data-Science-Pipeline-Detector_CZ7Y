{"cells":[{"metadata":{"_uuid":"148073b4f81822c8cee908a1fff8054ac84eb26c"},"cell_type":"markdown","source":"In this notebook, you will learn easy way to extract video and audio dataset from given data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport os","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"path_label_dict = os.path.join('../input', 'label_names_2018.csv')\n\npath_video_train = os.path.join('../input', 'video-sample', 'video', 'train00.tfrecord')\npath_video_test = os.path.join('../input', 'video-sample', 'video', 'train01.tfrecord')\n\npath_frame_train = os.path.join('../input', 'frame-sample', 'frame', 'train00.tfrecord')\npath_frame_test = os.path.join('../input', 'frame-sample', 'frame', 'train01.tfrecord')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0963ae478a647613f2d15961c4f103de302d5be4"},"cell_type":"markdown","source":"count the label size"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d5b4cfa08a7689f7b653b7fd1e543673c27b869a"},"cell_type":"code","source":"dfLabel = pd.read_csv(path_label_dict)\nnum_labels = len(dfLabel.label_name.unique())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4d7bb67b2242592b27aad18a0353834fffd65c9f"},"cell_type":"markdown","source":"pasing function for the video dataset.\n* the video dataset is single but not sequence, so I use fixed len to extract.\n* I use parse single as its name to parse the dataset.\n* I concatenate aforementioned dataset. \n\nNow, you ready to serve :D"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"39ba08dacb0375c225980cbc8c4447976c8bd64f"},"cell_type":"code","source":"def tfRecord_parse(record, num_labels, train=False):\n    features = {\n        'mean_rgb': tf.FixedLenFeature([1024], tf.float32),\n        'mean_audio': tf.FixedLenFeature([128], tf.float32)\n    }\n    if train:\n        features['labels'] = tf.VarLenFeature(tf.int64)\n    parsed = tf.parse_single_example(record, features)\n    x = tf.concat([parsed['mean_rgb'], parsed['mean_audio']], axis=0)\n    if train:\n        y = tf.sparse_to_dense(parsed['labels'].values, [num_labels], 1)\n        return x, y\n    return x","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"caf77fd12133d7ee9d98ecadb736f9fca12092aa"},"cell_type":"markdown","source":"In this part, it a little bit different because of sequential data.\n* Using FixedLenSequenceFeature when you want to extract sequence data, and passing to `sequence_features`\n* rgb, and audio are bytes list data, so have to use string as type to read.\n* as the given data length, I reshape the data\n* transform data type from uint8 to float32"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0f47370a13ec6e3af9b07037578df635a0a7c9b3"},"cell_type":"code","source":"def tfRecord_seq_parse(record, num_labels, train=False):\n    sequence_features = {\n        'rgb': tf.FixedLenSequenceFeature([], tf.string),\n        'audio': tf.FixedLenSequenceFeature([], tf.string)\n    }\n    context_features = {}\n    if train:\n        context_features['labels'] = tf.VarLenFeature(tf.int64)\n    ctx, parsed = tf.parse_single_sequence_example(record, context_features=context_features, sequence_features=sequence_features)\n    \n    decode_seq_rgb = tf.decode_raw(parsed['rgb'], tf.uint8)\n    decode_seq_rgb = tf.reshape(decode_seq_rgb, [-1, 1024])\n    decode_seq_rgb = tf.cast(decode_seq_rgb, dtype=tf.float32)\n    \n    decode_seq_audio = tf.decode_raw(parsed['audio'], tf.uint8)\n    decode_seq_audio = tf.reshape(decode_seq_audio, [-1, 128])\n    decode_seq_audio = tf.cast(decode_seq_audio, dtype=tf.float32)\n    \n    x = tf.concat([decode_seq_rgb, decode_seq_audio], axis=1)\n    if train:\n        y = tf.sparse_to_dense(ctx['labels'].values, [num_labels], 1)\n        return x, y\n    return x","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"267ac4103ee7e337d45c1bcdae2e2afd458f7ba8"},"cell_type":"markdown","source":"generate batch data"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f80809ca24a3517f9f8cb37b45b322c07d0e33f3"},"cell_type":"code","source":"def generate(path, num_labels, batch_size=1, train=False, isFrame=False, num_parallel_calls=12):\n    dataset = tf.data.TFRecordDataset(path)\n    if isFrame:\n        dataset = dataset.map(map_func=lambda x: tfRecord_seq_parse(x, num_labels, train=train), num_parallel_calls=num_parallel_calls)\n    else:\n        dataset = dataset.map(map_func=lambda x: tfRecord_parse(x, num_labels, train=train), num_parallel_calls=num_parallel_calls)\n    dataset = dataset.repeat(1000)\n    dataset = dataset.shuffle(buffer_size=1000)\n    dataset = dataset.batch(batch_size)\n    \n    return dataset.make_one_shot_iterator()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8e001b545bdba7b17369f38e362202e9bb9c8650"},"cell_type":"markdown","source":"testing"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7825f0332d00bf0ea13f68085bb9db2fb1032ffb"},"cell_type":"code","source":"video_train = generate(path_video_train, num_labels, train=True)\nframe_train = generate(path_frame_train, num_labels, train=True, isFrame=True)\nnext_video_val = video_train.get_next()\nnext_frame_val = frame_train.get_next()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c8f690fa0074e44950211f736ce482dfee7a6160"},"cell_type":"markdown","source":"video"},{"metadata":{"trusted":true,"_uuid":"d6272c331d97e4156dc20252829d172f33063a62"},"cell_type":"code","source":"with tf.Session() as session:\n    x, y = session.run(next_video_val)\nprint('x: {}, y: {}'.format(x.shape, y.shape))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9137d142406d66d83a9883bfb6b3afe0ae67b00e"},"cell_type":"markdown","source":"frame"},{"metadata":{"trusted":true,"_uuid":"5c8824a048bfa87bc6f6b499ac829772ad325783"},"cell_type":"code","source":"with tf.Session() as session:\n    x, y = session.run(next_frame_val)\nprint('x: {}, y: {}'.format(x.shape, y.shape))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}