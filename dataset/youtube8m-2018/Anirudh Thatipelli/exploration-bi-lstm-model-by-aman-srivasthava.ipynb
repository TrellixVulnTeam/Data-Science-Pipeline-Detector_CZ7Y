{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Reference: https://www.kaggle.com/amansrivastava/exploration-bi-lstm-model"},{"metadata":{},"cell_type":"markdown","source":"## Import libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport seaborn as sns\nfrom IPython.display import YouTubeVideo\nimport matplotlib.pyplot as plt\nimport plotly.plotly as py\nimport networkx as nx\nimport PIL\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# video level feature file\nprint(os.listdir(\"../input/video-sample/video/\"))\n# frame level features file\nprint(os.listdir(\"../input/frame-sample/frame/\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploratory data analysis "},{"metadata":{},"cell_type":"markdown","source":"### label_names_2018.csv contains the mapping between the label ids and the label names"},{"metadata":{"trusted":true},"cell_type":"code","source":"labels_df = pd.read_csv('../input/label_names_2018.csv', error_bad_lines= False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Total nubers of labels in sample dataset: %s\" %(len(labels_df['label_name'].unique())))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Check the vocabulary file. It has the description of the video indices and the descriptions for the videos."},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab = pd.read_csv('../input/vocabulary.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Exploring the video data"},{"metadata":{"trusted":true},"cell_type":"code","source":"video_files = [\"../input/video-sample/video/{}\".format(i) for i in os.listdir(\"../input/video-sample/video/\")]\nprint(video_files)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vid_ids = []\nlabels = []\nmean_rgb = []\nmean_audio = []","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Extract the values from the tfrecords"},{"metadata":{"trusted":true},"cell_type":"code","source":"for file in video_files:\n    for example in tf.python_io.tf_record_iterator(file):\n        tf_example = tf.train.Example.FromString(example)\n        \n        vid_ids.append(tf_example.features.feature['id'].bytes_list.value[0].decode(encoding='UTF-8'))\n        labels.append(tf_example.features.feature['labels'].int64_list.value)\n        mean_rgb.append(tf_example.features.feature['mean_rgb'].float_list.value)\n        mean_audio.append(tf_example.features.feature['mean_audio'].float_list.value)\n        \nprint('Number of videos in Sample data set: %s' % str(len(vid_ids)))\nprint('Picking a youtube video id: %s' % vid_ids[13])\nprint('List of label ids for youtube video id %s, are - %s' % (vid_ids[13], str(labels[13])))\nprint('First 20 rgb feature of a youtube video (',vid_ids[13],'): are - %s' % str(mean_rgb[13][:20]))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"len(vid_ids)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vid_ids","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_audio","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(mean_audio)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(mean_audio[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(mean_rgb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(mean_rgb[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Exploring the most common labels."},{"metadata":{},"cell_type":"markdown","source":"### Mapping the label ids with the label names"},{"metadata":{"trusted":true},"cell_type":"code","source":"labels_name = []\nfor row in labels:\n    n_labels = []\n    for label_id in row:\n        # some labels ids are missing so have put try/except\n        try:\n            n_labels.append(str(labels_df[labels_df['label_id']==label_id]['label_name'].values[0]))\n        except:\n            continue\n    labels_name.append(n_labels)\n\nprint('List of label names for youtube video id %s, are - %s' % (vid_ids[13], str(labels_name[13])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels_name[143]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Labels count dictionary"},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\nimport operator\n\nall_labels = []\nfor each in labels_name:\n    all_labels.extend(each)\n    \nlabels_count_dict = dict(Counter(all_labels))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels_count_dict","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Looking at the distribution of top 25 labels"},{"metadata":{"trusted":true},"cell_type":"code","source":"labels_count_df = pd.DataFrame.from_dict(labels_count_dict, orient= 'index').reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels_count_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels_count_df.columns = ['label', 'count']\nsorted_labels_count_df = labels_count_df.sort_values('count', ascending= False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted_labels_count_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Game label has the most number of examples while Recipe has the least"},{"metadata":{"trusted":true},"cell_type":"code","source":"TOP = 25\nTOP_labels = list(sorted_labels_count_df['label'])[:TOP]\nfig, ax = plt.subplots(figsize=(10, 7))\nsns.barplot(y='label', x='count', data=sorted_labels_count_df.iloc[0:TOP, :])\nplt.title('Top {} labels with sample count'.format(TOP))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Within these top 25 labels, explore the most common ones"},{"metadata":{"trusted":true},"cell_type":"code","source":"common_occur_top_label_dict = {}\nfor row in labels_name:\n    for label in row:\n        if label in TOP_labels:\n            c_labels = [label + \"|\" + x for x in row if x != label]\n            for c_label in c_labels:\n                common_occur_top_label_dict[c_label] = common_occur_top_label_dict.get(c_label, 0) + 1\n                \n# Putting these into a dataframe\ncommon_occur_top_label_df = pd.DataFrame.from_dict(common_occur_top_label_dict, orient= 'index').reset_index()\ncommon_occur_top_label_df.columns = ['common_label', 'count']\nsorted_common_occur_top_label_df = common_occur_top_label_df.sort_values('count', ascending=False)\n\n\n# plotting 25 common occurs labels from top labels\nTOP = 25\nfig, ax = plt.subplots(figsize=(10,7))\nsns.barplot(y='common_label', x='count', data=sorted_common_occur_top_label_df.iloc[0:TOP, :])\nplt.title('Top {} common occur labels with sample count'.format(TOP))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted_common_occur_top_label_df.head","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The Video Game | Game is the highest occuring label in the data"},{"metadata":{},"cell_type":"markdown","source":"## Creating a Network Graph to explore the relations amongst the TOP labels"},{"metadata":{"trusted":true},"cell_type":"code","source":"top_coocurance_label_dict = {}\nfor row in labels_name:\n    for label in row:\n        if label in TOP_labels:\n            top_label_siblings = [x for x in row if x != label]\n            for sibling in top_label_siblings:\n                if label not in top_coocurance_label_dict:\n                    top_coocurance_label_dict[label] = {}\n                top_coocurance_label_dict[label][sibling] = top_coocurance_label_dict.get(label, {}).get(sibling, 0) + 11","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from_label = []\nto_label = []\nvalue = []\nfor key, val in top_coocurance_label_dict.items():\n    for key2, val2 in val.items():\n        from_label.append(key)\n        to_label.append(key2)\n        value.append(val2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame({ 'from': from_label, 'to': to_label, 'value': value})\nsorted_df = df.sort_values('value', ascending=False)\nsorted_df = sorted_df.iloc[:50, ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"node_colors = ['turquoise', 'turquoise', 'green', 'crimson', 'grey', 'turquoise', 'turquoise', \n'grey', 'skyblue', 'crimson', 'yellow', 'green', 'turquoise', \n'skyblue', 'skyblue', 'green', 'green', 'lightcoral', 'grey', 'yellow', \n'turquoise', 'skyblue', 'orange', 'green', 'skyblue', 'green', 'turquoise']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(node_colors)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create a graph using the columns of the dataframe df"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = sorted_df\nG= nx.from_pandas_edgelist(df, 'from', 'to', 'value', create_using=nx.Graph())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10,10))\nnx.draw(G, pos=nx.circular_layout(G), node_size=1000, with_labels=True, node_color=node_colors)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Position the nodes on a circle"},{"metadata":{"trusted":true},"cell_type":"code","source":"nx.draw_networkx_edge_labels(G, pos=nx.circular_layout(G), edge_labels=nx.get_edge_attributes(G, 'value'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.title('Network graph representing the co-occurance between the categories', size=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = sorted_df\nG= nx.from_pandas_edgelist(df, 'from', 'to', 'value', create_using=nx.Graph())\nplt.figure(figsize = (10,10))\nnx.draw(G, pos=nx.circular_layout(G), node_size=1000, with_labels=True, node_color=node_colors)\nnx.draw_networkx_edge_labels(G, pos=nx.circular_layout(G), edge_labels=nx.get_edge_attributes(G, 'value'))\nplt.title('Network graph representing the co-occurance between the categories', size=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploring the frame level of the video."},{"metadata":{"trusted":true},"cell_type":"code","source":"frame_files = [\"../input/frame-sample/frame/{}\".format(i) for i in os.listdir(\"../input/frame-sample/frame/\")]\nfeat_rgb = []\nfeat_audio = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"frame_files","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for file in frame_files:\n    for example in tf.python_io.tf_record_iterator(file):        \n        tf_seq_example = tf.train.SequenceExample.FromString(example)\n        n_frames = len(tf_seq_example.feature_lists.feature_list['audio'].feature)\n        sess = tf.InteractiveSession()\n        rgb_frame = []\n        audio_frame = []\n        # iterate through frames\n        for i in range(n_frames):\n            rgb_frame.append(tf.cast(tf.decode_raw(\n                    tf_seq_example.feature_lists.feature_list['rgb'].feature[i].bytes_list.value[0],tf.uint8)\n                           ,tf.float32).eval())\n            audio_frame.append(tf.cast(tf.decode_raw(\n                    tf_seq_example.feature_lists.feature_list['audio'].feature[i].bytes_list.value[0],tf.uint8)\n                           ,tf.float32).eval())\n\n\n        sess.close()\n        feat_rgb.append(rgb_frame)\n        feat_audio.append(audio_frame)\n        break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_audio","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_rgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"No. of videos %d\" % len(feat_rgb))\nprint('The first video has %d frames' %len(feat_rgb[0]))\nprint(\"Max frame length is: %d\" % max([len(x) for x in feat_rgb]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Bi-LSTM Multilabel classification"},{"metadata":{},"cell_type":"markdown","source":"#### Since the frames are sequential in nature, we use a LSTM to extract this type of information and merge this with the video data. These will be passed through the sigmoid layer with units equal to the number of features."},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Dense, Input, LSTM, Dropout, Bidirectional\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.merge import concatenate\nfrom keras.callbacks import TensorBoard\nfrom keras.models import load_model\nfrom keras.models import Model\nfrom keras.utils.vis_utils import plot_model\nimport operator\nimport time \nimport gc\nimport os","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating a train, dev test by combining video_rgb, video_audio, frame_rgb, frame_audio and labels"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_video_rgb = []\ntrain_video_audio = []\ntrain_frame_rgb = []\ntrain_frame_audio = []\ntrain_labels = []\n\nval_video_rgb = []\nval_video_audio = []\nval_frame_rgb = []\nval_frame_audio = []\nval_labels = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_train_dev_dataset(video_rgb, video_audio, frame_rgb, frame_audio, labels):\n    shuffle_indices = np.random.permutation(np.arange(len(labels)))\n    video_rgb_shuffled = video_rgb[shuffle_indices]\n    video_audio_shuffled = video_audio[shuffle_indices]\n    frame_rgb_shuffled = frame_rgb[shuffle_indices]\n    frame_audio_shuffled = frame_audio[shuffle_indices]\n    labels_shuffled = labels[shuffle_indices]\n    \n    dev_idx = max(1, int(len(labels_shuffled) * validation_split_ratio))\n    \n#     del video_rgb\n#     del video_audio\n#     del frame_rgb\n#     del frame_audio\n#     gc.collect()\n    \n    train_video_rgb, val_video_rgb = video_rgb_shuffled[:-dev_idx], video_rgb_shuffled[-dev_idx:]\n    train_video_audio, val_video_audio = video_audio_shuffled[:-dev_idx], video_audio_shuffled[-dev_idx:]\n    \n    train_frame_rgb, val_frame_rgb = frame_rgb_shuffled[:-dev_idx], frame_rgb_shuffled[-dev_idx:]\n    train_frame_audio, val_frame_audio = frame_audio_shuffled[:-dev_idx], frame_audio_shuffled[-dev_idx:]\n    \n    train_labels, val_labels = labels_shuffled[:-dev_idx], labels_shuffled[-dev_idx:]\n    \n    del video_rgb_shuffled, video_audio_shuffled, frame_rgb_shuffled, frame_audio_shuffled, labels_shuffled\n    gc.collect()\n    \n    return (train_video_rgb, train_video_audio, train_frame_rgb, train_frame_audio, train_labels, val_video_rgb, \n            val_video_audio, val_frame_rgb, val_frame_audio, val_labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Defining the Model architecture"},{"metadata":{"trusted":true},"cell_type":"code","source":"max_frame_rgb_sequence_length = 10\nframe_rgb_embedding_size = 1024\n\nmax_frame_audio_sequence_length = 10\nframe_audio_embedding_size = 128\n\nnumber_dense_units = 1000\nnumber_lstm_units = 100\nrate_drop_lstm = 0.2\nrate_drop_dense = 0.2\nactivation_function='relu'\nvalidation_split_ratio = 0.2\nlabel_feature_size = 10","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating a dataset of random values having the same size and dimension as the training data set to test the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_length = 1000\n\nvideo_rgb = np.random.rand(sample_length, 1024)\nvideo_audio = np.random.rand(sample_length, 128)\n\nframe_rgb = np.random.rand(sample_length, 10, 1024)\nframe_audio = np.random.rand(sample_length, 10, 128)\n\n# Here I have considered i have only 10 labels\nlabels = np.zeros([sample_length,10])\nfor i in range(len(labels)):\n    j = random.randint(0,9)\n    labels[i][j] = 1 ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Using checkpoint to store the best model and use it for future"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model(video_rgb, video_audio, frame_rgb, frame_audio, labels):\n    \"\"\"Create and store best model at `checkpoint` path ustilising bi-lstm layer for frame level data of videos\"\"\"\n    train_video_rgb, train_video_audio, train_frame_rgb, train_frame_audio, train_labels, val_video_rgb, val_video_audio, val_frame_rgb, val_frame_audio, val_labels = create_train_dev_dataset(video_rgb, video_audio, frame_rgb, frame_audio, labels) \n    \n    # Creating 2 bi-lstm layer, one for rgb and other for audio level data\n    lstm_layer_1 = Bidirectional(LSTM(number_lstm_units, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm))\n    lstm_layer_2 = Bidirectional(LSTM(number_lstm_units, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm))\n    \n    # creating input layer for frame-level data\n    frame_rgb_sequence_input = Input(shape=(max_frame_rgb_sequence_length, frame_rgb_embedding_size), dtype='float32')\n    frame_audio_sequence_input = Input(shape=(max_frame_audio_sequence_length, frame_audio_embedding_size), dtype='float32')\n    \n    frame_x1 = lstm_layer_1(frame_rgb_sequence_input)\n    frame_x2 = lstm_layer_2(frame_audio_sequence_input)\n    \n    # creating input layer for video-level data\n    video_rgb_input = Input(shape=(video_rgb.shape[1],))\n    video_rgb_dense = Dense(int(number_dense_units/2), activation=activation_function)(video_rgb_input)\n    \n    video_audio_input = Input(shape=(video_audio.shape[1],))\n    video_audio_dense = Dense(int(number_dense_units/2), activation=activation_function)(video_audio_input)\n    \n    # merging frame-level bi-lstm output and later passed to dense layer by applying batch-normalisation and dropout\n    merged_frame = concatenate([frame_x1, frame_x2])\n    merged_frame = BatchNormalization()(merged_frame)\n    merged_frame = Dropout(rate_drop_dense)(merged_frame)\n    merged_frame_dense = Dense(int(number_dense_units/2), activation=activation_function)(merged_frame)\n    \n    # merging video-level dense layer output\n    merged_video = concatenate([video_rgb_dense, video_audio_dense])\n    merged_video = BatchNormalization()(merged_video)\n    merged_video = Dropout(rate_drop_dense)(merged_video)\n    merged_video_dense = Dense(int(number_dense_units/2), activation=activation_function)(merged_video)\n    \n    # merging frame-level and video-level dense layer output\n    merged = concatenate([merged_frame_dense, merged_video_dense])\n    merged = BatchNormalization()(merged)\n    merged = Dropout(rate_drop_dense)(merged)\n     \n    merged = Dense(number_dense_units, activation=activation_function)(merged)\n    merged = BatchNormalization()(merged)\n    merged = Dropout(rate_drop_dense)(merged)\n    preds = Dense(label_feature_size, activation='sigmoid')(merged)\n    \n    model = Model(inputs=[frame_rgb_sequence_input, frame_audio_sequence_input, video_rgb_input, video_audio_input], outputs=preds)\n    print(model.summary())\n    model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['acc'])\n    early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n    \n    STAMP = 'lstm_%d_%d_%.2f_%.2f' % (number_lstm_units, number_dense_units, rate_drop_lstm, rate_drop_dense)\n\n    checkpoint_dir = 'checkpoints/' + str(int(time.time())) + '/'\n\n    if not os.path.exists(checkpoint_dir):\n        os.makedirs(checkpoint_dir)\n\n    bst_model_path = checkpoint_dir + STAMP + '.h5'\n    model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=False)\n    tensorboard = TensorBoard(log_dir=checkpoint_dir + \"logs/{}\".format(time.time()))\n    \n    model.fit([train_frame_rgb, train_frame_audio, train_video_rgb, train_video_audio], train_labels,\n              validation_data=([val_frame_rgb, val_frame_audio, val_video_rgb, val_video_audio], val_labels),\n              epochs=200, batch_size=64, shuffle=True, callbacks=[early_stopping, model_checkpoint, tensorboard])    \n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training model"},{"metadata":{"trusted":true},"cell_type":"code","source":"len(video_audio)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = create_model(video_audio=video_audio, video_rgb=video_rgb, frame_audio=frame_audio, frame_rgb=frame_rgb, \n                     labels = labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_img = plt.imread('model_plot.png')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}