{"cells":[{"metadata":{"id":"b7OkxS36u3_C"},"cell_type":"markdown","source":"# References\n\n- The competition is https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition (it ended long time ago)\n- Pytorch official docs:\n    - [Finetuning Torchvision Models](https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html)\n    - [Writing Custom Datasets, DataLoaders and Transforms](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html)\n    - Any other docs refered to train model or whatever, left comments on each section.\n- Refered kernels:\n    - [Dog_vs_Cat Transfer Learning - VGG16 by Pytorch](https://www.kaggle.com/bootiu/dog-vs-cat-transfer-learning-vgg16-by-pytorch)","execution_count":null},{"metadata":{"id":"BtnC7L1DjJJe"},"cell_type":"markdown","source":"# Import libraries","execution_count":null},{"metadata":{"id":"EIYPRiPDTI71","trusted":true},"cell_type":"code","source":"# Standard library\nimport copy\nimport glob\nimport multiprocessing\nimport os\nimport time\nimport zipfile\n\n# Pytorch\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import models, transforms\n\n# Related third party\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom skimage import io, transform\nfrom sklearn.model_selection import train_test_split\nfrom tqdm.notebook import tqdm","execution_count":null,"outputs":[]},{"metadata":{"id":"jYmov1HJIDCJ"},"cell_type":"markdown","source":"# Pre process for each environment","execution_count":null},{"metadata":{"id":"ZI4kPwGkH8ID"},"cell_type":"markdown","source":"### For Kaggle kernel","execution_count":null},{"metadata":{"id":"r1Gm9UbE2HVS","trusted":true},"cell_type":"code","source":"base_dir = '../input/dogs-vs-cats-redux-kernels-edition'\nwith zipfile.ZipFile(os.path.join(base_dir, 'train.zip')) as train_zip:\n    train_zip.extractall('../data')\nwith zipfile.ZipFile(os.path.join(base_dir, 'test.zip')) as test_zip:\n    test_zip.extractall('../data')\n\ntrain_dir = '../data/train'\ntest_dir = '../data/test'","execution_count":null,"outputs":[]},{"metadata":{"id":"EEdVm4F1H-tC"},"cell_type":"markdown","source":"### For Google Colab\n\nIt assumes that downloaded data will be located under `'My Drive/Data Set/'`","execution_count":null},{"metadata":{"id":"axQSzMXeH-Wz","trusted":true},"cell_type":"code","source":"# with zipfile.ZipFile('./drive/My Drive/Data Set/dogs-vs-cats-redux-kernels-edition.zip') as entire_zip:\n#     entire_zip.extractall('.')\n# with zipfile.ZipFile('./train.zip') as train_zip:\n#     train_zip.extractall('.')\n# with zipfile.ZipFile('./test.zip') as test_zip:\n#     test_zip.extractall('.')\n\n# train_dir = './train'\n# test_dir = './test'","execution_count":null,"outputs":[]},{"metadata":{"id":"W91auWwe1M6R"},"cell_type":"markdown","source":"# Global Declarations","execution_count":null},{"metadata":{"id":"AAit-F1ijnY_"},"cell_type":"markdown","source":"## Constants\n\nRegarding `input_size`, `mean` and `std`, all pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using `mean = [0.485, 0.456, 0.406]` and `std = [0.229, 0.224, 0.225]`. See [torchvision.models](https://pytorch.org/docs/stable/torchvision/models.html) for details. ","execution_count":null},{"metadata":{"id":"giTRYBjlZQQM","trusted":true},"cell_type":"code","source":"input_size = 224\nmean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]\n\n# Number of classes in the dataset\nnum_classes = 2 # dog, cat\n\n# Batch size for training (change depending on how much memory you have)\nbatch_size = 32\n\n# Number of epochs to train for\nnum_epochs = 2\n\n# Flag for feature extracting. When False, we finetune the whole model,\n#   when True we only update the reshaped layer params\nfeature_extract = True\n\n# Switch to perform multi-process data loading\nnum_workers = multiprocessing.cpu_count()","execution_count":null,"outputs":[]},{"metadata":{"id":"SQkSsiGQjRUR"},"cell_type":"markdown","source":"## Helper Functions","execution_count":null},{"metadata":{"id":"J5MyzmeIfQU4","trusted":true},"cell_type":"code","source":"# train data file looks './train/dog.10435.jpg'\n# test data file looks './test/10435.jpg'\ndef extract_class_from(path):\n    file = path.split('/')[-1]\n    return file.split('.')[0]","execution_count":null,"outputs":[]},{"metadata":{"id":"MYI0tW1Cx6Dg"},"cell_type":"markdown","source":"This `train_model` function comes from https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html#model-training-and-validation-code.","execution_count":null},{"metadata":{"id":"lBTPEedns0ZV","trusted":true},"cell_type":"code","source":"def train_model(model, dataloaders, criterion, optimizer, num_epochs=25):\n    since = time.time()\n\n    history = {'accuracy': [],\n               'val_accuracy': [],\n               'loss': [],\n               'val_loss': []}\n\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n        print('-' * 10)\n\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()  # Set model to training mode\n            else:\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.0\n            running_corrects = 0\n\n            # Iterate over data.\n            for inputs, labels in tqdm(dataloaders[phase]):\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                # forward\n                # track history if only in train\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    loss = criterion(outputs, labels)\n\n                    _, preds = torch.max(outputs, 1)\n\n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n\n                # statistics\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n\n            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n\n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n\n            # deep copy the model\n            if phase == 'val' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n\n            if phase == 'train':\n                history['accuracy'].append(epoch_acc.item())\n                history['loss'].append(epoch_loss)\n            else:\n                history['val_accuracy'].append(epoch_acc.item())\n                history['val_loss'].append(epoch_loss) \n\n        print()\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n    print('Best val Acc: {:4f}'.format(best_acc))\n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model, history","execution_count":null,"outputs":[]},{"metadata":{"id":"bOUrAeG5X5pt"},"cell_type":"markdown","source":"# Load Data","execution_count":null},{"metadata":{"id":"bLNGKWkeX7Xt","trusted":true},"cell_type":"code","source":"all_train_files = glob.glob(os.path.join(train_dir, '*.jpg'))\ntrain_list, val_list = train_test_split(all_train_files, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"id":"6gcueQhGfVpb","outputId":"318760a0-c18e-4314-bb74-9bc3fe879046","trusted":true},"cell_type":"code","source":"print(len(train_list))\nprint(len(val_list))","execution_count":null,"outputs":[]},{"metadata":{"id":"ql8hbhRVv9ot"},"cell_type":"markdown","source":"## Check what train data looks like","execution_count":null},{"metadata":{"id":"DUE_m0dMwBH2","outputId":"2fe6ef81-d235-4608-8c9d-080943522263","trusted":false},"cell_type":"code","source":"fig, axes = plt.subplots(nrows=2,\n                         ncols=3,\n                         figsize=(18, 12))\nfor img_path, ax in zip(train_list, axes.ravel()):\n    ax.set_title(img_path)\n    ax.imshow(Image.open(img_path))","execution_count":null,"outputs":[]},{"metadata":{"id":"RRE7mKb1b3SW"},"cell_type":"markdown","source":"## Dataset class\n\nSee https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class for details.","execution_count":null},{"metadata":{"id":"2VFbJ_RahO0T","trusted":true},"cell_type":"code","source":"class DogVsCatDataset(Dataset):\n  \n    def __init__(self, file_list, transform=None):\n        self.file_list = file_list\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.file_list)\n  \n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n       \n        img_name = self.file_list[idx]\n        image = Image.open(img_name)\n        if self.transform:\n            image = self.transform(image)\n    \n        label_category = extract_class_from(img_name)\n        label = 1 if label_category == 'dog' else 0\n    \n        return image, label","execution_count":null,"outputs":[]},{"metadata":{"id":"TaDWc8tobuZ_"},"cell_type":"markdown","source":"## Create dataloaders\n\nSee https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html#load-data for details.","execution_count":null},{"metadata":{"id":"lNaNM8X-Qgks","trusted":false},"cell_type":"code","source":"data_transforms = {\n    'train': transforms.Compose([\n        transforms.RandomResizedCrop(input_size, scale=(0.5, 1.0)),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize(mean, std)\n    ]),\n    'val': transforms.Compose([\n        transforms.Resize(input_size),\n        transforms.CenterCrop(input_size),\n        transforms.ToTensor(),\n        transforms.Normalize(mean, std)\n    ])\n}","execution_count":null,"outputs":[]},{"metadata":{"id":"JI12IHfujr_Y","trusted":true},"cell_type":"code","source":"# Create training and validation datasets\nimage_datasets = {\n    'train': DogVsCatDataset(train_list,\n                             transform=data_transforms['train']),\n    'val': DogVsCatDataset(val_list,\n                           transform=data_transforms['val'])\n}\n\n# Create training and validation dataloaders\ndataloaders_dict = {x: DataLoader(image_datasets[x],\n                                  batch_size=batch_size,\n                                  shuffle=True,\n                                  num_workers=num_workers) for x in ['train', 'val']}\n\n# Detect if we have a GPU available\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","execution_count":null,"outputs":[]},{"metadata":{"id":"zdXNdTZwcDVQ"},"cell_type":"markdown","source":"# Initialize and Reshape the Networks\n\nRegarding tuning VGG, see https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html#vgg.\n\n","execution_count":null},{"metadata":{"id":"jRY3V7P-0D_K","trusted":true},"cell_type":"code","source":"model_ft = models.vgg16(pretrained=True)\nmodel_ft.classifier[6] = nn.Linear(4096, num_classes)","execution_count":null,"outputs":[]},{"metadata":{"id":"do0wSVBeLYr5"},"cell_type":"markdown","source":"# Create the Optimizer\n\nSee https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html#create-the-optimizer for details.","execution_count":null},{"metadata":{"id":"ko6DPWZ8xhKZ","trusted":true,"outputId":"e77c16f6-e6fa-473f-eafa-9c2d484da1a3"},"cell_type":"code","source":"# Send the model to GPU\nmodel_ft = model_ft.to(device)\n\n# Gather the parameters to be optimized/updated in this run. If we are\n#  finetuning we will be updating all parameters. However, if we are\n#  doing feature extract method, we will only update the parameters\n#  that we have just initialized, i.e. the parameters with requires_grad\n#  is True.\nparams_to_update = model_ft.parameters()\nprint(\"Params to learn:\")\nif feature_extract:\n    params_to_update = []\n    for name,param in model_ft.named_parameters():\n        if param.requires_grad == True:\n            params_to_update.append(param)\n            print(\"\\t\",name)\nelse:\n    for name,param in model_ft.named_parameters():\n        if param.requires_grad == True:\n            print(\"\\t\",name)\n\n# Observe that all parameters are being optimized\noptimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)","execution_count":null,"outputs":[]},{"metadata":{"id":"MNuqAgyBdeIs"},"cell_type":"markdown","source":"# Run Training and Validation Step","execution_count":null},{"metadata":{"id":"QFceXYTey2FI","trusted":true,"outputId":"a08c06dd-62ae-4b01-b29d-50535d8f2288"},"cell_type":"code","source":"# Setup the loss fxn\ncriterion = nn.CrossEntropyLoss()\n\n# Train and evaluate\nmodel_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs)","execution_count":null,"outputs":[]},{"metadata":{"id":"SuaL38c3E1Wr"},"cell_type":"markdown","source":"## Visualize training results\n\nThis procedure comes from https://www.tensorflow.org/tutorials/images/classification#visualize_training_results.","execution_count":null},{"metadata":{"id":"lCzmHtEfEwfc","trusted":true,"outputId":"e9138e5f-748f-491d-90f2-54b0fe209f90"},"cell_type":"code","source":"acc = hist['accuracy']\nval_acc = hist['val_accuracy']\nloss = hist['loss']\nval_loss = hist['val_loss']\nepochs_range = range(num_epochs)\n\nplt.figure(figsize=(24, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"toHjy4Iajf3V"},"cell_type":"markdown","source":"# Predict","execution_count":null},{"metadata":{"id":"Sea_fIAsTyK-","trusted":true,"outputId":"0c6458e3-db80-438f-a073-c0eb4aab077f"},"cell_type":"code","source":"test_list = glob.glob(os.path.join(test_dir, '*.jpg'))\ntest_data_transform = data_transforms['val']\n\nids = []\nlabels = []\n\nwith torch.no_grad():\n    for test_path in tqdm(test_list):\n        img = Image.open(test_path)\n        img = test_data_transform(img)\n        img = img.unsqueeze(0)\n        img = img.to(device)\n\n        model_ft.eval()\n        outputs = model_ft(img)\n        preds = F.softmax(outputs, dim=1)[:, 1].tolist()\n\n        test_id = extract_class_from(test_path)\n        ids.append(int(test_id))\n        labels.append(preds[0])","execution_count":null,"outputs":[]},{"metadata":{"id":"NkKIJbYo5rKy"},"cell_type":"markdown","source":"## Check how well the prediction went","execution_count":null},{"metadata":{"id":"-WhwOwuN6X-_","outputId":"6dae7037-8584-4ee9-aa8d-cfdf3af61101","trusted":false},"cell_type":"code","source":"template = '\"{}\" with {:.2%} confidence'\ndef pred_result_message(pred):\n    if pred > 0.5:\n        return template.format('dog', pred)\n    else:\n        return template.format('cat', 1 - pred)\n\nfig, axes = plt.subplots(nrows=2,\n                         ncols=3,\n                         figsize=(18, 12))\nfor img_path, label, ax in zip(test_list, labels, axes.ravel()):\n    ax.set_title(pred_result_message(label))\n    ax.imshow(Image.open(img_path))","execution_count":null,"outputs":[]},{"metadata":{"id":"EEBvETUujpZb"},"cell_type":"markdown","source":"# Generate submittion.csv","execution_count":null},{"metadata":{"id":"S0hI7a9GgLM_","trusted":true},"cell_type":"code","source":"output = pd.DataFrame({'id': ids,\n                       'label': np.round(labels)})\n\noutput.sort_values(by='id', inplace=True)\noutput.reset_index(drop=True, inplace=True)\n\noutput.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}