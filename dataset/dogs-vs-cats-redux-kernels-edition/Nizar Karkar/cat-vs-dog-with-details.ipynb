{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-11T19:12:50.483718Z","iopub.execute_input":"2022-04-11T19:12:50.484115Z","iopub.status.idle":"2022-04-11T19:12:50.510965Z","shell.execute_reply.started":"2022-04-11T19:12:50.484024Z","shell.execute_reply":"2022-04-11T19:12:50.51023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I'm using in this competition CNN.First of all let's understand what is CNN ?.In deep learning, a convolutional neural network (CNN/ConvNet) is a class of deep neural networks, most commonly applied to analyze visual imagery.CNN is used to classify images or identify patterns between them ","metadata":{}},{"cell_type":"markdown","source":"To begin with we extract the data from the Zipped files in order to train our model.In deep learning, a convolutional neural network (CNN/ConvNet) is a class of deep neural networks, most commonly applied to analyze visual imagery.CNN is mainly used to recognize pattern betwwen images and classify them.As images move through a convolutional network, different patterns are recognised just like a normal neural network. But here rather than focussing on one pixel at a time, a convolutional net takes in square patches of pixels and passes them through a filter. That filter is also a square matrix smaller than the image itself, and equal in size to the patch. It is also called a kernel.","metadata":{}},{"cell_type":"markdown","source":"I used for this competition basic CNN model to classify images of cats and dogs .So first of let's define what is CNN technique.","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport cv2\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Dropout, Activation, Conv2D, MaxPooling2D\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\nimport zipfile\n\nwith zipfile.ZipFile(\"../input/dogs-vs-cats-redux-kernels-edition/train.zip\",\"r\") as z:\n    z.extractall(\".\")\n    \nwith zipfile.ZipFile(\"../input/dogs-vs-cats-redux-kernels-edition/test.zip\",\"r\") as z:\n    z.extractall(\".\")","metadata":{"execution":{"iopub.status.busy":"2022-04-09T15:40:14.861261Z","iopub.execute_input":"2022-04-09T15:40:14.861802Z","iopub.status.idle":"2022-04-09T15:40:40.11232Z","shell.execute_reply.started":"2022-04-09T15:40:14.861768Z","shell.execute_reply":"2022-04-09T15:40:40.111484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After that we iterate throw all images in train folder and then we split our images into two categories one for dog's images and the other for the cat'images \nto make our model learn better we will refere to dag for 1 and the cat for zeros .\nNow every image is actually a set of pixels so how to get our computer know that. Its simple convert all those pixels into an array. So we are going to use here a cv2 library to read our image into an array and also it will read as a gray scale image.\n\ncv2.imread(os.path.join(path,p),cv2.IMREAD_GRAYSCALE)\n\nNow we have got here images of all sizes . We have landscape, portrait etc etc.. We need to make them all of a single size so it can be analysed pretty easily. How to do that very very simple again. Use cv2\n\ncv2.resize(img_array, dsize=(80, 80))\n\nOk so we have got image array and its resized but do you believe whatever I just did was correct. Was the resizing of 80 X 80 good or is it bad. Should check it. How can we do that. There is one answer matplotlib. Using the below code we can display the image.\n\nplt.imshow(new_img_array,cmap=\"gray\")","metadata":{}},{"cell_type":"code","source":"main_dir = \"/kaggle/working/\"\ntrain_dir = \"train\"\npath=os.path.join(main_dir,train_dir)\nfor p in os.listdir(path):\n    category=p.split(\".\")[0]\n    img_array = cv2.imread(os.path.join(path,p),cv2.IMREAD_GRAYSCALE)\n    new_img_array=cv2.resize(img_array,dsize=(80,80))\n    plt.imshow(new_img_array,cmap='gray')\n    break ","metadata":{"execution":{"iopub.status.busy":"2022-04-09T15:40:40.1142Z","iopub.execute_input":"2022-04-09T15:40:40.114524Z","iopub.status.idle":"2022-04-09T15:40:40.361777Z","shell.execute_reply.started":"2022-04-09T15:40:40.114482Z","shell.execute_reply":"2022-04-09T15:40:40.360774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then we declare our training array X and target Y .X will be the array of pixels and Y the array of 0 and 1 . It similar to a classic classification problem","metadata":{}},{"cell_type":"code","source":"X=[]\ny=[]\nconvert=lambda category:int(category=='dog')\nfor p in os.listdir(path):\n    category=p.split('.')[0]\n    category=convert(category)\n    img_array= cv2.imread(os.path.join(path,p),cv2.IMREAD_GRAYSCALE)\n    new_img_array=cv2.resize(img_array,dsize=(80,80))\n    X.append(new_img_array)\n    y.append(category)\n    ","metadata":{"execution":{"iopub.status.busy":"2022-04-09T15:40:40.363119Z","iopub.execute_input":"2022-04-09T15:40:40.363321Z","iopub.status.idle":"2022-04-09T15:41:09.093839Z","shell.execute_reply.started":"2022-04-09T15:40:40.363296Z","shell.execute_reply":"2022-04-09T15:41:09.092608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = np.array(X).reshape(-1, 80,80,1)\ny = np.array(y)","metadata":{"execution":{"iopub.status.busy":"2022-04-09T15:41:09.09702Z","iopub.execute_input":"2022-04-09T15:41:09.097326Z","iopub.status.idle":"2022-04-09T15:41:09.215073Z","shell.execute_reply.started":"2022-04-09T15:41:09.097293Z","shell.execute_reply":"2022-04-09T15:41:09.214091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We devide our training array toi 255 in order to make our model learn faster and more effecient .It's called scaling the training data ","metadata":{}},{"cell_type":"code","source":"X=X/255.0","metadata":{"execution":{"iopub.status.busy":"2022-04-09T15:41:09.216271Z","iopub.execute_input":"2022-04-09T15:41:09.216515Z","iopub.status.idle":"2022-04-09T15:41:09.813404Z","shell.execute_reply.started":"2022-04-09T15:41:09.216485Z","shell.execute_reply":"2022-04-09T15:41:09.81244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's pass to training our model following these steps \n\nDefine a Sequential model\nStart adding layers to it.\nFirst we will add a Conv2D layer with 64 nodes and kernel size of (3,3). You can also experiment with different values here like 32, 128 etc. Also we have to specify input shape which is your X shape. Activation we will take 'relu' for now however there are many others to experiment with.\nNow after every Conv layer we always do max pooling so we will add max pooling layer with a size of (2,2)\nWe will repeat this combination again because come on 2 is better than one. Haha. We you can also add 3 or more convolution layers but keep in mind the more layers you add more time it will take to train.\nBut we don't have much time so we will add a flatten layer now. As we have to feed our data to Dense layer later.\nWe will now add a Dense layer of 64 nodes. Note for all these layers we are using activation as 'relu' because I found results better with this. You can skip specifying activation but this might make a model a conveniently linear which might not work for us.\nIn the end for getting our result we will add final Dense layer . Activation can be sigmoid or softmax (if you need probability use sigmoid else use softmax). Here I have used sigmoid.\nFinally we will compile the model . There are 3 things to mention here . Loss, Optimizer, Metrics\nLoss :- To make our model better we either minimize loss or maximize accuracy. NN always minimize loss. To measure it we can use different formulas like 'categorical_crossentropy' or 'binary_crossentropy'. Here I have used binary_crossentropy\n\nOptimizer :- If you know a lil bit about mathematics of machine learning you might be familier with local minima or global minima or cost function. To minimize cost function we use different methods For ex :- like gradient descent, stochastic gradient descent. So these are call optimizers. We are using a default one here which is adam\n\nMetrics :- This is to denote the measure of your model. Can be accuracy or some other metric.","metadata":{}},{"cell_type":"code","source":"model=Sequential()\n\nmodel.add(Conv2D(64,(3,3),activation='relu',input_shape=X.shape[1:]))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\n#add another\nmodel.add(Conv2D(64,(3,3),activation='relu',input_shape=X.shape[1:]))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\n\nmodel.add(Flatten())\nmodel.add(Dense(64,activation='relu'))\n#Add a softmax layer with 10 output units\nmodel.add(Dense(1,activation='sigmoid'))\n\n\nmodel.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2022-04-09T15:41:09.814692Z","iopub.execute_input":"2022-04-09T15:41:09.814937Z","iopub.status.idle":"2022-04-09T15:41:10.026121Z","shell.execute_reply.started":"2022-04-09T15:41:09.814907Z","shell.execute_reply":"2022-04-09T15:41:10.025057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"now we will fit our model to the training data \nepochs is how many times we train our model \nthe more epochs we have the better result we will get \nbut 10 epochs is enought because it takes too long to run \nBatch size :- How much amount of data at once you wanna pass through the model\n\nvalidation_split :- How much amount of data (in this case its 20 %) you will need to check cross validation error\n\n","metadata":{}},{"cell_type":"code","source":"model.fit(X,y,epochs=10,batch_size=32,validation_split=0.2)","metadata":{"execution":{"iopub.status.busy":"2022-04-09T15:41:10.027283Z","iopub.execute_input":"2022-04-09T15:41:10.027514Z","iopub.status.idle":"2022-04-09T15:57:33.879254Z","shell.execute_reply.started":"2022-04-09T15:41:10.027482Z","shell.execute_reply":"2022-04-09T15:57:33.878047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we have to preprocess our test data also same as that our training data.","metadata":{}},{"cell_type":"code","source":"train_dir='test'\npath=os.path.join(main_dir,train_dir)\n\n\nX_test=[]\nid_line=[]\ndef create_test1_data(path):\n    for p in os.listdir(path):\n        id_line.append(p.split(\".\")[0])\n        img_array=cv2.imread(os.path.join(path,p),cv2.IMREAD_GRAYSCALE)\n        new_img_array=cv2.resize(img_array,dsize=(80,80))\n        X_test.append(new_img_array)\n        \ncreate_test1_data(path)\nX_test = np.array(X_test).reshape(-1,80,80,1)\nX_test = X_test/255        \n        ","metadata":{"execution":{"iopub.status.busy":"2022-04-09T15:57:33.881099Z","iopub.execute_input":"2022-04-09T15:57:33.881399Z","iopub.status.idle":"2022-04-09T15:57:47.520172Z","shell.execute_reply.started":"2022-04-09T15:57:33.881365Z","shell.execute_reply":"2022-04-09T15:57:47.519209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions=model.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-04-09T15:57:47.521416Z","iopub.execute_input":"2022-04-09T15:57:47.521661Z","iopub.status.idle":"2022-04-09T15:58:03.2935Z","shell.execute_reply.started":"2022-04-09T15:57:47.521632Z","shell.execute_reply":"2022-04-09T15:58:03.292448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted_val = [int(round(p[0])) for p in predictions]","metadata":{"execution":{"iopub.status.busy":"2022-04-09T15:58:03.295959Z","iopub.execute_input":"2022-04-09T15:58:03.296474Z","iopub.status.idle":"2022-04-09T15:58:03.333749Z","shell.execute_reply.started":"2022-04-09T15:58:03.296439Z","shell.execute_reply":"2022-04-09T15:58:03.333087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now you have to make submission data frame to submit your resultset","metadata":{}},{"cell_type":"code","source":"submission_df=pd.DataFrame({'id':id_line,'label':predicted_val})","metadata":{"execution":{"iopub.status.busy":"2022-04-09T15:58:57.052762Z","iopub.execute_input":"2022-04-09T15:58:57.053821Z","iopub.status.idle":"2022-04-09T15:58:57.066702Z","shell.execute_reply.started":"2022-04-09T15:58:57.053761Z","shell.execute_reply":"2022-04-09T15:58:57.065649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.to_csv(\"submission.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-09T15:58:58.663837Z","iopub.execute_input":"2022-04-09T15:58:58.664266Z","iopub.status.idle":"2022-04-09T15:58:58.701917Z","shell.execute_reply.started":"2022-04-09T15:58:58.664215Z","shell.execute_reply":"2022-04-09T15:58:58.70112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df","metadata":{"execution":{"iopub.status.busy":"2022-04-09T15:59:16.328829Z","iopub.execute_input":"2022-04-09T15:59:16.329229Z","iopub.status.idle":"2022-04-09T15:59:16.365251Z","shell.execute_reply.started":"2022-04-09T15:59:16.329184Z","shell.execute_reply":"2022-04-09T15:59:16.364457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}