{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\n#import config\nimport os\nfrom sklearn.model_selection import StratifiedKFold\nfrom tqdm import tqdm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pretrainedmodels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from zipfile import ZipFile\n\ntest_file_name = \"../input/dogs-vs-cats-redux-kernels-edition/test.zip\"\nwith ZipFile(test_file_name, 'r') as zip: \n    print('Extracting all the files now...') \n    zip.extractall('../output/kaggle/working/') \n    print('Done!') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_file_name = \"../input/dogs-vs-cats-redux-kernels-edition/train.zip\"\n\nwith ZipFile(train_file_name, 'r') as zip: \n    print('Extracting all the files now...') \n    zip.extractall('../output/kaggle/working/') \n    print('Done!') ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"class config:\n    TRAIN_DIR = '../output/kaggle/working/train/'\n    TEST_DIR = '../output/kaggle/working//test/'\n    EPOCHS = 3\n    IMG_HEIGHT = 224\n    IMG_WIDTH = 224\n    TRAIN_BATCH_SIZE = 32\n    TEST_BATCH_SIZE = 16\n    BASE_MODEL = 'resnet34'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_images = [i for i in os.listdir(config.TRAIN_DIR)]\ntrain = pd.DataFrame({'id':train_images})\ntrain['label'] = train['id'].apply(lambda x: str(x).split('.')[0])\nlabels = {'cat':0, 'dog':1}\ntrain['label'] = train['label'].map(labels)\ntrain['kfold'] = -1\ntrain = train.sample(frac=1).reset_index(drop=True)\nkf = StratifiedKFold(n_splits=5)\nfor fold, (trn_, val_) in enumerate(kf.split(X=train, y=train.label.values)):\n    print(len(trn_), len(val_))\n    train.loc[val_, 'kfold'] = fold\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport cv2\nimport albumentations\nimport numpy as np\nimport torch\n#import config\n\n\nclass DogCatDataset:\n    def __init__(self, ids, labels, transform=None):\n        super(DogCatDataset, self).__init__()\n        self.ids = ids\n        self.labels = labels\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.ids)\n\n    def __getitem__(self, item):\n        img = cv2.imread(config.TRAIN_DIR+self.ids[item])\n        img = cv2.resize(img,(config.IMG_HEIGHT,config.IMG_WIDTH), interpolation = cv2.INTER_AREA)\n        \n        if self.transform is not None:\n            res = self.transform(image=img)\n            img = res['image'].astype(np.float32)\n        \n        img = img.transpose((2,0,1))\n        label = self.labels[item]\n\n\n        return {\n            'image': torch.tensor(img, dtype=torch.float),\n            'label': torch.tensor(label, dtype=torch.long),\n        }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn as nn\nimport pretrainedmodels\nfrom torch.nn import functional as F\n\n\n\nclass ResNet34(nn.Module):\n    def __init__(self, pretrained=True):\n        super(ResNet34, self).__init__()\n        if pretrained is True:\n            self.model = pretrainedmodels.__dict__['resnet34'](pretrained='imagenet')\n        else:\n            self.model = pretrainedmodels.__dict__['resnet34'](pretrained=None)\n\n        self.l0 = nn.Linear(512, 2)\n        \n\n    def forward(self, x):\n        bs, _, _, _ = x.shape\n        x = self.model.features(x)\n        x = F.adaptive_avg_pool2d(x, 1).reshape(bs, -1)\n        l0 = self.l0(x)\n        \n        return l0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MODEL_DISPATCHER = {\n    'resnet34':ResNet34\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def loss_fn(outputs, targets):\n    loss = nn.CrossEntropyLoss()(outputs, targets)\n    return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_epoch(dataset, data_loader, model, optimizer, device):\n    model.train()\n\n    for bi, d in tqdm(enumerate(data_loader), total=int(len(dataset) / data_loader.batch_size)):\n        image = d['image']\n        label = d['label']\n\n        image = image.to(device, dtype=torch.float)\n        label = label.to(device, dtype=torch.long)\n        optimizer.zero_grad()\n        output = model(image)\n        loss = loss_fn(output, label)\n\n        loss.backward()\n        optimizer.step()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate_epoch(dataset, data_loader, model, device):\n    model.eval()\n    final_loss = 0\n    counter = 0\n    with torch.no_grad():\n        for bi, d in tqdm(enumerate(data_loader), total=int(len(dataset) / data_loader.batch_size)):\n            counter = counter + 1\n            image = d['image']\n            label = d['label']\n\n            image = image.to(device, dtype=torch.float)\n            label = label.to(device, dtype=torch.long)\n\n            outputs = model(image)\n            loss = loss_fn(outputs, label)\n            final_loss += loss.detach().cpu().numpy()\n        return final_loss / counter","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run(fold):\n    df_train = train[train.kfold != fold].reset_index(drop=True)\n    df_valid = train[train.kfold == fold].reset_index(drop=True)\n\n    train_dataset = DogCatDataset(\n        ids = df_train.id.values,\n        labels = df_train.label.values\n    )\n\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=config.TRAIN_BATCH_SIZE,\n        num_workers=4\n    )\n\n    valid_dataset = DogCatDataset(\n        ids = df_valid.id.values,\n        labels = df_valid.label.values\n    )\n\n    valid_loader = torch.utils.data.DataLoader(\n        valid_dataset,\n        batch_size=config.TRAIN_BATCH_SIZE,\n        num_workers=4\n    )\n\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    model = MODEL_DISPATCHER[config.BASE_MODEL](pretrained=True)\n    model.to(device)\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.3, verbose=True)\n\n    if torch.cuda.device_count() > 1:\n        model = nn.DataParallel(model)\n\n    best_val_score = 100\n    for epoch in range(config.EPOCHS):\n        train_epoch(train_dataset, train_loader, model, optimizer, device)\n        val_score = evaluate_epoch(valid_dataset, valid_loader, model, device)\n        print(val_score)\n        scheduler.step(val_score)\n        if val_score < best_val_score:\n            print('save model!')\n            best_val_score = val_score\n            torch.save(model.state_dict(), f\"{config.BASE_MODEL}_{fold}.bin\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"run(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"run(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"run(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"run(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"run(4)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}