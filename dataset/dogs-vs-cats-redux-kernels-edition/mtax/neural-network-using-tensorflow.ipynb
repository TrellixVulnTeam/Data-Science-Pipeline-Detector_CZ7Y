{"cells":[{"metadata":{"_uuid":"816e8ac3dcf679e2603a8ae3b8027d99a8dbd7f9","_cell_guid":"190cb7a5-f6b8-471e-8951-3dc417d25b7b","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport os\nimport datetime\nimport random\nimport math\nimport cv2\nimport tensorflow as tf\nimport numpy as np # linear algebra\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm","execution_count":1,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"3f970207c76ea4db203e9d384e3e53e39495d21c","_cell_guid":"6af43ef7-dbd4-4b86-a0c8-4c58951cad80","trusted":true},"cell_type":"code","source":"## constants\nTRAIN_DIR = \"../input/train/\"\nTEST_DIR = \"../input/test/\"\nTRAIN_SIZE = 22500\nTEST_SIZE = 2500\nDEV_RATIO = 0.1\nIMAGE_HEIGHT = IMAGE_WIDTH = 128\n\nLEARNING_RATE = 0.0001\nMINIBATCH_SIZE = 32\nINPUT_SIZE = IMAGE_HEIGHT * IMAGE_WIDTH * 3\nOUTPUT_SIZE = 2","execution_count":10,"outputs":[]},{"metadata":{"_uuid":"12e4f1a6fe204e35500a17fcea8bcebda7481586","_cell_guid":"1ff26ed6-bcf4-4c0d-b656-679092098bff"},"cell_type":"markdown","source":"# Data preparation\nTo start, we read provided data. \n\nThe *../input/train/* dir contains 12500 cat images and 12500 dog images.\nEach filename contains \"cat\" or \"dog\" as label."},{"metadata":{"collapsed":true,"_uuid":"95dbbbe4ea8ea240c25e6bf5bb20075f20bd98ec","_cell_guid":"e4d548d3-8ff4-4130-9f3e-b203aaaf79fb","trusted":true},"cell_type":"code","source":"## tool functions\ndef ex_time(func):\n    start_time = datetime.datetime.now()\n    \n    def wrapper(*args, **kwargs):\n        print(\"start time: {}\".format(start_time))\n        res = func(*args, **kwargs)\n        \n        end_time = datetime.datetime.now()\n        ex_time = end_time - start_time\n        print(\"end time: {}\".format(end_time))\n        print(\"excute time: {} seconds\".format(ex_time.seconds))\n\n        return res\n       \n    return wrapper\n\ndef display(image, image_width=IMAGE_HEIGHT, image_height=IMAGE_HEIGHT, interpolation=3):\n    # (784) => (28,28)\n    one_image = image.reshape(image_width,image_height, interpolation)\n    \n    new_f = plt.figure()\n    plt.axis('off')\n    plt.imshow(one_image, cmap=cm.binary)\n    plt.show()\n    plt.close()","execution_count":11,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"357a381fb5f3d585cfdf50a5cf932365841cb2b9","_cell_guid":"0ca77d68-3d29-4a73-810d-83e6f0dae3e1","trusted":true},"cell_type":"code","source":"## data utility functions\ndef dense_to_one_hot(labels_dense, num_classes):\n    \"\"\"\n    # convert class labels from scalars to one-hot vectors\n    # 0 => [1 0 0 0 0 0 0 0 0 0]\n    # 1 => [0 1 0 0 0 0 0 0 0 0]\n    # ...\n    # 9 => [0 0 0 0 0 0 0 0 0 1]\n    \"\"\"\n    num_labels = labels_dense.shape[0]\n    #print(\"num_labels:\", num_labels)\n    index_offset = np.arange(num_labels) * num_classes\n    #print(\"index_offset:\", index_offset)\n    labels_one_hot = np.zeros((num_labels, num_classes))\n    #print(\"labels_one_hot:\", labels_one_hot)\n    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n    #print(index_offset + labels_dense.ravel())\n    #print(\"labels_one_hot2:\", labels_one_hot)\n    return labels_one_hot\n\ndef split_data(images, labels, dev_ratio=DEV_RATIO):\n    dev_count = int(labels.shape[1] * DEV_RATIO)\n    dev_images = images[:, :dev_count]\n    train_images = images[:, dev_count:]\n    dev_labels = labels[:, :dev_count]\n    train_labels = labels[:, dev_count:]\n    print(\"train images shape: {}, train labels shape:{}, \\\n    dev images shape: {}, dev labels shape: {}\".format(train_images.shape, train_labels.shape, dev_images.shape, dev_labels.shape))\n    return train_images, train_labels, dev_images, dev_labels","execution_count":12,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"353e7de945ef05d124dc8830602d7ed52fec80be","_cell_guid":"5e3c391d-730d-414a-b04f-69905466325f","trusted":true},"cell_type":"code","source":"#@ex_time\ndef pre_data(dirname=TRAIN_DIR, file_count=1000):\n    all_filenames = os.listdir(dirname)\n    random.shuffle(all_filenames)\n    filenames = all_filenames[:file_count]\n    \n    ## images\n    images = np.zeros((file_count, IMAGE_HEIGHT*IMAGE_WIDTH*3))\n    for i in range(file_count):\n        imgnd_origin = cv2.imread(dirname+filenames[i])\n        imgnd_resized = cv2.resize(imgnd_origin, (IMAGE_HEIGHT, IMAGE_WIDTH), interpolation=cv2.INTER_CUBIC)\n        imgnd_flatten = imgnd_resized.reshape(1,-1)\n        images[i] = imgnd_flatten\n    \n    ## labels from filenames\n    labels_list = [\"dog\" in filename for filename in filenames]\n    labels = np.array(labels_list, dtype='int8').reshape(file_count, 1)\n    \n    ## shuffle\n    permutation = list(np.random.permutation(labels.shape[0]))\n    shuffled_labels = labels[permutation, :]\n    shuffled_images = images[permutation, :]\n    \n    ## dense to one hot\n    labels = dense_to_one_hot(shuffled_labels, OUTPUT_SIZE)\n    ## normalization\n    images = shuffled_images/255.0\n    \n    return images.T, labels.T\n","execution_count":13,"outputs":[]},{"metadata":{"_uuid":"c4980cccb3868737224d51f0d443d3f5b7b11df0","_cell_guid":"32a6e22f-4f4e-419c-a92a-b8eb2cfbae59","trusted":true},"cell_type":"code","source":"images, labels = pre_data(file_count=100)\n\ntrain_images, train_labels, dev_images, dev_labels = split_data(images, labels)","execution_count":14,"outputs":[]},{"metadata":{"_uuid":"9cae80daac699c2636f82ee3469ab994bfe6a8d3","_cell_guid":"25daee12-4365-4151-8bf7-8781d7ef4cf3","trusted":true},"cell_type":"code","source":"print(train_images.shape, train_labels.shape, dev_images.shape, dev_labels.shape)","execution_count":15,"outputs":[]},{"metadata":{"_uuid":"6b36929356ed27db60e8c35dce7846a1fb3b0684","_cell_guid":"766c369f-ff6f-44ae-8281-2663c1d9fd32"},"cell_type":"markdown","source":"## Model"},{"metadata":{"collapsed":true,"_uuid":"d1d81c3fb7e7b7e9bed6d2432f37ef36191a854e","_cell_guid":"25cb713e-6691-497f-9776-7b2ab9464038","trusted":true},"cell_type":"code","source":"def init_params(layers_dims):\n    '''\n    Initializes parameters to build a neural network with tensorflow.\n    \n    Arguments:\n        layers_dims: python array (list) containing the size of each layer.\n                     e.g.:[n_x=n_l0, n_l1, n_l2, ..., n_lL=n_Y].n_l2 is size of second hidden layer.\n    \n    Returns:\n        params: a dictionary of tensors containing W1, b1, W2, b2, ..., WL, bL. e.g.:\n                {\n                    \"W1\": W1,\n                    \"b1\": b1,\n                    \"W2\": W2,\n                    \"b2\": b2\n                }\n        \n    \n    '''\n    L = len(layers_dims)\n    params = {}\n    \n    for l in range(1, L):\n        params['W' + str(l)] = tf.get_variable('W' + str(l), [layers_dims[l], layers_dims[l-1]], initializer = tf.contrib.layers.xavier_initializer())\n        params['b' + str(l)] = tf.get_variable('b' + str(l), [layers_dims[l], 1], initializer = tf.zeros_initializer())\n    return params\n\ndef forward_propagation_with_dropout(X, params, keep_prob=0.1):\n    \"\"\"\n    Implements the forward propagation: LINEAR -> RELU + DROPOUT -> LINEAR -> RELU + DROPOUT -> LINEAR -> SIGMOID.\n    \n    Arguments:\n    X -- input dataset placeholder, of shape (input size, number of examples)\n    params -- python dictionary containing your parameters(tf.Variable) \"W1\", \"b1\", \"W2\", \"b2\", ..., \"WL\", \"bL\":\n                    W1 -- weight matrix of shape (20, 2)\n                    b1 -- bias vector of shape (20, 1)\n                    W2 -- weight matrix of shape (3, 20)\n                    b2 -- bias vector of shape (3, 1)\n                    W3 -- weight matrix of shape (1, 3)\n                    b3 -- bias vector of shape (1, 1)\n    keep_prob - probability of keeping a neuron active during drop-out, scalar\n    \n    Returns:\n    ZL -- the output of the last LINEAR unit\n    \"\"\"\n    keep_prob = tf.placeholder(dtype=tf.float32, name=\"keep_prob\")\n    L = int(len(params)/2)\n    cache = {\"A0\": X}\n    for l in range(1, L+1):\n        cache[\"Z\"+str(l)] = tf.matmul(params[\"W\"+str(l)], cache[\"A\"+str(l-1)]) + params[\"b\"+str(l)]\n        cache[\"Droped_Z\"+str(l)] = tf.nn.dropout(cache[\"Z\"+str(l)], keep_prob)\n        cache[\"A\"+str(l)] = tf.nn.relu(cache[\"Z\"+str(l)])\n    return cache[\"Z\"+str(L)]\n\ndef compute_cost(Z, Y):\n    \"\"\"\n    Computes the cost\n    \n    Arguments:\n    Z -- output of forward propagation (output of the last LINEAR unit), of shape (n_Y, number of examples)\n    Y -- labels vector placeholder, same shape as Z\n    \n    Returns:\n    cost - Tensor of the cost function\n    \"\"\"\n    # to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)\n    logits = tf.transpose(Z)\n    labels = tf.transpose(Y)\n    \n    # compute cost\n    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n    return cost\n    \ndef random_mini_batches(X, Y, mini_batch_size = 64):\n    \"\"\"\n    Creates a list of random minibatches from (X, Y)\n    \n    Arguments:\n    X -- input data, of shape (input size, number of examples)\n    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n    mini_batch_size -- size of the mini-batches, integer\n    \n    Returns:\n    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n    \"\"\"\n    \n    m = X.shape[1]                  # number of training examples\n    mini_batches = []\n        \n    # Step 1: Shuffle (X, Y)\n    permutation = list(np.random.permutation(m))\n    shuffled_X = X[:, permutation]\n    shuffled_Y = Y[:, permutation]\n\n    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n    num_complete_minibatches = int(math.floor(m/mini_batch_size)) # number of mini batches of size mini_batch_size in your partitionning\n    for k in range(0, num_complete_minibatches):\n        ### START CODE HERE ### (approx. 2 lines)\n        mini_batch_X = shuffled_X[:, k*mini_batch_size:(k+1)*mini_batch_size]\n        mini_batch_Y = shuffled_Y[:, k*mini_batch_size:(k+1)*mini_batch_size]\n        ### END CODE HERE ###\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    # Handling the end case (last mini-batch < mini_batch_size)\n    if m % mini_batch_size != 0:\n        ### START CODE HERE ### (approx. 2 lines)\n        mini_batch_X = shuffled_X[:, num_complete_minibatches*mini_batch_size:]\n        mini_batch_Y = shuffled_Y[:, num_complete_minibatches*mini_batch_size:]\n        ### END CODE HERE ###\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    return mini_batches","execution_count":16,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"96d67fd9f54a23922d340065eab9b0246c4ee756","_cell_guid":"9899fc95-00e1-41cf-a106-da9e5eb521c3","trusted":true},"cell_type":"code","source":"def model(X_train, Y_train, X_test, Y_test, learning_rate=LEARNING_RATE, decay_rate=0,\n          num_epochs=2500, minibatch_size=MINIBATCH_SIZE, print_cost=True,\n          layers_dims=[784, 3,3,10], optimizer=\"GradientDecent\"):\n    '''\n    Implements a tensorflow neural network: e.g. LINEAR->RELU->LINEAR->RELU->LINEAR->SOFTMAX.\n    \n    Arguments:\n    X_train -- training set, of shape (input size, number of training examples)\n    Y_train -- test set, of shape (output size, number of training examples)\n    X_test -- training set, of shape (input size, number of training examples)\n    Y_test -- test set, of shape (output size, number of test examples)\n    learning_rate -- learning rate of the optimization\n    num_epochs -- number of epochs of the optimization loop\n    minibatch_size -- size of a minibatch\n    print_cost -- True to print the cost every 10 epochs\n    layers_dims: python array (list) containing the size of each layer.\n                 e.g.:[n_x=n_l0, n_l1, n_l2, ..., n_lL=n_Y].n_l2 is size of second hidden layer.\n    \n    Returns:\n    parameters -- parameters learnt by the model. They can then be used to predict.\n    '''\n    (n_x, m) = X_train.shape\n    n_y = Y_train.shape[0]\n    costs_log = []\n    \n    X = tf.placeholder(dtype=tf.float32, shape=(n_x, None), name=\"X\")\n    Y = tf.placeholder(dtype=tf.float32, shape=(n_y, None), name=\"Y\")\n    epoch_p = tf.placeholder(dtype=tf.float32, name=\"epoch_p\")\n    #### tool init_params\n    params = init_params(layers_dims)\n    #### tool foward_propa\n    Z = forward_propagation_with_dropout(X, params)\n    #### tool compute_cost\n    cost = compute_cost(Z, Y)\n    #### learning_rate decay\n    learning_rate = learning_rate * np.power((10/(epoch_p+1)), decay_rate)\n\n    \n    if optimizer == \"GradientDescent\":\n        optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n    elif optimizer == \"Adam\":\n        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n    \n    ## let's go\n    init = tf.global_variables_initializer()\n    \n    with tf.Session() as sess:\n        sess.run(init)\n        for epoch in range(num_epochs):\n            epoch_cost = 0\n            n_minibatches = int(m/minibatch_size)\n            #### tool random_mini_batches\n            minibatches = random_mini_batches(X_train, Y_train, mini_batch_size=minibatch_size)\n            \n            for minibatch in minibatches:\n                mini_X, mini_Y = minibatch\n                o, minibatch_cost = sess.run((optimizer, cost), feed_dict={X: mini_X, Y: mini_Y, epoch_p: epoch})\n                epoch_cost += minibatch_cost / n_minibatches\n                \n            if print_cost and (epoch%10 == 0):\n                print(\"Cost after epoch {} is {}\".format(epoch, epoch_cost))\n\n            if print_cost and (epoch%2 == 0):\n                costs_log.append(epoch_cost)\n        plt.plot(np.squeeze(costs_log))\n        plt.ylabel('cost')\n        plt.xlabel('iterations (per 5)')\n        plt.title(\"Learning Rate = {}\".format(learning_rate))\n        plt.show()\n        # lets save the parameters in a variable\n        params = sess.run(params)\n        print (\"Parameters have been trained!\")\n\n        # Calculate the correct predictions\n        correct_prediction = tf.equal(tf.argmax(Z), tf.argmax(Y))\n\n        # Calculate accuracy on the test set\n        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n\n        print (\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train}))\n        print (\"Test Accuracy:\", accuracy.eval({X: X_test, Y: Y_test}))\n\n        return params, costs_log\n    ","execution_count":17,"outputs":[]},{"metadata":{"_uuid":"3af284c61890613fb035b26d1b19f17b2c72e419","_cell_guid":"664d2012-9cb0-4426-b25c-3c53b8852b75"},"cell_type":"markdown","source":"## Train"},{"metadata":{"_uuid":"27034d28b8429d2360a2b6ef47eb1a22544ebf8e","_cell_guid":"2e790ef6-1a0d-4d0b-8145-2111f8f4608c","trusted":true},"cell_type":"code","source":"#@ex_time\ndef train():\n    tf.reset_default_graph()\n    params, costs_log = model(train_images, train_labels, dev_images, dev_labels,\n                              num_epochs =101, learning_rate=LEARNING_RATE, optimizer=\"Adam\",\n                             layers_dims=[INPUT_SIZE, 3, OUTPUT_SIZE], decay_rate=100)\n    return params, costs_log\n\nparams, costs_log = train()","execution_count":19,"outputs":[]},{"metadata":{"_uuid":"efd311e3bc6b22f5d9544b05b0df0e03af5034ee","_cell_guid":"7ff45bbb-214c-41be-9910-cf08872da3a2"},"cell_type":"markdown","source":"## Predict"},{"metadata":{"collapsed":true,"_uuid":"4882141f0cdf43dac434e394bfde07726f01187a","_cell_guid":"866dcb88-6fa5-4888-b3e8-95c3b0e1cf8e","trusted":true},"cell_type":"code","source":"def predict(X, params):\n    \"\"\"\n    Implements a tensorflow neural network prediction using given params.\n    \n    Arguments:\n    X -- Images to predict, ndarry set of shape (input size, number of images)\n    params -- parameters learnt by some model. They can then be used to predict.\n    \n    Returns:\n    result -- list of prediction shape of (1, number of input images)\n    \"\"\"\n    # conver X to tf Placeholder\n    X_placeholder = tf.placeholder(tf.float32, shape=X.shape, name=\"X_placeholder\")\n    # conver params to tensors\n    L = int(len(params)/2)\n    params_tensor = {}\n    for l in range(1, L+1):\n        params_tensor[\"W\"+str(l)] = tf.convert_to_tensor(params[\"W\"+str(l)])\n        params_tensor[\"b\"+str(l)] = tf.convert_to_tensor(params[\"b\"+str(l)])\n    # foward propagation\n    Z = forward_propagation_with_dropout(X_placeholder, params_tensor, keep_prob=1.0)\n    prediction = tf.nn.softmax(Z)\n    \n    #run tf Session\n    with tf.Session() as sess:\n        result = sess.run(prediction, feed_dict={X_placeholder: X})\n    return result","execution_count":20,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"e7f22bebac67b37a9d6223c76fd2c556e37af781","_cell_guid":"485c9e7c-98bc-4bd7-be80-db3bf94e0702","trusted":true},"cell_type":"code","source":"res = predict(dev_images, params)","execution_count":21,"outputs":[]},{"metadata":{"_uuid":"5676c5cb961d296478cc9b160fd71b2974a37f8e","_cell_guid":"4d070cfd-c839-4c34-a67e-03c00ba4d1d3","trusted":true},"cell_type":"code","source":"res","execution_count":22,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"ede17cac30857e5f72427153c4383aaf79458c89","_cell_guid":"6281cb19-e325-48fc-aeab-39663994ae75"},"cell_type":"markdown","source":"## Laboratory (Optional)"},{"metadata":{"_uuid":"649f113eceda7fc06560c06f8b23999781d4ddcb","_cell_guid":"e2ea037c-cec3-48af-8fbb-f9e1431e2ef7"},"cell_type":"markdown","source":"#### 1. too slow training, but Why?\n\n* 1.1 cProfile"},{"metadata":{"collapsed":true,"_uuid":"d9948f87ae6f595e30b2bb392ea9f612f53c5a8e","_cell_guid":"28f978eb-c29b-4a46-924e-e220e8dc66be","trusted":true},"cell_type":"code","source":"def why_time(func):\n    \"\"\"\n    其中，输出每列的具体解释如下：\n    ncalls：表示函数调用的次数；\n    tottime：表示指定函数的总的运行时间，除掉函数中调用子函数的运行时间；\n    percall：（第一个percall）等于 tottime/ncalls；\n    cumtime：表示该函数及其所有子函数的调用运行的时间，即函数开始调用到返回的时间；\n    percall：（第二个percall）即函数运行一次的平均时间，等于 cumtime/ncalls；\n    filename:lineno(function)：每个函数调用的具体信息；\n    \"\"\"\n    \n    import cProfile\n    cmd = \"{}()\".format(func.__name__)\n    \n    # 直接把分析结果打印到控制台\n    cProfile.run(cmd, sort=\"cumulative\")\n    \n    # print out to file\n    #cProfile.run(\"test()\", filename=\"result.out\")\n    \n    # sort by excute time\n    #cProfile.run(\"test()\", filename=\"result.out\", sort=\"cumulative\")","execution_count":23,"outputs":[]},{"metadata":{"_uuid":"61ba9c95764ae9c8280d38480da123ab80c148fb","_cell_guid":"8d6d9538-1bf2-4018-a5e7-83d3feaca3b8","trusted":true},"cell_type":"code","source":"why_time(train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fbde5298b0a5dcdf4ff2ca82a457cc97e092cc64","_cell_guid":"274d86f0-bfbf-4177-b290-550a936e7686"},"cell_type":"markdown","source":"good boy but mass\n\ntry profile by line\n\n* 1.2 More Data"},{"metadata":{"_uuid":"ce94266e54ea7bf09036dd1b14b568506e38fa3a","_cell_guid":"55b9869c-1889-416d-8fd7-1469847fa49d","trusted":true,"collapsed":true},"cell_type":"code","source":"images, labels = pre_data(file_count=1000)\ntrain_images, train_labels, dev_images, dev_labels = split_data(images, labels)\n\nparams, costs_log = train()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dcf7e9dd1618bed97ca1f1f5340c4b12332b5030","_cell_guid":"198892d2-0789-4693-86eb-34d2a466c165"},"cell_type":"markdown","source":"* 1.3 Higher Accuracy"},{"metadata":{"_uuid":"f3d38f4b1855ffc623169381270ba65e5d3097cb","_cell_guid":"fbe5f9c6-9422-4dda-bb48-7f40b5b3b2e0","trusted":true,"collapsed":true},"cell_type":"code","source":"tf.reset_default_graph()\nparams, costs_log = model(train_images, train_labels, dev_images, dev_labels,\n                              num_epochs =101, learning_rate=LEARNING_RATE, optimizer=\"Adam\",\n                             layers_dims=[INPUT_SIZE, 30, 20, OUTPUT_SIZE], decay_rate=100)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"98ab9d7c7edb4427ed0dee24129f5e7d8fc49db8","_cell_guid":"91009281-5e5f-422d-af53-ff8c9dc70e94","trusted":false,"collapsed":true},"cell_type":"code","source":"tf.reset_default_graph()\nparams, costs_log = model(train_images, train_labels, dev_images, dev_labels,\n                              num_epochs =101, learning_rate=LEARNING_RATE, optimizer=\"Adam\",\n                             layers_dims=[INPUT_SIZE, 30, 20, OUTPUT_SIZE], decay_rate=100)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bf987d6942f74d711caf81fd3de5c1d2cef27642","_cell_guid":"afaa019b-2c59-4796-bf5a-20fed4da190d","trusted":false,"collapsed":true},"cell_type":"code","source":"tf.reset_default_graph()\nparams, costs_log = model(train_images, train_labels, dev_images, dev_labels,\n                              num_epochs =101, learning_rate=LEARNING_RATE, optimizer=\"Adam\",\n                             layers_dims=[INPUT_SIZE, 3, 20, OUTPUT_SIZE], decay_rate=100)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c01b9358f732e508562a30362ccc042af2df8552","_cell_guid":"2097c022-ebc1-45a5-9846-507880f1a790"},"cell_type":"markdown","source":"* 1.4 More iterations"},{"metadata":{"_uuid":"5db9ad9648ca31073c4e76a58d3086ca7d131d2b","_cell_guid":"a3a5ea53-8339-4c75-9f5c-7def7c8fe6f6","trusted":false,"collapsed":true},"cell_type":"code","source":"tf.reset_default_graph()\nparams, costs_log = model(train_images, train_labels, dev_images, dev_labels,\n                              num_epochs =1001, learning_rate=LEARNING_RATE, optimizer=\"Adam\",\n                             layers_dims=[INPUT_SIZE, 30, 20, OUTPUT_SIZE], decay_rate=100)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"299f04a1425d166cd9572ae5d0d4c93fec96ce04","_cell_guid":"d734aafc-f27c-4faf-b9e7-724098809ac3","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}