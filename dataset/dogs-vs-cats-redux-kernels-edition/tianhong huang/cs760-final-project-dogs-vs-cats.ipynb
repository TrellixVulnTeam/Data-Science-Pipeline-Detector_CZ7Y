{"cells":[{"metadata":{},"cell_type":"markdown","source":"# <a id=\"1\">VGG-16/19</a>"},{"metadata":{},"cell_type":"markdown","source":"## Loading images"},{"metadata":{},"cell_type":"markdown","source":"Packages:"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os, random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.image import ImageDataGenerator, load_img\n\nprint(os.listdir(\"../input/dogs-vs-cats-redux-kernels-edition/\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Parameters:"},{"metadata":{"trusted":true},"cell_type":"code","source":"PATH = '/kaggle/input/dogs-vs-cats-redux-kernels-edition/'\n\nnum_classes  = 2\nsample_size  = 25000\nIMG_size     = 224\nbatch_size   = 50\nepoch_num    = 50","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Extract pictures from zip:"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"code","source":"train_img_path = os.path.join(PATH, \"train.zip\")\ntest_img_path  = os.path.join(PATH, \"test.zip\")\n\nimport zipfile\nwith zipfile.ZipFile(train_img_path, \"r\") as z:\n   z.extractall(\".\")\nwith zipfile.ZipFile(test_img_path, \"r\") as z:\n   z.extractall(\".\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Prepare data frame:"},{"metadata":{"trusted":true},"cell_type":"code","source":"filenames  = os.listdir(\"./train/\")\ncategories = []\nfor filename in filenames:\n    category = filename.split('.')[0]\n    if category == 'dog':\n        categories.append(1)\n    else:\n        categories.append(0)\n\ndf = pd.DataFrame({\n    'filename': filenames,\n    'category': categories\n})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prepare train and validation data generator"},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"category\"] = df[\"category\"].replace({0: 'cat', 1: 'dog'}) \n\ntrain_df, val_df = train_test_split(df, test_size=0.4, random_state=2020)\n\ntrain_df  = train_df.reset_index(drop=True)\nval_df    = val_df.reset_index(drop=True)\ntrain_num = train_df.shape[0]\nval_num   = val_df.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['category'].value_counts().plot.bar()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Training generator:"},{"metadata":{"trusted":true},"cell_type":"code","source":"datagen = ImageDataGenerator(rescale=1./255.)\n\ntrain_generator = datagen.flow_from_dataframe(\nx_col = \"filename\",\ny_col = \"category\",\ndataframe = train_df,\ndirectory = \"./train/\",\nbatch_size = batch_size,\nshuffle    = True,\nclass_mode = \"categorical\",\ntarget_size = (IMG_size, IMG_size))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Validation generator:"},{"metadata":{"trusted":true},"cell_type":"code","source":"val_generator = datagen.flow_from_dataframe(\nx_col = \"filename\",\ny_col = \"category\",\ndataframe = val_df,\ndirectory = \"./train/\",\nbatch_size = batch_size,\nshuffle    = True,\nclass_mode = \"categorical\",\ntarget_size = (IMG_size, IMG_size))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id=\"13\">VGG-16 model</a>"},{"metadata":{},"cell_type":"markdown","source":"Packages:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras,os\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, MaxPool2D , Flatten\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Build VGG-16 model:\n\n* CNN model contains 16 layers in which weights and bias parameters are learned.\n* 13 convolutional layers are stacked one by one and 3 dense layers for classification.\n* the dense layers comprises of 4096, 4096, 2 nodes each.\n* dense layers action = ReLU + ReLU + Softmax.\n\n![](https://tech.showmax.com/2017/10/convnet-architectures/image_0-8fa3b810.png)"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_vgg16 = Sequential()\n\n# CONV3-64 + POOL2\nmodel_vgg16.add(Conv2D(input_shape=(224,224,3),filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\nmodel_vgg16.add(Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\nmodel_vgg16.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n\n# CONV3-128 + POOL2\nmodel_vgg16.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel_vgg16.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel_vgg16.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n\n# CONV3-256 + POOL2\nmodel_vgg16.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel_vgg16.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel_vgg16.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel_vgg16.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n\n# CONV3-512 + POOL2\nmodel_vgg16.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel_vgg16.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel_vgg16.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel_vgg16.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n\n# CONV3-512 + POOL2\nmodel_vgg16.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel_vgg16.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel_vgg16.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel_vgg16.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n\n# DENSE\nmodel_vgg16.add(Flatten())\nmodel_vgg16.add(Dense(units=4096,activation=\"relu\"))\nmodel_vgg16.add(Dense(units=4096,activation=\"relu\"))\nmodel_vgg16.add(Dense(units=2, activation=\"softmax\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Compile VGG-16 model: \n* using Adam optimiser to reach global minimum, with learning rate = 0.00001.\n* loss function set as categorical_crossentropy, metrics = accuracy.\n* total 134,268,738 parameters, with trainable parameters 134,268,738."},{"metadata":{"trusted":true},"cell_type":"code","source":"opt = Adam(lr = 0.00001)\n\nmodel_vgg16.compile(optimizer=opt, loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])\nmodel_vgg16.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Callbacks: ModelCheckpoint and EarlyStopping method.\n\n* monitoring validation accuracy by passing **val_acc** to ModelCheckpoint.\n* The model will only be saved to disk if val_acc in current epoch is greater than the last epoch\n* passing val_acc to EarlyStopping, set patience = 10 means the model will stop if there is no rise in val_acc in 10 epochs."},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint = ModelCheckpoint(\"vgg16_1.h5\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n\nearly = EarlyStopping(monitor='val_acc', min_delta=0, patience=10, verbose=1, mode='auto')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Training VGG-16 model:"},{"metadata":{"trusted":true},"cell_type":"code","source":"hist_vgg16 = model_vgg16.fit_generator(\n    generator = train_generator, \n    epochs = epoch_num,\n    validation_data  = val_generator,\n    validation_steps = val_num//batch_size,\n    steps_per_epoch  = train_num//batch_size,\n    callbacks = [checkpoint,early])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visualize validation acc and loss:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_acc_los(model_history):\n    hist = model_history.history\n    acc = hist['accuracy']\n    los = hist['loss']\n    val_acc = hist['val_accuracy']\n    val_los = hist['val_loss']\n    epochs = range(len(acc))\n    f,  ax = plt.subplots(1,2, figsize=(14,6))\n    ax[0].plot(epochs, acc, label='Training accuracy')\n    ax[0].plot(epochs, val_acc, label='Validation accuracy')\n    ax[0].set_title('Training and validation accuracy')\n    ax[0].legend()\n    ax[1].plot(epochs, los, label='Training loss')\n    ax[1].plot(epochs, val_los, label='Validation loss')\n    ax[1].set_title('Training and validation loss')\n    ax[1].legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_acc_los(hist_vgg16)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predictions"},{"metadata":{},"cell_type":"markdown","source":"Prepare test data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_filenames = os.listdir(\"./test/\")\n\ntest_df  = pd.DataFrame({'filename': test_filenames})\ntest_num = test_df.shape[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Test generator:"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_generator = datagen.flow_from_dataframe(\nx_col = \"filename\",\ny_col = None,\ndataframe = test_df,\ndirectory = \"./test/\",\nbatch_size = batch_size,\nshuffle    = False,\nclass_mode = None,\ntarget_size = (IMG_size, IMG_size))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Get predictions:"},{"metadata":{"trusted":true},"cell_type":"code","source":"predict = model_vgg16.predict_generator(test_generator, steps = np.ceil(test_num/batch_size))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creat labels:"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df['category'] = np.argmax(predict, axis=-1)\ntest_df['category'] = test_df['category'].replace({ 1: 'dog', 0: 'cat' })\n\ntest_df['category'].value_counts().plot.bar()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Samples with predicted labels:"},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_test = test_df.head(9)\nsample_test.head()\n\nplt.figure(figsize=(12, 24))\nfor index, row in sample_test.iterrows():\n    filename = row['filename']\n    category = row['category']\n    img = load_img(\"./test/\" + filename, target_size = (IMG_size,IMG_size))\n    plt.subplot(6, 3, index+1)\n    plt.imshow(img)\n    plt.title(\"Predicted:\" + format(category))\n    plt.axis('off')\nplt.tight_layout()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id=\"15\">VGG-19 model</a>"},{"metadata":{},"cell_type":"markdown","source":"* Difference between VGG-16 and VGG-19: extra CONV3-512 layer."},{"metadata":{"trusted":true},"cell_type":"code","source":"model_vgg19 = Sequential()\n\n# CONV3-64 + POOL2\nmodel_vgg19.add(Conv2D(input_shape=(224,224,3),filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\nmodel_vgg19.add(Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\nmodel_vgg19.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n\n# CONV3-128 + POOL2\nmodel_vgg19.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel_vgg19.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel_vgg19.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n\n# CONV3-256 + POOL2\nmodel_vgg19.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel_vgg19.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel_vgg19.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel_vgg19.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n\n# CONV3-512 + POOL2\nmodel_vgg19.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel_vgg19.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel_vgg19.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel_vgg19.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n\n# CONV3-512 + POOL2\nmodel_vgg19.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel_vgg19.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel_vgg19.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel_vgg19.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n\n# CONV3-512 + POOL2\nmodel_vgg19.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel_vgg19.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel_vgg19.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel_vgg19.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n\n# DENSE\nmodel_vgg19.add(Flatten())\nmodel_vgg19.add(Dense(units=4096,activation=\"relu\"))\nmodel_vgg19.add(Dense(units=4096,activation=\"relu\"))\nmodel_vgg19.add(Dense(units=2, activation=\"softmax\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Compiling VGG-19 model:"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_vgg19.compile(optimizer=opt, loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])\nmodel_vgg19.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using the same callbacks and train VGG-19 model:"},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint = ModelCheckpoint(\"vgg19_300.h5\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\nearly      = EarlyStopping(monitor='val_acc', min_delta=0, patience=10, verbose=1, mode='auto')\n\nhist_vgg19 = model_vgg19.fit_generator(\n    generator = train_generator, \n    epochs = epoch_num,\n    validation_data  = val_generator,\n    validation_steps = val_num//batch_size,\n    steps_per_epoch  = train_num//batch_size,\n    callbacks = [checkpoint,early])    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visualize validation acc and loss:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_acc_los(hist_vgg19)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id=\"16\">Pre-trained VGG-16 model</a>"},{"metadata":{},"cell_type":"markdown","source":"Pretrained model: \n* Importing from Keras and weights from ImagNet.\n* Adding 2 dense layers with 4096 units each and 1 softmax layer with 2 units."},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import layers, models, optimizers\nfrom keras.applications import VGG16\n\nconv_base = VGG16(weights='imagenet',\n                  include_top=False,\n                  input_shape=(IMG_size, IMG_size, 3))\n\nmodel_pre_vgg16 = models.Sequential()\nmodel_pre_vgg16.add(conv_base)\n\nmodel_pre_vgg16.add(Flatten())\nmodel_pre_vgg16.add(Dense(units=4096,activation=\"relu\"))\nmodel_pre_vgg16.add(Dense(units=4096,activation=\"relu\"))\nmodel_pre_vgg16.add(Dense(units=2, activation=\"softmax\"))\n\nmodel_pre_vgg16.compile(optimizer=opt, loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Training this model on our data set:"},{"metadata":{"trusted":true},"cell_type":"code","source":"hist_pre_vgg16 = model_pre_vgg16.fit_generator(\n    generator = train_generator, \n    epochs = 10,\n    validation_data  = val_generator,\n    validation_steps = val_num//batch_size,\n    steps_per_epoch  = train_num//batch_size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visualiz validation acc and loss:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_acc_los(hist_pre_vgg16)  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id=\"2\">ResNet / InceptionNet</a>"},{"metadata":{},"cell_type":"markdown","source":"## Loading and data preparing"},{"metadata":{},"cell_type":"markdown","source":"Packages and parameters:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os, cv2, random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nfrom random import shuffle \n\nPATH = '/kaggle/input/dogs-vs-cats-redux-kernels-edition/'\nFOLDER_TRAIN = './train/'\nFOLDER_TEST  = './test/'\nIMG_SIZE     = 224\nNUM_CLASSES  = 2\nSAMPLE_SIZE  = 25000","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Loading images:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_img_path = os.path.join(PATH, \"train.zip\")\ntest_img_path  = os.path.join(PATH, \"test.zip\")\n\nimport zipfile\nwith zipfile.ZipFile(train_img_path, \"r\") as z:\n   z.extractall(\".\")\nwith zipfile.ZipFile(test_img_path, \"r\") as z:\n   z.extractall(\".\")\n\ntrain_img_list = os.listdir(\"./train/\")[0: SAMPLE_SIZE]\ntest_img_list  = os.listdir(\"./test/\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Define functions for processing image data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def label_pet(img):\n    pet = img.split('.')[-3]\n    if pet == 'cat': return [1,0]\n    elif pet == 'dog': return [0,1]\n    \ndef process_data(data_img_list, DATA_FOLDER, isTrain=True):\n    data_df = []\n    for img in tqdm(data_img_list):\n        path = os.path.join(DATA_FOLDER,img)\n        if(isTrain):\n            label = label_pet(img)\n        else:\n            label = img.split('.')[0]\n        img = cv2.imread(path,cv2.IMREAD_COLOR)\n        img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))\n        data_df.append([np.array(img),np.array(label)])\n    shuffle(data_df)\n    return data_df\n\ndef plot_image_list_count(data_image_list):\n    labels = []\n    for img in data_image_list:\n        labels.append(img.split('.')[-3])\n    sns.countplot(labels)\n    plt.title('Cats vs Dogs')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data exploration:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_image_list_count(train_img_list)    \n\ntrain = process_data(train_img_list, FOLDER_TRAIN)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Preparing training and validation data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = np.array([i[0] for i in train]).reshape(-1,IMG_SIZE,IMG_SIZE,3)\ny = np.array([i[1] for i in train])\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.4, random_state = 2020)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id=\"22\">ResNet50 model</a>"},{"metadata":{},"cell_type":"markdown","source":"Packages and parameters:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.models import Sequential, Model, load_model\nfrom tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten, GlobalAveragePooling2D,Dropout\n\nBATCH_SIZE = 50\nEPOCH_NUM  = 10","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Build and complie ResNet50 model:\n\n* Importing pre-trained ResNet50 model from Keras, weights from ImageNet.\n* Adding additional layer of Dense with softmax activation function.\n* The first layer is not trainable, used pre-trained model.\n* Compling this model with using sigmoid optimizer.\n* Total 23,591,810 parameters, 23,538,690 trainable.\n\n![](https://i.stack.imgur.com/gI4zT.png)"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_RN50 = Sequential()\n\nmodel_RN50.add(ResNet50(include_top=False, pooling='max', weights='imagenet'))\nmodel_RN50.add(Dense(NUM_CLASSES, activation='softmax'))\nmodel_RN50.layers[0].trainable = True\n\nmodel_RN50.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\nmodel_RN50.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Training ResNet50 model on our data set:"},{"metadata":{"trusted":true},"cell_type":"code","source":"hist_RN50 = model_RN50.fit(X_train, y_train,\n                  batch_size = BATCH_SIZE,\n                  epochs  = EPOCH_NUM,\n                  verbose = 1,\n                  validation_data = (X_val, y_val))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visualize validation acc and loss:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_acc_loss(hist_RN50)    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Numeric validation acc and loss:"},{"metadata":{"trusted":true},"cell_type":"code","source":"score = model_RN50.evaluate(X_val, y_val, verbose=0)\nprint('Validation loss:', score[0])\nprint('Validation accuracy:', score[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predictions:"},{"metadata":{},"cell_type":"markdown","source":"Getting predictions and show sample images:"},{"metadata":{"trusted":true},"cell_type":"code","source":"test = process_data(test_img_list, FOLDER_TEST, False)\n\nf, ax = plt.subplots(5,5, figsize=(15,15))\nfor i,data in enumerate(test[:25]):\n    img_data = data[0]\n    orig = img_data\n    data = img_data.reshape(-1,IMG_SIZE,IMG_SIZE,3)\n    model_out = model_RN50.predict([data])[0]\n    \n    if np.argmax(model_out) == 1: \n        str_predicted='Dog'\n    else: \n        str_predicted='Cat'\n    ax[i//5, i%5].imshow(orig)\n    ax[i//5, i%5].axis('off')\n    ax[i//5, i%5].set_title(\"Predicted:{}\".format(str_predicted))    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id=\"24\">InceptionNet model</a>"},{"metadata":{},"cell_type":"markdown","source":"Build and compile InceptionNet model:\n\n![](https://hackathonprojects.files.wordpress.com/2016/09/inception_implement.png?w=649&h=337)\n\n* Inception modules performed as local network topology in InceptionNets.\n* Inception modules acts as the multi-level feature extractor in which convolutions of different sizes are obtained to create a diversified feature map.\n\n![](https://cdn-images-1.medium.com/max/2000/1*uXfC5fcbDsL0TJG4T8PsVw.png)\n\n* Total 22 layers in pretrained model.\n* Importing pre-trained InceptionV3 model from Keras, weights from ImageNet.\n* Adding 1 global avg pooling layer, 1 ReLU dense layer with 1024 units and softmax dense layer with 2 units.\n* Using sigmoid optimizer to compile InceptionNet model."},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.applications import InceptionV3\n\nIncep = InceptionV3(weights=INCEP_PATH, include_top=False)\nx     = Incep.output\nx_pool  = GlobalAveragePooling2D()(x)\nx_dense = Dense(1024,activation='relu')(x_pool)\nfinal_pred  = Dense(NUM_CLASSES,activation='softmax')(x_dense)\n\nmodel_Incep = Model(inputs=Incep.input,outputs=final_pred)\n\nmodel_Incep.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\nmodel_Incep.summary","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Training Inception model on our data set:"},{"metadata":{"trusted":true},"cell_type":"code","source":"hist_Incep = model_Incep.fit(X_train, y_train,\n                  batch_size = BATCH_SIZE,\n                  epochs  = EPOCH_NUM,\n                  verbose = 1,\n                  validation_data = (X_val, y_val))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visualize validation acc and loss:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_acc_loss(hist_Incep)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Numeric validation acc and loss:"},{"metadata":{"trusted":true},"cell_type":"code","source":"score = model_Incep.evaluate(X_val, y_val, verbose=0)\nprint('Validation loss:', score[0])\nprint('Validation accuracy:', score[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id=\"3\">References</a>\n\n[1] Dogs vs. Cats Redux: Kernels Edition, https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition  \n[2] CNN Arthitectures: VGG, Resnet, InceptionNet, XceptionNet, https://www.kaggle.com/shivamb/cnn-architectures-vgg-resnet-inception-tl\n[3] Cats or Dogs - Using CNN with Transfer Learning, https://www.kaggle.com/gpreda/cats-or-dogs-using-cnn-with-transfer-learning"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}