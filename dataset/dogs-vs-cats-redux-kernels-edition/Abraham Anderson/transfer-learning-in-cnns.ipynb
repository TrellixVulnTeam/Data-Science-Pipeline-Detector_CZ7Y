{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-01T11:17:47.131632Z","iopub.execute_input":"2021-06-01T11:17:47.132001Z","iopub.status.idle":"2021-06-01T11:17:47.1525Z","shell.execute_reply.started":"2021-06-01T11:17:47.131969Z","shell.execute_reply":"2021-06-01T11:17:47.151238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2021-06-01T11:17:47.154302Z","iopub.execute_input":"2021-06-01T11:17:47.154587Z","iopub.status.idle":"2021-06-01T11:17:47.158551Z","shell.execute_reply.started":"2021-06-01T11:17:47.154553Z","shell.execute_reply":"2021-06-01T11:17:47.157425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. What is Transfer learning:","metadata":{}},{"cell_type":"markdown","source":"<font color =\"blue\">\nTransfer learning consists of taking features learned on one problem, and leveraging them on a new, similar problem. For instance, features from a model that has learned to identify racoons may be useful to kick-start a model meant to identify tanukis.\n\nTransfer learning is usually done for tasks where your dataset has too little data to train a full-scale model from scratch.","metadata":{}},{"cell_type":"markdown","source":"<font color =\"blue\">\nA pre-trained model is a saved network that was previously trained on a large dataset, typically on a large-scale image-classification task. You either use the pretrained model as is or use transfer learning to customize this model to a given task.\n\nThe intuition behind transfer learning for image classification is that if a model is trained on a large and general enough dataset, this model will effectively serve as a generic model of the visual world. You can then take advantage of these learned feature maps without having to start from scratch by training a large model on a large dataset.","metadata":{}},{"cell_type":"markdown","source":"<font color =\"red\">\n1. Feature Extraction: Use the representations learned by a previous network to extract meaningful features from new samples. You simply add a new classifier, which will be trained from scratch, on top of the pretrained model so that you can repurpose the feature maps learned previously for the dataset.\n\nYou do not need to (re)train the entire model. The base convolutional network already contains features that are generically useful for classifying pictures. However, the final, classification part of the pretrained model is specific to the original classification task, and subsequently specific to the set of classes on which the model was trained.","metadata":{}},{"cell_type":"markdown","source":"<font color =\"red\">\n2. Fine-Tuning: Unfreeze a few of the top layers of a frozen model base and jointly train both the newly-added classifier layers and the last layers of the base model. This allows us to \"fine-tune\" the higher-order feature representations in the base model in order to make them more relevant for the specific task.","metadata":{}},{"cell_type":"markdown","source":"<font color =\"green\">\nYou will follow the general machine learning workflow.\n\n1. Take layers from a previously trained model.\n    \n2. Freeze them, so as to avoid destroying any of the information they contain during future training rounds.\n\n3. Add some new, trainable layers on top of the frozen layers. They will learn to turn the old features into predictions on a new dataset.\n\n4. Train the new layers on your dataset.\n\n","metadata":{}},{"cell_type":"markdown","source":"## 1. Using Transfer Learning Without Using Our Own Layers","metadata":{}},{"cell_type":"markdown","source":"## Examples from Resnet50:","metadata":{}},{"cell_type":"markdown","source":"<font color=\"blue\">\nResNet-50 is a convolutional neural network that is 50 layers deep. You can load a pretrained version of the network trained on more than a million images from the ImageNet database [1]. The pretrained network can classify images into 1000 object categories, such as keyboard, mouse, pencil, and many animals. As a result, the network has learned rich feature representations for a wide range of images. The network has an image input size of 224-by-224.\n\n         ResNet50(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000, **kwargs)","metadata":{}},{"cell_type":"code","source":"plt.imshow(plt.imread(\"../input/elephant/elephant.jfif\"))\n# We have this image and without training our model, we will transfer reshnet50","metadata":{"execution":{"iopub.status.busy":"2021-06-01T11:17:47.163428Z","iopub.execute_input":"2021-06-01T11:17:47.163825Z","iopub.status.idle":"2021-06-01T11:17:47.355443Z","shell.execute_reply.started":"2021-06-01T11:17:47.163789Z","shell.execute_reply":"2021-06-01T11:17:47.354486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.applications.resnet50 import ResNet50\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\nimport numpy as np\n\nmodel = ResNet50(weights='imagenet')\n\nimg_path = '../input/elephant/elephant.jfif'\nimg = image.load_img(img_path, target_size=(224, 224))\n# we convert imgae into 224*224 because resnet has this size of images\nx = image.img_to_array(img)\nx = np.expand_dims(x, axis=0)\nx = preprocess_input(x)\n\npreds = model.predict(x)\n# decode the results into a list of tuples (class, description, probability)\n# (one such list for each sample in the batch)\nprint('Predicted:', decode_predictions(preds, top=3)[0])","metadata":{"execution":{"iopub.status.busy":"2021-06-01T11:17:47.357059Z","iopub.execute_input":"2021-06-01T11:17:47.357592Z","iopub.status.idle":"2021-06-01T11:17:50.116424Z","shell.execute_reply.started":"2021-06-01T11:17:47.357556Z","shell.execute_reply":"2021-06-01T11:17:50.115472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds.size","metadata":{"execution":{"iopub.status.busy":"2021-06-01T11:17:50.118004Z","iopub.execute_input":"2021-06-01T11:17:50.118255Z","iopub.status.idle":"2021-06-01T11:17:50.123519Z","shell.execute_reply.started":"2021-06-01T11:17:50.118231Z","shell.execute_reply":"2021-06-01T11:17:50.122525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"decode_predictions(preds)","metadata":{"execution":{"iopub.status.busy":"2021-06-01T11:17:50.124852Z","iopub.execute_input":"2021-06-01T11:17:50.125325Z","iopub.status.idle":"2021-06-01T11:17:50.136059Z","shell.execute_reply.started":"2021-06-01T11:17:50.125282Z","shell.execute_reply":"2021-06-01T11:17:50.135273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font color=\"red\">\nPredicted: [('n01871265', 'tusker', 0.5553246), ('n02504458', 'African_elephant', 0.37550497), ('n02504013', 'Indian_elephant', 0.0690713)]","metadata":{}},{"cell_type":"code","source":"print(f\"It is a {decode_predictions(preds)[0][0][1]} with {decode_predictions(preds)[0][0][2]}\")","metadata":{"execution":{"iopub.status.busy":"2021-06-01T11:17:50.137063Z","iopub.execute_input":"2021-06-01T11:17:50.137486Z","iopub.status.idle":"2021-06-01T11:17:50.14932Z","shell.execute_reply.started":"2021-06-01T11:17:50.137447Z","shell.execute_reply":"2021-06-01T11:17:50.148283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(plt.imread('../input/download1/download.jfif'))\n\n","metadata":{"execution":{"iopub.status.busy":"2021-06-01T11:17:50.150685Z","iopub.execute_input":"2021-06-01T11:17:50.150993Z","iopub.status.idle":"2021-06-01T11:17:50.308375Z","shell.execute_reply.started":"2021-06-01T11:17:50.150964Z","shell.execute_reply":"2021-06-01T11:17:50.307664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing import image\nimg_path = '../input/download1/download.jfif'\nimg = image.load_img(img_path, target_size=(224, 224))\n# we convert imgae into 224*224 because resnet has this size of images\nx = image.img_to_array(img)\nprint(x)","metadata":{"execution":{"iopub.status.busy":"2021-06-01T11:17:50.309323Z","iopub.execute_input":"2021-06-01T11:17:50.309711Z","iopub.status.idle":"2021-06-01T11:17:50.318889Z","shell.execute_reply.started":"2021-06-01T11:17:50.309683Z","shell.execute_reply":"2021-06-01T11:17:50.317822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing import image\nimg_path = '../input/download1/download.jfif'\nimg = image.load_img(img_path, target_size=(224, 224))\n# we convert imgae into 224*224 because resnet has this size of images\nx = image.img_to_array(img)\nx= np.expand_dims(x,axis=0)\nprint(x)","metadata":{"execution":{"iopub.status.busy":"2021-06-01T11:17:50.321568Z","iopub.execute_input":"2021-06-01T11:17:50.321985Z","iopub.status.idle":"2021-06-01T11:17:50.332681Z","shell.execute_reply.started":"2021-06-01T11:17:50.321956Z","shell.execute_reply":"2021-06-01T11:17:50.331003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing import image\nimg_path = '../input/download1/download.jfif'\nimg = image.load_img(img_path, target_size=(224, 224))\n# we convert imgae into 224*224 because resnet has this size of images\nx = image.img_to_array(img)\nx= np.expand_dims(x,axis=0)\nx= preprocess_input(x)\nprint(x)","metadata":{"execution":{"iopub.status.busy":"2021-06-01T11:17:50.334784Z","iopub.execute_input":"2021-06-01T11:17:50.335067Z","iopub.status.idle":"2021-06-01T11:17:50.345161Z","shell.execute_reply.started":"2021-06-01T11:17:50.335042Z","shell.execute_reply":"2021-06-01T11:17:50.343415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing import image\nimg_path = '../input/download1/download.jfif'\nimg = image.load_img(img_path, target_size=(224, 224))\n# we convert imgae into 224*224 because resnet has this size of images\nx = image.img_to_array(img)\nx= np.expand_dims(x,axis=0)\nx= preprocess_input(x)\npreds = model.predict(x)\n# decode the results into a list of tuples (class, description, probability)\n# (one such list for each sample in the batch)\nprint('Predicted:', decode_predictions(preds, top=3)[0])\n#The model truely predict the image as desktop computer with %96 chance","metadata":{"execution":{"iopub.status.busy":"2021-06-01T11:17:50.346726Z","iopub.execute_input":"2021-06-01T11:17:50.347342Z","iopub.status.idle":"2021-06-01T11:17:50.501779Z","shell.execute_reply.started":"2021-06-01T11:17:50.347306Z","shell.execute_reply":"2021-06-01T11:17:50.501099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Using Transfer Learning's Trained Convolutions and Adding Our Own Layers","metadata":{}},{"cell_type":"markdown","source":"## An Example From InceptionV3","metadata":{}},{"cell_type":"markdown","source":"<font color =\"blue\">\ntf.keras.applications.InceptionV3(\n    include_top=True,\n    weights=\"imagenet\",\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation=\"softmax\",\n)","metadata":{}},{"cell_type":"markdown","source":"1. First, instantiate a base model with pre-trained weights.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.applications.inception_v3 import InceptionV3\nbase_model = InceptionV3(weights='imagenet',  # Load weights pre-trained on ImageNet.\n    input_shape=(150, 150, 3),#we can specify the inpur shape with this parameter\n    include_top=False) # Do not include the ImageNet classifier at the top.","metadata":{"execution":{"iopub.status.busy":"2021-06-01T11:17:50.502624Z","iopub.execute_input":"2021-06-01T11:17:50.502848Z","iopub.status.idle":"2021-06-01T11:17:52.708701Z","shell.execute_reply.started":"2021-06-01T11:17:50.502826Z","shell.execute_reply":"2021-06-01T11:17:52.707201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2. Then, freeze the base model.","metadata":{}},{"cell_type":"code","source":"base_model.trainable = False # We freeze the training of the convolutions","metadata":{"execution":{"iopub.status.busy":"2021-06-01T11:17:52.710334Z","iopub.execute_input":"2021-06-01T11:17:52.710763Z","iopub.status.idle":"2021-06-01T11:17:52.724005Z","shell.execute_reply.started":"2021-06-01T11:17:52.710717Z","shell.execute_reply":"2021-06-01T11:17:52.722824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font color=\"blue\">\nIt is important to freeze the convolutional base before you compile and train the model. Freezing (by setting layer.trainable = False) prevents the weights in a given layer from being updated during training. MobileNet V2 has many layers, so setting the entire model's trainable flag to False will freeze all of them.","metadata":{}},{"cell_type":"markdown","source":"3. Create a new model on top.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import Flatten, Dense, Dropout\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D\nfrom tensorflow.keras.optimizers import RMSprop\nbase = base_model.output\n# Flatten the output layer to 1 dimension\nmodel = Flatten()(base)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-06-01T11:17:52.725588Z","iopub.execute_input":"2021-06-01T11:17:52.726037Z","iopub.status.idle":"2021-06-01T11:17:52.742133Z","shell.execute_reply.started":"2021-06-01T11:17:52.725994Z","shell.execute_reply":"2021-06-01T11:17:52.741069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Add a fully connected layer with 1,024 hidden units and ReLU activation\nmodel = Dense(units=1024, activation =\"relu\")(model)\n# Add a dropout rate of 0.2\nmodel = Dropout(0.2)(model)\n# Add a final sigmoid layer for classification\nmodel = Dense(units = 1, activation = \"sigmoid\")(model)\nmodel = Model( base_model.input, model)\nmodel.compile(optimizer = RMSprop(lr=0.0001), \n              loss = 'binary_crossentropy', \n              metrics = ['accuracy'])\n","metadata":{"execution":{"iopub.status.busy":"2021-06-01T11:17:52.743624Z","iopub.execute_input":"2021-06-01T11:17:52.744058Z","iopub.status.idle":"2021-06-01T11:17:53.017226Z","shell.execute_reply.started":"2021-06-01T11:17:52.74402Z","shell.execute_reply":"2021-06-01T11:17:53.016357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-06-01T11:17:53.018366Z","iopub.execute_input":"2021-06-01T11:17:53.018637Z","iopub.status.idle":"2021-06-01T11:17:53.130743Z","shell.execute_reply.started":"2021-06-01T11:17:53.018585Z","shell.execute_reply":"2021-06-01T11:17:53.129706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import zipfile\nlocal_zip = \"../input/dogs-vs-cats-redux-kernels-edition/train.zip\"\nzip_ref = zipfile.ZipFile(local_zip, 'r')\nzip_ref.extractall('/kaggle/working/')\nzip_ref.close()","metadata":{"execution":{"iopub.status.busy":"2021-06-01T11:17:53.132196Z","iopub.execute_input":"2021-06-01T11:17:53.132572Z","iopub.status.idle":"2021-06-01T11:18:03.351172Z","shell.execute_reply.started":"2021-06-01T11:17:53.132533Z","shell.execute_reply":"2021-06-01T11:18:03.350058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_dir = '/kaggle/working/'\ntrain_dir = os.path.join(base_dir, 'train')\ntrain_img_names = os.listdir(train_dir)","metadata":{"execution":{"iopub.status.busy":"2021-06-01T11:18:03.352314Z","iopub.execute_input":"2021-06-01T11:18:03.352558Z","iopub.status.idle":"2021-06-01T11:18:03.368894Z","shell.execute_reply.started":"2021-06-01T11:18:03.352534Z","shell.execute_reply":"2021-06-01T11:18:03.36797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"local_zip = \"../input/dogs-vs-cats-redux-kernels-edition/test.zip\"\nzip_ref = zipfile.ZipFile(local_zip, 'r')\nzip_ref.extractall('/kaggle/working/')\nzip_ref.close()","metadata":{"execution":{"iopub.status.busy":"2021-06-01T11:18:03.370075Z","iopub.execute_input":"2021-06-01T11:18:03.37034Z","iopub.status.idle":"2021-06-01T11:18:08.582534Z","shell.execute_reply.started":"2021-06-01T11:18:03.370314Z","shell.execute_reply":"2021-06-01T11:18:08.581585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_dir = '/kaggle/working/'\ntest_dir = os.path.join(base_dir, 'test')\ntest_img_names = os.listdir(test_dir)","metadata":{"execution":{"iopub.status.busy":"2021-06-01T11:18:08.583922Z","iopub.execute_input":"2021-06-01T11:18:08.584288Z","iopub.status.idle":"2021-06-01T11:18:08.59677Z","shell.execute_reply.started":"2021-06-01T11:18:08.584251Z","shell.execute_reply":"2021-06-01T11:18:08.595542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_img_names[:10])","metadata":{"execution":{"iopub.status.busy":"2021-06-01T11:18:08.598044Z","iopub.execute_input":"2021-06-01T11:18:08.598342Z","iopub.status.idle":"2021-06-01T11:18:08.607492Z","shell.execute_reply.started":"2021-06-01T11:18:08.598313Z","shell.execute_reply":"2021-06-01T11:18:08.606446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(test_img_names[:10])","metadata":{"execution":{"iopub.status.busy":"2021-06-01T11:18:08.608822Z","iopub.execute_input":"2021-06-01T11:18:08.609354Z","iopub.status.idle":"2021-06-01T11:18:08.619822Z","shell.execute_reply.started":"2021-06-01T11:18:08.609324Z","shell.execute_reply":"2021-06-01T11:18:08.618691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('total training images :', len(train_img_names ))","metadata":{"execution":{"iopub.status.busy":"2021-06-01T11:18:08.621222Z","iopub.execute_input":"2021-06-01T11:18:08.621693Z","iopub.status.idle":"2021-06-01T11:18:08.631904Z","shell.execute_reply.started":"2021-06-01T11:18:08.621663Z","shell.execute_reply":"2021-06-01T11:18:08.630761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('total training images :', len(test_img_names ))","metadata":{"execution":{"iopub.status.busy":"2021-06-01T11:18:08.635357Z","iopub.execute_input":"2021-06-01T11:18:08.635719Z","iopub.status.idle":"2021-06-01T11:18:08.644613Z","shell.execute_reply.started":"2021-06-01T11:18:08.635687Z","shell.execute_reply":"2021-06-01T11:18:08.643539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets separate  dogs and cats for  dataset","metadata":{}},{"cell_type":"code","source":"categories = list()\nfor image in train_img_names:\n    categories.append(image[:3])\ncategories[:10]\n","metadata":{"execution":{"iopub.status.busy":"2021-06-01T11:18:08.646955Z","iopub.execute_input":"2021-06-01T11:18:08.64741Z","iopub.status.idle":"2021-06-01T11:18:08.664702Z","shell.execute_reply.started":"2021-06-01T11:18:08.647366Z","shell.execute_reply":"2021-06-01T11:18:08.663935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame({\"Image\":train_img_names, \"Category\": categories})\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-01T11:18:08.665969Z","iopub.execute_input":"2021-06-01T11:18:08.666366Z","iopub.status.idle":"2021-06-01T11:18:08.68946Z","shell.execute_reply.started":"2021-06-01T11:18:08.666337Z","shell.execute_reply":"2021-06-01T11:18:08.688443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,10))\nsns.countplot(data=df, x=\"Category\",palette=\"magma\")","metadata":{"execution":{"iopub.status.busy":"2021-06-01T11:18:08.691204Z","iopub.execute_input":"2021-06-01T11:18:08.691522Z","iopub.status.idle":"2021-06-01T11:18:08.834482Z","shell.execute_reply.started":"2021-06-01T11:18:08.691479Z","shell.execute_reply":"2021-06-01T11:18:08.83347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\nsample = random.choice(train_img_names)\nplt.imshow(plt.imread((\"/kaggle/working/train/\"+sample)))","metadata":{"execution":{"iopub.status.busy":"2021-06-01T11:18:08.836003Z","iopub.execute_input":"2021-06-01T11:18:08.836399Z","iopub.status.idle":"2021-06-01T11:18:09.031999Z","shell.execute_reply.started":"2021-06-01T11:18:08.83636Z","shell.execute_reply":"2021-06-01T11:18:09.031008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain,validation= train_test_split(df, test_size=0.1)\ntrain = train.reset_index(drop=True)\nvalidation = validation.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-01T11:18:09.033282Z","iopub.execute_input":"2021-06-01T11:18:09.03356Z","iopub.status.idle":"2021-06-01T11:18:09.922139Z","shell.execute_reply.started":"2021-06-01T11:18:09.033531Z","shell.execute_reply":"2021-06-01T11:18:09.920941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train.shape)\nprint(validation.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-01T11:18:09.923549Z","iopub.execute_input":"2021-06-01T11:18:09.923898Z","iopub.status.idle":"2021-06-01T11:18:09.931617Z","shell.execute_reply.started":"2021-06-01T11:18:09.923867Z","shell.execute_reply":"2021-06-01T11:18:09.930506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# All images will be rescaled by 1./255.\ntrain_datagen = ImageDataGenerator(rescale = 1./255.,\n                                   rotation_range = 40,\n                                   width_shift_range = 0.2,\n                                   height_shift_range = 0.2,\n                                   shear_range = 0.2,\n                                   zoom_range = 0.2,\n                                   horizontal_flip = True)\n\n\n# --------------------\n# Flow training images in batches of 20 using train_datagen generator\n# --------------------\ntrain_generator = train_datagen.flow_from_dataframe(train,\n                                                    directory=\"./train\",\n                                                    x_col='Image',\n                                                    y_col='Category',\n                                                    batch_size=20,\n                                                    class_mode='binary',\n                                                    target_size=(150, 150)) ","metadata":{"execution":{"iopub.status.busy":"2021-06-01T11:18:09.933212Z","iopub.execute_input":"2021-06-01T11:18:09.933573Z","iopub.status.idle":"2021-06-01T11:18:10.151551Z","shell.execute_reply.started":"2021-06-01T11:18:09.933541Z","shell.execute_reply":"2021-06-01T11:18:10.150587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Note that the validation data should not be augmented!\nvalidation_datagen  = ImageDataGenerator( rescale = 1.0/255.)\nvalidation_generator =  validation_datagen.flow_from_dataframe(validation,\n                                                            directory=\"./train\",\n                                                              x_col='Image',\n                                                             y_col='Category',\n                                                              batch_size=20,\n                                                              class_mode  = 'binary',\n                                                              target_size = (150, 150))","metadata":{"execution":{"iopub.status.busy":"2021-06-01T11:18:10.153145Z","iopub.execute_input":"2021-06-01T11:18:10.153447Z","iopub.status.idle":"2021-06-01T11:18:10.187477Z","shell.execute_reply.started":"2021-06-01T11:18:10.153417Z","shell.execute_reply":"2021-06-01T11:18:10.186561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping\ncallback=EarlyStopping(monitor=\"val_loss\", patience=4)\nhistory=model.fit(train_generator, \n                  validation_data=validation_generator,\n                  steps_per_epoch = 100,\n                  epochs=20,\n                  validation_steps = 50,\n                  callbacks=[callback])","metadata":{"execution":{"iopub.status.busy":"2021-06-01T11:18:10.188746Z","iopub.execute_input":"2021-06-01T11:18:10.189017Z","iopub.status.idle":"2021-06-01T11:34:49.061329Z","shell.execute_reply.started":"2021-06-01T11:18:10.18899Z","shell.execute_reply":"2021-06-01T11:34:49.060286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set_style(\"darkgrid\")\npd.DataFrame(model.history.history).plot(figsize=(15,10))","metadata":{"execution":{"iopub.status.busy":"2021-06-01T11:59:45.325379Z","iopub.execute_input":"2021-06-01T11:59:45.32575Z","iopub.status.idle":"2021-06-01T11:59:45.625692Z","shell.execute_reply.started":"2021-06-01T11:59:45.325718Z","shell.execute_reply":"2021-06-01T11:59:45.624304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font color=\"blue\">\n","metadata":{}}]}