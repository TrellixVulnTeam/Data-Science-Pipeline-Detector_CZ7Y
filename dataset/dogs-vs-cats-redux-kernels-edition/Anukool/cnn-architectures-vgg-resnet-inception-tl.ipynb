{"cells":[{"metadata":{"_uuid":"5a9fbf4a-cc99-4a9b-b23b-53e183222e12","_cell_guid":"5e3fffd3-adcb-485b-80c1-106d92bb5d55","trusted":true},"cell_type":"markdown","source":"## CNN Architectures : VGG, Resnet, InceptionNet, XceptionNet \n\n### UseCases : Image Feature Extraction + Transfer Learning\n\n\n<br>\n\nA Gold mine dataset for comuter vision is the ImageNet dataset. It consists of about 14 M hand-labelled annotated images which contains over 22,000 day-to-day categories. Every year ImageNet competition is hosted in which the smaller version of this dataset (with 1000 categories) is used with an aim to accurately classify the images. Many winning solutions of the ImageNet Challenge have used state of the art convolutional neural network architectures to beat the best possible accuracy thresholds. In this kernel, I have discussed these popular architectures such as VGG16, 19, ResNet, AlexNet etc. In the end, I have explained how to generate image features using pretrained models and use them in machine learning models. \n\n## Contents \n\n<br>\n\nFrom the high level perspective, I have discussed three main components \n\n<ul>\n    <li>1. CNN Architectures   </li>\n<ul>\n    <li>1. 1 VGG16</li>\n    <li>1.2 VGG19 </li>\n    <li>1.3 InceptionNet</li>\n    <li>1.4 Resnet </li>\n    <li>1.5 XceptionNet</li>\n</ul>\n    <li>2. Image Feature Extraction  </li>\n    <li>3. Transfer Learning  </li>\n</ul>\n\n\n## 1. CNN Architectures\n## 1.1 &nbsp;&nbsp; VGG16 \n\nVGG16 was publised in 2014 and is one of the simplest (among the other cnn architectures used in Imagenet competition). It's Key Characteristics are:   \n\n1. This network contains total 16 layers in which weights and bias parameters are learnt.    \n2. A total of 13 convolutional layers are stacked one after the other and 3 dense layers for classification.     \n3. The number of filters in the convolution layers follow an increasing pattern (similar to decoder architecture of autoencoder).     \n4. The informative features are obtained by max pooling layers applied at different steps in the architecture.    \n5. The dense layers comprises of 4096, 4096, and 1000 nodes each.   \n6. The cons of this architecture are that it is slow to train and produces the model with very large size.   \n\nThe VGG16 architecture is given below: \n\n![](https://tech.showmax.com/2017/10/convnet-architectures/image_0-8fa3b810.png)\n\n## Implementation : VGG16\nLet's see how we can create this architecture using python's keras library. The following code block shows the implementation of VGG16 in keras.","execution_count":null},{"metadata":{"_uuid":"8d40fafe-f328-42ea-918e-17d5de12c2c9","_cell_guid":"e3a8b3a0-9515-4675-8cfc-b09ac108128b","trusted":true},"cell_type":"code","source":"from keras.layers import Input, Conv2D, MaxPooling2D\nfrom keras.layers import Dense, Flatten\nfrom keras.models import Model\n\n_input = Input((224,224,1)) \n\nconv1  = Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(_input)\nconv2  = Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv1)\npool1  = MaxPooling2D((2, 2))(conv2)\n\nconv3  = Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(pool1)\nconv4  = Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv3)\npool2  = MaxPooling2D((2, 2))(conv4)\n\nconv5  = Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(pool2)\nconv6  = Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv5)\nconv7  = Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv6)\npool3  = MaxPooling2D((2, 2))(conv7)\n\nconv8  = Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(pool3)\nconv9  = Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv8)\nconv10 = Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv9)\npool4  = MaxPooling2D((2, 2))(conv10)\n\nconv11 = Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(pool4)\nconv12 = Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv11)\nconv13 = Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv12)\npool5  = MaxPooling2D((2, 2))(conv13)\n\nflat   = Flatten()(pool5)\ndense1 = Dense(4096, activation=\"relu\")(flat)\ndense2 = Dense(4096, activation=\"relu\")(dense1)\noutput = Dense(1000, activation=\"softmax\")(dense2)\n\nvgg16_model  = Model(inputs=_input, outputs=output)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4262ef70-03b3-4304-93b6-b9abb0519c4c","_cell_guid":"6e68bffd-7f5c-4c0a-a448-e6fc533f55b0","trusted":true},"cell_type":"markdown","source":"## PreTrained Model : VGG16\n\n\n\nKeras library also provides the pre-trained model in which one can load the saved model weights, and use them for different purposes : transfer learning, image feature extraction, and object detection. We can load the model architecture given in the library, and then add all the weights to the respective layers. \n\nBefore using the pretrained models, lets write a few functions which will be used to make some predictions. First, load some images and preprocess them.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import os \nimport zipfile\n\nwith zipfile.ZipFile(\"../input/dogs-vs-cats-redux-kernels-edition/train.zip\",\"r\") as z:\n     z.extractall(\"../working/dogs-vs-cats/train\")\n    \nwith zipfile.ZipFile(\"../input/dogs-vs-cats-redux-kernels-edition/test.zip\",\"r\") as z:\n    z.extractall(\"../working/dogs-vs-cats/test\")\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nprint(os.listdir('../working/dogs-vs-cats/'))\n\n\n# os.mkdir('../working/dogs-vs-cats/train')\n# os.mkdir('../working/dogs-vs-cats/test')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"32589743-61b3-42a0-991d-78a1d34679e1","_cell_guid":"6f9fa20e-b816-465c-aa14-cb8532fc3e3d","trusted":true},"cell_type":"code","source":"from keras.applications.vgg16 import decode_predictions\nfrom keras.applications.vgg16 import preprocess_input\nfrom keras.preprocessing import image\nimport matplotlib.pyplot as plt \nfrom PIL import Image \nimport seaborn as sns\nimport pandas as pd \nimport numpy as np \n\n\n# ../input/output/train\n# ../input/dogs-vs-cats-redux-kernels-edition/sample_submission.csv\n# ../input/output/train\n    \nimg1 = \"../working/dogs-vs-cats/train/train/cat.283.jpg\"\nimg2 = \"../working/dogs-vs-cats/train/train/dog.2811.jpg\"\nimg3 = \"../input/flowers-recognition/flowers/flowers/sunflower/7791014076_07a897cb85_n.jpg\"\nimg4 = \"../input/fruits/fruits-360/Training/Banana/254_100.jpg\"\nimgs = [img1, img2, img3, img4]\n\ndef _load_image(img_path):\n    img = image.load_img(img_path, target_size=(224, 224))\n    img = image.img_to_array(img)\n    img = np.expand_dims(img, axis=0)\n    img = preprocess_input(img)\n    return img \n\ndef _get_predictions(_model):\n    all_preds=[]\n    f, axes = plt.subplots(1, 4)\n    f.set_size_inches(80, 20)\n    for i,img_path in enumerate(imgs):\n        img = _load_image(img_path)\n        preds  = decode_predictions(_model.predict(img), top=3)[0]\n        all_preds.append(preds[0][1])\n        b = sns.barplot(y=[c[1] for c in preds], x=[c[2] for c in preds], color=\"gray\", ax=axes[i])\n        b.tick_params(labelsize=55)\n        f.tight_layout()\n        \n    f, ax = plt.subplots(1, 4)\n    f.set_size_inches(80, 40)\n\n    for i in range(4):\n        ax[i].imshow(Image.open(imgs[i]).resize((200, 200), Image.ANTIALIAS))    \n        ax[i].set_title(all_preds[i],size=60)\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f7e5a24c-c222-4c32-b460-7b66b5beea78","_cell_guid":"595eee7c-c8ad-4fdb-b74e-7bb869be97f7","trusted":true},"cell_type":"markdown","source":"Now, we can perform following steps : \n1. import VGG16 architecture from keras.applications  \n2. Add the saved weights to the architecture \n3. Use model to perform predictions","execution_count":null},{"metadata":{"_uuid":"117b541d-02f6-4f32-8c88-609fb248a324","_cell_guid":"61929913-0d09-4272-aa85-a3b7ddac9f4a","trusted":true},"cell_type":"code","source":"from keras.applications.vgg16 import VGG16\nvgg16_weights = '../input/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5'\nvgg16_model = VGG16(weights=vgg16_weights)\n_get_predictions(vgg16_model)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"64a52ed3-2614-41b6-a97f-f9e32852f733","_cell_guid":"4a3236cc-7471-42c5-aab2-f34be9e51ea8","trusted":true},"cell_type":"markdown","source":"## 1.2 &nbsp;&nbsp; VGG19 \n\nVGG19 is a similar model architecure as VGG16 with three additional convolutional layers, it consists of a total of 16 Convolution layers and 3 dense layers.  Following is the architecture of VGG19 model. In VGG networks, the use of 3 x 3 convolutions with stride 1 gives an effective receptive filed equivalent to 7 * 7. This means there are fewer parameters to train. \n\n![](https://cdn-images-1.medium.com/max/1600/1*cufAO77aeSWdShs3ba5ndg.jpeg)","execution_count":null},{"metadata":{"_uuid":"58e0d126-88a3-4839-893f-38a1e27a3e71","_cell_guid":"52c09648-188b-48a7-b3c7-51583653be67","trusted":true},"cell_type":"code","source":"from keras.applications.vgg19 import VGG19\nvgg19_weights = '../input/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels.h5'\nvgg19_model = VGG19(weights=vgg19_weights)\n_get_predictions(vgg19_model)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"94954efa-235c-4ca6-abee-da46041e9b70","_cell_guid":"89b9a7e1-f792-4965-9168-8132dc3a77fe","trusted":true},"cell_type":"markdown","source":"## &nbsp;&nbsp; 1.3 InceptionNets\n\nAlso known as GoogleNet consists of total 22 layers and was the winning model of 2014 image net challenge. \n\n- Inception modules are the fundamental block of InceptionNets. The key idea of inception module is to design good local network topology (network within a network)  \n- These modules or blocks acts as the multi-level feature extractor in which convolutions of different sizes are obtained to create a diversified feature map\n- The inception modules also consists of 1 x 1 convolution blocks whose role is to perform dimentionaltiy reduction.  \n- By performing the 1x1 convolution, the inception block preserves the spatial dimentions but reduces the depth. So the overall network's dimentions are not increased exponentially.  \n- Apart from the regular output layer, this network also consists of two auxillary classification outputs which are used to inject gradients at lower layers.  \n\n<br><br>\n\nThe inception module is shown in the following figure:  \n\n![](https://hackathonprojects.files.wordpress.com/2016/09/inception_implement.png?w=649&h=337)\n\nThe complete architecture is shown below: \n\n![](https://cdn-images-1.medium.com/max/2000/1*uXfC5fcbDsL0TJG4T8PsVw.png)\n\n<br>\n\n### Pre-Trained Model : InceptionV3","execution_count":null},{"metadata":{"_uuid":"a9b4e25d-47a8-4cb8-9665-91fbd0d85303","_cell_guid":"4c272176-54eb-41da-bf8f-0af5da908954","trusted":true},"cell_type":"code","source":"from keras.applications.inception_v3 import InceptionV3\ninception_weights = '../input/inceptionv3/inception_v3_weights_tf_dim_ordering_tf_kernels.h5'\ninception_model = InceptionV3(weights=inception_weights)\n_get_predictions(inception_model)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e716bf67-c6cd-4928-a8a8-cb31232fa3ea","_cell_guid":"27335447-9313-4554-934c-5d054a05ef75","trusted":true},"cell_type":"markdown","source":"## 1.4 Resnets\n\nOriginal Paper : https://arxiv.org/pdf/1512.03385.pdf\n\nAll the previous models used deep neural networks in which they stacked many convolution layers one after the other. It was learnt that deeper networks are performing better. However, it turned out that this is not really true. Following are the problems with deeper networks: \n\n- Network becomes difficult to optimize  \n- Vanishing / Exploding Gradeints  \n- Degradation Problem ( accuracy first saturates and then degrades )  \n\n### Skip Connections   \n\nSo to address these problems, authors of the resnet architecture came up with the idea of skip connections with the hypothesis that the deeper layers should be able to learn something as equal as shallower layers. A possible solution is copying the activations from shallower layers and setting additional layers to identity mapping.   These connections are enabled by skip connections which are shown in the following figure. \n\n![](https://cdn-images-1.medium.com/max/987/1*pUyst_ciesOz_LUg0HocYg.png)\n\nSo the role of these connections is to perform identity function over the activation of shallower layer, which in-turn produces the same activation. This output is then added with the activation of the next layer. To enable these connections or essentially enable this addition operation, one need to ensure the same dimentions of convolutions through out the network, that's why resnets have same 3 by 3 convolutions throughout. \n\n### Key Advantage \n\nBy using residual blocks in the network, one can construct networks of any depth with the hypothesis that new layers are actually helping to learn new underlying patterns in the input data. The authors of the paper were able to create the deep neural network architecture with 152 layers. The variants of Resnets such as resnet34, resnet50, resnet101 have produced the solutions with very high accuracy in Imagenet competitions. \n\n### Why it works ? \n\nLets discuss why residual networks are successful and enables the addition of more and more layers without the key problems ie. without hurting the network performance. \n\nConsider a plain neural network (A) without residual network as shown. So in the network (A) the input X is passed to this Neural Network (NN) to give the activation A1. \n\n  <br>\n\n![](https://i.imgur.com/9j8bKaY.png)\n\nNow, consider a more deeper network (B) in which a residual block (with 2 extra layers and a skip connection) is added in the previous network. So now, the activation A1 is being passed to Residual Block which in turns gives new activation A3. \n\nif there was no skip connection, then A3 was: \n\n>  A3 = relu ( W2 . A2 + b2)              ..... (without skip connection)\n\nwhere W2 and b2 are weights and bias associated with layer L2. But, with skip connection another term A1 will be passed to L2. So the equation of A3 will be modified as: \n\n> A3 = relu ( W2 . A2 + b2 + A1) \n\nIf we use L2 regularization or the weight decay methods, they will force W2 and b2 to become close to zero. In the worst case, if these become zero, then \n\n> A3 = relu (A1)   \n\nbecause relu will output 0 for negative, A1 for positive and we know that A1 is previous activation from relu which is positive. \n\n> A3 = A1 \n\nThis means that Identitiy function is easy for residual blocks to learn. By addition of residual blocks, model complexity was not increased. As this is only copying the previous activation to the next layers. However this is only the worst case situation, but the it may turn out that these additional layers learns something useful. In that case, the network performance will improve. \n\nHence, adding the residual blocks / skip connections does not hurt the network performance but infact increases the chances that new layers will learn something useful. \n\nLet's look at the usage using pre-trained resnet 50 model.","execution_count":null},{"metadata":{"_uuid":"f3136b8f-f549-4fb5-ac82-01ce84391cab","_cell_guid":"693a91df-a324-4b4c-81f5-fb9c49048eba","trusted":true},"cell_type":"code","source":"from keras.applications.resnet50 import ResNet50\nresnet_weights = '../input/resnet50/resnet50_weights_tf_dim_ordering_tf_kernels.h5'\nresnet_model = ResNet50(weights=resnet_weights)\n_get_predictions(resnet_model)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2d16a67f-756a-445e-9f0e-3f3f732571f0","_cell_guid":"bb108692-d55a-4f5f-aed1-6d0c0841e6d1","trusted":true},"cell_type":"markdown","source":"## 1.5 Xception Nets\n\nXception is an extension of the Inception architecture which replaces the standard Inception modules with depthwise separable convolutions.","execution_count":null},{"metadata":{"_uuid":"3071baa6-d0b8-4c02-a95a-712155919e1c","_cell_guid":"5e7f5248-110b-40d2-b69e-90bf74e6534c","trusted":true},"cell_type":"code","source":"from keras.applications.xception import Xception\nxception_weights = '../input/xception/xception_weights_tf_dim_ordering_tf_kernels.h5'\nxception_model = Xception(weights=xception_weights)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_get_predictions(xception_model)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fdf0f824-bfb4-47b8-b342-4fb17ed557cf","_cell_guid":"0fd3f5a1-7f05-405c-8091-bed3ae4ae017","trusted":true},"cell_type":"markdown","source":"#### Comparison of different architectures: \n\nThe following image describes the relative comparison of these architectures in terms of performance as size. \n<br><br>\n\n![](http://www.houseofbots.com/images/news/573/cover.png)\n\n<br><br>\n\n## 2. Image Feature Extraction using PreTrained Models \n\nLets look at how one can use pre-trained models for feature extraction, The extracted features can be used for Machine Learning purposes. \n\nFirst step is to load the weights of the pre-trained model in the model architecture. Notice, that an additional argument is passed include_top = False, which states that we do not want to add the last layer of this architecture.","execution_count":null},{"metadata":{"_uuid":"01f7ff20-5392-46f2-9027-ef8e6dc46f0b","_cell_guid":"34d867fb-d9d9-4aff-9eff-6b4973d2597f","trusted":true},"cell_type":"code","source":"resnet50 = ResNet50(weights='imagenet', include_top=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"48bac323-4ae7-4531-88f5-abce9cec345e","_cell_guid":"dcfcd33f-115b-41ad-bb2f-ac50dfadb46c","trusted":true},"cell_type":"markdown","source":"As the next step, we will pass an image to this model and identify the features.","execution_count":null},{"metadata":{"_uuid":"25f2d605-d182-4e08-a218-24f36e80d2b6","_cell_guid":"4c972f86-7e18-4104-b546-e397485c6277","trusted":true},"cell_type":"code","source":"def _get_features(img_path):\n    img = image.load_img(img_path, target_size=(224, 224))\n    img_data = image.img_to_array(img)\n    img_data = np.expand_dims(img_data, axis=0)\n    img_data = preprocess_input(img_data)\n    resnet_features = resnet50.predict(img_data)\n    return resnet_features\n\nimg_path = \"../working/dogs-vs-cats/train/train/dog.2811.jpg\"\nresnet_features = _get_features(img_path)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f89f1844-cf42-4fee-b1cd-599462139fc1","_cell_guid":"a18c56ee-fe45-4dfe-be6f-ae85c85bf5f1","trusted":true},"cell_type":"markdown","source":"now the extracted features are stored in the variable resnet_features. One can flatten them or sequee them in order to use them in ML models.  Flatten will produce a long vector of feature elements. Squeeze will produce a 3D matrix of the features","execution_count":null},{"metadata":{"_uuid":"8ce78e8b-2582-4fd7-be7b-6f370baf292c","_cell_guid":"717c4821-bbf5-42dd-8728-df7547029c26","trusted":true},"cell_type":"code","source":"features_representation_1 = resnet_features.flatten()\nfeatures_representation_2 = resnet_features.squeeze()\n\nprint (\"Shape 1: \", features_representation_1.shape)\nprint (\"Shape 2: \", features_representation_2.shape)\nprint(resnet_features.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"717dea3e-858a-442c-84c1-535042bf4728","_cell_guid":"c6ef3bf5-13ea-422b-8473-7e5bfac6dc3a","trusted":true},"cell_type":"markdown","source":"## 3. Transfer Learning Example \n\n<br>\n\nLets look at the implemetation of transfer learning using pre-trained model features. First, we 'll create a dataset containing two classes of images : bananas and strawberrys. Also add a test dataset contianing images from both classes.\n\n### 3.1 Dataset Preparation","execution_count":null},{"metadata":{"_uuid":"8ccbbc26-05f8-4496-93ae-6aafced97568","_cell_guid":"be4097f9-2ac8-4ede-88de-94258d1f5191","trusted":true},"cell_type":"code","source":"basepath = \"../input/fruits/fruits-360/Training/\"\nclass1 = os.listdir(basepath + \"Banana/\")\nclass2 = os.listdir(basepath + \"Strawberry/\")\n\ndata = {'banana': class1[:10], \n        'strawberry': class2[:10], \n        'test': [class1[11], class2[11]]}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6414fd2e-8205-4566-82d7-5311ee94c62b","_cell_guid":"64eb98fb-abc3-4faf-98fe-3a01bb78547e","trusted":true},"cell_type":"markdown","source":"Transfer learning can be implemented in two steps: \n\nStep 1 : Image Feature Exraction  \nStep 2 : Training a Classifier  \n\n### Step 1 : Feature Extraction using pre-trained models (resnet50)\n\nIterate in the images, call the same function used in point 2 for image feature extraction, we will use the flatten representation of these features","execution_count":null},{"metadata":{"_uuid":"6bb5459e-44dd-42b9-857d-537ba65e173d","_cell_guid":"d04f7247-6706-40d7-b448-33fa167a3904","trusted":true},"cell_type":"code","source":"features = {\"banana\" : [], \"strawberry\" : [], \"test\" : []}\ntestimgs = []\nfor label, val in data.items():\n    for k, each in enumerate(val):        \n        if label == \"test\" and k == 0:\n            img_path = basepath + \"/Banana/\" + each\n            testimgs.append(img_path)\n        elif label == \"test\" and k == 1:\n            img_path = basepath + \"/Strawberry/\" + each\n            testimgs.append(img_path)\n        else: \n            img_path = basepath + label.title() + \"/\" + each\n        feats = _get_features(img_path)\n        features[label].append(feats.flatten())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"32672776-8649-4612-b5a8-9816bb1e639c","_cell_guid":"697400e3-cf57-4a1d-8815-ef28c51d2726","trusted":true},"cell_type":"markdown","source":"Next, Convert the features from dictionary format to pandas dataframe. A long dataframe will be created. I will be applying variance filter later on this dataframe to reduce the dimentionality. Other ideas to avoid this step : perform PCA / SVD to obtain the dense features.","execution_count":null},{"metadata":{"_uuid":"7a5fc01d-3aa2-415e-8c2d-0498bd38b665","_cell_guid":"65fa89b6-7967-4d90-a1ad-b3802092461e","trusted":true,"scrolled":true},"cell_type":"code","source":"dataset = pd.DataFrame()\nfor label, feats in features.items():\n    temp_df = pd.DataFrame(feats)\n    temp_df['label'] = label\n    dataset = dataset.append(temp_df, ignore_index=True)\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9ead1def-5a32-42c2-9ea4-e3aa2df497ee","_cell_guid":"3cf45096-66c0-4c42-aeaa-1c784400a250","trusted":true},"cell_type":"markdown","source":"Prepare X (predictors) and y (target) from the dataset","execution_count":null},{"metadata":{"_uuid":"01cf9cbc-7352-4d48-8bba-79173229ed60","_cell_guid":"4c1868b7-0799-4544-afbf-7d0e79745a40","trusted":true},"cell_type":"code","source":"y = dataset[dataset.label != 'test'].label\nX = dataset[dataset.label != 'test'].drop('label', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eceac3a0-2b09-4e58-a91d-f9d9ddddf99d","_cell_guid":"2618c2f3-636d-4c9d-bf6e-4074aaafed4b","trusted":true},"cell_type":"markdown","source":"### Step 2: Write a classifier to predict two classes\n\nwe will write a simple neural network (multi layer perceptron classifier) using sklearn for training purposes.","execution_count":null},{"metadata":{"_uuid":"948bf055-f2f6-4811-a8d7-37befc3a109d","_cell_guid":"005564a0-2cc2-48c8-94de-51e9b34e2327","trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import VarianceThreshold\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.pipeline import Pipeline\n\nmodel = MLPClassifier(hidden_layer_sizes=(100, 10))\npipeline = Pipeline([('low_variance_filter', VarianceThreshold()), ('model', model)])\npipeline.fit(X, y)\n\nprint (\"Model Trained on pre-trained features\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d7013549-4148-4bc8-8515-fa7d7c4593a7","_cell_guid":"81a7aae3-8fb0-4223-9f92-fb41827fbc11","trusted":true},"cell_type":"markdown","source":"Let's predict the output on new images and check the outcome.","execution_count":null},{"metadata":{"_uuid":"2c7ffb29-73e0-479c-98f6-c7edb5970ab2","_cell_guid":"d6017ef0-56e5-419a-9f03-3df3d8c6568c","trusted":true},"cell_type":"code","source":"preds = pipeline.predict(features['test'])\n\nf, ax = plt.subplots(1, 2)\nfor i in range(2):\n    ax[i].imshow(Image.open(testimgs[i]).resize((200, 200), Image.ANTIALIAS))\n    ax[i].text(10, 180, 'Predicted: %s' % preds[i], color='k', backgroundcolor='red', alpha=0.8)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2f3a9189-1048-4592-a396-5bf4c8e0b71e","_cell_guid":"7e687792-6fb3-4f0c-8692-cc612454058a","trusted":true},"cell_type":"markdown","source":"So a simple neural network with only 20 rows of training data is able to correctly classify the two images on test set. \n\n### EndNotes \nThanks for viewing this kernel, If you liked it, please upvote.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}