{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"padding:16px;color:white;margin:0;font-size:180%;text-align:left;display:fill;border-radius:5px;background-color:#48C9B0 ;overflow:hidden\"><center>Kaggle JPX Leaderboard Submission</center></div>\n\nThis notebook describes an example of training and building a model with simple feature generation to leaderboard.\n\n\n# Introduction of Features from Source Data\n\nThe feature data will be stored at the following two directory: \n\n* Training Data:  `/train_files`\n\n* Supplemental Data:  `/supplemental_files`","metadata":{"papermill":{"duration":0.045711,"end_time":"2022-04-02T13:51:39.052709","exception":false,"start_time":"2022-04-02T13:51:39.006998","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import os\nfrom decimal import ROUND_HALF_UP, Decimal\n\nimport numpy as np\nimport pandas as pd\nfrom lightgbm import LGBMRegressor\nfrom tqdm import tqdm","metadata":{"papermill":{"duration":2.039395,"end_time":"2022-04-02T13:51:41.227973","exception":false,"start_time":"2022-04-02T13:51:39.188578","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input/jpx-tokyo-stock-exchange-prediction'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set base_dir to load data\nbase_dir = \"../input/jpx-tokyo-stock-exchange-prediction\"\n# There are three types of stock_price.csv\n# We use one in the train_files folder for this notebook.\ntrain_files_dir = f\"{base_dir}/train_files\"\n\n# for forecasting phase leaderboard, you may want to include stock_price.csv in the supplemental_files folder.\n# You can remove \"forecasting phase leaderboard\" comments in this notebook to use stock_price.csv in the supplemental_files folder.\n# forecasting phase leaderboard:\nsupport_files_dir = f\"{base_dir}/supplemental_files\"\n","metadata":{"papermill":{"duration":0.053109,"end_time":"2022-04-02T13:51:41.466618","exception":false,"start_time":"2022-04-02T13:51:41.413509","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load stock price data\ndf_price = pd.read_csv(f\"{train_files_dir}/stock_prices.csv\")\n\n# forecasting phase leaderboard:\ndf_price_supplemental = pd.read_csv(f\"{support_files_dir}/stock_prices.csv\")\ndf_price = pd.concat([df_price, df_price_supplemental])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_price.tail()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stock_list = pd.read_csv(os.path.join(base_dir,'stock_list.csv'))\n\n# include Date\nstock_list['Date'] = stock_list['EffectiveDate'].apply(lambda x: pd.to_datetime(x, format=\"%Y%m%d\")\\\n                                                            .strftime(\"%Y-%m-%d\")\n                                                       )\n\nshare_df = pd.pivot_table(stock_list,\n                           values='IssuedShares',\n                           index= \"SecuritiesCode\", columns=\"Date\",\n                           aggfunc=\"first\")\n\n# trading share as of 2021\nshare_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pre-processing for model building\n\nThis notebook presents a simple model using LightGBM.\n\nFirst, the features are generated using the price change and historical volatility described above.","metadata":{"papermill":{"duration":0.056286,"end_time":"2022-04-02T13:52:12.515867","exception":false,"start_time":"2022-04-02T13:52:12.459581","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def get_features_for_predict(price, code):\n    \"\"\"\n    Args:\n        price (pd.DataFrame)  : pd.DataFrame include stock_price\n        code (int)  : A local code for a listed company\n    Returns:\n        feature DataFrame (pd.DataFrame)\n    \"\"\"\n    close_col = \"AdjustedClose\"\n    feats = price.loc[price[\"SecuritiesCode\"] == code, [\"SecuritiesCode\", close_col]].copy()\n\n    # calculate 2 week return using AdjustedClose\n    feats[\"return_2week\"] = feats[close_col].pct_change(10)\n    # calculate last 1 month return using AdjustedClose\n    feats[\"return_1month\"] = feats[close_col].pct_change(21)\n    # calculate last 3 months return using AdjustedClose\n    feats[\"return_3month\"] = feats[close_col].pct_change(63)\n\n    # calculate 2 week historical volatility using AdjustedClose\n    feats[\"volatility_2week\"] = (\n        np.log(feats[close_col]).diff().rolling(10).std()\n    )\n    # calculate last 1 month historical volatility using AdjustedClose\n    feats[\"volatility_1month\"] = (\n        np.log(feats[close_col]).diff().rolling(21).std()\n    )\n    # calculate last 3 months historical volatility using AdjustedClose\n    feats[\"volatility_3month\"] = (\n        np.log(feats[close_col]).diff().rolling(63).std()\n    )\n\n    # filling data for nan and inf\n    feats = feats.fillna(0)\n    feats = feats.replace([np.inf, -np.inf], 0)\n    # drop AdjustedClose column\n    feats = feats.drop([close_col], axis=1)\n\n    return feats","metadata":{"papermill":{"duration":0.069843,"end_time":"2022-04-02T13:52:12.754025","exception":false,"start_time":"2022-04-02T13:52:12.684182","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fetch prediction target SecuritiesCodes\ncodes = sorted(df_price[\"SecuritiesCode\"].unique())\nlen(codes)","metadata":{"papermill":{"duration":0.079997,"end_time":"2022-04-02T13:52:12.891901","exception":false,"start_time":"2022-04-02T13:52:12.811904","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Label creation\n\nNext, we obtain the labels to be used for training the model (this is where we load and split the label data).","metadata":{"papermill":{"duration":0.145502,"end_time":"2022-04-02T13:52:39.586544","exception":false,"start_time":"2022-04-02T13:52:39.441042","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def get_label(price, code):\n    \"\"\" Labelizer\n    Args:\n        price (pd.DataFrame): dataframe of stock_price.csv\n        code (int): Local Code in the universe\n    Returns:\n        df (pd.DataFrame): label data\n    \"\"\"\n    df = price.loc[price[\"SecuritiesCode\"] == code].copy()\n    df.loc[:, \"label\"] = df[\"Target\"]\n\n    return df.loc[:, [\"SecuritiesCode\", \"label\"]]","metadata":{"papermill":{"duration":0.154664,"end_time":"2022-04-02T13:52:40.181476","exception":false,"start_time":"2022-04-02T13:52:40.026812","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split data into TRAIN and TEST\nTRAIN_END = \"2021-12-01\"\n# We put a week gap between TRAIN_END and TEST_START\n# to avoid leakage of test data information from label\nTEST_START = \"2021-12-06\"\n\ndef get_features_and_label(price, codes, features):\n    \"\"\"\n    Args:\n        price (pd.DataFrame): loaded price data\n        codes  (array) : target codes\n        feature (pd.DataFrame): features\n    Returns:\n        train_X (pd.DataFrame): training data\n        train_y (pd.DataFrame): label for train_X\n        test_X (pd.DataFrame): test data\n        test_y (pd.DataFrame): label for test_X\n    \"\"\"\n    # to store splited data\n    trains_X, tests_X = [], []\n    trains_y, tests_y = [], []\n\n    # generate feature one by one\n    for code in tqdm(codes):\n\n        feats = features[features[\"SecuritiesCode\"] == code].dropna()\n        labels = get_label(price, code).dropna()\n\n        if feats.shape[0] > 0 and labels.shape[0] > 0:\n            # align label and feature indexes\n            labels = labels.loc[labels.index.isin(feats.index)]\n            feats = feats.loc[feats.index.isin(labels.index)]\n\n            assert (labels.loc[:, \"SecuritiesCode\"] == feats.loc[:, \"SecuritiesCode\"]).all()\n            labels = labels.loc[:, \"label\"]\n\n            # split data into TRAIN and TEST\n            _train_X = feats[: TRAIN_END]\n            _test_X = feats[TEST_START:]\n\n            _train_y = labels[: TRAIN_END]\n            _test_y = labels[TEST_START:]\n            \n            assert len(_train_X) == len(_train_y)\n            assert len(_test_X) == len(_test_y)\n\n            # store features\n            trains_X.append(_train_X)\n            tests_X.append(_test_X)\n            # store labels\n            trains_y.append(_train_y)\n            tests_y.append(_test_y)\n            \n    # combine features for each codes\n    train_X = pd.concat(trains_X)\n    test_X = pd.concat(tests_X)\n    # combine label for each codes\n    train_y = pd.concat(trains_y)\n    test_y = pd.concat(tests_y)\n\n    return train_X, train_y, test_X, test_y","metadata":{"papermill":{"duration":0.15998,"end_time":"2022-04-02T13:52:40.488053","exception":false,"start_time":"2022-04-02T13:52:40.328073","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# generate feature/label\n#train_X, train_y, test_X, test_y = get_features_and_label(\n#    df_price, codes, feature\n#)","metadata":{"papermill":{"duration":37.867923,"end_time":"2022-04-02T13:53:18.500899","exception":false,"start_time":"2022-04-02T13:52:40.632976","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Building a simple model\n\nUsing the created features and labels, build a model using the following procedure","metadata":{"papermill":{"duration":0.273296,"end_time":"2022-04-02T13:53:19.062225","exception":false,"start_time":"2022-04-02T13:53:18.788929","status":"completed"},"tags":[]}},{"cell_type":"code","source":"lgbm_params = {\n    'seed': 42,\n    'n_jobs': -1,\n}\n\nfeat_cols = [\n    \"return_2week\",\n    \"return_1month\",\n    \"return_3month\",\n    \"volatility_2week\",\n    \"volatility_1month\",\n    \"volatility_3month\",\n]","metadata":{"papermill":{"duration":0.276851,"end_time":"2022-04-02T13:53:20.157205","exception":false,"start_time":"2022-04-02T13:53:19.880354","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if False:\n    # initialize model\n    pred_model = LGBMRegressor(**lgbm_params)\n    # train\n    pred_model.fit(train_X[feat_cols].values, train_y)\n    # prepare result data\n    result = test_X[[\"SecuritiesCode\"]].copy()\n    # predict\n    result.loc[:, \"predict\"] = pred_model.predict(test_X[feat_cols])\n    # actual result\n    result.loc[:, \"Target\"] = test_y.values\n\n    def set_rank(df):\n        \"\"\"\n        Args:\n            df (pd.DataFrame): including predict column\n        Returns:\n            df (pd.DataFrame): df with Rank\n        \"\"\"\n        # sort records to set Rank\n        df = df.sort_values(\"predict\", ascending=False)\n        # set Rank starting from 0\n        df.loc[:, \"Rank\"] = np.arange(len(df[\"predict\"]))\n        return df\n\n    result = result.sort_values([\"Date\", \"predict\"], ascending=[True, False])\n    result = result.groupby(\"Date\").apply(set_rank)\n    result.tail()","metadata":{"papermill":{"duration":7.86376,"end_time":"2022-04-02T13:53:28.327193","exception":false,"start_time":"2022-04-02T13:53:20.463433","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"padding:16px;color:white;margin:0;font-size:180%;text-align:left;display:fill;border-radius:5px;background-color:#ABEBC6;overflow:hidden\"><center>Evaluation Section</center></div>\n\n\n# Performance Evaluation\n\nInput the output of the forecasts of the constructed model into the evaluation function and plot the daily returns.\n\nThe evaluation function for this competition is as follows.\n\nPlease read [here](https://www.kaggle.com/code/smeitoma/jpx-competition-metric-definition) to know the evaluation function more.","metadata":{"papermill":{"duration":0.269682,"end_time":"2022-04-02T13:53:29.426688","exception":false,"start_time":"2022-04-02T13:53:29.157006","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def calc_spread_return_sharpe(df: pd.DataFrame, portfolio_size: int = 200, toprank_weight_ratio: float = 2) -> float:\n    \"\"\"\n    Args:\n        df (pd.DataFrame): predicted results\n        portfolio_size (int): # of equities to buy/sell\n        toprank_weight_ratio (float): the relative weight of the most highly ranked stock compared to the least.\n    Returns:\n        (float): sharpe ratio\n    \"\"\"\n    def _calc_spread_return_per_day(df, portfolio_size, toprank_weight_ratio):\n        \"\"\"\n        Args:\n            df (pd.DataFrame): predicted results\n            portfolio_size (int): # of equities to buy/sell\n            toprank_weight_ratio (float): the relative weight of the most highly ranked stock compared to the least.\n        Returns:\n            (float): spread return\n        \"\"\"\n        assert df['Rank'].min() == 0\n        assert df['Rank'].max() == len(df['Rank']) - 1\n        weights = np.linspace(start=toprank_weight_ratio, stop=1, num=portfolio_size)\n        purchase = (df.sort_values(by='Rank')['Target'][:portfolio_size] * weights).sum() / weights.mean()\n        short = (df.sort_values(by='Rank', ascending=False)['Target'][:portfolio_size] * weights).sum() / weights.mean()\n        return purchase - short\n\n    buf = df.groupby('Date').apply(_calc_spread_return_per_day, portfolio_size, toprank_weight_ratio)\n    sharpe_ratio = buf.mean() / buf.std()\n    return sharpe_ratio","metadata":{"papermill":{"duration":0.27819,"end_time":"2022-04-02T13:53:30.512824","exception":false,"start_time":"2022-04-02T13:53:30.234634","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# calc spread return sharpe\n#calc_spread_return_sharpe(result, portfolio_size=200)","metadata":{"papermill":{"duration":1.191834,"end_time":"2022-04-02T13:53:31.971406","exception":false,"start_time":"2022-04-02T13:53:30.779572","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then, we will show daily spread return of the model.","metadata":{"papermill":{"duration":0.267867,"end_time":"2022-04-02T13:53:32.5075","exception":false,"start_time":"2022-04-02T13:53:32.239633","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def _calc_spread_return_per_day(df, portfolio_size, toprank_weight_ratio):\n    \"\"\"\n    Args:\n        df (pd.DataFrame): predicted results\n        portfolio_size (int): # of equities to buy/sell\n        toprank_weight_ratio (float): the relative weight of the most highly ranked stock compared to the least.\n    Returns:\n        (float): spread return\n    \"\"\"\n    assert df['Rank'].min() == 0\n    assert df['Rank'].max() == len(df['Rank']) - 1\n    weights = np.linspace(start=toprank_weight_ratio, stop=1, num=portfolio_size)\n    purchase = (df.sort_values(by='Rank')['Target'][:portfolio_size] * weights).sum() / weights.mean()\n    short = (df.sort_values(by='Rank', ascending=False)['Target'][:portfolio_size] * weights).sum() / weights.mean()\n    return purchase - short\n\n#df_result = result.groupby('Date').apply(_calc_spread_return_per_day, 200, 2)","metadata":{"papermill":{"duration":1.236094,"end_time":"2022-04-02T13:53:34.011016","exception":false,"start_time":"2022-04-02T13:53:32.774922","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df_result.plot(figsize=(20, 8))","metadata":{"papermill":{"duration":0.59649,"end_time":"2022-04-02T13:53:34.882518","exception":false,"start_time":"2022-04-02T13:53:34.286028","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We also show a cumulative spread return of the mode","metadata":{"papermill":{"duration":0.27175,"end_time":"2022-04-02T13:53:35.429118","exception":false,"start_time":"2022-04-02T13:53:35.157368","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# wealth curve\n#df_result.cumsum().plot(figsize=(20, 8))","metadata":{"papermill":{"duration":0.60694,"end_time":"2022-04-02T13:53:36.327383","exception":false,"start_time":"2022-04-02T13:53:35.720443","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model in this notebook is now complete! Try different features and training methods through trial and error!","metadata":{"papermill":{"duration":0.271931,"end_time":"2022-04-02T13:53:36.871257","exception":false,"start_time":"2022-04-02T13:53:36.599326","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Saving model","metadata":{"papermill":{"duration":0.272106,"end_time":"2022-04-02T13:53:37.417349","exception":false,"start_time":"2022-04-02T13:53:37.145243","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"You need to save your model parameter to use created model for your submission.","metadata":{"papermill":{"duration":0.275818,"end_time":"2022-04-02T13:53:37.970603","exception":false,"start_time":"2022-04-02T13:53:37.694785","status":"completed"},"tags":[]}},{"cell_type":"code","source":"#pred_model.booster_.save_model(\"simple-model.txt\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"import sys\nfrom decimal import ROUND_HALF_UP, Decimal\n\nimport numpy as np\nimport pandas as pd\nfrom lightgbm import Booster, LGBMRegressor\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_dir = \"../input/jpx-tokyo-stock-exchange-prediction\"\n\ntrain_files_dir = f\"{base_dir}/train_files\"\nsupplemental_files_dir = f\"{base_dir}/supplemental_files\"\n\n# model_file = \"../input/simplemodel/simple-model.txt\"\nmodel_file = \"./simple-model.txt\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# feature column names\nfeat_cols = [\n    \"return_2week\",\n    \"return_1month\",\n    \"return_3month\",\n    \"volatility_2week\",\n    \"volatility_1month\",\n    \"volatility_3month\",\n]\n\ndef generate_adjusted_close(df):\n    \"\"\"\n    Args:\n        df (pd.DataFrame)  : stock_price for a single SecuritiesCode\n    Returns:\n        df (pd.DataFrame): stock_price with AdjustedClose for a single SecuritiesCode\n    \"\"\"\n    # sort data to generate CumulativeAdjustmentFactor\n    df = df.sort_values(\"Date\", ascending=False)\n    # generate CumulativeAdjustmentFactor\n    df.loc[:, \"CumulativeAdjustmentFactor\"] = df[\"AdjustmentFactor\"].cumprod()\n    # generate AdjustedClose\n    df.loc[:, \"AdjustedClose\"] = (\n        df[\"CumulativeAdjustmentFactor\"] * df[\"Close\"]\n    ).map(lambda x: float(\n        Decimal(str(x)).quantize(Decimal('0.1'), rounding=ROUND_HALF_UP)\n    ))\n    # reverse order\n    df = df.sort_values(\"Date\")\n    # to fill AdjustedClose, replace 0 into np.nan\n    df.loc[df[\"AdjustedClose\"] == 0, \"AdjustedClose\"] = np.nan\n    # forward fill AdjustedClose\n    df.loc[:, \"AdjustedClose\"] = df.loc[:, \"AdjustedClose\"].ffill()\n    return df\n\ndef adjust_price(price):\n    \"\"\"\n    Args:\n        price (pd.DataFrame)  : pd.DataFrame include stock_price\n    Returns:\n        price DataFrame (pd.DataFrame): stock_price with generated AdjustedClose\n    \"\"\"\n    # copy to edit\n    price = price.copy()\n    # transform Date column into datetime\n    price.loc[: ,\"Date\"] = pd.to_datetime(price.loc[: ,\"Date\"], format=\"%Y-%m-%d\")\n\n    # generate AdjustedClose\n    price = price.sort_values([\"SecuritiesCode\", \"Date\"])\n    price = price.groupby(\"SecuritiesCode\").apply(generate_adjusted_close).reset_index(drop=True)\n\n    price.set_index(\"Date\", inplace=True)\n    return price\n\ndef get_features_for_predict(price, code):\n    \"\"\"\n    Args:\n        price (pd.DataFrame)  : pd.DataFrame include stock_price\n        code (int)  : A local code for a listed company\n    Returns:\n        feature DataFrame (pd.DataFrame)\n    \"\"\"\n    close_col = \"AdjustedClose\"\n    feats = price.loc[price[\"SecuritiesCode\"] == code, [\"SecuritiesCode\", close_col]].copy()\n\n    # calculate 2 week return using AdjustedClose\n    feats[\"return_2week\"] = feats[close_col].pct_change(10)\n    # calculate last 1 month return using AdjustedClose\n    feats[\"return_1month\"] = feats[close_col].pct_change(21)\n    # calculate last 3 months return using AdjustedClose\n    feats[\"return_3month\"] = feats[close_col].pct_change(63)\n\n    # calculate 2 week historical volatility using AdjustedClose\n    feats[\"volatility_2week\"] = (\n        np.log(feats[close_col]).diff().rolling(10).std()\n    )\n    # calculate last 1 month historical volatility using AdjustedClose\n    feats[\"volatility_1month\"] = (\n        np.log(feats[close_col]).diff().rolling(21).std()\n    )\n    # calculate last 3 months historical volatility using AdjustedClose\n    feats[\"volatility_3month\"] = (\n        np.log(feats[close_col]).diff().rolling(63).std()\n    )\n\n    # filling data for nan and inf\n    feats = feats.fillna(0)\n    feats = feats.replace([np.inf, -np.inf], 0)\n    # drop AdjustedClose column\n    feats = feats.drop([close_col], axis=1)\n\n    return feats\n\n\n### Live Mode\nif True:\n    # load stock price data\n    df_price_raw = pd.read_csv(f\"{train_files_dir}/stock_prices.csv\")\n    price_cols = [\n        \"Date\",\n        \"SecuritiesCode\",\n        \"Close\",\n        \"AdjustmentFactor\",\n    ]\n    df_price_raw = df_price_raw[price_cols]\n\n    # filter data to reduce culculation cost \n    df_price_raw = df_price_raw.loc[df_price_raw[\"Date\"] >= \"2021-08-01\"]\n\n    # forecasting phase leaderboard:\n    df_price_supplemental = pd.read_csv(f\"{supplemental_files_dir}/stock_prices.csv\")\n    df_price_supplemental = df_price_supplemental[price_cols]\n    df_price_raw = pd.concat([df_price_raw, df_price_supplemental])\n    \n    print(\" > The training history data is loaded!\")\n    \n#pred_model = Booster(model_file=model_file)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n## Official API Submission\n\nCall the API function from `jpx_tokyo_market_prediction`\n","metadata":{}},{"cell_type":"code","source":"# load Time Series API\nimport jpx_tokyo_market_prediction\n# make Time Series API environment (this function can be called only once in a session)\nenv = jpx_tokyo_market_prediction.make_env()\n# get iterator to fetch data day by day\niter_test = env.iter_test()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"counter = 0\n# fetch data day by day\nfor (prices, options, financials, trades, secondary_prices, sample_prediction) in iter_test:\n    current_date = prices[\"Date\"].iloc[0]\n    sample_prediction_date = sample_prediction[\"Date\"].iloc[0]\n    print(f\" => Current_date: {current_date}, sample_prediction_date: @{sample_prediction_date}\")\n\n    if counter == 0:\n        # to avoid data leakage\n        df_price_raw = df_price_raw.loc[df_price_raw[\"Date\"] < current_date]\n\n    ################  Data Loading  ################\n    # filter data to reduce culculation cost\n    threshold = (pd.Timestamp(current_date) - pd.offsets.BDay(80)).strftime(\"%Y-%m-%d\")\n    print(f\"threshold: {threshold}\")\n    df_price_raw = df_price_raw.loc[df_price_raw[\"Date\"] >= threshold]\n\n    # to generate AdjustedClose, increment price data\n    df_price_raw = pd.concat([df_price_raw, prices[price_cols]])\n    # generate AdjustedClose\n    df_price = adjust_price(df_price_raw)\n\n    # get target SecuritiesCodes\n    codes = sorted(prices[\"SecuritiesCode\"].unique())\n    \n    ################  Feature Processing  ################\n    # generate feature\n    #feature = pd.concat([get_features_for_predict(df_price, code) for code in codes])\n    # filter feature for this iteration\n    #feature = feature.loc[feature.index == current_date]\n    feature = df_price.loc[current_date].copy()\n    \n    size = feature['AdjustedClose'].values  * share_df.loc[feature['SecuritiesCode'].values].iloc[:,0].astype(float)\n    \n    \n    ################  Model Inference  ################\n    \n    # prediction\n    feature.loc[:, \"predict\"] =  -np.log(size)  #pred_model.predict(feature[feat_cols])\n    \n    # set rank by predict\n    feature = feature.sort_values(\"predict\", ascending=False).drop_duplicates(subset=['SecuritiesCode'])\n    feature.loc[:, \"Rank\"] = np.arange(len(feature))\n    feature_map = feature.set_index('SecuritiesCode')['Rank'].to_dict()\n    \n    ##############  Prediction Uploading  ################\n    \n    sample_prediction['Rank'] = sample_prediction['SecuritiesCode'].map(feature_map)\n\n    # check Rank\n    assert sample_prediction[\"Rank\"].notna().all()\n    assert sample_prediction[\"Rank\"].min() == 0\n    assert sample_prediction[\"Rank\"].max() == len(sample_prediction[\"Rank\"]) - 1\n\n    # register your predictions\n    env.predict(sample_prediction)\n    counter += 1","metadata":{"execution":{"iopub.status.busy":"2022-06-26T06:32:39.726954Z","iopub.execute_input":"2022-06-26T06:32:39.727257Z","iopub.status.idle":"2022-06-26T06:32:39.760375Z","shell.execute_reply.started":"2022-06-26T06:32:39.727226Z","shell.execute_reply":"2022-06-26T06:32:39.759252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reference","metadata":{}}]}