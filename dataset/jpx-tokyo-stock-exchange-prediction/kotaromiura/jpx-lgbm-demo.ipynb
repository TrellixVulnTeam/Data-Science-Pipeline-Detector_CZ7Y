{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport math\nimport datetime as dt\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n###\nfrom pathlib import Path\nfrom decimal import ROUND_HALF_UP, Decimal\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import TimeSeriesSplit\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport lightgbm as lgb\n# import optuna.integration.lightgbm as lgb\nimport optuna\n\nimport warnings\nwarnings.simplefilter('ignore')\n\n#最大表示列数・行数の指定\npd.set_option('display.max_columns', 50)\npd.set_option('display.max_rows', 7)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-27T02:28:29.697477Z","iopub.execute_input":"2022-06-27T02:28:29.697975Z","iopub.status.idle":"2022-06-27T02:28:33.440823Z","shell.execute_reply.started":"2022-06-27T02:28:29.697876Z","shell.execute_reply":"2022-06-27T02:28:33.439747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_files(dir_name):\n    base_path        = Path(f'../input/jpx-tokyo-stock-exchange-prediction/{dir_name}')\n    prices           = pd.read_csv(base_path / 'stock_prices.csv')\n    options          = pd.read_csv(base_path / 'options.csv')\n    financials       = pd.read_csv(base_path / 'financials.csv')\n    trades           = pd.read_csv(base_path / 'trades.csv')\n    secondary_prices = pd.read_csv(base_path / 'secondary_stock_prices.csv')\n    return prices, options, financials, trades, secondary_prices","metadata":{"execution":{"iopub.status.busy":"2022-06-27T02:29:15.04725Z","iopub.execute_input":"2022-06-27T02:29:15.047671Z","iopub.status.idle":"2022-06-27T02:29:15.055451Z","shell.execute_reply.started":"2022-06-27T02:29:15.04764Z","shell.execute_reply":"2022-06-27T02:29:15.054055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# stock list、train files(５つ), sup files(５つ)を読み込む\n# train files  2017-2021/12/3\n# sup files    2021/12/6-\nstock_list         = pd.read_csv('../input/jpx-tokyo-stock-exchange-prediction/stock_list.csv')\ntrain_files        = read_files('train_files')\nsupplemental_files = read_files('supplemental_files')","metadata":{"execution":{"iopub.status.busy":"2022-06-27T02:29:15.592498Z","iopub.execute_input":"2022-06-27T02:29:15.592887Z","iopub.status.idle":"2022-06-27T02:30:00.625365Z","shell.execute_reply.started":"2022-06-27T02:29:15.592855Z","shell.execute_reply":"2022-06-27T02:30:00.624182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def merge_data(prices, options, financials, trades, secondary_prices, stock_list):\n\n    base_df = prices.copy()\n    \n    # stock_listと結合\n    _stock_list = stock_list.copy()\n    _stock_list.rename(columns={'Close': 'Close_x'}, inplace=True) \n    base_df = base_df.merge(_stock_list, on='SecuritiesCode', how=\"left\") \n\n    # tradesと結合\n    # stock_listのNewMarketSegmentと紐づくよう、tradesのSection項目を編集する\n    # _trades = trades.copy()\n    # _trades['NewMarketSegment'] = _trades['Section'].str.split(' \\(', expand=True)[0]\n    # base_df = base_df.merge(_trades, on=['Date', 'NewMarketSegment'], how=\"left\")\n\n    # financialsと結合\n    # _financials = financials.copy()\n    # _financials.rename(columns={'Date': 'Date_x', 'SecuritiesCode': 'SecuritiesCode_x'}, inplace=True)\n    # base_df = base_df.merge(_financials, left_on='RowId', right_on='DateCode', how=\"left\")\n    \n    return base_df","metadata":{"execution":{"iopub.status.busy":"2022-06-27T02:30:06.741688Z","iopub.execute_input":"2022-06-27T02:30:06.742087Z","iopub.status.idle":"2022-06-27T02:30:06.749222Z","shell.execute_reply.started":"2022-06-27T02:30:06.742049Z","shell.execute_reply":"2022-06-27T02:30:06.747753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 分割統合したところを調整したcloseを計算するmethod\n# 0と算出された場合は、前のCloseで置換する\ndef generate_adjusted_close(df):\n\n    df = df.sort_values(\"Date\", ascending=False)\n    df[\"CumAdjustmentFactor\"] = df[\"AdjustmentFactor\"].cumprod() # 分割統合する割合を累積させる\n    \n    #↓少数点四捨五入する関数ついていたが、無視してみた\n    #df[\"AdClose\"] = (df[\"CumAdjustmentFactor\"] * df[\"Close\"]).map(lambda x: Decimal(x).quantize(Decimal('0.1'), rounding=ROUND_HALF_UP))\n    \n    df[\"Close\"] = df[\"Close\"].fillna(method='bfill') #nanの部分を直前の値に置換\n    df[\"Open\"] = df[\"Open\"].fillna(method='bfill') #nanの部分を直前の値に置換\n    df[\"High\"] = df[\"High\"].fillna(method='bfill') #nanの部分を直前の値に置換\n    df[\"Low\"] = df[\"Low\"].fillna(method='bfill') #nanの部分を直前の値に置換\n    \n    \n    df[\"AdClose\"] = df[\"CumAdjustmentFactor\"] * df[\"Close\"]\n    df[\"AdOpen\"] = df[\"CumAdjustmentFactor\"] * df[\"Open\"]\n    df[\"AdHigh\"] = df[\"CumAdjustmentFactor\"] * df[\"High\"]\n    df[\"AdLow\"] = df[\"CumAdjustmentFactor\"] * df[\"Low\"]\n    \n    df = df.sort_values(\"Date\")\n    \n    df[\"AdClose\"] = df[\"AdClose\"].replace(0, np.nan)\n    df[\"AdClose\"] = df[\"AdClose\"].fillna(method='ffill') #nanの部分を直前の値に置換\n    df[\"AdOpen\"] = df[\"AdOpen\"].replace(0, np.nan)\n    df[\"AdOpen\"] = df[\"AdOpen\"].fillna(method='ffill') #nanの部分を直前の値に置換\n    df[\"AdHigh\"] = df[\"AdHigh\"].replace(0, np.nan)\n    df[\"AdHigh\"] = df[\"AdHigh\"].fillna(method='ffill') #nanの部分を直前の値に置換\n    df[\"AdLow\"] = df[\"AdLow\"].replace(0, np.nan)\n    df[\"AdLow\"] = df[\"AdLow\"].fillna(method='ffill') #nanの部分を直前の値に置換\n    return df\n\n# dfに上記methodを適応\ndef adjust_price(price):\n\n    price[\"Date\"] = pd.to_datetime(price[\"Date\"], format=\"%Y-%m-%d\")\n    price = price.sort_values([\"SecuritiesCode\", \"Date\"])\n    price['SecuritiesCode'] = price['SecuritiesCode'].astype('category')\n    price = price.groupby(\"SecuritiesCode\").apply(generate_adjusted_close).reset_index(drop=True)\n\n    return price\n\n# 全てのdfをmergeしAdCloseを算出・追加\ndef collector(prices, options, financials, trades, secondary_prices, stock_list):\n    \n    base_df = merge_data(prices, options, financials, trades, secondary_prices, stock_list)\n    base_df = adjust_price(base_df)# AdCloseを追加する\n    \n    return base_df","metadata":{"execution":{"iopub.status.busy":"2022-06-27T02:30:07.856864Z","iopub.execute_input":"2022-06-27T02:30:07.857274Z","iopub.status.idle":"2022-06-27T02:30:07.875063Z","shell.execute_reply.started":"2022-06-27T02:30:07.857239Z","shell.execute_reply":"2022-06-27T02:30:07.87357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# trainとsuppleを統合\nbase_df         = collector(*train_files, stock_list)        # 2017-2021/12/3\nsupplemental_df = collector(*supplemental_files, stock_list) # 2021/12/6-5/27\nbase_df         = pd.concat([base_df, supplemental_df]).reset_index(drop=True)\n\n# 基本となるdfの表示 \nbase_df","metadata":{"execution":{"iopub.status.busy":"2022-06-27T02:30:08.947237Z","iopub.execute_input":"2022-06-27T02:30:08.948382Z","iopub.status.idle":"2022-06-27T02:30:58.347276Z","shell.execute_reply.started":"2022-06-27T02:30:08.948339Z","shell.execute_reply":"2022-06-27T02:30:58.346209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#   **ここまでデータ整理**","metadata":{}},{"cell_type":"code","source":"# # 「column_name で指定された項目の、periodsで指定された期間（複数）での変化率を導出し、項目として追加する関数」を生成する関数\n# # 生成された関数は、特定証券コードだけを持つデータフレームが入力されることを前提としている。\n# def calc_change_rate_base(column_name, periods):\n#     def func(price):\n#         for period in periods:\n#             price[f\"{column_name}_change_rate_{period}\"] = price[column_name].pct_change(period)\n#         return price\n#     return func\n\n\n# # 「column_name で指定された項目の、periodsで指定された期間（複数）での変動の度合いを導出し、項目として追加する関数」を生成する関数\n# def calc_volatility_base(column_name, periods):\n#     def func(price):\n#         for period in periods:\n#             price[f\"{column_name}_volatility_{period}\"] = np.log(price[column_name]).diff().rolling(window=period, min_periods=1).std()\n#         return price\n#     return func\n\n# # 「column_name で指定された項目の、periodsで指定された期間（複数）での移動平均値と現在値の比率を導出し、項目として追加する関数」を生成する関数\n# # 移動平均値そのものではなく、現在値に対する比率としているのは、今回のTargetが比率であるため。\n# def calc_moving_average_rate_base(column_name, periods):\n#     def func(price):\n#         for period in periods:\n#             price[f\"{column_name}_average_rate_{period}\"] = price[column_name].rolling(window=period, min_periods=1).mean() / price[column_name]\n#         return price\n#     return func\n\n# # 終値の変動率を生成し、項目として追加する関数。これをShift-2するとTargetになる。\n# def calc_target_shift2(price):\n#     price['Target_shift2'] = price['Close'].pct_change()\n#     return price\n\n# # 入力データフレームを証券コード毎にグルーピングし、引数で渡された関数を適用する関数\n# # functionsには↑で定義したcalc_xxxの関数のリストが渡される想定。\n# def add_columns_per_code(price, functions):\n#     def func(df):\n#         for f in functions:\n#             df = f(df)\n#         return df\n#     price = price.sort_values([\"SecuritiesCode\", \"Date\"])\n#     price = price.groupby(\"SecuritiesCode\").apply(func)\n#     price = price.reset_index(drop=True)\n#     return price\n\n# # 入力データフレームに特徴量を追加する関数\n# # 追加する項目は、基本的にレコード内の値だけを使う想定\n# def add_columns_per_day(base_df):\n#     \"\"\"\n#     ここに特徴量を入れていく\n#     \"\"\"\n#     base_df['body'] = (base_df['AdClose'] - base_df['AdOpen']) / base_df['AdClose']\n#     base_df['whole'] = (base_df['AdHigh'] - base_df['AdLow']) / base_df['AdClose']    \n#     base_df[\"hige_upper\"] = (base_df[\"AdHigh\"] - np.maximum(base_df[\"AdClose\"], base_df[\"AdOpen\"]))/ base_df[\"AdClose\"]\n#     base_df[\"hige_lower\"] = (np.minimum(base_df[\"AdClose\"], base_df[\"AdOpen\"]) - base_df[\"AdLow\"])/ base_df[\"AdClose\"]\n#     base_df[\"body_upper_ratio\"] = base_df[\"hige_upper\"] / np.maximum(base_df[\"body\"],0.000001)\n#     base_df[\"body_lower_ratio\"] = base_df[\"hige_lower\"] / np.maximum(base_df[\"body\"],0.000001)\n#     base_df[\"upper_lower_ratio\"] = base_df[\"hige_lower\"] / np.maximum(base_df[\"hige_upper\"], 0.00001)\n    \n#     # 平均足\n#     base_df['heikin_cl'] = 0.25 * (base_df['AdOpen'] + base_df['AdHigh'] + base_df['AdLow'] + base_df['AdClose'])\n#     base_df['heikin_op'] = base_df['heikin_cl'].ewm(1, adjust=False).mean().shift(1)  \n\n#     # 時系列\n#     base_df[\"month\"] = base_df[\"Date\"].dt.month\n#     base_df[\"day\"]   = base_df[\"Date\"].dt.day\n#     base_df[\"dow\"]   = base_df[\"Date\"].dt.dayofweek      # 曜日0-6(月-日曜日)\n\n#     return base_df\n\n# # 入力データフレームに特徴量を追加する関数\n# def generate_features(base_df):\n#     prev_column_names = base_df.columns\n    \n#     \"\"\"\n#     ここに特徴量をどんどん作成していく\n#     \"\"\"\n#     periods = [3, 9]\n#     functions = [\n#         calc_change_rate_base(\"AdClose\", periods), \n#         calc_volatility_base(\"AdClose\", periods), \n#         calc_moving_average_rate_base(\"Volume\", periods), \n#         calc_target_shift2\n#     ]\n    \n#     # 証券コード単位の特徴量（移動平均等、一定期間のレコードをインプットに生成する特徴量）を追加\n#     base_df = add_columns_per_code(base_df, functions)\n#     # 日単位の特徴量（レコード内の値で導出できる特徴量）を追加\n#     base_df = add_columns_per_day(base_df)\n    \n#     # 後で特徴量を選択しやすくするため、追加した項目名のリストを生成\n#     add_column_names = list(set(base_df.columns) - set(prev_column_names))\n#     return base_df, add_column_names","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n特徴量再度作り直した部分\nここを編集していく。\n上は元々のオリジナル\n\"\"\"\n\n\n\ndef add_columns_per_day(base_df, periods):\n    \"\"\"\n    ここに特徴量を入れていく\n    \"\"\"\n    ## ①時系列使用しない特徴量## \n    # 足の形を特徴量に\n    base_df['body'] = abs((base_df['AdClose'] - base_df['AdOpen']) / base_df['AdClose'])\n    base_df['whole'] = abs((base_df['AdHigh'] - base_df['AdLow']) / base_df['AdClose'])\n    base_df[\"hige_upper\"] = (base_df[\"AdHigh\"] - np.maximum(base_df[\"AdClose\"], base_df[\"AdOpen\"]))/ base_df[\"AdClose\"]\n    base_df[\"hige_lower\"] = (np.minimum(base_df[\"AdClose\"], base_df[\"AdOpen\"]) - base_df[\"AdLow\"])/ base_df[\"AdClose\"]\n    base_df[\"body_upper_ratio\"] = base_df[\"hige_upper\"] / np.maximum(base_df[\"body\"],0)\n    base_df[\"body_lower_ratio\"] = base_df[\"hige_lower\"] / np.maximum(base_df[\"body\"],0)\n    base_df[\"upper_lower_ratio\"] = base_df[\"hige_lower\"] / np.maximum(base_df[\"hige_upper\"], 0)\n\n    # 時系列\n    base_df[\"month\"] = base_df[\"Date\"].dt.month\n    base_df[\"day\"]   = base_df[\"Date\"].dt.day\n    base_df[\"dow\"]   = base_df[\"Date\"].dt.dayofweek      # 曜日0-6(月-日曜日)\n    \n    ## ②ラグ特徴量 ##\n    # 平均足\n    base_df['heikin_cl'] = 0.25 * (base_df['AdOpen'] + base_df['AdHigh'] + base_df['AdLow'] + base_df['AdClose'])\n\n    # AdClose の%change\n    for i in periods:\n        base_df[f\"AdClose_change_rate_{i}\"] = base_df.groupby(\"SecuritiesCode\")[\"AdClose\"].pct_change(i)\n    # Volumeの移動中央値\n    for i in periods:\n        base_df[f'{i}d_norm_vol'] = base_df.groupby(\"SecuritiesCode\")[\"Volume\"].apply(lambda x: x/ x.rolling(window=i).median())  \n    # logをとったAdCloseのボラ(標準偏差)\n    base_df[\"logAdClose\"] = np.log(base_df[\"AdClose\"])\n    for i in periods:\n        base_df[f\"AdClose_vola_{i}\"] = base_df.groupby(\"SecuritiesCode\")[\"logAdClose\"].diff().rolling(window=i, min_periods=1).std() \n    # Volumeのボラ\n    for i in periods:\n        base_df[f\"Volume_vola_{i}\"] = base_df.groupby(\"SecuritiesCode\")[\"Volume\"].diff().rolling(window=i, min_periods=1).std() \n    \n    \n    \n    return base_df\n\n\ndef generate_features(base_df):\n    prev_column_names = base_df.columns\n\n    periods = [3,9,25]\n    \n    # 特徴量を追加\n    base_df = add_columns_per_day(base_df, periods)\n    \n    # 後で特徴量を選択しやすくするため、追加した項目名のリストを生成\n    add_column_names = list(set(base_df.columns) - set(prev_column_names))\n    return base_df, add_column_names","metadata":{"execution":{"iopub.status.busy":"2022-06-27T02:31:30.344702Z","iopub.execute_input":"2022-06-27T02:31:30.346605Z","iopub.status.idle":"2022-06-27T02:31:30.370161Z","shell.execute_reply.started":"2022-06-27T02:31:30.346536Z","shell.execute_reply":"2022-06-27T02:31:30.368962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# c = a_df[a_df[\"SecuritiesCode\"]==1301]\n# plt.plot(c[\"Date\"], c[\"AdClose_vola_9\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 特徴量選択\ndef select_features(feature_df, add_column_names):\n    \n    base_cols = ['RowId', 'Date', 'SecuritiesCode']\n    # 数値系の特徴量\n    numerical_cols = sorted(add_column_names)\n    # カテゴリ系の特徴量\n    categorical_cols = ['NewMarketSegment', '33SectorCode', '17SectorCode']\n    # 目的変数\n    label_col = ['Target']\n    \n    # 選んだ特徴量\n    feat_cols = numerical_cols + categorical_cols\n\n    # データフレームの項目を選択された項目だけに絞込\n    feature_df = feature_df[base_cols + feat_cols + label_col]\n    # カテゴリ系項目はdtypeをcategoryに変更\n    feature_df[categorical_cols] = feature_df[categorical_cols].astype('category')\n\n    return feature_df, feat_cols, label_col\n\n# 特徴量生成と選択する\ndef preprocessor(base_df):\n    feature_df = base_df.copy()\n    \n    ## 特徴量生成\n    feature_df, add_column_names = generate_features(feature_df)\n    \n    ## 特徴量選択\n    feature_df, feat_cols, label_col = select_features(feature_df, add_column_names)\n\n    return feature_df, feat_cols, label_col","metadata":{"execution":{"iopub.status.busy":"2022-06-27T02:31:33.786947Z","iopub.execute_input":"2022-06-27T02:31:33.787898Z","iopub.status.idle":"2022-06-27T02:31:33.812302Z","shell.execute_reply.started":"2022-06-27T02:31:33.787829Z","shell.execute_reply":"2022-06-27T02:31:33.811188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_df, feat_cols, label_col = preprocessor(base_df)\n\n# # 特徴量\n# feat_cols","metadata":{"execution":{"iopub.status.busy":"2022-06-27T02:31:35.896609Z","iopub.execute_input":"2022-06-27T02:31:35.89702Z","iopub.status.idle":"2022-06-27T02:31:54.899796Z","shell.execute_reply.started":"2022-06-27T02:31:35.896986Z","shell.execute_reply":"2022-06-27T02:31:54.898916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# sharp ratioを計算","metadata":{}},{"cell_type":"code","source":"# 予測値の順位番号を振る関数\ndef add_rank(df, col_name=\"pred\"):\n    df[\"Rank\"] = df.groupby(\"Date\")[col_name].rank(ascending=False, method=\"first\") - 1 \n    df[\"Rank\"] = df[\"Rank\"].astype(\"int\")\n    return df\n\n# 日ごとに分けられているDFをランクごとに並べ替え、上200, 下200に対してweightを掛けて、平均でわりその差を取っている=sharp ratioの計算\n# ここの計算方法の意味ががわかっていない！！\ndef calc_return_per_day(df):\n    weights = np.linspace(start=2, stop=1, num=200)\n    long  = (df.sort_values('Rank', ascending=True )['Target'][:200] * weights).sum() / weights.mean()\n    short = (df.sort_values('Rank', ascending=False)['Target'][:200] * weights).sum() / weights.mean()\n    return long - short\n\n# sharp ratio計算\ndef calc_sharpe_ratio(df):\n    daily_ret = df.groupby('Date').apply(calc_return_per_day)\n    sharpe_ratio = daily_ret.mean() / daily_ret.std()\n    return sharpe_ratio\n\n# 予測用のデータフレームと、予測結果をもとに、sharp ratioを計算する関数\ndef evaluator(df, pred):\n    df[\"pred\"] = pred\n    df = add_rank(df)\n    sharp_ratio = calc_sharpe_ratio(df)\n    return sharp_ratio","metadata":{"execution":{"iopub.status.busy":"2022-06-27T02:32:03.181988Z","iopub.execute_input":"2022-06-27T02:32:03.182382Z","iopub.status.idle":"2022-06-27T02:32:03.193241Z","shell.execute_reply.started":"2022-06-27T02:32:03.182349Z","shell.execute_reply":"2022-06-27T02:32:03.192235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# モデル","metadata":{}},{"cell_type":"code","source":"fold_params = [\n    ('2020-12-23', '2021-11-01', '2021-12-01'),\n    ('2021-01-23', '2021-12-01', '2022-01-01'),\n    ('2021-02-23', '2022-01-01', '2022-02-01'),\n]\n\ndef objective(trial):\n\n    scores = []\n    models = []\n    params = []\n\n    for param in fold_params:\n        ################################\n        # データ準備\n        ################################\n        # データを train と valid　に分ける\n        train = feature_df[(param[0] <= feature_df['Date']) & (feature_df['Date'] < param[1])]\n        valid = feature_df[(param[1] <= feature_df['Date']) & (feature_df['Date'] < param[2])]\n\n        X_train = train[feat_cols]\n        y_train = train[label_col]\n        X_valid = valid[feat_cols]\n        y_valid = valid[label_col]\n\n\n        lgb_train = lgb.Dataset(X_train, y_train)\n        lgb_valid = lgb.Dataset(X_valid, y_valid, reference=lgb_train)\n\n        learning_rate = trial.suggest_uniform('learning_rate', 0.1, 1.0)\n        num_leaves =  trial.suggest_int(\"num_leaves\", 5, 50)\n        lambda_l1 = trial.suggest_loguniform('lambda_l1', 1e-8, 10.0)\n        lambda_l2 = trial.suggest_loguniform('lambda_l2', 1e-8, 10.0)\n        feature_fraction = trial.suggest_uniform('feature_fraction', 0.4, 1.0)\n        bagging_fraction = trial.suggest_uniform('bagging_fraction', 0.4, 1.0)\n        bagging_freq = trial.suggest_int('bagging_freq', 1, 7)\n        min_child_samples = trial.suggest_int('min_child_samples', 5, 100)\n\n        params = {\n                'task': 'train',                   # 学習\n                'boosting_type': 'gbdt',           # GBDT\n                'objective': 'regression',         # 回帰\n                'metric': 'rmse',                  # 損失（誤差）\n\n                'learning_rate': learning_rate,  # 学習率\n                'num_leaves': num_leaves,\n                'lambda_l1': lambda_l1,               \n                'lambda_l2': lambda_l2 ,           \n                'feature_fraction': feature_fraction,           # ランダムに抽出される列の割合\n                'bagging_fraction': bagging_fraction,           # ランダムに抽出される標本の割合\n                'bagging_freq': bagging_freq,                 # バギング実施頻度\n                'min_child_samples': min_child_samples,           # 葉に含まれる最小データ数\n\n                'seed': 2022,                         # シード値\n                \"force_col_wise\":True,\n                \"extra_trees\":True,\n                }\n\n        lgb_results = {}   \n\n        model = lgb.train(\n                        params=params,                    # ハイパーパラメータをセット\n                        train_set=lgb_train,              # 訓練データを訓練用にセット\n                        valid_sets=[lgb_train, lgb_valid], # 訓練データとテストデータをセット\n                        valid_names=['Train', 'Valid'],    # データセットの名前をそれぞれ設定\n                        num_boost_round=100,              # 計算回数\n                        early_stopping_rounds=50,         # アーリーストッピング設定\n                        evals_result=lgb_results,\n                        verbose_eval=-1,                  # ログを最後の1つだけ表示\n                        )\n\n\n        pred =  model.predict(X_valid, num_iteration=model.best_iteration)\n        score = evaluator(valid, pred)\n\n    return score\n\nopt = optuna.create_study(direction='maximize')\nopt.optimize(objective, n_trials=50)\n\nopt.best_params\n","metadata":{"execution":{"iopub.status.busy":"2022-06-27T02:35:07.633345Z","iopub.execute_input":"2022-06-27T02:35:07.633876Z","iopub.status.idle":"2022-06-27T02:41:26.096284Z","shell.execute_reply.started":"2022-06-27T02:35:07.633836Z","shell.execute_reply":"2022-06-27T02:41:26.095149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"{'learning_rate': 0.4309272728230116,\n 'num_leaves': 26,\n 'lambda_l1': 0.015061607016359854,\n 'lambda_l2': 0.014098154084387682,\n 'feature_fraction': 0.8241286866590853,\n 'bagging_fraction': 0.6249932406838286,\n 'bagging_freq': 5,\n 'min_child_samples': 77}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 学習\ndef trainer(feature_df, feat_cols, label_col, fold_params):\n    scores = []\n    models = []\n    params = []\n\n    for param in fold_params:\n        ################################\n        # データ準備\n        ################################\n        # データを train と valid　に分ける\n        train = feature_df[(param[0] <= feature_df['Date']) & (feature_df['Date'] < param[1])]\n        valid = feature_df[(param[1] <= feature_df['Date']) & (feature_df['Date'] < param[2])]\n\n        X_train = train[feat_cols]\n        y_train = train[label_col]\n        X_valid = valid[feat_cols]\n        y_valid = valid[label_col]\n        \n        lgb_train = lgb.Dataset(X_train, y_train)\n        lgb_valid = lgb.Dataset(X_valid, y_valid, reference=lgb_train)\n\n        ################################\n        # 学習\n        ################################\n        params = {\n            'task': 'train',                   # 学習\n            'boosting_type': 'gbdt',           # GBDT\n            'objective': 'regression',         # 回帰\n            'metric': 'rmse',                  # 損失（誤差）\n            \n            'learning_rate': 0.4309272728230116,\n            'num_leaves': 26,\n            'lambda_l1': 0.015061607016359854,\n            'lambda_l2': 0.014098154084387682,\n            'feature_fraction': 0.8241286866590853,\n            'bagging_fraction': 0.6249932406838286,\n            'bagging_freq': 5,\n            'min_child_samples': 77,\n#             'learning_rate': 0.001,             # 学習率\n#             'lambda_l1': 0.5,                  # L1正則化項の係数\n#             'lambda_l2': 0.5,                  # L2正則化項の係数\n#             'num_leaves': 15,                  # 最大葉枚数\n#             'feature_fraction': 0.5,           # ランダムに抽出される列の割合\n#             'bagging_fraction': 0.5,           # ランダムに抽出される標本の割合\n#             'bagging_freq': 5,                 # バギング実施頻度\n#             'min_child_samples': 10,           # 葉に含まれる最小データ数\n            'seed': 2022,                         # シード値\n            \"force_col_wise\":True,\n            \"extra_trees\":True,\n        } \n \n        lgb_results = {}             \n    \n        model = lgb.train( \n            params,                            # ハイパーパラメータ\n            lgb_train,                         # 訓練データ\n            valid_sets=[lgb_train, lgb_valid], # 検証データ\n            valid_names=['Train', 'Valid'],    # データセット名前\n            num_boost_round=2000,              # 計算回数\n            early_stopping_rounds=100,         # 計算打ち切り設定\n            evals_result=lgb_results,          # 学習の履歴\n            verbose_eval=-1,                   # 学習過程の表示サイクル\n        )  \n\n        ################################\n        # 結果描画\n        ################################\n        \n        fig = plt.figure(figsize=(10, 4))\n\n        # loss\n        plt.subplot(1,2,1)\n        loss_train = lgb_results['Train']['rmse']\n        loss_test = lgb_results['Valid']['rmse']   \n        plt.xlabel('Iteration')\n        plt.ylabel('logloss')\n        plt.plot(loss_train, label='train loss')\n        plt.plot(loss_test, label='valid loss')\n        plt.legend()\n\n        # feature importance\n        plt.subplot(1,2,2)\n        importance = pd.DataFrame({'feature':feat_cols, 'importance':model.feature_importance()})\n        sns.barplot(x = 'importance', y = 'feature', data = importance.sort_values('importance', ascending=False))\n\n        plt.tight_layout()#文字が重らないように\n        plt.show()\n\n        ################################\n        # 評価\n        ################################\n        pred =  model.predict(X_valid, num_iteration=model.best_iteration)\n        # 評価\n        score = evaluator(valid, pred)\n\n        scores.append(score)\n        models.append(model)\n\n    print(\"CV_SCORES:\", scores)\n    print(\"CV_SCORE:\", np.mean(scores))\n    \n    return models\n\n# 結果出力\n# 2020-12-23よりも前のデータは証券コードが2000個すべて揃っていないため、これ以降のデータのみを使う。\n# (学習用データの開始日、学習用データの終了日＝検証用データの開始日、検証用データの終了日)\nfold_params = [\n    ('2020-12-23', '2021-11-01', '2021-12-01'),\n    ('2021-01-23', '2021-12-01', '2022-01-01'),\n    ('2021-02-23', '2022-01-01', '2022-02-01'),\n]\nmodels = trainer(feature_df, feat_cols, label_col, fold_params)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T02:44:09.651049Z","iopub.execute_input":"2022-06-27T02:44:09.652046Z","iopub.status.idle":"2022-06-27T02:44:23.040146Z","shell.execute_reply.started":"2022-06-27T02:44:09.65199Z","shell.execute_reply":"2022-06-27T02:44:23.039086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# テストデータで最終検証\ndef predictor(feature_df, feat_cols, models, is_train=True):\n    X = feature_df[feat_cols]\n    \n    preds = list(map(lambda model: model.predict(X, num_iteration=model.best_iteration), models))\n    \n    # スコアは学習時のみ計算\n    if is_train:\n        scores = list(map(lambda pred: evaluator(feature_df, pred), preds))\n        print(\"SCORES:\", scores)\n\n    # 推論結果をバギング\n    pred = np.array(preds).mean(axis=0)\n\n    # スコアは学習時のみ計算\n    if is_train:\n        score = evaluator(feature_df, pred)\n        print(\"SCORE:\", score)\n    \n    return pred\n\n# 試験用データは学習用にも検証用にも使用していないものを使う\ntest_df = feature_df[('2022-02-01' <= feature_df['Date'])].copy()\npred = predictor(test_df, feat_cols, models)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T02:44:23.042121Z","iopub.execute_input":"2022-06-27T02:44:23.042792Z","iopub.status.idle":"2022-06-27T02:44:24.595887Z","shell.execute_reply.started":"2022-06-27T02:44:23.042745Z","shell.execute_reply":"2022-06-27T02:44:24.594935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 時系列APIのロード\nimport jpx_tokyo_market_prediction\nenv = jpx_tokyo_market_prediction.make_env()\niter_test = env.iter_test()\n\n# supplemental filesを履歴データの初期状態としてセットアップ\npast_df = supplemental_df.copy()\n\n# 日次で推論・登録\nfor i, (prices, options, financials, trades, secondary_prices, sample_prediction) in enumerate(iter_test):\n    current_date = prices[\"Date\"].iloc[0]\n\n    if i == 0:\n        # リークを防止するため、時系列APIから受け取ったデータより未来のデータを削除\n        past_df = past_df[past_df[\"Date\"] < current_date]\n\n    # リソース確保のため古い履歴を削除\n    threshold = (pd.Timestamp(current_date) - pd.offsets.BDay(80)).strftime(\"%Y-%m-%d\")\n    past_df = past_df[past_df[\"Date\"] >= threshold]\n    \n    # 時系列APIから受け取ったデータを履歴データに統合\n    base_df = collector(prices, options, financials, trades, secondary_prices, stock_list)\n    past_df = pd.concat([past_df, base_df]).reset_index(drop=True)\n\n    # 特徴量エンジニアリング\n    feature_df, feat_cols, label_col = preprocessor(past_df)\n\n    # 予測対象レコードだけを抽出\n    feature_df = feature_df[feature_df['Date'] == current_date]\n\n    # 推論\n    feature_df[\"pred\"] = predictor(feature_df, feat_cols, models, False)\n\n    # 推論結果からRANKを導出し、提出データに反映\n    feature_df = add_rank(feature_df)\n    feature_map = feature_df.set_index('SecuritiesCode')['Rank'].to_dict()\n    sample_prediction['Rank'] = sample_prediction['SecuritiesCode'].map(feature_map)\n\n    # 結果を登録\n    env.predict(sample_prediction)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T02:44:30.896962Z","iopub.execute_input":"2022-06-27T02:44:30.897368Z","iopub.status.idle":"2022-06-27T02:45:16.625521Z","shell.execute_reply.started":"2022-06-27T02:44:30.897335Z","shell.execute_reply":"2022-06-27T02:45:16.624205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}