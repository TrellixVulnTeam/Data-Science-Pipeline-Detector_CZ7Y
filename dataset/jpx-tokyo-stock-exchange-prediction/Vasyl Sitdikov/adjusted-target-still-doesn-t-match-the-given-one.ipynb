{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Adjusted Target. Still doesn't match the Given one","metadata":{}},{"cell_type":"markdown","source":"This notebook is the continuation of the previos one:   \n***Be careful! Target is not what it claims to be!***     \nhttps://www.kaggle.com/code/vasiliisitdikov/be-careful-target-is-not-what-it-claims-to-be/notebook","metadata":{}},{"cell_type":"markdown","source":"## 1. Description","metadata":{}},{"cell_type":"markdown","source":"In the previous notebook we computed the Target column using the defenition provided in **JPX Competition Metric Definition** article. https://www.kaggle.com/code/smeitoma/jpx-competition-metric-definition     \nWe used pure Close column previous time. It occures that more then 2% of calculated values don't match the given ones.    \n\nThe discussion revealed two possible causes:     \n1. Close Adjustment\n2. Rounding\n\n**In this notebook we introduce the way to calculate the Adjusted Target without Close price recalculations and compare the new computed values of Target with given ones**","metadata":{}},{"cell_type":"markdown","source":"## 2. Data loading and AdjustmentFactor description","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2022-05-04T08:14:24.985677Z","iopub.execute_input":"2022-05-04T08:14:24.986029Z","iopub.status.idle":"2022-05-04T08:14:26.037519Z","shell.execute_reply.started":"2022-05-04T08:14:24.985933Z","shell.execute_reply":"2022-05-04T08:14:26.036642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### We will use the same 'train' data of stock prices as for the previous notebook","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/jpx-tokyo-stock-exchange-prediction/train_files/stock_prices.csv\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-04T08:14:37.856568Z","iopub.execute_input":"2022-05-04T08:14:37.856846Z","iopub.status.idle":"2022-05-04T08:14:43.464977Z","shell.execute_reply.started":"2022-05-04T08:14:37.856812Z","shell.execute_reply":"2022-05-04T08:14:43.464043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us check the main statistics of AdjustmentFactor","metadata":{}},{"cell_type":"code","source":"df.AdjustmentFactor.describe().to_frame('Value')","metadata":{"execution":{"iopub.status.busy":"2022-05-04T08:15:29.924671Z","iopub.execute_input":"2022-05-04T08:15:29.926226Z","iopub.status.idle":"2022-05-04T08:15:30.049908Z","shell.execute_reply.started":"2022-05-04T08:15:29.926149Z","shell.execute_reply":"2022-05-04T08:15:30.049064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'There are {df.AdjustmentFactor.isna().sum()} NAN values in AdjustmentFactor feature')","metadata":{"execution":{"iopub.status.busy":"2022-05-04T08:17:19.586174Z","iopub.execute_input":"2022-05-04T08:17:19.58645Z","iopub.status.idle":"2022-05-04T08:17:19.596052Z","shell.execute_reply.started":"2022-05-04T08:17:19.586423Z","shell.execute_reply":"2022-05-04T08:17:19.595383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'There are {df[df.AdjustmentFactor != 1].shape[0]} different from 1 values in AdjustmentFactor feature')\ndf[df.AdjustmentFactor != 1].head()","metadata":{"execution":{"iopub.status.busy":"2022-05-04T08:20:28.008501Z","iopub.execute_input":"2022-05-04T08:20:28.008776Z","iopub.status.idle":"2022-05-04T08:20:28.036275Z","shell.execute_reply.started":"2022-05-04T08:20:28.008747Z","shell.execute_reply":"2022-05-04T08:20:28.035314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Let's also check the feature distribution:","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 2, figsize=(16, 4))\nsns.histplot(df.loc[df.AdjustmentFactor != 1, 'AdjustmentFactor'], ax=ax[0])\nsns.histplot(df.loc[df.AdjustmentFactor < 1, 'AdjustmentFactor'], ax=ax[1])\nplt.suptitle('AdjustmentFactor distribution')\nax[0].set_title('AF != 1')\nplt.title ('AF < 1')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-04T08:18:48.175477Z","iopub.execute_input":"2022-05-04T08:18:48.176353Z","iopub.status.idle":"2022-05-04T08:18:48.607428Z","shell.execute_reply.started":"2022-05-04T08:18:48.176318Z","shell.execute_reply":"2022-05-04T08:18:48.606126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. What Adjustment factor deals with Close and Target","metadata":{}},{"cell_type":"markdown","source":"#### To investigate what's going on around the non equal to 1 AdjustmentFactor, let's create the useful function:","metadata":{}},{"cell_type":"code","source":"def nearest_rows(data: pd.DataFrame, date: str, sec_code: int, nrows: int = 3, before: bool = True) -> pd.DataFrame:\n    \"\"\"\n    Parameters:\n        data (pd.DataFrame): The data (table) for the look up\n        date (str): The date we are looking for from the Date column of data\n        sec_code (int): The numerical code of security we are looking for from the SecuritiesCode of data\n        nrows (int): The number of next rows we want to see in addition to the strict look up (3 by default)\n        before (bool): 'True' if we want to see the rows before the strict look up row id addition to the next ones (True by default)\n    Returns:\n        pd.DataFrame: The 'window' of input dataframe with seeking Date and SecuritiesCode and several rows around this strict lookup\n    \"\"\"\n    td = data[data.SecuritiesCode == sec_code]\n    td = td.sort_values('Date').reset_index(drop=True)\n    indx = td[td.Date == date].index[0]\n    min_index, max_index = max(indx - nrows*before, 0), min(indx + nrows, td.shape[0] - 1)\n    return td.loc[min_index:max_index, :]","metadata":{"execution":{"iopub.status.busy":"2022-05-04T08:40:03.531562Z","iopub.execute_input":"2022-05-04T08:40:03.533531Z","iopub.status.idle":"2022-05-04T08:40:03.540585Z","shell.execute_reply.started":"2022-05-04T08:40:03.533491Z","shell.execute_reply":"2022-05-04T08:40:03.539694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nearest_rows(df, date='2017-01-17', sec_code=6861)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T08:40:17.22646Z","iopub.execute_input":"2022-05-04T08:40:17.22707Z","iopub.status.idle":"2022-05-04T08:40:17.267546Z","shell.execute_reply.started":"2022-05-04T08:40:17.227022Z","shell.execute_reply":"2022-05-04T08:40:17.266776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nearest_rows(df, date='2017-01-17', sec_code=8057)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T08:42:41.578779Z","iopub.execute_input":"2022-05-04T08:42:41.579611Z","iopub.status.idle":"2022-05-04T08:42:41.614822Z","shell.execute_reply.started":"2022-05-04T08:42:41.579554Z","shell.execute_reply":"2022-05-04T08:42:41.614068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### In the examples above we can see that AdjustmentFactor is not equal to 1 for the day, **before** the day of Closing price denomination","metadata":{}},{"cell_type":"markdown","source":"#### Let us check the difference between Adjusted Close Target calculations, Simple Close Target Calculations and the given Target the the example above manually:","metadata":{}},{"cell_type":"code","source":"print(\"Date '2017-01-16', SecuritiesCode 8057\")\nprint(f'The Simple Close Target Calculations Value: {(2402.0 - 490.0)/490:.6f}')\nprint(f'The Adjusted Close Target Calculations Value: {(2402.0 - 490.0*5)/(490*5):.6f}')\nprint(f\"The Given Target Value: {df.loc[(df.Date=='2017-01-16') & (df.SecuritiesCode==8057), 'Target'].values[0]:.6f}\")","metadata":{"execution":{"iopub.status.busy":"2022-05-04T08:56:22.317908Z","iopub.execute_input":"2022-05-04T08:56:22.318377Z","iopub.status.idle":"2022-05-04T08:56:22.472177Z","shell.execute_reply.started":"2022-05-04T08:56:22.318332Z","shell.execute_reply":"2022-05-04T08:56:22.471407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### We can see that the Adjusted Close Target Calculations value is equal to the given Target value. So we can spread these kind of calculations for the whole data.     \nHowever, it is worth to point out that computing the Adjusted Target we use the day **before** the 'adjustment day'!","metadata":{}},{"cell_type":"markdown","source":"## 4. Adjusted Target calculations","metadata":{}},{"cell_type":"markdown","source":"There are two ways to compute the Adjusted target:\n1. Using cumulated product reasign AdjustedFactor and recompute the Close price, then calculate the target.\n2. To come up with some way to avoid Close price reculculations.","metadata":{}},{"cell_type":"markdown","source":"Since the first (**cumprod**) aproach is quite stright forward, for this notebook **I introduce another one**.     \nYou are welcome to compare them an use one you like more.","metadata":{}},{"cell_type":"markdown","source":"#### The NOT Cumprod aproach summary:","metadata":{}},{"cell_type":"markdown","source":"1. It is clear that for the most days we should not recalculate Target since there are 730 'adjusted' cases only.\n2. We need to recalculate Target for the days before the adjustment days.\n3. We cannot use the recalculated (adjusted) Close as a 'close' price of our deal, only as an open (base) one, c.e in denominator only.     \n\nThus, let's us use the mask to bring the new adjusted Targets to our 'simple close' calculations","metadata":{}},{"cell_type":"markdown","source":"#### In the previos notebook the pivot_table aproach was introdused as an alternative to the grouping one. Let us use it again:","metadata":{}},{"cell_type":"code","source":"def pivot_pct_calculation(data: pd.DataFrame, periods: int = 1, shift: int = 0, dropna: bool = True, close_col='Close') -> pd.DataFrame:\n    \"\"\"\n    Parameters:\n        data (pd.DataFrame): The data (table) for the transformation\n        periods (int): The parameter to use in pct_change function = Periods to shift for forming percent change (1 by default)\n        shift (int): the post-calculations shift of Target. To make a non shifted table use 0 (by default). To compare with JPX Target column use -2\n        dropna (bool): Do not include columns whose entries are all NaN for pivot_table creation (True by default)\n        close_col (str): The column name used for calculations ('Close' by default)\n    Returns:\n        pd.DataFrame: The pivot table with Dates as index, SecuritiesCodes as columns, returns as values\n    \"\"\"\n    td = pd.pivot_table(data, index='Date', columns='SecuritiesCode', values=close_col, dropna=dropna)\n    print(td.shape)\n    td = td.pct_change(periods)\n    return td.shift(shift)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T09:23:48.677805Z","iopub.execute_input":"2022-05-04T09:23:48.67816Z","iopub.status.idle":"2022-05-04T09:23:48.685058Z","shell.execute_reply.started":"2022-05-04T09:23:48.678108Z","shell.execute_reply":"2022-05-04T09:23:48.684418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### For the Simple Close return in the previous notebook we had:","metadata":{}},{"cell_type":"code","source":"prc_df = pivot_pct_calculation(df, shift=-2, dropna=False)\nprc_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-04T09:25:06.049189Z","iopub.execute_input":"2022-05-04T09:25:06.049485Z","iopub.status.idle":"2022-05-04T09:25:07.793838Z","shell.execute_reply.started":"2022-05-04T09:25:06.049454Z","shell.execute_reply":"2022-05-04T09:25:07.792935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### For the new, Adjusted Close, return we have:","metadata":{}},{"cell_type":"code","source":"adj_close_df = df.loc[:, ['Date', 'SecuritiesCode', 'Close', 'AdjustmentFactor']]\nadj_close_df.loc[~adj_close_df.Close.isna(), 'AdjustedClose'] = adj_close_df.loc[~adj_close_df.Close.isna(), 'Close']*adj_close_df.loc[~adj_close_df.Close.isna(), 'AdjustmentFactor']\nprc_adj_df = pivot_pct_calculation(adj_close_df, shift=-2, dropna=False, close_col='AdjustedClose')\nprc_adj_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-04T09:26:14.711817Z","iopub.execute_input":"2022-05-04T09:26:14.712453Z","iopub.status.idle":"2022-05-04T09:26:16.485646Z","shell.execute_reply.started":"2022-05-04T09:26:14.712411Z","shell.execute_reply":"2022-05-04T09:26:16.484793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### It is clear that:\n1. All values for the days before the adjustment are wrong for prc_df table\n2. All values for the days with lag 2 from the adjustment are wrong for prc_adj_df.\n3. All other values are the same     \n\n#### Let us use the mask to combine these tables:","metadata":{}},{"cell_type":"markdown","source":"Firstly, let's compute the AdgustmentFactor pivot table:","metadata":{}},{"cell_type":"code","source":"af_df = pd.pivot_table(df, index='Date', columns='SecuritiesCode', values='AdjustmentFactor')\naf_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-04T09:32:55.993651Z","iopub.execute_input":"2022-05-04T09:32:55.994271Z","iopub.status.idle":"2022-05-04T09:32:57.746841Z","shell.execute_reply.started":"2022-05-04T09:32:55.994219Z","shell.execute_reply":"2022-05-04T09:32:57.746118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then, let's create the mask from it:","metadata":{}},{"cell_type":"code","source":"mask = (af_df.shift(-1) != 1) & (~af_df.shift(-1).isna())\nmask.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-04T09:34:06.319886Z","iopub.execute_input":"2022-05-04T09:34:06.320623Z","iopub.status.idle":"2022-05-04T09:34:06.356396Z","shell.execute_reply.started":"2022-05-04T09:34:06.320585Z","shell.execute_reply":"2022-05-04T09:34:06.35555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"FInally, let's check that our dataframes are consistant and apply the mask:","metadata":{}},{"cell_type":"code","source":"assert (prc_adj_df.index == prc_df.index).prod()\nassert (prc_adj_df.columns == prc_df.columns).prod()\nassert (prc_adj_df.index == mask.index).prod()\nassert (prc_adj_df.columns == mask.columns).prod()\nprc_masked_df = prc_df*(~mask) + prc_adj_df*mask\nprc_masked_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-04T09:37:08.270932Z","iopub.execute_input":"2022-05-04T09:37:08.271227Z","iopub.status.idle":"2022-05-04T09:37:08.308146Z","shell.execute_reply.started":"2022-05-04T09:37:08.271195Z","shell.execute_reply":"2022-05-04T09:37:08.307486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### That is what we were looking for.    \n#### It's the time to check the results.","metadata":{}},{"cell_type":"markdown","source":"## 5. The results comparison","metadata":{}},{"cell_type":"markdown","source":"#### Firstly, let's bring all cells to proceed with comparison in one function:","metadata":{}},{"cell_type":"code","source":"def calculated_vs_given_target(calculated_data: pd.DataFrame, given_data: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Parameters:\n        calculated_data (pd.DataFrame): The pivot table with calculated return\n        given_data (pd.DataFrame): The given data with return as Target feature\n    Returns:\n        pd.DataFrame: A table with Date and SecuritiesCode as a look up columns aligned with Calculated and Given columns for return\n    \"\"\"\n    calculated_target = pd.melt(calculated_data, ignore_index=False, value_name='Target').reset_index().sort_values(['Date','SecuritiesCode']).dropna(subset=['Target']).reset_index(drop=True)\n    given_target = given_data.loc[given_data.Date <= calculated_target.Date.max(), ['Date', 'SecuritiesCode', 'Target']].sort_values(['Date','SecuritiesCode']).dropna(subset=['Target']).reset_index(drop=True)\n    min_gd = given_target.groupby('SecuritiesCode')['Date'].min()\n    calculated_target['MinDate'] = calculated_target['SecuritiesCode'].map(min_gd)\n    calculated_target = calculated_target[calculated_target.Date >= calculated_target.MinDate].drop(columns=['MinDate']).reset_index(drop=True)\n    assert calculated_target.shape[0] == given_target.shape[0]\n    assert (calculated_target.index == given_target.index).prod()\n    assert (calculated_target.Date == given_target.Date).prod()\n    assert (calculated_target.SecuritiesCode == given_target.SecuritiesCode).prod()\n    calculated_target = calculated_target.rename(columns= {'Target': 'Calculated'})\n    calculated_target = calculated_target.merge(given_target, on=['Date', 'SecuritiesCode']).rename(columns= {'Target': 'Given'})\n    return calculated_target","metadata":{"execution":{"iopub.status.busy":"2022-05-04T09:44:18.766858Z","iopub.execute_input":"2022-05-04T09:44:18.767778Z","iopub.status.idle":"2022-05-04T09:44:18.776074Z","shell.execute_reply.started":"2022-05-04T09:44:18.767741Z","shell.execute_reply":"2022-05-04T09:44:18.775226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Then, we apply this function to the Not Adjusted and Adjusted calculations","metadata":{}},{"cell_type":"code","source":"diff_notadjusted_df = calculated_vs_given_target(prc_df, df)\ndiff_notadjusted_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-04T09:44:36.845848Z","iopub.execute_input":"2022-05-04T09:44:36.846349Z","iopub.status.idle":"2022-05-04T09:44:41.812514Z","shell.execute_reply.started":"2022-05-04T09:44:36.846301Z","shell.execute_reply":"2022-05-04T09:44:41.811722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"diff_adjusted_df = calculated_vs_given_target(prc_masked_df, df)\ndiff_adjusted_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-04T09:44:55.133363Z","iopub.execute_input":"2022-05-04T09:44:55.133646Z","iopub.status.idle":"2022-05-04T09:44:59.772064Z","shell.execute_reply.started":"2022-05-04T09:44:55.133614Z","shell.execute_reply":"2022-05-04T09:44:59.771178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### One more function to combine all results:","metadata":{}},{"cell_type":"code","source":"def calculated_vs_given_target_comparison(diff_data: pd.DataFrame, threshold: float = 10**(-10)) -> pd.DataFrame:\n    \"\"\"\n    Parameters:\n        diff_data (pd.DataFrame): The dataframe table with Date, SecuritiesCode, Calculated, Given columns\n        threshold (float): The threshold to compare significance of the difference\n    Returns:\n        pd.DataFrame: The input dataframe with data filtered by threshold (only 'significantly' different rows)\n    \"\"\"\n    diff_data['Diff'] = diff_data['Calculated'] - diff_data['Given']\n    diff_data['Diff'] = abs(diff_data['Diff']) > threshold\n    diff_data = diff_data[diff_data.Diff]\n    diff_data = diff_data.drop(columns=['Diff'])\n    return diff_data","metadata":{"execution":{"iopub.status.busy":"2022-05-04T09:57:08.424747Z","iopub.execute_input":"2022-05-04T09:57:08.425627Z","iopub.status.idle":"2022-05-04T09:57:08.431392Z","shell.execute_reply.started":"2022-05-04T09:57:08.425581Z","shell.execute_reply":"2022-05-04T09:57:08.430626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Now we can build the table to compare our number of differences with given Target value within simple Close and Adjusted Close tables in one look:","metadata":{}},{"cell_type":"code","source":"adj_n = list()\nnadj_n = list()\nfor i in range(1, 11):\n    adj_n.append(calculated_vs_given_target_comparison(diff_adjusted_df, 10**(-i)).shape[0])\n    nadj_n.append(calculated_vs_given_target_comparison(diff_notadjusted_df, 10**(-i)).shape[0])","metadata":{"execution":{"iopub.status.busy":"2022-05-04T10:03:33.4924Z","iopub.execute_input":"2022-05-04T10:03:33.493197Z","iopub.status.idle":"2022-05-04T10:03:33.877115Z","shell.execute_reply.started":"2022-05-04T10:03:33.493149Z","shell.execute_reply":"2022-05-04T10:03:33.876161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"compare_df = pd.DataFrame({'Threshold':[f'10**({-x})' for x in range(1, 11)],\n             'Not Adjusted': nadj_n,\n             'Adjusted': adj_n})\ncompare_df","metadata":{"execution":{"iopub.status.busy":"2022-05-04T10:03:34.176055Z","iopub.execute_input":"2022-05-04T10:03:34.176374Z","iopub.status.idle":"2022-05-04T10:03:34.189241Z","shell.execute_reply.started":"2022-05-04T10:03:34.176339Z","shell.execute_reply":"2022-05-04T10:03:34.188468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(16, 5))\nplt.plot(compare_df.Threshold, compare_df['Not Adjusted'], label='Not Adjusted')\nplt.plot(compare_df.Threshold, compare_df.Adjusted, label='Adjusted')\nplt.yscale('log')\nplt.title('Counts of different values for calculated and given Target (log)')\nplt.xlabel('Threshold')\nplt.ylabel('Number of difference with Given Target (log)')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-04T10:03:41.52554Z","iopub.execute_input":"2022-05-04T10:03:41.52583Z","iopub.status.idle":"2022-05-04T10:03:42.014985Z","shell.execute_reply.started":"2022-05-04T10:03:41.5258Z","shell.execute_reply":"2022-05-04T10:03:42.014322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6. Among Us","metadata":{}},{"cell_type":"markdown","source":"#### There is the only one case with the huge difference in our adjusted calculations:","metadata":{}},{"cell_type":"code","source":"calculated_vs_given_target_comparison(diff_adjusted_df, 10**(-1))","metadata":{"execution":{"iopub.status.busy":"2022-05-04T10:06:00.901926Z","iopub.execute_input":"2022-05-04T10:06:00.902651Z","iopub.status.idle":"2022-05-04T10:06:00.929518Z","shell.execute_reply.started":"2022-05-04T10:06:00.902604Z","shell.execute_reply":"2022-05-04T10:06:00.928811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nearest_rows(df, date='2018-09-21', sec_code=4628)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T10:06:01.920688Z","iopub.execute_input":"2022-05-04T10:06:01.921446Z","iopub.status.idle":"2022-05-04T10:06:01.945612Z","shell.execute_reply.started":"2022-05-04T10:06:01.921411Z","shell.execute_reply":"2022-05-04T10:06:01.944805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Let me skip it for now. You are welcome to bring your thoughts in discussion about this case","metadata":{}},{"cell_type":"markdown","source":"# Conclusion","metadata":{}},{"cell_type":"markdown","source":"#### In this notebook we continued the topic of Target calculations from the Close feature, applying 'not cumprod' approach for this adjustment\nSee the previous notebook ***Be careful! Target is not what it claims to be!***     \nhttps://www.kaggle.com/code/vasiliisitdikov/be-careful-target-is-not-what-it-claims-to-be/notebook","metadata":{}},{"cell_type":"markdown","source":"#### Using Target Adjustment we got more consistent result than we have got before. There is only 1 case with huge difference and 19 with threshhold of 10**(-3)","metadata":{}},{"cell_type":"markdown","source":"#### We assume that the reason for the difference in the rest cases is the rounding of price (see my comment on the previous discussion). However, you are welcome to check this assumption.","metadata":{}}]}