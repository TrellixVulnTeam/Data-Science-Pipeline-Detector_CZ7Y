{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Introduction","metadata":{}},{"cell_type":"markdown","source":"This Notebook was created as an experiment for the JPX competition.  \nI consider this competition to be a prediction of the rate of increase in the stock price between the next day and the following day.  \nIt seems extremely difficult to me and seems to depend on happenstance.  \n\nI have attempted to use LGB to predict Target.","metadata":{}},{"cell_type":"markdown","source":"## Preparations","metadata":{}},{"cell_type":"code","source":"#===== Libraries =====\n\nimport os\nimport gc\nimport re\nimport numpy as np\nimport pandas as pd\n#pd.set_option('display.max_rows', 500)\n#pd.set_option('display.max_columns', 500)\n#pd.set_option('display.width', 1000)\n\nimport matplotlib.pyplot as plt\n\nfrom datetime import datetime\nfrom datetime import timedelta, date\n\nfrom sklearn.metrics import mean_squared_error\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport lightgbm as lgb\n\nimport jpx_tokyo_market_prediction","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#===== Path =====\n\ndata_path = \"../input/jpx-tokyo-stock-exchange-prediction/\"\ntrain_path = os.path.join(data_path, \"train_files/\")\nsupplemental_path = os.path.join(data_path, \"supplemental_files/\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Config","metadata":{}},{"cell_type":"code","source":"#===== CFG =====\n   \nclass CFG:\n    start_day = \"2020-9-20\"\n    train_days = 5\n    num_lags = 31\n    lag_itv = 1\n    #lag_list = []\n    lag_list = [1, 2, 3, 4, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60]\n    #lag_list = [1, 2, 3, 4, 5, 10, 20, 30, 40, 50, 60]\n    ma_list = [5, 10, 25]\n    \n    cat_features = None\n    #cat_features = ['SecuritiesCode', 'SectorCode33']\n\n    select_model_to_pred = 5\n    without_growth_m = True","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data loading","metadata":{}},{"cell_type":"code","source":"stock_list = pd.read_csv(os.path.join(data_path, \"stock_list.csv\"))\nstock_prices_bak = pd.read_csv(os.path.join(train_path, \"stock_prices.csv\"))\nfinancials = pd.read_csv(os.path.join(train_path, \"financials.csv\"))\n\ns1 = stock_list.columns.tolist()\ns1[5] = 'SectorCode33'\ns1[7] = 'SectorCode17'\nstock_list.columns = s1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## EDA\n\nEDA was performed on the following NoteBook.  \nPrease take a look at it, as the number of views is very small ;)  \n\n**[JPX EDA(price volatility): Data Understanding by R](https://www.kaggle.com/code/kei96kag/jpx-eda-price-volatility-data-understanding-by-r)**","metadata":{}},{"cell_type":"markdown","source":"### Functions","metadata":{}},{"cell_type":"markdown","source":"reduce_mem_usage","metadata":{}},{"cell_type":"code","source":"# https://www.kaggle.com/competitions/foursquare-location-matching/discussion/321520\n\ndef reduce_mem_usage(df, verbose=False):\n\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2\n\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n\n    return df\n\n#","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Functions for FE","metadata":{}},{"cell_type":"code","source":"def make_date_num(df):\n     \n    tmp = df.groupby([\"Date\"]).groups #dict\n    k = list(tmp.keys())\n    \n    for num in range(len(k)):\n        df.loc[tmp[k[num]], \"Date_num\"] = num\n        \n    df['Date_num'] = df['Date_num'].astype(np.int16)\n    \n    return df\n#\ndef add_Close_lag(df):\n\n    df['Close_tmp'] = df['Close'] * df['AdjustmentFactor']\n    df['Close_1'] = df[['SecuritiesCode', 'Close_tmp']].groupby('SecuritiesCode').shift(1)\n    df = df.drop('Close_tmp', axis=1)\n    \n    return df\n#\ndef add_psych12(df):\n\n    df['u_or_d'] = 1\n    idx = df.query('Close-Close_1 < 0').index\n    df.loc[idx, 'u_or_d'] = 0  \n    df['psych12'] = df.groupby('SecuritiesCode')['u_or_d'].transform(\n        lambda x: x.rolling(12).mean()) \n    \n    return df\n#\ndef add_moving_avg(df):\n    \n    for n in CFG.ma_list:\n        ma_col = f'ma_{n}'\n        d_ma_col = f'd_ma_{n}'\n        df[ma_col] = df.groupby('SecuritiesCode')['Close'].transform(\n            lambda x: x.rolling(n).mean())\n        df[d_ma_col] = (df['Close']-df[ma_col])/df['Close']\n        \n    return df\n#\ndef add_ma_lags(df):\n    \n    for n in CFG.ma_list:\n        ma_feature = f'ma_{n}'\n        #df[ma_feature] = df[ma_feature].fillna(0)\n        for i in range(1, 3): #2日前まで傾きを出す\n            df[f'{ma_feature}_{i}'] =\\\n                df[['SecuritiesCode',ma_feature]].groupby('SecuritiesCode').shift(i)\n    return df        \n#\ndef add_ma_slopes(df):\n\n    for n in CFG.ma_list:\n        #break\n        n1 = f'ma_{n}'\n        n2 = f'{n1}_1'\n        n3 = f'{n1}_2'            \n        x = np.array([1,2,3])\n        y = df[[n3, n2, n1]].values\n        \n        slopes = []\n        c=0\n        for i in range(len(df)):\n            try:\n                slope, intercept =np.polyfit(x,y[i],1)\n                slopes.append(slope/y[i][2])\n            except:\n                slope = 0.\n                slopes.append(slope)\n                c += 1\n        df[f'slope_ma_{n}'] = slopes\n        \n    return df\n#\ndef add_rate_and_lags(df):\n    \n    df['rate'] =\\\n        (df['Close'] - df['Close_1'])/(df['Close_1'])\n        \n    df['rate_f'] = df[['SecuritiesCode', 'rate']].groupby('SecuritiesCode')['rate'].shift(-1)\n\n    reduce_mem_usage(df)\n    \n    #===== lags =====\n    \n    if len(CFG.lag_list) != 0:\n        lags = CFG.lag_list\n        CFG.num_lags = max(lags)\n    else:\n        lags = np.arange(1, CFG.num_lags, CFG.lag_itv)#+1\n\n    lag_cols = [f'rate_{lag}' for lag in lags]\n    \n    for lag, lag_col in zip(lags, lag_cols):\n        print(f'Create lag cols... {lag_col}')\n        df[lag_col] = df[['SecuritiesCode', 'rate']].groupby('SecuritiesCode')['rate'].shift(lag)\n    \n    return df\n#\ndef remove_na_rows(df):\n    \n    #最初の方のna行を削除\n    df_date = df['Date'].unique()\n    df_date_start = df_date[CFG.num_lags + 2]\n    df = df[df['Date'] >= df_date_start].reset_index(drop = True)\n    \n    return df    \n#\ndef create_df(folds):\n    \n    folds_date = folds['Date'].unique() \n    folds_date_end = folds_date[-2] # 最後の日はrate_fがない\n    \n    test_fds = folds[folds['Date'] == folds_date[-1]].reset_index(drop = True)\n    folds = folds[folds['Date'] <= folds_date_end].reset_index(drop = True)\n    \n    folds = make_date_num(folds)\n    test_fds = make_date_num(test_fds)\n    \n    d1 = folds['Date_num'].unique()\n    map1 = dict(zip(d1, reversed(range(len(d1)))))\n    folds['Date_num_r'] = folds['Date_num'].map(map1)\n    \n    folds['fold'] = folds['Date_num_r']/CFG.train_days\n    folds['fold'] = folds['fold'].astype(np.int16)\n    #folds.groupby(['fold']).size()\n    test_fds['fold'] = 0\n    \n    return folds, test_fds","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### LGB","metadata":{}},{"cell_type":"code","source":"def run_lgb_loop(df, fet_cols, cat_fet, params, imp_gra):\n\n    f_loop = len(df['fold'].unique()) # 6/1修正 -1 を削除\n    scores = []\n    models = []\n    \n    for n in range(1, f_loop):\n        #validは必ず#0なのでループは１から回す\n        train_fold = df.query('fold == @n').reset_index(drop = True)\n        valid_fold = df.query('fold == 0').reset_index(drop = True)\n        \n        X_train = train_fold.loc[:, fet_cols]\n        y_train = train_fold[\"rate_f\"].values\n        \n        X_valid = valid_fold.loc[:, fet_cols]\n        y_valid = valid_fold['rate_f'].values\n        \n        \n        train_data = lgb.Dataset(X_train, label = y_train,\n                                categorical_feature = cat_fet,\n                                free_raw_data = False)\n        \n        valid_data = lgb.Dataset(X_valid, y_valid,\n                            categorical_feature = cat_fet,\n                                free_raw_data = False)\n        \n        model_reg = lgb.train(params,\n                          train_data,\n                          valid_sets = [train_data, valid_data],\n                          verbose_eval = 250)\n        \n        pred = model_reg.predict(X_valid)\n        \n        mask_na = ~np.isnan(y_valid)\n        score = np.sqrt(mean_squared_error(y_valid[mask_na], pred[mask_na]))\n        print(n, score)\n        scores.append(score)\n        models.append(model_reg)\n        \n        if imp_gra:\n            i_title =f'fold-{n}: score--> {score:.4f}' \n            lgb.plot_importance(model_reg, figsize=(8,4), max_num_features=20, importance_type='gain', title = i_title)\n    \n    #score_dictはゼロ番はじまりでよい\n    score_dict = dict(zip(range(f_loop-1), scores))\n    score_dict_sorted = sorted(score_dict.items(), key = lambda x:x[1])\n    \n    return score_dict_sorted, models","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predictions","metadata":{}},{"cell_type":"markdown","source":"### Features\nI attempted to forecast in the following way.  \n* The target of the forecast was the stock price appreciation rate for the next day (variable: rate_f).  \n\n**The idea was to predict this value using phases of similar price movements in the past.**\n\nThe feature is as follows.  \nNumerical values were assumed to be expressible as percentages.  \n\n* Number of days in the training period (number of days in 1fold)\n* SecuritiesCode\n* 33SectorCode\n* Stock price appreciation rate (rate) and its lag\n* Psychlogical-12 \n* Moving average deviation rate of stock price (5 days/10 days/25 days)\n* Slope rate of the moving average: Rejected from the middle of the moving average because of the calculation time.\n\nvalid is the fold including the last day of train (fold0), and other folds were evaluated by LGB.  \nThe average value of the prediction using multiple folds with small errors was used as the next day's rate of increase (Prediction1).\n\nOnce the next day's rate of increase was calculated (Prediction1), the Feature was updated to further estimate the next day's rate of increase (Prediction2). We considered this to be Target.","metadata":{}},{"cell_type":"markdown","source":"以下のような方法で予測を試みました。  \n* 予測対象は翌日の株価上昇率(変数：rate_f)としました。   \n\n**この値を過去の同じような値動きの局面を使用して予測することを考えました。**\n\nfeatureは以下です。  \n数値は、パーセンテージで表現できるものとしました。  \n\n* training期間の日数（1foldの日数）\n* 株式Code\n* 33業種Code\n* 株価上昇率(rate)とそのlag\n* Psychlogical12 \n* 株価の移動平均乖離率(5日/10日/25日)\n* 移動平均の傾き率：計算時間がかかるので途中から不採用\n\nvalidはtrainの最終日を含むfold(fold0)とし、それ以外のfoldについてLGBで評価しました。  \n誤差の小さいfoldを複数用いたpredictionの平均値を翌日の上昇率(Prediction1)としました。\n\n翌日の上昇率が算出されたら(Prediction1)、Featureを更新して、さらに翌日の上昇率を推定します(Prediction2)。これがTargetと考えました。","metadata":{}},{"cell_type":"code","source":"env = jpx_tokyo_market_prediction.make_env()\niter_test = env.iter_test()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Prediction1","metadata":{}},{"cell_type":"markdown","source":"First, the rate of the return for the next day is predicted.  \nThe 'rate_f' is the predicted rate of return for the next day.","metadata":{}},{"cell_type":"code","source":"def prediction_1(df):\n    \n    #===== Prediction1 =====\n    \n    test_pred1 = df.loc[:, feature_cols]\n\n    predictions1 = []\n\n    for i in model_num:\n        model  = lgb_models[i]\n        pred1 = model.predict(test_pred1)\n        predictions1.append(pred1)\n\n    prediction1  = np.mean(predictions1, axis = 0)\n    len(prediction1)\n    test_pred1['rate_f'] = prediction1 #This in the preicted return for nect day.\n    \n    return test_pred1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Prediction2","metadata":{}},{"cell_type":"markdown","source":"The next day's closing price is calculated from the Prediction1 result.  \nFeture is calculated again and the prediction is made using the Prediction1 model.","metadata":{}},{"cell_type":"markdown","source":"Prediction1の結果から翌日の終値を算出します。  \n再びFetureを計算して、Prediction1のモデルで予測をおこないます。","metadata":{}},{"cell_type":"code","source":"def prediction_2(df):\n\n    #===== Prediction2 =====\n\n    s1 = df.columns\n    s2 =[s for s in s1 if \"rate\" in s]\n\n    date_pred1 = train_saved['Date'].unique()[-1]\n    test_pred2 = train_saved.query('Date == @date_pred1').reset_index(drop = True)\n\n    date_pred2 = pd.to_datetime(date_pred1)+ timedelta(days=1)\n    test_pred2['Date'] = date_pred2\n    \n    #Cal Close values\n    test_pred2['Close'] = round(test_pred2['Close']*test_pred2['AdjustmentFactor']*(1 + test_pred1['rate_f']),1)\n    test_pred2['AdjustmentFactor'] = 1.\n    reduce_mem_usage(test_pred2)\n\n    test_pred2 = pd.concat([train_saved, test_pred2], axis = 0)\n    test_pred2 = test_pred2.reset_index(drop = True)\n    \n    #FE\n    test_pred2 = add_Close_lag(test_pred2)\n    test_pred2 = add_psych12(test_pred2)\n    test_pred2 = add_moving_avg(test_pred2)\n    test_pred2 = add_ma_lags(test_pred2)\n    \n    test_pred2 = test_pred2.query('Date == @date_pred2').reset_index(drop = True)\n    \n    s3 = ['rate_f']\n    for s in s2[:-1]:\n        s3.append(s)\n\n    test_pred2_rate = df.loc[:, s3[:-1]]\n    test_pred2_rate.columns = s2[:-1]\n\n    test_pred2 = pd.concat([test_pred2, test_pred2_rate], axis = 1)\n    test_pred2 = test_pred2.loc[:, feature_cols]\n\n    predictions2 = []\n\n    for i in model_num:\n        model  = lgb_models[i]\n        pred2 = model.predict(test_pred2)\n        predictions2.append(pred2)\n\n    prediction2  = np.mean(predictions2, axis = 0)\n    len(prediction2)\n    \n    return prediction2","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Summary of results","metadata":{}},{"cell_type":"markdown","source":"### Overall","metadata":{}},{"cell_type":"markdown","source":"I varied the parameters and evaluated the LGB based on 2021 data.  \n**Overall, the LB scores tended to be negative.**  \nThe variation in the valid score suggests that I have not been able to extract the most promising features.  \n\nThe main results are as follows. The data used for the forecast is for the year 2021.  \n\n* Including 'SecuritiesCode' and 'SectorCode33' in the features often results in negative LB scores.\n* In many cases, the score improves slightly when data from the Growth market is removed.\n* When the training period (train_days) was set to 5/10/15/20 days, the score became positive only on 5 days case.","metadata":{}},{"cell_type":"markdown","source":"パラメータを変化させて、2021年のデータをもとにLGBで評価をおこないました。  \n**全体的にLBスコアは負の値が多い結果となりました。**  \nvalidスコアの変化から、有力な特徴量が抽出できていないと思われます。  \n\n主な結果は以下になります。予測に使用したデータは2021年のものです。  \n\n* 'SecuritiesCode', 'SectorCode33'を特徴量に含めると、LBスコアはマイナスになるケースが多い。\n* Growth市場のデータを抜くとややスコアが向上するケースが多い。\n* training期間（train_days)を5/10/15/20日としたところ、スコアがプラスになったのは5日だけであった。","metadata":{}},{"cell_type":"markdown","source":"### LB scores","metadata":{}},{"cell_type":"markdown","source":"A summary of the LB scores is shown below.  \n\nLB was positive in the case of 5 Training days.  \nScores are often better when 'SecuritiesCode' and 'SectorCode33' are not included in the features.\n* train_days = 5\n* LB = 0.049 ~ 0.262 \n\nThe result is negative when the number of training days is 10 or more.  \nThe absolute value is higher than in the case of 5 days, so multiplying by -1 and submitting will result in a better score.  \nThe absolute value of the score is higher when the 'SecuritiesCode' and 'SectorCode33' features are included.\n* train_days = 10\n* LB = -0.1 ~ -0.3 (approximately)\n\nIn other words, individual stocks are likely to move in the opposite direction from a similar phase in 2021.","metadata":{}},{"cell_type":"markdown","source":"LBスコアのまとめは以下のようになります。  \n\nLBがプラスになったのはTraining日数が5日のケースでした。  \n'SecuritiesCode', 'SectorCode33'を特徴量に含めない方がスコアは良くなることが多い。\n* train_days = 5\n* LB = 0.049 ~ 0.221 \n\nTraining日数が10日またはそれ以上の場合、結果はマイナスの値になりました。  \n5日の場合より絶対値が大きいため、-1をかけてsubmitするとスコアは良くなります。。。  \n(予測に逆張りせよと)  \n'SecuritiesCode', 'SectorCode33'特徴量に含めた方がスコアの絶対値は大きくなります。\n* train_days = 10\n* LB = -0.1 ~ -0.3 (おおよそ)\n\nつまり、個々の銘柄は2021年の同じような局面とは逆の動きをしやすいと思われます。\n","metadata":{}},{"cell_type":"markdown","source":"## Prediction loop for submission","metadata":{}},{"cell_type":"code","source":"cc = 1\nfor (prices, options, financials, trades, secondary_prices, sample_prediction) in iter_test:\n\n    #===== Trainning データの作成 =====\n        \n    stock_prices = stock_prices_bak.copy()\n    #break\n    \n    #データを1日接続\n    stock_prices = pd.concat([stock_prices, prices])\n    stock_prices_bak = stock_prices.copy()\n\n    stock_prices = stock_prices.merge(stock_list[['SecuritiesCode', 'SectorCode33', 'NewMarketSegment']], \n                                     how='left', on = 'SecuritiesCode')\n    stock_prices['SectorCode33'] =stock_prices['SectorCode33'].astype(np.int16)\n    stock_prices['Date'] = pd.to_datetime(stock_prices['Date'], format = \"%Y-%m-%d\")\n    #\n    price_features = ['Date', \n                       'SecuritiesCode', \n                       'Open', 'High', 'Low', 'Close', 'Volume',\n                       'AdjustmentFactor','SectorCode33', 'NewMarketSegment']\n\n    train = stock_prices.loc[:, price_features]\n    train = train[train['Date'] >= CFG.start_day].reset_index(drop = True)\n    \n    #\n    train_seg = train.groupby('NewMarketSegment')['SecuritiesCode'].unique().reset_index()\n    train_seg['n_codes'] = train_seg['SecuritiesCode'].apply(lambda x: len(x))\n    train = train.drop('NewMarketSegment', axis = 1)\n    \n    if CFG.without_growth_m:\n        mask = ~train['SecuritiesCode'].isin(train_seg['SecuritiesCode'][0])\n        train = train[mask].reset_index(drop = True)\n    \n    #===== Crete features =====\n\n    train = add_Close_lag(train)\n    train = add_psych12(train) #サイコロ12\n    train = add_moving_avg(train) #移動平均\n    train = add_ma_lags(train) #移動平均のlag\n\n    train = add_rate_and_lags(train) #rateのlag\n    train = remove_na_rows(train)\n    #train = add_ma_slopes(train) #移動平均の傾き\n\n    #Prediction2のFE用\n    base_cols = ['Date', 'SecuritiesCode', 'Close', 'AdjustmentFactor', 'SectorCode33']\n    date_tmp = train['Date'].unique()[-30]\n    train_saved = train.query('Date >= @date_tmp')\n    train_saved = train_saved.loc[:, base_cols].reset_index(drop = True)\n\n    train_folds, test_folds = create_df(train)\n    \n    #全銘柄が含まれるのfoldを残す\n    df_n = train_folds.groupby('fold').size()\n    growth_num = 0\n    if CFG.without_growth_m:\n        growth_num = len(train_seg.iloc[0, 1])\n    fold_mem = CFG.train_days * (2000 - growth_num)\n    fold_true = df_n[df_n == fold_mem].index\n    mask = train_folds['fold'].apply(lambda x: x in fold_true.tolist())\n    train_folds = train_folds[mask].reset_index(drop = True)\n    print(\"train_folds\", fold_true)\n\n    del(train)\n    gc.collect()    \n\n    #===== LGB =====\n\n    s1 = train_folds.columns.tolist()\n\n    drop_cols = ['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'AdjustmentFactor', \n                 'Close_1', 'rate_f', 'Date_num', 'Date_num_r', \n                 'fold', 'u_or_d']\n    if CFG.cat_features is None:\n        for s in ['SecuritiesCode', 'SectorCode33']:\n            drop_cols.append(s)\n        \n    drop_cols2 = [s for s in s1 if re.search(r'^ma', s)]\n\n    train_tmp = train_folds.drop(drop_cols, axis = 1).iloc[0:1, :]\n    feature_cols = train_tmp.drop(drop_cols2, axis = 1).columns\n\n    cat_features = CFG.cat_features\n    \n    lgb_params = {\n            \"objective\": \"regression\",\n            \"metric\" :\"rmse\",\n            \"force_col_wise\" : True,\n            \"learning_rate\" : 0.1,\n            'feature_fraction' : 0.8,\n            \"lambda_l2\" : 0.1,\n            'random_state': 11,\n            'verbosity': 1,\n            'num_iterations' : 500,\n    }#\n    \n    if cc ==1:\n        IMP_GRA = True\n    else:\n        IMP_GRA = False\n        \n    lgb_scores, lgb_models = run_lgb_loop(train_folds, feature_cols, cat_features, lgb_params, IMP_GRA)\n\n\n    #===== Prediction-1 =====\n    \n    #上位のlist番号\n    model_num = [i[0] for i in lgb_scores[0:CFG.select_model_to_pred]]\n    test_pred1 = prediction_1(test_folds)\n\n\n    #===== Prediction-2 =====\n\n    target = prediction_2(test_pred1)\n\n#\n    df_pred = pd.DataFrame()\n    df_pred['SecuritiesCode'] = sample_prediction['SecuritiesCode']\n    \n    if CFG.without_growth_m:\n        mask = ~df_pred['SecuritiesCode'].isin(train_seg.iloc[0,1])\n        df_pred.loc[df_pred[mask].index, \"Prediction\"] = target\n        df_pred['Prediction'] = df_pred['Prediction'].fillna(0)\n    else:\n        df_pred['Prediction'] = target\n        #df_pred['Prediction'] = prediction1\n\n#      \n    sample_prediction[\"Prediction\"] = df_pred['Prediction']\n    sample_prediction = sample_prediction.sort_values(by = \"Prediction\", ascending=False)\n    sample_prediction['Rank'] = np.arange(0,2000)\n    sample_prediction = sample_prediction.sort_values(by = \"SecuritiesCode\", ascending=True)\n    sample_prediction.drop([\"Prediction\"],axis=1)\n    submission = sample_prediction[[\"Date\",\"SecuritiesCode\",\"Rank\"]].copy()\n    env.predict(submission)\n    \n    display(submission)\n    \n    del(stock_prices, train_folds, test_folds)\n    cc += 1\n    gc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## What's next?","metadata":{}},{"cell_type":"markdown","source":"Unfortunately, the model seems to be unstable.  \n\nI am considering the following\n* Increase training data. Go further back in time.\n* Search for other features?","metadata":{}}]}