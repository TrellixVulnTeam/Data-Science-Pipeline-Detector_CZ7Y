{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"![](https://bigdataanalyticsnews.com/wp-content/uploads/2021/04/Feature-Engineering.png)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-14T18:06:33.779856Z","iopub.execute_input":"2022-05-14T18:06:33.780194Z","iopub.status.idle":"2022-05-14T18:06:37.223715Z","shell.execute_reply.started":"2022-05-14T18:06:33.780161Z","shell.execute_reply":"2022-05-14T18:06:37.222649Z"}}},{"cell_type":"code","source":"\nimport os\nimport warnings\nfrom pathlib import Path\n\n# Basic libraries\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport time\nimport pandas_profiling as pp\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom plotly.offline import init_notebook_mode, iplot\nfrom IPython.core.display import display, HTML #To display html content in a code cell\n%matplotlib inline \n\n#This is a function that downcast the integer columns\ndef downcast_df_int_columns(df):\n    list_of_columns = list(df.select_dtypes(include=[\"int32\", \"int64\"]).columns)\n        \n    if len(list_of_columns)>=1:\n        max_string_length = max([len(col) for col in list_of_columns]) # finds max string length for better status printing\n        print(\"downcasting integers for:\", list_of_columns, \"\\n\")\n        \n        for col in list_of_columns:\n            print(\"reduced memory usage for:  \", col.ljust(max_string_length+2)[:max_string_length+2],\n                  \"from\", str(round(df[col].memory_usage(deep=True)*1e-6,2)).rjust(8), \"to\", end=\" \")\n            df[col] = pd.to_numeric(df[col], downcast=\"integer\")\n            print(str(round(df[col].memory_usage(deep=True)*1e-6,2)).rjust(8))\n    else:\n        print(\"no columns to downcast\")\n    \n    gc.collect()\n    \n    print(\"done\")\n    \n    \n#This is a function that downcast the float columns,\n#if you have too many columns to adjust and do not want to see to many messages proceesing, you could comment our the print() columns\ndef downcast_df_float_columns(df):\n    list_of_columns = list(df.select_dtypes(include=[\"float64\"]).columns)\n        \n    if len(list_of_columns)>=1:\n        max_string_length = max([len(col) for col in list_of_columns]) # finds max string length for better status printing\n        print(\"downcasting float for:\", list_of_columns, \"\\n\")\n        \n        for col in list_of_columns:\n            print(\"reduced memory usage for:  \", col.ljust(max_string_length+2)[:max_string_length+2],\n                  \"from\", str(round(df[col].memory_usage(deep=True)*1e-6,2)).rjust(8), \"to\", end=\" \")\n            df[col] = pd.to_numeric(df[col], downcast=\"float\")\n            print(str(round(df[col].memory_usage(deep=True)*1e-6,2)).rjust(8))\n    else:\n        print(\"no columns to downcast\")\n    \n    gc.collect()\n    print(\"done\")\n    \n\nwarnings.filterwarnings(\"ignore\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class='alert alert-info'>\n<h3><center>Import the data</center></h3>\n</div>","metadata":{}},{"cell_type":"code","source":"data=pd.read_csv('../input/jpx-tokyo-stock-exchange-prediction/train_files/stock_prices.csv')\ndata['Date']=pd.to_datetime(data['Date'],format='%Y-%m-%d')\nprint('Shape of the data:',data.shape)\nprint('\\n')\nprint('#of unique row_ids:',data['RowId'].nunique())\nprint('\\n')\nprint('#Unique dates:',data['Date'].nunique())\nprint('\\n')\nprint('# of securities:',data['SecuritiesCode'].nunique())\nprint('\\n')","metadata":{"execution":{"iopub.status.busy":"2022-05-14T18:23:56.636579Z","iopub.execute_input":"2022-05-14T18:23:56.637024Z","iopub.status.idle":"2022-05-14T18:24:02.754384Z","shell.execute_reply.started":"2022-05-14T18:23:56.636987Z","shell.execute_reply":"2022-05-14T18:24:02.753388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class='alert alert-info'>\n<h3><center>Reduce the size of the data</center></h3>\n</div>","metadata":{}},{"cell_type":"code","source":"data.info(memory_usage='deep')","metadata":{"execution":{"iopub.status.busy":"2022-05-14T18:24:02.756272Z","iopub.execute_input":"2022-05-14T18:24:02.756686Z","iopub.status.idle":"2022-05-14T18:24:03.200118Z","shell.execute_reply.started":"2022-05-14T18:24:02.756656Z","shell.execute_reply":"2022-05-14T18:24:03.199181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class='alert alert-info'>\n<h3> <center> Reducing the size of the Integer columns</center></h3>\n</div>","metadata":{}},{"cell_type":"code","source":"import gc \n#Reducing Size of the Numerical dtypes\n\nprint(' \\n\\t\\t\\t\\t\\t\\t\\tReducing the size of integer columns by converting them from int64 to int32\\n\\n\\n')\ndowncast_df_int_columns(data)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-05-14T18:24:03.201788Z","iopub.execute_input":"2022-05-14T18:24:03.202085Z","iopub.status.idle":"2022-05-14T18:24:03.517342Z","shell.execute_reply.started":"2022-05-14T18:24:03.202044Z","shell.execute_reply":"2022-05-14T18:24:03.516706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class='alert alert-info'>\n<h3> <center> Reducing the size of the Float columns</center></h3>\n</div>","metadata":{"execution":{"iopub.status.busy":"2022-05-14T18:14:42.858264Z","iopub.execute_input":"2022-05-14T18:14:42.860889Z","iopub.status.idle":"2022-05-14T18:14:42.873262Z","shell.execute_reply.started":"2022-05-14T18:14:42.860762Z","shell.execute_reply":"2022-05-14T18:14:42.871247Z"}}},{"cell_type":"code","source":"print(' \\n\\t\\t\\t\\t\\t\\t\\tReducing the size of integer columns by converting them from int64 to int32\\n\\n\\n')\n\ndowncast_df_float_columns(data)","metadata":{"execution":{"iopub.status.busy":"2022-05-14T18:24:03.518769Z","iopub.execute_input":"2022-05-14T18:24:03.518981Z","iopub.status.idle":"2022-05-14T18:24:03.876762Z","shell.execute_reply.started":"2022-05-14T18:24:03.518955Z","shell.execute_reply":"2022-05-14T18:24:03.875842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class='alert alert-info'>\n<h3> <center> The reduced size of the data</center></h3>\n</div>","metadata":{}},{"cell_type":"code","source":"data.info(memory_usage='deep')\n\nprint('\\n\\n\\n\\t\\t\\t\\t\\t\\tThe dataset size has been reduced from 336 MB to 251 MB')","metadata":{"execution":{"iopub.status.busy":"2022-05-14T18:24:04.304031Z","iopub.execute_input":"2022-05-14T18:24:04.304339Z","iopub.status.idle":"2022-05-14T18:24:04.75654Z","shell.execute_reply.started":"2022-05-14T18:24:04.304307Z","shell.execute_reply":"2022-05-14T18:24:04.755359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_month(dt):\n    x = dt.strftime(\"%b\")\n    return(x)\n\nfrom calendar import monthcalendar\ndef get_week_of_month(year, month, day):\n    return next(\n        (\n            week_number\n            for week_number, days_of_week in enumerate(monthcalendar(year, month), start=1)\n            if day in days_of_week\n        ),\n        None,\n    )\n\ndef upper_shadow(df):\n    return df['High'] - np.maximum(df['Close'], df['Open'])\n\ndef lower_shadow(df):\n    return np.minimum(df['Close'], df['Open']) - df['Low']\n\ndef feature_engineering(trainer,tag):\n    if tag==1: trainer.dropna(subset=['Close'],inplace=True) #Some records has nan values in closing price column\n    trainer['Date']=pd.to_datetime(trainer['Date'],format='%Y-%m-%d')\n    trainer['Month']=trainer['Date'].dt.month\n    trainer['Year']=trainer['Date'].dt.year\n    trainer['Day']=trainer['Date'].dt.day\n    \n    \n    trainer['week_of_month']=trainer.apply(lambda x: get_week_of_month(x.Year,x.Month,x.Day),axis=1)\n    trainer.drop(columns=['Month'],inplace=True)\n    \n    trainer['Month_name']=trainer['Date'].apply(lambda x: get_month(x))\n    one_hot = pd.get_dummies(trainer['Month_name'])\n    trainer=trainer.drop('Month_name',axis=1)\n    trainer=trainer.join(one_hot)\n    \n    #trainer['Year']=trainer['Date'].dt.year.astype('category')\n    #one_hot = pd.get_dummies(trainer['Year'])\n    trainer=trainer.drop('Year',axis=1)\n    #trainer=trainer.join(one_hot)\n    \n    trainer['dayofweek_num']=trainer['Date'].dt.dayofweek\n    trainer['is_quater_start']=trainer['Date'].dt.is_quarter_start.map({False:0,True:1})\n    trainer['is_month_start']=trainer['Date'].dt.is_month_start.map({False:0,True:1})\n    trainer['is_month_end']=trainer['Date'].dt.is_month_end.map({False:0,True:1})\n\n    #lag features\n    trainer['lag_1'] = trainer['Close'].shift(1)\n    trainer['lag_2'] = trainer['Close'].shift(2)\n    trainer['lag_3'] = trainer['Close'].shift(3)\n    trainer['lag_4'] = trainer['Close'].shift(4)\n    trainer['lag_5'] = trainer['Close'].shift(5)\n    trainer['lag_6'] = trainer['Close'].shift(6)\n    trainer['lag_7'] = trainer['Close'].shift(7)\n    \n    #SMA Features\n    trainer['SMA5'] = trainer.Close.rolling(5).mean()\n    trainer['SMA20'] = trainer.Close.rolling(20).mean()\n    trainer['SMA50'] = trainer.Close.rolling(50).mean()\n    trainer['SMA200'] = trainer.Close.rolling(200).mean()\n    trainer['SMA500'] = trainer.Close.rolling(500).mean()\n\n    #EMA features\n    trainer['EMA5'] = trainer.Close.ewm(span=5, adjust=False).mean()\n    trainer['EMA20'] = trainer.Close.ewm(span=20, adjust=False).mean()\n    trainer['EMA50'] = trainer.Close.ewm(span=50, adjust=False).mean()\n    trainer['EMA200'] = trainer.Close.ewm(span=200, adjust=False).mean()\n    trainer['EMA500'] = trainer.Close.ewm(span=500, adjust=False).mean()\n\n    # Domain Specific features\n    #Difference features \n    trainer['Diff_co']=trainer['Close']-trainer['Open']\n    trainer['Diff_hl']=trainer['High']-trainer['Low']\n    trainer['pclose']=trainer['Close'].shift(-1)\n    trainer['delta']=trainer['Close']-trainer['pclose']\n    trainer['daily_return']=(trainer['Close']/trainer['Open'])-1\n    trainer['upper_shadow']=upper_shadow(trainer)\n    trainer['lower_shadow']=lower_shadow(trainer)\n\n    \n    \n    \n    trainer.drop(columns=['Volume','ExpectedDividend','SupervisionFlag','AdjustmentFactor'],inplace=True)\n    \n    return trainer.set_index(['RowId','Date','SecuritiesCode'])","metadata":{"execution":{"iopub.status.busy":"2022-05-14T18:24:05.585574Z","iopub.execute_input":"2022-05-14T18:24:05.585879Z","iopub.status.idle":"2022-05-14T18:24:05.612341Z","shell.execute_reply.started":"2022-05-14T18:24:05.585845Z","shell.execute_reply":"2022-05-14T18:24:05.611296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class='alert alert-info'>\n<h3><center>Since the dataset has the prices of 2000 securities, I have split them 1000 securities(50%) for training, 700(35%) for validation, and 300(15%) securities for testing</center></h3>\n\n<h6>Note: You can change the percentage of Train/Val/Test according to your strategy</h6>\n</div>","metadata":{}},{"cell_type":"code","source":"unique_sec=list(data['SecuritiesCode'].unique())\nprint('Total number of Securities in the dataset:', len(unique_sec))\nprint('\\n')\n\ntrain=data[data['SecuritiesCode'].isin(unique_sec[0:1000])]\nval=data[data['SecuritiesCode'].isin(unique_sec[1000:1700])]\ntest=data[data['SecuritiesCode'].isin(unique_sec[1700:len(unique_sec)])]\nprint('Considered training securities shape: ',train.shape)\nprint('Taken number of securities for training:',data[data['SecuritiesCode'].isin(unique_sec[0:1000])]['SecuritiesCode'].nunique())\nprint('Train data percentage(%):',round(((data[data['SecuritiesCode'].isin(unique_sec[0:1000])]['SecuritiesCode'].nunique()/len(unique_sec))*100),2))\nprint('\\n')\nprint('Considered Validation securities shape: ',val.shape)\nprint('Considered securities in val:',val['SecuritiesCode'].nunique())\nprint('Validation data percentage(%):',round(((data[data['SecuritiesCode'].isin(unique_sec[1000:1700])]['SecuritiesCode'].nunique()/len(unique_sec))*100),2))\nprint('\\n')\nprint('Considered testing securities shape: ',test.shape)\nprint('Considered securities in val:',test['SecuritiesCode'].nunique())\nprint('Test data percentage(%):',round(((data[data['SecuritiesCode'].isin(unique_sec[1700:len(unique_sec)])]['SecuritiesCode'].nunique()/len(unique_sec))*100),2))","metadata":{"execution":{"iopub.status.busy":"2022-05-14T18:26:58.176669Z","iopub.execute_input":"2022-05-14T18:26:58.176959Z","iopub.status.idle":"2022-05-14T18:26:58.816881Z","shell.execute_reply.started":"2022-05-14T18:26:58.176929Z","shell.execute_reply":"2022-05-14T18:26:58.815922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n#del data\n\n#IF you dont like to split the data before feature engineering\n#data=feature_engineering(data,1)\n\ntrain_copy=train.copy()\ntest_copy=test.copy()\nval_copy=val.copy()\ngc.collect()\n\ntrain=feature_engineering(train,1)\ntest=feature_engineering(test,0)\nval=feature_engineering(val,0)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-14T18:27:08.570771Z","iopub.execute_input":"2022-05-14T18:27:08.571052Z","iopub.status.idle":"2022-05-14T18:29:49.175086Z","shell.execute_reply.started":"2022-05-14T18:27:08.571024Z","shell.execute_reply":"2022-05-14T18:29:49.174352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class='alert alert-info'>\n<h3><center>Save the feature engineered dataset 📚</center></h3>\n</div>","metadata":{}},{"cell_type":"code","source":"train.to_csv('train.csv')\ntest.to_csv('test.csv')\nval.to_csv('val.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}