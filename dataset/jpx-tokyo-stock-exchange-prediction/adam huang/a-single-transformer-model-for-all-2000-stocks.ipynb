{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# JPX Tokyo Stock exchange prediction competition\n\nThis competition involves building portfolios from stocks in Tokyo Stock Exchange (around 2,000 stocks). Specifically, given the historical pricing data, each participant ranks the stocks from highest to lowest expected returns and is evaluated on the difference in return.\n \n# Time series modeling with transformer network\nThere have been many applications of neural network model to predictions of time series data such as stock prices. Transformer network has been proven to be remarkably superior to earlier RNNs in NLP applications. When it comes to stock price prediction with transformer model, most publicly available samples treat the stock price as univariant time series, where the input is simply the previous closing prices of a stock, which is then used to predict a future price. This approach ignores other potential features, such as volume, daily low, high, option etc., that may potentially influence the future price movements. The stochastic nature of stock price aside, a multi-variant time series approach should obviously provide an advantage.  \nThe other challenge of this particular JPX competition is that with around 2000 stocks in the pool, a one-model-per-stock approach will likely exceed the total time allowed for training (9 hours) if we simply train and predict each stock individually. This approach also misses out the over-all market trends, in other words, the covariance between stocks. \nTo address these two challenges, a single transformer encoder is trained with time series data of all 2000 stocks. The token of the time series has a dimension of 2000, so has the label. Each element of the token represents a single stock. The window size of the time series is a hyper parameter.  To include  other features, besides the stock price itself, a customizer layer is introduced before the transformer block. This so-called diagonal dense layer can be thought of as a dense layer where all off diagonal elements of the weight matrix are set to 0. This layer acts as a linear regression function that combines all features in the input for a single stock and outputs a single value, which is then placed in the time series token to feed into the transformer. This approach allows us to keep the embedding dimension of the token manageable at 2000, thus reduces the size of the model and overfitting tendency. It also retains the association of the features with particular stocks. ","metadata":{}},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport jpx_tokyo_market_prediction\n\n#from IPython.core.debugger import set_trace\nfrom sklearn.preprocessing import OrdinalEncoder\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras import layers\nfrom tensorflow.python.ops.numpy_ops import np_config\nnp_config.enable_numpy_behavior()\n","metadata":{"execution":{"iopub.status.busy":"2022-05-31T20:10:45.993064Z","iopub.execute_input":"2022-05-31T20:10:45.993672Z","iopub.status.idle":"2022-05-31T20:10:52.168614Z","shell.execute_reply.started":"2022-05-31T20:10:45.993569Z","shell.execute_reply":"2022-05-31T20:10:52.167831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Helper functions for preparing the time series data set.","metadata":{}},{"cell_type":"code","source":"# not all days have the same number of stocks, so we need to pad the missing data\ndef pad_missing_stock_code( sample, codes):\n    # missing code\n    missing_codes = set( list( range(0, len(codes)))) - set( [i[0] for i in sample])\n    # drop the code column\n    x = sample[:,1:]\n    for idx in sorted(missing_codes):\n        x = np.insert( x, idx, 0.0, axis=0)\n    return x\n\n\ndef windowed_dataset(series, window_size, batch_size):\n    #series = tf.expand_dims(series, axis=-1)\n    ds = tf.data.Dataset.from_tensor_slices(series)\n    ds = ds.window(window_size, shift=1, drop_remainder=True)\n    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n  #  ds = ds.shuffle( len( series))\n    # first 2 columns are closing price, volume, last column is the label - target\n    ds = ds.map(lambda w: (w[:,:,0:-1], w[-1,:,-1].reshape(-1)))\n    #if batch_size == 1: return ds\n    return ds.batch(batch_size).prefetch(1)\n\n\n#calculate the change percentage between two consecutive days, day1 and day2 have the shape of (stock_list, features_list+label)\n#col_list is the list of features needed to calculate the change percentage, the rest features should stay\ndef calculate_change_percentage_per_day( day1, day2, col_list):\n    r = day2.copy()\n    for k in col_list:\n        for j in range(0, day1.shape[0]):\n            if k == 0:  # the difference of day2 open to day1 close\n                r[j, k] = 0.0 if day1[j, 0] < 1.e-8 or day2[j, 1] < 1.e-8 else (day2[j, 0] - day1[j, 1]) / day1[j, 0]\n            else:\n                r[j, k] = 0.0 if day1[j, k] < 1.e-8 or day2[j, k] < 1.e-8 else (day2[j,k] - day1[j,k]) / day1[j,k]\n\n    return r\n\n\n#calculate the change percentage between two consecutive days\ndef calculate_change_percentage( series, cols_to_calculate):\n    for i in range(1, len(series)):\n        series[i-1] = calculate_change_percentage_per_day(series[i-1], series[i], cols_to_calculate)\n\n    series.pop() # remove the first element\n    return series\n\n\n# prep time series data set for training and validation\ndef prep_time_series_dataset( prices,  window_size, batch_size):   \n    codes = list(prices.SecuritiesCode.unique())\n    date_list = list(prices.Date.unique())\n    prices = prices[['Date', 'SecuritiesCode', 'Open', 'High', 'Low', 'Close', 'Volume', 'Target']].dropna()\n    prices['Low_high_ratio'] = (1 - prices['Low'] / prices['High'])\n    prices.dropna()\n    prices = prices[['Date', 'SecuritiesCode', 'Open', 'Close', 'Volume', 'Low_high_ratio', 'Target']]\n    \n    #normalize target value to percentage\n    prices[\"Target\"] = prices[\"Target\"]*100\n    \n    price_series = prices.sort_values(by=['Date', 'SecuritiesCode']).reset_index(drop=True).dropna()\n\n    daily_data_list =[]\n    for dt in date_list:\n        daily_data = price_series[price_series.Date == dt ].drop(['Date'], axis=1).sort_values(by=['SecuritiesCode'])\n        daily_data_list.append( pad_missing_stock_code( daily_data.to_numpy(), codes))\n\n    # daily_data_list is a 1201 long list of 1-d (2000) array, each array is a day's data, sorted by stock code\n    # need to calculate the change percentage between two consecutive days per stock\n    ds = windowed_dataset( calculate_change_percentage( daily_data_list,[0,1,2]), window_size, batch_size )\n    return ds, np.array(daily_data_list)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-05-31T20:10:52.1722Z","iopub.execute_input":"2022-05-31T20:10:52.172406Z","iopub.status.idle":"2022-05-31T20:10:52.190241Z","shell.execute_reply.started":"2022-05-31T20:10:52.172381Z","shell.execute_reply":"2022-05-31T20:10:52.189618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Transformer Block\n\nThis is mostly a copy from Keras transformer tutorial on [transformer model](https://www.tensorflow.org/text/tutorials/transformer), except no tokenizer is needed here. ","metadata":{}},{"cell_type":"code","source":"class TransformerBlock(layers.Layer):\n    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n        super(TransformerBlock, self).__init__()\n        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=256, #embed_dim,\n                                             kernel_initializer=\"glorot_uniform\",\n                                             dropout=rate)\n        self.ffn = keras.Sequential(\n            [layers.Dense(ff_dim, activation=\"elu\",\n                            bias_initializer=keras.initializers.HeNormal()\n                          ),\n             layers.Dense(embed_dim)]\n        )\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = layers.Dropout(rate)\n        self.dropout2 = layers.Dropout(rate)\n      \n    def call(self, inputs, training):\n        attn_output = self.att(inputs, inputs)\n        attn_output = self.dropout1(attn_output, training=training)\n\n        out1 = self.layernorm1(inputs + attn_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        return self.layernorm2(out1 + ffn_output)\n\n\nclass TokenAndPositionEmbedding(layers.Layer):\n    def __init__(self, maxlen, embed_dim):\n        super(TokenAndPositionEmbedding, self).__init__()\n\n        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n        self.width = maxlen\n        self.embed_dim = embed_dim\n\n    def call(self, x):\n        positions = tf.range(start=0, limit=self.width, delta=1)\n        positions = self.pos_emb(positions)\n        return x + positions","metadata":{"execution":{"iopub.status.busy":"2022-05-31T20:10:52.191532Z","iopub.execute_input":"2022-05-31T20:10:52.192008Z","iopub.status.idle":"2022-05-31T20:10:52.206945Z","shell.execute_reply.started":"2022-05-31T20:10:52.191968Z","shell.execute_reply":"2022-05-31T20:10:52.20592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This so-called Diagonal Dense layer inherits from Keras Dense layer. In essence, it only keeps the diagonal elements of the weight matrix of the dense layer, and ignore the rest.","metadata":{}},{"cell_type":"code","source":"class DiagonalDense(layers.Dense):\n    def __init__(self ,\n                 units,\n                 activation,\n               use_bias=True,\n               kernel_initializer='glorot_uniform',\n               bias_initializer='zeros',\n               kernel_regularizer=None,\n               bias_regularizer=None,\n               activity_regularizer=None,\n               kernel_constraint=None,\n               bias_constraint=None,\n               **kwargs):\n        super(DiagonalDense, self).__init__(\n            units, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\n\n\n    def call(self, inputs):\n        if inputs.shape[-2] != self.units:\n            raise ValueError('DiagonalDense layer requires the second to last dimension of inputs to be equal to the number of units.'\n                             f' Received: inputs.shape={inputs.shape}, units={self.units}')\n        rank = inputs.shape.rank\n        #no need to do anything\n        if rank == 2 or rank is None:\n            return super(DiagonalDense,self).call(inputs)\n        else:\n            #return tf.linalg.diag_part( super(DiagonalDense,self).call(inputs))\n            outputs = tf.reduce_sum( tf.math.multiply(inputs, tf.transpose(self.kernel)), axis=-1)\n\n        if self.use_bias:\n            outputs = tf.nn.bias_add(outputs, self.bias)\n\n        if self.activation is not None:\n            outputs = self.activation(outputs)\n\n        return outputs\n       # return tf.einsum( einsum_str, super(DiagonalDense,self).call(inputs))\n\n    #override the parent method the output shape now is simply the input shape without the last dimension\n    def compute_output_shape(self, input_shape):\n        input_shape = tf.TensorShape(input_shape)\n        input_shape = input_shape.with_rank_at_least(2)\n        if tf.compat.dimension_value(input_shape[-1]) is None:\n          raise ValueError('The last dimension of the input shape of a Dense layer '\n                           'should be defined. Found None. '\n                           f'Received: input_shape={input_shape}')\n        return input_shape[:-1]\n","metadata":{"execution":{"iopub.status.busy":"2022-05-31T20:10:52.2096Z","iopub.execute_input":"2022-05-31T20:10:52.210138Z","iopub.status.idle":"2022-05-31T20:10:52.223979Z","shell.execute_reply.started":"2022-05-31T20:10:52.210097Z","shell.execute_reply":"2022-05-31T20:10:52.222832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def learnRatefunction(epoch):\n    if epoch < 100:\n        lr = 1.0e-1\n    elif epoch < 1500:\n        lr = 2.0e-1\n    else:\n        lr = 5.e-2\n    return lr","metadata":{"execution":{"iopub.status.busy":"2022-05-31T20:10:52.225109Z","iopub.execute_input":"2022-05-31T20:10:52.225515Z","iopub.status.idle":"2022-05-31T20:10:52.237146Z","shell.execute_reply.started":"2022-05-31T20:10:52.225475Z","shell.execute_reply":"2022-05-31T20:10:52.236435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Generating dataset from the provided csv file","metadata":{}},{"cell_type":"code","source":"hist_prices = pd.read_csv(\"../input/jpx-tokyo-stock-exchange-prediction/train_files/stock_prices.csv\")\n# use a subset of the data provided for debugging, as this step takes quite some time\n#hist_prices = hist_prices[hist_prices.Date > '2020-01-01']\n\nstock_code_encoder = OrdinalEncoder(dtype=np.int32)\nencoded_stocks_array = stock_code_encoder.fit_transform(hist_prices[[\"SecuritiesCode\"]])\nhist_prices[\"SecuritiesCode\"] = encoded_stocks_array\nencoded_stocks_list = np.unique( encoded_stocks_array)\n\nwindow_size = 7\nbatch_size = 64\nds, daily_price_series = prep_time_series_dataset( hist_prices, window_size, batch_size)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T20:10:52.238462Z","iopub.execute_input":"2022-05-31T20:10:52.238883Z","iopub.status.idle":"2022-05-31T20:18:24.572571Z","shell.execute_reply.started":"2022-05-31T20:10:52.238844Z","shell.execute_reply":"2022-05-31T20:18:24.57173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build and train the transformer model","metadata":{}},{"cell_type":"code","source":"embed_dim = encoded_stocks_list.shape[0]  # Embedding size for each token, should be 2000\nnum_heads = 4  # Number of attention heads\nff_dim =128  # Hidden layer size in feed forward network inside transformer\n\nno_epoches = 1000\n\n# split into 90% train, 10% val\n#split =18  # batchs  derived from 90% of total no. of sample/batch_size\n#train_ds = ds.take( split)\n#val_ds = ds.skip(split)\n\noptions = tf.data.Options()\noptions.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\n#train_ds = train_ds.with_options(options)\n#val_ds = val_ds.with_options(options)\ntrain_ds = ds.with_options(options)\n\ntf.debugging.set_log_device_placement(True)\ngpus = tf.config.list_logical_devices('GPU')\nstrategy = tf.distribute.MirroredStrategy(gpus)\n\nwith strategy.scope():\n    inputs= layers.Input(shape=( window_size, embed_dim, 4))\n    x= DiagonalDense(embed_dim, activation='elu', use_bias=True)(inputs)\n    embedding_layer = TokenAndPositionEmbedding(window_size,  embed_dim)\n    x = embedding_layer(x)\n    tb_1 = TransformerBlock(embed_dim, num_heads, ff_dim, rate=0.2)\n    x = tb_1(x)\n    x = layers.GlobalAveragePooling1D()(x)\n    x = layers.Dropout(0.8)(x)\n    outputs = layers.Dense(embed_dim)(x)\n\n    model = keras.Model(inputs=inputs, outputs=outputs)\n\n    lr_schedule = tf.keras.callbacks.LearningRateScheduler( learnRatefunction  ) \n    opt = tf.keras.optimizers.Adam(learning_rate=5.0e-1, epsilon=1)\n    model.compile( optimizer=opt, metrics=[\"mae\"], loss=\"mse\")#, run_eagerly=True)#, keras.losses.Huber(), )\n\n    model.summary()\n    history = model.fit( train_ds, epochs=no_epoches, callbacks=[lr_schedule], verbose=0)\n    \n    print('final loss', history.history['loss'][no_epoches-1])#, 'val_loss:'),  history.history['val_loss'][no_epoches-1])","metadata":{"execution":{"iopub.status.busy":"2022-05-31T20:55:59.718289Z","iopub.execute_input":"2022-05-31T20:55:59.718576Z","iopub.status.idle":"2022-05-31T21:07:05.604107Z","shell.execute_reply.started":"2022-05-31T20:55:59.718544Z","shell.execute_reply":"2022-05-31T21:07:05.603268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission\nBefore we can predict when given with a \"future\" day's market prices, we need to construct the time series input with historical data with the new data.","metadata":{}},{"cell_type":"code","source":"def predict_with_latest_price( model, latest_prices, historical_prices,  code_encoder, sharding_options,ts_window_size ):\n    # take the most recent chuck of historical data, append the current_date prices to form time series samples\n    recent_prices = historical_prices[historical_prices.Date > '2021-10-01']\n    # pad the latest data with dummy target value so that it can be concatenated with the historical data\n    latest_prices['Target'] = 0.0\n    latest_prices[\"SecuritiesCode\"] = code_encoder.fit_transform(latest_prices[[\"SecuritiesCode\"]])\n    recent_prices = pd.concat([recent_prices, latest_prices])\n    ds, _ = prep_time_series_dataset( recent_prices, window_size, 1)\n    pred = model.predict( ds.with_options(sharding_options), batch_size =1 )\n    \n    # pred is of shape (no_of_time_series_windows, no_of_all_stocks), take the last prediction for all stocks\n    return pred[pred.shape[0] -1, :]\n","metadata":{"execution":{"iopub.status.busy":"2022-05-31T21:11:33.555454Z","iopub.execute_input":"2022-05-31T21:11:33.556025Z","iopub.status.idle":"2022-05-31T21:11:33.562581Z","shell.execute_reply.started":"2022-05-31T21:11:33.555986Z","shell.execute_reply":"2022-05-31T21:11:33.561572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nimport jpx_tokyo_market_prediction\nenv = jpx_tokyo_market_prediction.make_env()\niter_test = env.iter_test()\n\nfor (prices, ops, financials, trades, secondary_prices, sample_prediction) in iter_test:\n    sample_prediction = sample_prediction.drop('Rank', axis=1)\n    pred = predict_with_latest_price( model, prices, hist_prices, stock_code_encoder, options, window_size)\n    dfTemp = pd.DataFrame( pred.reshape(-1,1))\n    dfTemp.columns = [ 'Prediction']\n    dfTemp['SecuritiesCode'] = stock_code_encoder.inverse_transform( encoded_stocks_list.reshape(-1,1))\n    dfTemp['Rank'] = dfTemp['Prediction'].rank(ascending=False,method='first') -1\n    dfTemp['Rank'] = dfTemp['Rank'].astype(int)\n    dfTemp = dfTemp.drop('Prediction', axis=1)\n    sample_prediction = sample_prediction.merge(dfTemp, on='SecuritiesCode', how='left')\n \n    env.predict(sample_prediction)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-31T21:11:43.04026Z","iopub.execute_input":"2022-05-31T21:11:43.040537Z","iopub.status.idle":"2022-05-31T21:11:43.075108Z","shell.execute_reply.started":"2022-05-31T21:11:43.040507Z","shell.execute_reply":"2022-05-31T21:11:43.073683Z"},"trusted":true},"execution_count":null,"outputs":[]}]}