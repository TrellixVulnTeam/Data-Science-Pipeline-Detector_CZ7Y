{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import math # Mathematical functions \nimport numpy as np # Fundamental package for scientific computing with Python\nimport pandas as pd # Additional functions for analysing and manipulating data\nfrom datetime import date, timedelta, datetime # Date Functions\nfrom pandas.plotting import register_matplotlib_converters # This function adds plotting functions for calender dates\nimport matplotlib.pyplot as plt # Important package for visualization - we use this to plot the market data\nimport matplotlib.dates as mdates # Formatting dates\nimport tensorflow as tf\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error # Packages for measuring model performance / errors\nfrom tensorflow.keras import Sequential # Deep learning library, used for neural networks\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout # Deep learning classes for recurrent and regular densely-connected layers\nfrom tensorflow.keras.callbacks import EarlyStopping # EarlyStopping during model training\nfrom sklearn.preprocessing import RobustScaler, MinMaxScaler # This Scaler removes the median and scales the data according to the quantile range to normalize the price data \nimport seaborn as sns # Visualization","metadata":{"id":"gj-9_FWoAHRm","execution":{"iopub.status.busy":"2022-06-12T21:33:50.906063Z","iopub.execute_input":"2022-06-12T21:33:50.907081Z","iopub.status.idle":"2022-06-12T21:33:58.155423Z","shell.execute_reply.started":"2022-06-12T21:33:50.907042Z","shell.execute_reply":"2022-06-12T21:33:58.154368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport os\nfrom decimal import ROUND_HALF_UP, Decimal\nimport numpy as np\nimport pandas as pd\nfrom lightgbm import LGBMRegressor\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nplt.style.use(\"fivethirtyeight\")\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM\nimport math\nfrom sklearn.metrics import mean_squared_error\nfrom keras.layers import Dense, LSTM, Dropout\nfrom sklearn.preprocessing import MinMaxScaler","metadata":{"id":"9SlzB1vS9QJq","execution":{"iopub.status.busy":"2022-06-12T21:34:37.729118Z","iopub.execute_input":"2022-06-12T21:34:37.729523Z","iopub.status.idle":"2022-06-12T21:34:37.737845Z","shell.execute_reply.started":"2022-06-12T21:34:37.729491Z","shell.execute_reply":"2022-06-12T21:34:37.736908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Time Series Forecasting - Multivariate Time Series Models for Stock Market Prediction\n# A tutorial for this file is available at www.relataly.com\n\nimport math # Mathematical functions \nimport numpy as np # Fundamental package for scientific computing with Python\nimport pandas as pd # Additional functions for analysing and manipulating data\nfrom datetime import date, timedelta, datetime # Date Functions\nfrom pandas.plotting import register_matplotlib_converters # This function adds plotting functions for calender dates\nimport matplotlib.pyplot as plt # Important package for visualization - we use this to plot the market data\nimport matplotlib.dates as mdates # Formatting dates\nimport tensorflow as tf\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error # Packages for measuring model performance / errors\nfrom tensorflow.keras import Sequential # Deep learning library, used for neural networks\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout # Deep learning classes for recurrent and regular densely-connected layers\nfrom tensorflow.keras.callbacks import EarlyStopping # EarlyStopping during model training\nfrom sklearn.preprocessing import RobustScaler, MinMaxScaler # This Scaler removes the median and scales the data according to the quantile range to normalize the price data \nimport seaborn as sns # Visualization\nsns.set_style('white', { 'axes.spines.right': False, 'axes.spines.top': False})","metadata":{"execution":{"iopub.status.busy":"2022-06-12T21:35:30.8625Z","iopub.execute_input":"2022-06-12T21:35:30.86291Z","iopub.status.idle":"2022-06-12T21:35:30.871935Z","shell.execute_reply.started":"2022-06-12T21:35:30.862874Z","shell.execute_reply":"2022-06-12T21:35:30.870736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def adjust_price(price):\n    \"\"\"\n    Args:\n        price (pd.DataFrame)  : pd.DataFrame include stock_price\n    Returns:\n        price DataFrame (pd.DataFrame): stock_price with generated AdjustedClose\n    \"\"\"\n    # transform Date column into datetime\n    price.loc[: ,\"Date\"] = pd.to_datetime(price.loc[: ,\"Date\"], format=\"%Y-%m-%d\")\n\n    def generate_adjusted_close(df):\n        \"\"\"\n        Args:\n            df (pd.DataFrame)  : stock_price for a single SecuritiesCode\n        Returns:\n            df (pd.DataFrame): stock_price with AdjustedClose for a single SecuritiesCode\n        \"\"\"\n        # sort data to generate CumulativeAdjustmentFactor\n        df = df.sort_values(\"Date\", ascending=False)\n        # generate CumulativeAdjustmentFactor\n        df.loc[:, \"CumulativeAdjustmentFactor\"] = df[\"AdjustmentFactor\"].cumprod()\n        # generate AdjustedClose\n        df.loc[:, \"AdjustedClose\"] = (\n            df[\"CumulativeAdjustmentFactor\"] * df[\"Close\"]\n        ).map(lambda x: float(\n            Decimal(str(x)).quantize(Decimal('0.1'), rounding=ROUND_HALF_UP)\n        ))\n        # reverse order\n        df = df.sort_values(\"Date\")\n        # to fill AdjustedClose, replace 0 into np.nan\n        df.loc[df[\"AdjustedClose\"] == 0, \"AdjustedClose\"] = np.nan\n        # forward fill AdjustedClose\n        df.loc[:, \"AdjustedClose\"] = df.loc[:, \"AdjustedClose\"].ffill()\n        return df\n\n    # generate AdjustedClose\n    price = price.sort_values([\"SecuritiesCode\", \"Date\"])\n    price = price.groupby(\"SecuritiesCode\").apply(generate_adjusted_close).reset_index(drop=True)\n\n    price.set_index(\"Date\", inplace=True)\n    return price","metadata":{"id":"B86sC46B7jGY","execution":{"iopub.status.busy":"2022-06-12T21:33:19.244083Z","iopub.execute_input":"2022-06-12T21:33:19.245026Z","iopub.status.idle":"2022-06-12T21:33:19.27756Z","shell.execute_reply.started":"2022-06-12T21:33:19.244934Z","shell.execute_reply":"2022-06-12T21:33:19.27686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/jpx-tokyo-stock-exchange-prediction/train_files/stock_prices.csv\")\ndf = adjust_price(df)\ndf_supp =  pd.read_csv(\"/kaggle/input/jpx-tokyo-stock-exchange-prediction/supplemental_files/stock_prices.csv\")\ndf_supp = adjust_price(df_supp)\n\nSclist = df.SecuritiesCode.unique()","metadata":{"id":"DIGo3xNx8NL2","execution":{"iopub.status.busy":"2022-06-12T21:34:41.226865Z","iopub.execute_input":"2022-06-12T21:34:41.227525Z","iopub.status.idle":"2022-06-12T21:35:18.633241Z","shell.execute_reply.started":"2022-06-12T21:34:41.227486Z","shell.execute_reply":"2022-06-12T21:35:18.63218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"col = \"AdjustedClose\"\ndf = df.loc[df[\"SecuritiesCode\"] == Sclist[1]]\ndf_supp = df_supp.loc[df_supp[\"SecuritiesCode\"] == Sclist[1]]","metadata":{"id":"XgQjWmE-8gHi","execution":{"iopub.status.busy":"2022-06-12T21:35:39.605344Z","iopub.execute_input":"2022-06-12T21:35:39.60627Z","iopub.status.idle":"2022-06-12T21:35:39.616858Z","shell.execute_reply.started":"2022-06-12T21:35:39.606229Z","shell.execute_reply":"2022-06-12T21:35:39.616177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.fillna(method='ffill')\ndf = df.fillna(method='bfill')\ndf_supp = df_supp.fillna(method='ffill')\ndf_supp = df_supp.fillna(method='bfill')","metadata":{"id":"3F63G3tiDSoX","execution":{"iopub.status.busy":"2022-06-12T21:35:41.124698Z","iopub.execute_input":"2022-06-12T21:35:41.125403Z","iopub.status.idle":"2022-06-12T21:35:41.134141Z","shell.execute_reply.started":"2022-06-12T21:35:41.125361Z","shell.execute_reply":"2022-06-12T21:35:41.133023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"col = \"Close\"","metadata":{"id":"zx_SGllRGl8v","execution":{"iopub.status.busy":"2022-06-12T21:44:16.540755Z","iopub.execute_input":"2022-06-12T21:44:16.541164Z","iopub.status.idle":"2022-06-12T21:44:16.545548Z","shell.execute_reply.started":"2022-06-12T21:44:16.541131Z","shell.execute_reply":"2022-06-12T21:44:16.544487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df[[\"Open\",\"High\",\"Low\",\"Close\",\"AdjustedClose\",\"Volume\"]]\ndf_supp = df_supp[[\"Open\",\"High\",\"Low\",\"Close\",\"AdjustedClose\",\"Volume\"]]","metadata":{"id":"2g5Z5QRWF2n9","execution":{"iopub.status.busy":"2022-06-12T21:44:16.591083Z","iopub.execute_input":"2022-06-12T21:44:16.591494Z","iopub.status.idle":"2022-06-12T21:44:16.599591Z","shell.execute_reply.started":"2022-06-12T21:44:16.591459Z","shell.execute_reply":"2022-06-12T21:44:16.598838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot line charts\ndf_plot = df.copy()\n\nncols = 2\nnrows = int(round(df_plot.shape[1] / ncols, 0))\n\nfig, ax = plt.subplots(nrows=nrows, ncols=ncols, sharex=True, figsize=(14, 7))\nfor i, ax in enumerate(fig.axes):\n        sns.lineplot(data = df_plot.iloc[:, i], ax=ax)\n        ax.tick_params(axis=\"x\", rotation=30, labelsize=10, length=0)\n        ax.xaxis.set_major_locator(mdates.AutoDateLocator())\nfig.tight_layout()\nplt.show()","metadata":{"id":"iKdCZEU9Frul","outputId":"74ecc8ed-ef4a-4d9f-b70a-ac32d9be1cad","execution":{"iopub.status.busy":"2022-06-12T21:44:16.653315Z","iopub.execute_input":"2022-06-12T21:44:16.654141Z","iopub.status.idle":"2022-06-12T21:44:18.403527Z","shell.execute_reply.started":"2022-06-12T21:44:16.654088Z","shell.execute_reply":"2022-06-12T21:44:18.40259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Indexing Batches\ntrain_df = df.sort_values(by=['Date']).copy()\n\n# List of considered Features\nFEATURES = ['High', 'Low', 'Open', 'Close', 'Volume',\n            #, 'Month', 'Year', 'Adj Close'\n           ]\n\nprint('FEATURE LIST')\nprint([f for f in FEATURES])\n\n# Create the dataset with features and filter the data to the list of FEATURES\ndata = pd.DataFrame(train_df)\ndata_filtered = data[FEATURES]\n\n# We add a prediction column and set dummy values to prepare the data for scaling\ndata_filtered_ext = data_filtered.copy()\ndata_filtered_ext['Prediction'] = data_filtered_ext[col]\n\n# Print the tail of the dataframe\ndata_filtered_ext.tail()","metadata":{"id":"OQYo01cQG2Sg","outputId":"4d4a76ea-aa5f-484d-fe45-b1c0f424c77c","execution":{"iopub.status.busy":"2022-06-12T21:44:18.405087Z","iopub.execute_input":"2022-06-12T21:44:18.405427Z","iopub.status.idle":"2022-06-12T21:44:18.429783Z","shell.execute_reply.started":"2022-06-12T21:44:18.405396Z","shell.execute_reply":"2022-06-12T21:44:18.428682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the number of rows in the data\nnrows = data_filtered.shape[0]\n\n# Convert the data to numpy values\nnp_data_unscaled = np.array(data_filtered)\nnp_data = np.reshape(np_data_unscaled, (nrows, -1))\nprint(np_data.shape)\n\n# Transform the data by scaling each feature to a range between 0 and 1\nscaler = MinMaxScaler()\nnp_data_scaled = scaler.fit_transform(np_data_unscaled)\n\n# Creating a separate scaler that works on a single column for scaling predictions\nscaler_pred = MinMaxScaler()\ndf_Close = pd.DataFrame(data_filtered_ext[col])\nnp_Close_scaled = scaler_pred.fit_transform(df_Close)","metadata":{"id":"gOa5FqDCG4eQ","outputId":"66b1d9e1-375c-4e3d-c967-34f9def06059","execution":{"iopub.status.busy":"2022-06-12T21:44:18.431345Z","iopub.execute_input":"2022-06-12T21:44:18.431809Z","iopub.status.idle":"2022-06-12T21:44:18.446328Z","shell.execute_reply.started":"2022-06-12T21:44:18.431764Z","shell.execute_reply":"2022-06-12T21:44:18.445284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set the sequence length - this is the timeframe used to make a single prediction\nsequence_length = 50\n\n# Prediction Index\nindex_Close = data.columns.get_loc(col)\n\n# Split the training data into train and train data sets\n# As a first step, we get the number of rows to train the model on 80% of the data \ntrain_data_len = math.ceil(np_data_scaled.shape[0] * 0.8)\n\n# Create the training and test data\ntrain_data = np_data_scaled[0:train_data_len, :]\ntest_data = np_data_scaled[train_data_len - sequence_length:, :]\n\n# The RNN needs data with the format of [samples, time steps, features]\n# Here, we create N samples, sequence_length time steps per sample, and 6 features\ndef partition_dataset(sequence_length, data):\n    x, y = [], []\n    data_len = data.shape[0]\n    for i in range(sequence_length, data_len):\n        x.append(data[i-sequence_length:i,:]) #contains sequence_length values 0-sequence_length * columsn\n        y.append(data[i, index_Close]) #contains the prediction values for validation,  for single-step prediction\n    \n    # Convert the x and y to numpy arrays\n    x = np.array(x)\n    y = np.array(y)\n    return x, y\n\n# Generate training data and test data\nx_train, y_train = partition_dataset(sequence_length, train_data)\nx_test, y_test = partition_dataset(sequence_length, test_data)\n\n# Print the shapes: the result is: (rows, training_sequence, features) (prediction value, )\nprint(x_train.shape, y_train.shape)\nprint(x_test.shape, y_test.shape)\n\n# Validate that the prediction value and the input match up\n# The last close price of the second input sample should equal the first prediction value\nprint(x_train[1][sequence_length-1][index_Close])\nprint(y_train[0])","metadata":{"id":"or1zeGLKG6oQ","outputId":"c6cccdb5-8d41-4894-b2e4-e26806850a0f","execution":{"iopub.status.busy":"2022-06-12T21:44:18.448913Z","iopub.execute_input":"2022-06-12T21:44:18.449274Z","iopub.status.idle":"2022-06-12T21:44:18.466399Z","shell.execute_reply.started":"2022-06-12T21:44:18.449242Z","shell.execute_reply":"2022-06-12T21:44:18.465208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Configure the neural network model\nmodel = Sequential()\n\n# Model with n_neurons = inputshape Timestamps, each with x_train.shape[2] variables\nn_neurons = x_train.shape[1] * x_train.shape[2]\nprint(n_neurons, x_train.shape[1], x_train.shape[2])\nmodel.add(LSTM(n_neurons, return_sequences=True, input_shape=(x_train.shape[1], x_train.shape[2]))) \nmodel.add(LSTM(n_neurons, return_sequences=False))\nmodel.add(Dense(5))\nmodel.add(Dense(1))\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='mse')","metadata":{"id":"c3pqQJNqG8m4","outputId":"13202099-5886-4320-f2f2-5af5044a828d","execution":{"iopub.status.busy":"2022-06-12T21:44:18.467563Z","iopub.execute_input":"2022-06-12T21:44:18.468417Z","iopub.status.idle":"2022-06-12T21:44:18.979431Z","shell.execute_reply.started":"2022-06-12T21:44:18.468378Z","shell.execute_reply":"2022-06-12T21:44:18.978405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training the model\nepochs = 6\nbatch_size = 16\nearly_stop = EarlyStopping(monitor='loss', patience=5, verbose=1)\nhistory = model.fit(x_train, y_train, \n                    batch_size=batch_size, \n                    epochs=epochs,\n                    validation_data=(x_test, y_test)\n                   )\n                    \n                    #callbacks=[early_stop])","metadata":{"id":"K8-79Kg4G-_a","outputId":"453f8cbf-0128-44f5-b1e0-44d27e571eb9","execution":{"iopub.status.busy":"2022-06-12T21:44:18.981082Z","iopub.execute_input":"2022-06-12T21:44:18.981417Z","iopub.status.idle":"2022-06-12T21:45:09.814799Z","shell.execute_reply.started":"2022-06-12T21:44:18.981386Z","shell.execute_reply":"2022-06-12T21:45:09.813856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot training & validation loss values\nfig, ax = plt.subplots(figsize=(16, 5), sharex=True)\nsns.lineplot(data=history.history[\"loss\"])\nplt.title(\"Model loss\")\nplt.ylabel(\"Loss\")\nplt.xlabel(\"Epoch\")\nax.xaxis.set_major_locator(plt.MaxNLocator(epochs))\nplt.legend([\"Train\", \"Test\"], loc=\"upper left\")\nplt.grid()\nplt.show()","metadata":{"id":"GBax5ZRLHDHk","outputId":"703a4eb5-89d1-4fdc-a209-b1ddc5bd2942","execution":{"iopub.status.busy":"2022-06-12T21:45:09.816267Z","iopub.execute_input":"2022-06-12T21:45:09.816602Z","iopub.status.idle":"2022-06-12T21:45:10.084844Z","shell.execute_reply.started":"2022-06-12T21:45:09.816571Z","shell.execute_reply":"2022-06-12T21:45:10.084142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the predicted values\ny_pred_scaled = model.predict(x_test)\n\n# Unscale the predicted values\ny_pred = scaler_pred.inverse_transform(y_pred_scaled)\ny_test_unscaled = scaler_pred.inverse_transform(y_test.reshape(-1, 1))\n\n# Mean Absolute Error (MAE)\nMAE = mean_absolute_error(y_test_unscaled, y_pred)\nprint(f'Median Absolute Error (MAE): {np.round(MAE, 2)}')\n\n# Mean Absolute Percentage Error (MAPE)\nMAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\nprint(f'Mean Absolute Percentage Error (MAPE): {np.round(MAPE, 2)} %')\n\n# Median Absolute Percentage Error (MDAPE)\nMDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled)) ) * 100\nprint(f'Median Absolute Percentage Error (MDAPE): {np.round(MDAPE, 2)} %')","metadata":{"id":"rZlJtEjHHFDf","outputId":"2ff46c94-93ac-4208-b98e-bb317bf818da","execution":{"iopub.status.busy":"2022-06-12T21:45:10.086054Z","iopub.execute_input":"2022-06-12T21:45:10.086533Z","iopub.status.idle":"2022-06-12T21:45:11.342946Z","shell.execute_reply.started":"2022-06-12T21:45:10.0865Z","shell.execute_reply":"2022-06-12T21:45:11.341915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The date from which on the date is displayed\ndisplay_start_date = \"2020-6-01\" \n\n# Add the difference between the valid and predicted prices\ntrain = pd.DataFrame(data_filtered_ext[col][:train_data_len + 1]).rename(columns={col: 'train'})\nvalid = pd.DataFrame(data_filtered_ext[col][train_data_len:]).rename(columns={col: 'valid'})\nvalid.insert(1, \"predict\", y_pred, True)\nvalid.insert(1, \"residuals\", valid[\"predict\"] - valid[\"valid\"], True)\ndf_union = pd.concat([train, valid])\n\n# Zoom in to a closer timeframe\ndf_union_zoom = df_union[df_union.index > display_start_date]\n\n# Create the lineplot\nplt.style.use(\"fivethirtyeight\")\nfig, ax1 = plt.subplots(figsize=(16, 8))\nplt.title(\"predict vs valid\")\nplt.ylabel(1301, fontsize=18)\nsns.set_palette([\"#090364\", \"#1960EF\", \"#EF5919\"])\nsns.lineplot(data=df_union_zoom[['predict', 'train', 'valid']], linewidth=3.0, dashes=False, ax=ax1)\n\n","metadata":{"id":"NTAQLl2UJOOe","outputId":"0adc3654-78d5-4494-81e0-6b6b94f13843","execution":{"iopub.status.busy":"2022-06-12T21:50:19.954796Z","iopub.execute_input":"2022-06-12T21:50:19.955687Z","iopub.status.idle":"2022-06-12T21:50:20.499442Z","shell.execute_reply.started":"2022-06-12T21:50:19.955627Z","shell.execute_reply":"2022-06-12T21:50:20.498758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"XCT02zUKJ6eW"},"execution_count":null,"outputs":[]}]}