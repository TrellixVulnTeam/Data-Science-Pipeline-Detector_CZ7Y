{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Topological Data Analysis (TDA) for feature extraction with Catboost for training and submission\nTopologicial Data Analysis (TDA) uses techniques from topology to analyse datasets. This notebook demonstrates how to perform TDA feature extraction with <cite>[giotto-tda: A Topological Data Analysis Toolkit for Machine Learning and Data Exploration, Tauzin et al, arXiv:2004.02551, 2020.](https://arxiv.org/abs/2004.02551)</cite>\n\n- Several non-TDA features were selected heuristically and additional features were then extracted using TDA.\n- A Catboost model is used for training and submission.\n- The training code is based on [smeitoma](https://www.kaggle.com/smeitoma)'s [https://www.kaggle.com/code/smeitoma/train-demo](https://www.kaggle.com/code/smeitoma/train-demo) and the submission code is based on [https://www.kaggle.com/code/smeitoma/submission-demo](https://www.kaggle.com/code/smeitoma/submission-demo).\n- All files including required python packages are provided in the [https://www.kaggle.com/datasets/aemulcahy/jpx-dataset-001](https://www.kaggle.com/datasets/aemulcahy/jpx-dataset-001) dataset.\n","metadata":{"papermill":{"duration":0.017854,"end_time":"2022-06-17T05:18:23.423557","exception":false,"start_time":"2022-06-17T05:18:23.405703","status":"completed"},"pycharm":{"name":"#%% md\n"},"tags":[]}},{"cell_type":"code","source":"from decimal import ROUND_HALF_UP, Decimal\nimport importlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\nimport warnings\n\nfrom catboost import CatBoostRegressor\nfrom tqdm import tqdm\n\nif os.environ.get('KAGGLE_KERNEL_RUN_TYPE', 'Localhost') != 'Localhost':\n    !pip install ../input/jpx-dataset-001/pyflagser-0.4.4-cp37-cp37m-manylinux2010_x86_64.whl\n    !pip install ../input/jpx-dataset-001/giotto_tda-0.5.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl\nelse:\n    giotto_tda_spec = importlib.util.find_spec('gtda')\n    if giotto_tda_spec is None:\n        !pip install giotto-tda\n\nfrom gtda.diagrams import Amplitude\nfrom gtda.homology import VietorisRipsPersistence\nfrom gtda.pipeline import Pipeline\nfrom gtda.time_series import SlidingWindow, TakensEmbedding\n","metadata":{"execution":{"iopub.execute_input":"2022-06-17T05:18:23.496153Z","iopub.status.busy":"2022-06-17T05:18:23.490339Z","iopub.status.idle":"2022-06-17T05:19:24.308799Z","shell.execute_reply":"2022-06-17T05:19:24.308166Z","shell.execute_reply.started":"2022-04-05T06:07:42.848032Z"},"papermill":{"duration":60.866914,"end_time":"2022-06-17T05:19:24.308975","exception":false,"start_time":"2022-06-17T05:18:23.442061","status":"completed"},"pycharm":{"name":"#%%\n"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    TRAIN = True\n    SUBMIT = True\n    WINDOW_SIZE = 5\n    ZWINDOW_SIZE = 200\n    STRIDE = 1\n    TIME_DELAY = 1\n    DIMENSION = 3\n    LOCALHOST = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', 'Localhost') == 'Localhost'\n    TDA_DATA_PATH = 'Xy_v2l.bz2' if LOCALHOST else '../input/jpx-dataset-001/Xy_v2l.bz2'\n    # MODEL_PATH = 'model.cbm' if LOCALHOST else '../input/jpx-dataset-001/model.cbm'\n    MODEL_PATH = 'model_all.cbm' if LOCALHOST else '../input/jpx-dataset-001/model_all.cbm'\n","metadata":{"collapsed":false,"execution":{"iopub.execute_input":"2022-06-17T05:19:24.362932Z","iopub.status.busy":"2022-06-17T05:19:24.36201Z","iopub.status.idle":"2022-06-17T05:19:24.363734Z","shell.execute_reply":"2022-06-17T05:19:24.36423Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.032268,"end_time":"2022-06-17T05:19:24.364375","exception":false,"start_time":"2022-06-17T05:19:24.332107","status":"completed"},"pycharm":{"name":"#%%\n"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_dir = \"../input/jpx-tokyo-stock-exchange-prediction\"\ntrain_files_dir = f\"{base_dir}/train_files\"","metadata":{"execution":{"iopub.execute_input":"2022-06-17T05:19:24.418241Z","iopub.status.busy":"2022-06-17T05:19:24.417369Z","iopub.status.idle":"2022-06-17T05:19:24.419236Z","shell.execute_reply":"2022-06-17T05:19:24.419703Z","shell.execute_reply.started":"2022-04-05T06:07:44.942997Z"},"papermill":{"duration":0.031004,"end_time":"2022-06-17T05:19:24.419845","exception":false,"start_time":"2022-06-17T05:19:24.388841","status":"completed"},"pycharm":{"name":"#%%\n"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef adjust_price_v2(price):\n    # transform Date column into datetime\n    price.loc[: ,\"Date\"] = pd.to_datetime(price.loc[: ,\"Date\"], format=\"%Y-%m-%d\")\n\n    def generate_adjusted_price(df, orig_str):\n        adjusted_str = \"Adjusted\"+orig_str\n        # sort data to generate CumulativeAdjustmentFactor\n        df = df.sort_values(\"Date\", ascending=False)\n        # generate CumulativeAdjustmentFactor\n        df.loc[:, \"CumulativeAdjustmentFactor\"] = df[\"AdjustmentFactor\"].cumprod()\n        # generate Adjusted%str%\n        df.loc[:, adjusted_str] = (\n                df[\"CumulativeAdjustmentFactor\"] * df[orig_str]\n        ).map(lambda x: float(\n            Decimal(str(x)).quantize(Decimal('0.1'), rounding=ROUND_HALF_UP)\n        ))\n        # reverse order\n        df = df.sort_values(\"Date\")\n        # to fill AdjustedClose, replace 0 into np.nan\n        df.loc[df[adjusted_str] == 0, adjusted_str] = np.nan\n        # forward fill AdjustedClose\n        df.loc[:, adjusted_str] = df.loc[:, adjusted_str].ffill()\n        return df\n\n    def generate_adjusted_volume(df):\n        orig_str = \"Volume\"\n        adjusted_str = \"Adjusted\"+orig_str\n        # sort data to generate CumulativeAdjustmentFactor\n        df = df.sort_values(\"Date\", ascending=False)\n        # generate CumulativeAdjustmentFactor\n        df.loc[:, \"CumulativeAdjustmentFactor\"] = df[\"AdjustmentFactor\"].cumprod()\n        # generate Adjusted%str%\n        df.loc[:, adjusted_str] = (\n            df[orig_str] / df[\"CumulativeAdjustmentFactor\"]\n        ).map(lambda x: float(\n            Decimal(str(x)).quantize(Decimal('0.1'), rounding=ROUND_HALF_UP)\n        ))\n        # reverse order\n        df = df.sort_values(\"Date\")\n        # to fill AdjustedClose, replace 0 into np.nan\n        df.loc[df[adjusted_str] == 0, adjusted_str] = np.nan\n        # forward fill AdjustedClose\n        df.loc[:, adjusted_str] = df.loc[:, adjusted_str].ffill()\n        return df\n\n    generate_adjusted_close = lambda df: generate_adjusted_price(df, 'Close')\n    generate_adjusted_open = lambda df: generate_adjusted_price(df, 'Open')\n\n    # generate AdjustedClose\n    price = price.sort_values([\"SecuritiesCode\", \"Date\"])\n    price = price.groupby(\"SecuritiesCode\").apply(generate_adjusted_close).reset_index(drop=True)\n    price = price.groupby(\"SecuritiesCode\").apply(generate_adjusted_open).reset_index(drop=True)\n    price = price.groupby(\"SecuritiesCode\").apply(generate_adjusted_volume).reset_index(drop=True)\n\n    price.set_index(\"Date\", inplace=True)\n    return price\n","metadata":{"execution":{"iopub.execute_input":"2022-06-17T05:19:24.536646Z","iopub.status.busy":"2022-06-17T05:19:24.516609Z","iopub.status.idle":"2022-06-17T05:19:24.538928Z","shell.execute_reply":"2022-06-17T05:19:24.538442Z","shell.execute_reply.started":"2022-04-05T06:07:59.844692Z"},"papermill":{"duration":0.050448,"end_time":"2022-06-17T05:19:24.539043","exception":false,"start_time":"2022-06-17T05:19:24.488595","status":"completed"},"pycharm":{"name":"#%%\n"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load stock price data\ndf_price = pd.read_csv(f\"{train_files_dir}/stock_prices.csv\")\ndf_price = adjust_price_v2(df_price)\n\ncodes = sorted(df_price[\"SecuritiesCode\"].unique())\ndisplay('len(codes)', len(codes))\ndisplay(df_price)\n","metadata":{"collapsed":false,"execution":{"iopub.execute_input":"2022-06-17T05:19:24.591164Z","iopub.status.busy":"2022-06-17T05:19:24.590628Z","iopub.status.idle":"2022-06-17T05:22:21.218068Z","shell.execute_reply":"2022-06-17T05:22:21.217347Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":176.65565,"end_time":"2022-06-17T05:22:21.218241","exception":false,"start_time":"2022-06-17T05:19:24.562591","status":"completed"},"pycharm":{"name":"#%%\n"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_rank(df):\n    \"\"\"\n    Args:\n        df (pd.DataFrame): including predict column\n    Returns:\n        df (pd.DataFrame): df with Rank\n    \"\"\"\n    # sort records to set Rank\n    df = df.sort_values(\"predict\", ascending=False)\n    # set Rank starting from 0\n    df.loc[:, \"Rank\"] = np.arange(len(df[\"predict\"]))\n    return df\n","metadata":{"collapsed":false,"execution":{"iopub.execute_input":"2022-06-17T05:22:21.356939Z","iopub.status.busy":"2022-06-17T05:22:21.339497Z","iopub.status.idle":"2022-06-17T05:22:21.363911Z","shell.execute_reply":"2022-06-17T05:22:21.363078Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.056558,"end_time":"2022-06-17T05:22:21.36405","exception":false,"start_time":"2022-06-17T05:22:21.307492","status":"completed"},"pycharm":{"name":"#%%\n"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def _calc_spread_return_per_day(df, portfolio_size, toprank_weight_ratio):\n    \"\"\"\n    Args:\n        df (pd.DataFrame): predicted results\n        portfolio_size (int): # of equities to buy/sell\n        toprank_weight_ratio (float): the relative weight of the most highly ranked stock compared to the least.\n    Returns:\n        (float): spread return\n    \"\"\"\n    assert df['Rank'].min() == 0\n    assert df['Rank'].max() == len(df['Rank']) - 1\n    weights = np.linspace(start=toprank_weight_ratio, stop=1, num=portfolio_size)\n    purchase = (df.sort_values(by='Rank')['Target'][:portfolio_size] * weights).sum() / weights.mean()\n    short = (df.sort_values(by='Rank', ascending=False)['Target'][:portfolio_size] * weights).sum() / weights.mean()\n    return purchase - short\n\ndef calc_spread_return_sharpe(df: pd.DataFrame, portfolio_size: int = 200, toprank_weight_ratio: float = 2) -> float:\n    \"\"\"\n    Args:\n        df (pd.DataFrame): predicted results\n        portfolio_size (int): # of equities to buy/sell\n        toprank_weight_ratio (float): the relative weight of the most highly ranked stock compared to the least.\n    Returns:\n        (float): sharpe ratio\n    \"\"\"\n\n    buf = df.groupby('Date').apply(_calc_spread_return_per_day, portfolio_size, toprank_weight_ratio)\n    sharpe_ratio = buf.mean() / buf.std()\n    return sharpe_ratio\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nXy_buff = []\npipe = Pipeline([\n    ('SW', SlidingWindow(size=CFG.WINDOW_SIZE, stride=CFG.STRIDE)),\n    ('TE', TakensEmbedding(time_delay=CFG.TIME_DELAY, dimension=CFG.DIMENSION)),\n    ('VR', VietorisRipsPersistence(collapse_edges=True, n_jobs=1, homology_dimensions=[0, 1])),\n    ('Ampl', Amplitude()),\n])\n\ndef get_features_for_predict_v2e(price, code):\n    _Xy = price.loc[price[\"SecuritiesCode\"] == code].copy()\n\n    _Xy['f_1'] = np.log1p(_Xy['AdjustedVolume'].pct_change())\n    _Xy['f_3'] = _Xy['AdjustedVolume'].pct_change()\n    _Xy_mean = _Xy['AdjustedClose'].rolling(window=CFG.ZWINDOW_SIZE).mean()\n    _Xy_std = _Xy['AdjustedClose'].rolling(window=CFG.ZWINDOW_SIZE).std()\n    _Xy['zscore'] = (_Xy['AdjustedClose'] - _Xy_mean)/_Xy_std\n\n    _Xy = _Xy.fillna(0)\n    _Xy = _Xy.replace([np.inf, -np.inf], 0)\n\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n\n        X_adjvol = pipe.fit_transform(_Xy['AdjustedVolume'])\n        X_a_zscore = pipe.fit_transform(_Xy['zscore'])\n        X_a_f_1 = pipe.fit_transform(_Xy['f_1'])\n        _Xy = _Xy.tail(X_adjvol.shape[0])\n\n        _Xy['tda_0adjvol'] = X_adjvol[:, 0].tolist()\n        _Xy['tda_1adjvol'] = X_adjvol[:, 1].tolist()\n        _Xy['tda_0_f_1'] = X_a_f_1[:, 0].tolist()\n        _Xy['tda_1_f_1'] = X_a_f_1[:, 1].tolist()\n        _Xy['tda_0_zscore'] = X_a_zscore[:, 0].tolist()\n        _Xy['tda_1_zscore'] = X_a_zscore[:, 1].tolist()\n\n    # filling data for nan and inf\n    _Xy = _Xy.fillna(0)\n    _Xy = _Xy.replace([np.inf, -np.inf], 0)\n\n    return _Xy\n\n# if True:\nif not os.path.isfile(CFG.TDA_DATA_PATH):\n    for code in tqdm(codes):\n        _Xy = get_features_for_predict_v2e(df_price, code)\n        Xy_buff.append(_Xy)\n    Xy = pd.concat(Xy_buff)\n    if CFG.LOCALHOST:\n        Xy.to_pickle(CFG.TDA_DATA_PATH)\nelse:\n    Xy = pd.read_pickle(CFG.TDA_DATA_PATH)\n\ndisplay(Xy)\n","metadata":{"collapsed":false,"execution":{"iopub.execute_input":"2022-06-17T05:22:21.437093Z","iopub.status.busy":"2022-06-17T05:22:21.436172Z","iopub.status.idle":"2022-06-17T05:22:26.77252Z","shell.execute_reply":"2022-06-17T05:22:26.773522Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":5.384256,"end_time":"2022-06-17T05:22:26.773732","exception":false,"start_time":"2022-06-17T05:22:21.389476","status":"completed"},"pycharm":{"name":"#%%\n"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_model = CatBoostRegressor(random_seed=42,\n                               task_type='GPU',\n                               loss_function='RMSE',\n                               eval_metric='RMSE',\n                               n_estimators=900,\n                               verbose=False)\n\nfeat_cols_v2 = [\n    'AdjustedClose',\n    'AdjustedOpen',\n    'AdjustedVolume',\n    'f_1',\n    'f_3',\n    'zscore',\n    'tda_0adjvol',\n    'tda_1adjvol',\n    'tda_0_f_1',\n    'tda_1_f_1',\n    'tda_0_zscore',\n    'tda_1_zscore',\n]\n\n# split data into TRAIN and TEST\nTRAIN_END = \"2019-12-31\"\n# We put a week gap between TRAIN_END and TEST_START\n# to avoid leakage of test data information from label\nTEST_START = \"2020-01-06\"\n\ntrain_X = Xy[: TRAIN_END][['SecuritiesCode']+feat_cols_v2]\ntest_X = Xy[TEST_START:][['SecuritiesCode']+feat_cols_v2]\ntrain_full_X = Xy[['SecuritiesCode']+feat_cols_v2]\ntrain_y = Xy[: TRAIN_END]['Target']\ntest_y = Xy[TEST_START:]['Target']\ntrain_full_y = Xy['Target']\n","metadata":{"collapsed":false,"execution":{"iopub.execute_input":"2022-06-17T05:22:26.869352Z","iopub.status.busy":"2022-06-17T05:22:26.868535Z","iopub.status.idle":"2022-06-17T05:24:12.780393Z","shell.execute_reply":"2022-06-17T05:24:12.780823Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":105.964649,"end_time":"2022-06-17T05:24:12.780975","exception":false,"start_time":"2022-06-17T05:22:26.816326","status":"completed"},"pycharm":{"name":"#%%\n"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\npred_model.fit(train_X[feat_cols_v2], train_y, early_stopping_rounds=10)\n# if CFG.LOCALHOST:\n#     pred_model.save_model('model.cbm', format=\"cbm\",)\n\nresult = test_X[[\"SecuritiesCode\"]].copy()\nresult.loc[:, \"predict\"] = pred_model.predict(test_X[feat_cols_v2])\nresult.loc[:, \"Target\"] = test_y.values\n\nresult = result.sort_values([\"Date\", \"predict\"], ascending=[True, False])\nresult = result.groupby(\"Date\").apply(set_rank)\n\n# display(calc_spread_return_sharpe_v2(result, portfolio_size=200))\n# df_result = result.groupby('Date').apply(_calc_spread_return_per_day, 200, 2)\ndisplay(calc_spread_return_sharpe(result, portfolio_size=200))\ndf_result = result.groupby('Date').apply(_calc_spread_return_per_day, 200, 2)\n# 0.15730880249765788\n","metadata":{"collapsed":false,"execution":{"iopub.execute_input":"2022-06-17T05:24:12.855281Z","iopub.status.busy":"2022-06-17T05:24:12.847899Z","iopub.status.idle":"2022-06-17T05:24:36.614508Z","shell.execute_reply":"2022-06-17T05:24:36.614916Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":23.804079,"end_time":"2022-06-17T05:24:36.615073","exception":false,"start_time":"2022-06-17T05:24:12.810994","status":"completed"},"pycharm":{"name":"#%%\n"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(df_result.plot(figsize=(20, 8)))\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(df_result.cumsum().plot(figsize=(20, 8)))\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_importance = pred_model.feature_importances_\nsorted_idx = np.argsort(feature_importance)\nfig = plt.figure(figsize=(12, 6))\nplt.barh(range(len(sorted_idx)), feature_importance[sorted_idx], align='center')\nplt.yticks(range(len(sorted_idx)), np.array(feat_cols_v2)[sorted_idx])\nplt.title('Feature Importance')","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from catboost import Pool, cv\n\ncv_dataset = Pool(data=Xy[feat_cols_v2],\n                  label=Xy['Target'])\n\nparams = {'random_seed': 42,\n          # 'task_type': 'GPU',\n          'loss_function': 'RMSE',\n          'eval_metric': 'RMSE',\n          'n_estimators': 900,\n          'verbose': False}\n\nscores = cv(cv_dataset,\n            params,\n            fold_count=5,\n             type='TimeSeries',\n            plot=\"True\")","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\npred_model.fit(Xy[feat_cols_v2], Xy['Target'], early_stopping_rounds=10)\nif CFG.LOCALHOST:\n    pred_model.save_model('model_all.cbm', format=\"cbm\",)\n\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_model = CatBoostRegressor()\npred_model.load_model(CFG.MODEL_PATH, format=\"cbm\",)\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if CFG.SUBMIT:\n    # load Time Series API\n    import jpx_tokyo_market_prediction\n    # make Time Series API environment (this function can be called only once in a session)\n    env = jpx_tokyo_market_prediction.make_env()\n    # get iterator to fetch data day by day\n    iter_test = env.iter_test()\n","metadata":{"collapsed":false,"execution":{"iopub.execute_input":"2022-06-17T05:24:36.695172Z","iopub.status.busy":"2022-06-17T05:24:36.694239Z","iopub.status.idle":"2022-06-17T05:24:36.718681Z","shell.execute_reply":"2022-06-17T05:24:36.718147Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.065821,"end_time":"2022-06-17T05:24:36.718814","exception":false,"start_time":"2022-06-17T05:24:36.652993","status":"completed"},"pycharm":{"name":"#%%\n"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if CFG.SUBMIT:\n    df_price_raw = pd.read_csv(f\"{train_files_dir}/stock_prices.csv\") #TODO\n    price_cols = [\n        \"Date\",\n        \"SecuritiesCode\",\n        \"Close\",\n        \"Open\",\n        \"Volume\",\n        \"AdjustmentFactor\",\n    ]\n    df_price_raw = df_price_raw[price_cols]\n\n    # filter data to reduce culculation cost\n    # df_price_raw = df_price_raw.loc[df_price_raw[\"Date\"] >= \"2021-08-01\"]\n    counter = 0\n    # fetch data day by day\n    for (prices, options, financials, trades, secondary_prices, sample_prediction) in iter_test:\n        current_date = prices[\"Date\"].iloc[0]\n        sample_prediction_date = sample_prediction[\"Date\"].iloc[0]\n        print(f\"current_date: {current_date}, sample_prediction_date: {sample_prediction_date}\")\n\n        if counter == 0:\n            # to avoid data leakage\n            df_price_raw = df_price_raw.loc[df_price_raw[\"Date\"] < current_date]\n\n        # filter data to reduce culculation cost\n        threshold = (pd.Timestamp(current_date) - pd.offsets.BDay(CFG.ZWINDOW_SIZE)).strftime(\"%Y-%m-%d\")\n        # threshold = (pd.Timestamp(current_date) - pd.offsets.BDay(30)).strftime(\"%Y-%m-%d\")\n        # threshold = (pd.Timestamp(current_date) - pd.offsets.BDay(15)).strftime(\"%Y-%m-%d\")\n        print(f\"threshold: {threshold}\")\n        df_price_raw = df_price_raw.loc[df_price_raw[\"Date\"] >= threshold]\n\n        # to generate AdjustedClose, increment price data\n        df_price_raw = pd.concat([df_price_raw, prices[price_cols]])\n        # generate AdjustedClose\n        df_price = adjust_price_v2(df_price_raw)\n\n        # get target SecuritiesCodes\n        codes = sorted(prices[\"SecuritiesCode\"].unique())\n\n        feature = pd.concat([get_features_for_predict_v2e(df_price, code) for code in codes])\n        feature = feature.loc[feature.index == current_date]\n\n        feature.loc[:, \"predict\"] = pred_model.predict(feature[feat_cols_v2])\n\n        # set rank by predict\n        feature = feature.sort_values(\"predict\", ascending=False).drop_duplicates(subset=['SecuritiesCode'])\n        feature.loc[:, \"Rank\"] = np.arange(len(feature))\n        feature_map = feature.set_index('SecuritiesCode')['Rank'].to_dict()\n        sample_prediction['Rank'] = sample_prediction['SecuritiesCode'].map(feature_map)\n\n        # check Rank\n        assert sample_prediction[\"Rank\"].notna().all()\n        assert sample_prediction[\"Rank\"].min() == 0\n        assert sample_prediction[\"Rank\"].max() == len(sample_prediction[\"Rank\"]) - 1\n\n        # register your predictions\n        env.predict(sample_prediction)\n        counter += 1","metadata":{"collapsed":false,"execution":{"iopub.execute_input":"2022-06-17T05:24:36.967883Z","iopub.status.busy":"2022-06-17T05:24:36.966491Z","iopub.status.idle":"2022-06-17T05:31:41.96935Z","shell.execute_reply":"2022-06-17T05:31:41.968828Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":425.21254,"end_time":"2022-06-17T05:31:41.969503","exception":false,"start_time":"2022-06-17T05:24:36.756963","status":"completed"},"pycharm":{"name":"#%%\n"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if CFG.SUBMIT:\n    ! head submission.csv","metadata":{"collapsed":false,"execution":{"iopub.execute_input":"2022-06-17T05:31:42.841667Z","iopub.status.busy":"2022-06-17T05:31:42.822031Z","iopub.status.idle":"2022-06-17T05:31:43.516543Z","shell.execute_reply":"2022-06-17T05:31:43.515883Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":1.123934,"end_time":"2022-06-17T05:31:43.516777","exception":false,"start_time":"2022-06-17T05:31:42.392843","status":"completed"},"pycharm":{"name":"#%%\n"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if CFG.SUBMIT:\n    ! tail submission.csv","metadata":{"collapsed":false,"execution":{"iopub.execute_input":"2022-06-17T05:31:44.354757Z","iopub.status.busy":"2022-06-17T05:31:44.354018Z","iopub.status.idle":"2022-06-17T05:31:45.038729Z","shell.execute_reply":"2022-06-17T05:31:45.038217Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":1.105589,"end_time":"2022-06-17T05:31:45.03887","exception":false,"start_time":"2022-06-17T05:31:43.933281","status":"completed"},"pycharm":{"name":"#%%\n"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.421977,"end_time":"2022-06-17T05:31:45.893949","exception":false,"start_time":"2022-06-17T05:31:45.471972","status":"completed"},"pycharm":{"name":"#%%\n"},"tags":[]},"execution_count":null,"outputs":[]}]}