{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction","metadata":{}},{"cell_type":"markdown","source":"This is the inference part of my previous notebook:[**【JPX】Window-based Regression with XGBoost**](https://www.kaggle.com/code/daosword/jpx-window-based-regression-with-xgboost-train).  \n\nIn this noteboook, I will load the trained XGBoost model, preprocess testing data, and make predictions and submission.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport joblib\nfrom tqdm.notebook import tqdm\nimport jpx_tokyo_market_prediction","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-05-19T16:41:19.40917Z","iopub.execute_input":"2022-05-19T16:41:19.409519Z","iopub.status.idle":"2022-05-19T16:41:19.414857Z","shell.execute_reply.started":"2022-05-19T16:41:19.409481Z","shell.execute_reply":"2022-05-19T16:41:19.413999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load history data","metadata":{}},{"cell_type":"code","source":"%%time\n# Load history data\ndf_prices = pd.read_csv('../input/jpx-tokyo-stock-exchange-prediction/train_files/stock_prices.csv')\ndf_prices = df_prices[df_prices['Date'] >= '2021-10-01'].reset_index(drop=True)\ndf_prices","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:41:39.177511Z","iopub.execute_input":"2022-05-19T16:41:39.177819Z","iopub.status.idle":"2022-05-19T16:41:44.390254Z","shell.execute_reply.started":"2022-05-19T16:41:39.177786Z","shell.execute_reply":"2022-05-19T16:41:44.389358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocess data","metadata":{}},{"cell_type":"code","source":"# Date features\ndef get_date_features(df, date_col):\n    \"\"\"\n    Add datetime features to original dataframe\n    \"\"\"\n    df = df.copy()\n    df[date_col] = pd.to_datetime(df[date_col], format='%Y-%m-%d')\n    df['year'] = df[date_col].dt.year\n    df['month'] = df[date_col].dt.month\n    df['week'] = df[date_col].dt.isocalendar().week\n    df['day'] = df[date_col].dt.day\n    df['dayofweek'] = df[date_col].dt.dayofweek\n    df['dayofyear'] = df[date_col].dt.dayofyear\n    df[date_col] = df[date_col].astype(str)\n    return df\n\n# Shadow features - https://www.kaggle.com/code/satoshidatamoto/jpx-xgboost-with-gpu-fit-in-1-min\ndef get_shadow_features(df):\n    \"\"\"\n    Add shadow features to original dataframe\n    \"\"\"\n    df = df.copy()\n    df['upper_shadow'] = df['High'] - np.maximum(df['Close'], df['Open'])\n    df['lower_shadow'] = np.minimum(df['Close'], df['Open']) - df['Low']\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:41:53.621061Z","iopub.execute_input":"2022-05-19T16:41:53.621722Z","iopub.status.idle":"2022-05-19T16:41:53.63279Z","shell.execute_reply.started":"2022-05-19T16:41:53.621673Z","shell.execute_reply":"2022-05-19T16:41:53.631978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocessing(df, date_col, feature_col, target_col, group_col, \n                  training_cutoff, num_periods_input, num_periods_output, \n                  fill_missing_train=False, fill_missing_test=False, training=True, \n                  backward_gap=1):\n    \"\"\"\n    Preprocess training and testing data\n    \"\"\"\n    # Add date features\n    print(\"Adding date features..\")\n    df = get_date_features(df, date_col)\n    date_features = ['year', 'month', 'week', 'day', 'dayofweek', 'dayofyear']\n    \n    # Add shadow features\n    print(\"Adding shadow features..\")\n    df = get_shadow_features(df)\n    shadow_features = ['upper_shadow', 'lower_shadow']\n    \n    # Train test split\n    print(\"Spliting train and test data..\")\n    real_training_cutoff = df[df[date_col] >= training_cutoff][date_col].values[0]\n    all_dates = sorted(df.Date.unique().tolist())\n    cutoff_idx = all_dates.index(real_training_cutoff)\n    training_cutoff_adjust = all_dates[cutoff_idx - num_periods_input - backward_gap]\n    train = df[df[date_col] < training_cutoff_adjust]\n    test = df[df[date_col] >= training_cutoff_adjust]\n        \n    # Get all features and target\n    all_features = feature_col + date_features + shadow_features\n    number_of_features = len(all_features)\n    \n    # Process missing training targets\n    df_train = train[[group_col] + all_features + [target_col]].copy()\n    df_train = df_train.dropna(subset=[target_col]).reset_index(drop=True).copy()\n    df_test = test[[group_col] + all_features + [target_col]].copy()\n            \n    x_batches = []\n    y_batches = []\n    x_testbatches = []\n    y_testbatches = []\n    \n    # Create train and test batches\n    print(\"Constructing training and testing batches..\")\n    for group in tqdm(df[group_col].unique()):\n        limit = num_periods_output + num_periods_input + backward_gap\n        \n        ############################ TRAIN windows ############################\n        train = df_train[df_train[group_col] == group].copy().reset_index(drop=True)\n        train = train.drop(group_col, axis=1)\n        \n        # Process missing features in training data\n        if fill_missing_train:\n            train = train.fillna(method='ffill')\n        else:\n            train = train.dropna()\n        \n        if training:\n            y_train = train[target_col].values.astype('float32')\n            x_train = train[all_features].values\n            y_train = np.reshape(y_train, (len(y_train), 1))\n            train = np.append(x_train, y_train, axis=1)\n            \n            end_train = len(train)\n            start_train = 0\n            next_train = 0\n\n            while start_train + limit <= end_train:\n                next_train = start_train + num_periods_input\n                history_targets = train[start_train:next_train, -1].reshape(num_periods_input, 1)\n                covariates = train[start_train+1+backward_gap:next_train+1+backward_gap, :-1]\n                x_batches.append(np.append(covariates, history_targets, axis=1))\n                y_batches.append(train[next_train+backward_gap:next_train+backward_gap+num_periods_output, -1])\n                start_train = start_train + 1\n\n        ############################ TEST windows ############################\n        test = df_test[df_test[group_col] == group].copy().reset_index(drop=True)\n        test = test.drop(group_col, axis=1)\n        \n        # Process missing features in training data\n        if fill_missing_test:\n            test = test.fillna(method='ffill')\n        else:\n            test = test.dropna()\n        \n        y_test = test[target_col].values.astype('float32')\n        x_test = test[all_features].values\n        y_test = np.reshape(y_test, (len(y_test), 1))\n        test = np.append(x_test, y_test, axis=1)\n\n        end_test = len(test)\n        start_test = 0\n        next_test = 0\n\n        while start_test + limit <= end_test:\n            next_test = start_test + num_periods_input\n            history_targets = test[start_test:next_test, -1].reshape(num_periods_input, 1)\n            covariates = test[start_test+1+backward_gap:next_test+1+backward_gap, :-1]\n            x_testbatches.append(np.append(covariates, history_targets, axis=1))\n            y_testbatches.append(test[next_test+backward_gap:next_test+backward_gap+num_periods_output, -1])\n            start_test = start_test + 1\n    \n    if training:\n        x_batches = np.asarray(x_batches)\n        y_batches = np.asarray(y_batches)\n        y_batches = y_batches.reshape(-1, num_periods_output, 1)\n        print(\"X_train: {}, y_train: {}\".format(x_batches.shape, y_batches.shape))\n    \n    x_testbatches = np.asarray(x_testbatches)\n    y_testbatches = np.asarray(y_testbatches)\n    y_testbatches = y_testbatches.reshape(-1, num_periods_output, 1)\n    print(\"X_test:  {}, y_test:  {}\".format(x_testbatches.shape, y_testbatches.shape))\n    \n    if training:\n        return x_batches, y_batches, x_testbatches, y_testbatches\n    return x_testbatches, y_testbatches","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:41:56.678037Z","iopub.execute_input":"2022-05-19T16:41:56.678371Z","iopub.status.idle":"2022-05-19T16:41:56.716426Z","shell.execute_reply.started":"2022-05-19T16:41:56.678337Z","shell.execute_reply":"2022-05-19T16:41:56.715037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nfeature_col = ['Open', 'High', 'Low', 'Close', 'Volume']\nnum_periods_input = 5\nnum_periods_output = 1\n\nX_test, y_test = preprocessing(\n    df=df_prices, \n    date_col='Date', \n    feature_col=feature_col, \n    target_col='Target', \n    group_col='SecuritiesCode', \n    training_cutoff='2021-12-01', \n    num_periods_input=num_periods_input, \n    num_periods_output=num_periods_output,\n    fill_missing_train=False,\n    fill_missing_test=True,\n    training=False,\n    backward_gap=1,\n)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:41:59.886866Z","iopub.execute_input":"2022-05-19T16:41:59.887841Z","iopub.status.idle":"2022-05-19T16:42:11.894684Z","shell.execute_reply.started":"2022-05-19T16:41:59.887785Z","shell.execute_reply":"2022-05-19T16:42:11.893617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_test_instances(X_test):\n    All_Testing_Instances = []\n    for i in tqdm(range(len(X_test))):\n        hold = []\n        for j in range(len(X_test[i])):\n            if j == (len(X_test[i])-1):\n                hold = np.concatenate((hold, X_test[i][j][:]), axis=None)\n            else:\n                hold = np.concatenate((hold, X_test[i][j][-1]), axis=None)\n        All_Testing_Instances.append(hold)\n    \n    All_Testing_Instances = np.reshape(All_Testing_Instances, (len(All_Testing_Instances), len(All_Testing_Instances[0])))\n    return All_Testing_Instances","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:42:15.79901Z","iopub.execute_input":"2022-05-19T16:42:15.799977Z","iopub.status.idle":"2022-05-19T16:42:15.808862Z","shell.execute_reply.started":"2022-05-19T16:42:15.799936Z","shell.execute_reply":"2022-05-19T16:42:15.807713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"All_Testing_Instances = create_test_instances(X_test)\nAll_Testing_Instances.shape","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:42:20.174718Z","iopub.execute_input":"2022-05-19T16:42:20.175014Z","iopub.status.idle":"2022-05-19T16:42:20.403295Z","shell.execute_reply.started":"2022-05-19T16:42:20.174985Z","shell.execute_reply":"2022-05-19T16:42:20.402656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load trained model","metadata":{}},{"cell_type":"code","source":"# Load trained xgboost model\nmodel = joblib.load('../input/jpx-trained-models-v3/JPX_xgboost_2017_2021.pkl')\nmodel","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:51:25.562949Z","iopub.execute_input":"2022-05-19T16:51:25.563257Z","iopub.status.idle":"2022-05-19T16:51:25.621374Z","shell.execute_reply.started":"2022-05-19T16:51:25.563228Z","shell.execute_reply":"2022-05-19T16:51:25.620589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test predictions\npredictions = model.predict(All_Testing_Instances)\nprint(predictions.shape)\nprint(predictions[:10])","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:42:26.247655Z","iopub.execute_input":"2022-05-19T16:42:26.248491Z","iopub.status.idle":"2022-05-19T16:42:26.301269Z","shell.execute_reply.started":"2022-05-19T16:42:26.248446Z","shell.execute_reply":"2022-05-19T16:42:26.30057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"# Make predictions and submission\nenv = jpx_tokyo_market_prediction.make_env()   # initialize the environment\niter_test = env.iter_test()    # an iterator which loops over the test files\nfor (prices, options, financials, trades, secondary_prices, sample_prediction) in iter_test:\n    # Combine history data with incoming new data\n    display(prices)\n    df_prices = pd.concat([df_prices, prices], ignore_index=True)\n    training_cutoff = prices['Date'].values[0]\n    print(\"Training cutoff: \", training_cutoff)\n    \n    # Get processed test data\n    X_test, _ = preprocessing(\n        df=df_prices, \n        date_col='Date', \n        feature_col=feature_col, \n        target_col='Target', \n        group_col='SecuritiesCode', \n        training_cutoff=training_cutoff, \n        num_periods_input=num_periods_input, \n        num_periods_output=num_periods_output,\n        fill_missing_train=False,\n        fill_missing_test=True,\n        training=False, \n        backward_gap=1,\n    )\n    X_test = create_test_instances(X_test)\n    \n    # Make predictions\n    sample_prediction['target_pred'] = model.predict(X_test)\n    sample_prediction = sample_prediction.sort_values(by=\"target_pred\", ascending=False)\n    sample_prediction['Rank'] = np.arange(2000)\n    sample_prediction = sample_prediction.sort_values(by=\"SecuritiesCode\", ascending=True)\n    display(sample_prediction)\n    sample_prediction.drop(['target_pred'], axis=1, inplace=True)\n    env.predict(sample_prediction)  # register your predictions","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:42:38.042046Z","iopub.execute_input":"2022-05-19T16:42:38.042931Z","iopub.status.idle":"2022-05-19T16:43:02.340642Z","shell.execute_reply.started":"2022-05-19T16:42:38.04289Z","shell.execute_reply":"2022-05-19T16:43:02.339637Z"},"trusted":true},"execution_count":null,"outputs":[]}]}