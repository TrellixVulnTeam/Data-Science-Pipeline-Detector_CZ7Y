{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"In this starter notebook, I am trying to build a basic neural network using Keras for the JPX challenge.  \nThis notebook is not aiming for high score, my purpose is setting a start point for deeper analysis and model experiment. By doing this I can also get familiarBatchNormalizationth the competition dataset and practice what I have learned about deep learning and neural network.  \nHope everyone enjoy this competition ðŸ˜„","metadata":{}},{"cell_type":"markdown","source":"# Setup","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport random\nimport jpx_tokyo_market_prediction\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers","metadata":{"execution":{"iopub.status.busy":"2022-04-21T08:44:43.643708Z","iopub.execute_input":"2022-04-21T08:44:43.644104Z","iopub.status.idle":"2022-04-21T08:44:48.70443Z","shell.execute_reply.started":"2022-04-21T08:44:43.643997Z","shell.execute_reply":"2022-04-21T08:44:48.703653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set random seed\nseed = 30\nrandom.seed(seed)\nos.environ['PYTHONHASHSEED'] = str(seed)\nnp.random.seed(seed)\ntf.random.set_seed(seed)","metadata":{"execution":{"iopub.status.busy":"2022-04-21T08:44:48.70755Z","iopub.execute_input":"2022-04-21T08:44:48.707781Z","iopub.status.idle":"2022-04-21T08:44:48.713144Z","shell.execute_reply.started":"2022-04-21T08:44:48.707748Z","shell.execute_reply":"2022-04-21T08:44:48.711703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Prep","metadata":{}},{"cell_type":"markdown","source":"## Dataset choice","metadata":{}},{"cell_type":"markdown","source":"To keep things simple, I will only use the data in *stock_prices.csv* to train the model.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/jpx-tokyo-stock-exchange-prediction/train_files/stock_prices.csv\")\ndf","metadata":{"execution":{"iopub.status.busy":"2022-04-21T08:44:48.714403Z","iopub.execute_input":"2022-04-21T08:44:48.715202Z","iopub.status.idle":"2022-04-21T08:44:54.412215Z","shell.execute_reply.started":"2022-04-21T08:44:48.715155Z","shell.execute_reply":"2022-04-21T08:44:54.4115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train & valid split","metadata":{}},{"cell_type":"code","source":"df.Date.describe()","metadata":{"execution":{"iopub.status.busy":"2022-04-21T08:44:54.414513Z","iopub.execute_input":"2022-04-21T08:44:54.41498Z","iopub.status.idle":"2022-04-21T08:44:54.862067Z","shell.execute_reply.started":"2022-04-21T08:44:54.414941Z","shell.execute_reply":"2022-04-21T08:44:54.861225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have 2332531 rows in the raw data table. The *Date* starts from *2017-01-04* and ends at *2021-12-03*.","metadata":{}},{"cell_type":"code","source":"print(\"Start date: {}, end date: {}\".format(df.Date.unique().min(), df.Date.unique().max()))","metadata":{"execution":{"iopub.status.busy":"2022-04-21T08:44:54.863776Z","iopub.execute_input":"2022-04-21T08:44:54.864072Z","iopub.status.idle":"2022-04-21T08:44:55.156147Z","shell.execute_reply.started":"2022-04-21T08:44:54.864038Z","shell.execute_reply":"2022-04-21T08:44:55.155408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I want to avoid some complicated time series split or CV strategy, so let's use the 2021's data for validation and the rest of data for training.","metadata":{}},{"cell_type":"code","source":"df_train = df[df['Date'] < '2021-01-01'].copy()\ndf_train.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-21T08:44:55.157285Z","iopub.execute_input":"2022-04-21T08:44:55.15795Z","iopub.status.idle":"2022-04-21T08:44:55.647052Z","shell.execute_reply.started":"2022-04-21T08:44:55.157912Z","shell.execute_reply":"2022-04-21T08:44:55.646343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_valid = df[df['Date'] >= '2021-01-01'].copy()\ndf_valid.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-21T08:44:55.648169Z","iopub.execute_input":"2022-04-21T08:44:55.648598Z","iopub.status.idle":"2022-04-21T08:44:55.960955Z","shell.execute_reply.started":"2022-04-21T08:44:55.648561Z","shell.execute_reply":"2022-04-21T08:44:55.96023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There will be approximately 20% data for validation, which is reasonable.","metadata":{}},{"cell_type":"code","source":"df_valid.shape[0] / df.shape[0] * 100","metadata":{"execution":{"iopub.status.busy":"2022-04-21T08:44:55.962167Z","iopub.execute_input":"2022-04-21T08:44:55.962462Z","iopub.status.idle":"2022-04-21T08:44:55.96801Z","shell.execute_reply.started":"2022-04-21T08:44:55.962424Z","shell.execute_reply":"2022-04-21T08:44:55.967397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Select Feature ","metadata":{}},{"cell_type":"markdown","source":"For this toy model, we will choose **Open**, **High**, **Low**, **Close** and **Volume** as 5 numerical predicting features, and ignore other information like Date, SecuritiesCode, etc.  ","metadata":{}},{"cell_type":"code","source":"num_features = ['Open', 'High', 'Low', 'Close', 'Volume']\ntarget = ['Target']\ndf_train = df_train[num_features + target].reset_index(drop=True).copy()\ndf_valid = df_valid[num_features + target].reset_index(drop=True).copy()\ndf_valid.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-21T08:44:55.969247Z","iopub.execute_input":"2022-04-21T08:44:55.96966Z","iopub.status.idle":"2022-04-21T08:44:56.109282Z","shell.execute_reply.started":"2022-04-21T08:44:55.969624Z","shell.execute_reply":"2022-04-21T08:44:56.108587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are some missing values in the corresponding columns and I just drop them this time.","metadata":{}},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-04-21T08:44:56.112302Z","iopub.execute_input":"2022-04-21T08:44:56.112545Z","iopub.status.idle":"2022-04-21T08:44:56.574475Z","shell.execute_reply.started":"2022-04-21T08:44:56.112519Z","shell.execute_reply":"2022-04-21T08:44:56.573746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.dropna(subset=num_features + target, axis=0, inplace=True)\ndf_valid.dropna(subset=num_features + target, axis=0, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-21T08:44:56.575747Z","iopub.execute_input":"2022-04-21T08:44:56.576116Z","iopub.status.idle":"2022-04-21T08:44:56.715101Z","shell.execute_reply.started":"2022-04-21T08:44:56.576082Z","shell.execute_reply":"2022-04-21T08:44:56.714381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.isnull().sum() + df_valid.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-04-21T08:44:56.71641Z","iopub.execute_input":"2022-04-21T08:44:56.716658Z","iopub.status.idle":"2022-04-21T08:44:56.752533Z","shell.execute_reply.started":"2022-04-21T08:44:56.716627Z","shell.execute_reply":"2022-04-21T08:44:56.751744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looks good.","metadata":{}},{"cell_type":"markdown","source":"## Preprocessing","metadata":{}},{"cell_type":"markdown","source":"The data preprocessing part mainly includes two operations:\n* feature normalization\n* create tensorflow dataset\n\nKeras official document provides great examples:\nhttps://keras.io/examples/structured_data/structured_data_classification_from_scratch/#preparing-the-data  \nFor each of the continuous numerical features, we will use Keras Normalization layer to make sure the mean of each feature is 0 and its standard deviation is 1.","metadata":{}},{"cell_type":"code","source":"# Define encoding function for numerical features\ndef encode_numerical_feature(feature, name, dataset):\n    # Create a Normalization layer for our feature\n    normalizer = layers.Normalization()\n\n    # Prepare a Dataset that only yields our feature\n    feature_ds = dataset.map(lambda x, y: x[name])\n    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n\n    # Learn the statistics of the data\n    normalizer.adapt(feature_ds)\n\n    # Normalize the input feature\n    encoded_feature = normalizer(feature)\n    return encoded_feature","metadata":{"execution":{"iopub.status.busy":"2022-04-21T08:44:56.753775Z","iopub.execute_input":"2022-04-21T08:44:56.754092Z","iopub.status.idle":"2022-04-21T08:44:56.759768Z","shell.execute_reply.started":"2022-04-21T08:44:56.754056Z","shell.execute_reply":"2022-04-21T08:44:56.758924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate tensorflow dataset\ndef dataframe_to_dataset(dataframe):\n    dataframe = dataframe.copy()\n    labels = dataframe.pop(\"Target\")\n    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n    ds = ds.shuffle(buffer_size=len(dataframe))\n    return ds\n\ntrain_ds = dataframe_to_dataset(df_train)\nvalid_ds = dataframe_to_dataset(df_valid)","metadata":{"execution":{"iopub.status.busy":"2022-04-21T08:44:56.761415Z","iopub.execute_input":"2022-04-21T08:44:56.761668Z","iopub.status.idle":"2022-04-21T08:44:59.193753Z","shell.execute_reply.started":"2022-04-21T08:44:56.761636Z","shell.execute_reply":"2022-04-21T08:44:59.191893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Each Dataset yields a tuple (input, target) where input is a dictionary of features","metadata":{}},{"cell_type":"code","source":"for x, y in train_ds.take(1):\n    print(\"Input:\", x)\n    print(\"Target:\", y)","metadata":{"execution":{"iopub.status.busy":"2022-04-21T08:44:59.195172Z","iopub.execute_input":"2022-04-21T08:44:59.19543Z","iopub.status.idle":"2022-04-21T08:45:06.566666Z","shell.execute_reply.started":"2022-04-21T08:44:59.195395Z","shell.execute_reply":"2022-04-21T08:45:06.565901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Batch the dataset\ntrain_ds = train_ds.batch(1024)\nvalid_ds = valid_ds.batch(1024)","metadata":{"execution":{"iopub.status.busy":"2022-04-21T08:45:06.568082Z","iopub.execute_input":"2022-04-21T08:45:06.568348Z","iopub.status.idle":"2022-04-21T08:45:06.576449Z","shell.execute_reply.started":"2022-04-21T08:45:06.568292Z","shell.execute_reply":"2022-04-21T08:45:06.575767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build Model","metadata":{}},{"cell_type":"markdown","source":"First we define the input layers of our NN model, then encoding them.","metadata":{}},{"cell_type":"code","source":"%%time\n# Raw numerical features\nOpen = keras.Input(shape=(1,), name=\"Open\")\nHigh = keras.Input(shape=(1,), name=\"High\")\nLow = keras.Input(shape=(1,), name=\"Low\")\nClose = keras.Input(shape=(1,), name=\"Close\")\nVolume = keras.Input(shape=(1,), name=\"Volume\")\n\nall_inputs = [Open, High, Low, Close, Volume]\n\n# Encode numerical features\nopen_encoded = encode_numerical_feature(Open, \"Open\", train_ds)\nhigh_encoded = encode_numerical_feature(High, \"High\", train_ds)\nlow_encoded = encode_numerical_feature(Low, \"Low\", train_ds)\nclose_encoded = encode_numerical_feature(Close, \"Close\", train_ds)\nvolume_encoded = encode_numerical_feature(Volume, \"Volume\", train_ds)","metadata":{"execution":{"iopub.status.busy":"2022-04-21T08:45:06.577588Z","iopub.execute_input":"2022-04-21T08:45:06.578054Z","iopub.status.idle":"2022-04-21T08:46:46.970303Z","shell.execute_reply.started":"2022-04-21T08:45:06.578016Z","shell.execute_reply":"2022-04-21T08:46:46.969585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The code block above runs for a while. After that, we could concat all input layers and connect them to multiple hidden Dense layers.","metadata":{}},{"cell_type":"code","source":"# Concat all features of input layer\nall_features = layers.concatenate(\n    [\n        open_encoded,\n        high_encoded,\n        low_encoded,\n        close_encoded,\n        volume_encoded,\n    ]\n)\n\n# Add several hidden layers with batch_norm and dropout\nx = layers.Dense(128, activation=\"relu\")(all_features)\nx = layers.BatchNormalization()(x)\nx = layers.Dropout(0.2)(x)\nx = layers.Dense(64, activation=\"relu\")(x)\nx = layers.BatchNormalization()(x)\nx = layers.Dropout(0.2)(x)\nx = layers.Dense(32, activation=\"relu\")(x)\nx = layers.BatchNormalization()(x)\nx = layers.Dropout(0.2)(x)\n\n# Output layer for regression task\noutput = layers.Dense(1, activation=\"linear\")(x)\n\n# Create our NN model\nmodel = keras.Model(all_inputs, output)\nmodel.compile(\n    optimizer='adam', \n    loss=\"mse\", \n    metrics=[tf.keras.metrics.RootMeanSquaredError()]\n)","metadata":{"execution":{"iopub.status.busy":"2022-04-21T08:50:03.431394Z","iopub.execute_input":"2022-04-21T08:50:03.431959Z","iopub.status.idle":"2022-04-21T08:50:03.517559Z","shell.execute_reply.started":"2022-04-21T08:50:03.431919Z","shell.execute_reply":"2022-04-21T08:50:03.516725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# NN model structure\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-04-21T08:50:05.786966Z","iopub.execute_input":"2022-04-21T08:50:05.787774Z","iopub.status.idle":"2022-04-21T08:50:05.805224Z","shell.execute_reply.started":"2022-04-21T08:50:05.787724Z","shell.execute_reply":"2022-04-21T08:50:05.804539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model visualization\nkeras.utils.plot_model(model, show_shapes=True, expand_nested=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-21T08:50:08.022744Z","iopub.execute_input":"2022-04-21T08:50:08.023214Z","iopub.status.idle":"2022-04-21T08:50:08.647575Z","shell.execute_reply.started":"2022-04-21T08:50:08.023174Z","shell.execute_reply":"2022-04-21T08:50:08.646731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train Model","metadata":{}},{"cell_type":"markdown","source":"Before start training our model, we could set an early-stopping callback. If validation loss does not improve for some number of epochs, stop training and restore best model weights.","metadata":{}},{"cell_type":"code","source":"# Set early_stopping callbacks, if val_loss does not improve for 10 epochs, stop training and restore best model weights\nearly_stopping = keras.callbacks.EarlyStopping(\n    monitor='val_loss',\n    patience=10,\n    min_delta=1e-3,\n    restore_best_weights=True,\n)","metadata":{"execution":{"iopub.status.busy":"2022-04-21T08:50:15.605835Z","iopub.execute_input":"2022-04-21T08:50:15.606154Z","iopub.status.idle":"2022-04-21T08:50:15.611221Z","shell.execute_reply.started":"2022-04-21T08:50:15.606121Z","shell.execute_reply":"2022-04-21T08:50:15.610435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's train the model","metadata":{}},{"cell_type":"code","source":"# Model training \nmodel.fit(\n    train_ds, \n    validation_data=valid_ds, \n    epochs=50, \n    callbacks=[early_stopping])","metadata":{"execution":{"iopub.status.busy":"2022-04-21T08:50:22.750374Z","iopub.execute_input":"2022-04-21T08:50:22.750785Z","iopub.status.idle":"2022-04-21T08:53:14.198319Z","shell.execute_reply.started":"2022-04-21T08:50:22.750748Z","shell.execute_reply":"2022-04-21T08:53:14.1952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save model\nmodel.save(\"spx_toy_model.h5\")","metadata":{"execution":{"iopub.status.busy":"2022-04-21T08:49:45.895003Z","iopub.status.idle":"2022-04-21T08:49:45.895311Z","shell.execute_reply.started":"2022-04-21T08:49:45.895151Z","shell.execute_reply":"2022-04-21T08:49:45.895175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"# Load trained model\nbest_model = keras.models.load_model(\"spx_toy_model.h5\")","metadata":{"execution":{"iopub.status.busy":"2022-04-21T08:49:45.896505Z","iopub.status.idle":"2022-04-21T08:49:45.897544Z","shell.execute_reply.started":"2022-04-21T08:49:45.897279Z","shell.execute_reply":"2022-04-21T08:49:45.897305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate tensorflow dataset for test data\ndef dataframe_to_dataset_test(dataframe):\n    dataframe = dataframe.copy()\n    ds = tf.data.Dataset.from_tensor_slices(dict(dataframe))\n    return ds","metadata":{"execution":{"iopub.status.busy":"2022-04-21T08:49:45.898774Z","iopub.status.idle":"2022-04-21T08:49:45.899196Z","shell.execute_reply.started":"2022-04-21T08:49:45.89897Z","shell.execute_reply":"2022-04-21T08:49:45.898993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make predictions and submission\nenv = jpx_tokyo_market_prediction.make_env()   # initialize the environment\niter_test = env.iter_test()    # an iterator which loops over the test files\nfor (prices, options, financials, trades, secondary_prices, sample_prediction) in iter_test:\n    test_ds = dataframe_to_dataset_test(prices)\n    sample_prediction['target_pred'] = best_model.predict(test_ds)\n    sample_prediction = sample_prediction.sort_values(by=\"target_pred\", ascending=False)\n    sample_prediction['Rank'] = np.arange(2000)\n    sample_prediction = sample_prediction.sort_values(by=\"SecuritiesCode\", ascending=True)\n    sample_prediction.drop(['target_pred'], axis=1, inplace=True)\n    display(sample_prediction)\n    env.predict(sample_prediction)   # register your predictions","metadata":{"execution":{"iopub.status.busy":"2022-04-21T08:49:45.900433Z","iopub.status.idle":"2022-04-21T08:49:45.901064Z","shell.execute_reply.started":"2022-04-21T08:49:45.900817Z","shell.execute_reply":"2022-04-21T08:49:45.900843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reference\nThere are other good starter notebooks about NN, such as:  \n* Ravi trained both NN and LGBM baseline models with cross validation in https://www.kaggle.com/code/ravishah1/jpx-dnn-lgbm-with-cross-validation. His NN model design is a little different, using SecuritiesCode as an input featrue but without using Volume column.","metadata":{}}]}