{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Initial setup.\nImport packages and libraries used in most projects, and perform basic project configuration.","metadata":{}},{"cell_type":"code","source":"# Import the basic packages to get started.\nimport os                          # Misc. operating system interface\nimport pandas as pd                # Panel Data manipulation & analysis\nimport numpy as np                 # Math & Arrays\nimport gc                          # Garbage Collector. Frees up memory.\nimport matplotlib.pyplot as plt    # Data visualization. Plots & charts of math functions.\nfrom tqdm import tqdm              # For displaying cool progress indicators. From the Arabic name taqaddum which means 'progress'.\n\n\n# For EDA:\nfrom sklearn.feature_selection import mutual_info_regression\n\n# Some extras for this project:\nfrom decimal import ROUND_HALF_UP, Decimal    # For quick rounding calculations\nfrom lightgbm import LGBMRegressor            # Light Gradient Boosting Machine, good for ranking\nfrom lightgbm import Booster\nimport sys\n\n\n# Packages for the models we'll use:\n#import tensorflow as tf\n#from tensorflow.keras import layers\n#from tensorflow import keras\n\n# Set random seed for reproducibility:\nnp.random.seed(0)\n\n# Notes:\n# Input data files are available in the read-only \"../input/\" directory\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\nprint(\"Done with setup.\")","metadata":{"execution":{"iopub.status.busy":"2022-04-05T05:02:16.678286Z","iopub.execute_input":"2022-04-05T05:02:16.679097Z","iopub.status.idle":"2022-04-05T05:02:16.860499Z","shell.execute_reply.started":"2022-04-05T05:02:16.678979Z","shell.execute_reply":"2022-04-05T05:02:16.85951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Let's look at the input data.\nFiles under ../input/jpx-tokyo-stock-exchange-prediction\n\n**Copies of data files exist in multiple folders that cover different time windows and serve different purposes.**\n\n* */stock_prices.csv The core file of interest. Includes the daily closing price for each stock and the target column.\n\n* */options.csv Data on the status of a variety of options based on the broader market. Many options include implicit predictions of the future price of the stock market and so may be of interest even though the options are not scored directly.\n\n* */secondary_stock_prices.csv The core dataset contains on the 2,000 most commonly traded equities but many less liquid securities are also traded on the Tokyo market. This file contains data for those securities, which aren't scored but may be of interest for assessing the market as a whole.\n\n* */trades.csv Aggregated summary of trading volumes from the previous business week.\n\n* */financials.csv Results from quarterly earnings reports.\n\n* stock_list.csv - Mapping between the SecuritiesCode and company names, plus general information about which industry the company is in.\nFolders\n\ndata_specifications/ - Definitions for individual columns.\n\njpx_tokyo_market_prediction/ Files that enable the API. Expect the API to deliver all rows in under five minutes and to reserve less than 0.5 GB of memory.\n\n\n\ntrain_files/ Data folder covering the main training period.\n\nsupplemental_files/ Data folder containing a dynamic window of supplemental training data. This will be updated with new data during the main phase of the competition in early May, early June, and roughly a week before the submissions are locked.\n\nexample_test_files/ Data folder covering the public test period. Intended to facilitate offline testing. Includes the same columns delivered by the API (ie no Target column). You can calculate the Target column from the Close column; it's the return from buying a stock the next day and selling the day after that. This folder also includes an example of the sample submission file that will be delivered by the API.\n\n# We need to choose the correct directory.\nThe code blocks below come from Kaggler s-meitoma's [Train Demo Notebook](https://www.kaggle.com/code/smeitoma/train-demo)","metadata":{}},{"cell_type":"code","source":"# set base_dir to load data\nbase_dir = \"../input/jpx-tokyo-stock-exchange-prediction\"\n\n# There are three types of stock_price.csv\n# We use one in the train_files folder for this notebook.\ntrain_files_dir = f\"{base_dir}/train_files\"\n\n# for forecasting phase leaderboard, you may want to include stock_price.csv in the supplemental_files folder.\n# You can remove \"forecasting phase leaderboard\" comments in this notebook to use stock_price.csv in the supplemental_files folder.\n# forecasting phase leaderboard:\n# supplemental_files_dir = f\"{base_dir}/supplemental_files\"","metadata":{"execution":{"iopub.status.busy":"2022-04-05T05:02:23.567669Z","iopub.execute_input":"2022-04-05T05:02:23.567942Z","iopub.status.idle":"2022-04-05T05:02:23.57308Z","shell.execute_reply.started":"2022-04-05T05:02:23.567913Z","shell.execute_reply":"2022-04-05T05:02:23.572051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n# Generating AdjustedClose price\n\nWe will generate AdjustedClose using AdjustmentFactor value. This should reduce historical price gap caused by split/reverse-split.\n","metadata":{}},{"cell_type":"code","source":"def adjust_price(price):\n    \"\"\"\n    Args:\n        price (pd.DataFrame)  : pd.DataFrame include stock_price\n    Returns:\n        price DataFrame (pd.DataFrame): stock_price with generated AdjustedClose\n    \"\"\"\n    # transform Date column into datetime\n    price.loc[: ,\"Date\"] = pd.to_datetime(price.loc[: ,\"Date\"], format=\"%Y-%m-%d\")\n\n    def generate_adjusted_close(df):\n        \"\"\"\n        Args:\n            df (pd.DataFrame)  : stock_price for a single SecuritiesCode\n        Returns:\n            df (pd.DataFrame): stock_price with AdjustedClose for a single SecuritiesCode\n        \"\"\"\n        # sort data to generate CumulativeAdjustmentFactor\n        df = df.sort_values(\"Date\", ascending=False)\n        # generate CumulativeAdjustmentFactor\n        df.loc[:, \"CumulativeAdjustmentFactor\"] = df[\"AdjustmentFactor\"].cumprod()\n        # generate AdjustedClose\n        df.loc[:, \"AdjustedClose\"] = (\n            df[\"CumulativeAdjustmentFactor\"] * df[\"Close\"]\n        ).map(lambda x: float(\n            Decimal(str(x)).quantize(Decimal('0.1'), rounding=ROUND_HALF_UP)\n        ))\n        # reverse order\n        df = df.sort_values(\"Date\")\n        # to fill AdjustedClose, replace 0 into np.nan\n        df.loc[df[\"AdjustedClose\"] == 0, \"AdjustedClose\"] = np.nan\n        # forward fill AdjustedClose\n        df.loc[:, \"AdjustedClose\"] = df.loc[:, \"AdjustedClose\"].ffill()\n        return df\n\n    # generate AdjustedClose\n    price = price.sort_values([\"SecuritiesCode\", \"Date\"])\n    price = price.groupby(\"SecuritiesCode\").apply(generate_adjusted_close).reset_index(drop=True)\n\n    price.set_index(\"Date\", inplace=True)\n    return price","metadata":{"execution":{"iopub.status.busy":"2022-04-05T05:02:34.993361Z","iopub.execute_input":"2022-04-05T05:02:34.994135Z","iopub.status.idle":"2022-04-05T05:02:35.00696Z","shell.execute_reply.started":"2022-04-05T05:02:34.994092Z","shell.execute_reply":"2022-04-05T05:02:35.006206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load stock price data\ndf_price = pd.read_csv(f\"{train_files_dir}/stock_prices.csv\")\n\n# forecasting phase leaderboard:\n# df_price_supplemental = pd.read_csv(f\"{supplemental_files_dir}/stock_prices.csv\")\n# df_price = pd.concat([df_price, df_price_supplemental])\n\n# generate AdjustedClose\ndf_price = adjust_price(df_price)","metadata":{"execution":{"iopub.status.busy":"2022-04-05T05:02:39.499226Z","iopub.execute_input":"2022-04-05T05:02:39.500299Z","iopub.status.idle":"2022-04-05T05:03:07.477842Z","shell.execute_reply.started":"2022-04-05T05:02:39.500248Z","shell.execute_reply":"2022-04-05T05:03:07.477101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n# Generating simple features\n\nLet us visualize various numerical values that can be calculated from stock price data. Two examples are generated here: price change rate and historical volatility. These two are basic characteristic quantities in stock analysis.\n\nFor this analysis, we will use Toyota Motor Corporation (SecuritiesCode: 7203)\n","metadata":{}},{"cell_type":"code","source":"price = df_price.loc[df_price[\"SecuritiesCode\"] == 7203].copy()","metadata":{"execution":{"iopub.status.busy":"2022-04-05T05:03:12.539504Z","iopub.execute_input":"2022-04-05T05:03:12.540403Z","iopub.status.idle":"2022-04-05T05:03:12.684302Z","shell.execute_reply.started":"2022-04-05T05:03:12.540325Z","shell.execute_reply":"2022-04-05T05:03:12.683308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n# Percentage change in price\nTwo main rates of price change are used: the rate of change from the past and the rate of change in the future. The past rate of change is calculated, for example, in terms of how much change occurred from the closing price three days ago to the closing price today.\n\nWhen calculating the rate of price change in the last 2 week, last 1 month, or last 3 months, it is often treated as X business days later, such as 10 business days, 21 business days, or 63 business days later, as shown below, rather than X months later on the calendar to simplify handling in practical terms. In this case, the pct_change function of pandas is used for the calculation.\n","metadata":{}},{"cell_type":"code","source":"periods = [10, 21, 63]\nreturn_names = []\nfor period in periods:\n    return_names.append(f\"return_{period}\")\n    price.loc[:, f\"return_{period}\"] = price[\"AdjustedClose\"].pct_change(period)","metadata":{"execution":{"iopub.status.busy":"2022-04-05T05:03:17.483218Z","iopub.execute_input":"2022-04-05T05:03:17.483731Z","iopub.status.idle":"2022-04-05T05:03:17.501349Z","shell.execute_reply.started":"2022-04-05T05:03:17.483689Z","shell.execute_reply":"2022-04-05T05:03:17.500465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n# Historical volatility\nNext, we calculate the historical volatility. The historical volatility calculated here is the standard deviation of the logarithmic price change over the last 10, 21, and 63 business days. Historical volatility is a risk indicator and is used to determine how violently prices have fluctuated. In general, stocks with large historical volatility are considered relatively riskier to hold as assets than stocks with small historical volatility.\n","metadata":{}},{"cell_type":"code","source":"periods = [10, 21, 63]\nvol_names = []\nfor period in periods:\n    vol_names.append(f\"volatility_{period}\")\n    price.loc[:, f\"volatility_{period}\"] = np.log(price[\"AdjustedClose\"]).diff().rolling(period).std()","metadata":{"execution":{"iopub.status.busy":"2022-04-05T05:03:22.613369Z","iopub.execute_input":"2022-04-05T05:03:22.614138Z","iopub.status.idle":"2022-04-05T05:03:22.629276Z","shell.execute_reply.started":"2022-04-05T05:03:22.61409Z","shell.execute_reply":"2022-04-05T05:03:22.628309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Both price change and historical volatility are very commonly used characteristics in equity analysis. As an example, the price change and volatility of Toyota Motor Corporation can be visualized as follows","metadata":{}},{"cell_type":"code","source":"price[return_names].plot(figsize=(20, 8))","metadata":{"execution":{"iopub.status.busy":"2022-04-05T05:03:26.269586Z","iopub.execute_input":"2022-04-05T05:03:26.269872Z","iopub.status.idle":"2022-04-05T05:03:26.733158Z","shell.execute_reply.started":"2022-04-05T05:03:26.26984Z","shell.execute_reply":"2022-04-05T05:03:26.732446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"price[vol_names].plot(figsize=(20, 8))","metadata":{"execution":{"iopub.status.busy":"2022-04-05T05:03:32.310657Z","iopub.execute_input":"2022-04-05T05:03:32.311078Z","iopub.status.idle":"2022-04-05T05:03:32.688308Z","shell.execute_reply.started":"2022-04-05T05:03:32.31103Z","shell.execute_reply":"2022-04-05T05:03:32.687394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The financial data (stock_fin.csv) included in this competition also has a variety of features in the field of fundamental analysis, so we encourage you to design various features and use them in the competition.\n\n# Pre-processing for model building\n\nThis notebook presents a simple model using LightGBM.\n\nFirst, the features are generated using the price change and historical volatility described above.","metadata":{}},{"cell_type":"code","source":"def get_features_for_predict(price, code):\n    \"\"\"\n    Args:\n        price (pd.DataFrame)  : pd.DataFrame include stock_price\n        code (int)  : A local code for a listed company\n    Returns:\n        feature DataFrame (pd.DataFrame)\n    \"\"\"\n    close_col = \"AdjustedClose\"\n    feats = price.loc[price[\"SecuritiesCode\"] == code, [\"SecuritiesCode\", close_col]].copy()\n\n    # calculate 2 week return using AdjustedClose\n    feats[\"return_2week\"] = feats[close_col].pct_change(10)\n    # calculate last 1 month return using AdjustedClose\n    feats[\"return_1month\"] = feats[close_col].pct_change(21)\n    # calculate last 3 months return using AdjustedClose\n    feats[\"return_3month\"] = feats[close_col].pct_change(63)\n\n    # calculate 2 week historical volatility using AdjustedClose\n    feats[\"volatility_2week\"] = (\n        np.log(feats[close_col]).diff().rolling(10).std()\n    )\n    # calculate last 1 month historical volatility using AdjustedClose\n    feats[\"volatility_1month\"] = (\n        np.log(feats[close_col]).diff().rolling(21).std()\n    )\n    # calculate last 3 months historical volatility using AdjustedClose\n    feats[\"volatility_3month\"] = (\n        np.log(feats[close_col]).diff().rolling(63).std()\n    )\n\n    # filling data for nan and inf\n    feats = feats.fillna(0)\n    feats = feats.replace([np.inf, -np.inf], 0)\n    # drop AdjustedClose column\n    feats = feats.drop([close_col], axis=1)\n\n    return feats","metadata":{"execution":{"iopub.status.busy":"2022-04-05T05:03:38.260266Z","iopub.execute_input":"2022-04-05T05:03:38.260587Z","iopub.status.idle":"2022-04-05T05:03:38.270934Z","shell.execute_reply.started":"2022-04-05T05:03:38.260552Z","shell.execute_reply":"2022-04-05T05:03:38.270209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fetch prediction target SecuritiesCodes\ncodes = sorted(df_price[\"SecuritiesCode\"].unique())\nlen(codes)","metadata":{"execution":{"iopub.status.busy":"2022-04-05T05:03:42.588107Z","iopub.execute_input":"2022-04-05T05:03:42.588645Z","iopub.status.idle":"2022-04-05T05:03:42.612049Z","shell.execute_reply.started":"2022-04-05T05:03:42.588599Z","shell.execute_reply":"2022-04-05T05:03:42.611069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# generate feature\nbuff = []\nfor code in tqdm(codes):\n    feat = get_features_for_predict(df_price, code)\n    buff.append(feat)\nfeature = pd.concat(buff)","metadata":{"execution":{"iopub.status.busy":"2022-04-05T05:03:46.051293Z","iopub.execute_input":"2022-04-05T05:03:46.051604Z","iopub.status.idle":"2022-04-05T05:04:13.302042Z","shell.execute_reply.started":"2022-04-05T05:03:46.051572Z","shell.execute_reply":"2022-04-05T05:04:13.301178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Label creation\nNext, we obtain the labels to be used for training the model (this is where we load and split the label data).","metadata":{}},{"cell_type":"code","source":"def get_label(price, code):\n    \"\"\" Labelizer\n    Args:\n        price (pd.DataFrame): dataframe of stock_price.csv\n        code (int): Local Code in the universe\n    Returns:\n        df (pd.DataFrame): label data\n    \"\"\"\n    df = price.loc[price[\"SecuritiesCode\"] == code].copy()\n    df.loc[:, \"label\"] = df[\"Target\"]\n\n    return df.loc[:, [\"SecuritiesCode\", \"label\"]]","metadata":{"execution":{"iopub.status.busy":"2022-04-05T05:04:25.284823Z","iopub.execute_input":"2022-04-05T05:04:25.285132Z","iopub.status.idle":"2022-04-05T05:04:25.291021Z","shell.execute_reply.started":"2022-04-05T05:04:25.285095Z","shell.execute_reply":"2022-04-05T05:04:25.290034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split data into TRAIN and TEST\nTRAIN_END = \"2019-12-31\"\n# We put a week gap between TRAIN_END and TEST_START\n# to avoid leakage of test data information from label\nTEST_START = \"2020-01-06\"\n\ndef get_features_and_label(price, codes, features):\n    \"\"\"\n    Args:\n        price (pd.DataFrame): loaded price data\n        codes  (array) : target codes\n        feature (pd.DataFrame): features\n    Returns:\n        train_X (pd.DataFrame): training data\n        train_y (pd.DataFrame): label for train_X\n        test_X (pd.DataFrame): test data\n        test_y (pd.DataFrame): label for test_X\n    \"\"\"\n    # to store splited data\n    trains_X, tests_X = [], []\n    trains_y, tests_y = [], []\n\n    # generate feature one by one\n    for code in tqdm(codes):\n\n        feats = features[features[\"SecuritiesCode\"] == code].dropna()\n        labels = get_label(price, code).dropna()\n\n        if feats.shape[0] > 0 and labels.shape[0] > 0:\n            # align label and feature indexes\n            labels = labels.loc[labels.index.isin(feats.index)]\n            feats = feats.loc[feats.index.isin(labels.index)]\n\n            assert (labels.loc[:, \"SecuritiesCode\"] == feats.loc[:, \"SecuritiesCode\"]).all()\n            labels = labels.loc[:, \"label\"]\n\n            # split data into TRAIN and TEST\n            _train_X = feats[: TRAIN_END]\n            _test_X = feats[TEST_START:]\n\n            _train_y = labels[: TRAIN_END]\n            _test_y = labels[TEST_START:]\n            \n            assert len(_train_X) == len(_train_y)\n            assert len(_test_X) == len(_test_y)\n\n            # store features\n            trains_X.append(_train_X)\n            tests_X.append(_test_X)\n            # store labels\n            trains_y.append(_train_y)\n            tests_y.append(_test_y)\n            \n    # combine features for each codes\n    train_X = pd.concat(trains_X)\n    test_X = pd.concat(tests_X)\n    # combine label for each codes\n    train_y = pd.concat(trains_y)\n    test_y = pd.concat(tests_y)\n\n    return train_X, train_y, test_X, test_y","metadata":{"execution":{"iopub.status.busy":"2022-04-05T05:04:33.130698Z","iopub.execute_input":"2022-04-05T05:04:33.131513Z","iopub.status.idle":"2022-04-05T05:04:33.145811Z","shell.execute_reply.started":"2022-04-05T05:04:33.131462Z","shell.execute_reply":"2022-04-05T05:04:33.144591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# generate feature/label\ntrain_X, train_y, test_X, test_y = get_features_and_label(\n    df_price, codes, feature\n)","metadata":{"execution":{"iopub.status.busy":"2022-04-05T05:04:37.979325Z","iopub.execute_input":"2022-04-05T05:04:37.980462Z","iopub.status.idle":"2022-04-05T05:05:17.597308Z","shell.execute_reply.started":"2022-04-05T05:04:37.980402Z","shell.execute_reply":"2022-04-05T05:05:17.59662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n# Building a simple model\nUsing the created features and labels, build a model using the following procedure","metadata":{}},{"cell_type":"code","source":"lgbm_params = {\n    'seed': 42,\n    'n_jobs': -1,\n}\n\nfeat_cols = [\n    \"return_2week\",\n    \"return_1month\",\n    \"return_3month\",\n    \"volatility_2week\",\n    \"volatility_1month\",\n    \"volatility_3month\",\n]","metadata":{"execution":{"iopub.status.busy":"2022-04-05T05:05:24.593783Z","iopub.execute_input":"2022-04-05T05:05:24.594624Z","iopub.status.idle":"2022-04-05T05:05:24.599311Z","shell.execute_reply.started":"2022-04-05T05:05:24.59457Z","shell.execute_reply":"2022-04-05T05:05:24.598547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# initialize model\npred_model = LGBMRegressor(**lgbm_params)\n# train\npred_model.fit(train_X[feat_cols].values, train_y)\n# prepare result data\nresult = test_X[[\"SecuritiesCode\"]].copy()\n# predict\nresult.loc[:, \"predict\"] = pred_model.predict(test_X[feat_cols])\n# actual result\nresult.loc[:, \"Target\"] = test_y.values\n\ndef set_rank(df):\n    \"\"\"\n    Args:\n        df (pd.DataFrame): including predict column\n    Returns:\n        df (pd.DataFrame): df with Rank\n    \"\"\"\n    # sort records to set Rank\n    df = df.sort_values(\"predict\", ascending=False)\n    # set Rank starting from 0\n    df.loc[:, \"Rank\"] = np.arange(len(df[\"predict\"]))\n    return df\n\nresult = result.sort_values([\"Date\", \"predict\"], ascending=[True, False])\nresult = result.groupby(\"Date\").apply(set_rank)","metadata":{"execution":{"iopub.status.busy":"2022-04-05T05:05:33.53743Z","iopub.execute_input":"2022-04-05T05:05:33.537698Z","iopub.status.idle":"2022-04-05T05:05:38.586152Z","shell.execute_reply.started":"2022-04-05T05:05:33.537669Z","shell.execute_reply":"2022-04-05T05:05:38.585171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result.tail()","metadata":{"execution":{"iopub.status.busy":"2022-04-05T05:05:47.138451Z","iopub.execute_input":"2022-04-05T05:05:47.138726Z","iopub.status.idle":"2022-04-05T05:05:47.156915Z","shell.execute_reply.started":"2022-04-05T05:05:47.138695Z","shell.execute_reply":"2022-04-05T05:05:47.155943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n# Evaluation\nInput the output of the forecasts of the constructed model into the evaluation function and plot the daily returns.\n\nThe evaluation function for this competition is as follows.\n\nPlease read [here](https://www.kaggle.com/code/smeitoma/jpx-competition-metric-definition) to know the evaluation function more.","metadata":{}},{"cell_type":"code","source":"def calc_spread_return_sharpe(df: pd.DataFrame, portfolio_size: int = 200, toprank_weight_ratio: float = 2) -> float:\n    \"\"\"\n    Args:\n        df (pd.DataFrame): predicted results\n        portfolio_size (int): # of equities to buy/sell\n        toprank_weight_ratio (float): the relative weight of the most highly ranked stock compared to the least.\n    Returns:\n        (float): sharpe ratio\n    \"\"\"\n    def _calc_spread_return_per_day(df, portfolio_size, toprank_weight_ratio):\n        \"\"\"\n        Args:\n            df (pd.DataFrame): predicted results\n            portfolio_size (int): # of equities to buy/sell\n            toprank_weight_ratio (float): the relative weight of the most highly ranked stock compared to the least.\n        Returns:\n            (float): spread return\n        \"\"\"\n        assert df['Rank'].min() == 0\n        assert df['Rank'].max() == len(df['Rank']) - 1\n        weights = np.linspace(start=toprank_weight_ratio, stop=1, num=portfolio_size)\n        purchase = (df.sort_values(by='Rank')['Target'][:portfolio_size] * weights).sum() / weights.mean()\n        short = (df.sort_values(by='Rank', ascending=False)['Target'][:portfolio_size] * weights).sum() / weights.mean()\n        return purchase - short\n\n    buf = df.groupby('Date').apply(_calc_spread_return_per_day, portfolio_size, toprank_weight_ratio)\n    sharpe_ratio = buf.mean() / buf.std()\n    return sharpe_ratio","metadata":{"execution":{"iopub.status.busy":"2022-04-05T05:05:52.605815Z","iopub.execute_input":"2022-04-05T05:05:52.606116Z","iopub.status.idle":"2022-04-05T05:05:52.615778Z","shell.execute_reply.started":"2022-04-05T05:05:52.606081Z","shell.execute_reply":"2022-04-05T05:05:52.615029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# calc spread return sharpe\ncalc_spread_return_sharpe(result, portfolio_size=200)","metadata":{"execution":{"iopub.status.busy":"2022-04-05T05:06:00.372794Z","iopub.execute_input":"2022-04-05T05:06:00.373087Z","iopub.status.idle":"2022-04-05T05:06:01.291174Z","shell.execute_reply.started":"2022-04-05T05:06:00.373056Z","shell.execute_reply":"2022-04-05T05:06:01.290206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then, we will show daily spread return of the model.","metadata":{}},{"cell_type":"code","source":"def _calc_spread_return_per_day(df, portfolio_size, toprank_weight_ratio):\n    \"\"\"\n    Args:\n        df (pd.DataFrame): predicted results\n        portfolio_size (int): # of equities to buy/sell\n        toprank_weight_ratio (float): the relative weight of the most highly ranked stock compared to the least.\n    Returns:\n        (float): spread return\n    \"\"\"\n    assert df['Rank'].min() == 0\n    assert df['Rank'].max() == len(df['Rank']) - 1\n    weights = np.linspace(start=toprank_weight_ratio, stop=1, num=portfolio_size)\n    purchase = (df.sort_values(by='Rank')['Target'][:portfolio_size] * weights).sum() / weights.mean()\n    short = (df.sort_values(by='Rank', ascending=False)['Target'][:portfolio_size] * weights).sum() / weights.mean()\n    return purchase - short\n\ndf_result = result.groupby('Date').apply(_calc_spread_return_per_day, 200, 2)","metadata":{"execution":{"iopub.status.busy":"2022-04-05T05:06:08.604277Z","iopub.execute_input":"2022-04-05T05:06:08.604587Z","iopub.status.idle":"2022-04-05T05:06:09.522571Z","shell.execute_reply.started":"2022-04-05T05:06:08.604554Z","shell.execute_reply":"2022-04-05T05:06:09.521564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_result.plot(figsize=(20, 8))","metadata":{"execution":{"iopub.status.busy":"2022-04-05T05:06:21.003155Z","iopub.execute_input":"2022-04-05T05:06:21.003473Z","iopub.status.idle":"2022-04-05T05:06:21.336916Z","shell.execute_reply.started":"2022-04-05T05:06:21.003437Z","shell.execute_reply":"2022-04-05T05:06:21.33594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We also show a cumulative spread return of the mode","metadata":{}},{"cell_type":"markdown","source":"df_result.cumsum().plot(figsize=(20, 8))","metadata":{}},{"cell_type":"markdown","source":"The model in this notebook is now complete! Try different features and training methods through trial and error!","metadata":{}},{"cell_type":"markdown","source":"\n# Saving model\nYou need to save your model parameter to use created model for your submission.","metadata":{}},{"cell_type":"code","source":"pred_model.booster_.save_model(\"simple-model.txt\")","metadata":{"execution":{"iopub.status.busy":"2022-04-05T05:06:29.12399Z","iopub.execute_input":"2022-04-05T05:06:29.124308Z","iopub.status.idle":"2022-04-05T05:06:29.137931Z","shell.execute_reply.started":"2022-04-05T05:06:29.124272Z","shell.execute_reply":"2022-04-05T05:06:29.136987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"----\n# Submission\nThe code blocks below come from s-meitoma's [Submission Demo](https://www.kaggle.com/code/smeitoma/submission-demo) notebook.\n\nDon't forget the code requirements from the competition page:\n\n\n   * CPU Notebook <= 9 hours run-time\n   * GPU Notebook <= 9 hours run-time\n   * Internet access disabled\n   * Freely & publicly available external data is allowed, including pre-trained models\n   * Submission file must be named submission.csv. The API will generate this submission file for you.\n\nIf training + submission takes longer than the maximum run-time, you may wish to separate your notebook into a fork which does the training and writes the models (all the code above this) and fork which reads the pre-trained models and submits your work (all the code below this).","metadata":{}},{"cell_type":"code","source":"# model parameters generated by https://www.kaggle.com/ksadhfoia/train\n#model_file = \"../input/simplemodel/simple-model.txt\"\nmodel_file = \"/kaggle/working/simple-model.txt\"","metadata":{"execution":{"iopub.status.busy":"2022-04-05T05:12:00.15396Z","iopub.execute_input":"2022-04-05T05:12:00.154286Z","iopub.status.idle":"2022-04-05T05:12:00.158966Z","shell.execute_reply.started":"2022-04-05T05:12:00.154253Z","shell.execute_reply":"2022-04-05T05:12:00.157805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# feature column names\nfeat_cols = [\n    \"return_2week\",\n    \"return_1month\",\n    \"return_3month\",\n    \"volatility_2week\",\n    \"volatility_1month\",\n    \"volatility_3month\",\n]","metadata":{"execution":{"iopub.status.busy":"2022-04-05T05:06:37.953813Z","iopub.execute_input":"2022-04-05T05:06:37.954094Z","iopub.status.idle":"2022-04-05T05:06:37.958108Z","shell.execute_reply.started":"2022-04-05T05:06:37.954064Z","shell.execute_reply":"2022-04-05T05:06:37.957383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_adjusted_close(df):\n    \"\"\"\n    Args:\n        df (pd.DataFrame)  : stock_price for a single SecuritiesCode\n    Returns:\n        df (pd.DataFrame): stock_price with AdjustedClose for a single SecuritiesCode\n    \"\"\"\n    # sort data to generate CumulativeAdjustmentFactor\n    df = df.sort_values(\"Date\", ascending=False)\n    # generate CumulativeAdjustmentFactor\n    df.loc[:, \"CumulativeAdjustmentFactor\"] = df[\"AdjustmentFactor\"].cumprod()\n    # generate AdjustedClose\n    df.loc[:, \"AdjustedClose\"] = (\n        df[\"CumulativeAdjustmentFactor\"] * df[\"Close\"]\n    ).map(lambda x: float(\n        Decimal(str(x)).quantize(Decimal('0.1'), rounding=ROUND_HALF_UP)\n    ))\n    # reverse order\n    df = df.sort_values(\"Date\")\n    # to fill AdjustedClose, replace 0 into np.nan\n    df.loc[df[\"AdjustedClose\"] == 0, \"AdjustedClose\"] = np.nan\n    # forward fill AdjustedClose\n    df.loc[:, \"AdjustedClose\"] = df.loc[:, \"AdjustedClose\"].ffill()\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-04-05T05:06:43.922081Z","iopub.execute_input":"2022-04-05T05:06:43.923045Z","iopub.status.idle":"2022-04-05T05:06:43.931327Z","shell.execute_reply.started":"2022-04-05T05:06:43.923002Z","shell.execute_reply":"2022-04-05T05:06:43.930293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def adjust_price(price):\n    \"\"\"\n    Args:\n        price (pd.DataFrame)  : pd.DataFrame include stock_price\n    Returns:\n        price DataFrame (pd.DataFrame): stock_price with generated AdjustedClose\n    \"\"\"\n    # copy to edit\n    price = price.copy()\n    # transform Date column into datetime\n    price.loc[: ,\"Date\"] = pd.to_datetime(price.loc[: ,\"Date\"], format=\"%Y-%m-%d\")\n\n    # generate AdjustedClose\n    price = price.sort_values([\"SecuritiesCode\", \"Date\"])\n    price = price.groupby(\"SecuritiesCode\").apply(generate_adjusted_close).reset_index(drop=True)\n\n    price.set_index(\"Date\", inplace=True)\n    return price","metadata":{"execution":{"iopub.status.busy":"2022-04-05T05:06:48.355911Z","iopub.execute_input":"2022-04-05T05:06:48.356215Z","iopub.status.idle":"2022-04-05T05:06:48.362225Z","shell.execute_reply.started":"2022-04-05T05:06:48.35618Z","shell.execute_reply":"2022-04-05T05:06:48.361589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_features_for_predict(price, code):\n    \"\"\"\n    Args:\n        price (pd.DataFrame)  : pd.DataFrame include stock_price\n        code (int)  : A local code for a listed company\n    Returns:\n        feature DataFrame (pd.DataFrame)\n    \"\"\"\n    close_col = \"AdjustedClose\"\n    feats = price.loc[price[\"SecuritiesCode\"] == code, [\"SecuritiesCode\", close_col]].copy()\n\n    # calculate 2 week return using AdjustedClose\n    feats[\"return_2week\"] = feats[close_col].pct_change(10)\n    # calculate last 1 month return using AdjustedClose\n    feats[\"return_1month\"] = feats[close_col].pct_change(21)\n    # calculate last 3 months return using AdjustedClose\n    feats[\"return_3month\"] = feats[close_col].pct_change(63)\n\n    # calculate 2 week historical volatility using AdjustedClose\n    feats[\"volatility_2week\"] = (\n        np.log(feats[close_col]).diff().rolling(10).std()\n    )\n    # calculate last 1 month historical volatility using AdjustedClose\n    feats[\"volatility_1month\"] = (\n        np.log(feats[close_col]).diff().rolling(21).std()\n    )\n    # calculate last 3 months historical volatility using AdjustedClose\n    feats[\"volatility_3month\"] = (\n        np.log(feats[close_col]).diff().rolling(63).std()\n    )\n\n    # filling data for nan and inf\n    feats = feats.fillna(0)\n    feats = feats.replace([np.inf, -np.inf], 0)\n    # drop AdjustedClose column\n    feats = feats.drop([close_col], axis=1)\n\n    return feats","metadata":{"execution":{"iopub.status.busy":"2022-04-05T05:06:52.287577Z","iopub.execute_input":"2022-04-05T05:06:52.288132Z","iopub.status.idle":"2022-04-05T05:06:52.298351Z","shell.execute_reply.started":"2022-04-05T05:06:52.288095Z","shell.execute_reply":"2022-04-05T05:06:52.297109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load stock price data\ndf_price_raw = pd.read_csv(f\"{train_files_dir}/stock_prices.csv\")\nprice_cols = [\n    \"Date\",\n    \"SecuritiesCode\",\n    \"Close\",\n    \"AdjustmentFactor\",\n]\ndf_price_raw = df_price_raw[price_cols]\n\n# filter data to reduce culculation cost \ndf_price_raw = df_price_raw.loc[df_price_raw[\"Date\"] >= \"2021-08-01\"]\n\n# forecasting phase leaderboard:\n# df_price_supplemental = pd.read_csv(f\"{supplemental_files_dir}/stock_prices.csv\")\n# df_price_supplemental = df_price_supplemental[price_cols]\n# df_price_raw = pd.concat([df_price_raw, df_price_supplemental])","metadata":{"execution":{"iopub.status.busy":"2022-04-05T05:06:57.615353Z","iopub.execute_input":"2022-04-05T05:06:57.615723Z","iopub.status.idle":"2022-04-05T05:07:03.074431Z","shell.execute_reply.started":"2022-04-05T05:06:57.61568Z","shell.execute_reply":"2022-04-05T05:07:03.073526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_price_raw.tail(2)","metadata":{"execution":{"iopub.status.busy":"2022-04-05T05:07:06.844217Z","iopub.execute_input":"2022-04-05T05:07:06.844506Z","iopub.status.idle":"2022-04-05T05:07:06.855841Z","shell.execute_reply.started":"2022-04-05T05:07:06.844476Z","shell.execute_reply":"2022-04-05T05:07:06.854992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load pre-trained model\npred_model = Booster(model_file=model_file)","metadata":{"execution":{"iopub.status.busy":"2022-04-05T05:12:06.517234Z","iopub.execute_input":"2022-04-05T05:12:06.517552Z","iopub.status.idle":"2022-04-05T05:12:06.535966Z","shell.execute_reply.started":"2022-04-05T05:12:06.517518Z","shell.execute_reply":"2022-04-05T05:12:06.535231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load Time Series API\nimport jpx_tokyo_market_prediction\n# make Time Series API environment (this function can be called only once in a session)\nenv = jpx_tokyo_market_prediction.make_env()\n# get iterator to fetch data day by day\niter_test = env.iter_test()","metadata":{"execution":{"iopub.status.busy":"2022-04-05T05:12:13.001623Z","iopub.execute_input":"2022-04-05T05:12:13.001971Z","iopub.status.idle":"2022-04-05T05:12:13.031302Z","shell.execute_reply.started":"2022-04-05T05:12:13.001934Z","shell.execute_reply":"2022-04-05T05:12:13.030189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"counter = 0\n# fetch data day by day\nfor (prices, options, financials, trades, secondary_prices, sample_prediction) in iter_test:\n    current_date = prices[\"Date\"].iloc[0]\n    sample_prediction_date = sample_prediction[\"Date\"].iloc[0]\n    print(f\"current_date: {current_date}, sample_prediction_date: {sample_prediction_date}\")\n\n    if counter == 0:\n        # to avoid data leakage\n        df_price_raw = df_price_raw.loc[df_price_raw[\"Date\"] < current_date]\n\n    # filter data to reduce culculation cost\n    threshold = (pd.Timestamp(current_date) - pd.offsets.BDay(80)).strftime(\"%Y-%m-%d\")\n    print(f\"threshold: {threshold}\")\n    df_price_raw = df_price_raw.loc[df_price_raw[\"Date\"] >= threshold]\n\n    # to generate AdjustedClose, increment price data\n    df_price_raw = pd.concat([df_price_raw, prices[price_cols]])\n    # generate AdjustedClose\n    df_price = adjust_price(df_price_raw)\n\n    # get target SecuritiesCodes\n    codes = sorted(prices[\"SecuritiesCode\"].unique())\n\n    # generate feature\n    feature = pd.concat([get_features_for_predict(df_price, code) for code in codes])\n    # filter feature for this iteration\n    feature = feature.loc[feature.index == current_date]\n\n    # prediction\n    feature.loc[:, \"predict\"] = pred_model.predict(feature[feat_cols])\n\n    # set rank by predict\n    feature = feature.sort_values(\"predict\", ascending=False).drop_duplicates(subset=['SecuritiesCode'])\n    feature.loc[:, \"Rank\"] = np.arange(len(feature))\n    feature_map = feature.set_index('SecuritiesCode')['Rank'].to_dict()\n    sample_prediction['Rank'] = sample_prediction['SecuritiesCode'].map(feature_map)\n\n    # check Rank\n    assert sample_prediction[\"Rank\"].notna().all()\n    assert sample_prediction[\"Rank\"].min() == 0\n    assert sample_prediction[\"Rank\"].max() == len(sample_prediction[\"Rank\"]) - 1\n\n    # register your predictions\n    env.predict(sample_prediction)\n    counter += 1","metadata":{"execution":{"iopub.status.busy":"2022-04-05T05:12:16.978034Z","iopub.execute_input":"2022-04-05T05:12:16.978377Z","iopub.status.idle":"2022-04-05T05:13:14.799257Z","shell.execute_reply.started":"2022-04-05T05:12:16.978312Z","shell.execute_reply":"2022-04-05T05:13:14.798096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! head submission.csv","metadata":{"execution":{"iopub.status.busy":"2022-04-05T05:13:35.497287Z","iopub.execute_input":"2022-04-05T05:13:35.497597Z","iopub.status.idle":"2022-04-05T05:13:36.269949Z","shell.execute_reply.started":"2022-04-05T05:13:35.497566Z","shell.execute_reply":"2022-04-05T05:13:36.269057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! tail submission.csv","metadata":{"execution":{"iopub.status.busy":"2022-04-05T05:13:40.061156Z","iopub.execute_input":"2022-04-05T05:13:40.061499Z","iopub.status.idle":"2022-04-05T05:13:40.837991Z","shell.execute_reply.started":"2022-04-05T05:13:40.061462Z","shell.execute_reply":"2022-04-05T05:13:40.836773Z"},"trusted":true},"execution_count":null,"outputs":[]}]}