{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"padding:20px;color:#76B900;margin:0;font-size:240%;text-align:center;display:fill;border-radius:5px;background-color:white;overflow:hidden;font-weight:600\">JPX Tokyo Stock Exchange Prediction with NVIDIA-TSPP</div>\n\nThis notebook shows how to use NVIDIA Time Series Prediction Platform. ","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","scrolled":true,"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-07T05:08:03.607163Z","iopub.execute_input":"2022-06-07T05:08:03.607404Z","iopub.status.idle":"2022-06-07T05:08:04.029654Z","shell.execute_reply.started":"2022-06-07T05:08:03.607334Z","shell.execute_reply":"2022-06-07T05:08:04.028921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <div style=\"padding:20px;color:#76B900;margin:0;font-size:100%;text-align:left;display:fill;border-radius:5px;background-color:#5E5E5E;overflow:hidden\">1 | About NVIDIA-TSPP</div>\n<br>\nNVIDIA's Time Series Prediction Platform (NVIDIA-TSPP) enables users to mix and match time-series datasets and models. In this case, the user has complete control over the following settings, and can compare side-by-side results obtained from various solutions. \n","metadata":{}},{"cell_type":"markdown","source":"![TSPP Features](https://i.imgur.com/YlwY9Td.png)","metadata":{}},{"cell_type":"markdown","source":"\n1. NVIDIA's Time Series Prediction Platform ([NVIDIA-TSPP](https://github.com/NVIDIA/DeepLearningExamples/tree/master/Tools/PyTorch/TimeSeriesPredictionPlatform)) attempts to unify all time-series models under one evaluation framework <br>\n2. Enables users to mix and match time-series datasets and models <br>\n3. Pytorch Framework <br>\n4. Modularized components (dataset, model, loss, evaluation metrics, etc) <br>\n5. Allows for common point(s) of comparison <br>\n6. Comparisons should be low-effort <br>\n7. Easily swap in your own dataset to compare portfolio modelsâ€™ accuracy <br>\n8. Easily swap in your model to compare its accuracy across datasets <br>\n9. Supports Triton based deployment <br>\n10. Enables quick experimentation with Multi-GPU and AMP <br>\n11. Optimized for Command-Line Interface with [hydra](https://hydra.cc/) <br>\n<br>\nThe NVIDIA-TSPP uses hydra to support experiment configuration. Hydra makes selecting models, datasets, training and evaluation parameters very easy<br>\n**NOTE: NVIDIA-TSPP code in this notebook is a development version. We will soon have an official update available on the [Nvidia Deep Learning Examples github](https://github.com/NVIDIA/DeepLearningExamples/tree/master/Tools/PyTorch/TimeSeriesPredictionPlatform)**","metadata":{}},{"cell_type":"markdown","source":"# <div style=\"padding:20px;color:#76B900;margin:0;font-size:100%;text-align:left;display:fill;border-radius:5px;background-color:#5E5E5E;overflow:hidden\">2 | Setup and Installations</div>\n","metadata":{}},{"cell_type":"markdown","source":"This section covers the installation, setup, and imports. Given this is a code competition we uploaded the NVIDIA-TSPP code in a dataset. We also uploaded all required packages in a dataset to allow an offline pip install. Code for installation is hidden for the sake of clarity but you certainly can have a look if you are interested.","metadata":{}},{"cell_type":"markdown","source":"# <div style=\"padding:10px;color:#76B900;margin:0;font-size:60%;text-align:left;display:fill;border-radius:5px;background-color:#5E5E5E;overflow:hidden\">Installing NVIDIA-TSPP's libraries - Offline</div>","metadata":{}},{"cell_type":"markdown","source":"Install libraries required by NVIDIA-TSPP","metadata":{}},{"cell_type":"code","source":"!ls ../input/nvidia-tspp-libraries/offline_whls/","metadata":{"scrolled":true,"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-06T05:16:40.513362Z","iopub.execute_input":"2022-06-06T05:16:40.513821Z","iopub.status.idle":"2022-06-06T05:16:41.267029Z","shell.execute_reply.started":"2022-06-06T05:16:40.513784Z","shell.execute_reply":"2022-06-06T05:16:41.266074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install --no-index ../input/nvidia-tspp-libraries/offline_whls/hydra_core-1.1.1-py3-none-any.whl --find-links=../input/nvidia-tspp-libraries/offline_whls/\n!pip install --no-index ../input/nvidia-tspp-libraries/offline_whls/patool-1.12-py2.py3-none-any.whl --find-links=../input/nvidia-tspp-libraries/offline_whls/\n!pip install --no-index ../input/nvidia-tspp-libraries/offline_whls/optuna_dashboard-0.6.4-py3-none-any.whl --find-links=../input/nvidia-tspp-libraries/offline_whls/\n!pip install --no-index ../input/nvidia-tspp-libraries/offline_whls/hydra_optuna_sweeper-1.1.2-py3-none-any.whl --find-links=../input/nvidia-tspp-libraries/offline_whls/\n!pip install --no-index ../input/nvidia-tspp-libraries/offline_whls/hydra_joblib_launcher-1.1.5-py3-none-any.whl --find-links=../input/nvidia-tspp-libraries/offline_whls/\n!pip install --no-index ../input/nvidia-tspp-libraries/offline_whls/nvidia-pyindex-1.0.9.tar.gz --find-links=../input/nvidia-tspp-libraries/offline_whls/\n!pip install --no-index ../input/nvidia-tspp-libraries/offline_whls/dgl-0.6.1-cp37-cp37m-manylinux1_x86_64.whl --find-links=../input/nvidia-tspp-libraries/offline_whls/\n!pip install --no-index ../input/nvidia-tspp-libraries/offline_whls/dask_cuda-22.2.0-py3-none-any.whl --find-links=../input/nvidia-tspp-libraries/offline_whls/\n!pip install --no-index ../input/nvidia-tspp-libraries/offline_whls/DLLogger-1.0.0.zip --find-links=../input/nvidia-tspp-libraries/offline_whls/\n!pip install --no-index ../input/nvidia-tspp-libraries/offline_whls/dtw_python-1.1.12-cp37-cp37m-manylinux2010_x86_64.whl --find-links=../input/nvidia-tspp-libraries/offline_whls/\n!pip install --no-index ../input/nvidia-tspp-libraries/offline_whls/mlflow-1.26.1-py3-none-any.whl --find-links=../input/nvidia-tspp-libraries/offline_whls/\n!pip install --no-index ../input/nvidia-tspp-libraries/offline_whls/pmdarima-1.8.0-cp37-cp37m-manylinux1_x86_64.whl --find-links=../input/nvidia-tspp-libraries/offline_whls/","metadata":{"scrolled":true,"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-07T05:08:16.62114Z","iopub.execute_input":"2022-06-07T05:08:16.621389Z","iopub.status.idle":"2022-06-07T05:10:39.019101Z","shell.execute_reply.started":"2022-06-07T05:08:16.621361Z","shell.execute_reply":"2022-06-07T05:10:39.018281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Adding NVIDIA-TSPP code to the search path","metadata":{}},{"cell_type":"code","source":"import sys\ntspp_ws = \"/kaggle/input/nvidia-tspp-code/nvidia_tspp_code\"\nsys.path.insert(1, tspp_ws)\nsys.path","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-07T05:10:46.210811Z","iopub.execute_input":"2022-06-07T05:10:46.211316Z","iopub.status.idle":"2022-06-07T05:10:46.219564Z","shell.execute_reply.started":"2022-06-07T05:10:46.211275Z","shell.execute_reply":"2022-06-07T05:10:46.218719Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Imports","metadata":{}},{"cell_type":"code","source":"import os\nfrom decimal import ROUND_HALF_UP, Decimal\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport datetime\nimport hydra\nimport warnings\nimport torch\nfrom omegaconf import OmegaConf\nimport os\nimport pickle\nfrom hydra.utils import get_original_cwd\nimport conf.conf_utils\nfrom hydra_utils import get_config\nfrom data.data_utils import Preprocessor\nfrom training.utils import set_seed\n\ncurr_workdir = globals()['_dh'][0]\nconfig_path = \"./conf\"\nwarnings.filterwarnings(\"ignore\")","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-07T05:10:50.39058Z","iopub.execute_input":"2022-06-07T05:10:50.391139Z","iopub.status.idle":"2022-06-07T05:10:53.042908Z","shell.execute_reply.started":"2022-06-07T05:10:50.391097Z","shell.execute_reply":"2022-06-07T05:10:53.040603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <div style=\"padding:20px;color:#76B900;margin:0;font-size:100%;text-align:left;display:fill;border-radius:5px;background-color:#5E5E5E;overflow:hidden\">3 | Feature Engineering</div>","metadata":{}},{"cell_type":"markdown","source":"In this section, we will review the dataset and create features for training. Our features are various aggregates on lag variables. We provide these as examples, and these could certainly be modified and improved.","metadata":{}},{"cell_type":"markdown","source":"Load stock_prices training dataset","metadata":{}},{"cell_type":"code","source":"dataset_path = \"/kaggle/input/jpx-tokyo-stock-exchange-prediction\"\ntrain_sp_file = \"train_files/stock_prices.csv\"\ntrain_sp_file = os.path.join(dataset_path, train_sp_file)","metadata":{"execution":{"iopub.status.busy":"2022-06-07T05:10:57.113698Z","iopub.execute_input":"2022-06-07T05:10:57.114183Z","iopub.status.idle":"2022-06-07T05:10:57.118557Z","shell.execute_reply.started":"2022-06-07T05:10:57.114144Z","shell.execute_reply":"2022-06-07T05:10:57.117673Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(train_sp_file)\nprint(df.head())","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-07T05:10:57.986415Z","iopub.execute_input":"2022-06-07T05:10:57.987267Z","iopub.status.idle":"2022-06-07T05:11:05.93114Z","shell.execute_reply.started":"2022-06-07T05:10:57.987217Z","shell.execute_reply":"2022-06-07T05:11:05.930325Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Cell below checks if the all the 2000 stocks are available on all dates","metadata":{}},{"cell_type":"code","source":"%%script false --no-raise-error\nidcount = df.groupby(\"Date\")[\"SecuritiesCode\"].count().reset_index()\nplt.plot(idcount[\"Date\"],idcount[\"SecuritiesCode\"])\nidcount.loc[idcount[\"SecuritiesCode\"]==2000,:]","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-07T05:11:08.605137Z","iopub.execute_input":"2022-06-07T05:11:08.605391Z","iopub.status.idle":"2022-06-07T05:11:08.634559Z","shell.execute_reply.started":"2022-06-07T05:11:08.605362Z","shell.execute_reply":"2022-06-07T05:11:08.633661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will update the prices using AdjustmentFactor value. This should reduce historical price gap caused by split/reverse-split. <br>\n","metadata":{}},{"cell_type":"code","source":"def adjust_price(price):\n    \"\"\"\n    Ref: https://www.kaggle.com/code/smeitoma/train-demo#Generating-AdjustedClose-price\n    Args:\n        price (pd.DataFrame)  : pd.DataFrame include stock_price\n    Returns:\n        price DataFrame (pd.DataFrame): stock_price with generated Adjusted Prices\n    \"\"\"\n\n    def generate_adjusted_price(df):\n        \"\"\"\n        Args:\n            df (pd.DataFrame)  : stock_price for a single SecuritiesCode\n        Returns:\n            df (pd.DataFrame): stock_price with Adjusted Price for a single SecuritiesCode\n        \"\"\"\n        # sort data to generate CumulativeAdjustmentFactor\n        df = df.sort_values(\"Date\", ascending=False)\n        # generate CumulativeAdjustmentFactor\n        df.loc[:, \"CumulativeAdjustmentFactor\"] = df[\"AdjustmentFactor\"].cumprod()\n        # generate AdjustedClose\n        df.loc[:, \"AdjustedClose\"] = (\n            df[\"CumulativeAdjustmentFactor\"] * df[\"Close\"]\n        ).map(lambda x: float(\n            Decimal(str(x)).quantize(Decimal('0.1'), rounding=ROUND_HALF_UP)\n        ))\n        df.loc[:, \"AdjustedOpen\"] = (\n            df[\"CumulativeAdjustmentFactor\"] * df[\"Open\"]\n        ).map(lambda x: float(\n            Decimal(str(x)).quantize(Decimal('0.1'), rounding=ROUND_HALF_UP)\n        ))\n        df.loc[:, \"AdjustedLow\"] = (\n            df[\"CumulativeAdjustmentFactor\"] * df[\"Low\"]\n        ).map(lambda x: float(\n            Decimal(str(x)).quantize(Decimal('0.1'), rounding=ROUND_HALF_UP)\n        ))\n        df.loc[:, \"AdjustedHigh\"] = (\n            df[\"CumulativeAdjustmentFactor\"] * df[\"High\"]\n        ).map(lambda x: float(\n            Decimal(str(x)).quantize(Decimal('0.1'), rounding=ROUND_HALF_UP)\n        ))\n        # reverse order\n        df = df.sort_values(\"Date\")\n        # to fill AdjustedClose, replace 0 into np.nan\n        df.loc[df[\"AdjustedClose\"] == 0, \"AdjustedClose\"] = np.nan\n        # forward fill AdjustedClose\n        df.loc[:, \"AdjustedClose\"] = df.loc[:, \"AdjustedClose\"].ffill()\n\n        df.loc[df[\"AdjustedOpen\"] == 0, \"AdjustedOpen\"] = np.nan\n        df.loc[:, \"AdjustedOpen\"] = df.loc[:, \"AdjustedOpen\"].ffill()\n\n        df.loc[df[\"AdjustedLow\"] == 0, \"AdjustedLow\"] = np.nan\n        df.loc[:, \"AdjustedLow\"] = df.loc[:, \"AdjustedLow\"].ffill()\n        \n        df.loc[df[\"AdjustedHigh\"] == 0, \"AdjustedHigh\"] = np.nan\n        df.loc[:, \"AdjustedHigh\"] = df.loc[:, \"AdjustedHigh\"].ffill()\n        \n        return df\n\n    # generate AdjustedClose\n    price = price.sort_values([\"SecuritiesCode\", \"Date\"])\n    price = price.groupby(\"SecuritiesCode\").apply(generate_adjusted_price).reset_index(drop=True)\n\n    return price","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-07T05:11:09.857156Z","iopub.execute_input":"2022-06-07T05:11:09.857665Z","iopub.status.idle":"2022-06-07T05:11:09.873509Z","shell.execute_reply.started":"2022-06-07T05:11:09.857598Z","shell.execute_reply":"2022-06-07T05:11:09.872671Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will create a new set of features with Adjusted Close Price: mean, std, min, and max each over a period of 5 and 20 days respectively.","metadata":{}},{"cell_type":"code","source":"def create_features(price):\n    \"\"\"\n    Args:\n        price (pd.DataFrame)  : pd.DataFrame include stock_price\n    Returns:\n        price DataFrame (pd.DataFrame): stock_price with new generated features\n    \"\"\"\n\n    def generate_features_single_stock(df):\n        \"\"\"\n        Args:\n            df (pd.DataFrame)  : stock_price for a single SecuritiesCode\n        Returns:\n            df (pd.DataFrame): stock_price with new features for a single SecuritiesCode\n        \"\"\"\n        \n        df['Close_1week_mean'] = df['AdjustedClose'].rolling(window = 5).mean().fillna(0)\n        df['Close_4weeks_mean'] = df['AdjustedClose'].rolling(window = 20).mean().fillna(0)\n        df['Close_1week_std'] = df['AdjustedClose'].rolling(window = 5).std().fillna(0)\n        df['Close_4weeks_std'] = df['AdjustedClose'].rolling(window = 20).std().fillna(0)\n        df['Close_1week_min'] = df['AdjustedClose'].rolling(window = 5).min().fillna(0)\n        df['Close_4weeks_min'] = df['AdjustedClose'].rolling(window = 20).min().fillna(0)\n        df['Close_1week_max'] = df['AdjustedClose'].rolling(window = 5).max().fillna(0)\n        df['Close_4weeks_max'] = df['AdjustedClose'].rolling(window = 20).max().fillna(0)\n        return df\n\n    price = price.sort_values([\"SecuritiesCode\", \"Date\"])\n    price = price.groupby(\"SecuritiesCode\").apply(generate_features_single_stock).reset_index(drop=True)\n\n    return price","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-07T05:11:12.563331Z","iopub.execute_input":"2022-06-07T05:11:12.563607Z","iopub.status.idle":"2022-06-07T05:11:12.573773Z","shell.execute_reply.started":"2022-06-07T05:11:12.563576Z","shell.execute_reply":"2022-06-07T05:11:12.57299Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The NVIDIA-TSPP provides download functions for a number of datasets. For some datasets, it supports an end-2-end flow starting from automatic download all the way to creating train, valid and test binaries (for example: [electricity dataset](http://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014)). For other datasets, it expects users to download the dataset and from there, the NVIDIA-TSPP will do the remaining steps <br>\nFor downloading and cleaning up the datasets, the NVIDIA-TSPP provides the script: <code>{tspp_ws}/data/script_download_data.py --dataset {dataset_name} --output_dir {output_dir_path} </code>\nThis script generates the output at the directory: <code>{output_dir}/{dataset}</code> <br>\nHere, since we already have the dataset, we define a function for creating dataset features for NVIDIA-TSPP's preprocessing\n","metadata":{}},{"cell_type":"code","source":"def process_raw_data(df, min_timestamp=None):\n    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n    df['DayOfWeek'] = df['Date'].apply(lambda x: x.dayofweek)\n    df['Month'] = df['Date'].apply(lambda x: x.month)\n    df['Day'] = df['Date'].apply(lambda x: x.day)\n    df['Hour'] = df['Date'].apply(lambda x: x.hour)\n    df['Minute'] = df['Date'].apply(lambda x: x.minute)\n    df['Timestamp'] = df['Date']\n    df['id'] = df[\"SecuritiesCode\"]\n    df['Weight'] = 1\n    df = adjust_price(df)\n    df = create_features(df)\n    df['DayOfYear'] = df['Date'].apply(lambda x: x.dayofyear)\n    df['Year'] = df['Date'].apply(lambda x: x.year)\n    df['IsMonday'] = (df['DayOfWeek'] == 0).astype(int)\n    # Add continuous timeline\n    if min_timestamp:\n        min_timestamp = pd.to_datetime(min_timestamp)\n    else:\n        min_timestamp = min(df['Timestamp'])\n    df['Timestep'] = df['Timestamp'].apply(lambda x: (x-min_timestamp)/datetime.timedelta(days=1))\n    df['Timestep_id'] = df['Timestep']\n    df = df.set_index('Date')\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-06-07T05:11:15.24704Z","iopub.execute_input":"2022-06-07T05:11:15.2473Z","iopub.status.idle":"2022-06-07T05:11:15.256584Z","shell.execute_reply.started":"2022-06-07T05:11:15.247265Z","shell.execute_reply":"2022-06-07T05:11:15.25592Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create output directory for dataset","metadata":{}},{"cell_type":"code","source":"out_dataset_dir = \"/kaggle/working/dataset\"\nos.makedirs(out_dataset_dir, exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-07T05:11:16.078424Z","iopub.execute_input":"2022-06-07T05:11:16.078705Z","iopub.status.idle":"2022-06-07T05:11:16.083127Z","shell.execute_reply.started":"2022-06-07T05:11:16.078673Z","shell.execute_reply":"2022-06-07T05:11:16.082093Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note that we can model missing stocks too, because NVIDIA-TSPP maintains a separate scaler per stock id. We went for simplicity here.","metadata":{}},{"cell_type":"markdown","source":"Select stock prices for dates from 2021-01-01 onwards and save the updated features dataframe","metadata":{}},{"cell_type":"code","source":"train_df = df.loc[df[\"Date\"]>= \"2021-01-01\"].reset_index(drop=True)\ntrain_df = process_raw_data(train_df)\ntrain_df.to_csv(os.path.join(out_dataset_dir, 'filtered_stock_prices.csv'))","metadata":{"execution":{"iopub.status.busy":"2022-06-07T05:11:17.523232Z","iopub.execute_input":"2022-06-07T05:11:17.523493Z","iopub.status.idle":"2022-06-07T05:12:39.988325Z","shell.execute_reply.started":"2022-06-07T05:11:17.523465Z","shell.execute_reply":"2022-06-07T05:12:39.987623Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <div style=\"padding:20px;color:#76B900;margin:0;font-size:100%;text-align:left;display:fill;border-radius:5px;background-color:#5E5E5E;overflow:hidden\">4 | Dataset Preprocessing</div>\n","metadata":{}},{"cell_type":"markdown","source":"In this section, we will preprocess the dataset for NVIDIA-TSPP ","metadata":{}},{"cell_type":"markdown","source":"# <div style=\"padding:10px;color:#76B900;margin:0;font-size:60%;text-align:left;display:fill;border-radius:5px;background-color:#5E5E5E;overflow:hidden\">NVIDIA-TSPP dataset parameters</div>","metadata":{}},{"cell_type":"markdown","source":"![TSPP Features](https://i.imgur.com/3jRx1Ad.png)","metadata":{}},{"cell_type":"markdown","source":"As NVIDIA-TSPP works with yaml-based configuration files, we need to create a configuration file for the competition dataset <br>\nCell below will print the configuration file for the electricity dataset","metadata":{}},{"cell_type":"code","source":"with open(os.path.join(tspp_ws, \"conf/dataset/electricity.yaml\"), \"rb\") as f:\n    elec_config = OmegaConf.load(f)\n    print(\"\\nDataset Config\\n\", OmegaConf.to_yaml(elec_config))","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-07T05:12:53.104688Z","iopub.execute_input":"2022-06-07T05:12:53.105222Z","iopub.status.idle":"2022-06-07T05:12:53.138629Z","shell.execute_reply.started":"2022-06-07T05:12:53.105184Z","shell.execute_reply":"2022-06-07T05:12:53.137934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Some of the relevant fields are described below:","metadata":{}},{"cell_type":"markdown","source":"\n1. <code>config.source_path</code>: path to the source CSV file that contains the dataset\n2. <code>config.dest_path</code>: output directory path for NVIDIA-TSPP's preprocessing\n3. <code>config.time_ids</code>: The name of the column within the source CSV that is used to split training, validation, and test datasets. In the configuration, we can specify the range as shown above using: <code>config.train_range</code> for the train dataset, the example above has the range \\[0, 1315). Similarly we can use <code>config.valid_range</code> & <code>config.test_range</code> for the validation and test sets respectively. Remember that there can be overlap between subsets since predicting the first â€˜unseen elementâ€™ requires the input of the seen elements before it.\n4. <code>config.dataset_stride</code>: This is used to define how far apart in time different examples are in the dataset. When set to 1, all nearest examples will differ only by one time step\n5. <code>config.scale_per_id</code>: If true, preprocessing uses different scaling factors and biases for each of the time-series in the dataset\n6. <code>config.encoder_length</code>: The length of data known up until the â€˜presentâ€™ for a single sample\n7. <code>config.example_length</code>: The length of all data, including data known into the future for a single sample. The target you are predicting lies on the difference between the example_length and encoder_length.\n8. <code>config.input_length</code>: same as encoder_length\n9. <code>config.features</code>: We will go over features and their properties in a bit more detail in the next cell.\n10. <code>config.train_samples</code>, <code>config.valid_samples</code>: Randomly subsample train and valid splits to reduce amount of correlated data fed to the model during a training epoch\n11. <code>config.binarized</code>: Store train, validation and test splits in binarized versions to improve the data-loading speed (by default csv files are created)\n12. <code>config.time_series_count</code>: The number of unique time-series contained in the dataset\n13. <code>config.graph</code>: option is relevant for models that can use graph related information. For example, time-series forecasting using gnns\n14. <code>config.MultiID</code>: option is relevant for models that can use multiple time-series as inputs or predict multiple time-series in a single step ","metadata":{"_kg_hide-input":true}},{"cell_type":"markdown","source":"Feature Specification in Dataset configuration files: <br>","metadata":{}},{"cell_type":"markdown","source":"\nThe NVIDIA-TSPP requires a feature list for each dataset that the models take as input.  Each feature should be represented by an object containing descriptive attributes based on the supported types. These are used for mapping input dataset columns to types. Different types of features are treated differently during preprocessing and model runtime. <br><br>Each feature should have atleast: <br> \n<code>config.features.feature_type</code>: OPTIONS(**ID, KNOWN, STATIC, OBSERVED, TARGET, WEIGHT, TIME**) <br><br>\n**ID**: Unique Identifier representing a time-series node/channel/sensor. Example: Securities Code <br>\n**KNOWN**: Features known in advance for both history and future. Example: Hour, day of week, etc <br>\n**STATIC**: Constant feature throughout a single time series. Ex: physical location of a sensor <br>\n**OBSERVED**: Features known only for historical data <br>\n**TARGET**: Target value to predict. Use history target value as model's input. Use Future targets for calculating loss. Example: Power Usage, Stock Closing Price <br>\n**WEIGHT**: Some of the target values could be missing from the dataset for some timesteps. This can be used to mask those entries during loss calculation. <br>\n**TIME**: Unique timestamp to create the time-series. Example: Hours from Start, Time Steps <br>\n<br>and <br>\n<code>config.features.feature_embed_type</code>: OPTIONS(**CATEGORICAL, CONTINUOUS**) <br><br>\n**CATEGORICAL**: Features that usually have a fixed number possible values. For some models, they can be passed through categorical embedding tables that are trainable. Categorical columns should have a cardinality attribute that represents the number of unique values that the feature takes using <code>config.features.cardinality</code>. Cardinality is 1+number of categories where extra category is for supporting NaNs. <br>\n**CONTINUOUS**: Represents real-valued features.  Continuous features may have a scaler attribute that represents the type of scaler used in preprocessing using <code>config.features.scaler</code> <br>\n<br>\n<b>Required features are one TIME feature, at least one ID feature, one TARGET feature, and at least one KNOWN, OBSERVED, or STATIC feature. </b><br>\n<br>\nA single example is a dictionary of features grouped into the categories specified above. The Dataset class provides a utility to iterate over a monolithic csv (or it's binarized version) creating examples on the fly. The NVIDIA-TSPP assumes that a model doesn't need any more information than the type of a group and the underlying feature tensor.","metadata":{"_kg_hide-input":true}},{"cell_type":"markdown","source":"# <div style=\"padding:10px;color:#76B900;margin:0;font-size:60%;text-align:left;display:fill;border-radius:5px;background-color:#5E5E5E;overflow:hidden\">Create a new dataset configuration file</div>\n\nIn the cell below, we will start with the electricity dataset configuration that is already available within NVIDIA-TSPP's code and make changes to it for this competition's dataset <br>","metadata":{}},{"cell_type":"code","source":"# CREATE DATASET CONFIG HERE\nconfig = None\nout_conf_dir = \"/kaggle/working/custom_conf/dataset\"\nos.makedirs(out_conf_dir, exist_ok=True)\nwith open(os.path.join(tspp_ws, \"conf/dataset/electricity.yaml\"), \"rb\") as f:\n    config = OmegaConf.load(f)\n#print(config)\nconfig.config.source_path = os.path.join(out_dataset_dir, 'filtered_stock_prices.csv')\nconfig.config.dest_path = out_dataset_dir\nconfig.config.time_ids = 'Timestep_id'\n\nconfig.config.encoder_length = 10\nconfig.config.example_length = 10+2\nconfig.config.input_length = 10\n\n# This feature is using by xgboost model only: Its similar to setting encoder_length of 10 xgboost\nconfig.config.lag_features=[{'name': 'AdjustedClose', 'min_value': 1, 'max_value': 10}]\n\n# Train-Valid-Test Split\nnum_samples = len(train_df[\"Timestep_id\"].unique())\nnum_test = round(num_samples * 0.1)\nnum_train = round(num_samples * 0.8)\nnum_val = num_samples - num_test - num_train\ntimesteps = train_df[\"Timestep_id\"].unique()\ntrain_start = int(timesteps[0])\ntrain_end = int(timesteps[num_train])\nvalid_start = int(timesteps[num_train - config.config.encoder_length])\nvalid_end = int(timesteps[num_train + num_val])\ntest_start = int(timesteps[num_train + num_val - config.config.encoder_length])\ntest_end = int(timesteps[-1]+1)\nprint(F\"Train Range: [{train_start}, {train_end})\")\nprint(F\"Valid Range: [{valid_start}, {valid_end})\")\nprint(F\"Test Range: [{test_start}, {test_end})\")\nconfig.config.train_range = [train_start, train_end]\nconfig.config.valid_range = [valid_start, valid_end]\nconfig.config.test_range = [test_start, test_end]\n\nconfig.config.scale_per_id = True\n#Generate Feature Spec\n\ntime_series_count = len(train_df[\"SecuritiesCode\"].unique())\n\nfeatures = [\n    {'name': 'id', 'feature_type': 'ID', 'feature_embed_type': 'CATEGORICAL', 'cardinality': time_series_count+1},\n    {'name': 'Timestep', 'feature_type': 'TIME', 'feature_embed_type': 'CONTINUOUS'},\n    {'name': 'Weight', 'feature_type': 'WEIGHT', 'feature_embed_type': 'CONTINUOUS'},\n    {'name': 'AdjustedClose', 'feature_type': 'TARGET', 'feature_embed_type': 'CONTINUOUS', 'scaler': {'_target_': 'sklearn.preprocessing.StandardScaler'}},\n    {'name': 'DayOfWeek', 'feature_type': 'KNOWN', 'feature_embed_type': 'CATEGORICAL', 'cardinality': 6},\n    {'name': 'Timestep', 'feature_type': 'KNOWN', 'feature_embed_type': 'CONTINUOUS', 'scaler': {'_target_': 'sklearn.preprocessing.StandardScaler'}},\n    {'name': 'Month', 'feature_type': 'KNOWN', 'feature_embed_type': 'CATEGORICAL', 'cardinality': 13},\n    {'name': 'Day', 'feature_type': 'KNOWN', 'feature_embed_type': 'CATEGORICAL', 'cardinality': 32},\n    {'name': 'AdjustedOpen', 'feature_type': 'OBSERVED', 'feature_embed_type': 'CONTINUOUS', 'scaler': {'_target_': 'sklearn.preprocessing.StandardScaler'}},\n    {'name': 'AdjustedHigh', 'feature_type': 'OBSERVED', 'feature_embed_type': 'CONTINUOUS', 'scaler': {'_target_': 'sklearn.preprocessing.StandardScaler'}},\n    {'name': 'AdjustedLow', 'feature_type': 'OBSERVED', 'feature_embed_type': 'CONTINUOUS', 'scaler': {'_target_': 'sklearn.preprocessing.StandardScaler'}},\n    {'name': 'Volume', 'feature_type': 'OBSERVED', 'feature_embed_type': 'CONTINUOUS', 'scaler': {'_target_': 'sklearn.preprocessing.StandardScaler'}},\n    {'name': 'id', 'feature_type': 'STATIC', 'feature_embed_type': 'CATEGORICAL', 'cardinality': time_series_count+1},\n    {'name': 'IsMonday', 'feature_type': 'KNOWN', 'feature_embed_type': 'CATEGORICAL', 'cardinality': 3},\n    {'name': 'Close_1week_mean', 'feature_type': 'OBSERVED', 'feature_embed_type': 'CONTINUOUS', 'scaler': {'_target_': 'sklearn.preprocessing.StandardScaler'}},\n    {'name': 'Close_1week_std', 'feature_type': 'OBSERVED', 'feature_embed_type': 'CONTINUOUS', 'scaler': {'_target_': 'sklearn.preprocessing.StandardScaler'}},\n    {'name': 'Close_1week_min', 'feature_type': 'OBSERVED', 'feature_embed_type': 'CONTINUOUS', 'scaler': {'_target_': 'sklearn.preprocessing.StandardScaler'}},\n    {'name': 'Close_1week_max', 'feature_type': 'OBSERVED', 'feature_embed_type': 'CONTINUOUS', 'scaler': {'_target_': 'sklearn.preprocessing.StandardScaler'}},\n    {'name': 'Close_4weeks_mean', 'feature_type': 'OBSERVED', 'feature_embed_type': 'CONTINUOUS', 'scaler': {'_target_': 'sklearn.preprocessing.StandardScaler'}},\n    {'name': 'Close_4weeks_std', 'feature_type': 'OBSERVED', 'feature_embed_type': 'CONTINUOUS', 'scaler': {'_target_': 'sklearn.preprocessing.StandardScaler'}},\n    {'name': 'Close_4weeks_min', 'feature_type': 'OBSERVED', 'feature_embed_type': 'CONTINUOUS', 'scaler': {'_target_': 'sklearn.preprocessing.StandardScaler'}},\n    {'name': 'Close_4weeks_max', 'feature_type': 'OBSERVED', 'feature_embed_type': 'CONTINUOUS', 'scaler': {'_target_': 'sklearn.preprocessing.StandardScaler'}},\n    {'name': 'DayOfYear', 'feature_type': 'KNOWN', 'feature_embed_type': 'CONTINUOUS', 'scaler': {'_target_': 'sklearn.preprocessing.StandardScaler'}},\n    {'name': 'Year', 'feature_type': 'KNOWN', 'feature_embed_type': 'CONTINUOUS', 'scaler': {'_target_': 'sklearn.preprocessing.StandardScaler'}},\n]\n    \nconfig.config.features = features\nconfig.config.time_series_count = time_series_count\nconfig.config.train_samples = -1\nconfig.config.valid_samples = -1\n\nwith open(os.path.join(out_conf_dir, \"jpx.yaml\"), \"wb\") as f:\n    OmegaConf.save(config=config, f=f.name)\n\nprint(config)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-07T05:13:02.968535Z","iopub.execute_input":"2022-06-07T05:13:02.968993Z","iopub.status.idle":"2022-06-07T05:13:03.057898Z","shell.execute_reply.started":"2022-06-07T05:13:02.968955Z","shell.execute_reply":"2022-06-07T05:13:03.056317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <div style=\"padding:10px;color:#76B900;margin:0;font-size:60%;text-align:left;display:fill;border-radius:5px;background-color:#5E5E5E;overflow:hidden\">Run Dataset Preprocessing</div>\n\nCell below will generate train.csv, valid.csv and test.csv (also .bin files if <code>config.binarized</code> is set to True) at the output directory set with option: <code>config.dest_path</code> <br>\n<code>hydra.searchpath</code> is required as the dataset config is created in the new directory and is not present in the *{tspp_ws}/conf* directory which is the default path set for hydra in the NVIDIA-TSPP. ","metadata":{}},{"cell_type":"code","source":"dataset = \"jpx\"\nskip_preprocessing = False\n\n# Dataset Proprocessing\nif not skip_preprocessing:\n    # Hydra Setup: this is just needed for notebook example\n    preproc_cfg = get_config(\"preproc_config\", config_path, override_list=[F\"hydra.searchpath=[pkg://custom_conf]\", F\"dataset={dataset}\"])\n    preprocessor = hydra.utils.instantiate(preproc_cfg, _recursive_=False)\n    train, valid, test = preprocessor.preprocess()\n    preprocessor.fit_scalers(train)\n    train = preprocessor.apply_scalers(train)\n    valid = preprocessor.apply_scalers(valid)\n    test = preprocessor.apply_scalers(test)\n    train = preprocessor.impute(train)\n    valid = preprocessor.impute(valid)\n    test = preprocessor.impute(test)\n    preprocessor.save_state()\n    preprocessor.save_datasets(train, valid, test)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-07T05:13:06.92049Z","iopub.execute_input":"2022-06-07T05:13:06.92083Z","iopub.status.idle":"2022-06-07T05:14:27.215215Z","shell.execute_reply.started":"2022-06-07T05:13:06.920792Z","shell.execute_reply":"2022-06-07T05:14:27.21448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <div style=\"padding:20px;color:#76B900;margin:0;font-size:100%;text-align:left;display:fill;border-radius:5px;background-color:#5E5E5E;overflow:hidden\">5 | Training</div>\n\nThe platform has the following architecture <br>\n![TSPP Architecture](https://i.imgur.com/BsFpL6D.png)\n<br>\nIn the above figure, the command line feeds input to the NVIDIA-TSPP launcher, which uses said input to configure the components (green blocks) required to train and test the model.\n\n# <div style=\"padding:10px;color:#76B900;margin:0;font-size:60%;text-align:left;display:fill;border-radius:5px;background-color:#5E5E5E;overflow:hidden\">Training Setup</div>\n\n\nAfter setting the dataset configuration, we can select the training experiment configuration. The NVIDIA-TSPP offers flexibility while setting up the training for time-series models. <br>\nIn the cell below, we have the training setup for 3 models: **[TFT](https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Forecasting/TFT), [N-BEATS](https://arxiv.org/abs/1905.10437), [XGBoost](https://www.kdd.org/kdd2016/papers/files/rfp0697-chenAemb.pdf)**. Model implementations can be found at: <i>{tspp_ws}/models</i> directory <br>\nAll the model related parameter are represented as <code>model.config.parameter_name</code>. This will depend on the model implementation. <br>\nIn the later cells, we will go over the available features in the NVIDIA-TSPP. <br>\n<code>hydra.run.dir</code>: explicitly sets the output directory for training to prevent the autogenerated output directory by hydra. <br>\n<code>hydra.searchpath</code> is required as the dataset config is created in the new directory and is not present in the *{tspp_ws}/conf* directory which is the default path set for hydra in the NVIDIA-TSPP","metadata":{}},{"cell_type":"code","source":"# This is training setup part on NVIDIA-TSPP\ndataset = \"jpx\"\ndef get_training_config(model_name):\n\n    # Hydra Setup: this is just needed for notebook example\n    model_params_list = []\n    hydra_override_list = []\n    if model_name == \"tft\":\n        model_params_list = [\n            \"trainer/optimizer=TorchAdam\",\n            \"trainer.config.num_epochs=20\",\n            \"trainer.config.batch_size=1024\",\n            \"model.config.hidden_size=168\",\n            \"model.config.n_head=4\",\n            \"model.config.dropout=0.1\",\n            \"trainer.optimizer.lr=0.0009171768778020634\",\n            \"++trainer.config.ema=False\"\n        ]\n    elif model_name == \"nbeats\":\n        model_params_list = [\n            \"model.config.stacks=[{type:generic,num_blocks:2,theta_dim:0,share_weights:False,hidden_size:256},{type:generic,num_blocks:2,theta_dim:0,share_weights:True,hidden_size:256}]\",\n            \"trainer/optimizer=TorchAdam\",\n            \"trainer.optimizer.lr=0.008472935555938357\",\n            \"trainer.config.num_epochs=20\",\n            \"trainer.config.batch_size=1024\",\n            \"++trainer.config.ema=False\"\n        ]\n    elif model_name == \"xgboost\":\n        model_params_list = [\n            \"++model.config.max_depth=6\",\n            \"++model.config.colsample_bylevel=1.0\",\n            \"++model.config.colsample_bytree=0.6\",\n            \"++model.config.subsample=1.0\",\n            \"++model.config.gamma=0.0001\",\n            \"++model.config.learning_rate=0.010213742143683735\",\n            \"++model.config.n_rounds=1000\",\n            \"++trainer.callbacks.early_stopping.patience=50\"\n        ]\n    else:\n        raise ValueError(F\"model: {model_name} not supported\")\n\n    output_workdir = os.path.join(curr_workdir, F'outputs/kaggle_{model_name}_{dataset}')\n    os.makedirs(output_workdir, exist_ok = True)\n    \n    hydra_override_list = [\n        F\"model={model_name}\",\n        F\"hydra.searchpath=[pkg://custom_conf]\",\n        F\"dataset={dataset}\",\n        F\"hydra.run.dir={output_workdir}\",\n        \"+trainer.config.force_rerun=True\",\n        \"++trainer.config.log_interval=200\"\n    ] + model_params_list\n        \n\n    cfg = get_config(\"train_config\", config_path, override_list=hydra_override_list, return_hydra_config=True)\n    output_hydra_path = os.path.join(output_workdir, \".hydra\")\n    os.makedirs(output_hydra_path, exist_ok=True)\n    with open(os.path.join(output_hydra_path, \"config.yaml\"), \"w\") as f:\n        OmegaConf.save(cfg, f=f)\n    return cfg, output_workdir","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-07T05:14:27.217141Z","iopub.execute_input":"2022-06-07T05:14:27.217413Z","iopub.status.idle":"2022-06-07T05:14:27.229354Z","shell.execute_reply.started":"2022-06-07T05:14:27.217377Z","shell.execute_reply":"2022-06-07T05:14:27.228548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Cell below creates the config for a given model","metadata":{}},{"cell_type":"code","source":"tft_cfg, tft_outdir = get_training_config(\"tft\")\nnbeats_cfg, nbeats_outdir = get_training_config(\"nbeats\")\nxgb_cfg, xgb_outdir = get_training_config(\"xgboost\")","metadata":{"execution":{"iopub.status.busy":"2022-06-07T05:14:27.23047Z","iopub.execute_input":"2022-06-07T05:14:27.230826Z","iopub.status.idle":"2022-06-07T05:14:28.752709Z","shell.execute_reply.started":"2022-06-07T05:14:27.23079Z","shell.execute_reply":"2022-06-07T05:14:28.751938Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <div style=\"padding:10px;color:#76B900;margin:0;font-size:60%;text-align:left;display:fill;border-radius:5px;background-color:#5E5E5E;overflow:hidden\">Training Parameters</div>","metadata":{}},{"cell_type":"markdown","source":"In the cell below we will go over the training features available in the NVIDIA-TSPP","metadata":{}},{"cell_type":"code","source":"print(\"\\nTrainer Config\\n\", OmegaConf.to_yaml(tft_cfg.trainer))","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Training parameters are described below","metadata":{}},{"cell_type":"markdown","source":"The NVIDIA-TSPP supports three types of trainers:\n<code>trainer</code>=OPTIONS(ctltrainer, xgbtrainer, stattrainer). ctltrainer is used for all DL model. xgbtrainer and stattrainer are for xgboost and autoarima respectively. trainer is selected based on the model config automatically <br>\n<code>trainer.config.force_rerun</code> should be set to True for this notebook. This feature is used to resume training from a saved checkpoint and is not supported on the notebook at this point. <br>\n<code>trainer.config.log_interval</code>: Setting iteration interval for printing the training status <br>\n<code>trainer.config.logfile_name</code>: output log file name <br>\n<code>trainer.config.world_size</code>:  the number of GPUs the launcher is running on <br>\n<code>trainer.config.device</code>: Target device for running training. default: 'cuda' (can set to 'cpu' for cpu only training)<br>\nFeatures like early stopping, logging, saving checkpoints, throughput measurement are available through the callback mechanism (<code>trainer.callbacks</code>). By default, all these callbacks are enabled. config files are available at *{tspp_ws}/trainer/callbacks* <br>\n<br>Parameters specific to DL models<br>\n<code>trainer.config.batch_size</code>: batch size per training iteration <br>\n<code>trainer.config.num_workers</code>: the number of workers to use for dataloading <br>\n<code>trainer.config.num_epochs</code>:  the number of epochs to train the model for <br>\n<code>trainer.config.amp</code>:  whether to enable AMP for accelerated training (This requires APEX library in the NVIDIA-TSPP currently and is not applicable for this notebook) <br>\n<code>trainer.config.ema</code>: Enables Exponential moving average feature when set to True. The model weights are integrated into a weighted moving average, and the weighted moving average is used in lieu of the directly trained model weights at test time. Our experiments have found this technique improves the convergence properties of most models and datasets we work with. [EMA Paper](https://arxiv.org/pdf/1803.05407.pdf)<br>\n<br>\n<code>trainer/optimizer</code>: selecting training optimizer. (default: 'Adam'. This is APEX optimized Adam optimizer which will not work with this notebook). Available optimizers: TorchAdam, ASGD, Adadelta, Adagrad, AdamW, Adamax, LBFGS, RMSProp, Rprop, SGD, SparseAdam. Optimizer configs are available at *{tspp_ws}/conf/trainer/optimizer* directory. Optimizer specific parameters are set using: <code>trainer.optimizer.parameter_name=..</code><br>\n<code>trainer/criterion</code>: selecting training criterion. (default: 'MSE'). Available criterion: GLL, L1, MSE, quantile. Criterion configs are available at <i>{tspp_ws}/conf/trainer/criterion</i> directory <br><br>\nFor evalualtion, the NVIDIA-TSPP provides several metrics: MSE, MAE, RMSE, SMAPE, TDI (Temporal Distortion Index), ND (Normalized Deviation). Evaluation configs are available at: *{tspp_ws}/conf/evaluator* directory. These metrics can be used for hyper-parameter search using Optuna (example in the later cell)\n\n","metadata":{"_kg_hide-input":true}},{"cell_type":"code","source":"%%script false --no-raise-error\n# Uncomment lines below to look at all the parameters for this experiment\n\n#print(\"\\nModel Config\\n\", OmegaConf.to_yaml(tft_cfg.model))\n#print(\"\\nDataset Config\\n\", OmegaConf.to_yaml(tft_cfg.dataset))\n#print(\"\\nEvaluater Config\\n\", OmegaConf.to_yaml(tft_cfg.evaluator))","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <div style=\"padding:10px;color:#76B900;margin:0;font-size:60%;text-align:left;display:fill;border-radius:5px;background-color:#5E5E5E;overflow:hidden\">Launch Training</div>","metadata":{}},{"cell_type":"markdown","source":"Method to launch training on the NVIDIA-TSPP","metadata":{}},{"cell_type":"code","source":"def launch_training(model_name, cfg, output_workdir):\n    os.chdir(curr_workdir)\n    os.chdir(output_workdir)\n    # Training\n    set_seed(cfg.get(\"seed\", None))\n    train, valid, test = hydra.utils.call(cfg.dataset)\n    model = hydra.utils.instantiate(cfg.model)\n    if model_name == \"xgboost\":\n        if cfg.trainer.get(\"criterion\", None):\n            del cfg.trainer.criterion\n        trainer = hydra.utils.instantiate(\n            cfg.trainer,\n            model=model,\n            train_dataset=train,\n            valid_dataset=valid,\n            patience=cfg.trainer.callbacks.early_stopping.patience,\n            log_interval=cfg.trainer.config.get('log_interval', 25)\n        )\n        trainer.train()\n    else:\n        model = model.to(device=cfg.model.config.device)\n        trainer = hydra.utils.instantiate(\n                cfg.trainer,\n                optimizer={'params': model.parameters()},\n                model=model,\n                train_dataset=train,\n                valid_dataset=valid)\n        trainer.train()\n    os.chdir(curr_workdir)\n    print(output_workdir)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-07T05:14:28.754465Z","iopub.execute_input":"2022-06-07T05:14:28.75479Z","iopub.status.idle":"2022-06-07T05:14:28.764109Z","shell.execute_reply.started":"2022-06-07T05:14:28.754753Z","shell.execute_reply":"2022-06-07T05:14:28.7633Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Select model for training","metadata":{}},{"cell_type":"code","source":"print(\"==========TFT===========\")\nlaunch_training(\"tft\", tft_cfg, tft_outdir)\nbase_cfg = tft_cfg\nbase_outdir = tft_outdir\nbase_name = \"tft\"\n# print(\"==========NBEATS===========\")\n# launch_training(\"nbeats\", nbeats_cfg, nbeats_outdir)\n# base_cfg = nbeats_cfg\n# base_outdir = nbeats_outdir\n# base_name = \"nbeats\"\n# print(\"==========XGB===========\")\n# launch_training(\"xgboost\", xgb_cfg, xgb_outdir)\n# base_cfg = xgb_cfg\n# base_outdir = xgb_outdir\n# base_name = \"xgboost\"\n","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-07T05:14:28.765206Z","iopub.execute_input":"2022-06-07T05:14:28.765592Z","iopub.status.idle":"2022-06-07T05:15:59.263067Z","shell.execute_reply.started":"2022-06-07T05:14:28.765556Z","shell.execute_reply":"2022-06-07T05:15:59.261979Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <div style=\"padding:10px;color:#76B900;margin:0;font-size:60%;text-align:left;display:fill;border-radius:5px;background-color:#5E5E5E;overflow:hidden\">HP Search with Optuna</div>\n","metadata":{}},{"cell_type":"markdown","source":"Hyperparameter search can be used to find semi-optimal hyperparameter configurations for a given model or dataset. In the NVIDIA-TSPP, hyperparameter search is driven by Optuna. <br>\nCell shows examples for running hp search with NVIDIA-TSPP","metadata":{}},{"cell_type":"code","source":"%%script false --no-raise-error\n#Optuna Support\nnum_trials = 2\nmodel_name = \"tft\"\n#model_name = \"xgboost\"\ndataset = \"jpx\"\noutput_workdir_optuna = os.path.join(curr_workdir, F'outputs/kaggle_{model_name}_{dataset}_optuna')\nos.makedirs(output_workdir_optuna, exist_ok = True)\nif model_name == \"tft\":\n    !python {tspp_ws}/launch_training.py --config-dir custom_conf -m 'model.config.n_head=choice(1,2,4)' 'trainer.optimizer.lr=tag(log, interval(1e-5, 1e-2))' model={model_name} dataset={dataset} \\\n    trainer.config.batch_size=1024 trainer.config.num_epochs=1   ++trainer.config.log_interval=-1  ++trainer.config.force_rerun=True trainer/optimizer=TorchAdam +optuna_objectives=[MAE] hydra/sweeper=optuna \\\n    hydra.sweeper.n_trials={num_trials} hydra.sweeper.n_jobs=1 hydra.sweeper.storage=sqlite:///{output_workdir_optuna}/hp_search_multiobjective.db \\\n    hydra.sweep.dir={output_workdir_optuna}\n\nif model_name == \"xgboost\":\n    !python {tspp_ws}/launch_training.py --config-dir custom_conf -m '++model.config.max_depth=choice(2, 3, 4, 5, 6)' model={model_name} dataset={dataset} \\\n    ++trainer.config.log_interval=200 ++trainer.config.force_rerun=True ++trainer.callbacks.early_stopping.patience=20 \\\n    ++model.config.learning_rate=0.017 ++model.config.subsample=0.8 \\\n    ++model.config.colsample_bytree=1.0 ++model.config.colsample_bylevel=0.4 ++model.config.gamma=0.3 ++model.config.n_rounds=1000 \\\n    +optuna_objectives=[MSE] hydra/sweeper=optuna \\\n    hydra.sweeper.n_trials={num_trials} hydra.sweeper.n_jobs=1 hydra.sweeper.storage=sqlite:///{output_workdir_optuna}/hp_search_multiobjective.db \\\n    hydra.sweep.dir={output_workdir_optuna}\n\n# Find the directory and model with the best results    \nn_trials = num_trials\nprint(F\"Optuna Results: {output_workdir_optuna}\")\nbest_model_dir = None\nbest_params = None\nwith open(os.path.join(output_workdir_optuna, \"optimization_results.yaml\"), \"r\") as f:\n    best_params = OmegaConf.load(f)\n    best_params = best_params.best_params\n    best_params = set([str(k)+\"=\"+str(v) for k,v in best_params.items()])\nprint(best_params)\nfor trial_id in range(n_trials):\n    trial_path = os.path.join(output_workdir_optuna, str(trial_id))\n    overrides_file_path = os.path.join(trial_path, \".hydra/overrides.yaml\")\n    if os.path.exists(overrides_file_path):\n        with open(overrides_file_path, \"r\") as f:\n            overrides_list = set(OmegaConf.load(f))\n            #print(overrides_list)\n            if best_params.issubset(overrides_list):\n                #print(F\"Best Model in the directory: {trial_path}\")\n                best_model_dir = trial_path\n                break\n    else:\n        print(F\"Couldn't find directory or config files at: {trail_path}\")\n\nprint(F\"best model dir: {best_model_dir}\")","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-07T05:26:39.70176Z","iopub.execute_input":"2022-06-07T05:26:39.702144Z","iopub.status.idle":"2022-06-07T05:29:42.334459Z","shell.execute_reply.started":"2022-06-07T05:26:39.702099Z","shell.execute_reply":"2022-06-07T05:29:42.333641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <div style=\"padding:20px;color:#76B900;margin:0;font-size:100%;text-align:left;display:fill;border-radius:5px;background-color:#5E5E5E;overflow:hidden\">6 | Evaluation</div>","metadata":{}},{"cell_type":"markdown","source":"Method to check Inference Performance <br>","metadata":{}},{"cell_type":"code","source":"def evaluate_model(model_name, cfg, output_workdir):\n    print(F\"Dataset: {dataset}\")\n    print(F\"Model:{model_name}\")\n    print(F\"Curr Directory: {curr_workdir}\")\n    print(F\"Checkpoint Directory: {output_workdir}\")\n    os.chdir(curr_workdir)\n    train, valid, test = hydra.utils.call(cfg.dataset)\n    del train, valid\n    evaluator = hydra.utils.instantiate(cfg.evaluator, test_data=test)\n    model = hydra.utils.instantiate(cfg.model)\n\n    if model_name == \"xgboost\":\n        model.load(output_workdir)\n    else:\n        state_dict = torch.load(os.path.join(output_workdir, \"best_checkpoint.zip\"))['model_state_dict']\n        model.load_state_dict(state_dict)\n        device = torch.device(cfg.model.config.device)  # maybe change depending on evaluator\n        model.to(device=device)\n\n    preds_full, labels_full, ids_full, weights_full = evaluator.predict(model)\n    eval_metrics = evaluator.evaluate(preds_full, labels_full, ids_full, weights_full)\n    print(eval_metrics)\n\n    #Command Line\n    #! python launch_inference.py dataset={dataset} model={model} evaluator.config.checkpoint={output_workdir}","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-07T05:15:59.265438Z","iopub.execute_input":"2022-06-07T05:15:59.265745Z","iopub.status.idle":"2022-06-07T05:15:59.276496Z","shell.execute_reply.started":"2022-06-07T05:15:59.265704Z","shell.execute_reply":"2022-06-07T05:15:59.275747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Select model for evaluation","metadata":{}},{"cell_type":"code","source":"evaluate_model(base_name, base_cfg, base_outdir)\n#print(\"====TFT Perf===\")\n#evaluate_model(\"tft\", tft_cfg, tft_outdir)\n#print(\"====Nbeats Perf===\")\n#evaluate_model(\"nbeats\", nbeats_cfg, nbeats_outdir)\n#print(\"====Xgboost Perf===\")\n#evaluate_model(\"xgboost\", xgb_cfg, xgb_outdir)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-07T05:15:59.27776Z","iopub.execute_input":"2022-06-07T05:15:59.278684Z","iopub.status.idle":"2022-06-07T05:16:15.308045Z","shell.execute_reply.started":"2022-06-07T05:15:59.278644Z","shell.execute_reply":"2022-06-07T05:16:15.307039Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <div style=\"padding:20px;color:#76B900;margin:0;font-size:100%;text-align:left;display:fill;border-radius:5px;background-color:#5E5E5E;overflow:hidden\">7 | Inference Pipeline</div>\nBelow we describe the steps for inference for this competition\n\n1. For each target date (T), first select historical entries from training set (based on encoder_length)\n2. Append target date entry to the historical entry dataframe\n3. Also append prediction date entries to this dataframe (based on example_length - encoder_length). For this example, it's set to 2 to predict Closing price of T+1 and T+2 (This is primarily needed for known future features: Day of Week, Day, Month for Target predictions)\n4. After this, first extract Time features from this dataframe, then preprocess this dataframe to apply scalers and categorical mappings\n5. Run inference\n6. Unscale predictions to get closing prices for T+1 and T+2\n7. Calculate Target\n8. Calculate Sharpe Ratio\n9. Compare it with the actual target value","metadata":{}},{"cell_type":"markdown","source":"Method to calculate Sharpe Ratio <br>","metadata":{}},{"cell_type":"code","source":"def calc_spread_return_sharpe(df: pd.DataFrame, portfolio_size: int = 200, toprank_weight_ratio: float = 2) -> float:\n    \"\"\"\n    Ref: https://www.kaggle.com/code/smeitoma/jpx-competition-metric-definition\n    Args:\n        df (pd.DataFrame): predicted results\n        portfolio_size (int): # of equities to buy/sell\n        toprank_weight_ratio (float): the relative weight of the most highly ranked stock compared to the least.\n    Returns:\n        (float): sharpe ratio\n    \"\"\"\n    def _calc_spread_return_per_day(df, portfolio_size, toprank_weight_ratio):\n        \"\"\"\n        Args:\n            df (pd.DataFrame): predicted results\n            portfolio_size (int): # of equities to buy/sell\n            toprank_weight_ratio (float): the relative weight of the most highly ranked stock compared to the least.\n        Returns:\n            (float): spread return\n        \"\"\"\n        assert df['Rank'].min() == 0\n        assert df['Rank'].max() == len(df['Rank']) - 1\n        weights = np.linspace(start=toprank_weight_ratio, stop=1, num=portfolio_size)\n        purchase = (df.sort_values(by='Rank')['Target'][:portfolio_size] * weights).sum() / weights.mean()\n        short = (df.sort_values(by='Rank', ascending=False)['Target'][:portfolio_size] * weights).sum() / weights.mean()\n        return purchase - short\n\n    buf = df.groupby('Date').apply(_calc_spread_return_per_day, portfolio_size, toprank_weight_ratio)\n    sharpe_ratio = buf.mean() / buf.std()\n    return sharpe_ratio, buf","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-06T05:27:14.027626Z","iopub.execute_input":"2022-06-06T05:27:14.02813Z","iopub.status.idle":"2022-06-06T05:27:14.037596Z","shell.execute_reply.started":"2022-06-06T05:27:14.028078Z","shell.execute_reply":"2022-06-06T05:27:14.03674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Loading supplemental files for Inference Performance","metadata":{}},{"cell_type":"code","source":"supp_file = \"supplemental_files/stock_prices.csv\"\nsupp_file = os.path.join(dataset_path, supp_file)\nsupp_df = pd.read_csv(supp_file)\nmin_date = pd.to_datetime(supp_df[\"Date\"]).min()\nmax_date = pd.to_datetime(supp_df[\"Date\"]).max()\nprint(F\"min_date={min_date}, max_date={max_date}, num_days = {max_date - min_date}\")","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-06T05:27:14.038707Z","iopub.execute_input":"2022-06-06T05:27:14.039032Z","iopub.status.idle":"2022-06-06T05:27:14.578948Z","shell.execute_reply.started":"2022-06-06T05:27:14.03899Z","shell.execute_reply":"2022-06-06T05:27:14.577152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Loading Dataset Mappings for inference","metadata":{}},{"cell_type":"code","source":"dataset_cfg = base_cfg.dataset\nencoder_length = dataset_cfg.config.encoder_length\nf = open(os.path.join(dataset_cfg.config.dest_path, \"tspp_preprocess.bin\"), \"rb\")\npreprocess_map = pickle.load(f)\nscalers = preprocess_map[\"scalers\"]\nnode_mappings_df = preprocess_map[\"id_mappings\"]\nid_mappings_dict = pd.Series(node_mappings_df['id'].values,index=node_mappings_df[\"_id_\"]).to_dict()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-06T05:27:14.580288Z","iopub.execute_input":"2022-06-06T05:27:14.580798Z","iopub.status.idle":"2022-06-06T05:27:14.673009Z","shell.execute_reply.started":"2022-06-06T05:27:14.580752Z","shell.execute_reply":"2022-06-06T05:27:14.672166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Method to run inference","metadata":{}},{"cell_type":"code","source":"def get_inference(model_name, cfg, output_workdir, pred_df):\n    model = hydra.utils.instantiate(cfg.model)\n    if model_name != \"xgboost\":\n        state_dict = torch.load(os.path.join(output_workdir, \"best_checkpoint.zip\"))['model_state_dict']\n        model.load_state_dict(state_dict)\n        device = torch.device(cfg.model.config.device)  # maybe change depending on evaluator\n        model.to(device=device)\n    else:\n        model.load(output_workdir)\n    \n    _, _, test_dataobj = hydra.utils.call(cfg.dataset, input_df=pred_df)\n    evaluator = hydra.utils.instantiate(cfg.evaluator, test_data=test_dataobj)\n    preds_full, labels_full, ids_full, weights_full = evaluator.predict(model)\n    return preds_full, labels_full, ids_full, weights_full","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-06T05:27:14.674702Z","iopub.execute_input":"2022-06-06T05:27:14.675033Z","iopub.status.idle":"2022-06-06T05:27:14.683938Z","shell.execute_reply.started":"2022-06-06T05:27:14.674965Z","shell.execute_reply":"2022-06-06T05:27:14.682838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Select Model","metadata":{}},{"cell_type":"code","source":"#models_list = [(\"xgboost\", xgb_cfg, xgb_outdir), (\"tft\", tft_cfg, tft_outdir), (\"nbeats\", nbeats_cfg, nbeats_outdir)]\nmodels_list = [(base_name, base_cfg, base_outdir)]","metadata":{"execution":{"iopub.status.busy":"2022-06-06T05:27:14.685638Z","iopub.execute_input":"2022-06-06T05:27:14.685979Z","iopub.status.idle":"2022-06-06T05:27:14.69425Z","shell.execute_reply.started":"2022-06-06T05:27:14.685895Z","shell.execute_reply":"2022-06-06T05:27:14.693265Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Inference Test","metadata":{}},{"cell_type":"code","source":"#%%time\n# For inference, we need historical data for encoder part\ndf = pd.read_csv(train_sp_file)\ndf[\"Date\"] = pd.to_datetime(df[\"Date\"])\n\nhistory_date_start = np.sort(df[\"Date\"].unique())[-60]\ndf = df[df[\"Date\"] >= history_date_start].reset_index()\n\nsupp_df[\"Date\"] = pd.to_datetime(supp_df[\"Date\"])\nsupp_df = supp_df.sort_values([\"Date\"])\n\nif df.Date.iloc[-1] < supp_df.Date.iloc[0]:\n    print(F\"Last date on train file: {df.Date.iloc[-1]}, First date on supplemental file: {supp_df.Date.iloc[0]}\")\n    df = df.append(supp_df, ignore_index=True)\nelse:\n    print(\"Overlap in dates of supplemental files and train files\")\n\nhdays = encoder_length\nprediction_length = dataset_cfg.config.example_length - dataset_cfg.config.encoder_length\nnum_codes = len(supp_df[\"SecuritiesCode\"].unique())\ntest_dates = supp_df[\"Date\"].unique()\nfinal_df = pd.DataFrame()\n\npreprocessor = Preprocessor(dataset_cfg.config)\npreprocessor.load_state(os.path.join(dataset_cfg.config.dest_path, \"tspp_preprocess.bin\"))\n\n# Predict first two dates in a loop\nfor test_date in test_dates[10:12]:\n    print(test_date)\n    test_entry_df = supp_df[supp_df[\"Date\"] == test_date]\n    print(\"History Date Before\", df.Date.iloc[-1])\n    df = df[df[\"Date\"] < test_date]\n    print(\"History Date After\", df.Date.iloc[-1])\n    df = pd.concat([df, test_entry_df], ignore_index=True)\n    pred_df = pd.DataFrame({'Date': pd.date_range(start=test_entry_df.Date.iloc[-1], periods=prediction_length+1, freq='B', closed='right')})\n    codes = test_entry_df.SecuritiesCode.unique()\n    pred_df = pd.concat([pred_df]*len(codes), ignore_index=True)\n    expanded_codes = [element for element in codes for i in range(prediction_length)]\n    pred_df[\"SecuritiesCode\"] = expanded_codes\n    pred_df = pd.concat([df, pred_df], ignore_index=True)\n    pred_df = pred_df.sort_values([\"SecuritiesCode\", \"Date\"])\n    pred_df.ffill(inplace=True)\n    pred_df = process_raw_data(pred_df)\n    pred_df = pred_df.groupby(\"SecuritiesCode\").tail(hdays+prediction_length).copy()\n    pred_df = pred_df.reset_index()\n    pred_df = preprocessor.preprocess_test(dataset=pred_df)\n    pred_df = preprocessor.apply_scalers(pred_df)\n    pred_df = preprocessor.impute(pred_df)\n    \n    model_results_dict = {}\n    \n    for model_name, cfg, model_dir in models_list:\n        print(\"Running Inference on model: \", model_name)\n        preds_full, labels_full, ids_full, weights_full = get_inference(model_name, cfg, model_dir, pred_df)\n        upreds = np.stack([scalers.inverse_transform_targets(preds_full[...,i], ids_full) for i in range(preds_full.shape[-1])], axis=-1)\n        targets = upreds[:, 1]/upreds[:, 0] - 1\n        targets = list(targets.squeeze())\n        securities = [id_mappings_dict[i] for i in ids_full]\n        target_dict = dict(zip(securities, targets))\n        model_results_dict[model_name] = target_dict\n    \n    out_df = pd.DataFrame(codes, columns=[\"SecuritiesCode\"])\n    out_df[\"Date\"] = test_entry_df.Date.iloc[-1]\n    out_df[\"Target\"] = 0\n    for model_name, _, _ in models_list:\n        out_df[\"Target\"] += out_df[\"SecuritiesCode\"].map(model_results_dict[model_name])\n    out_df[\"Target\"] /= len(models_list) #, \"xgboost\"\n    out_df['Rank'] = out_df.groupby('Date')['Target'].rank(ascending = False, method = 'first') - 1 \n    out_df['Rank'] = out_df['Rank'].astype(\"int\")\n    \n    _, predicted_spread_return = calc_spread_return_sharpe(out_df, 200, 2)\n    final_df = final_df.append(out_df, ignore_index=True)\n    #Valid Part:\n    valid_df = test_entry_df.copy()\n    valid_df['Rank'] = valid_df.groupby('Date')['Target'].rank(ascending = False, method = 'first') - 1 \n    valid_df['Rank'] = valid_df['Rank'].astype(\"int\")\n    _, actual_spread_return = calc_spread_return_sharpe(valid_df, 200, 2)\n    print(F\"Spread Return on date: {pd.to_datetime(test_date).strftime('%Y-%m-%d')}, Predicted: {predicted_spread_return[0]}, Actual: {actual_spread_return[0]}\")\n    \ncalc_spread_return_sharpe(final_df, 200, 2)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-06T05:27:14.695895Z","iopub.execute_input":"2022-06-06T05:27:14.696264Z","iopub.status.idle":"2022-06-06T05:29:32.035602Z","shell.execute_reply.started":"2022-06-06T05:27:14.696224Z","shell.execute_reply":"2022-06-06T05:29:32.034789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <div style=\"padding:20px;color:#76B900;margin:0;font-size:100%;text-align:left;display:fill;border-radius:5px;background-color:#5E5E5E;overflow:hidden\">8 | Submission</div>","metadata":{}},{"cell_type":"markdown","source":"Spread Return Per Day","metadata":{}},{"cell_type":"code","source":"def calc_spread_return_per_day(df, portfolio_size, toprank_weight_ratio):\n    \"\"\"\n    Args:\n        df (pd.DataFrame): predicted results\n        portfolio_size (int): # of equities to buy/sell\n        toprank_weight_ratio (float): the relative weight of the most highly ranked stock compared to the least.\n    Returns:\n        (float): spread return\n    \"\"\"\n    assert df['Rank'].min() == 0\n    assert df['Rank'].max() == len(df['Rank']) - 1\n    weights = np.linspace(start=toprank_weight_ratio, stop=1, num=portfolio_size)\n    purchase = (df.sort_values(by='Rank')['Target'][:portfolio_size] * weights).sum() / weights.mean()\n    short = (df.sort_values(by='Rank', ascending=False)['Target'][:portfolio_size] * weights).sum() / weights.mean()\n    return purchase - short","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-06T05:38:41.841261Z","iopub.execute_input":"2022-06-06T05:38:41.841546Z","iopub.status.idle":"2022-06-06T05:38:41.849189Z","shell.execute_reply.started":"2022-06-06T05:38:41.841514Z","shell.execute_reply":"2022-06-06T05:38:41.848165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Inference Run for Submission","metadata":{}},{"cell_type":"code","source":"%%time\n\nimport jpx_tokyo_market_prediction\nenv = jpx_tokyo_market_prediction.make_env()\niter_test = env.iter_test()\n\n# For inference, we need historical data for encoder part\ndf = pd.read_csv(train_sp_file)\ndf[\"Date\"] = pd.to_datetime(df[\"Date\"])\n\nhistory_date_start = np.sort(df[\"Date\"].unique())[-60]\ndf = df[df[\"Date\"] >= history_date_start].reset_index()\n\nsupp_file = \"supplemental_files/stock_prices.csv\"\nsupp_file = os.path.join(dataset_path, supp_file)\nsupp_df = pd.read_csv(supp_file)\nsupp_df[\"Date\"] = pd.to_datetime(supp_df[\"Date\"])\nsupp_df = supp_df.sort_values([\"Date\"])\n\nif df.Date.iloc[-1] < supp_df.Date.iloc[0]:\n    print(F\"Last date on train file: {df.Date.iloc[-1]}, First date on supplemental file: {supp_df.Date.iloc[0]}\")\n    df = df.append(supp_df, ignore_index=True)\nelse:\n    print(\"Overlap in dates of supplemental files and train files\")\n\nhdays = encoder_length\nprediction_length = dataset_cfg.config.example_length - dataset_cfg.config.encoder_length\n\n\npreprocessor = Preprocessor(dataset_cfg.config)\npreprocessor.load_state(os.path.join(dataset_cfg.config.dest_path, \"tspp_preprocess.bin\"))\n\n\nfor prices, options, financials, trades, secondary_prices, sample_prediction in iter_test:\n    print(F\"Current date: {prices.Date.iloc[-1]}\")\n    print(\"History Date Before\", df.Date.iloc[-1])\n    df = df[df[\"Date\"] < prices.Date.iloc[-1]]\n    print(\"History Date After\", df.Date.iloc[-1])\n    df = pd.concat([df, prices], ignore_index=True)\n    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n    pred_df = pd.DataFrame({'Date': pd.date_range(start=prices.Date.iloc[-1], periods=prediction_length+1, freq='B', closed='right')})\n    codes = prices.SecuritiesCode.unique()\n    pred_df = pd.concat([pred_df]*len(codes), ignore_index=True)\n    expanded_codes = [element for element in codes for i in range(prediction_length)]\n    pred_df[\"SecuritiesCode\"] = expanded_codes\n    pred_df = pd.concat([df, pred_df], ignore_index=True)\n    pred_df = pred_df.sort_values([\"SecuritiesCode\", \"Date\"])\n    pred_df.ffill(inplace=True)\n    pred_df = process_raw_data(pred_df)\n    pred_df = pred_df.groupby(\"SecuritiesCode\").tail(hdays+prediction_length).copy()\n    pred_df = pred_df.reset_index()\n    pred_df = preprocessor.preprocess_test(dataset=pred_df)\n    pred_df = preprocessor.apply_scalers(pred_df)\n    pred_df = preprocessor.impute(pred_df)\n\n    model_results_dict = {}\n    \n    for model_name, cfg, model_dir in models_list:\n        print(\"Running Inference on model: \", model_name)\n        preds_full, labels_full, ids_full, weights_full = get_inference(model_name, cfg, model_dir, pred_df)\n        upreds = np.stack([scalers.inverse_transform_targets(preds_full[...,i], ids_full) for i in range(preds_full.shape[-1])], axis=-1)\n        targets = upreds[:, 1]/upreds[:, 0] - 1\n        targets = list(targets.squeeze())\n        securities = [id_mappings_dict[i] for i in ids_full]\n        target_dict = dict(zip(securities, targets))\n        model_results_dict[model_name] = target_dict\n\n    \n    out_df = pd.DataFrame(codes, columns=[\"SecuritiesCode\"])\n    out_df[\"Date\"] = prices.Date.iloc[-1]\n    out_df[\"Target\"] = 0\n    for model_name, _, _ in models_list:\n        out_df[\"Target\"] += out_df[\"SecuritiesCode\"].map(model_results_dict[model_name])\n    out_df[\"Target\"] /= len(models_list)\n    out_df['Rank'] = out_df.groupby('Date')['Target'].rank(ascending = False, method = 'first') - 1 \n    out_df['Rank'] = out_df['Rank'].astype(\"int\")\n    \n    score = calc_spread_return_per_day(out_df, 200, 2)\n    print(F\"Score: {score}\")\n    subm_preds = out_df.set_index(\"SecuritiesCode\")[\"Rank\"]\n    sample_prediction['Rank'] = sample_prediction[\"SecuritiesCode\"].map(subm_preds)\n    env.predict(sample_prediction)\n    ","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-05T07:04:25.765888Z","iopub.execute_input":"2022-06-05T07:04:25.766162Z","iopub.status.idle":"2022-06-05T07:06:41.039582Z","shell.execute_reply.started":"2022-06-05T07:04:25.766131Z","shell.execute_reply":"2022-06-05T07:06:41.037968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <div style=\"padding:20px;color:#76B900;margin:0;font-size:100%;text-align:left;display:fill;border-radius:5px;background-color:#5E5E5E;overflow:hidden\">9 | References</div>","metadata":{}},{"cell_type":"markdown","source":"Here are the references to notebooks we started from as well as pointers to additional material if you want to know more about TSPP.  We hope you enjoyed this notebook and that it will be useful to you!\n\n1. Kaggle Notebooks\n* https://www.kaggle.com/code/smeitoma/train-demo#Generating-AdjustedClose-price\n* https://www.kaggle.com/code/smeitoma/jpx-competition-metric-definition\n* https://www.kaggle.com/code/datahobbit/jpx-network-models-and-feature-generation\n* https://www.kaggle.com/code/kellibelcher/jpx-stock-market-analysis-prediction-with-lgbm\n* https://www.kaggle.com/code/chumajin/easy-to-understand-the-competition\n2. NVIDIA-TSPP: https://github.com/NVIDIA/DeepLearningExamples/tree/master/Tools/PyTorch/TimeSeriesPredictionPlatform\n3. NVIDIA-TFT code: https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Forecasting/TFT\n4. NVIDIA-TSPP Blog: https://developer.nvidia.com/blog/time-series-forecasting-with-the-nvidia-time-series-prediction-platform-and-triton-inference-server/\n5. XGBoost: https://www.kdd.org/kdd2016/papers/files/rfp0697-chenAemb.pdf\n6. TFT: https://arxiv.org/abs/1912.09363\n7. N-Beats: https://arxiv.org/abs/1905.10437\n8. N-Beats code: https://github.com/philipperemy/n-beats\n","metadata":{}}]}