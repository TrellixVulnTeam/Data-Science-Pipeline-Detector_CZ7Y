{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Baseline: RNN (GRU) model","metadata":{}},{"cell_type":"markdown","source":"I only used train_files data so there is no leak to testing data.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom tensorflow.keras.models import load_model\nimport pickle","metadata":{"execution":{"iopub.status.busy":"2022-05-30T08:59:43.374763Z","iopub.execute_input":"2022-05-30T08:59:43.375292Z","iopub.status.idle":"2022-05-30T08:59:49.865128Z","shell.execute_reply.started":"2022-05-30T08:59:43.375196Z","shell.execute_reply":"2022-05-30T08:59:49.864065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stock_prices = pd.read_csv('../input/jpx-tokyo-stock-exchange-prediction/train_files/stock_prices.csv', index_col=False)\nprint(f\"Original stock prices: {len(stock_prices)}\")\ncleaned_stock_prices = stock_prices[stock_prices['Close'].notna()]\nprint(f\"Cleaned stock prices have: {len(cleaned_stock_prices)}\")\nsplit_time = int(len(cleaned_stock_prices)*0.8)\nprint(f\"Split time: {split_time}\")\ntrain_close_series = cleaned_stock_prices[:split_time]\ntest_close_series = cleaned_stock_prices[split_time:]\n\ntrain_series = train_close_series.groupby('SecuritiesCode')['Close'].apply(list)\ntrain_series = train_series.values\ntest_series = test_close_series.groupby('SecuritiesCode')['Close'].apply(list)\ntest_series = test_series.values","metadata":{"execution":{"iopub.status.busy":"2022-05-30T08:59:51.37551Z","iopub.execute_input":"2022-05-30T08:59:51.376426Z","iopub.status.idle":"2022-05-30T08:59:58.525991Z","shell.execute_reply.started":"2022-05-30T08:59:51.376378Z","shell.execute_reply":"2022-05-30T08:59:58.524854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"markdown","source":"The competition needs to calculate \"Target\" to rank 2000 stocks. So I just predict the next 2 days to get the \"Target\" according to the Metric Definition(https://www.kaggle.com/code/smeitoma/jpx-competition-metric-definition)","metadata":{}},{"cell_type":"code","source":"AUTO = tf.data.experimental.AUTOTUNE\nwindow_size = 90\nbatch_size = 2048\nbuffer_size = 10000\nOUT_STEPS = 2   # predict next 2 days\nRESUME = False\nRESUME_EPOCH = 20\nEPOCHS = 20","metadata":{"execution":{"iopub.status.busy":"2022-05-30T09:00:02.012209Z","iopub.execute_input":"2022-05-30T09:00:02.01266Z","iopub.status.idle":"2022-05-30T09:00:02.018859Z","shell.execute_reply.started":"2022-05-30T09:00:02.012626Z","shell.execute_reply":"2022-05-30T09:00:02.017287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def windowed_dataset(series, window_size):\n    dataset = tf.data.Dataset.from_tensor_slices(series)\n    dataset = dataset.window(window_size + OUT_STEPS, shift=1, drop_remainder=True)\n    dataset = dataset.flat_map(lambda window: window.batch(window_size + OUT_STEPS))\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2022-05-30T09:00:04.158681Z","iopub.execute_input":"2022-05-30T09:00:04.159096Z","iopub.status.idle":"2022-05-30T09:00:04.16653Z","shell.execute_reply.started":"2022-05-30T09:00:04.159064Z","shell.execute_reply":"2022-05-30T09:00:04.164958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Generate train and test windowed data.","metadata":{}},{"cell_type":"code","source":"train_windowed_data = []\nfor i in tqdm(range(len(train_series))):\n    windowed_series = windowed_dataset(train_series[i], window_size)\n    for j in windowed_series:\n        train_windowed_data.append(j.numpy())\nwith open('windowed_train_data90', 'wb') as fp:\n    pickle.dump(train_windowed_data, fp)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T08:37:16.789332Z","iopub.execute_input":"2022-05-30T08:37:16.789972Z","iopub.status.idle":"2022-05-30T08:37:30.013937Z","shell.execute_reply.started":"2022-05-30T08:37:16.789894Z","shell.execute_reply":"2022-05-30T08:37:30.012679Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_windowed_data = []\n\nfor i in tqdm(range(len(test_series))):\n    windowed_series = windowed_dataset(test_series[i], window_size)\n    for j in windowed_series:\n        test_windowed_data.append(j.numpy())\n\nwith open('windowed_test_data90', 'wb') as fp:\n    pickle.dump(test_windowed_data, fp)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Normalize training data. The mean and standard deviation should only be computed using the training data so that the models have no access to the values in the validation and test sets.","metadata":{}},{"cell_type":"code","source":"train_d = np.array(train_windowed_data)\ntest_d = np.array(test_windowed_data)\nt_mean = train_d.mean()\nt_std = train_d.std()\ntrain_series_norm = []\nfor i in train_series:\n    train_series_norm.append((i - t_mean)/t_std)\ntest_series_norm = []\nfor i in test_series:\n    test_series_norm.append((i - t_mean)/t_std)\n\ntrain_windowed_norm = (train_d - t_mean)/t_std\ntest_windowed_norm = (test_d - t_mean)/t_std","metadata":{"execution":{"iopub.status.busy":"2022-05-30T09:00:43.19163Z","iopub.execute_input":"2022-05-30T09:00:43.192045Z","iopub.status.idle":"2022-05-30T09:00:45.607977Z","shell.execute_reply.started":"2022-05-30T09:00:43.192014Z","shell.execute_reply":"2022-05-30T09:00:45.606817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First, we can take a look at native prediction.","metadata":{}},{"cell_type":"code","source":"total_abs_error = 0\ntotal_squ_error = 0\nnum = 0\nfor sample in test_windowed_norm:\n    test_pred = sample[:-1]\n    test_ground = sample[1:]\n    total_abs_error += tf.keras.metrics.mean_absolute_error(test_ground, test_pred).numpy()\n    total_squ_error += tf.keras.metrics.mean_squared_error(test_ground, test_pred).numpy()\n    num += 1\navg_abs_error = total_abs_error/num\navg_squ_error = total_squ_error/num\nprint(\"Test MAE: \", avg_abs_error)\nprint(\"Test MSE: \", avg_squ_error)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T08:39:07.488156Z","iopub.execute_input":"2022-05-30T08:39:07.488973Z","iopub.status.idle":"2022-05-30T08:39:18.272979Z","shell.execute_reply.started":"2022-05-30T08:39:07.488933Z","shell.execute_reply":"2022-05-30T08:39:18.271574Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_series(ids, series):\n      for i in ids:\n        s = series[i]\n        ground_s = s[1:]\n        predicted_v = s[:-1]\n        mae = tf.keras.metrics.mean_absolute_error(ground_s, predicted_v).numpy()\n        mse = tf.keras.metrics.mean_squared_error(ground_s, predicted_v).numpy()\n        print(f\"MSE {mse}\")\n        print(f\"MAE {mae}\")\n        time = range(len(ground_s))\n        plt.figure(figsize=(10,6))\n        plt.plot(time, ground_s)\n        plt.plot(time, predicted_v)\n        plt.show()\n\nplot_series([1, 3, 50], test_series_norm)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T08:39:38.021515Z","iopub.execute_input":"2022-05-30T08:39:38.022254Z","iopub.status.idle":"2022-05-30T08:39:38.523296Z","shell.execute_reply.started":"2022-05-30T08:39:38.022221Z","shell.execute_reply":"2022-05-30T08:39:38.522552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"def get_dataset(dataset, batch_size, buffer_size, mode='train'):\n    data = tf.data.Dataset.from_tensor_slices(dataset)\n    data = data.map(lambda window: (window[:-OUT_STEPS], window[-OUT_STEPS:]))\n    if mode == 'train':\n        data = data.shuffle(buffer_size)\n    data = data.batch(batch_size).prefetch(AUTO)\n    return data","metadata":{"execution":{"iopub.status.busy":"2022-05-30T09:00:53.113323Z","iopub.execute_input":"2022-05-30T09:00:53.113707Z","iopub.status.idle":"2022-05-30T09:00:53.120374Z","shell.execute_reply.started":"2022-05-30T09:00:53.113678Z","shell.execute_reply":"2022-05-30T09:00:53.11901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = get_dataset(train_windowed_norm, batch_size, buffer_size, mode='train')\ntest_dataset = get_dataset(test_windowed_norm, batch_size, buffer_size, mode='test')","metadata":{"execution":{"iopub.status.busy":"2022-05-30T09:00:59.676655Z","iopub.execute_input":"2022-05-30T09:00:59.677045Z","iopub.status.idle":"2022-05-30T09:01:00.913209Z","shell.execute_reply.started":"2022-05-30T09:00:59.677014Z","shell.execute_reply":"2022-05-30T09:01:00.912138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_model():\n    model = tf.keras.Sequential([tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1), input_shape=[window_size]),\n                                tf.keras.layers.GRU(256, return_sequences=True),\n                                tf.keras.layers.GRU(256, return_sequences=True),\n                                tf.keras.layers.GRU(512, return_sequences=True),\n                                tf.keras.layers.GRU(512, return_sequences=True),\n                                tf.keras.layers.GRU(256, return_sequences=True),\n                                tf.keras.layers.GRU(256, return_sequences=True),\n                                tf.keras.layers.GRU(128, return_sequences=True),\n                                tf.keras.layers.GRU(128),\n                                # tf.keras.layers.Dropout(0.1),\n                                tf.keras.layers.Dense(512, activation='relu'),\n                                tf.keras.layers.Dense(256, activation='relu'),\n                                tf.keras.layers.Dense(128, activation='relu'),\n                                tf.keras.layers.Dense(64, activation='relu'),\n                                tf.keras.layers.Dense(32, activation='relu'),\n                                # tf.keras.layers.Dropout(0.1),\n                                tf.keras.layers.Dense(OUT_STEPS*1),\n                                tf.keras.layers.Reshape([OUT_STEPS, 1])])\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-05-30T09:01:04.899039Z","iopub.execute_input":"2022-05-30T09:01:04.900573Z","iopub.status.idle":"2022-05-30T09:01:04.912821Z","shell.execute_reply.started":"2022-05-30T09:01:04.900523Z","shell.execute_reply":"2022-05-30T09:01:04.91102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_lr_callback(plot=False):\n    lr_start   = 1e-6\n    lr_max     = 1e-4\n    lr_min     = 1e-7\n    lr_ramp_ep = 1\n    lr_sus_ep  = 0\n    lr_decay   = 0.8\n   \n    def lrfn(epoch):\n        if RESUME:\n            epoch = epoch + RESUME_EPOCH\n        if epoch < lr_ramp_ep:\n            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n            \n        elif epoch < lr_ramp_ep + lr_sus_ep:\n            lr = lr_max\n            \n        else:\n            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n            \n        return lr\n        \n    if plot:\n        epochs = list(range(EPOCHS))\n        learning_rates = [lrfn(x) for x in epochs]\n#         print(learning_rates)\n        plt.scatter(epochs,learning_rates)\n        plt.show()\n\n    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=False)\n    return lr_callback\nget_lr_callback(plot=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T09:01:07.135077Z","iopub.execute_input":"2022-05-30T09:01:07.135879Z","iopub.status.idle":"2022-05-30T09:01:07.409153Z","shell.execute_reply.started":"2022-05-30T09:01:07.135846Z","shell.execute_reply":"2022-05-30T09:01:07.408175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.backend.clear_session()\nmodel = create_model()\nmodel.summary()\nmodel.compile(optimizer=tf.keras.optimizers.Adam(),\n             loss=tf.keras.losses.MeanSquaredError(),\n             metrics=['mae'])\nsv_path = './model/mymodel'\ncp_callback = tf.keras.callbacks.ModelCheckpoint(sv_path, monitor='val_loss', save_best_only=True, save_weights_only=True)\nhistory = model.fit(train_dataset, epochs=EPOCHS, validation_data=test_dataset, callbacks=[get_lr_callback(), cp_callback])","metadata":{"execution":{"iopub.status.busy":"2022-05-30T08:41:07.188469Z","iopub.execute_input":"2022-05-30T08:41:07.189065Z","iopub.status.idle":"2022-05-30T08:41:40.62849Z","shell.execute_reply.started":"2022-05-30T08:41:07.189019Z","shell.execute_reply":"2022-05-30T08:41:40.627283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plot history figures","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,6))\nepochs = range(len(history.history['loss']))\nplt.plot(epochs, history.history['loss'], label='Training loss')\nplt.plot(epochs, history.history['val_loss'], label=\"Validation loss\")\nplt.legend(loc=0)\nplt.figure(figsize=(10,6))\nplt.plot(epochs, history.history['mae'], label='Training mae')\nplt.plot(epochs, history.history['val_mae'], label=\"Validation mae\")\nplt.legend(loc=0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Zoom in","metadata":{}},{"cell_type":"code","source":"zoom_split = int(epochs[-1]*0.2)\nplt.figure(figsize=(10,6))\nplt.plot(epochs[zoom_split:], history.history['loss'][zoom_split:], label='Loss')\nplt.plot(epochs[zoom_split:], history.history['val_loss'][zoom_split:], label='Val_loss')\nplt.grid(True)\nplt.legend(loc=0)\nplt.figure(figsize=(10,6))\nplt.plot(epochs[zoom_split:], history.history['mae'][zoom_split:], label='Training mae')\nplt.plot(epochs[zoom_split:], history.history['val_mae'][zoom_split:], label=\"Validation mae\")\nplt.legend(loc=0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction","metadata":{}},{"cell_type":"code","source":"model = create_model()\nmodel.load_weights('./model/mymodel')\nmodel.compile(optimizer=tf.keras.optimizers.Adam(),\n             loss=tf.keras.losses.MeanSquaredError(),\n             metrics=['mae'])","metadata":{"execution":{"iopub.status.busy":"2022-05-30T09:01:28.176435Z","iopub.execute_input":"2022-05-30T09:01:28.177564Z","iopub.status.idle":"2022-05-30T09:01:31.615261Z","shell.execute_reply.started":"2022-05-30T09:01:28.177532Z","shell.execute_reply":"2022-05-30T09:01:31.614169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.evaluate(test_dataset)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def windowed_test(series, batch_size, shift=1):\n    dataset = tf.data.Dataset.from_tensor_slices(series)\n    dataset = dataset.window(window_size, shift=shift, drop_remainder=True)\n    dataset = dataset.flat_map(lambda window: window.batch(window_size))\n    dataset = dataset.batch(batch_size).prefetch(1)\n    return dataset\ndef plot_result(time0, time1, result, test_example_groundtruth):\n    plt.figure(figsize=(10, 6))\n    plt.plot(time0, test_example_groundtruth, label=\"Groundtruth\")\n    plt.plot(time0, result[:,0], label='Predicted Next Day')\n    plt.plot(time1, result[:,1], label='Predicted Next Next Day')\n    plt.legend(loc=0)\n    plt.show()\n\ndef show_random_test(test_ids, series):\n    for id in test_ids:\n        test_example = series[id]\n        test_example_input = test_example[:-1]\n        test_example_groundtruth = test_example[window_size:]\n        test_example_windowed = windowed_test(test_example_input, batch_size, shift=1)\n        result = model.predict(test_example_windowed).squeeze()\n        time0 = range(len(test_example_groundtruth))\n        time1 = range(1, len(test_example_groundtruth)+1)\n        plot_result(time0, time1, result, test_example_groundtruth)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T09:01:35.31139Z","iopub.execute_input":"2022-05-30T09:01:35.311869Z","iopub.status.idle":"2022-05-30T09:01:35.32413Z","shell.execute_reply.started":"2022-05-30T09:01:35.311839Z","shell.execute_reply":"2022-05-30T09:01:35.322908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_random_test([1, 3, 20, 50, 60], test_series_norm)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T08:48:32.973957Z","iopub.execute_input":"2022-05-30T08:48:32.974718Z","iopub.status.idle":"2022-05-30T08:48:42.13828Z","shell.execute_reply.started":"2022-05-30T08:48:32.974682Z","shell.execute_reply":"2022-05-30T08:48:42.13756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"markdown","source":"It is recommended to first train the model and save the checkpoint and then submit the score to avoid unnecessary running time.","metadata":{}},{"cell_type":"code","source":"import jpx_tokyo_market_prediction\nenv = jpx_tokyo_market_prediction.make_env()   # initialize the environment\niter_test = env.iter_test()    # an iterator which loops over the test files","metadata":{"execution":{"iopub.status.busy":"2022-05-30T09:01:40.758419Z","iopub.execute_input":"2022-05-30T09:01:40.75912Z","iopub.status.idle":"2022-05-30T09:01:40.781959Z","shell.execute_reply.started":"2022-05-30T09:01:40.759086Z","shell.execute_reply":"2022-05-30T09:01:40.780958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tag = 0\nfor (prices, options, financials, trades, secondary_prices, sample_prediction) in iter_test:\n    if tag == 0:\n        train_series_for_predict = cleaned_stock_prices[cleaned_stock_prices['Date'] > '2021-01-01']\n        train_series_for_predict['Close'] = (train_series_for_predict['Close'] - t_mean)/t_std\n        target_series = train_series_for_predict\n        tag = 1\n    cleaned_prices = prices[prices['Close'].notna()]\n    cleaned_prices['Close'] = (cleaned_prices['Close'] - t_mean)/t_std\n    target_series = target_series.append(cleaned_prices)\n    new_securities = target_series.groupby('SecuritiesCode')['Close'].apply(list)\n    security_code = []\n    next_day = []\n    next2_day = []\n    for idx, value in new_securities.items():\n        s_id = idx\n        if len(value) < window_size:\n            sv_padded = np.zeros(window_size)\n            sv_padded[-len(value):] = value\n            s_v = sv_padded\n        else:\n            s_v = value[-window_size:]\n        s_input = windowed_test(s_v, 1)\n        pred = model.predict(s_input).squeeze()\n        security_code.append(s_id)\n        next_day.append(pred[0]*t_std + t_mean)\n        next2_day.append(pred[1]*t_std + t_mean)\n    tuple_list = list(zip(security_code, next_day, next2_day))\n    next_day_df = pd.DataFrame(tuple_list, columns=['SecuritiesCode', 'NextDay', \"Next2Day\"])\n    new_df = pd.merge(prices, next_day_df, on=\"SecuritiesCode\")\n    new_df['Target'] = (new_df['Next2Day'] - new_df['NextDay']) / new_df['NextDay']\n    new_df['Rank'] = new_df['Target'].rank(ascending=False, method='first') - 1\n    new_df = new_df.sort_values('Rank').reset_index(drop=True)\n    new_df['Rank'] = new_df['Rank'].astype('int')\n    rankdict = dict(zip(new_df[\"SecuritiesCode\"],new_df[\"Rank\"]))\n    sample_prediction['Rank'] = sample_prediction[\"SecuritiesCode\"].map(rankdict)\n    env.predict(sample_prediction)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T09:01:47.625296Z","iopub.execute_input":"2022-05-30T09:01:47.625712Z","iopub.status.idle":"2022-05-30T09:05:12.364982Z","shell.execute_reply.started":"2022-05-30T09:01:47.625679Z","shell.execute_reply":"2022-05-30T09:05:12.363934Z"},"trusted":true},"execution_count":null,"outputs":[]}]}