{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**This notebook is English translated version of [LanceZero's.](https://www.kaggle.com/code/lancezero/simple-leakage-submission-test/notebook?scriptVersionId=95129549)<br>\nFor educational purposes only.**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport itertools\nfrom warnings import filterwarnings\nfilterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2022-05-10T09:15:31.183294Z","iopub.execute_input":"2022-05-10T09:15:31.183687Z","iopub.status.idle":"2022-05-10T09:15:31.21139Z","shell.execute_reply.started":"2022-05-10T09:15:31.18358Z","shell.execute_reply":"2022-05-10T09:15:31.210665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The submission file of the competition looks like this**","metadata":{}},{"cell_type":"code","source":"submission = pd.read_csv('../input/jpx-tokyo-stock-exchange-prediction/example_test_files/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-05-10T09:15:31.216012Z","iopub.execute_input":"2022-05-10T09:15:31.216771Z","iopub.status.idle":"2022-05-10T09:15:31.300245Z","shell.execute_reply.started":"2022-05-10T09:15:31.216728Z","shell.execute_reply":"2022-05-10T09:15:31.299437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission","metadata":{"execution":{"iopub.status.busy":"2022-05-10T09:15:31.30261Z","iopub.execute_input":"2022-05-10T09:15:31.303018Z","iopub.status.idle":"2022-05-10T09:15:31.328943Z","shell.execute_reply.started":"2022-05-10T09:15:31.302967Z","shell.execute_reply":"2022-05-10T09:15:31.32791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**A total of 56 trading days, each trading day has 2000 stocks to choose from. <br>\nWe need to predict the rank of each stock for each day separately, and then overwrite the initial rank above. The final submission is this file.**","metadata":{}},{"cell_type":"markdown","source":"**Then the background will calculate the score for this table. <br>\nThe basis for the calculation is to take the top 200 and the bottom 200 stocks each day, one long and one short, and then calculate the interest rate difference for each day, and calculate the interest rate spread for a total of 56 days. <br>\nThen score = mean/standard deviation of spread. So the score is not that the bigger the spread, the better. <br>\nOn the contrary, I think that if you can find a spread that is basically unchanged every day, even if it is very small, but the fluctuation is small, the score will be high. The idea is as follows**","metadata":{}},{"cell_type":"code","source":"spread_return_1 = np.full(56,12) + np.random.normal(loc=0,scale=1,size=56)\nspread_return_2 = np.full(56,15) + np.random.normal(loc=0,scale=2,size=56)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T09:15:31.330619Z","iopub.execute_input":"2022-05-10T09:15:31.331091Z","iopub.status.idle":"2022-05-10T09:15:31.338099Z","shell.execute_reply.started":"2022-05-10T09:15:31.331044Z","shell.execute_reply":"2022-05-10T09:15:31.336975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"spread_return_1.mean(),spread_return_2.mean()","metadata":{"execution":{"iopub.status.busy":"2022-05-10T09:15:31.340516Z","iopub.execute_input":"2022-05-10T09:15:31.341536Z","iopub.status.idle":"2022-05-10T09:15:31.357504Z","shell.execute_reply.started":"2022-05-10T09:15:31.341478Z","shell.execute_reply":"2022-05-10T09:15:31.3566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"spread_return_1.std(),spread_return_2.std()","metadata":{"execution":{"iopub.status.busy":"2022-05-10T09:15:31.360515Z","iopub.execute_input":"2022-05-10T09:15:31.361041Z","iopub.status.idle":"2022-05-10T09:15:31.373405Z","shell.execute_reply.started":"2022-05-10T09:15:31.360983Z","shell.execute_reply":"2022-05-10T09:15:31.372182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sharp_ratio_1 = spread_return_1.mean()/spread_return_1.std()\nsharp_ratio_2 = spread_return_2.mean()/spread_return_2.std()","metadata":{"execution":{"iopub.status.busy":"2022-05-10T09:15:31.378266Z","iopub.execute_input":"2022-05-10T09:15:31.378622Z","iopub.status.idle":"2022-05-10T09:15:31.385462Z","shell.execute_reply.started":"2022-05-10T09:15:31.378585Z","shell.execute_reply":"2022-05-10T09:15:31.384425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sharp_ratio_1,sharp_ratio_2  ","metadata":{"execution":{"iopub.status.busy":"2022-05-10T09:15:31.408974Z","iopub.execute_input":"2022-05-10T09:15:31.40935Z","iopub.status.idle":"2022-05-10T09:15:31.415391Z","shell.execute_reply.started":"2022-05-10T09:15:31.409297Z","shell.execute_reply":"2022-05-10T09:15:31.414779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**It can be seen that even though the average return of 2 is 3% larger than that of 1, the volatility is 1 times larger, so the Sharpe is smaller than that of 1. <br>\nThrough this, we found that the goal is not to rank stocks according to the predicted target, that is, the rate of return, but to rank stocks in consideration of fluctuations to ensure that the daily rate of return is large and stable.**","metadata":{}},{"cell_type":"code","source":"def calc_spread_return_per_day(df, portfolio_size=200, toprank_weight_ratio=2):\n    #Given a sorted table on a certain day, the spread of the day will be calculated return return\n    assert df['Rank'].min() == 0\n    assert df['Rank'].max() == len(df['Rank']) - 1\n    weights = np.linspace(start=toprank_weight_ratio, stop=1, num=portfolio_size)\n    purchase = (df.sort_values(by='Rank')['Target'][:portfolio_size] * weights).sum() / weights.mean()\n    short = (df.sort_values(by='Rank', ascending=False)['Target'][:portfolio_size] * weights).sum() / weights.mean()\n    return purchase - short","metadata":{"execution":{"iopub.status.busy":"2022-05-10T09:15:31.440628Z","iopub.execute_input":"2022-05-10T09:15:31.441548Z","iopub.status.idle":"2022-05-10T09:15:31.4486Z","shell.execute_reply.started":"2022-05-10T09:15:31.441496Z","shell.execute_reply":"2022-05-10T09:15:31.447563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calc_spread_return_sharpe(df: pd.DataFrame, portfolio_size=200, toprank_weight_ratio=2):\n    buf = df.groupby('Date').apply(calc_spread_return_per_day, portfolio_size, toprank_weight_ratio)\n    sharpe_ratio = buf.mean() / buf.std()\n    return sharpe_ratio, buf \n# buf is a yield group consisting of a spread return","metadata":{"execution":{"iopub.status.busy":"2022-05-10T09:15:31.475038Z","iopub.execute_input":"2022-05-10T09:15:31.475528Z","iopub.status.idle":"2022-05-10T09:15:31.480272Z","shell.execute_reply.started":"2022-05-10T09:15:31.475493Z","shell.execute_reply":"2022-05-10T09:15:31.479226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The deadline for training data is Friday 21.12.3. <br>\nThe start time of the supplementary data is Monday 21.12.6 to 22.2.28. <br>\nIn theory, we want to predict the data target starting from 21.12.6, and then submit it to the background, and the background will give a score based on the real data after 21.12.6, but now the data of 21.12.6 has told us that it is in supplemental_files. <br>\nNow we have three options. <br>\nOne is to not train the model directly, submit real data, and get a super high score. <br>\nAlternatively, you can use this data set as test_data, and train the model to score by yourself, which should have the same results as version A. <br>\nOr it can be added to train the model together, so that there is more data, and there should be an advantage in submitting at the end.**","metadata":{}},{"cell_type":"markdown","source":"# 1. Use supplemental_files to feel the submission process first","metadata":{}},{"cell_type":"code","source":"# parse_dates=[\"Date\"] Convert the Date column to time format\ndf = pd.read_csv('../input/jpx-tokyo-stock-exchange-prediction/supplemental_files/stock_prices.csv', parse_dates=[\"Date\"])","metadata":{"execution":{"iopub.status.busy":"2022-05-10T09:15:31.509604Z","iopub.execute_input":"2022-05-10T09:15:31.509976Z","iopub.status.idle":"2022-05-10T09:15:31.895283Z","shell.execute_reply.started":"2022-05-10T09:15:31.509937Z","shell.execute_reply":"2022-05-10T09:15:31.894195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Supplementary data for a total of 56 time points\ndf['Date'].nunique()","metadata":{"execution":{"iopub.status.busy":"2022-05-10T09:15:31.897345Z","iopub.execute_input":"2022-05-10T09:15:31.897721Z","iopub.status.idle":"2022-05-10T09:15:31.907693Z","shell.execute_reply.started":"2022-05-10T09:15:31.897659Z","shell.execute_reply":"2022-05-10T09:15:31.906938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df.groupby(\"Date\") Divide the df table into 56 sub-tables, each sub-table represents the data of a time\n# Then only take out the column of Target under each date, which is equivalent to 56 Series.\ndf.groupby(\"Date\")[\"Target\"]","metadata":{"execution":{"iopub.status.busy":"2022-05-10T09:15:31.908979Z","iopub.execute_input":"2022-05-10T09:15:31.90947Z","iopub.status.idle":"2022-05-10T09:15:31.923032Z","shell.execute_reply.started":"2022-05-10T09:15:31.909431Z","shell.execute_reply":"2022-05-10T09:15:31.922066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(df.groupby(\"Date\")[\"Target\"])","metadata":{"execution":{"iopub.status.busy":"2022-05-10T09:15:31.924917Z","iopub.execute_input":"2022-05-10T09:15:31.925405Z","iopub.status.idle":"2022-05-10T09:15:31.95383Z","shell.execute_reply.started":"2022-05-10T09:15:31.925357Z","shell.execute_reply":"2022-05-10T09:15:31.952698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list(df.groupby(\"Date\")[\"Target\"])[1]","metadata":{"execution":{"iopub.status.busy":"2022-05-10T09:15:31.95745Z","iopub.execute_input":"2022-05-10T09:15:31.957787Z","iopub.status.idle":"2022-05-10T09:15:31.982502Z","shell.execute_reply.started":"2022-05-10T09:15:31.957745Z","shell.execute_reply":"2022-05-10T09:15:31.981789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Sort stocks according to target, regardless of volatility, think that the higher the yield, the better","metadata":{}},{"cell_type":"code","source":"# Sort the target of 2000 stocks for each date\ndf.groupby(\"Date\")[\"Target\"].rank(ascending=False, method=\"first\") - 1","metadata":{"execution":{"iopub.status.busy":"2022-05-10T09:15:31.983863Z","iopub.execute_input":"2022-05-10T09:15:31.984198Z","iopub.status.idle":"2022-05-10T09:15:32.029543Z","shell.execute_reply.started":"2022-05-10T09:15:31.984151Z","shell.execute_reply":"2022-05-10T09:15:32.028927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = pd.Series([23,34,13,13,44,51])\na","metadata":{"execution":{"iopub.status.busy":"2022-05-10T09:15:32.03053Z","iopub.execute_input":"2022-05-10T09:15:32.031531Z","iopub.status.idle":"2022-05-10T09:15:32.040954Z","shell.execute_reply.started":"2022-05-10T09:15:32.031478Z","shell.execute_reply":"2022-05-10T09:15:32.039893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Without method, the same will be shot the same and not an integer\na.rank(ascending=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T09:15:32.042379Z","iopub.execute_input":"2022-05-10T09:15:32.04264Z","iopub.status.idle":"2022-05-10T09:15:32.058475Z","shell.execute_reply.started":"2022-05-10T09:15:32.042613Z","shell.execute_reply":"2022-05-10T09:15:32.05716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sort first\na.rank(ascending=False,method='first')","metadata":{"execution":{"iopub.status.busy":"2022-05-10T09:15:32.060133Z","iopub.execute_input":"2022-05-10T09:15:32.061073Z","iopub.status.idle":"2022-05-10T09:15:32.077143Z","shell.execute_reply.started":"2022-05-10T09:15:32.061022Z","shell.execute_reply":"2022-05-10T09:15:32.076165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a.rank(ascending=False,method='first') -1 ","metadata":{"execution":{"iopub.status.busy":"2022-05-10T09:15:32.081061Z","iopub.execute_input":"2022-05-10T09:15:32.081391Z","iopub.status.idle":"2022-05-10T09:15:32.09425Z","shell.execute_reply.started":"2022-05-10T09:15:32.081354Z","shell.execute_reply":"2022-05-10T09:15:32.093293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Rank'] = df.groupby(\"Date\")[\"Target\"].rank(ascending=False, method=\"first\") - 1","metadata":{"execution":{"iopub.status.busy":"2022-05-10T09:15:32.095564Z","iopub.execute_input":"2022-05-10T09:15:32.096446Z","iopub.status.idle":"2022-05-10T09:15:32.13835Z","shell.execute_reply.started":"2022-05-10T09:15:32.096379Z","shell.execute_reply":"2022-05-10T09:15:32.137221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Rank'] = df['Rank'].astype('int')","metadata":{"execution":{"iopub.status.busy":"2022-05-10T09:15:32.139727Z","iopub.execute_input":"2022-05-10T09:15:32.140001Z","iopub.status.idle":"2022-05-10T09:15:32.146658Z","shell.execute_reply.started":"2022-05-10T09:15:32.139968Z","shell.execute_reply":"2022-05-10T09:15:32.145477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Get the rank column, which indicates the rank of the target of each stock in the 2000 stocks on that day.**","metadata":{"execution":{"iopub.status.busy":"2022-05-09T07:00:01.503895Z","iopub.execute_input":"2022-05-09T07:00:01.504415Z","iopub.status.idle":"2022-05-09T07:00:01.532841Z","shell.execute_reply.started":"2022-05-09T07:00:01.504382Z","shell.execute_reply":"2022-05-09T07:00:01.531933Z"}}},{"cell_type":"markdown","source":"**But the submitted documents require that the table be sorted by date, and then each date is sorted by rank. From small to large, from 0-1999.**","metadata":{}},{"cell_type":"code","source":"# Sort by date first, then each date by rank.\ndf.sort_values([\"Date\", \"Rank\"],ascending=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T09:15:32.148268Z","iopub.execute_input":"2022-05-10T09:15:32.149217Z","iopub.status.idle":"2022-05-10T09:15:32.226175Z","shell.execute_reply.started":"2022-05-10T09:15:32.149165Z","shell.execute_reply":"2022-05-10T09:15:32.224993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_submission = df.sort_values([\"Date\", \"Rank\"],ascending=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T09:15:32.228025Z","iopub.execute_input":"2022-05-10T09:15:32.228299Z","iopub.status.idle":"2022-05-10T09:15:32.282838Z","shell.execute_reply.started":"2022-05-10T09:15:32.228267Z","shell.execute_reply":"2022-05-10T09:15:32.281935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Then look at the submission file, you need to match the rank column of the sunmission file with our prediction, that is, through the SecuritiesCode column, and then find the corresponding date of each day, what is the rank corresponding to this SecuritiesCode in df_submisson.**","metadata":{}},{"cell_type":"code","source":"submission","metadata":{"execution":{"iopub.status.busy":"2022-05-10T09:15:32.284091Z","iopub.execute_input":"2022-05-10T09:15:32.284351Z","iopub.status.idle":"2022-05-10T09:15:32.299761Z","shell.execute_reply.started":"2022-05-10T09:15:32.28432Z","shell.execute_reply":"2022-05-10T09:15:32.298818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**In fact, this is already done, the rest is to submit.**","metadata":{"execution":{"iopub.status.busy":"2022-05-09T07:05:17.929203Z","iopub.execute_input":"2022-05-09T07:05:17.929485Z","iopub.status.idle":"2022-05-09T07:05:17.93509Z","shell.execute_reply.started":"2022-05-09T07:05:17.929456Z","shell.execute_reply":"2022-05-09T07:05:17.93412Z"}}},{"cell_type":"markdown","source":"**It is in an iterative way, changing the Rank column of the submission file one date by one.**","metadata":{}},{"cell_type":"markdown","source":"## 1.1. Calculate the score yourself first, in theory, we don't need to calculate it, and the background will calculate after submitting the file","metadata":{}},{"cell_type":"code","source":"return_list = []\nfor date in df_submission.Date.unique():\n    today_return = calc_spread_return_per_day(df_submission.loc[ df_submission.Date == date])\n    return_list.append(today_return)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T09:15:32.301105Z","iopub.execute_input":"2022-05-10T09:15:32.301386Z","iopub.status.idle":"2022-05-10T09:15:32.530323Z","shell.execute_reply.started":"2022-05-10T09:15:32.301339Z","shell.execute_reply":"2022-05-10T09:15:32.528971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"return_list = np.array(return_list)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T09:15:32.533555Z","iopub.execute_input":"2022-05-10T09:15:32.533983Z","iopub.status.idle":"2022-05-10T09:15:32.539287Z","shell.execute_reply.started":"2022-05-10T09:15:32.533935Z","shell.execute_reply":"2022-05-10T09:15:32.538208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"min(return_list) # Minimum are 11.44","metadata":{"execution":{"iopub.status.busy":"2022-05-10T09:15:32.541039Z","iopub.execute_input":"2022-05-10T09:15:32.541812Z","iopub.status.idle":"2022-05-10T09:15:32.555135Z","shell.execute_reply.started":"2022-05-10T09:15:32.541749Z","shell.execute_reply":"2022-05-10T09:15:32.553927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sharp_ratio = return_list.mean() / return_list.std()","metadata":{"execution":{"iopub.status.busy":"2022-05-10T09:15:32.557053Z","iopub.execute_input":"2022-05-10T09:15:32.557455Z","iopub.status.idle":"2022-05-10T09:15:32.569446Z","shell.execute_reply.started":"2022-05-10T09:15:32.557403Z","shell.execute_reply":"2022-05-10T09:15:32.568552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sharp_ratio ","metadata":{"execution":{"iopub.status.busy":"2022-05-10T09:15:32.571077Z","iopub.execute_input":"2022-05-10T09:15:32.571429Z","iopub.status.idle":"2022-05-10T09:15:32.585095Z","shell.execute_reply.started":"2022-05-10T09:15:32.571383Z","shell.execute_reply":"2022-05-10T09:15:32.583965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Submission","metadata":{}},{"cell_type":"code","source":"import jpx_tokyo_market_prediction\nenv = jpx_tokyo_market_prediction.make_env()\niter_test = env.iter_test()\nfor prices, _, _, _, _, sample_prediction in iter_test:\n    day_df = df_submission[df_submission['Date']==prices[\"Date\"].iloc[0]]\n    map_dict = day_df.set_index(\"SecuritiesCode\")[\"Rank\"]\n    sample_prediction[\"Rank\"] = sample_prediction.SecuritiesCode.map(map_dict)\n    env.predict(sample_prediction)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T09:15:32.586244Z","iopub.execute_input":"2022-05-10T09:15:32.586972Z","iopub.status.idle":"2022-05-10T09:15:32.834822Z","shell.execute_reply.started":"2022-05-10T09:15:32.586906Z","shell.execute_reply":"2022-05-10T09:15:32.833799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}