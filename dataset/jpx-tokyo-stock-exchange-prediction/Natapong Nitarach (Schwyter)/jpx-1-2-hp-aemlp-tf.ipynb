{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!cp ../input/talibinstall/ta-lib-0.4.0-src.tar.gzh  ./ta-lib-0.4.0-src.tar.gz\n!tar -xzvf ta-lib-0.4.0-src.tar.gz > null\n!cd ta-lib && ./configure --prefix=/usr > null && make  > null && make install > null\n!cp ../input/talibinstall/TA-Lib-0.4.21.tar.gzh TA-Lib-0.4.21.tar.gz\n!pip install TA-Lib-0.4.21.tar.gz > null\n!pip install ../input/talibinstall/numpy-1.21.4-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl >null\n\n!rm -rf ./ta-lib\n!rm  ./TA-Lib-0.4.21.tar.gz\n!rm  ./ta-lib-0.4.0-src.tar.gz\n!rm  ./null\n\nimport talib as ta","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-14T05:03:24.360595Z","iopub.execute_input":"2022-06-14T05:03:24.361219Z","iopub.status.idle":"2022-06-14T05:06:51.619972Z","shell.execute_reply.started":"2022-06-14T05:03:24.361181Z","shell.execute_reply":"2022-06-14T05:06:51.619067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os, gc, warnings, random, datetime, traceback, joblib\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow import keras\nfrom scipy import stats\nimport numpy.polynomial.hermite as Herm\nimport math\nimport tensorflow_probability as tfp\n\nfrom tensorflow.keras import regularizers\nfrom sklearn.preprocessing import RobustScaler\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.keras import backend as K","metadata":{"execution":{"iopub.status.busy":"2022-06-14T05:06:51.623025Z","iopub.execute_input":"2022-06-14T05:06:51.623305Z","iopub.status.idle":"2022-06-14T05:06:58.53925Z","shell.execute_reply.started":"2022-06-14T05:06:51.623253Z","shell.execute_reply":"2022-06-14T05:06:58.538465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path_s = \"../input/jpx-tokyo-stock-exchange-prediction/supplemental_files/\"\npath_t = \"../input/jpx-tokyo-stock-exchange-prediction/train_files/\"\npath_e = \"../input/jpx-tokyo-stock-exchange-prediction/example_test_files/\"","metadata":{"execution":{"iopub.status.busy":"2022-06-14T05:06:58.544064Z","iopub.execute_input":"2022-06-14T05:06:58.546036Z","iopub.status.idle":"2022-06-14T05:06:58.559099Z","shell.execute_reply.started":"2022-06-14T05:06:58.545997Z","shell.execute_reply":"2022-06-14T05:06:58.553513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prices1 = pd.read_csv(f\"{path_s}stock_prices.csv\") #2021\nprices2 = pd.read_csv(f\"{path_t}stock_prices.csv\") #2017\nprices3 = pd.read_csv(f\"{path_t}secondary_stock_prices.csv\")\n\nprices1.shape , prices2.shape, prices3.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-14T05:06:58.564697Z","iopub.execute_input":"2022-06-14T05:06:58.565599Z","iopub.status.idle":"2022-06-14T05:07:10.428785Z","shell.execute_reply.started":"2022-06-14T05:06:58.56556Z","shell.execute_reply":"2022-06-14T05:07:10.428078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prices2['Target'].isnull().sum(), prices1['Target'].isnull().sum(), prices3['Target'].isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-06-14T05:07:10.430012Z","iopub.execute_input":"2022-06-14T05:07:10.431552Z","iopub.status.idle":"2022-06-14T05:07:10.454591Z","shell.execute_reply.started":"2022-06-14T05:07:10.431511Z","shell.execute_reply":"2022-06-14T05:07:10.453972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train2 = prices2.dropna(subset=['Target'])\ntrain1 = prices1.dropna(subset=['Target'])","metadata":{"execution":{"iopub.status.busy":"2022-06-14T05:07:10.455892Z","iopub.execute_input":"2022-06-14T05:07:10.456132Z","iopub.status.idle":"2022-06-14T05:07:10.655727Z","shell.execute_reply.started":"2022-06-14T05:07:10.4561Z","shell.execute_reply":"2022-06-14T05:07:10.654999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train2['Target'].isnull().sum(), train1['Target'].isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-06-14T05:07:10.657011Z","iopub.execute_input":"2022-06-14T05:07:10.657257Z","iopub.status.idle":"2022-06-14T05:07:10.670867Z","shell.execute_reply.started":"2022-06-14T05:07:10.657224Z","shell.execute_reply":"2022-06-14T05:07:10.670159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.concat([train2,train1])","metadata":{"execution":{"iopub.status.busy":"2022-06-14T05:07:10.672059Z","iopub.execute_input":"2022-06-14T05:07:10.672402Z","iopub.status.idle":"2022-06-14T05:07:10.871782Z","shell.execute_reply.started":"2022-06-14T05:07:10.672363Z","shell.execute_reply":"2022-06-14T05:07:10.870692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = \"GPU\" ","metadata":{"execution":{"iopub.status.busy":"2022-06-14T05:07:10.873112Z","iopub.execute_input":"2022-06-14T05:07:10.873957Z","iopub.status.idle":"2022-06-14T05:07:10.87772Z","shell.execute_reply.started":"2022-06-14T05:07:10.873911Z","shell.execute_reply":"2022-06-14T05:07:10.876728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if device == \"TPU\":\n    print(\"connecting to TPU...\")\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        print('Running on TPU ', tpu.master())\n    except ValueError:\n        tpu = None\n    if tpu:\n        try:\n            print(\"initializing  TPU ...\")\n            tf.config.experimental_connect_to_cluster(tpu)\n            tf.tpu.experimental.initialize_tpu_system(tpu)\n            strategy = tf.distribute.TPUStrategy(tpu)\n            print(\"TPU initialized\")\n        except: print(\"failed to initialize TPU\")\n    else: device = \"GPU\"\n\nif device != \"TPU\": strategy = tf.distribute.get_strategy()\nif device == \"GPU\": print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\nAUTO     = tf.data.experimental.AUTOTUNE\nREPLICAS = strategy.num_replicas_in_sync","metadata":{"execution":{"iopub.status.busy":"2022-06-14T05:07:10.881307Z","iopub.execute_input":"2022-06-14T05:07:10.882026Z","iopub.status.idle":"2022-06-14T05:07:11.058958Z","shell.execute_reply.started":"2022-06-14T05:07:10.881982Z","shell.execute_reply":"2022-06-14T05:07:11.057957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_all_seeds(seed):\n    np.random.seed(seed)\n    random.seed(seed)\n    tf.random.set_seed(seed)","metadata":{"execution":{"iopub.status.busy":"2022-06-14T05:07:11.059955Z","iopub.execute_input":"2022-06-14T05:07:11.060358Z","iopub.status.idle":"2022-06-14T05:07:11.065979Z","shell.execute_reply.started":"2022-06-14T05:07:11.060321Z","shell.execute_reply":"2022-06-14T05:07:11.065263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SEED = 2022\nset_all_seeds(SEED)","metadata":{"execution":{"iopub.status.busy":"2022-06-14T05:07:11.06756Z","iopub.execute_input":"2022-06-14T05:07:11.068042Z","iopub.status.idle":"2022-06-14T05:07:11.072841Z","shell.execute_reply.started":"2022-06-14T05:07:11.068003Z","shell.execute_reply":"2022-06-14T05:07:11.07199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Config:\n    is_training = True\n    output_dataset_path = \"../input/models-for-jpx-aemlp-tf/\"\nconfig = Config()","metadata":{"execution":{"iopub.status.busy":"2022-06-14T05:07:11.074445Z","iopub.execute_input":"2022-06-14T05:07:11.07507Z","iopub.status.idle":"2022-06-14T05:07:11.080634Z","shell.execute_reply.started":"2022-06-14T05:07:11.075033Z","shell.execute_reply":"2022-06-14T05:07:11.079749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nsecurities_codes = pd.read_csv(\"../input/jpx-cgkf-f98-tf/securities_codes.csv\")\nsecurities_codes_size = len(securities_codes) + 1\nwith tf.device(\"cpu\"):\n    securities_codes_lookup_layer = layers.IntegerLookup(max_tokens=securities_codes_size)\n    securities_codes_lookup_layer.adapt(securities_codes)","metadata":{"execution":{"iopub.status.busy":"2022-06-14T05:07:11.08218Z","iopub.execute_input":"2022-06-14T05:07:11.082834Z","iopub.status.idle":"2022-06-14T05:07:13.687669Z","shell.execute_reply.started":"2022-06-14T05:07:11.082663Z","shell.execute_reply":"2022-06-14T05:07:13.686959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prep_prices(prices):\n    prices.fillna(0,inplace=True)\n    return prices","metadata":{"execution":{"iopub.status.busy":"2022-06-14T05:07:13.689046Z","iopub.execute_input":"2022-06-14T05:07:13.689853Z","iopub.status.idle":"2022-06-14T05:07:13.694575Z","shell.execute_reply.started":"2022-06-14T05:07:13.689816Z","shell.execute_reply":"2022-06-14T05:07:13.693625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NDAYS = 180\nlastdays = df[df[\"Date\"]>=df.Date.iat[-2000*NDAYS]].reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-14T05:07:13.695843Z","iopub.execute_input":"2022-06-14T05:07:13.696382Z","iopub.status.idle":"2022-06-14T05:07:14.163727Z","shell.execute_reply.started":"2022-06-14T05:07:13.696339Z","shell.execute_reply":"2022-06-14T05:07:14.162969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lastdays = pd.DataFrame(df.groupby(\"SecuritiesCode\").Target.mean())\ndef get_avg(_id_):\n    return lastdays.loc[_id_]","metadata":{"execution":{"iopub.status.busy":"2022-06-14T05:07:14.165009Z","iopub.execute_input":"2022-06-14T05:07:14.165246Z","iopub.status.idle":"2022-06-14T05:07:14.22272Z","shell.execute_reply.started":"2022-06-14T05:07:14.165213Z","shell.execute_reply":"2022-06-14T05:07:14.222018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#simple units\nhbar = 1.0\nm    = 1.0\nw    = 1.0\n\ndef hermite(x, n):\n    xi             = np.sqrt(m*w/hbar)*x\n    herm_coeffs    = np.zeros(n+1)\n    herm_coeffs[n] = 1\n    return Herm.hermval(xi, herm_coeffs)\n\ndef stationary_state(x,n):\n    xi        = np.sqrt(m*w/hbar)*x\n    prefactor = 1.0/math.sqrt(2.0**n * math.factorial(n)) * (m*w/(np.pi*hbar))**(0.25)\n    psi       = prefactor * np.exp(- xi**2 / 2) * hermite(x,n)\n    return psi","metadata":{"execution":{"iopub.status.busy":"2022-06-14T05:07:14.224112Z","iopub.execute_input":"2022-06-14T05:07:14.22438Z","iopub.status.idle":"2022-06-14T05:07:14.231141Z","shell.execute_reply.started":"2022-06-14T05:07:14.224346Z","shell.execute_reply":"2022-06-14T05:07:14.230334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from decimal import *\ndef adjust_price(price):\n    \"\"\"\n    Args:\n        price (pd.DataFrame)  : pd.DataFrame include stock_price\n    Returns:\n        price DataFrame (pd.DataFrame): stock_price with generated AdjustedClose\n    \"\"\"\n    # transform Date column into datetime\n    price.loc[: ,\"Date\"] = pd.to_datetime(price.loc[: ,\"Date\"], format=\"%Y-%m-%d\")\n\n    def generate_adjusted_close(df):\n        \"\"\"\n        Args:\n            df (pd.DataFrame)  : stock_price for a single SecuritiesCode\n        Returns:\n            df (pd.DataFrame): stock_price with AdjustedClose for a single SecuritiesCode\n        \"\"\"\n        # sort data to generate CumulativeAdjustmentFactor\n        df = df.sort_values(\"Date\", ascending=False)\n        # generate CumulativeAdjustmentFactor\n        df.loc[:, \"CumulativeAdjustmentFactor\"] = df[\"AdjustmentFactor\"].cumprod()\n        # generate AdjustedClose\n        df.loc[:, \"AdjustedClose\"] = (\n            df[\"CumulativeAdjustmentFactor\"] * df[\"Close\"]\n        ).map(lambda x: float(\n            Decimal(str(x)).quantize(Decimal('0.1'), rounding=ROUND_HALF_UP)\n        ))\n        # reverse order\n        df = df.sort_values(\"Date\")\n        # to fill AdjustedClose, replace 0 into np.nan\n        df.loc[df[\"AdjustedClose\"] == 0, \"AdjustedClose\"] = np.nan\n        # forward fill AdjustedClose\n        df.loc[:, \"AdjustedClose\"] = df.loc[:, \"AdjustedClose\"].ffill()\n        return df\n\n    # generate AdjustedClose\n    price = price.sort_values([\"SecuritiesCode\", \"Date\"])\n    price = price.groupby(\"SecuritiesCode\").apply(generate_adjusted_close).reset_index(drop=True)\n\n    price.set_index(\"Date\", inplace=True)\n    return price\n\ndef adjust_features(df):\n    df=df.copy()\n    col='AdjustedClose'\n    periods=[3,5,8,12,15,26,30,35,40,45,50,60, 100,200]\n    for period in periods:\n        df.loc[:,\"Return_{}Day\".format(period)] = df.groupby(\"SecuritiesCode\")[col].pct_change(period)\n        df.loc[:,\"MovingAvg_{}Day\".format(period)] = df.groupby(\"SecuritiesCode\")[col].rolling(window=period).mean().values\n        df.loc[:,\"ExpMovingAvg_{}Day\".format(period)] = df.groupby(\"SecuritiesCode\")[col].ewm(span=period,adjust=False).mean().values\n        df.loc[:,\"Volatility_{}Day\".format(period)] = np.log(df[col]).groupby(df[\"SecuritiesCode\"]).diff().rolling(period).std()\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-06-14T05:07:14.232796Z","iopub.execute_input":"2022-06-14T05:07:14.233169Z","iopub.status.idle":"2022-06-14T05:07:14.248806Z","shell.execute_reply.started":"2022-06-14T05:07:14.233134Z","shell.execute_reply":"2022-06-14T05:07:14.247954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_features(df, tr=True):    \n    df = adjust_price(df)\n    df = adjust_features(df)\n    df = prep_prices(df)\n    \n    df['upper_Shadow']   = df['High'] - np.maximum(df['Close'], df['Open'])\n    df['lower_Shadow']   = np.minimum(df['Close'], df['Open']) - df['Low'] \n\n    # The Golden Ratio Multiplier \n    df['GRM_0']    = (ta.MA(df['Close'], timeperiod=350, matype=0)) \n    df['GRM_1']    = (ta.MA(df['Close'], timeperiod=350, matype=0))*1.6  \n    df['GRM_2']    = (ta.MA(df['Close'], timeperiod=350, matype=0))*2\n    df['GRM_3']    = (ta.MA(df['Close'], timeperiod=350, matype=0))*3\n\n    df['Pi_Cycle'] = ta.MA(df['Close'], timeperiod=111, matype=0) \n    \n    # Momentum\n    df['RSI_14'] = ta.RSI(df['Close'], timeperiod=14)\n    df['RSI_24'] = ta.RSI(df['Close'], timeperiod=24)\n    df['RSI_42'] = ta.RSI(df['Close'], timeperiod=42)\n    \n    df['RSI1']   = df['RSI_14'].shift(-1) \n    df['RSI4']   = df['RSI_14'].shift(-4) \n    df['RSI7']   = df['RSI_14'].shift(-7) \n    df['RSI10']  = df['RSI_14'].shift(-10) \n    df['RSI13']  = df['RSI_14'].shift(-13) \n    df['RSI16']  = df['RSI_14'].shift(-16) \n    \n    df['MACD_12'], df['macdsignal_12'], df['MACD_HIST_12'] = ta.MACD(df['Close'], fastperiod=12, slowperiod=26, signalperiod=9) \n    df['MACD_48'], df['macdsignal_48'], df['MACD_HIST_48'] = ta.MACD(df['Close'], fastperiod=48, slowperiod=104, signalperiod=36)\n    \n    df['macdsignal1'] = df['macdsignal_12'].shift(-1)\n    df['macdsignal4'] = df['macdsignal_12'].shift(-4)\n    df['macdsignal7'] = df['macdsignal_12'].shift(-7)\n    df['MACD_HIST1']  = df['MACD_HIST_12'].shift(-1) \n    df['MACD_HIST4']  = df['MACD_HIST_12'].shift(-4) \n    df['MACD_HIST7']  = df['MACD_HIST_12'].shift(-7) \n    df['ROCP']     = ta.ROCP(df['Open'])\n    df['momentam'] = ta.MOM(df['Open'])\n    df['CMO']      = ta.CMO(df['Open']) \n    df['PPO']      = ta.PPO(df['Open'])\n    df['SAR']       = ta.SAR(df['High'], df['Low'], acceleration=0, maximum=0) \n    df['DI_minus']  = ta.MINUS_DI(df['High'], df['Low'],np.array(df.loc[:, 'Close']), timeperiod=14) \n    df['DI_minus1'] = df['DI_minus'].shift(-1) \n    df['DI_minus4'] = df['DI_minus'].shift(-4) \n    df['DI_minus7'] = df['DI_minus'].shift(-7)  \n    df['adx']    = ta.ADX(df['High'], df['Low'],np.array(df.loc[:, 'Close']),timeperiod=14) \n    df['adx1']   = df['adx'].shift(-1) \n    df['adx4']   = df['adx'].shift(-4) \n    df['adx+1']  = df['adx'].shift(1) \n    df['adx7']   = df['adx'].shift(-7)\n    df['DI_plus']   = ta.PLUS_DI(df['High'], df['Low'],np.array(df.loc[:, 'Close']), timeperiod=14) \n    df['DI_plus1']  = df['DI_plus'].shift(-1) \n    df['DI_plus4']  = df['DI_plus'].shift(-4) \n    df['DI_plus7']  = df['DI_plus'].shift(-7) \n    df['DI_plus10'] = df['DI_plus'].shift(-10)\n    df['APO']      = ta.APO(df['Open'])\n    df['APO1']     = df['APO'].shift(-1)\n    df['APO4']     = df['APO'].shift(-4)\n    df['APO7']     = df['APO'].shift(-7)\n    df['ROCR100']  = ta.AD(df['High'], df['Low'], df['Close'], df['Volume'])\n    df['OBV']      = ta.OBV(df['Close'], df['Volume'])\n    df['ADOSC']    = ta.ADOSC(df['High'], df['Low'], df['Close'], df['Volume'], fastperiod=3, slowperiod=10)\n    df['ATR']    = ta.ATR(df['High'], df['Low'], df['Close'], timeperiod=14)\n    df['NATR']   = ta.NATR(df['High'], df['Low'], df['Close'], timeperiod=14)\n    df['Variance'] = ta.VAR(df['Close'], timeperiod=5, nbdev=1)\n    df['CORREL']   = ta.CORREL(df['High'], df['Low'], timeperiod=15)\n    df['TSF']      = ta.TSF(df['Close'], timeperiod=14) \n    df['TSF-14']   = ta.TSF(df['Close'], timeperiod=14).shift(-14)\n    df['TSF-7']    = ta.TSF(df['Close'], timeperiod=14).shift(-7)\n    df['ATR']    = ta.ATR(df['High'], df['Low'], df['Close'], timeperiod=14)\n    df['NATR']   = ta.NATR(df['High'], df['Low'], df['Close'], timeperiod=14)\n    \n    df['s_avg']      = df['SecuritiesCode'].apply(get_avg)\n    df['qhm_115v']   = stationary_state(df['s_avg'], 115) \n    \n    ema_set = [3,5,8,12,15,26,30,35,40,45,50,60, 100,200]\n    # EMA\n    for i in range(len(ema_set)):        \n        sma = df['Close'].rolling(ema_set[i]).mean()\n        ema = sma.ewm(span=ema_set[i], adjust=False).mean()\n        df[\"EMA_C_%d\"%(ema_set[i])] = ema\n                                                 \n        df = prep_prices(df)\n    \n    if tr:\n        df = df.drop(['RowId','AdjustmentFactor','ExpectedDividend','SupervisionFlag','AdjustedClose'],axis=1)\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-06-14T05:07:14.25019Z","iopub.execute_input":"2022-06-14T05:07:14.250562Z","iopub.status.idle":"2022-06-14T05:07:14.284965Z","shell.execute_reply.started":"2022-06-14T05:07:14.250526Z","shell.execute_reply":"2022-06-14T05:07:14.284148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = get_features(df)\ngroups   = pd.factorize(train_df.index)[0]\ntrain    = train_df.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-14T05:07:14.286444Z","iopub.execute_input":"2022-06-14T05:07:14.286908Z","iopub.status.idle":"2022-06-14T05:12:54.18841Z","shell.execute_reply.started":"2022-06-14T05:07:14.286844Z","shell.execute_reply":"2022-06-14T05:12:54.187652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"securities_code = train.pop(\"SecuritiesCode\")\ny               = train.pop(\"Target\")","metadata":{"execution":{"iopub.status.busy":"2022-06-14T05:12:54.189894Z","iopub.execute_input":"2022-06-14T05:12:54.190127Z","iopub.status.idle":"2022-06-14T05:12:54.197619Z","shell.execute_reply.started":"2022-06-14T05:12:54.190094Z","shell.execute_reply":"2022-06-14T05:12:54.196946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feat = train.columns\nfeat.shape[-1]","metadata":{"execution":{"iopub.status.busy":"2022-06-14T05:12:54.199031Z","iopub.execute_input":"2022-06-14T05:12:54.199304Z","iopub.status.idle":"2022-06-14T05:12:54.211133Z","shell.execute_reply.started":"2022-06-14T05:12:54.199255Z","shell.execute_reply":"2022-06-14T05:12:54.210337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del df\ndel train_df\ndel train","metadata":{"execution":{"iopub.status.busy":"2022-06-14T05:12:54.212578Z","iopub.execute_input":"2022-06-14T05:12:54.212982Z","iopub.status.idle":"2022-06-14T05:12:54.24234Z","shell.execute_reply.started":"2022-06-14T05:12:54.212945Z","shell.execute_reply":"2022-06-14T05:12:54.241554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def decode_function(record_bytes):\n      return tf.io.parse_single_example(\n      record_bytes,\n      {\n          \"features\": tf.io.FixedLenFeature([feat.shape[-1]], dtype=tf.float32),\n          \"securities_code\": tf.io.FixedLenFeature([], dtype=tf.int64),\n          \"Target\": tf.io.FixedLenFeature([], dtype=tf.float32)\n      }\n  )\ndef preprocess(item):\n    return (item[\"securities_code\"], item[\"features\"]), item[\"Target\"]\ndef make_dataset(file_paths, batch_size=4096, mode=\"train\"):\n    ds = tf.data.TFRecordDataset(file_paths)\n    ds = ds.map(decode_function)\n    ds = ds.map(preprocess)\n    if mode == \"train\":\n        ds = ds.shuffle(batch_size*4*REPLICAS)\n    ds = ds.batch(batch_size).cache().prefetch(tf.data.AUTOTUNE)\n    return ds","metadata":{"execution":{"iopub.status.busy":"2022-06-14T05:12:54.245177Z","iopub.execute_input":"2022-06-14T05:12:54.245429Z","iopub.status.idle":"2022-06-14T05:12:54.254372Z","shell.execute_reply.started":"2022-06-14T05:12:54.245405Z","shell.execute_reply":"2022-06-14T05:12:54.253621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def e_swish(beta=0.25):\n    def beta_swish(x): return x*K.sigmoid(x)*(1+beta)\n    return beta_swish","metadata":{"execution":{"iopub.status.busy":"2022-06-14T05:12:54.25578Z","iopub.execute_input":"2022-06-14T05:12:54.256319Z","iopub.status.idle":"2022-06-14T05:12:54.26566Z","shell.execute_reply.started":"2022-06-14T05:12:54.256265Z","shell.execute_reply":"2022-06-14T05:12:54.264807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def correlation(x, y, axis=-2):\n    \"\"\"Metric returning the Pearson correlation coefficient of two tensors over some axis, default -2.\"\"\"\n    x = tf.convert_to_tensor(x)\n    y = math_ops.cast(y, x.dtype)\n    n = tf.cast(tf.shape(x)[axis], x.dtype)\n    xsum = tf.reduce_sum(x, axis=axis)\n    ysum = tf.reduce_sum(y, axis=axis)\n    xmean = xsum / n\n    ymean = ysum / n\n    xvar = tf.reduce_sum( tf.math.squared_difference(x, xmean), axis=axis)\n    yvar = tf.reduce_sum( tf.math.squared_difference(y, ymean), axis=axis)\n    cov = tf.reduce_sum( (x - xmean) * (y - ymean), axis=axis)\n    corr = cov / tf.sqrt(xvar * yvar)\n    return tf.constant(1.0, dtype=x.dtype) - corr","metadata":{"execution":{"iopub.status.busy":"2022-06-14T05:12:54.267043Z","iopub.execute_input":"2022-06-14T05:12:54.26755Z","iopub.status.idle":"2022-06-14T05:12:54.277229Z","shell.execute_reply.started":"2022-06-14T05:12:54.267511Z","shell.execute_reply":"2022-06-14T05:12:54.276494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def correlationLoss(x,y, axis=-2):\n    \n    \"\"\"Loss function that maximizes the pearson correlation coefficient between the predicted values and the labels,\n    while trying to have the same mean and variance\"\"\"\n    x = tf.convert_to_tensor(x)\n    y = math_ops.cast(y, x.dtype)\n    n = tf.cast(tf.shape(x)[axis], x.dtype)\n    xsum = tf.reduce_sum(x, axis=axis)\n    ysum = tf.reduce_sum(y, axis=axis)\n    xmean = xsum / n\n    ymean = ysum / n\n    xsqsum = tf.reduce_sum( tf.math.squared_difference(x, xmean), axis=axis)\n    ysqsum = tf.reduce_sum( tf.math.squared_difference(y, ymean), axis=axis)\n    cov = tf.reduce_sum( (x - xmean) * (y - ymean), axis=axis)\n    corr = cov / tf.sqrt(xsqsum * ysqsum)\n    return tf.convert_to_tensor( K.mean(tf.constant(1.0, dtype=x.dtype) - corr ) , dtype=tf.float32 )","metadata":{"execution":{"iopub.status.busy":"2022-06-14T05:12:54.28171Z","iopub.execute_input":"2022-06-14T05:12:54.282494Z","iopub.status.idle":"2022-06-14T05:12:54.291041Z","shell.execute_reply.started":"2022-06-14T05:12:54.282452Z","shell.execute_reply":"2022-06-14T05:12:54.290103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sharpe_loss(X_train,y_pred):\n    y_pred = tf.Variable(y_pred,dtype=tf.float64)\n    port_ret = tf.reduce_sum(tf.multiply(X_train,y_pred),axis=1)\n    s_ratio = K.mean(port_ret)/K.std(port_ret)\n    \n    return tf.math.exp(-s_ratio,  name='sharpe_loss')","metadata":{"execution":{"iopub.status.busy":"2022-06-14T05:12:54.292531Z","iopub.execute_input":"2022-06-14T05:12:54.29311Z","iopub.status.idle":"2022-06-14T05:12:54.302165Z","shell.execute_reply.started":"2022-06-14T05:12:54.293072Z","shell.execute_reply":"2022-06-14T05:12:54.301455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_model(hp, dim=139, fold=0):\n\n    features_inputs = tf.keras.layers.Input(shape = (dim, ))\n    securities_code_inputs = tf.keras.Input((1, ), dtype=tf.uint32)\n    x0      =  tf.keras.layers.BatchNormalization()(features_inputs)\n\n    weight = tf.Variable(tf.keras.backend.random_normal((dim, 1), stddev=hp.Float(f'weight_{fold}',1e-10, 0.09), dtype=tf.float32))\n    var    = tf.Variable(tf.zeros((1,1), dtype=tf.float32))\n\n    securities_code_x = (securities_code_inputs)\n    securities_code_x = layers.Embedding(securities_codes_size, hp.Int(f'layer_emb{fold}',32, 512), input_length=1)(securities_code_x)\n    securities_code_x = layers.Reshape((-1, ))(securities_code_x)\n    securities_code_x = layers.Dense(hp.Int(f'layer_idx{fold}',32, 512), activation=e_swish(beta=hp.Float(f'e_idx_{fold}',0.15, 1, step=0.10)))(securities_code_x)\n    \n    encoder = tf.keras.layers.GaussianNoise(hp.Float(f'noise_{fold}',0.0001, 0.8))(x0)\n    encoder = tf.keras.layers.Dense(hp.Int(f'layers{fold}_en0',32, 1024))(encoder)\n    encoder = tf.keras.layers.Dense(hp.Int(f'layers{fold}_en1',32, 1024))(encoder)\n    encoder = tf.keras.layers.Dense(hp.Int(f'layers{fold}_en2',32, 1024))(encoder)\n    encoder = tf.keras.layers.BatchNormalization()(encoder)\n    encoder = tf.keras.layers.Activation(e_swish(beta=hp.Float(f'e{fold}_en0',0.001, 1 )))(encoder)\n    \n    decoder = tf.keras.layers.Dropout(hp.Float(f'dropout{fold}_de0',0.001, 0.8))(encoder)\n    decoder = tf.keras.layers.Dense(hp.Int(f'layers{fold}_de0',32, 1024), name='decoder')(decoder)\n\n    x_ae = tf.keras.layers.Concatenate()([securities_code_x, decoder])\n    x_ae = tf.keras.layers.Dense(hp.Int(f'layers{fold}_ae0',32, 1024))(x_ae)\n    x_ae = tf.keras.layers.BatchNormalization()(x_ae)\n    x_ae = tf.keras.layers.Activation(e_swish(beta=hp.Float(f'e{fold}_ae0',0.001, 1 )))(x_ae)\n    x_ae = tf.keras.layers.Dropout(hp.Float(f'dropout{fold}_ae0',0.001, 0.8))(x_ae) \n    \n    feature_x = tf.keras.layers.Concatenate()([x0, encoder])\n    feature_x = tf.keras.layers.BatchNormalization()(feature_x)\n    feature_x = tf.keras.layers.Dense(hp.Int(f'layers{fold}_fx0',32, 1024))(feature_x)\n    feature_x = tf.keras.layers.Activation(e_swish(beta=hp.Float(f'e_fx0',0.001, 1 )))(feature_x)\n    feature_x = tf.keras.layers.Dropout(hp.Float(f'dropout{fold}_fx0',0.001, 0.8))(feature_x)\n\n    x = layers.Concatenate(axis=1)([securities_code_x, feature_x])\n\n    x = layers.Dense(hp.Int(f'layers{fold}_x0',32, 1024), activation= e_swish(beta=hp.Float(f'e{fold}_x0',0.001, 1 )), kernel_regularizer=\"l2\")(x)\n    x = tf.keras.layers.Dropout(hp.Float(f'dropout{fold}_x0',0.0003, 0.9))(x)\n    x = layers.Dense(hp.Int(f'layers{fold}_x1',32, 1024), activation= e_swish(beta=hp.Float(f'e{fold}_x1',0.001, 1 )), kernel_regularizer=\"l2\")(x)\n    x = tf.keras.layers.Dropout(hp.Float(f'dropout{fold}_x1',0.0003, 0.9))(x)\n    x = layers.Dense(hp.Int(f'layers{fold}_x2',32, 1024), activation= e_swish(beta=hp.Float(f'e{fold}_x2',0.001, 1 )), kernel_regularizer=\"l2\")(x)\n    x = tf.keras.layers.Dropout(hp.Float(f'dropout{fold}_x2',0.0003, 0.9))(x)\n\n    mlp_out = layers.Dense(1, name ='mlp_out')(x)\n\n    model  = tf.keras.Model(inputs=[securities_code_inputs, features_inputs], outputs=[decoder, mlp_out])\n\n    loss_out = tf.add(tf.matmul(features_inputs,weight), tf.math.reduce_sum(weight*var))\n    tf.compat.v1.losses.add_loss(loss_out)\n  \n    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=hp.Float(f'lr_adam{fold}',1e-2, 1e-12)),\n                  loss = {'decoder': [tf.keras.losses.CosineSimilarity(axis=-2), \n                                      tf.keras.losses.MeanSquaredError(), \n                                      correlationLoss],         \n                          \n                          'mlp_out' : [tf.keras.losses.MeanSquaredError(), sharpe_loss],\n                         },\n                  metrics = {'decoder': [tf.keras.metrics.CosineSimilarity(name='cosine', axis=-2),\n                                         tf.keras.metrics.MeanAbsoluteError(name=\"mae\"), \n                                         correlation, \n                                         tf.keras.metrics.RootMeanSquaredError(name='rmse')], \n                             \n                             'mlp_out' : [tf.keras.metrics.CosineSimilarity(name='cosine', axis=-2),\n                                          tf.keras.metrics.MeanAbsoluteError(name=\"mae\"), \n                                          correlation, \n                                          tf.keras.metrics.RootMeanSquaredError(name='rmse')],\n                            },\n                 ) \n    return model","metadata":{"execution":{"iopub.status.busy":"2022-06-14T05:12:54.303837Z","iopub.execute_input":"2022-06-14T05:12:54.3044Z","iopub.status.idle":"2022-06-14T05:12:54.334545Z","shell.execute_reply.started":"2022-06-14T05:12:54.304358Z","shell.execute_reply":"2022-06-14T05:12:54.333777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hp = pd.read_pickle('../input/hp-jpx-aemlp/hp_aempp_tf_139f_5f_tf.pkl')\ntf.keras.utils.plot_model(build_model(hp), show_shapes=True, expand_nested=True, show_dtype=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-14T05:12:54.337921Z","iopub.execute_input":"2022-06-14T05:12:54.338118Z","iopub.status.idle":"2022-06-14T05:12:56.057085Z","shell.execute_reply.started":"2022-06-14T05:12:54.338094Z","shell.execute_reply":"2022-06-14T05:12:56.056333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_lr_callback(batch_size = 8):\n    lr_start   = 3e-3\n    lr_max     = 1.25e-10 * REPLICAS * batch_size\n    lr_min     = 3e-12\n    lr_ramp_ep = 3\n    lr_sus_ep  = 0\n    lr_decay   = 0.98\n    def lrfn(epoch):\n        if epoch < lr_ramp_ep: lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n        elif epoch < lr_ramp_ep + lr_sus_ep: lr = lr_max\n        else: lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n        return lr\n    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=False)\n    return lr_callback","metadata":{"execution":{"iopub.status.busy":"2022-06-14T05:12:56.058393Z","iopub.execute_input":"2022-06-14T05:12:56.058675Z","iopub.status.idle":"2022-06-14T05:12:56.067088Z","shell.execute_reply.started":"2022-06-14T05:12:56.058633Z","shell.execute_reply":"2022-06-14T05:12:56.066454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ngc.collect()\nmodels = []\n\nfold = [0,1,2,3,4]\nfor i in range(len(fold)):\n    train_path = f\"../input/jpx-cgkf-f98-tf/fold_{fold[i]}_train.tfrecords\"\n    valid_path = f\"../input/jpx-cgkf-f98-tf/fold_{fold[i]}_test.tfrecords\"\n    valid_ds   = make_dataset([valid_path], mode=\"valid\")\n    checkpoint = keras.callbacks.ModelCheckpoint(f\"model_{fold[i]}.tf\", monitor=\"val_mlp_out_loss\", mode=\"min\", save_best_only=True, save_weights_only=True)\n    early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_mlp_out_loss', patience=7, mode='min', restore_best_weights=True)\n    K.clear_session()\n    with strategy.scope(): model = build_model(hp, dim=feat.shape[-1], fold=fold[i])\n    if config.is_training:\n        train_ds = make_dataset([train_path])\n        history = model.fit(train_ds, epochs=100, validation_data=valid_ds, callbacks=[checkpoint, \n                                                                                       early_stop,\n                                                                                       get_lr_callback(batch_size = 4096*REPLICAS)])\n        gc.collect()\n        model.load_weights(f\"model_{fold[i]}.tf\")\n        \n        for metric in [\"mlp_out_loss\", \"mlp_out_rmse\", \"mlp_out_cosine\", \"mlp_out_correlation\"]:\n            pd.DataFrame(history.history, columns=[metric, f\"val_{metric}\"]).plot()\n            plt.title(metric.upper())\n            plt.show()\n            gc.collect()\n        \n    else:\n        model.load_weights(f\"{config.output_dataset_path}model_{fold[i]}.tf\")\n    y_vals = []\n    for _, y in valid_ds:\n        y_vals += list(y.numpy().reshape(-1))\n    y_val = np.array(y_vals)\n    pearson_score = stats.pearsonr((model.predict(valid_ds)[-1].reshape(-1)), y_val)[0]\n    models.append(model)\n    print(f\"Pearson Score: {pearson_score}\")\n    gc.collect()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-14T05:12:56.068662Z","iopub.execute_input":"2022-06-14T05:12:56.069159Z","iopub.status.idle":"2022-06-14T05:14:57.451443Z","shell.execute_reply.started":"2022-06-14T05:12:56.069121Z","shell.execute_reply":"2022-06-14T05:14:57.450668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_test(securities_code, feature):\n    return (securities_code, feature), 0\n\ndef make_test_dataset(feature, securities_code, batch_size=4096*REPLICAS):\n    ds = tf.data.Dataset.from_tensor_slices(((securities_code, feature)))\n    ds = ds.map(preprocess_test)\n    ds = ds.batch(batch_size).cache().prefetch(tf.data.AUTOTUNE)\n    return ds\n\ndef inference(models, ds):\n    y_preds = []\n    for model in models:\n        y_pred = model.predict(ds)[-1].reshape(-1)\n        y_preds.append(y_pred)\n    return np.mean(y_preds, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-06-14T05:14:57.453266Z","iopub.execute_input":"2022-06-14T05:14:57.453548Z","iopub.status.idle":"2022-06-14T05:14:57.461057Z","shell.execute_reply.started":"2022-06-14T05:14:57.453508Z","shell.execute_reply":"2022-06-14T05:14:57.460024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import jpx_tokyo_market_prediction\nenv = jpx_tokyo_market_prediction.make_env()   # initialize the environment\niter_test = env.iter_test()                   # an iterator which loops over the test files\nfor (prices, options, financials, trades, secondary_prices, sample_prediction) in iter_test:\n    \n    prices_df                        = get_features(prices, tr=False)\n    sample_prediction[\"Prediction\"]  = inference(models, make_test_dataset(prices_df[feat], prices['SecuritiesCode']))\n    sample_prediction[\"rate\"]        = sample_prediction[\"Prediction\"]\n    sample_prediction.sort_values(by = \"rate\", ascending=False, inplace=True)\n    sample_prediction.Rank           = np.arange(0,2000)\n    sample_prediction.sort_values(by = \"SecuritiesCode\", ascending=True, inplace=True)\n    submission                       = sample_prediction[[\"Date\",\"SecuritiesCode\",\"Rank\"]]\n    display(submission)\n    env.predict(submission)  ","metadata":{"execution":{"iopub.status.busy":"2022-06-14T05:17:07.236836Z","iopub.execute_input":"2022-06-14T05:17:07.237354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PREVIOUS LEVEL\n* [JPX(0/2): AEMLP-GKF: Natapong Nitarach](https://www.kaggle.com/code/natnitarach/jpx-aemlp-gkf)","metadata":{}},{"cell_type":"markdown","source":"# NEXT LEVEL\n* [JPX(2/2): LSTM+GRU-TF: Natapong Nitarach](https://www.kaggle.com/natnitarach/jpx-2-2-lstm-gru-tf/)","metadata":{}},{"cell_type":"markdown","source":"# CHANGE UNIVERSE\n* [Crypto Forecasting(1/1) : SAEMLP-QHO: Natapong Nitarach](https://www.kaggle.com/code/natnitarach/crypto-forecasting-1-1-saemlp-qho?scriptVersionId=87744356)","metadata":{}}]}