{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Preamble\n\n**I have seen some of the notebook which are using regular CV methods for time series data, I wanted to implement TS-CV technique by myself.**\n\n**Unfortunately, I couldn't manage to find a good implementation of TS-CV library, so I have created algorithm by myself.**","metadata":{}},{"cell_type":"markdown","source":"# Config Parameters and Imports","metadata":{}},{"cell_type":"code","source":"# You can tweak the hyperparameters for different results.\n# Best Model Hyperparameters (Optuna) {'num_leaves': 2818, 'n_estimators': 713, 'max_bin': 100, 'learning_rate': 0.6268164565853203} = Score: 0.537631972137107.\nclass CFG:\n    Debug = False # Enable/Disable debug mode True = Enable, False = Disable\n    \n    folds = 5\n    val_ratio = 20 # validation dataset to train dataset ratio in % format\n    \n    mean = True # if it is True, submission API uses the mean value of our folds, uses median value if it is False.\n    \n    seed = 1889\n    LR = 0.6268164565853203\n    num_leaves = 2818\n    n_estimators = 713\n    max_bin = 100","metadata":{"execution":{"iopub.status.busy":"2022-06-25T18:49:37.04281Z","iopub.execute_input":"2022-06-25T18:49:37.043161Z","iopub.status.idle":"2022-06-25T18:49:37.049561Z","shell.execute_reply.started":"2022-06-25T18:49:37.04313Z","shell.execute_reply":"2022-06-25T18:49:37.048414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\nimport pandas as pd \npd.options.mode.chained_assignment = None \n\nfrom lightgbm import LGBMRegressor\n\n#from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n#from sklearn.model_selection import cross_val_score\n\nfrom decimal import ROUND_HALF_UP, Decimal\n\n#import optuna\n\n!pip install ../input/talib0419/talib_binary-0.4.19-cp37-cp37m-manylinux1_x86_64.whl\nimport talib","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-25T18:49:37.050836Z","iopub.execute_input":"2022-06-25T18:49:37.051383Z","iopub.status.idle":"2022-06-25T18:49:37.065058Z","shell.execute_reply.started":"2022-06-25T18:49:37.051344Z","shell.execute_reply":"2022-06-25T18:49:37.064134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import Data and Handle Invalid Values\n\n**Be careful, do not drop NaN values before dropping Expected Dividend features, it causes a major issue.**","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('../input/jpx-tokyo-stock-exchange-prediction/train_files/stock_prices.csv')\ntrain_df = train_df.drop(['ExpectedDividend'], axis=1) # trivial imo\n#train_df = train_df.dropna() # DO NOT! (note to myself lol)\ntrain_df.interpolate(method='linear', inplace=True) # pandas interpolation fills NaN values with the mean of two upper and lower neighbour values.\ntrain_df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-06-25T18:49:45.320816Z","iopub.execute_input":"2022-06-25T18:49:45.321638Z","iopub.status.idle":"2022-06-25T18:49:52.150712Z","shell.execute_reply.started":"2022-06-25T18:49:45.321594Z","shell.execute_reply":"2022-06-25T18:49:52.149412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering\n\n**Now, we are going to create new features from existing features.**\n\n**Creating new features by combining existing features will help us to represent our dataset better.**\n\n**Here I just added 6 new features.**","metadata":{}},{"cell_type":"code","source":"def features(df):\n    \n    close = df['Close']\n    volume = df['Volume']\n    opening = df['Open']\n    high = df['High']\n    low = df['Low']\n    \n    \n    df['EMA'] = talib.EMA(close, timeperiod=30) # Exponential Moving Average\n    df['SMA'] = talib.SMA(close, timeperiod=30) # Simple Moving Average\n    df['RSI'] = talib.RSI(close, timeperiod=14) # Relative Strength Index\n    df['STDDEV'] = talib.STDDEV(close, timeperiod=5, nbdev=1) # Standard Deviation\n    df['macd'], df['macdsignal'], df['macdhist'] = talib.MACD(close, fastperiod=12, slowperiod=26, signalperiod=9) # Moving Average Convergence/Divergence\n    df['upperband'], df['middleband'], df['lowerband'] = talib.BBANDS(close, timeperiod=5, nbdevup=2, nbdevdn=2, matype=0) # Bollinger Bands\n    return df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Here, we are adjusting the close price according to Adjustment Factor. It is important, because adjustment factor is not comprehensive for our model.**","metadata":{}},{"cell_type":"code","source":"# https://www.kaggle.com/code/smeitoma/train-demo\n\ndef adjust_price(price):\n    \"\"\"\n    Args:\n        price (pd.DataFrame)  : pd.DataFrame include stock_price\n    Returns:\n        price DataFrame (pd.DataFrame): stock_price with generated AdjustedClose\n    \"\"\"\n    # transform Date column into datetime\n    price.loc[: ,\"Date\"] = pd.to_datetime(price.loc[: ,\"Date\"], format=\"%Y-%m-%d\")\n\n    def generate_adjusted_close(df):\n        \"\"\"\n        Args:\n            df (pd.DataFrame)  : stock_price for a single SecuritiesCode\n        Returns:\n            df (pd.DataFrame): stock_price with AdjustedClose for a single SecuritiesCode\n        \"\"\"\n        # sort data to generate CumulativeAdjustmentFactor\n        df = df.sort_values(\"Date\", ascending=False)\n        # generate CumulativeAdjustmentFactor\n        df.loc[:, \"CumulativeAdjustmentFactor\"] = df[\"AdjustmentFactor\"].cumprod()\n        # generate AdjustedClose\n        df.loc[:, \"AdjustedClose\"] = (\n            df[\"CumulativeAdjustmentFactor\"] * df[\"Close\"]\n        ).map(lambda x: float(\n            Decimal(str(x)).quantize(Decimal('0.1'), rounding=ROUND_HALF_UP)\n        ))\n        # reverse order\n        df = df.sort_values(\"Date\")\n        # to fill AdjustedClose, replace 0 into np.nan\n        df.loc[df[\"AdjustedClose\"] == 0, \"AdjustedClose\"] = np.nan\n        # forward fill AdjustedClose\n        df.loc[:, \"AdjustedClose\"] = df.loc[:, \"AdjustedClose\"].ffill()\n        return df\n\n    # generate AdjustedClose\n    price = price.sort_values([\"SecuritiesCode\", \"Date\"])\n    price = price.groupby(\"SecuritiesCode\").apply(generate_adjusted_close).reset_index(drop=True)\n\n    price.set_index(\"Date\", inplace=True)\n    return price\n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-25T04:19:38.10913Z","iopub.execute_input":"2022-06-25T04:19:38.110234Z","iopub.status.idle":"2022-06-25T04:19:56.174978Z","shell.execute_reply.started":"2022-06-25T04:19:38.110192Z","shell.execute_reply":"2022-06-25T04:19:56.173678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = adjust_price(train_df)\ntrain_df = train_df.groupby('SecuritiesCode').apply(features)\ntrain_df = train_df.dropna(axis=0).reset_index(drop=True) # drop\ntrain_df","metadata":{"execution":{"iopub.status.busy":"2022-06-25T04:19:56.176373Z","iopub.execute_input":"2022-06-25T04:19:56.176708Z","iopub.status.idle":"2022-06-25T04:19:56.208458Z","shell.execute_reply.started":"2022-06-25T04:19:56.176677Z","shell.execute_reply":"2022-06-25T04:19:56.207165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Nice.","metadata":{}},{"cell_type":"markdown","source":"# LGBM Model Implementation and Time Series Cross Validation Implementation\n\n**My Loss Metric is MSE(Mean Squared Error), it is your optional choice.**","metadata":{}},{"cell_type":"markdown","source":"**Optuna Results:**\n\nPhase 1 = {'num_leaves': 2818, 'n_estimators': 713, 'max_bin': 100, 'learning_rate': 0.6268164565853203} = 0.537631972137107.\n\nPhase 2 = {'num_leaves': 2818, 'n_estimators': 713, 'max_bin': 100, 'learning_rate': 0.6268164565853203} = 0.537631972137107.","metadata":{"_kg_hide-input":true}},{"cell_type":"code","source":"def split_group(df):\n    \"\"\"Splits groups by giving them unique labels.\"\"\"\n    df['fold'] = 0\n    num_split = len(df)//CFG.folds\n    next_val = 0\n    for i in range(1, CFG.folds+1):\n        df['fold'].iloc[num_split*next_val:num_split*(i)] = (i)\n        if (i) > next_val:\n            next_val = (i)\n    df.fold.loc[df['fold'] == 0] = CFG.folds\n    return df\n\ndef LGBM_Model(X_train, y_train, X_val, y_val, i):\n    if CFG.Debug:\n        model = LGBMRegressor()\n    else:\n        model = LGBMRegressor(num_leaves= CFG.num_leaves, learning_rate = CFG.LR, n_estimators = CFG.n_estimators, max_bin=CFG.max_bin)\n    \n    model.fit(X_train, y_train)\n    preds = model.predict(X_val)\n    score = np.sqrt(mean_squared_error(y_val, preds))\n    print(f'{i}. Fold MSE: {score:.5f}')\n    return model, score\n\ndef perc(num, percent):\n    \"\"\"Rounded percent calculator\"\"\"\n    return round(float(num/100)*percent)\n\ndef tscv(raw_df):\n    \"\"\"Creates a time series cross validation method, you can specify your model and percentage.\"\"\"\n    df = raw_df.copy()\n    df = split_group(df)\n    models = []\n    errs = []\n    #last_num = 0\n    for i in range(1, CFG.folds+1):\n        \n        rest_df = df.loc[df['fold'] <= i]\n        tmp_df = df.loc[df['fold'] == i]\n        #print(tmp_df)\n        rest_df = rest_df.sort_values(\"Date\", ascending=True)\n        tmp_df = tmp_df.sort_values(\"Date\", ascending=True)\n        #print(rest_df)\n        \n\n        X_train = rest_df[0:-perc(len(tmp_df), CFG.val_ratio)]\n        y_train = rest_df[0:-perc(len(tmp_df), CFG.val_ratio)]\n        \n\n        X_val = tmp_df[-perc(len(tmp_df), CFG.val_ratio):-1] \n        y_val = tmp_df[-perc(len(tmp_df), CFG.val_ratio):-1]\n        \n        X_train = X_train[['Open', 'High', 'Low', 'AdjustedClose', 'Volume']] # we include only these parameters.\n        y_train = y_train[['Target']]\n        \n        X_val = X_val[['Open', 'High', 'Low', 'AdjustedClose', 'Volume']]\n        y_val = y_val[['Target']]\n        \n        model, err = LGBM_Model(X_train, y_train, X_val, y_val, i)\n        errs.append(err)\n        models.append(model)\n        if len(errs) == CFG.folds:\n            print('\\n')\n            print('-'*30)\n            print(f'\\nAverage MSE is: {np.mean(errs):.5f}')\n            \n        #if i == 3:\n         #   break\n    return models\n\n            \nmodels = tscv(train_df)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T04:20:30.352059Z","iopub.execute_input":"2022-06-25T04:20:30.352521Z","iopub.status.idle":"2022-06-25T04:20:47.306528Z","shell.execute_reply.started":"2022-06-25T04:20:30.35248Z","shell.execute_reply":"2022-06-25T04:20:47.305159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submit by Competition API","metadata":{}},{"cell_type":"code","source":"import jpx_tokyo_market_prediction\nenv = jpx_tokyo_market_prediction.make_env()\niter_test = env.iter_test()","metadata":{"execution":{"iopub.status.busy":"2022-06-25T04:20:54.572445Z","iopub.execute_input":"2022-06-25T04:20:54.573196Z","iopub.status.idle":"2022-06-25T04:20:54.582864Z","shell.execute_reply.started":"2022-06-25T04:20:54.573157Z","shell.execute_reply":"2022-06-25T04:20:54.581389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for (stock_prices, options, financials, trades, secondary_prices, sample_prediction) in iter_test:\n    \n    stock_prices = adjust_price(stock_prices)\n    X_test = stock_prices[['Open', 'High', 'Low', 'AdjustedClose', 'Volume']]\n    preds = []\n    for model in models:\n        preds.append(model.predict(X_test))\n    preds = np.mean(preds, axis=0) if CFG.mean else np.median(preds, axis=0)\n\n    sample_prediction[\"Prediction\"] = preds\n    sample_prediction = sample_prediction.sort_values(by = \"Prediction\", ascending=False)\n    sample_prediction.Rank = np.arange(0,2000)\n    sample_prediction = sample_prediction.sort_values(by = \"SecuritiesCode\", ascending=True)\n    sample_prediction.drop([\"Prediction\"],axis=1)\n    submission = sample_prediction[[\"Date\",\"SecuritiesCode\",\"Rank\"]]\n    env.predict(submission)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T04:20:56.328355Z","iopub.execute_input":"2022-06-25T04:20:56.329227Z","iopub.status.idle":"2022-06-25T04:21:13.17808Z","shell.execute_reply.started":"2022-06-25T04:20:56.32918Z","shell.execute_reply":"2022-06-25T04:21:13.176588Z"},"trusted":true},"execution_count":null,"outputs":[]}]}