{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-07-01T02:21:25.37553Z","iopub.execute_input":"2022-07-01T02:21:25.37619Z","iopub.status.idle":"2022-07-01T02:21:25.440165Z","shell.execute_reply.started":"2022-07-01T02:21:25.376025Z","shell.execute_reply":"2022-07-01T02:21:25.439165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import necessary libraries\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport seaborn as sns\n%matplotlib inline\nfrom matplotlib.pylab import rcParams\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nfrom scipy import stats\nimport random\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.datasets import make_regression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RandomizedSearchCV\n\nimport jpx_tokyo_market_prediction","metadata":{"execution":{"iopub.status.busy":"2022-07-01T02:21:26.031215Z","iopub.execute_input":"2022-07-01T02:21:26.032124Z","iopub.status.idle":"2022-07-01T02:21:27.296944Z","shell.execute_reply.started":"2022-07-01T02:21:26.032067Z","shell.execute_reply":"2022-07-01T02:21:27.296019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#financials = pd.read_csv(\"../input/jpx-tokyo-stock-exchange-prediction/train_files/financials.csv\")\n#stock_list = pd.read_csv(\"../input/jpx-tokyo-stock-exchange-prediction/stock_list.csv\")\nprices = pd.read_csv(\"../input/jpx-tokyo-stock-exchange-prediction/train_files/stock_prices.csv\")\n#financials = pd.read_csv(\"../input/jpx-tokyo-stock-exchange-prediction/train_files/financials.csv\")\n#options = pd.read_csv(\"../input/jpx-tokyo-stock-exchange-prediction/train_files/options.csv\")\n#sprices = pd.read_csv(\"../input/jpx-tokyo-stock-exchange-prediction/train_files/secondary_stock_prices.csv\")\nsupplemental_prices = pd.read_csv(\"../input/jpx-tokyo-stock-exchange-prediction/supplemental_files/stock_prices.csv\")\n#supplemental_sprices = pd.read_csv(\"../input/jpx-tokyo-stock-exchange-prediction/supplemental_files/secondary_stock_prices.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-07-01T02:21:27.298428Z","iopub.execute_input":"2022-07-01T02:21:27.299405Z","iopub.status.idle":"2022-07-01T02:21:35.811223Z","shell.execute_reply.started":"2022-07-01T02:21:27.299358Z","shell.execute_reply":"2022-07-01T02:21:35.809969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prices=prices.append(supplemental_prices,ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2022-07-01T02:21:35.813637Z","iopub.execute_input":"2022-07-01T02:21:35.814204Z","iopub.status.idle":"2022-07-01T02:21:36.08529Z","shell.execute_reply.started":"2022-07-01T02:21:35.814146Z","shell.execute_reply":"2022-07-01T02:21:36.083765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prices['DateValue']=prices['Date'].str.replace('-','')\nsmall_prices = prices[prices['DateValue']>'20180620']\nsmall_prices=small_prices.drop(['DateValue'],axis=1)\nsmall_prices.info()","metadata":{"execution":{"iopub.status.busy":"2022-07-01T02:21:36.089369Z","iopub.execute_input":"2022-07-01T02:21:36.090584Z","iopub.status.idle":"2022-07-01T02:21:39.525814Z","shell.execute_reply.started":"2022-07-01T02:21:36.090514Z","shell.execute_reply":"2022-07-01T02:21:39.524496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering Functions","metadata":{}},{"cell_type":"code","source":"def feature_train(train):\n    \n    #train = train.groupby(\"SecuritiesCode\")\n    \n    # Add Lag Features\n    lag_features = [\"High\", \"Low\", \"Volume\", \"Close\", \"Open\"]\n    df_rolled_7d = train[lag_features].rolling(window=4, min_periods=0)\n    df_mean_7d = df_rolled_7d.mean().shift(1).reset_index().astype(np.float32)\n    df_mean_7d = df_mean_7d.drop('index', axis=1)\n    df_mean_7d = df_mean_7d.fillna(0)\n    df_mean_7d = df_mean_7d.round(2)\n    df_mean_7d['High_lag_1'] = df_mean_7d['High']\n    df_mean_7d['Open_lag_1'] = df_mean_7d['Open']\n    df_mean_7d['Close_lag_1'] = df_mean_7d['Close']\n    df_mean_7d['Volume_lag_1'] = df_mean_7d['Volume']\n    df_mean_7d['Low_lag_1'] = df_mean_7d['Low']\n    df_mean_7d = df_mean_7d.drop([\"High\", \"Low\", \"Volume\", \"Close\", \"Open\"], axis=1)\n    train = train.reset_index(drop=True)\n    train = pd.concat([train, df_mean_7d], axis=1)\n\n    # Convert Date to float\n    train['Date_Float'] = train['Date'].str.replace('-', '')\n    train['Date_Float'] = train['Date_Float'].astype(float) \n    \n    # Drop irrelevant columns for training\n    train = train.drop(\n                        ['RowId', \n                         'SupervisionFlag',\n                         'AdjustmentFactor'], axis=1)\n    \n    # Bool to int for SupervisionFlag\n    #train[\"SupervisionFlag\"] = train[\"SupervisionFlag\"].astype(int)\n    \n    # Backward, then forward fill missing values in cols\n    cols = ['Target','Open', 'High', 'Low', 'Close']\n    train.loc[:,cols] = train.loc[:,cols].bfill()\n    train.loc[:,cols] = train.loc[:,cols].ffill()\n    \n    # Add Spread Features\n    train['Daily_Spread'] = train['Close'] - train['Open']\n    train['Daily_Max_Min'] = train['High'] - train['Low']\n    train['1_Day_Spread_Close'] = train['Close'].diff()\n    train['2_Day_Spread_Close'] = train['Close'].diff(periods=2)\n    train['1_Day_Spread_Open'] = train['Open'].diff()\n    train['2_Day_Spread_Open'] = train['Open'].diff(periods=2)\n    train['1_Week_Spread'] = train['Close'].diff(periods=5)\n    \n    # Fill NaN's \n    train = train.fillna(0)\n    \n    # Add Return Features\n    train['Return_Lag_1_Close'] = (train['Close'] - train['1_Day_Spread_Close'])/train['Close']\n    train['Return_Lag_2_Close'] = (train['Close'] - train['2_Day_Spread_Close'])/train['Close']\n    \n    train['Return_Lag_1_Open'] = (train['Open'] - train['1_Day_Spread_Open'])/train['Open']\n    train['Return_Lag_2_Open'] = (train['Open'] - train['2_Day_Spread_Open'])/train['Open'] \n\n    # Fill missing and inf/-inf values with 0\n    train.replace([np.inf, -np.inf], 0, inplace=True)\n    train = train.fillna(0)\n        \n    # Add rolling ratio of mean/std of forward 1 day return\n    indexer = pd.api.indexers.FixedForwardWindowIndexer(window_size=5)\n    train['ExPost_SR_Close'] = (train['Return_Lag_1_Close'].rolling(\n        window=indexer, min_periods=1).mean())/(\n        train['Return_Lag_1_Close'].std())\n    \n    train['ExPost_SR_2_Close'] = (train['Return_Lag_2_Close'].rolling(\n        window=indexer, min_periods=1).mean())/(\n        train['Return_Lag_2_Close'].std())\n    \n    train['ExPost_SR_Open'] = (train['Return_Lag_1_Open'].rolling(\n        window=indexer, min_periods=1).mean())/(\n        train['Return_Lag_1_Open'].std())\n    \n    train['ExPost_SR_2_Open'] = (train['Return_Lag_2_Open'].rolling(\n        window=indexer, min_periods=1).mean())/(\n        train['Return_Lag_2_Open'].std())\n    \n    # Fill missing and inf/-inf values with 0\n    train.replace([np.inf, -np.inf], 0, inplace=True)\n    train = train.fillna(0)\n    \n    # Fill missing values with 0\n    #train = train.fillna(0)\n    \n    return train","metadata":{"execution":{"iopub.status.busy":"2022-06-30T21:48:14.265379Z","iopub.execute_input":"2022-06-30T21:48:14.265594Z","iopub.status.idle":"2022-06-30T21:48:14.290362Z","shell.execute_reply.started":"2022-06-30T21:48:14.265568Z","shell.execute_reply":"2022-06-30T21:48:14.289229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Min Max Scaler Function","metadata":{}},{"cell_type":"code","source":"def min_max(df):\n    # MinMax Scale columns (-1, 1 scale)   \n    scaler = MinMaxScaler(feature_range=(-1, 1))\n    scaled = scaler.fit_transform(df)\n    train_cols = df.columns.values.tolist()\n    trained = pd.DataFrame(data=scaled, columns=train_cols, index=df.index)\n    return trained","metadata":{"execution":{"iopub.status.busy":"2022-06-30T21:48:14.291859Z","iopub.execute_input":"2022-06-30T21:48:14.292363Z","iopub.status.idle":"2022-06-30T21:48:14.307763Z","shell.execute_reply.started":"2022-06-30T21:48:14.292327Z","shell.execute_reply":"2022-06-30T21:48:14.306501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build Training DataFrame Function","metadata":{}},{"cell_type":"code","source":"def build_train(df):\n    # Groupby for feature engineering by securities code\n    stock_list_df = df.groupby(\"SecuritiesCode\").apply(feature_train)\n\n    # reset index\n    stock_list_df = stock_list_df.reset_index(drop=True)\n\n    # Create Securities Code df\n    sec_code_df = stock_list_df[['SecuritiesCode', 'Date']]\n    \n    # Drop Date column\n    stock_list_df = stock_list_df.drop('Date', axis=1)\n\n    # min max scale prices df\n    stock_list_df = min_max(stock_list_df)\n\n    # drop SecuritiesCode column before adding it back\n    stock_list_df = stock_list_df.drop(['SecuritiesCode'], axis=1)\n\n    # Concat dfs together\n    stock_list_df = pd.concat([sec_code_df, stock_list_df], axis=1)\n\n    # Sort df by date\n    stock_list_df = stock_list_df.sort_values(by='Date')\n\n    return stock_list_df","metadata":{"execution":{"iopub.status.busy":"2022-06-30T22:11:40.288082Z","iopub.execute_input":"2022-06-30T22:11:40.288402Z","iopub.status.idle":"2022-06-30T22:11:40.295428Z","shell.execute_reply.started":"2022-06-30T22:11:40.288372Z","shell.execute_reply":"2022-06-30T22:11:40.294556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"env = jpx_tokyo_market_prediction.make_env()\niter_test = env.iter_test()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T20:12:39.063092Z","iopub.execute_input":"2022-06-30T20:12:39.063479Z","iopub.status.idle":"2022-06-30T20:12:39.068979Z","shell.execute_reply.started":"2022-06-30T20:12:39.063437Z","shell.execute_reply":"2022-06-30T20:12:39.067961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for (prices, options, financials, trades, secondary_prices, sample_prediction) in iter_test:\n    date_df = prices[['Date', 'SecuritiesCode']]\n\n    # Append small_prices and prices\n    concat_df = small_prices.append(prices)\n    train_df = build_train(concat_df) \n\n    # Create test_df from prices - date_df\n    df_test = pd.merge(date_df, train_df, how='left', on=[\"Date\",\"SecuritiesCode\"])\n    df_test = df_test.drop('Target', axis=1)\n\n    # Drop Date column from df_test\n    df_test = df_test.drop('Date', axis=1)\n\n    # Cut out last date as it is likely the prices data\n    single_model = train_df.loc[train_df['Date_Float'] != 1.0]\n\n    # Drop Date column from multi_model\n    single_model = single_model.drop('Date', axis=1)\n    \n    # create Train X and y df's\n    X_train = single_model.drop('Target', axis=1)\n    y_train = single_model['Target']\n\n    # create Test X and y df's\n    X_test = df_test\n\n    # Instantiate and Fit Model\n    rf = RandomForestRegressor(criterion=\"absolute_error\", random_state = 42)\n\n    # Train the model on training data\n    rf.fit(X_train, y_train);\n\n    # df of preds by sec code\n    predictions = rf.predict(X_test)\n\n    # Create predictions df\n    sample_prediction['Target'] = predictions\n    \n    # Sort predictions df by Target\n    sample_prediction = sample_prediction.sort_values(by = \"Target\", ascending = False)\n    \n    # Rank predictions df by Target, create Rank column\n    sample_prediction['Rank'] = np.arange(len(sample_prediction.index))\n    \n    # Sort predictions column by Securities Code\n    sample_prediction = sample_prediction.sort_values(by = \"SecuritiesCode\", ascending = True)\n    \n    # Drop Target feature\n    sample_prediction.drop([\"Target\"], axis = 1)\n    \n    # Create Submission df\n    submission = sample_prediction[[\"Date\", \"SecuritiesCode\", \"Rank\"]]    \n    \n    env.predict(submission)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T20:12:39.736317Z","iopub.execute_input":"2022-06-30T20:12:39.736675Z","iopub.status.idle":"2022-06-30T20:31:38.564769Z","shell.execute_reply.started":"2022-06-30T20:12:39.736634Z","shell.execute_reply":"2022-06-30T20:31:38.563002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(prices)","metadata":{"execution":{"iopub.status.busy":"2022-06-08T21:55:29.227823Z","iopub.execute_input":"2022-06-08T21:55:29.22833Z","iopub.status.idle":"2022-06-08T21:55:29.246855Z","shell.execute_reply.started":"2022-06-08T21:55:29.228284Z","shell.execute_reply":"2022-06-08T21:55:29.24559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission","metadata":{"execution":{"iopub.status.busy":"2022-06-30T20:38:24.458384Z","iopub.execute_input":"2022-06-30T20:38:24.458695Z","iopub.status.idle":"2022-06-30T20:38:24.472005Z","shell.execute_reply.started":"2022-06-30T20:38:24.458658Z","shell.execute_reply":"2022-06-30T20:38:24.471433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}