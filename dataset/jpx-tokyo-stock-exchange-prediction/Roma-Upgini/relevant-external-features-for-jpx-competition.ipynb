{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# üîé How to find relevant external features & data for kaggle competitions in 10 minutes\n\n### Table of contents\n* [Intro](#Intro)\n* [How external data & features might help on Kaggle?](#How-external-data-&-features-might-help-on-Kaggle?)\n* [Packages and functions](#Packages-and-functions)\n* [Find relevant external features](#2Ô∏è‚É£-Find-relevant-external-features) \n* [Submission](#3Ô∏è‚É£-Submition) \n* [Relevant external features & data sources](#%F0%9F%8C%8E-Relevant-external-features-&-data-sources)\n\n## Intro\n**Competition**: [JPX Tokyo Stock Exchange Prediction](https://www.kaggle.com/competitions/jpx-tokyo-stock-exchange-prediction), the Sharpe Ratio of the daily spread returns as a target metric  \n\nüìö In this notebook we'll use:\n* [Upgini](https://github.com/upgini/upgini#readme) - Low-code Feature search and enrichment library for supervised machine learning applications.   \n<a href=\"https://github.com/upgini/upgini\">\n    <img src=\"https://img.shields.io/badge/GitHub-100000?style=for-the-badge&logo=github&logoColor=white\"  align='center'></a>  ","metadata":{}},{"cell_type":"markdown","source":"## How external data & features might help on Kaggle?\nKaggle is always about learning and leader board progress (hopefully from learning, not cheating ;-))  \nAnd every Kaggler wants to progress as fast as possible, so time saving tips & tricks is a big deal as well.  \nThat's why low-code tools is adopted among kagglers.\n\nSo, there are **two major scenarios** of external features & data introduction in competitions on Kaggle:\n\n1. **Final improvement of a polished kernel**  \nIn this scenario you want **to improve already polished kernel** (optimized features, model architecture and hyperparams) with new external features.  \nBefore that, most of the juice already has been \"squeezed\" from competition data by significant efforts in feature engineering.  \nAnd you want to answer the simple question - *Is there any external data sources and features which might boost accuracy a bit more?*  \nHowever, there is a caveat to this approach: current model architecture & hyperparameters might be suboptimal for the new feature set, after introduction even single new var.  \nSo extra step back for model tuning might be needed.\n\n2. **Low-code initial feature engineering - add relevant external features @start**  \nHere you want to **save time on feature search and engineering**. If there are some ready-to-use external features and data, let's use it to speed up the overall progress.  \nIn this scenario always make sense to check that new external features have optimal representation for specific task and target model architecture. Example - category features for linear regression models should be one-hot-encoded.\nThis type of feature preparation should be done manually in any case.  \nSame as scenario #1, there is a caveat to this approach: a lot of features not always a good thing - they might lead to dimensionality increase and model overfitting.  \nSo you have to check model accuracy improvement metrics after enrichment with the new features and ALWAYS with appropriate cross-validation strategy.\n \nIn this Notebook we'll go with **Scenario #2** but we will stop on the step of searching new external features.\nExample of full Scenario you may find here: [**Scenario #2**](https://www.kaggle.com/code/romaupgini/zero-feature-engineering-with-upgini-pycaret).","metadata":{}},{"cell_type":"markdown","source":"## Packages and functions","metadata":{}},{"cell_type":"code","source":"%pip install -Uq upgini","metadata":{"execution":{"iopub.status.busy":"2022-06-08T14:55:45.530304Z","iopub.execute_input":"2022-06-08T14:55:45.531259Z","iopub.status.idle":"2022-06-08T14:56:03.438474Z","shell.execute_reply.started":"2022-06-08T14:55:45.531136Z","shell.execute_reply":"2022-06-08T14:56:03.437203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.tree import DecisionTreeRegressor\nfrom tqdm.notebook import tqdm\nimport optuna\noptuna.logging.set_verbosity(optuna.logging.CRITICAL)\nimport jpx_tokyo_market_prediction","metadata":{"execution":{"iopub.status.busy":"2022-06-08T14:56:03.440673Z","iopub.execute_input":"2022-06-08T14:56:03.441055Z","iopub.status.idle":"2022-06-08T14:56:05.11547Z","shell.execute_reply.started":"2022-06-08T14:56:03.441018Z","shell.execute_reply":"2022-06-08T14:56:05.114132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Low-code feature engineering - add relevant external features @start and without manual effort\nThe main idea of this notebook is to build baseline solution using only low-code Machine Learning tools. Namely, we search, generate and select relevant features with Upgini, then we prepare data and build final model with PyCaret. \n\nThe entire code of data preparation, feature engineering and modelling takes only a few lines, so you don't have to spend a lot of time doing all these operations manually.","metadata":{}},{"cell_type":"markdown","source":"## 1Ô∏è‚É£ Read train & test data\nRead train/test data from csv and combine them in one dataframe:","metadata":{}},{"cell_type":"code","source":"path = \"../input/jpx-tokyo-stock-exchange-prediction/\"\ndf_prices = pd.read_csv(f\"{path}train_files/stock_prices.csv\")\ndf_prices = df_prices[~df_prices[\"Target\"].isnull()]\nprices = pd.read_csv(f\"{path}supplemental_files/stock_prices.csv\")\ndf_prices = pd.concat([df_prices, prices])\ndf_prices['Date']=pd.to_datetime(df_prices['Date'], format='%Y-%m-%d')","metadata":{"execution":{"iopub.status.busy":"2022-06-08T14:56:05.119705Z","iopub.execute_input":"2022-06-08T14:56:05.120513Z","iopub.status.idle":"2022-06-08T14:56:13.072978Z","shell.execute_reply.started":"2022-06-08T14:56:05.120461Z","shell.execute_reply":"2022-06-08T14:56:13.071876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's predict logarithm of the \"Target\" field, instead of predicting \"Target\" field directly:","metadata":{}},{"cell_type":"code","source":"df_prices[\"Target\"] = np.log1p(df_prices[\"Target\"])\ndf_prices[\"country\"] = 'JP'","metadata":{"execution":{"iopub.status.busy":"2022-06-08T14:56:13.076278Z","iopub.execute_input":"2022-06-08T14:56:13.0768Z","iopub.status.idle":"2022-06-08T14:56:13.114126Z","shell.execute_reply.started":"2022-06-08T14:56:13.076751Z","shell.execute_reply":"2022-06-08T14:56:13.112917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2Ô∏è‚É£ Find relevant external features\nTo find new features we'll use [Upgini Feature search and enrichment library for supervised machine learning applications](https://github.com/upgini/upgini#readme)  \nTo initiate search with Upgini library, you need to define so called [*search keys*](https://github.com/upgini/upgini#-search-key-types-we-support-more-is-coming) - a set of columns to join external data sources. In this competition we can use the following keys:\n\n1. Column **Date** should be used as **SearchKey.DATE**.;  \n2. Column **country** (after conversion to ISO-3166 country code) should be used as **SearchKey.COUNTRY**.\n    \nWith this set of search keys, our dataset will be matched with [different time-specific features (such as weather data, calendar data, financial data, etc)](https://github.com/upgini/upgini#-connected-data-sources-and-coverage), taking into account the country where sales happened. Than relevant selection and ranking will be done.  \nAs a result, we'll add new, only relevant features with additional information about specific dates and countries.","metadata":{}},{"cell_type":"code","source":"from upgini import SearchKey \n\n\n## define search keys\nsearch_keys = {\n    \"Date\": SearchKey.DATE,\n    \"country\": SearchKey.COUNTRY\n}","metadata":{"execution":{"iopub.status.busy":"2022-06-08T14:56:13.115445Z","iopub.execute_input":"2022-06-08T14:56:13.115792Z","iopub.status.idle":"2022-06-08T14:56:15.104736Z","shell.execute_reply.started":"2022-06-08T14:56:13.11576Z","shell.execute_reply":"2022-06-08T14:56:15.103644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To start the search, we need to initiate *scikit-learn* compartible `FeaturesEnricher` transformer with appropriate **search** parameters and cross-validation type (here we use [TimeSeries](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html) CV, because our target variable strongly depends on time, ie we have TS prediction task).  \nAfter that, we can call the **fit** method of `features_enricher` to start the search.","metadata":{}},{"cell_type":"code","source":"from upgini import FeaturesEnricher\nfrom upgini.metadata import CVType, RuntimeParameters\n\n## define X_train & y_train\nX_train = df_prices[[\"Date\", \"country\", \"SecuritiesCode\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"AdjustmentFactor\", \"ExpectedDividend\", \"SupervisionFlag\" ]]\ny_train = df_prices.Target\n\n## define FeaturesEnricher\nfeatures_enricher = FeaturesEnricher(\n    search_keys=search_keys, \n    cv=CVType.time_series\n)","metadata":{"execution":{"iopub.status.busy":"2022-06-08T14:56:15.106138Z","iopub.execute_input":"2022-06-08T14:56:15.106604Z","iopub.status.idle":"2022-06-08T14:56:15.374723Z","shell.execute_reply.started":"2022-06-08T14:56:15.106559Z","shell.execute_reply":"2022-06-08T14:56:15.369758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`FeaturesEnricher.fit()` has a flag `calculate_metrics` for the quick estimation of quality improvement on cross-validation and eval sets. This step is quite similar to [sklearn.model_selection.cross_val_score](https://scikit-learn.org/stable/modules/cross_validation.html#computing-cross-validated-metrics), so you can pass exact metric with `scoring` parameter:\n\n1. Built-in scoring [functions](https://github.com/upgini/upgini/blob/main/README.md#-accuracy-and-uplift-metrics-calculations) (in this case - scorer based on Mean Squared Error);\n2. Custom scorer.    \n\nNotice that you should pass **X_train** as the first argument and **y_train** as the second argument for `FeaturesEnricher.fit()`, just like in scikit-learn.  \n\nIt will take some time (2-5 minutes).","metadata":{}},{"cell_type":"code","source":"features_enricher.fit(X_train, y_train, calculate_metrics=True, max_features=1)","metadata":{"execution":{"iopub.status.busy":"2022-06-08T14:56:15.376528Z","iopub.status.idle":"2022-06-08T14:56:15.377767Z","shell.execute_reply.started":"2022-06-08T14:56:15.377509Z","shell.execute_reply":"2022-06-08T14:56:15.377538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We've got some relevant features, which might improve accuracy of the model, ranked by SHAP values.\n\nUplift after enrichment with all of the new external features is positive - so, the features from search actually contain some useful information about our target variable. Let's enrich initial feature space with found features.\n\nStep will take around 10 minutes","metadata":{}},{"cell_type":"code","source":"import gc\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-06-08T14:56:15.37909Z","iopub.status.idle":"2022-06-08T14:56:15.380043Z","shell.execute_reply.started":"2022-06-08T14:56:15.379599Z","shell.execute_reply":"2022-06-08T14:56:15.379625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Utilities \n\ndef calc_spread_return_per_day(df, portfolio_size, toprank_weight_ratio):\n    weights = np.linspace(start=toprank_weight_ratio, stop=1, num=portfolio_size)\n    weights_mean = weights.mean()\n    df = df.sort_values(by='Rank')\n    purchase = (df['Target'][:portfolio_size]  * weights).sum() / weights_mean\n    short    = (df['Target'][-portfolio_size:] * weights[::-1]).sum() / weights_mean\n    return purchase - short\n\ndef calc_spread_return_sharpe(df, portfolio_size=200, toprank_weight_ratio=2):\n    grp = df.groupby('Date')\n    min_size = grp[\"Target\"].count().min()\n    if min_size<2*portfolio_size:\n        portfolio_size=min_size//2\n        if portfolio_size<1:\n            return 0, None\n    buf = grp.apply(calc_spread_return_per_day, portfolio_size, toprank_weight_ratio)\n    sharpe_ratio = buf.mean() / buf.std()\n    return sharpe_ratio, buf\n\ndef add_rank(df, col_name=\"pred\"):\n    df[\"Rank\"] = df.groupby(\"Date\")[col_name].rank(ascending=False, method=\"first\") - 1 \n    df[\"Rank\"] = df[\"Rank\"].astype(\"int\")\n    return df\n\ndef fill_nans(prices):\n    prices.set_index([\"SecuritiesCode\", \"Date\"], inplace=True)\n    prices.ExpectedDividend.fillna(0,inplace=True)\n    prices.ffill(inplace=True)\n    prices.fillna(0,inplace=True)\n    prices.reset_index(inplace=True)\n    return prices","metadata":{"execution":{"iopub.status.busy":"2022-06-08T14:56:15.38237Z","iopub.status.idle":"2022-06-08T14:56:15.383362Z","shell.execute_reply.started":"2022-06-08T14:56:15.383046Z","shell.execute_reply":"2022-06-08T14:56:15.383082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = \"../input/jpx-tokyo-stock-exchange-prediction/\"\ndf_prices = pd.read_csv(f\"{path}train_files/stock_prices.csv\")\ndf_prices = df_prices[~df_prices[\"Target\"].isnull()]\nprices = pd.read_csv(f\"{path}supplemental_files/stock_prices.csv\")\ndf_prices = pd.concat([df_prices, prices])\ndf_prices['Date']=pd.to_datetime(df_prices['Date'], format='%Y-%m-%d')\nprices['Date']=pd.to_datetime(prices['Date'], format='%Y-%m-%d')\n\ndf_prices = fill_nans(df_prices)\nprices = fill_nans(prices)\npd.options.display.float_format = '{:,.6g}'.format\ndf_prices.describe()","metadata":{"execution":{"iopub.status.busy":"2022-06-08T14:56:15.385296Z","iopub.status.idle":"2022-06-08T14:56:15.386317Z","shell.execute_reply.started":"2022-06-08T14:56:15.385993Z","shell.execute_reply":"2022-06-08T14:56:15.386029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## call transform anaggd enrich dataset\ndf_prices[\"country\"] = 'JP'\ndf_prices = features_enricher.transform(df_prices, keep_input=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-08T14:56:15.388204Z","iopub.status.idle":"2022-06-08T14:56:15.389235Z","shell.execute_reply.started":"2022-06-08T14:56:15.38886Z","shell.execute_reply":"2022-06-08T14:56:15.388893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prices[\"country\"] = 'JP'\nprices = features_enricher.transform(prices, keep_input=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-08T14:56:15.391037Z","iopub.status.idle":"2022-06-08T14:56:15.392016Z","shell.execute_reply.started":"2022-06-08T14:56:15.391689Z","shell.execute_reply":"2022-06-08T14:56:15.391723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3Ô∏è‚É£ Submition\nLet's estimate model quality and make a submission.\n\n**Special thanks**: PAULO PINTO [@paulorzp](https://www.kaggle.com/paulorzp) for the public [notebook] (https://www.kaggle.com/code/paulorzp/jpx-simple-overfitting-model-lb-3/notebook?scriptVersionId=94507587) ","metadata":{}},{"cell_type":"code","source":"## By Yuike - https://www.kaggle.com/code/ikeppyo/examples-of-higher-scores-than-perfect-predictions\n\n# This function adjusts the predictions so that the daily spread return approaches a certain value.\n        \ndef adjuster(df):\n    def calc_pred(df, x, y, z):\n        return df['Target'].where(df['Target'].abs() < x, df['Target'] * y + np.sign(df['Target']) * z)\n\n    def objective(trial, df):\n        x = trial.suggest_uniform('x', 0, 0.2)\n        y = trial.suggest_uniform('y', 0, 0.05)\n        z = trial.suggest_uniform('z', 0, 1e-3)\n        df[\"Rank\"] = calc_pred(df, x, y, z).rank(ascending=False, method=\"first\") - 1 \n        return calc_spread_return_per_day(df, 200, 2)\n\n    def predictor_per_day(df):\n        study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=SD))#5187\n        study.optimize(lambda trial: abs(objective(trial, df) - 3), 3)\n        return calc_pred(df, *study.best_params.values())\n\n    return df.groupby(\"Date\").apply(predictor_per_day).reset_index(level=0, drop=True)\n\ndef _predictor_base(feature_df):\n    return model.predict(feature_df[feats])\n\ndef _predictor_with_adjuster(feature_df):\n    df_pred = feature_df.copy()\n    df_pred[\"Target\"] = model.predict(feature_df[feats])\n    return adjuster(df_pred).values.T","metadata":{"execution":{"iopub.status.busy":"2022-06-08T14:56:15.393785Z","iopub.status.idle":"2022-06-08T14:56:15.39443Z","shell.execute_reply.started":"2022-06-08T14:56:15.394118Z","shell.execute_reply":"2022-06-08T14:56:15.394152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.random.seed(0)\nfeats =  features_enricher.feature_names_\nfeats.append(\"Close\")\nmax_score = 0\nmax_depth = 0","metadata":{"execution":{"iopub.status.busy":"2022-06-08T14:56:15.395775Z","iopub.status.idle":"2022-06-08T14:56:15.396438Z","shell.execute_reply.started":"2022-06-08T14:56:15.396139Z","shell.execute_reply":"2022-06-08T14:56:15.396171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_prices = fill_nans(df_prices)\nprices = fill_nans(prices)","metadata":{"execution":{"iopub.status.busy":"2022-06-08T14:56:15.39828Z","iopub.status.idle":"2022-06-08T14:56:15.3989Z","shell.execute_reply.started":"2022-06-08T14:56:15.398601Z","shell.execute_reply":"2022-06-08T14:56:15.398633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for md in tqdm(range(3,40)):\n    model = DecisionTreeRegressor( max_depth=md ) # Controlling the overfit with max_depth parameter\n    model.fit(df_prices[feats],df_prices[\"Target\"])\n    predictor = _predictor_base\n    prices[\"pred\"] = predictor(prices)\n    score, buf = calc_spread_return_sharpe(add_rank(prices))\n    if score>max_score:\n        max_score = score\n        max_depth = md\n        \nmodel = DecisionTreeRegressor( max_depth=max_depth )\nmodel.fit(df_prices[feats],df_prices[\"Target\"])\nprint(f'Max_deph={max_depth} : Sharpe Ratio Score base -> {max_score}')","metadata":{"execution":{"iopub.status.busy":"2022-06-08T14:56:15.400722Z","iopub.status.idle":"2022-06-08T14:56:15.401237Z","shell.execute_reply.started":"2022-06-08T14:56:15.401026Z","shell.execute_reply":"2022-06-08T14:56:15.401047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Controlling the Sharpe Ratio Score (‚âÉ3)\n# predictor = _predictor_with_adjuster\nerr = 1\nmaxSD = 3683\nfor SD in tqdm(range(maxSD,4000)):\n    prices[\"pred\"] = predictor(prices)\n    score, buf = calc_spread_return_sharpe(add_rank(prices))\n    if abs(score-3)<=err and score<3:\n        err=abs(score-3)\n        maxSD = SD\n        print(f'{maxSD} Sharpe Ratio Score with adjuster -> {score}')\n        \nSD = maxSD","metadata":{"execution":{"iopub.status.busy":"2022-06-08T14:56:15.402669Z","iopub.status.idle":"2022-06-08T14:56:15.403119Z","shell.execute_reply.started":"2022-06-08T14:56:15.402879Z","shell.execute_reply":"2022-06-08T14:56:15.402899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import jpx_tokyo_market_prediction\nenv = jpx_tokyo_market_prediction.make_env()\niter_test = env.iter_test()","metadata":{"execution":{"iopub.status.busy":"2022-06-08T14:56:15.404762Z","iopub.status.idle":"2022-06-08T14:56:15.405924Z","shell.execute_reply.started":"2022-06-08T14:56:15.405683Z","shell.execute_reply":"2022-06-08T14:56:15.405709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for prices, options, financials, trades, secondary_prices, sample_prediction in iter_test:\n    prices['Date']=pd.to_datetime(prices['Date'], format='%Y-%m-%d')\n    prices[\"country\"] = 'JP'\n    prices = features_enricher.transform(prices, keep_input=True)\n    prices = fill_nans(prices)\n    prices.loc[:,\"pred\"] = predictor(prices)\n    prices = add_rank(prices)\n    rank = prices.set_index('SecuritiesCode')['Rank'].to_dict()\n    sample_prediction['Rank'] = sample_prediction['SecuritiesCode'].map(rank)\n    env.predict(sample_prediction)","metadata":{"execution":{"iopub.status.busy":"2022-06-08T14:56:15.407322Z","iopub.status.idle":"2022-06-08T14:56:15.407753Z","shell.execute_reply.started":"2022-06-08T14:56:15.407551Z","shell.execute_reply":"2022-06-08T14:56:15.407572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## üåé Relevant external features & data sources\nUpgini found several relevant external features for this conest.\n\n* **f_usd_1d_to_7d_b6c6e46e** - In general, the U.S. Dollar Index  is an index (or measure) of the value of the United States dollar relative to a basket of foreign currencies, often referred to as a basket of U.S. trade partners' currencies. This metric calculate as ratio of Dollar index on reporting date to average value on 7 days window.\n\n* **f_cpi_pca_8_cd4f50c7** -  Consumer Price index tranforming through Principal component analysis. In general, Consumer Price indexes are index numbers that measure changes in the prices of goods and services purchased or otherwise acquired by households, which households use directly, or indirectly, to satisfy their own needs and wants.  \nSo it has a lot of information about inflation in specific country and for specific type of services and goods.  \nIt's been updated by the [Organisation for Economic Cooperation and Development (OECD)](https://data.oecd.org/price/inflation-cpi.htm) on a monthly basis.","metadata":{}}]}