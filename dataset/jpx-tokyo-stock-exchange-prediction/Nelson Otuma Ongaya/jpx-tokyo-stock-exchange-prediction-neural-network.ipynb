{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-22T19:21:41.881614Z","iopub.execute_input":"2022-04-22T19:21:41.882113Z","iopub.status.idle":"2022-04-22T19:21:41.915903Z","shell.execute_reply.started":"2022-04-22T19:21:41.882079Z","shell.execute_reply":"2022-04-22T19:21:41.914852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing standard libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2022-04-22T19:20:37.324691Z","iopub.execute_input":"2022-04-22T19:20:37.325455Z","iopub.status.idle":"2022-04-22T19:20:37.329189Z","shell.execute_reply.started":"2022-04-22T19:20:37.325419Z","shell.execute_reply":"2022-04-22T19:20:37.328308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_df = pd.read_csv(\"../input/jpx-tokyo-stock-exchange-prediction/train_files/stock_prices.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-04-22T19:21:51.360316Z","iopub.execute_input":"2022-04-22T19:21:51.361073Z","iopub.status.idle":"2022-04-22T19:21:55.897246Z","shell.execute_reply.started":"2022-04-22T19:21:51.361015Z","shell.execute_reply":"2022-04-22T19:21:55.896416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-22T19:21:55.898455Z","iopub.execute_input":"2022-04-22T19:21:55.899205Z","iopub.status.idle":"2022-04-22T19:21:55.916982Z","shell.execute_reply.started":"2022-04-22T19:21:55.899171Z","shell.execute_reply":"2022-04-22T19:21:55.916221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_df.sample(5)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T19:22:44.871981Z","iopub.execute_input":"2022-04-22T19:22:44.87256Z","iopub.status.idle":"2022-04-22T19:22:44.984953Z","shell.execute_reply.started":"2022-04-22T19:22:44.872517Z","shell.execute_reply":"2022-04-22T19:22:44.983909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_df.Date.describe()","metadata":{"execution":{"iopub.status.busy":"2022-04-22T19:24:15.408451Z","iopub.execute_input":"2022-04-22T19:24:15.408737Z","iopub.status.idle":"2022-04-22T19:24:15.925498Z","shell.execute_reply.started":"2022-04-22T19:24:15.408705Z","shell.execute_reply":"2022-04-22T19:24:15.924608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Count shows 2332531 rows and the start date 03.12.2021","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Count shows 2332531 rows and the start date 03.12.2021","metadata":{}},{"cell_type":"code","source":"print(\"Start date: {}, end date: {}\".format(my_df.Date.unique().min(), my_df.Date.unique().max()))","metadata":{"execution":{"iopub.status.busy":"2022-04-22T19:27:29.594102Z","iopub.execute_input":"2022-04-22T19:27:29.594392Z","iopub.status.idle":"2022-04-22T19:27:29.935848Z","shell.execute_reply.started":"2022-04-22T19:27:29.594364Z","shell.execute_reply":"2022-04-22T19:27:29.934711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"i will be using 2021 data for validation and the rest for training","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"my_df_train = my_df[my_df['Date'] < '2021-01-01'].copy()\nmy_df_train.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-22T19:29:47.644785Z","iopub.execute_input":"2022-04-22T19:29:47.645755Z","iopub.status.idle":"2022-04-22T19:29:48.258396Z","shell.execute_reply.started":"2022-04-22T19:29:47.64571Z","shell.execute_reply":"2022-04-22T19:29:48.257526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"The data that we will be working with has 1880531 rows and 21 columns this is our training set","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Validation Data","metadata":{}},{"cell_type":"code","source":"my_df_valid = my_df[my_df['Date'] >= '2021-01-01'].copy()\nmy_df_valid.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-22T19:32:14.20347Z","iopub.execute_input":"2022-04-22T19:32:14.203762Z","iopub.status.idle":"2022-04-22T19:32:14.582589Z","shell.execute_reply.started":"2022-04-22T19:32:14.203727Z","shell.execute_reply":"2022-04-22T19:32:14.581998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#we will use atleast 20% of the data for validation.","metadata":{"execution":{"iopub.status.busy":"2022-04-22T19:32:52.259967Z","iopub.execute_input":"2022-04-22T19:32:52.260403Z","iopub.status.idle":"2022-04-22T19:32:52.263745Z","shell.execute_reply.started":"2022-04-22T19:32:52.260371Z","shell.execute_reply":"2022-04-22T19:32:52.262743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_df_valid.shape[0] / my_df.shape[0] * 100","metadata":{"execution":{"iopub.status.busy":"2022-04-22T19:33:24.669572Z","iopub.execute_input":"2022-04-22T19:33:24.669875Z","iopub.status.idle":"2022-04-22T19:33:24.676613Z","shell.execute_reply.started":"2022-04-22T19:33:24.669843Z","shell.execute_reply":"2022-04-22T19:33:24.675748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_df.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T19:35:01.761444Z","iopub.execute_input":"2022-04-22T19:35:01.761726Z","iopub.status.idle":"2022-04-22T19:35:01.781178Z","shell.execute_reply.started":"2022-04-22T19:35:01.761699Z","shell.execute_reply":"2022-04-22T19:35:01.780166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Selection","metadata":{}},{"cell_type":"code","source":"my_df.head(10)#we need to select numerical features for our model building.'Open', 'High', 'Low', 'Close', 'Volume","metadata":{"execution":{"iopub.status.busy":"2022-04-22T19:35:24.451678Z","iopub.execute_input":"2022-04-22T19:35:24.451941Z","iopub.status.idle":"2022-04-22T19:35:24.477592Z","shell.execute_reply.started":"2022-04-22T19:35:24.451913Z","shell.execute_reply":"2022-04-22T19:35:24.476804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#we now select the features for our model.\nfeatures = ['Open', 'High', 'Low', 'Close', 'Volume']\ntarget = ['Target']\nmy_df_train = my_df_train[features + target].reset_index(drop=True).copy()\nmy_df_valid = my_df_valid[features + target].reset_index(drop=True).copy()\nmy_df_valid.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-22T19:37:52.119853Z","iopub.execute_input":"2022-04-22T19:37:52.120172Z","iopub.status.idle":"2022-04-22T19:37:52.327438Z","shell.execute_reply.started":"2022-04-22T19:37:52.120129Z","shell.execute_reply":"2022-04-22T19:37:52.326722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"We need to check for missing values for us to standardize our data set that we will be working with.","metadata":{}},{"cell_type":"code","source":"my_df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-04-22T19:52:36.618467Z","iopub.execute_input":"2022-04-22T19:52:36.619292Z","iopub.status.idle":"2022-04-22T19:52:37.162533Z","shell.execute_reply.started":"2022-04-22T19:52:36.619244Z","shell.execute_reply":"2022-04-22T19:52:37.161564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_df_train.dropna(subset=features + target, axis=0, inplace=True)\nmy_df_valid.dropna(subset=features + target, axis=0, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T19:53:11.985565Z","iopub.execute_input":"2022-04-22T19:53:11.985823Z","iopub.status.idle":"2022-04-22T19:53:12.180817Z","shell.execute_reply.started":"2022-04-22T19:53:11.985796Z","shell.execute_reply":"2022-04-22T19:53:12.179658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"#we also check if training split and valid split has missing values\nmy_df_train.isnull().sum() + my_df_valid.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-04-22T19:54:05.957899Z","iopub.execute_input":"2022-04-22T19:54:05.958244Z","iopub.status.idle":"2022-04-22T19:54:05.994927Z","shell.execute_reply.started":"2022-04-22T19:54:05.958197Z","shell.execute_reply":"2022-04-22T19:54:05.994087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Therefore from the above our training set and validation set looks clean.","metadata":{}},{"cell_type":"code","source":"# Statistical summary\nmy_df_train.describe()","metadata":{"execution":{"iopub.status.busy":"2022-04-22T19:56:11.345126Z","iopub.execute_input":"2022-04-22T19:56:11.34566Z","iopub.status.idle":"2022-04-22T19:56:11.86837Z","shell.execute_reply.started":"2022-04-22T19:56:11.345624Z","shell.execute_reply":"2022-04-22T19:56:11.867578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing****","metadata":{}},{"cell_type":"code","source":"#We will need to perform feature normalization and create tensorflow dataset for our model.","metadata":{"execution":{"iopub.status.busy":"2022-04-22T20:03:53.692794Z","iopub.execute_input":"2022-04-22T20:03:53.693108Z","iopub.status.idle":"2022-04-22T20:03:53.697454Z","shell.execute_reply.started":"2022-04-22T20:03:53.693056Z","shell.execute_reply":"2022-04-22T20:03:53.696617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define encoding function for numerical features\ndef encode_feature(feature, name, dataset):\n    # Create a Normalization layer for our feature\n    normalizer = layers.Normalization()\n\n    # Prepare a Dataset that only yields our feature\n    feature_ds = dataset.map(lambda x, y: x[name])\n    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n\n    # Learn the statistics of the data\n    normalizer.adapt(feature_ds)\n\n    # Normalize the input feature\n    encoded_feature = normalizer(feature)\n    return encoded_feature","metadata":{"execution":{"iopub.status.busy":"2022-04-22T20:07:05.667227Z","iopub.execute_input":"2022-04-22T20:07:05.667515Z","iopub.status.idle":"2022-04-22T20:07:05.673504Z","shell.execute_reply.started":"2022-04-22T20:07:05.667485Z","shell.execute_reply":"2022-04-22T20:07:05.672496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\nimport jpx_tokyo_market_prediction\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers","metadata":{"execution":{"iopub.status.busy":"2022-04-22T20:08:52.672941Z","iopub.execute_input":"2022-04-22T20:08:52.67362Z","iopub.status.idle":"2022-04-22T20:08:59.559341Z","shell.execute_reply.started":"2022-04-22T20:08:52.673563Z","shell.execute_reply":"2022-04-22T20:08:59.55841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate tensorflow dataset\ndef dataframe_to_dataset(dataframe):\n    dataframe = dataframe.copy()\n    labels = dataframe.pop(\"Target\")\n    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n    ds = ds.shuffle(buffer_size=len(dataframe))\n    return ds\n\ntrain_ds = dataframe_to_dataset(my_df_train)\nvalid_ds = dataframe_to_dataset(my_df_valid)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T20:09:03.852849Z","iopub.execute_input":"2022-04-22T20:09:03.853155Z","iopub.status.idle":"2022-04-22T20:09:04.065311Z","shell.execute_reply.started":"2022-04-22T20:09:03.853122Z","shell.execute_reply":"2022-04-22T20:09:04.064209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"we note that each of our dataset will yield a tuple(input,target) where input is a dictionary of features","metadata":{}},{"cell_type":"code","source":"#we create a loop for our train_ds\nfor x, y in train_ds.take(1):\n    print(\"Input:\", x)\n    print(\"Target:\", y)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T20:13:08.698256Z","iopub.execute_input":"2022-04-22T20:13:08.698535Z","iopub.status.idle":"2022-04-22T20:13:17.493467Z","shell.execute_reply.started":"2022-04-22T20:13:08.698507Z","shell.execute_reply":"2022-04-22T20:13:17.49254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Batch the dataset\ntrain_ds = train_ds.batch(1024)\nvalid_ds = valid_ds.batch(1024)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T20:13:35.398166Z","iopub.execute_input":"2022-04-22T20:13:35.398985Z","iopub.status.idle":"2022-04-22T20:13:35.405671Z","shell.execute_reply.started":"2022-04-22T20:13:35.398935Z","shell.execute_reply":"2022-04-22T20:13:35.404587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Building Neural Network Model\nwe will first define our input layers of our Neural Network model then perform encoding to it.","metadata":{}},{"cell_type":"code","source":"%%time\n# Raw numerical features\nOpen = keras.Input(shape=(1,), name=\"Open\")\nHigh = keras.Input(shape=(1,), name=\"High\")\nLow = keras.Input(shape=(1,), name=\"Low\")\nClose = keras.Input(shape=(1,), name=\"Close\")\nVolume = keras.Input(shape=(1,), name=\"Volume\")\n\nall_inputs = [Open, High, Low, Close, Volume]\n\n# Encode nfeatures\nopen_encoded = encode_feature(Open, \"Open\", train_ds)\nhigh_encoded = encode_feature(High, \"High\", train_ds)\nlow_encoded = encode_feature(Low, \"Low\", train_ds)\nclose_encoded = encode_feature(Close, \"Close\", train_ds)\nvolume_encoded = encode_feature(Volume, \"Volume\", train_ds)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T20:16:15.933909Z","iopub.execute_input":"2022-04-22T20:16:15.934318Z","iopub.status.idle":"2022-04-22T20:17:57.957711Z","shell.execute_reply.started":"2022-04-22T20:16:15.934289Z","shell.execute_reply":"2022-04-22T20:17:57.956865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will need to concat all the input layers and connect them to multiple hidden dense layers.","metadata":{}},{"cell_type":"code","source":"# Concat all features of input layer\nall_features = layers.concatenate(\n    [\n        open_encoded,\n        high_encoded,\n        low_encoded,\n        close_encoded,\n        volume_encoded,\n    ]\n)\n\n# Add several hidden layers with batch_norm and dropout\nx = layers.Dense(256, activation=\"relu\")(all_features)\nx = layers.BatchNormalization()(x)\nx = layers.Dropout(0.2)(x)\nx = layers.Dense(128, activation=\"relu\")(x)\nx = layers.BatchNormalization()(x)\nx = layers.Dropout(0.2)(x)\nx = layers.Dense(64, activation=\"relu\")(x)\nx = layers.BatchNormalization()(x)\nx = layers.Dropout(0.2)(x)\n\n# Output layer for regression task\noutput = layers.Dense(1, activation=\"linear\")(x)\n\n# Create our NN model\nmodel = keras.Model(all_inputs, output)\nmodel.compile(\"adam\", \"mse\", metrics=[tf.keras.metrics.RootMeanSquaredError()])","metadata":{"execution":{"iopub.status.busy":"2022-04-22T20:19:21.497992Z","iopub.execute_input":"2022-04-22T20:19:21.498687Z","iopub.status.idle":"2022-04-22T20:19:21.612946Z","shell.execute_reply.started":"2022-04-22T20:19:21.498639Z","shell.execute_reply":"2022-04-22T20:19:21.61217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# How does our model look like?????................?****","metadata":{}},{"cell_type":"code","source":"# Lets check our Neural Network model  structure\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-04-22T20:20:49.678334Z","iopub.execute_input":"2022-04-22T20:20:49.678614Z","iopub.status.idle":"2022-04-22T20:20:49.695882Z","shell.execute_reply.started":"2022-04-22T20:20:49.678586Z","shell.execute_reply":"2022-04-22T20:20:49.694841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Visualization","metadata":{}},{"cell_type":"code","source":"#Lets visualize our model\nkeras.utils.plot_model(model, show_shapes=True, rankdir=\"LR\")","metadata":{"execution":{"iopub.status.busy":"2022-04-22T20:22:12.567006Z","iopub.execute_input":"2022-04-22T20:22:12.567345Z","iopub.status.idle":"2022-04-22T20:22:13.976551Z","shell.execute_reply.started":"2022-04-22T20:22:12.567308Z","shell.execute_reply":"2022-04-22T20:22:13.975563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Training****\nwe need to set up epochs ","metadata":{}},{"cell_type":"code","source":"# Set early_stopping callbacks,\nearly_stopping = keras.callbacks.EarlyStopping(\n    monitor='val_loss',\n    patience=10,\n    min_delta=1e-3,\n    restore_best_weights=True,\n)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T20:25:14.287015Z","iopub.execute_input":"2022-04-22T20:25:14.287564Z","iopub.status.idle":"2022-04-22T20:25:14.292419Z","shell.execute_reply.started":"2022-04-22T20:25:14.287516Z","shell.execute_reply":"2022-04-22T20:25:14.291762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(train_ds, epochs=50, validation_data=valid_ds, callbacks=[early_stopping])","metadata":{"execution":{"iopub.status.busy":"2022-04-22T20:25:31.745869Z","iopub.execute_input":"2022-04-22T20:25:31.746392Z","iopub.status.idle":"2022-04-22T20:39:15.161299Z","shell.execute_reply.started":"2022-04-22T20:25:31.746358Z","shell.execute_reply":"2022-04-22T20:39:15.16038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We save our model\nmodel.save(\"nn_model.h5\")","metadata":{"execution":{"iopub.status.busy":"2022-04-22T20:39:30.285169Z","iopub.execute_input":"2022-04-22T20:39:30.285884Z","iopub.status.idle":"2022-04-22T20:39:30.341395Z","shell.execute_reply.started":"2022-04-22T20:39:30.285838Z","shell.execute_reply":"2022-04-22T20:39:30.340575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load trained model\nbest_model = keras.models.load_model(\"nn_model.h5\")","metadata":{"execution":{"iopub.status.busy":"2022-04-22T20:39:34.692364Z","iopub.execute_input":"2022-04-22T20:39:34.69315Z","iopub.status.idle":"2022-04-22T20:39:34.89387Z","shell.execute_reply.started":"2022-04-22T20:39:34.693101Z","shell.execute_reply":"2022-04-22T20:39:34.892987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate tensorflow dataset for test data\ndef dataframe_to_dataset_test(dataframe):\n    dataframe = dataframe.copy()\n    ds = tf.data.Dataset.from_tensor_slices(dict(dataframe))\n    return ds","metadata":{"execution":{"iopub.status.busy":"2022-04-22T20:39:38.006502Z","iopub.execute_input":"2022-04-22T20:39:38.006795Z","iopub.status.idle":"2022-04-22T20:39:38.01214Z","shell.execute_reply.started":"2022-04-22T20:39:38.006763Z","shell.execute_reply":"2022-04-22T20:39:38.011556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set random seed\nseed = 30\nrandom.seed(seed)\nos.environ['PYTHONHASHSEED'] = str(seed)\nnp.random.seed(seed)\ntf.random.set_seed(seed)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T20:39:42.944573Z","iopub.execute_input":"2022-04-22T20:39:42.945042Z","iopub.status.idle":"2022-04-22T20:39:42.980296Z","shell.execute_reply.started":"2022-04-22T20:39:42.944993Z","shell.execute_reply":"2022-04-22T20:39:42.979013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make predictions and submission\nenv = jpx_tokyo_market_prediction.make_env()   # initialize the environment\niter_test = env.iter_test()    # an iterator which loops over the test files\nfor (prices, options, financials, trades, secondary_prices, sample_prediction) in iter_test:\n    test_ds = dataframe_to_dataset_test(prices)\n    sample_prediction['target_pred'] = best_model.predict(test_ds)\n    sample_prediction = sample_prediction.sort_values(by=\"target_pred\", ascending=False)\n    sample_prediction['Rank'] = np.arange(2000)\n    sample_prediction = sample_prediction.sort_values(by=\"SecuritiesCode\", ascending=True)\n    sample_prediction.drop(['target_pred'], axis=1, inplace=True)\n    display(sample_prediction)\n    env.predict(sample_prediction)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T20:39:49.925939Z","iopub.execute_input":"2022-04-22T20:39:49.926229Z","iopub.status.idle":"2022-04-22T20:39:55.991843Z","shell.execute_reply.started":"2022-04-22T20:39:49.9262Z","shell.execute_reply":"2022-04-22T20:39:55.991074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Enhanced Recurrent Neural Networks","metadata":{}},{"cell_type":"code","source":"#Data Preparation\n# Getting our train dataset\ntraining_set = my_df_train.iloc[:,1:2].values\nprint(training_set)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T20:53:30.577355Z","iopub.execute_input":"2022-04-22T20:53:30.577616Z","iopub.status.idle":"2022-04-22T20:53:30.587516Z","shell.execute_reply.started":"2022-04-22T20:53:30.577588Z","shell.execute_reply":"2022-04-22T20:53:30.586873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Performing Feature scaling\nfrom sklearn.preprocessing import MinMaxScaler\nsc = MinMaxScaler(feature_range=(0,1))\ntraining_set_scaled = sc.fit_transform(training_set) ","metadata":{"execution":{"iopub.status.busy":"2022-04-22T20:53:56.376338Z","iopub.execute_input":"2022-04-22T20:53:56.37691Z","iopub.status.idle":"2022-04-22T20:53:57.229043Z","shell.execute_reply.started":"2022-04-22T20:53:56.376864Z","shell.execute_reply":"2022-04-22T20:53:57.228138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating a dataset with 60 timesteps and 1 output\nX_train = []\nY_train = []\nfor i in range(60, 751):\n    X_train.append(training_set_scaled[i-60 : i, 0])\n    Y_train.append(training_set_scaled[i, 0])\nX_train, Y_train = np.array(X_train), np.array(Y_train) ","metadata":{"execution":{"iopub.status.busy":"2022-04-22T20:54:24.004808Z","iopub.execute_input":"2022-04-22T20:54:24.005085Z","iopub.status.idle":"2022-04-22T20:54:24.012256Z","shell.execute_reply.started":"2022-04-22T20:54:24.00504Z","shell.execute_reply":"2022-04-22T20:54:24.011337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reshaping \nX_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1],1))","metadata":{"execution":{"iopub.status.busy":"2022-04-22T20:54:45.305733Z","iopub.execute_input":"2022-04-22T20:54:45.306608Z","iopub.status.idle":"2022-04-22T20:54:45.311827Z","shell.execute_reply.started":"2022-04-22T20:54:45.306562Z","shell.execute_reply":"2022-04-22T20:54:45.310999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# Data Modeling","metadata":{}},{"cell_type":"code","source":"# Building the RNN\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.layers import LSTM","metadata":{"execution":{"iopub.status.busy":"2022-04-22T20:55:28.784874Z","iopub.execute_input":"2022-04-22T20:55:28.785501Z","iopub.status.idle":"2022-04-22T20:55:28.789783Z","shell.execute_reply.started":"2022-04-22T20:55:28.785462Z","shell.execute_reply":"2022-04-22T20:55:28.78906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialising the RNN\n# ---\n#\nregressor = Sequential()\n\n# Adding the first LSTM layer and some Dropout regularisation\n# ---\n#\nregressor.add(LSTM(units = 100, return_sequences = True, input_shape = (X_train.shape[1], 1)))\nregressor.add(Dropout(0.3))\n\n# Adding the second LSTM layer and some dropout regularisation\n# ---\n#\nregressor.add(LSTM(units = 100, return_sequences = True))\nregressor.add(Dropout(0.3))\n\n# Adding the third LSTM layer and some dropout regularisation\n# ---\n#\nregressor.add(LSTM(units = 100, return_sequences = True))\nregressor.add(Dropout(0.3))\n\n# Adding the fourth LSTM layer with some dropout\n# ---\n#\nregressor.add(LSTM(units = 100, return_sequences = False))\nregressor.add(Dropout(0.3))\n\n# Adding the output layer\n# ---\n#\nregressor.add(Dense(units = 1))","metadata":{"execution":{"iopub.status.busy":"2022-04-22T20:56:04.684836Z","iopub.execute_input":"2022-04-22T20:56:04.685747Z","iopub.status.idle":"2022-04-22T20:56:05.651636Z","shell.execute_reply.started":"2022-04-22T20:56:04.685702Z","shell.execute_reply":"2022-04-22T20:56:05.650632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compile the RNN \nregressor.compile(optimizer = 'adam', loss = 'mean_squared_error')","metadata":{"execution":{"iopub.status.busy":"2022-04-22T20:56:30.765262Z","iopub.execute_input":"2022-04-22T20:56:30.765813Z","iopub.status.idle":"2022-04-22T20:56:30.775843Z","shell.execute_reply.started":"2022-04-22T20:56:30.765773Z","shell.execute_reply":"2022-04-22T20:56:30.7749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fitting the RNN to the training set\nregressor.fit(X_train, Y_train, epochs = 50, batch_size = 32)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T20:56:45.32974Z","iopub.execute_input":"2022-04-22T20:56:45.330045Z","iopub.status.idle":"2022-04-22T21:01:21.260469Z","shell.execute_reply.started":"2022-04-22T20:56:45.330011Z","shell.execute_reply":"2022-04-22T21:01:21.259594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"What can be done to improve the solution?\nGetting more training data\nIncreasing the timesteps\nAdding some other indicators\nAdding more LSTM layers\nAdding more neurons in the LSTM layers etc.","metadata":{}}]}