{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Unzipping and reading from data files\n\nimport zipfile\n\nzf = zipfile.ZipFile('../input/airbnb-recruiting-new-user-bookings/train_users_2.csv.zip') \ntrain_users = pd.read_csv(zf.open('train_users_2.csv'))\nzf = zipfile.ZipFile('/kaggle/input/airbnb-recruiting-new-user-bookings/test_users.csv.zip') \ntest_users = pd.read_csv(zf.open('test_users.csv'))\ntest_users.head()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Exploration\nIn the next few cells I am going to explore the data, both train_users and test_useres, to get some insights from it and discover the outliers and possible errors."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"We have\", train_users.shape[0], \"users in the training set and\", \n      test_users.shape[0], \"in the test set.\")\nprint(\"In total we have\", train_users.shape[0] + test_users.shape[0], \"users.\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merge train and test users\nusers = pd.concat((train_users, test_users), axis=0, ignore_index=True, sort=True)\n\n\nusers.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"users.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The maximum value of age is 2014 which means there is some errors in the age column, so let's see the number and the nature of the records where this value exists to determine whether or not dropping these records would significantly affect the dataset.\nWe can also notice that the count of values in age column is much less than number of records which means there is a lot of missing data in this field (will be investigated later)."},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntrain_users.loc[train_users['age'] == 2014]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the country_destination we can be sure that there are different values associated with this age, which makes dropping these 710 records out of over 213000 records a quite suitable solution that probably wouldn't ommit valuable information.\nHowever, noticing that the only field that seems to be consistent with records of age=2014 is that signup_app=web, I'll investigate more.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"web_2014 = users.loc[users['age'] == 2014, 'signup_app'].value_counts() \nprint (web_2014)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(users.signup_app.value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see, the web value has nothing to do with the age=2014, it just happens to be the most common value of signup_app.\nLet's see now other inconsistent values of age, considering that the longest confirmed human lifespan record=122.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"np.unique(users[users.age > 122]['age'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sum(users.age > 122))\nprint(sum(users.age < 18))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":" For now, we can set an acceptance range and replace those out of it with NaN."},{"metadata":{"trusted":true},"cell_type":"code","source":"users.loc[users.age > 95, 'age'].count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"users.loc[users.age < 13, 'age'].count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"users.loc[users.age > 95, 'age'] = np.nan\nusers.loc[users.age < 13, 'age'] = np.nan","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"users.age.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see linear correlation between the input features"},{"metadata":{"trusted":true},"cell_type":"code","source":"users.corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's see non-linear correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nsns.pairplot(train_users)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see a very low correlation which makes sense as there is no dependency between these three features."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Data Cleaning"},{"metadata":{},"cell_type":"markdown","source":"1) Handeling missing data"},{"metadata":{"trusted":true},"cell_type":"code","source":"users[\"gender\"]=users[\"gender\"].fillna(\"-unknown-\" )\nusers.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#How much data is missing from the dataset (apart from destination country)\nusers_nan = (users.isnull().sum() / users.shape[0]) * 100\nusers_nan[users_nan > 0].drop('country_destination')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have quite a lot of *NaN* in the `age` and `gender` wich will yield in lesser performance of the classifiers we will build. The feature `date_first_booking` has a 67.7% of NaN values probably because this feature is not present at the tests users, and therefore, we won't need it at the *modeling* part. Let's see the NaN values of `date_first_booking` in train_data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_users.date_first_booking.isnull().sum() / train_users.shape[0] * 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_users.country_destination.isnull().sum() / train_users.shape[0] * 100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since we will not need this feature in the modeling we don't need to handle its missing data.\nThe other feature with a high rate of *NaN* was `gender`. Let's see:"},{"metadata":{"trusted":true},"cell_type":"code","source":"users.gender.value_counts(dropna=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"users.gender.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The third feature with a high rate of NaN was age. Let's see:\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"users.age.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#For now, let's fill the missing values of age with the median since the mean is highly affectd by extreme values\nusers[\"age\"]=users[\"age\"].fillna( users[\"age\"].median())\n\nusers.head()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2) Data Types\n\nLet's treat each feature as what they are. This means we need to transform into categorical those features that we treat as categories and the same with the dates:\n\n\n\n\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_features = [\n    'affiliate_channel',\n    'affiliate_provider',\n    'country_destination',\n    'first_affiliate_tracked',\n    'first_browser',\n    'first_device_type',\n    'gender',\n    'language',\n    'signup_app',\n    'signup_method'\n]\n\nfor categorical_feature in categorical_features:\n    print (categorical_feature,users[categorical_feature].unique())\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see there are some features with large number of categories, whereas there are others with relatively small number of categories.Hence, in the data encoding step we will use one hot encoding with features of small number of categories."},{"metadata":{"trusted":true},"cell_type":"code","source":"users['date_account_created'] = pd.to_datetime(users['date_account_created'])\nusers['date_first_booking'] = pd.to_datetime(users['date_first_booking'])\nusers['date_first_active'] = pd.to_datetime((users.timestamp_first_active // 1000000), format='%Y%m%d')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"users.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Visualization"},{"metadata":{},"cell_type":"markdown","source":"First, let's see how the gender porpotion is visualized"},{"metadata":{"trusted":true},"cell_type":"code","source":"users.gender.value_counts(dropna=False).plot(kind='bar', color='#FD5C64', rot=0)\nplt.xlabel('Gender')\nsns.despine()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see now if there is any gender preferences when travelling."},{"metadata":{"trusted":true},"cell_type":"code","source":"women = sum(users['gender'] == 'FEMALE')\nmen = sum(users['gender'] == 'MALE')\n\nfemale_destinations = users.loc[users['gender'] == 'FEMALE', 'country_destination'].value_counts() / women * 100\nmale_destinations = users.loc[users['gender'] == 'MALE', 'country_destination'].value_counts() / men * 100\n\n# Bar width\nwidth = 0.4\n\nmale_destinations.plot(kind='bar', width=width, color='#4DD3C9', position=0, label='Male', rot=0)\nfemale_destinations.plot(kind='bar', width=width, color='#FFA35D', position=1, label='Female', rot=0)\n\nplt.legend()\nplt.xlabel('Destination Country')\nplt.ylabel('Percentage')\n\nsns.despine()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, There are no big differences between the 2 main genders.\n\nLet's see now the relative destination frecuency of the countries."},{"metadata":{"trusted":true},"cell_type":"code","source":"destination_percentage = users.country_destination.value_counts() / users.shape[0] * 100\ndestination_percentage.plot(kind='bar',color='#FD5C64', rot=0)\n# Using seaborn can also be plotted\n# sns.countplot(x=\"country_destination\", data=users, order=list(users.country_destination.value_counts().keys()))\nplt.xlabel('Destination Country')\nplt.ylabel('Percentage')\nsns.despine()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The first thing we can see that if there is a reservation, it's likely to be inside the US. But there is around 45% of people that never did a reservation.\n\nLet's now see the age relations with the destination."},{"metadata":{"trusted":true},"cell_type":"code","source":"age = 45\n\nyounger = sum(users.loc[users['age'] < age, 'country_destination'].value_counts())\nolder = sum(users.loc[users['age'] > age, 'country_destination'].value_counts())\n\nyounger_destinations = users.loc[users['age'] < age, 'country_destination'].value_counts() / younger * 100\nolder_destinations = users.loc[users['age'] > age, 'country_destination'].value_counts() / older * 100\n\nyounger_destinations.plot(kind='bar', width=width, color='#63EA55', position=0, label='Youngers', rot=0)\nolder_destinations.plot(kind='bar', width=width, color='#4DD3C9', position=1, label='Olders', rot=0)\n\nplt.legend()\nplt.xlabel('Destination Country')\nplt.ylabel('Percentage')\n\nsns.despine()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the young people tends to stay in the US, and the older people choose to travel outside the country. Of vourse, there are no big differences between them and we must remember that we do not have the 42% of the ages.\n\nNow, Let's see the language of users."},{"metadata":{"trusted":true},"cell_type":"code","source":"print((sum(users.language == 'en') / users.shape[0])*100)\n\nEn = sum(users.loc[users['language']==\"en\", 'country_destination'].value_counts());\nNo_En=sum(users.loc[users['language']!=\"en\", 'country_destination'].value_counts());\nEn_destinations = users.loc[users['language']==\"en\" , 'country_destination'].value_counts() / En * 100\nNo_En_destinations = users.loc[users['language'] !=\"en\", 'country_destination'].value_counts() / No_En * 100\n\nyounger_destinations.plot(kind='bar', width=width, color='#63EA55', position=0, label='English', rot=0)\nolder_destinations.plot(kind='bar', width=width, color='#4DD3C9', position=1, label='Non English', rot=0)\n\nplt.legend()\nplt.xlabel('Destination Country')\nplt.ylabel('Percentage')\n\nsns.despine()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"English people seem to be more determined to book unlike non-english whom percentage is quite large in NDF!"},{"metadata":{},"cell_type":"markdown","source":"# Feature Encoding"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#One hot encoder only takes numerical categorical values, hence any value of string type should be label encoded before one hot encoded.\n\nfrom sklearn.preprocessing import LabelEncoder \n  \nle = LabelEncoder() \n  \nusers['gender']= le.fit_transform(users['gender']) \nusers.head()\n\n#creating one hot encoder object with categorical feature 0 \n#indicating the 9th column which is gender\nfrom sklearn.preprocessing import OneHotEncoder \nonehotencoder = OneHotEncoder(categorical_features = [9]) \ndata = onehotencoder.fit_transform(users).toarray() "},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Random Forest Model"},{"metadata":{},"cell_type":"markdown","source":"1) Check that there is no null values:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\nusers[\"age\"].isna().any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"users[\"gender\"].isnull().values.any()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"users[\"age\"].isnull().values.any()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2) Split the data again to train set and test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_users = users.iloc[:213451 , :]\ntrain_users","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_users = users.iloc[213451: , :]\ntest_users.drop(['country_destination'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3) Build the model and calculate the cross validation accuracy\n\nDue to time limitation, I only considered two features to use in the training. This is of course not an efficient way to train the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import validation_curve\n\n#Convert categorical variable into dummy/indicator variables.\n\ny = train_users[\"country_destination\"]\nfeatures = [\"gender\",\"age\"]\nX = pd.get_dummies(train_users[features])\nX_test = pd.get_dummies(test_users[features])\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=3, random_state=1)\nmodel_random_cv=cross_val_score(model, X, y, cv=5) \nprint (model_random_cv.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is a relatively low accuracy but I will accept it for now, again due to time limitation.\n4) Fit the model to the train-set and then predicting y for the test-set "},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmodel.fit(X, y)\npredictions = model.predict(X_test)\noutput = pd.DataFrame({'id': test_users.id, 'country': predictions})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}