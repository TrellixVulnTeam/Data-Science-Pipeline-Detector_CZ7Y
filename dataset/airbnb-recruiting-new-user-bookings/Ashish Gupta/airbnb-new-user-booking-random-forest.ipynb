{"cells":[{"metadata":{"trusted":true,"_uuid":"e40f5982e2e52d9093247da4b98a7d8a65462e1c"},"cell_type":"code","source":"# Import the necessary libraries\nimport numpy as np\nimport pandas as pd\nimport os\nimport time\nimport warnings\nimport gc\nfrom six.moves import urllib\nimport matplotlib\nimport matplotlib.pyplot as plt\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"89c21c4cbea0e6ca8557610d42115e85dd54a71f"},"cell_type":"code","source":"#Add All the Models Libraries\n\n# Scalers\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import shuffle\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.preprocessing import LabelEncoder\n\n# Models\nfrom sklearn.linear_model import LogisticRegression #logistic regression\nfrom sklearn.svm import SVC # Support Vector Classifier\nfrom sklearn.ensemble import RandomForestClassifier #Random Forest\nfrom sklearn.neighbors import KNeighborsClassifier #KNN\nfrom sklearn.ensemble import ExtraTreesClassifier \nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.tree import DecisionTreeClassifier #Decision Tree\nfrom sklearn.ensemble import BaggingClassifier\n\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import train_test_split #training and testing data split\nfrom sklearn import metrics #accuracy measure\nfrom sklearn.metrics import confusion_matrix #for confusion matrix\nfrom scipy.stats import reciprocal, uniform\n\nfrom sklearn.ensemble import AdaBoostClassifier\n\n\n# Cross-validation\nfrom sklearn.model_selection import KFold #for K-fold cross validation\nfrom sklearn.model_selection import cross_val_score #score evaluation\nfrom sklearn.model_selection import cross_val_predict #prediction\nfrom sklearn.model_selection import cross_validate\n\n# GridSearchCV\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n\n#Common data processors\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.utils import check_array\nfrom scipy import sparse\n\n#Accuracy Score\nfrom sklearn.metrics import accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"97b97819eeca05829f1862bee784aa6de81d8641"},"cell_type":"code","source":"# to make this notebook's output stable across runs\nnp.random.seed(123)\n\n# To plot pretty figures\n%matplotlib inline\nplt.rcParams['axes.labelsize'] = 14\nplt.rcParams['xtick.labelsize'] = 12\nplt.rcParams['ytick.labelsize'] = 12","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2ee9f28686bf69227950ed128ec15e3040d9ef20"},"cell_type":"code","source":"train = pd.read_csv(\"../input/train_users_2.csv\")\ntest = pd.read_csv(\"../input/test_users.csv\")\nid_test = test['id']\nlabels = train['country_destination'].values\ndf_train = train.drop(['country_destination'], axis=1)\ntrain_flag = df_train.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9cbed7f2b204f29038d98d2dce3a1668209a7cb7"},"cell_type":"code","source":"#We now concat Training and Test set\ndf_total = pd.concat((df_train, test),axis=0, ignore_index = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e6d4ce490b44eb1fa449b626515c41bf0a454d88"},"cell_type":"code","source":"df_total = df_total.drop(['id','date_first_booking'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"88ebf09ea1af057202fd268b87c966f182b736a5"},"cell_type":"code","source":"#Date Account created - Capture Date, month and year seperately.\n\ndate_ac = np.vstack(df_total.date_account_created.astype(str).apply(lambda x:list(map(int,x.split('-')))).values)\ndf_total['Day'] = date_ac[:,0]\ndf_total['Month']= date_ac[:,1]\ndf_total['year'] = date_ac[:,2]\n\ndf_total = df_total.drop(['date_account_created'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f1ca47e57d98a72c33462714377cc79f5e37de66"},"cell_type":"code","source":"#Time Stamp first active\n\ntime_stp = np.vstack(df_total.timestamp_first_active.astype(str)\n                     .apply(lambda x: list(map(int,[x[:4],x[4:6],x[6:8],x[8:]]))).values)\n\ndf_total['tfa_day'] = time_stp[:,0]\ndf_total['tfa_Month'] = time_stp[:,1]\ndf_total['tfa_year'] = time_stp[:,2]\n\ndf_total = df_total.drop(['timestamp_first_active'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"abe01dad9e10f4eaab974c893e5728d48175fc57"},"cell_type":"code","source":"#impute the missing age\nval = df_total.age.values\ndf_total['age'] = np.where(np.logical_or(val<14,val>100),-1,val)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b473dee50b8a973b21aa6f3362dc479a7b7aa06b"},"cell_type":"markdown","source":"One Hot Encoding for Characters"},{"metadata":{"trusted":true,"_uuid":"6cfd992a55d1d23027fe135c947a3f81c2719614"},"cell_type":"code","source":"# The CategoricalEncoder class will allow us to convert categorical attributes to one-hot vectors.\n\nclass CategoricalEncoder(BaseEstimator, TransformerMixin):\n    def __init__(self, encoding='onehot', categories='auto', dtype=np.float64,\n                 handle_unknown='error'):\n        self.encoding = encoding\n        self.categories = categories\n        self.dtype = dtype\n        self.handle_unknown = handle_unknown\n\n    def fit(self, X, y=None):\n        \"\"\"Fit the CategoricalEncoder to X.\n        Parameters\n        ----------\n        X : array-like, shape [n_samples, n_feature]\n            The data to determine the categories of each feature.\n        Returns\n        -------\n        self\n        \"\"\"\n\n        if self.encoding not in ['onehot', 'onehot-dense', 'ordinal']:\n            template = (\"encoding should be either 'onehot', 'onehot-dense' \"\n                        \"or 'ordinal', got %s\")\n            raise ValueError(template % self.handle_unknown)\n\n        if self.handle_unknown not in ['error', 'ignore']:\n            template = (\"handle_unknown should be either 'error' or \"\n                        \"'ignore', got %s\")\n            raise ValueError(template % self.handle_unknown)\n\n        if self.encoding == 'ordinal' and self.handle_unknown == 'ignore':\n            raise ValueError(\"handle_unknown='ignore' is not supported for\"\n                             \" encoding='ordinal'\")\n\n        X = check_array(X, dtype=np.object, accept_sparse='csc', copy=True)\n        n_samples, n_features = X.shape\n\n        self._label_encoders_ = [LabelEncoder() for _ in range(n_features)]\n\n        for i in range(n_features):\n            le = self._label_encoders_[i]\n            Xi = X[:, i]\n            if self.categories == 'auto':\n                le.fit(Xi)\n            else:\n                valid_mask = np.in1d(Xi, self.categories[i])\n                if not np.all(valid_mask):\n                    if self.handle_unknown == 'error':\n                        diff = np.unique(Xi[~valid_mask])\n                        msg = (\"Found unknown categories {0} in column {1}\"\n                               \" during fit\".format(diff, i))\n                        raise ValueError(msg)\n                le.classes_ = np.array(np.sort(self.categories[i]))\n\n        self.categories_ = [le.classes_ for le in self._label_encoders_]\n\n        return self\n\n    def transform(self, X):\n        \"\"\"Transform X using one-hot encoding.\n        Parameters\n        ----------\n        X : array-like, shape [n_samples, n_features]\n            The data to encode.\n        Returns\n        -------\n        X_out : sparse matrix or a 2-d array\n            Transformed input.\n        \"\"\"\n        X = check_array(X, accept_sparse='csc', dtype=np.object, copy=True)\n        n_samples, n_features = X.shape\n        X_int = np.zeros_like(X, dtype=np.int)\n        X_mask = np.ones_like(X, dtype=np.bool)\n\n        for i in range(n_features):\n            valid_mask = np.in1d(X[:, i], self.categories_[i])\n\n            if not np.all(valid_mask):\n                if self.handle_unknown == 'error':\n                    diff = np.unique(X[~valid_mask, i])\n                    msg = (\"Found unknown categories {0} in column {1}\"\n                           \" during transform\".format(diff, i))\n                    raise ValueError(msg)\n                else:\n                    # Set the problematic rows to an acceptable value and\n                    # continue `The rows are marked `X_mask` and will be\n                    # removed later.\n                    X_mask[:, i] = valid_mask\n                    X[:, i][~valid_mask] = self.categories_[i][0]\n            X_int[:, i] = self._label_encoders_[i].transform(X[:, i])\n\n        if self.encoding == 'ordinal':\n            return X_int.astype(self.dtype, copy=False)\n\n        mask = X_mask.ravel()\n        n_values = [cats.shape[0] for cats in self.categories_]\n        n_values = np.array([0] + n_values)\n        indices = np.cumsum(n_values)\n\n        column_indices = (X_int + indices[:-1]).ravel()[mask]\n        row_indices = np.repeat(np.arange(n_samples, dtype=np.int32),\n                                n_features)[mask]\n        data = np.ones(n_samples * n_features)[mask]\n\n        out = sparse.csc_matrix((data, (row_indices, column_indices)),\n                                shape=(n_samples, indices[-1]),\n                                dtype=self.dtype).tocsr()\n        if self.encoding == 'onehot-dense':\n            return out.toarray()\n        else:\n            return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e7a5a5ea4dc107e75f01fd57efc38b5b107d06c9"},"cell_type":"code","source":"class DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return X[self.attribute_names]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c9bf073f7c178cd1e4b69f80c8d3308ab257cd7b"},"cell_type":"code","source":"cat_pipeline = Pipeline([\n        (\"selector\", DataFrameSelector(['affiliate_channel',\n                                        'affiliate_provider','first_device_type'])),\n        (\"cat_encoder\", CategoricalEncoder(encoding='onehot-dense')),\n    ])\n\nnum_pipeline = Pipeline([\n        (\"selector\", DataFrameSelector(['signup_flow','Day','Month','year','tfa_day','tfa_Month','tfa_year'])),\n        ('std_scaler', StandardScaler()),\n      ])\n\nfull_pipeline = FeatureUnion(transformer_list=[\n    (\"cat_pipeline\", cat_pipeline),\n    (\"num_pipeline\", num_pipeline)\n \n    ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8f3f1ec2078cc5494e25a9eb9ecc2c2d952004c7"},"cell_type":"code","source":"#create the dummies for the other categorical variables to apply transformation.\n\nfeatures = ['gender','language','signup_method','first_affiliate_tracked', 'signup_app','first_browser']\n\nfor f in features:\n    df_total_dummy = pd.get_dummies(df_total[f], prefix=f)\n    df_total = df_total.drop([f], axis=1)\n    df_total = pd.concat((df_total, df_total_dummy), axis=1)\n\n# Now splitting train and test\nx = df_total[:train_flag]\nx_test = df_total[train_flag:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c15dfdd9f64376467834edc1a4dfb51739aed1c3"},"cell_type":"code","source":"final_train_X = full_pipeline.fit_transform(x)\nfinal_test_X = full_pipeline.transform(x_test)\nle = LabelEncoder()\ntrain_set_y = le.fit_transform(labels)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5255b4de6fc884835ecdcaba63ccdc76d83e4940"},"cell_type":"markdown","source":"Model Development"},{"metadata":{"trusted":true,"_uuid":"5f7c935581dc347a1ae8d7d1ca1381c37a57af6f"},"cell_type":"code","source":"forest_class = RandomForestClassifier(random_state = 42)\n\nn_estimators = [100, 500]\nmin_samples_split = [10, 20]\n\nparam_grid_forest = {'n_estimators' : n_estimators, 'min_samples_split' : min_samples_split}\n\n\nrand_search_forest = GridSearchCV(forest_class, param_grid_forest, cv = 4, refit = True,\n                                 n_jobs = -1, verbose=2)\n\nrand_search_forest.fit(final_train_X, train_set_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"62f147415c7bc1c485130c876da37b7d4b8f4f84"},"cell_type":"code","source":"random_estimator = rand_search_forest.best_estimator_\n\ny_pred_random_estimator = random_estimator.predict_proba(final_train_X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"32ae848cc161531b8ad8f49cae1162aa79a5e9bc"},"cell_type":"code","source":"y_pred = random_estimator.predict_proba(final_test_X) \n\n# We take the 5 highest probabilities for each person\nids = []  #list of ids\ncts = []  #list of countries\nfor i in range(len(id_test)):\n    idx = id_test[i]\n    ids += [idx] * 5\n    cts += le.inverse_transform(np.argsort(y_pred[i])[::-1])[:5].tolist()\n\n# Generating a csv file with the predictions \nsub = pd.DataFrame(np.column_stack((ids, cts)), columns=['id', 'country'])\nsub.to_csv('output_randomForest.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":1}