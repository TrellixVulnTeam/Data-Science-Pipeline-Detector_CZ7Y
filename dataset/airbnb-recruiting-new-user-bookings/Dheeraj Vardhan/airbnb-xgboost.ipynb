{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.decomposition import TruncatedSVD,PCA\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.ensemble import RandomForestRegressor\nimport pickle\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('../input/airbnb-recruiting-new-user-bookings/train_users_2.csv.zip') # Training Set\ndf_test = pd.read_csv('../input/airbnb-recruiting-new-user-bookings/test_users.csv.zip') # Testing Set\ndf_original = df_train.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head() # Head of the dataframe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The Dimensions of the training set is \" , df_train.shape)\nprint(\"The Dimensions of the testing set is \",df_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = df_train['country_destination'].values\n\ndf_train.drop('country_destination',axis=1,inplace=True) # Droping Target Variable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Concatenating training and testing sets for further use\ndf_all = pd.concat([df_train,df_test],axis=0,ignore_index=True) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The Dimensions of Total Set is \",df_all.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function that returns the missing percentage of each column in the dataset.\ndef missing_percentage(df):\n    total = df.isnull().sum().sort_values(ascending=False)\n    percent = total/len(df)*100\n    df = pd.concat([total,percent],axis=1,keys=['Total','Percent'])\n    return df.sort_values(by='Percent',ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_percentage(df_all) # No need to store in a variable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Droping 'date_first_booking' column as it is having roughly 67% missing data \ndf_all.drop(['id','date_first_booking'],axis=1,inplace=True)\ndf_all.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all['date_dac'] = df_all['date_account_created'].apply(lambda x:x.split('-')[2])\ndf_all['date_dac'] = df_all['date_dac'].astype('int')\n\n# We can also do this following like above \"date_dac\" column.\ndf_all['date_account_created'] = pd.to_datetime(df_all['date_account_created'])\ndf_all['month_dac'] = df_all['date_account_created'].apply(lambda x:x.month)\ndf_all['year_dac'] = df_all['date_account_created'].apply(lambda x:x.year)\n\n# No further use of 'date_account_created' column because we retrieved every information from it.\ndf_all.drop('date_account_created',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Retrieving year,month and date from timestamp.\ntfa = np.vstack(df_all.timestamp_first_active.astype(str).\n                apply(lambda x: list(map(int, [x[:4],x[4:6],x[6:8],x[8:10],x[10:12],x[12:14]]))).values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all['tfa_year'] = tfa[:,0]\ndf_all['tfa_month'] = tfa[:,1]\ndf_all['tfa_date'] = tfa[:,2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all = df_all.drop('timestamp_first_active',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function that returns the Categorical columns\ndef Categorical(df):\n    categories = []\n    for column in df.columns:\n        if df[column].dtype =='object' or len(df[column].value_counts()) < 20:\n            categories.append(column)\n    return categories","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Categories = Categorical(df_all)\n\n# Some of the Categorical columns are int-type. So, Changed them to object.\ndf_all[Categories] = df_all[Categories].astype('object')\n\nCategories","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_percentage(df_all)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"order1 = df_original['country_destination'].value_counts()\norder2 = order1.index\nplt.figure(figsize=(10,7))\nsns.countplot(df_original['country_destination'],order=order2)\nplt.xlabel('Country Destination')\nplt.ylabel('Country Destination Count')\nfor i in range(order1.shape[0]):\n    count = order1[i]\n    strg = '{:0.2f}%'.format(100*count/df_all.shape[0])\n    plt.text(i,count+1000,strg,ha='center')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_style('whitegrid')\nplt.figure(figsize=(10,7))\nsns.distplot(df_all['age'].dropna(),kde=False,bins=50)\nplt.xlabel('Age')\nplt.ylabel('Age Count')\nplt.title('The Distribution of Age')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"order1 = df_all['gender'].value_counts()\norder2 = order1.index\nplt.figure(figsize=(10,7))\nsns.countplot(df_all['gender'],order=order2)\nplt.xlabel('Gender')\nfor i in range(order1.shape[0]):\n    count = order1[i]\n    strg = '{:0.2f}%'.format(100*count/df_all.shape[0])\n    plt.text(i,count+1000,strg,ha='center')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"order1 = df_all['signup_method'].value_counts()\norder2 = order1.index\nplt.figure(figsize=(10,7))\nsns.countplot(df_all['signup_method'],order=order2)\nplt.xlabel('Signup Method')\nfor i in range(order1.shape[0]):\n    count = order1[i]\n    strg = '{:0.2f}%'.format(100*count/df_all.shape[0])\n    plt.text(i,count+1000,strg,ha='center')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,7))\nsns.countplot(df_original['country_destination'],hue=df_original['gender'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filling 'first_affiliate_tracked' column using Random Forest Classifier\n\ndef completing_fa_tracked(df):\n    age = df['age']\n    df = df.drop('age',axis=1)\n    y = df['first_affiliate_tracked']\n    df.drop('first_affiliate_tracked',axis=1,inplace=True)\n    \n    cato = Categorical(df)\n    onehot = pd.get_dummies(df[cato],drop_first = True)\n    df = df.drop(cato,axis=1)\n    df = pd.concat([df,onehot],axis=1)\n    df = pd.concat([df,y],axis=1)\n    \n    temp_train = df.loc[df.first_affiliate_tracked.notnull()] \n    temp_test = df.loc[df.first_affiliate_tracked.isnull()]\n    \n    X_train= temp_train.drop('first_affiliate_tracked',axis=1)\n    y_train = temp_train['first_affiliate_tracked']\n    X_test = temp_test.drop('first_affiliate_tracked',axis=1)\n    \n    le = LabelEncoder()\n    le.fit(y_train)\n    y_train = le.transform(y_train)\n    \n    pca = PCA(n_components=12)\n    pca.fit(X_train)\n    X_train_pca = pca.transform(X_train)\n    X_test_pca = pca.transform(X_test)\n    \n    \n    rfc = RandomForestClassifier(n_estimators=400, n_jobs=-1)\n    rfc.fit(X_train_pca, y_train)\n    predicted_fa = rfc.predict(X_test_pca)\n    \n    print(\"score:\" ,rfc.score(X_train_pca,y_train))\n    \n    df.loc[df.first_affiliate_tracked.notnull(),\"first_affiliate_tracked\"] = y_train\n    df.loc[df.first_affiliate_tracked.isnull(), \"first_affiliate_tracked\"] = predicted_fa\n    df['age'] = age\n    \n    df['first_affiliate_tracked'] = df['first_affiliate_tracked'].astype('object')\n    fa_t = pd.get_dummies(df['first_affiliate_tracked'],drop_first=True)\n    df = df.drop('first_affiliate_tracked',axis=1)\n    df = pd.concat([df,fa_t],axis=1)\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all = completing_fa_tracked(df_all)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_percentage(df_all)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filling 'age' column using Random Forest Regressor\n\ndef completing_age(df):\n    \n    temp_train = df.loc[df.age.notnull()] \n    temp_test = df.loc[df.age.isnull()]\n    \n    X_train= temp_train.drop('age',axis=1)\n    y_train = temp_train['age']\n    X_test = temp_test.drop('age',axis=1)\n\n    pca = PCA(n_components=12)\n    pca.fit(X_train)\n    X_train_pca = pca.transform(X_train)\n    X_test_pca = pca.transform(X_test)\n    \n    # Truncated SVD Can work efficiently with Sparse dataset. So it's better to use.\n    \n    t_svd = TruncatedSVD(n_components=12) \n    t_svd.fit(X_train)\n    X_train_tsvd = t_svd.transform(X_train)\n    X_test_tsvd = t_svd.transform(X_test)\n    \n    X_train_new = np.concatenate([X_train_pca,X_train_tsvd],axis=1)\n    X_test_new = np.concatenate([X_test_pca,X_test_tsvd],axis=1)\n    \n    rfr = RandomForestRegressor(n_estimators=400, n_jobs=-1)\n    rfr.fit(X_train_new, y_train)\n    predicted_age = rfr.predict(X_test_new)\n    \n    print(\"score:\" ,rfr.score(X_train_new,y_train))\n    \n    df.loc[df.age.notnull(),\"age\"] = y_train\n    df.loc[df.age.isnull(), \"age\"] = predicted_age\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all = completing_age(df_all)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_percentage(df_all)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Hurrah ..!! Successfully filled Missing values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all['age'] = df_all['age'].astype('int32') # Converting float to int.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = df_all.iloc[0:len(df_train),:]\ntest = df_all.iloc[len(df_train):,:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"le = LabelEncoder()\n# Label Encoding the labels \n\ny = le.fit_transform(labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb = XGBClassifier(max_depth=6, learning_rate=0.3, n_estimators=25,\n                    objective='multi:softprob', subsample=0.5, colsample_bytree=0.5, seed=0)                  \nxgb.fit(train, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predictions using predict_proba to get the probabilities of classes\n\ny_pred = xgb.predict_proba(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Considering the 5 Classes with Highest Probabilities\nid_test = df_test['id']\nids = []\ncts = []\nfor i in range(len(id_test)):\n    idx = id_test[i]\n    ids += [idx] * 5\n    cts += le.inverse_transform(np.argsort(y_pred[i])[::-1])[:5].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Submission \nsub = pd.DataFrame(np.column_stack((ids, cts)), columns=['id', 'country'])\nsub.to_csv('sub.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filename = 'Airbnb.pkl'\n\n# Saving model using pickle\npickle.dump(xgb, open(filename, 'wb'))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}