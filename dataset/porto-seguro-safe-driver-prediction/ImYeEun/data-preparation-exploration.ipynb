{"cells":[{"metadata":{"_uuid":"792ff5c4688181a4f4f14deccbb5354c5c867b73"},"cell_type":"markdown","source":"**Loading packages**"},{"metadata":{"trusted":true,"_uuid":"2e0f7d1e683c7a3f57971fe7bd3cec3c7f468aec"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.utils import shuffle\nfrom sklearn.ensemble import RandomForestClassifier\npd.set_option('display.max_columns', 100)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b6b0da2923e438a481a8a33fd3da8a5169328a0f"},"cell_type":"markdown","source":"**Loading data**"},{"metadata":{"trusted":true,"_uuid":"7b290047af59b8b17833428a09ce8a0e28834b0e"},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3e705659fe0f7fad8da095179a682abfa41e08dc"},"cell_type":"markdown","source":"**Data at first sight**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"Here is an excerpt of the the data description for the competition:\n다음은 경기에 대한 데이터 설명이다.\n\nFeatures that belong to similar groupings are tagged as such in the feature names (e.g., ind, reg, car, calc).\n유사한 그룹에 속하는 feature들은 feature 이름(예: ind, reg, car, calc)에서와 같이 태그 지정된다.\n\nFeature names include the postfix bin to indicate binary features and cat to indicate categorical features.\nfeature \n이름에는 2진수 특징을 나타내는 사후 수정 bin과 범주형 특징을 나타내는 cat이 포함된다.\n\nFeatures without these designations are either continuous or ordinal.\n이러한 지정이 없는 형상은 연속 또는 순서형이다.\n\nValues of -1 indicate that the feature was missing from the observation.\n-1 값은 feature가이 관찰에서 누락되었음을 나타낸다.\n\nThe target columns signifies whether or not a claim was filed for that policy holder.\ntarget 열은 해당 정책 보유자에 대한 클레임이 접수되었는지 여부를 나타낸다.\n\nOk, that's important information to get us started. Let's have a quick look at the first and last rows to confirm all of this.\n좋아, 시작하는 데 중요한 정보야. 이 모든 것을 확인하기 위해 첫 번째 행과 마지막 행들을 간단히 살펴봅시다."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1639986e2fd8634ff56251226744c47953b5f9cf"},"cell_type":"code","source":"train.tail()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a3fe6bc5da8bc81243e8367624ee7fca90c8120f"},"cell_type":"markdown","source":"We indeed see the following\n우리는 정말로 다음을 본다.\n\n* binary variables\n이항 변수\n\n* categorical variables of which the category values are integers\n범주 값이 정수인 범주형 변수\n\n* other variables with integer or float values\n정수 또는 부동 값을 가진 다른 변수\n\n* variables with -1 representing missing values\n결측값을 나타내는 -1의 변수\n\n* the target variable and an ID variable\ntarget 변수 및 ID 변수\n\nLet's look at the number of rows and columns in the train \ndata.\ntrain 데이터의 행과 열의 수를 살펴보자."},{"metadata":{"trusted":true,"_uuid":"4129c6cf173685d2c13dac4e878ac020e0012b04"},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ace663c018c6f3bc2fe9d622a27f8c74f7c47bef"},"cell_type":"markdown","source":"We have 59 variables and 595.212 rows.\n59개의 변수와 595.212개의 행이 있다.\n\nLet's see if we have the same number of variables in the test data.\n테스트 데이터에 동일한 수의 변수가 있는지 확인하십시오.\n\nLet's see if there are duplicate rows in the training data.\ntraining 데이터에 중복된 행이 있는지 확인하십시오."},{"metadata":{"trusted":true,"_uuid":"e180fec354143a8fc76cc19d68f080dfc3539698"},"cell_type":"code","source":"train.drop_duplicates()\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c951f88bf7e62c4e44280ae2c3322304987bfd76"},"cell_type":"markdown","source":"No duplicate rows, so that's fine.\n중복되는 행이 없으니 괜찮아."},{"metadata":{"trusted":true,"_uuid":"0b63da7caefe6bc295feb704edbf28aa94ec6688"},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"49a40fe593f898de73e646580c2b6342e8a34906"},"cell_type":"markdown","source":"We are missing one variable in the test set, but this is the target variable. So that's fine.\n테스트 세트에 변수가 한 개 누락되어 있지만 대상 변수 입니다. 그래, 괜찮아.\n\nLet's now invesigate how many variables of each type we have.\n이제 각 유형의 변수가 몇 개나 있는지 살펴봅시다.\n\nSo later on we can create dummy variables for the 14 categorical variables.\n그래서 나중에 14개의 범주형 변수에 대한 더미 변수를 만들 수 있다.\n\nThe bin variables are already binary and do not need dummification.\nbinary 변수는 이미 2진법이기 때문에 dummization이 필요하지 않다."},{"metadata":{"trusted":true,"_uuid":"9b5e9ef12c6a6f5aab6431657f1d695ae2ca4e87"},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b47fa7e68a2db08022713a32321d9ca5ebc40135"},"cell_type":"markdown","source":"Again, with the info() method we see that the data type is integer or float. \n다시, info() 방법을 사용하여 데이터 유형이 정수 또는 부동임을 알 수 있다.\n\nNo null values are present in the data set. \n데이터 세트에 null 값이 없음.\n\nThat's normal because missing values are replaced by -1. \n결측값이 -1로 대체되기 때문에 정상이다.\n\nWe'll look into that later.\n우리는 그것을 나중에 살펴볼 것이다.\n\n"},{"metadata":{"_uuid":"79a71a533bf8acbde306eb4492c106cedca0b412"},"cell_type":"markdown","source":"**Metadata**"},{"metadata":{"_uuid":"56f6eb4d1c3027f34686031e5b7fea2477053360"},"cell_type":"markdown","source":"To facilitate the data management, we'll store meta-information about the variables in a DataFrame.\n데이터 관리를 용이하게 하기 위해 변수에 대한 메타 정보를 DataFrame에 저장할 것이다.\n\nThis will be helpful when we want to select specific variables for analysis, visualization, modeling, ...\n이는 분석, 시각화, 모델링 등을 위한 특정 변수를 선택하고자 할 때 도움이 될 것이다.\n\nConcretely we will store:\n구체적으로 저장:\n\n* role: input, ID, target\n* level: nominal(명목/범주형자료), interval(구간자료), ordinal(순서형자료), binary\nhttp://blog.heartcount.io/dd\n* keep: True or False\n* dtype: int, float, str"},{"metadata":{"trusted":true,"_uuid":"db15849c64fcce34c2b8a083b0f7d4975e9d8a22"},"cell_type":"code","source":"data = []\nfor f in train.columns:\n    # defining the role\n    if f=='target':\n        role='target'\n    elif f=='id':\n        role='id'\n    else:\n        role='input'\n    \n    # defining the level\n    if 'bin' in f or f == 'target':\n        level = 'binary'\n    elif 'cat' in f or f == 'id':\n        level = 'nominal'\n    elif train[f].dtype ==float:\n        level = 'interval'\n    elif train[f].dtype == int:\n        level = 'ordinal'\n        \n    #Initialize keep to True for all variables except f or id\n    keep = True\n    if f =='id':\n        keep = False\n    \n    # Defining the data type\n    dtype = train[f].dtype\n    \n    #Creating a Dict that contains all the metadata for the variable\n    f_dict = {\n        'varname' : f,\n        'role' : role,\n        'level' : level,\n        'keep' : keep,\n        'dtype' : dtype\n    }\n    data.append(f_dict)\n    \nmeta = pd.DataFrame(data, columns = ['varname', 'role', 'level', 'keep', 'dtype'])\nmeta.set_index('varname',inplace = True)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"40423823532c4e6b6491e9d4ea8df4ea2b604ff2"},"cell_type":"code","source":"meta","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1bdf7072123869b06df42b088b0df1797f5058e2"},"cell_type":"markdown","source":"Example to extract all nominal variables that are not dropped\n\n삭제되지 않은 모든 nominal 변수를 추출하는 예제\n"},{"metadata":{"trusted":true,"_uuid":"a4e5137eff666ba6392887363119e274defa2021"},"cell_type":"code","source":"meta[(meta.level == 'nominal') & (meta.keep)].index","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b5010563f1166fd7b8c6fdbddb37bd56f58f6c57"},"cell_type":"markdown","source":"Below the number of variables per role and level are displayed.\n역할 및 수준별 변수 수 아래에 표시된다."},{"metadata":{"trusted":true,"_uuid":"6c2a7ef6ee61095c213bbdcffd8397aa91edf638"},"cell_type":"code","source":"pd.DataFrame({'count':meta.groupby(['role','level'])['role'].size()}).reset_index()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"da80702b3c089bd93646d61d3689441dae8351e8"},"cell_type":"markdown","source":"**Descriptive statistics**"},{"metadata":{"_uuid":"2c287f1aaa5ecaec5f4b2da547e7ae20acd71cbe"},"cell_type":"markdown","source":"We can also apply the describe method on the dataframe. \n우리는 또한 데이터프레임에 설명법을 적용할 수 있다. \n\nHowever, it doesn't make much sense to calculate the mean, std, ... on categorical variables and the id variable. \n그러나 범주형 변수와 ID 변수에 대한 평균, std, ...을 계산하는 것은 그다지 이치에 맞지 않는다. \n\nWe'll explore the categorical variables visually later.\n우리는 나중에 시각적으로 범주형 변수를 탐구할 것이다.\n\nThanks to our meta file we can easily select the variables on which we want to compute the descriptive statistics.\n메타 파일 덕분에 우리는 기술 통계량을 계산할 변수를 쉽게 선택할 수 있다.\n\nTo keep things clear, we'll do this per data type.\n확실히 하기 위해, 우리는 데이터 유형별로 이것을 할 것이다.\n "},{"metadata":{"_uuid":"42e9d6f280e9e5807caf041dbe1ad5eff1efa846"},"cell_type":"markdown","source":"**Interval variables**"},{"metadata":{"trusted":true,"_uuid":"3c77b6e5e955e1c23b81af05e94d37ed3a3440fa"},"cell_type":"code","source":"v = meta[(meta.level == 'interval') & (meta.keep)].index\ntrain[v].describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8de9b660fa0327516856ec605078f5213c626cf4"},"cell_type":"markdown","source":"reg variables\n* only ps_reg_03 has missing values\nps_reg_03에만 결측값이 있음\n\n* the range (min to max) differs between the variables. We could apply scaling (e.g. StandardScaler), but it depends on the classifier we will want to use.\n변수마다 범위(min to max)가 다르다. **스케일링(예: StandardScaler)을 적용할 수 있지만, 사용할 분류기에 따라 달라진다.**\n\n\ncar variables\n* ps_car_12 and ps_car_15 have missing values\nps_car_12 및 ps_car_15에 결측값이 있음\n\n* again, the range differs and we could apply scaling.\n다시 한 번, 그 범위는 달라졌고, 우리는 스케일링을 적용할 수 있었다.\n\n\ncalc variables\n* no missing values\n결측값 없음\n\n* this seems to be some kind of ratio as the maximum is 0.9\n이것은 최대치가 0.9이기 때문에 일종의 비율인 것 같다.\n\n* all three _calc variables have very similar distributions\n세 가지 _calc 변수는 모두 매우 유사한 분포를 가지고 있다.\n\n\nOverall, we can see that the range of the interval variables is rather small. \n전체적으로, 우리는 간격 변수의 범위가 다소 작다는 것을 알 수 있다. \nPerhaps some transformation (e.g. log) is already applied in order to anonymize the data?\n데이터를 익명화하기 위해 일부 변환(예: 로그)이 이미 적용되었을 수 있는가?\n\n\n\n"},{"metadata":{"_uuid":"a46c24d1f7ec194796d5ce152be2ba47e792d101"},"cell_type":"markdown","source":"**Ordinal variables**"},{"metadata":{"trusted":true,"_uuid":"e48130dc60cf530e5cd918564945966712b462c4"},"cell_type":"code","source":"v = meta[(meta.level == 'ordinal') & (meta.keep)].index\ntrain[v].describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b52d642ac2e23350dbb4c95050851c8bd3acc413"},"cell_type":"markdown","source":"* Only one missing variable: ps_car_11\n누락된 변수만: ps_car_11\n\n* We could apply scaling to deal with the different ranges\n다양한 범위를 처리하기 위해 스케일링을 적용할 수 있다.\n\n"},{"metadata":{"_uuid":"f41d7bf3c4f3b51a26d39367eac2b2431ad8ed73"},"cell_type":"markdown","source":"**Binary variables**"},{"metadata":{"trusted":true,"_uuid":"50dc025fee7e4f03c07566bb428d244d3d967c05"},"cell_type":"code","source":"v = meta[(meta.level == 'binary') & (meta.keep)].index\ntrain[v].describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f24eba9c8bcce796df4f67ceb6f745dd3cb3ae7e"},"cell_type":"markdown","source":"* ** A priori in the train data is 3.645%, which is strongly imbalanced.\ntrain 데이터의 선취율은 3.645%로 매우 불균형적이다.**\n\n* From the means we can conclude that for most variables the value is zero in most cases.\nmean으로부터 우리는 대부분의 변수에 대한 값이 0이라고 결론을 내릴 수 있다."},{"metadata":{"_uuid":"484535ea6edd48f79d1b5adda210f41205b53487"},"cell_type":"markdown","source":"**Handling imbalanced classes**"},{"metadata":{"_uuid":"a0e17eb80b443e30b8b85bf8d9e9def4cafa1f61"},"cell_type":"markdown","source":"As we mentioned above the proportion of records with target=1 is far less than target=0.\n위에서 언급한 바와 같이, target=1을 갖는 기록의 비율은 target=0보다 훨씬 낮다.\n\nThis can lead to a model that has great accuracy but does have any added value in practice. \n따라서 정확도가 높지만 실제로 가치가 있는 모델이 될 수 있다.\n\nTwo possible strategies to deal with this problem are:\n이 문제를 해결하기 위한 두 가지 가능한 전략은 다음과 같다.\n\n* oversampling records with target=1\ntarget=1로 레코드 **오버샘플링**\n\n* undersampling records with target=0\ntarget=0으로 레코드 **언더샘플링**\n\nThere are many more strategies of course and MachineLearningMastery.com gives a nice overview.\n물론 더 많은 전략들이 있고 MachineLearningMastery.com은 멋진 개요를 제공한다.\nAs we have a rather large training set, we can go for undersampling.\n우리는 꽤 큰 훈련 세트를 가지고 있기 때문에, 언더샘플링을 할 수 있다.\n"},{"metadata":{"_uuid":"5d7de91cd5ddafdf8713386dabaf908c45a4f089"},"cell_type":"markdown","source":"96 ->0\n4 -> 1\n\n* 오버샘플링은 4를 늘려서 50 부풀린 다음 ->1\n총 146개의 샘플이 생기는 것.\n\n* 언더샘플링은 96개를 50개 ->0\n4->1\n54개의 샘플이 생기는 것."},{"metadata":{"_uuid":"db72790ffd95fb05aa849f950ad941acfa1e9a5f"},"cell_type":"markdown","source":"**smote**\n캐글코리아에 공유되어있음..! imbalanced-learn 라이브러리\n\nQ. 명목형변수(categorical var) smote 사용할수 있을까?"},{"metadata":{"_uuid":"3038d888eb66efa57384826f6f88a17cf24bd153"},"cell_type":"markdown","source":""},{"metadata":{"trusted":true,"_uuid":"6d545c5ef09a9a56897e7c3cbbdc1d4c83035df6"},"cell_type":"code","source":"desired_apriori=0.10\n\n# Get the indices per target value\nidx_0 = train[train.target==0].index\nidx_1 = train[train.target==1].index\n\n# Get original number of records per target value\nnb_0 = len(train.loc[idx_0])\nnb_1 = len(train.loc[idx_1])\n\n# Calculate the undersampling rate and resulting number of records with target =0\nundersampling_rate = ((1-desired_apriori)*nb_1)/(nb_0*desired_apriori)\nundersampled_nb_0 = int(undersampling_rate*nb_0)\nprint('Rate to undersample records with target = 0 : {}'.format(undersampling_rate))\nprint('Number of records with target=0 after undersampling: {}'.format(undersampled_nb_0))\n\n# Randomly select records with target=0 to get at the desired a priori\nundersampled_idx = shuffle(idx_0, random_state=37, n_samples = undersampled_nb_0)\n#몇개 뽑을래 하니깐 195246만큼.\n# Construct list with remaining indices\nidx_list = list(undersampled_idx) + list(idx_1)\n\n# Return undersample data frame\ntrain = train.loc[idx_list].reset_index(drop=True)\n#셔플했으니 리셋인덱스 해준다.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6c6b4f9473bc3a19d8b211136c39a0f14ea344d4"},"cell_type":"code","source":"print(idx_list)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4170b86f181d648f79b75c30caceacc4990f3e8e"},"cell_type":"markdown","source":"**Data Quality Checks**"},{"metadata":{"trusted":true,"_uuid":"562f0ac4a0960b6dae074ba76f6736fcc0eea86c"},"cell_type":"markdown","source":"Checking missing values\n\nMissings are represented as -1"},{"metadata":{"trusted":true,"_uuid":"a5c9d2724c815939122de0fcbea21fab132fb628"},"cell_type":"code","source":"vars_with_missing = []\n\nfor f in train.columns:\n    missings = train[train[f] == -1][f].count()\n    if missings>0:\n        vars_with_missing.append(f)\n        missings_perc = missings/train.shape[0]\n        \n        print('Variable {} has {} records ({:.2%}) with missings values'.format(f, missings, missings_perc))\n\nprint('In total, there are {} variables with missing values'.format(len(vars_with_missing)))        \n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d73e29f1a66470ca60c2769cc3e99a58532c72e1"},"cell_type":"markdown","source":"* **ps_car_03_cat and ps_car_05_cat** have a large proportion of records with missing values. Remove these variables.\nps_car_03_cat와 ps_car_05_cat은 결측값을 가진 기록의 많은 부분을 가지고 있다. 이러한 변수를 제거하십시오.\n\n* For the other categorical variables with missing values, we can leave the missing value -1 as such.\n결측값이 있는 다른 범주형 변수의 경우 결측값 -1을 그대로 남겨둘 수 있다.\n\n* **ps_reg_03** (continuous) has missing values for 18% of all records. Replace by the mean.\nps_reg_03(연속)은 모든 기록의 18%에 대한 결측값을 가지고 있다. 평균으로 대체한다.\n\n* **ps_car_11** (ordinal) has only 5 records with misisng values. Replace by the mode.\nps_car_11(주문)에는 오수 값이 있는 레코드가 5개뿐입니다. 모드(최빈값)로 교체한다.\n\n* **ps_car_12** (continuous) has only 1 records with missing value. Replace by the mean.\nps_car_12(연속)에는 결측값이 있는 레코드가 1개만 있다. 평균으로 대체한다.\n\n* **ps_car_14** (continuous) has missing values for 7% of all records. Replace by the mean.\nps_car_14(연속)는 모든 기록의 7%에 대한 결측값을 가지고 있다. 평균으로 대체한다.\n"},{"metadata":{"trusted":true,"_uuid":"2ad1c8171bc2806b11d2337c5ff8bc6844c2a243"},"cell_type":"code","source":"# Dropping the variables with too many missing values \nvars_to_drop = ['ps_car_03_cat', 'ps_car_05_cat']\ntrain.drop(vars_to_drop, inplace=True,axis=1)\nmeta.loc[(vars_to_drop), 'keep'] = False #Updating the meta\n\n#Imputing with mean or mode\nmean_imp = Imputer(missing_values=-1,strategy='mean', axis=0)\nmode_imp = Imputer(missing_values=-1, strategy='most_frequent', axis=0)\n\ntrain['ps_reg_03'] = mean_imp.fit_transform(train[['ps_reg_03']]).ravel()\ntrain['ps_car_12'] = mean_imp.fit_transform(train[['ps_car_12']]).ravel()\ntrain['ps_car_14'] = mean_imp.fit_transform(train[['ps_car_14']]).ravel()\ntrain['ps_car_11'] = mode_imp.fit_transform(train[['ps_car_11']]).ravel()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7de4480fdf854174ea6188e6eda98794da573fbe"},"cell_type":"markdown","source":"**Checking the cardinality of the categorical variables**"},{"metadata":{"_uuid":"7a4f65b1b4ff1bb948c37a921251f24252441735"},"cell_type":"markdown","source":"Cardinality refers to the number of different values in a variable. \n카디널리티는 변수에 있는 다른 값의 수를 의미한다.\n\nAs we will create dummy variables from the categorical variables later on, we need to check whether there are variables with many distinct values.\n추후 범주형 변수에서 더미 변수를 만들 예정이므로 구별되는 값이 많은 변수가 있는지 확인해야 한다.\n\nWe should handle these variables differently as they would result in many dummy variables.\n우리는 이러한 변수들이 많은 더미 변수를 초래할 수 있기 때문에 다르게 취급해야 한다."},{"metadata":{"trusted":true,"_uuid":"af798b800c94f35f139d5d4e0491dd017b292032"},"cell_type":"code","source":"v = meta[(meta.level == 'nominal') & (meta.keep)].index\n\nfor f in v:\n    dist_values = train[f].value_counts().shape[0]\n    print('Variable {} has {} distinct values'.format(f,dist_values))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1351ec6cb30e2db3db40fb043077d96e3ced871b"},"cell_type":"markdown","source":"Only **ps_car_11_cat** has many distinct values, although it is still reasonable.\nps_car_11_cat만이 많은 뚜렷한 가치를 가지고 있지만, 여전히 합리적이다."},{"metadata":{"trusted":true,"_uuid":"29f6a651e966bd7ed1f517011f2d11e5398b66be"},"cell_type":"code","source":"# Script by https://www.kaggle.com/ogrellier\n# Code: https://www.kaggle.com/ogrellier/python-target-encoding-for-categorical-features\ndef add_noise(series, noise_level):\n    return series * (1 + noise_level * np.random.randn(len(series)))\n\ndef target_encode(trn_series=None, \n                  tst_series=None, \n                  target=None, \n                  min_samples_leaf=1, \n                  smoothing=1,\n                  noise_level=0):\n    \"\"\"\n    Smoothing is computed like in the following paper by Daniele Micci-Barreca\n    https://kaggle2.blob.core.windows.net/forum-message-attachments/225952/7441/high%20cardinality%20categoricals.pdf\n    trn_series : training categorical feature as a pd.Series\n    tst_series : test categorical feature as a pd.Series\n    target : target data as a pd.Series\n    min_samples_leaf (int) : minimum samples to take category average into account\n    smoothing (int) : smoothing effect to balance categorical average vs prior  \n    \"\"\" \n    assert len(trn_series) == len(target)\n    assert trn_series.name == tst_series.name\n    temp = pd.concat([trn_series, target], axis=1)\n    # Compute target mean \n    averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n    # Compute smoothing\n    smoothing = 1 / (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) / smoothing))\n    # Apply average function to all target data\n    prior = target.mean()\n    # The bigger the count the less full_avg is taken into account\n    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n    # Apply averages to trn and tst series\n    ft_trn_series = pd.merge(\n        trn_series.to_frame(trn_series.name),\n        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n        on=trn_series.name,\n        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n    # pd.merge does not keep the index so restore it\n    ft_trn_series.index = trn_series.index \n    ft_tst_series = pd.merge(\n        tst_series.to_frame(tst_series.name),\n        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n        on=tst_series.name,\n        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n    # pd.merge does not keep the index so restore it\n    ft_tst_series.index = tst_series.index\n    return add_noise(ft_trn_series, noise_level), add_noise(ft_tst_series, noise_level)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"099184c49a79b99b955d2c3636b0c9c326171e38"},"cell_type":"code","source":"train_encoded, test_encoded = target_encode(train[\"ps_car_11_cat\"], \n                             test[\"ps_car_11_cat\"], \n                             target=train.target, \n                             min_samples_leaf=100,\n                             smoothing=10,\n                             noise_level=0.01)\n    \ntrain['ps_car_11_cat_te'] = train_encoded\ntrain.drop('ps_car_11_cat', axis=1, inplace=True)\nmeta.loc['ps_car_11_cat','keep'] = False  # Updating the meta\ntest['ps_car_11_cat_te'] = test_encoded\ntest.drop('ps_car_11_cat', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6cb2a6b844eceaedc301a5fcafe951f4cec62689"},"cell_type":"markdown","source":"**Exploratory Data Visualization**"},{"metadata":{"_uuid":"0ab61aa66bb1f9dfd9da5b396fdd7e22939b7edf"},"cell_type":"markdown","source":"Categorical variables\n\nLet's look into the categorical variables and the proportion of customers with target = 1\n범주형 변수와 target = 1인 고객 비율을 살펴봅시다.\n"},{"metadata":{"trusted":true,"_uuid":"6dd2dc1032e826d66ac8b3c3c78a71e59005584d"},"cell_type":"code","source":"v = meta[(meta.level == 'nominal') & (meta.keep)].index\n\nfor f in v:\n    plt.figure()\n    fig, ax = plt.subplots(figsize = (20,10))\n    #Calculate the percentage of target = 1 per category value\n    \n    cat_perc = train[[f,'target']].groupby([f],as_index=False).mean()\n    cat_perc.sort_values(by='target', ascending = False, inplace = True)\n    # Bar plot\n    # Order the bars descending on target mean\n    sns.barplot(ax = ax, x= f, y='target', data=cat_perc,order=cat_perc[f])\n    plt.ylabel('% target', fontsize= 18)\n    plt.xlabel(f, fontsize= 18)\n    plt.tick_params(axis='both', which='major',labelsize=18)\n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2fe1abf7c6260044e6d72ff6d1368ebebe4a79b9"},"cell_type":"markdown","source":"As we can see from the variables **with missing values**, it is a good idea to keep the missing values as a separate category value, instead of replacing them by the mode for instance. \n결측값이 있는 변수에서 알 수 있듯이, 결측값을 모드로 대체하는 대신 별도의 범주값으로 유지하는 것이 좋다. \n\nThe customers with a missing value appear to have a much higher (in some cases much lower) probability to ask for an insurance claim.\n결손가치가 있는 고객들은 보험금을 청구할 확률이 훨씬 더 높은 것 같다.\n"},{"metadata":{"_uuid":"eef124c36c268cbc2a8fadd543caaeaf33b93838"},"cell_type":"markdown","source":"**Interval variables**\nChecking the correlations between interval variables.\n구간 변수 간의 상관 관계 확인.\n\nA heatmap is a good way to visualize the correlation between variables. \n열 지도는 변수 간의 상관 관계를 시각화하는 좋은 방법이다. \n\nThe code below is based on an example by Michael Waskom\n아래 코드는 Michael Waskom의 예를 기반으로 한다"},{"metadata":{"trusted":true,"_uuid":"dc2ebd47338579d1e0ddf21dc1b5ddb0738c01d0"},"cell_type":"code","source":"def corr_heatmap(v):\n    correlations = train[v].corr()\n    \n    # Create color map ranging between two colors\n    \n    cmap = sns.diverging_palette(220,10,as_cmap=True)\n    \n    fig, ax = plt.subplots(figsize = (10,10))\n    sns.heatmap(correlations, cmap = cmap, vmax=1.0, center = 0, fmt='.2f',\n               square=True, linewidths = .5, annot = True,\n               cbar_kws={'shrink':.75})\n    plt.show();\nv = meta[(meta.level=='interval') & (meta.keep)].index\ncorr_heatmap(v)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"084f5f094d1f67a3d224fc27168aabc3f35d89b2"},"cell_type":"markdown","source":"There are a strong correlations between the variables:\n\n* ps_reg_02 and ps_reg_03 (0.7)\n* ps_car_12 and ps_car13 (0.67)\n* ps_car_12 and ps_car14 (0.58)\n* ps_car_13 and ps_car15 (0.67)\nSeaborn has some handy plots to visualize the (linear) relationship between variables.\nSeaborn은 변수 간의 (선형) 관계를 시각화하기 위한 몇 가지 편리한 그림을 가지고 있다.\n\nWe could use a pairplot to visualize the relationship between the variables.\n우리는 변수 사이의 관계를 시각화하기 위해 쌍 그림을 사용할 수 있다. \n\nBut because the heatmap already showed the limited number of correlated variables, we'll look at each of the highly correlated variables separately.\n그러나 열 지도는 이미 상관 변수의 제한된 수를 보여 주었기 때문에, 각각의 높은 상관 변수를 별도로 살펴볼 것이다.\n\nNOTE: I take a sample of the train data to speed up the process. \n주: 속도를 높이기 위해 train 데이터의 샘플을 채취한다."},{"metadata":{"trusted":true,"_uuid":"7048da68c8082f8b02696491ee164be89ffcfd57"},"cell_type":"code","source":"s = train.sample(frac=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a4fb998a2dd1c4135e25500f3841faec2f8c337c"},"cell_type":"markdown","source":"ps_reg_02 and ps_reg_03\n\nAs the regression line shows, there is a linear relationship between these variables. \n회귀선이 보여주는 것처럼, 이 변수들 사이에는 선형 관계가 있다.\n\nThanks to the hue parameter we can see that the regression lines for target=0 and target=1 are the same.\n hue 파라미터 덕분에 target=0과 target=1에 대한 회귀선이 동일하다는 것을 알 수 있다.\n"},{"metadata":{"trusted":true,"_uuid":"fbd54f171e91e122c9911a1b2c06f91a59606769"},"cell_type":"code","source":"sns.lmplot(x='ps_reg_02', y = 'ps_reg_03', data=s, hue='target',palette = 'Set1',scatter_kws={'alpha':0.3})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e9f0840d91b9904e73bf6cb66d1f8192419eb356"},"cell_type":"code","source":"sns.lmplot(x='ps_car_12', y ='ps_car_13', data=s, hue='target',palette='Set1', scatter_kws={'alpha':0.3})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"81bbb9ceb3e4739e957ec25e37bc1b1f0ef4d1d2"},"cell_type":"code","source":"sns.lmplot(x='ps_car_12',y='ps_car_14',data=s,hue='target',palette='Set1',scatter_kws={'alpha':0.3})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b03c385b8296e3008a025e6f1c75558127dae5d8"},"cell_type":"code","source":"sns.lmplot(x='ps_car_15',y='ps_car_13',data=s,hue='target',palette='Set1',scatter_kws={'alpha':0.3})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"63adda7ab5aac56076dd6d04f1347d748f76ea44"},"cell_type":"markdown","source":"Allright, so now what? How can we decide which of the correlated variables to keep?\n어떤 상관 변수를 유지할 것인지 어떻게 결정할 수 있는가?\n\nWe could perform Principal Component Analysis (PCA) on the variables to reduce the dimensions.\n우리는 차수를 줄이기 위해 변수에 대한 주성분분석(PCA)을 수행할 수 있다.\n\nIn the AllState Claims Severity Competition I made this kernel to do that.\n올스테이트는 심각성을 주장하지만, 나는 그것을 하기 위해 이 커널을 만들었다.\n\nBut as the number of correlated variables is rather low, we will let the model do the heavy-lifting.\n그러나 상관 변수의 수가 상대적으로 적기 때문에 우리는 모델이 무거운 것을 들도록 할 것이다.\n\n"},{"metadata":{"_uuid":"1b38c9737d11e4b96d85eb6bf1ef02395bd74a6d"},"cell_type":"markdown","source":"**Checking the correlations between ordinal variables**"},{"metadata":{"trusted":true,"_uuid":"107d519180ab283bdf242d8d6a7f097ed007f01b"},"cell_type":"code","source":"v = meta[(meta.level == 'ordinal') & (meta.keep)].index\ncorr_heatmap(v)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a1c63f92c6161069c95efadad215f38e4b7c8ce5"},"cell_type":"markdown","source":"For the ordinal variables we do not see many correlations. \n순서형 변수에 대해서는 많은 상관관계가 보이지 않는다. \n\nWe could, on the other hand, look at how the distributions are when grouping by the target value.\n반면에 목표값을 기준으로 그룹화할 때 분포가 어떤지 살펴볼 수 있다.\n"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"bc98ffc89d0d45cbfdc629f59e10198bd2e9d25d"},"cell_type":"code","source":"v = meta[(meta.level == 'nominal') & (meta.keep)].index\nprint('Before dummification we have {} variables in train'.format(train.shape[1]))\ntrain = pd.get_dummies(train,columns=v, drop_first = True)\nprint('After dummification we have {} variables in train'.format(train.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b8b0f3231bcb6e74a817f8faa58885cdaf50e702"},"cell_type":"markdown","source":"So, creating dummy variables adds 52 variables to the training set.\n따라서 더미 변수를 만드는 것은 52개의 변수를 훈련 세트에 추가한다."},{"metadata":{"_uuid":"889bbb158f234f5f9fc6e510d996d427810833d4"},"cell_type":"markdown","source":"**Creating interaction variables**"},{"metadata":{"trusted":true,"_uuid":"c1bce33da9a6d92b14f51886a0f29d417cffd3ec"},"cell_type":"code","source":"v = meta[(meta.level == 'interval') & (meta.keep)].index\npoly = PolynomialFeatures(degree=2, interaction_only=False,include_bias=False)\ninteractions = pd.DataFrame(data=poly.fit_transform(train[v]),columns=poly.get_feature_names(v))\ninteractions.drop(v, axis=1,inplace=True)#Remove the original columns\n#concat the interaction variables to the train data\nprint('Before creating interactions we have {} variables in train'.format(train.shape[1]))\ntrain = pd.concat([train,interactions],axis=1)\nprint('After creating interactions we have {} variables in train'.format(train.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"65566f3224f5b72f13ea8a0e41354277f30d6cb1"},"cell_type":"markdown","source":"This adds extra interaction variables to the train data. \n이것은 train 데이터에 추가적인 상호작용 변수를 추가한다. \n\nThanks to the get_feature_names method we can assign column names to these new variables.\nget_feature_names 메서드 덕분에 이 새 변수에 열 이름을 할당할 수 있다."},{"metadata":{"_uuid":"8f54e46ca1bd74e101dabf8a02e05cd76c8fbcac"},"cell_type":"markdown","source":"**Feature selection**"},{"metadata":{"_uuid":"7a426b8476a79b9bffe3d98d46df948b5afaa425"},"cell_type":"markdown","source":"**Removing features with low or zero variance**\n\nPersonally, I prefer to let the classifier algorithm chose which features to keep.\n개인적으로, 나는 분류기 알고리즘이 어떤 특징을 유지할지를 선택하도록 하는 것을 선호한다. \n\nBut there is one thing that we can do ourselves.\n하지만 우리가 스스로 할 수 있는 일이 한 가지 있다.\n\nThat is removing features with no or a very low variance. \n그것은 분산이 없거나 매우 낮은 형상을 제거하는 것이다. \n\nSklearn has a handy method to do that: VarianceThreshold. \nSklearn은 VariantThreshold라는 유용한 방법을 가지고 있다.\n\nBy default it removes features with zero variance.\n기본적으로 분산이 0인 형상을 제거한다.\n\nThis will not be applicable for this competition as we saw there are no zero-variance variables in the previous steps.\n이전 단계에서 분산 변수가 없는 것을 보았으므로 이 경기에는 적용되지 않을 것이다. \n\nBut if we would remove features with less than 1% variance, we would remove 31 variables.\n그러나 1% 미만의 차이를 가진 형상을 제거하면 31개의 변수를 제거할 것이다.\n"},{"metadata":{"trusted":true,"_uuid":"81501abdc898b133f8061023e96802fb7f033a88"},"cell_type":"code","source":"selector = VarianceThreshold(threshold=.01)\nselector.fit(train.drop(['id','target'],axis=1)) #fit to train without id and target variables\n\nf = np.vectorize(lambda x : not x) # Function to toggle boolean array elements\n\nv = train.drop(['id','target'], axis=1).columns[f(selector.get_support())]\nprint('{} variables have too low variance.'.format(len(v)))\nprint('These variables are {}'.format(list(v)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bf5842d94122babe592fe68727986b8b6a602680"},"cell_type":"markdown","source":"We would lose rather many variables if we would select based on variance. 만약 우리가 분산에 근거하여 선택한다면 우리는 오히려 많은 변수를 잃게 될 것이다. \n\nBut because we do not have so many variables, we'll let the classifier chose.\n하지만 우리는 변수가 많지 않기 때문에 분류자를 선택하게 될 것이다. \n\nFor data sets with many more variables this could reduce the processing time\n변수가 더 많은 데이터 세트의 경우 처리 시간을 줄일 수 있다.\n\n\n\nSklearn also comes with other feature selection methods. \nsklearn은 또한 다른 기능 선택 방법과 함께 제공된다.\n\nOne of these methods is SelectFromModel in which you let another classifier select the best features and continue with these.\n이러한 방법 중 하나는 SelectFromModel로, 당신은 다른 분류자가 가장 좋은 특징을 선택하고 이것들을 계속하도록 한다.\n\nBelow I'll show you how to do that with a Random Forest.\n아래에서는 랜덤 포레스트로 어떻게 하는지 알려줄게."},{"metadata":{"_uuid":"52bba8c0e4cf2457841b76b033265d6fd43d5565"},"cell_type":"markdown","source":"**Selecting features with a Random Forest and SelectFromModel**"},{"metadata":{"_uuid":"6a6c17e30d91c5236908f926dcacbfd0c31d7951"},"cell_type":"markdown","source":"Here we'll base feature selection on the feature importances of a random forest. \n여기서는 랜덤 포리스트의 피쳐 선택을 기반으로 한다.\n\nWith Sklearn's SelectFromModel you can then specify how many variables you want to keep.\nSklearn's SelectFromModel을 사용하여 보관할 변수 수를 지정할 수 있다. \n\nYou can set a threshold on the level of feature importance manually.\n형상 중요도 수준에 대한 임계값을 수동으로 설정할 수 있다. \n\nBut we'll simply select the top 50% best variables.\n그러나 우리는 단지 상위 50%의 변수를 선택할 것이다.\n"},{"metadata":{"trusted":true,"_uuid":"a998a1e6b6e58bb1c3e3431b3de201440629b8c5"},"cell_type":"code","source":"X_train= train.drop(['id','target'], axis=1)\ny_train = train['target']\n\nfeat_labels = X_train.columns\n\nrf = RandomForestClassifier(n_estimators=1000,random_state=0,n_jobs=-1)\n\nrf.fit(X_train, y_train)\nimportances = rf.feature_importances_\n\nindices = np.argsort(rf.feature_importances_)[::-1]\n\nfor f in range(X_train.shape[1]):\n    print(\"%2d) %-*s %f\"%(f+1,30,feat_labels[indices[f]],importances[indices[f]]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1a95b782a6d7644bb69022eb4a05014755fafd9b"},"cell_type":"markdown","source":"With SelectFromModel we can specify which prefit classifier to use and what the threshold is for the feature importances. \nSelectFromModel을 사용하여 사용할 프리핏 분류기 및 피쳐가져오기에 대한 임계값을 지정할 수 있다. \n\nWith the get_support method we can then limit the number of variables in the train data.\nget_support 방법을 사용하여 train 데이터의 변수 수를 제한할 수 있다."},{"metadata":{"trusted":true,"_uuid":"4462195d67c4ca1cffe80ecff09e8d5947b4ceca"},"cell_type":"code","source":"sfm = SelectFromModel(rf, threshold='median', prefit=True)\nprint('Number of features before selection: {}'.format(X_train.shape[1]))\nn_features = sfm.transform(X_train).shape[1]\nprint('Number of features after selection: {}'.format(n_features))\nselected_vars = list(feat_labels[sfm.get_support()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"65d67369a72d83fb0e0e9ad645d1ef63348ea74c"},"cell_type":"code","source":"train = train[selected_vars + ['target']]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c31910635af345bfb7f40888f0323d5b360421da"},"cell_type":"markdown","source":"**Feature scaling**"},{"metadata":{"_uuid":"d887aae357bd7cd778d6a4826015405b0ee8abd4"},"cell_type":"markdown","source":"As mentioned before, we can apply standard scaling to the training data.\n앞에서 언급한 바와 같이 표준 스케일링을 training 데이터에 적용할 수 있다.\n\nSome classifiers perform better when this is done.\n어떤 분류자들은 이것이 끝났을 때 더 잘 한다.\n\n"},{"metadata":{"trusted":true,"_uuid":"969ccdf13c0c01e9196335925e87f36296a124bf"},"cell_type":"code","source":"scaler = StandardScaler()\nscaler.fit_transform(train.drop(['target'], axis=1))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8fb9fee72c939787c8d7785ee07ba245d3a08d73"},"cell_type":"markdown","source":"**Conclusion**\n\n"},{"metadata":{"trusted":true,"_uuid":"77404fa7d19d144ffe634a898848637e5490f46a"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}