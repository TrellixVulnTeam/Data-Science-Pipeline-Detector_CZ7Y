{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\n![Porto Seguro Auto](https://segurodecarroaqui.com.br/wp-content/uploads/2017/12/sulamerica-seguro-auto.png)\n\nThis notebook starts by giving an introduction in the data of Porto Seguro competition.  Then follows with preparing and running few predictive models using cross-validation and stacking and prepares a submission.\n\nThe notebook is using elements from the following kernels:\n* [Data Preparation and Exploration](https://www.kaggle.com/bertcarremans/data-preparation-exploration) by Bert Carremans.  \n* [Steering Whell of Fortune - Porto Seguro EDA](https://www.kaggle.com/headsortails/steering-wheel-of-fortune-porto-seguro-eda) by Heads or Tails  \n* [Interactive Porto Insights - A Plot.ly Tutorial](https://www.kaggle.com/arthurtok/interactive-porto-insights-a-plot-ly-tutorial) by Anisotropic \n* [Simple Stacker](https://www.kaggle.com/yekenot/simple-stacker-lb-0-284) by Vladimir Demidov\n\n\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Analysis packages**"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.utils import shuffle\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.feature_selection import SelectFromModel\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_val_score\n\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import LogisticRegression\n\npd.set_option('display.max_columns', 100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Load The Data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"trainset = pd.read_csv('/kaggle/input/porto-seguro-safe-driver-prediction/train.csv')\ntestset = pd.read_csv('/kaggle/input/porto-seguro-safe-driver-prediction/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Few quick observations**"},{"metadata":{},"cell_type":"markdown","source":"Based on the data description\n* Few **groups** are defined and featuers that belongs to these groups include patterns in the name(ind, reg, car, calc).\nThe **ind** indicates most probably **individual reg** is probably **registration, car** is self-explanatory, **calc** suggest a **calculated** field:\n* The postfix **bin** is used for binary features;\n* The postfix **cat** to is used for categorical features;\n* Features without the **bin** or **cat** indications are real numbers (continous values) of integers ( ordinal values );\n* A missing value is indicated by **-1**;\n* The value that is subject of prediction is in the **target** column. This one indicates whether or not a claim was field for that insured person;\n* **id** is a data input ordinal number.\n\nLet's check the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"trainset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* cat values : categorical\n* integer values : ranging from 0 to n\n* bin values : binary ( either 0 or 1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# how many rows and columns\nprint(\"Train dataset (rows, cols): \", trainset.shape, \"\\nTest dataset (rows, cols):\", testset.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* => missing \"target\" variable in Test dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Columns in train not in test dataset:\", set(trainset.columns)-set(testset.columns))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Introduction of metadata**"},{"metadata":{},"cell_type":"markdown","source":"To make easier the manipulation of data\n* use : 특정 field(ID, target), 입력 값들(input)\n* type : nominal, interval, ordinal, binary\n* preserve : True or False\n* dataType : int, float, char\n* category : ind, reg, car, calc"},{"metadata":{"trusted":true},"cell_type":"code","source":"# uses code from https://www.kaggle.com/bertcarremans/data-preparation-exploration ( see references )\ndata = []\nfor feature in trainset.columns:\n    # Defining the role\n    if feature == 'target':\n        use = 'target'\n    elif feature == 'id':\n        use = 'id'\n    else:\n        use = 'input'\n        \n    # Defining the type\n    if 'bin' in feature or feature == 'target':\n        type = 'binary'\n    elif 'cat' in feature or feature == 'id':\n        type = 'categorical'\n    elif trainset[feature].dtype == float or isinstance(trainset[feature].dtype, float):\n        type = 'real'\n    elif trainset[feature].dtype == int:\n        type = 'integer'\n        \n    # Initialize preserve to True for all variables except for id\n    preserve = True\n    if feature == 'id':\n        preserve = False\n        \n    # Defining the data type\n    dtype = trainset[feature].dtype\n    \n    category = 'none'\n    # Defining the category\n    if 'ind' in feature:\n        category = 'individual'\n    elif 'reg' in feature:\n        category = 'registration'\n    elif 'car' in feature:\n        category = 'car'\n    elif 'calc' in feature:\n        category = 'calculated'\n        \n    # Creating a Dict that contains all the metadata for the variable\n    feature_dictionary = {\n        'varname': feature,\n        'use': use,\n        'type': type,\n        'preserve': preserve,\n        'dtype': dtype,\n        'category': category\n    }\n    data.append(feature_dictionary)\n    \nmetadata = pd.DataFrame(data, columns=['varname', 'use', 'type', 'preserve', 'dtype', 'category'])\nmetadata.set_index('varname', inplace=True)\nmetadata","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"=> \n* varname 'id'\n* use : 'id', role is 'id', Role = ( target/id/input )\n* type : 'categorical' ( binary/categorical/real/integer )\n* preserve : False ( varname = 'id', False )\n* dtype : int64 ( int / float / char )\n* category : none ( individual / registration / car / calculated )"},{"metadata":{"trusted":true},"cell_type":"code","source":"# all categorical values : \nmetadata[(metadata.type == 'categorical') & (metadata.preserve)].index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# count all features, \"category\"\npd.DataFrame({'count' : metadata.groupby(['category'])['category'].size()}).reset_index()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* => 20 calculated features\n* => 18 individual \n* => 3 registration"},{"metadata":{"trusted":true},"cell_type":"code","source":"# count all features, \"use\" and \"type\"\n# type (nominal, interval, ordinal, binary)\npd.DataFrame({'count' : metadata.groupby(['use', 'type'])['use'].size()}).reset_index()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* => \"id\" : \n* => type ( nominal, interval, ordinal, binary )"},{"metadata":{},"cell_type":"markdown","source":"**Data analysis and Statistics**"},{"metadata":{},"cell_type":"markdown","source":"* Target variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nfig, ax = plt.subplots(figsize=(6,6))\nx = trainset['target'].value_counts().index.values\ny = trainset['target'].value_counts().values\n\n# Bar plot\n# Order\nsns.barplot(ax=ax, x=x, y=y)\nplt.ylabel('Number of values', fontsize=12)\nplt.xlabel('Target value', fontsize=12)\nplt.tick_params(axis='both', which='major', labelsize=12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* => 1 value : only 3.64%\n* => highly imbalanced\n* => target=1 => oversampling or target=0 => undersampling\n* => will do undersampling with target=0"},{"metadata":{},"cell_type":"markdown","source":"* Real features\n( type : 'categorical' ( binary/categorical/real/integer ) "},{"metadata":{"trusted":true},"cell_type":"code","source":"variable = metadata[(metadata.type == 'real') & (metadata.preserve)].index\ntrainset[variable].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ps_car_12 값 x 10 한 다음에 2승\n# 값을 크게해서 보기 쉽게 하려는 의도?\n(pow(trainset['ps_car_12']*10, 2)).head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(pow(trainset['ps_car_15'],2)).head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Features with missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"trainset['ps_reg_03'].head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(trainset['ps_reg_03'] == -1).sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(trainset['ps_car_12'] == -1).sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(trainset['ps_car_14'] == -1).sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Car features**\n* 의미있는 그래프를 보기위해 ps_car_12는 자연수 (근사치와 함께) 제곱근(10으로 나눔)하고,\n* ps_car_12는 자연수 제곱근 사용. "},{"metadata":{"trusted":true},"cell_type":"code","source":"sample = trainset.sample(frac=0.05)\nvar = ['ps_car_12', 'ps_car_15', 'target']\nsample = sample[var]\nsns.pairplot(sample, hue='target', palette='Set1', diag_kind='kde')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Calculated features**\n* ps_calc_01, ps_calc_02, ps_calc_03 : very similar distribution\n* maximum value : all three 0.9"},{"metadata":{"trusted":true},"cell_type":"code","source":"# ( type : 'categorical' ( binary/categorical/real/integer ) \nvar = metadata[(metadata.type == 'real') & (metadata.preserve)].index\ni = 0\nt1 = trainset.loc[trainset['target'] != 0]\nt0 = trainset.loc[trainset['target'] == 0]\n\nsns.set_style('whitegrid')\nplt.figure()\nfig, ax = plt.subplots(3, 4, figsize=(16,12))\n\nfor feature in var:\n    i += 1\n    plt.subplot(3,4,i)\n    sns.kdeplot(t1[feature], bw=0.5, label=\"target = 1\")\n    sns.kdeplot(t0[feature], bw=0.5, label=\"target = 0\")\n    plt.ylabel('Density plot', fontsize=12) # 히스트그램을 매끄럽게 표현함.\n    plt.xlabel(feature, fontsize=12)\n    locs, labels = plt.xticks()\n    plt.tick_params(axis='both', which='major', labelsize=12)\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* => ps_reg_02, ps_car_13, ps_car_15 는 the most different distributions between set of values associated with target=0 and targe=1\n* => target=0 과 target=1 과 관련 없다고? 차트 모양이 어떤데????"},{"metadata":{},"cell_type":"markdown","source":"**real features => all correlation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def corr_heatmap(var):\n    correlations = trainset[var].corr()\n    \n    # Create color map ranging between two colors\n    cmap = sns.diverging_palette(50, 10, as_cmap=True)\n    \n    fig, ax =  plt.subplots(figsize=(10,10))\n    sns.heatmap(correlations, cmap=cmap, vmax=1.0, center=0, fmt='.2f',\n               square=True, linewidths=.5, annot=True, cbar_kws={\"shrink\": .75})\n    plt.show();\n    \nvar = metadata[(metadata.type == 'real') & (metadata.preserve)].index\ncorr_heatmap(var)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* strong correlations\n* ps_reg_01 with ps_reg_02(0.47);\n* ps_reg_01 with ps_reg_03(0.64)\n* ps_reg_02 with ps_reg_03(0.52)\n* ps_car_12 with ps_car_13(0.67)\n* ps_car_13 with ps_car_15(0.53)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# heatmap 결과, strong correlations field 와 target 리뷰\nsample = trainset.sample(frac=0.05)\nvar = ['ps_reg_01', 'ps_reg_02', 'ps_reg_03', 'ps_car_12', 'ps_car_13', 'ps_car_15', 'target']\nsample = sample[var]\nsns.pairplot(sample, hue='target', palette='Set1', diag_kind='kde')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Binary features**"},{"metadata":{"trusted":true},"cell_type":"code","source":"v = metadata[(metadata.type == 'binary') & (metadata.preserve)].index\ntrainset[v].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* distribution of binary data \n* blue : target=0 (%)\n* red : target=1 (%)"},{"metadata":{"trusted":true},"cell_type":"code","source":"bin_col = [col for col in trainset.columns if '_bin' in col]\nzero_list = []\none_list = []\nfor col in bin_col:\n    zero_list.append((trainset[col]==0).sum()/trainset.shape[0]*100)\n    one_list.append((trainset[col]==1).sum()/trainset.shape[0]*100)\nplt.figure()\nfig, ax = plt.subplots(figsize=(6,6))\n\n#Bar plot\np1 = sns.barplot(ax=ax, x=bin_col, y=zero_list, color=\"blue\")\np2 = sns.barplot(ax=ax, x=bin_col, y=one_list, bottom=zero_list, color=\"red\")\nplt.ylabel('Percent of zero/one [%]', fontsize=12)\nplt.xlabel('Binary features', fontsize=12)\nlocs, labels = plt.xticks()\nplt.setp(labels, rotation=90)\nplt.tick_params(axis='both', which='major', labelsize=12)\nplt.legend((p1, p2), ('Zero', 'One'))\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* => ps_ind_10_bin, ps_ind_11_bin, ps_ind_12_bin, ps_ind_13_bin : very small values(1)\n* => ps_ind_16_bin, ps_calc_16_bin : very large values(1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# * Distribution of binary data and target values\n\nvar = metadata[(metadata.type == 'binary') & (metadata.preserve)].index\nvar = [col for col in trainset.columns if '_bin' in col]\ni = 0\nt1 = trainset.loc[trainset['target'] != 0]\nt0 = trainset.loc[trainset['target'] == 0]\n\nsns.set_style('whitegrid')\nplt.figure()\nfig, ax = plt.subplots(6, 3, figsize=(12,24))\n\nfor feature in var:\n    i += 1\n    plt.subplot(6, 3, i)\n    sns.kdeplot(t1[feature], bw=0.5, label=\"target = 1\")\n    sns.kdeplot(t0[feature], bw=0.5, label=\"target = 0\")\n    plt.ylabel('Density plot', fontsize=12)\n    plt.xlabel(feature, fontsize=12)\n    locs, labels = plt.xticks()\n    plt.tick_params(axis='both', which='major', labelsize=12)\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* => ps_ind_06_bin, ps_ind_07_bin, ps_ind_16_bin, ps_ind_17_bin : high inbalance ( 1 vs 0 of target value)\n* => ps_ind_08_bin : small inbalance "},{"metadata":{},"cell_type":"markdown","source":"**Categorical features**\n* type : 'categorical' ( binary/categorical/real/integer )\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# target=1 per category value, using bar plots\n\nvar = metadata[(metadata.type == 'categorical') & (metadata.preserve)].index\n\nfor feature in var:\n    fig, ax = plt.subplots(figsize=(6,6))\n    \n    # Calculate the percentage of target=1 \n    cat_perc = trainset[[feature, 'target']].groupby([feature], as_index=False).mean()\n    cat_perc.sort_values(by='target', ascending=False, inplace=True)\n    \n    # Bar plot\n    # Order the bars descending on target mean\n    sns.barplot(ax=ax, x=feature, y='target', data=cat_perc, order=cat_perc[feature])\n    plt.ylabel('Percent of target with value 1[%]', fontsize=12)\n    plt.xlabel(feature, fontsize=12)\n    plt.tick_params(axis='both', which='major', labelsize=12)\n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* categorical features using density plot ( above same field )"},{"metadata":{"trusted":true},"cell_type":"code","source":"var = metadata[(metadata.type == 'categorical') & (metadata.preserve)].index\ni = 0\nt1 = trainset.loc[trainset['target'] != 0]\nt0 = trainset.loc[trainset['target'] == 0]\n\nsns.set_style('whitegrid')\nplt.figure()\nfig, ax = plt.subplots(4, 4, figsize=(16,16))\n\nfor feature in var:\n    i += 1\n    plt.subplot(4, 4, i)\n    sns.kdeplot(t1[feature], bw=0.5, label=\"target = 1\")\n    sns.kdeplot(t0[feature], bw=0.5, label=\"target = 0\")\n    plt.ylabel('Density plot', fontsize=12)\n    plt.xlabel(feature, fontsize=12)\n    locs, labels = plt.xticks()\n    plt.tick_params(axis='both', which='major', labelsize=12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* => ps_car_03_cat, ps_car_05_cat : the most different density plot"},{"metadata":{},"cell_type":"markdown","source":"> **Data unbalance between train and test data**"},{"metadata":{},"cell_type":"markdown","source":"* distribution of the features in the train and test datasets\n* Case of reg or registration features"},{"metadata":{"trusted":true},"cell_type":"code","source":"var = metadata[(metadata.category == 'registration') & (metadata.preserve)].index\n\n# Bar Plot\nsns.set_style('whitegrid')\n\nplt.figure()\nfig, ax = plt.subplots(1, 3, figsize=(12, 4))\ni = 0\nfor feature in var:\n    i = i + 1\n    plt.subplot(1, 3, i)\n    sns.kdeplot(trainset[feature], bw=0.5, label=\"train\")\n    sns.kdeplot(trainset[feature], bw=0.5, label=\"test\")\n    plt.ylabel('Distribution', fontsize=12)\n    plt.xlabel(feature, fontsize=12)\n    locs, labels = plt.xticks()\n    #plt.setp(labels, rotation=90)\n    plt.tick_params(axis='both', which='major', labelsize=12)\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* => All \"reg\" features : well balanced train and test sets."},{"metadata":{},"cell_type":"markdown","source":"* What about \"car\" features?"},{"metadata":{"trusted":true},"cell_type":"code","source":"var = metadata[(metadata.category == 'car') & (metadata.preserve)].index\n\n# Bar plot\nsns.set_style('whitegrid')\n\nplt.figure()\nfig, ax = plt.subplots(4, 4, figsize=(20, 16))\ni = 0\nfor feature in var:\n    i = i + 1\n    plt.subplot(4, 4, i)\n    sns.kdeplot(trainset[feature], bw=0.5, label=\"train\")\n    sns.kdeplot(testset[feature], bw=0.5, label=\"test\")\n    plt.ylabel('Distribution', fontsize=12)\n    plt.xlabel(feature, fontsize=12)\n    locs, labels = plt.xticks()\n    #plt.setp(labels, rotation=90)\n    plt.tick_params(axis='both', which='major', labelsize=12)\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* => car features : all variables well balanced ( train vs test ) set"},{"metadata":{},"cell_type":"markdown","source":"* ind(individual) values"},{"metadata":{"trusted":true},"cell_type":"code","source":"var = metadata[(metadata.category == 'individual') & (metadata.preserve)].index\n\n# Bar plot\nsns.set_style('whitegrid')\n\nplt.figure()\nfig, ax = plt.subplots(5,4,figsize=(20,16))\ni = 0\nfor feature in var:\n    i = i + 1\n    plt.subplot(5,4,i)\n    sns.kdeplot(trainset[feature], bw=0.5, label=\"train\")\n    sns.kdeplot(testset[feature], bw=0.5, label=\"test\")\n    plt.ylabel('Distribution', fontsize=12)\n    plt.xlabel(feature, fontsize=12)\n    locs, labels = plt.xticks()\n    #plt.setp(labels, rotation=90)\n    plt.tick_params(axis='both', which='major', labelsize=12)\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* => \"ind\" features : well balanced"},{"metadata":{},"cell_type":"markdown","source":"* \"calc\" features"},{"metadata":{"trusted":true},"cell_type":"code","source":"var = metadata[(metadata.category == 'calculated') & (metadata.preserve)].index\n\n# Bar plot\nsns.set_style('whitegrid')\n\nplt.figure()\nfig, ax = plt.subplots(5,4,figsize=(20,16))\ni = 0\nfor feature in var:\n    i = i + 1\n    plt.subplot(5,4,i)\n    sns.kdeplot(trainset[feature], bw=0.5, label=\"train\")\n    sns.kdeplot(testset[feature], bw=0.5, label=\"test\")\n    plt.ylabel('Distribution', fontsize=12)\n    plt.xlabel(feature, fontsize=12)\n    locs, labels = plt.xticks()\n    #plt.setp(labels, rotation=90)\n    plt.tick_params(axis='both', which='major', labelsize=12)\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"* => calc features : well balanced","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Check Data Quality**\n* check missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"vars_with_missing = []\n\nfor feature in trainset.columns:\n    missings = trainset[trainset[feature] == -1][feature].count()\n    if missings > 0:\n        vars_with_missing.append(feature)\n        missing_perc = missings / trainset.shape[0]\n        \n        print('Variable {} has {} records ({:.2%}) with missing values'.format(feature, missings, missing_perc))\n        \nprint('In total, there are {} variables with missing values'.format(len(vars_with_missing)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Prepare the data for model**"},{"metadata":{},"cell_type":"markdown","source":"* Drop \"calc\" columns\n* [5] Dmitry Altukhov, Kaggle Porto Seguro's Safe Driver Prediction (3rd place solution), https://www.youtube.com/watch?v=mbxZ_zqHV9c => drop the \"calc\" columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"col_to_drop = trainset.columns[trainset.columns.str.startswith('ps_calc_')]\ntrainset = trainset.drop(col_to_drop, axis=1)\ntestset = testset.drop(col_to_drop, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Drop variables with too many missig values\n* ps_car_03_cat, ps_car_05_cat"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping the variables with too many missing values\nvars_to_drop = ['ps_car_03_cat', 'ps_car_05_cat']\ntrainset.drop(vars_to_drop, inplace=True, axis=1)\ntestset.drop(vars_to_drop, inplace=True, axis=1)\nmetadata.loc[(vars_to_drop), 'keep'] = False #Updating the meta","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Define**\n* **Fit**\n* **Predict**\n* **Evaluate**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Script by https://www.kaggle.com/ogrellier\n# Code: https://www.kaggle.com/ogrellier/python-target-encoding-for-categorical-features\ndef add_noise(series, noise_level):\n    return series * (1 + noise_level * np.random.randn(len(series)))\n\ndef target_encode(trn_series=None, \n                  tst_series=None, \n                  target=None, \n                  min_samples_leaf=1, \n                  smoothing=1,\n                  noise_level=0):\n    \"\"\"\n    Smoothing is computed like in the following paper by Daniele Micci-Barreca\n    https://kaggle2.blob.core.windows.net/forum-message-attachments/225952/7441/high%20cardinality%20categoricals.pdf\n    trn_series : training categorical feature as a pd.Series\n    tst_series : test categorical feature as a pd.Series\n    target : target data as a pd.Series\n    min_samples_leaf (int) : minimum samples to take category average into account\n    smoothing (int) : smoothing effect to balance categorical average vs prior  \n    \"\"\" \n    assert len(trn_series) == len(target)\n    assert trn_series.name == tst_series.name\n    temp = pd.concat([trn_series, target], axis=1)\n    # Compute target mean \n    averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n    # Compute smoothing\n    smoothing = 1 / (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) / smoothing))\n    # Apply average function to all target data\n    prior = target.mean()\n    # The bigger the count the less full_avg is taken into account\n    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n    # Apply averages to trn and tst series\n    ft_trn_series = pd.merge(\n        trn_series.to_frame(trn_series.name),\n        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n        on=trn_series.name,\n        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n    # pd.merge does not keep the index so restore it\n    ft_trn_series.index = trn_series.index \n    ft_tst_series = pd.merge(\n        tst_series.to_frame(tst_series.name),\n        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n        on=tst_series.name,\n        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n    # pd.merge does not keep the index so restore it\n    ft_tst_series.index = tst_series.index\n    return add_noise(ft_trn_series, noise_level), add_noise(ft_tst_series, noise_level)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Replace ps_car_11_cat with encoded value\n* target_encoe() "},{"metadata":{"trusted":true},"cell_type":"code","source":"train_encoded, test_encoded = target_encode(trainset[\"ps_car_11_cat\"], \n                             testset[\"ps_car_11_cat\"], \n                             target=trainset.target, \n                             min_samples_leaf=100,\n                             smoothing=10,\n                             noise_level=0.01)\n    \ntrainset['ps_car_11_cat_te'] = train_encoded\ntrainset.drop('ps_car_11_cat', axis=1, inplace=True)\nmetadata.loc['ps_car_11_cat','keep'] = False  # Updating the metadata\ntestset['ps_car_11_cat_te'] = test_encoded\ntestset.drop('ps_car_11_cat', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Balance target variable\n* => undersampling ( cause of lage training set )"},{"metadata":{"trusted":true},"cell_type":"code","source":"desired_apriori=0.10\n\n# Get the indices per target value\nidx_0 = trainset[trainset.target == 0].index\nidx_1 = trainset[trainset.target == 1].index\n\n# Get original number of records per target value\nnb_0 = len(trainset.loc[idx_0])\nnb_1 = len(trainset.loc[idx_1])\n\n# Calculate the undersampling rate and resulting number of records with target=0\nundersampling_rate = ((1-desired_apriori)*nb_1)/(nb_0*desired_apriori)\nundersampled_nb_0 = int(undersampling_rate*nb_0)\nprint('Rate to undersample records with target=0: {}'.format(undersampling_rate))\nprint('Number of records with target=0 after undersampling: {}'.format(undersampled_nb_0))\n\n# Randomly select records with target=0 to get at the desired a priori\nundersampled_idx = shuffle(idx_0, random_state=314, n_samples=undersampled_nb_0)\n\n# Construct list with remaining indices\nidx_list = list(undersampled_idx) + list(idx_1)\n\n# Return undersample data frame\ntrainset = trainset.loc[idx_list].reset_index(drop=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Replace -1 values => NaN"},{"metadata":{"trusted":true},"cell_type":"code","source":"trainset = trainset.replace(-1, np.nan)\ntestset = testset.replace(-1, np.nan)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Dummify \"cat\" values\n* => create dummy variables for the categorical(cat) features"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_features = [a for a in trainset.columns if a.endswith('cat')]\n\nfor column in cat_features:\n    temp = pd.get_dummies(pd.Series(trainset[column]))\n    trainset = pd.concat([trainset, temp], axis=1)\n    trainset = trainset.drop([column], axis=1)\n    \nfor column in cat_features:\n    temp = pd.get_dummies(pd.Series(testset[column]))\n    testset = pd.concat([testset,temp],axis=1)\n    testset = testset.drop([column],axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Drop unused and \"target\" columns\n* => \"id\", \"target\" drop..."},{"metadata":{"trusted":true},"cell_type":"code","source":"id_test = testset['id'].values\ntarget_train = trainset['target'].values\n\ntrainset = trainset.drop(['target','id'], axis = 1)\ntestset = testset.drop(['id'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Review training and test sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Train dataset (rows, cols): \", trainset.values.shape, \"\\nTest dataset (rows, cols): \", testset.values.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** Prepare the model**"},{"metadata":{},"cell_type":"markdown","source":"* Ensamble class \n* => for cross validation\n* => 4 parameters :\n* => self : the object to be initialized\n* => n_splits : the number of cross-validation splits to be used\n* => stacker : the model used for stacking the prediction results from the trained base models\n* => base_models : the list of base models used in training"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Ensemble(object):\n    def __init__(self, n_splits, stacker, base_models):\n        self.n_splits = n_splits\n        self.stacker = stacker\n        self.base_models = base_models\n\n    def fit_predict(self, X, y, T):\n        X = np.array(X)\n        y = np.array(y)\n        T = np.array(T)\n\n        folds = list(StratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state=314).split(X, y))\n\n        S_train = np.zeros((X.shape[0], len(self.base_models)))\n        S_test = np.zeros((T.shape[0], len(self.base_models)))\n        for i, clf in enumerate(self.base_models):\n\n            S_test_i = np.zeros((T.shape[0], self.n_splits))\n\n            for j, (train_idx, test_idx) in enumerate(folds):\n                X_train = X[train_idx]\n                y_train = y[train_idx]\n                X_holdout = X[test_idx]\n\n\n                print (\"Base model %d: fit %s model | fold %d\" % (i+1, str(clf).split('(')[0], j+1))\n                clf.fit(X_train, y_train)\n                cross_score = cross_val_score(clf, X_train, y_train, cv=3, scoring='roc_auc')\n                print(\"cross_score [roc-auc]: %.5f [gini]: %.5f\" % (cross_score.mean(), 2*cross_score.mean()-1))\n                y_pred = clf.predict_proba(X_holdout)[:,1]                \n\n                S_train[test_idx, i] = y_pred\n                S_test_i[:, j] = clf.predict_proba(T)[:,1]\n            S_test[:, i] = S_test_i.mean(axis=1)\n\n        results = cross_val_score(self.stacker, S_train, y, cv=3, scoring='roc_auc')\n        # Calculate gini factor as 2 * AUC - 1\n        print(\"Stacker score [gini]: %.5f\" % (2 * results.mean() - 1))\n\n        self.stacker.fit(S_train, y)\n        res = self.stacker.predict_proba(S_test)[:,1]\n        return res","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Parameters for the base models\n* => LightGBM / XGB model\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# LightGBM params\n# lgb_1\nlgb_params1 = {}\nlgb_params1['learning_rate'] = 0.02\nlgb_params1['n_estimators'] = 650\nlgb_params1['max_bin'] = 10\nlgb_params1['subsample'] = 0.8\nlgb_params1['subsample_freq'] = 10\nlgb_params1['colsample_bytree'] = 0.8   \nlgb_params1['min_child_samples'] = 500\nlgb_params1['seed'] = 314\nlgb_params1['num_threads'] = 4\n\n# lgb2\nlgb_params2 = {}\nlgb_params2['n_estimators'] = 1090\nlgb_params2['learning_rate'] = 0.02\nlgb_params2['colsample_bytree'] = 0.3   \nlgb_params2['subsample'] = 0.7\nlgb_params2['subsample_freq'] = 2\nlgb_params2['num_leaves'] = 16\nlgb_params2['seed'] = 314\nlgb_params2['num_threads'] = 4\n\n# lgb3\nlgb_params3 = {}\nlgb_params3['n_estimators'] = 1100\nlgb_params3['max_depth'] = 4\nlgb_params3['learning_rate'] = 0.02\nlgb_params3['seed'] = 314\nlgb_params3['num_threads'] = 4\n\n# XGBoost params\nxgb_params = {}\nxgb_params['objective'] = 'binary:logistic'\nxgb_params['learning_rate'] = 0.04\nxgb_params['n_estimators'] = 490\nxgb_params['max_depth'] = 4\nxgb_params['subsample'] = 0.9\nxgb_params['colsample_bytree'] = 0.9  \nxgb_params['min_child_weight'] = 10\nxgb_params['num_threads'] = 4","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Initialize the models with the paramets "},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_model1 = LGBMClassifier(**lgb_params1)\n\nlgb_model2 = LGBMClassifier(**lgb_params2)\n       \nlgb_model3 = LGBMClassifier(**lgb_params3)\n\nxgb_model = XGBClassifier(**xgb_params)\n\n# Stacking model\nlog_model = LogisticRegression()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Initialize the ensambling object\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"stack = Ensemble(n_splits=3,\n        stacker = log_model,\n        base_models = (lgb_model1, lgb_model2, lgb_model3, xgb_model))  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Run the predictive models**"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_prediction = stack.fit_predict(trainset, target_train, testset)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Prepare the submission**"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame()\nsubmission['id'] = id_test\nsubmission['target'] = y_prediction\nsubmission.to_csv('stacked.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}