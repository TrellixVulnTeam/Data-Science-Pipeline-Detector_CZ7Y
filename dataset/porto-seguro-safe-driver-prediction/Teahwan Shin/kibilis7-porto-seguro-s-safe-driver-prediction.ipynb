{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.utils import shuffle\nfrom sklearn.ensemble import RandomForestClassifier\n\npd.set_option('display.max_columns', 100)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"339a4e977aabd2e1304ffdc4a0bc43a6492c41b8"},"cell_type":"markdown","source":"****bin은 바이너리 피쳐, cat은 카테고리형 피쳐를 나타낸다.\n\n****이 언급이 없으면 연속형 또는 순서형이다. (continuous, ordinal)\n\n****-1 값은 누락된 값을 의미한다\n\ntarget 열은 claim이 접속되었는지 아닌지 여부.****"},{"metadata":{"trusted":true,"_uuid":"59f8ec5484a82f16b6a22f49ba5bb41cb83bf2d4"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1dc7376fa049cb4a98d589a68862faea07950078"},"cell_type":"code","source":"train.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fd645ff1fa664fd712e7c6b637dcc4ebdd6b5a0d"},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"d5013665052b8818798c73e3a5324e9d3e3a4475"},"cell_type":"code","source":"train.drop_duplicates()\ntrain.shape\n\n#shape가 동일하므로, 중복 없다.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"46a12417b798e9c7a0031ac36584d11001be6495","scrolled":false},"cell_type":"code","source":"test.shape\n# column 한개는 목표인 target이 빠진거에요.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"47831ceaee84becbe80c4976bec603c4a5c197bf"},"cell_type":"code","source":"train.info()\n# 데이터는 int와 float형만 있습니다.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"238dad44d195c0a9ec9e724af07d7b31e691608d"},"cell_type":"code","source":"# 메타 데이터 만들기. (메타 데이터는 데이터를 위한 데이터)\n# 이미 주어진 데이터는 더미화 되어있는 것들이 많다. 이는 사용자가 보기엔 불편할 수 있으므로\n# 사용자가 보기 편리하게 메타 데이터를 만들어보자.\n\ndata = [] # 빈 시리즈를 만든다.\nfor f in train.columns: # 각 column별로 f.\n    # role, level, keep, dtype\n    # Defining the role\n    if f == 'target':\n        role = 'target'\n    elif f == 'id':\n        role = 'id'\n    else:\n        role = 'input'\n         \n    # Defining the level\n    if 'bin' in f or f == 'target':\n        level = 'binary'\n    elif 'cat' in f or f == 'id':\n        level = 'nominal'\n    elif train[f].dtype == float:\n        level = 'interval'\n    elif train[f].dtype == int:\n        level = 'ordinal'\n        \n    # Initialize keep to True for all variables except for id\n    keep = True\n    if f == 'id':\n        keep = False\n    \n    # Defining the data type \n    dtype = train[f].dtype\n    \n    # Creating a Dict that contains all the metadata for the variable\n    f_dict = {\n        'varname': f,\n        'role': role,\n        'level': level,\n        'keep': keep,\n        'dtype': dtype\n    } # 중괄호로 묶인 것은 dict(dictionary type)형 이다. 키와 밸류 한쌍을 가지는 형태.\n    data.append(f_dict) # 만든 딕셔너리를 리스트에 추가해줍니다.\n\nmeta = pd.DataFrame(data, columns=['varname', 'role', 'level', 'keep', 'dtype'])\nmeta.set_index('varname', inplace=True)\n# 반복문을 통해 만들어진 Data를 이용하여 데이터 프레임을 만들고, 이름을 meta라고 하겠습니다.\n# meta는 varname을 인덱스로 취합니다. inplace=True면 적용.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4f4492681bc83a016a60c0dc479a0d02fedeac2b"},"cell_type":"code","source":"type(f_dict) # 혼자 확인해 본 것. f_dict는 딕셔너리 형. { }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7cacd8737eb31652878b61f752dae20cb20b8849"},"cell_type":"code","source":"meta","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f5de4c8fc239985ebe7a67746cf7e24a68a93cb2"},"cell_type":"code","source":"# 메타 데이터 활용 예시. 메타의 level column이 'nomial'이고,\n# meta의 keep이 True인 것들만 인덱싱 해 봅시다.\nmeta[(meta.level == 'nominal') & (meta.keep)].index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8ff0ea9e3cddfbbcbb48f11f1a2ff5814eadc593"},"cell_type":"code","source":"pd.DataFrame({'count' : meta.groupby(['role', 'level'])['role']})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cae230dd44df5059f4bdb360a3e8b76c821de157","scrolled":true},"cell_type":"code","source":"# role과 level로 메타를 그룹화하고 count만 column인 상태인데\n# .reset_index()로 바꿔서 display함. (좀 어렵다)\npd.DataFrame({'count' : meta.groupby(['role', 'level'])['role'].size()}).reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d5920890a8773341461bc9bd5f6de18ef4253c5e","scrolled":true},"cell_type":"code","source":"v = meta[(meta.level == 'interval') & (meta.keep)].index\ntrain[v].describe() # .describe()를 이용해 해당 v의 특징 출력\n\n# min을 확인했을 때, -1이면 missing values를 가진 것\n# 각각의 min~max range다르다. scaling이 필요하다.\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"54a95f72f5555226c382809cbdd698d2e66539ab"},"cell_type":"code","source":"v = meta[(meta.level == 'binary') & (meta.keep)].index\ntrain[v].describe()\n# 대부분 0 값이 많다.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3b097341db948fa2cb3038d9299931c5893e88bb"},"cell_type":"code","source":"# target은 0이 1보다 훨씬 많고,\n# 높은 정확도와 높은 평가.\n# 오버 샘플링, 언더 샘플링. 여러 전략 있다.\n\ndesired_apriori=0.10\n\n# Get the indices per target value\nidx_0 = train[train.target == 0].index\nidx_1 = train[train.target == 1].index\n\n# Get original number of records per target value\nnb_0 = len(train.loc[idx_0])\nnb_1 = len(train.loc[idx_1])\n\n# Calculate the undersampling rate and resulting number of records with target=0\nundersampling_rate = ((1-desired_apriori)*nb_1)/(nb_0*desired_apriori)\nundersampled_nb_0 = int(undersampling_rate*nb_0)\nprint('Rate to undersample records with target=0: {}'.format(undersampling_rate))\nprint('Number of records with target=0 after undersampling: {}'.format(undersampled_nb_0))\n\n# Randomly select records with target=0 to get at the desired a priori\nundersampled_idx = shuffle(idx_0, random_state=37, n_samples=undersampled_nb_0)\n\n# Construct list with remaining indices\nidx_list = list(undersampled_idx) + list(idx_1)\n\n# Return undersample data frame\ntrain = train.loc[idx_list].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d3e5300de795faec67d57e4ed4acc5fd56fd7025"},"cell_type":"code","source":"vars_with_missing = []\n\nfor f in train.columns:\n    missings = train[train[f] == -1][f].count()\n    if missings > 0:\n        vars_with_missing.append(f)\n        missings_perc = missings/train.shape[0]\n        \n        print('Variable {} has {} records ({:.2%}) with missing values'.format(f, missings, missings_perc))\n        \nprint('In total, there are {} variables with missing values'.format(len(vars_with_missing)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a50c08a543fe01f7f72e9066ad07ef59ab8086d3"},"cell_type":"code","source":"# Dropping the variables with too many missing values\nvars_to_drop = ['ps_car_03_cat', 'ps_car_05_cat']\ntrain.drop(vars_to_drop, inplace=True, axis=1)\nmeta.loc[(vars_to_drop),'keep'] = False  # Updating the meta\n\n# Imputing with the mean or mode\nmean_imp = Imputer(missing_values=-1, strategy='mean', axis=0)\nmode_imp = Imputer(missing_values=-1, strategy='most_frequent', axis=0)\ntrain['ps_reg_03'] = mean_imp.fit_transform(train[['ps_reg_03']]).ravel()\ntrain['ps_car_12'] = mean_imp.fit_transform(train[['ps_car_12']]).ravel()\ntrain['ps_car_14'] = mean_imp.fit_transform(train[['ps_car_14']]).ravel()\ntrain['ps_car_11'] = mode_imp.fit_transform(train[['ps_car_11']]).ravel()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"08a338ce6558cb20e9223563769cdfca19ae686e"},"cell_type":"code","source":"v = meta[(meta.level == 'nominal') & (meta.keep)].index\n\nfor f in v:\n    dist_values = train[f].value_counts().shape[0]\n    print('Variable {} has {} distinct values'.format(f, dist_values))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5d8a7cb2cb725158e294564790e4712e89ff19e4"},"cell_type":"code","source":"# Script by https://www.kaggle.com/ogrellier\n# Code: https://www.kaggle.com/ogrellier/python-target-encoding-for-categorical-features\ndef add_noise(series, noise_level):\n    return series * (1 + noise_level * np.random.randn(len(series)))\n\ndef target_encode(trn_series=None, \n                  tst_series=None, \n                  target=None, \n                  min_samples_leaf=1, \n                  smoothing=1,\n                  noise_level=0):\n    assert len(trn_series) == len(target)\n    assert trn_series.name == tst_series.name\n    temp = pd.concat([trn_series, target], axis=1)\n    # Compute target mean \n    averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n    # Compute smoothing\n    smoothing = 1 / (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) / smoothing))\n    # Apply average function to all target data\n    prior = target.mean()\n    # The bigger the count the less full_avg is taken into account\n    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n    # Apply averages to trn and tst series\n    ft_trn_series = pd.merge(\n        trn_series.to_frame(trn_series.name),\n        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n        on=trn_series.name,\n        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n    # pd.merge does not keep the index so restore it\n    ft_trn_series.index = trn_series.index \n    ft_tst_series = pd.merge(\n        tst_series.to_frame(tst_series.name),\n        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n        on=tst_series.name,\n        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n    # pd.merge does not keep the index so restore it\n    ft_tst_series.index = tst_series.index\n    return add_noise(ft_trn_series, noise_level), add_noise(ft_tst_series, noise_level)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"507424b6f12d4f364648f4c9bb28fef1f52f2ff8"},"cell_type":"code","source":"train_encoded, test_encoded = target_encode(train[\"ps_car_11_cat\"], \n                             test[\"ps_car_11_cat\"], \n                             target=train.target, \n                             min_samples_leaf=100,\n                             smoothing=10,\n                             noise_level=0.01)\n    \ntrain['ps_car_11_cat_te'] = train_encoded\ntrain.drop('ps_car_11_cat', axis=1, inplace=True)\nmeta.loc['ps_car_11_cat','keep'] = False  # Updating the meta\ntest['ps_car_11_cat_te'] = test_encoded\ntest.drop('ps_car_11_cat', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"aeb19a41e2719942bebf849ac0ef827fcb7f3297"},"cell_type":"code","source":"v = meta[(meta.level == 'nominal') & (meta.keep)].index\n\nfor f in v:\n    plt.figure()\n    fig, ax = plt.subplots(figsize=(20,10))\n    # Calculate the percentage of target=1 per category value\n    cat_perc = train[[f, 'target']].groupby([f],as_index=False).mean()\n    cat_perc.sort_values(by='target', ascending=False, inplace=True)\n    # Bar plot\n    # Order the bars descending on target mean\n    sns.barplot(ax=ax, x=f, y='target', data=cat_perc, order=cat_perc[f])\n    plt.ylabel('% target', fontsize=18)\n    plt.xlabel(f, fontsize=18)\n    plt.tick_params(axis='both', which='major', labelsize=18)\n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"26283444a17070daa5d5b5ca61d05190db1a6d4b"},"cell_type":"code","source":"def corr_heatmap(v):\n    correlations = train[v].corr() # 상관관계 테이블\n\n    # Create color map ranging between two colors\n    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n    fig, ax = plt.subplots(figsize=(10,10))\n    sns.heatmap(correlations, cmap=cmap, vmax=1.0, center=0, fmt='.2f',\n                square=True, linecolor='skyblue',linewidths=.5, annot=True, cbar_kws={\"shrink\": .75})\n    plt.show();\n    \nv = meta[(meta.level == 'interval') & (meta.keep)].index\ncorr_heatmap(v)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"f86ae3eb7bd78d611240006ab2892ccd20169386"},"cell_type":"code","source":"s = train.sample(frac=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"92ee94ffa4ec288579925b3dc35e9762b5b04116"},"cell_type":"code","source":"sns.lmplot(x='ps_reg_02', y='ps_reg_03', data=s, hue='target', palette='Set1', scatter_kws={'alpha':0.3})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"28e998acd25b6e721467215eba13ff2701574b7d"},"cell_type":"code","source":"sns.lmplot(x='ps_car_12', y='ps_car_13', data=s, hue='target', palette='Set1', scatter_kws={'alpha':0.3})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"60b499fab622a52b51e494d664765c438dab0116"},"cell_type":"code","source":"sns.lmplot(x='ps_car_12', y='ps_car_13', data=s, hue='target', palette='Set1', scatter_kws={'alpha':0.3})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"192d6b423bb4169159eafd64f29c6a4fda166cce"},"cell_type":"code","source":"sns.lmplot(x='ps_car_12', y='ps_car_14', data=s, hue='target', palette='Set1', scatter_kws={'alpha':0.3})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"27659ff55ac2842d4d013f51f26fba1dc158544c"},"cell_type":"code","source":"sns.lmplot(x='ps_car_15', y='ps_car_13', data=s, hue='target', palette='Set1', scatter_kws={'alpha':0.3})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"10e05c4c50ba46514e413a7a5e32b1e38870da70","scrolled":true},"cell_type":"code","source":"# Feature Engineering\n# Creating dummy variables\n# 카테고리형(범주형)을 더미형으로 변환하여 능률을 높이자.\n# 카테고리 형태의 값은 순서, 크기가 아니다. 중요한 것은\n# 어디에 속해있느냐이다. (0or1) 따라서 더미화 한다.\n\n# level이 nomial이면서 meta.keep이 True인 것은 카테고리형 뿐.\n# 이들을 더미화합니다.\n# drop_first=True를 이용하여 더미에 사용한 항목을 지운다.\nv = meta[(meta.level == 'nominal') & (meta.keep)].index\nprint('Before dummification we have {} variables in train'.format(train.shape[1]))\ntrain = pd.get_dummies(train, columns=v, drop_first=True)\nprint('After dummification we have {} variables in train'.format(train.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"6837d9f09e5a962c247bcdd973f7f7e1a1a7aed4"},"cell_type":"code","source":"# extra interaction variables\n# 정수형(interval)은 PolynomialFeatures를 이용하여 능률 상승\n# 그들의 조합으로 이루어진 것을 column으로 사용하자.\n\n# PolynomialFeatures? (다항식 피쳐)\n# PolynomialFeatures(degree=2, interaction_only=False, include_bias=True)\n# 지정한 degree 이하의 모든 피쳐의 다항식 조합으로 구성된 새 피쳐 매트릭스 생성\n# For example, if an input sample is two dimensional and of the form [a, b],\n# the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].\n# interaction_only가 True면 같은 항을 2회 이상 곱하지 않는다. (2^2등 안쓴다.)\n# include_bias가 False면 0승은 사용 안한다.\n# 유의점 : degree와 입력 수에 따라 overfitting(과적) 유발 가능.\n\n# 그렇다면 degree가 2이고, 0승을 제외한 조합 결과들이 interactions가 된다.\n# 이후 v를 드랍, 즉 원래 데이터를 지운다.\nv = meta[(meta.level == 'interval') & (meta.keep)].index\npoly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\ninteractions = pd.DataFrame(data=poly.fit_transform(train[v]), columns=poly.get_feature_names(v))\ninteractions.drop(v, axis=1, inplace=True)  # Remove the original columns\n# Concat the interaction variables to the train data\nprint('Before creating interactions we have {} variables in train'.format(train.shape[1]))\ntrain = pd.concat([train, interactions], axis=1)\nprint('After creating interactions we have {} variables in train'.format(train.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b4029530d7434b3c55a4c82535b788455369347a"},"cell_type":"code","source":"poly","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b14840c7a8a370ec23d07e0b5ddd351a47f96e28"},"cell_type":"code","source":"# X는 numpy를 이용하여 0~5까지 수를 생성하고, 2개씩 쌍을 지음\nX = np.arange(6).reshape(3, 2)\nX","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9d8a060a3a1e44c31214b4638e90ba7be795f1b2"},"cell_type":"code","source":"# degree를 2 주었다. 각 쌍에서 그 들로 만들 수 있는 값들 나타남\npoly = PolynomialFeatures(2)\npoly.fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6f8053fd707b434a9731e40db5e0cfc451da36cb"},"cell_type":"code","source":"poly = PolynomialFeatures(degree = 3,interaction_only=True)\npoly.fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7dfa1c05bb0a3eaac7ea78429217641968be7cfc"},"cell_type":"code","source":"poly = PolynomialFeatures(include_bias=False)\npoly.fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"7d8072fb014a4a69556d473b85f21e1976c2b9f3"},"cell_type":"code","source":"# Feature Selection (머신에게 모두 할당할 수 있으나, 할 수 있는 부분들은 직접 해줌으로써 처리속도 향상)\n# 분산이 없거나 매우 낮은 feature를 제거하자.\n# Variant Thresould(from sklearn)을 이용.\n# 0짜리는 전단계에서 지웠다. 1% 미만 지우면 31개 지워진다.\nselector = VarianceThreshold(threshold=.01)\nselector.fit(train.drop(['id', 'target'], axis=1)) # Fit to train without id and target variables\nf = np.vectorize(lambda x : not x) # Function to toggle boolean array elements\n# f는 토글(0과1을 전환)해주는 함수.\nv = train.drop(['id', 'target'], axis=1).columns[f(selector.get_support())]\n# .get_support는 임계점을 통과한 값인지 아닌지 Boolean으로 가르쳐준다. 이 값을 뒤집으면 곧\n# 임계점을 넘지 못한 값들이고, v에는 그 값들이 들어갑니다.\nprint('{} variables have too low variance.'.format(len(v)))\nprint('These variables are {}'.format(list(v)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"326ba013b99637fe6c88bac4466f89c765b886e8"},"cell_type":"code","source":"selector.fit(train.drop(['id', 'target'], axis=1))\nselector.get_support()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"92173748780d8326abeb3cd12e677227be728995","scrolled":false},"cell_type":"code","source":"# Selecting features with a Random Forest and SelectFromModel\n# 여기는 처리시간을 줄이는게 목적. 우리 손으로 줄여줄 수 있는 것 빼줌으로써.\n# Sklearn's SelectFromModel을 사용하여 보관할 변수 수를 지정할 수 있다. \n# threshold on the level of feature importance를 임의 지정 가능.\n\nX_train = train.drop(['id', 'target'], axis=1)\ny_train = train['target']\n\nfeat_labels = X_train.columns\n\nrf = RandomForestClassifier(n_estimators=10, random_state=0, n_jobs=-1)\n# (트리갯수=1000개(오래걸려), 랜덤시드=0, 병렬작업수=1(fit, predict 둘다 가능))\nrf.fit(X_train, y_train)\nimportances = rf.feature_importances_\n# Feature importance?\n# 학습된 모델은 Feature importance를 가진다. 결과물이라고 봐도 된다.\nindices = np.argsort(rf.feature_importances_)[::-1]\n# np.argsort로 정렬한다. 오름차순으로 나타나는데, [::-1]을 통해 슬라이싱은 안하고, 전체 표현\nfor f in range(X_train.shape[1]):\n    print(\"%2d) %-*s %f\" % (f + 1, 30,feat_labels[indices[f]], importances[indices[f]]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bf832bcbec7da33bbe8d31a106e46521e33f4c5e"},"cell_type":"code","source":"tt = np.array([42, 38, 12, 25])\nabb = np.argsort(tt)[::-1]\nabb\n# np.argsort로 정렬한다.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b0bdd8d22d536492fbb498747fd94cf2240c2f0d"},"cell_type":"code","source":"# SelectFrom Model을 이용하여 사용할 적절한 분류기,\n# feature importance에 대한 임계값 지정가능\n# get_support를 이용하여 train data 변수 수 제한 가능\n# Random Forest 결과의 일부만 사용합시다. (feature_importance 기준)\nsfm = SelectFromModel(rf, threshold='median', prefit = True)\nprint('Number of features before selection: {}'.format(X_train.shape[1]))\nn_features = sfm.transform(X_train).shape[1]\nprint('Number of features after seleciton: {}'.format(n_features))\nselected_vars = list(feat_labels[sfm.get_support()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"db5f2e969da926dca32384819c3697cb95b88747"},"cell_type":"code","source":"train = train[selected_vars + ['target']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"18a881518df57abf9769610c90dd216838d15dfa"},"cell_type":"code","source":"# Feature scaling\n# StandardScaler를 이용, trainset에 적용시킨다.\n# 여기서는 classifiers들마다 성능 다르다.\nscaler = StandardScaler()\nscaler.fit_transform(train.drop(['target'], axis=1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3a04f34190bc6b4ef5b1417d31b236fa394eaa8b"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}