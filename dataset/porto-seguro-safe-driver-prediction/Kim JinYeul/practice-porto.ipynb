{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"이 글은 https://www.kaggle.com/arthurtok/interactive-porto-insights-a-plot-ly-tutorial,\n       https://www.kaggle.com/bertcarremans/data-preparation-exploration 에서 데이터 기초 처리를 \n       https://www.kaggle.com/sudosudoohio/stratified-kfold-xgboost-eda-tutorial-0-281 에서 XGB를 보고 했습니다."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.utils import shuffle\nfrom sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('../input/porto-seguro-safe-driver-prediction/train.csv')\ndf_test = pd.read_csv('../input/porto-seguro-safe-driver-prediction/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# porto-seguro-safe-driver-prediction\n\n1. data 준비\n2. 변수 준비\n3. Training/Predicting\n "},{"metadata":{},"cell_type":"markdown","source":"# 1. data 준비"},{"metadata":{"trusted":true},"cell_type":"code","source":"target_count = df_train.target.value_counts()\nprint('Class 0:', target_count[0])\nprint('Class 1:', target_count[1])\nprint('Proportion:', round(target_count[0] / target_count[1], 2), ': 1')\n\ntarget_count.plot(kind='bar', title='Count (target)')\n\ndf_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"target의 비율이 26.44:1로 비율이 안좋다. 비율이 안 좋은 데이터로 구한 정확도는 허상과 같으므로,\n리샘플링을 통해서 비율을 맞춰준다.\n\n리샘플링은 UNDER이나 OVER중 쓸데없는 데이터가 추가되거나 중복되지 않도록 UNDER-resampling을 시행할 예정이다."},{"metadata":{},"cell_type":"markdown","source":"In the train and test data, features that belong to similar groupings are tagged as such in the feature names (e.g., ind, reg, car, calc). In addition, feature names include the postfix **(1)bin to indicate binary features** and **(2)cat to indicate categorical features**. Features without these designations are either continuous or ordinal. **(3)Values of -1 indicate that the feature was missing from the observation**. The target columns signifies whether or not a claim was filed for that policy holder."},{"metadata":{},"cell_type":"markdown","source":"위 글은 데이터 설명이다. \n\n* ind, reg, 등등은 그냥 변수이며, binary, catagory 형식의 변수가 있다.\n \n* -1값은 결측값이다."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.isnull().any().any()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Null 값을 -1로 표현되었기 때문에, -1값을 Null 처리를 해준다. (각주 (3))"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_copy = df_train.replace(-1, np.NaN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import missingno as msno\n# Nullity or missing values by columns\nmsno.matrix(df=df_train_copy.iloc[:,2:39], figsize=(20, 14), color=(0.42, 0.1, 0.05))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"결측값들이 보인다. 결측값들은 상황에 맞게 처리해준다."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style=\"white\")\n\n\n# Compute the correlation matrix\ncorr = df_train.corr()\n\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"각 변수의 corr이다. \n\nps_calc_%%들의 전체에 대한 corr이 0인 상태이다."},{"metadata":{},"cell_type":"markdown","source":"# 2. 변수 준비\n\n      1) 변수 정리\n      2) interval\n      3) ordinal\n      4) binary\n      5) catagory\n      6) undersampling과 null 값 처리\n      7) 상관 계수에 따른 변수통제"},{"metadata":{},"cell_type":"raw","source":" 1) 변수정리\n각주(1),(2)에 근거 해서 데이터를 분류해준다.\nrole: input, ID, target\nlevel: nominal, interval, ordinal, binary\nkeep: True or False\ndtype: int, float, str"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = []\nfor i in df_train.columns:\n    # Defining the role\n    if i == 'target':\n        role = 'target'\n    elif i == 'id':\n        role = 'id'\n    else:\n        role = 'input'\n         \n    # Defining the level\n    if 'bin' in i or i == 'target':\n        level = 'binary'\n    elif 'cat' in i or i == 'id':\n        level = 'nominal'\n    elif df_train[i].dtype == float:\n        level = 'interval'\n    elif df_train[i].dtype == int:\n        level = 'ordinal'\n        \n    # Initialize keep to True for all variables except for id\n    keep = True\n    if i == 'id':\n        keep = False\n    \n    # Defining the data type \n    dtype = df_train[i].dtype\n    \n    # Creating a Dict that contains all the metadata for the variable\n    i_dict = {\n        'varname': i,\n        'role': role,\n        'level': level,\n        'keep': keep,\n        'dtype': dtype\n    }\n    data.append(i_dict)\n    \nmeta = pd.DataFrame(data, columns=['varname', 'role', 'level', 'keep', 'dtype'])\nmeta.set_index('varname', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame({'count' : meta.groupby(['role', 'level'])['role'].size()}).reset_index()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**분류결과 target, id, input(4)[bin, cat(nom), int(ord), float(inter)]로 나뉘였다.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"v = meta[(meta.level == 'interval') & (meta.keep)].index\ndf_train[v].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2) interval\n\n    * reg : 03에서 결측값 존재\n    * car : 12, 15에서 결측값 존재\n    * calc : 결측값 없음, 서로간의 형태는 비슷할 것으로 예측"},{"metadata":{"trusted":true},"cell_type":"code","source":"v = meta[(meta.level == 'ordinal') & (meta.keep)].index\ndf_train[v].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3) ordinal\n\nps_car_11에만 결측값 존재"},{"metadata":{"trusted":true},"cell_type":"code","source":"v = meta[(meta.level == 'binary') & (meta.keep)].index\ndf_train[v].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4) binary\n\nmean <0.05(19:1) 인것만 봐도 여기에 0이 압도적으로 많음을 알 수 있다.\n= 여기가 target의 imbalance의 원인중 하나다. "},{"metadata":{"trusted":true},"cell_type":"code","source":"v = meta[(meta.level == 'nominal') & (meta.keep)].index\ndf_train[v].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"5) catagory\n\nps_car_04_cat,ps_car_06_cat, ps_car_08_cat,ps_car_10_cat을 제외하고는 Null 값이 존재한다."},{"metadata":{},"cell_type":"markdown","source":"6-1) undersampling\n  \n target의 0과 1의 비율이 약26:1을 2:1로 낮춰 밸런스 있게 만들었다."},{"metadata":{"trusted":true},"cell_type":"code","source":"desired_apriori=0.10\nfrom sklearn.utils import shuffle\n# Get the indices per target value\nidx_0 = df_train[df_train.target == 0].index\nidx_1 = df_train[df_train.target == 1].index\n\n# Get original number of records per target value\nnb_0 = len(df_train.loc[idx_0])\nnb_1 = len(df_train.loc[idx_1])\n\n# Calculate the undersampling rate and resulting number of records with target=0\nundersampling_rate = ((1-desired_apriori)*nb_1)/(nb_0*desired_apriori)\nundersampled_nb_0 = int(undersampling_rate*nb_0)\nprint('Rate to undersample records with target=0: {}'.format(undersampling_rate))\nprint('Number of records with target=0 after undersampling: {}'.format(undersampled_nb_0))\n\n# Randomly select records with target=0 to get at the desired a priori\nundersampled_idx = shuffle(idx_0, random_state=37, n_samples=undersampled_nb_0)\n\n# Construct list with remaining indices\nidx_list = list(undersampled_idx) + list(idx_1)\n\n# Return undersample data frame\ntrain = df_train.loc[idx_list].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"6-2) Null 값 처리\n\nundersampling 하고 난 뒤 Null값들의 비율을 보면"},{"metadata":{"trusted":true},"cell_type":"code","source":"vars_with_missing = []\n\nfor f in train.columns:\n    missings = train[train[f] == -1][f].count()\n    if missings > 0:\n        vars_with_missing.append(f)\n        missings_perc = missings/train.shape[0]\n        \n        print('Variable {} has {} records ({:.2%}) with missing values'.format(f, missings, missings_perc))\n        \nprint('In total, there are {} variables with missing values'.format(len(vars_with_missing)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ps_car_05_cat, ps_ind_02_cat등 _cat_은 Null값이 크고 작은 것을 떠나 catagory변수이므로 무응답 또한 하나의 catagoy라 생각해서 특별한 처리를 하지 않는다.\n \nps_reg_03, ps_car_14 평균값으로 (float형 변수)\n\nps_car_11은 중간값으로 대체해준다.(int형 변수)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Imputing with the mean or mode\nmean_imp = Imputer(missing_values=-1, strategy='mean', axis=0)\nmode_imp = Imputer(missing_values=-1, strategy='most_frequent', axis=0)\ntrain['ps_reg_03'] = mean_imp.fit_transform(train[['ps_reg_03']]).ravel()\ntrain['ps_car_14'] = mean_imp.fit_transform(train[['ps_car_14']]).ravel()\ntrain['ps_car_11'] = mode_imp.fit_transform(train[['ps_car_11']]).ravel()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"v = meta[(meta.level == 'nominal') & (meta.keep)].index\n\nfor f in v:\n    plt.figure()\n    fig, ax = plt.subplots(figsize=(20,10))\n    # Calculate the percentage of target=1 per category value\n    cat_perc = train[[f, 'target']].groupby([f],as_index=False).mean()\n    cat_perc.sort_values(by='target', ascending=False, inplace=True)\n    # Bar plot\n    # Order the bars descending on target mean\n    sns.barplot(ax=ax, x=f, y='target', data=cat_perc, order=cat_perc[f])\n    plt.ylabel('% target', fontsize=18)\n    plt.xlabel(f, fontsize=18)\n    plt.tick_params(axis='both', which='major', labelsize=18)\n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"7) 변수통제\n\ncorr가 <0.01 인 변수들과 위 상관계수 그래프에 따라 ps_cal이 들어단 변수들을 제거해준다."},{"metadata":{"trusted":true},"cell_type":"code","source":"selector = VarianceThreshold(threshold=.01)\nselector.fit(train.drop(['id', 'target'], axis=1)) # Fit to train without id and target variables\n\nf = np.vectorize(lambda x : not x) # Function to toggle boolean array elements\n\nv = train.drop(['id', 'target'], axis=1).columns[f(selector.get_support())]\nprint('{} variables have too low variance.'.format(len(v)))\nprint('These variables are {}'.format(list(v)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_drop2 =['ps_ind_10_bin', 'ps_ind_11_bin', 'ps_ind_12_bin', 'ps_ind_13_bin', 'ps_car_10_cat', 'ps_car_12', 'ps_car_14']\ntrain.drop(train_drop2, inplace=True, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_drop=['ps_ind_10_bin', 'ps_ind_11_bin', 'ps_ind_12_bin', 'ps_ind_13_bin', 'ps_car_10_cat', 'ps_car_12', 'ps_car_14']\ndf_test.drop(test_drop, inplace = True, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unwanted = train.columns[train.columns.str.startswith('ps_calc_')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop(unwanted, axis=1)\ntest = df_test.drop(unwanted, axis =1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Training/Predicting\n    1) gini matrix\n    2) Stratified KFold\n    3) XGBoost\n    4) Prediction"},{"metadata":{},"cell_type":"markdown","source":" 1) gini matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"def gini(actual, pred, cmpcol = 0, sortcol = 1):\n    assert( len(actual) == len(pred) )\n    all = np.asarray(np.c_[ actual, pred, np.arange(len(actual)) ], dtype=np.float)\n    all = all[ np.lexsort((all[:,2], -1*all[:,1])) ]\n    totalLosses = all[:,0].sum()\n    giniSum = all[:,0].cumsum().sum() / totalLosses\n    \n    giniSum -= (len(actual) + 1) / 2.\n    return giniSum / len(actual)\n \ndef gini_normalized(a, p):\n    return gini(a, p) / gini(a, a)\n\ndef gini_xgb(preds, dtrain):\n    labels = dtrain.get_label()\n    gini_score = gini_normalized(labels, preds)\n    return 'gini', gini_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2) Stratified KFold"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold, KFold\nkfold = 5\nskf = StratifiedKFold(n_splits=kfold, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.datasets import load_iris \nfrom sklearn.model_selection import train_test_split \nfrom sklearn.metrics import accuracy_score \nfrom sklearn.ensemble import ExtraTreesClassifier \nfrom sklearn.ensemble import RandomForestClassifier \nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import LogisticRegression #logistic regression\nfrom sklearn import svm #support vector Machine\nfrom sklearn import metrics #accuracy measure","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3) XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n    'min_child_weight': 10.0,\n    'objective': 'binary:logistic',\n    'max_depth': 7,\n    'max_delta_step': 1.8,\n    'colsample_bytree': 0.4,\n    'subsample': 0.8,\n    'eta': 0.025,\n    'gamma': 0.65,\n    'num_boost_round' : 700\n    }","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4) Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train.drop(['id', 'target'], axis=1).values\ny = train.target.values\ntest_id = test.id.values\ntest = test.drop('id', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.DataFrame()\nsub['id'] = test_id\nsub['target'] = np.zeros_like(test_id)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nfor i, (train_index, test_index) in enumerate(skf.split(X, y)):\n    print('[Fold %d/%d]' % (i + 1, kfold))\n    X_train, X_valid = X[train_index], X[test_index]\n    y_train, y_valid = y[train_index], y[test_index]\n    # Convert our data into XGBoost format\n    d_train = xgb.DMatrix(X_train, y_train)\n    d_valid = xgb.DMatrix(X_valid, y_valid)\n    d_test = xgb.DMatrix(test.values)\n    watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n\n    # Train the model! We pass in a max of 1,600 rounds (with early stopping after 70)\n    # and the custom metric (maximize=True tells xgb that higher metric is better)\n    mdl = xgb.train(params, d_train, 1600, watchlist, early_stopping_rounds=70, feval=gini_xgb, maximize=True, verbose_eval=100)\n\n    print('[Fold %d/%d Prediciton:]' % (i + 1, kfold))\n    # Predict on our test data\n    p_test = mdl.predict(d_test, ntree_limit=mdl.best_ntree_limit)\n    sub['target'] += p_test/kfold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv('StratifiedKFold.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from sklearn.ensemble import VotingClassifier\n#ensemble_lin_rbf=VotingClassifier(estimators=[('RBF',svm.SVC(probability=True,kernel='rbf',C=0.5,gamma=0.1)),\n#                                             ('RFor',RandomForestClassifier(n_estimators=100,random_state=0)),\n #                                             ('LR',LogisticRegression(C=0.05)),\n  #                                            ('ET',ExtraTreesClassifier(random_state=0)),\n   #                                           ('svm',svm.SVC(kernel='linear',probability=True))\n    #                                         ], \n     #                  voting='soft')\n#ensemble_lin_rbf.fit(train.drop(['id', 'target'],axis=1),train.target)\n#print('The accuracy for ensembled model is:',ensemble_lin_rbf.score(test_X,test_Y.values.ravel()))\n#cross=cross_val_score(ensemble_lin_rbf,X,Y, cv = 10,scoring = \"accuracy\")\n#print('The cross validated score is',cross.mean())","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}