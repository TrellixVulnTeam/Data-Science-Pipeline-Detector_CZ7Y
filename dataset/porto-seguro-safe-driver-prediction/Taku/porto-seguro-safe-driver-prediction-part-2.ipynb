{"cells":[{"metadata":{"_uuid":"8e2d018fa225fb28e9876c55bb98dd9cb7c315d5"},"cell_type":"markdown","source":"# Porto Seguro Safe Driver Prediction Part 2\n"},{"metadata":{"_uuid":"41d794f5e6869cac4fa72f8402050344b2506f63"},"cell_type":"markdown","source":"****Hello, welcome to the modelling section of my Porto Seguro Safe Driver Prediction attempt. I'll be using 4 classfifcation models in this kernel: Logistic,Stochastic Gradient Descent, Random Forest and Gradient Boosting.\n\nThis is a continuation from the work I did in an earlier kernel. Link here: https://www.kaggle.com/tmunyanyi22/data-exploration-for-porto-seguro\nCredit to this kernel for some inspiration : https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction :)\n\n\nLet's begin!"},{"metadata":{"_uuid":"b10bc3e3816aac62c8eb9e1991ee5cf765dea40c"},"cell_type":"markdown","source":"**Let's start by loading in prerequisites such as useful packages, the actual data and also, let's carry over changes from the previous kernel to this one**"},{"metadata":{"trusted":true,"_uuid":"a9a36544253596b3d7d6570afb444e21e36fc3b8"},"cell_type":"code","source":"####Loading useful packages\n\n#For data manipulation\nimport numpy as np\nimport pandas as pd\n\n#For plotting\nimport matplotlib.pyplot as pp\nimport seaborn as sns\n\n#This just ensures that our plots appear\n%matplotlib inline                   \n\n\n#For surpressing warnings\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"04bf63db59edbbbf63ada818c6d0fa65f2e05318"},"cell_type":"code","source":"#loading in the data\ntrain_data = pd.read_csv(\"../input/train.csv\")\ntest_data = pd.read_csv(\"../input/test.csv\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f145f0da04be15c9def3876a361b2d57bfdaadfd"},"cell_type":"code","source":"#Making's lists on variables which belong to each group\ncategorical_variables = [train_data.columns[i] for i in range(len(train_data.columns)) if 'cat' in train_data.columns[i]]\n\nbinary_variables = [train_data.columns[i] for i in range(len(train_data.columns)) if 'bin' in train_data.columns[i]]\n\ninterval_variables = [train_data.columns[i] for i in range(len(train_data.columns)) if (train_data.loc[:,train_data.columns[i]].dtype==float and 'cat' not in train_data.columns[i] and 'bin' not in train_data.columns[i])]\n\nordinal_variables = [train_data.columns[i] for i in range(len(train_data.columns)) if (train_data.loc[:,train_data.columns[i]].dtype == 'int64' and 'cat' not in train_data.columns[i] and 'bin' not in train_data.columns[i])][2:]\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"01dbf6d646cac7ef5f375bb37f8f8da5daaa2247"},"cell_type":"code","source":"#Let's encode the variables\nfrom sklearn.preprocessing import LabelEncoder\n\n#Create a label encoder object\nle = LabelEncoder()\nle_count = 0\n\n#Iterate through the columns\nfor col in train_data:\n    if col in categorical_variables:\n        #If 2 or fewer unique categories\n        if len(list(train_data[col].unique())) <=2:\n            #Train on the training data\n            le.fit(train_data[col])\n            #Transform both training and testing data\n            train_data[col] = le.transform(train_data[col])\n            test_data[col] = le.transform(test_data[col])\n            \n            #Keep track of how many columns were label encoded\n            le_count +=1\n\nprint('%d columns were label encoded.' % le_count)\n\n#One-hot encode variables\ntrain_data = pd.get_dummies(train_data)\ntest_data = pd.get_dummies(test_data)\n\nprint('Training Features shape: ', train_data.shape)\nprint('Testing Features shape: ', test_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f16bc52190cb96907d71692c298ca1446252c99d"},"cell_type":"markdown","source":"We will now scale the data to make it appropiate for the machine learning processes we are about to undertake.....\n"},{"metadata":{"trusted":true,"_uuid":"dbbb37b0c8505751210de11c97230687dd14cf18"},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\n#Let's drop 'target' from the training data\ntrain = train_data.drop(['target','id'],axis=1)\n\n#Make a list of feature names\nfeatures = list(train.columns)\n\n#Copy of testing data\ntest = test_data.copy()\ntest = test.drop('id',axis =1)\n#Scale each feature from 0 to 1\nscaler = MinMaxScaler(feature_range=(0,1))\n\n#Fit and transform data\nscaler.fit(train)\ntrain = scaler.transform(train)\ntest = scaler.transform(test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6fa82609bc84c9a612c6f68edc599f5881bad33c"},"cell_type":"markdown","source":"**Logistic Regression**"},{"metadata":{"trusted":true,"_uuid":"97ef9888e58458ac47e4f7723a5844d349796341"},"cell_type":"code","source":"#Let's load in the required package for a logistic regression\nfrom sklearn.linear_model import LogisticRegression\n\n#Make a logistic regression with the specified regularization parameter\nlog_reg = LogisticRegression(C = 0.5)\n\n#Train on the data\nlog_reg.fit(train,train_data.loc[:,'target'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"186c6e0a9191e83a52e46e6baf6315df9f73cf1f"},"cell_type":"code","source":"##Making predictions\n#Make sure to selct the second column only. This is because the \".predict_proba()\" method outputs two columns: the first being the probability of not\n#claiming and the second being the probability of making a claim. We're only interested in the probability of making a claim.\n\nlog_regs_pred = log_reg.predict_proba(test)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"994ff9f953d822f1b824520d53f80052076cf67a"},"cell_type":"code","source":"##Making  a csv file to submit our predictions\n\n#Submission data frame\nsubmit = test_data[['id']]\n\n#Add in prediction column as 'target'\nsubmit['target'] = log_regs_pred\n\nsubmit.to_csv('log_reg_pred.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7f1ab93afcb1f22b756af38fbede554ea79195c2"},"cell_type":"markdown","source":"This Logisitc model scored **0.2409** on the Private Leaderboard"},{"metadata":{"_uuid":"4acb26e13432e212c1496a1e129116e0e7747fd0"},"cell_type":"markdown","source":"**Stochastic Gradient Descent**\n"},{"metadata":{"trusted":true,"_uuid":"a05d272b88b4e438d32ce7a3d19816889816e6d8"},"cell_type":"code","source":"#Let's load in the required package for a Stochastic Gradient Descent Classifier\nfrom sklearn.linear_model import SGDClassifier\n\n#Make a Stochastic Gradient Descent Classifier\nsgdc = SGDClassifier(loss='log')\n\n#Train on the data\nsgdc.fit(train, train_data.loc[:,'target'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bc6d5b43bd9a2244295a4933893069644e4d9e92"},"cell_type":"code","source":"##Making predictions\nsgdc_preds = sgdc.predict_proba(test)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1bc5164a87c0dc6b20f812421bbbfa812f34a042"},"cell_type":"code","source":"##Making  a csv file to submit our predictions\n\n#Submission data frame\nsubmit2 = test_data[['id']]\n\n#Add in prediction column as 'target'\nsubmit2['target'] = sgdc_preds\n\nsubmit2.to_csv('sgdc_pred.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f2c2656d4b7c1fb1292ec535b0ed49f685170df3"},"cell_type":"markdown","source":"This Stochastic Gradient Descent model scored 0.23650 on the **Private Leaderboard**"},{"metadata":{"_uuid":"e104325ed646ac3db8284a72e812d535edfd6d9b"},"cell_type":"markdown","source":"**Random Forest**"},{"metadata":{"trusted":true,"_uuid":"845b5a280caa82266316fbc88b3e6b12c3e074c1"},"cell_type":"code","source":"#Let's load in the required package for a Random Forest Classifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n#Make a random forest classifier with the specified parameters\nrandom_forest = RandomForestClassifier(n_estimators =100, random_state =78, verbose=1, n_jobs=-1)\n\n#Train on the training data\nrandom_forest.fit(train, train_data.loc[:,'target'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"36e231e3fc8e6ca4a82086bdb3c09285c41debd0"},"cell_type":"code","source":"#Random Forest's also allow us to see the importance of each feature. Let's use that ability\nrf_feature_importance_values = random_forest.feature_importances_\nrf_feature_importances = pd.DataFrame({'feature':features,'importance':rf_feature_importance_values})\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"73bc2a76f1dbe957de257f1d08a0b5fd99f095be"},"cell_type":"code","source":"#Make predictions on the test data\nrf_preds = random_forest.predict_proba(test)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d5801778454869d62692ac9094e32d6e35bf8e56"},"cell_type":"code","source":"##Making  a csv file to submit our predictions\n\n#Submission data frame\nsubmit3 = test_data[['id']]\n\n#Add in prediction column as 'target'\nsubmit3['target'] = rf_preds\n\nsubmit3.to_csv('rf_pred.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6e73abaa565c82c3f9fd76bc51c3afc8aac76bfa"},"cell_type":"markdown","source":"This Random Forest model scored **0.16901** on the Private Leaderboard"},{"metadata":{"_uuid":"888693c9597bcdea310d984ea5d4b8ba0e6aa2e3"},"cell_type":"markdown","source":"**Gradient Boosting**"},{"metadata":{"trusted":true,"_uuid":"1ffe174b88c5463971b0a775a74d7fbff92abb08"},"cell_type":"code","source":"#Let's load in the required package for a Gradient Boosting Classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n#Make a gradient boosting classifier\ngbm = GradientBoostingClassifier(n_estimators = 100, random_state=50)\n\n#Train on the training data\ngbm.fit(train, train_data.loc[:,'target'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"07ab1419311ae1b6774b0439fb3e26e605b101c3"},"cell_type":"code","source":"#Extract features importance\ngbm_feature_importance_values = gbm.feature_importances_\ngbm_feature_importances = pd.DataFrame({'feature':features,'importance':gbm_feature_importance_values})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"89d7296e6e94b62b2e1266fb7a773cc7f8effd4a"},"cell_type":"code","source":"#Make predictions on the test data\ngbm_preds = gbm.predict_proba(test)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"66b930e544e6219a2509e5d09b6751e160f1847b"},"cell_type":"code","source":"##Making  a csv file to submit our predictions\nsubmit4 = test_data[['id']]\n\nsubmit4['target'] = gbm_preds\n\nsubmit4.to_csv('gbm_pred.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ffc55ae312e2af3aa3a90a78957b639897716b52"},"cell_type":"markdown","source":"This Gradient Boosting model scored **0.27233**"},{"metadata":{"_uuid":"b6f3eee645395676c9ee9bfeebd7f92182ccb231"},"cell_type":"markdown","source":"The scores so far are okay, we'll definitely get better going forward. Let's take a look the feature importance graphs..."},{"metadata":{"trusted":true,"_uuid":"0dfce92360abc64247073208bff29aaadc529566"},"cell_type":"code","source":"rf_feature_importances.set_index('feature').sort_values(by='importance',ascending=False).iloc[:15,:].plot.bar()\npp.title(\"Feature importance according to Random Forest\")\npp.ylabel(\"Normalized Importance\")\npp.xlabel(\"Feature\")\npp.show()\n\ngbm_feature_importances.set_index('feature').sort_values(by='importance',ascending=False).iloc[:15,:].plot.bar()\npp.title(\"Feature importance according to Gradient Boosting\")\npp.ylabel(\"Normalized Importance\")\npp.xlabel(\"Feature\")\npp.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"decf8b9d8d282b5764d5469cbd25f4d38bffe19c"},"cell_type":"markdown","source":"It seems that,as I had suspected earlier, ps_car_13 is the most important feature for predicting whether a claim will happen. ps_ind_03\nseems reasonably strong too. We'll rely on this information going forward in our next kernel :)"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":1}