{"nbformat":4,"cells":[{"cell_type":"markdown","metadata":{"_uuid":"148bec7a0a52ff07f70d55280be7084a1bd4e57d","_cell_guid":"29edc74d-43d0-4c86-8839-34d838227050"},"source":"Since this solution uses a Naive Bayesian Network to predict the target I select the features based on their mutual information with the target and their relative indepedence from each other.  This will strengthen the biggest assumption of any Naive BN model: all features are independent. But in this data there is wide dependency among the features. \n\nMy approach to finding 'relatively' independent features is to first group the features into Concepts. My exploration has shown about 14 Concepts in the data (ps: I excluded all _calc_ features as white noise). To identify Concepts I first learn a BN model of the data with great restrictions on the learning process: features/nodes may have a single parent. Here is a DAG of such a model:\n\n![BN 4 showing concepts](http://elmtreegarden.com/wp-content/uploads/2017/03/bn4-w-concepts.jpg)\n\nThe 14 Concepts are color coded. The small \"?\" symbol means the feature has missing data. The color coding is imperfect and is is difficult to see that ps_ind_16_bin and ps_ind_14 are different Concepts.\n\nThe mutual information of each feature with the target is calculated and the features with at least 0.001 bits of mutual information are selected from each Concept. A total of 11 features were selected and shown below. Sons and Spouses learning resulted in the final 7 feature model.\n\n![Final model](http://elmtreegarden.com/wp-content/uploads/2017/03/BN-5-w-all-fv-NB-22-8-gini-1.jpg)\n\nNote the very large Entropy of car_11_cat ! This deserves much more exploration."},{"source":"import pandas as pd\nimport numpy as np\n\n#mdf = 'c:/Users/John/Documents/Kaggle/Porto Seguro/'\nmdf = '../input/'\n\ntrain = pd.read_csv(mdf + \"train.csv\", usecols = ['target', 'ps_car_07_cat',\n    'ps_car_02_cat', 'ps_car_13','ps_reg_02', 'ps_ind_06_bin', 'ps_ind_16_bin', 'ps_ind_17_bin'])\n\ntest = pd.read_csv(mdf + \"test.csv\", usecols = ['id', 'ps_car_07_cat', 'ps_car_02_cat',\n    'ps_car_13','ps_reg_02', 'ps_ind_06_bin', 'ps_ind_16_bin', 'ps_ind_17_bin'])\nbins = [0.0, 0.639, 0.784, 1.093, 4.4]\ntrain['ps_car_13_d'] = pd.cut(train['ps_car_13'], bins)\ntest['ps_car_13_d'] = pd.cut(test['ps_car_13'], bins)\nbins2 = [-0.1, 0.25, 0.75, 2.0]\ntrain['ps_reg_02_d'] = pd.cut(train['ps_reg_02'], bins2)\ntest['ps_reg_02_d'] = pd.cut(test['ps_reg_02'], bins2)\ntrain.head(4)","cell_type":"code","outputs":[],"metadata":{"collapsed":true,"_uuid":"71f836c6b8f638a0573562d9ae5afa8d05ce02df","_cell_guid":"e0b11f24-d7c0-432d-a541-989f21715755"},"execution_count":null},{"cell_type":"markdown","metadata":{"_uuid":"6fd4073e5845adb5154ae17aa65777b9cf6692ff","_cell_guid":"e813807d-dc0a-41c4-9fa0-2c852e1c4eac"},"source":"A Naive model was chosen in part because it is so easy to calculate in Python (any language really). It is a simple calculation of the conditional probability of the target given features 1 through 7:\n\np(target | feature1, ... , feature7) = p(target=1) x { p(feature1 | target) x ... x  p(feature7 | target) }  / Z\n\nZ is a normalizing constant. It is calculated and used to normalize the whole prediction so that it has an average of about 3.75% (the average probability that target = 1).\n"},{"source":"# Now calculate the each factor associated with each feature, (fi): p( feature_i | target)\n\nf1 = pd.DataFrame()\nf2 = pd.DataFrame()\nf3 = pd.DataFrame()\nf4 = pd.DataFrame()\nf5 = pd.DataFrame()\nf6 = pd.DataFrame()\nf7 = pd.DataFrame()\n\nf1 = train.groupby('ps_car_13_d')['target'].agg([('p_f1','mean')]).reset_index()\nf2 = train.groupby('ps_reg_02_d')['target'].agg([('p_f2','mean')]).reset_index()\nf3 = train.groupby(['ps_car_07_cat'])['target'].agg([('p_f3','mean')]).reset_index()\nf4 = train.groupby(['ps_car_02_cat'])['target'].agg([('p_f4','mean')]).reset_index()\nf5 = train.groupby('ps_ind_06_bin')['target'].agg([('p_f5','mean')]).reset_index()\nf6 = train.groupby('ps_ind_16_bin')['target'].agg([('p_f6','mean')]).reset_index()\nf7 = train.groupby('ps_ind_17_bin')['target'].agg([('p_f7','mean')]).reset_index()\nf3.head(10)","cell_type":"code","outputs":[],"metadata":{"collapsed":true,"_uuid":"3013d781133d273c7224c286f667eef61c568aa7","_cell_guid":"f47a8024-de41-4733-a4ad-6622b625bbc9"},"execution_count":null},{"cell_type":"markdown","metadata":{"_uuid":"db4f87ef8f5bfc28e9f69fc3754d10fe75c4b60c","_cell_guid":"ff75c549-c624-4660-9a07-d0a684f59e32"},"source":"Above you can see that ps_car_07_cat has some missing data (notice the -1 state). My theory is that there is no missing data. Porto Seguro has given us all the information available. Therefore missing data is really a filtered state. It also seems from the DAG of Concepts that missing data seems concentrated in certain Concepts. This makes perfect sense if the missing data represents a filtered state.\n\n"},{"source":"sol1 = pd.DataFrame()\nsol1 = test.merge(f1, on = 'ps_car_13_d')\nsol2 = pd.DataFrame()\nsol2 = sol1.merge(f2, on = 'ps_reg_02_d')\ndel sol1\nsol3 = pd.DataFrame()\nsol3 = sol2.merge(f3, on = 'ps_car_07_cat')\ndel sol2\nsol4 = pd.DataFrame()\nsol4 = sol3.merge(f4, on = 'ps_car_02_cat')\ndel sol3\nsol5 = pd.DataFrame()\nsol5 = sol4.merge(f5, on = 'ps_ind_06_bin')\ndel sol4\nsol6 = pd.DataFrame()\nsol6 = sol5.merge(f6, on = 'ps_ind_16_bin')\ndel sol5\nsol = pd.DataFrame()\nsol = sol6.merge(f7, on = 'ps_ind_17_bin')\ndel sol6\nsol.head(5)","cell_type":"code","outputs":[],"metadata":{"collapsed":true,"_uuid":"17094678f64f361130d5db08341116c398fd5e70","_cell_guid":"52c426f9-9c6e-4d6f-bbe6-970df9fcab70"},"execution_count":null},{"source":"# f is the product of factors of feaures\nsol.loc[:,'f'] = sol.loc[:,'p_f1'] * sol.loc[:,'p_f2'] * sol.loc[:,'p_f3'] * sol.loc[:,'p_f4'] \\\n                * sol.loc[:,'p_f5'] * sol.loc[:,'p_f6'] * sol.loc[:,'p_f7'] \n\nz = sol.f.sum() / len(sol.f)\n# z is the normalizing factor\nsol['target'] = 0.03645 * sol.loc[:,'f'] / z\nsol[['id', 'target']].to_csv('bn_5_output_7_nodes.csv', index = False, float_format='%.4f')\nsol.shape","cell_type":"code","outputs":[],"metadata":{"collapsed":true,"_uuid":"4859697868e008247825228e19cd9e2657a15717","_cell_guid":"44c8eea5-5b7a-457d-88be-e6327c045f50"},"execution_count":null},{"source":"# thanks to cpmpml for : https://www.kaggle.com/cpmpml/extremely-fast-gini-computation\nfrom numba import jit\n\n@jit\ndef eval_gini(y_true, y_prob):\n    y_true = np.asarray(y_true)\n    y_true = y_true[np.argsort(y_prob)]\n    ntrue = 0\n    gini = 0\n    delta = 0\n    n = len(y_true)\n    for i in range(n-1, -1, -1):\n        y_i = y_true[i]\n        ntrue += y_i\n        gini += y_i * delta\n        delta += 1 - y_i\n    gini = 1 - 2 * gini / (ntrue * (n - ntrue))\n    return gini\n\nsol1 = pd.DataFrame()\nsol1 = train.merge(f1, on = 'ps_car_13_d')\nsol2 = pd.DataFrame()\nsol2 = sol1.merge(f2, on = 'ps_reg_02_d')\ndel sol1\nsol3 = pd.DataFrame()\nsol3 = sol2.merge(f3, on = 'ps_car_07_cat')\ndel sol2\nsol4 = pd.DataFrame()\nsol4 = sol3.merge(f4, on = 'ps_car_02_cat')\ndel sol3\nsol5 = pd.DataFrame()\nsol5 = sol4.merge(f5, on = 'ps_ind_06_bin')\ndel sol4\nsol6 = pd.DataFrame()\nsol6 = sol5.merge(f6, on = 'ps_ind_16_bin')\ndel sol5\nsol = pd.DataFrame()\nsol = sol6.merge(f7, on = 'ps_ind_17_bin')\ndel sol6\nsol.loc[:,'f'] = sol.loc[:,'p_f1'] * sol.loc[:,'p_f2'] * sol.loc[:,'p_f3'] * sol.loc[:,'p_f4'] \\\n                * sol.loc[:,'p_f5'] * sol.loc[:,'p_f6'] * sol.loc[:,'p_f7'] \nz = sol.f.sum() / len(sol.f)\nsol['exp_target'] = 0.03645 * sol.loc[:,'f'] / z\n\n# Calculate GINI score\neval_gini(sol['target'], sol['exp_target'])","cell_type":"code","outputs":[],"metadata":{"collapsed":true,"_uuid":"07e408ccfe9548929c3bb1f8daa171fd3978255c","_cell_guid":"b48e564f-195d-4f95-9245-7e9bf0f6e96c"},"execution_count":null}],"metadata":{"language_info":{"codemirror_mode":{"version":3,"name":"ipython"},"nbconvert_exporter":"python","version":"3.6.3","pygments_lexer":"ipython3","file_extension":".py","mimetype":"text/x-python","name":"python"},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"}},"nbformat_minor":1}