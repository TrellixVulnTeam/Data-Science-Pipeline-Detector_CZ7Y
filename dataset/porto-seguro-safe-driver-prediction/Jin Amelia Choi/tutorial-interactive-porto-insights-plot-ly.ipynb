{"cells":[{"metadata":{},"cell_type":"markdown","source":"## [무작정 kaggle 따라하기] Interactive Porto Insights (Plot.ly)"},{"metadata":{},"cell_type":"markdown","source":"*kaggle 연습을 위한 notebook입니다.* <br>\n해당 커널은 다음 자료를 참고하였습니다."},{"metadata":{},"cell_type":"markdown","source":"- [Anisotropic - Interactive Porto Insights - A Plot.ly Tutorial](https://www.kaggle.com/arthurtok/interactive-porto-insights-a-plot-ly-tutorial)"},{"metadata":{},"cell_type":"markdown","source":"## Introduction"},{"metadata":{},"cell_type":"markdown","source":"해당 notebook은 브라질에서 세번째로 큰 보험회사 [Porto Seguro](https://en.wikipedia.org/wiki/Porto_Seguro_S.A.)의 예측 문제(운전자가 내년에 보험을 가동할 가능성)을 담고 있다.\n\n이 notebook은 Python의 시각화 라이브러리 Plot.ly를 이용하여 해당 데이터를 시각화하고 인사이트를 도출하는데 의미를 두고 있다. Plot.ly는 웹 온라인 상에서 그래픽, 통계학 시각화를 제공하는데 특화된 소프트웨어 회사로써 Python, R, Matlb, Node.js 등 다양한 프로그래밍 언어에 접근이 가능하게 API를 제공한다."},{"metadata":{},"cell_type":"markdown","source":"**해당 노트북에서 보여줄 Plot.ly 차트는 다음과 같다.**\n\n- 일반 수평 bar plot (타겟변수분포를 조사하는데 사용함)\n\n- 상관관계 heatmap plot\n\n- 산점도 scatter plot\n\n- 수직 bar plot (내림차순으로 정렬, 다양한 변수들의 중요성 따짐)\n\n- 3D scatter plot"},{"metadata":{},"cell_type":"markdown","source":"이 notebook은 다음과 같이 이뤄진다.\n\n- 데이터 품질 검사 (시각화 및 결측치 값 처리)\n\n- 변수 검사와 필터링 (상관관계와 타겟 변수와 관련된 상호정보량 알아보기. 이진, 범주형 및 다른 변수들 조사)\n\n- 머신러닝 모델을 통한 변수 중요도 순위 매기기 (Random Forest와 Gradient Boost 이용함)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# setting\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport warnings\nfrom collections import Counter\nfrom sklearn.feature_selection import mutual_info_classif\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# load the training data\ntrain = pd.read_csv('/kaggle/input/porto-seguro-safe-driver-prediction/train.csv')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rows = train.shape[0]\ncolumns = train.shape[1]\nprint('The train dataset contains {0} rows and {1} columns'.format(rows, columns))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"## 1. 데이터 품질 검사 Data Quality Checks"},{"metadata":{},"cell_type":"markdown","source":"### 결측치 검사하기"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().any().any()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"여기서 False가 나오니 null값이 없다고 볼 수 있다. [하지만 '-1' 값이 들어있으면 해당 변수를 결측치로 본다고 정보란에 적혀 있다.](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/data)\n\n따라서 '-1'값을 NaN으로 변환하여 처리하려고 한다."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_copy = train\ntrain_copy = train_copy.replace(-1, np.NaN)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"이번에는 Kaggler [Alekse Bilogur](https://www.kaggle.com/residentmario)의 'Missingno' 패키지를 활용하여 결측치를 체크한다."},{"metadata":{"trusted":true},"cell_type":"code","source":"import missingno as msno\nmsno.matrix(df=train_copy.iloc[:, 2:39], figsize=(20,14), color=(0.42, 0.1, 0.05))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"그래프를 보면 알 수 있듯이, 빈 공간이 많은 부분일수록 결측치가 많은 것을 알 수 있다.\n\nmissingno plot은 각 40개의 변수들이 한 plot에 들어오는 것이 가장 자연스럽다. <br>\n따라서 몇 개의 column이 누락되었고 그 중 5개의 null column이 배제되었다.\n\n**ps_ind_05_cat | ps_reg_03 | ps_car_03_cat | ps_car_05_cat | ps_car_07_cat | ps_car_09_cat | ps_car_14** <br>\n이렇게 7개의 null column을 확인할 수 있다.\n\n그리고 **ps_car_03_cat | ps_car_05_cat | ps_car_07_cat** column은 결측 정도가 심한 것을 확인할 수 있다. \n\n> 따라서 '-1'을 NaN값으로 처리하는 것은 좋은 방식이 아니다."},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"## 2. 변수 검사와 필터핑 Feature Inspection and Filtering"},{"metadata":{},"cell_type":"markdown","source":"### 타겟 변수 검사하기"},{"metadata":{},"cell_type":"markdown","source":"타겟변수는 주로 머신러닝 지도학습에서 연관 데이터와 함께 사용된다. <br>\n학습 함수의 표준화 및 데이터 예측을 위해 사용하는 예측함수가 스스로 학습을 할 때 타겟변수가 중요한 역할을 한다."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = [go.Bar(\n            x = train['target'].value_counts().index.values,\n            y = train['target'].value_counts().values,\n            text= 'Distribution of target variable'\n    )]\n\nlayout = go.Layout(\n    title = 'Target variable distribution'\n)\n\nfig = go.Figure(data=data, layout=layout)\n\npy.iplot(fig, filename='basic-bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"타겟변수의 불균형이 너무 심하다...! 확인 해두기"},{"metadata":{},"cell_type":"markdown","source":"### 데이터 타입 확인하기"},{"metadata":{"trusted":true},"cell_type":"code","source":"Counter(train.dtypes.values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"정수형 변수가 49개, 실수형 변수가 10개로 카운트된다.\n\n또 다른 점은 Porto Seguro는 '_bin', '_cat', '_reg'와 같은 접미사를 데이터 헤더에 약어로 사용하는 것이다. <br>****\n이런 약어들은 대략적인 설명을 담고 있다 (_bin은 이진 변수, _cat은 범주형 변수 등으로)"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_float = train.select_dtypes(include=['float64'])\ntrain_int = train.select_dtypes(include=['int64'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_float","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Correlation plots"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 실수형 변수의 상관관계 heatmap\ncolormap = plt.cm.magma\nplt.figure(figsize=(16,12))\nplt.title('Pearson correlation of continuous features', y=1.05, size=15)\nsns.heatmap(train_float.corr(), linewidths=0.1, vmax=1.0, square=True,\n           cmap=colormap, linecolor='white', annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"양의 상관관계에 있는 변수들은 다음과 같다:\n\n(ps_reg_01, ps_reg_02)\n\n(ps_reg_02, ps_reg_03)\n\n(ps_car_12, ps_car_13)\n\n(ps_car_13, ps_car_15)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 정수형 변수의 상관관계\ndata = [\n    go.Heatmap(\n        z = train_int.corr().values,\n        x = train_int.columns.values,\n        y = train_int.columns.values,\n        colorscale = 'Viridis',\n        reversescale = False,\n        opacity = 1.0 )\n]\n\nlayout = go.Layout(\n    title = 'Pearson Correlation of Integer-type features',\n    xaxis = dict(ticks='', nticks=36),\n    yaxis = dict(ticks='' ),\n    width = 900, height = 700)\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='labelled-heatmap')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 상호정보량 Mutual Information plots"},{"metadata":{},"cell_type":"markdown","source":"상호정보량은 타겟변수와 그에 상응하는 변수들의 상호 관계를 확인할 수 있는 유용한 기법 중 하나이다. <br>\nsklearn의 mutual_info_classif 방법을 사용하여 두가지 랜덤 변수의 의존성을 측정한다. <br>\n이는 타겟 변수가 다른 변수들 사이에서 얼마나 많은 정보를 가지고 있는 지 확인할 수 있다."},{"metadata":{"trusted":true},"cell_type":"code","source":"mf = mutual_info_classif(train_float.values, train.target.values, n_neighbors=3, random_state=17)\nprint(mf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 이진 변수 검사하기"},{"metadata":{},"cell_type":"markdown","source":"이진 변수만을 포함하는 column에 대해서 조사하기"},{"metadata":{"trusted":true},"cell_type":"code","source":"bin_col = [col for col in train.columns if '_bin' in col]\nzero_list = []\none_list = []\nfor col in bin_col:\n    zero_list.append((train[col]==0).sum())\n    one_list.append((train[col]==1).sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trace1 = go.Bar(\n    x = bin_col,\n    y = zero_list,\n    name = 'Zero Count'\n)\n\ntrace2 = go.Bar(\n    x = bin_col,\n    y = one_list,\n    name = 'One Count'\n)\n\ndata = [trace1, trace2]\nlayout = go.Layout(\n    barmode = 'stack',\n    title = 'Count of 1 and 0 in binary variables'\n)\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='stacked-bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ps_ind_10_bin, ps_ind_11_bin, ps_ind_12_bin, ps_ind_13_bin <br>\n이 4가지 변수는 zero count가 대부분의 범위를 차지하고 있다."},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"## 3. 머신러닝을 통한 변수 중요도 순위 매기기"},{"metadata":{},"cell_type":"markdown","source":"### Random Forest를 통해 알아보는 변수 중요도 "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=150, max_depth=8, min_samples_leaf=4, max_features=0.2, n_jobs=-1, random_state=0)\nrf.fit(train.drop(['id', 'target'], axis=1), train.target)\nfeatures = train.drop(['id', 'target'], axis=1).columns.values\nprint(\"----- Training Done -----\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# scatter plot\ntrace = go.Scatter(\n    y = rf.feature_importances_,\n    x = features,\n    mode = 'markers',\n    marker = dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 13,\n        color = rf.feature_importances_,\n        colorscale = 'Portland',\n        showscale = True\n    ),\n    text = features\n)\ndata = [trace]\n\nlayout = go.Layout(\n    autosize = True,\n    title = 'Random Forest Feature Importance',\n    hovermode = 'closest',\n    xaxis = dict(\n        ticklen = 5,\n        showgrid = False,\n        zeroline = False,\n        showline = False\n    ),\n    yaxis = dict(\n        title = 'Feature Importance',\n        showgrid = False,\n        zeroline = False,\n        ticklen = 5,\n        gridwidth = 2\n    ),\n    showlegend = False\n)\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='scatter2010')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"이와 더불어, 모든 변수들을 중요도 순으로 내림차순 정렬하여 bar plot으로 나타낼 수 있다."},{"metadata":{"trusted":true},"cell_type":"code","source":"x, y = (list(x) for x in zip(*sorted(zip(rf.feature_importances_, features),\n                                    reverse = False)))\n\ntrace2 = go.Bar(\n    x = x,\n    y = y,\n    marker = dict(\n        color = x,\n        colorscale = 'Viridis',\n        reversescale = True\n    ),\n    name = 'Random Forest Feature Importances',\n    orientation = 'h',\n)\n\nlayout = dict(\n    title = 'Barplot of Feature Importances',\n    width = 900, height = 1500,\n    yaxis = dict(\n        showgrid = False,\n        showline = False,\n        showticklabels = True,\n            domain = [0, 0.85],\n    ))\n\nfig1 = go.Figure(data=[trace2])\nfig1['layout'].update(layout)\npy.iplot(fig1, filename='plots')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Decision Tree Visualisation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import tree\nfrom IPython.display import Image as PImage\nfrom subprocess import check_call\nfrom PIL import Image, ImageDraw, ImageFont\nimport re\n\ndecision_tree = tree.DecisionTreeClassifier(max_depth=3)\ndecision_tree.fit(train.drop(['id', 'target'], axis=1), train.target)\n\n# 훈련된 모델을 .dot 파일로 변환하기\nwith open('tree1.dot', 'w') as f:\n    f = tree.export_graphviz(decision_tree,\n                            out_file = f,\n                            max_depth = 4,\n                            impurity = False,\n                            feature_names = train.drop(['id', 'target'], axis=1).columns.values,\n                            class_names = ['No', 'Yes'],\n                            rounded = True,\n                            filled = True)\n    \n# .dot 파일을 .png 파일로 변환하여 웹 노트북에서 볼 수 있게 하기\ncheck_call(['dot', '-Tpng', 'tree1.dot', '-o', 'tree1.png'])\n\n# Annoting chart with PIL\nimg = Image.open('tree1.png')\ndraw = ImageDraw.Draw(img)\nimg.save('sample-out.png')\nPImage('sample-out.png')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Gradient Boosting 모델을 통해 알아보는 변수 중요도"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\ngb = GradientBoostingClassifier(n_estimators=100, max_depth=3, min_samples_leaf=4, max_features=0.2, random_state=0)\ngb.fit(train.drop(['id', 'target'], axis=1), train.target)\nfeatures = train.drop(['id', 'target'], axis=1).columns.values\nprint('----- Training Done -----')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scatter plot \ntrace = go.Scatter(\n    y = gb.feature_importances_,\n    x = features,\n    mode='markers',\n    marker=dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 13,\n        #size= rf.feature_importances_,\n        #color = np.random.randn(500), #set color equal to a variable\n        color = gb.feature_importances_,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = features\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Gradient Boosting Machine Feature Importance',\n    hovermode= 'closest',\n     xaxis= dict(\n         ticklen= 5,\n         showgrid=False,\n        zeroline=False,\n        showline=False\n     ),\n    yaxis=dict(\n        title= 'Feature Importance',\n        showgrid=False,\n        zeroline=False,\n        ticklen= 5,\n        gridwidth= 2\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatter2010')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x, y = (list(x) for x in zip(*sorted(zip(gb.feature_importances_, features), \n                                                            reverse = False)))\ntrace2 = go.Bar(\n    x=x ,\n    y=y,\n    marker=dict(\n        color=x,\n        colorscale = 'Viridis',\n        reversescale = True\n    ),\n    name='Gradient Boosting Classifer Feature importance',\n    orientation='h',\n)\n\nlayout = dict(\n    title='Barplot of Feature importances',\n     width = 900, height = 2000,\n    yaxis=dict(\n        showgrid=False,\n        showline=False,\n        showticklabels=True,\n    ))\n\nfig1 = go.Figure(data=[trace2])\nfig1['layout'].update(layout)\npy.iplot(fig1, filename='plots')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Observations: <br>\nRandomForest과 Gradient Boosting model 둘다 제일 중요한 변수로 ps_car_13을 꼽았다.\n"},{"metadata":{},"cell_type":"markdown","source":"## Conclusion"},{"metadata":{},"cell_type":"markdown","source":"Porto Seguro 데이터셋을 가지고 결측치와 데이터 품질 검사를 진행하였고, <br>\n변수들간의 선형 상관관계를 조사하며, Random Forest와 Gradient Boosting model을 통해 변수 중요도를 판단해보았다.\n\n이를 통해 중요한 변수를 선별하는 작업이 가능하였다."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}