{"cells":[{"metadata":{"_uuid":"ecb402a1b7efe9eb6db47e7cab2c38861609ad94","_cell_guid":"9113065c-6cec-4d35-8daa-57e9fdc6313e"},"cell_type":"markdown","source":"## Introdução"},{"metadata":{"_uuid":"7f8a20e1db4cf09d9e53245de8461ce409447649","_cell_guid":"2dee8ae8-2bee-4609-96a8-8150b7e770cd"},"cell_type":"markdown","source":"Este trabalho foi realizado por Filipe C. L. Duarte (fcld), Hélio Gonçalves de Souza Junior (hgsj) e Matheus de Farias Cavalcanti Santos (mfcs) como parte da avaliação da disciplina de Aprendizagem de Máquina da Pós-Graduação em Ciências da Computação da UFPE no primeiro semestre de 2019. \n\n\n1. [Carregando bibliotecas](#carregando_bibliotecas)\n2. [Conhecendo os dados](#conhecendo_dados)\n3. [Estatística descritiva](#estat_descritiva)\n4. [Pré-processamento](#pre_process)\n5. [Análise exploratória de dados](#eda)\n7. [Modelos](#modelos)\n8. [Autoencoder](#autoencoder)\n9. [Modelos após redução da dimensionalidade](#modelosauto)\n10. [XGBoost com Grid Search](#xgboost_grid)"},{"metadata":{},"cell_type":"markdown","source":"<a id='carregando_bibliotecas'></a>"},{"metadata":{"_uuid":"1fa2f6de517095b84ec92276aa092ed42b17c963","_cell_guid":"10e44fab-4986-4a1a-aea4-71a75dde226e"},"cell_type":"markdown","source":"## Carregando bibliotecas"},{"metadata":{"_uuid":"d476415eb97241fd781c60b072dbdd7d687f3c56","_cell_guid":"9e5063ed-1cdb-40a1-8ef7-6db3eae052e8","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.model_selection import KFold, StratifiedKFold, cross_val_score, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\npd.set_option('display.max_columns', 100)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"619ddb7e5af828b65a422bc5096b77339e089998","_cell_guid":"4999c076-25c2-49ca-a9b5-e588a22a3483"},"cell_type":"markdown","source":"## Loading data"},{"metadata":{"_uuid":"9ec9e8d9a85ae83c7659b08b10ee0cbd348fd189","_cell_guid":"90743a5b-947a-4606-b069-2184c46d360c","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"76e1f30f15f393e53d0beb081c3662a72bdf6f5b","_cell_guid":"2d6af15a-5d84-42e7-a9ff-66747f77c02f"},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"conhecendo_dados\"></a>"},{"metadata":{"_uuid":"70d5a5616d07b4cab8f95af01d9439f2f48d020c","_cell_guid":"7fc6f68b-a256-49c5-962f-7d111189b0b2"},"cell_type":"markdown","source":"## Conhecendo os dados"},{"metadata":{"_uuid":"40bef8a97ee39315a8cdf242e286d82cb99d5476","_cell_guid":"2ef0379a-bb4f-4aaf-b6dd-1d6de4290ad3"},"cell_type":"markdown","source":"Algumas informações sobre os dados:\n\n* as variáveis com o sufixo **\"bin\"** são binárias; e **\"cat\"** para indicar variáveis categóricas.\n* Os dados omissos **missing data** foram codificados com o valor **-1** . \n* A variável de interesse, **target** significa se o segurado incorreu em sinistou ou não. \n\nPortanto, o objetivo deste prejeto é o de construir um algoritmo de classificação com o objetivo de prever se o segurado irá sinistrar. \n"},{"metadata":{},"cell_type":"markdown","source":"5 primeiras linhas dos dados de treinamento"},{"metadata":{"_uuid":"625795d926a8a30f82afce4326788d2426baf11f","_cell_guid":"11f69996-f99e-4416-a16c-7f3c32d735b2","trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"5 últimas linhas dos dados de treinamento"},{"metadata":{"_uuid":"d1b2e1027f17fb5ea238e55d3ddba02bc4787e75","_cell_guid":"575e4fe9-7cc6-47a6-90dc-45ff15aaeb34","trusted":true},"cell_type":"code","source":"train.tail()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"747c8bac1a0549b803325f2de3c2173b384ed9e0","_cell_guid":"8a25bd9a-88cf-4dc3-8640-f5006b9d343c"},"cell_type":"markdown","source":"A partir desses dados de treinamento, vemos:\n* variáveis binárias\n* variáveis categóricas codigicadas como valores inteiros\n* variáveis restantes como valores reais (float) ou inteiros\n* -1 representando os valores faltantes\n* a variável **target** e o **id**"},{"metadata":{"_uuid":"c642b9834400a789f560d53dabb5c63ebbbe4196","_cell_guid":"bcf7b21e-134b-4be6-b352-a5d8eff5b23e"},"cell_type":"markdown","source":"Visualizando a quantidade de linhas e colunas dos dados de treinamento."},{"metadata":{"_uuid":"cf8d1a98a0655cdcf1e12cb84dff9e61430a7eeb","_cell_guid":"8ae01653-e1c9-4209-8bb1-2734f8d1302c","trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c86138e65f4c51c83d3dddc31352739cc24a1771","_cell_guid":"4317596e-2187-4660-973a-1f53a2f26d66"},"cell_type":"markdown","source":"Temos 59 colunas e 595212 linhas. <br>\nVamos analisar abaixo se há duplicação de linhas.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# test head\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"O conjunto de dados de teste também possui a variável **id** que representa a identificação do segurado. \nVamos excluí-la de ambos os conjuntos de dados (treinamento e teste)."},{"metadata":{"_uuid":"c8caf45461f3a4699fbdea55036a5954d9668437","_cell_guid":"8849f509-7745-465b-83ab-3f1240a7b05c","trusted":true},"cell_type":"code","source":"train.drop_duplicates()\ntrain = train.drop(['id'], axis = 1)\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"36f73dc547915f8f76d1bf8353ad55b88f7458b1","_cell_guid":"6250b262-91a0-4f8c-8ebe-1e77f2fd014b"},"cell_type":"markdown","source":"Não há linhas em duplicidade.\nPrecisamos verificar a estrutura da tabela dos dados de teste para ver se está semelhante."},{"metadata":{"_uuid":"625dfedf9b72916b41bac4d65162c8795f629ef5","_cell_guid":"166c851a-fec1-4b56-9773-42c0387c6ec8","trusted":true},"cell_type":"code","source":"test.drop_duplicates()\ntest_id = test['id']\ntest = test.drop(['id'], axis = 1)\ntest.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8497a0ad66cfaabdae29008ba5858eea6220237a","_cell_guid":"6026084c-7c7f-41ec-8997-71092654c88b"},"cell_type":"markdown","source":"Percebemos que falta uma coluna no conjunto de teste. Qual seria?"},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Falta a variável **target**. <br>\nTudo bem, vamos analisar a estrutura dos dados com a função `info()`."},{"metadata":{"_uuid":"99afca7b33389f5804fed9a726a93ef43de5447f","_cell_guid":"5b42fec3-d32b-4da5-b0f9-981671c30bda"},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"estat_descritiva\"></a>"},{"metadata":{"_uuid":"47800f2eb3e5741591b161f6cf3f0e78c1a20ff8","_cell_guid":"d25e679c-dc55-46cb-a2fa-077da401f8e7"},"cell_type":"markdown","source":"## Estatística descritiva"},{"metadata":{"_uuid":"f32de991963ae1ff014705c3b9446e76c103d717","_cell_guid":"f0486d67-ddb2-4e64-b9c4-7ede3a8a1754"},"cell_type":"markdown","source":"O método `describe` apresenta as estatísticas descritivas para todas as colunas do data frame. \nContudo, só fará sentido aplicá-lo nas variáveis contínuas, isto é, naquelas representadas pelo conjunto dos valores reais. \nPara analisar as variáveis categóricas, utilizaremos os gráficos na análise exploratória. "},{"metadata":{"_uuid":"44aba55b3de816f8d9696b12e5e7c4e1bde4532b","_cell_guid":"1cb950e7-c7db-4bbb-ba74-f94170e8446e"},"cell_type":"markdown","source":"#### Criando vetores com os nome das variáveis pelos grupos (reg, bin, car ...)"},{"metadata":{"_uuid":"d445810faefa50cc233d92e9fb5ab5d050dfd77d","_cell_guid":"7bad38f8-a60e-49ac-9b42-ed3f25c018cf","trusted":true},"cell_type":"code","source":"colunas = train.columns.tolist()\ncolunas_reg = [col for col in colunas if 'reg' in col]\ncolunas_cat = [col for col in colunas if 'cat' in col]\ncolunas_bin = [col for col in colunas if 'bin' in col]\ncolunas_car = [col for col in colunas if 'car' in col and 'cat' not in col]\ncolunas_calc = [col for col in colunas if 'calc' in col]\nprint(colunas_cat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.loc[:,colunas_reg].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Apenas a variável **reg_03** possui missing. <br>\nPara solucionar esse problema, vamos usar um método de imputar a mediana onde existe o valor **-1**. <br>\nApós, faremos a normalização **min-max**  em **reg_02** e **reg_03** para reduzir a escala, fixando-as no intervalor [0,1]."},{"metadata":{},"cell_type":"markdown","source":"#### variáveis 'car'"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.loc[:, colunas_car].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Apenas ps_car_12 e ps_car_15 possuem dados faltantes\nVamos aplicar a normalização **min-max** para padronizar a escala. "},{"metadata":{},"cell_type":"markdown","source":"### Qual a proporção de segurados com sinistros (priori)."},{"metadata":{"trusted":true},"cell_type":"code","source":"id_0 = train[train.target == 0].index\nid_1 = train[train.target == 1].index\nproporção = id_1.shape[0]/train.shape[0]\nprint('Qual a probabilidade a priori de um segurado sinistrar: {}'. format(proporção))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f48e8ee8ed611da6c42de9c595fdaf34ecf5f23b","_cell_guid":"209074dd-ee1f-465c-8cd7-d0b7b692f54f"},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"pre_process\"></a>"},{"metadata":{"_uuid":"a0fc1970db23b67afabe087dd724764f4eef62cf","_cell_guid":"a1a31824-fa60-4140-ab09-72582a6ea446"},"cell_type":"markdown","source":"## Pré-processamento"},{"metadata":{"_uuid":"9304c0f92541692305b9fe77705d55a8ffb7d11a","_cell_guid":"19c7aeed-e009-43fb-a462-4a39707b3b86"},"cell_type":"markdown","source":"### Checando os valores omissos (*missing*)\nOs valores omissos foram representados pelo valor -1."},{"metadata":{"_uuid":"2d4770d2d4b49b2a0b62d84f3e4d57382d36f2cf","_cell_guid":"46adfb57-916f-4cf8-87ae-f03ad861e621","trusted":true},"cell_type":"code","source":"var_missing = []\n\nfor f in train.columns:\n    missings = train[train[f] == -1][f].count()\n    if missings > 0:\n        var_missing.append(f)\n        missings_perc = missings/train.shape[0]\n        \n        print('Variável {} tem {} exemplos ({:.2%}) com valores omissos'.format(f, missings, missings_perc))\n        \nprint('No total, existem {} variáveis com valores omissos'.format(len(var_missing)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"869694ad63ab68d25588768879d26b49d7624d09","_cell_guid":"88daeb5b-9dbf-4e15-af12-e952aac4b874"},"cell_type":"markdown","source":"- **ps_car_03_cat and ps_car_05_cat** têm uma elevada quantidade de dados omissos - optamos pela remoção dessas variáveis.\n- Para a variável **ps_reg_03** e **ps_car_14**, aplicaremos a imputação da média.\n- Enquanto que, para **ps_car_11**, imputaremos a moda (valor mais frequente).\n- Para as demais variáveis categóricas, optamos por deixar o missing como uma característica. "},{"metadata":{"_uuid":"9cbd1e802e35b7d3e21ed696084d12d777210b3f","_cell_guid":"183be251-710e-408f-b72e-1e5f40d1890c","trusted":true},"cell_type":"code","source":"# Excluindo variáveis com muitos dados omissos\nvariaveis_excluir = ['ps_car_03_cat', 'ps_car_05_cat']\ntrain.drop(variaveis_excluir, inplace=True, axis=1)\ntrain.drop(colunas_calc, inplace = True, axis = 1)\n# Imputando com a média e a moda\nmedia_imp = Imputer(missing_values=-1, strategy='mean', axis=0)\nmoda_imp = Imputer(missing_values=-1, strategy='most_frequent', axis=0)\ntrain['ps_reg_03'] = media_imp.fit_transform(train[['ps_reg_03']]).ravel()\ntrain['ps_car_14'] = media_imp.fit_transform(train[['ps_car_14']]).ravel()\ntrain['ps_car_11'] = moda_imp.fit_transform(train[['ps_car_11']]).ravel()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e31feea1985ee709ccc37bc5c6dc31e8632c0070","_cell_guid":"555fdddf-e3c5-4c84-a017-d292eaab07ac"},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"eda\"></a>"},{"metadata":{"_uuid":"d6f7d15dc7b90269ccf1fcdef0762e0352a3fe5b","_cell_guid":"21e70f2e-84f2-422d-9962-b783ae4a4024"},"cell_type":"markdown","source":"## Análise exploratória de dados"},{"metadata":{"_uuid":"e06225d3d3dde0380c7502471ab580ae883329c4","_cell_guid":"b3c403c4-943a-45c2-a0ab-96e72c04fe6d"},"cell_type":"markdown","source":"### Categóricas\nVamos criar gráficos de barras para as variáveis categóricas e analisar como está a distribuição e os dados omissos."},{"metadata":{"trusted":true},"cell_type":"code","source":"# excluir a variável 'ps_car_03_cat' e 'ps_car_05_cat'\ncolunas_cat.remove('ps_car_03_cat')\ncolunas_cat.remove('ps_car_05_cat')\nprint(colunas_cat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in colunas_cat:\n    plt.figure()\n    fig, ax = plt.subplots(figsize = (20,10))\n    sns.barplot(ax = ax, x = i, y = 'target', data = train)\n    plt.ylabel('% target', fontsize = 18)\n    plt.xlabel(i, fontsize = 18)\n    plt.tick_params(axis = 'both', which= 'major', labelsize = 18)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c403f713d0adf7f294c6f8f439e94ad27ba99c74","_cell_guid":"466bf1c0-fc53-40db-9f11-ae81b12b41fb"},"cell_type":"markdown","source":"Verificamos que os dados omissos (missing) são representativos. Dessa forma, optamos por deixá-los como uma categoria adicional, pois o segurado que não apresentou informação apresenta maior probabilidade de sinistrar (acidentar)."},{"metadata":{"_uuid":"21e20154f1c847567ea99b7240305f6a3145368c","_cell_guid":"93facc26-1116-41a7-8bc3-a3de5cc1e0d9"},"cell_type":"markdown","source":"### Variáveis contínuas - Correlação"},{"metadata":{"trusted":true},"cell_type":"code","source":"continuas = [colunas_reg, colunas_car]\ndef correl(t):\n    correlacao = train[t].corr()\n    cmap = sns.diverging_palette(220, 10, as_cmap = True)\n\n    fig, ax = plt.subplots(figsize = (10,10))\n    sns.heatmap(correlacao, cmap = cmap, vmax = 1.0, center = 0, fmt = '.2f',\n           square = True, linewidths = .5, annot = True, cbar_kws ={\"shrink\": .75})\n    plt.show();\n    \n# Variáveis reg\nfor j in continuas:\n    print('Heat Map de correlações para as variáveis {}' .format(j))\n    correl(j)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9dbe7c73bf99bd8526d4b2a3e95e18f0ce8734b4","_cell_guid":"bc2bb0c9-7b8b-4536-9f2a-5b1279475d19"},"cell_type":"markdown","source":"Verificamos correlação forte para as variáveis:\n- ps_reg_02 e ps_reg_03 (0,7)\n- ps_car_12 e ps_car_13 (0,67)\n- ps_car_12 e ps_car_14 (0,58)\n- ps_car_13 and ps_car_15 (0,67)"},{"metadata":{},"cell_type":"markdown","source":"#### Codificação das variáveis 'int64' como 'category' \nFizemos a codificação em `category` para realizar o procedimento de one-hot-encoding das variáveis categóricas."},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in train.columns:\n    if train[i].dtype == 'int64' and i != 'target':\n        train[i] = train[i].astype('category')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checando\ntrain.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a617be4ecc6c94a2a6fde837377b4915b3f372a0","_cell_guid":"c5137cbb-6779-4aad-898c-19a5ba815a9c"},"cell_type":"markdown","source":"### Realizando one-hot-encoding nas variáveis categóricas\nPara as variáveis que possuem mais de 2 categorias, realizamos o processo de one-hot-encoding que é a criação de atributos (variáveis) para cada categoria da variável. Essas variáveis serão binárias, assumindo o valor 1 quando da presença da categoria, e 0, na ausência."},{"metadata":{"scrolled":true,"_uuid":"12554d624df68deff6fd0e9c5f90a61c1e1c83c0","_cell_guid":"6606cf2b-9082-4d5e-b278-2779780258a6","trusted":true},"cell_type":"code","source":"# função get_dummies transforma as categorias em variáveis binárias\ntrain = pd.get_dummies(train)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checando a dimensão\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"50174c5dd05a9fafbe4a12a604dbb7951235774f","_cell_guid":"afa36be5-c104-4b7e-bf68-3a6a52cd36ce"},"cell_type":"markdown","source":"**Criamos as variáveis binárias. "},{"metadata":{},"cell_type":"markdown","source":"Vamos separar a variável **target** dos atributos, criando uma matriz X e um vetor Y."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train.drop([\"target\"], axis = 1)\ny = train[\"target\"]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"58b2f554c11134975e5d1d8ef3d77ea3fbbaa46f","_cell_guid":"fb9265c9-771b-4ca7-a889-9b37b9eab6c0"},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"modelos\"></a>"},{"metadata":{},"cell_type":"markdown","source":"## Modelos"},{"metadata":{},"cell_type":"markdown","source":"#### Divindindo os dados em treinamento e teste\nVamos dividir os dados em treinamento e teste para realizar a avaliação por meio da validação cruzada com o intuito de melhorar a capacidade de generalização dos classificadores. \nUsamos a função `train_test_split` do sklearn com a amostragem estratificada para manter a proporção da variável target. "},{"metadata":{},"cell_type":"markdown","source":"#### Normalização min-max "},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(\n    X, y, stratify=y, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = MinMaxScaler()\nscaler.fit(X)\nX = scaler.transform(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Coeficiente de Gini Normalizado"},{"metadata":{},"cell_type":"markdown","source":"Utilizamos a implementação do coeficiente de gini normalizado em python obtido neste tópico: http://www.kaggle.com/c/ClaimPredictionChallenge/discussion/703"},{"metadata":{"trusted":true},"cell_type":"code","source":"def gini(actual, pred):\n    assert (len(actual) == len(pred))\n    all = np.asarray(np.c_[actual, pred, np.arange(len(actual))], dtype=np.float)\n    all = all[np.lexsort((all[:, 2], -1 * all[:, 1]))]\n    totalLosses = all[:, 0].sum()\n    giniSum = all[:, 0].cumsum().sum() / totalLosses\n\n    giniSum -= (len(actual) + 1) / 2.\n    return giniSum / len(actual)\n\n\ndef gini_normalized(actual, pred):\n    return gini(actual, pred) / gini(actual, actual)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Regressão logística"},{"metadata":{},"cell_type":"markdown","source":"Primeiro teste da regressão logística com a penalização L2. \nUsamos o pipeline para fazer a normalização min-max."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Testando com a regressão logística com penalização L2\nlr = LogisticRegression(penalty='l2', random_state=1)\nlr.fit(X_train, y_train)\nprob = lr.predict_proba(X_test)[:,1]\nprint(\"Índice de Gini normalizado para a Regressão Logística: \",gini_normalized(y_test, prob))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest"},{"metadata":{},"cell_type":"markdown","source":"Testamos também o Random Forest com 200 árvores, 15 atributos e com profundidade máxima de 4 nós."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Forest com 20 árvores\nrf = RandomForestClassifier(n_estimators = 20, max_depth = 4, random_state = 1, max_features = 20)\nrf.fit(X_train, y_train)\npredictions_prob = rf.predict_proba(X_test)[:,1]\nprint(\"Índice de Gini normalizado para o Random Forest: \", gini_normalized(y_test, predictions_prob))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XGBoost "},{"metadata":{},"cell_type":"markdown","source":"Testamos o XGBoost com 100 estimações, profundidade máxima de 5, e taxa de aprendizagem de 0.05."},{"metadata":{"trusted":true},"cell_type":"code","source":"# taxa de aprendizagem = 0.05\nxgbm = XGBClassifier(max_depth=5, n_estimators=100, learning_rate=0.05, random_state = 1)\nxgbm.fit(X_train, y_train)\nprob_xgb = xgbm.predict_proba(X_test)[:,1]\nprint(\"--------------------------------------------------------------------------------------------\")\nprint(\"Índice de Gini normalizado para o XGBoost com learning_rate = 0.05: \", gini_normalized(y_test, prob_xgb))\nprint(\"--------------------------------------------------------------------------------------------\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Optamos por tentar submeter o modelo com melhor score nos dados X_test - XGBoost com . \n# Testaremos nos dados completo de treinamento.\nprob_xgb_y = xgbm.predict_proba(X)[:,1]\nprint(\"--------------------------------------------------------------------------------------------\")\nprint(\"Índice de Gini normalizado para o XGBoost com learning_rate = 0.05 dados treino: \", gini_normalized(y, prob_xgb_y))\nprint(\"--------------------------------------------------------------------------------------------\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Aplicar esses procedimentos aos dados de teste"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Excluindo variáveis com muitos dados omissos\nvariaveis_excluir = ['ps_car_03_cat', 'ps_car_05_cat']\ntest.drop(variaveis_excluir, inplace=True, axis=1)\ntest.drop(colunas_calc, inplace = True, axis = 1)\n# Imputando com a média e a moda\nmedia_imp = Imputer(missing_values=-1, strategy='mean', axis=0)\nmoda_imp = Imputer(missing_values=-1, strategy='most_frequent', axis=0)\ntest['ps_reg_03'] = media_imp.fit_transform(test[['ps_reg_03']]).ravel()\ntest['ps_car_14'] = media_imp.fit_transform(test[['ps_car_14']]).ravel()\ntest['ps_car_11'] = moda_imp.fit_transform(test[['ps_car_11']]).ravel()\n\n# categorizando \nfor i in test.columns:\n    if test[i].dtype == 'int64' and i != 'target':\n        test[i] = test[i].astype('category')\n        \n# função get_dummies transforma as categorias em variáveis binárias\ntest = pd.get_dummies(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Normalização\nscaler = MinMaxScaler()\nscaler.fit(test)\ntest = scaler.transform(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Aplicando no modelo XGBoost\ny_test = xgbm.predict_proba(test)[:,1]\n\n# Em results_df está a base de teste escorada, a coluna target possui as probabilidades\nresults_df = pd.DataFrame(data={'id':test_id, 'target':y_test})\nprint(results_df)\nresults_df.to_csv('submissão_1.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Verificamos que os modelos não produziram bons resultados em comparação com o score da competição.\nPortanto, vamos tentar os modelos modelos com os dados sem pré-processamento, apenas com a normalização min-max. \nOptamos por essa abordagem em razão de que os competidores que alcançaram bons resultados não excluíram variáveis, e não realizaram a exclusão de dados missing. "},{"metadata":{},"cell_type":"markdown","source":"### Testando com dados brutos"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Testando nos dados sem exclusão de variáveis\n#train = pd.read_csv('../input/train.csv')\n#X = train.iloc[:,2:]\n#y = train.iloc[:,1:2]\n#X = train.drop([\"id\",\"target\"], axis = 1)\n#y = train[\"target\"]\n# Separando um conjunto para avaliar\n#X_train, X_test, y_train, y_test = train_test_split(\n#    X, y, stratify=y, random_state=0)\n# Normalização Min-Max\n#scaler = MinMaxScaler()\n#scaler.fit(X_train)\n#X_train = scaler.transform(X_train)\n#scaler = MinMaxScaler()\n#scaler.fit(X_test)\n#X_test = scaler.transform(X_test)\n# Modelos\n# Regressão Logística\n# Testando com a regressão logística com penalização L2\n#lr = LogisticRegression(penalty='l2', random_state=1)\n#lr.fit(X_train, y_train)\n#prob_lr = lr.predict_proba(X_test)[:,1]\n#print(\"--------------------------------------------------------------------------------------------\")\n#print(\"Índice de Gini normalizado para a Regressão Logística: \",gini_normalized(y_test, prob_lr))\n#print(\"--------------------------------------------------------------------------------------------\")\n# Random Forest com 200 árvores\n#rf = RandomForestClassifier(n_estimators = 200, max_depth = 4, random_state = 1, max_features = 15)\n#rf.fit(X_train, y_train)\n#prob_rf = rf.predict_proba(X_test)[:,1]\n#print(\"--------------------------------------------------------------------------------------------\")\n#print(\"Índice de Gini normalizado para o Random Forest: \", gini_normalized(y_test, prob_rf))\n#print(\"--------------------------------------------------------------------------------------------\")\n# XGBoost \n#xgbm = XGBClassifier(max_depth=5, n_estimators=100, learning_rate=0.05, random_state = 1)\n#xgbm.fit(X_train, y_train)\n#prob_xgb = xgbm.predict_proba(X_test)[:,1]\n#print(\"--------------------------------------------------------------------------------------------\")\n#print(\"Índice de Gini normalizado para o XGBoost: \", gini_normalized(y_test, prob_xgb))\n#print(\"--------------------------------------------------------------------------------------------\")\n# LightGBM\n#lgb = LGBMClassifier(n_estimators = 100, learning_rate = 0.02, subsample = 0.7, num_leaves = 15, seed = 1)\n#lgb.fit(X_train, y_train)\n#prob_lgb = lgb.predict_proba(X_test)[:, 1]\n#print(\"--------------------------------------------------------------------------------------------\")\n#print(\"Índice de Gini normalizado para o LightGBM: \", gini_normalized(y_test, prob_lgb))\n#print(\"--------------------------------------------------------------------------------------------\")\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Percebe-se claramente que os modelos apresentaram maiores valores para o coeficiente de gini normalizado, isto é, o tratamento dos dados (exclusão de variáveis e imputação de média e moda) não ajudou. \nDessa forma, tentaremos rodar um autoencoder que serve para reduzir a dimensionalidade dos atributos. "},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"autoencoder\"></a>"},{"metadata":{},"cell_type":"markdown","source":"# Autoencoder"},{"metadata":{},"cell_type":"markdown","source":"#### Importação das bibliotecas necessárias"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom pandas import read_csv, DataFrame\nimport numpy as np\nfrom numpy.random import seed\nfrom sklearn.preprocessing import minmax_scale\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import datasets\nfrom keras.layers import Input, Dense\nfrom keras.models import Model\nfrom matplotlib import pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Carregamento e normalização da base de treinamento"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Carregamento das bases de treinamento e teste em dataframes\ntrain = pd.read_csv('../input/train.csv')\n\nprint(train.shape)\n\n# X armazena dos dados em um dataframe\nX = train.iloc[:,2:]\n# y armazena os labels em um dataframe\ny = train.iloc[:,1:2]\n\n# target_names armazena os valores distintos dos labels\ntarget_names = train['target'].unique()\n\n# Normaliza os dados de treinamento\nscaler = MinMaxScaler()\nscaler.fit(X)\nX_scaled = scaler.transform(X)\n\nprint(X_scaled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Número de Colunas: \", X.shape[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Configuração e execução do treinamento do Autoencoder"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Criação do AutoEncoder com 3 neurônios na camada escondida usando Keras.\n#input_dim = X_scaled.shape[1]\n\n# Definição do número de variáveis resultantes do Encoder\n#encoding_dim = 10\n\n#input_data = Input(shape=(input_dim,))\n\n# Configurações do Encoder\n#encoded = Dense(encoding_dim, activation='linear')(input_data)\n#encoded = Dense(encoding_dim, activation='sgmoid')(input_data)\n#encoded = Dense(encoding_dim, activation='relu', activity_regularizer=regularizers.l1(10e-5))(input_data)\n\n#encoded1 = Dense(20, activation = 'relu')(input_data)\n#encoded2 = Dense(10, activation = 'relu')(encoded1)\n#encoded3 = Dense(5, activation = 'relu')(encoded2)\n#encoded4 = Dense(encoding_dim, activation = 'relu')(encoded3)\n\n# Configurações do Decoder\n#decoded = Dense(input_dim, activation='linear')(encoded)\n#decoded = Dense(input_dim, activation='sgmoid')(encoded)\n\n#decoded1 = Dense(5, activation = 'relu')(encoded4)\n#decoded2 = Dense(10, activation = 'relu')(decoded1)\n#decoded3 = Dense(20, activation = 'relu')(decoded2)\n#decoded4 = Dense(input_dim, activation = 'sigmoid')(decoded3)\n\n# Combinando o Encoder e o Decoder em um modelo AutoEncoder\n#autoencoder = Model(input_data, decoded4)\n#autoencoder.compile(optimizer='adam', loss='mse')\n#print(autoencoder.summary())\n# Treinamento de fato - Definição de alguns parâmetros como número de épocas, batch size, por exemplo.\n#history = autoencoder.fit(X_scaled, X_scaled, epochs=30, batch_size=256, shuffle=True, validation_split=0.1, verbose = 1)\n\n#plot our loss \n#plt.plot(history.history['loss'])\n#plt.plot(history.history['val_loss'])\n#plt.title('Model Train vs Validation Loss')\n#plt.ylabel('Loss')\n#plt.xlabel('Epoch')\n#plt.legend(['Train', 'Validation'], loc='upper right')\n#plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Utilização do Encoder gerado para realizar a compressão e reduzir a dimensão da base de treinamento"},{"metadata":{"trusted":true},"cell_type":"code","source":"#test = pd.read_csv('../input/test.csv')\n\n#print(test.shape)\n\n# X armazena dos dados em um dataframe\n#X = test.iloc[:,1:]\n\n# Normaliza os dados de treinamento\n#scaler = MinMaxScaler()\n#scaler.fit(X)\n#X_scaled = scaler.transform(X)\n\n# Utilizar o Encoder para codificar os dados de entrada\n#encoder = Model(input_data, encoded4)\n#encoded_data = encoder.predict(X_scaled)\n\n#print(encoded_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"modelosauto\"></a>"},{"metadata":{},"cell_type":"markdown","source":"## Avaliação dos Modelos após o Autoencoder"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Carregamento das bases de treinamento e teste em dataframes\n#train = pd.read_csv('../input/train.csv')\n#X = train.drop([\"id\",\"target\"], axis = 1)\n#y = train[\"target\"]\n\n# Separando um conjunto para avaliar\n#X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\n\n# Normalização Min-Max\n#scaler = MinMaxScaler()\n#scaler.fit(X_train)\n#X_train = scaler.transform(X_train)\n#scaler = MinMaxScaler()\n#scaler.fit(X_test)\n#X_test = scaler.transform(X_test)\n\n# aplicando autoencoder nos dados de treinamento e teste\n#encoder = Model(input_data, encoded4)\n#encoded_data_train = encoder.predict(X_train)\n#encoder = Model(input_data, encoded4)\n#encoded_data_test = encoder.predict(X_test)\n\n# Modelos\n# Regressão Logística\n#lr = LogisticRegression(penalty='l2', random_state=1)\n#lr.fit(encoded_data_train, y_train)\n#prob_lr = lr.predict_proba(encoded_data_test)[:,1]\n#print(\"--------------------------------------------------------------------------------------------\")\n#print(\"Índice de Gini normalizado para a Regressão Logística com autoencoder: \",gini_normalized(y_test, prob_lr))\n#print(\"--------------------------------------------------------------------------------------------\")\n\n\n# Random Forest com 200 árvores\n#rf = RandomForestClassifier(n_estimators = 200, max_depth = 5, random_state = 1, max_features = 7)\n#rf.fit(encoded_data_train, y_train)\n#prob_rf = rf.predict_proba(encoded_data_test)[:,1]\n#print(\"--------------------------------------------------------------------------------------------\")\n#print(\"Índice de Gini normalizado para o Random Forest com autoencoder: \", gini_normalized(y_test, prob_rf))\n#print(\"--------------------------------------------------------------------------------------------\")\n\n\n# XGBoost \n#import xgboost as xgb\n#xgbm = xgb.XGBClassifier(max_depth=5, n_estimators=100, learning_rate=0.05, random_state = 1)\n#xgbm.fit(encoded_data_train, y_train)\n#prob_xgb = xgbm.predict_proba(encoded_data_test)[:,1]\n#print(\"--------------------------------------------------------------------------------------------\")\n#print(\"Índice de Gini normalizado para o XGBoost com autoencoder: \", gini_normalized(y_test, prob_xgb))\n#print(\"--------------------------------------------------------------------------------------------\")\n\n\n# LightGBM\n#from lightgbm import LGBMClassifier\n#lgb = LGBMClassifier(n_estimators = 100, learning_rate = 0.02, subsample = 0.7, num_leaves = 15, seed = 1)\n#lgb.fit(encoded_data_train, y_train)\n#prob_lgb = lgb.predict_proba(encoded_data_test)[:, 1]\n#print(\"--------------------------------------------------------------------------------------------\")\n#print(\"Índice de Gini normalizado para o LightGBM com autoencoder: \", gini_normalized(y_test, prob_lgb))\n#print(\"--------------------------------------------------------------------------------------------\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"xgboost_grid\"></a>\n# XGBoost com Grid Search"},{"metadata":{"trusted":true},"cell_type":"code","source":"#import warnings\n#warnings.filterwarnings('ignore')\n#import numpy as np\n#import pandas as pd\n#from datetime import datetime\n#from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n#from sklearn.metrics import roc_auc_score\n#from sklearn.model_selection import StratifiedKFold\n#from sklearn.model_selection import train_test_split\n#from xgboost import XGBClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#def timer(start_time=None):\n#    if not start_time:\n#        start_time = datetime.now()\n#        return start_time\n#    elif start_time:\n#        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n#        tmin, tsec = divmod(temp_sec, 60)\n#        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))\n\n\n# Carregamento das bases de treinamento e teste em dataframes\n#train = pd.read_csv('../input/train.csv', dtype={'id': np.int32, 'target': np.int8})\n#X = train.drop([\"id\",\"target\"], axis = 1)\n#Y = train[\"target\"].values\n\n# Separando um conjunto para avaliar\n#X_train, X_test, y_train, y_test = train_test_split(X, Y, stratify=Y, random_state=0)\n\n######################################\n\n#train_df = pd.read_csv('../input/train.csv', dtype={'id': np.int32, 'target': np.int8})\n#Y = train_df['target'].values\n#X = train_df.drop(['target', 'id'], axis=1)\n#test_df = pd.read_csv('../input/test.csv', dtype={'id': np.int32})\n#test = test_df.drop(['id'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Grid de Parâmetros para o XGBoost\n#params = {\n    #    'min_child_weight': [1, 5, 10],\n   #     'gamma': [0.5, 1, 1.5, 2, 5],\n  #      'subsample': [0.6, 0.8, 1.0],\n #       'colsample_bytree': [0.6, 0.8, 1.0],\n#        'max_depth': [3, 4, 5]}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#xgb = XGBClassifier(learning_rate=0.02, n_estimators=100, objective='binary:logistic', silent=True, nthread=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Número de folds para Cross-Validation\n#folds = 2\n\n# Número de combinações a serem feitos no Grid Search. No Total podem ser feitas 3x5x3x3x3 = 405 combinações. Quantos mais combinações, mais tempo leva.\n#param_comb = 1\n\n# Configuração dos folds estratificados para o Cross-Validation\n#skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)\n\n# Configuração do Grid Search\n#random_search = RandomizedSearchCV(xgb, param_distributions=params, n_iter=param_comb, scoring='roc_auc', n_jobs=4, cv=skf.split(X,Y), verbose=3, random_state=1001 )\n#grid = GridSearchCV(estimator=xgb, param_grid=params ,scoring='roc_auc', n_jobs=4, cv=skf.split(X,Y), verbose=3)\n\n# Execução do Treinamento com Grid Search\n#start_time = timer(None) # timing starts from this point for \"start_time\" variable\n#random_search.fit(X, Y)\n#grid.fit(X, Y)\n#timer(start_time) # timing ends here for \"start_time\" variable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print('\\n All results:')\n#print(random_search.cv_results_)\n#print('\\n Best estimator:')\n#print(random_search.best_estimator_)\n#print('\\n Best normalized gini score for %d-fold search with %d parameter combinations:' % (folds, param_comb))\n#print(random_search.best_score_ * 2 - 1)\n#print('\\n Best hyperparameters:')\n#print(random_search.best_params_)\n#results = pd.DataFrame(random_search.cv_results_)\n#results.to_csv('xgb-random-grid-search-results-01.csv', index=False)\n\n# print('\\n All results:')\n# print(grid.cv_results_)\n# print('\\n Best estimator:')\n# print(grid.best_estimator_)\n# print('\\n Best score:')\n# print(grid.best_score_ * 2 - 1)\n# print('\\n Best parameters:')\n# print(grid.best_params_)\n# results = pd.DataFrame(grid.cv_results_)\n# results.to_csv('xgb-grid-search-results-01.csv', index=False)\n\n# y_test = grid.best_estimator_.predict_proba(test)\n# results_df = pd.DataFrame(data={'id':test_df['id'], 'target':y_test[:,1]})\n# results_df.to_csv('submission-grid-search-xgb-porto-01.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predict_proba já utiliza o modelo com os melhores hyperparâmetros para realizar o predict da base de teste.\n#y_test = random_search.predict_proba(test)\n\n# Em results_df está a base de teste escorada, a coluna target possui as probabilidades\n#results_df = pd.DataFrame(data={'id':test_df['id'], 'target':y_test[:,1]})\n#print(results_df)\n#results_df.to_csv('submission-random-grid-search-xgb-porto-01.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## TESTE - FINAL"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Carregando Bibliotecas\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.model_selection import KFold, StratifiedKFold, cross_val_score, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\npd.set_option('display.max_columns', 100)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Computing gini coefficient ( Coursey Kaggle)\n# from CPMP's kernel https://www.kaggle.com/cpmpml/extremely-fast-gini-computation @jit\n#def eval_gini(y_true, y_prob):\n#    y_true = np.asarray(y_true)\n#    y_true = y_true[np.argsort(y_prob)]\n#    ntrue = 0\n#    gini = 0\n#    delta = 0\n#    n = len(y_true)\n#    for i in range(n-1, -1, -1):\n#        y_i = y_true[i]\n#        ntrue += y_i\n#        gini += y_i * delta\n#        delta += 1 - y_i\n#    gini = 1 - 2 * gini / (ntrue * (n - ntrue))\n#    return gini","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# IMPORTAÇÃO DOS DADOS\n#train = pd.read_csv('../input/train.csv')\n#test = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Separando X e y\n#X = train.drop([\"id\",\"target\"], axis = 1)\n#y = train[\"target\"].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Excluindo variáveis com 'calc'\n#colunas = X.columns.tolist()\n#colunas_cat = [col for col in colunas if 'cat' in col]\n#colunas_calc = [col for col in colunas if 'calc' in col]\n\n#variaveis_excluir = colunas_calc\n#X.drop(variaveis_excluir, inplace=True, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# One-hot nas categóricas\n#X = pd.get_dummies(X)\n#X.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Separando um conjunto para avaliar\n#X_train, X_test, y_train, y_test = train_test_split(\n#    X, y, stratify=y, random_state=0)\n# Normalização Min-Max\n#scaler = MinMaxScaler()\n#scaler.fit(X_train)\n#X_train = scaler.transform(X_train)\n#scaler = MinMaxScaler()\n#scaler.fit(X_test)\n#X_test = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Testar os modelos\n# Modelos\n# Regressão Logística\n# Testando com a regressão logística com penalização L2\n#lr = LogisticRegression(penalty='l2', random_state=1)\n#lr.fit(X_train, y_train)\n#prob_lr = lr.predict_proba(X_test)[:,1]\n#print(\"--------------------------------------------------------------------------------------------\")\n#print(\"Índice de Gini normalizado para a Regressão Logística: \",eval_gini(y_test, prob_lr))\n#print(\"--------------------------------------------------------------------------------------------\")\n# Random Forest com 200 árvores\n#rf = RandomForestClassifier(n_estimators = 200, max_depth = 4, random_state = 1, max_features = 15)\n#rf.fit(X_train, y_train)\n#prob_rf = rf.predict_proba(X_test)[:,1]\n#print(\"--------------------------------------------------------------------------------------------\")\n#print(\"Índice de Gini normalizado para o Random Forest: \", eval_gini(y_test, prob_rf))\n#print(\"--------------------------------------------------------------------------------------------\")\n# XGBoost \n#xgbm = XGBClassifier(max_depth=4, n_estimators=100, learning_rate=0.05, random_state = 1)\n#xgbm.fit(X_train, y_train)\n#prob_xgb = xgbm.predict_proba(X_test)[:,1]\n#print(\"--------------------------------------------------------------------------------------------\")\n#print(\"Índice de Gini normalizado para o XGBoost: \", eval_gini(y_test, prob_xgb))\n#print(\"--------------------------------------------------------------------------------------------\")\n# LightGBM\n#lgb = LGBMClassifier(n_estimators = 100, learning_rate = 0.02, subsample = 0.7, num_leaves = 15, seed = 1)\n#lgb.fit(X_train, y_train)\n#prob_lgb = lgb.predict_proba(X_test)[:, 1]\n#print(\"--------------------------------------------------------------------------------------------\")\n#print(\"Índice de Gini normalizado para o LightGBM: \", eval_gini(y_test, prob_lgb))\n#print(\"--------------------------------------------------------------------------------------------\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Testando na base de treinamento completa\n# Normalização Min-Max\n#scaler = MinMaxScaler()\n#scaler.fit(X)\n#X = scaler.transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# base completa\n# Modelos\n#prob_lr = lr.predict_proba(X)[:,1]\n#print(\"--------------------------------------------------------------------------------------------\")\n#print(\"Índice de Gini normalizado para a Regressão Logística: \",eval_gini(y, prob_lr))\n#print(\"--------------------------------------------------------------------------------------------\")\n#prob_rf = rf.predict_proba(X)[:,1]\n#print(\"--------------------------------------------------------------------------------------------\")\n#print(\"Índice de Gini normalizado para o Random Forest: \", eval_gini(y, prob_rf))\n#print(\"--------------------------------------------------------------------------------------------\")\n#prob_xgb = xgbm.predict_proba(X)[:,1]\n#print(\"--------------------------------------------------------------------------------------------\")\n#print(\"Índice de Gini normalizado para o XGBoost: \", eval_gini(y, prob_xgb))\n#print(\"--------------------------------------------------------------------------------------------\")\n#prob_lgb = lgb.predict_proba(X)[:, 1]\n#print(\"--------------------------------------------------------------------------------------------\")\n#print(\"Índice de Gini normalizado para o LightGBM: \", eval_gini(y, prob_lgb))\n#print(\"--------------------------------------------------------------------------------------------\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Criando o arquivo para submissão\n#test_df = pd.read_csv('../input/test.csv', dtype={'id': np.int32})\n#test = test_df.drop(['id'], axis=1)\n#test.drop(variaveis_excluir, inplace=True, axis=1)\n#test = pd.get_dummies(test)\n#scaler = MinMaxScaler()\n#scaler.fit(test)\n#test = scaler.transform(test)\n#y_test = xgbm.predict_proba(test)[:,1]\n\n# Em results_df está a base de teste escorada, a coluna target possui as probabilidades\n#results_df = pd.DataFrame(data={'id':test_df['id'], 'target':y_test})\n#print(results_df)\n#results_df.to_csv('submission-random-grid-search-xgb-01.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"},"language_info":{"nbconvert_exporter":"python","name":"python","pygments_lexer":"ipython3","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"mimetype":"text/x-python","version":"3.6.3"}},"nbformat":4,"nbformat_minor":1}