{"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"name":"python","version":"3.6.3","nbconvert_exporter":"python","pygments_lexer":"ipython3","file_extension":".py","mimetype":"text/x-python"},"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"}},"cells":[{"execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"7ddd0764-1f6b-4f72-b3d4-692f829e99c1","_uuid":"fad0cc2d3b352084b07bb30e4091379bf163f5c9"},"outputs":[],"cell_type":"code","source":"\"\"\"\nReferences or inspired by\n1. https://www.kaggle.com/sudosudoohio/stratified-kfold-xgboost-eda-tutorial-0-281?scriptVersionId=1579846\n2. https://www.kaggle.com/youhanlee/eda-stratifiedshufflesplit-xgboost-for-starter?scriptVersionId=1583200\n\"\"\"\nimport pandas as pd\nimport numpy as np\nimport warnings\nimport matplotlib.pyplot as plt   \nimport seaborn as sns             # not used\n\nfrom sklearn import preprocessing \nfrom sklearn.model_selection import StratifiedKFold\nimport xgboost as xgb\n\n%matplotlib inline\nwarnings.filterwarnings('ignore')"},{"execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"581eab18-3129-4600-ab99-3a255b37e5e4","_uuid":"b45b5378a8c326e3a12b1c56939c17efd46bbf7b"},"outputs":[],"cell_type":"code","source":"# common gini metric for the gradient boost\n# Define the gini metric - from https://www.kaggle.com/c/ClaimPredictionChallenge/discussion/703#5897\ndef gini(actual, pred, cmpcol = 0, sortcol = 1):\n    assert( len(actual) == len(pred) )\n    all = np.asarray(np.c_[ actual, pred, np.arange(len(actual)) ], dtype=np.float)\n    all = all[ np.lexsort((all[:,2], -1*all[:,1])) ]\n    totalLosses = all[:,0].sum()\n    giniSum = all[:,0].cumsum().sum() / totalLosses\n    \n    giniSum -= (len(actual) + 1) / 2.\n    return giniSum / len(actual)\n \ndef gini_normalized(a, p):\n    return gini(a, p) / gini(a, a)\n\ndef gini_xgb(preds, dtrain):\n    labels = dtrain.get_label()\n    gini_score = gini_normalized(labels, preds)\n    return 'gini', gini_score"},{"execution_count":null,"metadata":{"_cell_guid":"260c3530-ce62-4a4a-bafe-e375446dc482","_uuid":"e970d64f273cf88399005f3693db6114691d77fc"},"outputs":[],"cell_type":"code","source":"# train and test data\ntrain = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")\ntrain.head(10)\ntest.head(10)"},{"execution_count":null,"metadata":{"_cell_guid":"10928ad1-7e12-40b5-97d5-076b89ec74de","_uuid":"c0291b678326726ec9245add654091c37fc693eb"},"outputs":[],"cell_type":"code","source":"# pre-processing, null data\ntrain_null = train.isnull().values.any()\nprint(train_null)\n\n# features and drop related columns \nfeatures = train.drop(['id','target'], axis=1).values\ntargets = train.target.values\ndrop_columns = train.columns[train.columns.str.startswith('ps_calc_')]\ntrain = train.drop(drop_columns, axis=1)  \ntest = test.drop(drop_columns, axis=1)  "},{"execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"2e27086c-2e83-4aeb-a4df-ccbb4c949f4a","_uuid":"90b8f32639454e347e157571e9cc23c1273b28c6"},"outputs":[],"cell_type":"code","source":"# model setting\nkfold = 5\nskf = StratifiedKFold(n_splits=kfold, random_state=99)\nparams = {\n    'min_child_weight': 10.0,\n    'objective': 'binary:logistic',\n    'max_depth': 7,\n    'max_delta_step': 1.8,\n    'colsample_bytree': 0.4,\n    'subsample': 0.8,\n    'eta': 0.025,\n    'gamma': 0.65,\n    'num_boost_round' : 700\n    }\n\nX = train.drop(['id', 'target'], axis=1).values\ny = train.target.values\ntest_id = test.id.values\ntest = test.drop('id', axis=1)"},{"execution_count":null,"metadata":{"_cell_guid":"c9ef1da6-a3bb-435d-a258-fcd0ca739886","_uuid":"bc3cd0c6f8f7e95f515576d6aac3a063c6de8683"},"outputs":[],"cell_type":"code","source":"# submission preparation\nsub = pd.DataFrame()\nsub['id'] = test_id\nsub['target'] = np.zeros_like(test_id)\n\nfor i, (train_index, test_index) in enumerate(skf.split(X, y)):\n    print('[Fold %d/%d]' % (i + 1, kfold))\n    X_train, X_valid = X[train_index], X[test_index]\n    y_train, y_valid = y[train_index], y[test_index]\n    \n    # Convert our data into XGBoost format\n    d_train = xgb.DMatrix(X_train, y_train)\n    d_valid = xgb.DMatrix(X_valid, y_valid)\n    d_test = xgb.DMatrix(test.values)\n    watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n\n    # Train the model! We pass in a max of 2,000 rounds (with early stopping after 100)\n    # and the custom metric (maximize=True tells xgb that higher metric is better)\n    mdl = xgb.train(params, d_train, 1600, watchlist, early_stopping_rounds=70, feval=gini_xgb, maximize=True, verbose_eval=100)\n\n    print('[Fold %d/%d Prediciton:]' % (i + 1, kfold))\n    # Predict on our test data\n    p_test = mdl.predict(d_test)\n    sub['target'] += p_test/kfold"},{"execution_count":null,"metadata":{},"outputs":[],"cell_type":"code","source":"# csv file\nsub.to_csv('StratifiedKFold.csv', index=False)"}],"nbformat_minor":1,"nbformat":4}