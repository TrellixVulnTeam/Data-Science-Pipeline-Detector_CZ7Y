{"cells":[{"metadata":{},"cell_type":"markdown","source":"# (Top 9th) LightGBM & XGBoost Ensemble Modeling\n## This is LightGBM and XGBoost ensemble modeling notebook. This model reaches the top 9th. I appreciate if you upvote!\n## I also shared [Basic and Informative EDA Notebook](https://www.kaggle.com/werooring/basic-eda-for-everyone). It is quite easy so that even beginners can understand\n- [reference notebook](https://www.kaggle.com/xiaozhouwang/2nd-place-lightgbm-solution)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\n# file path\ndata_path = '/kaggle/input/porto-seguro-safe-driver-prediction/'\n\ntrain = pd.read_csv(data_path + 'train.csv', index_col='id')\ntest = pd.read_csv(data_path + 'test.csv', index_col='id')\nsubmission = pd.read_csv(data_path + 'sample_submission.csv', index_col='id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data = pd.concat([train, test], ignore_index=True)\nall_data = all_data.drop('target', axis=1) # Remove target value\n\nall_features = all_data.columns.tolist() # All features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"### Generate missing values as a new feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add 'number of missing values per data' as a new feature\nall_data['num_missing'] = (all_data==-1).sum(axis=1)\n\n# Features excluding nominal feature, features with calc on tag\nremaining_features = [col for col in all_features \\\n                      if ('cat' not in col and 'calc' not in col)] \n# Add num_missin to remaining_features\nremaining_features.append('num_missing')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Apply One-Hot Encoding to nominal features"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\ncat_features = [col for col in all_features if 'cat' in col] # Nominal features\n\n# Apply One-Hot encoding\nonehot_encoder = OneHotEncoder()\nencoded_cat_matrix = onehot_encoder.fit_transform(all_data[cat_features]) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create a new features `mix_ind` that combines unique values of an `ind` features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature with 'ind' on tag\nind_features = [col for col in all_features if 'ind' in col]\n\nfirst_col=True\nfor col in ind_features:\n    if first_col:\n        all_data['mix_ind'] = all_data[col].astype(str)+'_'\n        first_col = False\n    else:\n        all_data['mix_ind'] += all_data[col].astype(str)+'_'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create new feature, the number of eigenvalues for nominal features"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_count_features = []\nfor col in cat_features+['mix_ind']:\n    val_counts_dic = all_data[col].value_counts().to_dict()\n    all_data[f'{col}_count'] = all_data[col].apply(lambda x: val_counts_dic[x])\n    cat_count_features.append(f'{col}_count')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Final dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import sparse\n\ndrop_features = ['ps_ind_14', 'ps_ind_10_bin','ps_ind_11_bin', \n                 'ps_ind_12_bin','ps_ind_13_bin','ps_car_14']\n\n# Data to remove drop_features from remaining_features, cat_count_features\nall_data_remaining = all_data[remaining_features+cat_count_features].drop(drop_features, axis=1)\n\n# Concatenate Data\nall_data_sprs = sparse.hstack([sparse.csr_matrix(all_data_remaining),\n                               encoded_cat_matrix],\n                              format='csr')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_train = train.shape[0] # Number of train data \n\n# Divide train data and test data\nX = all_data_sprs[:num_train]\nX_test = all_data_sprs[num_train:]\n\ny = train['target'].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluation Matrics"},{"metadata":{"trusted":true},"cell_type":"code","source":"def eval_gini(y_true, y_pred):\n    # Verify that the actual and predicted values are the same size (different values raise errors)\n    assert y_true.shape == y_pred.shape\n\n    n_samples = y_true.shape[0] # Number of data\n    L_mid = np.linspace(1 / n_samples, 1, n_samples) # Diagonal value\n\n    # 1) Gini coefficient for predicted values\n    pred_order = y_true[y_pred.argsort()] # Sort y_true values by y_pred size\n    L_pred = np.cumsum(pred_order) / np.sum(pred_order) # Lorentz Curve\n    G_pred = np.sum(L_mid - L_pred) # Gini coefficient for predicted values\n\n    # 2) Gini coefficient when prediction is perfect\n    true_order = y_true[y_true.argsort()] # Sort y_true values by y_true size\n    L_true = np.cumsum(true_order) / np.sum(true_order) # Lorentz Curve\n    G_true = np.sum(L_mid - L_true) #  Gini coefficient when prediction is perfect\n\n    # Normalized Gini coefficient\n    return G_pred / G_true","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def gini_lgb(preds, dtrain):\n    labels = dtrain.get_label()\n    return 'gini', eval_gini(labels, preds), True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def gini_xgb(preds, dtrain):\n    labels = dtrain.get_label()\n    return 'gini', eval_gini(labels, preds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\n\n# Create Stratified K Fold Cross-Verifier\nfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1991)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LightGBM"},{"metadata":{"trusted":true},"cell_type":"code","source":"max_params_lgb = {'bagging_fraction': 0.8043696643500143,\n 'feature_fraction': 0.6829323879981047,\n 'lambda_l1': 0.9264555612104627,\n 'lambda_l2': 0.9774233689434216,\n 'min_child_samples': 10,\n 'min_child_weight': 125.68433948868649,\n 'num_leaves': 28,\n 'objective': 'binary',\n 'learning_rate': 0.01,\n 'bagging_freq': 1,\n 'verbosity': 0,\n 'random_state': 1991}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgbm\n\n# One-dimensional array of probabilities for predicting validation data target values with an OOF-trained model\noof_val_preds_lgb = np.zeros(X.shape[0]) \n# One-dimensional array of probabilities for predicting test data target values with an OOF-trained model\noof_test_preds_lgb = np.zeros(X_test.shape[0]) \n\n# Train, validate, and predict models by OOF\nfor idx, (train_idx, valid_idx) in enumerate(folds.split(X, y)):\n    # The phrase that separates each fold.\n    print('#'*40, f'Fold {idx+1} out of {folds.n_splits}', '#'*40)\n    \n    # Set train data, valid data\n    X_train, y_train = X[train_idx], y[train_idx] # Train data\n    X_valid, y_valid = X[valid_idx], y[valid_idx] # Valid data\n\n    # Create lgbm dataset\n    dtrain = lgbm.Dataset(X_train, y_train) # lgbm train dataset\n    dvalid = lgbm.Dataset(X_valid, y_valid) # lgbm valid dataset\n\n    # Train LightGBM\n    lgb_model = lgbm.train(params=max_params_lgb, # Optimal Hyper-parameters\n                           train_set=dtrain, # Train data\n                           num_boost_round=1500, # Number of boosting iterations\n                           valid_sets=dvalid, # Valid data for model performance evaluation\n                           feval=gini_lgb, # Evaluation metrics for validation\n                           early_stopping_rounds=150, # Early stopping condition\n                           verbose_eval=100)\n    \n    # The number of boosting iterations when the model performs best \n    best_iter = lgb_model.best_iteration\n    # Predict probabilities using test data\n    oof_test_preds_lgb += lgb_model.predict(X_test, \n                                    num_iteration=best_iter)/folds.n_splits\n    # OOF prediction for model performance evaluation\n    oof_val_preds_lgb[valid_idx] += lgb_model.predict(X_valid, num_iteration=best_iter)\n    \n    # Normalized Gini coefficient for oof prediction probabilities\n    gini_score = eval_gini(y_valid, oof_val_preds_lgb[valid_idx])\n    print(f'Fold {idx+1} gini score: {gini_score}\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"max_params_xgb = {'colsample_bytree': 0.8927325521002059,\n 'gamma': 9.766883037651555,\n 'max_depth': 7,\n 'min_child_weight': 6.0577898395058085,\n 'reg_alpha': 8.136089122187865,\n 'reg_lambda': 1.385119327658532,\n 'scale_pos_weight': 1.5142072116395773,\n 'subsample': 0.717425859940308,\n 'objective': 'binary:logistic',\n 'learning_rate': 0.05,\n 'random_state': 1991}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\n\n# One-dimensional array of probabilities for predicting validation data target values with an OOF-trained model\noof_val_preds_xgb = np.zeros(X.shape[0]) \n# One-dimensional array of probabilities for predicting test data target values with an OOF-trained model\noof_test_preds_xgb = np.zeros(X_test.shape[0]) \n\n# Train, validate, and predict models by OOF\nfor idx, (train_idx, valid_idx) in enumerate(folds.split(X, y)):\n    # The phrase that separates each fold.\n    print('#'*40, f'Fold {idx+1} out of {folds.n_splits}', '#'*40)\n    \n    # Set train data, valid data\n    X_train, y_train = X[train_idx], y[train_idx]\n    X_valid, y_valid = X[valid_idx], y[valid_idx]\n\n    # Create xgboost dmatrix\n    dtrain = xgb.DMatrix(X_train, y_train)\n    dvalid = xgb.DMatrix(X_valid, y_valid)\n    dtest = xgb.DMatrix(X_test)\n    \n    watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n\n    xgb_model = xgb.train(params=max_params_xgb, \n                           dtrain=dtrain,\n                           num_boost_round=1000,\n                           evals=watchlist,\n                          maximize=True,\n                           feval=gini_xgb,\n                           early_stopping_rounds=150,\n                           verbose_eval=100)\n\n    # The number of boosting iterations when the model performs best \n    best_iter = xgb_model.best_iteration\n    # Predict probabilities using test data\n    oof_test_preds_xgb += xgb_model.predict(dtest, \n                                    ntree_limit=best_iter)/folds.n_splits\n    # OOF prediction for model performance evaluation\n    oof_val_preds_xgb[valid_idx] += xgb_model.predict(dvalid, ntree_limit=best_iter)\n    \n    # Normalized Gini coefficient for oof prediction probabilities\n    gini_score = eval_gini(y_valid, oof_val_preds_xgb[valid_idx])\n    print(f'Fold {idx+1} gini score: {gini_score}\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### OOF Valid Gini Score"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('LightGBM OOF Gini Score:', eval_gini(y, oof_val_preds_lgb))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('XGBoost OOF Gini Score:', eval_gini(y, oof_val_preds_xgb))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ensemble and Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"oof_test_preds = oof_test_preds_lgb * 0.6 + oof_test_preds_xgb * 0.4\nsubmission['target'] = oof_test_preds\nsubmission.to_csv('submission.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}