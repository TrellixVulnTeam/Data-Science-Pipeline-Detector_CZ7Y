{"cells":[{"metadata":{},"cell_type":"markdown","source":"# *Boosting Claims Predictions: an Analysis on Porto Seguro Data*\n"},{"metadata":{},"cell_type":"markdown","source":"Source: \nThis is a copy of the great script Boosting_PS_Case_Study.ipynb (see https://github.com/JSchelldorfer/ActuarialDataScience) by Andrea Ferrario and Roger Hämmerli (June 2019). See also \"On Boosting: Theory and Applications\" (https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3402687). \n\nChanges: \na) Kaggle Kernel settings, b) due to runtime restrictions some correlation plots and the grid search are reduced and the section on AdaBoost is removed."},{"metadata":{},"cell_type":"markdown","source":"# Abstract"},{"metadata":{},"cell_type":"markdown","source":"Predictive modeling on data from the Porto Seguro’s Safe Driver Prediction competition hosted on the Kaggle platform is performed leveraging machine learning boosting algorithms (`AdaBoost` and `XGBoost`). We refer to the document \"On Boosting: Theory and Applications\" to complement the analysis presented in this notebook."},{"metadata":{},"cell_type":"markdown","source":"# Getting Started with Python and Jupyter Notebook"},{"metadata":{},"cell_type":"markdown","source":"In this section, Jupyter Notebook and Python settings are initialized. For code in Python, the [PEP8 standard](https://www.python.org/dev/peps/pep-0008/) (\"PEP = Python Enhancement Proposal\") is enforced with minor variations to improve readibility. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Notebook settings\n###################\n\n# resetting variables\n#get_ipython().magic('reset -sf') \n\n# formatting: cell width\n#from IPython.core.display import display, HTML\n#display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n\n# plotting\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing xgboost REMARK: run this cell only if other imports failed. Delete it in case xgboost has been already imported\n#import pip\n#pip.main(['install', 'xgboost'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# loading Python packages\n#########################\n\n# scientific packages\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn\nimport scipy\n\n# boosting\nimport xgboost\nfrom xgboost import XGBClassifier\nfrom xgboost import plot_importance\n\n# scipy\nfrom scipy.stats import chi2\n\n# pandas: selected modules and functions\nfrom pandas.plotting import scatter_matrix\n\n# sklearn: selected modules and functions\nfrom sklearn import decomposition\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import scale\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\n\nfrom sklearn.utils import shuffle\n\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.feature_selection import SelectFromModel\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc \nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import zero_one_loss\n\n# utilities\nfrom datetime import datetime","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Import"},{"metadata":{},"cell_type":"markdown","source":"For the given Kaggle competition, two datasets are provided, one for training data and the other for generating predictions to be submitted. The latter lacks of the target variable (denoted by `target` in the former): it will not be imported in this notebook. Therefore, the whole analysis is carried out on the original 'training' data. A remark on data acquisition procedure: a direct download from the Kaggle [Porto Seguro Challenge website](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/data) in a landing folder on a local machine is performed; original data are then copied in a separate folder and there unzipped. Import of unzipped data is performed using `pandas` as follows:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# data import: specify the path to the competition training data\n#path = '...'\n#data = pd.read_csv(path)\ndata = pd.read_csv('../input/train.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Imported dataset, i.e. `data`, comprises of 595212 records and 59 columns:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Structure of imported data:', data.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Memory usage of `data`: "},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Memory usage of `data` (in bytes):', pd.DataFrame.memory_usage(data, index=True, deep=True).sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Structural Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"From the data description on the Kaggle [Porto Seguro Challenge website](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/data) the following information on the features in `data` are provided:\n- Features that belong to similar groupings are tagged as such in the feature names (e.g.,  `ind`, `reg`, `car`, `calc`).\n- Feature names include the postfix `bin` to indicate binary features and `cat` to indicate categorical features. \n- Features without these designations are either continuous or ordinal. \n- Values of `-1` indicate that the feature was missing from the observation. \n- The target column `target` signifies whether or not a claim was filed for that policy holder.\n\nThe variable `target` is therefore the label used to train the classifiers. Data types for all columns in `data` are shown below:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# data types\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Both integer and float data types are present; the variable `id` uniquely identifies data records; a quick check shows that no duplicate record exists:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# duplicates? None\ndata.drop_duplicates().shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"One continues by checking the first 10 entries of the `data` dataset before moving to a high level summary using `describe()`."},{"metadata":{"trusted":true},"cell_type":"code","source":"# imported data: first 10 entries\ndata.head(10).T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# imported data: statistical summaries with describe\ndata.describe().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Missing Values"},{"metadata":{},"cell_type":"markdown","source":"Missing values are encoded with `-1`, as discussed in the official data description on the  Kaggle [Porto Seguro Challenge website](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/data); therefore the following code is implemented to have an overview of all variables with missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"# missing values (encoded as -1)\nfeat_missing = []\n\nfor f in data.columns:\n    missings = data[data[f] == -1][f].count()\n    if missings > 0:\n        feat_missing.append(f)\n        missings_perc = missings/data.shape[0]\n        \n        # printing summary of missing values\n        print('Variable {} has {} records ({:.2%}) with missing values'.format(f, missings, missings_perc))\n\n# how many variables do present missing values?\nprint()\nprint('In total, there are {} variables with missing values'.format(len(feat_missing)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Missing value imputation will be discussed in section [Imputation of missing values](#Imputation-of-missing-values).\nDue to the high number and different types of the features in `data`, univariate analysis is performed to gather insights on the provided data, as shown in the forthcoming section. "},{"metadata":{},"cell_type":"markdown","source":"# Univariate Analysis"},{"metadata":{},"cell_type":"markdown","source":"## Meta-Information-Encoding"},{"metadata":{},"cell_type":"markdown","source":"One starts the univariate analysis of `data` by recording the feature type into a meta-information data frame, as shown in [B. Carremans' kernel](https://www.kaggle.com/bertcarremans/data-preparation-exploration). This encoding will be extremely useful in the following, allowing for a feature type-targeted analyisis."},{"metadata":{"trusted":true},"cell_type":"code","source":"# B. Carremans: recording meta-information for each column in train, following the official data description on the Kaggle Porto Seguro Challenge\ninfo = []\n\nfor f in data.columns:\n\n    # defining the role (target and id have to be separated from the other features)\n    if f == 'target':\n        role = 'target'\n    elif f == 'id':\n        role = 'id'\n    else:\n        role = 'input'\n         \n    # defining the levels    \n    \n    # _bin postfix = binary feature (target is binary as well)\n    if 'bin' in f or f == 'target':\n        level = 'binary'\n    \n    # _cat postfix = categorical feature\n    elif 'cat' in f or f == 'id':\n        level = 'categorical'\n        \n    # continuous or ordinal features: those which are neither _bin nor _cat    \n    elif data[f].dtype == float:\n        level = 'interval'\n    else:\n        level = 'ordinal'    \n        \n    # initialize 'keep' to True for all variables except for id\n    keep = True\n    if f == 'id':\n        keep = False\n    \n    # defining the data type \n    dtype = data[f].dtype\n    \n    # creating a dictionary that contains all the metadata for the variable\n    f_dict = {\n        'varname': f,\n        'role': role,\n        'level': level,\n        'keep': keep,\n        'dtype': dtype\n    }\n    info.append(f_dict)\n\n# collecting all meta-information into a meta dataframe\nmeta = pd.DataFrame(info, columns = ['varname', 'role', 'level', 'keep', 'dtype'])\nmeta.set_index('varname', inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In summary, variables in `data` are of level `categorical`, `binary`, `ordinal` (i.e. categorical variables with an ordering of levels) and `interval` (i.e. discrete or semi-continuous numerical variables). The `meta` data frame collects all these meta-information, by construction."},{"metadata":{"trusted":true},"cell_type":"code","source":"# showing meta-information data frame\nprint(meta.shape)\nmeta","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"An aggregated view of `meta` is performed by grouping by `role` and `level`:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# showing meta-information aggregated view\npd.DataFrame({'count' : meta.groupby(['role', 'level'])['role'].size()}).reset_index()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Frequency tables, bar-plots and histograms"},{"metadata":{},"cell_type":"markdown","source":"Thanks to the meta-information encoding, a quick check on frequency tables for all variables in `train` is easily performed; for each feature level (i.e. `binary`, `ordinal`, `categorical` and `interval`) one generates a dictionary whose keys are the features of the selected level and values given by the frequency of the feature levels. More explicitly:"},{"metadata":{},"cell_type":"markdown","source":"### Binary"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# choose feature level \nlevel = 'binary'\n\n# creating the dictionary with feature level counts \nctabs = {}\n\nfor f in meta[(meta.level == level) & (meta.keep)].index:\n    ctabs[f]=( pd.value_counts(data[f]) / data.shape[0] ) * 100\n    \n# printing the dictionary, with rounding of frequencies\nfor f in ctabs.keys():\n    print(ctabs[f].round(2))\n    print() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Categorical"},{"metadata":{},"cell_type":"markdown","source":"The `14` categorical variables show different numbers of levels; they are summarized using the following code:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# categorical variables: summary of distinct levels\n\n# choose feature level \nlevel = 'categorical'\n\nfor f in meta[(meta.level == 'categorical') & (meta.keep)].index:\n    dist_values = data[f].value_counts().shape[0]\n    print('Variable {} has {} distinct values'.format(f, dist_values))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# categorical variables: tabulation\n\n# choose feature level \nlevel = 'categorical'\n\n# creating the dictionary with feature level counts \nctabs = {}\n\nfor f in meta[(meta.level == level) & (meta.keep)].index:\n    ctabs[f] = ( pd.value_counts(data[f]) / data.shape[0] ) * 100\n    \n# printing the dictionary, with rounding of frequencies    \nfor f in ctabs.keys():\n    print(ctabs[f].round(2))\n    print() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Interval"},{"metadata":{},"cell_type":"markdown","source":"Interval variables are characterized by quite different distributions; in fact some variables are semi-continuous (e.g. `ps_reg_03` or `ps_car_12`), while others are discretized ( e.g. `ps_reg_01` or  `ps_calc_01`): the code sample below provides an complete overview. "},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# interval variables: tabulation \n\n# choose feature level \nlevel = 'interval'\n\n# creating the dictionary with feature level counts (missing values are dropped)\nctabs = {}\n\nfor f in meta[(meta.level == level) & (meta.keep)].index:\n    ctabs[f] = ( pd.value_counts(data[f]) / data.shape[0] ) * 100\n    \n# printing the dictionary, with rounding of frequencies    \nfor f in ctabs.keys():\n    print(ctabs[f].round(2))\n    print() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Therefore, `ps_reg_01`, `ps_calc_01`, `ps_calc_02`, `ps_calc_03` and `ps_reg_02` are displayed with bar charts, while `ps_reg_03`, `ps_car_12`, `ps_car_13`, `ps_car_14` and `ps_car_15` with histograms (missing values are removed to improve visualization)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# producing bar charts for selected variables\nv = list(['ps_reg_01', 'ps_reg_02', 'ps_calc_01', 'ps_calc_02', 'ps_calc_03'])\nsns.set( rc = {'figure.figsize': (10, 10)})\n\nfor f in v:\n    plt.figure()\n    sns.countplot(x = f, data = data, linewidth=2, palette=\"Blues\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The variables `ps_calc_01`, `ps_calc_02`, `ps_calc_03` show an highly homogeneous distribution."},{"metadata":{"trusted":true},"cell_type":"code","source":"# producing histograms for selected variables\nv = list(['ps_reg_03', 'ps_car_12', 'ps_car_13', 'ps_car_14', 'ps_car_15'])\nsns.set( rc = {'figure.figsize': (10, 10)})\n\nfor f in v:\n    plt.figure()\n    sns.distplot(data.loc[data[f] != -1, f], kde = False)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Variables `ps_reg_03`, `ps_car_12`, `ps_car_13`, `ps_car_14` show distributions with right tails. Additional considerations on interval `car` variables are collected in the forthcoming section."},{"metadata":{},"cell_type":"markdown","source":"#### On interval `car` variables: `ps_car_12`, ..., `ps_car_15`"},{"metadata":{},"cell_type":"markdown","source":"`car` variables are vehicle-related features, as stated in the Porto Seguro Kaggle challenge description. Therefore, it would interesting to inference on the nature of such variables, in the limits given by anonymization and lack of additional information. The most relevant case is represented by `ps_car_15`, which is obtained by square root of a feature with integer values"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"squared = data['ps_car_15'] ** 2\nsquared.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Therefore, one could suggest that `ps_car_15**2` represents the car manufacture age  with 0.0 =2000, 1.0 = 2001,..., 14.0 =2014, but no final confirmation of the statement is possible. Going backwards, one can try squaring the remaining variables and try to map results to typical car-relevant information for insurance, i.e. CC, weight, value etc. for the purpose of feature selection. However, in absence of additional information on the provided `car` variables, no additional analysis is performed."},{"metadata":{},"cell_type":"markdown","source":"### Ordinal"},{"metadata":{},"cell_type":"markdown","source":"The `16` ordinal variables are quickly displayed below. All variables except `ps_ind_01`, `ps_ind_03`, `ps_ind_14`, `ps_ind_15` and `ps_car_11` are of the `calc` type."},{"metadata":{"trusted":true},"cell_type":"code","source":"# ordinal features: visualization\n\n# choose feature level \nlevel = 'ordinal'\n\n# producing histograms\nv = meta[(meta.level == level) & (meta.keep)].index\nsns.set( rc = {'figure.figsize': (10, 10)})\n\nfor f in v:\n    plt.figure()\n     \n    sns.distplot(data[f], kde = False)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`ps_ind_14` shows a strong skewness between ordinal levels, with concentration at `ps_ind_14 == 0` at 99% and monotonic decrease among levels. On the other hand, for `ps_car_11` the trend is monotone increasing. Level `5` in `ps_ind_01` breaks monotonicity. An overview of `ordinal` features is provided by tabulating the frequencies of levels, as done below."},{"metadata":{"trusted":true},"cell_type":"code","source":"# ordinal features: tabulation\n\n# creating the dictionary with feature level counts (missing values are dropped)\nctabs = {}\nlevel = 'ordinal'\n\nfor f in meta[(meta.level == level) & (meta.keep)].index:\n    ctabs[f] = ( pd.value_counts(data[f]) / data.shape[0] ) * 100\n    \n# printing the dictionary    \nfor f in ctabs.keys():\n    print(ctabs[f].round(2))\n    print() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## `target` variable: class imbalance"},{"metadata":{},"cell_type":"markdown","source":"The `target` variable in the `data` dataset is highly imbalanced, as shown in the following code chunk:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# levels for the target variable \nlev_target = ( pd.crosstab(index = data['target'], columns = 'Frequency') / data.shape[0] ) * 100\nlev_target.round(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Visualization and Multivariate Analysis"},{"metadata":{},"cell_type":"markdown","source":"## Analysis of correlation for interval features"},{"metadata":{},"cell_type":"markdown","source":"Pearson and Spearman correlation of interval variables is discussed.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pearson correlation matrix: computation and visualization\n\n# use method='pearson' resp. method='spearman' to compute Pearson resp. Spearman correlations\ndef corr_heatmap(v):\n    correlations = data[v].corr(method='pearson')\n    fig = plt.subplots(figsize=(10, 10))\n\n    sns.heatmap(correlations,  center=0, fmt='.2f', cbar=False,\n                square=True, linewidths=1, annot=True,  cmap=\"YlGnBu\")\n    plt.xticks(rotation=90) \n    plt.yticks(rotation=0) \n    plt.show()\n\n# one applies the corr_heatmap function on the interval features    \nv = meta[(meta.level == 'interval') & (meta.keep)].index\ncorr_heatmap(v)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Scatterplots of high correlation variables are provided below, instead (**warning**: computationally intensive visualization)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# scatterplot high correlation interval variables\nimport seaborn\n#due to runtime restrictions not executed:\n#high = pd.Index(['ps_reg_01', 'ps_reg_02', 'ps_reg_03', 'ps_car_12', 'ps_car_13', 'ps_car_15'])\n#pd.plotting.scatter_matrix(data[high], alpha = 0.2, figsize = (40, 40), diagonal = 'kde')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## `target` vs. features"},{"metadata":{},"cell_type":"markdown","source":"### `target` vs. interval features"},{"metadata":{},"cell_type":"markdown","source":"The `10` `interval` features can be plotted against the `target` variable in either jitter plots or boxplots for the purpose of visual exploration of their distributions. Below we show a simple code chunk for producing jitter plots for the variables `ps_reg_03`, `ps_car_12` is provided; however, for sake of clarity in the exposition boxplots will be preferred."},{"metadata":{"trusted":true},"cell_type":"code","source":"# jitter plots: an example\nsns.set( rc = {'figure.figsize': (10, 10)})\nfeat = list(['ps_reg_03', 'ps_car_12'])\n\nfor f in feat:\n    plt.figure()\n    sns.stripplot(y=f, x='target', data=data, jitter=True, palette=\"Blues\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Additional diagrams are given below. One starts with `ps_reg_01`:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# ps_reg_01\nsns.set( rc = {'figure.figsize': (10, 10)})\nsns.boxplot(x='target', y='ps_reg_01', data=data, linewidth=2, palette=\"Blues\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No relevant discrimination of the variable distributions along the `target` levels is detected. Then `ps_reg_02`:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# ps_reg_02\nsns.set( rc = {'figure.figsize': (10, 10)})\nsns.boxplot(x= \"target\", y =\"ps_reg_02\", data=data, linewidth=2, palette=\"Blues\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" The variable distribution along the `target=1` level shows a wider spread w.r.t. the distribution along `target=0`. Few outliers are detected. One continues with `ps_reg_03`:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# ps_reg_03\nsns.set( rc = {'figure.figsize': (10, 10)})\nsns.boxplot(x='target', y ='ps_reg_03', data=data, linewidth=2, palette=\"Blues\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Relevant presence of outliers is identified; missing values are present for records with both `0` and `1` values of the `target` variable. On the other hand `ps_car_12`:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# ps_car_12\nsns.set( rc = {'figure.figsize': (10, 10)})\nsns.boxplot(x='target', y='ps_car_12', data=data, linewidth=2, palette=\"Blues\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Missing values are not present for records with `target=1`. Moving to `ps_car_13`:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# ps_car_13\nsns.set( rc = {'figure.figsize': (10, 10)})\nsns.boxplot(x='target', y='ps_car_13', data=data, linewidth=2, palette=\"Blues\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Strong presence of outliers, especially along `target=0`. Then `ps_car_14`:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# ps_car_14\nsns.set( rc = {'figure.figsize': (10, 10)})\nsns.boxplot(x='target', y='ps_car_14', data=data, linewidth=2, palette=\"Blues\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Outliers and missing values for both `target` levels are detected. Then `ps_car_15`:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# ps_car_15\nsns.set( rc = {'figure.figsize': (10, 10)})\nsns.boxplot(x='target', y='ps_car_15', data=data, linewidth=2, palette=\"Blues\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It shows the presence of both outliers and missing values for both `target` levels. We finish with `calc` features, starting with `ps_calc_01`:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# ps_calc_01\nsns.set( rc = {'figure.figsize': (10, 10)})\nsns.boxplot(x='target', y='ps_calc_01', data=data, linewidth=2, palette=\"Blues\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Continuing with `ps_calc_02`"},{"metadata":{"trusted":true},"cell_type":"code","source":"# ps_calc_02\nsns.set( rc = {'figure.figsize': (10, 10)})\nsns.boxplot(x='target', y='ps_calc_02', data=data, linewidth=2, palette=\"Blues\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"and finishing with `ps_calc_03`"},{"metadata":{"trusted":true},"cell_type":"code","source":"# ps_calc_03\nsns.set( rc = {'figure.figsize': (10, 10)})\nsns.boxplot(x='target', y='ps_calc_03', data=data,linewidth=2, palette=\"Blues\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All `calc` variables, i.e. `ps_calc_01`, `ps_calc_02` and `ps_calc_03` show no discrimination along the `target` variable levels."},{"metadata":{},"cell_type":"markdown","source":"### `target` vs. binary, categorical and ordinal features"},{"metadata":{},"cell_type":"markdown","source":"One moves to the visual analysis of plots of `target` against the remaining variables in `data` and cross-tabulations for `binary` data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# binary features\nsns.set( rc = {'figure.figsize': (10, 10)})\nfeat = meta[(meta.level == 'binary') & (meta.keep)].index\n\nfor f in feat:\n    plt.figure()\n    sns.countplot(x=data[f], hue=data.target, data=data, palette=\"Blues\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for binary make a cross-tabulation rows : levels columns: 0/1 in %; sum of every row = 100%\nv = meta[(meta.level == 'binary') & (meta.keep)].index.drop('target')\n\nfor f in v:\n    crosstab = pd.crosstab(index=data[f], columns=data['target'], margins=True) \n    cross = pd.DataFrame(data=crosstab.div(crosstab['All'], axis=0).drop('All', 1))\n    cross['All'] = crosstab.iloc[:,2]\n    print(cross)\n    print()    \n    print()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A quick visual check suggests that no binary variable presents `0` or `1` levels showing a significant deviation from the global 96.4-3.6 noise-signal ratio. Moreover, no zero frequency cell is identified; zero cells can lead to numerical instability during the training routines (e.g. for logistic regression models). One continues with cross-tabulation of `target` against ordinal variables."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":" # for ordinal make a cross-tabulation rows : levels columns: 0/1 in %; sum of every row = 100%\nv = meta[(meta.level == 'ordinal') & (meta.keep)].index\n\nfor f in v:\n    crosstab = pd.crosstab(index=data[f], columns=data['target'], margins=True) \n    cross = pd.DataFrame(data=crosstab.div(crosstab['All'], axis=0).drop('All', 1))\n    cross['All'] = crosstab.iloc[:,2]\n    print(cross)\n    print()    \n    print()    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`calc` variables are characterized by zero cells for a limited number of variable levels. For example, zero cells appear for `ps_calc_12` at levels `ps_calc_12 == 9` and `ps_calc_12 == 10`. The number of records associated to levels for which complete separation holds, i.e. the emergence of zero cells in the cross-tabulation against `target`, is extremely low for all `calc` variables. In absence of additional information on provided data, it is not possible to produce inference on those data points, which could represent outliers to remove before modeling or records for which the absence of `target == 1` is fully justified by business considerations."},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering & Data Preparation for Modeling"},{"metadata":{},"cell_type":"markdown","source":"In this section we perform a series of transformations on `data` to pave the way to predictive modeling. "},{"metadata":{},"cell_type":"markdown","source":"## On numerical `calc` variables and univariate screening\nAs mentioned in the document, the numerical `calc` variables will be dropped from further analysis, starting with a quick structural check on `data` and ending with a drop check."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Structure of data before calc variable drop:', data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dropping 'ps_calc_01',... 'ps_calc_14' variables and updating meta information\nvars_to_drop = ['ps_calc_01', 'ps_calc_02','ps_calc_03','ps_calc_04','ps_calc_05','ps_calc_06','ps_calc_07','ps_calc_08','ps_calc_09','ps_calc_10','ps_calc_11','ps_calc_12','ps_calc_13','ps_calc_14']\ndata.drop(vars_to_drop, inplace = True, axis = 1)\nmeta.loc[(vars_to_drop), 'keep'] = False  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Structure of data after calc variable drop:', data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Imputation of missing values\nAs discussed in the document, a straighforward imputation scheme is chosen and applied in the forthcoming code chunk:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# dropping 'ps_car_03_cat', 'ps_car_05_cat' and updating meta information\nvars_to_drop = ['ps_car_03_cat', 'ps_car_05_cat']\ndata.drop(vars_to_drop, inplace = True, axis = 1)\nmeta.loc[(vars_to_drop), 'keep'] = False  \n\n# imputing with the mean or mode using Imputer from sklearn.preprocessing\nfrom sklearn.preprocessing import Imputer\n\nmean_imp = Imputer(missing_values = -1, strategy = 'mean', axis = 0)\nmode_imp = Imputer(missing_values = -1, strategy = 'most_frequent', axis = 0)\n\ndata['ps_reg_03'] = mean_imp.fit_transform(data[['ps_reg_03']]).ravel()\ndata['ps_car_12'] = mean_imp.fit_transform(data[['ps_car_12']]).ravel()\ndata['ps_car_14'] = mean_imp.fit_transform(data[['ps_car_14']]).ravel()\ndata['ps_car_11'] = mode_imp.fit_transform(data[['ps_car_11']]).ravel()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After imputation one has "},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dummies\nDummy variables for categorical features in `data` are created; the function `get_dummies` converts categorical variables into dummies dropping the original categorical variable from which the corresponding dummies are created from the resulting dataset and the first level. One-hot-encoding does not drop the first level, instead."},{"metadata":{"trusted":true},"cell_type":"code","source":"# selecting categorical variables\nv = meta[(meta.level == 'categorical') & (meta.keep)].index\nprint('Before dummification we have {} variables in train'.format(data.shape[1]))\n\n# creating dummy variables\ndata = pd.get_dummies(data, columns = v, drop_first = True)\nprint('After dummification we have {} variables in data'.format(data.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After generation of dummy variables, the memory usage of the `data` dataset is increased:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Memory usage of `data` (in bytes):', pd.DataFrame.memory_usage(data,index=True, deep = True).sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"One can check that no categorical variable is present in the resulting dataset, as expected:"},{"metadata":{"trusted":true},"cell_type":"code","source":"#v = meta[(meta.level == 'categorical') & (meta.keep)].index\n#data[v]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A quick check on the columns of `data` after dummification ends this section:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data.columns.values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## On Randomness"},{"metadata":{},"cell_type":"markdown","source":"We now fix a `random_state` for reproducibility of results; it will be used in both train vs. test dataset splitting and modeling."},{"metadata":{"trusted":true},"cell_type":"code","source":"random_state = 123","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## `train` and `test` datasets"},{"metadata":{},"cell_type":"markdown","source":"As mentioned in the document, `data` dataset is randomly split into `train` and `test` datasets. A not stratified approach is chosen and removal of both columns `id` and `target` from the training dataset is performed. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# split 80-20% (no stratification)\nX_train, X_test, y_train, y_test = train_test_split(data.drop(['id', 'target'], axis=1), \n                                                    data['target'], \n                                                    test_size=0.2,\n                                                    random_state=random_state\n                                                   )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After the split, the following checks on the `train` and `test` datasets are performed:"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# structural checks\nprint('Training dataset - dimensions:', X_train.shape)\nprint('Test dataset - dimensions:', X_test.shape)\nprint()\nprint('Random split check:', X_train.shape[0] + X_test.shape[0] == data.shape[0])\nprint()\n\n# imbalancing: check\nlev_target = ( pd.crosstab(index = data['target'], columns = 'count') / data.shape[0] ) * 100\nlev_target_train = ( pd.crosstab(index = y_train, columns = 'count') / y_train.shape[0] ) * 100\nlev_target_test = ( pd.crosstab(index = y_test, columns = 'count') / y_test.shape[0] ) * 100\n\nprint('target class imbalance data:')\nprint(lev_target)\nprint()\nprint('target class imbalance train:')\nprint(lev_target_train)\nprint()\nprint('target class imbalance test:')\nprint(lev_target_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We then remove the original dataset `data` to free up some memory."},{"metadata":{"trusted":true},"cell_type":"code","source":"del data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can export both `X_train` and `X_test` for later use, if needed:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# insert paths to export training and test data sets\n#X_train.to_csv('...')\n#X_test.to_csv('...')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling"},{"metadata":{},"cell_type":"markdown","source":"## Modeling Strategies: short description"},{"metadata":{"collapsed":true},"cell_type":"markdown","source":"For the overview of the forthcoming modeling strategies we refer to the document."},{"metadata":{},"cell_type":"markdown","source":"## On the Porto Seguro challenge performance metric: normalized Gini coefficient"},{"metadata":{},"cell_type":"markdown","source":"The evaluation of fitted model on out-of-sample data is performed in the Porto Seguro Kaggle challenge by considering an ad-hoc performance measure, called normalized Gini coefficient, as discussed [here](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction#evaluation). The code for the computation of the normalized Gini coefficient is provided in this [thread](https://www.kaggle.com/c/ClaimPredictionChallenge/discussion/703), for different programming languages. Below the Python implementation is presented."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import make_scorer\n\n# Gini coefficient\ndef gini(actual, pred):\n    \n    # a structural check\n    assert (len(actual) == len(pred))\n    \n    # introducing an array called all\n    all = np.asarray(np.c_[actual, pred, np.arange(len(actual))], dtype=np.float)  #slicing along second axis\n    \n    # sorting the array along predicted probabilities (descending order) and along the index axis all[:, 2] in case of ties\n    all = all[np.lexsort((all[:, 2], -1 * all[:, 1]))]                             #\n\n    # towards the Gini coefficient\n    totalLosses = all[:, 0].sum()\n    giniSum = all[:, 0].cumsum().sum() / totalLosses\n\n    giniSum -= (len(actual) + 1) / 2.\n    return giniSum / len(actual)\n\n# normalized Gini coefficient\ndef gini_normalized_score(actual, pred):\n    return gini(actual, pred) / gini(actual, actual)\n\n# score using the normalized Gini\nscore_gini = make_scorer(gini_normalized_score, greater_is_better=True, needs_threshold = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Boosting: AdaBoost - deleted"},{"metadata":{},"cell_type":"markdown","source":"## Boosting: XGBoost  "},{"metadata":{},"cell_type":"markdown","source":"We continue the boosting analysis with **XGBoost**. We do implement 2 separate strategies: **ST0** and **ST1**."},{"metadata":{},"cell_type":"markdown","source":"#### ST0: default XGBoost with `XGBClassifier()`"},{"metadata":{},"cell_type":"markdown","source":"\nBelow the hyper-parameters of the default `XGBClassifier()` implementation are presented and fitting on `X_train` is performed."},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit model default hyper-parameters\nxgb_default = XGBClassifier(random_state=random_state)\nxgb_default.set_params()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fitting the default XGBoostClassifier()\nxgb_default.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# default XGBoost classifier performance\ny_pred_proba_xgb_default = xgb_default.predict_proba(X_test)\nfpr, tpr, _ = roc_curve(y_test, y_pred_proba_xgb_default[:, 1])\nroc_auc = auc(fpr, tpr)\n\n# AUC on test dataset\nprint('AUC:', roc_auc_score(y_test, y_pred_proba_xgb_default[:, 1]).round(3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### ST1: hyper parameter tuning:`max_depth`, `learning_rate`, `n_estimators` with fixed subsampling "},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# GridSearch XGBoost optimization (change the hyperparameters according to the run under consideration)\n#Mod: Grid changed from 'learning_rate': [0.001, 0.01, 0.1, 1], 'n_estimators': [100, 300, 500] due to runtime restrictions\nparam_grid = {'max_depth': [3],                                                            \n              'learning_rate': [0.1],\n              'n_estimators': [10, 300],\n              'subsample': [0.5],\n              'colsample_bytree': [0.75]\n             } \n\n# cross-validation\ncv = StratifiedKFold(n_splits=5, \n                     shuffle=False, \n                     random_state=random_state)\n\nxgb = GridSearchCV(estimator=XGBClassifier(random_state=random_state), \n                   param_grid=param_grid, \n                   scoring=score_gini, \n                   cv=cv, \n                   verbose=10)\n\nxgb.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# best score on CV-data\nprint('Best Gini on CV-data:', xgb.best_score_)\n\n# parameters for best model\nprint('Best set of hyperparameters:', xgb.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# best xgb model\nxgb_best = xgb.best_estimator_\nxgb_best.fit(X_train, y_train)\n\n# best xgb predictions on test data and AUC\ny_pred_proba_xgb = xgb_best.predict_proba(X_test)\nfpr, tpr, _ = roc_curve(y_test, y_pred_proba_xgb[:, 1])\nroc_auc = auc(fpr, tpr)\n\n# plotting ROC and showing AUC (on test data)\nplt.figure()\nlw = 2\nplt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.3f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()    \n\n# AUC on test dataset\nprint('AUC:', roc_auc_score(y_test, y_pred_proba_xgb[:, 1]).round(3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And now we plot the feature importance from the best XGBoost model so far in **ST1**."},{"metadata":{"trusted":true},"cell_type":"code","source":"# fitting best model (again, if not saved beforehand)\nxgb_best_ov = XGBClassifier(max_depth=3,                                      \n                            learning_rate=0.1,\n                            n_estimators=300,\n                            subsample=0.5,\n                            colsample_bytree=0.75,\n                            random_state=random_state)\nxgb_best_ov.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plotting feature importance graph: top 10 features\nplot_importance(xgb_best_ov, max_num_features=10)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"toc":{"nav_menu":{"height":"143px","width":"704px"},"number_sections":false,"sideBar":true,"skip_h1_title":false,"toc_cell":true,"toc_position":{"height":"838px","left":"0px","right":"1820px","top":"111px","width":"100px"},"toc_section_display":"none","toc_window_display":false}},"nbformat":4,"nbformat_minor":1}