{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('../input/porto-seguro-safe-driver-prediction/train.csv')\n\ntarget_count = df_train.target.value_counts()\nprint('Class 0:', target_count[0])\nprint('Class 1:', target_count[1])\nprint('Proportion:', round(target_count[0] / target_count[1], 2), ': 1')\n\ntarget_count.plot(kind='bar', title='Count (target)');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Vamos olhar para o comportamento dos dados","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">> não nos fala muito, mas a temos um id na primiera coluna que podemos dropar e o target que precisamos dropar das features","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**<h2 id=\"t2\" style=\"margin-bottom: 18px\">Paradoxo da Acuácia</h2>**\n\nUm dos maiores erros que data scientists inexperientes cometem quando lidam com datasets desbalanceados é confiar em uma métrica simples como <code>accuracy_score</code> Apesar de um score elevado nessa métrica, vamos provar como ela pode ser enganosa.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\n\n# Remove 'id' and 'target' columns\nlabels = df_train.columns[2:]\n\nX = df_train[labels]\ny = df_train['target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, \n                                                    random_state=42, stratify=y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmodel = XGBClassifier()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Vamos piorar o nosso modelo\nSe esse modelo estiver certo com uma acurácia de 96%, podemos piorá-lo ao treinar e testar com apenas uma feature ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = XGBClassifier()\nmodel.fit(X_train[['ps_calc_01']], y_train)\ny_pred = model.predict(X_test[['ps_calc_01']])\n\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2 id=\"t3\" style=\"margin-bottom: 18px\">Matriz de Confusão</h2>\n\nUma forma interessante de avaliar os resultados é através da matriz de confusão que mostra os valores preditos e esperados ou reais.  \nNa primeira linha, a primeira coluna indica quantos \"Classe 0\" foram preditos corretamente (como \"Classe 0\") \nNa primeira linha, segunda coluna os erroneamente classificados como \"Classe 1\".\nNa segunda linha, a primeira coluna nos mostra quantos \"Classe 1\" foram preditos erroneamente (como \"Classe 0\") \nNa segunda linha, e segunda coluna os corretamente classificados como \"Classe1\"\n\nA diagonal descendente nos mostra as predições corretas. Modelos que acertam mais possuem concentração de valores maiores na primeira linha, primeira coluna e na segunda linha, segunda coluna.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nfrom matplotlib import pyplot as plt\n\nconf_mat = confusion_matrix(y_true=y_test, y_pred=y_pred)\nprint('Confusion matrix:\\n', conf_mat)\n\nlabels = ['Class 0', 'Class 1']\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(conf_mat, cmap=plt.cm.Blues)\nfig.colorbar(cax)\nax.set_xticklabels([''] + labels)\nax.set_yticklabels([''] + labels)\nplt.xlabel('Predicted')\nplt.ylabel('Expected')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Não é o nosso caso! Assim como no exemplo de fraudes, o modelo não previu nenhuma observação como \"Classe 1\"","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<h1 id=\"t5\">Resampling</h1>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Função para devolver a contagem de amostras na classe 0 e na classe 1\nlen_class_0, len_class_1 = df_train.target.value_counts()\n\n# devidir o df por classe\ndf_class_0 = df_train[df_train['target'] == 0] \ndf_class_1 = df_train[df_train['target'] == 1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2 id=\"t5\">Random undersampling</h2>\nmétodo raiz: vai gerar os mesmos resultados que usando o imbalearn do scikit learn.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#reduzindo as observações da classe 0 de acordo com o tamanho da classe 1\ndf_class_0_under = df_class_0.sample(len_class_1, \n                                     random_state=42)\ndf_test_under = pd.concat([df_class_0_under, df_class_1], axis=0)\n\nprint('Random under-sampling:')\nprint(df_test_under.target.value_counts())\n\ndf_test_under.target.value_counts().plot(kind='bar', title='Count (target)')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Vamos testar modelar e testar**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# separando X e y do df_test_under\nlabels = df_train.columns[2:]\nX_tun = df_test_under[labels]\ny_tun = df_test_under['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = XGBClassifier()\nmodel.fit(X_tun, y_tun)\ny_pred = model.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nfrom matplotlib import pyplot as plt\n\nconf_mat = confusion_matrix(y_true=y_test, y_pred=y_pred)\nprint('Confusion matrix:\\n', conf_mat)\n\nlabels = ['Class 0', 'Class 1']\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(conf_mat, cmap=plt.cm.Blues)\nfig.colorbar(cax)\nax.set_xticklabels([''] + labels)\nax.set_yticklabels([''] + labels)\nplt.xlabel('Predicted')\nplt.ylabel('Expected')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2 id=\"t5\">Random oversampling</h2>\nmétodo raiz: vai gerar os mesmos resultados muito parecidos aos do imblearn do scikit learn.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#vamos aumentar o número de observações da classe 1\n#precisamos repor ou replace as amostras para isso ser possível\ndf_class_1_over = df_class_1.sample(len_class_0, \n                                    replace=True,\n                                    random_state=42)\ndf_test_over = pd.concat([df_class_0, df_class_1_over], axis=0)\n\nprint('Random over-sampling:')\nprint(df_test_over.target.value_counts())\n\ndf_test_over.target.value_counts().plot(kind='bar', title='Count (target)');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# separando X e y do df_test_over\nlabels = df_train.columns[2:]\nX_tov = df_test_over[labels]\ny_tov = df_test_over['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = XGBClassifier()\nmodel.fit(X_tov, y_tov)\ny_pred = model.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"conf_mat = confusion_matrix(y_true=y_test, y_pred=y_pred)\nprint('Confusion matrix:\\n', conf_mat)\n\nlabels = ['Class 0', 'Class 1']\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(conf_mat, cmap=plt.cm.Blues)\nfig.colorbar(cax)\nax.set_xticklabels([''] + labels)\nax.set_yticklabels([''] + labels)\nplt.xlabel('Predicted')\nplt.ylabel('Expected')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = f1_score(y_test, y_pred)\nprint(\"F1_score: %.2f\" %(score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2 id=\"t5\">Usando o Imblearn do Scikit Learn</h2>\nEssa biblioteca possui o mesmo random que fizemos na mão e outros metodos mais elaborados de oversampling e undersampling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import imbalace technique algorithims\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\nfrom imblearn.over_sampling import SMOTE, ADASYN\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import RandomOverSampler\nfrom collections import Counter # counter takes values returns value_counts dictionary","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Undersampling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Original dataset shape %s' % Counter(y))\n\nrus = RandomUnderSampler(random_state=42)\nX_rus, y_rus = rus.fit_resample(X, y)\n\nprint('Resampled dataset shape %s' % Counter(y_rus))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = XGBClassifier()\nmodel.fit(X_rus, y_rus)\ny_pred = model.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nfrom matplotlib import pyplot as plt\n\nconf_mat = confusion_matrix(y_true=y_test, y_pred=y_pred)\nprint('Confusion matrix:\\n', conf_mat)\n\nlabels = ['Class 0', 'Class 1']\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(conf_mat, cmap=plt.cm.Blues)\nfig.colorbar(cax)\nax.set_xticklabels([''] + labels)\nax.set_yticklabels([''] + labels)\nplt.xlabel('Predicted')\nplt.ylabel('Expected')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Oversampling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Original dataset shape %s' % Counter(y))\nros = RandomOverSampler(random_state=42)\nX_ros, y_ros = ros.fit_sample(X, y)\n\nprint(X_ros.shape[0] - X.shape[0], 'new random picked points')\n\nprint('Resampled dataset shape %s' % Counter(y_ros))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = XGBClassifier()\nmodel.fit(X_ros, y_ros)\ny_pred = model.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"conf_mat = confusion_matrix(y_true=y_test, y_pred=y_pred)\nprint('Confusion matrix:\\n', conf_mat)\n\nlabels = ['Class 0', 'Class 1']\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(conf_mat, cmap=plt.cm.Blues)\nfig.colorbar(cax)\nax.set_xticklabels([''] + labels)\nax.set_yticklabels([''] + labels)\nplt.xlabel('Predicted')\nplt.ylabel('Expected')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = f1_score(y_test, y_pred)\nprint(\"F1_score: %.2f\" % (score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id='smote'>Synthetic Minority OverSampling Technique (SMOTE)</a>\n\nSMOTE (Synthetic Minority Oversampling TEchnique) consiste em sintetizar elementos da classe minoritária baseado nos elementos que já existem. Funciona de forma randomica selecionando aleatoriamente observações da classe minoritária e computando pontos através de um KNN. Os pontos sintéticos são adicionados entre os pontos escolhidos e seus vizinhos. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":" ![](https://raw.githubusercontent.com/rafjaa/machine_learning_fecib/master/src/static/img/smote.png)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Original dataset shape %s' % Counter(y))\nsmote = SMOTE(sampling_strategy='minority')\nX_sm, y_sm = smote.fit_sample(X, y)\nprint('Resampled dataset shape %s' % Counter(y_sm))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = XGBClassifier()\nmodel.fit(X_sm, y_sm)\ny_pred = model.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"conf_mat = confusion_matrix(y_true=y_test, y_pred=y_pred)\nprint('Confusion matrix:\\n', conf_mat)\n\nlabels = ['Class 0', 'Class 1']\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(conf_mat, cmap=plt.cm.Blues)\nfig.colorbar(cax)\nax.set_xticklabels([''] + labels)\nax.set_yticklabels([''] + labels)\nplt.xlabel('Predicted')\nplt.ylabel('Expected')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id='adasyn'>Adaptive Synthetic Sampling Method for Imbalanced Data (ADASYN)</a>\n\nADASYN (Adaptive Synthetic) também é um algoritmo que gera dados sintéticos. Sua maior vantagem é que ele tenta aprender prioritariamente com os dados mais difíceis de aprender da classe minoritária. Sua principal vantagem pode virar uma fraquesa se os dados da classe minoritária forem muito esparsos.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Original dataset shape %s' % Counter(y))\n\nadasyn = ADASYN(random_state=42)\nX_ada, y_ada = adasyn.fit_resample(X, y)\n\nprint('Resampled dataset shape %s' % Counter(y_ada))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = XGBClassifier()\nmodel.fit(X_ada, y_ada)\ny_pred = model.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"conf_mat = confusion_matrix(y_true=y_test, y_pred=y_pred)\nprint('Confusion matrix:\\n', conf_mat)\n\nlabels = ['Class 0', 'Class 1']\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(conf_mat, cmap=plt.cm.Blues)\nfig.colorbar(cax)\nax.set_xticklabels([''] + labels)\nax.set_yticklabels([''] + labels)\nplt.xlabel('Predicted')\nplt.ylabel('Expected')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2 id=\"t8\" style=\"margin-bottom: 18px\">Under-sampling: Tomek links</h2>\nTomek links são pares de instancias próximas mas de classes opostas. Essa técnica de undersampling remove as observações das classes majoritárias, aumentando a fronteira entre as duas classes e dessa forma facilitando o processo de classificação","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![](https://raw.githubusercontent.com/rafjaa/machine_learning_fecib/master/src/static/img/tomek.png?v=2)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.under_sampling import TomekLinks\n\nprint('Original dataset shape %s' % Counter(y))\n\ntl = TomekLinks(sampling_strategy='majority')\nX_tl, y_tl = tl.fit_sample(X, y)\n\nprint('Removed indexes:', tl.sample_indices_)\nprint('Resampled dataset shape %s' % Counter(y_tl))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = XGBClassifier()\nmodel.fit(X_tl, y_tl)\ny_pred = model.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"conf_mat = confusion_matrix(y_true=y_test, y_pred=y_pred)\nprint('Confusion matrix:\\n', conf_mat)\n\nlabels = ['Class 0', 'Class 1']\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(conf_mat, cmap=plt.cm.Blues)\nfig.colorbar(cax)\nax.set_xticklabels([''] + labels)\nax.set_yticklabels([''] + labels)\nplt.xlabel('Predicted')\nplt.ylabel('Expected')\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}