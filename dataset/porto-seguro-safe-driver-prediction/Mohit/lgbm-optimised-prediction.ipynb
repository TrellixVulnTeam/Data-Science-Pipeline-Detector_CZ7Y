{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/porto-seguro-safe-driver-prediction/train.csv',na_values=\"-1\")\ntest= pd.read_csv('/kaggle/input/porto-seguro-safe-driver-prediction/test.csv', na_values=\"-1\")\nsub = pd.read_csv('/kaggle/input/porto-seguro-safe-driver-prediction/sample_submission.csv')\nprint(train.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.columns.difference(test.columns))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.isnull().sum().sum())\nprint(test.isnull().sum().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['target'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['missing'] = (train==-1).sum(axis=1).astype(float)\ntest['missing'] = (test==-1).sum(axis=1).astype(float)\nprint('done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_id = test['id']\ntest.drop('id',axis=1,inplace=True)\ntrain.drop('id',axis=1,inplace=True)\nprint('done')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_names = train.columns.tolist()[1:]\ncat_features = [c for c in feature_names if ('cat' in c and 'count' not in c)]\nnum_features = [c for c in feature_names if ('cat' not in c and 'calc' not in c)]\nprint('done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"cat_count_features = []\nfor c in cat_features+['new_ind']:\n    d = pd.concat([train[c],test[c]]).value_counts().to_dict()\n    train['%s_count'%c] = train[c].apply(lambda x:d.get(x,0))\n    test['%s_count'%c] = test[c].apply(lambda x:d.get(x,0))\n    cat_count_features.append('%s_count'%c)\n    \nprint('done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train[cat_features] = train[cat_features].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#unwanted = train.columns[train.columns.str.startswith('ps_calc_')]\n#print(unwanted)\n#train = train.drop(unwanted, axis=1)  \n#test = test.drop(unwanted, axis=1)  \n#print(train.shape)\n#print(test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"target = 'target'\npredictors=['ps_ind_01', 'ps_ind_02_cat', 'ps_ind_03', 'ps_ind_04_cat', 'ps_ind_05_cat', 'ps_ind_06_bin', 'ps_ind_07_bin',\n 'ps_ind_08_bin', 'ps_ind_09_bin', 'ps_ind_10_bin', 'ps_ind_11_bin', 'ps_ind_12_bin', 'ps_ind_13_bin', 'ps_ind_14',\n 'ps_ind_15', 'ps_ind_16_bin', 'ps_ind_17_bin', 'ps_ind_18_bin', 'ps_reg_01', 'ps_reg_02', 'ps_reg_03', 'ps_car_01_cat',\n 'ps_car_02_cat', 'ps_car_03_cat', 'ps_car_04_cat', 'ps_car_05_cat', 'ps_car_06_cat', 'ps_car_07_cat', 'ps_car_08_cat',\n 'ps_car_09_cat', 'ps_car_10_cat', 'ps_car_11_cat', 'ps_car_11', 'ps_car_12', 'ps_car_13', 'ps_car_14', 'ps_car_15',\n 'ps_calc_01', 'ps_calc_02', 'ps_calc_03', 'ps_calc_04', 'ps_calc_05', 'ps_calc_06', 'ps_calc_07', 'ps_calc_08', 'ps_calc_09',\n 'ps_calc_10', 'ps_calc_11', 'ps_calc_12', 'ps_calc_13', 'ps_calc_14', 'ps_calc_15_bin', 'ps_calc_16_bin', 'ps_calc_17_bin',\n 'ps_calc_18_bin', 'ps_calc_19_bin', 'ps_calc_20_bin', 'missing', 'ps_ind_02_cat_count', 'ps_ind_04_cat_count', 'ps_ind_05_cat_count',\n 'ps_car_01_cat_count', 'ps_car_02_cat_count', 'ps_car_03_cat_count', 'ps_car_04_cat_count', 'ps_car_05_cat_count',\n 'ps_car_06_cat_count', 'ps_car_07_cat_count', 'ps_car_08_cat_count', 'ps_car_09_cat_count', 'ps_car_10_cat_count',\n 'ps_car_11_cat_count', 'new_ind_count']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Gini(y_true, y_pred):\n    # check and get number of samples\n    assert y_true.shape == y_pred.shape\n    n_samples = y_true.shape[0]\n\n    # sort rows on prediction column\n    # (from largest to smallest)\n    arr = np.array([y_true, y_pred]).transpose()\n    true_order = arr[arr[:, 0].argsort()][::-1, 0]\n    pred_order = arr[arr[:, 1].argsort()][::-1, 0]\n\n    # get Lorenz curves\n    L_true = np.cumsum(true_order) * 1. / np.sum(true_order)\n    L_pred = np.cumsum(pred_order) * 1. / np.sum(pred_order)\n    L_ones = np.linspace(1 / n_samples, 1, n_samples)\n\n    # get Gini coefficients (area between curves)\n    G_true = np.sum(L_ones - L_true)\n    G_pred = np.sum(L_ones - L_pred)\n\n    # normalize to true Gini coefficient\n    return G_pred * 1. / G_true\n\ndef evalerror(preds, dtrain):\n    labels = dtrain.get_label()\n    return 'gini', Gini(labels, preds), True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\nimport lightgbm as lgb\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_auc_score\nimport pickle\nimport os\nimport gc\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\ngc.enable()\n\n\nbayesian_tr_index, bayesian_val_index  = list(StratifiedKFold(n_splits=5, shuffle=True, random_state=1).split(train,train.target.values))[0]\n\ndef LGB_bayesian(\n     num_leaves,  # int\n     min_data_in_leaf,  # int\n     learning_rate,\n     min_sum_hessian_in_leaf,    # int  \n     feature_fraction,\n     lambda_l1,\n     lambda_l2,\n     min_gain_to_split,\n     max_depth):\n    \n     # LightGBM expects next three parameters need to be integer. So we make them integer\n     num_leaves = int(round(num_leaves))\n     min_data_in_leaf = int(round(min_data_in_leaf))\n     max_depth = int(round(max_depth))\n\n     assert type(num_leaves) == int\n     assert type(min_data_in_leaf) == int\n     assert type(max_depth) == int\n\n     param = {\n         'num_leaves': num_leaves,\n         'max_bin': 63,\n         'min_data_in_leaf': min_data_in_leaf,\n         'learning_rate': learning_rate,\n         'min_sum_hessian_in_leaf': min_sum_hessian_in_leaf,\n         'bagging_fraction': 1.0,\n         'bagging_freq': 5,\n         'feature_fraction': feature_fraction,\n         'lambda_l1': lambda_l1,\n         'lambda_l2': lambda_l2,\n         'min_gain_to_split': min_gain_to_split,\n         'max_depth': max_depth,\n         'save_binary': True, \n         'seed': 1337,\n         'feature_fraction_seed': 1337,\n         'bagging_seed': 1337,\n         'drop_seed': 1337,\n         'data_random_seed': 1337,\n         'objective': 'binary',\n         'boosting_type': 'gbdt',\n         'verbose': 1,\n         'metric': 'auc',\n         'is_unbalance': True,\n         'boost_from_average': False,   \n\n     }    \n    \n    \n     xg_train = lgb.Dataset(train.iloc[bayesian_tr_index][predictors].values,\n                            label=train.iloc[bayesian_tr_index][target].values,\n                            feature_name=predictors,\n                            free_raw_data = False\n                            )\n     xg_valid = lgb.Dataset(train.iloc[bayesian_val_index][predictors].values,\n                            label=train.iloc[bayesian_val_index][target].values,\n                            feature_name=predictors,\n                            free_raw_data = False\n                            )   \n\n     num_round = 5000\n     clf = lgb.train(param, xg_train, num_round, valid_sets = [xg_valid],feval=evalerror, verbose_eval=50,early_stopping_rounds = 50)\n    \n     predictions = clf.predict(train.iloc[bayesian_val_index][predictors].values, num_iteration=clf.best_iteration)   \n    \n     score = Gini(train.iloc[bayesian_val_index][target].values, predictions)\n    \n     return score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Bounded region of parameter space\nbounds_LGB = {\n     'num_leaves': (2, 5), \n     'min_data_in_leaf': (1, 10),  \n     'learning_rate': (0.03, 0.07),\n     'min_sum_hessian_in_leaf': (0.1, 0.5),    \n     'feature_fraction': (0.3, 0.7),\n     'lambda_l1': (0, 1), \n     'lambda_l2': (0, 1), \n     'min_gain_to_split': (0.1, 1.0),\n     'max_depth':(2,10),\n }\n\nfrom bayes_opt import BayesianOptimization\n\nLGB_BO = BayesianOptimization(LGB_bayesian, bounds_LGB, random_state=13)\n\nprint(LGB_BO.space.keys)\n\ninit_points = 10\nn_iter = 10\n\ntarget = 'target'\n#predictors = train.columns.values.tolist()[1:]\n\nprint('-' * 130)\n\nwith warnings.catch_warnings():\n    warnings.filterwarnings('ignore')\n    LGB_BO.maximize(init_points=init_points, n_iter=n_iter, acq='ucb', xi=0.0, alpha=1e-6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" LGB_BO.max   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_lgb = {\n    'max_bin': 63,\n    'bagging_fraction': 1.0,\n    'bagging_freq': 5,\n    'feature_fraction': 0.557897278957675,\n    'lambda_l1': 0.9962765743947803,\n    'lambda_l2': 0.9167636072131916,\n    'learning_rate': 0.05183596549633812,\n    'max_depth': int(5.23946365804394),\n    'min_data_in_leaf': int(9.214731014239497),\n    'min_gain_to_split': 0.7987771215261199,\n    'min_sum_hessian_in_leaf': 0.11869234058908429,\n    'num_leaves': int(4.686698789103291),   \n    'save_binary': True, \n    'seed': 1337,\n    'feature_fraction_seed': 1337,\n    'bagging_seed': 1337,\n    'drop_seed': 1337,\n    'data_random_seed': 1337,\n    'objective': 'binary',\n    'boosting_type': 'gbdt',\n    'verbose': 1,\n    'metric': 'auc',\n    'is_unbalance': True,\n    'boost_from_average': False\n    \n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nfold = 5\n\nskf = StratifiedKFold(n_splits=nfold, shuffle=True, random_state=2019)\n\noof = np.zeros(len(X))\npredictions = np.zeros((len(test),nfold))\n\ni = 1\nfor train_index, valid_index in skf.split(train, train.target.values):\n    print(\"\\nfold {}\".format(i))\n\n    xg_train = lgb.Dataset(train.iloc[train_index][predictors].values,\n                           label=train.iloc[train_index][target].values,\n                           feature_name=predictors,\n                           free_raw_data = False\n                           )\n    xg_valid = lgb.Dataset(train.iloc[valid_index][predictors].values,\n                           label=train.iloc[valid_index][target].values,\n                           feature_name=predictors,\n                           free_raw_data = False\n                           )   \n\n    \n    clf = lgb.train(param_lgb, xg_train, 10000000, valid_sets = [xg_valid],feval=evalerror, verbose_eval=250, early_stopping_rounds = 100)\n    oof[valid_index] = clf.predict(train.iloc[valid_index][predictors].values, num_iteration=clf.best_iteration) \n    \n    predictions[:,i-1] += clf.predict(test[predictors].values, num_iteration=clf.best_iteration)\n    i = i + 1\n\nprint(\"\\n\\nCV GINI: {:<0.8f}\".format(Gini(train.target.values, oof)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_bay = []\n\nfor i in range(len(predictions)):\n    lgb_bay.append(predictions[i][-1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub['target'] = lgb_bay\nsub.to_csv('sub6.csv', index = False, header = True)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}