{"cells":[{"metadata":{},"cell_type":"markdown","source":"## 1.목적\n     의뢰사: Porto Seguro\n     의뢰사 가치: 적정한 보험료 찾기\n     (좋은운전자는 보험료를 적게, 나쁜운전자에게는 보험료를 많이 부과하기 위함)\n     목표: 운전자가 내년에 자동차 보험 청구를 시작할 확률 예측\n## 2.평가지표\n     표준화 지니계수\n## 3.데이터정의"},{"metadata":{},"cell_type":"markdown","source":"# 소개\n\n#### 1.데이터의 검사\n#### 2.메타데이터\n#### 3.기술통계\n#### 4.불균형데이터처리\n#### 5.데이터품질점검\n#### 6.변수 엔지니어링\n#### 7.변수 선택\n#### 8.변수 스케일링"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.utils import shuffle\nfrom sklearn.ensemble import RandomForestClassifier\n\npd.set_option('display.max_columns',100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 데이터 로드(Loading data)"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 데이터확인(Data at first sight)"},{"metadata":{},"cell_type":"markdown","source":"데이터 설명:\n* ** 유사 그룹화에 속하는 특징에는 피쳐 이름(예: in, reg, car, calcal)과 같이 **라는 태그가 붙는다.\n* 피쳐 이름에는 이진 피쳐를 나타내는 포스트픽스 **bin**과 범주형 피쳐를 나타내는 **cat**가 포함된다. \n* 이러한 지정이 없는 특징 **은 연속 또는 서수**이다. \n* **-1*의 값은 해당 특징이 관찰 결과 ** 누락*임을 나타낸다. \n* **표적** 열은 해당 정책 보유자에 대한 청구 여부를 나타낸다."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"위의 결과로 다음을 확인할 수 있다. \n\n* 이진수 변수\n* 범주 값이 정수인 범주형 변수\n* 정수 또는 플로트 값이 있는 기타 변수\n* -1이 결측값을 나타내는 변수\n* 대상변수 및 ID변수"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"train 데이터에 중복된 행이 있는지 알아보자."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop_duplicates()\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"중복된 행이 없음을 확인할 수 있다. "},{"metadata":{"trusted":true},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"test 세트에 변수가 하나 빠졌는데 이게 target_value이기 때문에 괜찮습니다.\n이제 각 유형의 변수가 얼마나 많은지 호출해 봅시다.\n\n그래서 나중에 우리는 14개의 범주형 변수에 대한 더미 변수를 만들 수 있다. 빈 변수는 이미 2진수여서 더밍이 필요하지 않다."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"info()함수로  데이터 유형이 정수 또는 부동임을 알 수 있다. 데이터 집합에는 null 값이 없다. 결측값이 -1로 대체되기 때문"},{"metadata":{},"cell_type":"markdown","source":"## Metadata\n데이터 관리를 용이하게 하기 위해, 우리는 변수에 대한 메타 정보를 DataFrame에 저장할 것이다. 이는 분석, 시각화, 모델링 등을 위한 특정 변수를 선택하고자 할 때 유용할 것이다.\n\n구체적으로 보관할 내용:\n- **role**: 입력, ID, 대상\n- **level**: 공칭, 간격, 서수, 이진\n- **keep**: 참 또는 거짓\n- **dtype**: int, float, str"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = []\nfor f in train.columns:\n    # Defining the role\n    if f == 'target':\n        role = 'target'\n    elif f == 'id':\n        role = 'id'\n    else:\n        role = 'input'\n         \n    # Defining the level\n    if 'bin' in f or f == 'target':\n        level = 'binary'\n    elif 'cat' in f or f == 'id':\n        level = 'nominal'\n    elif train[f].dtype == float:\n        level = 'interval'\n    elif train[f].dtype == int:\n        level = 'ordinal'\n        \n    # Initialize keep to True for all variables except for id\n    keep = True\n    if f == 'id':\n        keep = False\n    \n    # Defining the data type \n    dtype = train[f].dtype\n    \n    # Creating a Dict that contains all the metadata for the variable\n    f_dict = {\n        'varname': f,\n        'role': role,\n        'level': level,\n        'keep': keep,\n        'dtype': dtype\n    }\n    data.append(f_dict)\n    \nmeta = pd.DataFrame(data, columns=['varname', 'role', 'level', 'keep', 'dtype'])\nmeta.set_index('varname', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"meta","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Example to extract all nominal variables that are not dropped(삭제되지 않은 모든 공칭 변수 추출 예제)"},{"metadata":{"trusted":true},"cell_type":"code","source":"meta[(meta.level == 'nominal') & (meta.keep)].index","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below the number of variables per role and level are displayed. (역할 및 수준당 변수 수 아래에 표시된다.)"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame({'count' : meta.groupby(['role', 'level'])['role'].size()}).reset_index()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 기술통계(Descriptive statistics)\n\n\n우리는 또한 데이터 프레임에 기술 방법을 적용할 수 있다. 그러나, 범주형 변수와 ID 변수에 대한 평균, std를 계산하는 것은 별로 말이 되지 않는다. 우리는 나중에 시각적으로 범주형 변수를 탐구할 것이다.\n\n메타 파일 덕분에 우리는 기술 통계를 계산할 변수를 쉽게 선택할 수 있다. 확실히 하기 위해, 우리는 데이터 유형별로 이것을 할 것이다\n\n자료에 특성에 따른 변수 분류\n:https://m.blog.naver.com/libido1014/120113775017\n\n![pic](C:\\Users\\PAVILION\\Pictures\\pic.png)"},{"metadata":{},"cell_type":"markdown","source":"### 간격변수(interval variables)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"v=meta[(meta.level=='interval') &(meta.keep)].index #행을 기준으로 인덱싱 :행의 이름들과 데이터형태 반환\ntrain[v].describe() #describe:통계치반환","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.reg 변수\n* ps_reg_03에만 누락된 값이 있음\n* 변수 사이의 범위(최소 ~ 최대)가 다르다. 스케일링(예: StandardScaler)을 적용할 수 있지만, 사용할 분류기에 따라 다르다.\n\n### 2.car 변수\n* ps_car_12 및 ps_car_15에 누락된 값이 있음\n* 다시, 범위가 달라져서 스케일링을 적용할 수 있다.데이터 스케일링이란 데이터 전처리 과정의 하나입니다.\n\n  **(스케일링:전처리 방법중 하나로 데이터 스케일링을 해주는 이유는 데이터의 값이 너무 크거나 혹은 작은 경우에 모델 알고리즘 학습과정에서 0으로 수렴하거나 무한으로 발산해버릴 수 있기 때문**\n\n### 3.calc 변수\n* 누락된 값 없음\n  이것은 최대치가 0.9이기 때문에 일종의 비율인 것 같다.\n* 세 개의 calculate 변수는 모두 매우 유사한 분포를 가지고 있다.\n\n* 전체적으로 보면 구간변수의 범위가 다소 작다는 것을 알 수 있다. 데이터를 익명화하기 위해 일부 변환(예: 로그)이 이미 적용되었는가?"},{"metadata":{},"cell_type":"markdown","source":"### 순서형 변수(ordinal cariables)"},{"metadata":{"trusted":true},"cell_type":"code","source":"v = meta[(meta.level == 'ordinal') & (meta.keep)].index\ntrain[v].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ps_car_11에 누락값이 있다. \n* 다양한 범위를 처리하기 위해 스케일링을 적용할 수 있음"},{"metadata":{},"cell_type":"markdown","source":"### 이진 변수(Binary variables)"},{"metadata":{"trusted":true},"cell_type":"code","source":"v = meta[(meta.level == 'binary') & (meta.keep)].index\ntrain[v].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- train 데이터의 평균값는 3.645%로 **강력 불균형(다른값은 대부분 10퍼센터 자리)** \n- 대부분의 변수에 대해 값이 0이라고 결론을 내릴 수 있다."},{"metadata":{},"cell_type":"markdown","source":"## 불균형 데이터 문제해결(Handling imbalanced classes)\n\n위에서 언급한 바와 같이 target 갯수가  target=1 < target=0 이다  \n이 문제를 해결하기 위한 두 가지 가능한 전략은 다음과 같다.\n* target=1을 사용하여 레코드 오버샘플링 \n* target=0으로 레코드 언더샘플링\n위의 방법으로 데이터 비율을 맞추면 정밀도(precision)가 향상된다.\n\n우리는 상당히 큰 트레이닝 세트를 가지고 있기 때문에 **undersampling**을 할 수 있다.(target=0이 다수이기 때문에 일분만 사용할 것임)\n\n\n* 오버샘플링: 소수 클래스 데이터를 증가\n\n* 언더샘플링 :다수 클래스 데이터에서 일부만 사용\n\n\n\n(데이터 클래스 비율이 너무 차이가 나면(highly-imbalanced data) 단순히 우세한 클래스를 택하는 모형의 정확도가 높아지므로 모형의 성능판별이 어려워진다. 즉, 정확도(accuracy)가 높아도 데이터 갯수가 적은 클래스의 재현율(recall-rate)이 급격히 작아지는 현상이 발생할 수 있다.\n\n이렇게 각 클래스에 속한 데이터의 갯수의 차이에 의해 발생하는 문제들을 비대칭 데이터 문제(imbalanced data problem)이라고 한다)"},{"metadata":{"trusted":true},"cell_type":"code","source":"desired_apriori=0.10\n\n# Get the indices per target value(target당 인덱스 가져오기)\nidx_0 = train[train.target == 0].index\nidx_1 = train[train.target == 1].index\n\n# Get original number of records per target value(target당 원본 레코드 수 가져오기)\nnb_0 = len(train.loc[idx_0])\nnb_1 = len(train.loc[idx_1])\n\n# Calculate the undersampling rate and resulting number of records with target=0\n  (목표값=0을 사용하여 언더샘플링 속도 및 결과 레코드 수 계산)\nundersampling_rate = ((1-desired_apriori)*nb_1)/(nb_0*desired_apriori)\nundersampled_nb_0 = int(undersampling_rate*nb_0)\nprint('Rate to undersample records with target=0: {}'.format(undersampling_rate))\nprint('Number of records with target=0 after undersampling: {}'.format(undersampled_nb_0))\n\n# Randomly select records with target=0 to get at the desired a priori\n  (원하는 우선순위를 얻기 위해 대상=0인 레코드를 무작위로 선택)\nundersampled_idx = shuffle(idx_0, random_state=37, n_samples=undersampled_nb_0)\n\n# Construct list with remaining indices(나머지 인덱스를 사용하여 목록 구성)\nidx_list = list(undersampled_idx) + list(idx_1)\n\n# Return undersample data frame(언더샘플 데이터 프레임 반환)\ntrain = train.loc[idx_list].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 데이터품질확인(Data Qulity Checks)\n\n### 결측값확인(Checking missing values)\n\n#### 결측값은 -1로 표현된다( Missing are represented as -1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"vars_with_missing=[]\n\nfor f in train.columns:\n    missings = train[train[f] == -1][f].count()\n    if missings>0:\n        vars_with_missing.append(f)\n        missings_perc=missings/train.shape[0]\n        \n        print('Variable {} has records ({:.2%}) with missing values'.format(f,missings,missings_perc)\n             )\n        print('In total, ther are {} variables with missing values'.format(len(vars_with_missing)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- **ps_car_03_cat과 ps_car_05_cat**는 값이 빠진 레코드의 비율이 크다. 이러한 변수를 제거해야한다. \n- 결측값이 있는 다른 범주형 변수에 대해서는 결측값 -1을 그대로 둘 수 있다.(????????)\n- **ps_reg_03** (연속형) 18%에 대한 값이 누락됨 **평균**으로 대체한다.\n- **ps_car_11**(범주형)에는 잘못된 값이 있는 5개의 레코드만 있다. 모드로 교체하십시오.\n- **ps_car_12*** (연속형)은 값이 빠진 레코드가 1개뿐입니다. **평균**으로 대체한다.\n- **ps_car_14*** (연속형) 모든 기록의 7%에 대한 값이 누락됨 **평균**으로 대체한다."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping the variables with too many missing values\n  (#결측값이 너무 많은 변수 삭제)\nvars_to_drop = ['ps_car_03_cat', 'ps_car_05_cat']\ntrain.drop(vars_to_drop, inplace=True, axis=1)\nmeta.loc[(vars_to_drop),'keep'] = False  # Updating the meta\n\n# Imputing with the mean or mode\n  (#평균 또는 모드로 임팩트하기)\nmean_imp = Imputer(missing_values=-1, strategy='mean', axis=0)\nmode_imp = Imputer(missing_values=-1, strategy='most_frequent', axis=0)\ntrain['ps_reg_03'] = mean_imp.fit_transform(train[['ps_reg_03']]).ravel()\ntrain['ps_car_12'] = mean_imp.fit_transform(train[['ps_car_12']]).ravel()\ntrain['ps_car_14'] = mean_imp.fit_transform(train[['ps_car_14']]).ravel()\ntrain['ps_car_11'] = mode_imp.fit_transform(train[['ps_car_11']]).ravel()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 범주형 변수의 카디널리티 확인\n카디널리티(Cardinity)는 **변수에서 서로 다른 값의 수**를 말한다. 나중에 범주형 변수로부터 더미 변수를 만들 것이므로, 많은 뚜렷한 값을 가진 변수가 있는지 확인할 필요가 있다. 우리는 이러한 변수들이 많은 더미 변수를 야기할 수 있기 때문에 다르게 다루어야 한다.\n\n더미변수:더미변수는 반드시 0과 1이다.\n1.남자:1 여자:2 => 0,1\n2.학년의 경우 더미변수의 개수는 2개\n-1학년여부:1학년=1,2~3학년=0\n-2학년여부:2학년=1 1학년과 3학년 0\n\n--------------------------------------------------------------------------------------\n\n* 더미변수: 더미변수는 범주형 변수를 연속형 변수로 변환한 것인데, 정확히 따지자면 연속형 변수\"스럽게\" 만든 것\n\n* 더미변수를 만드는 이유: 범주형 변수로는 사용할 수 없고 연속형 변수로만 가능한 분석기법을 사용할 수 있게 해준다(ex:선형회귀분석)"},{"metadata":{"trusted":true},"cell_type":"code","source":"v= meta[(meta.level == 'nominal') & (meta.keep)].index\n\nfor f in v:\n    dist_values=train[f].value_counts().shape[0]\n    print('Variable {} has {} distinct values'.format(f,dist_values))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ps_car_11_cat 만이 여전히 합리적이지만 많은 뚜렷한 값을 가지고 있다.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"#스무딩: 범주형 평균과 이전 평균의 균형을 맞추기 위한 스무딩 효과 /데이터 스무딩은 데이터에서 원치 않는 잡음이나 동작을 제거하는 기법\n# Script by https://www.kaggle.com/ogrellier\n# Code: https://www.kaggle.com/ogrellier/python-target-encoding-for-categorical-features\ndef add_noise(series, noise_level):\n    return series * (1 + noise_level * np.random.randn(len(series)))\n\ndef target_encode(trn_series=None, \n                  tst_series=None, \n                  target=None, \n                  min_samples_leaf=1, \n                  smoothing=1,\n                  noise_level=0):\n    \"\"\"\n    Smoothing is computed like in the following paper by Daniele Micci-Barreca\n    https://kaggle2.blob.core.windows.net/forum-message-attachments/225952/7441/high%20cardinality%20categoricals.pdf\n    trn_series : training categorical feature as a pd.Series\n    tst_series : test categorical feature as a pd.Series\n    target : target data as a pd.Series\n    min_samples_leaf (int) : minimum samples to take category average into account\n    smoothing (int) : smoothing effect to balance categorical average vs prior  \n    \"\"\" \n    assert len(trn_series) == len(target)\n    assert trn_series.name == tst_series.name\n    temp = pd.concat([trn_series, target], axis=1)\n    # Compute target mean \n    averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n    # Compute smoothing\n    smoothing = 1 / (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) / smoothing))\n    # Apply average function to all target data\n    prior = target.mean()\n    # The bigger the count the less full_avg is taken into account\n    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n    # Apply averages to trn and tst series\n    ft_trn_series = pd.merge(\n        trn_series.to_frame(trn_series.name),\n        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n        on=trn_series.name,\n        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n    # pd.merge does not keep the index so restore it\n    ft_trn_series.index = trn_series.index \n    ft_tst_series = pd.merge(\n        tst_series.to_frame(tst_series.name),\n        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n        on=tst_series.name,\n        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n    # pd.merge does not keep the index so restore it\n    ft_tst_series.index = tst_series.index\n    return add_noise(ft_trn_series, noise_level), add_noise(ft_tst_series, noise_level)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_encoded, test_encoded = target_encode(train[\"ps_car_11_cat\"], \n                             test[\"ps_car_11_cat\"], \n                             target=train.target, \n                             min_samples_leaf=100,\n                             smoothing=10,\n                             noise_level=0.01)\n    \ntrain['ps_car_11_cat_te'] = train_encoded\ntrain.drop('ps_car_11_cat', axis=1, inplace=True)\nmeta.loc['ps_car_11_cat','keep'] = False  # Updating the meta\ntest['ps_car_11_cat_te'] = test_encoded\ntest.drop('ps_car_11_cat', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 탐색적 데이터 시각화(Exploratory Data Visualization)"},{"metadata":{},"cell_type":"markdown","source":"## 범주형변수(Categorical variables)\n\n범주형 변수와 target=1 의 고객 비율을 보자"},{"metadata":{"trusted":true},"cell_type":"code","source":"v = meta[(meta.level == 'nominal') & (meta.keep)].index\n\nfor f in v:\n    plt.figure()\n    fig, ax = plt.subplots(figsize=(20,10))\n    # Calculate the percentage of target=1 per category value\n    cat_perc = train[[f, 'target']].groupby([f],as_index=False).mean()\n    cat_perc.sort_values(by='target', ascending=False, inplace=True)\n    # Bar plot\n    # Order the bars descending on target mean\n    sns.barplot(ax=ax, x=f, y='target', data=cat_perc, order=cat_perc[f])\n    plt.ylabel('% target', fontsize=18)\n    plt.xlabel(f, fontsize=18)\n    plt.tick_params(axis='both', which='major', labelsize=18)\n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**결측값**  을 변수에서 볼 수 있듯이, 결측값을 예를 들어 모드로 대체하는 대신 별도의 범주값으로 유지하는 것이 좋다. (결측값이 많은 고객들은 보험금 청구(target=1)를 요구할 확률이 훨씬 더 높은 것으로 보인다.??)\n"},{"metadata":{},"cell_type":"markdown","source":"### 간격변수 (Interval variable)\n\n간격 변수 간의 상관 관계 확인. Heat map은 변수들 사이의 상관 관계를 시각화하는 좋은 방법이다."},{"metadata":{"trusted":true},"cell_type":"code","source":"def corr_heatmap(v):\n    correlations = train[v].corr()\n\n    # Create color map ranging between two colors\n    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n    fig, ax = plt.subplots(figsize=(10,10))\n    sns.heatmap(correlations, cmap=cmap, vmax=1.0, center=0, fmt='.2f',\n                square=True, linewidths=.5, annot=True, cbar_kws={\"shrink\": .75})\n    plt.show();\n    \nv = meta[(meta.level == 'interval') & (meta.keep)].index\ncorr_heatmap(v)\n                ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"변수들 사이에는 강한 상관관계가 있다.\n- ps_reg_02 및 ps_reg_03(0.7)\n- ps_car_12 및 ps_car13(0.67)\n- ps_car_12 및 ps_car14(0.58)\n- ps_car_13 및 ps_car15(0.67)\n\nSeaborn은 변수들 사이의 (선형적인) 관계를 시각화할 수 있는 몇 가지 유용한 그래프 를 가지고 있다. 우리는 변수들 사이의 관계를 시각화하기 위해 *쌍구*를 사용할 수 있다. 하지만 열 지도에서 이미 제한된 수의 상관 변수를 보여 주었기 때문에, 각각의 고도로 상관관계가 있는 변수를 개별적으로 살펴보도록 하겠다.\n**NOTE**: 속도를 높이기 위해 train 데이터의 샘플을 채취한다."},{"metadata":{"trusted":true},"cell_type":"code","source":"s=train.sample(frac=0.1)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### ps_reg_02 및 ps_reg_03\n회귀선이 보여주듯이 이들 변수 사이에는 선형 관계가 있다. *hue* 매개변수 덕분에 우리는 target=0과 target=1의 회귀선이 같다는 것을 알 수 있다."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lmplot(x='ps_reg_02', y='ps_reg_03', data=s, hue='target', palette='Set1', scatter_kws={'alpha':0.3})\nplt.show()\n#lmplot:상관관계를 바로 알수있는 그래프\n#hue :X의 유무를 통해 회귀선을 알 수 있다. ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### ps_car_12 and ps_car_13"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lmplot(x='ps_car_12', y='ps_car_13', data=s, hue='target', palette='Set1', scatter_kws={'alpha':0.3   \n                                                                                           }\n          )\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### ps_car_12 and ps_car_14"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lmplot(x='ps_car_12', y='ps_car_14', data=s, hue='target', palette='Set1', scatter_kws={'alpha':0.3})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### ps_car_13 and ps_car_15"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lmplot(x='ps_car_15', y='ps_car_13', data=s, hue='target', palette='Set1', scatter_kws={'alpha':0.3})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"어떤 상관 변수를 유지할 것인지 어떻게 결정할 수 있는가? 변수에 대한 ** 주성분 분석(PCA) ** 을 수행하여 치수를 줄일 수 있다. 올스테이트 클레임 심각성 대회에서 나는 그것을 하기 위해 [이 kernel](https://www.kaggle.com/bertcarremans/reducing-number-of-numerical-features-with-pca)를 만들었다. 그러나 상관 변수의 수가 다소 적기 때문에 우리는 모델이 무거운 것을 하도록 할 것이다"},{"metadata":{},"cell_type":"markdown","source":"### 순서형변수의 상관계수 확인(Checking the correlations between ordinal variables)"},{"metadata":{"trusted":true},"cell_type":"code","source":"v= meta[(meta.level == 'ordinal')&(meta.keep)].index\ncorr_heatmap(v)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"순서형변수의 경우 우리는 많은 상관관계를 보지 못한다. 반면에 우리는 target으로 그룹화할 때 분포가 어떤지 살펴볼 수 있다."},{"metadata":{},"cell_type":"markdown","source":"## 변수 엔지니어링(Feature engineering)\n"},{"metadata":{},"cell_type":"markdown","source":"### 더미 변수 작성\n범주형 변수의 값은 어떤 순서나 크기를 나타내지 않는다. 따라서 우리는 그것을 다루기 위해 더미 변수를 만들 수 있다. 이 정보가 원래 변수의 범주에 대해 생성된 다른 더미 변수에서 도출될 수 있기 때문에 우리는 **첫 번째 더미 변수**를 떨어뜨린다.(???????????????????)\n\n* 더미변수: 더미변수는 범주형 변수를 연속형 변수로 변환한 것인데, 정확히 따지자면 연속형 변수\"스럽게\" 만든 것\n\n* 더미변수를 만드는 이유: 범주형 변수로는 사용할 수 없고 연속형 변수로만 가능한 분석기법을 사용할 수 있게 해준다(ex:선형회귀분석)"},{"metadata":{"trusted":true},"cell_type":"code","source":"v = meta[(meta.level == 'nominal') & (meta.keep)].index\nprint('Before dummification we have {} variables in train'.format(train.shape[1]))\ntrain = pd.get_dummies(train, columns=v, drop_first=True)\nprint('After dummification we have {} variables in train'.format(train.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"따라서 더미 변수를 생성하면 52개의 변수가 훈련 세트에 추가된다","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 교호작용 변수 생성(Creating interaction variables)"},{"metadata":{"trusted":true},"cell_type":"code","source":"v = meta[(meta.level == 'interval') & (meta.keep)].index\npoly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\ninteractions = pd.DataFrame(data=poly.fit_transform(train[v]), columns=poly.get_feature_names(v))\ninteractions.drop(v, axis=1, inplace=True)  # Remove the original columns\n# Concat the interaction variables to the train data\nprint('Before creating interactions we have {} variables in train'.format(train.shape[1]))\ntrain = pd.concat([train, interactions], axis=1)\nprint('After creating interactions we have {} variables in train'.format(train.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"이것은 train 데이터에 추가적인 상호작용 변수를 추가한다. *get_feature_names* 방법 덕분에 열 이름을 다음 항목에 할당할 수 있음 \n새로운 변수"},{"metadata":{},"cell_type":"markdown","source":"## 변수선택(Feature selection)"},{"metadata":{},"cell_type":"markdown","source":"### 분산이 낮거나 0인 피쳐 제거(Removing features with low or zero variance)\n\n\n##### 변수의 분산이 거의 0인 경우\n예를 들어 데이터 1000개중에 990개에서 변수 A열의 값이 0이고, 나머지 10개가 10이라면 \n이 변수 A는 행을 구분하는데 거으이 의미가 없다. 이런변수는 분산이 거의 0인경우 분산이 겅의 0인 변수는\n제거해준다. \n\nhttps://jkook.tistory.com/19"},{"metadata":{},"cell_type":"markdown","source":"개인적으로, 나는 분류자 알고리즘이 유지할 특징을 선택하도록 하는 것을 선호한다. 그러나 우리가 스스로 할 수 있는 일이 하나 있다. 그것은 전혀 또는 매우 낮은 분산이 있는 특징들을 제거하는 것이다. Sklearn은 이를 위한 편리한 방법을 가지고 있다: **변성임계값**. 기본적으로 분산 없이 형상을 제거한다. 이는 이전 단계에서 무분산 변수가 없는 것을 보았기 때문에 이 경기에는 적용되지 않을 것이다. 그러나 1% 미만의 분산을 가진 형상을 제거한다면 31개의 변수를 제거하게 될 것이다"},{"metadata":{"trusted":true},"cell_type":"code","source":"selector = VarianceThreshold(threshold=.01)\nselector.fit(train.drop(['id', 'target'], axis=1)) # Fit to train without id and target variables\n\nf = np.vectorize(lambda x : not x) # Function to toggle boolean array elements\n\nv = train.drop(['id', 'target'], axis=1).columns[f(selector.get_support())]\nprint('{} variables have too low variance.'.format(len(v)))\nprint('These variables are {}'.format(list(v)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"만약 우리가 분산에 근거하여 선택한다면, 우리는 오히려 많은 변수들을 잃게 될 것이다. 그런데 워낙 변수가 많지 않기 때문에 selector가 선택하도록 하겠다. 변수가 많은 데이터 세트의 경우, 이는 처리 시간을 줄일 수 있다.\n\n스클리어엔 다른 [기능 선택 방법](http://scikit-learn.org/stable/modules/feature_selection.html)도 함께 제공된다. 이러한 방법 중 하나는 다른 분류자가 최상의 형상을 선택하고 이를 계속하도록 하는 *SelectFromModel*이다. 아래는 랜덤 포레스트로 어떻게 하는지 보여줄게."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = train.drop(['id', 'target'], axis=1)\ny_train = train['target']\n\nfeat_labels = X_train.columns\n\nrf = RandomForestClassifier(n_estimators=1000, random_state=0, n_jobs=-1)\n\nrf.fit(X_train, y_train)\nimportances = rf.feature_importances_\n\nindices = np.argsort(rf.feature_importances_)[::-1]\n\nfor f in range(X_train.shape[1]):\n    print(\"%2d) %-*s %f\" % (f + 1, 30,feat_labels[indices[f]], importances[indices[f]]))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"SelectFromModel을 사용하여 사용할 사전 적합 분류기와 형상 가져오기의 임계값을 지정할 수 있다. *get_support* 방법으로 열차 데이터의 변수 수를 제한할 수 있다"},{"metadata":{"trusted":true},"cell_type":"code","source":"sfm = SelectFromModel(rf, threshold='median', prefit=True)\nprint('Number of features before selection: {}'.format(X_train.shape[1]))\nn_features = sfm.transform(X_train).shape[1]\nprint('Number of features after selection: {}'.format(n_features))\nselected_vars = list(feat_labels[sfm.get_support()])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train[selected_vars + ['target']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 변수 스케일링(Feature scaling)\n앞에서 언급했듯이, 우리는 훈련 데이터에 표준 스케일링을 적용할 수 있다. 어떤 selector들은 이것이 완성되었을 때 더 나은 성능을 발휘한다."},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\nscaler.fit_transform(train.drop(['target'], axis=1))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}