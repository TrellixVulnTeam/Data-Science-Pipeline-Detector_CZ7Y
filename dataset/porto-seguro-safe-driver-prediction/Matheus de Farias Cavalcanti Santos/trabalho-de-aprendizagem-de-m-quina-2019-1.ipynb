{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Pós-Graduação em Ciências da Computação - CIN/UFPE - 2019.1\n## Projeto de Aprendizagem de Máquina"},{"metadata":{},"cell_type":"markdown","source":"> #### Equipe: \n- Filipe C. L. Duarte (fcld) \n- Hélio Gonçalves de Souza Junior (hgsj) \n- Matheus de Farias Cavalcanti Santos (mfcs)"},{"metadata":{},"cell_type":"markdown","source":"## Sumário\n\n1. [Introdução do Contexto da Competição](#Introdução)\n2. [Conhecendo os Dados](#Conhecendo_os_dados)\n3. [Análise Exploratória dos Dados](#analise_exploratoria)\n4. [Pré-processamento](#pre_proc)\n5. [Modelagem](#mod)\n6. [Submissão - dados de teste](#submission)\n7. [Considerações Finais](#consideracoes)\n8. [Apêndice](#apendice)"},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"Introdução\"></a>\n## Introdução do Contexto da Competição\n\nA Porto Seguro, uma das maiores seguradoras de automóveis e residências do Brasil, vem notando que imprecisões nas previsões de sinistro da companhia de seguros de automóveis aumentam o custo do seguro de bons motoristas e reduzem o preço dos maus.\n\nEsta competição, tem como objetivo criar um modelo que prevê a probabilidade de um motorista iniciar uma reivindicação de seguro de automóvel no próximo ano (sinistralidade). Uma previsão mais precisa permite adaptar ainda mais os seus preços e esperamos tornar a cobertura do seguro de automóvel mais acessível à mais condutores.\n\n[Link para a competição](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction)"},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"Conhecendo_os_dados\"></a>\n## Conhecendo os Dados\n\n### Import das bibliotecas utilizadas"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.plotly as py\nimport plotly.graph_objs as go\nfrom scipy import stats\nfrom scipy.stats import kstest\nfrom scipy.stats import ks_2samp\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.model_selection import KFold, StratifiedKFold, cross_val_score, GridSearchCV\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Carregando as bases\n\nOs Dados foram divididos em **Treinamento(train)** e **Teste(test)**\n* **train**: Dados disponíveis para desenvolvimento do algoritmo de classificação;\n* **test**: Dados para submissão da competição, ou seja, base não rótulada.\n\n**OBS**: A partir disto, todas as análises feitas à priori terão como referência a base de treinamento, **train**."},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualizando as 5 primeiras linhas da base de treinamento\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Estrutura da base quanto ao número de linhas e colunas: (linhas, colunas)\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Algumas informações sobre os dados:\n\n* 59 **Variáveis** e 595212 **Segurados**\n    * Variáveis:\n        * **id** é a chave do segurado (Código único)\n        * **\"bin\"** representam variáveis binárias\n        * **\"cat\"** representam variáveis categóricas\n        * As demais variáveis são númericas\n        * **target** significa se o segurado incorreu em sinistou ou não (Variável de interesse/resposta)\n* Os dados omissos, **missing data**, foram codificados com o valor **-1**.\n\nPortanto, o objetivo deste projeto é construir um algoritmo de classificação para prever se o segurado irá sinistrar."},{"metadata":{"trusted":true},"cell_type":"code","source":"Tab = pd.crosstab(index=train[\"target\"],columns=\"QTD\")\nTabela = pd.concat([Tab,100*(Tab/Tab.sum())],axis=1)\nTabela.columns = [\"QTD\",\"%TOTAL\"]\nTabela.index = [\"NÃO SINISTRO\",\"SINISTRO\"]\nTabela    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Verificar se existe dados duplicados, ou seja, segurados repetidos na base."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop_duplicates()\ntrain = train.drop(['id'], axis = 1)\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"analise_exploratoria\"></a>\n## Análise Exploratória dos Dados\nA partir dos dados, observa-se:\n* variáveis binárias;\n* variáveis categóricas codificadas com valores inteiros;\n* variáveis restantes com valores reais (float) ou inteiros;\n* -1 representando os valores faltantes;\n* a variável **target** e o **id**.\n\nO método `describe` apresenta as estatísticas descritivas para todas as colunas do `dataframe`. \nContudo, só fará sentido aplicá-lo nas variáveis contínuas, isto é, naquelas representadas pelo conjunto dos valores reais. \nPara analisar as variáveis categóricas, utilizaremos os gráficos na análise exploratória. "},{"metadata":{},"cell_type":"markdown","source":"#### Criando vetores com os nome das variáveis pelos grupos (reg, bin, car ...)"},{"metadata":{"trusted":true},"cell_type":"code","source":"colunas = train.columns.tolist()\ncolunas_reg = [col for col in colunas if 'reg' in col]\ncolunas_cat = [col for col in colunas if 'cat' in col]\ncolunas_bin = [col for col in colunas if 'bin' in col]\ncolunas_car = [col for col in colunas if 'car' in col and 'cat' not in col]\ncolunas_calc = [col for col in colunas if 'calc' in col]\nprint(colunas_cat)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Analisando as variáveis do grupo *reg* - Numérica"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.loc[:,colunas_reg].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Apenas a variável **reg_03** possui *missing*. \nPara solucionar esse problema, vamos usar um método de imputar a mediana onde existe o valor **-1**. \nApós, faremos a normalização *min-max* em **reg_02** e **reg_03** para reduzir a escala, fixando-as no intervalo **[0,1]**."},{"metadata":{},"cell_type":"markdown","source":"### Matriz de Correlação para *reg*"},{"metadata":{"trusted":true},"cell_type":"code","source":"continuas = [colunas_reg]\ndef correl(t):\n    correlacao = train[t].corr()\n    cmap = sns.diverging_palette(220, 10, as_cmap = True)\n\n    fig, ax = plt.subplots(figsize = (10,10))\n    sns.heatmap(correlacao, cmap = cmap, vmax = 1.0, center = 0, fmt = '.2f',\n           square = True, linewidths = .5, annot = True, cbar_kws ={\"shrink\": .75})\n    plt.show();\n    \n# Variáveis reg\nfor j in continuas:\n    correl(j)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Análise Bivariada para *reg*"},{"metadata":{},"cell_type":"markdown","source":"Testando se há diferença estatística, para variável com o termo **'reg'**, entre os indivíduos que sinistraram dos indivíduos que não sinistraram.\n<br>\nTestes estatísticos aplicados: \n* **t-Student**\n* **Kolmogorov-Smirnov**"},{"metadata":{"trusted":true},"cell_type":"code","source":"r = train.loc[:, colunas_reg]\nreg = pd.concat([train.target,train.loc[:, colunas_reg]],axis=1)\n#Tabela = reg.pivot_table(index=[\"target\"], aggfunc=np.mean)\n#Tabela\n\ndf_0 = reg[reg['target'] == 0]\ndf_1 = reg[reg['target'] == 1]\n\n#stats.ttest_ind(df_0.ps_calc_02,df_1.ps_calc_02).pvalue\n\nvar_MP = []\nfor f1 in r.columns:\n    MP = stats.ttest_ind(df_0[f1],df_1[f1]).pvalue\n    if MP < 0.05:\n        var_MP.append(f1)\n        print('Variável {} tem o pvalor {:.2}'.format(f1,MP))\nprint('Qtd de variáveis com média diferentes {} de um total de {}'.format(len(var_MP),r.shape[1]))\n\nvar_KS = []\nfor f2 in r.columns:\n    KSS = ks_2samp(df_0[f2],df_1[f2]).statistic\n    KSP = ks_2samp(df_0[f2],df_1[f2]).pvalue\n    if KSP < 0.05:\n        var_KS.append(f2)\n        print('A Variável {} tem um KS de {:.2} com um pvalue de {:.2}'.format(f2,KSS,KSP))\nprint('Qtd de variáveis com as distribuições diferentes {} de um total de {}'.format(len(var_KS),r.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Tabela = reg.pivot_table(index=[\"target\"], aggfunc=np.mean)\nTabela","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Analisando as variáveis do grupo *car* - Numérica"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.loc[:, colunas_car].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Apenas **ps_car_12** e **ps_car_15** possuem dados faltantes. Vamos aplicar a normalização *min-max* para padronizar a escala."},{"metadata":{},"cell_type":"markdown","source":"### Matriz de Correlação para *car*"},{"metadata":{"trusted":true},"cell_type":"code","source":"continuas = [colunas_car]\ndef correl(t):\n    correlacao = train[t].corr()\n    cmap = sns.diverging_palette(220, 10, as_cmap = True)\n\n    fig, ax = plt.subplots(figsize = (10,10))\n    sns.heatmap(correlacao, cmap = cmap, vmax = 1.0, center = 0, fmt = '.2f',\n           square = True, linewidths = .5, annot = True, cbar_kws ={\"shrink\": .75})\n    plt.show();\n    \n# Variáveis reg\nfor j in continuas:\n    correl(j)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Análise Bivariada para *car*"},{"metadata":{},"cell_type":"markdown","source":"Testando se há diferença estatística, para variável com o termo **'car'**, entre os indivíduos que sinistraram dos indivíduos que não sinistraram.\n<br>\nTestes estatísticos aplicados: \n* **t-Student**\n* **Kolmogorov-Smirnov**"},{"metadata":{"trusted":true},"cell_type":"code","source":"c1 = train.loc[:, colunas_car]\ncar = pd.concat([train.target,train.loc[:, colunas_car]],axis=1)\ndf_0 = car[car['target'] == 0]\ndf_1 = car[car['target'] == 1]\n\n#stats.ttest_ind(df_0.ps_calc_02,df_1.ps_calc_02).pvalue\n\nvar_MP = []\nfor f3 in c1.columns:\n    MP = stats.ttest_ind(df_0[f3],df_1[f3]).pvalue\n    if MP < 0.05:\n        var_MP.append(f3)\n        print('Variável {} tem o pvalor {:.2}'.format(f3,MP))\nprint('Qtd de variáveis com média diferentes {} de um total de {}'.format(len(var_MP),c1.shape[1]))\n\nvar_KS = []\nfor f4 in c1.columns:\n    KSS = ks_2samp(df_0[f4],df_1[f4]).statistic\n    KSP = ks_2samp(df_0[f4],df_1[f4]).pvalue\n    if KSP < 0.05:\n        var_KS.append(f4)\n        print('A Variável {} tem um KS de {:.2} com um pvalue de {:.2}'.format(f4,KSS,KSP))\nprint('Qtd de variáveis com as distribuições diferentes {} de um total de {}'.format(len(var_KS),c1.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Tabela = car.pivot_table(index=[\"target\"], aggfunc=np.mean)\nTabela","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Analisando as variáveis do grupo *calc* - Numérica"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.loc[:, colunas_calc].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Matriz de Correlação para *calc*"},{"metadata":{"trusted":true},"cell_type":"code","source":"continuas = [colunas_calc]\ndef correl(t):\n    correlacao = train[t].corr()\n    cmap = sns.diverging_palette(220, 10, as_cmap = True)\n\n    fig, ax = plt.subplots(figsize = (10,10))\n    sns.heatmap(correlacao, cmap = cmap, vmax = 1.0, center = 0, fmt = '.2f',\n           square = True, linewidths = .5, annot = True, cbar_kws ={\"shrink\": .75})\n    plt.show();\n    \n# Variáveis reg\nfor j in continuas:\n    correl(j)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Análise Bivariada para *calc*"},{"metadata":{},"cell_type":"markdown","source":"Testando se há diferença estatística, para variável com o termo **'calc'**, entre os indivíduos que sinistraram dos indivíduos que não sinistraram.\n<br>\nTestes estatísticos aplicados: \n* **t-Student**\n* **Kolmogorov-Smirnov**"},{"metadata":{"trusted":true},"cell_type":"code","source":"c2 = train.loc[:, colunas_calc]\ncalc = pd.concat([train.target,train.loc[:, colunas_calc]],axis=1)\n#Tabela = calc.pivot_table(index=[\"target\"], aggfunc=np.mean)\n#Tabela\n\ndf_0 = calc[calc['target'] == 0]\ndf_1 = calc[calc['target'] == 1]\n\n#stats.ttest_ind(df_0.ps_calc_02,df_1.ps_calc_02).pvalue\n\nvar_MP = []\nfor f5 in c2.columns:\n    MP = stats.ttest_ind(df_0[f5],df_1[f5]).pvalue\n    if MP < 0.05:\n        var_MP.append(f5)\n        print('Variável {} tem o pvalor {:.2}'.format(f5,MP))\nprint('Qtd de variáveis com média diferentes {} de um total de {}'.format(len(var_MP),c2.shape[1]))\n\nvar_KS = []\nfor f6 in c2.columns:\n    KSS = ks_2samp(df_0[f6],df_1[f6]).statistic\n    KSP = ks_2samp(df_0[f6],df_1[f6]).pvalue\n    if KSP < 0.05:\n        var_KS.append(f6)\n        print('A Variável {} tem um KS de {:.2} com um pvalue de {:.2}'.format(f6,KSS,KSP))\nprint('Qtd de variáveis com as distribuições diferentes {} de um total de {}'.format(len(var_KS),c2.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Tabela = calc.pivot_table(index=[\"target\"], aggfunc=np.mean)\nTabela","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**OBS:** Através dos testes **t-Student** e **Kolmogorov-Smirnov** percebemos que algumas das variáveis contínuas poderiam ser retiradas do modelo.\nAs variáveis contínuas a serem retiradas serão as que possuírem o termo **'calc'**."},{"metadata":{},"cell_type":"markdown","source":"### Checando os valores omissos (*missing*)\nOs valores omissos foram representados pelo valor -1."},{"metadata":{"trusted":true},"cell_type":"code","source":"var_missing = []\n\nfor f in train.columns:\n    missings = train[train[f] == -1][f].count()\n    if missings > 0:\n        var_missing.append(f)\n        missings_perc = missings/train.shape[0]\n        \n        print('Variável {} tem {} exemplos ({:.2%}) com valores omissos'.format(f, missings, missings_perc))\n        \nprint('No total, existem {} variáveis com valores omissos'.format(len(var_missing)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **ps_car_03_cat** e **ps_car_05_cat** têm uma elevada quantidade de dados omissos - optamos pela remoção dessas variáveis.\n* Para a variável **ps_reg_03** e **ps_car_14**, aplicaremos a imputação da média.\n* Enquanto que, para **ps_car_11**, imputaremos a moda (valor mais frequente).\n* Para as demais variáveis categóricas, optamos por deixar o missing como uma característica."},{"metadata":{"trusted":true},"cell_type":"code","source":"variaveis_excluir = ['ps_car_03_cat', 'ps_car_05_cat']\ntrain.drop(variaveis_excluir, inplace=True, axis=1)\ntrain.drop(colunas_calc, inplace=True, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Analisando as variáveis do grupo *cat* - Categóricas"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat = pd.concat([train.target,train.loc[:, colunas_cat]],axis=1)\ntab = 100*(cat.pivot_table(index=[\"ps_car_02_cat\"], values = ['target'],aggfunc=[np.mean]))\ntab2= pd.crosstab(cat[\"ps_car_02_cat\"],cat[\"target\"])\ntab3= tab2[0]+tab2[1]\ntab4= 100*(tab3/tab3.sum())\n\nTabela = pd.concat([tab2,tab3,tab4,tab],axis=1)\nTabela.columns = [\"N Sinistro\",\"Sinistro\",\"Total\",\"%Total\",\"%Taxa Sinistro\"]\nTabela","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c3 = train.loc[:, colunas_cat]\ncat = pd.concat([train.target,train.loc[:, colunas_cat]],axis=1)\n#Tabela = calc.pivot_table(index=[\"target\"], aggfunc=np.mean)\n#Tabela\n\ndf_0 = cat[cat['target'] == 0]\ndf_1 = cat[cat['target'] == 1]\n\nvar_KS = []\nfor f7 in c3.columns:\n    KSS = ks_2samp(df_0[f7],df_1[f7]).statistic\n    KSP = ks_2samp(df_0[f7],df_1[f7]).pvalue\n    if KSP < 0.05:\n        var_KS.append(f7)\n        print('A Variável {} tem um KS de {:.2} com um pvalue de {:.2}'.format(f7,KSS,KSP))\nprint('Qtd de variáveis com as distribuições diferentes {} de um total de {}'.format(len(var_KS),c3.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Analisando as variáveis do grupo *bin* - Binárias"},{"metadata":{"trusted":true},"cell_type":"code","source":"c4 = train.loc[:, colunas_bin]\nbbin = pd.concat([train.target,train.loc[:, colunas_bin]],axis=1)\n#Tabela = calc.pivot_table(index=[\"target\"], aggfunc=np.mean)\n#Tabela\n\ndf_0 = bbin[bbin['target'] == 0]\ndf_1 = bbin[bbin['target'] == 1]\n\nvar_KS = []\nfor f8 in c4.columns:\n    KSS = ks_2samp(df_0[f8],df_1[f8]).statistic\n    KSP = ks_2samp(df_0[f8],df_1[f8]).pvalue\n    if KSP < 0.05:\n        var_KS.append(f8)\n        print('A Variável {} tem um KS de {:.2} com um pvalue de {:.2}'.format(f8,KSS,KSP))\nprint('Qtd de variáveis com as distribuições diferentes {} de um total de {}'.format(len(var_KS),c4.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Por grupos, as variáveis elegívies seriam:\n* **reg**: `ps_reg_01 & ps_reg_02 & ps_reg_03`\n* **car**: `ps_car_11 & ps_car_12 & ps_car_13 & ps_car_14 & ps_car_15`\n* **calc**: Nenhuma das variáveis\n* **cat**: `ps_ind_02_cat & ps_ind_04_cat & ps_ind_05_cat & ps_car_01_cat & ps_car_02_cat & ps_car_04_cat & ps_car_06_cat & ps_car_07_cat & ps_car_08_cat & ps_car_09_cat & ps_car_11_cat &`\n* **bin**: `ps_ind_06_bin & ps_ind_07_bin & ps_ind_08_bin & ps_ind_09_bin & ps_ind_16_bin & ps_ind_17_bin`\n\nUm Total de 26 variáveis, sendo 9 continuas e 17 categoricas."},{"metadata":{},"cell_type":"markdown","source":"### Criou-se gráficos de barras para as variáveis categóricas e foi feita uma análise de como está a distribuição e os dados omissos."},{"metadata":{"trusted":true},"cell_type":"code","source":"colunas_cat.remove('ps_car_03_cat')\ncolunas_cat.remove('ps_car_05_cat')\nprint(colunas_cat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in colunas_cat:\n    plt.figure()\n    fig, ax = plt.subplots(figsize = (20,10))\n    sns.barplot(ax = ax, x = i, y = 'target', data = train)\n    plt.ylabel('% target', fontsize = 18)\n    plt.xlabel(i, fontsize = 18)\n    plt.tick_params(axis = 'both', which= 'major', labelsize = 18)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**OBS** Verificamos que os dados omissos (*missing*) são representativos. Dessa forma, optamos por deixá-los como uma categoria adicional, pois o segurado que não apresentou informação apresenta maior probabilidade de sinistrar (acidentar)."},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"pre_proc\"></a>\n## Pré-processamento"},{"metadata":{},"cell_type":"markdown","source":"Imputação de valores (média e moda)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Imputando com a média e a moda\nmedia_imp = Imputer(missing_values=-1, strategy='mean', axis=0)\nmoda_imp = Imputer(missing_values=-1, strategy='most_frequent', axis=0)\ntrain['ps_reg_03'] = media_imp.fit_transform(train[['ps_reg_03']]).ravel()\ntrain['ps_car_14'] = media_imp.fit_transform(train[['ps_car_14']]).ravel()\ntrain['ps_car_11'] = moda_imp.fit_transform(train[['ps_car_11']]).ravel()\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Codificação das variáveis 'int64' como 'category'"},{"metadata":{},"cell_type":"markdown","source":"Fizemos a codificação em category para realizar o procedimento de one-hot-encoding das variáveis categóricas."},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in train.columns:\n    if train[i].dtype == 'int64' and i != 'target':\n        train[i] = train[i].astype('category')\ntrain.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Vamos separar a variável target dos atributos, criando uma matriz X e um vetor Y."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train.drop([\"target\"], axis = 1)\ny = train[\"target\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checando a dimensão\nX.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### One-hot-encoding nas variáveis categóricas"},{"metadata":{},"cell_type":"markdown","source":"Para as variáveis que possuem categóricas realizamos o processo de one-hot-encoding que é a criação de atributos (variáveis) para cada categoria da variável. Essas variáveis serão binárias, assumindo o valor 1 quando da presença da categoria, e 0, na ausência."},{"metadata":{"trusted":true},"cell_type":"code","source":"# função get_dummies transforma as categorias em variáveis binárias\nX = pd.get_dummies(X)\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checando a dimensão\nX.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Divindindo os dados em treinamento e teste"},{"metadata":{},"cell_type":"markdown","source":"Vamos dividir os dados em treinamento e teste para realizar a avaliação por meio da validação cruzada com o intuito de melhorar a capacidade de generalização dos classificadores. Usamos a função train_test_split do sklearn com a amostragem estratificada para manter a proporção da variável target."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(\n    X, y, stratify=y, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Normalização min-max "},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_train\nscaler = MinMaxScaler()\nscaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n\n# X\nscaler = MinMaxScaler()\nscaler.fit(X)\nX_scaled = scaler.transform(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Coeficiente de Gini Normalizado"},{"metadata":{},"cell_type":"markdown","source":"Utilizamos a implementação do coeficiente de gini normalizado em python obtido neste fórum: \nhttp://www.kaggle.com/c/ClaimPredictionChallenge/discussion/703"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Coeficiente de Gini Normalizado:\ndef gini(actual, pred):\n    assert (len(actual) == len(pred))\n    all = np.asarray(np.c_[actual, pred, np.arange(len(actual))], dtype=np.float)\n    all = all[np.lexsort((all[:, 2], -1 * all[:, 1]))]\n    totalLosses = all[:, 0].sum()\n    giniSum = all[:, 0].cumsum().sum() / totalLosses\n\n    giniSum -= (len(actual) + 1) / 2.\n    return giniSum / len(actual)\n\n\ndef gini_normalized(actual, pred):\n    return gini(actual, pred) / gini(actual, actual)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"mod\"></a>\n## Modelagem"},{"metadata":{},"cell_type":"markdown","source":"### Modelos"},{"metadata":{},"cell_type":"markdown","source":"Foram selecionados 3 modelos:\n1. Regressão Logística\n2. Random Forest\n3. XGBoost"},{"metadata":{},"cell_type":"markdown","source":"### 1. Regressão Logística"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Testando com a regressão logística com penalização L2\nlr = LogisticRegression(penalty='l2', random_state=1)\nlr.fit(X_train_scaled, y_train)\nprob = lr.predict_proba(X_test_scaled)[:,1]\nprint(\"Índice de Gini normalizado para a Regressão Logística: \",gini_normalized(y_test, prob))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Forest com 20 árvores\nrf = RandomForestClassifier(n_estimators = 20, max_depth = 4, random_state = 1, max_features = 20)\nrf.fit(X_train_scaled, y_train)\npredictions_prob = rf.predict_proba(X_test_scaled)[:,1]\nprint(\"Índice de Gini normalizado para o Random Forest: \", gini_normalized(y_test, predictions_prob))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"# taxa de aprendizagem = 0.05\nxgb = XGBClassifier(max_depth=5, n_estimators=100, learning_rate=0.05, random_state = 1)\nxgb.fit(X_train_scaled, y_train)\nprob_xgb = xgb.predict_proba(X_test_scaled)[:,1]\nprint(\"--------------------------------------------------------------------------------------------\")\nprint(\"Coef. de Gini normalizado para o XGBoost com learning_rate = 0.05: \", gini_normalized(y_test, prob_xgb))\nprint(\"--------------------------------------------------------------------------------------------\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Verificando o coeficiente de gini nos dados de treinamento para a Regressão Logística e XGBoost\nVerificando qual é o coeficiente de gini para os dados de treinamento:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Testaremos nos dados completo de treinamento.\nprob_xgb_y = xgb.predict_proba(X_scaled)[:,1]\nprint(\"--------------------------------------------------------------------------------------------\")\nprint(\"Coef. de Gini para o XGBoost com learning_rate = 0.05 nos dados de treinamento: \", gini_normalized(y, prob_xgb_y))\nprint(\"--------------------------------------------------------------------------------------------\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Diante desses resultados, optamos por submeter os resultados com a utilização do modelo **XGBoost**."},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"submission\"></a>\n## Submissão - dados de teste"},{"metadata":{},"cell_type":"markdown","source":"Faremos a submissão do modelo XGBoost após verificar que é o modelo que apresenta o melhor score do coeficiente de gini para os dados de treinamento.\nSerá necessário realizar o pré-processamento nos dados de teste."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importação dos dados de teste\ntest = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Excluindo dados duplicados\ntest.drop_duplicates()\n# salvando id\ntest_id = test['id']\ntest.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Excluindo 'id'\ntest = test.drop(['id'], axis = 1)\ntest.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Excluindo as variáveis 'calc' e 'ps_car_03_cat' e 'ps_car_05_cat'\ntest.drop(variaveis_excluir, inplace=True, axis=1)\ntest.drop(colunas_calc, inplace=True, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Imputando a média e a moda\nmedia_imp = Imputer(missing_values=-1, strategy='mean', axis=0)\nmoda_imp = Imputer(missing_values=-1, strategy='most_frequent', axis=0)\ntest['ps_reg_03'] = media_imp.fit_transform(test[['ps_reg_03']]).ravel()\ntest['ps_car_14'] = media_imp.fit_transform(test[['ps_car_14']]).ravel()\ntest['ps_car_11'] = moda_imp.fit_transform(test[['ps_car_11']]).ravel()\ntest.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# transformando variáveis em 'category'\nfor i in test.columns:\n    if test[i].dtype == 'int64':\n        test[i] = test[i].astype('category')\ntest.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# One-hot encoding\n# função get_dummies transforma as categorias em variáveis binárias\ntest = pd.get_dummies(test)\ntest.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# garantindo que a base de teste sejam as mesmas colunas da base de treinamento\nmissing_cols = set( X.columns ) - set( test.columns )\nfor c in missing_cols:\n    test[c] = 0\ntest = test[X.columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Normalização min-max\nscaler = MinMaxScaler()\nscaler.fit(test)\ntest_scaled = scaler.transform(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Criando base para submissão com o modelo XGBoost\nprob_xgb_teste = xgb.predict_proba(test_scaled)[:,1]\n# Em results_df está a base de teste escorada, a coluna target possui as probabilidades\nresults_df = pd.DataFrame(data={'id':test_id, 'target':prob_xgb_teste})\nprint(results_df)\nresults_df.to_csv('submission1.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"consideracoes\"></a>\n## Considerações Finais"},{"metadata":{},"cell_type":"markdown","source":"#### Algumas considerações finais sobre as etapas desenvolvidas no projeto:\n- **Pré-processamento dos dados**\n    - Utilização do One Hot Encoding nas variáveis categóricas: Obtivemos resultados semelhantes quando não utilizamos essa abordagem. A opção pela criação de variáveis binárias (one-hot encoding) surgiu em virtude dela conseguir controlar e reduzir a dispersão dos dados.; \n    - Implementamos um Autoencoder (ver apêndice) na tentativa de reduzir a dimensionalidade da base, mas a performance dos modelos gerados com o Autoencoder foi pior e por esse motivo não utilizamos o Autoencoder.\n- **Modelagem**\n    - O modelo escolhido foi o do XGBoost, pois foi o que nos forneceu a melhor performance com relação ao coeficiente Gini.\n- **Resultados**     \n    - Dessa forma, utilizamos o modelo XGBoost para realizar a predição nos dados de teste. \n    - Essa predição gerou um vetor de probabilidades. \n    - Essa probabilidade está associada a ocorrência do sinistro, portanto quanto maior o seu valor, maior é a chance de o segurado incorrer em um sinistro de automóvel. \n    - A partir desse vetor de probabilidades, foi gerado um arquivo denominado 'submission1.csv' para ser submetido ao sistema do Kaggle para computar o coeficiente de gini. \n    - Por fim, o valor do coeficiente de gini foi de **0.25368**. \n"},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"apendice\"></a>\n## Apêndice"},{"metadata":{},"cell_type":"markdown","source":"### Autoencoder"},{"metadata":{},"cell_type":"markdown","source":"- Optamos por disponibilizar o código do autoencoder que foi testado durante o trabalho de pré-processamento dos dados.\n- O coeficiente de gini, para o modelo XGBoost, nos dados de treinamento com a utilização do autoencoder foi, aproximadamente, a metade do obtido sem a utilização dessa técnica. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#import pandas as pd\n#from pandas import read_csv, DataFrame\n#import numpy as np\n#from numpy.random import seed\n#from sklearn.preprocessing import minmax_scale\n#from sklearn.preprocessing import MinMaxScaler\n#from sklearn.model_selection import train_test_split\n#from sklearn import datasets\n#from keras.layers import Input, Dense\n#from keras.models import Model\n#from matplotlib import pyplot as plt\n\n# Carregamento das bases de treinamento e teste em dataframes\n#train = pd.read_csv('../input/train.csv')\n\n#print(train.shape)\n\n# X armazena dos dados em um dataframe\n#X = train.iloc[:,2:]\n# y armazena os labels em um dataframe\n#y = train.iloc[:,1:2]\n\n# target_names armazena os valores distintos dos labels\n#target_names = train['target'].unique()\n\n# Normaliza os dados de treinamento\n#scaler = MinMaxScaler()\n#scaler.fit(X)\n#X_scaled = scaler.transform(X)\n\n# Criação do AutoEncoder com 3 neurônios na camada escondida usando Keras.\n#input_dim = X_scaled.shape[1]\n\n# Definição do número de variáveis resultantes do Encoder\n#encoding_dim = 10\n\n#input_data = Input(shape=(input_dim,))\n\n# Configurações do Encoder\n#encoded = Dense(encoding_dim, activation='linear')(input_data)\n#encoded = Dense(encoding_dim, activation='sgmoid')(input_data)\n#encoded = Dense(encoding_dim, activation='relu', activity_regularizer=regularizers.l1(10e-5))(input_data)\n\n#encoded1 = Dense(20, activation = 'relu')(input_data)\n#encoded2 = Dense(10, activation = 'relu')(encoded1)\n#encoded3 = Dense(5, activation = 'relu')(encoded2)\n#encoded4 = Dense(encoding_dim, activation = 'relu')(encoded3)\n\n# Configurações do Decoder\n#decoded = Dense(input_dim, activation='linear')(encoded)\n#decoded = Dense(input_dim, activation='sgmoid')(encoded)\n\n#decoded1 = Dense(5, activation = 'relu')(encoded4)\n#decoded2 = Dense(10, activation = 'relu')(decoded1)\n#decoded3 = Dense(20, activation = 'relu')(decoded2)\n#decoded4 = Dense(input_dim, activation = 'sigmoid')(decoded3)\n\n# Combinando o Encoder e o Decoder em um modelo AutoEncoder\n#autoencoder = Model(input_data, decoded4)\n#autoencoder.compile(optimizer='adam', loss='mse')\n#print(autoencoder.summary())\n# Treinamento de fato - Definição de alguns parâmetros como número de épocas, batch size, por exemplo.\n#history = autoencoder.fit(X_scaled, X_scaled, epochs=30, batch_size=256, shuffle=True, validation_split=0.1, verbose = 1)\n\n#plot a loss \n#plt.plot(history.history['loss'])\n#plt.plot(history.history['val_loss'])\n#plt.title('Model Train vs Validation Loss')\n#plt.ylabel('Loss')\n#plt.xlabel('Epoch')\n#plt.legend(['Train', 'Validation'], loc='upper right')\n#plt.show()\n\n# Utilização do Encoder gerado para realizar a compressão e reduzir a dimensão da base de treinamento\n\n#test = pd.read_csv('../input/test.csv')\n\n#print(test.shape)\n\n# X armazena dos dados em um dataframe\n#X = test.iloc[:,1:]\n\n# Normaliza os dados de treinamento\n#scaler = MinMaxScaler()\n#scaler.fit(X)\n#X_scaled = scaler.transform(X)\n\n# Utilizar o Encoder para codificar os dados de entrada\n#encoder = Model(input_data, encoded4)\n#encoded_data = encoder.predict(X_scaled)\n\n#print(encoded_data)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}