{"cells":[{"metadata":{"_uuid":"717d81f5ecd4315723f1a436e0dc903886b79e07"},"cell_type":"markdown","source":"# Aprendizagem de Máquina (AM 2018.1)\nCentro de Informática, CIn / UFPE<br/>\nProfessor: Cleber Zanchettin<br/>\nEquipe:\n* Cleison Amorim\n* Dinaldo Pessoa"},{"metadata":{"_uuid":"307f41af68e5fa8320419a639139d64fc931a8e7"},"cell_type":"markdown","source":"# Introdução\n<p>\nEste trabalho aborda um problema de classificação binária, conforme descrito pelo enunciado da competição. Cada uma das amostras do conjunto de dados disponibilizado está relacionada a usuários do seguro automotivo Porto Seguro e indicam quando esses usuários acionaram ou não o seguro no decorrer do último ano. Dado isso, o propósito é então elaborar um modelo que seja capaz de prever quando novas amostras de usuários desse seguro informarão sinistro ou não no próximo ano. É esperado que a performance do modelo elaborado seja comparável com o resultado médio da leaderboard do Kaggle.\n</p>\nEste documento está estruturado da seguinte forma: \n* Introdução, traz uma breve explicação do problema tratado, do objetivo do trabalho e prepara o contexto de execução da implementação;\n* Análise dos dados, faz uma análise exploratória dos dados;\n* Engenharia de features, realiza a engenharia das features que vão compor o modelo;\n* Experimento, realiza a seleção do algoritmo de aprendizagem de máquina e a otimização dos hiperparâmetros do algoritmo escolhido;\n* Conclusão, considerações finais sobre os resultados obtidos."},{"metadata":{"_uuid":"b12388073fe570c5442fc6c97e86a9a1c78c29ac"},"cell_type":"markdown","source":"## Preparação do experimento\n<p>\nNesta seção, apenas são realizados algumas inicializações básicas para o experimento, como importação de dependências comuns e configuração de parâmetros.\n</p>"},{"metadata":{"trusted":true,"_uuid":"719279c9b9769475a3c2ec146ae2d17d37e516d4"},"cell_type":"code","source":"import os\nimport pprint as pp\nimport math as ma\nimport numpy as np\nimport pandas as pd\nfrom plotnine import *\nfrom plotnine.data import *\nfrom sklearn import base\nfrom sklearn import tree\nfrom sklearn import naive_bayes as nb\nfrom sklearn import svm as svm\nfrom sklearn import neighbors as nei\nfrom sklearn import preprocessing as pre\nfrom sklearn import feature_extraction as fe\nfrom sklearn import pipeline as pi\nfrom sklearn import model_selection as ms\nfrom sklearn import ensemble as ens\nfrom sklearn.metrics import make_scorer\nimport xgboost as xgb\nimport bayes_opt as bo\nimport category_encoders as ce","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4c6d913b88ebd9327f664c2fdad841fb596ceca9"},"cell_type":"code","source":"# Nível de verbosidade do log:\nverbose = 10\n\n# Semente de estado randômico:\nseed = round(ma.pi * 10**4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"99b5e5d125eefe3e1f3f6a8014cf59fa9e822c8c"},"cell_type":"code","source":"# Extrai tipos das colunas:\ndef cols_types_to_df(df, cols):\n    return pd.DataFrame(df[cols].dtypes.values.reshape(1, -1), columns=cols, index=['dtype'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7b8f53e73c14a2938dae47171dbc0d0694d68f7b"},"cell_type":"markdown","source":"# Análise dos dados\n<p>\nA questão apresenta um conjunto de dados de usuários de seguros automotivos da Porto Seguro. Para que fosse possível preservar o sigilo das informações dos usuários, todas as features tiveram seus nomes codificados e não foram fornecidos quaisquer detalhes adicionais quanto à semântica ou relação entre elas. Uma pré-visualização de seus primeiros registros permite-nos constatar isso:\n</p>"},{"metadata":{"trusted":true,"_uuid":"b7d35cdbe76032d3f559b2abb2ccc320b51f1ece","_kg_hide-output":false,"_kg_hide-input":false},"cell_type":"code","source":"ID_COL = 'id'\nTARGET_COL = 'target'\nSPECIAL_COL_NAMES = [ID_COL, TARGET_COL]\n\n# Carregando dados:\ntrain = pd.read_csv('../input/train.csv').drop(ID_COL, axis=1)#.sample(frac=0.25, random_state=seed)\nheaders = train.columns\n\n# Separando o rótulo:\nX = train.drop(TARGET_COL, axis=1)\ny = train[TARGET_COL]\n\ntrain.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e2e1b5073337e853b487466bdaf6ce6e3dade78b"},"cell_type":"markdown","source":"<p>\nPara tentar suprir a ausência de informação semância para as features, foram conduzidas algumas pesquisas para tentar identificar variáveis importantes para as seguradoras, verificar se há um grau de relevância ou de interdependência entre elas, e para relacioná-las com o conjunto de dados da questão. Algumas variáveis extraídas a partir dessa pesquisa foram:\n<ol>\n    <li>Região geográfica;</li>\n    <li>Idade;</li>\n    <li>Gênero;</li>\n    <li>Estado civil;</li>\n    <li>Tempo de habilitação;</li>\n    <li>Histórico de acidentes ao volante;</li>\n    <li>Registro de sinistros;</li>\n    <li>Histórico de crédito;</li>\n    <li>Tipo do veículo;</li>\n    <li>Uso do veículo;</li>\n    <li>Quilometragem dirigidas anualmente;</li>\n    <li>Cobertura de seguro anterior;</li>\n    <li>Coberturas do seguro atual.</li>\n</ol>\n</p>\n<p>\nInfelizmente, mesmo após essa pesquisa e entendimento das variáveis acima não foi possível encontrar fatores que estabelecessem uma relação sólida entre essas variáveis e aquelas representadas pelas features. Por esse motivo, a tentantiva de encontrar a semântica para os dados foi descartada.\n</p>"},{"metadata":{"_uuid":"5bcef9362afaddcb80a7bdc8b6ecac9b6bcf5f48"},"cell_type":"markdown","source":"## Origem das features\n<p>\nApesar da ausência de detalhes quanto à semântica, é possível observar que os dados apresentados pela Porto Seguro possuem em seu rótulo prefixos que os segregam em quatro origens distintas, conforme segue. Além disso, há algumas features calculadas, que aparentam ser informações processadas a partir de diversas outras variáveis relevantes para a seguradora nesse contexto.\n<ul>\n    <li><i>ps_ind</i>: features relacionadas ao condutor;</li>\n    <li><i>ps_reg</i>: features relacionadas à região de residência;</li>\n    <li><i>ps_car</i>: features relacionadas ao veículo propriamente;</li>\n    <li><i>ps_calc</i>: features calculadas, criadas a partir de outras variáveis.</li>\n</ul>\n</p>"},{"metadata":{"_uuid":"586e0580462fb84af90a39faa34fb89e01a90aea"},"cell_type":"markdown","source":"### Features do condutor"},{"metadata":{"trusted":true,"_uuid":"2fac7c44871c19b9e8e07f25a0cdf4e256786f97"},"cell_type":"code","source":"IND_COL_NAMES = [x for x in headers if x.startswith('ps_ind')]\ncols_types_to_df(train, IND_COL_NAMES)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0b66f977fff8327b54521807a87c32186ed0513e"},"cell_type":"markdown","source":"### Features da região"},{"metadata":{"trusted":true,"_uuid":"b65a0524a6b8fb223c59b0dfe3a5344ab9d359a7"},"cell_type":"code","source":"REG_COL_NAMES = [x for x in headers if x.startswith('ps_reg')]\ncols_types_to_df(train, REG_COL_NAMES)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1b987bc74d535ab30069fc4542329a02faf59d55"},"cell_type":"markdown","source":"### Features do veículo"},{"metadata":{"trusted":true,"_uuid":"c1135c172adffbbe9e1b994edae3e21858829dcc"},"cell_type":"code","source":"CAR_COL_NAMES = [x for x in headers if x.startswith('ps_car')]\ncols_types_to_df(train, CAR_COL_NAMES)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b766a4027f11fdc4d012a289375a78a742dfcba4"},"cell_type":"markdown","source":"### Features calculadas"},{"metadata":{"trusted":true,"_uuid":"52f6e44d5686cc46748c09bd7144431c0415d6f7"},"cell_type":"code","source":"CALC_COL_NAMES = [x for x in headers if x.startswith('ps_calc')]\ncols_types_to_df(train, CALC_COL_NAMES)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"90c6c42c29c22dc823d2434e868c72f29bb3786a"},"cell_type":"markdown","source":"## Tipos das features\n<p>\nAs features também apresentam em seu rótulo sufixos que as discriminam quanto a três diferentes tipos:\n<ul>\n    <li><i>cat</i>: features categóricas;</li>\n    <li><i>bin</i>: features binárias (no contexto da questão são features categóricas, mas que não possuem valores faltantes);</li>\n    <li><i>(sem sufixo)</i>: demais features com valores contínuos ou ordinais.</li>\n</ul>\n</p>"},{"metadata":{"_uuid":"cc541387d4c29c1644c1a470ac3b098a903dcbba"},"cell_type":"markdown","source":"### Features categóricas"},{"metadata":{"trusted":true,"_uuid":"165327a83565374b2881877c315422c47b5ef4cb"},"cell_type":"code","source":"CAT_COL_NAMES = [x for x in headers if x.endswith('cat')]\ncols_types_to_df(train, CAT_COL_NAMES)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dcbda13c3b7fec9e5125953120a5c6f0f59ef142"},"cell_type":"markdown","source":"### Features binárias"},{"metadata":{"trusted":true,"_uuid":"05655a31bd6f8749d1073de58ff5186f6dc62ad7"},"cell_type":"code","source":"BIN_COL_NAMES = [x for x in headers if x.endswith('bin')]\ncols_types_to_df(train, BIN_COL_NAMES)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"36b96fe4a0f81645edcbbd2d5f4ad0ee1e27d9b6"},"cell_type":"markdown","source":"### Features contínuas"},{"metadata":{"trusted":true,"_uuid":"9ef6290bc4789fa5c8a99dc35ec54794f426281d"},"cell_type":"code","source":"LIN_COL_NAMES = [x for x in headers if (x not in (CAT_COL_NAMES + BIN_COL_NAMES + SPECIAL_COL_NAMES))]\ncols_types_to_df(train, LIN_COL_NAMES)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"919640255e7cb9193ac0db8b964732f69ff7944a"},"cell_type":"markdown","source":"<p>\nPor fim, alguns detalhes adicionais apresentados pela questão serão relevantes para o experimento apresentado adiante:\n<ul>\n    <li><i>valores ausentes</i>: são representados no conjunto de dados pelo dígito -1;</li>\n    <li><i>target</i>: coluna que contém as classes das amostras. O valor 1 indica uma amostra onde o usuário acionou o seguro no último ano, e o valor 0 indica que o seguro não foi acionado.</li>\n</ul>\n</p>"},{"metadata":{"_uuid":"3d70c447272cd4e1064289084649bacb141e8589"},"cell_type":"markdown","source":"## Análise estatística\n<p>\nUma análise de características de média, variância, mínimo, máximo e percentis permite-nos obter uma visão incial acerca do comportamento e distribuição dos dados disponibilizados:\n</p>"},{"metadata":{"trusted":true,"_uuid":"fb5b332f09f6f21a185f8f51d10b8518e3586175"},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a1d97c3f97ace773fd22abff92e15496880998b0"},"cell_type":"markdown","source":"<p>\nOs gráficos representam a classe <i>\"1\"</i> (usuário acionou o seguro) em azul, e a classe <i>\"0\"</i> (usuário não acionou o seguro) em vermelho. Eles permitem-nos confirmar que há um desbalanceamento dos dados que indica uma tendência menor dos condutores informarem sinistro. Esse é um comportamento até certo ponto esperado e aceitável para o contexto de seguros automotivos, quando levado em consideração o tamanho muito maior da base de condutores com seguro ativo e a menor proporção daqueles que efetivamente incidem em sinistro no decorrer de um ano. O desbalanceamento das classes foi tratado por meio de um parâmetro do algoritmo escolhido, o parâmetro scale_pos_weight do XGBClassifier. Nossa configuração aumentou o peso do erro de classificação da classe minoritária.\n</p>"},{"metadata":{"trusted":true,"_uuid":"9cad1c64b89eed10eb723e4fdbcd3f5a3197cde8"},"cell_type":"code","source":"target_counts = train[TARGET_COL].value_counts()\ntarget_ratio = target_counts[0]/target_counts[1]\nprint(\"Proporção entre classes 0 e 1: %s\" % target_ratio)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e61420af26e004f943fd5637f4cf8a01e54f9565"},"cell_type":"code","source":"(ggplot(train.astype(dtype={'target': object}), aes(x=TARGET_COL, fill=TARGET_COL))\n + geom_bar()   \n + facet_wrap(TARGET_COL)\n )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b101ef27742b52840c697f2cd9c72b9e6dc1f745"},"cell_type":"markdown","source":"# Engenharia de features\n<p>\nNo decorrer do experimento, foram aplicadas algumas abordagens para tratamento e codificação das features. A primeira delas foi de criar um transformador para substituir os valores faltantes, que após análise das demais etapas de tranformação dos dados foi detectado que a grande maioria deles já possui tratamento para isso e que seria desnecessário criar um transformador a parte. \n</p>\n<p>\nA segunda delas foi de tentar agrupar valores categóricos que apareciam com frequencia inferior a 5% dentro de um novo valor que significaria \"outros\". Isso foi motivado após verificar que técincas de codificação como a OneHot poderia chegar a duplicar o número de features das amostras, tornando ainda mais lenta a execução do algoritmo. Conforme esperado, essa estratégia alcançou o objetivo de controlar o crescimento da dimensionalidade das amostras, mas não foi capaz de contribuir positivamente para os melhores resultados do experimento.\n</p>\n<p>\nA terceira (e provavelmente mais clássica) das abordagens foi a de aplicar a codificação OneHot para codificar os valores das features categórias. A implementação utilizada considera a possibilidade de tratar valores faltantes, e foi capaz de contribuir com bons resultados neste trabalho. Como alternativa para lidar com o problema da dimensionalidade, também tentou-se aplicar a estratégia codificação binária, mas seus resultados não superaram os da codificação OneHot neste experimento.\n</p>\n<p>\nPor esse motivo, apenas a codificação OneHot foi incluída como passo anterior à classificação no pipeline de execução mostrado mais adiante.\n</p>"},{"metadata":{"trusted":true,"_uuid":"86f9815cb7200ddd0dc798364a0c210c40b0b463"},"cell_type":"code","source":"def drop_cols_func(X):\n    '''Remove irrelevant columns.\n    \n    Parameters\n    ----------\n\n    X : pandas.DataFrame\n        DataFrame with the features.\n        \n    Returns\n    -------\n\n    p : pandas.DataFrame\n        DataFrame without *calc* features.\n    '''\n    return X.drop(columns=CALC_COL_NAMES)\n\ndrop_cols = pre.FunctionTransformer(func=drop_cols_func, validate=False)\ndrop_cols.transform(X).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"96a502f635843e604a69a261d74842a6805abc39"},"cell_type":"code","source":"one_hot = ce.OneHotEncoder(cols=CAT_COL_NAMES, drop_invariant=True, handle_unknown='impute')\none_hot.fit_transform(X).head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"602f0ab87c6ba3ed93830f2735541348a496637a"},"cell_type":"markdown","source":"# Experimento\n## Métrica de avaliação\n<br/>\nA métrica utilizada nesse trabalho segue a definição da competição do Kaggle, que é o Coeficiente de Gini Normalizado. Vale salientar que essa métrica é equivalente a área sobre a curva ROC [POWERS, 2011]."},{"metadata":{"trusted":true,"_uuid":"d1f4dadf35e8478352efd6d811fe8c9223141d20"},"cell_type":"code","source":"def gini(actual, pred):   \n    assert( len(actual) == len(pred) )\n    all = np.asarray(np.c_[ actual, pred, np.arange(len(actual)) ], dtype=np.float)    \n    # ordena por coluna da classe positiva de pred e por \n    all = all[ np.lexsort((all[:,2], -1*all[:,1])) ]    \n    totalLosses = all[:,0].sum()\n    giniSum = all[:,0].cumsum().sum() / totalLosses\n    \n    giniSum -= (len(actual) + 1) / 2.\n    return giniSum / len(actual)\n\ndef gini_normalized(actual, pred):    \n    return gini(actual, pred[:,1]) / gini(actual, actual)\n\ngini_normalized_scorer = make_scorer(score_func=gini_normalized, greater_is_better=True, needs_proba=True)\n\ndef cv_metric(estimator, X, y, fit_params=None):    \n    pred = ms.cross_val_predict(estimator, X, y, cv=ms.StratifiedKFold(n_splits=3, random_state=seed), verbose=verbose, fit_params=fit_params, method='predict_proba')\n    return gini_normalized(y, pred) \n\ndef xgb_metric(pred, dtrain):\n    print('xgb_gini_normalized')\n    return 'gini', gini_normalized(dtrain.get_labels(), pred)\n\nxgb_fit_params = {        \n    #'clf__silent': 1,\n    'clf__eval_metric': xgb_metric\n}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d4a65d6ece907456acbb704e00ecc750f2ea4221"},"cell_type":"markdown","source":"## Construção do pipeline\n<br/>\nEsse trabalho utilizou o modelo de programação do Scikit-learn [PEDREGOSA, 2011], o qual organiza o modelo de aprendizagem de máquina na forma de um pipeline. Um pipeline consiste na sequência de transformações das features seguida pelo algoritmo de aprendizagem. Essa composição cria um abstração que esconde os detalhes de engenharia de features. O modelo na forma de pipeline simplificou o reuso da solução em outras seções desse trabalho."},{"metadata":{"_uuid":"494193ac16ffd869996b48e30ae7b74b6a4f456f","trusted":true},"cell_type":"code","source":"xgb_params = {\n    'objective': 'binary:logistic'\n}\n\ndef pipeline():\n    steps = [('drop_cols', drop_cols),\n             ('one_hot', one_hot),\n             ('clf', xgb.XGBClassifier(random_state=seed))]\n    return pi.Pipeline(steps)\n\npipe = pipeline()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"73627f8d894ede5dc86c97b7d6b7b59bff69c4a8"},"cell_type":"markdown","source":"## Separação do conjunto de dados\n<br/>\n67% do conjunto de dados foram separados para seleção do algoritmo e para otimização de parâmetros do algoritmos escolhido, enquanto que os 33% restantes foram utilizados para avaliação do modelo otimizado."},{"metadata":{"trusted":true,"_uuid":"feb4722094cc03c1b3678432d202a17c8ff6c910"},"cell_type":"code","source":"X_train, X_test, y_train, y_test = ms.train_test_split(X, y, test_size=0.33, random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1e7259a0612e774bedff8cbb2aef9cae95c97c2d"},"cell_type":"markdown","source":"## Seleção do algoritmo\n<br/>\nA seleção do algoritmo de aprendizagem de máquina pode ser feita de forma combinada com a otimização dos hiperparâmetros, Combined Algorithm Selection and Hyperparameter\noptimization (CASH), ou de forma isolada, selecionar o algoritmo e depois otimizar os hiperparâmetros. A primeira abordagem é mais promissora, pois é sabido que os algoritmos possuem performance sensível a variação nos hiperparâmetros [GOMES, 2012]. Essa abordagem tem sido utilizada no ramo de AutoML [FEURER, 2015]. Porém, devido a indisponibilidade de bibliotecas de otimização adequadas para a tarefa no ambiente do Kaggle, esse trabalhou optou por escolher o algoritmo por meio de uma busca em grid com os parâmetros minimamente configurados e fazer a otimização de parâmetros do agoritmo escolhido em um segundo momento. \n<br/><br/>\nTécnicas de otimização adequadas para resolver um problema de CASH trabalham com variáveis discretas e interdependentes, como é o caso do Sequential Model-Based Optimization for\nGeneral Algorithm Configuration (SMAC). Por exemplo, o algoritmo é uma variável de decisão discreta e os hiperparâmetros desse algoritmo são variáveis de decisão dependentes da escolha do mesmo. Essas variáveis de decisão dependentes só precisam estar ativas na busca quando a variável principal está ativa. A biblioteca à disposição no Kaggle usa a técnica de Bayesian Global Optimization with Gaussian Processes (GP) [SNOEK, 2012], que trabalha apenas com variáveis contínuas e não aproveita o conhecimento sobre a dependência entre elas. Mesmo não sendo adequada para o problema de CASH, ela pode ser utilizada para a otimização de hiperparâmetros de forma isolada, como é o caso desse trabalho.\n<br/><br/>\nOs algoritmos avaliados nesse trabalho foram KNeighborsClassifier com features normalizadas e configurado para utilizar apenas um vizinho na predição; e SVC (classificador SVM), GaussianNB (classificador bayesiano gaussiano) e XGBClassifier (classificador XGBoost) com as configurações padrões. KNeighborsClassifier, GaussianNB e SVC são implementações encontradas na biblioteca Scikit-learn [PEDREGOSA, 2011]. XGBClassifier vem de uma biblioteca focada em XGBoost [CHEN, 2016]. KNeighborsClassifier e SVC foi removido da implementação final desse trabalho após apresentar um tempo de treinamento proibitivo para o conjunto de dados utilizado. O KNeighborsClassifier rodou em 2h e obteve uma performance com duas ordens de grandeza abaixo do GaussianNB. O SVC não chegou a terminar de rodar, mas sua inviabilidade foi confirmada na documentação da biblioteca, que afirma que sua implementação do SVC não é escalável para conjuntos de dados com mais de 10.000 exemplos.\n<br/><br/>\nO algoritmo com o melhor resultado foi o XGBClassifier. Por isso, ele passou para a etapa de otimização de parâmetros, que será apresentada a seguir."},{"metadata":{"trusted":true,"_uuid":"94a6d9ab02f9f75ae12d84e4a4f128cb92a12ab3"},"cell_type":"code","source":"# AVISO: desativado para não extrapolar o tempo de processamento do Kaggle\n'''\nparam_grid = {'clf': [nb.GaussianNB(), xgb.XGBClassifier(random_state=seed, **xgb_params)]}\ngrid_search = ms.GridSearchCV(pipe, param_grid=param_grid, cv=ms.StratifiedKFold(n_splits=3, random_state=seed), verbose=verbose, scoring=gini_normalized_scorer, return_train_score=False)\ngrid_search.fit(X_train, y_train)\npp.pprint(grid_search.best_score_)\npp.pprint(grid_search.best_estimator_)\n'''","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"31918fea5c528b5504638ff5e5ebb7f83f559a8f"},"cell_type":"markdown","source":"## Otimização de hiperparâmetros\n<br/>\nA otimização de hiperparâmetros desse trabalho é focada no algoritmo escolhido, o XGBClassifier. Mesmo sendo realizada com apenas um algoritmo, esse é um procedimento bem dispendioso, pois o custo computacional de calcular a função objetivo da otimização corresponde a uma validação cruzada do modelo no conjunto de treinamento. Além disso, a biblioteca de otimização precisa calcular a função objetivo diversas vezes antes de alcançar um máximo local ou global. Considerando o alto custo e a necessidade de repetir esse procedimento a cada mudança nas features, esse trabalhou optou por realizar a otimização de hiperparâmetros de forma incremental e iterativa. Incremental porque quando as features não são alteradas, os dados da função objetivo são persistidos e reusados entre uma execução e outra da otimização. Iterativa porque quando as features são alteradas, é possível utilizar as melhores configurações de hiperparâmetros da iteração anterior para inicializar a busca com as novas features, conforme a abordagem de meta-learning [GOMES, 2012]. Vale salientar que, no caso desse trabalho, o conjunto de dados é o mesmo e isso facilita ainda mais a aplicação da abordagem de meta-learning.\n<br/><br/>\nNesse trabalho, esse procedimento foi executado em duas iterações. A primeira iteração rodou durante um total de 24h, em um computador pessoal, com um conjunto de features que tem apenas uma transformação em relação ao conjunto de dados original, dados categóricos codificados por meio de One Hot Encoding. A segunda iteração corresponde a iteração final, com as novas features, e ela é a que se encontra pronta para ser reproduzida no kernel do Kaggle."},{"metadata":{"trusted":true,"_uuid":"1ac14cde765d3dabd12b71c9608c8a6ddacd81b5"},"cell_type":"code","source":"if os.path.isfile('cache.csv'):\n    # AVISO: atualize best_configs abaixo do 'else' sempre que houver uma busca ampla por melhores parâmetros\n    best_configs = pd.read_csv('cache.csv').sort_values(by='target', ascending=False)\n    best_configs = best_configs.head(5).drop(columns='target').to_dict(orient='list')\n    pp.pprint(best_configs)\nelse:\n    # melhores parâmetros hard-coded serão usados em ambientes sem o arquivo 'cache.csv'\n    best_configs = {'colsample_bylevel': [1.0, 1.0, 1.0, 1.0, 0.6210536972328008],\n         'colsample_bytree': [0.1639149309112309, 0.4719120160563106, 0.2738881448393379, 0.9597000000000002, 0.4998605743150732],\n         'gamma': [9.311242926501441, 9.106399343640415, 9.84350195229832, 1.3079, 8.495594735754976],\n         'learning_rate': [0.1, 0.1, 0.1, 0.1, 0.3],\n         'max_delta_step': [0.0, 0.0, 0.0, 0.0, 9.937144736135796],\n         'max_depth': [3.280267137915809, 3.744769687598659, 3.079866163690399, 3.8448, 3.0243950675489177],\n         'min_child_weight': [1.0, 1.0, 1.0, 1.0, 1.0],\n         'n_estimators': [496.3697387347585, 496.70662978009966, 362.50656654507236, 499.9892, 350.3584899983292],\n         'reg_alpha': [9.79619399515934, 0.7119072322272911, 0.05797462113553141, 8.128199999999998, 4.196426179286616],\n         'reg_lambda': [1.0, 1.0, 1.0, 1.0, 9.729243825652572],\n         'scale_pos_weight': [2.091440738145689, 1.7334510787353241, 1.327362597470453, 2.6812, 1.072600468578576],\n         'subsample': [0.7142488578466437, 0.9885285182085388, 0.7352809336635373, 0.9199, 0.8611579899201934]}\n    # AVISO: comentar a linha de código abaixo para fazer mais exploração\n    #best_configs = pd.DataFrame.from_dict(best_configs).head(1).to_dict(orient='list')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b0b1c75040d16f4437313c30a121d13428bd44fb"},"cell_type":"code","source":"def to_xgb_params(**kwargs):\n    params = {}\n    params['clf__max_depth'] = int(kwargs['max_depth']) \n    params['clf__learning_rate'] = max(kwargs['learning_rate'], 0)\n    params['clf__n_estimators'] = int(kwargs['n_estimators'])\n    params['clf__min_child_weight'] = int(kwargs['min_child_weight'])\n    params['clf__max_delta_step'] = int(kwargs['max_delta_step'])\n    params['clf__subsample'] = max(min(kwargs['subsample'], 1), 0)\n    params['clf__colsample_bytree'] = max(min(kwargs['colsample_bytree'], 1), 0)  \n    params['clf__colsample_bylevel'] = max(min(kwargs['colsample_bylevel'], 1), 0)\n    params['clf__gamma'] = max(kwargs['gamma'], 0)\n    params['clf__reg_alpha'] = max(kwargs['reg_alpha'], 0)\n    params['clf__scale_pos_weight'] = max(kwargs['scale_pos_weight'], 0.001)\n    return params\n\ndef xgb_cv(**kwargs):\n    xgb = base.clone(pipe)\n    xgb.set_params(**to_xgb_params(**kwargs))\n    return cv_metric(xgb, X_train, y_train, fit_params=xgb_fit_params)\n    \nbo_opt = bo.BayesianOptimization(xgb_cv, {'max_depth': (3, 15),\n                                      'learning_rate': (0.1, 0.0001),\n                                      'n_estimators': (100, 500),\n                                      'min_child_weight': (1, 1),\n                                      'max_delta_step': (0, 10),\n                                      'subsample': (0.5, 1.0),\n                                      'colsample_bytree': (0.1, 1.0),\n                                      'colsample_bylevel': (0.1, 1.0),\n                                      'gamma': (0.0, 10.0),\n                                      'reg_alpha': (0.0, 10.0),\n                                      'reg_lambda': (1.0, 10.0),\n                                      'scale_pos_weight': (1.0, target_ratio),\n                                     },\n                                    random_state=seed)\n                                    \n# AVISO: desativado para não extrapolar o tempo de processamento do Kaggle\n'''\n# manter True para execução de curta duração no Kaggle\n# manter False para ampliar a busca por melhores parâmetros\nfast_search = True\nif fast_search:\n    # usa melhores parâmetros como semente para o otimizador\n    bo_opt.explore(best_configs)\nelse:\n    if os.path.isfile('cache.csv'):\n        # carrega dados da função objetivo\n        bo_opt.initialize_df(pd.read_csv('cache.csv'))\n    else:\n        # explora configuração padrão e 5 configurações aleatórias        \n        bo_opt.explore({'max_depth': [3],\n                        'learning_rate': [0.1],\n                        'n_estimators': [100],\n                        'min_child_weight': [1],\n                        'max_delta_step': [0],\n                        'subsample': [1.0],\n                        'colsample_bytree': [1.0],\n                        'colsample_bylevel': [1.0],\n                        'gamma': [0.0],\n                        'reg_alpha': [0.0],\n                        'reg_lambda': [1.0],\n                        'scale_pos_weight': [1.0]})        \n        bo_opt.maximize(init_points=5, n_iter=0, acq='ei')\n        # explora as melhores parâmetros até então encontrados\n        bo_opt.explore(best_configs)\n        bo_opt.points_to_csv('cache.csv')        \n# otimiza com 5 rodadas\nbo_opt.maximize(init_points=0, n_iter=5, acq='ei')    \nbo_opt.points_to_csv('cache.csv')\npp.pprint(bo_opt.res['max'])\n\n# cria modelo otimizado\nbest_config = bo_opt.res['max']['max_params']\n'''\n# AVISO: comentar linha de código abaixo para usar resultado da otimização\nbest_config = {'colsample_bylevel': 0.45879276636748534,\n                'colsample_bytree': 0.7824778832374746,\n                'gamma': 5.489188804299045,\n                'learning_rate': 0.1,\n                'max_delta_step': 1.5254539296007386,\n                'max_depth': 3.4967941454908598,\n                'min_child_weight': 1.0,\n                'n_estimators': 461.216046669665,\n                'reg_alpha': 8.531590560075347,\n                'reg_lambda': 9.89766553793669,\n                'scale_pos_weight': 1.2379042177844632,\n                'subsample': 0.798095844721928}\nopt_params = to_xgb_params(**best_config)\nopt_pipe = base.clone(pipe)\nopt_pipe.set_params(**opt_params)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c045ca0234a57f1c25de9563a9a3b30ac8b951ca"},"cell_type":"markdown","source":"## Avaliação\n<br/>\nMesmo com restrições de tempo de processamento, a otimização de hiperparâmetros realizada nesse trabalho conseguiu elevar a métrica de avaliação de 0.27510 (configuração padrão) para 0.28511 (configuração otimizada), o que representa um aumento de 3,6%. Para se ter um noção melhor, o aumento de performance entre a nossa configuração padrão e o primeiro colocado do leaderboard do Kaggle é de 8%. Esse resultado reforça a importância de uma boa configuração para os algoritmos de aprendizagem. Por outro lado, para aplicações práticas com uma escala maior do que a encontrada nesse trabalho, é preciso investir em paralelismo ou dispor de mais tempo de processamento para a biblioteca de otimização conseguir mapear uma função objetivo com tantos parâmetros de forma adequada, como é o caso da otimização dos hiperparâmetros do XGBClassifier.\n<br/><br/>\nNa primeira iteração desse trabalho, a biblioteca foi configurada para realizar exploration e exploitation, mas, na segunda iteração, devido às restrições de tempo de execução do ambiente do Kaggle, a biblioteca de otimização foi configurada para focar em exploitation. Uma extensão desse trabalho poderia configurar a biblioteca de otimização para realizar exploration em todas as iterações e optar por um meio termo no trade-off exploration-exploitation [BROCHU, 2010]."},{"metadata":{"trusted":true,"_uuid":"b5ce50e5d681f216f197d029ca86bbeaea9ab9e9","scrolled":true},"cell_type":"code","source":"pipe.fit(X_train, y_train, **xgb_fit_params)\nprint('Configuração padrão: {}'.format(gini_normalized(y_test, pipe.predict_proba(X_test))))\nopt_pipe.fit(X_train, y_train, **xgb_fit_params)\nprint('Configuração otimizada: {}'.format(gini_normalized(y_test, opt_pipe.predict_proba(X_test))))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"21ecc461f0b863956f11e4ec93fa2f93e429a37e"},"cell_type":"markdown","source":"## Curvas de aprendizagem\n<br/>\nCom as curvas de aprendizagem, é possível avaliar de forma qualitativa se o modelo está sofrendo de overfitting ou underfitting. Além disso, o resultado do primeiro colocado no leaderboard do Kaggle, 0.29698, serve como referência para essa análise se ele for considerado como um topo de performance no conjunto de testes.\n<br/><br/>\nNo gráfico abaixo, é possível observar que a performance no conjunto de treinamento está acima de 0.29698, o que indica que o modelo não sofre de underfitting. Em outras palavras, nosso modelo consegue uma performance no conjunto de treinamento melhor do que a performance do modelo do primeiro colocado no conjunto de testes do leaderboard do Kaggle. Após essa constatação, esse trabalho passou a focar na melhoria da performance no conjunto de testes, mesmo que isso representasse uma piora da performance no conjunto de treinamento. Por fim, a performance no conjunto de testes também não apresenta indícios de overfitting, mas, como é possível observar pelo leaderboard, há margem para melhorias.\n![Curvas de aprendizagem](https://raw.githubusercontent.com/amorim-cleison/cin_am/develop/projeto_2/curvas_aprendizagem.png)"},{"metadata":{"trusted":true,"_uuid":"9f520aad9f697c6478ccba22998a5842a555ce30"},"cell_type":"code","source":"# Wrapper implementado como workaround de um bug de \"learning_cuve\" do sklearn\n# \"learning_curve\" não expõe o parâmetro \"fit_params\" e o Wrapper abaixo resolve isso\nclass FitParamsWrapper(base.BaseEstimator, base.ClassifierMixin):\n    def __init__(self, estimator, fit_params):\n        self.estimator = estimator\n        self.fit_params = fit_params\n\n    def fit(self, X, y=None):\n        self.estimator.fit(X, y, **self.fit_params)\n        return self\n\n    def predict(self, X, y=None):\n        return self.estimator.predict(X)\n\n    def predict_proba(self, X, y=None):        \n        return self.estimator.predict_proba(X)\n    \n    def score(self, X, y=None):        \n        return self.estimator.score(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d2d2270c91ba66bcf667025cafbe54e730ee56ea"},"cell_type":"code","source":"# AVISO: desativado para não extrapolar o tempo de processamento do Kaggle\n'''\nsizes, train_scores, test_scores = ms.learning_curve(FitParamsWrapper(opt_pipe, xgb_fit_params), X, y, cv=ms.StratifiedKFold(n_splits=3, random_state=seed), scoring=gini_normalized_scorer, verbose=verbose, random_state=seed)\ndf_lc = pd.DataFrame(data={'m': sizes, 'train': train_scores.mean(axis=1), 'test': test_scores.mean(axis=1)})\ndf_lc = pd.melt(df_lc, id_vars=['m'], value_vars=['train', 'test'], var_name='type', value_name='score')\n(ggplot(df_lc)\n + aes(x='m', y='score', fill='type', color='type')\n + geom_line()\n )\n '''","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"480e4d41cb2ada0aa8b0d512fea5835e5d0eb49d"},"cell_type":"markdown","source":"## Submissão\n<br/>\nAntes de realizar a predição e a submissão para o Kaggle, o modelo com hiperparâmetros otimizados passa pelo treinamento com o conjunto de dados completo. Isso é feito tendo em vista que quanto maior o conjunto de treinamento, maior as chances do modelo generalizar o conhecimento."},{"metadata":{"trusted":true,"_uuid":"806ad422b17222732b4d75437385650483e04da0"},"cell_type":"code","source":"opt_pipe.fit(X, y, **xgb_fit_params)\ntest = pd.read_csv('../input/test.csv')\nX_test = test[X.columns]\ntargets = opt_pipe.predict_proba(X_test)\ntargets = pd.DataFrame({'id': test.id, 'target': targets[:,1]})\ntargets.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"af17fd31315f561e3c5b049bf6e9effc1b2a90e4"},"cell_type":"markdown","source":"# Conclusão\n<br/>\nO resulado final desse trabalho conseguiu alcançar os objetivos estabelecidos, que era prever quais usuários desse seguro informarão sinistro ou não no próximo ano com uma performance comparável com a média do leaderboard do Kaggle. No dia 11/07/2018, o score do nosso modelo otimizado ficou igual ao resultado da posição 3062, de um total de 5169 competidores."},{"metadata":{"_uuid":"bc3b710d1038a58d6f240d19110d2b241eae8572"},"cell_type":"markdown","source":"# Referências bibliográficas\n* CHEN, Tianqi; GUESTRIN, Carlos. Xgboost: A scalable tree boosting system. In: Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining. ACM, 2016. p. 785-794.\n* FEURER, Matthias et al. Efficient and robust automated machine learning. In: Advances in Neural Information Processing Systems. 2015. p. 2962-2970.\n* GOMES, Taciana AF et al. Combining meta-learning and search techniques to select parameters for support vector machines. Neurocomputing, v. 75, n. 1, p. 3-13, 2012.\n* HUTTER, Frank; HOOS, Holger H.; LEYTON-BROWN, Kevin. Sequential model-based optimization for general algorithm configuration. In: International Conference on Learning and Intelligent Optimization. Springer, Berlin, Heidelberg, 2011. p. 507-523.\n* PEDREGOSA, Fabian et al. Scikit-learn: Machine learning in Python. Journal of machine learning research, v. 12, n. Oct, p. 2825-2830, 2011.\n* POWERS, David Martin. Evaluation: from precision, recall and F-measure to ROC, informedness, markedness and correlation. 2011.\n* SNOEK, Jasper; LAROCHELLE, Hugo; ADAMS, Ryan P. Practical bayesian optimization of machine learning algorithms. In: Advances in neural information processing systems. 2012. p. 2951-2959."}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}