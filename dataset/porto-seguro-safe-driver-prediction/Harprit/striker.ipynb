{"nbformat_minor":1,"nbformat":4,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"file_extension":".py","pygments_lexer":"ipython3","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"name":"python","nbconvert_exporter":"python","version":"3.6.3"}},"cells":[{"cell_type":"code","metadata":{"_uuid":"07560b8aaa5551b396a644be207a2197734e1f29","collapsed":true,"_cell_guid":"bd4fb2c8-5a25-4eb8-ab43-590e118968be"},"outputs":[],"source":"","execution_count":null},{"cell_type":"code","metadata":{"_kg_hide-input":true,"_uuid":"e0bb17227b0573ae22527a85e1a17dd8a7ecfc8f","_cell_guid":"59d497fb-0699-4890-a86d-41413f7d4db6"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\nimport pandas as pd\nimport numpy as np\nfrom  sklearn.metrics import accuracy_score\nfrom sklearn.utils import resample\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.base import TransformerMixin\n\ndef gini(actual, pred, cmpcol = 0, sortcol = 1):\n     assert( len(actual) == len(pred) )\n     all = np.asarray(np.c_[ actual, pred, np.arange(len(actual)) ], dtype=np.float)\n     all = all[ np.lexsort((all[:,2], -1*all[:,1])) ]\n     totalLosses = all[:,0].sum()\n     giniSum = all[:,0].cumsum().sum() / totalLosses\n \n     giniSum -= (len(actual) + 1) / 2.\n     return giniSum / len(actual)\n \ndef gini_normalized(a, p):\n     return gini(a, p) / gini(a, a)\n \ndef eval_gini(y_true, y_prob):\n    \"\"\"\n    Original author CPMP : https://www.kaggle.com/cpmpml\n    In kernel : https://www.kaggle.com/cpmpml/extremely-fast-gini-computation\n    \"\"\"\n    y_true = np.asarray(y_true)\n    y_true = y_true[np.argsort(y_prob)]\n    ntrue = 0\n    gini = 0\n    delta = 0\n    n = len(y_true)\n    for i in range(n-1, -1, -1):\n        y_i = y_true[i]\n        ntrue += y_i\n        gini += y_i * delta\n        delta += 1 - y_i\n    gini = 1 - 2 * gini / (ntrue * (n - ntrue))\n    return gini\n\ndef gini_xgb(preds, dtrain):\n    labels = dtrain.get_label()\n    gini_score = eval_gini(labels, preds)\n    return [('gini', gini_score)]\n\nprint(\"Reading Input\")\ndf = pd.read_csv(\"../input/train.csv\")\ndf_test = pd.read_csv(\"../input/test.csv\")\n\n\nclass DataFrameImputer(TransformerMixin):\n    def fit(self, X, y=None):\n        self.fill = pd.Series([X[c].value_counts().index[0]\n            if X[c].dtype == np.dtype('O') else X[c].median() for c in X],\n            index=X.columns)\n        return self\n    def transform(self, X, y=None):\n        return X.fillna(self.fill)\n\ntarget = df['target']\ndf.drop('target',axis=1)\n\nbig_X = df.append(df_test)\nbig_X_imputed = DataFrameImputer().fit_transform(big_X)\n\n\ndf = big_X_imputed[0:df.shape[0]]\ndf_test = big_X_imputed[df.shape[0]::]\n\n\n\n# Prepare the inputs for the model\n\n\ndf['target'] = target\ny = []\ndef upsampling(df):\n    df_majority=df[df.target==0]\n    df_minority=df[df.target==1]\n    # Upsample minority class\n    df_minority_upsampled = resample(df_minority, \n                                     replace=True,     # sample with replacement\n                                     n_samples=473518,    # to match majority class\n                                     random_state=123) # reproducible results\n\n    # Combine majority class with upsampled minority class  n_samples=573518\n    df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n    \n    return df_upsampled\n\nids = df_test['id'].as_matrix()\ndf_test.drop('id',axis=1)\n\n#upsampling\ndf_upsampled = upsampling(df)\ny= df_upsampled['target'].as_matrix().astype(float)\ndf_upsampled.drop('target',axis=1)\ndf_upsampled.drop('id',axis=1)\n\nX = df_upsampled.as_matrix()\nX_test_new = df_test.as_matrix()\n\n\n\n\nscaler = StandardScaler()\n# Fit only to the training data\nscaler.fit(X_test_new)\n# Now apply the transformations to the data:\n#X_train = scaler.transform(X_train)\nX_test_new = scaler.transform(X_test_new)\n\nscaler.fit(X)\nX = scaler.transform(X)\n\nfrom sklearn import decomposition\npca = decomposition.PCA(n_components=4)\n\nX = pca.fit_transform(X)\n\nX_test_1 = pca.fit_transform(X_test_new)\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.10)\n\nprint(\"Model Building\")\nfrom xgboost import XGBClassifier\n\nn_estimators = 300\nn_splits = 4\nprint(\"Kfold\")\nfrom sklearn.model_selection import StratifiedKFold\nfolds = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=15)\n\n'''\nparam = {\n 'n_estimators':[100,150,200,250],\n 'max_depth':[2,3,4,5,6,7,8,9],\n 'min_child_weight':[2,3,4,5],\n 'colsample_bytree':[0.2,0.6,0.8],\n 'colsample_bylevel':[0.2,0.6,0.8],\n 'learning_rate':[0.02,0.04,0.2]\n}\nfrom sklearn.grid_search import GridSearchCV\ngsearch1 = GridSearchCV(estimator = XGBClassifier( \n        objective= \"binary:logistic\", \n        seed=1), \n    param_grid = param, \n    scoring='neg_log_loss',\n    cv=4,\n    verbose = 1)\n    \n# http://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n# mean_squared_error alternative.\n\ngsearch1.fit(X_train, y_train)\nprint(gsearch1.bestscore)\nprint(gsearch1.bestparams)\n'''\n\nclf = XGBClassifier(n_estimators=n_estimators,\n                        max_depth=5,\n                        objective=\"binary:logistic\",\n                        learning_rate=.02, \n                        subsample=.8, \n                        colsample_bytree=.8,\n                        gamma=1,\n                        reg_alpha=0,\n                        reg_lambda=1,\n                        nthread=2)\n\nimport matplotlib.pyplot as plt\nimport scikitplot as skplt\nfrom sklearn import metrics\nfor train_index, test_index in folds.split(X,y):\n    \n    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\n\n    print(\"Model fitting\")\n    # Fit the best algorithm to the data. \n    clf.fit(X_train, y_train)\n    \n\n    predictions = clf.predict(X_test)\n    predictions_prob = clf.predict_proba(X_test)\n    print(accuracy_score(y_test, predictions))\n    predictions_prob_test = clf.predict_proba(X_test_1)\n    skplt.metrics.plot_precision_recall_curve(y_test, predictions_prob)\n    plt.show()\n    #roc\n    preds_roc = predictions_prob[:,1]\n    fpr, tpr, threshold = metrics.roc_curve(y_test, preds_roc)\n    roc_auc = metrics.auc(fpr, tpr)\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n    plt.legend(loc = 'lower right')\n    plt.plot([0, 1], [0, 1],'r--')\n    plt.xlim([0, 1])\n    plt.ylim([0, 1])\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()\n\n\n\n\n\ndf2 = pd.DataFrame({'id' : ids})\ndf2['target'] = predictions_prob_test[:,1]\ndf2.to_csv(\"submission_5_xgbKold_1.csv\",index= False)\nprint(df2)\n#print(predictions_prob)\n#print(predictions_prob[:,1])\n\n\n\n\nprint(\"Full OOF score : %.6f\" % gini_normalized(y_test,predictions_prob[:,1]))\n\n#print(accuracy_score(y_test, predictions))\n\n\n\n#print(accuracy_score(y_test, predictions))\n\n#print(pd.DataFrame(pca.components_,columns=df_norm.columns,index = ['PC-1','PC-2']))\n#list(X)\n# Any results you write to the current directory are saved as output.","execution_count":null}]}