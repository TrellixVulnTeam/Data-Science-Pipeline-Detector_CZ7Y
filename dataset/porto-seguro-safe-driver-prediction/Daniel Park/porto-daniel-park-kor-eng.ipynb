{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Binary classification**\n## Tabular data","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import PolynomialFeatures # 좋은 feature들끼리 곱하면 더 좋은 feature를 만들수도 있음.! 그때 곱하기를 하려면 'PolynomialFeatures'가 필요\nfrom sklearn.preprocessing import StandardScaler\n\n\n# VarianceThreshold : 좋은 feature를 찾기 위해 사용\n# variance가 작다는 말은 정보가 적다는 말과 비슷\n# 다양하게 스펙트럼이 넓어야 얻을 수 있는 정보가 있을 텐데  \n# example : 1개의 feature와 그 안에 100개의 sample(여기서 sample은 [0 or 1]의 값을 가진다고 하면)이 있다고 할때, sample의 구성이 1이 99개고 0이 1개다. \n# --> variance가 거의 없고 얻을 정보가 없는 feature라고 판단됨 --> 그런 feature를 제거하기 위해 'VarianceThreshold'를 사용!\n\nfrom sklearn.feature_selection import VarianceThreshold\n\n\n# SelectFromModel : model마다 학습을 할 때 Feature의 중요도를 결정하게 되는데 그 중요도에 따라서 selection하는 것.\n# 결국 모델을 하나씩 학습을 해야 함. 만약 feature가 1000개면 1000개의 모델을 만들어야 한다는 뜻 \nfrom sklearn.feature_selection import SelectFromModel\n\n\nfrom sklearn.utils import shuffle\nfrom sklearn.ensemble import RandomForestClassifier\n\npd.set_option('display.max_columns', 100) # 최대 100개까지 열의 수를 보여주도록\n\n\n\n# from sklearn.preprocessing import Imputer \n\n# ---->\n\n# 해결 방법\n# Imputer 3 버전 전에 사용되지 않으며 0.22에서 제거되었다. \n# Imputer 모듈을 불러오기 위해선 sklearn.impute를 사용하면 된다.\n# Imputer : 비어져있는 null value를 채우는 sklearn 내장 함수\n\nfrom sklearn.impute import SimpleImputer","metadata":{"execution":{"iopub.status.busy":"2021-10-16T07:01:20.304274Z","iopub.execute_input":"2021-10-16T07:01:20.304673Z","iopub.status.idle":"2021-10-16T07:01:21.55028Z","shell.execute_reply.started":"2021-10-16T07:01:20.30455Z","shell.execute_reply":"2021-10-16T07:01:21.549549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> DEBUGGING\n- DEBUGGING 하는데 굳이 전체 dataset을 다 읽어서 할 이유는 없다..\n- DEBUGGING을 통해 model을 만들기, ensemble,..등의 과정을 checking 해보고 잘 되면 \n- DEBUG = False 를 통해 다시 전체 dataset을 다 읽어서 학습을 진행하면 됨","metadata":{}},{"cell_type":"markdown","source":"##### There are only about 600,000 train data for this competition, so I'll just read them all!","metadata":{}},{"cell_type":"code","source":"# DEBUG = True\nDEBUG = False\n# debugging 마무리 후 \n# DEBUG = False 사용해서 학습 마무리 하기","metadata":{"execution":{"iopub.status.busy":"2021-10-16T07:01:21.551824Z","iopub.execute_input":"2021-10-16T07:01:21.552492Z","iopub.status.idle":"2021-10-16T07:01:21.555995Z","shell.execute_reply.started":"2021-10-16T07:01:21.552443Z","shell.execute_reply":"2021-10-16T07:01:21.555391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if DEBUG:\n    NROWS = 50000\nelse:\n    NROWS = None","metadata":{"execution":{"iopub.status.busy":"2021-10-16T07:01:21.556869Z","iopub.execute_input":"2021-10-16T07:01:21.557397Z","iopub.status.idle":"2021-10-16T07:01:21.570062Z","shell.execute_reply.started":"2021-10-16T07:01:21.557367Z","shell.execute_reply":"2021-10-16T07:01:21.569222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%time\n\ntrain = pd.read_csv('../input/porto-seguro-safe-driver-prediction/train.csv', nrows = NROWS)\ntest = pd.read_csv('../input/porto-seguro-safe-driver-prediction/test.csv', nrows = NROWS)\n\n\n##--------------------train set이 너무 클때!!-------------------------##\n\n# train = train.sample(frac=0.2)\n# train data의 20%를 sample로 가져오겠다.\n# 주로 data의 크기가 너무 클때 ex)train data가 80,000,00개 정도 있다고 하면 \n# 간단한 EDA정도와 feature engineering 진행할 때는 sample로 10~20%정도 (본인 컴퓨터 사양에 따라) sampling을 해서 진행하면 수월하다.! (여기서 sampling은 random_sampling)\n# 전체 set을 가지고 EDA를 하는것은 무모하고..쉽지 않다","metadata":{"execution":{"iopub.status.busy":"2021-10-16T07:01:21.572108Z","iopub.execute_input":"2021-10-16T07:01:21.572675Z","iopub.status.idle":"2021-10-16T07:01:31.63492Z","shell.execute_reply.started":"2021-10-16T07:01:21.572641Z","shell.execute_reply":"2021-10-16T07:01:31.6343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape","metadata":{"execution":{"iopub.status.busy":"2021-10-16T07:01:31.636096Z","iopub.execute_input":"2021-10-16T07:01:31.636454Z","iopub.status.idle":"2021-10-16T07:01:31.643992Z","shell.execute_reply.started":"2021-10-16T07:01:31.63641Z","shell.execute_reply":"2021-10-16T07:01:31.643062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-16T07:01:31.645255Z","iopub.execute_input":"2021-10-16T07:01:31.645742Z","iopub.status.idle":"2021-10-16T07:01:31.695634Z","shell.execute_reply.started":"2021-10-16T07:01:31.645714Z","shell.execute_reply":"2021-10-16T07:01:31.694763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.tail()","metadata":{"execution":{"iopub.status.busy":"2021-10-16T07:01:31.697049Z","iopub.execute_input":"2021-10-16T07:01:31.697629Z","iopub.status.idle":"2021-10-16T07:01:31.735228Z","shell.execute_reply.started":"2021-10-16T07:01:31.697583Z","shell.execute_reply":"2021-10-16T07:01:31.734303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> we can see 'categorical variables of which the category values are integers! let's check it out","metadata":{}},{"cell_type":"code","source":"cat_cols = [col for col in train.columns if 'cat' in col]","metadata":{"execution":{"iopub.status.busy":"2021-10-16T07:17:04.623974Z","iopub.execute_input":"2021-10-16T07:17:04.625636Z","iopub.status.idle":"2021-10-16T07:17:04.638536Z","shell.execute_reply.started":"2021-10-16T07:17:04.625551Z","shell.execute_reply":"2021-10-16T07:17:04.637387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for col in cat_cols:\n#     print(col, train[col].value_counts().shape[0])   #여기서 shape[0]은 각 col의 unique한 행의 수를 가져오기 위함\n\nfor col in cat_cols:\n    print(col, train[col].nunique())   # 'nunique'를 사용해도 됨","metadata":{"execution":{"iopub.status.busy":"2021-10-16T07:23:09.133021Z","iopub.execute_input":"2021-10-16T07:23:09.133723Z","iopub.status.idle":"2021-10-16T07:23:09.190756Z","shell.execute_reply.started":"2021-10-16T07:23:09.13368Z","shell.execute_reply":"2021-10-16T07:23:09.1898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> we can see 'high cardinality' in the 'ps_car_11_cat' category! **104 !!**","metadata":{}},{"cell_type":"markdown","source":"> let's see if there are duplicate rows in the training data. Using 'drop_duplicates()'\n","metadata":{}},{"cell_type":"code","source":"train.drop_duplicates()\ntrain.shape","metadata":{"execution":{"iopub.status.busy":"2021-10-16T07:01:31.736274Z","iopub.execute_input":"2021-10-16T07:01:31.736571Z","iopub.status.idle":"2021-10-16T07:01:32.529605Z","shell.execute_reply.started":"2021-10-16T07:01:31.736542Z","shell.execute_reply":"2021-10-16T07:01:32.528603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- no duplicate rows, so that's fine","metadata":{}},{"cell_type":"code","source":"test.shape","metadata":{"execution":{"iopub.status.busy":"2021-10-16T07:01:32.530989Z","iopub.execute_input":"2021-10-16T07:01:32.531375Z","iopub.status.idle":"2021-10-16T07:01:32.537988Z","shell.execute_reply.started":"2021-10-16T07:01:32.531335Z","shell.execute_reply":"2021-10-16T07:01:32.536841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":">  let's investigate how many variables of each type we have! ","metadata":{}},{"cell_type":"markdown","source":"> So later on we can create dummy variables for the 14 categorical variables. The bin variables are already binary and do not need dummification.! **(dummification == one-hot encoding)**","metadata":{}},{"cell_type":"markdown","source":"#### Dummy variable\n\n1. what is Dummy variable\n- categorical variable -> continuous variable\n- 범주형 변수를 연속형 변수로 변환. 즉 연속형 변수'스럽게' 만드는 것\n\n2. Why we should make Dummy variable\n- 범주형 변수로는 사용할 수 없고 연속형 변수로만 가능한 분석기법을 사용할 수 있게 해줌.\n- linear regression, logistic regression : 설명변수가 연속형 변수여야 사용 가능.\n- 만약, 설명변수 중 범주형 변수가 있다면, 그 변수를 더미 변수로 변환해 분석에 사용\n\n3. The features of Dummy variable\n- 0또는1의 값을 가진다\n- 더미변수는 원래 범주형 변수의 범주 개수보다 1개 적게 만들어진다(더미 변수로 만들어지지 않고 생략되는 범주는 기준이 되는 값이라고 이해하면 됨)\n\n4. Make Dummy variable\n- 범주형 변수의 범주 중 기준이 되는 값을 정한다\n- 기준이 되는 변수를 제외하고 더미변수를 만든다\n\n5. The meaning of Dummy variable\n- 회귀식에서 해당 변수의 효과를 0또는 상수값으로 만들어 준다\n- 더미변수는 회귀 기울기를 바꾸지는 않고 절편만 바꾸어 평행하게 움직이게 하는 역할을 한다","metadata":{}},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2021-10-16T07:01:32.54113Z","iopub.execute_input":"2021-10-16T07:01:32.541599Z","iopub.status.idle":"2021-10-16T07:01:32.617729Z","shell.execute_reply.started":"2021-10-16T07:01:32.541489Z","shell.execute_reply":"2021-10-16T07:01:32.616886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}