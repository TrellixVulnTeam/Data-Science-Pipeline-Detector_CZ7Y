{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"출처: https://www.kaggle.com/code/arthurtok/interactive-porto-insights-a-plot-ly-tutorial/notebook","metadata":{}},{"cell_type":"markdown","source":"# Introduction\n\n1. Data Quality Checks\n2. Feature inspection and filtering\n3. Feature imporatnce ranking via learning models","metadata":{}},{"cell_type":"markdown","source":"-----------------","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport warnings\nfrom collections import Counter\nfrom sklearn.feature_selection import mutual_info_classif\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2022-05-24T02:14:01.598737Z","iopub.execute_input":"2022-05-24T02:14:01.599324Z","iopub.status.idle":"2022-05-24T02:14:03.090493Z","shell.execute_reply.started":"2022-05-24T02:14:01.599203Z","shell.execute_reply":"2022-05-24T02:14:03.089631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"데이터 로드하기","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"../input/porto-seguro-safe-driver-prediction/train.csv\")\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-24T02:14:03.093165Z","iopub.execute_input":"2022-05-24T02:14:03.09369Z","iopub.status.idle":"2022-05-24T02:14:07.066552Z","shell.execute_reply.started":"2022-05-24T02:14:03.09365Z","shell.execute_reply":"2022-05-24T02:14:07.065696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rows, columns = train.shape[0], train.shape[1]\nprint(\"The train dataset contains {0} rows and {1} columns\".format(rows, columns))","metadata":{"execution":{"iopub.status.busy":"2022-05-24T02:14:07.068066Z","iopub.execute_input":"2022-05-24T02:14:07.068354Z","iopub.status.idle":"2022-05-24T02:14:07.07367Z","shell.execute_reply.started":"2022-05-24T02:14:07.068315Z","shell.execute_reply":"2022-05-24T02:14:07.072941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"-----\n\n# 1. Data Quality checks\n\n**Null or missing values check**","metadata":{}},{"cell_type":"code","source":"train.isnull().any().any()","metadata":{"execution":{"iopub.status.busy":"2022-05-24T02:14:07.075988Z","iopub.execute_input":"2022-05-24T02:14:07.076441Z","iopub.status.idle":"2022-05-24T02:14:07.106751Z","shell.execute_reply.started":"2022-05-24T02:14:07.076401Z","shell.execute_reply":"2022-05-24T02:14:07.105865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- null 값의 유무를 체크하면 False로 나오지만 데이터의 설명처럼 null 값은 -1로 표현되어 있습니다.\n\n이제 어떤 칼럼에 모든 -1을 null 값으로 바꿔봅시다.","metadata":{}},{"cell_type":"code","source":"train_copy = train\ntrain_copy = train_copy.replace(-1, np.NaN)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T02:14:07.108132Z","iopub.execute_input":"2022-05-24T02:14:07.108449Z","iopub.status.idle":"2022-05-24T02:14:07.246514Z","shell.execute_reply.started":"2022-05-24T02:14:07.108407Z","shell.execute_reply":"2022-05-24T02:14:07.245727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"이제 null 값을 시각화하는데 유용한 Missingno라는 패키지를 사용하겠습니다.","metadata":{}},{"cell_type":"code","source":"import missingno as msno\n\nmsno.matrix(df=train_copy.iloc[:, 2:39], figsize=(20, 14), color=(0.42, 0.1, 0.05))","metadata":{"execution":{"iopub.status.busy":"2022-05-24T02:14:07.248101Z","iopub.execute_input":"2022-05-24T02:14:07.248378Z","iopub.status.idle":"2022-05-24T02:14:12.856502Z","shell.execute_reply.started":"2022-05-24T02:14:07.24834Z","shell.execute_reply":"2022-05-24T02:14:12.855822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"수직의 갈색 밴드에 겹쳐 놓은 흰색 밴드는 각각의 칼럼의 null 값을 반영합니다. \n\n그리고 흰색의 비율에서 알 수 있듯이 3캐의 칼럼(ps_reg_03, ps_car_03_cat, ps_car_05_cat)은 null 값의 비율이 굉장히 높습니다. 따라서 이 세개의 칼럼에서는 -1을 null 값으로 바꾸는 것은 좋은 선택 같아 보이지 않습니다.","metadata":{}},{"cell_type":"markdown","source":"**Target variable inspection**","metadata":{}},{"cell_type":"code","source":"data = [go.Bar(\n    x=train[\"target\"].value_counts().index.values,\n    y=train[\"target\"].value_counts().values,\n    text=\"Distribution of target variable\"\n)]\n\nlayout = go.Layout(title=\"Target variable distribution\")\n\nfig = go.Figure(data=data, layout=layout)\n\npy.iplot(fig, filename=\"basic-bar\")","metadata":{"execution":{"iopub.status.busy":"2022-05-24T02:14:12.857569Z","iopub.execute_input":"2022-05-24T02:14:12.857945Z","iopub.status.idle":"2022-05-24T02:14:13.571757Z","shell.execute_reply.started":"2022-05-24T02:14:12.857911Z","shell.execute_reply":"2022-05-24T02:14:13.571013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"target 값의 분포가 매우 불균형한 모습","metadata":{}},{"cell_type":"markdown","source":"**Datatype check**","metadata":{}},{"cell_type":"code","source":"Counter(train.dtypes.values)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T02:14:13.572912Z","iopub.execute_input":"2022-05-24T02:14:13.57412Z","iopub.status.idle":"2022-05-24T02:14:13.581641Z","shell.execute_reply.started":"2022-05-24T02:14:13.574077Z","shell.execute_reply":"2022-05-24T02:14:13.58077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_float = train.select_dtypes(include=[\"float64\"])\ntrain_int = train.select_dtypes(include=[\"int64\"])","metadata":{"execution":{"iopub.status.busy":"2022-05-24T02:14:13.583362Z","iopub.execute_input":"2022-05-24T02:14:13.583631Z","iopub.status.idle":"2022-05-24T02:14:13.671248Z","shell.execute_reply.started":"2022-05-24T02:14:13.583593Z","shell.execute_reply":"2022-05-24T02:14:13.670255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Correlation plots\n\n**Correlation of float features**","metadata":{}},{"cell_type":"code","source":"colormap = plt.cm.magma\nplt.figure(figsize=(20, 16))\nplt.title(\"Pearson correlation of continuous features\", y=1.05, size=15)\nsns.heatmap(train_float.corr(), linewidths=0.1, vmax=1.0, square=True,\n            cmap=colormap, linecolor=\"white\", annot=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T02:14:13.674753Z","iopub.execute_input":"2022-05-24T02:14:13.675103Z","iopub.status.idle":"2022-05-24T02:14:14.601144Z","shell.execute_reply.started":"2022-05-24T02:14:13.675063Z","shell.execute_reply":"2022-05-24T02:14:14.600373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"위의 heatmap에서 보듯이 상관관계가 거의 없거나 0인 많은 대다수의 칼럼을 볼 수 있습니다. \n\n그리고 다음 칼럼들은 양의 상관관계를 가지고 있습니다:\n\n**(ps_reg_01, ps_reg_03)**\n\n**(ps_reg_02, ps_reg_03)**\n\n**(ps_car_12, ps_car_13)**\n\n**(ps_car_13, ps_car_15)**","metadata":{}},{"cell_type":"markdown","source":"<br/>\n\n**Correlation of integer features**","metadata":{}},{"cell_type":"code","source":"data = [\n    go.Heatmap(\n        x=train_int.columns.values,\n        y=train_int.columns.values,\n        z=train_int.corr().values,\n        colorscale=\"Viridis\",\n        reversescale=False,\n        opacity=1.0\n    )\n]\n\nlayout = go.Layout(\n    title=\"Pearson Correlation of Integer-type features\",\n    xaxis=dict(ticks=\"\", nticks=36),\n    yaxis=dict(ticks=\"\"),\n    width=900, height=700\n)\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename=\"labelled-heatmap\")","metadata":{"execution":{"iopub.status.busy":"2022-05-24T02:14:14.602194Z","iopub.execute_input":"2022-05-24T02:14:14.602465Z","iopub.status.idle":"2022-05-24T02:14:18.704873Z","shell.execute_reply.started":"2022-05-24T02:14:14.60243Z","shell.execute_reply":"2022-05-24T02:14:18.704202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"float 칼럼들과 마찬가지로 상당히 많은 상관관계가 0인 칼럼들이 관찰됩니다. 그리고 이러한 사실은 우리에게 유용한 정보를 줍니다. (PCA와 같은 차원축소는 어느정도의 상관관계가 필요하기 때문)\n\n**음의 상관관계를 가지는 feature** : ps_ind_06_bin, ps_ind_07_bin, ps_ind_08_bin, ps_ind_09_bin\n\n또 한가지 흥미로운 점은 null 값이 많았던 ps_car_03_cat과 ps_car_05_cat은 서로 강한 양의 상관관계를 지닌다는 것입니다. (null 값이 많기 때문에 데이터의 진짜 의미를 반영하지 못했을 수도 있습니다)","metadata":{}},{"cell_type":"markdown","source":"## Mutual Information plots\n\n**상호정보량(mutual information)**은 결합확률밀도함수 $p(x,y)$와 주변확률밀도함수의 곱 $p(x)p(y)$의 쿨벡-라이블러 발산이다. \n\n- Kullback–Leibler divergence(KLD), 쿨벡-라이블러 발산은 두 확률분포가 얼마나 다른지를 정량적으로 나타내는 수치다. 두 확률분포가 같으면 쿨벡-라이블러 발산은 0이 되고 다르면 다를수록 큰 값을 가진다 \\begin{align}\nKL(p||q) = \\sum_{i=1}^{K} p(y_i) \\log_2 \\left(\\dfrac{p(y_i)}{q(y_i)}\\right)\n\\end{align}\n\n즉 결합확률밀도함수와 주변확률밀도함수의 차이를 측정하므로써 두 확률변수의 상관관계를 측정하는 방법이다. 만약 두 확률변수가 독립이면 결합확률밀도함수는 주변확률밀도함수의 곱과 같으므로 상호정보량은 0이 된다. 반대로 상관관계가 있다면 그만큼 양의 상호정보량을 가진다.\n\\begin{align}\nMI[X,Y] = KL\\big(p(x,y)||p(x)p(y)\\big)\n= \\sum_{i=1}^{K} p(x_i,y_i) \\log_2 \\left(\\dfrac{p(x_i,y_i)}{p(x_i)p(y_i)}\\right)\n\\end{align}","metadata":{}},{"cell_type":"code","source":"# classification 문제의 경우 mutual_info_classif 메서드를 통해 간단하게 의존성을 확일할 수 있다.\n# 이를 통해 target의 정보가 얼마나 많이 feature에 담겨 있는지 확인할 수 있습니다.\n# sklearn에서의 mutual_info_classif 메서드는 knn에 기반합니다.\nmf = mutual_info_classif(train_float.values, train.target.values, n_neighbors=3,\n                         random_state=17)\nprint(mf)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T02:14:18.705929Z","iopub.execute_input":"2022-05-24T02:14:18.706318Z","iopub.status.idle":"2022-05-24T02:14:58.001292Z","shell.execute_reply.started":"2022-05-24T02:14:18.706266Z","shell.execute_reply":"2022-05-24T02:14:58.000425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mf = mutual_info_classif(train_int.values, train.target.values, n_neighbors=3,\n                         random_state=17)\nprint(mf)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T02:14:58.002691Z","iopub.execute_input":"2022-05-24T02:14:58.003002Z","iopub.status.idle":"2022-05-24T02:18:12.573152Z","shell.execute_reply.started":"2022-05-24T02:14:58.002941Z","shell.execute_reply":"2022-05-24T02:18:12.572256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Binary features inspection","metadata":{}},{"cell_type":"code","source":"bin_col = [col for col in train.columns if \"_bin\" in col]\nzero_list = []\none_list = []\nfor col in bin_col:\n    zero_list.append((train[col] == 0).sum())\n    one_list.append((train[col] == 1).sum())","metadata":{"execution":{"iopub.status.busy":"2022-05-24T02:18:12.574551Z","iopub.execute_input":"2022-05-24T02:18:12.577134Z","iopub.status.idle":"2022-05-24T02:18:12.630998Z","shell.execute_reply.started":"2022-05-24T02:18:12.577089Z","shell.execute_reply":"2022-05-24T02:18:12.63024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trace1 = go.Bar(\n    x=bin_col,\n    y=zero_list,\n    name=\"Zero count\"\n)\n\ntrace2 = go.Bar(\n    x=bin_col,\n    y=one_list,\n    name=\"One count\"\n)\n\ndata = [trace1, trace2]\nlayout = go.Layout(\n    barmode=\"stack\",\n    title=\"Count of 1 and - in binary variables\"\n)\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename=\"stacked-bar\")","metadata":{"execution":{"iopub.status.busy":"2022-05-24T02:18:12.632391Z","iopub.execute_input":"2022-05-24T02:18:12.632651Z","iopub.status.idle":"2022-05-24T02:18:12.672049Z","shell.execute_reply.started":"2022-05-24T02:18:12.632618Z","shell.execute_reply":"2022-05-24T02:18:12.671351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ps_ind_10_bin, ps_ind_11_bin, ps_ind_12_bin, ps_ind_13_bin 칼럼들은 0의 값이 대부분입니다. 위의 칼럼들은 target을 예측하는데 유용한지에 대한 질문을 던지게 합니다.","metadata":{}},{"cell_type":"markdown","source":"## Feature importance via Random Forest","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=150, max_depth=8, min_samples_leaf=4,\n                            max_features=0.2, n_jobs=-1, random_state=0)\nrf.fit(train.drop([\"id\", \"target\"], axis=1), train.target)\nfeatures = train.drop([\"id\", \"target\"], axis=1).columns.values","metadata":{"execution":{"iopub.status.busy":"2022-05-24T02:18:12.673428Z","iopub.execute_input":"2022-05-24T02:18:12.673694Z","iopub.status.idle":"2022-05-24T02:20:45.607702Z","shell.execute_reply.started":"2022-05-24T02:18:12.673659Z","shell.execute_reply":"2022-05-24T02:20:45.606872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Plot.ly Scatter Plot of feature importances**","metadata":{}},{"cell_type":"code","source":"# Scatter plot\ntrace = go.Scatter(\n    y=rf.feature_importances_,\n    x=features,\n    mode=\"markers\",\n    marker=dict(\n        sizemode=\"diameter\",\n        sizeref=1,\n        size=13,\n        color=rf.feature_importances_,\n        colorscale=\"Portland\",\n        showscale=True\n    ),\n    text=features\n)\n\ndata = [trace]\n\nlayout = go.Layout(\n    autosize=True,\n    title=\"Random Forest Feature Importance\",\n    hovermode=\"closest\",\n    xaxis=dict(\n        ticklen=5,\n        showgrid=False,\n        zeroline=False,\n        showline=False\n    ),\n    yaxis=dict(\n        title=\"Feature Importance\",\n        showgrid=False,\n        zeroline=False,\n        ticklen=5,\n        gridwidth=2\n    ),\n    showlegend=False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename=\"scatter2010\")","metadata":{"execution":{"iopub.status.busy":"2022-05-24T02:20:45.608947Z","iopub.execute_input":"2022-05-24T02:20:45.609222Z","iopub.status.idle":"2022-05-24T02:20:45.875021Z","shell.execute_reply.started":"2022-05-24T02:20:45.609188Z","shell.execute_reply":"2022-05-24T02:20:45.874289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x, y = (list(x) for x in zip(*sorted(zip(rf.feature_importances_, features),\n                                      reverse=False)))\n\n# Barplot\ntrace2 = go.Bar(\n    x=x,\n    y=y,\n    marker=dict(\n        color=x,\n        colorscale=\"Viridis\",\n        reversescale=True\n    ),\n    name=\"Random Forest Feature importance\",\n    orientation=\"h\"\n)\n\nlayout = dict(\n    title=\"Barplot of Feature importances\",\n    width=900, height=2000,\n    yaxis=dict(\n        showgrid=False,\n        showline=False,\n        showticklabels=True\n    )\n)\n\nfig1 = go.Figure(data=[trace2])\nfig1[\"layout\"].update(layout)\npy.iplot(fig1, filename=\"plots\")","metadata":{"execution":{"iopub.status.busy":"2022-05-24T02:20:45.876201Z","iopub.execute_input":"2022-05-24T02:20:45.87681Z","iopub.status.idle":"2022-05-24T02:20:46.029651Z","shell.execute_reply.started":"2022-05-24T02:20:45.876765Z","shell.execute_reply":"2022-05-24T02:20:46.028994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Decision Tree visualisation**","metadata":{}},{"cell_type":"code","source":"from sklearn import tree\nfrom IPython.display import Image as PImage\nfrom subprocess import check_call\nfrom PIL import Image, ImageDraw, ImageFont\nimport re\n\ndecision_tree = tree.DecisionTreeClassifier(max_depth=3)\ndecision_tree.fit(train.drop([\"id\", \"target\"], axis=1), train.target)\n\n# 훈련된 모델을 .dot 파일로 내보내기\nwith open(\"tree1.dot\", \"w\") as f:\n    f = tree.export_graphviz(decision_tree, out_file=f, max_depth=4,\n                             impurity=False,\n                             feature_names=train.drop([\"id\", \"target\"], axis=1).columns.values,\n                             class_names=[\"No\", \"Yes\"],\n                             rounded=True,\n                             filled=True)\n    \n    \n# jupyter notebook에 표시할 수 있도록 .dot를 .png로 변환\ncheck_call([\"dot\", \"-Tpng\", \"tree1.dot\", \"-o\", \"tree1.png\"])\n\n# PIL 라이브러리로 시각화\nimg = Image.open(\"tree1.png\")\ndraw = ImageDraw.Draw(img)\nimg.save(\"sample-out.png\")\nPImage(\"sample-out.png\")","metadata":{"execution":{"iopub.status.busy":"2022-05-24T02:20:46.030863Z","iopub.execute_input":"2022-05-24T02:20:46.031528Z","iopub.status.idle":"2022-05-24T02:20:49.721232Z","shell.execute_reply.started":"2022-05-24T02:20:46.031484Z","shell.execute_reply":"2022-05-24T02:20:49.720209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature importance via Gradient Boosting model","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\ngb = GradientBoostingClassifier(n_estimators=100, max_depth=3, min_samples_leaf=4,\n                                max_features=0.2, random_state=0)\ngb.fit(train.drop([\"id\", \"target\"], axis=1), train.target)\nfeatures = train.drop([\"id\", \"target\"], axis=1).columns.values","metadata":{"execution":{"iopub.status.busy":"2022-05-24T02:20:49.72419Z","iopub.execute_input":"2022-05-24T02:20:49.724866Z","iopub.status.idle":"2022-05-24T02:21:45.897474Z","shell.execute_reply.started":"2022-05-24T02:20:49.724797Z","shell.execute_reply":"2022-05-24T02:21:45.896636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scatter plot\ntrace = go.Scatter(\n    y=gb.feature_importances_,\n    x=features,\n    mode=\"markers\",\n    marker=dict(\n        sizemode=\"diameter\",\n        sizeref=1,\n        size=13,\n        color=gb.feature_importances_,\n        colorscale=\"Portland\",\n        showscale=True\n    ),\n    text=features\n)\ndata = [trace]\n\nlayout = go.Layout(\n    autosize=True,\n    title=\"Gradient Boosting Machine Feature Importance\",\n    hovermode=\"closest\",\n    xaxis=dict(\n        ticklen=5,\n        showgrid=False,\n        zeroline=False,\n        showline=False\n    ),\n    yaxis=dict(\n        title=\"Feature Importance\",\n        showgrid=False,\n        zeroline=False,\n        ticklen=5,\n        gridwidth=2\n    ),\n    showlegend=False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename=\"scatter2010\")","metadata":{"execution":{"iopub.status.busy":"2022-05-24T02:21:45.898908Z","iopub.execute_input":"2022-05-24T02:21:45.899197Z","iopub.status.idle":"2022-05-24T02:21:45.948105Z","shell.execute_reply.started":"2022-05-24T02:21:45.89915Z","shell.execute_reply":"2022-05-24T02:21:45.947371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x, y = (list(x) for x in zip(*sorted(zip(gb.feature_importances_, features), \n                                                            reverse = False)))\n\ntrace2 = go.Bar(\n    x=x,\n    y=y,\n    marker=dict(\n        color=x,\n        colorscale=\"Viridis\",\n        reversescale=True\n    ),\n    name=\"Gradient Boosting Classifier Feature importance\",\n    orientation=\"h\"\n)\n\nlayout = dict(\n    title=\"Barplot of Feature importances\",\n    width=900, height=2000,\n    yaxis=dict(\n        showgrid=False,\n        showline=False,\n        showticklabels=True\n    )\n)\n\nfig1 = go.Figure(data=[trace2])\nfig1[\"layout\"].update(layout)\npy.iplot(fig1, filename=\"plots\")","metadata":{"execution":{"iopub.status.busy":"2022-05-24T02:21:45.949245Z","iopub.execute_input":"2022-05-24T02:21:45.9501Z","iopub.status.idle":"2022-05-24T02:21:45.998283Z","shell.execute_reply.started":"2022-05-24T02:21:45.950062Z","shell.execute_reply":"2022-05-24T02:21:45.997306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Random Forest와 Gradient Boosting 모델에서 가장 중요한 칼럼은 **ps_car_13**","metadata":{}},{"cell_type":"markdown","source":"--------------\n# Conclusion\n\nnull 값과 데이터를 점검하여 Porto Seguro 데이터 세트에 대해 상당히 광범위한 검사를 수행하고, feature 간의 선형 상관 관계를 조사하고, 일부 feature 분포를 검사하고 몇 가지 학습 모델(Random Forest, Gradient Boosting)을 구현했습니다. (모델이 중요하다고 생각하는 feature을 식별하기 위해)","metadata":{}}]}