{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"출처: https://www.kaggle.com/code/bertcarremans/data-preparation-exploration/notebook","metadata":{}},{"cell_type":"markdown","source":"# Introduction\n\n1. Visual inspection of your data\n2. Defining the metadata\n3. Descriptive statistics\n4. Handling imbalanced classes\n5. Data quality checks\n6. Exploratory data visualization\n7. Feature engineering\n8. Feature selection\n9. Feature scaling\n\n------------","metadata":{}},{"cell_type":"markdown","source":"# Loading packages","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.utils import shuffle\nfrom sklearn.ensemble import RandomForestClassifier\n\npd.set_option(\"display.max_columns\", 100)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T04:47:33.492915Z","iopub.execute_input":"2022-05-21T04:47:33.493837Z","iopub.status.idle":"2022-05-21T04:47:33.647718Z","shell.execute_reply.started":"2022-05-21T04:47:33.493768Z","shell.execute_reply":"2022-05-21T04:47:33.64704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"../input/porto-seguro-safe-driver-prediction/train.csv\")\ntest = pd.read_csv(\"../input/porto-seguro-safe-driver-prediction/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-05-21T04:47:33.649298Z","iopub.execute_input":"2022-05-21T04:47:33.649616Z","iopub.status.idle":"2022-05-21T04:47:43.647868Z","shell.execute_reply.started":"2022-05-21T04:47:33.649579Z","shell.execute_reply":"2022-05-21T04:47:43.647114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data at first sight","metadata":{}},{"cell_type":"markdown","source":"- 비슷한 그룹에 속해있는 Feature는 ind, reg, car,calc 같은 이름이 붙어있다.\n- binary features인지 categorical features인지 나타내기 위해 맨뒤에 bin이나 cat라고 붙어있다.\n- 위의 명칭이 붙어 있지 않는 칼럼은 연속형이나 순서형 자료\n- -1은 결측치를 나타낸다.\n- Target 칼럼은 보험 계약자가 보험 청구 유무를 나타낸다.","metadata":{}},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-21T04:47:43.648999Z","iopub.execute_input":"2022-05-21T04:47:43.64945Z","iopub.status.idle":"2022-05-21T04:47:43.688218Z","shell.execute_reply.started":"2022-05-21T04:47:43.649413Z","shell.execute_reply":"2022-05-21T04:47:43.687467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.tail()","metadata":{"execution":{"iopub.status.busy":"2022-05-21T04:47:43.690227Z","iopub.execute_input":"2022-05-21T04:47:43.690489Z","iopub.status.idle":"2022-05-21T04:47:43.723105Z","shell.execute_reply.started":"2022-05-21T04:47:43.690455Z","shell.execute_reply":"2022-05-21T04:47:43.722363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape","metadata":{"execution":{"iopub.status.busy":"2022-05-21T04:47:43.724199Z","iopub.execute_input":"2022-05-21T04:47:43.724561Z","iopub.status.idle":"2022-05-21T04:47:43.730253Z","shell.execute_reply.started":"2022-05-21T04:47:43.724526Z","shell.execute_reply":"2022-05-21T04:47:43.729428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 중복이 있는지 확인\ntrain.drop_duplicates()\ntrain.shape","metadata":{"execution":{"iopub.status.busy":"2022-05-21T04:47:43.731737Z","iopub.execute_input":"2022-05-21T04:47:43.732043Z","iopub.status.idle":"2022-05-21T04:47:44.463371Z","shell.execute_reply.started":"2022-05-21T04:47:43.731989Z","shell.execute_reply":"2022-05-21T04:47:44.462575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"중복 열은 없습니다.","metadata":{}},{"cell_type":"code","source":"test.shape","metadata":{"execution":{"iopub.status.busy":"2022-05-21T04:47:44.464514Z","iopub.execute_input":"2022-05-21T04:47:44.465075Z","iopub.status.idle":"2022-05-21T04:47:44.471296Z","shell.execute_reply.started":"2022-05-21T04:47:44.465034Z","shell.execute_reply":"2022-05-21T04:47:44.470524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"각각의 변수 타입을 확인해봅시다.\n\n그래야 나중에 14개의 범주형 변수에 대해 dummy variables(가변수: 독립변수를 0과 1로 변환한 변수)로 만들 수 있습니다.\n\nbin이 들어간 칼럼은 이미 binary 값을 가지기 때문에 가변수화할 필요가 없습니다.","metadata":{}},{"cell_type":"code","source":"train.info()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-05-21T04:47:44.472905Z","iopub.execute_input":"2022-05-21T04:47:44.473254Z","iopub.status.idle":"2022-05-21T04:47:44.54098Z","shell.execute_reply.started":"2022-05-21T04:47:44.473209Z","shell.execute_reply":"2022-05-21T04:47:44.539922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Metadata","metadata":{}},{"cell_type":"markdown","source":"데이터 관리를 용이하게 하기 위해, 데이터 프레임의 변수에 관한 메타데이터를 저장할 것입니다. 이는 분석, 시각화, 모델링을 위한 특정한 변수를 선택하는데 도움을 줄 것입니다.\n\n구체적으로 저장할 것들:\n- **role**: input, ID, target\n- **level**: nominal, interval(구간 자료: 자료가 일정한 구간의 뜻을 가지고 있어, 나타난 숫자의 덧셈이나 뺄셈의 결과는 의미가 있지만 곱셈이나 나눗셈의 결과는 의미가 없는 자료), ordinal, binary\n- **keep**: True or False\n- **dtype**: int, float, str","metadata":{}},{"cell_type":"code","source":"data = []\nfor f in train.columns:\n    # role 정의\n    if f == \"target\":\n        role = \"target\"\n    elif f == \"id\":\n        role = \"id\"\n    else:\n        role = \"input\"\n        \n    # level 정의\n    if 'bin' in f or f == 'target':\n        level = 'binary'\n    elif 'cat' in f or f == 'id':\n        level = 'nominal'\n    elif np.issubdtype(train[f].dtype, np.floating):\n        level = 'interval'\n    elif np.issubdtype(train[f].dtype, np.integer):\n        level = 'ordinal'\n        \n    # id를 제외한 모든 변수에 대해 True로 초기화\n    keep = True\n    if f == \"id\":\n        keep = False\n    \n    # dtype 정의\n    dtype = train[f].dtype\n    \n    # 메타데이터를 담은 딕셔너리를 만든다\n    f_dict = {\n        'varname': f,\n        'role': role,\n        'level': level,\n        'keep': keep,\n        'dtype': dtype\n    }\n    data.append(f_dict)\n    \nmeta = pd.DataFrame(data, columns=['varname', 'role', 'level', 'keep', 'dtype'])\nmeta.set_index(\"varname\", inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T04:47:44.542339Z","iopub.execute_input":"2022-05-21T04:47:44.542574Z","iopub.status.idle":"2022-05-21T04:47:44.553226Z","shell.execute_reply.started":"2022-05-21T04:47:44.542541Z","shell.execute_reply":"2022-05-21T04:47:44.552525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"meta","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-05-21T04:47:44.557071Z","iopub.execute_input":"2022-05-21T04:47:44.557309Z","iopub.status.idle":"2022-05-21T04:47:44.579315Z","shell.execute_reply.started":"2022-05-21T04:47:44.557279Z","shell.execute_reply":"2022-05-21T04:47:44.57856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"삭제되지 않은 명목형 변수를 추출하는 예","metadata":{}},{"cell_type":"code","source":"meta[(meta.level == \"nominal\") & (meta.keep)].index","metadata":{"execution":{"iopub.status.busy":"2022-05-21T04:47:44.580536Z","iopub.execute_input":"2022-05-21T04:47:44.580823Z","iopub.status.idle":"2022-05-21T04:47:44.593511Z","shell.execute_reply.started":"2022-05-21T04:47:44.580789Z","shell.execute_reply":"2022-05-21T04:47:44.592655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"role과 level에 속한 변수의 수","metadata":{}},{"cell_type":"code","source":"pd.DataFrame({\"count\": meta.groupby([\"role\", \"level\"])[\"role\"].size()}).reset_index()","metadata":{"execution":{"iopub.status.busy":"2022-05-21T04:47:44.595099Z","iopub.execute_input":"2022-05-21T04:47:44.595416Z","iopub.status.idle":"2022-05-21T04:47:44.611143Z","shell.execute_reply.started":"2022-05-21T04:47:44.595384Z","shell.execute_reply":"2022-05-21T04:47:44.610146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Descriptive statistics\n카테고리형 변수나 id는 mean, std 등을 계산하는 것이 의미가 없습니다. 따라서 특정한 변수들만 골라 기술적 통계를 계산해봅니다\n## Interval variables","metadata":{}},{"cell_type":"code","source":"v = meta[(meta.level == \"interval\") & (meta.keep)].index\ntrain[v].describe()","metadata":{"execution":{"iopub.status.busy":"2022-05-21T04:47:44.612985Z","iopub.execute_input":"2022-05-21T04:47:44.613383Z","iopub.status.idle":"2022-05-21T04:47:44.881864Z","shell.execute_reply.started":"2022-05-21T04:47:44.613344Z","shell.execute_reply":"2022-05-21T04:47:44.880935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**reg variables**\n- 오직 ps_reg_03만 결측치를 가지고 있다.\n- 변수간 데이터 범주(min ~ max)가 다르다. 따라서, 스케일링을 적용해줄 수 있다. (하지만, 분류기에 따라 다름)\n\n**car variables**\n- ps_car_12와 ps_car_15는 결측치를 가지고 있다.\n- 데이터 범주가 다르기 때문에 스케일링을 적용할 수 있다.\n\n**calc variables**\n- 결측치가 없다.\n- 최댓값이 0.9인 것으로 보아 비율인 것 같습니다.\n- 3개의 _cal 변수는 비슷한 분포를 가지고 있습니다.\n\n","metadata":{}},{"cell_type":"markdown","source":"## Ordinal variables","metadata":{}},{"cell_type":"code","source":"v = meta[(meta.level == \"ordinal\") & (meta.keep)].index\ntrain[v].describe()","metadata":{"execution":{"iopub.status.busy":"2022-05-21T04:47:44.883508Z","iopub.execute_input":"2022-05-21T04:47:44.883796Z","iopub.status.idle":"2022-05-21T04:47:45.187982Z","shell.execute_reply.started":"2022-05-21T04:47:44.883757Z","shell.execute_reply":"2022-05-21T04:47:45.187155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- ps_car_11만 결측치가 있다.\n- 데이터 범주가 다르기 때문에 스케일링을 해줘야 한다.","metadata":{}},{"cell_type":"markdown","source":"## Binary variables","metadata":{}},{"cell_type":"code","source":"v = meta[(meta.level == \"binary\") & (meta.keep)].index\ntrain[v].describe()","metadata":{"execution":{"iopub.status.busy":"2022-05-21T04:47:45.189422Z","iopub.execute_input":"2022-05-21T04:47:45.189702Z","iopub.status.idle":"2022-05-21T04:47:45.469206Z","shell.execute_reply.started":"2022-05-21T04:47:45.189664Z","shell.execute_reply":"2022-05-21T04:47:45.468397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- train data에서의 a priori probability(원하는 결과/총 결과 수)는 3.645%에 불과합니다. 이는 데이터가 매우 불균형하다는 것을 알 수 있습니다.\n- mean 값을 통해 대부분의 값이 0이라는 것을 알 수 있다.","metadata":{}},{"cell_type":"markdown","source":"# Handling imbalanced classes\n\ntarget에서 1은 0보다 훨씬 적다. 이러한 문제들을 해결하기 위한 두가지 방법이 있다.\n\n- target=1을 오버샘플링\n- target=0을 언더샘플링\n\n여기서는 train data의 크기가 꽤 크므로 **언더샘플링**을 진행한다.\n\n\n(**언더 샘플링은 불균형한 데이터 셋에서 높은 비율을 차지하던 클래스의 데이터 수를 줄임으로써 데이터 불균형을 해소하는 아이디어. 하지만, 이 방법은 학습에 사용되는 전체 데이터 수를 급격하게 감소시켜 오히려 성능이 떨어질 수 있습니다.**\n\n**오버 샘플링은 낮은 비율 클래스의 데이터 수를 늘림으로써 데이터 불균형을 해소하는 아이디어 입니다.**)","metadata":{}},{"cell_type":"code","source":"desired_apriori = 0.1\n\nidx_0 = train[train.target == 0].index\nidx_1 = train[train.target == 1].index\n\nnb_0 = len(train.loc[idx_0])\nnb_1 = len(train.loc[idx_1])\n\n# undersampling 비율과 target=0의 개수를 프린트\nundersampling_rate = ((1 - desired_apriori) * nb_1) / (nb_0 * desired_apriori)\nundersampled_nb_0 = int(undersampling_rate * nb_0)\nprint('Rate to undersample records with target=0: {}'.format(undersampling_rate))\nprint('Number of records with target=0 after undersampling: {}'.format(undersampled_nb_0))\n\n# 랜덤하게 선택\nundersampled_idx = shuffle(idx_0, random_state=37, n_samples=undersampled_nb_0)\n\n\n# 남아있는 인덱스로 리스트 생성\nidx_list = list(undersampled_idx) + list(idx_1)\n\ntrain = train.loc[idx_list].reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T04:47:45.470647Z","iopub.execute_input":"2022-05-21T04:47:45.470896Z","iopub.status.idle":"2022-05-21T04:47:45.948668Z","shell.execute_reply.started":"2022-05-21T04:47:45.470862Z","shell.execute_reply":"2022-05-21T04:47:45.947951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Quality Checks\n## Checking missing values\n\n결측치는 -1","metadata":{}},{"cell_type":"code","source":"vars_with_missing = []\n\nfor f in train.columns:\n    missings = train[train[f] == -1][f].count()\n    if missings > 0:\n        vars_with_missing.append(f)\n        missings_perc = missings / train.shape[0]\n        \n        print(f\"Variable {f} has {missings} records ({missings_perc:.2%}) with missing values\")\nprint('In total, there are {} variables with missing values'.format(len(vars_with_missing)))","metadata":{"execution":{"iopub.status.busy":"2022-05-21T04:47:45.949817Z","iopub.execute_input":"2022-05-21T04:47:45.950143Z","iopub.status.idle":"2022-05-21T04:47:46.072848Z","shell.execute_reply.started":"2022-05-21T04:47:45.950104Z","shell.execute_reply":"2022-05-21T04:47:46.072146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- **ps_car_03_cat, ps_car_05_cat**은 결측치의 비율이 굉장히 높습니다. 따라서 이 변수들을 제거\n- 다른 카테고리형 변수들은 -1을 그대로 둡니다.\n- **ps_reg_03** (continuous)는 18% 정도가 결측치입니다. 결측치를 평균값으로 바꿔줍니다.\n- **ps_car_11** (ordinal)는 5개만 결측치입니다. 결측치를 최빈값으로 바꿔줍니다.\n- **ps_car_14** (continuous)는 7% 정도가 결측치입니다. 결측치를 평균값으로 바꿔줍니다.","metadata":{}},{"cell_type":"code","source":"vars_to_drop = ['ps_car_03_cat', 'ps_car_05_cat']\ntrain.drop(vars_to_drop, inplace=True, axis=1)\nmeta.loc[(vars_to_drop), \"keep\"] = False # meta 업데이트\n\n# Imputing mean or mode\nmean_imp = SimpleImputer(missing_values=-1, strategy=\"mean\")\nmode_imp = SimpleImputer(missing_values=-1, strategy=\"most_frequent\")\ntrain[\"ps_reg_03\"] = mean_imp.fit_transform(train[[\"ps_reg_03\"]]).ravel() # 2D를 넣어줘야 한다\ntrain[\"ps_car_14\"] = mean_imp.fit_transform(train[[\"ps_car_14\"]]).ravel()\ntrain[\"ps_car_11\"] = mode_imp.fit_transform(train[[\"ps_car_11\"]]).ravel()","metadata":{"execution":{"iopub.status.busy":"2022-05-21T04:47:46.075196Z","iopub.execute_input":"2022-05-21T04:47:46.075444Z","iopub.status.idle":"2022-05-21T04:47:46.137985Z","shell.execute_reply.started":"2022-05-21T04:47:46.075404Z","shell.execute_reply":"2022-05-21T04:47:46.137287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Checking the cardinality of the categorical variables\n\n- Cardinality는 칼럼 안에서의 다른 값의 개수를 나타냅니다. \n\n- 나중에 카테고리형 변수에서 가변수를 만들 것이기 때문에, 많은 distinct value(칼럼에서의 unique 값)가 있는지 확인해야합니다. \n\n- 많은 가변수를 만들 것이기 때문에 이러한 칼럼들을 다르게 처리해야 합니다.","metadata":{}},{"cell_type":"code","source":"v = meta[(meta.level == \"nominal\") & (meta.keep)].index\n\nfor f in v:\n    dist_values = train[f].value_counts().shape[0]\n    print('Variable {} has {} distinct values'.format(f, dist_values))","metadata":{"execution":{"iopub.status.busy":"2022-05-21T04:47:46.139166Z","iopub.execute_input":"2022-05-21T04:47:46.13974Z","iopub.status.idle":"2022-05-21T04:47:46.164001Z","shell.execute_reply.started":"2022-05-21T04:47:46.1397Z","shell.execute_reply":"2022-05-21T04:47:46.163245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ps_car_11_cat이 유독 많은 distnict value를 갖고 있습니다.","metadata":{}},{"cell_type":"markdown","source":"### Target encoding with smoothing\n\n**Target encoding 설명:** https://choisk7.github.io/ml/encoding/ \\\n**Data Leakage:** training 데이터 밖의 데이터가 모델을 만드는데 사용되는 것\n\n<br/>\n<br/>\n\nmin_samples_leaf는 prior mean과 target mean(주어진 categoy values에 대한)이 동일한 가중치를 갖는 임계값을 정의합니다.\n\nvalue count에 대한 weight 동작은 smoothing parameter에 의해 제어됩니다.","metadata":{}},{"cell_type":"code","source":"def add_noise(series, noise_level):\n    return series * (1 + noise_level * np.random.randn(len(series))) # 랜덤으로 표준정규분포 생성\n\ndef target_encode(trn_series=None,\n                  tst_series=None,\n                  target=None,\n                  min_samples_leaf=1,\n                  smoothing=1,\n                  noise_level=0):\n    '''\n    trn_series: training categorical feature (pd.Series)\n    tst_series: test cateogrical feature (pd.Series)\n    target: target data (pd.Series)\n    min_samples_leaf (int): 카테고리형 변수 평균을 고려하기 위한 최소 샘플\n    smoothing (int): smoothing은 categorical average와 prior의 균형을 맞춰준다.\n    '''\n    \n    assert len(trn_series) == len(target)\n    assert trn_series.name == tst_series.name\n    temp = pd.concat([trn_series, target], axis=1)\n    \n    # target mean 계산\n    averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n    print(\"averages\", averages)\n    \n    # smoothing 계산 (sigmoid)\n    smoothing = 1 / (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) / smoothing))\n    \n    # prior: 발생 확률\n    prior = target.mean()\n    \n    # count가 클수록 전체 평균이 덜 고려된다.\n    # target 킬럼 추가\n    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n    print(\"averages\", averages)\n    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n    print(\"averages\", averages)\n    \n    # trn_series와 tst_series에 averages를 적용\n    print(\"trn_series.to_frame(trn_series.name)\", trn_series.to_frame(trn_series.name))\n    print(\"averages.reset_index().rename(columns={'index': target.name, target.name: 'average'})\", averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}))\n    ft_trn_series = pd.merge(\n        trn_series.to_frame(trn_series.name), # tsn_series에 칼럼이름 추가 (ps_car_11_cat)\n        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n        on=trn_series.name,\n        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n    print(ft_trn_series)\n    \n    # pd.merge는 인덱스를 유지하지 않으므로 저장\n    ft_trn_series.index = trn_series.index \n    ft_tst_series = pd.merge(\n        tst_series.to_frame(tst_series.name),\n        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n        on=tst_series.name,\n        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n\n    ft_tst_series.index = tst_series.index\n    \n    return add_noise(ft_trn_series, noise_level), add_noise(ft_tst_series, noise_level)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T04:47:46.165332Z","iopub.execute_input":"2022-05-21T04:47:46.165583Z","iopub.status.idle":"2022-05-21T04:47:46.181266Z","shell.execute_reply.started":"2022-05-21T04:47:46.165549Z","shell.execute_reply":"2022-05-21T04:47:46.18052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_encoded, test_encoded = target_encode(train[\"ps_car_11_cat\"], \n                             test[\"ps_car_11_cat\"], \n                             target=train.target, \n                             min_samples_leaf=100,\n                             smoothing=10,\n                             noise_level=0.01)\n\ntrain[\"ps_car_11_cat_te\"] = train_encoded\ntrain.drop(\"ps_car_11_cat\", axis=1, inplace=True)\nmeta.loc[\"ps_car_11_cat\", \"keep\"] = False # meta 업데이트\ntest[\"ps_car_11_cat_te\"] = test_encoded\ntest.drop(\"ps_car_11_cat\", axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T04:47:46.183899Z","iopub.execute_input":"2022-05-21T04:47:46.1841Z","iopub.status.idle":"2022-05-21T04:47:46.562029Z","shell.execute_reply.started":"2022-05-21T04:47:46.18407Z","shell.execute_reply":"2022-05-21T04:47:46.561278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Visualization\n## Categorical variables\ncategory형 변수들과 target=1인 고객들의 비율을 살펴봅시다.","metadata":{}},{"cell_type":"code","source":"v = meta[(meta.level == \"nominal\") & (meta.keep)].index\n# ['ps_ind_02_cat', 'ps_ind_04_cat', 'ps_ind_05_cat', ..., 'ps_car_10_cat']\n\nfor f in v:\n    plt.figure()\n    fig, ax = plt.subplots(figsize=(20, 10))\n    cat_perc = train[[f, \"target\"]].groupby([f], as_index=False).mean()\n    cat_perc.sort_values(by=\"target\", ascending=False, inplace=True)\n    \n    # Barplot\n    sns.barplot(ax=ax, x=f, y=\"target\", data=cat_perc, order=cat_perc[f])\n    plt.ylabel('% target', fontsize=18)\n    plt.xlabel(f, fontsize=18)\n    plt.tick_params(axis=\"both\", which=\"major\", labelsize=18)\n    plt.show()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-05-21T04:47:46.563204Z","iopub.execute_input":"2022-05-21T04:47:46.563651Z","iopub.status.idle":"2022-05-21T04:47:49.029786Z","shell.execute_reply.started":"2022-05-21T04:47:46.563613Z","shell.execute_reply":"2022-05-21T04:47:49.02908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"결측값이 있는 변수에서 알 수 있듯이 결측값을 별도의 범주 값으로 유지하는 것이 좋습니다. (최빈값으로 대체하는 것 대신에)\n\n결측치에 해당하는 고객들은 더 많은 보험 청구를 하는 경향이 있습니다.","metadata":{}},{"cell_type":"markdown","source":"## Interval variables","metadata":{}},{"cell_type":"markdown","source":"heatmap을 통해 상관관계를 알아봅시다.","metadata":{}},{"cell_type":"code","source":"def corr_heatmap(v):\n    correlations = train[v].corr()\n    \n    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n    \n    fig, ax = plt.subplots(figsize=(10, 10))\n    sns.heatmap(correlations, cmap=cmap, vmax=1.0, center=0, fmt=\".2f\",\n                square=True, linewidths=.5, annot=True, cbar_kws={\"shrink\": .75})\n    plt.show()\n    \nv = meta[(meta.level == \"interval\") & (meta.keep)].index\ncorr_heatmap(v)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T04:47:49.031139Z","iopub.execute_input":"2022-05-21T04:47:49.031399Z","iopub.status.idle":"2022-05-21T04:47:49.679178Z","shell.execute_reply.started":"2022-05-21T04:47:49.031365Z","shell.execute_reply":"2022-05-21T04:47:49.678489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"강한 상관관계를 갖는 변수들:\n- ps_reg_02 and ps_reg_03 (0.7)\n- ps_car_12 and ps_car13 (0.67)\n- ps_car_12 and ps_car14 (0.58)\n- ps_car_13 and ps_car15 (0.53)\n\n이제 높은 상관관계를 갖는 변수들을 살펴봅니다.\n<br/>\n\n**NOTE:** 속도를 높이기 위해 샘플을 사용합니다.","metadata":{}},{"cell_type":"code","source":"s = train.sample(frac=0.1)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T04:47:49.680247Z","iopub.execute_input":"2022-05-21T04:47:49.680596Z","iopub.status.idle":"2022-05-21T04:47:49.708529Z","shell.execute_reply.started":"2022-05-21T04:47:49.680564Z","shell.execute_reply":"2022-05-21T04:47:49.707893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ps_reg_02 and ps_reg_03\n\ntarget=0과 target=1의 regression line이 갖다는 것을 알 수 있습니다.","metadata":{}},{"cell_type":"code","source":"sns.lmplot(x=\"ps_reg_02\", y=\"ps_reg_03\", hue=\"target\", data=s, scatter_kws={'alpha':0.3})\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-21T04:47:49.709596Z","iopub.execute_input":"2022-05-21T04:47:49.709827Z","iopub.status.idle":"2022-05-21T04:47:51.735529Z","shell.execute_reply.started":"2022-05-21T04:47:49.709793Z","shell.execute_reply":"2022-05-21T04:47:51.734818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ps_car_12 and ps_car_13","metadata":{}},{"cell_type":"code","source":"sns.lmplot(x=\"ps_car_12\", y=\"ps_car_13\", data=s, hue=\"target\", scatter_kws={\"alpha\": 0.3})\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-21T04:47:51.736482Z","iopub.execute_input":"2022-05-21T04:47:51.736716Z","iopub.status.idle":"2022-05-21T04:47:53.348877Z","shell.execute_reply.started":"2022-05-21T04:47:51.736676Z","shell.execute_reply":"2022-05-21T04:47:53.348139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ps_car_12 and ps_car_14","metadata":{}},{"cell_type":"code","source":"sns.lmplot(x=\"ps_car_12\", y=\"ps_car_14\", data=s, hue=\"target\", scatter_kws={\"alpha\": 0.3})\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-21T04:47:53.350282Z","iopub.execute_input":"2022-05-21T04:47:53.351119Z","iopub.status.idle":"2022-05-21T04:47:55.132975Z","shell.execute_reply.started":"2022-05-21T04:47:53.351076Z","shell.execute_reply":"2022-05-21T04:47:55.132297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ps_car_13 and ps_car_15","metadata":{}},{"cell_type":"code","source":"sns.lmplot(x=\"ps_car_15\", y =\"ps_car_13\", data=s, hue=\"target\", scatter_kws={'alpha':0.3})\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-21T04:47:55.136753Z","iopub.execute_input":"2022-05-21T04:47:55.13735Z","iopub.status.idle":"2022-05-21T04:47:56.721711Z","shell.execute_reply.started":"2022-05-21T04:47:55.13732Z","shell.execute_reply":"2022-05-21T04:47:56.721063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"이제 이런 상관관계가 있는 변수들을 유지할 것인지 어떻게 결정할 수 있을까요? 우리는 차원을 줄이기 위해 PCA를 수행할 수 있습니다. 하지만 상관관계가 있는 변수들의 수가 적기때문에 그대로 사용합니다.","metadata":{}},{"cell_type":"markdown","source":"## Checking the correlations between ordinal variables","metadata":{}},{"cell_type":"code","source":"v = meta[(meta.level == \"ordinal\") & (meta.keep)].index\ncorr_heatmap(v)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T04:47:56.723075Z","iopub.execute_input":"2022-05-21T04:47:56.723544Z","iopub.status.idle":"2022-05-21T04:47:57.933696Z","shell.execute_reply.started":"2022-05-21T04:47:56.723507Z","shell.execute_reply":"2022-05-21T04:47:57.933062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ordinal 변수에서는 상관관계가 커보이지 않습니다.","metadata":{}},{"cell_type":"markdown","source":"# Feature engineering\n## Creating dummy variables\n카테고리형 값들의 변수들은 크기나 순서를 나타내지 않습니다. 예를 들어, category 2는 category 1의 2배가 아닙니다. 따라서, 이러한 변수들을 가변수화 해줍니다.","metadata":{}},{"cell_type":"code","source":"v = meta[(meta.level == \"nominal\") & (meta.keep)].index\nprint('Before dummification we have {} variables in train'.format(train.shape[1]))\ntrain = pd.get_dummies(train, columns=v, drop_first=True)\n# drop_first=True는 더미 변수간의 상관관계를 줄여주기 때문에 사용합니다.\n# n개의 범주에 대해 n-1개의 결과를 알면 n번째 범주의 결과를 쉽게 예측할 수 있다고 가정합니다\nprint('After dummification we have {} variables in train'.format(train.shape[1]))","metadata":{"execution":{"iopub.status.busy":"2022-05-21T04:47:57.935103Z","iopub.execute_input":"2022-05-21T04:47:57.935562Z","iopub.status.idle":"2022-05-21T04:47:58.108601Z","shell.execute_reply.started":"2022-05-21T04:47:57.935525Z","shell.execute_reply":"2022-05-21T04:47:58.107776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"training set에 52개의 더미 변수들을 추가했습니다.","metadata":{}},{"cell_type":"markdown","source":"## Creating interaction variables","metadata":{}},{"cell_type":"code","source":"v = meta[(meta.level == \"interval\") & (meta.keep)].index\n# PolynomialFeatures\n# 데이터가 직선의 형태가 아닌 비선형 형태라도, 선형모델을 사용하여 비선형 모델을 학습시킬 수 있다.\n# 각 feature의 거듭제곱을 새로운 특성으로 추가, 확장된 feature을 포함한 데이터 세트에 선형모델을 훈련시킨다.\n# 이를 polynomial regression이라고 한다.\npoly = PolynomialFeatures(degree=2, include_bias=False) # include_bias=False : 맨앞의 1칼럼 제거\n# transform from (x1, x2) to (1, x1, x2, x1^2, x1*x2, x2^2)\ninteractions = pd.DataFrame(data=poly.fit_transform(train[v]), columns=poly.get_feature_names(v))\ninteractions.drop(v, axis=1, inplace=True) # 원래의 칼럼 제거\nprint('Before creating interactions we have {} variables in train'.format(train.shape[1]))\ntrain = pd.concat([train, interactions], axis=1)\nprint('After creating interactions we have {} variables in train'.format(train.shape[1]))","metadata":{"execution":{"iopub.status.busy":"2022-05-21T04:47:58.10989Z","iopub.execute_input":"2022-05-21T04:47:58.110599Z","iopub.status.idle":"2022-05-21T04:47:58.456706Z","shell.execute_reply.started":"2022-05-21T04:47:58.110569Z","shell.execute_reply":"2022-05-21T04:47:58.455917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature selection\n## Removing features with low or zero variance\n\n원래 예측모형에서 중요한 특징데이터란 종속데이터와의 상관관계가 크고 예측에 도움이 되는 데이터를 말한다. 하지만 상관관계 계산에 앞서 특징데이터의 값 자체가 표본에 따라 그다지 변하지 않는다면 종속데이터 예측에도 도움이 되지 않을 가능성이 높다. 따라서 표본 변화에 따른 데이터 값의 변화 즉, 분산이 기준치보다 낮은 특징 데이터는 사용하지 않는 방법이 분산에 의한 선택 방법이다. 예를 들어 종속데이터와 특징데이터가 모두 0 또는 1 두가지 값만 가지는데 종속데이터는 0과 1이 균형을 이루는데 반해 특징데이터가 대부분(예를 들어 90%)의 값이 0이라면 이 특징데이터는 분류에 도움이 되지 않을 가능성이 높다.\n\n하지만 분산에 의한 선택은 반드시 상관관계와 일치한다는 보장이 없기 때문에 신중하게 사용해야 한다.","metadata":{}},{"cell_type":"code","source":"selector = VarianceThreshold(threshold=.01)\nselector.fit(train.drop([\"id\", \"target\"], axis=1))\nf = np.vectorize(lambda x: not x) # map과 비슷\nv = train.drop([\"id\", \"target\"], axis=1).columns[f(selector.get_support())]\nprint('{} variables have too low variance.'.format(len(v)))\nprint('These variables are {}'.format(list(v)))","metadata":{"execution":{"iopub.status.busy":"2022-05-21T04:47:58.457891Z","iopub.execute_input":"2022-05-21T04:47:58.459406Z","iopub.status.idle":"2022-05-21T04:47:59.065769Z","shell.execute_reply.started":"2022-05-21T04:47:58.459364Z","shell.execute_reply":"2022-05-21T04:47:59.064777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Selecting features with a Random Forest and SelectFromModel","metadata":{}},{"cell_type":"code","source":"X_train = train.drop([\"id\", \"target\"], axis=1)\ny_train = train[\"target\"]\n\nfeat_labels = X_train.columns\n\nrf = RandomForestClassifier(n_estimators=1000, random_state=0, n_jobs=-1)\n\nrf.fit(X_train, y_train)\nimportances = rf.feature_importances_\nindices = np.argsort(rf.feature_importances_)[::-1]\n\n# \"[%-*s]\" % 30, feat_labels[indices[f]] => feat_labels[indices[f]] 뒤에 30칸 빈칸\nfor f in range(X_train.shape[1]):\n    print(\"%2d) %-*s %f\" % (f + 1, 30, feat_labels[indices[f]], importances[indices[f]]))","metadata":{"execution":{"iopub.status.busy":"2022-05-21T04:47:59.067425Z","iopub.execute_input":"2022-05-21T04:47:59.067705Z","iopub.status.idle":"2022-05-21T05:05:12.949489Z","shell.execute_reply.started":"2022-05-21T04:47:59.067667Z","shell.execute_reply":"2022-05-21T05:05:12.948778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"SelectFromModel을 통해 사용할 사전 적합 분류기와 feature importance에 대한 임계치를 정할 수 있습니다.\n\nget_support 메소드로는 training data에서의 변수 개수를 제한할 수 있습니다.","metadata":{}},{"cell_type":"code","source":"sfm = SelectFromModel(rf, threshold=\"median\", prefit=True) # feature 선택\nprint('Number of features before selection: {}'.format(X_train.shape[1]))\nn_features = sfm.transform(X_train).shape[1] # prefit=True인 경우, transform을 사용해야한다. false인 경우는 fit\nprint('Number of features after selection: {}'.format(n_features))\nselected_vars = list(feat_labels[sfm.get_support()])","metadata":{"execution":{"iopub.status.busy":"2022-05-21T05:05:12.95092Z","iopub.execute_input":"2022-05-21T05:05:12.951179Z","iopub.status.idle":"2022-05-21T05:05:15.116821Z","shell.execute_reply.started":"2022-05-21T05:05:12.951145Z","shell.execute_reply":"2022-05-21T05:05:15.116012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train[selected_vars + ['target']]","metadata":{"execution":{"iopub.status.busy":"2022-05-21T05:05:15.118193Z","iopub.execute_input":"2022-05-21T05:05:15.118461Z","iopub.status.idle":"2022-05-21T05:05:15.168599Z","shell.execute_reply.started":"2022-05-21T05:05:15.118427Z","shell.execute_reply":"2022-05-21T05:05:15.167789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature scaling\n\nscailing을 해주면 모델의 성능이 더 좋아진다.","metadata":{}},{"cell_type":"code","source":"scaler = StandardScaler()\nscaler.fit_transform(train.drop([\"target\"], axis=1))","metadata":{"execution":{"iopub.status.busy":"2022-05-21T05:05:15.170154Z","iopub.execute_input":"2022-05-21T05:05:15.170464Z","iopub.status.idle":"2022-05-21T05:05:15.524789Z","shell.execute_reply.started":"2022-05-21T05:05:15.170428Z","shell.execute_reply":"2022-05-21T05:05:15.524077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# End","metadata":{}}]}