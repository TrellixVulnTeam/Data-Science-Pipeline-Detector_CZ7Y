{"cells":[{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"ff9b277855bd8d8e4a1fbf2bbf2bcf9976fa0244","collapsed":true,"_cell_guid":"a9e0b8ba-6dff-4ef4-ac79-4ad913e77f3f"},"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n# Step 1: Data Preparation\n# Loading the required python package for analysis\nimport numpy as np  # linear algebra\nimport pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport plotly.offline as py\n\npy.init_notebook_mode(connected=True)\nfrom plotly.graph_objs import Scatter, Figure, Layout\n\nimport plotly.tools as tls\nimport warnings\n\nimport seaborn as sns\n\nplt.style.use('fivethirtyeight')\nsns.set_style(\"whitegrid\")\n\nfrom collections import Counter\n\nwarnings.filterwarnings('ignore')\n\nimport plotly.graph_objs as go\nimport plotly.plotly as plpl\n\n# Step 2: Data Overview: file structure & content\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\ntrain.head(10)\nprint(train.head(10))\npd.set_option('precision', 3)\ntrain.describe()\nprint(train.describe())\nid_test = test['id'].values\ntarget_train = train['target'].values\n\n\n# Step 3: Data Validation Checks\n# To check if there is any null information in the dataset\ntrain.isnull().any().any()\nprint(train.isnull().any().any())\n# We check if there's any NaN in the dataset\ntrain_cp = train\n# train_cp = train_cp.replace(-1, np.NaN)\n(train_cp == -1).sum()\n\ndata = train\ncol_with_nan = train_cp.columns[train_cp.isnull().any()].tolist()\nprint(\"this dataset has %s Rows. \\n\" % (train_cp.shape[0]))\n\nvars_with_missing = []\n\nfor f in train.columns:\n    missings = train[train[f] == -1][f].count()\n    if missings > 0:\n        vars_with_missing.append(f)\n        missings_perc = missings / train.shape[0]\n\nprint('Variable {} has {} records ({:.2%}) with missing values'.format(f, missings, missings_perc))\nprint('In total, there are {} variables with missing values'.format(len(vars_with_missing)))\n\nf, ax = plt.subplots(1, 2, figsize=(20, 15))\ntrain['target'].value_counts().plot.pie(explode=[0, 0.1], autopct='%1.1f%%', ax=ax[0], shadow=True)\nax[0].set_title('target')\nax[0].set_ylabel('')\nsns.countplot('target', data=train, ax=ax[1])\nax[1].set_title('target')\nplt.show()\n\n# Also, we can prepare a lists of numeric, categorical and binary columns\n# All features\nall_features = train.columns.tolist()\nall_features.remove('target')\n# Numerical features\nnumeric_features = [x for x in all_features if x[-3:] not in ['bin', 'cat']]\n# Categorical features\ncategorical_features = [x for x in all_features if x[-3:]=='cat']\n# Binary Features\nbinary_features = [x for x in all_features if x[-3:]=='bin']\ntrain['target_name'] = train['target'].map({0: 'Not Filed', 1: 'Filed'})\n\n# Very big imbalance in the dataset as we can see from the plot\ntrain_float = train.select_dtypes(include=['float64'])\ntrain_int = train.select_dtypes(include=['int64'])\nCounter(train.dtypes.values)\nprint(Counter(train.dtypes.values))\n\n# Step 4: Feature Inspection\n# We would be using correlation plots to inspect the data\n# Getting correlation matrix\ncor_matrix = train[numeric_features].corr().round(2)\n# Plotting heatmap\nfig = plt.figure(figsize=(18,18));\nsns.heatmap(cor_matrix, annot=True, center=0, cmap=sns.diverging_palette(250, 10, as_cmap=True), ax=plt.subplot(111));\nplt.show()\n\n# Exploring the numerical features in the dataset\n# Looping through and plotting the numerical features\nfor column in numeric_features:\n    fig = plt.figure(figsize=(20,12))\n\n    # Plotting the Distribution\n    sns.distplot(train[column], ax=plt.subplot(221));\n    # Label for X-axis\n    plt.xlabel(column, fontsize=16);\n    # Label for Y-axis\n    plt.ylabel('Density', fontsize=16);\n    # Adding a title (One for the figure)\n    plt.suptitle('Plots for '+column, fontsize=20);\n\n    # The distribution per claim value\n    # When the claim is not filed\n    sns.distplot(train.loc[train.target==0, column], color='red', label='Claim not filed', ax=plt.subplot(222));\n    # When the claim is filed\n    sns.distplot(train.loc[train.target==1, column], color='green', label='Claim filed', ax=plt.subplot(222));\n    # Legend\n    plt.legend(loc='best')\n    # Labelling the X-axis\n    plt.xlabel(column, fontsize=16);\n    # Labelling the Y-axis\n    plt.ylabel('Density per Claim Value', fontsize=16);\n\n    # Preparing a boxplot of column per claim value\n    sns.boxplot(x=\"target_name\", y=column, data=train, ax=plt.subplot(224));\n    # Labelling the X-axis\n    plt.xlabel('Is Filed Claim?', fontsize=16);\n    # Labelling the Y-axis\n    plt.ylabel(column, fontsize=16);\n    plt.show()\n\n# Exploring the categorical features\n# Looping through and Plotting Categorical features\nfor column in categorical_features:\n    # Figure initiation\n    fig = plt.figure(figsize=(18, 12))\n\n    # Number of occurrences per category\n    ax = sns.countplot(x=column, hue=\"target_name\", data=train, ax=plt.subplot(211));\n    # Labelling the X-axis\n    plt.xlabel(column, fontsize=16);\n    # Labelling the Y-axis\n    plt.ylabel('Number of occurrences', fontsize=16)\n    # Adding Title\n    plt.suptitle('Plots for ' + column, fontsize=16);\n\n    # Adding the percents over each bar\n    # Getting heights of the bars\n    height = [p.get_height() for p in ax.patches]\n    # Counting number of bar groups\n    ncol = int(len(height) / 2)\n    # Counting total height of groups\n    total = [height[i] + height[i + ncol] for i in range(ncol)] * 2\n    # Looping through bars\n    for i, p in enumerate(ax.patches):\n        # Adding percentages\n        ax.text(p.get_x() + p.get_width() / 2, height[i] * 1.01 + 1000,\n                '{:1.0%}'.format(height[i] / total[i]), ha=\"center\", size=14)\n\n    # Filed Claims percentage for every value of feature in teh dataset\n    sns.pointplot(x=column, y='target', data=train, ax=plt.subplot(212));\n    # Labelling the X-axis\n    plt.xlabel(column, fontsize=16);\n    # Labelling the Y-axis\n    plt.ylabel('Filed Claims Percentage', fontsize=16);\n    plt.show()\n\n# Exploring the Binary Features in the dataset\n# looping through and plotting binary features\nfor column in binary_features:\n    fig = plt.figure(figsize=(18, 12))\n\n    # Finding the number of occurrences per binary value\n    ax = sns.countplot(x=column, hue=\"target_name\", data=train, ax=plt.subplot(211));\n    # Labelling the X-axis\n    plt.xlabel(column, fontsize=16);\n    # Labelling the Y-axis\n    plt.ylabel('Number of occurrences', fontsize=16)\n    # Adding title\n    plt.suptitle('Plots for ' + column, fontsize=16);\n\n    # Adding percents over bars\n    # Getting heights of our bars\n    height = [p.get_height() for p in ax.patches]\n    # Counting number of bar groups\n    ncol = int(len(height) / 2)\n    # Counting total height of groups\n    total = [height[i] + height[i + ncol] for i in range(ncol)] * 2\n    # Looping through bars\n    for i, p in enumerate(ax.patches):\n        # Adding percentages\n        ax.text(p.get_x() + p.get_width() / 2, height[i] * 1.01 + 1000,\n                '{:1.0%}'.format(height[i] / total[i]), ha=\"center\", size=14)\n\n    # Filed Claims percentage for every value of feature\n    sns.pointplot(x=column, y='target', data=train, ax=plt.subplot(212));\n    # Labelling the X-axis\n    plt.xlabel(column, fontsize=16);\n    # Labelling the Y-axis\n    plt.ylabel('Filed Claims Percentage', fontsize=16);\n    plt.show()\n\n\n# Step 5: Feature Importance\n# In this step, we would be creating/trying out different baseline of performance on the problem and check it using some algorithms\n# For now, I'll use the following algorithms\n# Linear Algorithms\n# a. Logistic Regression\n# b. Linear Discriminant Analysis\n# Nonlinear Algorithms\n# a. Gaussian Naive Bayes\n# b. Classification & Regression Trees (CART)\n\n'''\n# Getting the required python packages\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\n\n# Preparing the test options and evaluation metrics\nnum_folds = 10\nseed = 8\nscoring = 'Accuracy'\n\nX = np.asarray(train.drop(['id', 'target'], axis=1))\nY = np.asarray(train['target'])\n\nvalidation_size = 0.4\nX_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=validation_size, random_state=seed)\n\n# We generate some results with the linear and non-linear algorithms\nmodels = [('LR', LogisticRegression()),\n          ('LDA', LinearDiscriminantAnalysis()),\n          ('CART', DecisionTreeClassifier()),\n          ('NB', GaussianNB())]\nresults = []\nnames = []\n\nfor name, model in models:\n    print(\"Training model %s\" % (name))\n    model.fit(X_train, Y_train)\n    result = model.score(X_validation, Y_validation)\n    kfold = KFold(n_splits=num_folds, random_state=seed)\n    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"Classifier score %s: %f\" % (name, result)\n    print(msg)\nprint(\"----- Training Completed!! -----\")\n'''\n\n# Trying out a different Machine Learning Algorithm learned in class\n# to predict the outcome for the drivers\n# Multilayer Perceptron using Keras is Implemented\nimport keras\nimport sklearn.model_selection\nimport pandas as pd\n\n# Load Datasets\ndf_train  = pd.read_csv('../input/train.csv')\ndf_test   = pd.read_csv('../input/test.csv')\ndf_submit = pd.read_csv('../input/sample_submission.csv')\n\n# To numpy array - dataset of train\nx_all = df_train.drop(['target', 'id'], axis=1).values\ny_all = keras.utils.np_utils.to_categorical(df_train['target'].values)\n\n# Catering for imbalanced data\ny_all_0 = y_all[y_all[:,1]==0]\ny_all_1 = y_all[y_all[:,1]==1]\nx_all   = np.concatenate([x_all[y_all[:,1]==0], np.repeat(x_all[y_all[:,1]==1], repeats=int(len(y_all_0)/len(y_all_1)), axis=0)], axis=0)\ny_all   = np.concatenate([y_all[y_all[:,1]==0], np.repeat(y_all[y_all[:,1]==1], repeats=int(len(y_all_0)/len(y_all_1)), axis=0)], axis=0)\n\n# Split train/valid datasets\nx_train, x_valid, y_train, y_valid = sklearn.model_selection.train_test_split(x_all, y_all, test_size=0.4, random_state=0)\n\n# Defining the model\nmodel = keras.models.Sequential()\nmodel.add(keras.layers.normalization.BatchNormalization(input_shape=tuple([x_train.shape[1]])))\nmodel.add(keras.layers.core.Dense(32, activation='relu'))\nmodel.add(keras.layers.core.Dropout(rate=0.5))\nmodel.add(keras.layers.normalization.BatchNormalization())\nmodel.add(keras.layers.core.Dense(32, activation='relu'))\nmodel.add(keras.layers.core.Dropout(rate=0.5))\nmodel.add(keras.layers.normalization.BatchNormalization())\nmodel.add(keras.layers.core.Dense(32, activation='relu'))\nmodel.add(keras.layers.core.Dropout(rate=0.5))\nmodel.add(keras.layers.core.Dense(2,   activation='sigmoid'))\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adadelta\",metrics=[\"accuracy\"])\nprint(model.summary())\n\n# Use Early-Stopping\ncallback_early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='auto')\n\n# Training the model\nmodel.fit(x_train, y_train, batch_size=1024, epochs=200, validation_data=(x_valid, y_valid), verbose=1, callbacks=[callback_early_stopping])\n\n# Predict test dataset\nx_test = df_test.drop(['id'], axis=1).values\ny_test = model.predict(x_test)\n\n# Output\ndf_submit['target'] = y_test[:, 1]\ndf_submit.to_csv('Output_Submission.csv', index=False)","outputs":[]}],"nbformat_minor":1,"nbformat":4,"metadata":{"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"},"language_info":{"file_extension":".py","nbconvert_exporter":"python","mimetype":"text/x-python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"name":"python","version":"3.6.3"}}}