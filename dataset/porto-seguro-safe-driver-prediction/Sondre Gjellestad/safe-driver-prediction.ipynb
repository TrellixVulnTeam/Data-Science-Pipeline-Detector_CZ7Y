{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Safe Driver Prediction"},{"metadata":{"toc":true},"cell_type":"markdown","source":"## Table of Contents\n\n1. [Loading the Data](#loading-the-data)\n    * [Setup](#setup)\n    * [Data](#data)\n2. [Understanding the Data](#understanding-the-data)\n    * [Conclusions](#conclusions)\n3. [Data Preparation](#data-preparation)\n    * [Cleaning up bad values](#cleaning-up-bad-values)\n    * [Separating values and labels](#separating-values-and-labels)\n    * [Splitting up the dataset](#splitting-up-the-dataset)\n4. [Machine Learning](#data-preparation-and-machine-learning)\n    * [Setting up the model](#setting-up-the-model)\n    * [Training the model](#training-the-model)\n    * [Testing the model](#testing-the-model)\n4. [Making a Benchmark Submission](#making-a-benchmark-submission)"},{"metadata":{},"cell_type":"markdown","source":"## Loading the Data <a class=\"anchor\" id=\"loading-the-data\"></a>\n\nBefore we do anything, we need to make sure we have all our data ready."},{"metadata":{},"cell_type":"markdown","source":"Importing Numpy now so that it is ready for later."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\n# Set the random seed for reproducability\nnp.random.seed(42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will use Pandas throughout the notebook to hold and manage our datasets."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then you should uncomment the code and run the following two cells. **Warning:** This doesn't work in this Kaggle hosted notebook! See below"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reads in the csv-files and creates a dataframe using pandas\n\n# base_set = pd.read_csv('data/housing_data.csv')\n# benchmark = pd.read_csv('data/housing_test_data.csv')\n# sampleSubmission = pd.read_csv('data/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_set = pd.read_csv('../input/porto-seguro-safe-driver-prediction/train.csv')\nbenchmark = pd.read_csv('../input/porto-seguro-safe-driver-prediction/test.csv')\nsample_submission = pd.read_csv('../input/porto-seguro-safe-driver-prediction/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Understanding the data\n\nNow that we have our data, we need to investigate it so that we are able to leverage it to the fullest extent."},{"metadata":{},"cell_type":"markdown","source":"We will use Matplotlib to plot various things throughout the notebook."},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"base_set.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"benchmark.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_set.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"benchmark.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_set.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the correlations between the values, we can see that the median income has the strongest correlation to the target value."},{"metadata":{"trusted":true},"cell_type":"code","source":"correlations = base_set.corr()\ncorrelations[\"target\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_set.hist(bins=50, figsize=(15,15))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Conclusions\n\n..."},{"metadata":{},"cell_type":"markdown","source":"## Data preparation\n\nIn this section we will preprocess the data and construct a model, which we will then train so we are able to make predictions with it. Lastly we will test in on a test set we create."},{"metadata":{},"cell_type":"markdown","source":"### Cleaning up bad values"},{"metadata":{},"cell_type":"markdown","source":"The `id` column in the sets is not needed, so we remove that."},{"metadata":{"trusted":true},"cell_type":"code","source":"base_set_id = base_set['id']\nbenchmark_id = benchmark['id']\n\nbase_set = base_set.drop(columns=['id'])\nbenchmark = benchmark.drop(columns=['id'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Features ending with _calc appear to be randomly generated noise and should be dropped."},{"metadata":{"trusted":true},"cell_type":"code","source":"base_non_calc_cols = [c for c in base_set.columns if (not c.startswith('ps_calc_'))]\nbenchmark_non_calc_cols = [c for c in benchmark.columns if (not c.startswith('ps_calc_'))]\n\nbase_set = base_set[base_non_calc_cols]\nbenchmark = benchmark[benchmark_non_calc_cols]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some of the columns are categorical and should be one-hot encoded."},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.utils import to_categorical\n\n# Not sure how to do this yet","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Values that have not been recorded are designated -1 in the dataset, we fill those with the median of the column."},{"metadata":{"trusted":true},"cell_type":"code","source":"base_set = base_set.replace(-1, np.NaN)\nbenchmark = benchmark.replace(-1, np.NaN)\n\nbase_set = base_set.fillna(base_set.median())\nbenchmark = benchmark.fillna(benchmark.median())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, we check to see that neither of the datasets contain `NaN` values."},{"metadata":{"trusted":true},"cell_type":"code","source":"base_set.isnull().any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"benchmark.isnull().any()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Separating values and labels\n\nIt is time to split the dataset into values and labels. To do that, we drop the label column and call that `X`, and take the label column alone and call that `Y`. Afterwards we are ready to start shaping our dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"labels_column = 'target'\n\nX = base_set.drop(columns=[labels_column])\nY = pd.DataFrame(base_set[labels_column], columns=[labels_column])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"benchmark.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Splitting up the dataset\n\nWe split our base set into separate datasets for training, testing and validation."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_to_valtest_ratio = .5\nvalidate_to_test_ratio = .5\n\n# First split our main set\n(X_train,\n X_validation_and_test,\n Y_train,\n Y_validation_and_test) = train_test_split(X, Y, test_size=train_to_valtest_ratio)\n\n# Then split our second set into validation and test\n(X_validation,\n X_test,\n Y_validation,\n Y_test) = train_test_split(X_validation_and_test, Y_validation_and_test, test_size=validate_to_test_ratio)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"### Machine Learning"},{"metadata":{},"cell_type":"markdown","source":"### Gini scoring function\n\nGini will be used to score the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"def gini(y_true, y_pred):\n    # check and get number of samples\n    assert y_true.shape == y_pred.shape\n    n_samples = y_true.shape[0]\n    \n    # sort rows on prediction column \n    # (from largest to smallest)\n    arr = np.array([y_true, y_pred]).transpose()\n    true_order = arr[arr[:,0].argsort()][::-1,0]\n    pred_order = arr[arr[:,1].argsort()][::-1,0]\n    \n    # get Lorenz curves\n    L_true = np.cumsum(true_order) / np.sum(true_order)\n    L_pred = np.cumsum(pred_order) / np.sum(pred_order)\n    L_ones = np.linspace(1/n_samples, 1, n_samples)\n    \n    # get Gini coefficients (area between curves)\n    G_true = np.sum(L_ones - L_true)\n    G_pred = np.sum(L_ones - L_pred)\n    \n    # normalize to true Gini coefficient\n    return G_pred/G_true","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Set up the model\n\nNow, it is time to set up the architecture."},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout\n\nmodel = Sequential([\n    Dense(64, activation='relu', input_dim=X_train.shape[1]),\n    Dropout(.30),\n    Dense(64, activation='relu'),\n    Dropout(.15),\n    Dense(32, activation='relu'),\n    Dropout(.15),\n    Dense(16, activation='relu'),\n    Dense(1),\n])\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras.backend as K\n\nmodel.compile(optimizer='adam', # adam, sgd, adadelta\n              loss='binary_crossentropy',\n              metrics=['binary_crossentropy'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training the model\n\nLet's fit the model on the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.callbacks import EarlyStopping\n\nearly_stopper = EarlyStopping(patience=3)\n\ntraining_result = model.fit(X_train, Y_train,\n                            batch_size=4096,\n                            epochs=256,\n                            validation_data=(X_validation, Y_validation),\n                            callbacks=[early_stopper])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let's look into how the fitting went."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(training_result.history)\n\n# Plot model accuracy over epoch\nplt.plot(training_result.history['binary_crossentropy'])\nplt.plot(training_result.history['val_binary_crossentropy'])\nplt.title('Model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n\n# Plot model loss over epoch\nplt.plot(training_result.history['loss'])\nplt.plot(training_result.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"validate_result = model.test_on_batch(X_validation, Y_validation)\nvalidate_result","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Testing the model\n\nFinally, we churn the test set through the model we created."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_result = model.test_on_batch(X_test, Y_test)\ntest_result","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Trying other models"},{"metadata":{},"cell_type":"markdown","source":"Testing with RandomForestRegressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nrfr_model = RandomForestRegressor()\nrfr_model.fit(X_train, Y_train)\n\nrfr_predictions = rfr_model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfr_error =  gini(Y_test['target'], rfr_predictions)\nrfr_error","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Testing with XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\nregex = re.compile(r\"[|]|<\", re.IGNORECASE)\n\n# XGBoost does not support some of the column names\n\nX_train.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in X_train.columns.values]\nX_test.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in X_test.columns.values]\n\nfrom xgboost.sklearn import XGBRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV\n\nimport scipy.stats as st\n\none_to_left = st.beta(10, 1)  \nfrom_zero_positive = st.expon(0, 50)\n\nxgb_reg = XGBRegressor(nthreads=-1)\n\nxgb_gs_params = {  \n    \"n_estimators\": st.randint(3, 40),\n    \"max_depth\": st.randint(3, 40),\n    \"learning_rate\": st.uniform(0.05, 0.4),\n    \"colsample_bytree\": one_to_left,\n    \"subsample\": one_to_left,\n    \"gamma\": st.uniform(0, 10),\n    'reg_alpha': from_zero_positive,\n    \"min_child_weight\": from_zero_positive,\n}\n\nxgb_gs = RandomizedSearchCV(xgb_reg, xgb_gs_params, n_jobs=1)  \nxgb_gs.fit(X_train.values, Y_train)  \n\nxgb_model = xgb_gs.best_estimator_ \n\nxgb_predictions = xgb_model.predict(X_test.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_error =  gini(Y_test['target'], xgb_predictions)\nxgb_error","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A comparison of all the models"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'NN:                                 {test_result[0]}')\nprint(f'RandomForestRegressor Gini:         {rfr_error}')\nprint(f'XGBRegressor Gini:                  {xgb_error}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Making a Benchmark Submission\n\nFor the benchmark data, it is important that we put it through the same preparation steps as the training set."},{"metadata":{"trusted":true},"cell_type":"code","source":"benchmark.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now it's time to make predictions."},{"metadata":{"trusted":true},"cell_type":"code","source":"target = xgb_model.predict(benchmark.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"target","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({\n    'id': benchmark_id,\n    'target': target.flatten()\n})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Stores a csv file to submit to the kaggle competition\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":true,"toc_position":{},"toc_section_display":true,"toc_window_display":false}},"nbformat":4,"nbformat_minor":1}