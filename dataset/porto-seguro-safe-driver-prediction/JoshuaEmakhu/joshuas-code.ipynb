{"nbformat_minor":1,"cells":[{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"ad51e33f288d508a297949ffa125f11b0576284f","_cell_guid":"85423104-78f3-4b37-86ee-02c548dba6ec"},"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.offline as py\n\nfrom collections import Counter\nfrom sklearn import metrics\nfrom sklearn import preprocessing\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import linear_model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_val_score\nfrom lightgbm import LGBMClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n# Read the train data set\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\nprint(train.head())\n\n# Check for null value\ntrain.isnull().any().any()\nprint(train.isnull().any().any())\n\nCounter(train.dtypes.values)\nprint(Counter(train.dtypes.values))\n\n# Check for missing vales by its features\ntrain2= (train.isnull().sum() / len(train)) * 100\nmisval = train2.drop(train2[train2 == 0].index).sort_values(ascending=False)[:30]\nmissing = pd.DataFrame({'Missing %' :misval})\nmissing.head(10)\nprint(missing.head(10))\n\n# Phishing out potentential values probably having -1\ntrain_copy = train.replace(-1, np.NaN)\ntrain_copy= (train_copy.isnull().sum() / len(train_copy)) * 100\ntrain_copy = train_copy.drop(train_copy[train_copy == 0].index).sort_values(ascending=False)[:30]\nmissing = pd.DataFrame({'Missing %' :train_copy})\nmissing.head(10)\nprint(missing.head(10))\n\n# group to either intger or floating data type\ntrain_float = train.select_dtypes(include=['float64'])\ntrain_int = train.select_dtypes(include=['int64'])\nprint(train_float)\nprint(train_int)\n\n# group by feature types\nbin_col = [col for col in train.columns if '_bin' in col] #binary\ncat_col = [col for col in train.columns if '_cat' in col] #categorical\n# group by numerical features\nnum_col = [x for x in train.columns if x[-3:] not in ['bin', 'cat']]\n# group by individual, car, region and calculated fields\nind_col = [col for col in train.columns if '_ind_' in col] #individual\ncar_col = [col for col in train.columns if '_car_' in col] #car\nreg_col = [col for col in train.columns if '_reg_' in col] #region\ncalc_col = [col for col in train.columns if '_calc_' in col] #calculation\n\nzero_list = []\none_list = []\nfor col in bin_col:\n    zero_list.append((train[col] == 0).sum())\n    one_list.append((train[col] == 1).sum())\nprint(zero_list)\nprint(one_list)\n\n# Corralation matrix\ncor_matrix = train[num_col].corr().round(2)\nprint(cor_matrix)\n\n# Correlation of float values\ncolormap = plt.cm.magma\nplt.figure(figsize=(16,12))\nplt.title('Correlation of float features', y=1.05, size=15)\nsns.heatmap(train_float.corr(),linewidths=0.1,vmax=1.0, square=True,\n            cmap=colormap, linecolor='white', annot=True)\nplt.show()\n\ntot_cat_col = list(train.select_dtypes(include=['category']).columns)\n\nother_cat_col = [c for c in tot_cat_col if c not in cat_col+ bin_col]\nother_cat_col\n\n# Using PCA\nX = train.drop(['id', 'target'], axis=1).values\ny = train['target'].values.astype(np.int8)\n\n# Standardize feature\nX_scaled = preprocessing.scale(X)\nprint(X)\n\ntarget_names = np.unique(y)\nprint('\\nThere are %d unique target valuess in this dataset:' % (len(target_names)), target_names)\nn_comp = 10\n\n# PCA\nprint('\\nRunning PCA ...')\npca = PCA(n_components=n_comp, svd_solver='full', random_state=1001)\nX_pca = pca.fit_transform(X)\nprint('Explained variance: %.4f' % pca.explained_variance_ratio_.sum())\n\nprint('Individual variance contributions:')\nfor j in range(n_comp):\n    print(pca.explained_variance_ratio_[j])\n\n# Plotting the scatter plot of the training data on the 1st and 2nd PC\ncolors = ['orange', 'blue']\nplt.figure(1, figsize=(11, 11))\n\nfor color, i, target_name in zip(colors, [0, 1], target_names):\n    plt.scatter(X_pca[y == i, 0], X_pca[y == i, 1], color=color, s=1,\n                alpha=.7, label=target_name, marker='.')\nplt.legend(loc='best', shadow=False, scatterpoints=3)\nplt.title(\n        \"Training data projected on the 1st \"\n        \"and 2nd principal components\")\nplt.xlabel(\"Principal axis 1 - Explains %.1f %% of the variance\" % (\n        pca.explained_variance_ratio_[0] * 100.0))\nplt.ylabel(\"Principal axis 2 - Explains %.1f %% of the variance\" % (\n        pca.explained_variance_ratio_[1] * 100.0))\nplt.show()\n\n# Validation of the train data\nnum_folds = 8\nseed = 8\nscoring = 'Accuracy'\n\nX = X_pca\nY = np.array(train['target'])\n\nvalidation_size = 0.25\nX_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=\n                                                                validation_size, random_state=seed)\n# generate results with linear algorithms\nmodels = [('LR', LogisticRegression()),\n          ('NB', GaussianNB())]\nresults =[]\nnames = []\nfor name, model in models:\n    print(\"Training model %s\" % (name))\n    model.fit(X_train, Y_train)\n    result = model.score(X_validation, Y_validation)\n    info = \"Classifier score %s: %f\" %(name, result)\n    print(info)\nprint(\"Done\")\n\n# Trying a boosting method using the python package Light Gradient Boosting Method(LGBM)\nid_test = test['id'].values\ntarget_train = train['target'].values\n\ntrain = train.drop(['target','id'], axis = 1)\ntest = test.drop(['id'], axis = 1)\n\ncol_to_drop = train.columns[train.columns.str.startswith('ps_calc_')]\ntrain = train.drop(col_to_drop, axis=1)\ntest = test.drop(col_to_drop, axis=1)\n\ntrain = train.replace(-1, np.nan)\ntest = test.replace(-1, np.nan)\n\ncat_features = [a for a in train.columns if a.endswith('cat')]\n\nfor column in cat_features:\n    temp = pd.get_dummies(pd.Series(train[column]))\n    train = pd.concat([train, temp], axis=1)\n    train = train.drop([column], axis=1)\n\nfor column in cat_features:\n    temp = pd.get_dummies(pd.Series(test[column]))\n    test = pd.concat([test, temp], axis=1)\n    test = test.drop([column], axis=1)\n\nprint(train.values.shape, test.values.shape)\n\nclass Ensemble(object):\n    def __init__(self, n_splits, stacker, base_models):\n        self.n_splits = n_splits\n        self.stacker = stacker\n        self.base_models = base_models\n\n    def fit_predict(self, X, y, T):\n        X = np.array(X)\n        y = np.array(y)\n        T = np.array(T)\n\n        folds = list(StratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state=2016).split(X, y))\n\n        S_train = np.zeros((X.shape[0], len(self.base_models)))\n        S_test = np.zeros((T.shape[0], len(self.base_models)))\n        for i, clf in enumerate(self.base_models):\n\n            S_test_i = np.zeros((T.shape[0], self.n_splits))\n\n            for j, (train_idx, test_idx) in enumerate(folds):\n                X_train = X[train_idx]\n                y_train = y[train_idx]\n                X_holdout = X[test_idx]\n\n                print (\"Fit %s fold %d\" % (str(clf).split('(')[0], j+1))\n                clf.fit(X_train, y_train)\n                y_pred = clf.predict_proba(X_holdout)[:,1]\n\n                S_train[test_idx, i] = y_pred\n                S_test_i[:, j] = clf.predict_proba(T)[:,1]\n            S_test[:, i] = S_test_i.mean(axis=1)\n\n        results = cross_val_score(self.stacker, S_train, y, cv=3, scoring='roc_auc')\n        print(\"Stacker score: %.5f\" % (results.mean()))\n        self.stacker.fit(S_train, y)\n        res = self.stacker.predict_proba(S_test)[:, 1]\n        return res\n\n# LightGBM params\nlgb_params = {}\nlgb_params['learning_rate'] = 0.02\nlgb_params['n_estimators'] = 650\nlgb_params['max_bin'] = 10\nlgb_params['subsample'] = 0.8\nlgb_params['subsample_freq'] = 10\nlgb_params['colsample_bytree'] = 0.8\nlgb_params['min_child_samples'] = 500\nlgb_params['seed'] = 99\n\nlgb_params2 = {}\nlgb_params2['n_estimators'] = 1090\nlgb_params2['learning_rate'] = 0.02\nlgb_params2['colsample_bytree'] = 0.3\nlgb_params2['subsample'] = 0.7\nlgb_params2['subsample_freq'] = 2\nlgb_params2['num_leaves'] = 16\nlgb_params2['seed'] = 99\n\nlgb_params3 = {}\nlgb_params3['n_estimators'] = 1100\nlgb_params3['max_depth'] = 4\nlgb_params3['learning_rate'] = 0.02\nlgb_params3['seed'] = 99\n\nlgb_model = LGBMClassifier(**lgb_params)\nlgb_model2 = LGBMClassifier(**lgb_params2)\nlgb_model3 = LGBMClassifier(**lgb_params3)\n\nlog_model = LogisticRegression()\n\nstack = Ensemble(n_splits=3,\n                 stacker=log_model,\n                 base_models=(lgb_model, lgb_model2, lgb_model3))\n\ny_pred = stack.fit_predict(train, target_train, test)\n\nsub = pd.DataFrame()\nsub['id'] = id_test\nsub['target'] = y_pred\nsub.to_csv('porto_output.csv', index=False)\n\n# Any results you write to the current directory are saved as output.","outputs":[]}],"metadata":{"language_info":{"mimetype":"text/x-python","pygments_lexer":"ipython3","codemirror_mode":{"version":3,"name":"ipython"},"version":"3.6.3","name":"python","file_extension":".py","nbconvert_exporter":"python"},"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"}},"nbformat":4}