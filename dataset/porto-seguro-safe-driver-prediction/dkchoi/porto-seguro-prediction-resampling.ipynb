{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"import pandas as pd\nsample_submission = pd.read_csv(\"../input/porto-seguro-safe-driver-prediction/sample_submission.csv\")\ntest = pd.read_csv(\"../input/porto-seguro-safe-driver-prediction/test.csv\")\ntrain = pd.read_csv(\"../input/porto-seguro-safe-driver-prediction/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Load Library"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.utils import shuffle\nfrom sklearn.ensemble import RandomForestClassifier\n\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_columns', None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Feature Names include the postfix cat to indicate categorical values \\n\n* Feature Names include the postfix bin to indicate binary values \\n\n* Features without these postfix are either continuous or ordinal \\n\n* -1 indicates that feature was missing"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop_duplicates()\nprint(train.shape)\nprint(test.shape)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = []\nfor col in train.columns:\n    if col == 'target':\n        role = 'target'\n    elif col == 'id':\n        role = 'id'\n    else:\n        role = 'input'\n        \n    if 'bin' in col or col == 'target':\n        level = 'binary'\n    elif 'cat' in col or col == 'id':\n        level ='nominal'\n    elif train[col].dtype == 'float':\n        level = 'interval'\n    elif train[col].dtype == 'int':\n        level = 'ordinal'\n        \n    keep = True\n    if col == 'id':\n        keep = False\n        \n    dtype = train[col].dtype\n        \n    col_dict = {\n        'varnames' : col,\n        'role' : role,\n        'level' : level,\n        'keep' : keep,\n        'dtype' : dtype\n    }\n    data.append(col_dict)\n    \ndata_dic = pd.DataFrame(data, columns = ['varnames', 'role', 'level', 'keep', 'dtype'])\ndata_dic.set_index('varnames', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dic","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dic[(data_dic.level == 'nominal') & (data_dic.keep)].index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame({'count' : data_dic.groupby(['role', 'level'])['role'].size()}).reset_index()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Statistical Overview"},{"metadata":{},"cell_type":"markdown","source":"#### Interval Variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"var = data_dic[(data_dic.level == 'interval') & (data_dic.keep)].index\ntrain[var].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ps reg : ps_reg_03 has missing values, may apply scaling\n* ps_car : ps_car12 and 14 have missing values, also need scaling\n* ps_calc : no missing values, no need scaling, similiar distributions"},{"metadata":{},"cell_type":"markdown","source":"#### Ordinal Variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"var = data_dic[(data_dic.level == 'ordinal') & (data_dic.keep)].index\ntrain[var].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ps_ind : range differs between variables and need scaling\n* ps_car : has missing values\n* ps_calc : range differs between variables"},{"metadata":{},"cell_type":"markdown","source":"#### Binary Variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"var = data_dic[(data_dic.level == 'binary') & (data_dic.keep)].index\ntrain[var].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* target data in train which is very **Strongly Imbalanced**"},{"metadata":{},"cell_type":"markdown","source":"### Handling Imbalanced Classes\n* oversampling with target = 1\n* undersampling with target = 0"},{"metadata":{},"cell_type":"markdown","source":"#### Undersampling"},{"metadata":{"trusted":true},"cell_type":"code","source":"apriori = 0.10\n\nidx_0 = train[train.target==0].index\nidx_1 = train[train.target==1].index\nprint(idx_0)\nprint(idx_1)\n\n\nnb_0 = len(train.loc[idx_0])\nnb_1 = len(train.loc[idx_1])\nprint(nb_0)\nprint(nb_1)\n\nundersampling_rate = ((1-apriori) * nb_1) / (nb_0 * apriori)\nundersampling_nb_0 = int(undersampling_rate*nb_0)\n\nprint('Rate to undersample records with target=0 : {}'.format(undersampling_rate))\nprint('Number of records with target=0 after undersampling: {}'.format(undersampling_nb_0))\n\n# Randomly select records with target\nundersampled_idx = shuffle(idx_0, random_state=42, n_samples=undersampling_nb_0)\n# print(undersampled_idx)\n\nidx_list = list(undersampled_idx) + list(idx_1)\n# print(idx_list)\n\n#return undersample to dataframe\ntrain = train.loc[idx_list].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Check Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"vars_with_missing = []\n\nfor col in train.columns:\n    missings = train[train[col] == -1][col].count()\n    if missings > 0:\n        vars_with_missing.append(col)\n        missing_per = missings / train.shape[0]\n        \n        print('Variable {} has {} records ({:.2f}) with missing values'.format(col, missings, missing_per))\n        \nprint('Total there are {} variables with missing values'.format(len(vars_with_missing)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### How Can I Deal With?\n* ps_car_03_cat and ps_car_05_cat have a large proportion of records with missing values : remove or interpolate???\n* ps_reg_03(continuous) has 18% missing values\n* ps_car_11(ordinal) has only one missing value\n* ps_car_14(continuous) has 7% missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('ps_car_03 mean : {}'.format(train['ps_car_03_cat'].mean()))\nprint('ps_car_03 median : {}'.format(train['ps_car_03_cat'].median()))\nprint('ps_car_03 mode : {}'.format(train['ps_car_03_cat'].mode()))\n\nprint('ps_car_03 mean : {}'.format(train['ps_car_05_cat'].mean()))\nprint('ps_car_03 median : {}'.format(train['ps_car_05_cat'].median()))\nprint('ps_car_03 mode : {}'.format(train['ps_car_05_cat'].mode()))\n\nprint('ps_car_11 mean : {}'.format(train['ps_car_11'].mean()))\nprint('ps_car_11 median : {}'.format(train['ps_car_11'].median()))\nprint('ps_car_11 mode : {}'.format(train['ps_car_11'].mode()))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Imputing\n\nmean_imp = SimpleImputer(missing_values=-1, strategy='mean')\nmode_imp = SimpleImputer(missing_values=-1, strategy='most_frequent')\n\ntrain['ps_car_03_cat'] = mode_imp.fit_transform(train[['ps_car_03_cat']]).ravel()\ntrain['ps_car_05_cat'] = mode_imp.fit_transform(train[['ps_car_05_cat']]).ravel()\ntrain['ps_car_07_cat'] = mode_imp.fit_transform(train[['ps_car_07_cat']]).ravel()\ntrain['ps_car_09_cat'] = mode_imp.fit_transform(train[['ps_car_09_cat']]).ravel()\ntrain['ps_reg_03'] = mean_imp.fit_transform(train[['ps_reg_03']]).ravel()\ntrain['ps_car_11'] = mode_imp.fit_transform(train[['ps_car_11']]).ravel()\ntrain['ps_car_14'] = mean_imp.fit_transform(train[['ps_car_14']]).ravel()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"var = data_dic[(data_dic.level == 'nominal') & (data_dic.keep)].index\n\nfor col in var:\n    dist_values = train[col].value_counts().shape[0]\n    print('Variable {} has {} distinct values'.format(col, dist_values))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ps_car_11_cat has many distinct value"},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_noise(series, noise_level):\n    return series * (1 + noise_level * np.random.randn(len(series)))\n\ndef target_encode(trn_series=None, \n                  tst_series=None, \n                  target=None, \n                  min_samples_leaf=1, \n                  smoothing=1,\n                  noise_level=0):\n\n    assert len(trn_series) == len(target)\n    assert trn_series.name == tst_series.name\n    temp = pd.concat([trn_series, target], axis=1)\n    # Compute target mean \n    averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n    # Compute smoothing\n    smoothing = 1 / (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) / smoothing))\n    # Apply average function to all target data\n    prior = target.mean()\n    # The bigger the count the less full_avg is taken into account\n    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n    # Apply averages to trn and tst series\n    ft_trn_series = pd.merge(\n        trn_series.to_frame(trn_series.name),\n        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n        on=trn_series.name,\n        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n    # pd.merge does not keep the index so restore it\n    ft_trn_series.index = trn_series.index \n    ft_tst_series = pd.merge(\n        tst_series.to_frame(tst_series.name),\n        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n        on=tst_series.name,\n        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n    # pd.merge does not keep the index so restore it\n    ft_tst_series.index = tst_series.index\n    return add_noise(ft_trn_series, noise_level), add_noise(ft_tst_series, noise_level)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_encoded, test_encoded = target_encode(train['ps_car_11_cat'],\n                                           test['ps_car_11_cat'],\n                                           target=train.target,\n                                           min_samples_leaf=100,\n                                           smoothing=10,\n                                           noise_level=0.01)\n\ntrain['ps_car_11_cat_te'] = train_encoded\ntrain.drop('ps_car_11_cat', axis=1, inplace=True)\ntest['ps_car_11_cat_te'] = train_encoded\ntest.drop('ps_car_11_cat', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Exploratory Data Visualization"},{"metadata":{},"cell_type":"markdown","source":"#### Categorical Variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"var = data_dic[(data_dic.level == 'nominal') & (data_dic.keep)].index\n\nfor f in var:\n    plt.figure()\n    fig, ax = plt.subplots(figsize=(20,10))\n    # Calculate the percentage of target=1 per category value\n    cat_perc = train[[f, 'target']].groupby([f],as_index=False).mean()\n    cat_perc.sort_values(by='target', ascending=False, inplace=True)\n    # Bar plot\n    # Order the bars descending on target mean\n    sns.barplot(ax=ax, x=f, y='target', data=cat_perc, order=cat_perc[f])\n    plt.ylabel('% target', fontsize=18)\n    plt.xlabel(f, fontsize=18)\n    plt.tick_params(axis='both', which='major', labelsize=18)\n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* it is better way to keep missing values(-1) as a separate category value, instead of replacing them"},{"metadata":{},"cell_type":"markdown","source":"#### Interval Variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"# corr map\n\ndef corr_heatmap(var):\n    correlations = train[var].corr()\n    \n    # create color map\n    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n    \n    fig, ax = plt.subplots(figsize=(10,10))\n    sns.heatmap(correlations, cmap=cmap, vmax=1.0, center=0,\n               fmt='.2f', square=True, linewidth=.5, annot=True, cbar_kws={'shrink':.75}\n               )\n    plt.show()\n    \nvar = data_dic[(data_dic.level=='interval') & (data_dic.keep)].index\ncorr_heatmap(var)\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### strong correlations between\n* ps_reg_02 and ps_reg_03 (0.7)\n* ps_car_12 and ps_car_13 (0.68)\n* ps_car_12 and ps_car_14(0.58)\n* ps_car_13 and ps_car_15(0.53)"},{"metadata":{"trusted":true},"cell_type":"code","source":"s = train.sample(frac=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ps_reg_02 and ps_reg_03\nsns.lmplot(x='ps_reg_02', y='ps_reg_03', data=s, hue='target', palette='Set1', scatter_kws={'alpha':0.3})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ps_car_12 and ps_car_13\nsns.lmplot(x='ps_car_12', y='ps_car_13', data=s, hue='target', palette='Set1', scatter_kws={'alpha':0.3})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ps_car_12 and ps_car_14\nsns.lmplot(x='ps_car_12', y='ps_car_14', data=s, hue='target', palette='Set1', scatter_kws={'alpha':0.3})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ps_car_13 and ps_car_15\nsns.lmplot(x='ps_car_15', y='ps_car_13', data=s, hue='target', palette='Set1', scatter_kws={'alpha':0.3})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### need PCA????"},{"metadata":{"trusted":true},"cell_type":"code","source":"var = data_dic[(data_dic.level=='ordinal') & (data_dic.keep)].index\ncorr_heatmap(var)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Engineeing"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dic.drop('ps_car_11_cat', axis=0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"var = data_dic[(data_dic.level=='nominal') & (data_dic.keep)].index\n\nprint('before get dummies ', train.shape)\n\ntrain = pd.get_dummies(train, columns=var, drop_first=True)\nprint('after get dummies ', train.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Creating Interaction Variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"var = data_dic[(data_dic.level=='interval') & (data_dic.keep)].index\n\npoly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\ninteractions = pd.DataFrame(data=poly.fit_transform(train[var]), columns=poly.get_feature_names(var))\ninteractions.drop(var, axis=1, inplace=True)\n\nprint('before creating interaction variables', train.shape[1])\ntrain = pd.concat([train, interactions], axis=1)\nprint('after creating interaction variables', train.shape[1])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Selection"},{"metadata":{},"cell_type":"markdown","source":"#### removing features with low and zero variance"},{"metadata":{"trusted":true},"cell_type":"code","source":"selector = VarianceThreshold(threshold=0.01)\n\nselector.fit(train.drop(['id', 'target'], axis=1))\n\nf = np.vectorize(lambda x : not x)\n\nv = train.drop(['id', 'target'], axis=1).columns[f(selector.get_support())]\n\nprint('{} variables have too low variance '.format(len(v)))\nprint('these vars are  {} '.format(list(v)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Selecting features with a RF"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = train.drop(['id', 'target'], axis=1)\ny_train = train['target']\n\nfeat_labels = X_train.columns\n\nrf = RandomForestClassifier(n_estimators=1000, random_state=0, n_jobs=-1)\n\nrf.fit(X_train, y_train)\nimportances = rf.feature_importances_\n\nindices = np.argsort(rf.feature_importances_)[::-1]\n\nfor f in range(X_train.shape[1]):\n    print(\"%2d) %-*s %f \" % (f + 1, 30, feat_labels[indices[f]], importances[indices[f]]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sfm = SelectFromModel(rf, threshold='median')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}