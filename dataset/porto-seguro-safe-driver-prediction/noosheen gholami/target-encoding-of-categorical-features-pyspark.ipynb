{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pyspark","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pyspark\nimport pandas as pd\nimport numpy as np\n\nspark = pyspark.sql.SparkSession.builder.appName(\"MyApp\") \\\n            .config(\"spark.jars.packages\", \"com.microsoft.ml.spark:mmlspark_2.11:1.0.0-rc1\") \\\n            .config(\"spark.jars.repositories\", \"https://mmlspark.azureedge.net/maven\") \\\n            .getOrCreate()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = spark.read.csv('/kaggle/input/porto-seguro-safe-driver-prediction/train.csv',header = True, inferSchema = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.printSchema()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.sql import functions as F\nfrom pyspark.sql.functions import mean, exp ,lit , col, pow\nfrom pyspark.sql import SQLContext\nfrom pyspark import SparkContext\n\n\n\nname = [\"ps_car_06_cat\",\"ps_car_01_cat\",\"ps_car_11_cat\" ]\nfor i in range(len(name)):\n    train = train.withColumnRenamed('target','label')\n    trn_series = train.select(name[i],\"label\")\n    tr1 , tr2, tr3 , tr4 , tr5 = trn_series.randomSplit([.2,.2,.2,.2,.2])\n\n    tr = [tr1, tr2,tr3, tr4, tr5]\n\n    min_samples_leaf = 1\n    smoothing = 1\n    averages = [[],[],[],[],[]]\n\n    for k in range(5):\n    # Compute target mean \n        n = tr[k]\n        a = n.groupBy(name[i]).agg(F.count(n.label),F.avg(n.label))\n        prior = n.agg(mean(F.col(\"label\").alias(\"mean\"))).collect()[0][\"avg(label AS `mean`)\"]\n        print(prior)\n    # The bigger the count the less full_avg is taken into account\n        a = a.withColumn(\"averages\",prior * (1 - smoothing) + a[\"avg(label)\"] * smoothing)\n        a.drop(\"avg(label)\", \"count(label)\")\n        averages[k] = a\n        print(a)\n    total = averages[0].union(averages[1])\n    total = total.union(averages[2])\n    total = total.union(averages[3])\n    total = total.union(averages[4])\n    targetkfoldmean = total.groupBy(name[i]).agg(F.mean(\"averages\"))\n    targetkfoldmean = targetkfoldmean.withColumnRenamed(name[i],name[i]+'encod')\n    train = train.withColumnRenamed(name[i],name[i]+'encod')\n    targetkfoldmean = targetkfoldmean.withColumnRenamed( \"avg(averages)\",name[i]+'replaced')\n\n    train = targetkfoldmean.join(train, on = name[i]+'encod' , how = 'full')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}