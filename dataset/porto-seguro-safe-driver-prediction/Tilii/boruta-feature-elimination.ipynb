{"nbformat_minor":1,"metadata":{"language_info":{"pygments_lexer":"ipython3","mimetype":"text/x-python","nbconvert_exporter":"python","name":"python","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.3","file_extension":".py"},"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{},"source":"This is feature elimination based on **[Boruta](https://m2.icm.edu.pl/boruta/)**. Thanks to **[@olivier](https://www.kaggle.com/ogrellier)** for **[discussions](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/41595#233852)** and for **[this notebook](https://www.kaggle.com/ogrellier/noise-analysis-of-porto-seguro-s-features)** that got me going in this direction.\n\nNote that olivier used LightGBM as his **[base estimator](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/41595#234273)** while I am using Random Forest. Because of that, the results are different. I have no way of telling which feature selection is better as I haven't tested either one yet. If you do test them, please leave a note here. I will do the same."},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"134f913c46ed9330d667fe844e7015cd8c24059a","collapsed":true,"_cell_guid":"524b2ce3-1454-43db-b022-3da107320a70"},"source":"from __future__ import print_function\n\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\nfrom sklearn.ensemble import RandomForestClassifier\nfrom boruta import BorutaPy\n\npd.set_option('display.max_rows', 1000)\npd.set_option('display.max_columns', 1000)\n\ndef timer(start_time=None):\n    if not start_time:\n        start_time = datetime.now()\n        return start_time\n    elif start_time:\n        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n        tmin, tsec = divmod(temp_sec, 60)\n        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"681a453844ecbc0ce36992aa1ed83cce739fc56a","_cell_guid":"175c8476-e632-40af-a528-dc62908fbf92"},"source":"Loading files."},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"e2f0ab2ca2c63557d0455f3ffc61cb1930995f8e","collapsed":true,"_cell_guid":"7b35b99d-91b1-4e69-95b7-5a98cf911445"},"source":"train = pd.read_csv('../input/train.csv', dtype={'target': np.int8, 'id': np.int32})\nX = train.drop(['id','target'], axis=1).values\ny = train['target'].values\ntr_ids = train['id'].values\nn_train = len(X)\ntest = pd.read_csv('../input/test.csv', dtype={'id': np.int32})\nX_test = test.drop(['id'], axis=1).values\nte_ids = test['id'].values","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"e1490b1f7abb55259cb7c0c3ad71cebe2ae46f49","_cell_guid":"8a01bb11-753b-47b7-8b6c-eabe8e87a9f4"},"source":"It is worth playing with **[RFC parameters](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)**. Initially, I had *n_estimators=100* and *max_depth=10* which was not selecting enough features. Boruta parameters are explained **[here](https://github.com/scikit-learn-contrib/boruta_py)**."},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"89347b7e8f8f3ceca68bcbe026a8a2a413d43ba1","_cell_guid":"79a24d6c-4215-4135-9b11-4d312af6daf9"},"source":"rfc = RandomForestClassifier(n_estimators=200, n_jobs=4, class_weight='balanced', max_depth=6)\nboruta_selector = BorutaPy(rfc, n_estimators='auto', verbose=2)\nstart_time = timer(None)\nboruta_selector.fit(X, y)\ntimer(start_time)","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"d9d007291afdaa42f6269f44e84a0d3a8ff554c8","_cell_guid":"b2f2d7fc-58a3-4526-bab2-bdcc1f93af66"},"source":"The summary of the whole run is shown here. Couple of attributes at the end are commented out. Finally, we save train and test datasets with a subset of selected features."},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"0cde287e95efbd9f516d1b9c44b7af3578373841","_cell_guid":"8a903aee-cd84-4589-bb21-41fa12870172"},"source":"print ('\\n Initial features: ', train.drop(['id','target'], axis=1).columns.tolist() )\n\n# number of selected features\nprint ('\\n Number of selected features:')\nprint (boruta_selector.n_features_)\n\nfeature_df = pd.DataFrame(train.drop(['id','target'], axis=1).columns.tolist(), columns=['features'])\nfeature_df['rank']=boruta_selector.ranking_\nfeature_df = feature_df.sort_values('rank', ascending=True).reset_index(drop=True)\nprint ('\\n Top %d features:' % boruta_selector.n_features_)\nprint (feature_df.head(boruta_selector.n_features_))\nfeature_df.to_csv('boruta-feature-ranking.csv', index=False)\n\n# check ranking of features\nprint ('\\n Feature ranking:')\nprint (boruta_selector.ranking_)\n\n# check selected features\n# print ('\\n Selected features:')\n# print (boruta_selector.support_)\n\n# check weak features\n# print ('\\n Support for weak features:')\n#print (boruta_selector.support_weak_)\n\nselected = train.drop(['id','target'], axis=1).columns[boruta_selector.support_]\ntrain = train[selected]\ntrain['id'] = tr_ids\ntrain['target'] = y\ntrain = train.set_index('id')\ntrain.to_csv('train_boruta_filtered.csv', index_label='id')\ntest = test[selected]\ntest['id'] = te_ids\ntest = test.set_index('id')\ntest.to_csv('test_boruta_filtered.csv', index_label='id')","outputs":[]}],"nbformat":4}