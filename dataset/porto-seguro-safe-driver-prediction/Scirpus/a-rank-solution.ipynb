{"nbformat_minor":1,"cells":[{"source":"import numpy as np \nimport pandas as pd\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import KFold,StratifiedKFold\nfrom numba import jit","outputs":[],"cell_type":"code","metadata":{"collapsed":true,"_uuid":"8e5984dd969fbfbf26a0238e4c056cc1ed3cf5a4","_cell_guid":"d024112a-0bca-4d7f-996e-f2ffd741ed27"},"execution_count":1},{"source":"@jit\ndef eval_gini(y_true, y_prob):\n    y_true = np.asarray(y_true)\n    y_true = y_true[np.argsort(y_prob)]\n    ntrue = 0\n    gini = 0\n    delta = 0\n    n = len(y_true)\n    for i in range(n-1, -1, -1):\n        y_i = y_true[i]\n        ntrue += y_i\n        gini += y_i * delta\n        delta += 1 - y_i\n    gini = 1 - 2 * gini / (ntrue * (n - ntrue))\n    return gini\n\ndef gini_xgb(preds, dtrain):\n    labels = dtrain.get_label()\n    preds -= preds.min()\n    preds / preds.max()\n    gini_score = -eval_gini(labels, preds)\n    return [('gini', gini_score)]\n\n\ndef add_noise(series, noise_level):\n    return series * (1 + noise_level * np.random.randn(len(series)))\n\n\ndef target_encode(trn_series=None,    # Revised to encode validation series\n                  val_series=None,\n                  tst_series=None,\n                  target=None,\n                  min_samples_leaf=1,\n                  smoothing=1,\n                  noise_level=0):\n    \"\"\"\n    Smoothing is computed like in the following paper by Daniele Micci-Barreca\n    https://kaggle2.blob.core.windows.net/forum-message-attachments/225952/7441/high%20cardinality%20categoricals.pdf\n    trn_series : training categorical feature as a pd.Series\n    tst_series : test categorical feature as a pd.Series\n    target : target data as a pd.Series\n    min_samples_leaf (int) : minimum samples to take category average into account\n    smoothing (int) : smoothing effect to balance categorical average vs prior\n    \"\"\"\n    assert len(trn_series) == len(target)\n    assert trn_series.name == tst_series.name\n    temp = pd.concat([trn_series, target], axis=1)\n    # Compute target mean\n    averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n    # Compute smoothing\n    smoothing = 1 / (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) / smoothing))\n    # Apply average function to all target data\n    prior = target.mean()\n    # The bigger the count the less full_avg is taken into account\n    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n    # Apply averages to trn and tst series\n    ft_trn_series = pd.merge(\n        trn_series.to_frame(trn_series.name),\n        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n        on=trn_series.name,\n        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n    # pd.merge does not keep the index so restore it\n    ft_trn_series.index = trn_series.index\n    ft_val_series = pd.merge(\n        val_series.to_frame(val_series.name),\n        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n        on=val_series.name,\n        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n    # pd.merge does not keep the index so restore it\n    ft_val_series.index = val_series.index\n    ft_tst_series = pd.merge(\n        tst_series.to_frame(tst_series.name),\n        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n        on=tst_series.name,\n        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n    # pd.merge does not keep the index so restore it\n    ft_tst_series.index = tst_series.index\n    return add_noise(ft_trn_series, noise_level), add_noise(ft_val_series, noise_level), add_noise(ft_tst_series, noise_level)","outputs":[],"cell_type":"code","metadata":{"collapsed":true,"_uuid":"196986d94aa749057f8613105abe7b9bc0428e9f","_cell_guid":"ba1003f0-f8c8-4796-8867-d15366c4af86"},"execution_count":6},{"source":"strdirectory = '../input/'\ntrain = pd.read_csv(strdirectory+'train.csv')\ntest = pd.read_csv(strdirectory+'test.csv')\n\n\ntest.insert(1,'target',0)\nprint(train.shape)\nprint(test.shape)\n\nx = pd.concat([train,test])\nx = x.reset_index(drop=True)\nunwanted = x.columns[x.columns.str.startswith('ps_calc_')]\nx.drop(unwanted,inplace=True,axis=1)\n\nx.loc[:,'ps_reg_03'] = pd.cut(x['ps_reg_03'], 50,labels=False)\nx.loc[:,'ps_car_12'] = pd.cut(x['ps_car_12'], 50,labels=False)\nx.loc[:,'ps_car_13'] = pd.cut(x['ps_car_13'], 50,labels=False)\nx.loc[:,'ps_car_14'] =  pd.cut(x['ps_car_14'], 50,labels=False)\nx.loc[:,'ps_car_15'] =  pd.cut(x['ps_car_15'], 50,labels=False)\n\ntest = x.iloc[train.shape[0]:].copy()\ntrain = x.iloc[:train.shape[0]].copy()\n","outputs":[],"cell_type":"code","metadata":{"collapsed":true,"_uuid":"bdefba5b97a34fe8260d1a5ed1582a2636663fa3","_cell_guid":"d94b25f0-d0b7-4869-9771-c7a66d684db8"},"execution_count":5},{"source":"features = train.columns[2:]\nranktestpreds = np.zeros(test.shape[0])\nkf = KFold(n_splits=5,shuffle=True,random_state=2017)\nfor i, (train_index, test_index) in enumerate(kf.split(list(train.index))):\n    print('Fold: ',i)\n    myfeatures = list(features[:])\n    blindtrain = train.iloc[test_index].copy()\n    vistrain = train.iloc[train_index].copy()\n    mytest = test.copy()\n    for column in features:\n        vis, blind, tst = target_encode(trn_series=vistrain[column],\n                                        val_series=blindtrain[column],\n                                        tst_series=mytest[column],\n                                        target=vistrain.target,\n                                        min_samples_leaf=200,\n                                        smoothing=10,\n                                        noise_level=0)\n        vistrain['te_' + column] = vis\n        blindtrain['te_' + column] = blind\n        mytest['te_' + column] = tst\n        myfeatures = myfeatures + list(['te_' + column])\n        \n    clf = XGBClassifier(n_estimators=2000,\n                        objective=\"rank:pairwise\",\n                        learning_rate = 0.04,\n                        max_depth = 5,\n                        min_child_weight = 9,\n                        subsample = 0.8,\n                        colsample_bytree = 0.8,\n                        reg_alpha = 10.4,\n                        reg_lambda = 0.59,\n                        seed = 2017,\n                        nthread = 8,\n                        silent = 1)\n    \n    eval_set=[(blindtrain[myfeatures],blindtrain.target)]\n    model = clf.fit(vistrain[myfeatures], vistrain.target, \n                    eval_set=eval_set,\n                    eval_metric=gini_xgb,\n                    early_stopping_rounds=70,\n                    verbose=False)\n    \n    print( \"  Best N trees = \", model.best_ntree_limit )\n    print( \"  Best gini = \", model.best_score )\n    trainpreds = model.predict_proba(blindtrain[myfeatures],ntree_limit=model.best_ntree_limit)[:,1]\n    print( \"  Best gini = \", eval_gini(blindtrain.target,trainpreds))\n    ranktestpreds += model.predict_proba(mytest[myfeatures],ntree_limit=model.best_ntree_limit)[:,1]\nranktestpreds /= 5\nranktestpreds -= ranktestpreds.min()\nranktestpreds /= ranktestpreds.max()\n","outputs":[],"cell_type":"code","metadata":{"collapsed":true,"_uuid":"7ec757f0872d097a9d06ab9d2c029f9a3f31b1c6","_cell_guid":"b8592c33-e90c-4cbe-bd20-120868269193"},"execution_count":10},{"source":"xgbtestpreds = np.zeros(test.shape[0])\nkf = KFold(n_splits=5,shuffle=True,random_state=2017)\nfor i, (train_index, test_index) in enumerate(kf.split(list(train.index))):\n    print('Fold: ',i)\n    myfeatures = list(features[:])\n    blindtrain = train.iloc[test_index].copy()\n    vistrain = train.iloc[train_index].copy()\n    mytest = test.copy()\n    for column in features:\n        vis, blind, tst = target_encode(trn_series=vistrain[column],\n                                        val_series=blindtrain[column],\n                                        tst_series=mytest[column],\n                                        target=vistrain.target,\n                                        min_samples_leaf=200,\n                                        smoothing=10,\n                                        noise_level=0)\n        vistrain['te_' + column] = vis\n        blindtrain['te_' + column] = blind\n        mytest['te_' + column] = tst\n        myfeatures = myfeatures + list(['te_' + column])\n        \n    clf = XGBClassifier(n_estimators=2000,\n                        learning_rate = 0.04,\n                        max_depth = 5,\n                        min_child_weight = 9,\n                        subsample = 0.8,\n                        colsample_bytree = 0.8,\n                        reg_alpha = 10.4,\n                        reg_lambda = 0.59,\n                        seed = 2017,\n                        nthread = 8,\n                        silent = 1)\n    \n    eval_set=[(blindtrain[myfeatures],blindtrain.target)]\n    model = clf.fit(vistrain[myfeatures], vistrain.target, \n                    eval_set=eval_set,\n                    eval_metric=gini_xgb,\n                    early_stopping_rounds=70,\n                    verbose=False)\n    \n    print( \"  Best N trees = \", model.best_ntree_limit )\n    print( \"  Best gini = \", model.best_score )\n    trainpreds = model.predict_proba(blindtrain[myfeatures],ntree_limit=model.best_ntree_limit)[:,1]\n    print( \"  Best gini = \", eval_gini(blindtrain.target,trainpreds))\n    xgbtestpreds += model.predict_proba(mytest[myfeatures],ntree_limit=model.best_ntree_limit)[:,1]\nxgbtestpreds /= 5\nxgbtestpreds -= xgbtestpreds.min()\nxgbtestpreds /= xgbtestpreds.max()","outputs":[],"cell_type":"code","metadata":{"collapsed":true},"execution_count":null},{"source":"rankdata = pd.DataFrame()\nrankdata['xgbnormal'] = xgbtestpreds\nrankdata['xgbrank'] = ranktestpreds","outputs":[],"cell_type":"code","metadata":{"collapsed":true},"execution_count":null},{"source":"sub = pd.read_csv('../input/sample_submission.csv')\nsub.target = (rankdata.xgbnormal.rank()+rankdata.xgbrank.rank())\nsub.target -= sub.target.min()\nsub.target /= sub.target.max()\nsub.to_csv('xgbsubmission.csv', index = False)","outputs":[],"cell_type":"code","metadata":{"collapsed":true},"execution_count":null}],"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","nbconvert_exporter":"python","mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","version":"3.6.3"},"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"}},"nbformat":4}