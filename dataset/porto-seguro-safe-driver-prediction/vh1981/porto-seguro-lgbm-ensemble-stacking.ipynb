{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Porto Seguro’s Safe Driver Prediction : LightGBM with stacking\n\n\nFrom : https://www.kaggle.com/yekenot/simple-stacker-lb-0-284   ---  very helpful!\n\n","execution_count":null},{"metadata":{},"cell_type":"raw","source":"","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nimport gc\nimport os\nimport numpy as np\nimport pandas as pd\n\nimport lightgbm as lgb\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom numba import jit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"N_SPLITS = 4","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing\n\nLoad Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"src_data_path = \"../input/porto-seguro-safe-driver-prediction\"\ndf_train = pd.read_csv(os.path.join(src_data_path, \"train.csv\"))\ndf_test = pd.read_csv(os.path.join(src_data_path, \"test.csv\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"id_test = df_test['id'].values\ntarget_train = df_train['target'].values\n\ndf_train = df_train.drop(['target', 'id'], axis=1)\ndf_test = df_test.drop(['id'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check missing data(-1 means data is missing.)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import missingno as msno\ndf_copy = df_train.copy()\ndf_copy = df_copy.replace(-1, np.nan)\n\n# print in order of number of NaN.\nprint(df_copy.count().sort_values()[0:5])\nmsno.matrix(df=df_copy.iloc[:, 2:39], figsize=(20, 14), color=(0.42, 0.1, 0.05))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Remove columns data is too much missing.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"incomplete_columns = ['ps_car_03_cat', 'ps_car_05_cat']\ndf_train = df_train.drop(incomplete_columns, axis=1)\ndf_test = df_test.drop(incomplete_columns, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = df_train.replace(-1, np.nan)\ndf_test = df_test.replace(-1, np.nan)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Do one-hot encoding conversion on each categorical features(features name ends with 'cat')","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_features = [a for a in df_train.columns if a.endswith('cat')]\n\ndef preprocess_cat_feature(df, cat_feats):\n    for column in cat_feats:\n        temp = pd.get_dummies(pd.Series(df[column]), prefix=column)        \n        df = pd.concat([df, temp],axis=1)\n        df = df.drop([column],axis=1)\n    return df\n\ndf_train = preprocess_cat_feature(df_train, cat_features)\ndf_test = preprocess_cat_feature(df_test, cat_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Calculate feature importance and drop less-important features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# check feature importances\nmodel = lgb.LGBMClassifier(n_estimators=2000, learning_rate=0.1, max_depth=-1, min_data_in_leaf = 1, min_sum_hessian_in_leaf = 1.0)\n\nX = df_train\ny = target_train\nX = X.replace(-1, np.nan)\nmodel.fit(X, y)\n\nfeatures_imp = pd.DataFrame(sorted(zip(model.feature_importances_, X.columns)), columns=[\"Value\", \"Feature\"])\nplt.figure(figsize=(16, 50))\nsns.barplot(x=\"Value\", y=\"Feature\", data=features_imp.sort_values(by=\"Value\", ascending=False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted_columns_by_fi = sorted(zip(model.feature_importances_, X.columns))\ncol_to_drop = [x[1] for x in sorted_columns_by_fi if x[0] < 100]\n\ndf_train = df_train.drop(col_to_drop, axis=1)\ndf_test = df_test.drop(col_to_drop, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most of other notebooks drop columns starts with 'ps_calc_' and get good result.<br>\nI don't know why dropping 'ps_calc_XXX' columns results in better result.<br>\nMaybe there's a problem of calculating feature importances.<br>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# col_to_drop = df_train.columns[df_train.columns.str.startswith('ps_calc_')]\n# df_train = df_train.drop(col_to_drop, axis=1)\n# df_test = df_test.drop(col_to_drop, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train + Predict\n\nCreate models with different sets of hyperparameters and use K-Fold cross validation for each model. Inferences from valid fold is used to train the ensemble model.<br>\nSubmit data also stack inference result for each model and pass to ensemble model.<br>\n<br>\nOverall process can be seen in the image below.\n\n\n<img src='http://drive.google.com/uc?export=view&id=1EI7Nt-TjbeL0hYvD-wugl0AyJPRgUU1m' /><br>\n<br>\n\nshape of train data for ensemble model:<br>\n\n    (len(train_data) , len(models))\n\nshape of submit data for ensemble model:<br>\n    \n    (len(submit_data) , len(models))","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nfrom sklearn.linear_model import LogisticRegression\n\n\nclass model_builder(object):\n    def __init__(self, n_splits):\n        self.n_splits = n_splits\n        self.stacker = LogisticRegression()\n        pass\n    \n    def fit_predict(self, X, y, T, params_list):\n    \n        # shape만 가져온다.\n        y_valid_pred = 0 * y\n        y_submit_pred = 0\n        \n        X = np.array(X)\n        y = np.array(y)\n        T = np.array(T)\n        \n        stack_train = np.zeros((X.shape[0], len(params_list)))\n        stack_test = np.zeros((T.shape[0], len(params_list)))\n\n        for param_index, params in enumerate(params_list):\n            # set up folds\n            kfold = KFold(n_splits = self.n_splits, random_state = 1, shuffle = True)\n\n            model = lgb.LGBMClassifier(**params)\n            \n            stack_test_i = np.zeros((T.shape[0], self.n_splits))            \n\n            for fold_index, (train_index, val_index) in enumerate(kfold.split(X)):                \n                print(\"model \", param_index, \" fold \", fold_index)\n\n                # create data for this fold\n                X_train = X[train_index]\n                y_train = y[train_index]\n                X_valid = X[val_index]\n\n                model.fit(X_train, y_train, verbose=True)\n\n                # inference validation data with trained model\n                # this will be used as train data for ensemble model(stacker)\n                y_pred = model.predict_proba(X_valid)[:, 1]\n                stack_train[val_index, param_index] = y_pred\n                \n                # inference submit data with trained model\n                # this will be used as input of ensemble model(stacker)\n                pred = model.predict_proba(T)\n                stack_test_i[:, fold_index] = pred[:, 1]\n            stack_test[:, param_index] = stack_test_i.mean(axis=1) # fold model 평균값을 저장\n        \n        # train ensemble model with stacked train data([?, len(params)])\n        self.stacker.fit(stack_train, y)\n        \n        # inference with stacked submit data\n        pred = self.stacker.predict_proba(stack_test)[:, 1]\n        return pred\n\n\nparams_list = [\n    {\n        'learning_rate' : 0.02,\n        'n_estimators' : 650,\n        'max_bin' : 10,\n        'subsample' : 0.8,\n        'subsample_freq' : 10,\n        'colsample_bytree' : 0.8,\n        'min_child_samples' : 500,\n        'seed' : 99\n    },\n    {\n        'learning_rate' : 0.02,\n        'n_estimators' : 1100,        \n        'subsample' : 0.7,\n        'subsample_freq' : 2,\n        'colsample_bytree' : 0.3,        \n        'num_leaves' : 16,\n        'seed' : 99\n    },\n    {\n        'n_estimators' : 1100,\n        'max_depth' : 4,\n        'learning_rate' : 0.02,\n        'seed' : 99\n    },\n]\n\nbuilder = model_builder(n_splits=N_SPLITS)\ny_test_pred = builder.fit_predict(X=df_train, y=target_train, T=df_test, params_list=params_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.DataFrame()\nsub['id'] = id_test\nsub['target'] = y_test_pred\nsub.to_csv('lgb_submit.csv', float_format='%.6f', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}