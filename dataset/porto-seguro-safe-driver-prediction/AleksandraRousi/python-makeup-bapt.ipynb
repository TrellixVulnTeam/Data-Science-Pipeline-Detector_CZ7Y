{"nbformat":4,"nbformat_minor":1,"metadata":{"language_info":{"version":"3.6.3","mimetype":"text/x-python","codemirror_mode":{"version":3,"name":"ipython"},"name":"python","pygments_lexer":"ipython3","file_extension":".py","nbconvert_exporter":"python"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"cells":[{"source":"# The first thing we need to do is to import all of the relevant python libraries that we will need for our analysis. \n# Libraries such as numpy, pandas, statsmodels, and scikit-learn are frequently utilised by the data science community.\n# Import useful packages. For every package you have not in Anaconda you could type \n# \"$pip install\" and the name of the package you want in Anaconda Prompt and then continue here in Notebook\nimport scipy as sc\nimport sklearn as sk\nimport sklearn.metrics as metrics\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport warnings\nfrom collections import Counter\nfrom sklearn.feature_selection import mutual_info_classif\nwarnings.filterwarnings('ignore')\nfrom pandas.tools.plotting import scatter_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.cross_validation import cross_val_score\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import StratifiedKFold\nimport time\nfrom sklearn import *\nimport random\nfrom IPython.display import display, HTML","cell_type":"code","metadata":{"collapsed":true},"execution_count":null,"outputs":[]},{"source":"# Get the path from where I could import my data.\nimport os\nos.getcwd()","cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"source":"# I have the dataset available in an easily accessible CSV, and I can use the convenient pandas method read_csv() to load it into our environment.\n# Import train data\ntrain = pd.read_csv('../input/train.csv')\n# Once our dataset is loaded we can inspect the data using the head() method to have a quick look at what columns and what kind of data we have available to work with.\ntrain.head()\n# Data Description\n# In this competition, we will predict the probability that an auto insurance policy holder files a claim.\n# In the train and test data, features that belong to similar groupings are tagged as such in the feature names \n# (e.g., ind, reg, car, calc). In addition, feature names include the postfix bin to indicate binary features and \n# cat to indicate categorical features. Features without these designations are either continuous or ordinal. \n# Values of -1 indicate that the feature was missing from the observation. The target columns signifies whether or not a claim\n# was filed for that policy holder.","cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"source":"# Import test data. In this dataset we have not the target column which we are called to predict.\ntest = pd.read_csv('../input/test.csv')\ntest.head()","cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"source":"## Exploration and Cleaning\n# Now we can perform some basic exploratory analysis to get a better understanding of what is in our data. For example we would like to know:\n# How much data we have\n# If there are any missing values\n# What data type each column is\n# The distribution of data in each column\n# We could also take this opportunity to plot some charts to help us get an idea of what variables / features will prove useful. For example, if we where thinking of doing some regression analysis, scatter charts could give us a visual indication of correlation between features.\n# The pandas library has plenty of built in functions to help us quickly understand summary information about our dataset. Below we use the shape() method to check how many rows are in our dataset and the describe() method to confirm whether or not our columns have missing values.\n# Train Dataset shape\ntrainshape = np.shape(train)\nprint(\"This train dataset is a 2D array and contains {0} rows and columns\".format(trainshape))","cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"source":"print(\"Number of rows: \", train.shape[0])\ncounts = train.describe().iloc[0]\ndisplay(\n    pd.DataFrame(\n        counts.tolist(), \n        columns=[\"Count of values\"], \n        index=counts.index.values\n    ).transpose()\n)","cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"source":"datacolumns = train.columns","cell_type":"code","metadata":{"collapsed":true},"execution_count":null,"outputs":[]},{"source":"target = train.target\ntarget.count(), target.min(), target.max(), target.mean(), target.std()","cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"source":"# Letâ€™s now take a look at the number of instances (rows) that belong to each class. We can view this as an absolute count.\nprint(train.groupby('target').size())\n\n# and let's take a look at the claims distribution:\ntrain['target'].value_counts().plot(kind='bar')\nplt.title('Claims distribution')\nplt.xlabel('Claim was filled or not')\nplt.ylabel('Number of vehicles')\nsns.despine","cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"source":"# Let's see the type of our variables:\ntrain.dtypes","cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"source":"# The first rows of the float64 type variables are:\nfloat_types = train.select_dtypes(include=['float64'])\nfloat_types.head()","cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"source":"# And the first 5 rows of the int64 type variables are:\nint_types = train.select_dtypes(include=['int64'])\nint_types.head()","cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"source":"# At this stage we would normally begin the process of cleaning our data set, which could involve: Filling in missing values\n# Let's see where we have NAs (True where the column includes NAs and False where the column does not include NAs)\ntrain.isnull().any()","cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"source":"# But in our dataset we have the NAs as -1. Let's see how many -1 we have per column.\n(train==-1).sum()\n# As we can see we have a lot of NAs as -1 in the columns: ps_reg_03, ps_car_03_cat, ps_car_05_cat, ps_car_07_cat and ps_car_14.","cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"source":"# For the test dataset we have:\n(test==-1).sum()","cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"source":"# From the dataset description we know that some features fall into a number of groups; this is indicated by a prefix (for example, ind_, ps_, car_).\n# -Categorical features have the _cat suffix.\n# -Binary features have the _bin suffix.\n# -Features without suffix are either continuous or ordinal.\n\n# Prepare lists of numeric, categorical and binary columns\n# Numeric Features\nnumdata = [x for x in datacolumns if x[-3:] not in ['bin', 'cat']]\n# Categorical Features\ncatdata = [x for x in datacolumns if x[-3:]=='cat']\n# Binary Features\nbindata = [x for x in datacolumns if x[-3:]=='bin']","cell_type":"code","metadata":{"collapsed":true},"execution_count":null,"outputs":[]},{"source":"# For the other categorical variables with missing values, we can leave the missing value -1 as it is.\n# For the continuous variables ps_reg_03, ps_car_12 and ps_car_14 which are continuous with missing values we will replace them by the mean.\nmean_imp = Imputer(missing_values=-1, strategy='mean')\nmode_imp = Imputer(missing_values=-1, strategy='most_frequent')\ntrain['ps_reg_03'] = mean_imp.fit_transform(train[['ps_reg_03']]).ravel()\ntrain['ps_car_01_cat'] = mode_imp.fit_transform(train[['ps_car_01_cat']]).ravel()\ntrain['ps_car_02_cat'] = mode_imp.fit_transform(train[['ps_car_02_cat']]).ravel()\ntrain['ps_car_04_cat'] = mode_imp.fit_transform(train[['ps_car_04_cat']]).ravel()\ntrain['ps_car_07_cat'] = mode_imp.fit_transform(train[['ps_car_07_cat']]).ravel()\ntrain['ps_car_09_cat'] = mode_imp.fit_transform(train[['ps_car_09_cat']]).ravel()\ntrain['ps_ind_02_cat'] = mode_imp.fit_transform(train[['ps_ind_02_cat']]).ravel()\ntrain['ps_ind_04_cat'] = mode_imp.fit_transform(train[['ps_ind_04_cat']]).ravel()\ntrain['ps_ind_05_cat'] = mode_imp.fit_transform(train[['ps_ind_05_cat']]).ravel()\ntrain['ps_car_12'] = mode_imp.fit_transform(train[['ps_car_12']]).ravel()\ntrain['ps_car_14'] = mode_imp.fit_transform(train[['ps_car_14']]).ravel()\ntrain['ps_car_11'] = mode_imp.fit_transform(train[['ps_car_11']]).ravel()\n# For the test dataset we do the same thing because we want to clean it too:\ntest['ps_reg_03'] = mean_imp.fit_transform(test[['ps_reg_03']]).ravel()\ntest['ps_car_01_cat'] = mode_imp.fit_transform(test[['ps_car_01_cat']]).ravel()\ntest['ps_car_02_cat'] = mode_imp.fit_transform(test[['ps_car_02_cat']]).ravel()\ntest['ps_car_04_cat'] = mode_imp.fit_transform(test[['ps_car_04_cat']]).ravel()\ntest['ps_car_07_cat'] = mode_imp.fit_transform(test[['ps_car_07_cat']]).ravel()\ntest['ps_car_09_cat'] = mode_imp.fit_transform(test[['ps_car_09_cat']]).ravel()\ntest['ps_ind_02_cat'] = mode_imp.fit_transform(test[['ps_ind_02_cat']]).ravel()\ntest['ps_ind_04_cat'] = mode_imp.fit_transform(test[['ps_ind_04_cat']]).ravel()\ntest['ps_ind_05_cat'] = mode_imp.fit_transform(test[['ps_ind_05_cat']]).ravel()\ntest['ps_car_12'] = mode_imp.fit_transform(test[['ps_car_12']]).ravel()\ntest['ps_car_14'] = mode_imp.fit_transform(test[['ps_car_14']]).ravel()\ntest['ps_car_11'] = mode_imp.fit_transform(test[['ps_car_11']]).ravel()","cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"source":"# Let's see if we have any missing values after the changes we made.\n# We leave the two variables ps_car_03_cat and ps_car_05_cat as they are for now because we will need them in the process below.\n(train==-1).sum()","cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"source":"# We leave the two variables ps_car_03_cat and ps_car_05_cat as they are for now because we will need them in the process below.\n(test==-1).sum()","cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"source":"for column in catdata:    \n    # Figure initiation \n    fig, ax = plt.subplots(figsize=(15, 4))\n    # Calculate the percentage of target=1 per category value\n    cat_perc = train[[column, 'target']].groupby([column],as_index=False).mean()\n    cat_perc.sort_values(by='target', ascending=False, inplace=True)\n    # Bar plot\n    # Order the bars descending on target mean\n    sns.barplot(ax=ax, x=column, y='target', data=cat_perc, order=cat_perc[column])\n    plt.ylabel('% target', fontsize=18)\n    plt.xlabel(column, fontsize=18)\n    plt.tick_params(axis='both', which='major', labelsize=18)\n    plt.show();","cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"source":"# As we can see the ps_car_11_cat column has a lot of unique values as a categorical variable. \n# Let's see how many unique values this column has.\nlen(set(train['ps_car_11_cat']))","cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"source":"for column in bindata:    \n    # Figure initiation \n    fig, ax = plt.subplots(figsize=(8,4))\n    # Calculate the percentage of target=1 per category value\n    cat_perc = train[[column, 'target']].groupby([column],as_index=False).mean()\n    cat_perc.sort_values(by='target', ascending=False, inplace=True)\n    # Bar plot\n    # Order the bars descending on target mean\n    sns.barplot(ax=ax, x=column, y='target', data=cat_perc, order=cat_perc[column])\n    plt.ylabel('% target', fontsize=18)\n    plt.xlabel(column, fontsize=18)\n    plt.tick_params(axis='both', which='major', labelsize=18)\n    plt.show();","cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"source":"# Correlation matrix for binary data.\ncorrbin = train[bindata].corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrbin, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10})","cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"source":"# Zoomed correlation matrix because of possible relationship between ps_ind_11_bin, ps_ind_12_bin and ps_ind_13_bin.\nk = 5 #number of variables for heatmap\ncols = corrbin.nlargest(k, 'ps_ind_12_bin')['ps_ind_12_bin'].index\ncm = np.corrcoef(train[cols].values.T)\nsns.set(font_scale=1.25)\nf, ax = plt.subplots(figsize=(12, 12))\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"source":"# Correlation matrix for numeric data.\ncorrnum = train[numdata].corr()\nf, ax = plt.subplots(figsize=(15, 15))\nsns.heatmap(corrnum, square=True, cbar=True, annot=True, fmt='.2f', annot_kws={'size': 10})","cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"source":"# My attention goes on two different relationship squares. So, I will have a zoomed correlation matrix.\n# The first relationship square is between ps_reg_01, ps_reg_02 and ps_reg_03 and \n# the second relationship square is between ps_car_12, ps_car_13 and ps_car_15.\nk = 11 #number of variables for heatmap\ncols = corrnum.nlargest(k, 'ps_reg_01')['ps_reg_01'].index\ncm = np.corrcoef(train[cols].values.T)\nsns.set(font_scale=1.25)\nf, ax = plt.subplots(figsize=(12, 12))\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"source":"# Correlation matrix for categorical data.\ncorrcat = train[catdata].corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrcat, square=True, cbar=True, annot=True, fmt='.2f', annot_kws={'size': 10})","cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"source":"# My attention goes on two different relationship squares. So, I will have a zoomed correlation matrix.\n# The first relationship square is between ps_reg_01, ps_reg_02 and ps_reg_03 and \n# the second relationship square is between ps_car_12, ps_car_13 and ps_car_15.\nk = 15 #number of variables for heatmap\ncols = corrcat.nlargest(k, 'ps_car_05_cat')['ps_car_05_cat'].index\ncm = np.corrcoef(train[cols].values.T)\nsns.set(font_scale=1.25)\nf, ax = plt.subplots(figsize=(12, 12))\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"source":"#scatterplot Binary data\nsns.set()\ncols = ['ps_ind_11_bin', 'ps_ind_12_bin', 'ps_ind_13_bin']\nsns.pairplot(train[cols], size = 2.5)\nplt.show();","cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"source":"#scatterplot Numeric data\nsns.set()\ncols = ['ps_reg_01', 'ps_reg_02', 'ps_reg_03', 'ps_car_12', 'ps_car_13']\nsns.pairplot(train[cols], size = 3)\nplt.show();","cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"source":"#scatterplot Categorical data\nsns.set()\ncols = ['ps_car_12', 'ps_car_13', 'ps_car_15']\nsns.pairplot(train[cols], size = 3)\nplt.show();","cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"source":"# We see that ps_car_03_cat and ps_car_05_cat have a large proportion of records with missing values. \n# We will remove these variables. (I didn't make it before because of not existing in the index the barplots couldn't appeared)\ndel train[\"ps_car_03_cat\"]\ndel train[\"ps_car_05_cat\"]\n# We delete the same column in the test dataset too:\ndel test[\"ps_car_03_cat\"]\ndel test[\"ps_car_05_cat\"]","cell_type":"code","metadata":{"collapsed":true},"execution_count":null,"outputs":[]},{"source":"# Drop the columns that we have decided won't be used in prediction\ntrain = train.drop([\"ps_calc_15_bin\", \"ps_calc_16_bin\", \"ps_calc_17_bin\", \"ps_calc_18_bin\", \"ps_calc_19_bin\", \"ps_calc_20_bin\", \"ps_ind_06_bin\", \"ps_ind_07_bin\", \"ps_ind_08_bin\", \"ps_ind_09_bin\"], axis=1)\nfeatures = train.drop([\"target\"], axis=1).columns\ntest = test.drop([\"ps_calc_15_bin\", \"ps_calc_16_bin\", \"ps_calc_17_bin\", \"ps_calc_18_bin\", \"ps_calc_19_bin\", \"ps_calc_20_bin\", \"ps_ind_06_bin\", \"ps_ind_07_bin\", \"ps_ind_08_bin\", \"ps_ind_09_bin\"], axis=1)","cell_type":"code","metadata":{"collapsed":true},"execution_count":null,"outputs":[]},{"source":"# Drop the columns that we have decided won't be used in prediction\ntrain = train.drop([\"ps_calc_01\", \"ps_calc_02\", \"ps_calc_03\", \"ps_calc_04\", \"ps_calc_05\", \"ps_calc_06\", \"ps_calc_07\", \"ps_calc_08\", \"ps_calc_09\", \"ps_calc_10\", \"ps_calc_11\", \"ps_calc_12\", \"ps_calc_13\", \"ps_calc_14\"], axis=1)\nfeatures = train.drop([\"target\"], axis=1).columns\ntest = test.drop([\"ps_calc_01\", \"ps_calc_02\", \"ps_calc_03\", \"ps_calc_04\", \"ps_calc_05\", \"ps_calc_06\", \"ps_calc_07\", \"ps_calc_08\", \"ps_calc_09\", \"ps_calc_10\", \"ps_calc_11\", \"ps_calc_12\", \"ps_calc_13\", \"ps_calc_14\"], axis=1)","cell_type":"code","metadata":{"collapsed":true},"execution_count":null,"outputs":[]},{"source":"# Correlation matrix for all variables:\ncorrmat = train.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True);","cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"source":"# At this point I can construct my model. The first thing to do is split our train dataset into training and test sets.\n# I will take a simple approach and take a 70:30 randomly sampled split..\ndf_train, df_test = train_test_split(train, test_size=0.3)","cell_type":"code","metadata":{"collapsed":true},"execution_count":null,"outputs":[]},{"source":"# Set up our RandomForestClassifier instance and fit to data\nclf = RandomForestClassifier(n_estimators=30)\nclf.fit(df_train[features], df_train[\"target\"])","cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"source":"# Make predictions\npredictions = clf.predict(df_test[features])\nprobs = clf.predict_proba(df_test[features])\ndisplay(predictions)","cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"source":"# Let's see the Accuracy of RandomForest Classifier:\nscore = clf.score(df_test[features], df_test[\"target\"])\nprint(\"Accuracy: \", score)","cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"source":"# Actual False and True predictions\nget_ipython().magic('matplotlib inline')\nconfusion_matrix = pd.DataFrame(\n    confusion_matrix(df_test[\"target\"], predictions), \n    columns=[\"Predicted False\", \"Predicted True\"], \n    index=[\"Actual False\", \"Actual True\"]\n)\ndisplay(confusion_matrix)","cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"source":"# Calculate the fpr and tpr for all thresholds of the classification\nfpr, tpr, threshold = roc_curve(df_test[\"target\"], probs[:,1])\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"source":"# In the following results we will see the id as a high important label but we know that it is not true. \n# So we don't care about id. The top important variables are ps_car_13, ps_reg_03 etc.\nfig = plt.figure(figsize=(20, 18))\nax = fig.add_subplot(111)\n\ndf_f = pd.DataFrame(clf.feature_importances_, columns=[\"importance\"])\ndf_f[\"labels\"] = features\ndf_f.sort_values(\"importance\", inplace=True, ascending=False)\ndisplay(df_f.head(6))\n\nindex = np.arange(len(clf.feature_importances_))\nbar_width = 0.5\nrects = plt.barh(index , df_f[\"importance\"], bar_width, alpha=0.4, color='b', label='Main')\nplt.yticks(index, df_f[\"labels\"])\nplt.show()","cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"source":"df_test[\"prob_true\"] = probs[:, 1]\ndf_risky = df_test[df_test[\"prob_true\"] > 0.5]\ndisplay(df_risky.head(5)[[\"prob_true\"]])","cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"source":"# We need to separate the target \"dataset\" from the whole dataset\nimport numpy as np\nfrom sklearn import datasets\ntrain_x = train.drop([\"target\"], axis=1)\ntrain_y = train.target\nnp.unique(train_y)","cell_type":"code","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"source":"kf = StratifiedKFold(n_splits=5,random_state=5,shuffle=True)\ntest_full=0\ncv_score=[]\ni=1\nfor train_index,test_index in kf.split(train_x, train_y):    \n    print('\\n{} of kfold {}'.format(i, kf.n_splits))\n    xtr, xvl = train_x.loc[train_index], train_x.loc[test_index]\n    ytr, yvl = train_y[train_index], train_y[test_index]\n    \n    lr = LogisticRegression(class_weight='balanced', C=0.003)\n    lr.fit(xtr, ytr)\n    pred_test = lr.predict_proba(xvl)[:,1]\n    score = roc_auc_score(yvl, pred_test)\n    print('roc_auc_score', score)\n    cv_score.append(score)\n    test_full += lr.predict_proba(test)[:,1]\n    i+=1","cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"source":"# Make predictions\npredictions = lr.predict(test[features])\nprobs = lr.predict_proba(test[features])\ndisplay(predictions)","cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"source":"test_pred = test_full/5\nsubmit = pd.DataFrame({'id':test['id'],'target':test_pred})\nsubmit.head()","cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"source":"# Let's split again our train dataset in train and test datasets.\ntest_size = 0.30\nseed = 7\ntrain_x_train, train_x_test = model_selection.train_test_split(train_x, test_size=test_size, random_state=seed)\ntrain_y_train, train_y_test = model_selection.train_test_split(train_y, test_size=test_size, random_state=seed)","cell_type":"code","metadata":{"collapsed":true},"execution_count":null,"outputs":[]},{"source":"np.random.seed(0)\nindices = np.random.permutation(len(train_x))\nindices","cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"source":"# And let's test a KNN Classifier\nknn = KNeighborsClassifier()\nknn.fit(train_x_train, train_y_train)","cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"source":"# And let's see the Accuracy of KNN Classifier:\nscore = knn.score(train_x_test, train_y_test)\nprint(\"Accuracy: \", score)","cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"source":"# The same method about LogisticRegression now\nlogistic = linear_model.LogisticRegression(C=1e5)\nlogistic.fit(train_x_train, train_y_train)\nlogistic.score(train_x_test, train_y_test)\nscore = logistic.score(train_x_test, train_y_test)\n# And let's see the accuracy of LogisticRegression Classifier\nprint(\"Accuracy: \", score)","cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"source":"# Predictions and probs about LogisticRegression:\npredictions = lr.predict(train_x_test)\nprobs = lr.predict_proba(train_x_test)\ndisplay(predictions)","cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"source":"# From scikil_learn I found these graphs where you can clearly see the comparison of classification of the classifiers.\nprint(__doc__)\n\nnp.random.seed(0)\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn import datasets\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.calibration import calibration_curve\n\ntrain, test = datasets.make_classification(n_samples=100000, n_features=20,\n                                    n_informative=2, n_redundant=2)\n\ntrain_samples = 100  # Samples used for training the models\n\ntrain_train = train[:train_samples]\ntrain_test = train[train_samples:]\ntest_train = test[:train_samples]\ntest_test = test[train_samples:]\n\n# Create classifiers\n# We saw before LogisticRegression and RandomForest classifiers in detail \nlr = LogisticRegression()\nrfc = RandomForestClassifier(n_estimators=100)\n# We will add GausianNB and LinearSVC\ngnb = GaussianNB()\nsvc = LinearSVC(C=1.0)\n\n# Plot calibration plots\n\nplt.figure(figsize=(10, 10))\nax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\nax2 = plt.subplot2grid((3, 1), (2, 0))\n\nax1.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\nfor clf, name in [(lr, 'Logistic'),\n                  (gnb, 'Naive Bayes'),\n                  (svc, 'Support Vector Classification'),\n                  (rfc, 'Random Forest')]:\n    clf.fit(train_train, test_train)\n    if hasattr(clf, \"predict_proba\"):\n        prob_pos = clf.predict_proba(train_test)[:, 1]\n    else:  # use decision function\n        prob_pos = clf.decision_function(train_test)\n        prob_pos = \\\n            (prob_pos - prob_pos.min()) / (prob_pos.max() - prob_pos.min())\n    fraction_of_positives, mean_predicted_value = \\\n        calibration_curve(test_test, prob_pos, n_bins=10)\n\n    ax1.plot(mean_predicted_value, fraction_of_positives, \"s-\",\n             label=\"%s\" % (name, ))\n\n    ax2.hist(prob_pos, range=(0, 1), bins=10, label=name,\n             histtype=\"step\", lw=2)\n\nax1.set_ylabel(\"Fraction of positives\")\nax1.set_ylim([-0.05, 1.05])\nax1.legend(loc=\"lower right\")\nax1.set_title('Calibration plots  (reliability curve)')\n\nax2.set_xlabel(\"Mean predicted value\")\nax2.set_ylabel(\"Count\")\nax2.legend(loc=\"upper center\", ncol=2)\n\nplt.tight_layout()\nplt.show()","cell_type":"code","metadata":{"scrolled":false},"execution_count":null,"outputs":[]},{"source":"# LogisticRegression returns well calibrated predictions as it directly optimizes log-loss.\n# So, I submit the results with the LogisticRegression.\n############################################## For educational reasons only ###############################################","cell_type":"code","metadata":{"collapsed":true},"execution_count":null,"outputs":[]}]}