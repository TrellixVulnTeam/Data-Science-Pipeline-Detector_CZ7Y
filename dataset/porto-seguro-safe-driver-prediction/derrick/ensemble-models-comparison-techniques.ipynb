{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","collapsed":true,"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":false},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0"},"cell_type":"markdown","source":"**Objective**\n\n1. Get a better understanding of the simplified predictive modelling framework\n\n2. Grasp the logic behind different coding methods & concise techniques used\n\n3. Comparisons between different models\n\n\n**Introduction**\n\nThis is my first Kernel, so do help by commenting any improvements you feel i could use! This is based on the Porto Seguro Safe Driver Prediction (Classification Problem) dataset. This is also mainly self-taught, hence most of my approaches here are pretty much simplified & also structured to contain alternative methods. \n\nI've also included some explanations within the codes that I've used to explain the logic behind it as well. Hopefully, those who are in similar self-taught circumstances will find this Kernel useful!\n\n**Some pointers to NOTE:**\n\n    -This Kernel does focuses more on data manipulation for 'Model Comparisons' (Chapter 8 onwards)\n\n    -Chapters 1 to 7 are mainly as a set-up for the Model Comparisons\n    \n    -Chapters 8 onwards deals directly with Model Comparison techniques\n\n\nI will be doing another Kernel that focuses more on 'Data Cleaning' & 'Feature Engineering' separately...\n\n**Chapter Outline**\n\n1.Open Dataset\n\n2.Preliminary Analysis\n\n    2.1 Structure\n    2.2 Composition (Correlation, Missing, Unique, Data-types)\n\n3.Data Cleaning\n\n    3.1 C1 Correction\n    3.2 C2 Complete\n    3.3 C3 Create\n    3.4 C4 Convert\n\n4.Prepare Data - A\n\n5.Exploratory Data Analysis (EDA)\n\n    5.1 Balance of dataset (Target Variable)\n    5.2 Uni-variates\n    5.3 Bi-variates\n    5.4 EDA Summative notes for Feature Importance comparison\n\n6.Parameter Tuning\n\n7.Feature Select - A\n\n    7.1 Pre-Drop Accuracy Score\n    7.2 Post-Drop Accuracy Score\n\n8.Feature Select & Individual Model Charting - B\n\n    8.1 Model Preparation\n    8.2 LASSO\n    8.3 Ridge\n    8.4 Balanced Logistic Regression\n    8.5 XGBoost\n    8.6 Random Forest Classifier\n\n9.Features (Side To Side Comparison)\n\n    9.1 (Coefficient Values) Quick Easy Method\n    9.2 (Coefficient Values) Neat DataFrame Method\n    9.3 (Plotting) Quick Method\n    9.4 (Plotting) Sorted Neat Method\n\n10.ROC AUC (Side To Side Comparison)\n\n    10.1 Brief Annotations\n\n11.Cross Validation (Side To Side Comparison)\n\n    11.1 Overall Conclusions"},{"metadata":{"_uuid":"35b86db7cd12574c4610b0b99066102d602929de","_cell_guid":"eecc9ba4-521c-41de-9fd8-0d3d42619298"},"cell_type":"markdown","source":"**Coding Techniques :**\n\n    A.List comprehensions\n    B.Samples to reduce computational cost\n    C.Concise 'def' functions that can be used repetitively\n    D.Pivoting using groupby\n    E.When & How to convert and reshape dictionaryâ€™s into lists or dataframes\n    F.Quickly split dataframe columns\n    G.loc & conditionals\n    H.Loop Sub-plots\n    I.Quick Lambda formulae functions\n    J.Quick looping print or DataFrame conversion of summative scores\n    K.Order plot components \n    L.Create & Plot Bulk Ensemble comparative results\n"},{"metadata":{"_uuid":"0b442a3ac416ed4393c3b6c0c54cbef18be5167e","collapsed":true,"_cell_guid":"ca5d392b-7a88-4be6-a5ed-792e86675b29","trusted":true},"cell_type":"code","source":"# Import Modules\n\n# Foundational Packages\nimport numpy as np\nimport pandas as pd\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\npd.options.display.max_columns = 100\nZZ = 15","execution_count":3,"outputs":[]},{"metadata":{"_uuid":"25ccce5f4078d0c4519c2a84c77c947d3b0dcdc8","_cell_guid":"758d3f72-3c42-44b8-bab7-ef7e19bba543"},"cell_type":"markdown","source":"**1. Open Dataset**\n\nWe will now use pandas to open the dataset. This part is pretty straightforward. \n\nI've also set a copy set. The primary set \"train_raw\" acts like a control piece where no adjustments (data cleaning) will be made at all. While the \"train_raw_copy\" set is where the adjustments will be made. You will see its use during the feature selection phase."},{"metadata":{"_uuid":"1676c86f35976bf86ec07577b1f5d6b8b4b89800","collapsed":true,"_cell_guid":"19b44d51-e187-436b-9086-ef79c50c911b","trusted":true},"cell_type":"code","source":"# Open Train & Test files\ntrain_raw = pd.read_csv('../input/train.csv', na_values=-1) #FYI na_values are defined in the original data page\ntest_raw = pd.read_csv('../input/test.csv', na_values=-1)\n# Copy Train file for workings\ntrain_raw_copy = train_raw.copy(deep=True)","execution_count":4,"outputs":[]},{"metadata":{"_uuid":"2238d55acb51554896f183b592e0f002fcfed98b","_cell_guid":"5d15620d-4fc4-4126-b3ad-1c04d22b9ef2"},"cell_type":"markdown","source":"**2. Preliminary Analysis**\n\nThis is just a preliminary schematic analysis to get a rough understanding of the data-set."},{"metadata":{"_uuid":"47b6663cf3f7ffa33974c4627b21270a9dcfeabe"},"cell_type":"markdown","source":"* **2.1** Structure\n\nThis is simply the size dimensions (length of rows and width of columns) of the data-set. Head and samples are only to give yourself a quick truncated visual."},{"metadata":{"trusted":true,"_uuid":"bceda66f5577db5f431a356ecae155048cfbaf0c","collapsed":true},"cell_type":"code","source":"# Shape\nprint('Train Shape: ', train_raw_copy.shape)\nprint('Test Shape: ', test_raw.shape)","execution_count":10,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f1f6f3925da0d8c7f644dde776794364cc342d9b","collapsed":true},"cell_type":"code","source":"# Brief Head Output\ndisplay(train_raw_copy.head())\ndisplay(test_raw.head())","execution_count":11,"outputs":[]},{"metadata":{"_uuid":"4b41ee3b6e35a7e26f4efa1e4f105cb8727f1911","_cell_guid":"b6cd2579-95d0-42c8-ad88-28860d0dbebf","trusted":true,"collapsed":true},"cell_type":"code","source":"# Brief Sample Output\nsamples_show = 10\ndisplay(train_raw_copy.sample(samples_show))\ndisplay(test_raw.sample(samples_show))","execution_count":12,"outputs":[]},{"metadata":{"_uuid":"b981c367f29a0a6c2cc2fa6debce2bada3787300"},"cell_type":"markdown","source":"* **2.2** Composition\n\nNow the actual composition of the data-set in preparation for data cleaning."},{"metadata":{"_uuid":"18688a1db2a8f4e31d227ab6c3ce4f57439902fa"},"cell_type":"markdown","source":"We will first deal with the internal relationships within the data, hence for this we will use the heatmap from the seaborn module\n\n-Correlation: The relationships between features. (Positive or Negative, Strong or Weak, None)"},{"metadata":{"_uuid":"f2911acf3ca815b54d48288b6536ad807df175ce","_cell_guid":"e959033e-9f0f-4494-86b2-ff825b952ddb","trusted":true,"collapsed":true},"cell_type":"code","source":"# Heatmap of correlations\ncor = train_raw_copy.corr()\nplt.figure(figsize=(12, 9))\nsns.heatmap(cor)\nplt.show()","execution_count":13,"outputs":[]},{"metadata":{"_uuid":"e73bc3e62d8f6272b70e7ba58eee92cdb89c0774","_cell_guid":"93d3e9ae-a39c-4c48-a754-dab825883244"},"cell_type":"markdown","source":"We see that the features with 'calc' seem to be independent. We'll remove them later during 'Data Cleaning' to reduce unnecessary compuational costs and noise."},{"metadata":{"_uuid":"d58480a8e44441754d8cf3539ab9fc892c80e51a"},"cell_type":"markdown","source":"Now to deal with the the data attributes. We will use a generic function that can be used repetitively as we proceed with data cleaning as a checking measure."},{"metadata":{"_uuid":"4b10f6496d5102bf43dedafaae5c46cd93931832"},"cell_type":"markdown","source":"-Missing values: For each respective column of feature, the count of empty data entries.\n\n-Unique values: For each respective feature, how many unique values there are.\n\n-Data-types:\n\nCategorical (Each category represents a specific class of a particular description)\n\nBinary (Yes/No or 1/0 indicator)\n\nInteger/Ordinal (A series of ordered value counts that represents a scale range)\n\nFloat/Interval (A numerical continuous value scale)"},{"metadata":{"_uuid":"94d8b0fc0c870b6774df787916bd76648ee60586","collapsed":true,"_cell_guid":"14636005-7c80-4ccd-b9cc-809c0353dfd1","trusted":true},"cell_type":"code","source":"# Function to output missing values & UniqueCounts & DataTypes\ndef basic_details(df):\n    details = pd.DataFrame()\n    details['Missing value'] = df.isnull().sum()\n    details['N unique value'] = df.nunique()\n    details['dtype'] = df.dtypes\n    print('\\n', details, '\\n')","execution_count":14,"outputs":[]},{"metadata":{"_uuid":"a1da0eb122b7f14171435dee35719eb44062352d"},"cell_type":"markdown","source":"Train set *Unhide to view output"},{"metadata":{"_kg_hide-output":true,"trusted":true,"_uuid":"99c331a62543aa41074e8a74528ef904a6aa5d3c","collapsed":true},"cell_type":"code","source":"basic_details(train_raw_copy)","execution_count":16,"outputs":[]},{"metadata":{"_uuid":"f43560a742191ee761de24fd4f0b893768b47d43"},"cell_type":"markdown","source":"Test set *Unhide to view output"},{"metadata":{"_kg_hide-output":true,"trusted":true,"_uuid":"f41b75df5a623cbf7b63f0a2886938992568089b","collapsed":true},"cell_type":"code","source":"basic_details(test_raw)","execution_count":17,"outputs":[]},{"metadata":{"_uuid":"efea4459731f027730419caef35d617558b068b5","_cell_guid":"2711e292-252d-4d1f-843e-bfa0d0907256"},"cell_type":"markdown","source":"**3. Data Cleaning **\n\nAfter getting a rough picture what the data-set â€˜â€™Hasâ€™â€™ and â€˜â€™Lacksâ€™â€™, we proceed to tidy these imperfections.."},{"metadata":{"_uuid":"cc0b20f36ffac9fafb9fabc9e094e58e8ca28ed7","_cell_guid":"ed6b45e9-f594-43b7-b009-b7cbc1f31906"},"cell_type":"markdown","source":"** C1â€Šâ€”â€ŠCorrection**\n\nThis step drops the uncorrelated features & removes features which contain excessive rows of empty data entries. The objective is to remove unnecessary noise (prediction errors) and computational costs (run time of code sequence) to the analysis and modelling process as we proceed.\n\nSome decision factors on deciding the cut-off threshold include the data-set dimensions and feature unique values found during the preliminary quick analysis earlier."},{"metadata":{"_uuid":"8853b71af6c7126f7b673c9b684b9f3b15082d74","collapsed":true,"_cell_guid":"9c6c6813-89c1-45f9-8271-c2b87a128025","trusted":true},"cell_type":"code","source":"##### C1 - Correction\n\n# Combine both df for easy referencing\ndata_cleaner = [train_raw_copy, test_raw]\n\n# Get List of Column names to drop\n# 1.Drop those that missing values exceeds threshold\nlimit = 569  # ps_car_09_cat from \"train_raw_copy\" used as threshold for Missing Values\nremove_cols_1 = [c for c in train_raw_copy.columns if train_raw_copy[c].isnull().sum() > limit]\n\n# 2.Drop those that are uncorrelated from Heatmap\n# **NOTE we will rectify this later during Feature Selection**\nremove_cols_2 = train_raw_copy.columns[train_raw_copy.columns.str.startswith('ps_calc')]\n\n# Dropping\nfor DataSet in data_cleaner:\n    DataSet.drop(columns=remove_cols_1, axis=1, inplace=True)\n    DataSet.drop(columns=remove_cols_2, axis=1, inplace=True)","execution_count":18,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4964ca913f47cff3fb8639096c8bd1994a1be3eb","collapsed":true},"cell_type":"code","source":"# Check New Shape\nprint('Train New Shape: ',train_raw_copy.shape)\nprint('Test New Shape: ', test_raw.shape)","execution_count":20,"outputs":[]},{"metadata":{"_uuid":"433e59ad5dbd7df73f800bc508c8d76948cf43eb","_cell_guid":"8da8c78f-3b04-4efd-934e-11e3aafba949"},"cell_type":"markdown","source":"**C2â€Šâ€”â€ŠComplete**\n\nNow to fill the residual missing empty data entries that fell below the threshold in C1. The most common approach is to replace it with either the mode/mean/median. In this case, we will replace with the Mode since we have little depth of knowledge on the features."},{"metadata":{"_uuid":"82d702bbcc8e1d762f3ae80eceac6a7779d726c5","collapsed":true,"_cell_guid":"6dd8b059-5adc-49dd-a5a8-d750bc581a20","trusted":true},"cell_type":"code","source":"##### C2 - Completing (Missing)\n# Choices : Median / Mean / Mode\n\n# Easy referencing\nfor df in data_cleaner:\n    # List Comprehension\n    Residual_Missing = [c for c in df.columns if df[c].isnull().sum() > 0]\n    for col in Residual_Missing:\n        df[col].fillna(df[col].mode()[0], inplace=True)","execution_count":21,"outputs":[]},{"metadata":{"_uuid":"f6b7a1cef14bc01acefecce18c87ae034aee383b"},"cell_type":"markdown","source":"Train set *Unhide to view output"},{"metadata":{"_kg_hide-output":true,"trusted":true,"_uuid":"98f54185f912adf9e06566c54074b944d981e67d","collapsed":true},"cell_type":"code","source":"# Check Missing\nprint('Train Missing: ',train_raw_copy.isnull().sum())","execution_count":23,"outputs":[]},{"metadata":{"_uuid":"223649869abee45680c8cadcbddc8b9c3d2d720d"},"cell_type":"markdown","source":"Test set *Unhide to view output"},{"metadata":{"_kg_hide-output":true,"trusted":true,"_uuid":"a31b7fc88d78ae492b3cdb861ab1f1d705095ebc","collapsed":true},"cell_type":"code","source":"# Check Missing\nprint('Test Missing: ',test_raw.isnull().sum())","execution_count":24,"outputs":[]},{"metadata":{"_uuid":"7a77cc26063e01d90ef22f0467088972622f51c1","_cell_guid":"fdacf7bb-4be3-40a0-968d-5de00258834d"},"cell_type":"markdown","source":"**C3â€Šâ€”â€ŠCreate**\n\nI have temporarily skipped this step as we do not have specific knowledge on the features. If you'd like to dwell into this further do check out my other Kernel titled \"Feature Engineering & EDA Focused\"."},{"metadata":{"_uuid":"3d9961ee7bd1126bda9fb18a47c64f5dc062a952","_cell_guid":"99e6d509-2600-48af-a04a-8d99538f7af1"},"cell_type":"markdown","source":"**C4â€Šâ€”â€ŠConvert**\n\nWe'll now convert each particular statistical data-types to their respective computational data-types."},{"metadata":{"_uuid":"29ebe30146438bc6ba4a6ef6b50a0cb039ba0a71"},"cell_type":"markdown","source":"* Metadata Loop\n\nHere we use a metadata loop to return the following 6x stats...\n\n1.use (The purpose it serves in this analysis): input, ID, target\n\n2.type (The statistical data-types, NOT computational data-types):\n\nNominal_Categorical_cat ->variables w/o order ranking sequence  (Discrete),\n\nBinary_bin                  ->variables w only 2 option either or   (Discrete),\n\nInterval_Real_float     ->Continuous                                         (Continuous), \n\nOrdinal_Integer_int   ->variables w an ordered series         (Discrete)\n\n3.preserve (Retain for prediction or not): True or False\n\n4.dataType (Computational data-type): int, float, char\n\n5.category (Feature type): ind-individual, reg-registration, car, calc-calculated\n\n6.NUnique: \"Number of unique values"},{"metadata":{"_uuid":"1ebc7673cd1e721b3adb5b880c0027844ff3af8f","collapsed":true,"_cell_guid":"8c75709e-a75c-4ce4-b669-9c21e7eeef13","trusted":true},"cell_type":"code","source":"##### C4 - Convert\ndata = []\nfor feature in train_raw_copy.columns:\n    # Defining the role of each variable\n    if feature == 'target':\n        use = 'target'\n    elif feature == 'id':\n        use = 'id'\n    else:\n        use = 'input'\n\n    # Defining the statistical data type\n    if 'bin' in feature or feature == 'target':\n        type = 'binary'\n    elif 'cat' in feature or feature == 'id':\n        type = 'categorical'\n    elif train_raw_copy[feature].dtype == float or isinstance(train_raw_copy[feature].dtype, float):\n        type = 'real'\n    elif train_raw_copy[feature].dtype == int:\n        type = 'integer'\n\n    # Initialize preserve to True for all variables except for id.\n    # Since ONLY id is not in use\n    preserve = True\n    if feature == 'id':\n        preserve = False\n\n    # Defining the data type\n    dtype = train_raw_copy[feature].dtype\n    \n    # Set default\n    category = 'none'\n    # Defining the category\n    if 'ind' in feature:\n        category = 'individual'\n    elif 'reg' in feature:\n        category = 'registration'\n    elif 'car' in feature:\n        category = 'car'\n    elif 'calc' in feature:\n        category = 'calculated'\n\n    # Define UniqueValue Count\n    NUnique = train_raw_copy[feature].nunique()\n\n    # Creating a Dictionary that contains all the metadata for the variable to allocate/append above derivations\n    feature_dictionary = {\n        'varname': feature,\n        'use': use,\n        'type': type,\n        'preserve': preserve,\n        'dtype': dtype,\n        'category': category,\n        'NUnique': NUnique\n    }\n    data.append(feature_dictionary)\n\n# Adjust & Define DataFrame\nmetadata = pd.DataFrame(data, columns=['varname', 'use', 'type', 'preserve', 'dtype', 'category', 'NUnique'])","execution_count":25,"outputs":[]},{"metadata":{"_uuid":"2df591d4d489b782b1c34138e82b9f5d9a480c34"},"cell_type":"markdown","source":"* Pivot Stats\n\nThis works just as the pivot table in Excel."},{"metadata":{"_uuid":"e98928a98f114ea1d97d2ee4006b8a7ef9cea7ae","_cell_guid":"2be65297-a360-4d43-a0c2-bfbcc2b3449d","trusted":true,"collapsed":true},"cell_type":"code","source":"# How many of each Feature types do we have?\nprint(metadata.groupby(['category'])['category'].count())","execution_count":26,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"987c64d7687b2ea4d4034d4becb54cfe41e7e627","collapsed":true},"cell_type":"code","source":"# How many of each Statistical data-types do we have?\nprint(metadata.groupby(['use', 'type'])['use'].count())","execution_count":27,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a4fa1da899e15a1dd13fe66579f5bd4cdaa87fed","collapsed":true},"cell_type":"code","source":"# Combining both of the above\nprint(metadata.groupby(['use', 'type', 'category'])['category'].count())","execution_count":28,"outputs":[]},{"metadata":{"_uuid":"25621f08d445962aa5b072a256fa8fbd90bd9796","_cell_guid":"8cab0368-e3de-4552-84f7-21533cf2c265"},"cell_type":"markdown","source":"* Convert computational data-types\n\nWe will now 'brute-force' convert the variable names for each feature based on their respective statistical data-type.\n\nHere you can see 2 different methods. Either using the metadata we did above, or a shortcut list comprehension.\n\n*Unhide to view output"},{"metadata":{"_kg_hide-output":true,"trusted":true,"_uuid":"7c6cbbee04f236bb6f6ce3c0e353cbcc7ba1fbd0","collapsed":true},"cell_type":"code","source":"# Cat_Categorical\n#BinaryLevel_cat_col = [col for col in train_raw_copy.columns if '_cat' in col] #Alternative List Comprehension Approach\nBinaryLevel_cat_col = metadata.loc[metadata['type'] == 'categorical']['varname'] # Uses the metadata we made earlier\nBinaryLevel_cat_col = list(BinaryLevel_cat_col)\nBinaryLevel_cat_col.remove('id')\n\nfor c in BinaryLevel_cat_col:\n    train_raw_copy[c] = train_raw_copy[c].astype('uint8')\n    test_raw[c] = test_raw[c].astype('uint8')","execution_count":33,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"collapsed":true,"_uuid":"1cfd06d113af955d33df135cf05e938d7325e4b8"},"cell_type":"code","source":"# Bin_Binary\n# NominalLevel_bin_col = [col for col in train_raw_copy.columns if 'bin' in col] #Alternative List Comprehension Approach\nNominalLevel_bin_col = metadata.loc[metadata['type'] == 'binary']['varname'] # Uses the metadata we made earlier\nNominalLevel_bin_col = list(NominalLevel_bin_col)\nNominalLevel_bin_col.remove('target')\n\nfor c in NominalLevel_bin_col:\n    train_raw_copy[c] = train_raw_copy[c].astype('uint8')\n    test_raw[c] = test_raw[c].astype('uint8')","execution_count":31,"outputs":[]},{"metadata":{"_uuid":"bbee25450127564d747a87d84a93932e6da90b48","_cell_guid":"63f5efb6-7f10-4461-a766-12608be1cfba","trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"# Other_Others / Numerical\n# Shortcut list comprehension method\nother_col = [c for c in train_raw_copy.columns if c not in BinaryLevel_cat_col + NominalLevel_bin_col]\nother_col.remove('id')\nother_col.remove('target')\nOrdinalLevel_other_col = [c for c in other_col if train_raw_copy[c].dtypes == 'int64']\nIntervalLevel_other_col = [c for c in other_col if train_raw_copy[c].dtypes == 'float64']","execution_count":32,"outputs":[]},{"metadata":{"_uuid":"df7f090ef19864e89cbed9cd4857b08a6d8afe34"},"cell_type":"markdown","source":"# Now to check again what we have cleaned up so far!"},{"metadata":{"_uuid":"2e9adb8f384654d0bd79d8eba2d377dbe7704bea"},"cell_type":"markdown","source":"*Unhide to view output"},{"metadata":{"_kg_hide-output":true,"trusted":true,"_uuid":"9383fb9b14fcd60403fda9d4d7450469a0b9b074","collapsed":true},"cell_type":"code","source":"basic_details(train_raw_copy)","execution_count":34,"outputs":[]},{"metadata":{"_uuid":"58a6c09b9de7d9e49b491da17ae21e189afa420f","_cell_guid":"5dfc2b5b-dff2-4803-87ee-0ab751939677"},"cell_type":"markdown","source":"**4.Prepare Data - A**\n\nHere we prepare the data by setting or assigning variables. This makes things easier when we chart graphs, conduct feature selection, model the dataset as we proceed."},{"metadata":{"_uuid":"acdfe955c6946e4b26ec892a4bc90359bd8ce120","collapsed":true,"_cell_guid":"906a7c2c-63c9-4c8e-b10f-fcb41eb9f633","trusted":true},"cell_type":"code","source":"# Break-Down WITHOUT 'id' & 'target'\n# Categorical_cat\nCategorical = BinaryLevel_cat_col\n# Binary_bin\nBinary = NominalLevel_bin_col\n# Integer_'int'_Ordinal\nInteger = OrdinalLevel_other_col\n# Real_'float'_Interval\nReal = IntervalLevel_other_col\n\n\n# Original\nOriginal_All_w = train_raw_copy.columns.get_values().tolist()\n# Original WITHOUT 'id' & 'target'\nOriginal_All_wo = [c for c in train_raw.columns if c not in ['id', 'target']]\n\n\n# Converted Dtypes WITHOUT FeatureEngineering\nConverted_dtypes_All_wo = Categorical + Binary + Integer + Real\n\n\n# For Graph Chart Plots\n# W/O 'id' & 'target'\nCategorical_Chart_wo = Categorical\nBinary_Chart_wo = Binary\nInteger_Chart_wo = Integer\nReal_Chart_wo = Real\n\n\n# For Feature Selection /OR Interaction Building /OR Pre- Model Benchmarks\nFeatures_PreSelect_Original = Original_All_wo\nFeatures_PreSelect = Converted_dtypes_All_wo","execution_count":35,"outputs":[]},{"metadata":{"_uuid":"d07da08129b369398f9afc8d7dc9edb6d00aae77"},"cell_type":"markdown","source":"Final Checks on Data for Model\n\n*Unhide to view output"},{"metadata":{"_uuid":"97a67fffd7210e240fb522c73571c89f788001c8","_cell_guid":"246f18bd-f31e-4d2d-9d54-d5d4c7cae4ca","trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"# Missing values\nprint(train_raw_copy.isnull().sum())\nprint(test_raw.isnull().sum())","execution_count":36,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"_uuid":"6318ce131f222acefd2df7f9e3e829b29998547b","collapsed":true},"cell_type":"code","source":"# Stats\nbasic_details(train_raw_copy)\nbasic_details(test_raw)","execution_count":37,"outputs":[]},{"metadata":{"_uuid":"9f74e9b3f9ba11e18d9f9fd05c87ef177920a33a","_cell_guid":"9bfbab9a-9a52-4050-8419-99bbd2692e04"},"cell_type":"markdown","source":"**5.Exploratory Data Analysis (EDA)**\n\nNow with a neaten data-set it, we can proceed to do some EDA to get a clearer idea of what relationships or abnormal relationships we have from the features. These may include outliers, a skewed data, reasonableness checks, feature selection etc. "},{"metadata":{"_uuid":"f16655a2aebcd8919bc82fa04eef171745ab5c03"},"cell_type":"markdown","source":"**5.1** Balance of dataset (Target variable)"},{"metadata":{"trusted":true,"_uuid":"d999aa56f65bc911b8d5cfd155546da9d390b9de","collapsed":true},"cell_type":"code","source":"\"\"\"target (i.e.Target Variable)\"\"\"\nprint(\"Exploring target (i.e.Target Variable)...\")\n\n# List Comprehension\nclass_0 = [c for c in train_raw_copy['target'] if c == 0]\nclass_1 = [c for c in train_raw_copy['target'] if c == 1]\n# # Alternative Mask Method\n# class_0 = train_raw_copy.SeriousDlqin2yrs.value_counts()[0]\n# class_1 = train_raw_copy.SeriousDlqin2yrs.value_counts()[1]\n\nclass_0_count = len(class_0)\nclass_1_count = len(class_1)\n\nprint(\"Target Variable Balance...\")\nprint(\"Total number of class_0: {}\".format(class_0_count))\nprint(\"Total number of class_1: {}\".format(class_1_count))\nprint(\"Event rate: {} %\".format(round(class_1_count/(class_0_count+class_1_count) * 100, 3)))   # round 3.dp\nprint('-' * ZZ)\n\n# Plot\nsns.countplot(\"target\", data=train_raw_copy)\nplt.show()","execution_count":41,"outputs":[]},{"metadata":{"_uuid":"f4ed638bb356aa63edf3542020870e644b4f69fe"},"cell_type":"markdown","source":"Quick Commentary: \n\nNot good...we have a very unbalanced dataset.... However, this isn't the objective of this Kernel. Hence, we will have to take it as it is. The focus is still on comparing ensemble models! \n\nMoving on!"},{"metadata":{"_uuid":"feb1f7c56cf15f371a83e2e2d98f1daaf252fc08"},"cell_type":"markdown","source":"**5.2** Uni-variates"},{"metadata":{"_uuid":"72b8885ba1f1d469e8191ceb3c72bc9e0b310493","_cell_guid":"624c823c-9f1b-4e74-aff3-e468841169f6"},"cell_type":"markdown","source":"Univariate - Categorical\n"},{"metadata":{"_uuid":"b556b011e4c029f2cffebf473945eabd0dd77e3b","_cell_guid":"d22098e8-9818-48cd-aaa8-a18691067208","trusted":true,"collapsed":true},"cell_type":"code","source":"# Bar Plot # N/A\n# Density Plot  # Chosen as opposed to histogram since this doesnt need bins parameter\nprint(\"Plotting Density Plot...for Categorical\")\ni = 0\n\n# Single out the 'target' & those that are not for easy reference\nt1 = train_raw_copy.loc[train_raw_copy['target'] != 0]\nt0 = train_raw_copy.loc[train_raw_copy['target'] == 0]\n\nsns.set_style('whitegrid')\n# plt.figure()\nfig, ax = plt.subplots(4, 4, figsize=(8, 8))\n\nfor feature in BinaryLevel_cat_col:\n    i += 1\n    plt.subplot(4, 4, i)\n    sns.kdeplot(t1[feature], bw=0.5, label=\"target = 1\")\n    sns.kdeplot(t0[feature], bw=0.5, label=\"target = 0\")\n    plt.ylabel('Density plot', fontsize=10)\n    plt.xlabel(feature, fontsize=10)\n    locs, labels = plt.xticks()\n    plt.tick_params(axis='both', which='major', labelsize=10)\nplt.show()","execution_count":38,"outputs":[]},{"metadata":{"_uuid":"6292d5eb2e3262e6a1b18a3e4d0c507e6cdead24"},"cell_type":"markdown","source":"Quick Commentary: we can easily see ps_car_11_cat is highly skewed."},{"metadata":{"_uuid":"e994c282b8e201759bbfb6ffb3867c6c564b6bb0","_cell_guid":"dcda106b-2630-4e15-90c2-32919b97e5e6"},"cell_type":"markdown","source":"Univariate - Binary"},{"metadata":{"_uuid":"6973ab8e2128badd92f6e21d845c78bcbf5a6f7a","_cell_guid":"6d964e93-81a1-43e8-bb14-dc77d45cdb02","trusted":true,"collapsed":true},"cell_type":"code","source":"# Bar Plot\"\"\"   # N/A\n# Density Plot\"\"\"  # Chosen\nprint(\"Plotting Density Plot...for Nominal\")\ni = 0\nt1 = train_raw_copy.loc[train_raw_copy['target'] != 0]\nt0 = train_raw_copy.loc[train_raw_copy['target'] == 0]\n\nsns.set_style('whitegrid')\n# plt.figure()\nfig, ax = plt.subplots(4, 4, figsize=(8, 8))\n\nfor feature in NominalLevel_bin_col:\n    i += 1\n    plt.subplot(4, 4, i)\n    sns.kdeplot(t1[feature], bw=0.5, label=\"target = 1\")\n    sns.kdeplot(t0[feature], bw=0.5, label=\"target = 0\")\n    plt.ylabel('Density plot', fontsize=10)\n    plt.xlabel(feature, fontsize=10)\n    locs, labels = plt.xticks()\n    plt.tick_params(axis='both', which='major', labelsize=10)\nplt.show()","execution_count":39,"outputs":[]},{"metadata":{"_uuid":"d8337f01aac566b82de1e1537ff3e477a05513f3"},"cell_type":"markdown","source":"Quick Commentary: Nothing abnormal"},{"metadata":{"_uuid":"5a3e195e7cb1606470bbe03ea64c9a4156b37ae4","_cell_guid":"30bf4c53-dd69-4904-b09a-932e66d76742"},"cell_type":"markdown","source":"Univariate - Ordinal / Int"},{"metadata":{"_uuid":"4e3d209d23023bbd9575be72595ad9f12d8fccd4","_cell_guid":"8ac57c48-b623-41a5-9bb0-8d8b7cf0fdf1","trusted":true,"collapsed":true},"cell_type":"code","source":"# Bar Plot\"\"\"   # N/A\n# Density Plot\"\"\"   # N/A\n# Violin Plot\"\"\"   # Chosen\nprint(\"Plotting Violin Plot...for Ordinal_Int\")\nsns.set_style(\"whitegrid\")  # Chosen\nfor col in OrdinalLevel_other_col:\n    ax = sns.violinplot(x=\"target\", y=col, data=train_raw_copy)\n    plt.show()","execution_count":40,"outputs":[]},{"metadata":{"_uuid":"b0e1d8b3b121500f97d77d6aced23e1a071aa1b5"},"cell_type":"markdown","source":"Quick Commentary: ps_ind_14 seems highly skewed too..."},{"metadata":{"_uuid":"65bfc53d4ae88215d451bbb74a7241f20e5477f6","_cell_guid":"ace8dcdd-31c7-4537-8969-1df7e95e9f5e"},"cell_type":"markdown","source":"Univariate - Interval / Float"},{"metadata":{"_uuid":"b7170f60d76388cab54ae3e341215dd5b7247a47","_cell_guid":"d82cc4d7-8ab9-4cf1-8b4e-97258eb2e2b2","trusted":true,"collapsed":true},"cell_type":"code","source":"# Bar Plot\"\"\"   # N/A\n# Density Plot\"\"\"   # N/A\n# Violin Plot\"\"\"   # Chosen\nprint(\"Plotting...for Interval_Float\")\nsns.set_style(\"whitegrid\")  # Chosen\nfor col in IntervalLevel_other_col:\n    ax = sns.violinplot(x=\"target\", y=col, data=train_raw_copy)\n    plt.show()","execution_count":42,"outputs":[]},{"metadata":{"_uuid":"d3a942ec52107dfb709b1f7f50baf71b1668b80d"},"cell_type":"markdown","source":"Quick Commentary: ps_car_12 & 13 & 15 seems highly skewed as well..."},{"metadata":{"_uuid":"328f800f8c7d28f7c03325e55a3fb27ccaca2062"},"cell_type":"markdown","source":"Uni-variates Summative: Highly skewed (ps_car_11_cat, ps_ind_14, ps_car_12 & 13 & 15)\n\nLets keep that in mind and note them later during the model ensemble comparisons for Feature Importance."},{"metadata":{"_uuid":"2fec454e3724a223f9cf67c19bbdf8162164c4f1"},"cell_type":"markdown","source":"**5.3** Bi-variates\n\nDo note that I have only taken a truncated sample size of 800 quantity of samples to reduce the computational costs when using the entire dataset."},{"metadata":{"_uuid":"c5f29085b95d033ecc7c98e93772aa77b3b09a42","_cell_guid":"d33f50c5-6be9-45a2-9add-3191a6449662"},"cell_type":"markdown","source":"Bivariate - Categorical"},{"metadata":{"_uuid":"4a8f7a4e675ea9178661a4ae5868a83f61797be7","_cell_guid":"3cdef48b-fe80-48aa-9b48-47e45fcf2a76","trusted":true,"collapsed":true},"cell_type":"code","source":"# Set sample size to reduce computational cost\nsample_SIZE = 800\nsample = train_raw_copy.sample(sample_SIZE)\nBinaryLevel_cat_col.extend(['target'])  # Add 'target' into list\nvar = BinaryLevel_cat_col\nsample = sample[var]\ng = sns.pairplot(sample,  hue='target', palette='Set1', size=1, diag_kind='kde', plot_kws={\"s\": 8})\nplt.show()\nBinaryLevel_cat_col.remove('target')  # Remove 'target' into list","execution_count":43,"outputs":[]},{"metadata":{"_uuid":"67aeefe5212bf389cf4de387e275e8ec2ca0d955"},"cell_type":"markdown","source":"Quick Commentary: Doesn't seem to bear any clear collinearity"},{"metadata":{"_uuid":"e65eacd1927064440eae3f6314a9bdde16558731","_cell_guid":"23a917a5-1ad2-4e7c-a861-9b5f67d01512"},"cell_type":"markdown","source":"Bivariate - Binary"},{"metadata":{"_uuid":"b0a9ac39e89698a3b11c851214c3dde0b3ba3f73","_cell_guid":"8b524f5a-2a73-4388-813c-3d443796c26a","trusted":true,"collapsed":true},"cell_type":"code","source":"# Set sample size to reduce computational cost\nsample_SIZE = 800\nsample = train_raw_copy.sample(sample_SIZE)\nNominalLevel_bin_col.extend(['target'])  # Add 'target' into list\nvar = NominalLevel_bin_col\nsample = sample[var]\ng = sns.pairplot(sample,  hue='target', palette='Set1', size=1, diag_kind='kde', plot_kws={\"s\": 8})\nplt.show()\nNominalLevel_bin_col.remove('target') # Remove to revert to original","execution_count":44,"outputs":[]},{"metadata":{"_uuid":"8e439bd4f5af11fa7e4f97bc638259c4222452ef","_cell_guid":"e578ec1a-dfc8-45bc-b505-2e1973838002"},"cell_type":"markdown","source":"We can hardly see anything here!!!!!! Let's try switching to a heatmap instead."},{"metadata":{"_uuid":"9f09cc6eccdfae1ebf65dfdfd85d7e276879abe6","_cell_guid":"b156d7a0-6173-44da-a9a0-8390a171b41e","trusted":true,"collapsed":true},"cell_type":"code","source":"cor = train_raw_copy[NominalLevel_bin_col].corr()\nplt.figure(figsize=(12, 9))\nsns.heatmap(cor,)\nplt.show()","execution_count":45,"outputs":[]},{"metadata":{"_uuid":"eeb3b8911b9eecd2287864893b8f4a4b67af0af5"},"cell_type":"markdown","source":"Quick Commentary: Ok since no correlations above 0.3"},{"metadata":{"_uuid":"df7e1aafa9270f35dab40caeef1a58e62e9c68c9","_cell_guid":"829c5aad-64b0-4dce-9ba9-a27f97516cd9"},"cell_type":"markdown","source":"Bivariate - Ordinal / Int"},{"metadata":{"_uuid":"4b9f3b98921f3835629364f930c36ab69ea1e423","_cell_guid":"7a8dc565-34ad-4f40-b4c0-eaeda51cb387","trusted":true,"collapsed":true},"cell_type":"code","source":"# Set sample size to reduce computational cost\nsample_SIZE = 800\nsample = train_raw_copy.sample(sample_SIZE)\nOrdinalLevel_other_col.extend(['target'])  # Add 'target' into list\nvar = OrdinalLevel_other_col\nsample = sample[var]\ng = sns.pairplot(sample,  hue='target', palette='Set1', size=1, diag_kind='kde', plot_kws={\"s\": 8})\nplt.show()\nOrdinalLevel_other_col.remove('target') # Remove to revert to original","execution_count":46,"outputs":[]},{"metadata":{"_uuid":"532f6d16add250ad21f83f9d7fa1ce81169ea2f3"},"cell_type":"markdown","source":"Quick Commentary: Doesn't seem to bear any clear collinearity"},{"metadata":{"_uuid":"6a6da3e5d62709ad79eaf152dcc2e204512c32f1","_cell_guid":"585d366e-4c86-4df7-90f3-ddd5b0da0ffe"},"cell_type":"markdown","source":"Bivariate - Interval / Float"},{"metadata":{"_uuid":"f085803aec67dd8b493e71bc34deac88ebe9a592","_cell_guid":"8e7ab18f-bdfd-49e4-8697-64aa57f0dbd1","trusted":true,"collapsed":true},"cell_type":"code","source":"# Set sample size to reduce computational cost\nsample_SIZE = 800\nsample = train_raw_copy.sample(sample_SIZE)\nIntervalLevel_other_col.extend(['target'])  # Add 'target' into list\nvar = IntervalLevel_other_col\nsample = sample[var]\ng = sns.pairplot(sample,  hue='target', palette='Set1', size=1, diag_kind='kde', plot_kws={\"s\": 8})\nplt.show()\nIntervalLevel_other_col.remove('target') # Remove to revert to original","execution_count":47,"outputs":[]},{"metadata":{"_uuid":"a3a7dc554e755d9274326f769d3d17c2b0db4b2e"},"cell_type":"markdown","source":"Quick Commentary: \n\nps_reg_01 & ps_reg_02 seem to bear some positive linear relationship\n\nps_car_12 & ps_car_13 seem to bear some positive linear relationship\n\nps_car_15 & ps_car_13 seem to bear some strong exponential relationship\n\nJust as before, we will note this during the model ensemble comparisons for Feature Importance."},{"metadata":{"_uuid":"020183e6023890b8255a04ef3dc13a374307037e","_cell_guid":"7f643c8f-0c7a-4ae0-ae8e-6d97296ee227"},"cell_type":"markdown","source":"Multivariate"},{"metadata":{"_uuid":"ceb9927d545f28aee711d5bdde28447731e3005c","_cell_guid":"455b6cc7-1c8f-404f-ba42-274319eeaec1","trusted":true,"collapsed":true},"cell_type":"code","source":"cor = train_raw_copy[Features_PreSelect].corr()\nplt.figure(figsize=(12, 9))\nsns.heatmap(cor)\nplt.show()","execution_count":48,"outputs":[]},{"metadata":{"_uuid":"4dc03455e9e704681b8a6a428c29ac0973bee29f"},"cell_type":"markdown","source":"Quick Commentary: Ok since no exceptionally high correlations. Except for ps_car_04_cat against ps_car_12 & 13."},{"metadata":{"_uuid":"13a4568ac413d7dff85a5832bf784ba1b85c34d5"},"cell_type":"markdown","source":"# 5.4 EDA Summative notes for Feature Importance comparison\n\nUni-variate: \nps_car_11_cat <> \nps_ind_14 <> \nps_car_12 <> \nps_car_13 <> \nps_car_15\n\nBi-Variate: \nps_reg_02 & ps_reg_02 <> \nps_car_12 & ps_car_13 <> \nps_car_15 & ps_car_13\n\nMulti-variate: \nps_car_04_cat & ps_car_12 <> \nps_car_04_cat & ps_car_13"},{"metadata":{"_uuid":"43c510a227145e0a91f95693faa33f92915f5aa6","_cell_guid":"3470934e-af3b-472e-844c-580ef1526eb2"},"cell_type":"markdown","source":"**6.Parameter Tuning**\n\nYou could run this, but I've left it out due to the huge computational cost that comes with it. \n\nIt's simply recursively running the code for each param_test and logging down the best parameters."},{"metadata":{"_uuid":"0b1b55c1696e67b0d9741023926b9d4f535ba8a0","collapsed":true,"_cell_guid":"7d3d0127-e485-4657-8200-a4c280522704","trusted":false},"cell_type":"code","source":"##################Lasso Parameter C Tuning\n# # COMMENT: Best Parameter was found as {'logisticregression__C': 0.1}\n#\n# from sklearn.pipeline import make_pipeline\n# from sklearn.model_selection import GridSearchCV\n#\n# X = train_raw_copy.drop(['id', 'target'], axis = 1)\n# y = train_raw_copy['target']\n#\n# # # {'logisticregression__C': [1, 10, 100, 1000]\n# param_grid = {'logisticregression__C': [0.001, 0.01, 0.1, 1, 10, 100]}\n# pipe = make_pipeline(StandardScaler(), LogisticRegression(penalty='l1'))\n# grid = GridSearchCV(pipe, param_grid, cv=10)\n# grid.fit(X, y)\n# print(grid.best_params_)\n#\n#\n#\n# #################XGB Classifier Tuning\n# from sklearn.model_selection import GridSearchCV\n# from xgboost.sklearn import XGBClassifier\n# from sklearn.preprocessing import StandardScaler\n\n# # Substitute this after exery run for new parameter grid to test\n# param_test1 = {\n#  'classifier__max_depth': range(3, 10, 2),\n#  'classifier__min_child_weight': range(1, 6, 2)\n# }\n#\n# param_test2 = {\n#  'classifier__gamma': [i/10.0 for i in range(0, 5)]\n# }\n#\n# param_test3 = {\n#  'classifier__learning_rate': [0.1, 0.01, 0.001],\n#  'classifier__n_estimators=100': [100, 140, 200]\n# }\n#\n# #Log down the best parameters\n# # 'classifier__gamma': 0,\n# # 'classifier__max_depth': 7,\n# # 'classifier__min_child_weight': 5\n#       \n# print(\"Tuning XGBClassifier Parameters\")\n# #\n# from sklearn.pipeline import make_pipeline\n# from sklearn.pipeline import Pipeline\n# print(\"Making XGBClassifier-Pipeline...\")\n# pipeXGBC = Pipeline([('scaler', StandardScaler()),\n#                       ('classifier', XGBClassifier(gamma=0, max_depth=7, min_child_weight=5))])\n# print(\"Running XGBClassifier-Pipeline Parameters GridSearchCV...\")\n# gsearchXGBC2 = GridSearchCV(pipeXGBC, cv=5, param_grid=param_test3)\n# print(\"Fitting XGBClassifier-Pipeline Parameters GridSearchCV...\")\n# gsearchXGBC2.fit(X_train, y_train)\n# print(\"Running XGBClassifier-Pipeline GridSearchCV Scores...\")\n# print(gsearchXGBC2.cv_results_, gsearchXGBC2.best_params_, gsearchXGBC2.best_score_)\n# print(\"Running XGBClassifier-Pipeline Best Estimator...\")\n# best_gridXGBC2 = gsearchXGBC2.best_estimator_\n# print(best_gridXGBC2)\n#\n#\n# #################Random Forest Classifier Tuning\n# # Create the parameter grid based on the results of random search\n# param_grid0 = {\n#      'bootstrap': [True],\n#      'max_depth': [80, 90, 100, 110],\n#      'max_features': [2, 3],\n#      'min_samples_leaf': [3, 4, 5],\n#      'min_samples_split': [8, 10, 12],\n#      'n_estimators': [100, 200, 300, 1000]\n# }\n#\n# param_grid1 = {\n#     'classifier__bootstrap': [True],\n#     'classifier__max_depth': [80, 100],\n#     'classifier__max_features': [2, 4],\n#     'classifier__min_samples_leaf': [4],\n#     'classifier__min_samples_split': [10],\n#     'classifier__n_estimators': [100, 200]\n# }\n#\n# param_grid2 = {\n#     'classifier__min_samples_leaf': [3, 5],\n#     'classifier__min_samples_split': [10],\n#     'classifier__n_estimators': [100, 200]\n# }\n#\n# #Log down the best parameters\n# #    'classifier__bootstrap': [True],\n# #    'classifier__max_depth': [80],\n# #    'classifier__max_features': [2],\n#\n# from sklearn.pipeline import make_pipeline\n# from sklearn.pipeline import Pipeline\n# from sklearn.preprocessing import StandardScaler\n# print(\"Making RFClassifier-Pipeline...\")\n# pipeRFC2 = Pipeline([('scaler', StandardScaler()),\n#                      ('classifier', RandomForestClassifier(bootstrap=True, max_depth=80, max_features=2,\n#                                                            criterion='entropy'))])\n# print(\"Running RFClassifier-Pipeline Parameters GridSearchCV...\")\n# gsearchRFC2 = GridSearchCV(pipeRFC2, cv=5, param_grid=param_grid2)\n# print(\"Fitting RFClassifier-Pipeline Parameters GridSearchCV...\")\n# gsearchRFC2.fit(X_train, y_train)\n# print(\"Running RFClassifier-Pipeline GridSearchCV Scores...\")\n# print(gsearchRFC2.cv_results_, gsearchRFC2.best_params_, gsearchRFC2.best_score_)\n# print(\"Running RFClassifier-Pipeline Best Estimator...\")\n# best_gridXGBC2 = gsearchRFC2.best_estimator_\n# print(best_gridXGBC2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"028863b1febb88c3638e7ee9019244d89a978762","_cell_guid":"cc608251-e1a0-426a-98ac-f3d0e0405b1b"},"cell_type":"markdown","source":"**7.Feature Select - A**\n\nThis segment simply uses LASSO regression via L1 penalty of Logistic Regression to validate the initial feature dropping during \n3.'Data Cleaning' C1 - Correction\n\nAs mentioned earlier in part'1, this is the part where we use the original control dataset 'train_raw'.\n\nIn short, we are comparing the LASSO accuracy scores before VS after the drop. Ideally, we should see that we have no difference in accuracy scores. Hence, indicating that the drop had no effect on the model accuracy."},{"metadata":{"_uuid":"348d7a782d4c1fe689f0183c446ad3141d310958"},"cell_type":"markdown","source":"**7.1 **Pre-Drop Accuracy Score"},{"metadata":{"_uuid":"654e5b4df9dba945a0f534b516f524ed73c1bcf8","_cell_guid":"e144a1f0-54f3-4de1-ab61-34b942ab331d","trusted":true,"collapsed":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n############# PRE DROPPING FEATURES\n##### Organizing to validate C1 Drop\n# ONLY C2->Fillna step iterated. NO COLUMNS DROPPED\nTempToBeFilled = [c for c in train_raw.columns if train_raw[c].isnull().sum() > 0]\nfor col in TempToBeFilled:\n    train_raw[col].fillna(train_raw[col].mode()[0], inplace=True)\n\ntrain_x1 = train_raw.drop(columns=['id', 'target'])      \nY1 = train_raw['target'].values\n\n# Preparing train/test split of dataset            \nX_train, X_validation, y_train, y_validation = train_test_split(train_x1, Y1, train_size=0.9, random_state=1234)\n\n##### Instantiate Logistic Regression \nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\n\n# Transform data for LogRef fitting\"\"\"\nscaler = StandardScaler()\nstd_data = scaler.fit_transform(X_train.values)\n\n# Establish Model\nRandomState=42\nmodel_LogRegLASSO1 = LogisticRegression(penalty='l1', C=0.1, random_state=RandomState, solver='liblinear', n_jobs=1)\nmodel_LogRegLASSO1.fit(std_data, y_train)\n\n# Run Accuracy score without any dropping of features\nprint(\"PRE DROPPING FEATURES: Running LASSO Accuracy Score without features drop...\")\n# make predictions for test data and evaluate\ny_pred = model_LogRegLASSO1.predict(X_validation)\npredictions = [round(value) for value in y_pred]\naccuracy = accuracy_score(y_validation, predictions)\nprint(\"PRE Accuracy: %.2f%%\" % (accuracy * 100.0))","execution_count":49,"outputs":[]},{"metadata":{"_uuid":"798efad60f63235680ce39a674a76d4554c87f1d"},"cell_type":"markdown","source":"**7.2 **Post-Drop Accuracy Score"},{"metadata":{"trusted":true,"_uuid":"0d5313020007e6042238edea7ca798caced014c2","collapsed":true},"cell_type":"code","source":"############# POST DROPPING FEATURES\ntrain_x2 = train_raw_copy[Features_PreSelect]   \nY2 = train_raw_copy['target'].values  \n\n# Preparing train/test split of dataset            \nX_train, X_validation, y_train, y_validation = train_test_split(train_x2, Y2, train_size=0.9, random_state=1234)\n\n##### Instantiate Logistic Regression \nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\n\n# Transform data for LogRef fitting\"\"\"\nscaler = StandardScaler()\nstd_data = scaler.fit_transform(X_train.values)\n\n# Establish Model\n\nmodel_LogRegLASSO1 = LogisticRegression(penalty='l1', C=0.1, random_state=RandomState, solver='liblinear', n_jobs=1)\nmodel_LogRegLASSO1.fit(std_data, y_train)\n\n# Run Accuracy score without any dropping of features\nprint(\"POST DROPPING FEATURES: Running LASSO Accuracy Score with features dropped...\")\n# make predictions for test data and evaluate\ny_pred = model_LogRegLASSO1.predict(X_validation)\npredictions = [round(value) for value in y_pred]\naccuracy = accuracy_score(y_validation, predictions)\nprint(\"POST Accuracy: %.2f%%\" % (accuracy * 100.0))","execution_count":50,"outputs":[]},{"metadata":{"_uuid":"bb4010ac55771d642fecb600d73e20788e62888d","_cell_guid":"df2a44cc-38e8-4f75-9ff1-aa4d70a7306f"},"cell_type":"markdown","source":"It's right both are 96.22%! No difference in Accuracy Scores! Moving on..."},{"metadata":{"_uuid":"1e655cf9fc5cddeaf1ef597b56ca95a6b9904fcf","_cell_guid":"8b131187-cfcd-4de5-92b2-4c65ca138683"},"cell_type":"markdown","source":"# 8.Feature Select & Individual Model Charting - B\n\nNow we will do the similar approach as before in part'A, but include extra model of a mix between BlackBox & WhiteBox models. Note that now we are resuming to without the 'calc' named features since we just validated that it wont affect accuracy scores.\n\n    /////I didn't use a bulk loop iteration hear as I kept getting the \"insufficient memory error\", Hence, had to split them up individually. But interestingly I could do it with the ROC AUC segment later on...If anyone knows a solution to this, do let me know! Much appreciated!/////\n\nWhiteBox Models> \nLASSO, Ridge, Logistic Regression Balanced weighted\n\nBlackBox Models>\nExtreme Gradiant Boosting Classifier, Random Forest Classifier"},{"metadata":{"_uuid":"a284d16f3518f25d92daa42c61e2c7863fb5f2bd"},"cell_type":"markdown","source":"# 8.1 Model Preparation\n\nWe will now prepare the model by first train/test splitting our dataset. Here we intentionally ignore 10% of the train set as a measure to avoid over-fitting.\n\nWe also establish a common function to scale all our features evaluation metrics to a common scaling range for comparative purposes. As this Kernel focuses on that! *Unhide to view output"},{"metadata":{"_uuid":"ba1150cb26f67d8604cc7d647908226087ff60a3","_cell_guid":"4b265134-fbb0-4598-9a0f-e6658670bf30","trusted":true,"collapsed":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\n# Preparing train/test split of dataset\ntrain_x = train_raw_copy[Features_PreSelect]   \nY = train_raw_copy['target'].values             \nX_train, X_validation, y_train, y_validation = train_test_split(train_x, Y, train_size=0.9, random_state=1234)\n\n# Preparing Side to Side Comparative Function\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Generic Function to Normalize Rankings/Coefficients\ndef rank_to_dict(ranks, names, order=1):\n    minmax = MinMaxScaler()\n    # Transposes array of 'ranks' into single column array, then applies Fit_Transforms with MinMax\n    ranks = minmax.fit_transform(order*np.array([ranks]).T).T[0]\n    # shortcut map & lambda function to round ranks to 2 precision dp\n    # Altenatively, You can use a list comprehension here as well. \n    # *See Mean rounding code at Chapter 9.Features (Side To Side Comparison) for example*\n    ranks = map(lambda x: round(x, 2), ranks)   \n    # Returns names with each respective rounded ranks\n    return dict(zip(names, ranks))\n\nnames = Features_PreSelect\nranks = {}\n\nprint('Prep done...')","execution_count":51,"outputs":[]},{"metadata":{"_uuid":"edcdc18cebdd7f00ec8657006cfdbe94be8da3df","_cell_guid":"dc44a99e-6818-42cd-bb75-5dff9c2e7d20"},"cell_type":"markdown","source":"# 8.2 LASSO\n\n*Unhide to view output"},{"metadata":{"_uuid":"5568f6a98ff77cc202f3a34cd96dbd98a7751676","_cell_guid":"80ec4dd2-30cf-42d8-b639-6ad8ec5d87a1","trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"# LASSO via LogisticRegression l1 penalty - WhiteBox Model\nprint('Running LASSO via LogisticRegression l1 penalty...')\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\n# Transform data for LogReg fitting\nscaler = StandardScaler()\nstd_data = scaler.fit_transform(X_train.values)\n\n# Establish Model\nmodel_LogRegLASSO = LogisticRegression(penalty='l1', C=0.1, random_state=RandomState, solver='liblinear', n_jobs=1)\nmodel_LogRegLASSO.fit(std_data, y_train)\n\n# For Side To Side\nranks[\"LogRegLASSO\"] = rank_to_dict(list(map(float, model_LogRegLASSO.coef_.reshape(len(Features_PreSelect), -1))),\n                                    names, order=1)\nprint(ranks[\"LogRegLASSO\"])\n\n\n######Alternative Direct Methods:\n\n### Method 1 without Coefficients shown\n#from sklearn.feature_selection import SelectFromModel\n#model = SelectFromModel(model_LogRegLASSO, prefit=True)\n#X_new = model.transform(X_train)\n#print(\"New Shape\", X_new.shape)\n#print(\"Old Shape\", X_train.shape)\n\n\n### Method 2 with Coefficients shown\n# Set df to append\n#zero_feat = []\n#nonzero_feat = []\n\n# Loop through feature coefficients & append accordingly\n#num_features = len(X_train.columns)\n#for i in range(num_features):\n#    coef = model_LogRegLASSO.coef_[0, i]\n#    if coef == 0:\n#        zero_feat.append(X_train.columns[i])\n#    else:\n#        nonzero_feat.append((coef, X_train.columns[i]))\n#print('Features that have coefficient of 0 are: ', zero_feat, '\\n')\n#print('Features that have non-zero coefficients are:')\n#print(sorted(nonzero_feat, reverse=True))","execution_count":53,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dbade7fee697cd19344c30eea8d4ce464222d1e8","collapsed":true},"cell_type":"code","source":"# Plotting\nimport operator\nlistsLASSO = sorted(ranks[\"LogRegLASSO\"].items(), key=operator.itemgetter(1))\n# convert list>array>dataframe\ndfLASSO = pd.DataFrame(np.array(listsLASSO).reshape(len(listsLASSO),2), columns = ['Features','Ranks']).sort_values('Ranks') \ndfLASSO['Ranks']=dfLASSO['Ranks'].astype(float)\n#df.sort_values('Ranks', ascending=True)\n\ndfLASSO.plot.bar(x='Features', y='Ranks', color='blue')\n#plt.xticks(rotation='vertical')\nplt.xticks(rotation=90)\n\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 7, 10\nplt.show()","execution_count":91,"outputs":[]},{"metadata":{"_uuid":"4c64722c335ce0a505e554449b238cf52edfd98d","_cell_guid":"5e93399e-6dee-41a1-b679-3929fe12438e"},"cell_type":"markdown","source":"# 8.3 Ridge\n\n*Unhide to view output"},{"metadata":{"_uuid":"d038bbdc2939214e6428867c0de60485f7513b1c","_cell_guid":"3e2eae9b-81f3-4cac-87e2-dacfe9d84a44","trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"# Ridge via LogisticRegression l2 penalty - WhiteBox Model\nprint('Running Ridge via LogisticRegression l2 penalty...')\n# Establish Model\nmodel_LogRegRidge = LogisticRegression(penalty='l2', C=0.1, random_state=RandomState, solver='liblinear', n_jobs=1)\nmodel_LogRegRidge.fit(std_data, y_train)\n\n# For Side To Side\nranks[\"LogRegRidge\"] = rank_to_dict(list(map(float, model_LogRegRidge.coef_.reshape(len(Features_PreSelect), -1))),\n                                    names, order=1)\nprint(ranks[\"LogRegRidge\"])","execution_count":55,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"adf83f94e776b0bb01dccf48cbc6cdad1d434d98","collapsed":true},"cell_type":"code","source":"# Plotting\nimport operator\nlistsRidge = sorted(ranks[\"LogRegRidge\"].items(), key=operator.itemgetter(1))\ndfRidge = pd.DataFrame(np.array(listsRidge).reshape(len(listsRidge),2), columns = ['Features','Ranks']).sort_values('Ranks') # convert list>array>dataframe\ndfRidge['Ranks']=dfRidge['Ranks'].astype(float)\n#df.sort_values('Ranks', ascending=True)\n\ndfRidge.plot.bar(x='Features', y='Ranks', color='blue')\n#plt.xticks(rotation='vertical')\nplt.xticks(rotation=90)\n\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 12, 8\nplt.show()","execution_count":103,"outputs":[]},{"metadata":{"_uuid":"699ac8e84ddb5442e54fc98359961ca6e29ef50b","_cell_guid":"c0c0d5c3-4656-4649-95e1-1fb7e2d65349"},"cell_type":"markdown","source":"# 8.4 Logistic Regression Balance weighted\n\n*Unhide to view output"},{"metadata":{"_uuid":"97d46ed07eb3476e8673105653ee943c39b63891","_cell_guid":"b5a75211-55b9-4f3a-b1fa-2c39422b56c4","trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"# LogisticRegression Standard 'Balanced' weighted - WhiteBox Model\nprint('RunningLogisticRegression Balanced...')\n# Establish Model\nmodel_LogRegBalance = LogisticRegression(class_weight='balanced', C=0.1, random_state=RandomState, solver='liblinear',\n                                         n_jobs=1)\nmodel_LogRegBalance.fit(std_data, y_train)\n\n# For Side To Side\nranks[\"LogRegBalance\"] = rank_to_dict(list(map(float, model_LogRegBalance.coef_.reshape(len(Features_PreSelect), -1))),\n                                      names, order=1)\nprint(ranks[\"LogRegBalance\"])","execution_count":57,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7b0d2db34b391d1b338cd027ff69457fc2262506","collapsed":true},"cell_type":"code","source":"#Plotting\nimport operator\nlistsBal = sorted(ranks[\"LogRegBalance\"].items(), key=operator.itemgetter(1))\ndfBal = pd.DataFrame(np.array(listsBal).reshape(len(listsBal),2), columns = ['Features','Ranks']).sort_values('Ranks') # convert list>array>dataframe\ndfBal['Ranks']=dfBal['Ranks'].astype(float)\n#df.sort_values('Ranks', ascending=True)\n\ndfBal.plot.bar(x='Features', y='Ranks', color='blue')\n#plt.xticks(rotation='vertical')\nplt.xticks(rotation=90)\n\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 7, 10\nplt.show()","execution_count":104,"outputs":[]},{"metadata":{"_uuid":"ff9a5b6d92782ddb4921f8a580aeea9ebef8f159","_cell_guid":"498270ad-3561-4494-b543-02f4a685fb12"},"cell_type":"markdown","source":"# 8.5 Extreme Gradiant Boosting\n\n*Unhide to view output"},{"metadata":{"_uuid":"080ec39bc1efd07a9cd0264a2bc95c998e0b4df9","_cell_guid":"b101ff65-16cb-4835-ac4c-6006a358205d","trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"# Extreme Gradiant Boosting Classifier - BlackBox Model\nfrom xgboost.sklearn import XGBClassifier\nfrom xgboost import plot_importance\n\nprint(\"Running XGBClassifier Feature Importance Part 1...\")\nmodel_XGBC = XGBClassifier(objective='binary:logistic',\n                           max_depth=7, min_child_weight=5,\n                           gamma=0,\n                           learning_rate=0.1, n_estimators=100,)\nmodel_XGBC.fit(X_train, y_train)\nprint(\"XGBClassifier Fitted\")\n\n# For Side To Side\nprint(\"Ranking Features with XGBClassifier...\")\nranks[\"XGBC\"] = rank_to_dict(model_XGBC.feature_importances_, names)\nprint(ranks[\"XGBC\"])","execution_count":61,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7a367647e19498e79dc8729dc0730851dafbb70a","collapsed":true},"cell_type":"code","source":"#Plotting\n# plot feature importance for feature selection using default inbuild function\nprint(\"Plotting XGBClassifier Feature Importance\")\nplot_importance(model_XGBC)\n\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 5, 10\nplt.show()","execution_count":78,"outputs":[]},{"metadata":{"_uuid":"38f0a56ab62a734a4445d45937b6a37bcb4be232","_cell_guid":"aed595da-042c-4036-b262-d5231f81d963"},"cell_type":"markdown","source":"# 8.6 Random Forest Classifier\n\n*Unhide to view output"},{"metadata":{"_uuid":"2782b5aedc61c1fdea5f18b81eea97b272ce9be9","_cell_guid":"e39d8648-e732-4032-90f2-c9718eb5ec36","trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"# Random Forest Classifier - BlackBox Model\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\nmodel_RFC = RandomForestClassifier(bootstrap=True, max_depth=80,\n                                   criterion='entropy',\n                                   min_samples_leaf=3, min_samples_split=10, n_estimators=100)\nmodel_RFC.fit(X_train, y_train)\n\n# For Side To Side\nprint(\"Ranking Features with RFClassifier...\")\nranks[\"RFC\"] = rank_to_dict(model_RFC.feature_importances_, names)\nprint(ranks[\"RFC\"])","execution_count":79,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b5fc0f4bdc34f4a9f6d2633070a6532b593738ed","collapsed":true},"cell_type":"code","source":"#Plotting\n# For Chart\nimportance = pd.DataFrame({'feature': X_train.columns, 'importance': np.round(model_RFC.feature_importances_, 3)})\nimportance_sorted = importance.sort_values('importance', ascending=False).set_index('feature')\n# plot feature importance for feature selection using default inbuild function\n#print(importance_sorted)\nimportance_sorted.plot.bar()\n\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 10, 20\nplt.show()","execution_count":83,"outputs":[]},{"metadata":{"_uuid":"7848404315c6389b986ac6d6a19f75f5b2e81192","_cell_guid":"b5f3a993-375f-4270-9ab5-9c976f8bdc19"},"cell_type":"markdown","source":"# 9.Features (Side To Side Comparison)\n\nNow we will collate all the feature coefficients & normalize them for a scaled comparison across all of them. This uses the \"rank_to_dict\" function we defined at\n\n8.Feature Select - B"},{"metadata":{"_uuid":"8bc214e0c6c51cefe75a7e9be7de650ac4114452"},"cell_type":"markdown","source":"# **9.1** (Coefficient Values) Quick Easy Method"},{"metadata":{"_uuid":"c0f367f3e635c8f1ee41267796bfec31ea9e35d5","_cell_guid":"721b2571-61a8-4c7e-992f-738d132f1a2c","trusted":true,"collapsed":true},"cell_type":"code","source":"pd.options.display.max_columns = 100\n##### Collate Feature Coefficients Side by Side\nprint(\"Collating Side To Side Feature Scores...\")\n\n######## Easy quick print Method\n# Create empty dictionary to store the mean value calculated across all the scores\nr = {}\nfor name in names:\n    # This is the alternative rounding method from the earlier map & lambda combination\n    r[name] = round(np.mean([ranks[method][name] for method in ranks.keys()]), 2)\n\nmethods = sorted(ranks.keys())\nranks[\"Mean\"] = r\nmethods.append(\"Mean\")\n\nprint(\"\\t%s\" % \"\\t\".join(methods))\nfor name in names:\n    print(\"%s\\t%s\" % (name, \"\\t\".join(map(str, [ranks[method][name] for method in methods]))))","execution_count":105,"outputs":[]},{"metadata":{"_uuid":"cce820d97465b4491207fbf412221fb07982d04b"},"cell_type":"markdown","source":"Evidently, you can see that this method isn't for OCD analyst...alignments are all off...\n\nNow lets instead use a much more presentable method!!!!"},{"metadata":{"_uuid":"3272de42239f5914cffc3c3d0cb913d0a58d648c"},"cell_type":"markdown","source":"# **9.2** (Coefficient Values) Neat DataFrame Method"},{"metadata":{"trusted":true,"_uuid":"c60bebc5f989b20b7ee45da0599faf7651a220e1","collapsed":true},"cell_type":"code","source":"######## Alternatively, set into Dataframe. Advantage is that we can plot here.\n# Loop through dictionary of scores to append into a dataframe\nrow_index = 0\nAllFeatures_columns = ['Feature', 'Scores']\nAllFeats = pd.DataFrame(columns=AllFeatures_columns)\nfor name in names:\n    AllFeats.loc[row_index, 'Feature'] = name\n    AllFeats.loc[row_index, 'Scores'] = [ranks[method][name] for method in methods]\n        \n    row_index += 1\n\n# Here the dataframe scores are a list in a list. \n# To split them, we convert the 'Scores' column from a dataframe into a list & back into a dataframe again\nAllFeatures_only = pd.DataFrame(AllFeats.Scores.tolist(), )\n# Now to rename the column headers\nAllFeatures_only.rename(columns={0:'LogRegBalance',1:'LogRegLASSO',2:'LogRegRidge',\n                                     3:'Random ForestClassifier',4:'XGB Classifier', 5:'Mean'},inplace=True)\nAllFeatures_only = AllFeatures_only[['LogRegBalance','LogRegLASSO','LogRegRidge', \n                                           'Random ForestClassifier', 'XGB Classifier', 'Mean']]\n# Now to join both dataframes\nAllFeatures_compare = AllFeats.join(AllFeatures_only).drop(['Scores'],  axis=1)\ndisplay(AllFeatures_compare)","execution_count":109,"outputs":[]},{"metadata":{"_uuid":"1370f6b0cb7b7700685e243521c1a1c974775e78"},"cell_type":"markdown","source":"Perfect ain't it!!! Now likewise for the plotting"},{"metadata":{"_uuid":"7157799d78bdafcffeecb9b593ac1bb596d133ac","_cell_guid":"3525d308-f81b-4d72-9492-ab28eb10479a"},"cell_type":"markdown","source":"# **Plot the Normalized Feature Rankings/Coefficients/Gini Importance**\n\n# **9.3** (Plotting) Quick Method"},{"metadata":{"_uuid":"2abe2481f20ae6abdaeeb089b1e5e0303d41bc5c","_cell_guid":"575d8153-7f2e-44eb-b828-66a46dc1f3dd","trusted":true,"collapsed":true},"cell_type":"code","source":"#Plotting\ndf = AllFeatures_compare.melt('Feature', var_name='cols',  value_name='vals')\ng = sns.factorplot(x=\"Feature\", y=\"vals\", hue='cols', data=df, size=10, aspect=2)\n\nplt.xticks(rotation=90)\nplt.show()","execution_count":110,"outputs":[]},{"metadata":{"_uuid":"7345f6e2f29250bd9471b4d48e0b56bf937d270e","_cell_guid":"4b5fccf0-3817-45f2-bbb0-36ce0802541b"},"cell_type":"markdown","source":"# **9.4** (Plotting) Sorted Neat Method\n\n**Alternatively, a better plot could be used, where we sort by the mean first.** Similar, to what the Excel Pivot Chart does."},{"metadata":{"_uuid":"2cf25b524f9fbd6982c472e27be4c604099d850b","_cell_guid":"8b6cc5fb-c3b4-424d-a25d-153d4922ed5f","trusted":true,"collapsed":true},"cell_type":"code","source":"AllFeatures_compare_sort = AllFeatures_compare.sort_values(by=['Mean'], ascending=True)\norder_ascending = AllFeatures_compare_sort['Feature']\n#Plotting\ndf2 = AllFeatures_compare_sort.melt('Feature', var_name='cols',  value_name='vals')\n# ONLY Difference is that now we use row_order to sort based on the above ascending Ascending Mean Features\ng2 = sns.factorplot(x=\"Feature\", y=\"vals\", hue='cols', data=df2, size=10, aspect=2, row_order=order_ascending)\n\nplt.xticks(rotation=90)\nplt.show()","execution_count":111,"outputs":[]},{"metadata":{"_uuid":"52a786bb9b87222a7c6038b37878659f7b78e586"},"cell_type":"markdown","source":"# Quick Commentary:\n\n* Some **Observations** include:\n\n1.Noise\n\nAmongst WhiteBox Models LASSO comparatively seems like the noisiest.\n\n2.Mean alignment\n\nWhiteBox Models tends to align closer\n\n3.RandomForest & XGB Classifier\n\nClosely aligned\n\n\n* Some **Subjective** conclusions:\n\n1.White-Box models (LASSO) better option when determining optimal *Feature* *Quantity*. \n\nAs it filters the top performing features, while forcing the residual features to be close to zero. Evidently, from the steep slope.\n\n\n2.Black-Box Models beter option for determining Features Interaction or specific *Feature Quality Selection*.\n\nSince, it generates relatively good accuracy and robustness, thereby easing the resulting data interpretation.\n"},{"metadata":{"_uuid":"1ce8b67a815459f6c8a58d505ba227eab4343b61","_cell_guid":"beeb4137-cec9-4544-b91c-fee59994bd1b"},"cell_type":"markdown","source":"**10.ROC AUC (Side To Side Comparison)**\n\nNow as before we will collate the ROC & AUC for each model & plot all of them for comparison\n\nJust a tip for a ridiculous error i faced. Dont forget to use 'predict_proba' for the roc_curve!! I didn't initially & had a 0.5 score."},{"metadata":{"_uuid":"dd39db18c69418a01b12a65224a3f8effb164532","_cell_guid":"984848f3-61a9-48d2-8074-e20b6a31547c","trusted":true,"collapsed":true},"cell_type":"code","source":"##### Ensemble Comparison of ROC AUC \nfrom sklearn import model_selection\nimport matplotlib.pyplot as plt\n\n# run model 10x with 60/30 split, but intentionally leaving out 10% avoiding overfitting\ncv_split = model_selection.ShuffleSplit(n_splits=10, test_size=.3, train_size=.6, random_state=0)\n\nprint(\"Charting ROC AUC for Ensembles...\")\nfrom sklearn.metrics import roc_curve, auc\n\n# Establish Models\nmodels = [\n    {\n        'label': 'LASSO',\n        'model': model_LogRegLASSO,\n    },\n    {\n        'label': 'Ridge',\n        'model': model_LogRegRidge,\n    },\n    {\n        'label': 'LogReg Balance',\n        'model': model_LogRegBalance,\n    },\n    {\n        'label': 'XGBoost Classifier',\n        'model': model_XGBC,\n    },\n    {\n        'label': 'Random Forest Classifier',\n        'model': model_RFC,\n    }\n]\n\n# Models Plot-loop\nfor m in models:\n    #scaler = StandardScaler()\n    #std_data2 = scaler.fit_transform(X_validation)\n    #fpr, tpr, thresholds = roc_curve(y_validation, m['model'].predict_proba(std_data2).T[0])\n    fpr, tpr, thresholds = roc_curve(y_validation, m['model'].predict_proba(X_validation).T[1])\n    roc_auc = auc(fpr, tpr)\n    plt.plot(fpr, tpr, label='%s ROC (area = %0.2f)' % (m['label'], roc_auc))\n\n# Set Plotting attributes\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend(loc=0, fontsize='small')\nplt.show()","execution_count":125,"outputs":[]},{"metadata":{"_uuid":"9e984702f5a1a15febed24b38ed0a471ef52d542","_cell_guid":"e089aca8-db95-48e3-b765-7fc45aa83521"},"cell_type":"markdown","source":"**10.1 Brief Annotations**\n\nAs expected given the way the XGBoost algorithm is structured (Bagging & Learning from errors & applying error emphasis) it triumphs all. Furthermore, we haven't done any Feature Engineering as well. Hence, this could possibly a sign of overfitting too. In a\n \nOverall the AUCs also aren't fancy but at least we know how to compare them going forward!"},{"metadata":{"_uuid":"93885f3883154e58654db5335e6f2ef5f62810c7","_cell_guid":"ac35449c-eda9-4627-bf99-3c604eeca368"},"cell_type":"markdown","source":"**11.Cross Validation Scores (Side To Side Comparison)**\n\nNow as before we will collate the relevant 'Cross Validated' (CV) accuracy scores for each model & compare all of them. "},{"metadata":{"_uuid":"e8ded2c156dd0b86d9c4d4be2846009ac0c35ae4","_cell_guid":"b7ded272-d4fc-4f77-943d-220b3268e6c9","trusted":true,"collapsed":true},"cell_type":"code","source":"##### Ensemble Comparison of Accuracy Scores \n# Set dataframe for appending``\npd.options.display.max_columns = 100\nScores_columns = ['Model Name', 'Model Parameters', 'Train Accuracy Mean', 'Test Accuracy Mean']\nScores_compare = pd.DataFrame(columns=Scores_columns)\n\n# Models CV-loop\nrow_index = 0\nfor m in models:\n    # Name of Model\n    Scores_compare.loc[row_index, 'Model Name'] = m['label']\n    # Model Parameters\n    Scores_compare.loc[row_index, 'Model Parameters'] = str(m['model'].get_params())\n    \n    # Execute Cross Validation (CV)\n    cv_results = model_selection.cross_validate(m['model'], X_train, y_train, cv=cv_split)\n    # Model Train Accuracy\n    Scores_compare.loc[row_index, 'Train Accuracy Mean'] = cv_results['train_score'].mean()\n    # Model Test Accuracy\n    Scores_compare.loc[row_index, 'Test Accuracy Mean'] = cv_results['test_score'].mean()\n\n    row_index += 1\n    \ndisplay(Scores_compare)","execution_count":126,"outputs":[]},{"metadata":{"_uuid":"b7985545bf5d7032b19cbc4960ce59dad7a9e927","_cell_guid":"726dad56-a31b-4ce1-bfed-b5958fd5ea74"},"cell_type":"markdown","source":"# 11.1 OVERALL CONCLUSIONS\n\nInteresting deviations in the AUC vs Accuracy scores...Low AUC but High Accuracy scores. \n\nBUT point to note, both are totally different derivations. In addition, we have to factor in the 'balance' of the class representation in the data to fully evaluate these metrics holistically. Given that we did not focus much on EDA & Feature Engineering here, there isn't much room for contention here... In addition, we can also exlclude the 'Balanced' LogisticRegression here since we have no secure knowledge on the data 'balance'.\n\nFurthermore, back to the basic definitions... AUC ROC (The Receiver Operating Characteristics) (ROC) curve is simply a trade-off graph between the True Positive rate (y-axis) against the False Positive rate (x-axis) at incremental threshold settings. Ideally, the more the curve is concave towards the top left corner the better. In turn indicating the optimal ability to attain a high proportion of correct detection of the condition presence, at a low expense of incorrect detection of the condition presence.\n\nHence, since we didn't even have a adequate proportion of presence conditions to begin with it further emphasizes why we should harp too much on AUC ROC scores. Therefore, we can actually place lower weightage on these low AUC scores. \n\nThus, i'll narrow on accuracy score for now instead. Superficially, the accuracy score does suggest models seem optimal. \n\nThen again, this was a unbalanced dataset and I'm pretty sure in reality 'real-world' models would be 1000x more complex which will likely further dampen the scores!!!\n\nBUT More Importantly, we now know more coding methods to deal compare & neaten coding outputs!!! "},{"metadata":{"_uuid":"aaa96486f861898ca2688b18a56d33f69546751f","_cell_guid":"3e359fdb-d60a-49de-b7c6-62560de5e0d9"},"cell_type":"markdown","source":"**Recap of Essentials Coding Techniques:**\n\n    A.List comprehensions\n    B.Samples to reduce computational cost\n    C.Concise 'def' functions that can be used repetitively\n    D.Pivoting using groupby\n    E.When & How to convert and reshape dictionaryâ€™s into lists or dataframes\n    F.Quickly split dataframe columns\n    G.loc & conditionals\n    H.Loop Sub-plots\n    I.Quick Lambda formulae functions\n    J.Quick looping print or DataFrame conversion of summative scores\n    K.Order plot components \n    L.Create & Plot Bulk Ensemble comparative results\n\nThank you so much for reading up till the end! Hopefully, you have a better understanding of Data Manipulation in respect of Ensemble Models now!"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}