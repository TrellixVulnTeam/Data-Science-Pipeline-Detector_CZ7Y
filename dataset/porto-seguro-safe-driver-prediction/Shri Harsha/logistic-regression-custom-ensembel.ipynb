{"cells":[{"metadata":{},"cell_type":"markdown","source":"For EDA, model selection and how information about the df_metedata pickle object click [here](https://www.kaggle.com/batofgotham/eda-and-feature-selection?scriptVersionId=28684443)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_path = '/kaggle/input/porto-seguro-safe-driver-prediction/'\ndf = pd.read_csv(input_path+'train.csv')\ndf_test = pd.read_csv(input_path+'test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"id_test = df_test['id']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target = df['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- As it is imbalanced dataset we have to synthesize for new data points "},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(columns=['target'],inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Getting the Metadata Dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\ndf_metadata = pickle.load(open('/kaggle/input/pssdpickledfmetedatapickle/df_metedata_pickle','rb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_metadata","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" - Before Synthesizing the new data lets complete the pre processing"},{"metadata":{},"cell_type":"markdown","source":"### PreProcessing"},{"metadata":{},"cell_type":"markdown","source":"- Dropping Stastically insignificant columns, Filling the Missing values and changing the datatypes of columns accordingly"},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocessing(df):\n    df.replace(to_replace=-1,value=np.nan,inplace=True)\n    for col in df.columns:\n        #Dropping Insignificant Columns\n        if df_metadata.loc[col,'Dropped']:\n            df.drop(columns=[col],inplace=True)\n            continue\n        #Filling Missing Values\n        df[col].fillna(df_metadata.loc[col,'Missing'],inplace=True)\n        #Changing the datatype of columns\n        if (df_metadata.loc[col,'DTypes'] == 'Categorical') or (df_metadata.loc[col,'DTypes'] == 'Ordinal'):\n            df[col] = df[col].astype('category')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocessing(df)\npreprocessing(df_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_test.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Operating with outliers"},{"metadata":{},"cell_type":"markdown","source":"- The Idea is to find the outliers and replace them accordingly"},{"metadata":{"trusted":false},"cell_type":"code","source":"def outlier_processing(df,df_test):\n    for col in df.columns:\n        if df[col].dtype.name != 'category':\n            first_quartile, third_quartile = np.percentile(df[col],[25,75])\n            first_percetnile, ninetynine_percentile = np.percentile(df[col],[1,99])\n            IQR = third_quartile - first_quartile\n            lower_bound = first_quartile - (1.5*IQR)\n            upper_bound = third_quartile + (1.5*IQR)\n            df[col].loc[df[col]>upper_bound] = ninetynine_percentile\n            df_test[col].loc[df_test[col]>upper_bound] = ninetynine_percentile\n            df[col].loc[df[col]<lower_bound] = first_percetnile\n            df_test[col].loc[df_test[col]<lower_bound] = first_percetnile\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"outlier_processing(df,df_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Encoding"},{"metadata":{},"cell_type":"markdown","source":"- The Idea is to encode the ordinal values with Ordinal Encoder and Categorical values with OneHot Encoder - unless they are binary"},{"metadata":{"trusted":false},"cell_type":"code","source":"ordinal_columns = [col for col in df.columns if df_metadata.loc[col,'DTypes'] == 'Ordinal' and df[col].nunique() > 2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"categorical_columns_great_2 = [col for col in df.columns if df_metadata.loc[col,'DTypes'] == 'Categorical' and df[col].nunique() > 2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfor col in ordinal_columns:\n    label_encode = LabelEncoder()\n    df[col+'label'] = label_encode.fit_transform(df[col])\n    df_test[col+'label'] = label_encode.transform(df_test[col])\n    df.drop(columns=[col],inplace=True)\n    df_test.drop(columns=[col],inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df = pd.get_dummies(df,prefix=col,columns=categorical_columns_great_2,drop_first=True)\ndf_test = pd.get_dummies(df_test,columns=categorical_columns_great_2,prefix=col,drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lets do the scaling"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_train_scale = scaler.fit_transform(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_test_scale = scaler.transform(df_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Making data in to multiple folds"},{"metadata":{"trusted":false},"cell_type":"code","source":"df_train_scale = pd.DataFrame(df_train_scale,columns=df.columns)\ndf_test_scale = pd.DataFrame(df_test_scale,columns=df_test.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"chunks = [df_train_scale,target]\ndf_train_scale_target = pd.concat(chunks,axis=1)\ndf_minority = df_train_scale_target.loc[df_train_scale_target['target'] == 1].copy()\ndf_majority = df_train_scale_target.loc[df_train_scale_target['target'] == 0].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"splitted_frame = np.array_split(df_majority, 20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nlog_reg= LogisticRegression(max_iter=100000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"param = {'C':[0.001,0.003,0.005,0.01,0.03,0.05,0.1,0.3,0.5,1]}","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.utils import shuffle\ny_train_pred_proba = 0\ny_test_pred_proba = 0\nfor frames in splitted_frame:\n    Glog_reg = GridSearchCV(estimator = log_reg,param_grid = param, scoring = 'accuracy', cv=5)\n    chunks_temp = [frames,df_minority]\n    df_temp_train = shuffle(pd.concat(chunks_temp,axis=0))\n    target_train = df_temp_train['target']\n    df_temp_train.drop(columns=['target'],inplace=True)\n    Glog_reg.fit(df_temp_train,target_train)\n    best_model = Glog_reg.best_estimator_\n    best_model.fit(df_temp_train,target_train)\n    y_train_pred_proba = y_train_pred_proba + best_model.predict_proba(df_train_scale)[:,1]\n    y_test_pred_proba = y_test_pred_proba + best_model.predict_proba(df_test_scale)[:,1]\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_train_pred_proba = y_train_pred_proba/20\ny_test_pred_proba = y_test_pred_proba/20","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Receiver Operator Characterstics"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.metrics import roc_curve, roc_auc_score\nfpr,tpr,thresold = roc_curve(target,y_train_pred_proba)\nauc = roc_auc_score(target,y_train_pred_proba)\nplt.figure(figsize=(14,8))\nplt.title('Reciever Operating Charactaristics')\nplt.plot(fpr,tpr,'b',label = 'AUC = %0.2f' % auc)\nplt.legend(loc='lower right')\nplt.plot([0,1],[0,1],'r--')\nplt.ylabel('True positive rate')\nplt.xlabel('False positive rate')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"auc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submition"},{"metadata":{"trusted":false},"cell_type":"code","source":"submit = pd.DataFrame({'id':id_test,'target':y_test_pred_proba})\nsubmit.to_csv('logreg_porto.csv',index=False) \nsubmit.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":4}