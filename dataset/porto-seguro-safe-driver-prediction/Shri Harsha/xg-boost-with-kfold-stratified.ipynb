{"cells":[{"metadata":{},"cell_type":"markdown","source":"For EDA, model selection and how information about the df_metedata pickle object click [here](https://www.kaggle.com/batofgotham/eda-and-feature-selection?scriptVersionId=28684443)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_path = '/kaggle/input/porto-seguro-safe-driver-prediction/'\ndf = pd.read_csv(input_path+'train.csv')\ndf_test = pd.read_csv(input_path+'test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"id_test = df_test['id']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target = df['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- As it is imbalanced dataset we have to synthesize for new data points "},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(columns=['target'],inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Getting the Metadata Dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\ndf_metadata = pickle.load(open('/kaggle/input/pssdpickledfmetedatapickle/df_metedata_pickle','rb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_metadata","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" - Before Synthesizing the new data lets complete the pre processing"},{"metadata":{},"cell_type":"markdown","source":"### PreProcessing"},{"metadata":{},"cell_type":"markdown","source":"- Dropping Stastically insignificant columns, Filling the Missing values and changing the datatypes of columns accordingly"},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocessing(df):\n    df.replace(to_replace=-1,value=np.nan,inplace=True)\n    for col in df.columns:\n        #Dropping Insignificant Columns\n        if df_metadata.loc[col,'Dropped']:\n            df.drop(columns=[col],inplace=True)\n            continue\n        #Filling Missing Values\n        df[col].fillna(df_metadata.loc[col,'Missing'],inplace=True)\n        #Changing the datatype of columns\n        if (df_metadata.loc[col,'DTypes'] == 'Categorical') or (df_metadata.loc[col,'DTypes'] == 'Ordinal'):\n            df[col] = df[col].astype('category')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocessing(df)\npreprocessing(df_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Operating with outliers"},{"metadata":{},"cell_type":"markdown","source":"- The Idea is to find the outliers and replace them accordingly"},{"metadata":{"trusted":true},"cell_type":"code","source":"def outlier_processing(df,df_test):\n    for col in df.columns:\n        if df[col].dtype.name != 'category':\n            first_quartile, third_quartile = np.percentile(df[col],[25,75])\n            first_percetnile, ninetynine_percentile = np.percentile(df[col],[1,99])\n            IQR = third_quartile - first_quartile\n            lower_bound = first_quartile - (1.5*IQR)\n            upper_bound = third_quartile + (1.5*IQR)\n            df[col].loc[df[col]>upper_bound] = ninetynine_percentile\n            df_test[col].loc[df_test[col]>upper_bound] = ninetynine_percentile\n            df[col].loc[df[col]<lower_bound] = first_percetnile\n            df_test[col].loc[df_test[col]<lower_bound] = first_percetnile\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#outlier_processing(df,df_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Encoding"},{"metadata":{},"cell_type":"markdown","source":"- The Idea is to encode the ordinal values with Ordinal Encoder and Categorical values with OneHot Encoder - unless they are binary"},{"metadata":{"trusted":true},"cell_type":"code","source":"ordinal_columns = [col for col in df.columns if df_metadata.loc[col,'DTypes'] == 'Ordinal' and df[col].nunique() > 2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_columns_great_2 = [col for col in df.columns if df_metadata.loc[col,'DTypes'] == 'Categorical' and df[col].nunique() > 2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfor col in ordinal_columns:\n    label_encode = LabelEncoder()\n    df[col+'label'] = label_encode.fit_transform(df[col])\n    df_test[col+'label'] = label_encode.transform(df_test[col])\n    df.drop(columns=[col],inplace=True)\n    df_test.drop(columns=[col],inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.get_dummies(df,prefix=col,columns=categorical_columns_great_2,drop_first=True)\ndf_test = pd.get_dummies(df_test,columns=categorical_columns_great_2,prefix=col,drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lets do the scaling"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_scale = scaler.fit_transform(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test_scale = scaler.transform(df_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Making data in to multiple folds"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=17)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Gini Custom Metric\n- Define the gini metric - from https://www.kaggle.com/c/ClaimPredictionChallenge/discussion/703#5897"},{"metadata":{"trusted":true},"cell_type":"code","source":"def gini(actual,pred,cmpcol = 0,sortcol = 1):\n    assert( len(actual) == len(pred) )\n    All = np.asarray(np.c_[ actual, pred, np.arange(len(actual)) ], dtype=np.float)\n    All = All[ np.lexsort((All[:,2], -1*All[:,1])) ]\n    totAllosses = All[:,0].sum()\n    giniSum = All[:,0].cumsum().sum() / totAllosses\n    giniSum -= (len(actual) + 1) / 2.\n    return giniSum / len(actual)\n\ndef gini_normalized(a, p):\n    return gini(a, p) / gini(a, a)\n\ndef gini_xgb(preds, dtrain):\n    labels = dtrain.get_label()\n    gini_score = gini_normalized(labels, preds)\n    return 'gini', gini_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n    'min_child_weight': 10.0,\n    'objective': 'binary:logistic',\n    'max_depth': 7,\n    'max_delta_step': 1.8,\n    'colsample_bytree': 0.4,\n    'subsample': 0.8,\n    'eta': 0.025,\n    'gamma': 0.65,\n    'num_boost_round' : 1000\n    }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_ensemble = []\nfor i,(train_index, valid_index) in enumerate(skf.split(df_train_scale,target)):\n    print('[FOLD %d/%d]'%(i+1,5))\n    X_train,X_valid = df_train_scale[train_index],df_train_scale[valid_index]\n    y_train,y_valid = target.loc[train_index],target.loc[valid_index]\n    #Convert Data in to XGBoost format\n    df_train_xgb = xgb.DMatrix(X_train,y_train)\n    df_valid_xgb = xgb.DMatrix(X_valid,y_valid)\n    valid_list = [(df_train_xgb, 'train'), (df_valid_xgb, 'valid')]\n    xgb_model = xgb.train(params, df_train_xgb, 3000, valid_list, feval=gini_xgb, maximize=True, early_stopping_rounds=70,verbose_eval=100)\n    model_ensemble.append(xgb_model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_proba = 0\ndf_test_xgb = xgb.DMatrix(df_test_scale)\nfor i, model in enumerate(model_ensemble):\n    print('[FOLD %d/%d Prediciton:]'%(i+1,5))\n    predictions = xgb_model.predict(df_test_xgb)\n    predict_proba += predictions\npredict_proba = predict_proba/5","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submition"},{"metadata":{"trusted":true},"cell_type":"code","source":"submit = pd.DataFrame({'id':id_test,'target':predict_proba})\nsubmit.to_csv('xgb_porto.csv',index=False) \nsubmit.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":4}