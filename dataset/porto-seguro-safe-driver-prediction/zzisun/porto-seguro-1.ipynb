{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\n[Bert Carremans 노트북](https://www.kaggle.com/bertcarremans/data-preparation-exploration/data#Introduction) 필사\n\n**KEY POINTS**\n* EDA(Exploratory Data Analysis)\n* Feature Engineering(Dummification, Interaction)\n* Feature Selection(Zero and Low Variance 제거, SelectFromModel) ","metadata":{}},{"cell_type":"markdown","source":"# Section\n1. [Visual inspection of your data](#1)\n2. **[Defining the metadata](#2)**\n3. [Descriptive statistics](#3)\n4. **[Handling imbalanced classes](#4)**\n5. [Data quality checks](#5)\n6. [Exploratory data visualization](#6)\n7. [Feature engineering](#7)\n8. **[Feature selection](#8)**\n9. [Feature scaling](#9)","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.utils import shuffle\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-29T08:51:15.284506Z","iopub.execute_input":"2021-08-29T08:51:15.285293Z","iopub.status.idle":"2021-08-29T08:51:15.299517Z","shell.execute_reply.started":"2021-08-29T08:51:15.28521Z","shell.execute_reply":"2021-08-29T08:51:15.298356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name='1'></a>\n# 1. Visual inspection of your data\n## 1.1 Data Load","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/porto-seguro-safe-driver-prediction/train.csv')\ntest = pd.read_csv('/kaggle/input/porto-seguro-safe-driver-prediction/test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-08-29T08:51:15.314432Z","iopub.execute_input":"2021-08-29T08:51:15.31473Z","iopub.status.idle":"2021-08-29T08:51:22.199368Z","shell.execute_reply.started":"2021-08-29T08:51:15.314703Z","shell.execute_reply":"2021-08-29T08:51:22.197645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.2 Data at first sight","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"## 데이터 분석 요약\n* [train set < test set](#1)\n* 비슷한 그룹에 속하는 feature에는 이름에 태그가 지정 (ind, reg, car, calc)\n* feature이름에는 binary와 category를 나타내는 bin, cat이 postfix로 포함\n* -1 값은 Null을 의미\n* target은 보험 청구를 한다(=1), 보험 청구를 하지 않는다(=0)인 binary 데이터, 0이 압도적으로 많음 \n\n","metadata":{}},{"cell_type":"code","source":"train.head()  # defalut number = 5","metadata":{"execution":{"iopub.status.busy":"2021-08-29T08:51:22.201363Z","iopub.execute_input":"2021-08-29T08:51:22.201725Z","iopub.status.idle":"2021-08-29T08:51:22.227653Z","shell.execute_reply.started":"2021-08-29T08:51:22.201693Z","shell.execute_reply":"2021-08-29T08:51:22.226634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.tail()","metadata":{"execution":{"iopub.status.busy":"2021-08-29T08:51:22.229617Z","iopub.execute_input":"2021-08-29T08:51:22.229901Z","iopub.status.idle":"2021-08-29T08:51:22.257562Z","shell.execute_reply.started":"2021-08-29T08:51:22.229872Z","shell.execute_reply":"2021-08-29T08:51:22.256421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"중복되는 row가 존재하는지 확인","metadata":{}},{"cell_type":"code","source":"train.duplicated()","metadata":{"execution":{"iopub.status.busy":"2021-08-29T08:51:22.259557Z","iopub.execute_input":"2021-08-29T08:51:22.260025Z","iopub.status.idle":"2021-08-29T08:51:23.001024Z","shell.execute_reply.started":"2021-08-29T08:51:22.25995Z","shell.execute_reply":"2021-08-29T08:51:22.999925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-29T08:51:23.002551Z","iopub.execute_input":"2021-08-29T08:51:23.002869Z","iopub.status.idle":"2021-08-29T08:51:23.009347Z","shell.execute_reply.started":"2021-08-29T08:51:23.00284Z","shell.execute_reply":"2021-08-29T08:51:23.008208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-29T08:51:23.010666Z","iopub.execute_input":"2021-08-29T08:51:23.010968Z","iopub.status.idle":"2021-08-29T08:51:23.026271Z","shell.execute_reply.started":"2021-08-29T08:51:23.010938Z","shell.execute_reply":"2021-08-29T08:51:23.02525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"test set은 target 변수가 빠져있기에 변수가 1개 줄어들음\n\n\n14개의 categorical 변수에 대한 dummy 변수(이진화)를 만들 수 있다. // 타이타닉에서 카테고리를 수치화 시킨 것과 같이","metadata":{}},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2021-08-29T08:51:23.027684Z","iopub.execute_input":"2021-08-29T08:51:23.027974Z","iopub.status.idle":"2021-08-29T08:51:23.112496Z","shell.execute_reply.started":"2021-08-29T08:51:23.027946Z","shell.execute_reply":"2021-08-29T08:51:23.111063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name='2'></a>\n# 2. Defining the metadata\n\n* 본 커널의 특징은 데이터 관리를 위해 Meta Data를 만들어 활용했다는 것\n> Meta Data란? \n  features에 대한 정보라고 이해하자\n\n어떤 조건의 feature만 사용하고 싶을 때 그때마다 코드를 작성하기 번거롭다. \n\n편리성을 위해 Meta Data를 미리 만들어 놓고 간단한 코드로 feature engineering!(분석, 시각화, 모델링..)\n\n\n**META DATA 구성**\n* role: input, ID, target \n* level: nominal, interval, ordinal, binary - [Feature type 참고자료](http://blog.heartcount.io/dd)\n* keep: True or False - 데이터 활용 여부\n* dtype: int, float, str","metadata":{}},{"cell_type":"code","source":"data = []\nfor f in train.columns:\n    # Defining the role\n    if f == 'target':\n        role = 'target'\n    elif f == 'id':\n        role = 'id'\n    else:\n        role = 'input'\n         \n    # Defining the level\n    if 'bin' in f or f == 'target':\n        level = 'binary'\n    elif 'cat' in f or f == 'id':\n        level = 'nominal'\n    elif train[f].dtype == float:\n        level = 'interval'\n    elif train[f].dtype == int:\n        level = 'ordinal'\n        \n    # Initialize keep to True for all variables except for id\n    keep = True\n    if f == 'id':\n        keep = False\n    \n    # Defining the data type \n    dtype = train[f].dtype\n    \n    # Creating a Dict that contains all the metadata for the variable\n    f_dict = {\n        'varname': f,\n        'role': role,\n        'level': level,\n        'keep': keep,\n        'dtype': dtype\n    }\n    data.append(f_dict)\n    \nmeta = pd.DataFrame(data, columns=['varname', 'role', 'level', 'keep', 'dtype'])\nmeta.set_index('varname', inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T08:51:23.11706Z","iopub.execute_input":"2021-08-29T08:51:23.117395Z","iopub.status.idle":"2021-08-29T08:51:23.131229Z","shell.execute_reply.started":"2021-08-29T08:51:23.117365Z","shell.execute_reply":"2021-08-29T08:51:23.130301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"meta","metadata":{"execution":{"iopub.status.busy":"2021-08-29T08:51:23.133042Z","iopub.execute_input":"2021-08-29T08:51:23.133782Z","iopub.status.idle":"2021-08-29T08:51:23.182264Z","shell.execute_reply.started":"2021-08-29T08:51:23.133739Z","shell.execute_reply":"2021-08-29T08:51:23.180502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 사용할 nominal 타입을 불러올 때\nmeta[(meta.level == 'nominal') & (meta.keep)].index  ","metadata":{"execution":{"iopub.status.busy":"2021-08-29T08:51:23.184563Z","iopub.execute_input":"2021-08-29T08:51:23.185217Z","iopub.status.idle":"2021-08-29T08:51:23.203258Z","shell.execute_reply.started":"2021-08-29T08:51:23.185164Z","shell.execute_reply":"2021-08-29T08:51:23.202119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame({'count' : meta.groupby(['role', 'level'])['role'].size()}).reset_index()","metadata":{"execution":{"iopub.status.busy":"2021-08-29T08:51:23.204594Z","iopub.execute_input":"2021-08-29T08:51:23.205075Z","iopub.status.idle":"2021-08-29T08:51:23.23279Z","shell.execute_reply.started":"2021-08-29T08:51:23.20503Z","shell.execute_reply":"2021-08-29T08:51:23.231547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"v = meta[(meta.level == 'interval') & (meta.keep)].index\ntrain[v].describe()","metadata":{"execution":{"iopub.status.busy":"2021-08-29T08:51:23.235489Z","iopub.execute_input":"2021-08-29T08:51:23.235948Z","iopub.status.idle":"2021-08-29T08:51:23.549596Z","shell.execute_reply.started":"2021-08-29T08:51:23.235901Z","shell.execute_reply":"2021-08-29T08:51:23.548323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name='3'></a>\n# 3. Descriptive statistics\n\n* meta data 덕분에 계산하고자 하는 변수를 쉽게 선택 가능\n* describe 메소드 사용하여 데이터의 통계 확인\n* 범주형 범수에는 의미가 없으니 실수형 변수에 사용하여 평균, 표준편차 등을 알수 있음","metadata":{}},{"cell_type":"markdown","source":"### 1. Interval 변수\n","metadata":{}},{"cell_type":"code","source":"v = meta[(meta.level == 'interval') & (meta.keep)].index\ntrain[v].describe()","metadata":{"execution":{"iopub.status.busy":"2021-08-29T08:51:23.551322Z","iopub.execute_input":"2021-08-29T08:51:23.551723Z","iopub.status.idle":"2021-08-29T08:51:23.836063Z","shell.execute_reply.started":"2021-08-29T08:51:23.55168Z","shell.execute_reply":"2021-08-29T08:51:23.83497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* reg 변수들중에는 ps_reg_03에만 -1(Null data)가 있다.\n* car 변수들중에는 ps_car_12, ps_car_14에 -1(Null data)가 있다.\n* calc 변수들에는 -1(NUll data)는 없다.\n* 변수별로 min과 max의 range가 다르기에, 스케일링을 적용 필요\n* interval 변수들의 범위는 그렇게 크지 않음.","metadata":{}},{"cell_type":"markdown","source":"### 2. Ordinal 변수","metadata":{}},{"cell_type":"code","source":"v = meta[(meta.level == 'ordinal') & (meta.keep)].index\ntrain[v].describe()","metadata":{"execution":{"iopub.status.busy":"2021-08-29T08:51:23.837874Z","iopub.execute_input":"2021-08-29T08:51:23.838353Z","iopub.status.idle":"2021-08-29T08:51:24.209083Z","shell.execute_reply.started":"2021-08-29T08:51:23.8383Z","shell.execute_reply":"2021-08-29T08:51:24.207898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* ps_car_11 변수에만 -1(Null data)가 있다.\n* 모두 min, max range가 다르므로 scaling을 진행 필요","metadata":{}},{"cell_type":"markdown","source":"### 3. Binary variables","metadata":{}},{"cell_type":"code","source":"v = meta[(meta.level == 'binary') & (meta.keep == True)].index\ntrain[v].describe()","metadata":{"execution":{"iopub.status.busy":"2021-08-29T08:51:24.210514Z","iopub.execute_input":"2021-08-29T08:51:24.210908Z","iopub.status.idle":"2021-08-29T08:51:24.549028Z","shell.execute_reply.started":"2021-08-29T08:51:24.210867Z","shell.execute_reply":"2021-08-29T08:51:24.547974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Train 데이터에서 target은 3.645% \n대부분의 값이 0으로 되어있는 **strongly imbalanced**","metadata":{}},{"cell_type":"markdown","source":"<a name='4'></a>\n# 4. Handling imbalanced classes","metadata":{}},{"cell_type":"markdown","source":"Target = 1인 Record의 비율이 너무 적다. 즉, 모두다 target을 0으로 예측해도 얼마안되는 1만 틀린것으로 파악됨\n[해결방법](https://dining-developer.tistory.com/27)들 중 **Under sampling** 선택\n\n**UnderSampling**은 많은 부분을 차지하는 세트에 적은 부분을 차지하는 세트 수준으로 감소시키는 방법이다.\n물론, overfitting 문제를 해결할 수는 있지만, 중요한 데이터가 삭제될 위험이 있다.","metadata":{}},{"cell_type":"code","source":"desired_apriori=0.10\n\n# Get the indices per target value\nidx_0 = train[train.target == 0].index\nidx_1 = train[train.target == 1].index\n\n# Get original number of records per target value\nnb_0 = len(train.loc[idx_0])\nnb_1 = len(train.loc[idx_1])\n\n# Calculate the undersampling rate and resulting number of records with target=0\nundersampling_rate = ((1-desired_apriori)*nb_1)/(nb_0*desired_apriori)\nundersampled_nb_0 = int(undersampling_rate*nb_0)\nprint('Rate to undersample records with target=0: {}'.format(undersampling_rate))\nprint('Number of records with target=0 after undersampling: {}'.format(undersampled_nb_0))\n\n# Randomly select records with target=0 to get at the desired a priori\nundersampled_idx = shuffle(idx_0, random_state=37, n_samples=undersampled_nb_0)\n\n# Construct list with remaining indices\nidx_list = list(undersampled_idx) + list(idx_1)\n\n# Return undersample data frame\ntrain = train.loc[idx_list].reset_index(drop=True)\nunder_rate = train['target'].sum() / train['target'].count()\nprint(f'Under sampling으로 변환된 target의 비율 : {under_rate} %')","metadata":{"execution":{"iopub.status.busy":"2021-08-29T08:51:24.550988Z","iopub.execute_input":"2021-08-29T08:51:24.55142Z","iopub.status.idle":"2021-08-29T08:51:25.146367Z","shell.execute_reply.started":"2021-08-29T08:51:24.551379Z","shell.execute_reply":"2021-08-29T08:51:25.145344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* undersampling_rate: target=0이 몇%가 되어야 target=1이 전체 데이터에서 desired_apriori가 되는지의 대한 비율\n\n* desired_apriori = 0.10 는 undersampling 후 나오게될 target = 1의 비율","metadata":{}},{"cell_type":"markdown","source":"<a name='5'></a>\n# 5. Data Quality Checks\n## 5.1 Checking missing values","metadata":{}},{"cell_type":"code","source":"vars_with_missing = []\n\nfor f in train.columns:\n    missings = train[train[f] == -1][f].count()\n    if missings > 0:\n        vars_with_missing.append(f)\n        missings_perc = missings / train.shape[0]\n        \n        print(f'Variable {f} has {missings} records {missings_perc:.2%} with missing values')\nprint(f'In total, there are {len(vars_with_missing)} varialbles with missing values')","metadata":{"execution":{"iopub.status.busy":"2021-08-29T08:51:25.147579Z","iopub.execute_input":"2021-08-29T08:51:25.14788Z","iopub.status.idle":"2021-08-29T08:51:25.332607Z","shell.execute_reply.started":"2021-08-29T08:51:25.147851Z","shell.execute_reply":"2021-08-29T08:51:25.331103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Missing Values(Null Data)인 -1을 각 변수별로 찾아서, 비율을 확인한것 \n* 생각보다 Missing Values가 많은 변수가 있다. ps_res_03, ps_car_03_cat, ps_car_05_cat …\n* 총 12개의 변수에서 Missing values 존재","metadata":{}},{"cell_type":"markdown","source":"## SimpleImputer\nSimpleImputer?\n: 결치 값을 채우기 위한 클래스이다.\n\nSimpleImputer 사용방법\n* SimpleImputer constructor 호출한다.\n* missing_values: 어떤 값이 결치 값인지, default은 NaN이다.\n* strategy: 'mean'은 평균값, 'most_frequent' 최빈 값이다.\n* 리턴값을 col=fit_transform(col).ravel() 호출한다.","metadata":{}},{"cell_type":"code","source":"# Dropping the variables with too many missing values\nvars_to_drop = ['ps_car_03_cat', 'ps_car_05_cat']\ntrain.drop(vars_to_drop, inplace=True, axis=1)\nmeta.loc[(vars_to_drop),'keep'] = False  # Updating the meta\n\n# Imputing with the mean or mode\nmean_imp = SimpleImputer(missing_values=-1, strategy='mean')\nmode_imp = SimpleImputer(missing_values=-1, strategy='most_frequent')\ntrain['ps_reg_03'] = mean_imp.fit_transform(train[['ps_reg_03']]).ravel()\ntrain['ps_car_12'] = mean_imp.fit_transform(train[['ps_car_12']]).ravel() \ntrain['ps_car_14'] = mean_imp.fit_transform(train[['ps_car_14']]).ravel()\ntrain['ps_car_11'] = mode_imp.fit_transform(train[['ps_car_11']]).ravel() # ordinal value\n","metadata":{"execution":{"iopub.status.busy":"2021-08-29T08:51:25.33418Z","iopub.execute_input":"2021-08-29T08:51:25.334582Z","iopub.status.idle":"2021-08-29T08:51:25.4132Z","shell.execute_reply.started":"2021-08-29T08:51:25.334539Z","shell.execute_reply":"2021-08-29T08:51:25.412039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Missing value이있는 다른범주형 변수의 경우 Missing value -1을 그대로 둠\n* ps_reg_03 (continuous)의 18%의 Missing value는 평균으로 바꿉니다.\n* ps_car_11 (ordinal)의 1개의 Misisng values는 최빈값으로 바꿉니다.\n* ps_car_12 (continuous)의 단 1개의 Missing value 평균으로 바꿉니다.\n* ps_car_14 (continuous)의 7% Missing values는 평균으로 바꿉니다.","metadata":{}},{"cell_type":"markdown","source":"## 5.2 Checking the cardinality of the categorical variables\n > cardinality: 변수에있는 서로 다른 값의 수","metadata":{}},{"cell_type":"code","source":"v = meta[(meta['level'] == 'nominal') & (meta['keep'])].index\n\nfor f in v:\n    dist_values = train[f].value_counts().shape[0]\n    print(f'Variable {f} has {dist_values} distinct values')","metadata":{"execution":{"iopub.status.busy":"2021-08-29T08:51:25.4145Z","iopub.execute_input":"2021-08-29T08:51:25.41481Z","iopub.status.idle":"2021-08-29T08:51:25.455203Z","shell.execute_reply.started":"2021-08-29T08:51:25.414781Z","shell.execute_reply":"2021-08-29T08:51:25.453954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* nominal 변수에서 더미 변수를 만들 것이므로 고유 한 값이 많은 변수가 있는지 확인해야함. 이러한 변수는 많은 더미 변수를 생성\n* ps_car_11_cat이 104개의 distinct data를 가집니다.","metadata":{}},{"cell_type":"code","source":"def add_noise(series, noise_level):\n    return series * (1 + noise_level * np.random.randn(len(series)))\n\ndef target_encode(trn_series=None, \n                  tst_series=None, \n                  target=None, \n                  min_samples_leaf=1, \n                  smoothing=1,\n                  noise_level=0):\n    \"\"\"\n    Smoothing is computed like in the following paper by Daniele Micci-Barreca\n    https://kaggle2.blob.core.windows.net/forum-message-attachments/225952/7441/high%20cardinality%20categoricals.pdf\n    trn_series : training categorical feature as a pd.Series\n    tst_series : test categorical feature as a pd.Series\n    target : target data as a pd.Series\n    min_samples_leaf (int) : minimum samples to take category average into account\n    smoothing (int) : smoothing effect to balance categorical average vs prior  \n    \"\"\" \n    assert len(trn_series) == len(target)\n    assert trn_series.name == tst_series.name\n    temp = pd.concat([trn_series, target], axis=1)\n    # Compute target mean \n    averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n    # Compute smoothing\n    smoothing = 1 / (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) / smoothing))\n    # Apply average function to all target data\n    prior = target.mean()\n    # The bigger the count the less full_avg is taken into account\n    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n    # Apply averages to trn and tst series\n    ft_trn_series = pd.merge(\n        trn_series.to_frame(trn_series.name),\n        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n        on=trn_series.name,\n        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n    # pd.merge does not keep the index so restore it\n    ft_trn_series.index = trn_series.index \n    ft_tst_series = pd.merge(\n        tst_series.to_frame(tst_series.name),\n        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n        on=tst_series.name,\n        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n    # pd.merge does not keep the index so restore it\n    ft_tst_series.index = tst_series.index\n    return add_noise(ft_trn_series, noise_level), add_noise(ft_tst_series, noise_level)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T08:51:25.456907Z","iopub.execute_input":"2021-08-29T08:51:25.458109Z","iopub.status.idle":"2021-08-29T08:51:25.475455Z","shell.execute_reply.started":"2021-08-29T08:51:25.457727Z","shell.execute_reply":"2021-08-29T08:51:25.474204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_encoded, test_encoded = target_encode(train['ps_car_11_cat'],\n                                            test['ps_car_11_cat'],\n                                            target=train.target,\n                                            min_samples_leaf=100,\n                                            smoothing=10,\n                                            noise_level=0.01)\n\ntrain['ps_car_11_cat_te'] = train_encoded\ntrain.drop('ps_car_11_cat', axis=1, inplace=True)\nmeta.loc['ps_car_11_cat', 'keep'] = False  # Updating the meta\ntest['ps_car_11_cat_te'] = test_encoded\ntest.drop('ps_car_11_cat', axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T08:51:25.477011Z","iopub.execute_input":"2021-08-29T08:51:25.477391Z","iopub.status.idle":"2021-08-29T08:51:25.870447Z","shell.execute_reply.started":"2021-08-29T08:51:25.47736Z","shell.execute_reply":"2021-08-29T08:51:25.869611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ps_car_11_cat 처리\nby using target_encoder\n\ncategorical 변수는 target encoder를 사용하여 renaming","metadata":{}},{"cell_type":"code","source":"train_encoded","metadata":{"execution":{"iopub.status.busy":"2021-08-29T10:16:13.402398Z","iopub.execute_input":"2021-08-29T10:16:13.40337Z","iopub.status.idle":"2021-08-29T10:16:13.415742Z","shell.execute_reply.started":"2021-08-29T10:16:13.403284Z","shell.execute_reply":"2021-08-29T10:16:13.414835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"v = meta[(meta['level'] == 'nominal') & (meta['keep'])].index\n\nfor f in v:\n    dist_values = train[f].value_counts().shape[0]\n    print(f'Variable {f} has {dist_values} distinct values')","metadata":{"execution":{"iopub.status.busy":"2021-08-29T08:51:25.871571Z","iopub.execute_input":"2021-08-29T08:51:25.87197Z","iopub.status.idle":"2021-08-29T08:51:25.910009Z","shell.execute_reply.started":"2021-08-29T08:51:25.871933Z","shell.execute_reply":"2021-08-29T08:51:25.908324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name='6'></a>\n# 6. Exploratory Data Visualization","metadata":{}},{"cell_type":"markdown","source":"### MetatData를 이용한 Visualization","metadata":{}},{"cell_type":"markdown","source":"## 6.1 Categorical variables\n\n다음과 같이 v에 동일한 특성(categorical)을 갖는 columns을 for 문을 이용해 데이터 시각화를 진행한다. 이렇게 되면 같은 특성인 경우에 같은 그래프를 통해 분석할 수 있는 통일성을 가질 수 있다.\n\n### barplot: 막대그래프 matplotlib 보다 색감 있는 그래프를 만들어준다\n[seaborn 참고이미지](https://chuyinchule.tistory.com/16)","metadata":{}},{"cell_type":"code","source":"v = meta[(meta.level == 'nominal') & (meta.keep)].index\n\nfor f in v:\n    plt.figure()\n    fig, ax = plt.subplots(figsize = (10,5))\n    \n    # Calculate the percentage of target = 1 per category value\n    cat_perc = train[[f, 'target']].groupby([f], as_index = False).mean()\n    cat_perc.sort_values(by = 'target', ascending = False, inplace = True)\n    \n    # Bar plot\n    # Order the bars descending on target mean\n    sns.barplot(ax = ax, x = f, y = 'target', data = cat_perc, order= cat_perc[f])\n    plt.ylabel('% Target', fontsize = 18)\n    plt.xlabel(f, fontsize = 18)\n    plt.tick_params(axis = 'both', which = 'major', labelsize = 18)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-29T08:51:25.913765Z","iopub.execute_input":"2021-08-29T08:51:25.914074Z","iopub.status.idle":"2021-08-29T08:51:27.905362Z","shell.execute_reply.started":"2021-08-29T08:51:25.914045Z","shell.execute_reply":"2021-08-29T08:51:27.903784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Categorical variables와 Target = 1인 고객 비율을 살펴보자.\n\nMissing value가 있는 변수에서 알 수 있듯이 \nMissing valued가 차지하는 범위가 크기에\nMissing value를 다른 값으로 대체하는 대신 별도의 범주 값으로 유지하는 것이 좋다.\n\nMissing value가 있는 고객은 보험 청구를 요청할 가능성이 훨씬 더 높은 것으로 보인다.","metadata":{}},{"cell_type":"markdown","source":"## 6.2 Interval variables ","metadata":{}},{"cell_type":"code","source":"def corr_heatmap(v):\n    correlations = train[v].corr()\n\n    # Create color map ranging between two colors\n    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n    fig, ax = plt.subplots(figsize=(10,10))\n    sns.heatmap(correlations, cmap=cmap, vmax=1.0, center=0, fmt='.2f',\n                square=True, linewidths=.5, annot=True, cbar_kws={\"shrink\": .75})\n    plt.show();\n    \nv = meta[(meta.level == 'interval') & (meta.keep)].index\ncorr_heatmap(v)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T08:51:27.907027Z","iopub.execute_input":"2021-08-29T08:51:27.907344Z","iopub.status.idle":"2021-08-29T08:51:28.79929Z","shell.execute_reply.started":"2021-08-29T08:51:27.907315Z","shell.execute_reply":"2021-08-29T08:51:28.798267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Interval variables 간의 상관 관계를 확인합니다.\nheatmap은 변수 간의 상관 관계를 시각화하는 좋은 방법입니다.\n아래의 변수들은 강한 상관 관계를 가집니다.\n* ps_reg_02 & ps_reg_03 (0.7)\n* ps_car_12 & ps_car13 (0.67)\n* ps_car_12 & ps_car14 (0.58)\n* ps_car_13 & ps_car15 (0.67)\n\nSeaborn은 변수들 사이의 (선형) 관계를 시각화할 수 있는 몇 가지 유용한 플롯을 가지고 있다. 우리는 변수들 사이의 관계를 시각화하기 위해 Pairplot 사용할 수 있습니다.\n하지만 Heatmap에서 이미 제한된 수의 상관 변수를 보여 주었기 때문에, 우리는 각각의 높은 상관 관계를 가진 변수들을 개별적으로 살펴보도록 하겠습니다.","metadata":{}},{"cell_type":"markdown","source":"### lmplot\n>lmplot은 x와 y의 상관관계를 파악하고 싶을 때, 이를 사용한다. \n 특히 여기서는 둘 feature간의 상관관계를 나타내기 위해서 사용한다. scatter_kws는 점의 크기를 설정하고 palette는 색을 정의한다.\n\nx，y ： x축 ,y축 이름 \ndata : 데이터 프레임\nfit_ref : 회귀선그리기\nhue : 항목별로 구별\n{scatter, line} _kws : 점의 속성 변경\nmarkers : 점의 모양 설정\n","metadata":{}},{"cell_type":"code","source":"# 속도를 높이기 위해 학습 데이터의 일부를 가져옵니다.\ns = train.sample(frac = 0.1)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T08:51:28.800603Z","iopub.execute_input":"2021-08-29T08:51:28.800872Z","iopub.status.idle":"2021-08-29T08:51:28.829577Z","shell.execute_reply.started":"2021-08-29T08:51:28.800846Z","shell.execute_reply":"2021-08-29T08:51:28.828568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.lmplot(x='ps_reg_02', y='ps_reg_03', data=s, hue='target',\n           palette='Set1', scatter_kws={'alpha': 0.3})\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-29T08:51:28.830987Z","iopub.execute_input":"2021-08-29T08:51:28.831304Z","iopub.status.idle":"2021-08-29T08:51:30.734759Z","shell.execute_reply.started":"2021-08-29T08:51:28.831276Z","shell.execute_reply":"2021-08-29T08:51:30.733702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ps_reg_02 및 ps_reg_03 regression line에서 알 수 있듯이 두 변수 사이에는 linear relationship이 있습니다.\nhue 매개 변수는 target = 0과 target = 1에 대한 regression line이 동일함을 알 수 있습니다.","metadata":{}},{"cell_type":"code","source":"sns.lmplot(x='ps_car_12', y='ps_car_13', data=s, hue='target',\n           palette='Set1', scatter_kws={'alpha': 0.3})\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-29T08:51:30.736089Z","iopub.execute_input":"2021-08-29T08:51:30.736443Z","iopub.status.idle":"2021-08-29T08:51:32.712015Z","shell.execute_reply.started":"2021-08-29T08:51:30.736403Z","shell.execute_reply":"2021-08-29T08:51:32.711311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.lmplot(x='ps_car_12', y='ps_car_14', data=s, hue='target',\n           palette='Set1', scatter_kws={'alpha': 0.3})\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-29T08:51:32.713013Z","iopub.execute_input":"2021-08-29T08:51:32.713453Z","iopub.status.idle":"2021-08-29T08:51:34.626545Z","shell.execute_reply.started":"2021-08-29T08:51:32.71341Z","shell.execute_reply":"2021-08-29T08:51:34.625335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.lmplot(x='ps_car_15', y='ps_car_13', data=s, hue='target',\n           palette='Set1', scatter_kws={'alpha': 0.3})\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-29T08:51:34.628082Z","iopub.execute_input":"2021-08-29T08:51:34.628622Z","iopub.status.idle":"2021-08-29T08:51:36.871111Z","shell.execute_reply.started":"2021-08-29T08:51:34.628538Z","shell.execute_reply":"2021-08-29T08:51:36.870054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**correlated variables**를 통해 PCA를 수행하여 차원을 줄일 수 있습니다.\n\n하지만 correlated variables의 수가 적기 때문에 모델이 heavy-lifting처럼 무겁게 수행됩니다.","metadata":{}},{"cell_type":"markdown","source":"## 6.3 Checking the correlations betwwen ordinal variables","metadata":{}},{"cell_type":"code","source":"v = meta[(meta.level == 'ordinal') & (meta.keep)].index\ncorr_heatmap(v)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T08:51:36.872466Z","iopub.execute_input":"2021-08-29T08:51:36.872778Z","iopub.status.idle":"2021-08-29T08:51:38.51879Z","shell.execute_reply.started":"2021-08-29T08:51:36.872747Z","shell.execute_reply":"2021-08-29T08:51:38.517983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ordinal 변수에서는 뚜렷한 correlations을 볼 수 없지만 \n\ntargeting value로 그룹화하면 어떻게 값이 분포되는지 알 수 있습니다.","metadata":{}},{"cell_type":"markdown","source":"<a name='7'></a>\n# 7. Feature engineering","metadata":{}},{"cell_type":"markdown","source":"## 7.1 Creating dummy variables","metadata":{}},{"cell_type":"code","source":"v = meta[(meta.level == 'nominal') & (meta.keep)].index\nprint(f'Before dummification we have {train.shape[1]} variables in train.')\ntrain = pd.get_dummies(train, columns= v, drop_first= True)\nprint(f'After dummification we have {train.shape[1]} variables in train.')","metadata":{"execution":{"iopub.status.busy":"2021-08-29T08:51:38.519917Z","iopub.execute_input":"2021-08-29T08:51:38.520406Z","iopub.status.idle":"2021-08-29T08:51:38.636244Z","shell.execute_reply.started":"2021-08-29T08:51:38.520362Z","shell.execute_reply":"2021-08-29T08:51:38.635198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Categorical variables의 값은 순서나 크기를 나타내지 않음. 예를들어, group\"2\" != group\"1\" * 2\n--> Categorical변수를 다룰 더미 변수를 만들 수 있다.\n\n[dummy 변수란?](https://kkokkilkon.tistory.com/37)\n총 52(109-57)개의 dummy 변수를 생성하였다.","metadata":{}},{"cell_type":"markdown","source":"## 7.2 Creating interaction variables","metadata":{}},{"cell_type":"code","source":"v = meta[(meta.level == 'interval') & (meta.keep)].index\npoly = PolynomialFeatures(degree = 2, interaction_only= False, include_bias= False)\ninteractions = pd.DataFrame(data = poly.fit_transform(train[v]), columns=poly.get_feature_names(v))\ninteractions.drop(v, axis = 1, inplace = True) # Remove the original columns\n\n# Concat the interaction variables to the train data\nprint(f'Before creating interactions we have {train.shape[1]} variables in train.')\n\ntrain = pd.concat([train, interactions], axis = 1)\n\nprint(f'After creating interactions we have {train.shape[1]} variables in train.')","metadata":{"execution":{"iopub.status.busy":"2021-08-29T08:51:38.637659Z","iopub.execute_input":"2021-08-29T08:51:38.638147Z","iopub.status.idle":"2021-08-29T08:51:39.000185Z","shell.execute_reply.started":"2021-08-29T08:51:38.638079Z","shell.execute_reply":"2021-08-29T08:51:38.99921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"get_feature_names 메서드를 사용하여 interactions variables을 추가한다.","metadata":{}},{"cell_type":"markdown","source":"<a name='8'></a>\n# 8. Feature selection\n## 8.1 Removing features with low or zero variance","metadata":{}},{"cell_type":"code","source":"selector = VarianceThreshold(threshold=0.01)\nselector.fit(train.drop(['id', 'target'], axis = 1)) # Fit to train without id and target variables\n\nf = np.vectorize(lambda x : not x) # Function to toggle boolean_array elements\nv = train.drop(['id', 'target'], axis = 1).columns[f(selector.get_support())]\nprint(f'{len(v)} variables have too low variance.')\nprint(f'These variables are {list(v)}')","metadata":{"execution":{"iopub.status.busy":"2021-08-29T08:51:39.001408Z","iopub.execute_input":"2021-08-29T08:51:39.001712Z","iopub.status.idle":"2021-08-29T08:51:39.664244Z","shell.execute_reply.started":"2021-08-29T08:51:39.001673Z","shell.execute_reply":"2021-08-29T08:51:39.663254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* 변동이 없거나 매우 낮은 특성을 제거하는 것이다.  **분산이 매우 작은 feature 삭제**\n* Sklearn에는 VarianceThreshold라는 편리한 방법이 있다. 기본적으로 분산이 0 인 기능을 제거한다.\n* 이전 단계에서 0 분산 변수가 없음을 확인 했으므로이 대회에는 적용되지 않습니다.\n* 그러나 분산이 1 % 미만인 특성을 제거하면 31 개의 변수가 제거됩니다.\n* 분산을 기반으로 선택하면 다소 많은 변수(31개)를 잃게됩니다. 그러나 변수가 너무 많지 않기 때문에 classifier가 선택하도록 할 것입니다.\n* 더 많은 변수가있는 데이터 세트의 경우 처리 시간을 줄일 수 있습니다.\n* Sklearn은 또한 다른 기능 선택 방법과 함께 제공됩니다.\n* 이러한 메서드 중 하나는 another classifier가 최상의 기능을 선택하고 계속 진행하도록하는 SelectFromModel입니다.\n* 아래에서는 Random Forest로 수행하겠습니다.","metadata":{}},{"cell_type":"markdown","source":"## 8.2 Selecting features with a Random Forest and SelectFromModel","metadata":{}},{"cell_type":"markdown","source":"RandomForest에서 선택한 Feature의 중요성을 파악하고 \n\nSelectFromModel에서 RandomForest에서 고른 Feature들의 평균값 보다 큰 feature들을 선택한다.","metadata":{}},{"cell_type":"code","source":"X_train = train.drop(['id', 'target'], axis=1)\ny_train = train['target']\n\nfeat_labels = X_train.columns\n\nrf = RandomForestClassifier(n_estimators=1000, random_state=0, n_jobs=-1)\n\nrf.fit(X_train, y_train)\nimportances = rf.feature_importances_\n\nindices = np.argsort(rf.feature_importances_)[::-1]\n# np.argsort 함수는 배열을 오름차순으로 정렬할 수 있는 인덱스들을 반환한다. 배열 [3, 1, 2]에 argsort 함수를 적용하면 [1, 2, 0]을 반환한다. \n# [::-1]에 의해 내림차순으로 정렬할 수 있는 인덱스가 된다.\n\nfor f in range(X_train.shape[1]):\n    print(\"%2d) %-*s %f\" % (f + 1, 30,feat_labels[indices[f]], importances[indices[f]]))","metadata":{"execution":{"iopub.status.busy":"2021-08-29T08:54:10.025941Z","iopub.execute_input":"2021-08-29T08:54:10.026359Z","iopub.status.idle":"2021-08-29T09:05:23.920666Z","shell.execute_reply.started":"2021-08-29T08:54:10.026323Z","shell.execute_reply":"2021-08-29T09:05:23.918197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"여기서는 랜덤 포레스트의 feature importances를 기준으로 기능 선택을 할 것입니다.\nSklearn의 SelectFromModel을 사용하면 유지할 변수 수를 지정할 수 있습니다.\nfeature importances 수준에 대한 threshold를 수동으로 설정할 수 있습니다.\n그러나 우리는 단순히 상위 50 % 최고의 변수를 선택합니다.\n위의 코드는 Sebastian Raschka의 GitHub 저장소에서 가져 왔습니다.","metadata":{}},{"cell_type":"code","source":"sfm = SelectFromModel(rf, threshold='median', prefit=True)\nprint(f'Number of features before selection : {X_train.shape[1]}')\n\nn_features = sfm.transform(X_train).shape[1]\nprint(f'Number of features after selection : {n_features}')\nselected_vars = list(feat_labels[sfm.get_support()])","metadata":{"execution":{"iopub.status.busy":"2021-08-29T09:09:11.023943Z","iopub.execute_input":"2021-08-29T09:09:11.024398Z","iopub.status.idle":"2021-08-29T09:09:12.132962Z","shell.execute_reply.started":"2021-08-29T09:09:11.024362Z","shell.execute_reply":"2021-08-29T09:09:12.132051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"SelectFromModel을 사용하여 사용할 prefit classifier와 feature importances에 대한 threshold을 지정할 수 있습니다.\n\nget_support 메소드를 사용하면 train 데이터의 변수 수를 제한 할 수 있습니다.","metadata":{}},{"cell_type":"code","source":"# train 데이터에 target까지 추가\ntrain = train[selected_vars + ['target']]","metadata":{"execution":{"iopub.status.busy":"2021-08-29T09:09:17.778073Z","iopub.execute_input":"2021-08-29T09:09:17.778631Z","iopub.status.idle":"2021-08-29T09:09:17.824718Z","shell.execute_reply.started":"2021-08-29T09:09:17.778582Z","shell.execute_reply":"2021-08-29T09:09:17.823547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name='9'></a>\n# 9. Feature scaling","metadata":{}},{"cell_type":"code","source":"scaler = StandardScaler()\nscaler.fit_transform(train.drop(['target'], axis = 1))","metadata":{"execution":{"iopub.status.busy":"2021-08-29T09:09:21.289601Z","iopub.execute_input":"2021-08-29T09:09:21.289944Z","iopub.status.idle":"2021-08-29T09:09:21.863145Z","shell.execute_reply.started":"2021-08-29T09:09:21.289914Z","shell.execute_reply":"2021-08-29T09:09:21.861971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"train 데이터에 standardscaler를 적용 할 수 있습니다.\n\n이 작업이 완료되면 일부 classifier가 더 잘 작동됩니다.","metadata":{}}]}