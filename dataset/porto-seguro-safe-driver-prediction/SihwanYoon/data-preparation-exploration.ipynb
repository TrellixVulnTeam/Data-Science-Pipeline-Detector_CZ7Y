{"cells":[{"metadata":{"_uuid":"ecb402a1b7efe9eb6db47e7cab2c38861609ad94","_cell_guid":"9113065c-6cec-4d35-8daa-57e9fdc6313e"},"cell_type":"markdown","source":"## Introduction"},{"metadata":{"_uuid":"7f8a20e1db4cf09d9e53245de8461ce409447649","_cell_guid":"2dee8ae8-2bee-4609-96a8-8150b7e770cd"},"cell_type":"markdown","source":"### 해당 커널은 Data Preparation & Exploration의 한글 번역본입니다. https://www.kaggle.com/bertcarremans/data-preparation-exploration\n\n이 노트북은 PorteSerguro Competition에서 멋진 인사이트를 얻기 위해 작성됐습니다. <br>\n뿐만 아니라 데이터 모델링을 준비하기 위한 Tip과 Tricks들을 준비했습니다. <br>\n목차는 다음과 같습니다. \n\n1. [Visual inspection of your data](#visual_inspection)\n2. [Defining the metadata](#metadata)\n3. [Descriptive statistics](#descriptive_stats)\n4. [Handling imbalanced classes](#imbalanced_data)\n5. [Data quality checks](#data_quality)\n6. [Exploratory data visualization](#eda)\n7. [Feature engineering](#feat_engineering)\n8. [Feature selection](#feat_selection)\n9. [Feature scaling](#feat_scaling)"},{"metadata":{"_uuid":"1fa2f6de517095b84ec92276aa092ed42b17c963","_cell_guid":"10e44fab-4986-4a1a-aea4-71a75dde226e"},"cell_type":"markdown","source":"## Loading packages"},{"metadata":{"_uuid":"d476415eb97241fd781c60b072dbdd7d687f3c56","_cell_guid":"9e5063ed-1cdb-40a1-8ef7-6db3eae052e8","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.utils import shuffle\nfrom sklearn.ensemble import RandomForestClassifier\n\npd.set_option('display.max_columns', 100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"from sklearn.preprocessing import Imputer가 \nfrom sklearn.impute import SimpleImputer 로 바뀌었습니다."},{"metadata":{"_uuid":"619ddb7e5af828b65a422bc5096b77339e089998","_cell_guid":"4999c076-25c2-49ca-a9b5-e588a22a3483"},"cell_type":"markdown","source":"## Loading data"},{"metadata":{"_uuid":"9ec9e8d9a85ae83c7659b08b10ee0cbd348fd189","_cell_guid":"90743a5b-947a-4606-b069-2184c46d360c","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"76e1f30f15f393e53d0beb081c3662a72bdf6f5b","_cell_guid":"2d6af15a-5d84-42e7-a9ff-66747f77c02f"},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"visual_inspection\"></a>"},{"metadata":{"_uuid":"70d5a5616d07b4cab8f95af01d9439f2f48d020c","_cell_guid":"7fc6f68b-a256-49c5-962f-7d111189b0b2"},"cell_type":"markdown","source":"## Data at first sight"},{"metadata":{"_uuid":"40bef8a97ee39315a8cdf242e286d82cb99d5476","_cell_guid":"2ef0379a-bb4f-4aaf-b6dd-1d6de4290ad3"},"cell_type":"markdown","source":"[](http://)데이터에 대한 정보들:\n* 비슷한 Group에 태그된 Feature들은 비슷한 이름을 가지고 있습니다 (예를 들어, ind, reg, car, calc).\n* **bin** 이라는 접미사를 가진 Feature는 Binary feature임을 나타내고, **cat** 이라는 접미사를 가진 Feature는 Categorical feature임을 나타냅니다.\n* 이외의 Feature들은 **Continious** 혹은 **Ordinal feature** 입니다. \n* 값이 -1 인 관측치는 **결측값(NaN)**을 의미합니다\n* **Target** 열은 Policy holder에게 청구 적용 여부(Y/N)를 의미합니다\n\n중요한 정보에 대해 파악을 했습니다! <br>\n데이터의 전체적인 모습을 확인하기 위하여 앞부분(head)과 뒷부분(tail)을 먼저 확인해보도록 합니다."},{"metadata":{"_uuid":"625795d926a8a30f82afce4326788d2426baf11f","_cell_guid":"11f69996-f99e-4416-a16c-7f3c32d735b2","trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d1b2e1027f17fb5ea238e55d3ddba02bc4787e75","_cell_guid":"575e4fe9-7cc6-47a6-90dc-45ff15aaeb34","trusted":true},"cell_type":"code","source":"train.tail()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"747c8bac1a0549b803325f2de3c2173b384ed9e0","_cell_guid":"8a25bd9a-88cf-4dc3-8640-f5006b9d343c"},"cell_type":"markdown","source":"우리는 다음과 같은 정보를 확인할 수 있습니다.\n* binary variables\n* 값이 정수로 이루어진 categorical variables\n* 값이 정수 혹은 소수로 이루어진 other variables\n* 관측값이 -1인 값은 결측치(NaN)를 나타냅니다.\n* Target Variables와 ID"},{"metadata":{"_uuid":"c642b9834400a789f560d53dabb5c63ebbbe4196","_cell_guid":"bcf7b21e-134b-4be6-b352-a5d8eff5b23e"},"cell_type":"markdown","source":"shape를 활용하여 전체 데이터의 행과 열 개수를 확인합니다."},{"metadata":{"_uuid":"cf8d1a98a0655cdcf1e12cb84dff9e61430a7eeb","_cell_guid":"8ae01653-e1c9-4209-8bb1-2734f8d1302c","trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c86138e65f4c51c83d3dddc31352739cc24a1771","_cell_guid":"4317596e-2187-4660-973a-1f53a2f26d66"},"cell_type":"markdown","source":"59개의 variables와 595,212개의 row가 있습니다. <br>\n다음으로 test 데이터도 똑같은 수의 variables가 있는지 확인해봅니다. <br>\n그 전에, training 데이터에 중복값이 있는지 확인합니다."},{"metadata":{"_uuid":"c8caf45461f3a4699fbdea55036a5954d9668437","_cell_guid":"8849f509-7745-465b-83ab-3f1240a7b05c","trusted":true},"cell_type":"code","source":"train.drop_duplicates()\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"36f73dc547915f8f76d1bf8353ad55b88f7458b1","_cell_guid":"6250b262-91a0-4f8c-8ebe-1e77f2fd014b"},"cell_type":"markdown","source":"중복값이 없습니다."},{"metadata":{"_uuid":"625dfedf9b72916b41bac4d65162c8795f629ef5","_cell_guid":"166c851a-fec1-4b56-9773-42c0387c6ec8","trusted":true},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8497a0ad66cfaabdae29008ba5858eea6220237a","_cell_guid":"6026084c-7c7f-41ec-8997-71092654c88b"},"cell_type":"markdown","source":"training 데이터와 달리 test 데이터에는 **58개**의 variables가 있습니다. <br>\n하지만 이 variable은 target variable이므로 상관 없습니다. <br>\n\n이제는 우리가 가진 variables들의 타입을 조사해보도록 하겠습니다."},{"metadata":{"_uuid":"b3811d638cd0ddc29c313909c484c09266955e2a","_cell_guid":"3acba6c1-0a13-4fc8-a44f-4d875231e305"},"cell_type":"markdown","source":"추후에 우리는 14개의 Categorial variables를 더미화 시킬 것입니다. <br>\n접미사로 **bin**이 붙은 variables는 이미 0과 1로 구성되어 있으므로 따로 더미화시킬 필요가 없습니다."},{"metadata":{"_uuid":"72b79686592a00d17cf3f7cb21c96297bb5c73e4","_cell_guid":"d6a388dc-5f8a-41fc-96be-2482d0a3065b","trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6f1c3ceabc4efc0f500701df2ea9843e9f4a5e19","_cell_guid":"3604dd16-8583-43c2-bc6d-8399769b5ce6"},"cell_type":"markdown","source":"info() 메소드를 사용함으로써 우리는 다시 한 번 데이터들의 타입이 integer 혹은 floatd임을 확인했습니다. info() 메소드 상으로는 결측값이 없는 것으로 확인됩니다. 앞서 언급했듯이 결측값이 -1로 대치되어있기 때문입니다. 우리는 이를 나중에 다루도록 하겠습니다."},{"metadata":{"_uuid":"99afca7b33389f5804fed9a726a93ef43de5447f","_cell_guid":"5b42fec3-d32b-4da5-b0f9-981671c30bda"},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"metadata\"></a>"},{"metadata":{"_uuid":"b5fee402ea3679bfe67a73d26ccb822f7c137e61","_cell_guid":"02caecab-1d07-4c23-adfa-75f695820466"},"cell_type":"markdown","source":"## Metadata\n\n데이터 관리를 용이하게 하기 위해서, 우리는 variables의 정보들을 데이터프레임의 형태로 저장하고자 합니다. 이 메타 데이터들은 분석에 필요한 특정한 variables를 선택할 때, 시각화를 할 때, 모델링을 할 때 등에 도움이 될 것입니다.\n\n구체적으로 우리가 저장해야 할 것들은 다음과 같습니다 :\n- **role**: input, ID, target\n- **level**: nominal, interval, ordinal, binary\n- **keep**: True or False\n- **dtype**: int, float, str"},{"metadata":{"_uuid":"e2f6389d37921cf5487c3a959c18ca9d24348567","_cell_guid":"21cb4f9d-bfb4-41cc-a3f6-f1977d372919","trusted":true},"cell_type":"code","source":"data = []\nfor f in train.columns:\n    # role을 정의합니다.\n      # target과 id를 지정해준 뒤, 나머지는 모두 input으로 지정합니다.\n    if f == 'target':\n        role = 'target'\n    elif f == 'id':\n        role = 'id'\n    else:\n        role = 'input'\n         \n    # level을 정의합니다.\n      # target과 bin은 binary, id와 cat은 nominal, 나머지는 데이터 타입에 따라 float과 int로 지정합니다.\n    if 'bin' in f or f == 'target':\n        level = 'binary'\n    elif 'cat' in f or f == 'id':\n        level = 'nominal'\n    elif train[f].dtype == 'float64':\n        level = 'interval'\n    elif train[f].dtype == 'int64':\n        level = 'ordinal'\n        \n    # 아이디를 제외한 모든 variables를 True로 지정합니다.    \n    keep = True\n    if f == 'id':\n        keep = False\n    \n    # 데이터 타입을 지정합니다. \n    dtype = train[f].dtype\n    \n    # 모든 variable의 메타 데이터를 담은 딕셔너리를 생성합니다.\n    f_dict = {\n        'varname': f,\n        'role': role,\n        'level': level,\n        'keep': keep,\n        'dtype': dtype\n    }\n    data.append(f_dict)\n    \nmeta = pd.DataFrame(data, columns=['varname', 'role', 'level', 'keep', 'dtype'])\nmeta.set_index('varname', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"원본에서는 메타 데이터의 level을 구분할때 train[f].dtype == float 과 같은 형식으로 코드가 작성됐습니다. 제 코드에서는 'float64'로 수정이 되어 있는데, 기존의 코드대로 진행하면 dtype 부분에서 원하는대로 데이터가 구분되지 않습니다. 데이터 형식 뒤에 **64** 도 반드시 입력해줘야 합니다!"},{"metadata":{"_uuid":"ddbf4683c3c5aeaa1472d82376375f9618b8926e","_cell_guid":"92fb93fd-d907-4221-8063-81880811e922","trusted":true},"cell_type":"code","source":"meta","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"00d5e1e9bc7175fa91974f6fcc0bb0b9861d757c","_cell_guid":"0049858b-f575-4f69-b6c2-562ffa10c3cf"},"cell_type":"markdown","source":"예시로 level이 nominal인 데이터의 인덱스를 추출해봅니다."},{"metadata":{"_uuid":"21b7012bf25c31060a7e7b9a1139246d1edfeec5","_cell_guid":"dc400033-7d77-4456-8913-f4de5c74a99c","trusted":true},"cell_type":"code","source":"meta[(meta.level == 'nominal') & (meta.keep)].index","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ce085886b6ff7b3a909515ae4c102726eca81f90","_cell_guid":"9ab11fa8-030b-4339-82f7-2e9f02fd356d"},"cell_type":"markdown","source":"role과 level에 따른 target의 수를 아래를 통해 확인해봅니다."},{"metadata":{"_uuid":"4a52ea8b5a9df61368ac239e1953e3d23e6e722a","_cell_guid":"ec2dd532-5d60-472a-9816-02bb02a79bdb","trusted":true},"cell_type":"code","source":"pd.DataFrame({'count' : meta.groupby(['role', 'level'])['role'].size()}).reset_index()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d07abccb011c4de4154df274d691edbf875ec3e2","_cell_guid":"628d9ce5-f07f-4f4f-9dac-bfd1a49dd4e9"},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"descriptive_stats\"></a>"},{"metadata":{"_uuid":"47800f2eb3e5741591b161f6cf3f0e78c1a20ff8","_cell_guid":"d25e679c-dc55-46cb-a2fa-077da401f8e7"},"cell_type":"markdown","source":"## Descriptive statistics"},{"metadata":{"_uuid":"f32de991963ae1ff014705c3b9446e76c103d717","_cell_guid":"f0486d67-ddb2-4e64-b9c4-7ede3a8a1754"},"cell_type":"markdown","source":"이제 데이터프레임에 *describe* 메소드를 사용하여 기술통계량을 살펴보도록 합니다. 그러나 *desecribe* 메소드는 categorical, id variable의 기술통계량은 계산해주지 않습니다. 따라서 추후에 catgorical variables를 살펴보도록 합니다.\n\n메타 파일을 통하여 손쉽게 기술통계량을 계산할 수 있습니다."},{"metadata":{"_uuid":"44aba55b3de816f8d9696b12e5e7c4e1bde4532b","_cell_guid":"1cb950e7-c7db-4bbb-ba74-f94170e8446e"},"cell_type":"markdown","source":"### Interval variables"},{"metadata":{"_uuid":"d445810faefa50cc233d92e9fb5ab5d050dfd77d","_cell_guid":"7bad38f8-a60e-49ac-9b42-ed3f25c018cf","trusted":true},"cell_type":"code","source":"v = meta[(meta.level == 'interval') & (meta.keep)].index\ntrain[v].describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"efd6d92517e7dde451d66a6509ec9b8300b0aacc","_cell_guid":"57db3176-f8dc-4aa8-86c8-e198a2e46bc5"},"cell_type":"markdown","source":"#### reg variables\n- 오직 **ps_reg_03** 만이 결측값을 가지고 있습니다. \n- variables 사이의 범위 (min to max)를 고려했을 때, 우리는 추후에 스케일링(예를 들어 StandardScaler)을 적용할 수도 있습니다. 하지만 우리가 사용하고자 하는 분류기에 따라 다를 것입니다.\n\n#### car variables\n- ps_car_12 와 ps_car_14 가 결측치를 가지고 있습니다.\n- 이 variables 역시, 스케일링이 필요해보입니다.\n\n#### calc variables\n- 결측치가 없습니다.\n- 이 variables는 최대값이 0.9인것으로 보아 일종의 비율인 것 같습니다.\n- 모든 3개의 *_calc* variables은 매우 비슷한 분포를 가지고 있습니다.\n\n\n**전반적으로**, interval variables들 간의 범위가 상대적으로 좁음을 확인 가능합니다. 아마도 데이터를 익명화시키기 위하여 몇몇 변환 작업(예를 들어 log)가 이미 적용된 것은 아닐까요?\n\n#### 기술통계를 살펴봄으로써 다음과 같은 결과를 얻었습니다.\n- Feature 내부에 결측치의 존재 유무\n- min과 max를 비교함으로써 스케일링의 필요성 판단\n- max 값을 살펴봄으로써 변수의 값이 비율인지 판단\n\n"},{"metadata":{"_uuid":"fdc9923f3a5e7b8a39c5944fccad494572795da6","_cell_guid":"6790f9d3-2ad0-4d6c-836d-e373460cf6a4"},"cell_type":"markdown","source":"### Ordinal variables"},{"metadata":{"_uuid":"1100d8fed9046398cba3faa68766727963012f3f","_cell_guid":"761bdb24-c3dc-44f5-b59b-44604841b616","trusted":true},"cell_type":"code","source":"v = meta[(meta.level == 'ordinal') & (meta.keep)].index\ntrain[v].describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"caaa8c80af099a150e10dc03742d3ef619cd3973","_cell_guid":"37b6f815-598a-4fea-b776-88b0862305ca"},"cell_type":"markdown","source":"- 결측치를 가지는 variable은 ps_car_11입니다.\n- 다른 범위를 가지는 값들에 대하여 스케일링이 필요해보입니다."},{"metadata":{"_uuid":"11e7bad44a795c3010f9a40bb8460ca0476fcc31","_cell_guid":"2ad928ad-9d2f-49cc-b84f-1950c4403346"},"cell_type":"markdown","source":"### Binary variables"},{"metadata":{"_uuid":"6be807e266e11f6ca095b19cd4624af7a93f00c2","_cell_guid":"9c3b1993-eb33-4eef-859f-97ef4d0b5811","trusted":true},"cell_type":"code","source":"v = meta[(meta.level == 'binary') & (meta.keep)].index\ntrain[v].describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"77b978ce89fc36e107513b534fec7a9a0f8cb543","_cell_guid":"6c03a6ec-3fd2-47e9-80b1-6407500c1285"},"cell_type":"markdown","source":"- train 데이터의 target 평균은 0.0036448로 3.645%입니다. 결과값이 **매우 불균형함**을 알 수 있습니다. \n- 평균값을 통해 대다수의 variables가 0이라고 결론내릴 수 있습니다."},{"metadata":{"_uuid":"02cad71ec7e2a7370ec81655f6320bb88ce43a1e","_cell_guid":"f226ac92-3d30-41ce-ba53-2a2813159843"},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"imbalanced_data\"></a>"},{"metadata":{"_uuid":"5b4380025b455201e73703d36ae805b06d05c02d","_cell_guid":"471736eb-598a-4b2a-a8c2-c091ecd5f945"},"cell_type":"markdown","source":"## Handling imbalanced classes\n\n위에 언급했듯이 target=1 의 비율이 traget=0 비율보다 매우 적습니다(0.963 VS 0.036).\n결과값이 불균형한 모델은 높은 정확도를 가지지만 실제로는 부가적인 value가 추가될 수 있습니다. 이러한 문제를 해결하기 위해서 두 가지 방법을 사용할 수 있습니다.\n\n* oversampling records with target=1 \n* undersampling records with target=0\n\n우리는 큰 training set을 가지고 있으므로, **undersampling**을 진행하겠습니다. 비율은 0.9 : 0.1로 지정합니다. 이를 통해 우리는 매우 불균형한 결과값 데이터는 10% 미만임을 확인 가능하며, 10% 정도로 undersampling을 진행하는 것을 확인할 수 있습니다."},{"metadata":{"collapsed":true,"_uuid":"4078855ef511dbbb2306f861e4b82ad45620e36f","_cell_guid":"e933f241-74eb-46ed-a7a8-02215f170db5","trusted":false},"cell_type":"code","source":"desired_apriori=0.10\n\n# target value의 인덱스를 추출합니다.\nidx_0 = train[train.target == 0].index\nidx_1 = train[train.target == 1].index\n\n# target value의 기존 record 수를 구합니다.\nnb_0 = len(train.loc[idx_0]) # 573518 개\nnb_1 = len(train.loc[idx_1]) # 21694 개\n\n# undersampling 비율을 계산하고 target == 0 인 record 수를 계산합니다.\nundersampling_rate = ((1-desired_apriori)*nb_1)/(nb_0*desired_apriori)\n# ((1-0.1) * 573518) / (0.1 * 21694)\n# Undersampling_Rate 계산 공식을 암기해둡시다.\nundersampled_nb_0 = int(undersampling_rate*nb_0)\n\nprint('Rate to undersample records with target=0: {}'.format(undersampling_rate))\nprint('Number of records with target=0 after undersampling: {}'.format(undersampled_nb_0))\n\n# shuffle을 활용하여 undersampling 된 개수만큼의 samples를 가지는 nb=0을 무작위로 추출합니다.\nundersampled_idx = shuffle(idx_0, random_state=37, n_samples=undersampled_nb_0)\n\n# 추출한 인덱스와 기존의 idx_1을 활용하여 리스트를 만듭니다.\nidx_list = list(undersampled_idx) + list(idx_1)\n\n# Undersample된 데이터 프레임을 돌려받습니다.\ntrain = train.loc[idx_list].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f48e8ee8ed611da6c42de9c595fdaf34ecf5f23b","_cell_guid":"209074dd-ee1f-465c-8cd7-d0b7b692f54f"},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"data_quality\"></a>"},{"metadata":{"_uuid":"a0fc1970db23b67afabe087dd724764f4eef62cf","_cell_guid":"a1a31824-fa60-4140-ab09-72582a6ea446"},"cell_type":"markdown","source":"## Data Quality Checks"},{"metadata":{"_uuid":"9304c0f92541692305b9fe77705d55a8ffb7d11a","_cell_guid":"19c7aeed-e009-43fb-a462-4a39707b3b86"},"cell_type":"markdown","source":"### Checking missing values\n결측값은 -1로 나타내지고 있습니다."},{"metadata":{"_uuid":"2d4770d2d4b49b2a0b62d84f3e4d57382d36f2cf","_cell_guid":"46adfb57-916f-4cf8-87ae-f03ad861e621","trusted":true},"cell_type":"code","source":"vars_with_missing = []\n\nfor f in train.columns:\n    missings = train[train[f] == -1][f].count()\n    if missings > 0:\n        vars_with_missing.append(f)\n        missings_perc = missings/train.shape[0]\n        \n        print('Variable {} has {} records ({:.2%}) with missing values'.format(f, missings, missings_perc))\n        \nprint('In total, there are {} variables with missing values'.format(len(vars_with_missing)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"869694ad63ab68d25588768879d26b49d7624d09","_cell_guid":"88daeb5b-9dbf-4e15-af12-e952aac4b874"},"cell_type":"markdown","source":"- **ps_car_03_cat 과 ps_car_05_cat** 는 높은 결측치 비율을 가지고 있습니다 (68.39%, 44.26%). 따라서 삭제해주도록 합니다.\n- 결측값이 있는 다른 Cat Variables는 결측값을 -1 그대로 둘 수 있습니다.\n- **ps_reg_03** (continuous)은 18%의 결측값을 지니고 있습니다. 평균값으로 대치해줍니다.\n- **ps_car_11** (ordinal)은 5개의 결측값을 지니고 있습니다. ordinal의 형태이므로 평균값으로 대치하면 안됩니다. 최빈값으로 대치해줍니다.\n- **ps_car_12** (continuous)은 1개의 결측값을 지니고 있습니다. 평균값으로 대치해줍니다.\n- **ps_car_14** (continuous)은 7%의 결측값을 지니고 있습니다. 평균값으로 대치해줍니다."},{"metadata":{"_uuid":"9cbd1e802e35b7d3e21ed696084d12d777210b3f","_cell_guid":"183be251-710e-408f-b72e-1e5f40d1890c","trusted":true},"cell_type":"code","source":"# 너무 많은 결측값을 지닌 Feature들을 제거합니다 (68.4%, 44.3%)\nvars_to_drop = ['ps_car_03_cat', 'ps_car_05_cat']\ntrain.drop(vars_to_drop, inplace=True, axis=1)\nmeta.loc[(vars_to_drop),'keep'] = False  # 메타데이터를 업데이트해줍니다.\n\n# 결측값을 Imputer를 활용하여 변환해줍니다.\nmean_imp = SimpleImputer(missing_values=-1, strategy='mean')\nmode_imp = SimpleImputer(missing_values=-1, strategy='most_frequent')\ntrain['ps_reg_03'] = mean_imp.fit_transform(train[['ps_reg_03']]).ravel()\ntrain['ps_car_12'] = mean_imp.fit_transform(train[['ps_car_12']]).ravel()\ntrain['ps_car_14'] = mean_imp.fit_transform(train[['ps_car_14']]).ravel()\ntrain['ps_car_11'] = mode_imp.fit_transform(train[['ps_car_11']]).ravel()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"279507e3c1143e9d786edaa95e57f8ae42bed1cd","_cell_guid":"b137e94d-b57d-4761-830f-30d6a512fde2"},"cell_type":"markdown","source":"### Checking the cardinality of the categorical variables\n\n카디날리티는 전체 행에 대한 특정 컬럼의 중복 수치를 나타내는 지표입니다.<br>\n중복도가 높으면 카디날리티가 낮으며, 중복도가 낮으면 카디날리티가 높습니다. <br>\n카디날리티는 상대적인 개념으로 이해해야 합니다.\n\n따라서 카디날리티는 variable 내에서 다른 value의 개수를 말합니다. 우리는 추후 categorical variables를 더미화시킬 것인데, variables 내에 다른 value들이 얼마나 많은지 체크해봐야 합니다.Value들이 많을 경우, 수 많은 더미 변수들이 만들어질 수 있기 때문입니다."},{"metadata":{"_uuid":"1cd88947790cf6422806529e62c9751dcbf0db89","_cell_guid":"55d5b4b2-f6f8-4c0d-9335-10c0e8d9e91e","trusted":true},"cell_type":"code","source":"v = meta[(meta.level == 'nominal') & (meta.keep)].index\n\nfor f in v:\n    dist_values = train[f].value_counts().shape[0]\n    print('Variable {} has {} distinct values'.format(f, dist_values))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d7f83fa79ddf0d76857c5a3bdb909c271574eab3","_cell_guid":"8662f47c-27b4-4f88-bccc-f229890ebc4d"},"cell_type":"markdown","source":"합리적이긴 하지만, **ps_car_11_cat**는 104개로 매우 많은 Value를 가지고 있습니다.\n\n**EDIT** : 최초 작성자분은 104개의 Value에 대해 가공을 하여 데이터 손실이 있었던 것으로 보입니다. 이후 최초 작성자분은 Oliver의 커널을 활용한 방법을 사용했습니다."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Script by https://www.kaggle.com/ogrellier\n# Code: https://www.kaggle.com/ogrellier/python-target-encoding-for-categorical-features\ndef add_noise(series, noise_level):\n    return series * (1 + noise_level * np.random.randn(len(series)))\n\ndef target_encode(trn_series=None, \n                  tst_series=None, \n                  target=None, \n                  min_samples_leaf=1, \n                  smoothing=1,\n                  noise_level=0):\n    \"\"\"\n    Smoothing is computed like in the following paper by Daniele Micci-Barreca\n    https://kaggle2.blob.core.windows.net/forum-message-attachments/225952/7441/high%20cardinality%20categoricals.pdf\n    trn_series : training categorical feature as a pd.Series\n    tst_series : test categorical feature as a pd.Series\n    target : target data as a pd.Series\n    min_samples_leaf (int) : minimum samples to take category average into account\n    smoothing (int) : smoothing effect to balance categorical average vs prior  \n    \"\"\" \n    assert len(trn_series) == len(target)\n    assert trn_series.name == tst_series.name\n    temp = pd.concat([trn_series, target], axis=1)\n    # Compute target mean \n    averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n    # Compute smoothing\n    smoothing = 1 / (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) / smoothing))\n    # Apply average function to all target data\n    prior = target.mean()\n    # The bigger the count the less full_avg is taken into account\n    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n    # Apply averages to trn and tst series\n    ft_trn_series = pd.merge(\n        trn_series.to_frame(trn_series.name),\n        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n        on=trn_series.name,\n        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n    # pd.merge does not keep the index so restore it\n    ft_trn_series.index = trn_series.index \n    ft_tst_series = pd.merge(\n        tst_series.to_frame(tst_series.name),\n        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n        on=tst_series.name,\n        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n    # pd.merge does not keep the index so restore it\n    ft_tst_series.index = tst_series.index\n    return add_noise(ft_trn_series, noise_level), add_noise(ft_tst_series, noise_level)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"필자는 커널을 번역하면서 필사를 하는 중에 상기 과정이 정확히 어떻게 이루어지는지 이해하기 어려웠습니다. 기본적으로 **너무 Value가 많은 Categorical Variables를 가공하는 과정**임은 여러분들도 이해하고 계실 것입니다.\n\n\n따라서 함수의 과정을 하나하나 따라가보도록 하겠습니다.<br>\n저처럼 이해가 안되셨던 분이라면 함께 하시면 좋을 것 같습니다."},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_noise(series, noise_level):\n    return series * (1 + noise_level * np.random.randn(len(series)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Noise를 일으키는 함수를 정의한 것 같습니다.\n\n시리즈값과 노이즈 레벨을 변수로 받아서, <br>\n시리즈 * (1 + 노이즈 레벨 * 표준정규분포로부터 샘플링된 난수) 를 되돌려줍니다.\n\ntarget encode 함수는 assert부터 진행하겠습니다."},{"metadata":{"trusted":true},"cell_type":"code","source":"assert len(train[\"ps_car_11_cat\"]) == len(train['target'])\nassert train[\"ps_car_11_cat\"].name == test[\"ps_car_11_cat\"].name","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"저희가 사용할 trn_series는 train['ps_car_11_cat'] 이고, target은 train['target'] 입니다. <br>\n두 시리즈의 길이가 같은지 확인하고, train['ps_car_11_cat']과 test['ps_car_11_cat']의 이름이 같은지도 확인해줍니다. "},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = pd.concat([train[\"ps_car_11_cat\"], train['target']], axis=1)\nprint(temp)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"두 시리즈를 열을 기준으로 concat 한 뒤, temp라는 이름의 변수로 저장해줍시다."},{"metadata":{"trusted":true},"cell_type":"code","source":"averages = temp.groupby(train['ps_car_11_cat'].name)[train['target'].name].agg(['mean', 'count'])\nprint(averages)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"target 열의 값들을 train['ps_car_11_cat'] 기준으로 그룹화한뒤 mean 함수와 count 함수를 적용한 값을 averages 변수에 저장합니다.\n\navearges 함수는, <br>\n각 Value별 target 평균과 횟수 정보를 담고 있습니다."},{"metadata":{"trusted":true},"cell_type":"code","source":"smoothing = 1 / (1 + np.exp(-(averages[\"count\"] - 100) / 10))\nprint(smoothing)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"정확히 **Smoothing**이 무슨 작업을 하는지는 좀 더 공부해봐야할 것 같습니다."},{"metadata":{"trusted":true},"cell_type":"code","source":"prior = train['target'].mean()\n\naverages[train['target'].name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\nprint(averages)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"prior 값은 train['target']의 평균값으로 합니다.\n앞서 저희는 undersampling을 통해 0과 1의 비율을 9:1로 맞추었기 때문에 prior값은 **0.1**이 됩니다.\n\n이후 averages에 target이라는 이름을 가진 열을 추가해줍니다. 값은 Smoothing을 활용하여 변환됩니다.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"averages.drop(['mean', 'count'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"필요한 값은 Smoothing한 값뿐인 것 같습니다. drop을 활용하여 mean과 count를 빼줍니다."},{"metadata":{"trusted":true},"cell_type":"code","source":"ft_trn_series = pd.merge(\n        train['ps_car_11_cat'].to_frame(train['ps_car_11_cat'].name),\n        averages.reset_index().rename(columns={'index': train['target'].name, train['target'].name: 'average'}),\n        on=train['ps_car_11_cat'].name,\n        how='left')['average'].rename(train['ps_car_11_cat'].name + '_mean').fillna(prior)\n\nprint(ft_trn_series)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. ps_car_11_cat 시리즈를 to_frame으로 데이터 프레임으로 가져옵니다.\n2. averages의 인덱스를 초기화하고, 인덱스 값의 명칭을 target, target이었던 열 이름을 average로 바꿔줍니다.\n3. on값과 how값을 지정해주어 merge 해줍니다.\n4. 시리즈의 명칭을 ps_car_11_cat_mean으로 rename해주고 결측값은 prior 값으로 대치해줍니다."},{"metadata":{"trusted":true},"cell_type":"code","source":"ft_trn_series.index = train[\"ps_car_11_cat\"].index ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"새롭게 만든 시리즈의 인덱스를 기존 트레이닝 데이터 시리즈 인덱스와 맞춰줍니다"},{"metadata":{"trusted":true},"cell_type":"code","source":"ft_tst_series = pd.merge(test['ps_car_11_cat'].to_frame(test['ps_car_11_cat'].name),\n                              averages.reset_index().rename(columns={'index' : 'target', 'target' : 'averages'}),\n                              on=test['ps_car_11_cat'].name,\n                              how='left')['averages'].rename(train['ps_car_11_cat'].name + '_mean').fillna(prior)\n\nft_tst_series.index = test[\"ps_car_11_cat\"].index","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"앞서 train 데이터에서 진행했던 작업을 test 데이터에서도 그대로 진행해줍니다."},{"metadata":{"trusted":true},"cell_type":"code","source":"add_noise(ft_trn_series, 0.01), add_noise(ft_tst_series, 0.01)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"처음에 정의했던 add_noise 함수를 활용하여 노이즈를 일으킨 값들을 반환받습니다. <br>\n함수에서 진행한 과정들을 요약해보면 다음과 같습니다.\n\n1. noise를 만들어줄 **add_noise** 함수 정의\n2. train 데이터와 target 데이터의 len이 같은지, test 데이터와 train 데이터의 이름이 같은지 확인\n3. train 시리즈와 target 시리즈를 concat\n4. Value 별 mean과 count 계산하여 Averages로 저장\n5. Smoothing을 계산\n6. prior를 target 데이터의 평균값으로 정의\n7. 앞서 진행했던 Value별 평균에 Smoothing을 진행하고 필요 없어진 mean과 count 제거\n8. Averages의 값으로 새로운 시리즈(trn/test_cat_mean) 정의\n9. 최초 정의한 add_noise를 적용한 시리즈 반환\n\n조금 더 필사해보면서 이해하도록 합시다."},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"train_encoded, test_encoded = target_encode(train[\"ps_car_11_cat\"], \n                             test[\"ps_car_11_cat\"], \n                             target=train.target, \n                             min_samples_leaf=100,\n                             smoothing=10,\n                             noise_level=0.01)\n    \ntrain['ps_car_11_cat_te'] = train_encoded\ntrain.drop('ps_car_11_cat', axis=1, inplace=True)\nmeta.loc['ps_car_11_cat','keep'] = False  # Updating the meta\ntest['ps_car_11_cat_te'] = test_encoded\ntest.drop('ps_car_11_cat', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e31feea1985ee709ccc37bc5c6dc31e8632c0070","_cell_guid":"555fdddf-e3c5-4c84-a017-d292eaab07ac"},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"eda\"></a>"},{"metadata":{"_uuid":"d6f7d15dc7b90269ccf1fcdef0762e0352a3fe5b","_cell_guid":"21e70f2e-84f2-422d-9962-b783ae4a4024"},"cell_type":"markdown","source":"## Exploratory Data Visualization"},{"metadata":{"_uuid":"e06225d3d3dde0380c7502471ab580ae883329c4","_cell_guid":"b3c403c4-943a-45c2-a0ab-96e72c04fe6d"},"cell_type":"markdown","source":"### Categorical variables\n\ntarget 값이 1인 categorical variables와 customers의 비율을 살펴보도록 합시다."},{"metadata":{"collapsed":true,"_uuid":"d526aedf99612576de7d7e57c488d3f5d0fd2742","_cell_guid":"65ae81d3-aec8-4936-ad4a-978fd3c15b2e","trusted":false},"cell_type":"code","source":"v = meta[(meta.level == 'nominal') & (meta.keep)].index\n\nfor f in v:\n    plt.figure()\n    fig, ax = plt.subplots(figsize=(20,10))\n    # Calculate the percentage of target=1 per category value\n    cat_perc = train[[f, 'target']].groupby([f],as_index=False).mean()\n    cat_perc.sort_values(by='target', ascending=False, inplace=True)\n    # Bar plot\n    # Order the bars descending on target mean\n    sns.barplot(ax=ax, x=f, y='target', data=cat_perc, order=cat_perc[f])\n    plt.ylabel('% target', fontsize=18)\n    plt.xlabel(f, fontsize=18)\n    plt.tick_params(axis='both', which='major', labelsize=18)\n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c403f713d0adf7f294c6f8f439e94ad27ba99c74","_cell_guid":"466bf1c0-fc53-40db-9f11-ae81b12b41fb"},"cell_type":"markdown","source":"막대 그래프들을 통해 **결측값이 있는** variables들을 확인할 수 있습니다. 앞서 결측값들을 치환했는데, categorical variables들은 따로 치환을 하지 않았습니다. 최빈값으로 대체하는 것보다 분리된 category value로서 결측값을 보는 것이 더 좋은 방법일 수 있습니다. <br>\n\n결측값을 가지고 있는 Customer들이 다른 Value들에 비하여 훨씬 높은 target 평균을 가지고 있기 때문입니다!"},{"metadata":{"_uuid":"21e20154f1c847567ea99b7240305f6a3145368c","_cell_guid":"93facc26-1116-41a7-8bc3-a3de5cc1e0d9"},"cell_type":"markdown","source":"### Interval variables\n\ninterval variables의 상관관계를 확인하고자 합니다. Heatmap은 Variables 간의 상관관계를 확인하는데 매우 효율적입니다. 하기 코드는  [an example by Michael Waskom](http://seaborn.pydata.org/examples/many_pairwise_correlations.html)에 기반하고 있습니다."},{"metadata":{"collapsed":true,"_uuid":"c0037596fe7df705bdad63e4f61a06a08c4cccc3","_cell_guid":"e969c00b-b09c-49ac-9acd-c07796a944df","trusted":false},"cell_type":"code","source":"def corr_heatmap(v):\n    correlations = train[v].corr()\n\n    # Create color map ranging between two colors\n    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n    fig, ax = plt.subplots(figsize=(10,10))\n    sns.heatmap(correlations, cmap=cmap, vmax=1.0, center=0, fmt='.2f',\n                square=True, linewidths=.5, annot=True, cbar_kws={\"shrink\": .75})\n    plt.show();\n    \nv = meta[(meta.level == 'interval') & (meta.keep)].index\ncorr_heatmap(v)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9dbe7c73bf99bd8526d4b2a3e95e18f0ce8734b4","_cell_guid":"bc2bb0c9-7b8b-4536-9f2a-5b1279475d19"},"cell_type":"markdown","source":"강한 상관관계를 가지고 있는 Variables들은 다음과 같습니다. :\n- ps_reg_02 and ps_reg_03 (0.7)\n- ps_car_12 and ps_car13 (0.67)\n- ps_car_12 and ps_car14 (0.58)\n- ps_car_13 and ps_car15 (0.67)\n\nSeaborn의 *pair plot*을 사용하면 variables들의 (선형) 관계를 손쉽게 시각화할 수 있습니다. 하지만 히트맵이 상관관계가 있는 variables들의 관계들을 시각화해주고 있기 때문에, 우리는 높은 상관관계를 보이는 variables들을 분리해서 보고자 합니다.\n\n**Note** 프로세스의 속도를 높이기 위하여 train 데이터의 sample을 사용합니다."},{"metadata":{"collapsed":true,"_uuid":"57003a9c9fb3371810d38077ccdfe0afbf1672b9","_cell_guid":"cd8e84fc-9a4b-4b2e-ae63-f0eec75ca5f1","trusted":false},"cell_type":"code","source":"s = train.sample(frac=0.1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"train 데이터에서 10%의 데이터를 샘플링합니다.\n\n* train.shape ▶ (216940, 57)\n* s.shape ▶ (21694, 57)"},{"metadata":{"_uuid":"8ceb63bc7cea9e0437e29eaf6e7a2c7cb0526894","_cell_guid":"2a2a231d-ce98-41f5-8803-f12173bdefc0"},"cell_type":"markdown","source":"1. #### ps_reg_02 and ps_reg_03 (0.7)\n\n회귀선이 보여주듯이, 두 variables들 간에는 선형 상관관계를 살펴볼 수 있습니다. *hue* 파라미터를 통해 target=0과 target=1에 대한 회귀선이 동일함을 알 수 있습니다."},{"metadata":{"collapsed":true,"_uuid":"3f7c86df8ddcd0ea5ae47ca59d365ef8e928309b","_cell_guid":"3b9ade09-a679-44d3-b28c-2c152abf1876","trusted":false},"cell_type":"code","source":"sns.lmplot(x='ps_reg_02', y='ps_reg_03', data=s, hue='target', palette='Set1', scatter_kws={'alpha':0.3})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0823b40bab165ced5c95f7409215b5c1fa626074","_cell_guid":"dd1ca3ee-c4bd-426d-88d2-d2f0e1b2e8c3"},"cell_type":"markdown","source":"#### ps_car_12 and ps_car_13 (0.67)"},{"metadata":{"collapsed":true,"_uuid":"8869035a748b543e838c4710e1c7ad2d9add413e","_cell_guid":"d6dbb6e9-199f-498c-ae08-932a5150b613","trusted":false},"cell_type":"code","source":"sns.lmplot(x='ps_car_12', y='ps_car_13', data=s, hue='target', palette='Set1', scatter_kws={'alpha':0.3})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9347a90f21d7d45f45329b2bbb3341ada1b41022","_cell_guid":"4882daa7-11cd-4f15-b876-fc058e03174c"},"cell_type":"markdown","source":"#### ps_car_12 and ps_car_14 (0.58)"},{"metadata":{"collapsed":true,"_uuid":"12f93ff9edad366928c61540dd34055f55aff5ca","_cell_guid":"78da0148-a6da-4b34-84c5-3ac7577c0b25","trusted":false},"cell_type":"code","source":"sns.lmplot(x='ps_car_12', y='ps_car_14', data=s, hue='target', palette='Set1', scatter_kws={'alpha':0.3})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1a89b9a550afaa8c0984a7418ae834c63c683752","_cell_guid":"ff26f47b-bff7-4547-ac9b-5a6df8d4d071"},"cell_type":"markdown","source":"#### ps_car_13 and ps_car_15 (0.67)"},{"metadata":{"collapsed":true,"_uuid":"74f1216cbc52e181b7512cd9145ebb313d457cdf","_cell_guid":"4b8cfd4a-cb8d-4c3e-b46b-6db77e158bac","trusted":false},"cell_type":"code","source":"sns.lmplot(x='ps_car_15', y='ps_car_13', data=s, hue='target', palette='Set1', scatter_kws={'alpha':0.3})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"60978caa946dae49052a52ec407b9a1fe3e17881","_cell_guid":"dec7b4dd-c93c-47e1-ab2d-548f432ce4a0"},"cell_type":"markdown","source":"이제는 어떤 correlated variables를 유지할지 결정해야 합니다. 이를 위하여 우리는 Principal Component Analysis (PCA), 주성분 분석을 실시하여 variables의 dimensions를 줄일 수 있습니다. 하지만 correlated variables의 수가 적은만큼, 우리는 모델이 heavy-lifting을 하도록 해야합니다."},{"metadata":{"_uuid":"d1fcda209f3dfc9cae005cdee9aafcb26b9e6475","_cell_guid":"9cbdbc6f-9ff4-4c28-9dba-b19e93bc9708"},"cell_type":"markdown","source":"### Checking the correlations between ordinal variables"},{"metadata":{"collapsed":true,"_uuid":"2c64593f289400b81ded88b2159e656786445981","_cell_guid":"8fb69015-aecb-4cb0-a803-1f304bd9d262","trusted":false},"cell_type":"code","source":"v = meta[(meta.level == 'ordinal') & (meta.keep)].index\ncorr_heatmap(v)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3bf34ff124004015f2176ecb42d469bcb22f048f","_cell_guid":"e2374e64-1327-475c-b375-8b6e73717864"},"cell_type":"markdown","source":"ordinal variables는 큰 상관관계를 가지고 있지 않은 것으로 보입니다. 반면에 target 값으로 그룹화할 때 분포가 어떻게 될지 확인할 수 있습니다."},{"metadata":{"_uuid":"c8be9e3000a2860cd78ac36633f850d366d6f9d8","_cell_guid":"e92bc0e5-268c-4d8b-818b-0309dfa18eae"},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"feat_engineering\"></a>"},{"metadata":{"_uuid":"1a68029b3e8b713d966d4ad931568726b0df75a0","_cell_guid":"40ef3784-1878-4e25-9854-b43746f63bf8"},"cell_type":"markdown","source":"## Feature engineering"},{"metadata":{"_uuid":"a617be4ecc6c94a2a6fde837377b4915b3f372a0","_cell_guid":"c5137cbb-6779-4aad-898c-19a5ba815a9c"},"cell_type":"markdown","source":"### Creating dummy variables\n\ncategorical variables는 어떤 순서나 경중이 담겨있지 않습니다. 예를 들어서 카테고리 2는 카테고리 1보다 2배의 값을 가지고 있지 않습니다. 이 문제는 더미 데이터를 만들어줌으로써 해결할 수 있습니다. 첫 번째 dummy variable의 정보는 원래 variables의 범주에 대해 생성된 다른 dummy variable에서 파생될 수 있으므로 삭제해주도록 합니다."},{"metadata":{"scrolled":true,"collapsed":true,"_uuid":"12554d624df68deff6fd0e9c5f90a61c1e1c83c0","_cell_guid":"6606cf2b-9082-4d5e-b278-2779780258a6","trusted":false},"cell_type":"code","source":"v = meta[(meta.level == 'nominal') & (meta.keep)].index\nprint('Before dummification we have {} variables in train'.format(train.shape[1]))\ntrain = pd.get_dummies(train, columns=v, drop_first=True)\nprint('After dummification we have {} variables in train'.format(train.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"50174c5dd05a9fafbe4a12a604dbb7951235774f","_cell_guid":"afa36be5-c104-4b7e-bf68-3a6a52cd36ce"},"cell_type":"markdown","source":"dummy variables는 training 데이터 세트에 52개의 variables를 추가했습니다."},{"metadata":{"_uuid":"1b62391ed9a0401d0e6bd6a0e5ab06cb5677b6f0","_cell_guid":"10ce6e10-d129-4596-b394-40f179627378"},"cell_type":"markdown","source":"### Creating interaction variables"},{"metadata":{"collapsed":true,"_uuid":"9dd8f8a47d5aaa03f24fa614aac9c295244ba416","_cell_guid":"ff3bc120-fa06-44b5-9a3b-73b766a47b5f","trusted":false},"cell_type":"code","source":"v = meta[(meta.level == 'interval') & (meta.keep)].index\npoly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\ninteractions = pd.DataFrame(data=poly.fit_transform(train[v]), columns=poly.get_feature_names(v))\ninteractions.drop(v, axis=1, inplace=True)  # poly 처리가 되지 않은 기존의 열들을 삭제합니다.\n\n# interactions와 train 데이터를 합쳐줍니다.\nprint('Before creating interactions we have {} variables in train'.format(train.shape[1]))\ntrain = pd.concat([train, interactions], axis=1)\nprint('After creating interactions we have {} variables in train'.format(train.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"02036e241b551d5f7bede25fc4f36de53150df6d","_cell_guid":"cd57b8d1-e14a-4242-b0b9-9afd1a132fe9"},"cell_type":"markdown","source":"PolynomialFeatures는 다항차수 변환을 진행을 도와주는 클래스입니다. 상기 코드의 경우 degree를 2로 설정했으니 2차항 변수로 만들어주는 것입니다.\n\n이를 통해 train 데이터에 interaction variables를 추가할 수 있습니다. **get_feature_names** 메소드 덕분에 열 이름을 할당할 수 있습니다."},{"metadata":{"_uuid":"58b2f554c11134975e5d1d8ef3d77ea3fbbaa46f","_cell_guid":"fb9265c9-771b-4ca7-a889-9b37b9eab6c0"},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"feat_selection\"></a>"},{"metadata":{"_uuid":"9a5a578a079d355fa2beff6eb88c5d4d17e38add","_cell_guid":"f2abaa9f-7cdc-4384-80b5-e63597d8cec8"},"cell_type":"markdown","source":"## Feature selection"},{"metadata":{"_uuid":"b78df6efd06d4d89406f993e606c29a1428a882a","_cell_guid":"07f5f01b-578a-45ba-86ca-96f1427fec20"},"cell_type":"markdown","source":"### Removing features with low or zero variance"},{"metadata":{"_uuid":"1e40858bb47e2bf41c3cce72f40a03024b0a4f9f","_cell_guid":"3c9e71d5-83e9-4dcb-ad59-4d0f8260266a"},"cell_type":"markdown","source":"개인적으로 작성자는 분류기의 알고리즘이 유지할 features를 선택하는 것을 선호한다고 합니다. 하지만 우리 스스로 할 수 있는 일도 있습니다. 분산이 0이거나 아주 적은 features들을 제거하는 것입니다. \n\n<br> 이를 위해 사이킷런의 **VarianceThreshold**라는 메소드를 사용할 수 있습니다. 기본적으로 이 메소드는 분산 값이 0인 features들을 제거해줍니다. \n\n<br> 하지만 저희는 이전 단계에서 이미 분산이 0인 features가 없음을 확인했기 때문에, 우리는 1% 미만의 분산이 있는 features들을 제거해주고자 합니다. 이를 통해 우리는 31개의 variables를 제거하게 됩니다."},{"metadata":{"collapsed":true,"_uuid":"f7e68cb2653dc241b3f86539675632b13c63e211","_cell_guid":"41a072b2-85f5-4bb9-afe7-3b5cf3ac51a6","trusted":false},"cell_type":"code","source":"selector = VarianceThreshold(threshold=.01)\nselector.fit(train.drop(['id', 'target'], axis=1)) # Fit to train without id and target variables\n\nf = np.vectorize(lambda x : not x) # Function to toggle boolean array elements\n\nv = train.drop(['id', 'target'], axis=1).columns[f(selector.get_support())]\nprint('{} variables have too low variance.'.format(len(v)))\nprint('These variables are {}'.format(list(v)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5fb5fc6f9d8b477f6a2edc1850621f0f92f6d239","_cell_guid":"6bcf15e1-e12b-4690-999a-a6b0287785a2"},"cell_type":"markdown","source":"만약 우리가 분산에 기반하여 선택을 진행한다면 많은 variables들을 잃게 될 것입니다. 하지만 우리는 많은 variables를 가지고 있지 않기 때문에, 분류기가 직접 선택하도록 합니다. variables가 더 많은 데이터 셋이라면 처리 시간을 줄여줄 수 있을 것입니다.\n\n사이킷런은 [feature selecetion methods]를 제공합니다. 이 메소드 중 하나가 'SelectFromModel' 인데, 다른 분류기에서 최상의 feature를 선택하고 기능을 계속할 수 있도록 합니다. 아래를 통해 랜덤 포레스트를 어떻게 사용하는지 확인해보도록 합니다."},{"metadata":{"_uuid":"8f7aa5366fdbe5abc662b66044eee51ba7b9199b","_cell_guid":"a5f91639-6eb2-4d08-a90e-7893b9ae871a"},"cell_type":"markdown","source":"### Selecting features with a Random Forest and SelectFromModel\n\n우리는 랜덤 포레스트의 feature importances에 따라 feature 선택의 기준을 삼습니다. SelectFromModel을 통하여 유지할 variables의 숫자를 구체화할 수 있습니다. feature의 중요도에 대한 임곗값을 수동으로 설정할수 있지만, 우리는 단순히 50% 이상의 최적의 variables를 선택해보도록 합시다.\n\n> 하기의 코드는 이곳에서 가져왔습니다. [GitHub repo of Sebastian Raschka](https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch04/ch04.ipynb). "},{"metadata":{"collapsed":true,"_uuid":"f2acc04bcfb97bb32d420d09760b691b389ec131","_cell_guid":"68538296-829c-4b96-bd0a-f87e6551c09b","trusted":false},"cell_type":"code","source":"X_train = train.drop(['id', 'target'], axis=1)\ny_train = train['target']\n\nfeat_labels = X_train.columns\n\nrf = RandomForestClassifier(n_estimators=1000, random_state=0, n_jobs=-1)\n\nrf.fit(X_train, y_train)\nimportances = rf.feature_importances_\n\nindices = np.argsort(rf.feature_importances_)[::-1]\n\nfor f in range(X_train.shape[1]):\n    print(\"%2d) %-*s %f\" % (f + 1, 30,feat_labels[indices[f]], importances[indices[f]]))\n","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"8374d3bcb725a9331b57baf209279593fe82f7b0","_cell_guid":"17703fb7-c45b-48c4-b397-97b60fd7bd02","trusted":false},"cell_type":"code","source":"sfm = SelectFromModel(rf, threshold='median', prefit=True)\nprint('Number of features before selection: {}'.format(X_train.shape[1]))\nn_features = sfm.transform(X_train).shape[1]\nprint('Number of features after selection: {}'.format(n_features))\nselected_vars = list(feat_labels[sfm.get_support()])\n","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"739729e399a58cef39570216b481486f6c9d4dac","_cell_guid":"608f476c-a707-4ae8-afda-d18766a237ec","trusted":false},"cell_type":"code","source":"train = train[selected_vars + ['target']]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bbe680ed6436dae96d6bbd1e10b5053ba6fe7f6d","_cell_guid":"525b9c98-40bd-4115-b9a7-c0688940fad7"},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"feat_scaling\"></a>"},{"metadata":{"_uuid":"2a3fd5dfb7a710eb74a590e978768675a21e5c34","_cell_guid":"ed8f51ef-a528-4dd7-9a9e-3fe6abae76cc"},"cell_type":"markdown","source":"## Feature scaling\n\n이전에 언급했듯이, 우리는 train 데이터에 정규화를 진행할 수 있습니다. 몇몇 분류기에서는 더 나은 결과를 가져올 수 있을 것입니다."},{"metadata":{"collapsed":true,"_uuid":"df5e30846f84916d5bee758d035790e94b50e04d","_cell_guid":"9e461415-d087-43c6-96e8-021a61bb76b1","trusted":false},"cell_type":"code","source":"scaler = StandardScaler()\nscaler.fit_transform(train.drop(['target'], axis=1))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b8aa7aee1c138ab00cecc0197b33d07d9a1cc1e0","_cell_guid":"de7a1f63-ccb4-49f0-bf3e-8e1181c5c5be"},"cell_type":"markdown","source":"## Conclusion\n\n필사하시느냐 다들 고생이 많습니다! <br>\n필사적으로 필사합시다!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}