{"nbformat":4,"cells":[{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"76746263-0920-470a-a5fe-3a97bb7fe0e3","collapsed":true,"_uuid":"e8904ac8f1c09bf420c2a220b5bf8dbde9e78115"},"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../working\"]).decode(\"utf8\"))\naaa = pd.read_csv('../working/cat_predicts.csv', sep=',')\naaa.head()\n# Any results you write to the current directory are saved as output."},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"dfa09bab-0455-40c5-883d-b0df811f7c14","_uuid":"e62ba992f61bf2e54c30e8710048087d54891505"},"source":"from sklearn import datasets\nfrom sklearn.utils import check_array\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.base import ClassifierMixin\nfrom collections import Counter\nfrom sklearn.feature_selection import VarianceThreshold\nfrom scipy.stats import pearsonr\nfrom sklearn.feature_selection import RFE\nfrom sklearn.cross_validation import KFold\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import mean_squared_error, make_scorer\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom xgboost import XGBRegressor\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.decomposition import PCA\nfrom imblearn.over_sampling import RandomOverSampler\nimport xgboost as xgb\nimport seaborn as sns\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import PassiveAggressiveClassifier\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import matthews_corrcoef\nfrom collections import Counter\nimport itertools\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom imblearn.datasets import make_imbalance\nfrom imblearn.ensemble import BalancedBaggingClassifier\nfrom imblearn.metrics import classification_report_imbalanced\nfrom sklearn import pipeline, metrics, grid_search\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import GradientBoostingClassifier\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt"},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"cb563564-cdf7-44ea-a853-16617b49ede9","collapsed":true,"_uuid":"6355bcb2083a52de93b05d1517805b07266a4b3a"},"source":"def gini(solution, submission):\n    df = zip(solution, submission, range(len(solution)))\n    df = sorted(df, key=lambda x: (x[1],-x[2]), reverse=True)\n    rand = [float(i+1)/float(len(df)) for i in range(len(df))]\n    totalPos = float(sum([x[0] for x in df]))\n    cumPosFound = [df[0][0]]\n    for i in range(1,len(df)):\n        cumPosFound.append(cumPosFound[len(cumPosFound)-1] + df[i][0])\n    Lorentz = [float(x)/totalPos for x in cumPosFound]\n    Gini = [Lorentz[i]-rand[i] for i in range(len(df))]\n    return sum(Gini)\n\ndef normalized_gini(solution, submission):\n    normalized_gini = gini(solution, submission)/gini(solution, solution)\n    return normalized_gini\ngini_scorer = metrics.make_scorer(normalized_gini, greater_is_better = True)"},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"f4ce16be-e2a3-4fe1-a7f9-32cfb8219dab","collapsed":true,"_uuid":"c8fe31520356a8fbffcfcb6e3988dbe3cf8a80ee"},"source":"trainDF = pd.read_csv('../input/kaggle-seguro/train/train.csv', sep=',')\ntestDF = pd.read_csv('../input/dataset/test/test.csv', sep=',')\ntarget = trainDF.pop('target')"},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"5c5ff5e3-73ce-4c89-a90b-3bcf0a702859","collapsed":true,"_uuid":"829c9fb28d6869d3dac6f0086a54acf0e208e20c"},"source":"plt.figure(figsize=(10,3))\nsns.countplot(trainDF['target'],palette='rainbow')\nplt.xlabel('Target')\ntrainDF['target'].value_counts()"},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"78080484-a253-4751-99cd-85069c4d5ea8","collapsed":true,"_uuid":"41993f013dee9c0c39fe54b9b6b9dfe1fd163402"},"source":"cor = trainDF.corr()\nplt.figure(figsize=(16,10))\nsns.heatmap(cor)"},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"6b1d77ad-be39-4c64-b346-1dc1a9971d19","collapsed":true,"_uuid":"3a846d1cf886d1c33e9387eababd803c5e24ede4"},"source":"ps_cal = trainDF.columns[trainDF.columns.str.startswith('ps_calc')] "},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"3e51a7d6-77de-466e-b3a7-e2348012753d","collapsed":true,"_uuid":"9d6fefdaac437adbe183006c2b97203ffb04943e"},"source":"id_test = testDF['id'].values\ntrainDF = trainDF.drop(ps_cal,axis =1)\ntrainDF = trainDF.drop(['id'],axis =1)\ntestDF = testDF.drop(ps_cal,axis =1)\ntestDF = testDF.drop(['id'],axis =1)"},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"a5916b77-0f47-422f-bb39-b3d337b64403","collapsed":true,"_uuid":"eaf936d41b018257d1f4796cc3e5388a6207529c"},"source":"cor = trainDF.corr()\nplt.figure(figsize=(16,10))\nsns.heatmap(cor)"},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"6e41e511-8d8d-4e77-a021-f6eed8ed31d4","collapsed":true,"_uuid":"515de4e6d5f5a7635e891aa07a85c190e8d5391a"},"source":"# def missing_value(df):\n#     col = df.columns\n#     for i in col:\n#         if df[i].isnull().sum()>0:\n#             df[i].fillna(df[i].mode()[0],inplace=True)\n# missing_value(trainDF)"},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"ee65aed2-0057-43fe-b16c-0023aa8562d9","collapsed":true,"_uuid":"64a9d1c9f23d0c1cf6254284baa69c2be0858daa"},"source":"trainDF = trainDF.fillna(999)\ntestDF = testDF.fillna(999)"},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"7672b398-a895-49a0-b61a-9bc702cb79d4","collapsed":true,"_uuid":"a30a512dd80f9cef766f06c1cc07c4b8f16eb94a"},"source":"for c in trainDF.select_dtypes(include=['float64']).columns:\n    trainDF[c]=trainDF[c].astype(np.float32)\n    testDF[c]=testDF[c].astype(np.float32)\nfor c in trainDF.select_dtypes(include=['int64']).columns[2:]:\n    trainDF[c]=trainDF[c].astype(np.int8)\n    testDF[c]=testDF[c].astype(np.int8)  "},{"cell_type":"markdown","source":"改变变量类型","metadata":{"_cell_guid":"0377d760-ea66-40da-a9f5-4eb5a8c53c87","_uuid":"945c2723feee3066528c62d1eb9cf1228d4a605b"}},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"58c565a1-4683-4523-8df0-87ae7b15caee","_uuid":"ac995339aa52a612e28d19620e918a822bdc9457"},"source":"from catboost import CatBoostClassifier, Pool\ny_train = target.values\nx_train = trainDF\nx_test = testDF\n\ntrain_data = Pool(x_train, y_train)\ntest_data = Pool(x_test)"},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"3e433e1c-3d48-4c78-8956-2dd920cfddb7","_uuid":"b0d4e1f68dc907908006db84a1393c89e66f2789"},"source":"props = {\n        \tleaf_estimation_method ='Newton',\n        \tlearning_rate=0.057,\n          \tl2_leaf_reg = 23,\n          \tdepth=6,\n          \tod_pval=0.0000001,\n          \titerations = 877,\n          \tloss_function='Logloss'\n          \n        }"},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"6938b5fa-5b51-485d-903a-3e8d8a12888d","_uuid":"2ed0958187b2aefc8731952a5562f7bfd1805b32"},"source":"from tqdm import tqdm\nprint('Starting the loop...')\nnum_ensembles = 6\ny_pred = 0.0\nfor i in tqdm(range(num_ensembles)):\n    model = CatBoostClassifier(random_seed = i+200, gradient_iterations = i+1 ,leaf_estimation_method ='Newton', learning_rate=0.057, l2_leaf_reg = 23, depth=6, od_pval=0.0000001, iterations = 877, loss_function='Logloss')\n    fit_model = model.fit(train_data)\n    y_pred +=  fit_model.predict_proba(test_data)[:,1]\ny_pred /= num_ensembles\ngc.collect()\n\n\n# Create a submission file\nsub = pd.DataFrame()\nsub['id'] = id_test\nsub['target'] = y_pred\nsub.to_csv('cat_predicts.csv', index=False)"},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"5ff07e56-695a-426b-9889-98b04f1b243e","_uuid":"68502d20848bcb272c9f46b56ba10469994a7ede"},"source":"print('done')"},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"4aa07485-f8c7-435c-9058-66382575c93d","collapsed":true,"_uuid":"804d7668a0f561c6a8197d1f2adcd2aa3752bda9"},"source":"cat_col = [col for col in trainDF.columns if '_cat' in col]\nprint(cat_col)\nfor c in cat_col:\n    trainDF[c] = trainDF[c].astype('uint8')\n#     test[c] = test[c].astype('uint8') "},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"ae583c35-2083-43d8-9917-39e2db753471","collapsed":true,"_uuid":"1d4fe89f9f1958f6c1593b93390bf20f3800f34e"},"source":"bin_col = [col for col in trainDF.columns if 'bin' in col]\nprint(bin_col)\nfor c in bin_col:\n    trainDF[c] = trainDF[c].astype('uint8')\n#     test[c] = test[c].astype('uint8') "},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"84bd8aa8-3ee2-4358-aca9-c82a8c7b7bc0","collapsed":true,"_uuid":"9534476dd3f31b6118b21051061b257e78584afe"},"source":"train_X = trainDF.loc[:,trainDF.columns[:len(trainDF.columns)-1]]\ntrain_y = trainDF.loc[:,['target']].values.ravel()"},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"fcdd716e-64fb-4efc-b4aa-0f5019fb155c","collapsed":true,"_uuid":"472b409847ba1a41f9ed38bc178e379a1ff5f3e4"},"source":"model = GradientBoostingClassifier(n_estimators=200)\nscore = cross_val_score(model,train_X,train_y,cv=5, scoring=\"accuracy\")\nprint(score.mean())"},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"5978f3f6-8df5-44dd-86ce-ac3ee62bb08b","collapsed":true,"_uuid":"10015f8631d647573123b75a3262ff18cfa2b737"},"source":"def runXGB(xtrain,xvalid,ytrain,yvalid,xtest,eta=0.1,num_rounds=100,max_depth=4):\n    params = {\n        'objective':'binary:logistic',        \n        'max_depth':max_depth,\n        'learning_rate':eta,\n        'eval_metric':'auc',\n        'min_child_weight':6,\n        'subsample':0.8,\n        'colsample_bytree':0.8,\n        'seed':seed,\n        'reg_lambda':1.3,\n        'reg_alpha':8,\n        'gamma':10,\n        'scale_pos_weight':1.6\n        #'n_thread':-1\n    }\n    \n    dtrain = xgb.DMatrix(xtrain,label=ytrain)\n    dvalid = xgb.DMatrix(xvalid,label=yvalid)\n    dtest = xgb.DMatrix(xtest)\n    watchlist = [(dtrain,'train'),(dvalid,'test')]\n    \n    model = xgb.train(params,dtrain,num_rounds,watchlist,early_stopping_rounds=50,verbose_eval=50)\n    pred = model.predict(dvalid,ntree_limit=model.best_ntree_limit)\n    pred_test = model.predict(dtest,ntree_limit=model.best_ntree_limit)\n    return pred_test,model\n    "},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"8f699d97-16e4-49bb-875e-a2d2e18206ae","collapsed":true,"_uuid":"9b2641cbf7c5b97aede0dcadd2898b6c76975b3c"},"source":"from catboost import CatBoostClassifier, Pool\nfrom tqdm import tqdm\ntrain_data = Pool(train_X, train_y)\ntest_data = Pool(train_X)\nnum_ensembles = 6\ny_pred = 0.0\nfor i in tqdm(range(num_ensembles)):\n    model = CatBoostClassifier(random_seed = i+200, gradient_iterations = i+1 ,leaf_estimation_method ='Newton', learning_rate=0.057, l2_leaf_reg = 23, depth=6, od_pval=0.0000001, iterations = 877, loss_function='Logloss')\n    fit_model = model.fit(train_data)\n    y_pred +=  fit_model.predict_proba(test_data)[:,1]\ny_pred /= num_ensembles\ngc.collect()\n# Create a submission file\nsub = pd.DataFrame()\nsub['id'] = id_test\nsub['target'] = y_pred\nsub.to_csv('cat_predicts.csv', index=False)"}],"metadata":{"language_info":{"name":"python","mimetype":"text/x-python","nbconvert_exporter":"python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","file_extension":".py","version":"3.6.3"},"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"}},"nbformat_minor":1}