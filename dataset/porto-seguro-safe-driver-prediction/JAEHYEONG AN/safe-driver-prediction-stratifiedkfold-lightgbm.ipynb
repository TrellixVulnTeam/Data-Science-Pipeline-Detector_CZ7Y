{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import StratifiedKFold\nimport lightgbm as lgbm\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load Data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# load data\ntrain = pd.read_csv('../input/train.csv')\ntrain_label = train['target']\ntrain_id = train['id']\ndel train['target'], train['id']\n\ntest = pd.read_csv('../input/test.csv')\ntest_id = test['id']\ndel test['id']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# target ratio by label"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_label.unique())\nprint('target 0:', len(train_label[train_label==0])/len(train) * 100)\nprint('target 1:', len(train_label[train_label==1])/len(train) * 100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualize Variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"def bar_plot(col, data, hue=None):\n    f, ax = plt.subplots(figsize=(10, 5))\n    sns.countplot(x=col, hue=hue, data=data, alpha=0.5)\n    plt.show()\n    \ndef dist_plot(col, data):\n    f, ax = plt.subplots(figsize=(10, 5))\n    sns.distplot(data[col].dropna(), kde=False, bins=10)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# binary variables\nbinary = ['ps_ind_06_bin', 'ps_ind_07_bin', 'ps_ind_08_bin', 'ps_ind_09_bin', 'ps_ind_10_bin', 'ps_ind_11_bin',\n          'ps_ind_12_bin', 'ps_ind_13_bin', 'ps_ind_16_bin', 'ps_ind_17_bin', 'ps_ind_18_bin', 'ps_calc_15_bin', \n          'ps_calc_16_bin', 'ps_calc_17_bin', 'ps_calc_18_bin', 'ps_calc_19_bin', 'ps_calc_20_bin']\n# categorical variables\ncategory = ['ps_ind_02_cat', 'ps_ind_04_cat', 'ps_ind_05_cat', 'ps_car_01_cat', 'ps_car_02_cat', 'ps_car_03_cat', \n            'ps_car_04_cat', 'ps_car_05_cat', 'ps_car_06_cat', 'ps_car_07_cat', 'ps_car_08_cat', 'ps_car_09_cat', \n            'ps_car_10_cat', 'ps_car_11_cat']\n# integer variables\ninteger = ['ps_ind_01', 'ps_ind_03', 'ps_ind_14', 'ps_ind_15', 'ps_calc_04', 'ps_calc_05', 'ps_calc_06', \n           'ps_calc_07', 'ps_calc_08', 'ps_calc_09', 'ps_calc_10', 'ps_calc_11', 'ps_calc_12', 'ps_calc_13', \n           'ps_calc_14', 'ps_car_11']\n# floats variables\nfloats = ['ps_reg_01', 'ps_reg_02', 'ps_reg_03', 'ps_calc_01', 'ps_calc_02', 'ps_calc_03', 'ps_car_12', 'ps_car_13',\n          'ps_car_14', 'ps_car_15']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# merge train & test data\ndf = pd.concat([train, test], axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ploting binary, category, integer variables\nfor col in binary+category+integer:\n    bar_plot(col, df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ploting float variables\nfor col in floats:\n    dist_plot(col, df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Corrleation"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = df.corr()\n\ncmap = sns.color_palette('Blues')\nf, ax = plt.subplots(figsize=(10, 7))\nsns.heatmap(corr, cmap=cmap)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create Derived Variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 파생변수 1: 결측값을 의미하는 '-1'의 개수\ntrain['missing'] = (train==-1).sum(axis=1).astype(float)\ntest['missing'] = (test==-1).sum(axis=1).astype(float)\n\n# 파생변수 2: 이진 변수의 합\nbin_features = [c for c in train.columns if 'bin' in c]\ntrain['bin_sum'] = train[bin_features].sum(axis=1)\ntest['bin_sum'] = test[bin_features].sum(axis=1)\n\n# 파생변수 3: target encoding\nfeatures = ['ps_ind_06_bin', 'ps_ind_07_bin', 'ps_ind_08_bin', 'ps_ind_09_bin', 'ps_ind_12_bin', 'ps_ind_16_bin', \n            'ps_ind_17_bin', 'ps_ind_18_bin', 'ps_ind_04_cat', 'ps_ind_05_cat', 'ps_car_01_cat', 'ps_car_02_cat', \n            'ps_car_03_cat', 'ps_car_04_cat', 'ps_car_06_cat', 'ps_car_07_cat', 'ps_car_08_cat', 'ps_car_09_cat', \n            'ps_car_11_cat', 'ps_ind_01', 'ps_ind_03', 'ps_ind_15', 'ps_car_11']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LightGBM "},{"metadata":{"trusted":true},"cell_type":"code","source":"def Gini(y_true, y_pred):\n    # check and get number of samples\n    assert y_true.shape == y_pred.shape\n    n_samples = y_true.shape[0]\n    \n    # sort rows on prediction column \n    # (from largest to smallest)\n    arr = np.array([y_true, y_pred]).transpose()\n    true_order = arr[arr[:,0].argsort()][::-1,0]\n    pred_order = arr[arr[:,1].argsort()][::-1,0]\n    \n    # get Lorenz curves\n    L_true = np.cumsum(true_order) / np.sum(true_order)\n    L_pred = np.cumsum(pred_order) / np.sum(pred_order)\n    L_ones = np.linspace(1/n_samples, 1, n_samples)\n    \n    # get Gini coefficients (area between curves)\n    G_true = np.sum(L_ones - L_true)\n    G_pred = np.sum(L_ones - L_pred)\n    \n    # normalize to true Gini coefficient\n    return G_pred/G_true\n\ndef evalerror(preds, dtrain):\n    labels = dtrain.get_label()\n    return 'gini', Gini(labels, preds), True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Parameters of LightGBM\nnum_boost_round = 10000\nparams = {\n    'objective':'binary',\n    'boosting_type':'gbdt',\n    'learning_rate':0.1,\n    'num_leaves':15,\n    'max_bin':256,\n    'feature_fraction':0.6,\n    'verbosity':0,\n    'drop_rate':0.1,\n    'is_unbalance':False,\n    'max_drop':50,\n    'min_child_samples':10,\n    'min_child_weight':150,\n    'min_split_gain':0,\n    'subsample':0.9,\n    'seed':2018\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model Training & Cross Validation\nNFOLDS = 5\nkfold = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=218)\nkf = kfold.split(train, train_label)\n\ncv_train = np.zeros(len(train_label))\ncv_pred = np.zeros(len(test_id))\nbest_trees = []\nfold_scores = []\n\nfor i, (train_fold, validate) in enumerate(kf):\n    # Split train/validate\n    X_train, X_validate, label_train, label_validate = train.iloc[train_fold, :], train.iloc[validate,:], train_label[train_fold], train_label[validate]\n    \n    # target encoding\n    for feature in features:\n        # 훈련 데이터에서 feature 고유값별로 타겟 변수의 평균을 구함 \n        map_dic = pd.DataFrame([X_train[feature], label_train]).T.groupby(feature).agg('mean')\n        map_dic = map_dic.to_dict()['target']\n        \n        X_train[feature+'_target_enc'] = X_train[feature].apply(lambda x: map_dic.get(x, 0))\n        X_validate[feature+'_target_enc'] = X_validate[feature].apply(lambda x: map_dic.get(x, 0))\n        test[feature+'_target_enc'] = test[feature].apply(lambda x: map_dic.get(x, 0))\n        \n    dtrain = lgbm.Dataset(X_train, label_train)\n    dvalid = lgbm.Dataset(X_validate, label_validate, reference=dtrain)\n    \n    # evalerror()를 통해 검증 데이터에 대한 정규화 Gini계수 점수를 기준으로 한 최적의 트리 개수\n    bst = lgbm.train(params, dtrain, num_boost_round, valid_sets=dvalid, feval=evalerror, \n                    verbose_eval=100, early_stopping_rounds=100)\n    best_trees.append(bst.best_iteration)\n    \n    # predict\n    cv_pred += bst.predict(test, num_iteration=bst.best_iteration)\n    cv_train[validate] += bst.predict(X_validate)\n    \n    # score\n    score = Gini(label_validate, cv_train[validate])\n    print(score)\n    fold_scores.append(score)\n    \ncv_pred /= NFOLDS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"cv score:\")\nprint(Gini(train_label, cv_train))\nprint(fold_scores)\nprint(best_trees, np.mean(best_trees))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save prediction\npd.DataFrame({'id': test_id, 'target': cv_pred}).to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}