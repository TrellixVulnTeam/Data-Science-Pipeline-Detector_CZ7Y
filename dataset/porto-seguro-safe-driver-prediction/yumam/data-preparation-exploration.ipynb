{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.utils import shuffle\nfrom sklearn.ensemble import RandomForestClassifier\n\npd.set_option('display.max_columns', 100)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Q. What is set_option?\n\n\nA. Set the value of a single option. We can change the default number of rows to be displayed. Here, we can see 100 columns.","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/porto-seguro-safe-driver-prediction/train.csv')\ntest = pd.read_csv('../input/porto-seguro-safe-driver-prediction/test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.tail()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.drop_duplicates()\ntrain.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is no duplicated value!","metadata":{}},{"cell_type":"code","source":"train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = []\nfor f in train.columns:\n    if f == 'target':\n        role = 'target'\n    elif f == 'id':\n        role = 'id'\n    else:\n        role = 'input'\n        \n    if 'bin' in f or f == 'target':\n        level = 'binary'\n    elif 'cat' in f or f == 'id':\n        level = 'nominal'\n    elif train[f].dtype == float:\n        level = 'interval'\n    elif train[f].dtype == int:\n        level = 'ordinal'\n    \n    keep = True\n    if f == 'id':\n        keep = False\n    \n    dtype = train[f].dtype\n    \n    f_dict = {\n        'varname' : f,\n        'role' : role,\n        'level' : level,\n        'keep' : keep,\n        'dtype' : dtype\n    }\n    data.append(f_dict)\n    \nmeta = pd.DataFrame(data, columns=['varname', 'role', 'level', 'keep', 'dtype'])\nmeta.set_index('varname', inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Q. What is set_index?\n\nA. It is a method to set a list, Series or Data Frame as index of Data Frame. Index column can be set while making a data frame too.\ninplace : Decide whether to change the original object.","metadata":{}},{"cell_type":"code","source":"meta","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"meta[(meta.level == 'nominal') & (meta.keep)].index","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame({'count':meta.groupby(['role','level'])['role'].size()}).reset_index()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Q. What is reset_index?\n\nA. It is a method to reset index of a Data Frame. It sets a list of interger ranging from 0 to length of data as index","metadata":{}},{"cell_type":"code","source":"v = meta[(meta.level=='interval')&(meta.keep)].index\ntrain[v].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"v = meta[(meta.level == 'ordinal') & (meta.keep)].index\ntrain[v].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"v = meta[(meta.level=='binary')&(meta.keep)].index\ntrain[v].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, target=1 is far less than target=0. We can derive two problems 'oversampling record with target=1' and 'undersampling record with target=0'.\n\nHere, we can go for undersampling!","metadata":{}},{"cell_type":"code","source":"desired_apriori = 0.10\nidx_0 = train[train.target==0].index\nidx_1 = train[train.target==1].index\n\nnb_0 = len(train.loc[idx_0])\nnb_1 = len(train.loc[idx_1])\n\nundersampling_rate = ((1-desired_apriori)*nb_1)/(nb_0*desired_apriori)\nundersampled_nb_0 = int(undersampling_rate*nb_0)\nprint('Rate to undersample records with target=0: {}'.format(undersampling_rate))\nprint('Number of records with target=0 after undersampling : {}'.format(undersampled_nb_0))\n\nundersampled_idx = shuffle(idx_0, random_state=37, n_samples=undersampled_nb_0)\n\nidx_list = list(undersampled_idx) + list(idx_1)\n\ntrain = train.loc[idx_list].reset_index(drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vars_with_missing = []\n\nfor f in train.columns :\n    missings = train[train[f] == -1][f].count()\n    if missings > 0:\n        vars_with_missing.append(f)\n        missings_perc = missings/train.shape[0]\n        \n        print('Variable {} has {} records ({:.2%}) with missing values'.format(f, missings, missings_perc))\n    \nprint('In total, there are {} variables with missing values'.format(len(vars_with_missing)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vars_to_drop = ['ps_car_03_cat', 'ps_car_05_cat']\ntrain.drop(vars_to_drop, inplace=True, axis=1)\n\nmeta.loc[(vars_to_drop), 'keep'] = False\n\nmean_imp = SimpleImputer(missing_values = -1, strategy='mean')\nmode_imp = SimpleImputer(missing_values = -1, strategy='most_frequent')\ntrain['ps_reg_03'] = mean_imp.fit_transform(train[['ps_reg_03']]).ravel()\ntrain['ps_car_12'] = mean_imp.fit_transform(train[['ps_car_12']]).ravel()\ntrain['ps_car_14'] = mean_imp.fit_transform(train[['ps_car_14']]).ravel()\ntrain['ps_car_11'] = mode_imp.fit_transform(train[['ps_car_11']]).ravel()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"v = meta[(meta.level == 'nominal') & (meta.keep)].index\n\nfor f in v:\n    dist_values = train[f].value_counts().shape[0]\n    print('Variable {} has {} distinct values'.format(f, dist_values))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def add_noise(series, noise_level):\n    return series * (1+noise_level*np.random.randn(len(series)))\n\ndef target_encode(trn_series=None, tst_series=None, target=None, min_samples_leaf=1, smoothing=1, noise_level=0):\n    \"\"\"Smoothing is computed like in the following paper by Daniele Micci-Barreca\n    https://kaggle2.blob.core.windows.net/forum-message-attachments/225952/7441/high%20cardinality%20categoricals.pdf\n    trn_series : training categorical feature as a pd.Series\n    tst_series : test categorical feature as a pd.Series\n    target : target data as a pd.Series\n    min_samples_leaf (int) : minimum samples to take category average into account\n    smoothing (int) : smoothing effect to balance categorical average vs prior \n    \"\"\"\n    assert len(trn_series) == len(target)\n    assert trn_series.name == tst_series.name\n    temp = pd.concat([trn_series, target], axis=1)\n    averages = temp.groupby(by=trn_series.name)[target.name].agg(['mean', 'count'])\n    smoothing = 1 / (1+np.exp(-(averages['count'] - min_samples_leaf) / smoothing))\n    prior = target.mean()\n    averages[target.name] = prior * (1 - smoothing) + averages['mean'] * smoothing\n    averages.drop(['mean', 'count'], axis=1, inplace=True)\n    ft_trn_series = pd.merge(trn_series.to_frame(trn_series.name),\n                            averages.reset_index().rename(columns={'index':target.name, target.name:'average'}),\n                            on = trn_series.name, how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n    ft_trn_series.index = trn_series.index\n    ft_tst_series = pd.merge(tst_series.to_frame(tst_series.name), averages.reset_index().rename(columns={'index':target.name, target.name : 'average'}),\n                            on = tst_series.name, how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n    ft_tst_series.index = tst_series.index\n    return add_noise(ft_trn_series, noise_level), add_noise(ft_tst_series, noise_level)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"check Youtube","metadata":{}},{"cell_type":"code","source":"train_encode, test_encode = target_encode(train['ps_car_11_cat'],\n                                         test['ps_car_11_cat'], target=train.target, min_samples_leaf=100,\n                                         smoothing = 10, noise_level=0.01)\ntrain['ps_car_11_cat_te'] = train_encode\ntrain.drop('ps_car_11_cat', axis = 1, inplace=True)\nmeta.loc['ps_car_11_cat', 'keep'] = False\ntest['ps_car_11_cat_te'] = test_encode\ntest.drop('ps_car_11_cat', axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Categorical variables\n\nLet's look into the categorical variables and the proportion of customers with target = 1","metadata":{}},{"cell_type":"code","source":"v = meta[(meta.level == 'nominal') & (meta.keep)].index\n\nfor f in v:\n    plt.figure()\n    fig, ax = plt.subplots(figsize=(20,10))\n    cat_perc = train[[f, 'target']].groupby([f], as_index = False).mean()\n    cat_perc.sort_values(by='target', ascending=False, inplace=True)\n    \n    sns.barplot(ax=ax, x=f, y='target', data=cat_perc, order=cat_perc[f])\n    plt.ylabel('% target', fontsize=18)\n    plt.xlabel(f, fontsize=18)\n    plt.tick_params(axis='both', which='major', labelsize=18)\n    plt.show();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def corr_heatmap(v):\n    correlations = train[v].corr()\n    \n    cmap = sns.diverging_palette(220, 10, as_cmap = True)\n    \n    fig, ax = plt.subplots(figsize=(10,10))\n    sns.heatmap(correlations ,cmap=cmap, vmax = 1.0, center = 0, fmt='.2f',\n               square=True, linewidths=.5, annot = True, cbar_kws={'shrink':.75})\n    plt.show();\n\nv = meta[(meta.level=='interval') & (meta.keep)].index\ncorr_heatmap(v)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"s = train.sample(frac=0.1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.lmplot(x='ps_reg_02', y='ps_reg_03', data=s, hue='target', palette='Set1', scatter_kws={'alpha':0.3})\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.lmplot(x='ps_car_12', y='ps_car_13', data=s, hue='target', palette='Set1', scatter_kws={'alpha':0.3})\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.lmplot(x='ps_car_12', y='ps_car_14', data=s, hue='target', palette='Set1', scatter_kws={'alpha':0.3})\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.lmplot(x='ps_car_15', y='ps_car_13', data=s, hue='target', palette='Set1', scatter_kws={'alpha':0.3})\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"v = meta[(meta.level=='ordinal') & (meta.keep)].index\ncorr_heatmap(v)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"v = meta[(meta.level=='nominal') & (meta.keep)].index\nprint('Before dummification we have {} variables in train'.format(train.shape[1]))\ntrain = pd.get_dummies(train, columns = v, drop_first = True)\nprint('After dummification we have {} variables in train'.format(train.shape[1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"v = meta[(meta.level=='interval') & (meta.keep)].index\npoly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\ninteractions = pd.DataFrame(data=poly.fit_transform(train[v]), columns=poly.get_feature_names(v))\ninteractions.drop(v, axis=1, inplace=True)\nprint('Before creating interactions we have {} variables in train'.format(train.shape[1]))\ntrain = pd.concat([train, interactions], axis = 1)\nprint('After creating interactions we have {} variables in train'.format(train.shape[1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This adds extra interaction variables to the train data. Thanks to the get_feature_names method we can assign column names to these new variables.","metadata":{}},{"cell_type":"code","source":"selector = VarianceThreshold(threshold=0.01)\nselector.fit(train.drop(['id','target'], axis=1))\n\nf = np.vectorize(lambda x : not x)\n\nv = train.drop(['id', 'target'], axis=1).columns[f(selector.get_support())]\nprint('{} variabels have too low variance.'.format(len(v)))\nprint('These variables are {}'.format(list(v)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = train.drop(['id', 'target'], axis=1)\ny_train = train['target']\n\nfeat_labels = X_train.columns\n\nrf = RandomForestClassifier(n_estimators=1000, random_state=0, n_jobs=-1)\n\nrf.fit(X_train, y_train)\nimportances = rf.feature_importances_\n\nindices = np.argsort(rf.feature_importances_)[::-1]\n\nfor f in range(X_train.shape[1]):\n    print(\"%2d) %-*s %f\" % (f + 1, 30,feat_labels[indices[f]], importances[indices[f]]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sfm = SelectFromModel(rf, threshold = 'median', prefit=True)\nprint('Number of features before selection: {}'.format(X_train.shape[1]))\nn_features = sfm.transform(X_train).shape[1]\nprint('Number of features after selection: {}'.format(n_features))\nselected_vars = list(feat_labels[sfm.get_support()])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train[selected_vars+['target']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = StandardScaler()\nscaler.fit_transform(train.drop(['target'], axis=1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}