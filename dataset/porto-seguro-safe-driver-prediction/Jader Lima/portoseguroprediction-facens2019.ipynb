{"cells":[{"metadata":{"_uuid":"f4a214a029db1d49476f203c6ccb3f34e61bf067"},"cell_type":"markdown","source":"# Porto Seguro’s Safe Driver Prediction\n\n## Predict if a driver will file an insurance claim next year\n\nNothing ruins the thrill of buying a brand new car more quickly than seeing your new insurance bill. The sting’s even more painful when you know you’re a good driver. It doesn’t seem fair that you have to pay so much if you’ve been cautious on the road for years.\n\nPorto Seguro, one of Brazil’s largest auto and homeowner insurance companies, completely agrees. Inaccuracies in car insurance company’s claim predictions raise the cost of insurance for good drivers and reduce the price for bad ones.\n\nIn this competition, you’re challenged to build a model that predicts the probability that a driver will initiate an auto insurance claim in the next year. While Porto Seguro has used machine learning for the past 20 years, they’re looking to Kaggle’s machine learning community to explore new, more powerful methods. A more accurate prediction will allow them to further tailor their prices, and hopefully make auto insurance coverage more accessible to more drivers.\n\nAlgumas características sobre o nome das features:\n1. •O nome dos atributos indica o grupo ao qual pertence (ind, reg, car);\n1. •Os prefixos bin e cat indicam atributos binários e categóricos, respectivamente;\n1. •Atributos sem os prefixos citados podem ser ordinais ou contínuos;\n1. •Atributos com -1 indicam dado faltante (missing); e\n1. •A coluna 'target' indica se houve sinistro para apólice ou não.\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport scipy.interpolate\nimport scipy.integrate\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#Carregando os csvs de treino e teste respectivamente\ndf  = pd.read_csv('../input/train.csv', header=0)\ndftest = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"64b531e72bd9036053088c66d0f044bf9f2cb8c1"},"cell_type":"code","source":"#Visualizando estatisticas descritivas do dataset de treino\ndf.describe()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7948df3aff1cdc27f0ed05493a7bd73ccdc0c0ee"},"cell_type":"code","source":"#Visualizando estatisticas descritivas do dataset de teste\ndftest.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e715fe7f63395311da4bac3b11efad0bd33d2b5d"},"cell_type":"code","source":"#Removendo dados duplicados do dataset de treino\nprint('Antes:', df.shape)\ndf.drop_duplicates()\nprint('Depois:', df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"684d21de157e0fc94f41f4fd5642daa7a854ccda"},"cell_type":"code","source":"#Removendo dados duplicados do dataset de test\nprint('Antes:', dftest.shape)\ndftest.drop_duplicates()\nprint('Depois:', dftest.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f670955fb36b1fab0bd2bae0059ef8a73cfa56b7"},"cell_type":"code","source":"#copiando os datasets carregados para as variaveis de treino e teste que serão utilizadas no experimento\ntrain = df\ntest  = dftest","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"751e9de7efbaf04edd1dc5778c7121839d5cb325"},"cell_type":"markdown","source":"Conforme citade anteriormente as features dos dataset seguem as nomeclaturas abaixo:\n1. •O nome dos atributos indica o grupo ao qual pertence (ind, reg, car);\n1. •Os prefixos bin e cat indicam atributos binários e categóricos, respectivamente;\n1. •Atributos sem os prefixos citados podem ser ordinais ou contínuos;\n1. •Atributos com -1 indicam dado faltante (missing); e\n1. •A coluna 'target' indica se houve sinistro para apólice ou não.\n\nEntão conforme o sugerido no experimento em https://www.kaggle.com/bertcarremans/data-preparation-exploration, convêm criar estrutura de metadados ."},{"metadata":{"trusted":true,"_uuid":"af9f0fd5f72b8f76a1762bf49f8e24e1b387e4d9"},"cell_type":"code","source":"\ndata = []\nfor f in train.columns:\n    # definindo o uso (entre rótulo, id e atributos)\n    if f == 'target':\n        role = 'target' # rótulo\n    elif f == 'id':\n        role = 'id'\n    else:\n        role = 'input' # atributos\n         \n    # definindo o tipo do dado\n    if 'bin' in f or f == 'target':\n        level = 'binary'\n    elif 'cat' in f or f == 'id':\n        level = 'nominal'\n    elif train[f].dtype == float:\n        level = 'interval'\n    elif train[f].dtype == int:\n        level = 'ordinal'\n        \n    # mantem keep como verdadeiro pra tudo, exceto id\n    keep = True\n    if f == 'id':\n        keep = False\n    \n    # cria o tipo de dado\n    dtype = train[f].dtype\n    \n    # cria dicionário de metadados\n    f_dict = {\n        'varname': f,\n        'role': role,\n        'level': level,\n        'keep': keep,\n        'dtype': dtype\n    }\n    data.append(f_dict)\n    \nmeta = pd.DataFrame(data, columns=['varname', 'role', 'level', 'keep', 'dtype'])\nmeta.set_index('varname', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"638d946956b6eec15d179eb4b5b5acf7b5bda220"},"cell_type":"markdown","source":"**Visualizando os metadados criados**"},{"metadata":{"trusted":true,"_uuid":"ec4801d26243471709c1d2bf4f7aecd972f08b1f"},"cell_type":"code","source":"meta","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"218a14a698a90f834db5ace791607c862133dc4b"},"cell_type":"markdown","source":"**Exibindo a quantidade de features por tipo**"},{"metadata":{"trusted":true,"_uuid":"caee9b4c2cd2162ba43351fc368aa48c3961dd50"},"cell_type":"code","source":"pd.DataFrame({'count' : meta.groupby(['role', 'level'])['role'].size()}).reset_index()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"da5c65d96fb4dc110e12fc1d9f786a1dec02f90b"},"cell_type":"markdown","source":"**Como mencionado anteriormente, os valores -1 indicam valores faltantes, então é interessante saber a quantidade absoluta de valores faltantes por feature, \nassim como sua representação percentual, baixo faremos a exibição mencionada para os dados de treino e teste do experimento**"},{"metadata":{"trusted":true,"_uuid":"aea25905c73231374e3bed0438bfb5c9a6523fc7"},"cell_type":"code","source":"atributos_missing = []\n\nfor f in train.columns:\n    missings = train[train[f] == -1][f].count()\n    if missings > 0:\n        atributos_missing.append(f)\n        missings_perc = missings/df.shape[0]\n        \n        print('Atributo {} tem {} amostras ({:.2%}) com valores faltantes'.format(f, missings, missings_perc))\n        \nprint('No total, há {} atributos com valores faltantes'.format(len(atributos_missing)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f12fb7a4ce3af77682124d6bac2de0b60da58f3a"},"cell_type":"code","source":"atributos_missing = []\n\nfor f in test.columns:\n    missings = test[test[f] == -1][f].count()\n    if missings > 0:\n        atributos_missing.append(f)\n        missings_perc = missings/dftest.shape[0]\n        \n        print('Atributo {} tem {} amostras ({:.2%}) com valores faltantes'.format(f, missings, missings_perc))\n        \nprint('No total, há {} atributos com valores faltantes'.format(len(atributos_missing)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4b26d82880ad75dac68d0f52c464b13531321505"},"cell_type":"markdown","source":"Podemos recorrer a duas estratégias para lidar com dados faltantes, a remoção da feature ou o prenchimento sintético, com dados ordinais não devemos utilizar a média, \ncom dados contínuos não podemos utilizar a moda.\nAs features ps_car_03_cat, ps_car_05_cat, tem respectivamente 411231 amostras (69.09%) com valores faltantes,e 266551 amostras (44.78%) com valores faltantes, nesse caso\na remoção do atributo é indicada.\n"},{"metadata":{"trusted":true,"_uuid":"9e74121fc887bef1b8de77da181179a4a4e3addc"},"cell_type":"code","source":"# removendo ps_car_03_cat e ps_car_05_cat que tem muitos valores faltantes\nvars_to_drop = ['ps_car_03_cat', 'ps_car_05_cat']\ntrain = train.drop(vars_to_drop, axis=1)\ntest = test.drop(vars_to_drop, axis=1)\nmeta.loc[(vars_to_drop),'keep'] = False  # atualiza os metadados para ter como referência (processar o test depois)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aecc2b3ef3eb267e11df28b1aff4a9340fca8bf2"},"cell_type":"markdown","source":"Para os demais casos, utilizaremos a média para dados contínuos e moda para ordinais"},{"metadata":{"trusted":true,"_uuid":"ed3d549846f168657967a571e2827a36b3a37d46"},"cell_type":"code","source":"from sklearn.preprocessing import Imputer\n\nmedia_imp = Imputer(missing_values=-1, strategy='mean', axis=0)\nmoda_imp = Imputer(missing_values=-1, strategy='most_frequent', axis=0)\ntrain['ps_reg_03'] = media_imp.fit_transform(train[['ps_reg_03']]).ravel()\ntrain['ps_car_12'] = media_imp.fit_transform(train[['ps_car_12']]).ravel()\ntrain['ps_car_14'] = media_imp.fit_transform(train[['ps_car_14']]).ravel()\ntrain['ps_car_11'] = moda_imp.fit_transform(train[['ps_car_11']]).ravel()\n\ntest['ps_reg_03'] = media_imp.fit_transform(test[['ps_reg_03']]).ravel()\ntest['ps_car_12'] = media_imp.fit_transform(test[['ps_car_12']]).ravel()\ntest['ps_car_14'] = media_imp.fit_transform(test[['ps_car_14']]).ravel()\ntest['ps_car_11'] = moda_imp.fit_transform(test[['ps_car_11']]).ravel()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9c02678a5baca453356fc2d842ac1853da7e9410"},"cell_type":"markdown","source":"## One-hot encoding (ou dummy variables)\n\nEstamos verificando a quantidade de valores categóricos únicos e criando uma coluna para cada tipo de valor do atributo "},{"metadata":{"trusted":true,"_uuid":"b41a150f69bd0653da08937c898968f6710db804"},"cell_type":"code","source":"v = meta[(meta.level == 'nominal') & (meta.keep)].index\n\nfor f in v:\n    dist_values = train[f].value_counts().shape[0]\n    print('Atributo {} tem {} valores distintos'.format(f, dist_values))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"85f6caf7cbccc113821589324359cf1cd217cf77"},"cell_type":"code","source":"v = meta[(meta.level == 'nominal') & (meta.keep)].index\nprint('Antes do one-hot encoding tinha-se {} atributos'.format(train.shape[1]))\ntrain = pd.get_dummies(train, columns=v, drop_first=True)\nprint('Depois do one-hot encoding tem-se {} atributos'.format(train.shape[1]))\n\ntest = pd.get_dummies(test, columns=v, drop_first=True)\nmissing_cols = set( train.columns ) - set( test.columns )\nfor c in missing_cols:\n    test[c] = 0\n    \ntrain, test = train.align(test, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"28b07fb36a7ec5a5186408065a8c9016de1a4351"},"cell_type":"markdown","source":"# **Verificando o tamos dos datasets utilizados após o trabalho de transformação**"},{"metadata":{"trusted":true,"_uuid":"e099b56c9535a26b44a935217806a83fa1c8fefb"},"cell_type":"code","source":"print(train.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cf260d1dfa5301ed1a068f19f4f8eefefdce4fb3"},"cell_type":"markdown","source":"## Agora é necessário separar os dados em dados de treino e teste, é importantissímos remover a coluna target do conjunto"},{"metadata":{"trusted":true,"_uuid":"f1456478da3690b8e00e59f9fa22c3f46acc0073"},"cell_type":"code","source":"X_train = train.drop(['id', 'target'], axis=1)\ny_train = train['target']\n\nX_test  = test.drop(['id', 'target'], axis=1)\ny_test  = test['target']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"008438949da88f3f862fb6097e402596406290ed"},"cell_type":"markdown","source":"A função baixo é uma implementação do índice Gini, a implementação da função foi retirada de https://www.kaggle.com/c/ClaimPredictionChallenge/discussion/703"},{"metadata":{"trusted":true,"_uuid":"b0e511ac80f4601b19aeb891417fe5e05e107e94"},"cell_type":"code","source":"def gini(actual, pred, cmpcol = 0, sortcol = 1):  \n       assert( len(actual) == len(pred) )  \n       all = np.asarray(np.c_[ actual, pred, np.arange(len(actual)) ], dtype=np.float)  \n       all = all[ np.lexsort((all[:,2], -1*all[:,1])) ]  \n       totalLosses = all[:,0].sum()  \n       giniSum = all[:,0].cumsum().sum() / totalLosses  \n  \n       giniSum -= (len(actual) + 1) / 2.  \n       return giniSum / len(actual)  \n  \ndef gini_normalized(a, p):  \n   return gini(a, p) / gini(a, a)  ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5462178cc89a153b2db2ffadb38d7af211da79e9"},"cell_type":"markdown","source":"**Abaixo utilizaremos o algoritmo de Classificação LogisticRegression para a classificação e exibiremos o a métrica de accuracy_score**"},{"metadata":{"trusted":true,"_uuid":"ff7e29cba74e93bbe2e27c0d52f07a3af1108eea"},"cell_type":"code","source":"model = LogisticRegression()\nmodel.fit(X_train, y_train)\ny_predl = model.predict(X_test)\nacc_logistic = round(accuracy_score(y_predl, y_test) * 100, 2)\nprint(acc_logistic)\n\n\ny_predlp = model.predict_proba(X_test)[:,1]\nginil = gini(y_predl, y_predlp)\nginiln = gini_normalized(y_predl, y_predlp)\nprint(ginil)\nprint(giniln)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c7fc2c89f04ddc21c141e169a08cec0dd312d58b"},"cell_type":"markdown","source":"**Abaixo utilizaremos o algoritmo de Classificação DecisionTree para a classificação e exibiremos o a métrica de accuracy_score**"},{"metadata":{"trusted":true,"_uuid":"ea72168745c7840ec5af140815af883cde7237ee"},"cell_type":"code","source":"decisiontree = DecisionTreeClassifier()\ndecisiontree.fit(X_train, y_train)\ny_predd = decisiontree.predict(X_test)\nacc_decisiontree = round(accuracy_score(y_predd, y_test) * 100, 2)\nprint(acc_decisiontree)\n\ny_preddp = decisiontree.predict_proba(X_test)[:,1]\nginid = gini(y_predd, y_preddp)\nginidn = gini_normalized(y_predd, y_preddp)\nprint(ginid)\nprint(ginidn)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ed68a42b357c5b5b70e6deb73bfbaa32ae3ea2b0"},"cell_type":"markdown","source":"**Abaixo utilizaremos o algoritmo de classificação gradientBoosting para a classificação e exibiremos o a métrica de accuracy_score**"},{"metadata":{"trusted":true,"_uuid":"531dd62c9508fb53fd8ac721dcdc2744181fdf83"},"cell_type":"code","source":"gbk = GradientBoostingClassifier()\ngbk.fit(X_train, y_train)\ny_predg = gbk.predict(X_test)\nacc_gbk = round(accuracy_score(y_predg, y_test) * 100, 2)\nprint(acc_gbk)\n\ny_predgp = gbk.predict_proba(X_test)[:,1]\nginigp = gini(y_predg, y_predgp)\nginigpn = gini_normalized(y_predg, y_predgp)\nprint(ginigp)\nprint(ginigpn)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d272a230ac1e5b397d9717eae4eb9eb28c608e1b"},"cell_type":"markdown","source":"**Abaixo utilizaremos o algoritmo de classificação RandomForest para a classificação e exibiremos o a métrica de accuracy_score**"},{"metadata":{"trusted":true,"_uuid":"3023a7a911659f15e96f8dabde28398de26e4f1e"},"cell_type":"code","source":"randomforest = RandomForestClassifier()\nrandomforest.fit(X_train, y_train)\ny_predr = randomforest.predict(X_test)\nacc_randomforest = round(accuracy_score(y_predr, y_test) * 100, 2)\nprint(acc_randomforest)\n\ny_predrp = gbk.predict_proba(X_test)[:,1]\nginir = gini(y_predr, y_predrp)\nginirn = gini_normalized(y_predr, y_predrp)\nprint(ginir)\nprint(ginirn)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a3285972def27476f69afa3a67f63131e8b6b005"},"cell_type":"markdown","source":"**Abaixo utilizaremos o algoritmo de classificação MultinomialNB para a classificação e exibiremos o a métrica de accuracy_score**"},{"metadata":{"trusted":true,"_uuid":"da7351cdf06f6fa12cc677387de9cde79f31dcaa"},"cell_type":"code","source":"clf = MultinomialNB()\nclf.fit(X_train, y_train)\nMultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\ny_pred_nb = clf.predict(X_test)\nacc_nb = round(accuracy_score(y_predl, y_test) * 100, 2)\nprint(acc_nb)\n\n\ny_pred_nbp = clf.predict_proba(X_test)[:,1]\ngininb = gini(y_pred_nb, y_pred_nbp)\nginidnb = gini_normalized(y_pred_nb, y_pred_nbp)\nprint(gininb)\nprint(ginidnb)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c3b135c5b6dd4f8ceb735f0ed6367126a22a1dcb"},"cell_type":"markdown","source":"**A função baixo é uma implementação do índice Gini, a implementação da função foi retirada de https://www.kaggle.com/c/ClaimPredictionChallenge/discussion/703 **"},{"metadata":{"_uuid":"cabbe0ac790d048551d77ddd1f20ba4bee41b614"},"cell_type":"markdown","source":"## **Função para calcular o curva lorenz **"},{"metadata":{"trusted":true,"_uuid":"e3f8ec3149b7572e43044ce875c0e83bf7ae31d2"},"cell_type":"code","source":"def lorenz(arr):\n    # this divides the prefix sum by the total sum\n    # this ensures all the values are between 0 and 1.0\n    scaled_prefix_sum = arr.cumsum() / arr.sum()\n    # this prepends the 0 value (because 0% of all people have 0% of all wealth)\n    return np.insert(scaled_prefix_sum, 0, 0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"49901274da29bc7965877ce5c3825dd2d6bd2936"},"cell_type":"code","source":"lorenz_curve = lorenz(np.sort(y_predrp, axis=None))\n# we need the X values to be between 0.0 to 1.0\nplt.plot(np.linspace(0.0, 1.0, lorenz_curve.size), lorenz_curve)\n# plot the straight line perfect equality curve\nplt.plot([0,1], [0,1])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ccb095c6b585023811abfc52004541a348f3b39d"},"cell_type":"markdown","source":"## **Criando arquivo CSV com resultado para Submissão**"},{"metadata":{"trusted":true,"_uuid":"44a0df7f6c7fab614af04a41c2568092d7954b0d"},"cell_type":"code","source":"# Create submission file\nsubmission = pd.DataFrame()\nsubmission['id'] = dftest['id']\nsubmission['target'] = y_predrp\nsubmission.to_csv('submit.csv', float_format='%.6f', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0d2867843cbe8e8cdd77e258f59c08c3228311d0"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}