{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import Image\nfrom IPython.core.display import HTML \n%matplotlib inline ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/landmark-recognition-challenge/train.csv.zip')\ntest_df = pd.read_csv('../input/landmark-recognition-challenge/test.csv.zip')\nsubmission = pd.read_csv('../input/landmark-recognition-challenge/sample_submission.csv.zip')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Train data shape -  rows:\",train_df.shape[0],\" columns:\", train_df.shape[1])\nprint(\"Test data size -  rows:\",test_df.shape[0],\" columns:\", test_df.shape[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# missing data in training data set\nmissing = train_df.isnull().sum()\nall_val = train_df.count()\n\nmissing_train_df = pd.concat([missing, all_val], axis=1, keys=['Missing', 'All'])\nmissing_train_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# missing data in training data set\nmissing = test_df.isnull().sum()\nall_val = test_df.count()\n\nmissing_test_df = pd.concat([missing, all_val], axis=1, keys=['Missing', 'All'])\nmissing_test_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# concatenate train and test datasets\nconcatenated = pd.concat([train_df, test_df])\n# print the shape of the resulted data.frame\nconcatenated.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"concatenated.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (8, 8))\nplt.title('Landmark id density plot')\nsns.kdeplot(train_df['landmark_id'], color=\"tomato\", shade=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (8, 8))\nplt.title('Landmark id distribuition and density plot')\nsns.distplot(train_df['landmark_id'],color='green', kde=True,bins=100)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"th10 = pd.DataFrame(train_df.landmark_id.value_counts().head(10))\nth10.reset_index(level=0, inplace=True)\nth10.columns = ['landmark_id','count']\nth10","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the most frequent landmark occurences\nplt.figure(figsize = (6, 6))\nplt.title('Most frequent landmarks')\nsns.set_color_codes(\"pastel\")\nsns.barplot(x=\"landmark_id\", y=\"count\", data=th10,\n            label=\"Count\", color=\"darkgreen\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tb10 = pd.DataFrame(train_df.landmark_id.value_counts().tail(10))\ntb10.reset_index(level=0, inplace=True)\ntb10.columns = ['landmark_id','count']\ntb10","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the least frequent landmark occurences\nplt.figure(figsize = (6,6))\nplt.title('Least frequent landmarks')\nsns.set_color_codes(\"pastel\")\nsns.barplot(x=\"landmark_id\", y=\"count\", data=tb10,\n            label=\"Count\", color=\"orange\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extract repositories names for train data\nll = list()\nfor path in train_df['url']:\n    ll.append((path.split('//', 1)[1]).split('/', 1)[0])\ntrain_df['site'] = ll\n# Extract repositories names for test data\nll = list()\nfor path in test_df['url']:\n    ll.append((path.split('//', 1)[1]).split('/', 1)[0])\ntest_df['site'] = ll","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Train data shape -  rows:\",train_df.shape[0],\" columns:\", train_df.shape[1])\nprint(\"Test data size -  rows:\",test_df.shape[0],\" columns:\", test_df.shape[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_site = pd.DataFrame(train_df.site.value_counts())\ntest_site = pd.DataFrame(test_df.site.value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_site","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trsite = pd.DataFrame(list(train_site.index),train_site['site'])\ntrsite.reset_index(level=0, inplace=True)\ntrsite.columns = ['count','site']\nplt.figure(figsize = (6,6))\nplt.title('Sites storing images - train dataset')\nsns.set_color_codes(\"pastel\")\nsns.barplot(x = 'site', y=\"count\", data=trsite, color=\"blue\")\nlocs, labels = plt.xticks()\nplt.setp(labels, rotation=90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_site","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tesite = pd.DataFrame(list(test_site.index),test_site['site'])\ntesite.reset_index(level=0, inplace=True)\ntesite.columns = ['count','site']\nplt.figure(figsize = (6,6))\nplt.title('Sites storing images - test dataset')\nsns.set_color_codes(\"pastel\")\nsns.barplot(x = 'site', y=\"count\", data=tesite, color=\"magenta\")\nlocs, labels = plt.xticks()\nplt.setp(labels, rotation=90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def displayLandmark(urls, landmarkName):\n    \n    img_style = \"height: 60px; margin: 2px; float: left; border: 1px solid blue;\"\n    images_list = ''.join([f\"<img style='{img_style}' src='{u}' />\" for _, u in urls.iteritems()])\n\n    display(HTML(images_list))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"IMAGES_NUMBER = 60\nlandmarkId = train_df['landmark_id'].value_counts().keys()[5]\nurls = train_df[train_df['landmark_id'] == landmarkId]['url'].head(IMAGES_NUMBER)\ndisplayLandmark(urls, \"Petronas\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"urls.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from PIL import Image\nfrom PIL.ExifTags import TAGS, GPSTAGS\n\n\nclass ImageMetaData(object):\n    '''\n    Extract the exif data from any image. Data includes GPS coordinates, \n    Focal Length, Manufacture, and more.\n    '''\n    exif_data = None\n    image = None\n\n    def __init__(self, img_path):\n        self.image = Image.open(img_path)\n        print(self.image._getexif())\n        #self.get_exif_data()\n        #super(ImageMetaData, self).__init__()\n\n    def get_exif_data(self):\n        \"\"\"Returns a dictionary from the exif data of an PIL Image item. Also converts the GPS Tags\"\"\"\n        exif_data = {}\n        info = self.image._getexif()\n        if info:\n            for tag, value in info.items():\n                decoded = TAGS.get(tag, tag)\n                if decoded == \"GPSInfo\":\n                    gps_data = {}\n                    for t in value:\n                        sub_decoded = GPSTAGS.get(t, t)\n                        gps_data[sub_decoded] = value[t]\n\n                    exif_data[decoded] = gps_data\n                else:\n                    exif_data[decoded] = value\n        self.exif_data = exif_data\n        return exif_data\n\n    def get_if_exist(self, data, key):\n        if key in data:\n            return data[key]\n        return None\n\n    def convert_to_degress(self, value):\n\n        \"\"\"Helper function to convert the GPS coordinates \n        stored in the EXIF to degress in float format\"\"\"\n        d0 = value[0][0]\n        d1 = value[0][1]\n        d = float(d0) / float(d1)\n\n        m0 = value[1][0]\n        m1 = value[1][1]\n        m = float(m0) / float(m1)\n\n        s0 = value[2][0]\n        s1 = value[2][1]\n        s = float(s0) / float(s1)\n\n        return d + (m / 60.0) + (s / 3600.0)\n\n    def get_lat_lng(self):\n        \"\"\"Returns the latitude and longitude, if available, from the provided exif_data (obtained through get_exif_data above)\"\"\"\n        lat = None\n        lng = None\n        exif_data = self.get_exif_data()\n        #print(exif_data)\n        if \"GPSInfo\" in exif_data:      \n            gps_info = exif_data[\"GPSInfo\"]\n            gps_latitude = self.get_if_exist(gps_info, \"GPSLatitude\")\n            gps_latitude_ref = self.get_if_exist(gps_info, 'GPSLatitudeRef')\n            gps_longitude = self.get_if_exist(gps_info, 'GPSLongitude')\n            gps_longitude_ref = self.get_if_exist(gps_info, 'GPSLongitudeRef')\n            if gps_latitude and gps_latitude_ref and gps_longitude and gps_longitude_ref:\n                lat = self.convert_to_degress(gps_latitude)\n                if gps_latitude_ref != \"N\":                     \n                    lat = 0 - lat\n                lng = self.convert_to_degress(gps_longitude)\n                if gps_longitude_ref != \"E\":\n                    lng = 0 - lng\n        return lat, lng\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# submit the most freq label\nsubmission['landmarks'] = '%d %2.2f' % (freq_label.index[0], freq_label.values[0])\nsubmission.to_csv('submission.csv', index=False)\n\nnp.random.seed(2018)\nr_idx = lambda : np.random.choice(freq_label.index, p = freq_label.values)\n\nr_score = lambda idx: '%d %2.4f' % (freq_label.index[idx], freq_label.values[idx])\nsubmission['landmarks'] = submission.id.map(lambda _: r_score(r_idx()))\nsubmission.to_csv('rand_submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = pd.read_csv('../input/landmark-recognition-challenge/train.csv.zip')\ntest_data = pd.read_csv('../input/landmark-recognition-challenge/test.csv.zip')\nsubmission = pd.read_csv(\"../input/landmark-recognition-challenge/sample_submission.csv.zip\")\n\nprint(\"Training data size\",train_data.shape)\nprint(\"test data size\",test_data.shape)\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now open the URL\ntemp = 4444\nprint('id', train_data['id'][temp])\nprint('url:', train_data['url'][temp])\nprint('landmark id:', train_data['landmark_id'][temp])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['landmark_id'].value_counts().hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# missing data in training data \ntotal = train_data.isnull().sum().sort_values(ascending = False)\npercent = (train_data.isnull().sum()/train_data.isnull().count()).sort_values(ascending = False)\nmissing_train_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# missing data in test data \ntotal = test_data.isnull().sum().sort_values(ascending = False)\npercent = (test_data.isnull().sum()/test_data.isnull().count()).sort_values(ascending = False)\nmissing_test_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_test_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Occurance of landmark_id in decreasing order(Top categories)\ntemp = pd.DataFrame(train_data.landmark_id.value_counts().head(8))\ntemp.reset_index(inplace=True)\ntemp.columns = ['landmark_id','count']\ntemp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the most frequent landmark_ids\nplt.figure(figsize = (9, 8))\nplt.title('Most frequent landmarks')\nsns.set_color_codes(\"pastel\")\nsns.barplot(x=\"landmark_id\", y=\"count\", data=temp,\n            label=\"Count\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Occurance of landmark_id in increasing order\ntemp = pd.DataFrame(train_data.landmark_id.value_counts().tail(8))\ntemp.reset_index(inplace=True)\ntemp.columns = ['landmark_id','count']\ntemp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Plot the least frequent landmark_ids\nplt.figure(figsize = (9, 8))\nplt.title('Least frequent landmarks')\nsns.set_color_codes(\"pastel\")\nsns.barplot(x=\"landmark_id\", y=\"count\", data=temp,\n            label=\"Count\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Unique URL's\ntrain_data.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Class distribution\nplt.figure(figsize = (10, 8))\nplt.title('Category Distribuition')\nsns.distplot(train_data['landmark_id'])\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint(\"Number of classes under 20 occurences\",(train_data['landmark_id'].value_counts() <= 20).sum(),'out of total number of categories',len(train_data['landmark_id'].unique()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import Image\nfrom IPython.core.display import HTML \n\ndef display_category(urls, category_name):\n    img_style = \"width: 180px; margin: 0px; float: left; border: 1px solid black;\"\n    images_list = ''.join([f\"<img style='{img_style}' src='{u}' />\" for _, u in urls.head(12).iteritems()])\n\n    display(HTML(images_list))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"category = train_data['landmark_id'].value_counts().keys()[0]\nurls = train_data[train_data['landmark_id'] == category]['url']\ndisplay_category(urls, \"\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extract site_names for train data\ntemp_list = list()\nfor path in train_data['url']:\n    temp_list.append((path.split('//', 1)[1]).split('/', 1)[0])\ntrain_data['site_name'] = temp_list\n# Extract site_names for test data\ntemp_list = list()\nfor path in test_data['url']:\n    temp_list.append((path.split('//', 1)[1]).split('/', 1)[0])\ntest_data['site_name'] = temp_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Training data size\",train_data.shape)\nprint(\"test data size\",test_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head(8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Occurance of site in decreasing order(Top categories)\ntemp = pd.DataFrame(train_data.site_name.value_counts())\ntemp.reset_index(inplace=True)\ntemp.columns = ['site_name','count']\ntemp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the Sites with their count\nplt.figure(figsize = (9, 8))\nplt.title('Sites with their count')\nsns.set_color_codes(\"pastel\")\nsns.barplot(x=\"site_name\", y=\"count\", data=temp,\n            label=\"Count\")\nlocs, labels = plt.xticks()\nplt.setp(labels, rotation=90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Occurance of site in decreasing order(Top categories)\ntemp = pd.DataFrame(test_data.site_name.value_counts())\ntemp.reset_index(inplace=True)\ntemp.columns = ['site_name','count']\ntemp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the Sites with their count\nplt.figure(figsize = (9, 8))\nplt.title('Sites with their count')\nsns.set_color_codes(\"pastel\")\nsns.barplot(x=\"site_name\", y=\"count\", data=temp,\n            label=\"Count\")\nlocs, labels = plt.xticks()\nplt.setp(labels, rotation=90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# take the most frequent label\nfreq_label = train_df['landmark_id'].value_counts()/train_df['landmark_id'].value_counts().sum()\n\n# submit the most freq label\nsubmission['landmarks'] = '%d %2.2f' % (freq_label.index[0], freq_label.values[0])\nsubmission.to_csv('submission.csv', index=False)\n\nnp.random.seed(2018)\nr_idx = lambda : np.random.choice(freq_label.index, p = freq_label.values)\n\nr_score = lambda idx: '%d %2.4f' % (freq_label.index[idx], freq_label.values[idx])\nsubmission['landmarks'] = submission.id.map(lambda _: r_score(r_idx()))\nsubmission.to_csv('rand_submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}