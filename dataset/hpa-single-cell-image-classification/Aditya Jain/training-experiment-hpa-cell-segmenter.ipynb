{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n! pip install pytorch-lightning\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install torchmetrics","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install \"../input/hpacellsegmentatorraman/HPA-Cell-Segmentation/\"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pycocotools\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install efficientnet_pytorch\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -U albumentations\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -q \"../input/zoohpa/pytorch_zoo-master\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#End to End Classification\nimport torch\nimport pytorch_lightning as pl\nimport torchvision.models as models\n\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchvision import models\n\nfrom skimage import io\nfrom skimage.transform import resize\nfrom skimage.color import rgb2gray,gray2rgb\nfrom matplotlib import pyplot as plt\n\nfrom PIL import Image\n\nimport csv\nimport pandas as pd\nimport numpy as np\nimport cv2\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nimport hpacellseg.cellsegmentator as cellsegmentator\nfrom hpacellseg.utils import label_cell, label_nuclei\nimport base64\nimport numpy as np\nfrom pycocotools import mask\nfrom pycocotools import _mask as coco_mask\nimport typing as t\nimport zlib","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nNUC_MODEL = \"../input/requiredfiles/nuclei-model.pth\"\nCELL_MODEL = \"../input/requiredfiles/cell-model.pth\"\n\n\nsegmentator = cellsegmentator.CellSegmentator(\n    NUC_MODEL,\n    CELL_MODEL,\n    device=\"cuda\",\n    multi_channel_model=True,\n)\n\n\ndef extract_data_from_csv(csvpath):\n    fields = [] \n    rows = [] \n    filename = csvpath\n    with open(filename, 'r') as csvfile: \n        # creating a csv reader object \n        csvreader = csv.reader(csvfile) \n\n        fields = next(csvreader)\n        # extracting each data row one by one \n        for row in csvreader: \n            rows.append(row) \n            \n    return rows\n\n# reading csv file\n\ndef get_segmented_image_and_label(container,index):\n    \n    '''\n    Takes csv_path and index as an input\n    Returns Segmented image and label if there is only single label\n    '''\n    \n    #for row in rows[index]: # Read image and index\n    Data = container[index]\n    image_id = Data[0]\n    labels = Data[1]\n    labels = labels.replace(\"|\",\",\")\n    list_labels = list(labels.split(\",\")) \n\n    if(len(list_labels) == 1):\n\n        img_blue = io.imread(\"../input/hpa-single-cell-image-classification/train/\"+ image_id+\"_blue.png\")\n        img_green = io.imread(\"../input/hpa-single-cell-image-classification/train/\"+ image_id+\"_green.png\")\n        img_red = io.imread(\"../input/hpa-single-cell-image-classification/train/\"+ image_id+\"_red.png\")\n        img_yellow = io.imread(\"../input/hpa-single-cell-image-classification/train/\"+ image_id+\"_yellow.png\")\n        \n#         img_blue = Image.open(\"../input/hpa-single-cell-image-classification/train/\"+ image_id+\"_blue.png\")\n#         img_green = Image.open(\"../input/hpa-single-cell-image-classification/train/\"+ image_id+\"_green.png\")\n#         img_red = Image.open(\"../input/hpa-single-cell-image-classification/train/\"+ image_id+\"_red.png\")\n#         img_yellow = Image.open(\"../input/hpa-single-cell-image-classification/train/\"+ image_id+\"_yellow.png\")\n        \n#         divider = 1\n#         img_blue_resized = resize(img_blue, (img_blue.shape[0] // divider, img_blue.shape[1] // divider),\n#                        anti_aliasing=True)\n#         img_green_resized = resize(img_green, (img_green.shape[0] // divider, img_green.shape[1] // divider),\n#                        anti_aliasing=True)\n#         img_red_resized = resize(img_green, (img_red.shape[0] // divider, img_red.shape[1] // divider),\n#                        anti_aliasing=True)\n#         img_yellow_resized = resize(img_yellow, (img_yellow.shape[0] // divider, img_yellow.shape[1] // divider),\n#                        anti_aliasing=True)\n\n        #https://www.kaggle.com/thedrcat/hpa-single-cell-classification-eda/comments#Contents\n        rgb = np.dstack((img_red_resized,img_yellow_resized+img_green_resized,img_blue_resized))\n\n\n        nuc_segmentations = segmentator.pred_nuclei([rgb[:,:,2]])\n\n        # For full cells\n        cell_segmentations = segmentator.pred_cells([rgb],precombined = True)\n\n        #print(cell_segmentations[0])\n        #for i, pred in enumerate(cell_segmentations):\n        nuclei_mask, cell_mask = label_cell(nuc_segmentations[0], cell_segmentations[0])\n\n        return rgb,cell_mask,image_id,list_labels\n    else:\n        return -1,-1,-1,-1\n   \n\n            \ndef binary_mask_to_rle(binary_mask):\n    rle = {'counts': [], 'size': list(binary_mask.shape)}\n    counts = rle.get('counts')\n\n    last_elem = 0\n    running_length = 0\n\n    for i, elem in enumerate(binary_mask.ravel(order='F')):\n        if elem == last_elem:\n            pass\n        else:\n            counts.append(running_length)\n            running_length = 0\n            last_elem = elem\n        running_length += 1\n\n    counts.append(running_length)\n    \n    return rle\n      \n\ndef compress_rle(rle):\n    compressed_rle = mask.frPyObjects(rle, rle.get('size')[0], rle.get('size')[1])\n    return compressed_rle\n\ndef decode_rle(compressed_rle):\n    mask_decode= mask.decode(compressed_rle)\n    return mask_decode\n\ndef convert_to_binary_and_normalize(seg_mask):\n    cell_mask = seg_mask[..., np.newaxis] # Added a dimension for channel\n    _,thresh = cv2.threshold(cell_mask,0,255,cv2.THRESH_BINARY) # binarize\n    thresh_norm = cv2.normalize(src=thresh, dst=None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n    return thresh_norm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #Create a cropped CSV list\n# #from IPython.display import FileLink\n# filename = \"Custom_Dataset.csv\"\n# fields = ['ImageId','Coordinate','Comp_RLE-Mask', 'label'] # Will Contain only single labelled cropped images\n# container = extract_data_from_csv(\"../input/hpa-single-cell-image-classification/train.csv\")\n\n\n# with open(filename, 'w') as csvfile:\n#     csvwriter = csv.writer(csvfile)  \n#     csvwriter.writerow(fields)\n#     #print(\"Started Writing\")\n#     for idx in range(8293,10000,1):#len(container)\n#         rgb,seg_mask,imageid,label = get_segmented_image_and_label(container,idx)\n#         print(idx)\n#         if(label != -1):\n#             thresh_norm = convert_to_binary_and_normalize(seg_mask)\n#             contours, hierarchy = cv2.findContours(thresh_norm, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n#             print(\"Total Contours: \", len(contours))\n#             for c in contours:\n#                 x, y, w, h = cv2.boundingRect(c)\n                \n#                 cropped_rgb = rgb[y:y+h,x:x+w]\n#                 cropped_thresh = thresh_norm[y:y+h,x:x+w]\n                \n#                 cropped_rgb_resize = cv2.resize(cropped_rgb, (300, 300), interpolation = cv2.INTER_NEAREST)\n#                 cropped_thresh_resize = cv2.resize(cropped_thresh, (300, 300), interpolation = cv2.INTER_NEAREST)\n                \n#                 rle = binary_mask_to_rle(cropped_thresh_resize)\n#                 compressed_rle = compress_rle(rle)\n#                 #print(compressed_rle.dtype)\n#                 #print(\"Writing into CSV\")\n#                 csvwriter.writerow([imageid,[x,y,w,h],compressed_rle,label[0]])\n                \n# #                 fig, ax = plt.subplots(1,3, figsize=(20,50))\n# #                 ax[0].imshow(rgb)\n# #                 ax[0].axis('off')\n\n# #                 ax[1].imshow(cropped_rgb_resize)\n# #                 ax[1].axis('off')\n                \n# #                 mask_decoded = decode_rle(compressed_rle)\n                \n# #                 ax[2].imshow(mask_decoded)\n# #                 ax[2].axis('off')\n# #                 plt.show()\n\n# print(\"END\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Read CSV and decode RLE\n# import ast\n# custom_data = extract_data_from_csv(\"./Custom_Dataset.csv\")\n\n# for index in range(1):\n#     Data = custom_data[index]\n#     image_id = Data[0]\n#     coord = ast.literal_eval(Data[1])\n#     compressed_rle = ast.literal_eval(Data[2])\n#     #print(coord)\n#     fig, ax = plt.subplots(1,3, figsize=(20,50))\n#     img_blue = io.imread(\"../input/hpa-single-cell-image-classification/train/\"+ image_id+\"_blue.png\")\n#     img_green = io.imread(\"../input/hpa-single-cell-image-classification/train/\"+ image_id+\"_green.png\")\n#     img_red = io.imread(\"../input/hpa-single-cell-image-classification/train/\"+ image_id+\"_red.png\")\n#     img_yellow = io.imread(\"../input/hpa-single-cell-image-classification/train/\"+ image_id+\"_yellow.png\")\n    \n#     rgb = np.dstack((img_red,img_yellow+img_green,img_blue))\n#     cropped_rgb = rgb[coord[1]:coord[1]+coord[3],coord[0]:coord[0]+coord[2]]\n#     cropped_rgb_resize = cv2.resize(cropped_rgb, (300, 300), interpolation = cv2.INTER_NEAREST)\n#     mask_decoded = decode_rle(compressed_rle)\n#     ax[0].imshow(rgb)\n#     ax[0].axis('off')\n#     ax[1].imshow(cropped_rgb_resize)\n#     ax[1].axis('off')\n#     ax[2].imshow(mask_decoded)\n#     ax[2].axis('off')\n#     plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# container = extract_data_from_csv(\"../input/hpa-single-cell-image-classification/train.csv\")\n\n# for i in range(6):\n#     rgb,seg_mask,imageid,label = get_segmented_image_and_label(container,i)\n       \n#     if(label!= -1):\n        \n#         thresh_norm = convert_to_binary_and_normalize(seg_mask)\n        \n#         fig, ax = plt.subplots(1,3, figsize=(20,50))\n#         ax[0].imshow(thresh_norm)\n#         ax[0].axis('off')\n\n#         ax[1].imshow(rgb)\n#         ax[1].axis('off')\n        \n       \n#         rle = binary_mask_to_rle(thresh_norm)\n#         compressed_rle = compress_rle(rle)\n#         mask_decoded = decode_rle(compressed_rle)\n        \n#         ax[2].imshow(mask_decode)\n#         ax[2].axis('off')\n        \n#         plt.show()\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from urllib.request import urlopen\n# from io import BytesIO\n# from zipfile import ZipFile\n# from subprocess import Popen\n# from os import chmod\n# from os.path import isfile\n# import json\n# import time\n# import psutil\n\n# def launch_tensorboard():\n#     tb_process, ngrok_process = None, None\n    \n#     # Launch TensorBoard\n#     if not is_process_running('tensorboard'):\n#         tb_command = 'tensorboard --logdir ./lightning_logs/ --host 0.0.0.0 --port 6006'\n#         tb_process = run_cmd_async_unsafe(tb_command)\n    \n#     # Install ngrok\n#     if not isfile('./ngrok'):\n#         ngrok_url = 'https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip'\n#         download_and_unzip(ngrok_url)\n#         chmod('./ngrok', 0o755)\n\n#     # Create ngrok tunnel and print its public URL\n#     if not is_process_running('ngrok'):\n#         ngrok_process = run_cmd_async_unsafe('./ngrok http 6006')\n#         time.sleep(1) # Waiting for ngrok to start the tunnel\n#     ngrok_api_res = urlopen('http://127.0.0.1:4040/api/tunnels', timeout=10)\n#     ngrok_api_res = json.load(ngrok_api_res)\n#     assert len(ngrok_api_res['tunnels']) > 0, 'ngrok tunnel not found'\n#     tb_public_url = ngrok_api_res['tunnels'][0]['public_url']\n#     print(f'TensorBoard URL: {tb_public_url}')\n\n#     return tb_process, ngrok_process\n\n\n# def download_and_unzip(url, extract_to='.'):\n#     http_response = urlopen(url)\n#     zipfile = ZipFile(BytesIO(http_response.read()))\n#     zipfile.extractall(path=extract_to)\n\n\n# def run_cmd_async_unsafe(cmd):\n#     return Popen(cmd, shell=True)\n\n\n# def is_process_running(process_name):\n#     running_process_names = (proc.name() for proc in psutil.process_iter())\n#     return process_name in running_process_names\n\n\n# tb_process, ngrok_process = launch_tensorboard()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a DataLoader which returns contour based cropped images\n\nimport ast\n\n#Training Loader\nclass CustomDataSet(Dataset): # To have 3 channel image ( r,y,b ) as input and g as mask with 19 class vector\n    \n    def __init__(self,root_dir_image,image_ids,labels,cord,transform):\n        self.root_dir_image = root_dir_image\n        self.augmentations = transform\n        self.X_fold = image_ids\n        self.y_fold = labels\n        self.crop_fold = cord\n        self.corrd = np.zeros((4,), dtype=int)\n        \n    \n    def __len__(self):\n        return len(self.X_fold)\n    \n   \n    def __getitem__(self,index): \n        self.coord  = ast.literal_eval(self.crop_fold[index])\n#         print(self.coord)\n#         print(len(self.coord))\n        \n        img_blue = np.array(Image.open(self.root_dir_image + self.X_fold[index]+\"_blue.png\"))\n        img_red = np.array(Image.open(self.root_dir_image + self.X_fold[index] +\"_red.png\"))\n        img_yellow = np.array(Image.open(self.root_dir_image + self.X_fold[index] +\"_yellow.png\"))\n        img_green = np.array(Image.open(self.root_dir_image + self.X_fold[index] +\"_green.png\"))\n        \n        #https://www.kaggle.com/thedrcat/hpa-single-cell-classification-eda/comments#Contents\n       \n        # Input Image\n        rgb = np.dstack((img_red,img_yellow + img_green,img_blue))\n        cropped_rgb = rgb[self.coord[1]:self.coord[1]+self.coord[3],self.coord[0]:self.coord[0]+self.coord[2]]\n        \n\n        torch_label = torch.from_numpy(np.asarray(self.y_fold[index]))\n        \n        if self.augmentations:\n            cropped_rgb_ = np.array(cropped_rgb)\n            torch_rgb_ = self.augmentations(image=cropped_rgb_)\n            torch_rgb = torch_rgb_['image']\n\n        \n        return torch_rgb,torch.tensor(torch_label, dtype=torch.long)\n            \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\n\nclass HPADataModule(pl.LightningDataModule):\n    def __init__(self,TrainFile,splits,validation_idx,BatchSize,ImagePath):\n        super().__init__()\n        self.csv_file = TrainFile\n        self.splits = splits\n        self.batch_size = BatchSize\n        self.Imagepath = ImagePath\n        self.validation_idx = validation_idx\n        \n        self.transform = A.Compose([\n        A.Resize(224,224),\n        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n        A.RandomBrightnessContrast(p=0.5),\n        A.HorizontalFlip(p=0.5),\n        A.Normalize(\n        \n            mean=[0,0,0],\n            std = [1,1,1],\n            max_pixel_value = 255,\n        \n        ),\n        ToTensorV2()])\n\n        \n    def prepare_data(self):\n        #prepare_data is called only once on 1- GPU in a distributed computing\n        df = pd.read_csv(self.csv_file)\n        df[\"kfold\"] =-1\n        \n        \n        df = df.sample(frac=1).reset_index(drop=True)\n        stratify = StratifiedKFold(n_splits=self.splits)\n        for i,(t_idx,v_idx) in enumerate(stratify.split(X=df.ImageId.values,y=df.label.values)):\n            df.loc[v_idx,\"kfold\"]=i\n            \n        \n        df.to_csv(\"train_folds.csv\",index=False)\n\n    def setup(self,stage=None):\n        dfx = pd.read_csv(\"train_folds.csv\")\n        train = dfx.loc[dfx[\"kfold\"]!=self.validation_idx]\n        val = dfx.loc[dfx[\"kfold\"]==self.validation_idx]\n\n        \n        self.train_dataset = CustomDataSet(self.Imagepath,\n                                            image_ids = train.ImageId.values,\n                                            labels = train.label.values,\n                                            cord = train.Coordinate.values,\n                                            transform = self.transform)\n\n        self.valid_dataset = CustomDataSet(self.Imagepath,\n                                            image_ids = val.ImageId.values,\n                                            labels = val.label.values,\n                                            cord = val.Coordinate.values,\n                                            transform = self.transform)\n\n    def train_dataloader(self):\n        return DataLoader(self.train_dataset,\n                      batch_size= self.batch_size,\n                      num_workers=4,\n                      shuffle=True)\n  \n    def val_dataloader(self):\n        return DataLoader(self.valid_dataset,\n                      batch_size= self.batch_size,\n                      num_workers=4)\n  \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.autograd import Variable\n\nclass WeightedFocalLoss(nn.Module):\n    \"Non weighted version of Focal Loss\"\n    def __init__(self,alpha=.25,gamma=2,eps=1e-7):\n        super(WeightedFocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.eps = eps\n    \n    def one_hot(self,index, classes):\n#         print(index)\n#         print(classes)\n        size = index.size() + (classes,)\n        view = index.size() + (1,)\n\n        mask = torch.Tensor(*size).fill_(0)\n        index = index.view(*view)\n        ones = 1.\n        mask = mask.cuda()\n        index = index.cuda()\n        if isinstance(index, Variable):\n            ones = Variable(torch.Tensor(index.size()).fill_(1)).cuda()\n            mask = Variable(mask, volatile=index.volatile)\n\n        #print(\"Inside Loss Function\")\n        #print(mask.shape)\n        #print(index.shape)\n        #print(ones.shape)\n        return mask.scatter_(1, index, ones)\n    \n   \n\n    def forward(self, inputs, target):\n        y = self.one_hot(target, inputs.size(-1))\n        #print(y)\n        logit = F.softmax(inputs, dim=-1)\n        logit = logit.clamp(self.eps, 1. - self.eps)\n\n        loss = -1 * y * torch.log(logit.cuda()) # cross entropy\n        loss = loss.cuda() * (1 - logit.cuda()) ** self.gamma # focal loss\n        return loss.sum()\n    \n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Class Architecture\nfrom efficientnet_pytorch import EfficientNet\nfrom pytorch_lightning.metrics import F1\nfrom pytorch_lightning.metrics.functional import confusion_matrix\nimport itertools\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.metrics import precision_recall_curve\n\nclass EffNet(pl.LightningModule):\n    \n    def __init__(self,learning_rate=1e-4):\n        super().__init__()\n        self.effnet = EfficientNet.from_pretrained('efficientnet-b3')\n        in_features = self.effnet._fc.in_features\n        self.effnet._fc = nn.Linear(in_features,19)\n        self.train_acc = pl.metrics.Accuracy()\n        self.valid_acc = pl.metrics.Accuracy()\n        self.learning_rate = learning_rate\n        self.classes = ['0','1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18']\n\n        \n        \n    def forward(self, x):\n        raw_predit = self.effnet(x)\n        return raw_predit\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=(self.learning_rate))\n        #optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n        #scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.01, max_lr=0.1)\n        \n        #return [optimizer], [scheduler]\n        return optimizer\n    \n    def loss_function(self,y_hat, y):\n#         creterion = nn.CrossEntropyLoss()\n#         loss = creterion(x_hat,x)\n\n        loss  = WeightedFocalLoss()(y_hat, y)\n        return loss\n        \n    def training_step(self, train_batch, batch_idx):\n        x, y = train_batch\n        y_hat = self(x.float())\n        loss = self.loss_function(y_hat, y)\n        \n        self.log('train_loss', loss, on_step=False, on_epoch=True,prog_bar=True,logger=True)\n        self.train_acc(y_hat, y)\n        self.log('train_acc', self.train_acc, on_step=False, on_epoch=True,prog_bar=True,logger=True)\n        return {'loss': loss}\n\n    def validation_step(self, val_batch, batch_idx):\n        x, y = val_batch\n      \n        y_hat = self(x.float()) \n        loss = self.loss_function(y_hat, y)\n        self.log('val_loss', loss, on_step=False, on_epoch=True,prog_bar=True,logger=True)\n        self.valid_acc(y_hat, y)\n        self.log('val_acc', self.valid_acc,on_step=False, on_epoch=True,prog_bar=True,logger=True)\n        return {'loss': loss, 'y': y, 'y_hat': y_hat.sigmoid()}\n    \n    def validation_epoch_end(self, outputs):\n   \n        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n        f1_score = self.get_f1(outputs)\n        #print(\"Before\")\n        self.average_precision(outputs)\n        #print(\"After\")\n        confusion_matrix = self.get_confusion_matrix(outputs)\n        print(f\"Epoch {self.current_epoch} | F1:{f1_score}\")\n        self.plot_confusion_matrix(confusion_matrix.numpy(),self.classes)\n       \n        return {'loss': avg_loss}\n    \n    def get_f1(self, outputs):\n        \n        y = torch.cat([x['y'] for x in outputs])\n        y_hat = torch.cat([x['y_hat'] for x in outputs])\n        f1 = F1(num_classes=19)\n\n        return f1(y_hat.cpu().detach(), y.cpu().detach())\n    \n    def average_precision(self,outputs):\n        print(\"Inside\")\n        y = torch.cat([x['y'] for x in outputs])\n        \n        y_hat = torch.cat([x['y_hat'] for x in outputs])\n        \n        y_hat_ = y_hat#torch.argmax(y_hat,dim=1)\n        \n        #target_data = y.cpu().detach().numpy()#pd.DataFrame(y.cpu().detach().numpy(),columns=['label'])\n        #target_data['is_valid'] = True\n        #target_data.reset_index(drop=True, inplace=True)\n        \n        predicted_data = y_hat_.cpu().detach().numpy()\n        \n        size = y.size() + (19,)\n        view = y.size() + (1,)\n\n        mask = torch.Tensor(*size).fill_(0)\n        index = y.view(*view)\n        ones = 1.\n        mask = mask.cuda()\n        index = index.cuda()\n        if isinstance(index, Variable):\n            ones = Variable(torch.Tensor(index.size()).fill_(1)).cuda()\n            mask = Variable(mask, volatile=index.volatile)\n        \n        y = y.unsqueeze(1)\n        #print(mask.shape)\n        #print(y.shape)\n        #print(ones.shape)\n        encoded_vect = mask.scatter_(1, y.cuda(), ones)\n    \n        precision = dict()\n        recall = dict()\n        average_precision = dict()\n        #print(\"Before LOOP\")\n       \n        target_data_list = encoded_vect.cpu().detach().numpy()\n        predicted_data_list = predicted_data\n        #print(encoded_vect.shape)\n        #print(predicted_data.shape)\n        for i in range(16):\n            precision[i], recall[i], _ = precision_recall_curve(target_data_list[i], predicted_data_list[i])\n            average_precision[i] = average_precision_score(target_data_list[i], predicted_data_list[i])\n\n        # A \"micro-average\": quantifying score on all classes jointly\n        precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(target_data_list.ravel(), predicted_data_list.ravel())\n        average_precision[\"micro\"] = average_precision_score(target_data_list, predicted_data_list, average=\"micro\")\n        #print(\"Before Final Print\")\n        print('Average precision score, micro-averaged over all classes: {0:0.2f}'.format(average_precision[\"micro\"]))\n        \n        \n        \n    def get_confusion_matrix(self,outputs):\n        y = torch.cat([x['y'] for x in outputs])\n        y_hat = torch.cat([x['y_hat'] for x in outputs])\n        \n        return confusion_matrix(y_hat.cpu().detach(), y.cpu().detach(), num_classes=19)\n    \n    \n\n\n    def plot_confusion_matrix(self,cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n        if normalize:\n            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n            print(\"Normalized confusion matrix\")\n        else:\n            print('Confusion matrix, without normalization')\n\n        print(cm)\n        plt.imshow(cm, interpolation='nearest', cmap=cmap)\n        plt.title(title)\n        plt.colorbar()\n        tick_marks = np.arange(len(classes))\n        plt.xticks(tick_marks, classes, rotation=45)\n        plt.yticks(tick_marks, classes)\n\n        fmt = '.2f'\n        thresh = cm.max() / 2.\n#         for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n#             plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n        \n\n        plt.figure(figsize=(12, 12))\n        #plt.tight_layout()\n        plt.ylabel('True label')\n        plt.xlabel('Predicted label')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model\n\nfrom pytorch_lightning.callbacks import ModelCheckpoint\n\ncheckpoint_callback = ModelCheckpoint(\nmonitor='val_acc',\ndirpath='./',\nsave_top_k=3,\nmode='max',\n)\n\nsplits = 5\nbatch_size = 16\nnums_epoch = 3;\nlearning_rate = 3e-4\nvalidation_index = 3\n\ndataModule = HPADataModule(\"../input/equi-dis-all-label/equal_dis_all_label.csv\",splits,validation_index,batch_size,\"../input/hpa-single-cell-image-classification/train/\")\n\nmodel = EffNet(learning_rate)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = pl.Trainer(resume_from_checkpoint='../input/fresh-epoch-1/epoch1-step7451.ckpt',gpus = 1, max_epochs = nums_epoch,callbacks=[checkpoint_callback]) #resume_from_checkpoint='../input/training-experiment-hpa-cell-segmenter/epoch=7-step=34139.ckpt'\ntrainer.fit(model,dataModule)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import pandas as pd\n# data_frame = pd.read_csv('../input/overall-data/Overall_customData.csv')\n# print(data_frame.label.value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_trimmed = data_frame[data_frame.label != 0]\n# df_trimmed = df_trimmed[df_trimmed.label != 14]\n# df_trimmed = df_trimmed[df_trimmed.label != 16]\n# df_trimmed = df_trimmed[df_trimmed.label != 4]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(df_trimmed.label.value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_trimmed.to_csv('../input/overall-data/equal_dis.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}