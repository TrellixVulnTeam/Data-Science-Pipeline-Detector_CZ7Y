{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os, cv2 \nfrom multiprocessing import Pool\nimport threading","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We first resize each single image to 512x512 :\n\nhttps://www.kaggle.com/tchaye59/hpa512x512dataset\n\nhttps://www.kaggle.com/tchaye59/hpa-512x512-dataset"},{"metadata":{},"cell_type":"markdown","source":"# This notebook creates a new dataset by combining the four channels in a single file\n\nThe directory /kaggle/working is only disk limited to 20GB because everything there is stored forever on when you hit Save > Run All.\n\n[We can have much more disk space outside this directory ](https://www.kaggle.com/product-feedback/155538#872885)(ex. create a directory /kaggle/tmp, you can write many GBs there and it won't count towards your disk usage, but also won't be saved on commit)."},{"metadata":{"trusted":true},"cell_type":"code","source":"! mkdir -p /root/.kaggle/\n! cp ../input/api-token/kaggle.json /root/.kaggle/kaggle.json","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! mkdir -p /kaggle/tmp/hpa_stacked\n!kaggle datasets init -p /kaggle/tmp/hpa_stacked","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%%bash\necho \"{\n  \\\"title\\\": \\\"HPA: Stacked channels 512x512\\\",\n  \\\"id\\\": \\\"tchaye59/HPASTACKEDCHANNELS512x512\\\",\n  \\\"licenses\\\": [\n    {\n      \\\"name\\\": \\\"CC0-1.0\\\"\n    }\n  ]\n}\" > /kaggle/tmp/hpa_stacked/dataset-metadata.json","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_DIR = \"../input/hpa-single-cell-image-classification/\"\nTRAIN_DIR = os.path.join('../input/hpa512x512dataset',\"train/\")\nTEST_DIR = os.path.join('../input/hpa512x512dataset',\"test/\")\nTRAIN_CSV = '../input/hpa512x512dataset/train.csv'\nTEST_CSV = os.path.join(DATA_DIR,\"sample_submission.csv\")\n\nDS_DIR = '/kaggle/tmp/hpa_stacked/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(TRAIN_CSV)\ntest_df = pd.read_csv(TEST_CSV)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Runs a process in a thread\nclass Worker(threading.Thread):\n    \n    def __init__(self, process,args,pbar=None):\n        super().__init__()\n        self.pbar = pbar\n        self.process = process\n        self.args = args\n\n    def run(self):\n        res = self.process(self.args)\n        if self.pbar:\n            self.pbar.update(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def worker_fn(args):\n    id_,src_path,dest_path = args\n    red_image = cv2.imread(src_path+id_+\"_red.png\", cv2.IMREAD_UNCHANGED)\n    green_image = cv2.imread(src_path+id_+\"_green.png\", cv2.IMREAD_UNCHANGED)\n    blue_image = cv2.imread(src_path+id_+\"_blue.png\", cv2.IMREAD_UNCHANGED)\n    yellow_image = cv2.imread(src_path+id_+\"_yellow.png\", cv2.IMREAD_UNCHANGED)\n    stacked_images = np.transpose(np.array([red_image, green_image, blue_image, yellow_image]), (1,2,0))\n    #stacked_images=stacked_images.astype(np.uint8) \n    \n    np.savez_compressed(dest_path+id_+'.npz',image=stacked_images)\n    \n    #np.save(dest_path+id_+'.npy',stacked_images,allow_pickle=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build Stacked images"},{"metadata":{},"cell_type":"markdown","source":"Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"src_dir = TRAIN_DIR\ndest_dir = DS_DIR+'train/'\nos.makedirs(dest_dir,exist_ok=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"size = len(train_df.ID)\npbar = tqdm(total=size)\nn = 5000\nfor i in range(0,size,n):\n    workers = []\n    for id_ in train_df.ID[i:min(size,i+n)]:\n        worker =  Worker(worker_fn,(id_,src_dir,dest_dir),pbar=pbar)\n        worker.start()\n        workers.append(worker)\n    for worker in workers:\n        worker.join()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"testing"},{"metadata":{"trusted":true},"cell_type":"code","source":"src_dir = TEST_DIR\ndest_dir = DS_DIR+'test/'\nos.makedirs(dest_dir,exist_ok=True)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"size = len(train_df.ID)\npbar = tqdm(total=size)\nn = 5000\nfor i in range(0,size,n):\n    workers = []\n    for id_ in test_df.ID[i:min(size,i+n)]:\n        worker =  Worker(worker_fn,(id_,src_dir,dest_dir),pbar=pbar)\n        worker.start()\n        workers.append(worker)\n    for worker in workers:\n        worker.join()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let read one image"},{"metadata":{"trusted":true},"cell_type":"code","source":"def readImage(id_):\n  imgPath = f'{DS_DIR}train/{id_}.npz'\n  return np.load(imgPath)['image']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img = readImage(train_df.ID[0])\nimg.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Only display the first 3 channels\nplt.imshow(img[:,:,:3])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"! kaggle datasets version -p /kaggle/tmp/hpa_stacked -m \"Update\" --dir-mode tar\n#! kaggle datasets create -p /kaggle/tmp/hpa_stacked -u --dir-mode tar","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! rm -rf /root/.kaggle/kaggle.json","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dataset link : https://www.kaggle.com/tchaye59/hpastackedchannels512x512"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}