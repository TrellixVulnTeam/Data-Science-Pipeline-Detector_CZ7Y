{"cells":[{"metadata":{"papermill":{"duration":0.045074,"end_time":"2021-01-13T22:39:03.980489","exception":false,"start_time":"2021-01-13T22:39:03.935415","status":"completed"},"tags":[]},"cell_type":"markdown","source":"<h1 style=\"text-align: center; font-family: Verdana; font-size: 32px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; font-variant: small-caps; letter-spacing: 3px; color: #74d5dd; background-color: #ffffff;\">Human Protein Atlas - Single Cell Classification</h1>\n<h2 style=\"text-align: center; font-family: Verdana; font-size: 18px; font-style: normal; font-weight: bold; text-decoration: underline; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\">Showing You How To Create TFRecords for the Original Slide Dataset</h2>\n<h5 style=\"text-align: center; font-family: Verdana; font-size: 12px; font-style: normal; font-weight: bold; text-decoration: None; text-transform: none; letter-spacing: 1px; color: black; background-color: #ffffff;\">CREATED BY: DARIEN SCHETTLER</h5>\n"},{"metadata":{"papermill":{"duration":0.042275,"end_time":"2021-01-13T22:39:04.065992","exception":false,"start_time":"2021-01-13T22:39:04.023717","status":"completed"},"tags":[]},"cell_type":"markdown","source":"<h2 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: navy; background-color: #ffffff;\">TABLE OF CONTENTS</h2>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#imports\">0&nbsp;&nbsp;&nbsp;&nbsp;IMPORTS</a></h3>\n\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#setup\">1&nbsp;&nbsp;&nbsp;&nbsp;SETUP</a></h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#helper_functions\">2&nbsp;&nbsp;&nbsp;&nbsp;HELPER FUNCTIONS</a></h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#dataset\">3&nbsp;&nbsp;&nbsp;&nbsp;DATASET PREPERATION</a></h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#create_tfrec\">4&nbsp;&nbsp;&nbsp;&nbsp;HOW TO CREATE TFRECORDS</a></h3>\n\n"},{"metadata":{"papermill":{"duration":0.043617,"end_time":"2021-01-13T22:39:04.153416","exception":false,"start_time":"2021-01-13T22:39:04.109799","status":"completed"},"tags":[]},"cell_type":"markdown","source":"<a style=\"text-align: font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #ffffff; color: navy;\" id=\"imports\">0&nbsp;&nbsp;IMPORTS</a>"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2021-01-13T22:39:04.243237Z","iopub.status.busy":"2021-01-13T22:39:04.242538Z","iopub.status.idle":"2021-01-13T22:39:39.252127Z","shell.execute_reply":"2021-01-13T22:39:39.252932Z"},"papermill":{"duration":35.05654,"end_time":"2021-01-13T22:39:39.253146","exception":false,"start_time":"2021-01-13T22:39:04.196606","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"print(\"\\n... IMPORTS STARTING ...\\n\")\nprint(\"\\n\\tVERSION INFORMATION\")\n\n# Machine Learning and Data Science Imports\nimport tensorflow_addons as tfa; print(f\"\\t\\t– TENSORFLOW ADDONS VERSION: {tfa.__version__}\");\nimport tensorflow as tf; print(f\"\\t\\t– TENSORFLOW VERSION: {tf.__version__}\");\nimport pandas as pd; pd.options.mode.chained_assignment = None;\nimport numpy as np; print(f\"\\t\\t– NUMPY VERSION: {np.__version__}\");\nimport scipy; print(f\"\\t\\t– SCIPY VERSION: {scipy.__version__}\");\n\n# Built In Imports\nfrom kaggle_datasets import KaggleDatasets\nfrom collections import Counter\nfrom datetime import datetime\nimport multiprocessing\nfrom glob import glob\nimport warnings\nimport requests\nimport imageio\nimport IPython\nimport urllib\nimport zipfile\nimport pickle\nimport random\nimport shutil\nimport string\nimport math\nimport tqdm\nimport time\nimport gzip\nimport io\nimport os\nimport gc\nimport re\n\n# Visualization Imports\nfrom matplotlib.colors import ListedColormap\nimport matplotlib.patches as patches\nimport plotly.graph_objects as go\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nimport plotly.express as px\nimport seaborn as sns\nfrom PIL import Image\nimport matplotlib; print(f\"\\t\\t– MATPLOTLIB VERSION: {matplotlib.__version__}\");\nimport plotly\nimport PIL\nimport cv2\n\n# PRESETS\nLBL_NAMES = [\"Nucleoplasm\", \"Nuclear Membrane\", \"Nucleoli\", \"Nucleoli Fibrillar Center\", \"Nuclear Speckles\", \"Nuclear Bodies\", \"Endoplasmic Reticulum\", \"Golgi Apparatus\", \"Intermediate Filaments\", \"Actin Filaments\", \"Microtubules\", \"Mitotic Spindle\", \"Centrosome\", \"Plasma Membrane\", \"Mitochondria\", \"Aggresome\", \"Cytosol\", \"Vesicles\", \"Negative\"]\nINT_2_STR = {x:LBL_NAMES[x] for x in np.arange(19)}\nINT_2_STR_LOWER = {k:v.lower().replace(\" \", \"_\") for k,v in INT_2_STR.items()}\nSTR_2_INT_LOWER = {v:k for k,v in INT_2_STR_LOWER.items()}\nSTR_2_INT = {v:k for k,v in INT_2_STR.items()}\nFIG_FONT = dict(family=\"Helvetica, Arial\", size=14, color=\"#7f7f7f\")\nLABEL_COLORS = [px.colors.label_rgb(px.colors.convert_to_RGB_255(x)) for x in sns.color_palette(\"Spectral\", len(LBL_NAMES))]\nLABEL_COL_MAP = {str(i):x for i,x in enumerate(LABEL_COLORS)}\n\nprint(\"\\n\\n... IMPORTS COMPLETE ...\\n\")","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.042459,"end_time":"2021-01-13T22:39:39.678624","exception":false,"start_time":"2021-01-13T22:39:39.636165","status":"completed"},"tags":[]},"cell_type":"markdown","source":"<a style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: navy; background-color: #ffffff;\" id=\"setup\">1&nbsp;&nbsp;NOTEBOOK SETUP</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define the root data directory\nDATA_DIR = \"/kaggle/input/hpa-single-cell-image-classification\"\n\n# Define the paths to the training and testing tfrecord and image folders respectively\nTRAIN_TFREC_DIR = os.path.join(DATA_DIR, \"train_tfrecords\")\nTEST_TFREC_DIR = os.path.join(DATA_DIR, \"test_tfrecords\")\n\n# Capture all the relevant full tfrec paths\nTRAIN_TFREC_PATHS = sorted(tf.io.gfile.glob(os.path.join(TRAIN_TFREC_DIR, '*.tfrec')))\nTEST_TFREC_PATHS = sorted(tf.io.gfile.glob(os.path.join(TEST_TFREC_DIR, '*.tfrec')))\nprint(f\"\\n... The number of training tfrecord files is {len(TRAIN_TFREC_PATHS)} ...\")\nprint(f\"... The number of testing tfrecord files is {len(TEST_TFREC_PATHS)} ...\\n\")\n\n# Define paths to the relevant csv files\nTRAIN_CSV = os.path.join(DATA_DIR, \"train.csv\")\nSS_CSV = os.path.join(DATA_DIR, \"sample_submission.csv\")\n\n# Create the relevant dataframe objects\ntrain_df = pd.read_csv(TRAIN_CSV)\nss_df = pd.read_csv(SS_CSV)\n\nprint(\"\\n\\nTRAIN DATAFRAME\\n\\n\")\ndisplay(train_df.head(3))\n\nprint(\"\\n\\nSAMPLE SUBMISSION DATAFRAME\\n\\n\")\ndisplay(ss_df.head(3))","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.043842,"end_time":"2021-01-13T22:39:40.85577","exception":false,"start_time":"2021-01-13T22:39:40.811928","status":"completed"},"tags":[]},"cell_type":"markdown","source":"<a style=\"text-align: font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: navy; background-color: #ffffff;\" id=\"helper_functions\">2&nbsp;&nbsp;HELPER FUNCTIONS</a>"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.execute_input":"2021-01-13T22:39:40.980411Z","iopub.status.busy":"2021-01-13T22:39:40.979421Z","iopub.status.idle":"2021-01-13T22:39:40.982404Z","shell.execute_reply":"2021-01-13T22:39:40.981881Z"},"papermill":{"duration":0.080032,"end_time":"2021-01-13T22:39:40.982523","exception":false,"start_time":"2021-01-13T22:39:40.902491","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def load_image_from_channel_paths(channel_paths):\n    rgby = [np.asarray(Image.open(path), np.uint8) for path in channel_paths]\n    return np.stack(rgby, axis=-1)\n\n    \ndef convert_rgby_to_rgb(arr):\n    \"\"\" Convert a 4 channel (RGBY) image to a 3 channel RGB image.\n    \n    Advice From Competition Host/User: lnhtrang\n\n    For annotation (by experts) and for the model, I guess we agree that individual \n    channels with full range px values are better. \n    In annotation, we toggled the channels. \n    For visualization purpose only, you can try blending the channels. \n    For example, \n        - red = red + yellow\n        - green = green + yellow/2\n        - blue=blue.\n        \n    Args:\n        arr (numpy array): The RGBY, 4 channel numpy array for a given image\n    \n    Returns:\n        RGB Image\n    \"\"\"\n    \n    rgb_arr = np.zeros_like(arr[..., :-1])\n    rgb_arr[..., 0] = arr[..., 0]\n    rgb_arr[..., 1] = arr[..., 1]+arr[..., 3]/2\n    rgb_arr[..., 2] = arr[..., 2]\n    \n    return rgb_arr\n\n\ndef plot_ex(arr, figsize=(20,6), title=None, plot_merged=True, rgb_only=False):\n    \"\"\" Plot 4 Channels Side by Side \"\"\"\n    if plot_merged and not rgb_only:\n        n_images=5 \n    elif plot_merged and rgb_only:\n        n_images=4\n    elif not plot_merged and rgb_only:\n        n_images=4\n    else:\n        n_images=3\n    plt.figure(figsize=figsize)\n    if type(title) == str:\n        plt.suptitle(title, fontsize=20, fontweight=\"bold\")\n\n    for i, c in enumerate([\"Red Channel – Microtubles\", \"Green Channel – Protein of Interest\", \"Blue - Nucleus\", \"Yellow – Endoplasmic Reticulum\"]):\n        if not rgb_only:\n            ch_arr = np.zeros_like(arr[..., :-1])        \n        else:\n            ch_arr = np.zeros_like(arr)\n        if c in [\"Red Channel – Microtubles\", \"Green Channel – Protein of Interest\", \"Blue - Nucleus\"]:\n            ch_arr[..., i] = arr[..., i]\n        else:\n            if rgb_only:\n                continue\n            ch_arr[..., 0] = arr[..., i]\n            ch_arr[..., 1] = arr[..., i]\n        plt.subplot(1,n_images,i+1)\n        plt.title(f\"{c.title()}\", fontweight=\"bold\")\n        plt.imshow(ch_arr)\n        plt.axis(False)\n        \n    if plot_merged:\n        plt.subplot(1,n_images,n_images)\n        \n        if rgb_only:\n            plt.title(f\"Merged RGB\", fontweight=\"bold\")\n            plt.imshow(arr)\n        else:\n            plt.title(f\"Merged RGBY into RGB\", fontweight=\"bold\")\n            plt.imshow(convert_rgby_to_rgb(arr))\n        plt.axis(False)\n        \n    plt.tight_layout(rect=[0, 0.2, 1, 0.97])\n    plt.show()\n    \n    \ndef flatten_list_of_lists(l_o_l):\n    return [item for sublist in l_o_l for item in sublist]\n\n\ndef decode_image(image_data, n_channels=1, resize_to=(512,512), cast_to=tf.uint8):\n    image = tf.image.decode_png(image_data, channels=n_channels)    \n    image = tf.image.resize(image, resize_to) \n    return tf.cast(image, cast_to)\n\n\ndef str_2_multi_hot_encoding(tfstring, n_classes=19):\n    ragged_indices = tf.strings.to_number(tf.strings.split(tfstring, sep=\"|\"), out_type=tf.int32)\n    one_hot_stack = tf.one_hot(ragged_indices, depth=n_classes)\n    return tf.reduce_max(one_hot_stack, axis=-2)\n\n\ndef decode(serialized_example, multihot=False, n_channels=1, resize_to=(512,512)):\n    \"\"\" Parses a set of features and label from the given `serialized_example`.\n        \n        It is used as a map function for `dataset.map`\n\n    Args:\n        serialized_example (tf.Example): A serialized example\n        is_test (bool, optional): Whether to allow for the label feature\n        \n    Returns:\n        A decoded tf.data.Dataset object representing the tfrecord dataset\n    \"\"\"\n    # Defaults are not specified since both keys are required.\n    feature_dict = {\n        'image': tf.io.FixedLenFeature(shape=(), dtype=tf.string),\n        'image_name': tf.io.FixedLenFeature(shape=(), dtype=tf.string),\n        'target': tf.io.FixedLenFeature(shape=(), dtype=tf.string),\n    }\n    # Define a parser\n    features = tf.io.parse_single_example(serialized_example, features=feature_dict)   \n    image = decode_image(features['image'], n_channels, resize_to)\n    image_name = features[\"image_name\"]\n    if multihot:\n        label = str_2_multi_hot_encoding(features[\"target\"])\n    else:\n        label = features[\"target\"]\n    return image, image_name, label\n\n\ndef preprocess_tfrec_ds(red, green, blue, yellow, drop_yellow=True, return_id=True):\n    (ri, rn, rl), (gi, gn, gl), (bi, bn, bl), (yi, yn, yl) = red, green, blue, yellow\n    if drop_yellow:\n        combo_img = tf.stack([ri[..., 0], gi[..., 0], bi[..., 0]], axis=-1)\n    else:\n        combo_img = tf.stack([ri[..., 0], gi[..., 0], bi[..., 0], yi[..., 0]], axis=-1)\n    \n    if return_id:\n        img_id = tf.strings.substr(rn, pos=0, len=36) # 36 is length of id (always)\n        return combo_img, img_id, rl\n    else:\n        return combo_img, rl","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a style=\"text-align: font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: navy; background-color: #ffffff;\" id=\"dataset\">3&nbsp;&nbsp;DATASET PREPERATION</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# num_parallel_reads=None forces the order to be preserved\nraw_train_ds = tf.data.TFRecordDataset(TRAIN_TFREC_PATHS, num_parallel_reads=None)\n\n# See an example\nfor raw in raw_train_ds.take(1):\n    example = tf.train.Example()\n    example.ParseFromString(raw.numpy())\n    for k,v in example.features.feature.items():\n        print(k)\n        if k!=\"image\":\n            print(f\"\\t--> {v.bytes_list.value[0]}\")\n        else:\n            print(f\"\\t-->{str(v.bytes_list.value[0][:25])+' ... '}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_ds = raw_train_ds.map(decode)\n\n# See examples\nprint(\"\\n ... NOTICE THE IMAGES ARE CLUMPED TOGETHER BY CHANNEL FOR A GIVEN ID ...\\n\")\nfor i, (img, image_name, lbl) in enumerate(train_ds.take(4)):\n    print(f\"IMAGE SHAPE : {img.shape}\")\n    print(f\"IMG #{i//4} -- IMAGE NAME  : {image_name.numpy().decode()}\")\n    print(f\"IMAGE LABEL : {lbl}\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#####################################################\n# ANNOYINGLY THIS DOES NOT WORK AS THE QUADRUPLE OF #\n# CHANNEL IMAGES IS IN A DIFFERENT ORDER EVERY TIME #\n#####################################################\n# red_train_ds = train_ds.shard(4, index=3)\n# green_train_ds = train_ds.shard(4, index=2)\n# blue_train_ds = train_ds.shard(4, index=0)\n# yellow_train_ds = train_ds.shard(4, index=1)\n#####################################################\n\nred_train_ds = train_ds.filter(lambda x,y,z: tf.strings.regex_full_match(y, \".*red.*\"))\ngreen_train_ds = train_ds.filter(lambda x,y,z: tf.strings.regex_full_match(y, \".*green.*\"))\nblue_train_ds = train_ds.filter(lambda x,y,z: tf.strings.regex_full_match(y, \".*blue.*\"))\nyellow_train_ds = train_ds.filter(lambda x,y,z: tf.strings.regex_full_match(y, \".*yellow.*\"))\n\n\nprint(\"\\n ... NOTICE THE IMAGES ARE NOW IN THEIR OWN DATASET BY COLOR ...\\n\")\nfor (img_r, image_name_r, lbl_r), (img_b, image_name_b, lbl_b) in zip(red_train_ds.take(4), blue_train_ds.take(4)):\n    print(f\"IMAGE SHAPE : R={img_r.shape} - B={img_b.shape}\")\n    print(f\"IMAGE NAME  : R={image_name_r.numpy().decode()} - B={image_name_b.numpy().decode()}\")\n    print(f\"IMAGE LABEL : R={lbl_r.numpy().decode()} - B={lbl_b.numpy().decode()}\")\n    print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_ds = tf.data.Dataset.zip((red_train_ds, green_train_ds, blue_train_ds, yellow_train_ds)).map(preprocess_tfrec_ds)\n\nprint(\"\\n\\t\\t... TRAIN EXAMPLES ...\\n\")\nfor x,y,z in train_ds.take(3):\n    plot_ex(x.numpy(), title=f\"ID = {y.numpy().decode()}\\nLABELS = {', '.join([INT_2_STR[int(lbl)] for lbl in z.numpy().decode().split('|')])}\", rgb_only=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a style=\"text-align: font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: navy; background-color: #ffffff;\" id=\"create_tfrec\">4&nbsp;&nbsp;HOW TO RECREATE THE TFRECORDS</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def _bytes_feature(value, is_list=False):\n    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n    if isinstance(value, type(tf.constant(0))):\n        value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n    \n    if not is_list:\n        value = [value]\n    \n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\n\ndef _float_feature(value, is_list=False):\n    \"\"\"Returns a float_list from a float / double.\"\"\"\n        \n    if not is_list:\n        value = [value]\n        \n    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n\ndef _int64_feature(value, is_list=False):\n    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n        \n    if not is_list:\n        value = [value]\n        \n    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n\n\ndef serialize(image, image_name, target):\n    \"\"\"\n    Creates a tf.Example message ready to be written to a file from 4 features.\n\n    Args:\n        image (TBD): TBD\n        image_name (str): TBD\n        target (str): | delimited integers\n    \n    Returns:\n        A tf.Example Message ready to be written to file\n    \"\"\"\n    \n    # Create a dictionary mapping the feature name to the \n    # tf.Example-compatible data type.\n    feature = {\n        'image': _bytes_feature(tf.io.encode_png(image), is_list=False),\n        'image_name': _bytes_feature(image_name, is_list=False),\n        'target': _bytes_feature(target, is_list=False),\n    }\n\n    # Create a Features message using tf.train.Example.\n    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n    return example_proto.SerializeToString()\n\ndef write_tfrecords(ds_np, n_recs=64, n_ex_per_rec=341, out_dir=\"/kaggle/working/train_records\"):\n    if not os.path.isdir(out_dir):\n        os.makedirs(out_dir, exist_ok=True)\n    for i in tqdm(range(n_recs), total=n_recs):\n        print(f\"\\n... Writing TFRecord {i+1} of {n_recs} ...\\n\")\n        tfrec_path = os.path.join(out_dir, f\"{out_dir.rsplit('_', 1)[1]}_{(i+1):02}_{n_recs:02}.tfrec\")\n        with tf.io.TFRecordWriter(tfrec_path) as writer:\n            for j in tqdm(range(n_ex_per_rec), total=n_ex_per_rec):\n                try:\n                    example = serialize(*next(ds_np))\n                    writer.write(example)\n                except:\n                    break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"N_TRAIN_RECS = 64\nN_EX = len(train_df)\nN_EX_PER_REC = int(np.ceil(N_EX/64))\nwrite_tfrecords(train_ds.as_numpy_iterator(), out_dir=\"/kaggle/working/train_slide_records\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NEW_TRAIN_TFREC_PATHS = [\n    os.path.join(\"/kaggle/working/train_slide_records\", f_name) \\\n    for f_name in os.listdir(\"/kaggle/working/train_slide_records\")\n]\nnew_raw_train_ds = tf.data.TFRecordDataset(NEW_TRAIN_TFREC_PATHS, num_parallel_reads=None)\nnew_train_ds = new_raw_train_ds.map(lambda x: decode(x, n_channels=3))\n\n# See examples\nprint(\"\\n ... NOTICE THE IMAGES ARE CLUMPED TOGETHER BY CHANNEL FOR A GIVEN ID ...\\n\")\nfor i, (img, image_name, lbl) in enumerate(new_train_ds.take(3)):\n    print(f\"IMAGE SHAPE : {img.shape}\")\n    print(f\"IMG #{i//4} -- IMAGE NAME  : {image_name.numpy().decode()}\")\n    print(f\"IMAGE LABEL : {lbl}\\n\")\n    plt.imshow(img.numpy().astype(np.uint8))\n    plt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}