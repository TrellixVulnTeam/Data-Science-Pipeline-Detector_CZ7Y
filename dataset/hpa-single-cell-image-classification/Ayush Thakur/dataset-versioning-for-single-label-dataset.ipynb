{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"%%capture\n!pip install wandb --upgrade","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport re\nimport cv2\nimport glob\nimport imageio\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nfrom skimage.transform import resize\nfrom sklearn.model_selection import train_test_split\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import wandb\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\nwandb_api = user_secrets.get_secret(\"wandb_api\")\n\nwandb.login(key=wandb_api)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"WORKING_DIR_PATH = '../input/hpa-single-cell-image-classification/'\nIMAGE_HEIGHT = 128\nIMAGE_WIDTH = 128","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ref: https://www.kaggle.com/divyanshuusingh/eda-image-segmentation\nlabel_names= {\n0: \"Nucleoplasm\",\n1: \"Nuclear membrane\",\n2: \"Nucleoli\",\n3: \"Nucleoli fibrillar center\",\n4: \"Nuclear speckles\",\n5: \"Nuclear bodies\",\n6: \"Endoplasmic reticulum\",\n7: \"Golgi apparatus\",\n8: \"Intermediate filaments\",\n9: \"Actin filaments\",\n10: \"Microtubules\",\n11: \"Mitotic spindle\",\n12: \"Centrosome\",\n13: \"Plasma membrane\",\n14: \"Mitochondria\",\n15: \"Aggresome\",\n16: \"Cytosol\",\n17: \"Vesicles and punctate cytosolic patterns\",\n18: \"Negative\"\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(WORKING_DIR_PATH+'train.csv')\n# Add a column - num_classes\ndf['num_classes'] = df['Label'].apply(lambda r: len(r.split('|')))\nprint(f'Total number of images: {len(df)}')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df['num_classes'].value_counts())\ndf['num_classes'].value_counts().plot.bar(title='Examples with multiple labels', xlabel='number of labels per example', ylabel='# train examples')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* First going to create single label dataset for images which have one image-level labels. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df_one_label = df.loc[df['num_classes'] == 1]\nprint(f'Number of images with one image-level labels: {len(df_one_label)}')\ndf_one_label.Label = df_one_label.Label.astype('int64')\ndf_one_label.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save as Artifacts\n\ndf_one_label.to_csv('train_single.csv', index=True)\n\nrun = wandb.init(entity='ayush-thakur', project='hpa', job_type='single_label_dataset')\nartifact_raw = run.use_artifact('ayush-thakur/hpa/raw:v0', type='dataset')\n\nartifact = wandb.Artifact('single_label', type='dataset')\nartifact.add_file('train_single.csv')\nrun.log_artifact(artifact)\nrun.join()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels, counts = np.unique(df_one_label.Label.values, return_counts=True)\nprint(f'The unique labels are: {labels} and there values are: {counts}')\n\nplt.figure(figsize=(15,5))\nplt.bar(labels, counts)\n\nfor index, value in enumerate(counts):\n    plt.text(index-0.25, value, str(value), fontdict=dict(fontsize=10))\n\nplt.xticks(np.arange(len(labels)), labels=label_names.values(), rotation=85)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Since it's going to take more than 9 hours to create this dataset and the Kernel capacity is 9 hrs currently, I am splitting the `df_one_label.csv` into four `csv` files. Each will have roughly 2500 image ids. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df_splits = np.array_split(df_one_label, 4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(df_one_label), len(df_splits[0]), len(df_splits[1]), len(df_splits[2]), len(df_splits[3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, df_split in enumerate(df_splits):\n    df_split.to_csv(f'train_single_{i}.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save as Artifacts\nrun = wandb.init(entity='ayush-thakur', project='hpa', job_type='single_label_dataset_split')\nartifact_single_label = run.use_artifact('ayush-thakur/hpa/single_label:v1', type='dataset')\n\nartifact = wandb.Artifact('single_label_split', type='dataset')\n\nfor i in range(4):\n    artifact.add_file(f'train_single_{i}.csv')\n    \nrun.log_artifact(artifact)\nrun.join()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}