{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# #Loading packages and data"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport tensorflow as tf\nsns.set()\nimport os\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# #Load dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels = pd.read_csv(\"../input/hpa-single-cell-image-classification/train.csv\")\ntrain_labels.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# #How many samples do we have"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# #Extract test names for submission\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_path = \"../input/human-protein-atlas-image-classification/test/\"\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv(\"../input/hpa-single-cell-image-classification/sample_submission.csv\")\nsubmission.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_names = submission.ID.values\nprint(len(test_names))\nprint(len(test_names[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# #There are 559 test images we are asked to make predictions."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# #Helper Code"},{"metadata":{"trusted":true},"cell_type":"code","source":"label_names = {\n    0: \"Nucleoplasm\",\n    1: \"Nuclear_membrane\",\n    2: \"Nucleoli\",\n    3: \"Nucleoli_fibrillar_center\",\n    4: \"Nuclear_speckles\",\n    5: \"Nuclear_bodies\",\n    6: \"Endoplasmic_reticulum\",\n    7: \"Golgi_apparatus\",\n    8: \"Intermediate_filaments\",\n    9: \"Actin filaments\",\n    10: \"Microtubules\",\n    11: \"Mitotic_spindle\",\n    12: \"Centrosome\",\n    13: \"Plasma_membrane\",\n    14: \"Mitochondria\",\n    15: \"Aggresome\",\n    16: \"Cytosol\",\n    17: \"Vesicles_and_punctate_cytosolic_patterns\",\n    18: \"Negative\"\n}\n\nreverse_train_labels = dict((v,k) for k,v in label_names.items())\n\ndef fill_targets(row):   \n    row.Label = np.array(row.Label.split(\"|\")).astype(np.int)\n    for num in row.Label:\n        name = label_names[int(num)]\n        row.loc[name] = 1\n    return row","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for key in label_names.keys():\n    train_labels[label_names[key]] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels = train_labels.apply(fill_targets, axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels.head(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ok, great now we can directly work with binary targets values, Lets create a dataframe for the test ids as well that we will use later to make our submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_labels = pd.DataFrame(data=test_names, columns=['ID'])\nfor col in train_labels.columns.values:\n    if col != \"ID\":\n        test_labels[col] = 0\ntest_labels.head(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ok, currently we haven't made any predictions and except from Id all entries are filled with 0.\n"},{"metadata":{},"cell_type":"markdown","source":"# Exploratory data analysis \n"},{"metadata":{},"cell_type":"markdown","source":"# Which protiens occur most often in train images"},{"metadata":{"trusted":true},"cell_type":"code","source":"target_count = train_labels.drop(['ID', 'Label'], axis = 1).sum(axis=0).sort_values(ascending=False)\nplt.figure(figsize=(15,15))\nsns.barplot(y=target_count.index.values, x = target_count.values, order=target_count.index)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Take-Away**\n* We can see that most common protein structures belong to coarse grained cellular components like the plasma membrane, the cytosol and the nucleus.\n\n* Consequently accuracy is not the right score here to measure your performance and validation strategy should be very fine."},{"metadata":{},"cell_type":"markdown","source":"# How many targets are most common?"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels['number_of_targets'] = train_labels.drop([\"ID\",\"Label\"], axis = 1).sum(axis = 1)\n\ncount_prec = np.round(100*train_labels['number_of_targets'].value_counts()/train_labels.shape[0], 2)\nplt.figure(figsize=(20,5))\nsns.barplot(x= count_prec.index.values, y = count_prec.values, palette = \"Reds\")\nplt.xlabel(\"Number of targets per image\")\nplt.ylabel(\"% of train data\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Take-away**\n* Most train images only have 1 or two target labels.\n* More than 3 targets are very seldom!"},{"metadata":{},"cell_type":"markdown","source":"# Which targets are correlated?"},{"metadata":{},"cell_type":"markdown","source":"Let's see if we find some correlations between our targets. This way we may already see that some proteins often come together."},{"metadata":{"trusted":true},"cell_type":"code","source":"\nplt.figure(figsize=(15,15))\nsns.heatmap(train_labels[train_labels.number_of_targets>1].drop(\n    [\"ID\", \"Label\", \"number_of_targets\"],axis=1\n).corr(), cmap=\"RdYlBu\", vmin=-1, vmax=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Take-away**\n* We can see that many targets only have very slight correlations.\n"},{"metadata":{},"cell_type":"markdown","source":"# Next Version comeing up with more details EDA and baseline.\n****\n# Please Upvote"},{"metadata":{},"cell_type":"markdown","source":"Thanks"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"\n# How do the images look like?\n"},{"metadata":{},"cell_type":"markdown","source":"**Peek into the directory**\n\n* Before we start loading images, let's have a look into the train directory to get an impression of what we can find there:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from os import listdir\nfiles = listdir(\"../input/hpa-single-cell-image-classification/train\")\nfor n in range(10):\n    print(files[n])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Ah, ok, great! It seems that for one image id, there are different color channels present. Looking into the data description of this competition we can find that:**\n\n* Each image is actually splitted into 4 different image files.\n* These 4 files correspond to 4 different filter:\n 1.  a green filter for the target protein structure of interest\n 2. blue landmark filter for the nucleus\n 3. red landmark filter for microtubules\n 4. yellow landmark filter for the endoplasmatic reticulum\n* Each image is of size 512 x 512"},{"metadata":{},"cell_type":"markdown","source":"Let's check if the number of files divided by 4 yields the number of target samples"},{"metadata":{"trusted":true},"cell_type":"code","source":"len(files)/4 == train_labels.shape[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# How do images of specific targets looks like.?"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_path = \"../input/hpa-single-cell-image-classification/train/\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_image(basepath, image_id):\n    images = np.zeros(shape=(4,512,512))\n    images[0,:,:] = imread(basepath + image_id + \"_green\" + \".png\")\n    images[1,:,:] = imread(basepath + image_id + \"_red\" + \".png\")\n    images[2,:,:] = imread(basepath + image_id + \"_blue\" + \".png\")\n    images[3,:,:] = imread(basepath + image_id + \"_yellow\" + \".png\")\n    return images\n\ndef make_image_row(image, subax, title):\n    subax[0].imshow(image[0], cmap=\"Greens\")\n    subax[1].imshow(image[1], cmap=\"Reds\")\n    subax[1].set_title(\"stained microtubules\")\n    subax[2].imshow(image[2], cmap=\"Blues\")\n    subax[2].set_title(\"stained nucleus\")\n    subax[3].imshow(image[3], cmap=\"Oranges\")\n    subax[3].set_title(\"stained endoplasmatic reticulum\")\n    subax[0].set_title(title)\n    return subax\n\ndef make_title(file_id):\n    file_targets = train_labels.loc[train_labels.Id==file_id, \"Target\"].values[0]\n    title = \" - \"\n    for n in file_targets:\n        title += label_names[n] + \" - \"\n    return title","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's try to visualize specific target groups. In this example we will see images that contain the protein structures lysosomes or endosomes. Set target values of your choice and the target group iterator will collect all images that are subset of your choice:"},{"metadata":{"trusted":true},"cell_type":"code","source":"class TargetGroupIterator:\n    \n    def __init__(self, target_names, batch_size, basepath):\n        self.target_names = target_names\n        self.target_list = [reverse_train_labels[key] for key in target_names]\n        self.batch_shape = (batch_size, 4, 512, 512)\n        self.basepath = basepath\n    \n    def find_matching_data_entries(self):\n        train_labels[\"check_col\"] = train_labels.Label.apply(\n            lambda l: self.check_subset(l)\n        )\n        self.images_identifier = train_labels[train_labels.check_col==1].ID.values\n        train_labels.drop(\"check_col\", axis=1, inplace=True)\n    \n    def check_subset(self, targets):\n        return np.where(set(targets).issubset(set(self.target_list)), 1, 0)\n    \n    def get_loader(self):\n        filenames = []\n        idx = 0\n        images = np.zeros(self.batch_shape)\n        for image_id in self.images_identifier:\n            images[idx,:,:,:] = load_image(self.basepath, image_id)\n            filenames.append(image_id)\n            idx += 1\n            if idx == self.batch_shape[0]:\n                yield filenames, images\n                filenames = []\n                images = np.zeros(self.batch_shape)\n                idx = 0\n        if idx > 0:\n            yield filenames, images\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nyour_choice = [\"Nucleoplasm\", \"Nuclear_bodies\"]\nyour_batch_size = 20","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imageloader = TargetGroupIterator(your_choice, your_batch_size, train_path)\nimageloader.find_matching_data_entries()\niterator = imageloader.get_loader()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To keep the kernel dense, the target group iterator has a batch size which stands for the number of examples you like to look at once. In this example you can see a maximum amount of 3 images at one iteration. To observe the next 3 examples of your target group, just run the cell below again. This way you can run the cell until you have seen all images of your group without polluting the kernel:"},{"metadata":{},"cell_type":"markdown","source":"# Building a baseline model <a class=\"anchor\" id=\"baseline\"></a>"},{"metadata":{},"cell_type":"markdown","source":"### K-Fold Cross-Validation"},{"metadata":{},"cell_type":"markdown","source":"Let's see how many test and train samples we have in this competition:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_files = \"../input/hpa-single-cell-image-classification/train/\"\ntest_files = \"../input/hpa-single-cell-image-classification/test/\"\npercentage = np.round(len(test_files)/len(train_files)*100)\nprint(\"The test size turn out to be {} % compared to the trainset.\".format(percentage))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To understand the performance of our model we will use k-fold cross validation. The train data is splitted into k chunks and each chunk is used once for testing the prediction performance whereas the others are used for training. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RepeatedKFold\nsplitter = RepeatedKFold(n_splits=5,n_repeats=1, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"partitions = []\n\nfor train_idx,  test_idx in splitter.split(train_labels.index):\n    partition = {}\n    partition[\"train\"] = train_labels.ID.values[train_idx]\n    partition[\"validation\"] = train_labels.ID.values[test_idx]\n    partitions.append(partition)\n    print(\"TRAIN:\", train_idx, \"TEST:\", test_idx)\n    print(\"TRAIN:\", len(train_idx), \"TEST:\", len(test_idx))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next we need to setup a simple baseline model. This need not be very complex or very good. Its our first attempt to play with and to figure out how to improve. For this purpose let's use the deep learning library keras."},{"metadata":{},"cell_type":"markdown","source":"### Shared Parameter class"},{"metadata":{"trusted":true},"cell_type":"code","source":"class ModelParameter:\n    \n    def __init__(self, basepath,\n                 num_classes=28,\n                 image_rows=512,\n                 image_cols=512,\n                 batch_size=200,\n                 n_channels=1,\n                 row_scale_factor=4,\n                 col_scale_factor=4,\n                 shuffle=False,\n                 n_epochs=1):\n        self.basepath = basepath\n        self.num_classes = num_classes\n        self.image_rows = image_rows\n        self.image_cols = image_cols\n        self.batch_size = batch_size\n        self.n_channels = n_channels\n        self.shuffle = shuffle\n        self.row_scale_factor = row_scale_factor\n        self.col_scale_factor = col_scale_factor\n        self.scaled_row_dim = np.int(self.image_rows / self.row_scale_factor)\n        self.scaled_col_dim = np.int(self.image_cols / self.col_scale_factor)\n        self.n_epochs = n_epochs","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ok, now we will create an instance of this class and pass it to the DataGenerator, the BaseLineModel and the ImagePreprocessor."},{"metadata":{"trusted":true},"cell_type":"code","source":"parameters = ModelParameter(train_path)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Image Preprocessor\n\nLet's write a simple image preprocessor that handles for example the rescaling of the images. Perhaps we can expand its functionality during improvement of the baseline model. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from skimage.transform import resize\n\nclass ImagePreprocessor:\n    \n    def __init__(self, modelparameter):\n        self.parameter = modelparameter\n        self.basepath = self.parameter.basepath\n        self.scaled_row_dim = self.parameter.scaled_row_dim\n        self.scaled_col_dim = self.parameter.scaled_col_dim\n        self.n_channels = self.parameter.n_channels\n    \n    def preprocess(self, image):\n        image = self.resize(image)\n        image = self.reshape(image)\n        image = self.normalize(image)\n        return image\n    \n    def resize(self, image):\n        image = resize(image, (self.scaled_row_dim, self.scaled_col_dim))\n        return image\n    \n    def reshape(self, image):\n        image = np.reshape(image, (image.shape[0], image.shape[1], self.n_channels))\n        return image\n    \n    def normalize(self, image):\n        image /= 255 \n        return image\n    \n    def load_image(self, image_id):\n        image = np.zeros(shape=(512,512,4))\n        image[:,:,0] = imread(self.basepath + image_id + \"_green\" + \".png\")\n        image[:,:,1] = imread(self.basepath + image_id + \"_blue\" + \".png\")\n        image[:,:,2] = imread(self.basepath + image_id + \"_red\" + \".png\")\n        image[:,:,3] = imread(self.basepath + image_id + \"_yellow\" + \".png\")\n        return image[:,:,0:self.parameter.n_channels]\n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's create an instance of this preprocessor and pass it to the data generator."},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocessor = ImagePreprocessor(parameters)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Looking at a preprocessed example image"},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\n\nclass DataGenerator(keras.utils.Sequence):\n    \n    def __init__(self, list_IDs, labels, modelparameter, imagepreprocessor):\n        self.current_epoch = 0\n        self.params = modelparameter\n        self.labels = labels\n        self.list_IDs = list_IDs\n        self.dim = (self.params.scaled_row_dim, self.params.scaled_col_dim)\n        self.batch_size = self.params.batch_size\n        self.n_channels = self.params.n_channels\n        self.num_classes = self.params.num_classes\n        self.shuffle = self.params.shuffle\n        self.preprocessor = imagepreprocessor\n        self.on_epoch_end()\n    \n    def on_epoch_end(self):\n        self.indexes = np.arange(len(self.list_IDs))\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes, random_state=self.current_epoch)\n            self.current_epoch += 1\n    \n    def get_targets_per_image(self, identifier):\n        return self.labels.loc[self.labels.Id==identifier].drop(\n                [\"Id\", \"Target\", \"number_of_targets\"], axis=1).values\n            \n    def __data_generation(self, list_IDs_temp):\n        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n        # Initialization\n        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n        y = np.empty((self.batch_size, self.num_classes), dtype=int)\n        # Generate data\n        for i, identifier in enumerate(list_IDs_temp):\n            # Store sample\n            image = self.preprocessor.load_image(identifier)\n            image = self.preprocessor.preprocess(image)\n            X[i] = image\n            # Store class\n            y[i] = self.get_targets_per_image(identifier)\n        return X, y\n    \n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return int(np.floor(len(self.list_IDs) / self.batch_size))\n    \n    def __getitem__(self, index):\n        'Generate one batch of data'\n        # Generate indexes of the batch\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        # Find list of IDs\n        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n        # Generate data\n        X, y = self.__data_generation(list_IDs_temp)\n        return X, y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class PredictGenerator:\n    \n    def __init__(self, predict_Ids, imagepreprocessor, predict_path):\n        self.preprocessor = imagepreprocessor\n        self.preprocessor.basepath = predict_path\n        self.identifiers = predict_Ids\n    \n    def predict(self, model):\n        y = np.empty(shape=(len(self.identifiers), self.preprocessor.parameter.num_classes))\n        for n in range(len(self.identifiers)):\n            image = self.preprocessor.load_image(self.identifiers[n])\n            image = self.preprocessor.preprocess(image)\n            image = image.reshape((1, *image.shape))\n            y[n] = model.predict(image)\n        return y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.losses import binary_crossentropy\nfrom keras.optimizers import Adadelta\nfrom keras.initializers import VarianceScaling\n\n\nclass BaseLineModel:\n    \n    def __init__(self, modelparameter):\n        self.params = modelparameter\n        self.num_classes = self.params.num_classes\n        self.img_rows = self.params.scaled_row_dim\n        self.img_cols = self.params.scaled_col_dim\n        self.n_channels = self.params.n_channels\n        self.input_shape = (self.img_rows, self.img_cols, self.n_channels)\n        self.my_metrics = ['accuracy']\n    \n    def build_model(self):\n        self.model = Sequential()\n        self.model.add(Conv2D(16, kernel_size=(3, 3), activation='relu', input_shape=self.input_shape,\n                             kernel_initializer=VarianceScaling(seed=0)))\n        self.model.add(Conv2D(32, (3, 3), activation='relu',\n                             kernel_initializer=VarianceScaling(seed=0)))\n        self.model.add(MaxPooling2D(pool_size=(2, 2)))\n        self.model.add(Dropout(0.25))\n        self.model.add(Flatten())\n        self.model.add(Dense(64, activation='relu',\n                            kernel_initializer=VarianceScaling(seed=0),))\n        self.model.add(Dropout(0.5))\n        self.model.add(Dense(self.num_classes, activation='sigmoid'))\n    \n    def compile_model(self):\n        self.model.compile(loss=keras.losses.binary_crossentropy,\n              optimizer=keras.optimizers.Adadelta(),\n              metrics=self.my_metrics)\n    \n    def set_generators(self, train_generator, validation_generator):\n        self.training_generator = train_generator\n        self.validation_generator = validation_generator\n    \n    def learn(self):\n        return self.model.fit_generator(generator=self.training_generator,\n                    validation_data=self.validation_generator,\n                    epochs=self.params.n_epochs, \n                    use_multiprocessing=True,\n                    workers=8)\n    \n    def score(self):\n        return self.model.evaluate_generator(generator=self.validation_generator,\n                                      use_multiprocessing=True, \n                                      workers=8)\n    \n    def predict(self, predict_generator):\n        y = predict_generator.predict(self.model)\n        return y\n    \n    def save(self, modeloutputpath):\n        self.model.save(modeloutputpath)\n    \n    def load(self, modelinputpath):\n        self.model = load_model(modelinputpath)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Datasets\npartition = partitions[0]\nlabels = train_labels\n\nprint(\"Number of samples in train: {}\".format(len(partition[\"train\"])))\nprint(\"Number of samples in validation: {}\".format(len(partition[\"validation\"])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_generator = DataGenerator(partition['train'], labels, parameters, preprocessor)\nvalidation_generator = DataGenerator(partition['validation'], labels, parameters, preprocessor)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_generator = PredictGenerator(partition['validation'], preprocessor, train_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preprocessor = ImagePreprocessor(parameters)\nsubmission_predict_generator = PredictGenerator(test_names, test_preprocessor, test_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class KernelSettings:\n    \n    def __init__(self, fit_baseline=False,\n                 fit_improved_baseline=True,\n                 fit_improved_higher_batchsize=False,\n                 fit_improved_without_dropout=False):\n        self.fit_baseline = fit_baseline\n        self.fit_improved_baseline = fit_improved_baseline\n        self.fit_improved_higher_batchsize = fit_improved_higher_batchsize\n        self.fit_improved_without_dropout = fit_improved_without_dropout","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kernelsettings = KernelSettings(fit_baseline=False,\n                                fit_improved_baseline=False,\n                                fit_improved_higher_batchsize=False,\n                                fit_improved_without_dropout=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Run computation and store results as csv\ntarget_names = train_labels.drop([\"Label\", \"number_of_targets\", \"ID\"], axis=1).columns\n\nif kernelsettings.fit_baseline == True:\n    model = BaseLineModel(parameter)\n    model.build_model()\n    model.compile_model()\n    model.set_generators(training_generator, validation_generator)\n    history = model.learn()\n    \n    proba_predictions = model.predict(predict_generator)\n    baseline_proba_predictions = pd.DataFrame(index = partition['validation'],\n                                              data=proba_predictions,\n                                              columns=target_names)\n    baseline_proba_predictions.to_csv(\"baseline_predictions.csv\")\n    baseline_losses = pd.DataFrame(history.history[\"loss\"], columns=[\"train_loss\"])\n    baseline_losses[\"val_loss\"] = history.history[\"val_loss\"]\n    baseline_losses.to_csv(\"baseline_losses.csv\")\n    \n    \n    submission_proba_predictions = model.predict(submission_predict_generator)\n    baseline_labels = test_labels.copy()\n    baseline_labels.loc[:, test_labels.drop([\"ID\", \"Label\"], axis=1).columns.values] = submission_proba_predictions\n    baseline_labels.to_csv(\"baseline_submission_proba.csv\")\n# If you already have done a baseline fit once, \n# you can load predictions as csv and further fitting is not neccessary:\nelse:\n    baseline_proba_predictions = pd.read_csv(\"../input/protein-atlas-eab-predictions/baseline_predictions.csv\", index_col=0)\n    baseline_losses = pd.read_csv(\"../input/protein-atlas-eab-predictions/baseline_losses.csv\", index_col=0)\n    baseline_labels = pd.read_csv(\"../input/protein-atlas-eab-predictions/baseline_submission_proba.csv\", index_col=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"validation_labels = train_labels.loc[train_labels.ID.isin(partition[\"validation\"])].copy()\nvalidation_labels.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"baseline_proba_predictions.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score as accuracy\n\ny_true = validation_labels.drop([\"ID\", \"Label\", \"number_of_targets\"], axis=1).values\ny_pred = np.where(baseline_proba_predictions.values > 0.5, 1, 0)\n\naccuracy(y_true.flatten(), y_pred.flatten())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_true[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"proba_predictions = baseline_proba_predictions.values\nhot_values = validation_labels.drop([\"ID\", \"Label\", \"number_of_targets\"], axis=1).values.flatten()\none_hot = (hot_values.sum()) / hot_values.shape[0] * 100\nzero_hot = (hot_values.shape[0] - hot_values.sum()) / hot_values.shape[0] * 100\n\nfig, ax = plt.subplots(1,2, figsize=(20,5))\nsns.distplot(proba_predictions.flatten() * 100, color=\"DodgerBlue\", ax=ax[0])\nax[0].set_xlabel(\"Probability in %\")\nax[0].set_ylabel(\"Density\")\nax[0].set_title(\"Predicted probabilities\")\nsns.barplot(x=[\"label = 0\", \"label = 1\"], y=[zero_hot, one_hot], ax=ax[1])\nax[1].set_ylim([0,100])\nax[1].set_title(\"True target label count\")\nax[1].set_ylabel(\"Percentage\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}