{"cells":[{"metadata":{},"cell_type":"markdown","source":"<html>\n<body>\n\n<p><font size=\"5\" color=\"orangered\">\nüéØ CSPNET: A NEW BACKBONE THAT CAN ENHANCE LEARNING CAPABILITY OF CNN\n    \n</font></p>\n</body>\n</html>"},{"metadata":{},"cell_type":"markdown","source":"![](https://miro.medium.com/max/2814/1*h1ouOibp3Ry7vj4r16cyVQ.png)\n\n<html>\n<body>\n\n<p><font size=\"5\" color=\"darkcyan\">\n\nNeural networks have enabled state-of-the-art approaches to achieve incredible results on computer vision tasks such as object detection. However, such success greatly relies on costly computation resources, which hinders people with cheap devices from appreciating the advanced technology. \n\nCross Stage Partial Network (CSPNet) help mitigate the problem that previous works require heavy inference computations from the network architecture perspective. This is attributed to the problem to the duplicate gradient information within network optimization. \n\nThe proposed network respect the variability of the gradients by integrating feature maps from the beginning and the end of a network stage, which, in our experiments, reduces computations by 20% with equivalent or even superior accuracy on the ImageNet dataset.\n\nThe CSPNet is easy to implement and general enough to cope with architectures based on ResNet, ResNeXt, and DenseNet\n\nüî•To find out more :- https://arxiv.org/pdf/1911.11929.pdf üî•\n\n</font></p>\n</body>\n</html>"},{"metadata":{},"cell_type":"markdown","source":"<html>\n<body>\n\n<p><font size=\"6\" color=\"dodgerblue\">\nüëçüèª Upvote the notebook if you find it insightful!\n    \n</font></p>\n</body>\n</html>\n"},{"metadata":{},"cell_type":"markdown","source":"<html>\n<body>\n\n<p><font size=\"5\" color=\"darkmagenta\">\nü™É Fetch the required libraries\n    \n</font></p>\n</body>\n</html>\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add the timm pytorch image model library from which we can extract CSPNet\nimport sys\nsys.path.append('../input/timm-pytorch-image-models/pytorch-image-models-master')\nimport timm","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Software library written for data manipulation and analysis.\nimport pandas as pd\n\n# Python library to interact with the file system.\nimport os\n\n# Python library for image augmentation\nimport albumentations as A\n\n# fastai library for computer vision tasks\nfrom fastai.vision.all import *\n\n# Developing and training neural network based deep learning models.\nimport torch\nfrom torch import nn","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<html>\n<body>\n\n<p><font size=\"5\" color=\"darkmagenta\">\nüíâ Data Loading\n\nMajor credits to [Darek K≈Çeczek](https://www.kaggle.com/thedrcat) for providing this dataset!\n    \n</font></p>\n</body>\n</html>\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define path to dataset, whose benefit is that this sample is more balanced than original train data.\npath = Path('../input/hpa-cell-tiles-sample-balanced-dataset')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(path/'cell_df.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# extract the the total number of target labels\nlabels = [str(i) for i in range(19)]\nfor x in labels: df[x] = df['image_labels'].apply(lambda r: int(x in r.split('|')))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here a sample of the dataset has been taken, change frac to 1 to train the entire dataset!\ndfs = df.sample(frac=0.1, random_state=42)\ndfs = dfs.reset_index(drop=True)\nlen(dfs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<html>\n<body>\n\n<p><font size=\"5\" color=\"darkmagenta\">\nüìù Data Preprocessing\n    \n</font></p>\n</body>\n</html>\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# obtain the input images.\ndef get_x(r): \n    return path/'cells'/(r['image_id']+'_'+str(r['cell_id'])+'.jpg')\n\n# obtain the targets.\ndef get_y(r): \n    return r['image_labels'].split('|')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<html>\n<body>\n\n<p><font size=\"5\" color=\"darkcyan\">\nüñ® Albumentations:\nAlbumentations is a Python library for image augmentation. Image augmentation is used in deep learning and computer vision tasks to increase the quality of trained models. The purpose of image augmentation is to create new training samples from the existing data.\n\nAlbumentations supports all common computer vision tasks such as classification, semantic segmentation, instance segmentation, object detection, and pose estimation.\nThe library provides a simple unified API to work with all data types: images (RBG-images, grayscale images, multispectral images), segmentation masks, bounding boxes, and keypoints.\nThe library contains more than 70 different augmentations to generate new training samples from the existing data.\nAlbumentations is fast.\n\nInstallation:- pip install -U albumentations\n    \n</font></p>\n</body>\n</html>\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"'''AlbumentationsTransform will perform different transforms over both\n   the training and validation datasets ''' \nclass AlbumentationsTransform(RandTransform):\n    \n    '''split_idx is None, which allows for us to say when we're setting our split_idx.\n       We set an order to 2 which means any resize operations are done first before our new transform. '''\n    split_idx, order = None, 2\n    \n    def __init__(self, train_aug, valid_aug): store_attr()\n    \n    # Inherit from RandTransform, allows for us to set that split_idx in our before_call.\n    def before_call(self, b, split_idx):\n        self.idx = split_idx\n    \n    # If split_idx is 0, run the trainining augmentation, otherwise run the validation augmentation. \n    def encodes(self, img: PILImage):\n        if self.idx == 0:\n            aug_img = self.train_aug(image=np.array(img))['image']\n        else:\n            aug_img = self.valid_aug(image=np.array(img))['image']\n        return PILImage.create(aug_img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_train_aug(size): \n    \n    return A.Compose([\n            # allows to combine RandomCrop and RandomScale\n            A.RandomResizedCrop(size,size),\n            \n            # Transpose the input by swapping rows and columns.\n            A.Transpose(p=0.5),\n        \n            # Flip the input horizontally around the y-axis.\n            A.HorizontalFlip(p=0.5),\n        \n            # Flip the input horizontally around the x-axis.\n            A.VerticalFlip(p=0.5),\n        \n            # Randomly apply affine transforms: translate, scale and rotate the input.\n            A.ShiftScaleRotate(p=0.5),\n        \n            # Randomly change hue, saturation and value of the input image.\n            A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n        \n            # Randomly change brightness and contrast of the input image.\n            A.RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n        \n            # CoarseDropout of the rectangular regions in the image.\n            A.CoarseDropout(p=0.5),\n        \n            # CoarseDropout of the square regions in the image.\n            A.Cutout(p=0.5) ])\n\ndef get_valid_aug(size): \n    \n    return A.Compose([\n    # Crop the central part of the input.   \n    A.CenterCrop(size, size, p=1.),\n    \n    # Resize the input to the given height and width.    \n    A.Resize(size,size)], p=1.)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''The first step item_tfms resizes all the images to the same size (this happens on the CPU) \n   and then batch_tfms happens on the GPU for the entire batch of images. '''\n# Transforms we need to do for each image in the dataset\nitem_tfms = [Resize(224), AlbumentationsTransform(get_train_aug(224), get_valid_aug(224))]\n\n# Transforms that can take place on a batch of images\nbatch_tfms = [Normalize.from_stats(*imagenet_stats)]\n\nbs=6","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock(vocab=labels)), # multi-label target\n                splitter=RandomSplitter(seed=42), # split data into training and validation subsets.\n                get_x=get_x, # obtain the input images.\n                get_y=get_y,  # obtain the targets.\n                item_tfms=item_tfms,\n                batch_tfms=batch_tfms\n                )\n\ndls = dblock.dataloaders(dfs, bs=bs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can call show_batch() to see what a sample of a batch looks like.\ndls.show_batch()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<html>\n<body>\n\n<p><font size=\"6\" color=\"darkmagenta\">\n‚åõÔ∏èModel Definition\n    \n</font></p>\n</body>\n</html>\n\n<html>\n<body>\n\n<p><font size=\"5\" color=\"darkcyan\">\nüõ†Cross Stage Partial Network (CSPNet) üõ†<br>\n    \nThe main purpose of designing CSPNet is to enable this architecture to achieve a richer gradient combination while reducing the amount of computation. This aim is achieved by partitioning feature map of the base layer into two parts and then merging them through a proposed cross-stage hierarchy. <br>\n    \nThe main concept is to make the gradient flow propagate through different network paths\nby splitting the gradient flow. In this way, we have confirmed that the propagated gradient information can have a large correlation difference by switching concatenation and transition steps. In addition, CSPNet can greatly reduce the amount of computation, and improve inference speed as well as accuracy. <br>\n\nThe proposed CSPNet deals with the following three problems: <br>\n    \n1) Strengthening learning ability of a CNN The accuracy of existing CNN is greatly degraded after lightweightening,so we CSPNet can maintain sufficient accuracy while being lightweightening.\nThe proposed CSPNet can be easily applied to ResNet, ResNeXt, and DenseNet. After applying CSPNet on the\nabove mentioned networks, the computation effort can be reduced from 10% to 20%, but it outperforms ResNet,\nResNeXt, DenseNet <br>\n\n2) Removing computational bottlenecks Too high a computational bottleneck will result in more cycles to complete the inference process, or some arithmetic units will often idle. Therefore, we hope we can evenly distribute the amount of computation at each layer in CNN so that we can effectively upgrade the utilization rate of each computation unit and thus reduce unnecessary energy consumption. <br>\n    \n3) Reducing memory costs The wafer fabrication cost of Dynamic Random-Access Memory (DRAM) is very expensive,\nand it also takes up a lot of space. If one can effectively reduce the memory cost, he/she will greatly reduce the cost of ASIC. In addition, a small area wafer can be used in a variety of edge computing devices. In reducing the use of memory usage, CSPNet adopt cross-channel pooling to compress the feature maps during the feature pyramid generating process. \n    \n</font></p>\n</body>\n</html>\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"class CSPNetModel(nn.Module):\n    \n    def __init__(self, num_classes=19, model_name='cspresnext50', pretrained=True):\n        super(CSPNetModel, self).__init__()\n        self.model = timm.create_model(model_name, pretrained=pretrained)\n        self.model.head.fc = nn.Linear(self.model.head.fc.in_features, num_classes)\n        \n    def forward(self, x):\n        x = self.model(x)\n        return x\n    \nmodel = CSPNetModel()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Group together some dls, a model, and metrics to handle training\nlearn = Learner(dls, model, metrics= accuracy_multi)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Choosing a good learning rate\nlearn.lr_find()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can use the fine_tune function to train a model with this given learning rate\nlearn.fine_tune(9,0.0012022644514217973)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot training and validation losses.\nlearn.recorder.plot_loss()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Interpretation methods for classification models.\ninterp = ClassificationInterpretation.from_learner(learn)\n\n# Show images in top_losses along with their prediction, actual, loss, and probability of actual class.\ninterp.plot_top_losses(5, nrows=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![](https://storage.googleapis.com/groundai-web-prod/media%2Fusers%2Fuser_299233%2Fproject_400509%2Fimages%2Ffig%2Fio.png)\n\n\n<html>\n<body>\n\n<p><font size=\"5\" color=\"darkcyan\">\nüéâ The above figure shows the size of each layer of ResNeXt50 and the proposed CSPResNeXt50. The\nCIO of the proposed CSPResNeXt (32.6M) is lower than that of the original ResNeXt50 (34.4M). In addition, the CSPResNeXt50 removes the bottleneck layers in the ResXBlock and maintains the same numbers of the input channel\nand the output channel and will have the most efficient computation when FLOPs are fixed. The low CIO and FLOPs enable our CSPResNeXt50 to outperform the vanilla ResNeXt50 by 22% in terms of computations.\n    \n</font></p>\n</body>\n</html>"},{"metadata":{},"cell_type":"markdown","source":"<html>\n<body>\n\n<p><font size=\"5\" color=\"mediumvioletred\">\nüéä That's it for today's review of one of the latest breakthroughs in Computer Vision!! üéä<br><br>\nüî• Let me know in the comments below if you would like me to explain the latest innovations in deep learning in notebooks like these with implementation in fastai or pytorch üî•\n    \n</font></p>\n</body>\n</html>"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}