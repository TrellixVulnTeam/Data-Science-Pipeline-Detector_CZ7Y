{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Installing h3 Lib","metadata":{}},{"cell_type":"code","source":"!pip install ../input/h3lib/h3-3.7.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl","metadata":{"_kg_hide-input":false,"_kg_hide-output":false,"execution":{"iopub.status.busy":"2022-05-19T13:57:27.430743Z","iopub.execute_input":"2022-05-19T13:57:27.431288Z","iopub.status.idle":"2022-05-19T13:57:59.558975Z","shell.execute_reply.started":"2022-05-19T13:57:27.431196Z","shell.execute_reply":"2022-05-19T13:57:59.557917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Limporting lib","metadata":{}},{"cell_type":"code","source":"import difflib\nimport gc\nfrom h3 import h3\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom unidecode import unidecode\nfrom tqdm.notebook import tqdm\nimport re\nimport pickle\nimport gensim.corpora as corpora # Create Dictionary\nimport gensim\nfrom gensim.utils import simple_preprocess\nfrom pprint import pprint# number of topics\n\n\ntqdm.pandas()\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-19T13:57:59.561516Z","iopub.execute_input":"2022-05-19T13:57:59.56186Z","iopub.status.idle":"2022-05-19T13:58:00.96733Z","shell.execute_reply.started":"2022-05-19T13:57:59.561813Z","shell.execute_reply":"2022-05-19T13:58:00.966175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class tool_box:\n    def __init__(self):\n        self.H3_res = 9\n        \n    def tokenization(self, text):\n        if text:\n            tokens = re.split(' ',text)\n        return tokens\n\n    def get_lda_cluster(self, category):\n        if category:\n            new_doc = self.tokenization(str(category))\n            new_doc_bow = id2word.doc2bow(new_doc)\n            clusters = lda_model.get_document_topics(new_doc_bow)\n\n            scores = []\n            for i in clusters:\n                scores.append(i[1])\n\n            max_index = scores.index(max(scores))\n            return clusters[max_index][0]\n        else:\n            return 2\n\n    def geo_to_h3(self, row):\n        return h3.geo_to_h3(lat=row.latitude,lng=row.longitude,resolution = self.H3_res)\n\n    def search_engine(self, id_poi, name, h3_cell, cluster):\n        first_step = training_set_[training_set_['h3_cell'] == h3_cell]\n        second_step = first_step[training_set_['category_cluster'] == cluster]\n\n        kw = difflib.get_close_matches(name, second_step['name'], n=3)\n\n        result = second_step[second_step['name'].isin(kw)]\n        if result['id'].empty:\n            return id_poi\n        else:\n            sep = ' '\n            match_formated = id_poi\n            for match in result['id']:\n                match_formated += sep \n                match_formated += match\n            return match_formated\n","metadata":{"execution":{"iopub.status.busy":"2022-05-19T13:58:00.968964Z","iopub.execute_input":"2022-05-19T13:58:00.969285Z","iopub.status.idle":"2022-05-19T13:58:00.981783Z","shell.execute_reply.started":"2022-05-19T13:58:00.969244Z","shell.execute_reply":"2022-05-19T13:58:00.980811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tool = tool_box()","metadata":{"execution":{"iopub.status.busy":"2022-05-19T13:58:00.984497Z","iopub.execute_input":"2022-05-19T13:58:00.985122Z","iopub.status.idle":"2022-05-19T13:58:00.998048Z","shell.execute_reply.started":"2022-05-19T13:58:00.985071Z","shell.execute_reply":"2022-05-19T13:58:00.997363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data importing (train and test sets)","metadata":{}},{"cell_type":"code","source":"training_set_ = pd.read_csv('../input/training-set-foursquare/training_set.csv')\ntraining_set_","metadata":{"execution":{"iopub.status.busy":"2022-05-19T13:58:00.999545Z","iopub.execute_input":"2022-05-19T13:58:00.999777Z","iopub.status.idle":"2022-05-19T13:58:11.704879Z","shell.execute_reply.started":"2022-05-19T13:58:00.99975Z","shell.execute_reply":"2022-05-19T13:58:11.703889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_set_ = pd.read_csv('../input/foursquare-location-matching/test.csv')\ntest_set_","metadata":{"execution":{"iopub.status.busy":"2022-05-19T13:58:11.706056Z","iopub.execute_input":"2022-05-19T13:58:11.706308Z","iopub.status.idle":"2022-05-19T13:58:11.731009Z","shell.execute_reply.started":"2022-05-19T13:58:11.706278Z","shell.execute_reply":"2022-05-19T13:58:11.730239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols = ['id', 'name', 'lat', 'lng', 'address', 'city', 'state', 'zip', 'country', 'url', 'phone', 'categories']\n\nnew_test_set_ = training_set_[cols]\nnew_test_set_ = new_test_set_.rename(columns={'lat':'latitude', 'lng': 'longitude'})\nnew_test_set_","metadata":{"execution":{"iopub.status.busy":"2022-05-19T13:58:11.732538Z","iopub.execute_input":"2022-05-19T13:58:11.733089Z","iopub.status.idle":"2022-05-19T13:58:12.123766Z","shell.execute_reply.started":"2022-05-19T13:58:11.733051Z","shell.execute_reply":"2022-05-19T13:58:12.122783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_set_.columns == new_test_set_.columns","metadata":{"execution":{"iopub.status.busy":"2022-05-19T13:58:12.124985Z","iopub.execute_input":"2022-05-19T13:58:12.125247Z","iopub.status.idle":"2022-05-19T13:58:12.131843Z","shell.execute_reply.started":"2022-05-19T13:58:12.125217Z","shell.execute_reply":"2022-05-19T13:58:12.13095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_set_ = new_test_set_.copy()\ntest_set_","metadata":{"execution":{"iopub.status.busy":"2022-05-19T13:58:12.133322Z","iopub.execute_input":"2022-05-19T13:58:12.134175Z","iopub.status.idle":"2022-05-19T13:58:12.332975Z","shell.execute_reply.started":"2022-05-19T13:58:12.134107Z","shell.execute_reply":"2022-05-19T13:58:12.332018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_set_.drop(['address', 'city', 'state','zip', 'country', 'url', 'phone'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T13:58:12.336801Z","iopub.execute_input":"2022-05-19T13:58:12.337102Z","iopub.status.idle":"2022-05-19T13:58:12.4095Z","shell.execute_reply.started":"2022-05-19T13:58:12.337065Z","shell.execute_reply":"2022-05-19T13:58:12.408927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data first check","metadata":{}},{"cell_type":"code","source":"# normalizing name to avoid mismatch due to accents or punctuations\ntest_set_['name'] = test_set_['name'].astype(str)\ntest_set_['name'] = test_set_['name'].apply(unidecode)\ntest_set_['name'] = test_set_['name'].str.replace('[^\\w\\s]','',regex=True)\ntest_set_['name'] = test_set_['name'].str.lower()\n\n# normalizing categories to avoid mismatch due to accents or punctuations\ntest_set_['categories'] = test_set_['categories'].astype(str)\ntest_set_['categories'] = test_set_['categories'].str.lower()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-19T13:58:12.41045Z","iopub.execute_input":"2022-05-19T13:58:12.410926Z","iopub.status.idle":"2022-05-19T13:58:15.722014Z","shell.execute_reply.started":"2022-05-19T13:58:12.410895Z","shell.execute_reply":"2022-05-19T13:58:15.721194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"collected = gc.collect() # or gc.collect(2)\nprint(\"Garbage collector: collected\",\n          \"%d objects.\" % collected)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T13:58:15.723342Z","iopub.execute_input":"2022-05-19T13:58:15.723691Z","iopub.status.idle":"2022-05-19T13:58:15.845213Z","shell.execute_reply.started":"2022-05-19T13:58:15.723649Z","shell.execute_reply":"2022-05-19T13:58:15.843925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineer","metadata":{}},{"cell_type":"markdown","source":"## So, considering the number of categories available, it was unviable to classify all of them by myself. After some research, I found the LDA method that allows to cluster of words into categories. (check: https://towardsdatascience.com/topic-modelling-in-python-with-nltk-and-gensim-4ef03213cd21).\n\n","metadata":{}},{"cell_type":"code","source":"test_set_['category_tokenied']= test_set_['categories'].apply(lambda x: tool.tokenization(x))","metadata":{"execution":{"iopub.status.busy":"2022-05-19T13:58:15.846343Z","iopub.execute_input":"2022-05-19T13:58:15.846597Z","iopub.status.idle":"2022-05-19T13:58:19.093736Z","shell.execute_reply.started":"2022-05-19T13:58:15.846559Z","shell.execute_reply":"2022-05-19T13:58:19.092857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"id2word = corpora.Dictionary.load('../input/training-set-foursquare/dictionary.gensim') # Create Disctionary\n\nloaded_model = pickle.load(open('../input/training-set-foursquare/corpus.pkl', 'rb'))\ncorpus = loaded_model # Create Corpus\n","metadata":{"execution":{"iopub.status.busy":"2022-05-19T13:58:19.09519Z","iopub.execute_input":"2022-05-19T13:58:19.095446Z","iopub.status.idle":"2022-05-19T13:58:21.66469Z","shell.execute_reply.started":"2022-05-19T13:58:19.095415Z","shell.execute_reply":"2022-05-19T13:58:21.663591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lda_model = gensim.models.LdaMulticore.load('../input/training-set-foursquare/model_trained', mmap='r')\ndoc_lda = lda_model[corpus]","metadata":{"execution":{"iopub.status.busy":"2022-05-19T13:58:21.667255Z","iopub.execute_input":"2022-05-19T13:58:21.667766Z","iopub.status.idle":"2022-05-19T13:58:21.6919Z","shell.execute_reply.started":"2022-05-19T13:58:21.667712Z","shell.execute_reply":"2022-05-19T13:58:21.691208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let's check how to get the cluster from this model","metadata":{}},{"cell_type":"markdown","source":"## So get_document_topics returns a list of tuples with the percent of similarity with each cluster. Let's use it to apply for all the row by definig a function and them apply it. ","metadata":{}},{"cell_type":"code","source":"test_set_['category_cluster'] = test_set_.progress_apply(lambda x: tool.get_lda_cluster(x['categories']),axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T13:58:21.692954Z","iopub.execute_input":"2022-05-19T13:58:21.693724Z","iopub.status.idle":"2022-05-19T14:02:02.091741Z","shell.execute_reply.started":"2022-05-19T13:58:21.693688Z","shell.execute_reply":"2022-05-19T14:02:02.090761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### My approach was using the h3 lib created by Uber (check documentation here: https://github.com/uber/h3) to cluster POIs into hex using the method h3.geo_to_h3 first. It will help us to check in a certain hex all the places.  \n### It allows to search new POIs according the hex_cell id which combined to others features to solve eventually conflits, can solve this bussiness problem. ","metadata":{}},{"cell_type":"code","source":"collected = gc.collect() # or gc.collect(2)\nprint(\"Garbage collector: collected\",\n          \"%d objects.\" % collected)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T14:02:02.094897Z","iopub.execute_input":"2022-05-19T14:02:02.095259Z","iopub.status.idle":"2022-05-19T14:02:02.67836Z","shell.execute_reply.started":"2022-05-19T14:02:02.095223Z","shell.execute_reply":"2022-05-19T14:02:02.677307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating the Hex_cell ids","metadata":{}},{"cell_type":"code","source":"test_set_['h3_cell'] = test_set_.progress_apply(tool.geo_to_h3,axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T14:02:02.679902Z","iopub.execute_input":"2022-05-19T14:02:02.680537Z","iopub.status.idle":"2022-05-19T14:02:44.809083Z","shell.execute_reply.started":"2022-05-19T14:02:02.680488Z","shell.execute_reply":"2022-05-19T14:02:44.808222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"collected = gc.collect() \nprint(\"Garbage collector: collected\",\n          \"%d objects.\" % collected)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-19T14:02:44.810552Z","iopub.execute_input":"2022-05-19T14:02:44.810782Z","iopub.status.idle":"2022-05-19T14:02:45.381365Z","shell.execute_reply.started":"2022-05-19T14:02:44.810755Z","shell.execute_reply":"2022-05-19T14:02:45.38071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Defining the search engine ","metadata":{}},{"cell_type":"markdown","source":"### Here I'm going to use all the features that I created to build a engine search. First i'm going to use the hex id to filter all the places of a certain area, than I will filter all of them by the category cluster. Than, to solve eventually conflits, I'm using the difflib.get_close_matches to get the most similar name str. ","metadata":{}},{"cell_type":"code","source":"test_set_['matches'] = test_set_.progress_apply(lambda x: tool.search_engine(x['id'], x['name'], x['h3_cell'], x['category_cluster']),axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T14:07:12.234329Z","iopub.execute_input":"2022-05-19T14:07:12.234988Z","iopub.status.idle":"2022-05-19T14:12:39.89441Z","shell.execute_reply.started":"2022-05-19T14:07:12.234953Z","shell.execute_reply":"2022-05-19T14:12:39.893044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## As we can see, it was enough to find a pretty good match for two of 5 rows of the sample","metadata":{}},{"cell_type":"code","source":"submission_sample = test_set_[['id','matches']]\n","metadata":{"execution":{"iopub.status.busy":"2022-05-19T14:07:07.558741Z","iopub.status.idle":"2022-05-19T14:07:07.559837Z","shell.execute_reply.started":"2022-05-19T14:07:07.559371Z","shell.execute_reply":"2022-05-19T14:07:07.559403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"collected = gc.collect() # or gc.collect(2)\nprint(\"Garbage collector: collected\",\n          \"%d objects.\" % collected)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-19T14:07:07.561263Z","iopub.status.idle":"2022-05-19T14:07:07.562287Z","shell.execute_reply.started":"2022-05-19T14:07:07.561985Z","shell.execute_reply":"2022-05-19T14:07:07.562016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_sample","metadata":{"execution":{"iopub.status.busy":"2022-05-19T14:07:07.563613Z","iopub.status.idle":"2022-05-19T14:07:07.5645Z","shell.execute_reply.started":"2022-05-19T14:07:07.564178Z","shell.execute_reply":"2022-05-19T14:07:07.564215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sample Submittion","metadata":{}},{"cell_type":"code","source":"submission_sample.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T14:07:07.566209Z","iopub.status.idle":"2022-05-19T14:07:07.566946Z","shell.execute_reply.started":"2022-05-19T14:07:07.566675Z","shell.execute_reply":"2022-05-19T14:07:07.566703Z"},"trusted":true},"execution_count":null,"outputs":[]}]}