{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# COS20083 Advanced Data Analytics\n\n## Assignment 2: Case Study and Algorithm Implementation\n\n### Semester 1, 2022","metadata":{"id":"xWskOwOjwhzJ"}},{"cell_type":"markdown","source":"#### Group Number: <p style =\"color: red;\"> 11</p>\n#### Group Members: <p style =\"color: red;\"> Lim Zong Xin (101232574), Justin Liu Shan Wei (101231403)</p>","metadata":{"id":"o-IoztYpwhzM"}},{"cell_type":"markdown","source":"# 1. Introduction","metadata":{"id":"E5OBF6tZwhzN"}},{"cell_type":"markdown","source":"### What is the purpose of the assignment?\n### What is the problem to be addressed by this case study?\n\nThe purpose of the assignment is to is to build a machine learning model to predict which Place entries represent the same point-of-interest. The problem to be addressed in this case study is to match Point-of-Interests using a simulated dataset from Foursquare which contains the Places and movement of customers of over one-and-a-half million Place entries to predict where new stores and businesses will benefit people the most.","metadata":{"id":"8Jc29yVXsgZq"}},{"cell_type":"markdown","source":"# 2. Data Collection","metadata":{"id":"KdJWbse3whzO"}},{"cell_type":"markdown","source":"### Describe the purpose and the process of data collection and understanding here\n\nThe purpose of performing data collection and understanding is to gather information in a systematic manner to allow data analysis. The csv files used in this assignment consists of train.csv, test.csv, sample_submission.csv and the pairs.csv. Several python libraries were imported and pandas was used to read the csv files using the pd.read_csv() function. The data types present in the dataframes are then shown using the df.info() function as listed below.","metadata":{"id":"NvEzuS2ss5qH"}},{"cell_type":"code","source":"# Show your code here (Step by Step) \n# Comment each step in your code","metadata":{"id":"35Sy4225whzP","execution":{"iopub.status.busy":"2022-05-19T06:02:10.82527Z","iopub.execute_input":"2022-05-19T06:02:10.825957Z","iopub.status.idle":"2022-05-19T06:02:10.84923Z","shell.execute_reply.started":"2022-05-19T06:02:10.825857Z","shell.execute_reply":"2022-05-19T06:02:10.848368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from google.colab import drive\n# drive.mount('/content/drive')","metadata":{"id":"aQwp_WcB0WK6","outputId":"69bbaa3f-0e8b-4577-b4a3-b4844d694152","execution":{"iopub.status.busy":"2022-05-19T06:02:10.850808Z","iopub.execute_input":"2022-05-19T06:02:10.851021Z","iopub.status.idle":"2022-05-19T06:02:10.8541Z","shell.execute_reply.started":"2022-05-19T06:02:10.850995Z","shell.execute_reply":"2022-05-19T06:02:10.853333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Import libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import BallTree\nfrom tqdm import tqdm","metadata":{"id":"d7h5aPsa0y2p","execution":{"iopub.status.busy":"2022-05-19T06:02:10.855229Z","iopub.execute_input":"2022-05-19T06:02:10.855595Z","iopub.status.idle":"2022-05-19T06:02:13.702336Z","shell.execute_reply.started":"2022-05-19T06:02:10.855565Z","shell.execute_reply":"2022-05-19T06:02:13.701297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Read csv files\ndf_train = pd.read_csv('../input/foursquare-location-matching/train.csv')\ndf_test = pd.read_csv('../input/foursquare-location-matching/test.csv')\n\nsample_submission = pd.read_csv('../input/foursquare-location-matching/sample_submission.csv')\npairs=pd.read_csv('../input/foursquare-location-matching/pairs.csv')","metadata":{"id":"sZR-MBM400mS","execution":{"iopub.status.busy":"2022-05-19T06:02:13.703712Z","iopub.execute_input":"2022-05-19T06:02:13.703987Z","iopub.status.idle":"2022-05-19T06:02:30.387554Z","shell.execute_reply.started":"2022-05-19T06:02:13.703956Z","shell.execute_reply":"2022-05-19T06:02:30.386627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Display data types in train dataset\ndf_train.info()","metadata":{"id":"-O_Ug0K10RxD","outputId":"782799aa-f31b-4522-cace-ac5b901b3a0e","execution":{"iopub.status.busy":"2022-05-19T06:02:30.389806Z","iopub.execute_input":"2022-05-19T06:02:30.390051Z","iopub.status.idle":"2022-05-19T06:02:31.565373Z","shell.execute_reply.started":"2022-05-19T06:02:30.39002Z","shell.execute_reply":"2022-05-19T06:02:31.564353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Display data types in test dataset\ndf_test.info()","metadata":{"id":"cA05eMtZ0RxE","outputId":"67361a87-a6ec-4c2d-c853-633f4a310868","execution":{"iopub.status.busy":"2022-05-19T06:02:31.566918Z","iopub.execute_input":"2022-05-19T06:02:31.567337Z","iopub.status.idle":"2022-05-19T06:02:31.58122Z","shell.execute_reply.started":"2022-05-19T06:02:31.567289Z","shell.execute_reply":"2022-05-19T06:02:31.580631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Display data types in sample_submission dataset\nsample_submission.info()","metadata":{"id":"_yhiZNpP0RxE","outputId":"39bbc4e3-1cfe-49b5-be56-0628c3a3f3c4","execution":{"iopub.status.busy":"2022-05-19T06:02:31.582179Z","iopub.execute_input":"2022-05-19T06:02:31.582772Z","iopub.status.idle":"2022-05-19T06:02:31.594032Z","shell.execute_reply.started":"2022-05-19T06:02:31.58272Z","shell.execute_reply":"2022-05-19T06:02:31.593108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Display data types in pairs dataset\npairs.info()","metadata":{"id":"8N7XeXUN0RxF","outputId":"5e787d83-82d5-4339-c9d5-c096a29e380c","execution":{"iopub.status.busy":"2022-05-19T06:02:31.595311Z","iopub.execute_input":"2022-05-19T06:02:31.595544Z","iopub.status.idle":"2022-05-19T06:02:32.668837Z","shell.execute_reply.started":"2022-05-19T06:02:31.595517Z","shell.execute_reply":"2022-05-19T06:02:32.667714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Exploratory Data Analysis","metadata":{"id":"w1QPbDe10RxF"}},{"cell_type":"markdown","source":"Exploratory data analysis is an approach of analyzing data sets to summarize their main characteristics, often using statistical graphics and other data visualization methods","metadata":{"id":"_XIWON5FrkSa"}},{"cell_type":"code","source":"# Show your code here (Step by Step) \n# Comment each step in your code","metadata":{"id":"sW-jB1Hasnq2","execution":{"iopub.status.busy":"2022-05-19T06:02:32.670797Z","iopub.execute_input":"2022-05-19T06:02:32.67113Z","iopub.status.idle":"2022-05-19T06:02:32.675257Z","shell.execute_reply.started":"2022-05-19T06:02:32.671085Z","shell.execute_reply":"2022-05-19T06:02:32.674455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Total number of rows and columns in datasets\nprint(df_train.shape)\nprint(df_test.shape)\nprint(sample_submission.shape)\nprint(pairs.shape)","metadata":{"id":"AxX_SWO_Yuls","outputId":"4d4dd759-3ac5-4d00-e929-4110163720d6","execution":{"iopub.status.busy":"2022-05-19T06:02:32.676698Z","iopub.execute_input":"2022-05-19T06:02:32.677009Z","iopub.status.idle":"2022-05-19T06:02:32.689939Z","shell.execute_reply.started":"2022-05-19T06:02:32.676976Z","shell.execute_reply":"2022-05-19T06:02:32.688933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Display first few rows of Train dataset\ndf_train.head()","metadata":{"id":"61pOBWAcxkaH","outputId":"3e6c5fdf-8906-447f-9c92-c836ffe13f9a","execution":{"iopub.status.busy":"2022-05-19T06:02:32.691227Z","iopub.execute_input":"2022-05-19T06:02:32.691481Z","iopub.status.idle":"2022-05-19T06:02:32.722147Z","shell.execute_reply.started":"2022-05-19T06:02:32.691448Z","shell.execute_reply":"2022-05-19T06:02:32.721069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Display first few rows of Test dataset\ndf_test.head()","metadata":{"id":"0v-92zT4yAYY","outputId":"c31a1a75-0445-40e9-c738-255e6642bce5","execution":{"iopub.status.busy":"2022-05-19T06:02:32.723894Z","iopub.execute_input":"2022-05-19T06:02:32.724201Z","iopub.status.idle":"2022-05-19T06:02:32.744777Z","shell.execute_reply.started":"2022-05-19T06:02:32.72416Z","shell.execute_reply":"2022-05-19T06:02:32.743828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Total number of missing values in Train dataset\nprint(df_train.isnull().sum())","metadata":{"id":"DmglHUYg086W","outputId":"6b514656-4d25-47c7-cd80-7cdb54ae9216","execution":{"iopub.status.busy":"2022-05-19T06:02:32.746496Z","iopub.execute_input":"2022-05-19T06:02:32.746844Z","iopub.status.idle":"2022-05-19T06:02:33.896523Z","shell.execute_reply.started":"2022-05-19T06:02:32.746801Z","shell.execute_reply":"2022-05-19T06:02:33.895472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Missing values in pairs dataset\nprint(pairs.isnull().sum())","metadata":{"id":"EDuqwQezj6wO","outputId":"31694819-e583-4b40-c9bf-f247ddb28c43","execution":{"iopub.status.busy":"2022-05-19T06:02:33.900039Z","iopub.execute_input":"2022-05-19T06:02:33.900299Z","iopub.status.idle":"2022-05-19T06:02:34.952283Z","shell.execute_reply.started":"2022-05-19T06:02:33.900267Z","shell.execute_reply":"2022-05-19T06:02:34.951385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# How many missing values for each example\nfig, axs = plt.subplots(1, 2, figsize=(15, 5))\ndf_train.isna().mean().sort_values(ascending=False).plot(\n    kind=\"bar\", title=\"Missing Values by Variable\", ax=axs[0]\n)\naxs[0].set_ylabel(\"% of Missing Values\")\n\ndf_train.isna().sum(axis=1).value_counts().sort_index().plot(\n    ax=axs[1], title=\"Missing Values by Observation\", kind=\"bar\"\n)\n\n#Plot visualization for missing values by variable\naxs[1].set_xlabel(\"Number of Missing Variable\")\naxs[1].set_ylabel(\"Number of Observations\")\nplt.xticks(rotation=0)\nplt.show()","metadata":{"id":"5nBRqWv10jpe","outputId":"613d8f91-8fe0-49a6-e09f-13a3e4bcc567","execution":{"iopub.status.busy":"2022-05-19T06:02:34.953638Z","iopub.execute_input":"2022-05-19T06:02:34.954352Z","iopub.status.idle":"2022-05-19T06:02:37.706537Z","shell.execute_reply.started":"2022-05-19T06:02:34.954308Z","shell.execute_reply":"2022-05-19T06:02:37.705651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Display number of unique values for each variable for train dataset\ndf_train.nunique()","metadata":{"id":"KbQ8vQeIsdoR","outputId":"c972c2c3-8799-477f-e369-c80b62b31689","execution":{"iopub.status.busy":"2022-05-19T06:02:37.708201Z","iopub.execute_input":"2022-05-19T06:02:37.708514Z","iopub.status.idle":"2022-05-19T06:02:40.883288Z","shell.execute_reply.started":"2022-05-19T06:02:37.70847Z","shell.execute_reply":"2022-05-19T06:02:40.882462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Display first few rows of Pairs dataframe\npairs.head()","metadata":{"id":"RNFktZCss1-n","outputId":"8a4eef8a-91ab-4c0e-fe97-08e1431f98a4","execution":{"iopub.status.busy":"2022-05-19T06:02:40.884496Z","iopub.execute_input":"2022-05-19T06:02:40.884912Z","iopub.status.idle":"2022-05-19T06:02:40.911059Z","shell.execute_reply.started":"2022-05-19T06:02:40.884879Z","shell.execute_reply":"2022-05-19T06:02:40.910138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Statistical summary for Pairs dataset\npairs.describe()","metadata":{"id":"vKU3R-q7tAon","outputId":"f5620204-dd7b-4bca-a0f4-43ff6a4d1f62","execution":{"iopub.status.busy":"2022-05-19T06:02:40.91227Z","iopub.execute_input":"2022-05-19T06:02:40.91249Z","iopub.status.idle":"2022-05-19T06:02:41.053111Z","shell.execute_reply.started":"2022-05-19T06:02:40.912463Z","shell.execute_reply":"2022-05-19T06:02:41.052222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Plot percentage of data by country in Train dataset\ncountry_stats=df_train['country'].value_counts()*100/df_train['country'].value_counts().sum()\ncountry_stats=country_stats.head(10)\n\nplt.figure(figsize=(8,7))\ncolor=[\"gray\"]*len(country_stats.index)\ncolor[0]=\"aqua\"\nsns.barplot(x=country_stats.index, y=country_stats.values,palette=color, saturation=.5)#, palette=clrs) # color=clrs)\nplt.title(\"% Data by Country\")\nplt.xlabel('country')\n_=plt.ylabel('Percentage')","metadata":{"id":"inM5lts9w569","outputId":"a77bb931-214f-457c-efd5-b3fc1ac2c42c","execution":{"iopub.status.busy":"2022-05-19T06:02:41.05425Z","iopub.execute_input":"2022-05-19T06:02:41.054477Z","iopub.status.idle":"2022-05-19T06:02:41.631651Z","shell.execute_reply.started":"2022-05-19T06:02:41.054448Z","shell.execute_reply":"2022-05-19T06:02:41.630662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the graph it can be seen that US has the most data entries","metadata":{"id":"kQ5Vg8WzxOC1"}},{"cell_type":"code","source":"#Plot percentage of data by state in the US\nstate_stats=df_train[df_train['country']=='US']['state'].value_counts()*100/df_train[df_train['country']=='US']['state'].value_counts().sum()\nstate_stats=state_stats.head(10)\n\nplt.figure(figsize=(8,7))\ncolor=[\"gray\"]*len(state_stats.index)\ncolor[0]=\"aqua\"\nsns.barplot(x=state_stats.index, y=state_stats.values,palette=color, saturation=.5)#, palette=clrs) # color=clrs)\nplt.title(\"% Data by State\")\nplt.xlabel('State')\n_=plt.ylabel('Percentage')","metadata":{"id":"kRjtKGjgUobE","outputId":"804fa9a3-951a-4a50-a2ac-5839b02cf741","execution":{"iopub.status.busy":"2022-05-19T06:02:41.633456Z","iopub.execute_input":"2022-05-19T06:02:41.633834Z","iopub.status.idle":"2022-05-19T06:02:42.477483Z","shell.execute_reply.started":"2022-05-19T06:02:41.633744Z","shell.execute_reply":"2022-05-19T06:02:42.476516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Plot the most frequent categories in the Train dataset\nprint(f'There are {df_train[\"categories\"].nunique()} unique categories')\n\n# Take a look at the most frequent categories\ndf_train[\"categories\"].value_counts().to_frame().query(\"categories > 5_000\")[\n    \"categories\"\n].sort_values(ascending=True).plot(\n    kind=\"barh\", title=\"Most Frequent Categories\", figsize=(5, 8)\n)\nplt.show()","metadata":{"id":"Clgf7F3CInqt","outputId":"d5fb49a4-eec4-436a-c490-0c44662493f5","execution":{"iopub.status.busy":"2022-05-19T06:02:42.478878Z","iopub.execute_input":"2022-05-19T06:02:42.479096Z","iopub.status.idle":"2022-05-19T06:02:43.234021Z","shell.execute_reply.started":"2022-05-19T06:02:42.479069Z","shell.execute_reply":"2022-05-19T06:02:43.233047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Drop unwanted variables from pairs dataset\npairs = pairs.drop(['address_1','city_1','state_1','zip_1','url_1','phone_1','address_2','city_2','state_2','zip_2','url_2','phone_2'],axis=1)\npairs = pairs.fillna(\"__nan__\")\npairs.head()","metadata":{"id":"xOMpAx37UOed","outputId":"63510506-ce56-461b-9a30-475423d2223d","execution":{"iopub.status.busy":"2022-05-19T06:02:43.235985Z","iopub.execute_input":"2022-05-19T06:02:43.236579Z","iopub.status.idle":"2022-05-19T06:02:43.911854Z","shell.execute_reply.started":"2022-05-19T06:02:43.236532Z","shell.execute_reply":"2022-05-19T06:02:43.910988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Perform one-hot encoding on the columns containing string variables to fit into model\npairs.country_1 = pairs.country_1.factorize()[0]\npairs.country_2 = pairs.country_2.factorize()[0]\npairs.categories_1 = pairs.categories_1.factorize()[0]\npairs.categories_2 = pairs.categories_2.factorize()[0]\npairs.name_1 = pairs.name_1.factorize()[0]\npairs.name_2 = pairs.name_2.factorize()[0]\npairs.match = pairs.match.factorize()[0]\n\npairs.head()\n","metadata":{"id":"uiv8XGD1UQMB","outputId":"a10aa0bc-9901-4c58-9d71-cc11873ec969","execution":{"iopub.status.busy":"2022-05-19T06:02:43.913164Z","iopub.execute_input":"2022-05-19T06:02:43.914041Z","iopub.status.idle":"2022-05-19T06:02:44.494713Z","shell.execute_reply.started":"2022-05-19T06:02:43.913995Z","shell.execute_reply":"2022-05-19T06:02:44.493828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Filling missing values in test and train dataset\ndf_test.categories = df_test.categories.fillna('__NAN__')\ndf_test.name = df_test.name.fillna('__NAN__')\ndf_train['country'].fillna('NA',inplace=True)\ndf_train.categories=df_train.categories.fillna('__NAN__')\ndf_train.name = df_train.name.fillna('__NAN__')","metadata":{"id":"WWCep0HO91Tu","execution":{"iopub.status.busy":"2022-05-19T06:02:44.496118Z","iopub.execute_input":"2022-05-19T06:02:44.496332Z","iopub.status.idle":"2022-05-19T06:02:44.938673Z","shell.execute_reply.started":"2022-05-19T06:02:44.496305Z","shell.execute_reply":"2022-05-19T06:02:44.937531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Explain:\n1. Description of dataframe\n2. Graphical plots of data\n3. Descriptive statistics of data\n\n- After reading all the csv files, the total number of rows and columns of all the data in each dataframe is shown by using the df.shape function. It can be seen that df_train has 1138812 rows and 13 columns, df_test has 5 rows and 2 columns, sample_submission has 5 rows and 2 columns and pairs has 578907 rows and 25 columns. The total number of missing data in df_train is shown by the df_train.isnull().sum() function while the total number of missing data in df_test is shown by the df_test.isnull().sum() function.\n\n- Two bar charts are plotted to show which countries have the most data entries. In the train dataset, it can be seen from the graph that the US has the most data entries. Another graph is plotted to see which states have the most data entries in the US, which is the CA state. The graph of most frequent categories is also plotted to show which categories appear most frequently in the Train Dataset which is the Residentual Buildings(Apartments / Condos) category.","metadata":{"id":"HBfTDKD0t3Y3"}},{"cell_type":"markdown","source":"# 4. Model Building","metadata":{"id":"LvE0KzxewhzS"}},{"cell_type":"markdown","source":"### Describe the process of model building","metadata":{"id":"aQAL2y_puFVo"}},{"cell_type":"code","source":"# Show your code here (Step by Step) \n# Comment each step in your code","metadata":{"id":"XjJkLy7lVwAW","execution":{"iopub.status.busy":"2022-05-19T06:02:44.940156Z","iopub.execute_input":"2022-05-19T06:02:44.940557Z","iopub.status.idle":"2022-05-19T06:02:44.943949Z","shell.execute_reply.started":"2022-05-19T06:02:44.940518Z","shell.execute_reply":"2022-05-19T06:02:44.943299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Select features from pairs dataset to be used \nfeatures = ['latitude_1', 'latitude_2', 'longitude_1', 'longitude_2','country_1','country_2','categories_1','categories_2','name_1','name_2']\n\n#Assign X and y\nX = pairs[features]\ny = pairs.match\n\n#Train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n#Initialize Random Forest Classifier Model\nmodel = RandomForestClassifier(n_jobs = -1)\n\n#Fit X and y Train into model\nmodel.fit(X_train, y_train)","metadata":{"id":"raW3wv9MDDrW","outputId":"e3f1f18e-3c17-4e24-c371-8ca522f74884","execution":{"iopub.status.busy":"2022-05-19T06:02:44.945177Z","iopub.execute_input":"2022-05-19T06:02:44.94594Z","iopub.status.idle":"2022-05-19T06:04:18.462617Z","shell.execute_reply.started":"2022-05-19T06:02:44.945906Z","shell.execute_reply":"2022-05-19T06:04:18.461601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reference from https://www.kaggle.com/code/andypenrose/spatial-neighbours-benchmark-name-and-category\n\n#Takes the latitude and longtitude values to construct ball tree\ntree = BallTree(np.deg2rad(df_test[['latitude', 'longitude']].values), metric='haversine')","metadata":{"id":"XM43Havz0RxJ","execution":{"iopub.status.busy":"2022-05-19T06:04:18.464133Z","iopub.execute_input":"2022-05-19T06:04:18.465065Z","iopub.status.idle":"2022-05-19T06:04:18.472768Z","shell.execute_reply.started":"2022-05-19T06:04:18.465013Z","shell.execute_reply":"2022-05-19T06:04:18.471551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# list for storing the points of interest\npois_out = []\n# number of neighbours considered\nn = min(20, len(df_test))\n# max number of recommended points of interest\nmax_poi = 2\n# max distance\nmax_dist_cat = 0.0005\nmax_dist_name = 0.005\nmax_dist = max(max_dist_cat, max_dist_name)\n\nfor i, row in tqdm(df_test.iterrows()):\n    distances, indices = tree.query(np.deg2rad(np.c_[row['latitude'], row['longitude']]), k = n)\n    poi = []\n    for d, j in zip(distances[0], indices[0]):\n        if d <= max_dist_cat and row['categories'] != '__NAN__' and (row['categories'] in df_test.categories.iloc[j] or df_test.categories.iloc[j] in row['categories']):\n            poi.append(df_test.id.iloc[j])\n        elif d <= max_dist_name and row['name'] != '__NAN__' and (row['name'].lower() == df_test.name.iloc[j].lower()):\n            poi.append(df_test.id.iloc[j])\n        if d > max_dist or len(poi) >= max_poi:\n            break\n\n    if len(poi) == 0:\n        pois_out.append(row['id'])\n    else:\n        pois_out.append(' '.join(poi))","metadata":{"id":"aSKtQRLE0RxJ","outputId":"df22df6e-2686-4eff-9f53-f1e07a1d7816","execution":{"iopub.status.busy":"2022-05-19T06:04:18.474552Z","iopub.execute_input":"2022-05-19T06:04:18.474885Z","iopub.status.idle":"2022-05-19T06:04:18.502997Z","shell.execute_reply.started":"2022-05-19T06:04:18.474842Z","shell.execute_reply":"2022-05-19T06:04:18.501982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Show matches\nsample_submission.matches = pois_out\nsample_submission.head()","metadata":{"id":"tZLLWPl60RxJ","outputId":"7665e4e9-7b92-40d0-a46b-d3e64b33dd93","execution":{"iopub.status.busy":"2022-05-19T06:04:18.504728Z","iopub.execute_input":"2022-05-19T06:04:18.505912Z","iopub.status.idle":"2022-05-19T06:04:18.51828Z","shell.execute_reply.started":"2022-05-19T06:04:18.505869Z","shell.execute_reply":"2022-05-19T06:04:18.517361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Copy output to csv file.\nsample_submission.to_csv('submission.csv', index=False)","metadata":{"id":"M3K6srkC0RxK","execution":{"iopub.status.busy":"2022-05-19T06:04:18.520038Z","iopub.execute_input":"2022-05-19T06:04:18.52084Z","iopub.status.idle":"2022-05-19T06:04:18.52865Z","shell.execute_reply.started":"2022-05-19T06:04:18.520792Z","shell.execute_reply":"2022-05-19T06:04:18.527737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##1. Partitioning of data\nTo build the model, the pairs.csv file is used to be split into training and test set. This is done by the train_test_split function from scikitlearn. For the BallTree model, the longtitude and latitude in the test dataset is used to construct the ball tree. A query will then be exceuted with the test dataset and return two arrays which consist of the distances and indices of the neighboring locations. The indices is then used to match the correct locations.\n\n##2. Model selection\nThe model selected to solve the problem is by using Random Forest Classifier which can be used to maintain accuracy of large propotion of data. Other than that, the team also tried to use the BallTree model. By using BallTree from the sklearn.neighbors library, it can be used to organise the points in a multi-dimensional space and assigned to the tree variable. It divides points based on radial distances to a centre. This is useful to solve our problem as it can approximately determine the actual distance between coordinates, which can be used to find the matches in location.\n\n##3. Model Training\nFor the Random Forest Classifier model, we have selected several features to be trained after changing the attributes that contained string values to numerical values so that it can be fitted into the model. For the BallTree model, the model is trained by using the latitute and longtitude given in the test dataset to construct the ball tree model. The coordinates are transformed from degree to radian using the deg2rad function as Haversine distance is used in the BallTree function.\n\n##4. Attribute that have greatest effect on matching result \nIt can be seen that the attributes that have the greatest effect on the matching results which we used to train the model includes Country, Latitude, Longtitude, Name and Category. For the BallTree model, we used the longtitude and latitude from the test dataset.","metadata":{"id":"1cHvrVyMB3Tm"}},{"cell_type":"markdown","source":"<!-- ### Explain: \n1. how the data is partitioned\n2. how the model is chosen\n3. how the model is trained\n4. the attributes that have the greatest effect on the matching results\n\n\n1. The longtitude and latitude in the train dataset is used to construct the ball tree. A query will then be exceuted with the test dataset and return two arrays which consist of the distances and indices of the neighboring locations. The indices is then used to match the correct locations.\n\n2. The model is chosen as it is able to deal with large datasets. In our case, a large test set will be fitted into our model to be tested. By using BallTree from the sklearn.neighbors library, it can be used to organise the points in a multi-dimensional space and assigned to the tree variable. It divides points based on radial distances to a centre. This is useful to solve our problem as it can approximately determine the actual distance between coordinates, which can be used to find the matches in location.\n\n3. The model is trained by using the latitute and longtitude given in the test dataset to construct the ball tree model. The coordinates are transformed from degree to radian using the deg2rad function as Haversine distance is used in the BallTree function.\n\n4. The attributes that have the greatest effect on the matching results are the longtitude and latitude. -->","metadata":{"id":"nUkarS38uYXW"}},{"cell_type":"markdown","source":"# 5. Model Evaluation","metadata":{"id":"msbLJnmOwhzT"}},{"cell_type":"markdown","source":"### Describe the process of model evaluation","metadata":{"id":"Ll-rnRKGDQ0k"}},{"cell_type":"code","source":"# Show your code here (Step by Step) \n# Comment each step in your code","metadata":{"id":"w1H-8vGMwhzU","execution":{"iopub.status.busy":"2022-05-19T06:04:18.530204Z","iopub.execute_input":"2022-05-19T06:04:18.530681Z","iopub.status.idle":"2022-05-19T06:04:18.539338Z","shell.execute_reply.started":"2022-05-19T06:04:18.530639Z","shell.execute_reply":"2022-05-19T06:04:18.538323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import library\nfrom sklearn.metrics import jaccard_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix","metadata":{"id":"8j3yakQQ1CIx","execution":{"iopub.status.busy":"2022-05-19T06:04:18.540991Z","iopub.execute_input":"2022-05-19T06:04:18.541477Z","iopub.status.idle":"2022-05-19T06:04:18.552445Z","shell.execute_reply.started":"2022-05-19T06:04:18.541434Z","shell.execute_reply":"2022-05-19T06:04:18.551291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# use the X_test feature to predict the value of y\ny_pred = model.predict(X_test)","metadata":{"id":"j2THpAJ8X0kW","execution":{"iopub.status.busy":"2022-05-19T06:04:18.554536Z","iopub.execute_input":"2022-05-19T06:04:18.555097Z","iopub.status.idle":"2022-05-19T06:04:20.581919Z","shell.execute_reply.started":"2022-05-19T06:04:18.55505Z","shell.execute_reply":"2022-05-19T06:04:20.581014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Since the submissions are evaluated by the mean Intersection over Union, which is Jaccard score\n# Also include the jaccard_score in the model evaluation\n\niou_score = jaccard_score(y_test, y_pred)\niou_score","metadata":{"id":"l6VW4Do7X0mh","outputId":"5484e771-5e68-4c06-c276-a13c3b7d4753","execution":{"iopub.status.busy":"2022-05-19T06:04:20.583202Z","iopub.execute_input":"2022-05-19T06:04:20.583427Z","iopub.status.idle":"2022-05-19T06:04:20.643921Z","shell.execute_reply.started":"2022-05-19T06:04:20.5834Z","shell.execute_reply":"2022-05-19T06:04:20.642984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display the classification report to show the accuracy of the model\nprint(classification_report(y_test, y_pred))","metadata":{"id":"xWWWNn4j1CRk","outputId":"c2db9b7e-fa07-4147-faff-67d2268be9c2","execution":{"iopub.status.busy":"2022-05-19T06:04:20.645033Z","iopub.execute_input":"2022-05-19T06:04:20.645247Z","iopub.status.idle":"2022-05-19T06:04:20.881732Z","shell.execute_reply.started":"2022-05-19T06:04:20.645221Z","shell.execute_reply":"2022-05-19T06:04:20.880877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display the confusion matrix to show how many observation it correctly predicted\n\ncm = pd.DataFrame(confusion_matrix(y_test, y_pred))\ncm","metadata":{"id":"LCjP-0-Qw9fj","outputId":"60cc9cd2-1b72-4be6-a3d7-ac95f0ed22d7","execution":{"iopub.status.busy":"2022-05-19T06:04:20.883052Z","iopub.execute_input":"2022-05-19T06:04:20.883265Z","iopub.status.idle":"2022-05-19T06:04:20.917577Z","shell.execute_reply.started":"2022-05-19T06:04:20.883239Z","shell.execute_reply":"2022-05-19T06:04:20.91666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Explain: \n1. the performance of the model created\n* The model used above is Random Forest model. From the IoU score, we know that the similarity of y_pred and y_test is not high. And from classification report, we can tell the model has good performance, it achieves around 79% of accuracy. By looking at the Confusion Matrix, although it correctly predicted the True Positive and True Negative, still a lot of observations are wrongly labeled.\n\n\n2. how the model can be used to predict or match the POIs accurately\n* Our model takes in the features to train the relationship between features and match column. Our model do not directly predict or match the POI, instead if the model predicted the match output is true, that's mean the POI is accurate and correctly predicted.","metadata":{"id":"vqVkEweDDGPt"}},{"cell_type":"markdown","source":"# 6. Model Validation (Challenge)","metadata":{"id":"X0R14MMrwhzU"}},{"cell_type":"markdown","source":"### Describe the process of model validation","metadata":{"id":"mIIKoXHqE_gs"}},{"cell_type":"code","source":"# Show your code here (Step by Step) \n# Comment each step in your code","metadata":{"id":"Ie2uPxmBwhzV","execution":{"iopub.status.busy":"2022-05-19T06:04:20.919335Z","iopub.execute_input":"2022-05-19T06:04:20.919664Z","iopub.status.idle":"2022-05-19T06:04:20.924188Z","shell.execute_reply.started":"2022-05-19T06:04:20.919621Z","shell.execute_reply":"2022-05-19T06:04:20.923306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import KFold, cross_val_score","metadata":{"id":"eTMvGobMIKdV","execution":{"iopub.status.busy":"2022-05-19T06:04:20.925337Z","iopub.execute_input":"2022-05-19T06:04:20.925574Z","iopub.status.idle":"2022-05-19T06:04:20.936373Z","shell.execute_reply.started":"2022-05-19T06:04:20.925545Z","shell.execute_reply":"2022-05-19T06:04:20.935727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Perform K-Fold cross validation\nkf=KFold(n_splits=10)\nscore=cross_val_score(model,X_test,y_test,cv=kf)\nprint(\"Cross Validation Scores are {}\".format(score))\nprint(\"Average Cross Validation score :{}\".format(score.mean()))","metadata":{"id":"e9Jg4mB9IKm0","outputId":"8524e809-cc05-4d90-b596-0949dee98daf","execution":{"iopub.status.busy":"2022-05-19T06:04:20.937969Z","iopub.execute_input":"2022-05-19T06:04:20.938302Z","iopub.status.idle":"2022-05-19T06:07:06.193796Z","shell.execute_reply.started":"2022-05-19T06:04:20.938259Z","shell.execute_reply":"2022-05-19T06:07:06.192743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Explain: \n###1. The Cross-Validation Approach\n* The cross-validation approach I applied here is K-fold, the number of folds is set to 10, and using the random forest model to do the model validation. The dataset applied here is the test set after getting split from pairs.csv.\n\n###2. The matching or predictive performance of the model created\n*  The average cross validation score of the random forest model is around 0.76. The model performance is decent, can be better if we have more useful features and less missing values.","metadata":{"id":"PRqbbfTwFCcs"}},{"cell_type":"markdown","source":"# 7. Discussion","metadata":{"id":"mz5poUpZwhzV"}},{"cell_type":"markdown","source":"### Identify:\n###1. The factors that have significant influences on location matching\n- The factors that infuences the location matching significantly are the latitude and longitude attributes.\n\n###2. Any interesting observation from this challenge\n- From this challenge, we know that the commercial points-of-interest (POI) is a immportant information to business. By knowing each shop's POI, we know that which category of shops have better place to set up their shops. We use longitude and latitude to calculate the POI, and see if it matches the other shop's POI, then the business owner can find the same POI to set up their shop.","metadata":{"id":"cWTrKY3TFHaf"}},{"cell_type":"markdown","source":"###Explain:\n###1. The limitations and weaknesses of the modelling approach\n- Random Forest Classification model: The pais dataset contained many data with null values, which will cause the prediction to be inaccurate.\n- BallTree model: The accuracy score cannot be obtained using our validation and evaluation methods used.\n\n###2. The steps taken to improve the matching accuracy in your modelling approach\n- From the pairs dataset, we have chosen only a few attributes to be used and dropped the attributes that are of no use. This will increase the accuracy as there are less missing values.","metadata":{"id":"l02CAd3bFu1a"}},{"cell_type":"markdown","source":"### Elaborate:\n###1. The experience in participating in a Kaggle challenge\n- The experience in participating in this particular Kaggle challenge is very interesting. On the competition page, there are many different code posted by other data scientists or users which can help one another to come up with ideas to solve the problem. The team has seen how many other data scientist develop their machine learning models.\n\n###2. The discussion and submission score on Kaggle (include the screenshot or link to your submission here)\n\n###3. The improvements that need to be done in order to win the challenge\n- We should increase our knowledge by performing more tasks related to data science. This can help us to gain more experience in participating in a real life data science problem. Our model should also be able to return a high accuracy to win this challenge.","metadata":{"id":"L0xNjqsvFy7q"}},{"cell_type":"markdown","source":"# Team contribution","metadata":{"id":"kr6c-sf9whzW"}},{"cell_type":"markdown","source":"\n##Participation Percentage:\n\n##(1) Justin Liu Shan Wei (50%)\nTasks: \n- Introduction\n- Data collection and understanding\n- Exploratory Data Analysis\n- Model Building\n- Discussion\n\n##(2) Lim Zong Xin (50%)\nTasks: \n- Model building\n- Model evaluation\n- Model validation\n- Discussion","metadata":{"id":"tpCkTCsTGBpZ"}}]}