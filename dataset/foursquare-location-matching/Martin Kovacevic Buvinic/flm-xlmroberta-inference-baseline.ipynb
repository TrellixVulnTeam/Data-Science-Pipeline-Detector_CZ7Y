{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ===============================================================================\n# Library\n# ===============================================================================\nimport os\nimport gc\nimport re\nimport joblib\nimport numpy as np\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\nimport random\nimport math\nfrom tqdm.notebook import tqdm\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.preprocessing import StandardScaler\nimport tensorflow as tf\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras import mixed_precision\nfrom transformers import AutoTokenizer, TFAutoModel, AutoConfig\nimport Levenshtein\nimport difflib\n\n# ===============================================================================\n# Configurations\n# ===============================================================================\nclass CFG:\n    input_path = '../input/foursquare-location-matching/'\n    target = 'point_of_interest'\n    model = '../input/xlmroberta/xlm-roberta-base/'\n    tokenizer = AutoTokenizer.from_pretrained(model)\n    max_len = 150\n    seed = 42\n    batch_size = 32\n    target_size = 1\n    rounds = 6\n    n_neighbors = 7\n    best_thres = 0.45\n    \n# ===============================================================================\n# Read data\n# ===============================================================================\ndef read_data():\n    test = pd.read_csv(CFG.input_path + 'test.csv')\n    return test\n    \n# ===============================================================================\n# Generate data for test\n# ===============================================================================\ndef generate_test_data(df, rounds = 2, n_neighbors = 10, features = ['id', 'latitude', 'longitude']):\n    # Scale data for KNN\n    scaler = StandardScaler()\n    scaled_data = scaler.fit_transform(df[features[2:4]])\n    # Fit KNN and predict indices\n    knn_model = NearestNeighbors(\n        n_neighbors = n_neighbors, \n        radius = 1.0, \n        algorithm = 'kd_tree', \n        leaf_size = 30, \n        metric = 'minkowski', \n        p = 2, \n        n_jobs = -1\n    )\n    knn_model.fit(scaled_data)\n    indices = knn_model.kneighbors(scaled_data, return_distance = False)\n    # Create a new dataframe to slice faster\n    df_features = df[features]\n    # Create a dataset to store final results\n    dataset = []\n    # Iterate through each round and get generated data\n    for j in range(rounds):\n        # Create temporal dataset to store round data\n        tmp_dataset = []\n        # Iterate through each row\n        for k in tqdm(range(len(df))):\n            neighbors = list(indices[k])\n            # Remove self from neighbors if exist\n            try:\n                neighbors.remove(k)\n            except:\n                pass\n            # Use iterator as first indices\n            ind1 = k\n            # Select from the neighbor list the second indices\n            ind2 = neighbors[j]\n            # Check if indices are the same, they should not be the same\n            if ind1 == ind2:\n                print('Indices are the same, error')\n            # Slice features dataframe\n            tmp1 = df_features.loc[ind1]\n            tmp2 = df_features.loc[ind2]\n            # Concatenate, don't add target, this is the test set\n            tmp = np.concatenate([tmp1, tmp2], axis = 0)\n            tmp_dataset.append(tmp)  \n        # Transform tmp_dataset to a pd.DataFrame\n        tmp_dataset = pd.DataFrame(tmp_dataset, columns = [i + '_1' for i in features] + [i + '_2' for i in features])\n        # Append round\n        dataset.append(tmp_dataset)\n    # Concatenate rounds to get final dataset\n    dataset = pd.concat(dataset, axis = 0)\n    # Remove duplicates\n    dataset.drop_duplicates(inplace = True)\n    # Reset index\n    dataset.reset_index(drop = True, inplace = True)\n    col_64 = list(dataset.dtypes[dataset.dtypes == np.float64].index)\n    for col in col_64:\n        dataset[col] = dataset[col].astype(np.float32)\n    return df, dataset\n\n# ===============================================================================\n# Get manhattan distance\n# ===============================================================================\ndef manhattan(lat1, long1, lat2, long2):\n    return np.abs(lat2 - lat1) + np.abs(long2 - long1)\n\n# ===============================================================================\n# Get haversine distance\n# ===============================================================================\ndef vectorized_haversine(lats1, lats2, longs1, longs2):\n    radius = 6371\n    dlat=np.radians(lats2 - lats1)\n    dlon=np.radians(longs2 - longs1)\n    a = np.sin(dlat/2) * np.sin(dlat/2) + np.cos(np.radians(lats1)) \\\n        * np.cos(np.radians(lats2)) * np.sin(dlon/2) * np.sin(dlon/2)\n    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n    d = radius * c\n    return d\n\n# ===============================================================================\n# Compute distances + Euclidean\n# ===============================================================================\ndef add_lat_lon_distance_features(df):\n    lat1 = df['latitude_1']\n    lat2 = df['latitude_2']\n    lon1 = df['longitude_1']\n    lon2 = df['longitude_2']\n    df['latdiff'] = (lat1 - lat2)\n    df['londiff'] = (lon1 - lon2)\n    df['manhattan'] = manhattan(lat1, lon1, lat2, lon2)\n    df['euclidean'] = (df['latdiff'] ** 2 + df['londiff'] ** 2) ** 0.5\n    df['haversine'] = vectorized_haversine(lat1, lat2, lon1, lon2)\n    col_64 = list(df.dtypes[df.dtypes == np.float64].index)\n    for col in col_64:\n        df[col] = df[col].astype(np.float32)\n    return df\n\n# ===============================================================================\n# Compute distances for categorical features\n# ===============================================================================\ndef get_distance_cat(df, column):\n    geshs = []\n    levens = []\n    jaros = []\n    for str1, str2 in df[[column + '_1', column + '_2']].values.astype(str):\n        if str1==str1 and str2==str2:\n            geshs.append(difflib.SequenceMatcher(None, str1, str2).ratio())\n            levens.append(Levenshtein.distance(str1, str2))\n            jaros.append(Levenshtein.jaro_winkler(str1, str2))\n        else:\n            geshs.append(-1)\n            levens.append(-1)\n            jaros.append(-1)\n    df1 = pd.DataFrame({\n        f\"{column}_geshs\": geshs,\n        f\"{column}_levens\": levens,\n        f\"{column}_jaros\": jaros,\n        })\n    if column not in ['country', 'phone', 'zip']:\n        df1[f\"{column}_len_1\"] = df[column + '_1'].astype(str).map(len)\n        df1[f\"{column}_len_2\"] = df[column + '_2'].astype(str).map(len)\n        df1[f\"{column}_nlevens\"] = df1[f\"{column}_levens\"] / df1[[f\"{column}_len_1\", f\"{column}_len_2\"]].max(axis = 1)\n    col_64 = list(df1.dtypes[df1.dtypes == np.float64].index)\n    for col in col_64:\n        df1[col] = df1[col].astype(np.float32)\n    df = pd.concat([df, df1], axis = 1)\n    return df\n\n# ===============================================================================\n# Add '[SEP]' token to all the categorical features we want to encode\n# ===============================================================================\ndef add_sep_token(df):\n    # Before concatenation, fill NAN with unknown\n    df.fillna('unknown', inplace = True)\n    df['text'] = df['name_1'] + '[SEP]' + df['address_1'] + '[SEP]' + df['city_1'] + '[SEP]' \\\n    + df['state_1'] + '[SEP]' + df['country_1'] + '[SEP]' + df['url_1'] + '[SEP]' + df['categories_1'] + '[SEP]' \\\n    + df['name_2'] + '[SEP]' + df['address_2'] + '[SEP]' + df['city_2'] + '[SEP]' \\\n    + df['state_2'] + '[SEP]' + df['country_2'] + '[SEP]' + df['url_2'] + '[SEP]' + df['categories_2']\n    return df\n\n# ===============================================================================\n# Create model \n# ===============================================================================\ndef build_model(cfg):\n    transformer = TFAutoModel.from_pretrained(cfg.model, from_pt = True)\n    input_word_ids = tf.keras.layers.Input(shape = (cfg.max_len, ), dtype = tf.int32, name = 'input_word_ids')\n    inp_num = tf.keras.layers.Input(shape = (cfg.num_shape, ), dtype = tf.float32, name = 'num_inputs')\n    last_hidden_state = transformer(input_word_ids)['last_hidden_state']\n    last_hidden_state_avg_pool = tf.keras.layers.GlobalAveragePooling1D()(last_hidden_state)\n    last_hidden_state_avg_pool = tf.keras.layers.Dropout(0.40)(last_hidden_state_avg_pool)\n    x = tf.keras.layers.Dense(1024, activation = 'relu')(inp_num)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout(0.15)(x)\n    x = tf.keras.layers.Dense(512, activation = 'relu')(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout(0.15)(x)\n    x = tf.keras.layers.Concatenate()([last_hidden_state_avg_pool, x])\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dense(256, activation = 'relu')(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout(0.20)(x)\n    output = tf.keras.layers.Dense(cfg.target_size, activation = 'sigmoid')(x)\n    model = tf.keras.models.Model(inputs = [input_word_ids, inp_num], outputs = [output])\n    return model\n\n# ====================================================\n# Prepare input using tokenizer\n# ====================================================\ndef prepare_input(data):\n    inputs_ids = []\n    for text in tqdm(data, total = len(data)):\n        inputs = CFG.tokenizer(\n            text,\n            add_special_tokens = True,\n            padding = 'max_length',\n            truncation = True,\n            return_offsets_mapping = False,\n            max_length = CFG.max_len,\n            return_token_type_ids = False,\n            return_attention_mask = False,\n        )\n        inputs_ids.append(inputs['input_ids'])\n    return np.array(inputs_ids)\n\n# ===============================================================================\n# Inference\n# ===============================================================================\ndef inference(test_dataset, test):\n    ds_len = len(test_dataset)\n    # Get numeric features and text features\n    ignore_cols = ['id_1', 'id_2', 'match', 'text']\n    num_features = [col for col in test_dataset.columns if col not in ignore_cols]\n    CFG.num_shape = len(num_features)\n    # Build model\n    model = build_model(CFG)\n    # Load weights\n    model.load_weights('../input/flm-models/baseline.h5')\n    # Use a for loop to avoid memory problem (we could do this with a generator also)\n    predictions = []\n    for i in range(5):\n        x_test_num = test_dataset[num_features].iloc[int(i * ds_len / 5) : int((i + 1) * ds_len / 5)]\n        x_test_text = test_dataset['text'].iloc[int(i * ds_len / 5) : int((i + 1) * ds_len / 5)]\n        # Scale numeric features\n        scaler = StandardScaler()\n        x_test_num = scaler.fit_transform(x_test_num)\n        # Tokenize text\n        x_test_text = prepare_input(x_test_text.tolist())\n        # Create a list to predict\n        x_test = [x_test_text, x_test_num]\n        pred = model.predict(x_test, batch_size = CFG.batch_size).astype(np.float32).reshape(-1)\n        print(pred.shape)\n        predictions.append(pred)\n    # Release memory\n    del x_test_num, x_test_text, x_test, pred\n    gc.collect()\n    test_dataset['predictions'] = np.concatenate(predictions, axis = 0)\n    # Slice val_dataset with only the required columns\n    test_dataset = test_dataset[['id_1', 'id_2', 'predictions']]\n    # Copy val dataset and swap ids so we have A, B -> B, A\n    test_dataset_c = test_dataset.copy()\n    id1 = test_dataset_c['id_1']\n    id2 = test_dataset_c['id_2']\n    test_dataset_c['id1'] = id2\n    test_dataset_c['id2'] = id1\n    test_dataset = pd.concat([test_dataset, test_dataset_c], axis = 0, ignore_index = True)\n    del id1, id2, test_dataset_c\n    gc.collect()\n    test_dataset['match_prediction'] = np.where(test_dataset['predictions'] >= CFG.best_thres, 1, 0)\n    # Filter all the matches and get ids\n    predictions = test_dataset[test_dataset['match_prediction'] == 1].groupby(['id_1'])['id_2'].apply(lambda x: list(np.unique(x))).reset_index()\n    predictions['id_2'] = predictions['id_2'].apply(lambda x: ' '.join(x))\n    # Add self\n    predictions['id_2'] = predictions['id_1'] + ' ' + predictions['id_2']\n    predictions.columns = ['id', 'prediction']\n    # Get all the ids that did not found a match\n    not_in = test[~test['id'].isin(predictions['id'])]['id'].values\n    # Create a dataframe with this ids\n    only_one = pd.DataFrame({'id': not_in, 'prediction': not_in})\n    # Concatenate\n    predictions = pd.concat([predictions, only_one], axis = 0, ignore_index = True)\n    # Change columns name to prediction format\n    predictions.columns = ['id', 'matches']\n    # Save submission to disk\n    predictions.to_csv('submission.csv', index = False)\n\n# Read data\ntest = read_data()\nif len(test) == 5:\n    CFG.rounds = 4\n    CFG.n_neighbors = 5\n    \n# Get initial features\nfeatures = [col for col in test.columns if col not in [CFG.target]]\n# Generate test data\ntest, test_dataset = generate_test_data(test, rounds = CFG.rounds, n_neighbors = CFG.n_neighbors, features = features)\n# Numerical Feature Engineering\ntest_dataset = add_lat_lon_distance_features(test_dataset)\n# Categorical Feature Engineering\ncat_columns = ['name', 'address', 'city', 'state', 'zip', 'country', 'url', 'phone', 'categories']\npair_cat_columns = [col + '_1' for col in cat_columns] + [col + '_2' for col in cat_columns]\nfor col in cat_columns:\n    test_dataset = get_distance_cat(test_dataset, col)\n# Get text column\ntest_dataset = add_sep_token(test_dataset)\n# Drop unwanted columns\ntest_dataset.drop(pair_cat_columns, axis = 1, inplace = True)\ninference(test_dataset, test)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-11T16:12:41.721472Z","iopub.execute_input":"2022-05-11T16:12:41.721835Z","iopub.status.idle":"2022-05-11T16:13:04.744672Z","shell.execute_reply.started":"2022-05-11T16:12:41.721787Z","shell.execute_reply":"2022-05-11T16:13:04.743864Z"},"trusted":true},"execution_count":null,"outputs":[]}]}