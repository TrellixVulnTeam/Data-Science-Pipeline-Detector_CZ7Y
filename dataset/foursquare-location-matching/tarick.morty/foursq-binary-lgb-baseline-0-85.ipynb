{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"The goal of this notebook is to show how we can further improve the simple LGB solution with more neighbouring candidates some more basic features.  \nIt takes around 6 hours for inference and scores 0.852 on public LB.  \nIt is based on the works of this awesome notebook - https://www.kaggle.com/code/guoyonfan/binary-lgb-baseline-0-834","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nimport joblib\nimport os\nimport gc\nimport time\nimport random\nimport Levenshtein\nimport difflib\nimport multiprocessing\nimport pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nfrom tqdm.auto import tqdm\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.neighbors import NearestNeighbors","metadata":{"papermill":{"duration":2.383827,"end_time":"2022-05-26T16:41:31.188301","exception":false,"start_time":"2022-05-26T16:41:28.804474","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-05T17:45:50.697209Z","iopub.execute_input":"2022-06-05T17:45:50.698369Z","iopub.status.idle":"2022-06-05T17:45:53.212718Z","shell.execute_reply.started":"2022-06-05T17:45:50.698229Z","shell.execute_reply":"2022-06-05T17:45:53.21214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAIN_FEATURES = ['kdist', 'kneighbors', 'kdist_country', 'kneighbors_country', 'name_sim', 'name_gesh','name_leven', \n                'name_jaro', 'name_lcs', 'name_len_diff', 'name_nleven', 'name_nlcsk', 'name_nlcs', 'address_sim', \n                'address_gesh', 'address_leven', 'address_jaro', 'address_lcs', 'address_len_diff', 'address_nleven', \n                'address_nlcsk', 'address_nlcs', 'city_gesh', 'city_leven', 'city_jaro', 'city_lcs', 'city_len_diff', \n                'city_nleven', 'city_nlcsk', 'city_nlcs', 'state_sim', 'state_gesh', 'state_leven', 'state_jaro', \n                'state_lcs', 'state_len_diff', 'state_nleven', 'state_nlcsk', 'state_nlcs', 'zip_gesh', 'zip_leven', \n                'zip_jaro', 'zip_lcs', 'url_sim', 'url_gesh', 'url_leven', 'url_jaro', 'url_lcs', 'url_len_diff', \n                'url_nleven', 'url_nlcsk', 'url_nlcs', 'phone_gesh', 'phone_leven', 'phone_jaro', 'phone_lcs', \n                'categories_sim', 'categories_gesh', 'categories_leven', 'categories_jaro', 'categories_lcs', \n                'categories_len_diff', 'categories_nleven', 'categories_nlcsk', 'categories_nlcs', 'country_sim', \n                'country_gesh', 'country_leven', 'country_nleven', 'kdist_diff', 'kneighbors_mean', \n                'name_1', 'categories_1', 'address_1', 'state_1', 'url_1', 'country_1', 'name_2',\n                'categories_2', 'address_2', 'state_2', 'url_2', 'country_2', 'sim_sum', 'gesh_sum',\n                'leven_sum', 'jaro_sum', 'lcs_sum', 'sim_std', 'gesh_std', 'leven_std',\n                'jaro_std', 'lcs_std', 'info_power_1', 'info_power_2', 'info_diff']","metadata":{"papermill":{"duration":0.035003,"end_time":"2022-05-26T16:41:31.24117","exception":false,"start_time":"2022-05-26T16:41:31.206167","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-05T17:45:53.214208Z","iopub.execute_input":"2022-06-05T17:45:53.214566Z","iopub.status.idle":"2022-06-05T17:45:53.221578Z","shell.execute_reply.started":"2022-06-05T17:45:53.214538Z","shell.execute_reply":"2022-06-05T17:45:53.220609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NUM_NEIGHBOR = 30\nSEED = 2022\nTHRESHOLD = 0.5\nNUM_SPLIT = 5\nfeat_columns = ['name', 'address', 'city','state', 'zip', 'url', 'phone', 'categories', 'country']\nvec_columns = ['name', 'categories', 'address', 'state', 'url', 'country']\n\ndef seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \nseed_everything(SEED)","metadata":{"papermill":{"duration":0.027006,"end_time":"2022-05-26T16:41:31.285501","exception":false,"start_time":"2022-05-26T16:41:31.258495","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-05T17:45:53.222685Z","iopub.execute_input":"2022-06-05T17:45:53.222877Z","iopub.status.idle":"2022-06-05T17:45:53.245039Z","shell.execute_reply.started":"2022-06-05T17:45:53.222853Z","shell.execute_reply":"2022-06-05T17:45:53.243967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%load_ext Cython","metadata":{"papermill":{"duration":1.153015,"end_time":"2022-05-26T16:41:32.4565","exception":false,"start_time":"2022-05-26T16:41:31.303485","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-05T17:45:53.247086Z","iopub.execute_input":"2022-06-05T17:45:53.247306Z","iopub.status.idle":"2022-06-05T17:45:54.653009Z","shell.execute_reply.started":"2022-06-05T17:45:53.247278Z","shell.execute_reply":"2022-06-05T17:45:54.652164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%cython\ndef LCS(str S, str T):\n    cdef int i, j\n    cdef list dp = [[0] * (len(T) + 1) for _ in range(len(S) + 1)]\n    for i in range(len(S)):\n        for j in range(len(T)):\n            dp[i + 1][j + 1] = max(dp[i][j] + (S[i] == T[j]), dp[i + 1][j], dp[i][j + 1], dp[i + 1][j + 1])\n    return dp[len(S)][len(T)]","metadata":{"papermill":{"duration":1.449139,"end_time":"2022-05-26T16:41:33.924155","exception":false,"start_time":"2022-05-26T16:41:32.475016","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-05T17:45:54.654282Z","iopub.execute_input":"2022-06-05T17:45:54.654662Z","iopub.status.idle":"2022-06-05T17:45:56.040125Z","shell.execute_reply.started":"2022-06-05T17:45:54.65463Z","shell.execute_reply":"2022-06-05T17:45:56.038318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def post_process(df):\n    id2match = dict(zip(df['id'].values, df['matches'].str.split()))\n\n    for base, match in df[['id', 'matches']].values:\n        match = match.split()\n        if len(match) == 1:        \n            continue\n\n        for m in match:\n            if base not in id2match[m]:\n                id2match[m].append(base)\n    df['matches'] = df['id'].map(id2match).map(' '.join)\n    return df ","metadata":{"papermill":{"duration":0.028681,"end_time":"2022-05-26T16:41:33.970686","exception":false,"start_time":"2022-05-26T16:41:33.942005","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-05T17:45:56.043752Z","iopub.execute_input":"2022-06-05T17:45:56.044135Z","iopub.status.idle":"2022-06-05T17:45:56.053753Z","shell.execute_reply.started":"2022-06-05T17:45:56.044087Z","shell.execute_reply":"2022-06-05T17:45:56.052631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def recall_knn(df, Neighbors = 10):\n    print('Start knn grouped by country')\n    train_df_country = []\n    for country, country_df in tqdm(df.groupby('country')):\n        country_df = country_df.reset_index(drop = True)\n\n        neighbors = min(len(country_df), Neighbors)\n        knn = KNeighborsRegressor(n_neighbors = neighbors,\n                                    metric = 'haversine',\n                                    n_jobs = -1)\n        knn.fit(country_df[['latitude','longitude']], country_df.index)\n        dists, nears = knn.kneighbors(country_df[['latitude', 'longitude']], \n                                        return_distance = True)\n\n        for k in range(neighbors):            \n            cur_df = country_df[['id']]\n            cur_df['match_id'] = country_df['id'].values[nears[:, k]]\n            cur_df['kdist_country'] = dists[:, k]\n            cur_df['kneighbors_country'] = k\n            \n            train_df_country.append(cur_df)\n    train_df_country = pd.concat(train_df_country)\n    \n    print('Start knn')\n    train_df = []\n    knn = NearestNeighbors(n_neighbors = Neighbors)\n    knn.fit(df[['latitude','longitude']], df.index)\n    dists, nears = knn.kneighbors(df[['latitude','longitude']])\n    \n    for k in range(Neighbors):            \n        cur_df = df[['id']]\n        cur_df['match_id'] = df['id'].values[nears[:, k]]\n        cur_df['kdist'] = dists[:, k]\n        cur_df['kneighbors'] = k\n        train_df.append(cur_df)\n    \n    train_df = pd.concat(train_df)\n    train_df = train_df.merge(train_df_country,\n                                 on = ['id', 'match_id'],\n                                 how = 'outer')\n    del train_df_country\n    \n    return train_df","metadata":{"papermill":{"duration":0.033743,"end_time":"2022-05-26T16:41:34.021778","exception":false,"start_time":"2022-05-26T16:41:33.988035","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-05T17:45:56.055406Z","iopub.execute_input":"2022-06-05T17:45:56.056001Z","iopub.status.idle":"2022-06-05T17:45:56.073651Z","shell.execute_reply.started":"2022-06-05T17:45:56.055957Z","shell.execute_reply":"2022-06-05T17:45:56.072711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def add_features(df):    \n    for col in tqdm(feat_columns):       \n        if col in vec_columns:\n            tv_fit = tfidf_d[col]\n            indexs = [id2index_d[i] for i in df['id']]\n            match_indexs = [id2index_d[i] for i in df['match_id']]                    \n            df[f'{col}_sim'] = np.array(tv_fit[indexs].multiply(tv_fit[match_indexs]).sum(axis = 1)).ravel()\n        \n        col_values = data.loc[df['id']][col].values.astype(str)\n        matcol_values = data.loc[df['match_id']][col].values.astype(str)\n        \n        geshs = []\n        levens = []\n        jaros = []\n        lcss = []\n        for s, match_s in zip(col_values, matcol_values):\n            if s != 'nan' and match_s != 'nan':                    \n                geshs.append(difflib.SequenceMatcher(None, s, match_s).ratio())\n                levens.append(Levenshtein.distance(s, match_s))\n                jaros.append(Levenshtein.jaro_winkler(s, match_s))\n                lcss.append(LCS(str(s), str(match_s)))\n            else:\n                geshs.append(np.nan)\n                levens.append(np.nan)\n                jaros.append(np.nan)\n                lcss.append(np.nan)\n        \n        df[f'{col}_gesh'] = geshs\n        df[f'{col}_leven'] = levens\n        df[f'{col}_jaro'] = jaros\n        df[f'{col}_lcs'] = lcss\n        \n        if col not in ['phone', 'zip']:\n            df[f'{col}_len'] = list(map(len, col_values))\n            df[f'match_{col}_len'] = list(map(len, matcol_values)) \n            df[f'{col}_len_diff'] = np.abs(df[f'{col}_len'] - df[f'match_{col}_len'])\n            df[f'{col}_nleven'] = df[f'{col}_leven'] / \\\n                                    df[[f'{col}_len', f'match_{col}_len']].max(axis = 1)\n            \n            df[f'{col}_nlcsk'] = df[f'{col}_lcs'] / df[f'match_{col}_len']\n            df[f'{col}_nlcs'] = df[f'{col}_lcs'] / df[f'{col}_len']\n            \n            df = df.drop(f'{col}_len', axis = 1)\n            df = df.drop(f'match_{col}_len', axis = 1)\n            gc.collect()\n            \n    return df","metadata":{"papermill":{"duration":0.037438,"end_time":"2022-05-26T16:41:34.07689","exception":false,"start_time":"2022-05-26T16:41:34.039452","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-05T17:45:56.075621Z","iopub.execute_input":"2022-06-05T17:45:56.07598Z","iopub.status.idle":"2022-06-05T17:45:56.099141Z","shell.execute_reply.started":"2022-06-05T17:45:56.075934Z","shell.execute_reply":"2022-06-05T17:45:56.097694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tdata = pd.read_csv('../input/foursquare-location-matching/test.csv')\ntdata['name'] = tdata['name'].isna().astype(int)\ntdata['categories'] = tdata['categories'].isna().astype(int)\ntdata['address'] = tdata['address'].isna().astype(int)\ntdata['state'] = tdata['state'].isna().astype(int)\ntdata['url'] = tdata['url'].isna().astype(int)\ntdata['country'] = tdata['country'].isna().astype(int)\n\ntdata = tdata[['id','name', 'categories', 'address', 'state', \n               'url', 'country']].drop_duplicates().reset_index(drop=True)","metadata":{"papermill":{"duration":0.061375,"end_time":"2022-05-26T16:41:34.155434","exception":false,"start_time":"2022-05-26T16:41:34.094059","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-05T17:45:56.101653Z","iopub.execute_input":"2022-06-05T17:45:56.102336Z","iopub.status.idle":"2022-06-05T17:45:56.148356Z","shell.execute_reply.started":"2022-06-05T17:45:56.102286Z","shell.execute_reply":"2022-06-05T17:45:56.147762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('../input/foursquare-location-matching/test.csv')\n\nif len(data) < 20:\n    data = pd.read_csv('../input/foursquare-location-matching/train.csv',\n                      nrows = 100)\n    data = data.drop('point_of_interest', axis = 1)\n    \nid2index_d = dict(zip(data['id'].values, data.index))\n\ntfidf_d = {}\nfor col in vec_columns:\n    tfidf = TfidfVectorizer()\n    tv_fit = tfidf.fit_transform(data[col].fillna('nan'))\n    tfidf_d[col] = tv_fit\n\nout_df = pd.DataFrame()\nout_df['id'] = data['id'].unique().tolist()\nout_df['match_id'] = out_df['id']\n\ntest_data = recall_knn(data, NUM_NEIGHBOR)\ndata = data.set_index('id')\n\nprint('Num of unique id: %s' % test_data['id'].nunique())\nprint('Num of test data: %s' % len(test_data))\nprint(test_data.sample(5))","metadata":{"papermill":{"duration":3.926058,"end_time":"2022-05-26T16:41:38.098683","exception":false,"start_time":"2022-05-26T16:41:34.172625","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-05T17:45:56.150249Z","iopub.execute_input":"2022-06-05T17:45:56.151318Z","iopub.status.idle":"2022-06-05T17:46:00.056958Z","shell.execute_reply.started":"2022-06-05T17:45:56.151257Z","shell.execute_reply":"2022-06-05T17:46:00.055842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def inference_lgbm(models, feat_df):\n    pred = np.array([model.predict_proba(feat_df) for model in models])\n    pred = np.mean(pred, axis=0)\n    return pred","metadata":{"papermill":{"duration":0.027175,"end_time":"2022-05-26T16:41:38.153049","exception":false,"start_time":"2022-05-26T16:41:38.125874","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-05T17:46:00.058873Z","iopub.execute_input":"2022-06-05T17:46:00.059183Z","iopub.status.idle":"2022-06-05T17:46:00.065805Z","shell.execute_reply.started":"2022-06-05T17:46:00.059146Z","shell.execute_reply":"2022-06-05T17:46:00.064488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NFOLDS = 5\nmodels = [joblib.load(f'../input/foursq-lgb-5-folds/lgb_binary{i}.pkl') for i in range(NFOLDS)]","metadata":{"papermill":{"duration":5.181456,"end_time":"2022-05-26T16:41:43.353027","exception":false,"start_time":"2022-05-26T16:41:38.171571","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-05T17:54:01.886181Z","iopub.execute_input":"2022-06-05T17:54:01.886427Z","iopub.status.idle":"2022-06-05T17:54:03.89089Z","shell.execute_reply.started":"2022-06-05T17:54:01.886404Z","shell.execute_reply":"2022-06-05T17:54:03.889475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Prediction\ncount = 0\nstart_row = 0\npred_df = pd.DataFrame()\nunique_id = test_data['id'].unique().tolist()\nnum_split_id = len(unique_id) // NUM_SPLIT\n\nfor k in range(1, NUM_SPLIT + 1):\n    print('Current split: %s' % k)\n    end_row = start_row + num_split_id\n    if k < NUM_SPLIT:\n        cur_id = unique_id[start_row : end_row]\n        cur_data = test_data[test_data['id'].isin(cur_id)]\n    else:\n        cur_id = unique_id[start_row: ]\n        cur_data = test_data[test_data['id'].isin(cur_id)]\n    \n    # add features & model prediction\n    cur_data = add_features(cur_data)\n    cur_data['kdist_diff'] = (cur_data['kdist'] - cur_data['kdist_country']) /\\\n                                cur_data['kdist_country']\n    cur_data['kneighbors_mean'] = cur_data[['kneighbors', 'kneighbors_country']].mean(axis = 1)\n    \n    cur_data['sim_sum'] = cur_data[[col for col in cur_data.columns if 'sim' in col]].sum(axis=1)\n    cur_data['gesh_sum'] = cur_data[[col for col in cur_data.columns if 'gesh' in col]].sum(axis=1)\n    cur_data['leven_sum'] = cur_data[[col for col in cur_data.columns if '_leven' in col]].sum(axis=1)\n    cur_data['jaro_sum'] = cur_data[[col for col in cur_data.columns if 'jaro' in col]].sum(axis=1)\n    cur_data['lcs_sum'] = cur_data[[col for col in cur_data.columns if '_lcs' in col]].sum(axis=1)\n\n    cur_data['sim_std'] = cur_data[[col for col in cur_data.columns if 'sim' in col]].std(axis=1)\n    cur_data['gesh_std'] = cur_data[[col for col in cur_data.columns if 'gesh' in col]].std(axis=1)\n    cur_data['leven_std'] = cur_data[[col for col in cur_data.columns if '_leven' in col]].std(axis=1)\n    cur_data['jaro_std'] = cur_data[[col for col in cur_data.columns if 'jaro' in col]].std(axis=1)\n    cur_data['lcs_std'] = cur_data[[col for col in cur_data.columns if '_lcs' in col]].std(axis=1)\n\n    cur_data = cur_data.merge(tdata, on='id', how='left')\n    cur_data = cur_data.merge(tdata, left_on='match_id', right_on='id', how='left', suffixes=['_1','_2'])\n    cur_data = cur_data.drop('id_2', axis=1).rename(columns={'id_1':'id'})\n\n    cur_data['info_power_1'] = cur_data[[col for col in cur_data.columns if '_1' in col]].lt(1).sum(axis=1)\n    cur_data['info_power_2'] = cur_data[[col for col in cur_data.columns if '_2' in col]].lt(1).sum(axis=1)\n    cur_data['info_diff'] = cur_data['info_power_1'] - cur_data['info_power_2']\n    \n    cur_data['pred'] = inference_lgbm(models, cur_data[TRAIN_FEATURES])[:,1]\n    \n    cur_pred_df = cur_data[cur_data['pred'] > THRESHOLD][['id', 'match_id']]\n    pred_df = pd.concat([pred_df, cur_pred_df])\n\n    start_row = end_row\n    count += len(cur_data)\n\n    del cur_data, cur_pred_df\n    gc.collect()\n    \nprint(count)","metadata":{"papermill":{"duration":9.098059,"end_time":"2022-05-26T16:41:52.473256","exception":false,"start_time":"2022-05-26T16:41:43.375197","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-05T17:54:04.241666Z","iopub.execute_input":"2022-06-05T17:54:04.242457Z","iopub.status.idle":"2022-06-05T17:54:14.34822Z","shell.execute_reply.started":"2022-06-05T17:54:04.242422Z","shell.execute_reply":"2022-06-05T17:54:14.347576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Submission    \nout_df = pd.concat([out_df, pred_df])\nout_df = out_df.groupby('id')['match_id'].\\\n                        apply(list).reset_index()\nout_df['matches'] = out_df['match_id'].apply(lambda x: ' '.join(set(x)))\nout_df = post_process(out_df)\nprint('Unique id: %s' % len(out_df))\nprint(out_df.head())\n\nout_df[['id', 'matches']].to_csv('submission.csv', index = False)","metadata":{"papermill":{"duration":0.050848,"end_time":"2022-05-26T16:41:52.550408","exception":false,"start_time":"2022-05-26T16:41:52.49956","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-05T17:54:14.349527Z","iopub.execute_input":"2022-06-05T17:54:14.349701Z","iopub.status.idle":"2022-06-05T17:54:14.372173Z","shell.execute_reply.started":"2022-06-05T17:54:14.349679Z","shell.execute_reply":"2022-06-05T17:54:14.371289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.023293,"end_time":"2022-05-26T16:41:52.598518","exception":false,"start_time":"2022-05-26T16:41:52.575225","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]}]}