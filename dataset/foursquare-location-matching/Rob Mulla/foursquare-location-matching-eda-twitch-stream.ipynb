{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Foursqare EDA and Starter Notebook\nThis notebook was created during a live stream on twitch. [Check out my channel here](https://www.twitch.tv/medallionstallion_)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"!pip install nb_black > /dev/null","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-22T01:56:00.565081Z","iopub.execute_input":"2022-04-22T01:56:00.565502Z","iopub.status.idle":"2022-04-22T01:56:11.800662Z","shell.execute_reply.started":"2022-04-22T01:56:00.565472Z","shell.execute_reply":"2022-04-22T01:56:11.799383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%load_ext lab_black\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom itertools import cycle\nimport plotly.express as px\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\nplt.style.use(\"ggplot\")\n\ncolor_pal = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\ncolor_cycle = cycle(plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"])","metadata":{"execution":{"iopub.status.busy":"2022-04-22T01:56:11.803074Z","iopub.execute_input":"2022-04-22T01:56:11.80394Z","iopub.status.idle":"2022-04-22T01:56:13.515409Z","shell.execute_reply.started":"2022-04-22T01:56:11.803866Z","shell.execute_reply":"2022-04-22T01:56:13.51442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading the data\nI made a parquet version of the training data for faster loading. As an example I'll load the csv versions and parquet versions so we can compare how long it takes to load. It actually didn't end up being that much faster, but still saves a few seconds.","metadata":{}},{"cell_type":"code","source":"%%time\ntrain = pd.read_csv('../input/foursquare-location-matching/train.csv')\ntest = pd.read_csv('../input/foursquare-location-matching/test.csv')\nss = pd.read_csv('../input/foursquare-location-matching/sample_submission.csv')\npairs = pd.read_csv('../input/foursquare-location-matching/pairs.csv')","metadata":{"execution":{"iopub.status.busy":"2022-04-22T01:56:13.516734Z","iopub.execute_input":"2022-04-22T01:56:13.516983Z","iopub.status.idle":"2022-04-22T01:56:29.591721Z","shell.execute_reply.started":"2022-04-22T01:56:13.516954Z","shell.execute_reply":"2022-04-22T01:56:29.590749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrain = pd.read_parquet('../input/foursquare-location-matching-parquet/train.parquet')\ntest = pd.read_parquet('../input/foursquare-location-matching-parquet//test.parquet')\nss = pd.read_parquet('../input/foursquare-location-matching-parquet/sample_submission.parquet')\npairs = pd.read_parquet('../input/foursquare-location-matching-parquet/pairs.parquet')","metadata":{"execution":{"iopub.status.busy":"2022-04-22T01:56:29.594032Z","iopub.execute_input":"2022-04-22T01:56:29.595082Z","iopub.status.idle":"2022-04-22T01:56:36.130865Z","shell.execute_reply.started":"2022-04-22T01:56:29.595028Z","shell.execute_reply":"2022-04-22T01:56:36.129906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print the shape of each file\ntrain.shape, test.shape, ss.shape, pairs.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-22T01:56:36.132303Z","iopub.execute_input":"2022-04-22T01:56:36.132545Z","iopub.status.idle":"2022-04-22T01:56:36.14496Z","shell.execute_reply.started":"2022-04-22T01:56:36.132516Z","shell.execute_reply":"2022-04-22T01:56:36.143663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## The Data\n- `train.csv` - The training set, comprising eleven attribute fields for over one million place entries. Main keys are `id` and `point_of_interest`\n- `pairs.csv` - A pregenerated set of pairs of place entries designed to improve detection of matches.\n- `test.csv` and example 5 rows of what the real test set with look like.\n- `sample_submission.csv` example of the format of the submission that is expected.\n","metadata":{"execution":{"iopub.status.busy":"2022-04-20T01:50:24.496124Z","iopub.execute_input":"2022-04-20T01:50:24.496669Z","iopub.status.idle":"2022-04-20T01:50:24.511671Z","shell.execute_reply.started":"2022-04-20T01:50:24.496636Z","shell.execute_reply":"2022-04-20T01:50:24.51107Z"}}},{"cell_type":"markdown","source":"## What's Missing?","metadata":{}},{"cell_type":"code","source":"# How many missing values for each example\nfig, axs = plt.subplots(1, 2, figsize=(15, 5))\ntrain.isna().mean().sort_values(ascending=False).plot(\n    kind=\"bar\", title=\"Missing Values by Feature\", ax=axs[0]\n)\naxs[0].set_ylabel(\"% of Missing Values\")\n\ntrain.isna().sum(axis=1).value_counts().sort_index().plot(\n    ax=axs[1], title=\"Missing Values by Observation\", kind=\"bar\", color=color_pal[1]\n)\n\naxs[1].set_xlabel(\"Number of Missing Features\")\naxs[1].set_ylabel(\"Number of Observations\")\nplt.xticks(rotation=0)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-22T01:56:36.1465Z","iopub.execute_input":"2022-04-22T01:56:36.147004Z","iopub.status.idle":"2022-04-22T01:56:39.062159Z","shell.execute_reply.started":"2022-04-22T01:56:36.146945Z","shell.execute_reply":"2022-04-22T01:56:39.061159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train features\nThe training dataset has 1138812 examples.\n- id\n- name ?\n- lat/long\n- address, city, state, zip & country\n- url\n- phone\n- categories","metadata":{"execution":{"iopub.status.busy":"2022-04-20T02:12:02.229335Z","iopub.execute_input":"2022-04-20T02:12:02.229619Z","iopub.status.idle":"2022-04-20T02:12:02.252611Z","shell.execute_reply.started":"2022-04-20T02:12:02.22959Z","shell.execute_reply":"2022-04-20T02:12:02.251818Z"}}},{"cell_type":"code","source":"print(f'There are {train[\"categories\"].nunique()} unique categories')\n# Take a look at the most frequent categories\ntrain[\"categories\"].value_counts().to_frame().query(\"categories > 5_000\")[\n    \"categories\"\n].sort_values(ascending=True).plot(\n    kind=\"barh\", color=color_pal[2], title=\"Most Frequent Categories\", figsize=(5, 8)\n)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-22T01:56:39.063344Z","iopub.execute_input":"2022-04-22T01:56:39.063591Z","iopub.status.idle":"2022-04-22T01:56:39.706034Z","shell.execute_reply.started":"2022-04-22T01:56:39.063563Z","shell.execute_reply":"2022-04-22T01:56:39.704981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Top 10 Values for Country/City/State etc.\nSome things to note:\n- US is the top country\n- CA and NY are the top states\n- There are some states that don't show up because of the font I'm using\n- Singapore is the top city, suprisingly although NY is the top state, New York City is not the top city.\n- Some of these zip codes look sus.","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(1, 4, figsize=(12, 4))\nfor i, col in enumerate([\"country\", \"state\", \"city\", \"zip\"]):\n    train[col].value_counts().head(10).sort_values(ascending=True).plot(\n        kind=\"barh\", ax=axs[i], color=next(color_cycle), title=col\n    )\nplt.tight_layout()\nfig.suptitle(\"Top Address Based Features\", y=1.05, fontsize=25)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-22T01:56:39.707479Z","iopub.execute_input":"2022-04-22T01:56:39.707797Z","iopub.status.idle":"2022-04-22T01:56:41.017716Z","shell.execute_reply.started":"2022-04-22T01:56:39.707744Z","shell.execute_reply":"2022-04-22T01:56:41.016685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Map Out the Lat / Long by Category\n\nThere are way too many points to plot at once (at least at first). Lets plot the points from a single city. We will pick Singapore because it is the top city in the dataset. We will only show the top 5 categories and then bin everything else into \"Other\"","metadata":{}},{"cell_type":"code","source":"def plot_points(\n    df,\n    lat_col=\"latitude\",\n    lon_col=\"longitude\",\n    hover_name=\"name\",\n    color_col=\"categories_plot\",\n    hover_data=[\"categories\", \"id\", \"phone\", \"url\"],\n    zoom=9,\n):\n    df = df.copy()\n    df = df.fillna(\"Missing\")\n    if color_col == \"categories_plot\":\n        top_5_cats = df[\"categories\"].value_counts().head(5).index.values.tolist()\n        df[\"categories_plot\"] = df[\"categories\"]\n        df.loc[~df[\"categories\"].isin(top_5_cats), \"categories_plot\"] = \"Other\"\n    fig = px.scatter_mapbox(\n        df,\n        lat=lat_col,\n        lon=lon_col,\n        hover_name=hover_name,\n        hover_data=hover_data,\n        color=color_col,\n        labels=\"POI\",\n        zoom=zoom,\n        height=600,\n        width=800,\n        opacity=0.9,\n    )\n    fig.update_layout(mapbox_style=\"stamen-terrain\")\n    fig.update_layout(margin={\"r\": 0, \"t\": 0, \"l\": 0, \"b\": 0})\n    fig.update_layout(title_text=\"Points in Singapore\")\n    fig.show()\n\n\nplot_points(train.query('city == \"Singapore\"'))","metadata":{"execution":{"iopub.status.busy":"2022-04-22T01:56:41.019783Z","iopub.execute_input":"2022-04-22T01:56:41.020175Z","iopub.status.idle":"2022-04-22T01:56:41.824994Z","shell.execute_reply.started":"2022-04-22T01:56:41.020131Z","shell.execute_reply":"2022-04-22T01:56:41.824022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Map and Color by Known Pairs\n- The blue points below have known pairs\n- The red points do not","metadata":{}},{"cell_type":"code","source":"train_sing = train.query('city == \"Singapore\"').reset_index(drop=True).copy()\n# Find all the pair ids that exist\nknown_pair_ids = pd.concat(\n    [pairs.query(\"match\")[\"id_1\"], pairs.query(\"match\")[\"id_2\"]]\n).unique()\ntrain_sing[\"is_pair_id\"] = train_sing[\"id\"].isin(known_pair_ids)\n\nplot_points(train_sing, color_col=\"is_pair_id\")","metadata":{"execution":{"iopub.status.busy":"2022-04-22T01:56:50.439433Z","iopub.execute_input":"2022-04-22T01:56:50.440227Z","iopub.status.idle":"2022-04-22T01:56:51.930196Z","shell.execute_reply.started":"2022-04-22T01:56:50.440188Z","shell.execute_reply":"2022-04-22T01:56:51.929247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Missing City Column Values\nBut wait! We are missing rows in Singapore that don't have a \"city\" column\n\nLets find the min/max lat and long values for singapore and include any points within this area even if they do not have a city feature. We will use the mean lat/lon +/- 3 std to filter to that area.\n\nDot colors in this plot show if the \"City\" value is missing.","metadata":{}},{"cell_type":"code","source":"min_max = train.query('city == \"Singapore\"')[[\"latitude\", \"longitude\"]].agg(\n    [\"min\", \"max\", \"mean\", \"std\"]\n)\n\nmin_max","metadata":{"execution":{"iopub.status.busy":"2022-04-22T01:57:05.949341Z","iopub.execute_input":"2022-04-22T01:57:05.949655Z","iopub.status.idle":"2022-04-22T01:57:06.051873Z","shell.execute_reply.started":"2022-04-22T01:57:05.949624Z","shell.execute_reply":"2022-04-22T01:57:06.051026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"min_lat = min_max[\"latitude\"][\"min\"]\nmax_lat = min_max[\"latitude\"][\"max\"]\nmin_lon = min_max[\"longitude\"][\"min\"]\nmax_lon = min_max[\"longitude\"][\"max\"]\n\nmean_lat = min_max[\"latitude\"][\"mean\"]\nmean_lon = min_max[\"longitude\"][\"mean\"]\nstd_lat = min_max[\"latitude\"][\"std\"]\nstd_lon = min_max[\"longitude\"][\"std\"]\n\nmin_lat = mean_lat - (3 * std_lat)\nmax_lat = mean_lat + (3 * std_lat)\nmin_lon = mean_lon - (3 * std_lon)\nmax_lon = mean_lon + (3 * std_lon)\n\ntrain_sing = train.query(\"latitude >= @min_lat and latitude <= @max_lat\").query(\n    \"longitude >= @min_lon and longitude <= @max_lon\"\n)\ntrain_sing[\"missing_city\"] = train_sing[\"city\"].isna()\n\nplot_points(train_sing, color_col=\"missing_city\")","metadata":{"execution":{"iopub.status.busy":"2022-04-22T01:57:10.195687Z","iopub.execute_input":"2022-04-22T01:57:10.196264Z","iopub.status.idle":"2022-04-22T01:57:11.458634Z","shell.execute_reply.started":"2022-04-22T01:57:10.196222Z","shell.execute_reply":"2022-04-22T01:57:11.457196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Better Understand the \"Pairs.csv\"\nThe data description says:\n- `pairs.csv` - A pregenerated set of pairs of place entries from train.csv designed to improve detection of matches. You may wish to generate additional pairs to improve your model's ability to discriminate POIs.\n    - `match` - Whether (True or False) the pair of entries describes a common POI.\n\nI don't fully understand this yet. So to better understand I will take a look at the POI which occurs the most in the `pairs.csv` and plot those locations on a mpa.","metadata":{}},{"cell_type":"code","source":"pairs.shape, train.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-22T01:57:31.728145Z","iopub.execute_input":"2022-04-22T01:57:31.72843Z","iopub.status.idle":"2022-04-22T01:57:31.7361Z","shell.execute_reply.started":"2022-04-22T01:57:31.728401Z","shell.execute_reply":"2022-04-22T01:57:31.735216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"true_pairs = pairs.query(\"match == True\").reset_index(drop=True).copy()\n# As an example, lets try to find the PIO\npaired_ids = pd.concat([true_pairs[\"id_1\"], true_pairs[\"id_2\"]]).value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-04-22T01:57:36.495094Z","iopub.execute_input":"2022-04-22T01:57:36.495375Z","iopub.status.idle":"2022-04-22T01:57:37.780854Z","shell.execute_reply.started":"2022-04-22T01:57:36.495347Z","shell.execute_reply":"2022-04-22T01:57:37.779716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plotting Points of the Most Common \"cluster\" of pairs.\nHow are all these considered the same point?","metadata":{}},{"cell_type":"code","source":"most_common_pair = paired_ids.index[0]\n# Locate pairs where the most common is one of the two ids\nmost_common_cluster = (\n    true_pairs.loc[\n        (true_pairs[\"id_1\"] == most_common_pair)\n        | (true_pairs[\"id_2\"] == most_common_pair)\n    ]\n    .reset_index(drop=True)\n    .copy()\n)\n\ncommon_cluster_ids = pd.concat(\n    [most_common_cluster[\"id_1\"], most_common_cluster[\"id_2\"]]\n).unique()\n\nplot_points(train.query(\"id in @common_cluster_ids\"), color_col=\"categories\")","metadata":{"execution":{"iopub.status.busy":"2022-04-22T02:07:13.543434Z","iopub.execute_input":"2022-04-22T02:07:13.54374Z","iopub.status.idle":"2022-04-22T02:07:14.016899Z","shell.execute_reply.started":"2022-04-22T02:07:13.543708Z","shell.execute_reply":"2022-04-22T02:07:14.016256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"most_common_pair = paired_ids.index[1]\n# Locate pairs where the most common is one of the two ids\nmost_common_cluster = (\n    true_pairs.loc[\n        (true_pairs[\"id_1\"] == most_common_pair)\n        | (true_pairs[\"id_2\"] == most_common_pair)\n    ]\n    .reset_index(drop=True)\n    .copy()\n)\n\ncommon_cluster_ids = pd.concat(\n    [most_common_cluster[\"id_1\"], most_common_cluster[\"id_2\"]]\n).unique()\n\nplot_points(train.query(\"id in @common_cluster_ids\"), color_col=\"categories\")","metadata":{"execution":{"iopub.status.busy":"2022-04-22T02:08:46.345223Z","iopub.execute_input":"2022-04-22T02:08:46.345715Z","iopub.status.idle":"2022-04-22T02:08:46.655859Z","shell.execute_reply.started":"2022-04-22T02:08:46.345683Z","shell.execute_reply":"2022-04-22T02:08:46.654879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"most_common_pair = paired_ids.index[2]\n# Locate pairs where the most common is one of the two ids\nmost_common_cluster = (\n    true_pairs.loc[\n        (true_pairs[\"id_1\"] == most_common_pair)\n        | (true_pairs[\"id_2\"] == most_common_pair)\n    ]\n    .reset_index(drop=True)\n    .copy()\n)\n\ncommon_cluster_ids = pd.concat(\n    [most_common_cluster[\"id_1\"], most_common_cluster[\"id_2\"]]\n).unique()\n\nplot_points(train.query(\"id in @common_cluster_ids\"), color_col=\"categories\")","metadata":{"execution":{"iopub.status.busy":"2022-04-22T02:09:39.291766Z","iopub.execute_input":"2022-04-22T02:09:39.292092Z","iopub.status.idle":"2022-04-22T02:09:39.677641Z","shell.execute_reply.started":"2022-04-22T02:09:39.292058Z","shell.execute_reply":"2022-04-22T02:09:39.676724Z"},"trusted":true},"execution_count":null,"outputs":[]}]}