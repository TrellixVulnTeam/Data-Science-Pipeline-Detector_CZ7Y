{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#### There are several duplicative pairs (*e.g.* index 2322 vs. 34007) in the pairs.csv as well as other binary datasets created by nearest-neighbor searches.\n#### Such pairs can cause leakage during CV.\n#### Herein, one of the ways to tackle with the problem is shown.","metadata":{}},{"cell_type":"code","source":"import pandas as pd \nfrom tqdm.notebook import tqdm\ntqdm.pandas()\npairs = pd.read_csv(\"../input/foursquare-location-matching/pairs.csv\", nrows=40000).reset_index(drop=True)\n\nprint(pairs.loc[2322])\nprint(pairs.loc[34007])","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-28T22:57:35.029067Z","iopub.execute_input":"2022-06-28T22:57:35.029539Z","iopub.status.idle":"2022-06-28T22:57:35.58802Z","shell.execute_reply.started":"2022-06-28T22:57:35.029446Z","shell.execute_reply":"2022-06-28T22:57:35.586602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def drop_duplicate(df):\n    del_count=0\n    check_id = df[\"id_1\"].values.tolist()\n    check_match_id = df[\"id_2\"].values.tolist()\n    del_idx_list = []\n    for idx in tqdm(reversed(range(len(check_id)))):\n        id_list = [i for i, val in enumerate(check_match_id) if val == check_id[idx]]\n        match_id_list = [i for i, val in enumerate(check_id) if val == check_match_id[idx]]\n        dup_list = list(set(id_list)&set(match_id_list))\n        if len(dup_list) != 0:\n            del_idx_list.append(idx)\n            del_count+=1\n        del check_id[idx], check_match_id[idx]\n    print(\"del_count:\", del_count)\n    df.drop(df.index[del_idx_list], inplace=True)\n    return df\n\npairs_modified = drop_duplicate(pairs)","metadata":{"execution":{"iopub.status.busy":"2022-06-28T22:57:35.590273Z","iopub.execute_input":"2022-06-28T22:57:35.590644Z","iopub.status.idle":"2022-06-28T23:00:26.039899Z","shell.execute_reply.started":"2022-06-28T22:57:35.59061Z","shell.execute_reply":"2022-06-28T23:00:26.038792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The duplicative row in the pair (index 34007) has been deleted.","metadata":{}},{"cell_type":"code","source":"print(pairs_modified.loc[2322])\ntry:\n    print(pairs_modified.loc[34007])\nexcept:\n    print(\"\\nCannot find the row.\")","metadata":{"execution":{"iopub.status.busy":"2022-06-28T23:00:26.041148Z","iopub.execute_input":"2022-06-28T23:00:26.041515Z","iopub.status.idle":"2022-06-28T23:00:26.056944Z","shell.execute_reply.started":"2022-06-28T23:00:26.041482Z","shell.execute_reply":"2022-06-28T23:00:26.055342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Grouping the data by country can save processing time.","metadata":{}},{"cell_type":"code","source":"def drop_duplicate(df):\n    del_count=0\n    check_id = df[\"id_1\"].values.tolist()\n    check_match_id = df[\"id_2\"].values.tolist()\n    del_idx_list = []\n    for idx in reversed(range(len(check_id))):\n        id_list = [i for i, val in enumerate(check_match_id) if val == check_id[idx]]\n        match_id_list = [i for i, val in enumerate(check_id) if val == check_match_id[idx]]\n        dup_list = list(set(id_list)&set(match_id_list))\n        if len(dup_list) != 0:\n            del_idx_list.append(idx)\n            del_count+=1\n        del check_id[idx], check_match_id[idx]\n    df.drop(df.index[del_idx_list], inplace=True)\n    return df, del_count\n\npairs = pd.read_csv(\"../input/foursquare-location-matching/pairs.csv\", nrows=40000).reset_index(drop=True)\nall_df = []\ndel_count_all=0\nfor country, country_df in tqdm(pairs.groupby(\"country_1\", as_index=False)):\n    tmp_df, del_count = drop_duplicate(country_df)\n    all_df.append(tmp_df)\n    del_count_all += del_count\npairs_modified_2 = pd.concat(all_df)\nprint(\"del_count:\", del_count_all)\n\nprint(pairs_modified_2.loc[2322])\ntry:\n    print(pairs_modified_2.loc[34007])\nexcept:\n    print(\"\\nCannot find the row.\")","metadata":{"execution":{"iopub.status.busy":"2022-06-28T23:00:26.060066Z","iopub.execute_input":"2022-06-28T23:00:26.060472Z","iopub.status.idle":"2022-06-28T23:00:42.476003Z","shell.execute_reply.started":"2022-06-28T23:00:26.060435Z","shell.execute_reply":"2022-06-28T23:00:42.47474Z"},"trusted":true},"execution_count":null,"outputs":[]}]}