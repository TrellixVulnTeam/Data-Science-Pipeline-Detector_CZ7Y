{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook is the binary lightgbm inference baseline.\nAnother notebook is for training data generation [[training_data_for_binary_lgb_baseline](https://www.kaggle.com/code/guoyonfan/training-data-for-binary-lgb-baseline-0-834)].","metadata":{}},{"cell_type":"code","source":"## Imports\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nimport gc\nimport time\nimport random\nimport Levenshtein\nimport difflib\nimport multiprocessing\nimport pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nfrom tqdm.auto import tqdm\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.neighbors import NearestNeighbors","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAIN_FEATURES = ['kdist',\n                'kneighbors',\n                'kdist_country',\n                'kneighbors_country',\n                'name_sim',\n                'name_gesh',\n                'name_leven',\n                'name_jaro',\n                'name_lcs',\n                'name_len_diff',\n                'name_nleven',\n                'name_nlcsk',\n                'name_nlcs',\n                'address_sim',\n                'address_gesh',\n                'address_leven',\n                'address_jaro',\n                'address_lcs',\n                'address_len_diff',\n                'address_nleven',\n                'address_nlcsk',\n                'address_nlcs',\n                'city_gesh',\n                'city_leven',\n                'city_jaro',\n                'city_lcs',\n                'city_len_diff',\n                'city_nleven',\n                'city_nlcsk',\n                'city_nlcs',\n                'state_sim',\n                'state_gesh',\n                'state_leven',\n                'state_jaro',\n                'state_lcs',\n                'state_len_diff',\n                'state_nleven',\n                'state_nlcsk',\n                'state_nlcs',\n                'zip_gesh',\n                'zip_leven',\n                'zip_jaro',\n                'zip_lcs',\n                'url_sim',\n                'url_gesh',\n                'url_leven',\n                'url_jaro',\n                'url_lcs',\n                'url_len_diff',\n                'url_nleven',\n                'url_nlcsk',\n                'url_nlcs',\n                'phone_gesh',\n                'phone_leven',\n                'phone_jaro',\n                'phone_lcs',\n                'categories_sim',\n                'categories_gesh',\n                'categories_leven',\n                'categories_jaro',\n                'categories_lcs',\n                'categories_len_diff',\n                'categories_nleven',\n                'categories_nlcsk',\n                'categories_nlcs',\n                'country_sim',\n                'country_gesh',\n                'country_leven',\n                'country_nleven',\n                'kdist_diff',\n                'kneighbors_mean',]","metadata":{"execution":{"iopub.status.busy":"2022-05-16T04:34:30.363067Z","iopub.execute_input":"2022-05-16T04:34:30.363389Z","iopub.status.idle":"2022-05-16T04:34:30.374313Z","shell.execute_reply.started":"2022-05-16T04:34:30.363361Z","shell.execute_reply":"2022-05-16T04:34:30.373747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Parameters\nNUM_NEIGHBOR = 20\nSEED = 2022\nTHRESHOLD = 0.5\nNUM_SPLIT = 5\nfeat_columns = ['name', 'address', 'city', \n            'state', 'zip', 'url', \n           'phone', 'categories', 'country']\nvec_columns = ['name', 'categories', 'address', \n               'state', 'url', 'country']\n\ndef seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \nseed_everything(SEED)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T04:34:30.375276Z","iopub.execute_input":"2022-05-16T04:34:30.375549Z","iopub.status.idle":"2022-05-16T04:34:30.39336Z","shell.execute_reply.started":"2022-05-16T04:34:30.37552Z","shell.execute_reply":"2022-05-16T04:34:30.392757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%load_ext Cython","metadata":{"execution":{"iopub.status.busy":"2022-05-16T04:34:30.395689Z","iopub.execute_input":"2022-05-16T04:34:30.396182Z","iopub.status.idle":"2022-05-16T04:34:31.402323Z","shell.execute_reply.started":"2022-05-16T04:34:30.396134Z","shell.execute_reply":"2022-05-16T04:34:31.401361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%cython\ndef LCS(str S, str T):\n    cdef int i, j\n    cdef list dp = [[0] * (len(T) + 1) for _ in range(len(S) + 1)]\n    for i in range(len(S)):\n        for j in range(len(T)):\n            dp[i + 1][j + 1] = max(dp[i][j] + (S[i] == T[j]), dp[i + 1][j], dp[i][j + 1], dp[i + 1][j + 1])\n    return dp[len(S)][len(T)]","metadata":{"execution":{"iopub.status.busy":"2022-05-16T04:34:31.403568Z","iopub.execute_input":"2022-05-16T04:34:31.403815Z","iopub.status.idle":"2022-05-16T04:34:32.825255Z","shell.execute_reply.started":"2022-05-16T04:34:31.403788Z","shell.execute_reply":"2022-05-16T04:34:32.824062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def post_process(df):\n    id2match = dict(zip(df['id'].values, df['matches'].str.split()))\n\n    for base, match in df[['id', 'matches']].values:\n        match = match.split()\n        if len(match) == 1:        \n            continue\n\n        for m in match:\n            if base not in id2match[m]:\n                id2match[m].append(base)\n    df['matches'] = df['id'].map(id2match).map(' '.join)\n    return df ","metadata":{"execution":{"iopub.status.busy":"2022-05-16T04:34:32.82718Z","iopub.execute_input":"2022-05-16T04:34:32.82759Z","iopub.status.idle":"2022-05-16T04:34:32.835263Z","shell.execute_reply.started":"2022-05-16T04:34:32.82755Z","shell.execute_reply":"2022-05-16T04:34:32.834672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def recall_knn(df, Neighbors = 10):\n    print('Start knn grouped by country')\n    train_df_country = []\n    for country, country_df in tqdm(df.groupby('country')):\n        country_df = country_df.reset_index(drop = True)\n\n        neighbors = min(len(country_df), Neighbors)\n        knn = KNeighborsRegressor(n_neighbors = neighbors,\n                                    metric = 'haversine',\n                                    n_jobs = -1)\n        knn.fit(country_df[['latitude','longitude']], country_df.index)\n        dists, nears = knn.kneighbors(country_df[['latitude', 'longitude']], \n                                        return_distance = True)\n\n        for k in range(neighbors):            \n            cur_df = country_df[['id']]\n            cur_df['match_id'] = country_df['id'].values[nears[:, k]]\n            cur_df['kdist_country'] = dists[:, k]\n            cur_df['kneighbors_country'] = k\n            \n            train_df_country.append(cur_df)\n    train_df_country = pd.concat(train_df_country)\n    \n    print('Start knn')\n    train_df = []\n    knn = NearestNeighbors(n_neighbors = Neighbors)\n    knn.fit(df[['latitude','longitude']], df.index)\n    dists, nears = knn.kneighbors(df[['latitude','longitude']])\n    \n    for k in range(Neighbors):            \n        cur_df = df[['id']]\n        cur_df['match_id'] = df['id'].values[nears[:, k]]\n        cur_df['kdist'] = dists[:, k]\n        cur_df['kneighbors'] = k\n        train_df.append(cur_df)\n    \n    train_df = pd.concat(train_df)\n    train_df = train_df.merge(train_df_country,\n                                 on = ['id', 'match_id'],\n                                 how = 'outer')\n    del train_df_country\n    \n    return train_df","metadata":{"execution":{"iopub.status.busy":"2022-05-16T04:34:32.836308Z","iopub.execute_input":"2022-05-16T04:34:32.836951Z","iopub.status.idle":"2022-05-16T04:34:32.852266Z","shell.execute_reply.started":"2022-05-16T04:34:32.836914Z","shell.execute_reply":"2022-05-16T04:34:32.851485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def add_features(df):    \n    for col in tqdm(feat_columns):       \n        if col in vec_columns:\n            tv_fit = tfidf_d[col]\n            indexs = [id2index_d[i] for i in df['id']]\n            match_indexs = [id2index_d[i] for i in df['match_id']]                    \n            df[f'{col}_sim'] = np.array(tv_fit[indexs].multiply(tv_fit[match_indexs]).sum(axis = 1)).ravel()\n        \n        col_values = data.loc[df['id']][col].values.astype(str)\n        matcol_values = data.loc[df['match_id']][col].values.astype(str)\n        \n        geshs = []\n        levens = []\n        jaros = []\n        lcss = []\n        for s, match_s in zip(col_values, matcol_values):\n            if s != 'nan' and match_s != 'nan':                    \n                geshs.append(difflib.SequenceMatcher(None, s, match_s).ratio())\n                levens.append(Levenshtein.distance(s, match_s))\n                jaros.append(Levenshtein.jaro_winkler(s, match_s))\n                lcss.append(LCS(str(s), str(match_s)))\n            else:\n                geshs.append(np.nan)\n                levens.append(np.nan)\n                jaros.append(np.nan)\n                lcss.append(np.nan)\n        \n        df[f'{col}_gesh'] = geshs\n        df[f'{col}_leven'] = levens\n        df[f'{col}_jaro'] = jaros\n        df[f'{col}_lcs'] = lcss\n        \n        if col not in ['phone', 'zip']:\n            df[f'{col}_len'] = list(map(len, col_values))\n            df[f'match_{col}_len'] = list(map(len, matcol_values)) \n            df[f'{col}_len_diff'] = np.abs(df[f'{col}_len'] - df[f'match_{col}_len'])\n            df[f'{col}_nleven'] = df[f'{col}_leven'] / \\\n                                    df[[f'{col}_len', f'match_{col}_len']].max(axis = 1)\n            \n            df[f'{col}_nlcsk'] = df[f'{col}_lcs'] / df[f'match_{col}_len']\n            df[f'{col}_nlcs'] = df[f'{col}_lcs'] / df[f'{col}_len']\n            \n            df = df.drop(f'{col}_len', axis = 1)\n            df = df.drop(f'match_{col}_len', axis = 1)\n            gc.collect()\n            \n    return df","metadata":{"execution":{"iopub.status.busy":"2022-05-16T04:34:32.853527Z","iopub.execute_input":"2022-05-16T04:34:32.853923Z","iopub.status.idle":"2022-05-16T04:34:32.870981Z","shell.execute_reply.started":"2022-05-16T04:34:32.85389Z","shell.execute_reply":"2022-05-16T04:34:32.870195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Dada process\ndata = pd.read_csv('../input/foursquare-location-matching/test.csv')\n\nif len(data) < 20:\n    data = pd.read_csv('../input/foursquare-location-matching/train.csv',\n                      nrows = 100)\n    data = data.drop('point_of_interest', axis = 1)\n    \nid2index_d = dict(zip(data['id'].values, data.index))\n\ntfidf_d = {}\nfor col in vec_columns:\n    tfidf = TfidfVectorizer()\n    tv_fit = tfidf.fit_transform(data[col].fillna('nan'))\n    tfidf_d[col] = tv_fit\n\nout_df = pd.DataFrame()\nout_df['id'] = data['id'].unique().tolist()\nout_df['match_id'] = out_df['id']\n\ntest_data = recall_knn(data, NUM_NEIGHBOR)\ndata = data.set_index('id')\n\nprint('Num of unique id: %s' % test_data['id'].nunique())\nprint('Num of test data: %s' % len(test_data))\nprint(test_data.sample(5))","metadata":{"execution":{"iopub.status.busy":"2022-05-16T04:34:32.872248Z","iopub.execute_input":"2022-05-16T04:34:32.87262Z","iopub.status.idle":"2022-05-16T04:34:36.751061Z","shell.execute_reply.started":"2022-05-16T04:34:32.872591Z","shell.execute_reply":"2022-05-16T04:34:36.750157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Model load\nlgb_model_path = '../input/binary-lgb-baseline/lgb_baseline.lgb'\nlgb_model = lgb.Booster(model_file = lgb_model_path)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T04:34:36.753944Z","iopub.execute_input":"2022-05-16T04:34:36.754278Z","iopub.status.idle":"2022-05-16T04:34:36.966351Z","shell.execute_reply.started":"2022-05-16T04:34:36.754235Z","shell.execute_reply":"2022-05-16T04:34:36.96567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Prediction\ncount = 0\nstart_row = 0\npred_df = pd.DataFrame()\nunique_id = test_data['id'].unique().tolist()\nnum_split_id = len(unique_id) // NUM_SPLIT\nfor k in range(1, NUM_SPLIT + 1):\n    print('Current split: %s' % k)\n    end_row = start_row + num_split_id\n    if k < NUM_SPLIT:\n        cur_id = unique_id[start_row : end_row]\n        cur_data = test_data[test_data['id'].isin(cur_id)]\n    else:\n        cur_id = unique_id[start_row: ]\n        cur_data = test_data[test_data['id'].isin(cur_id)]\n    \n    # add features & model prediction\n    cur_data = add_features(cur_data)\n    cur_data['kdist_diff'] = (cur_data['kdist'] - cur_data['kdist_country']) /\\\n                                cur_data['kdist_country']\n    cur_data['kneighbors_mean'] = cur_data[['kneighbors', 'kneighbors_country']].mean(axis = 1)\n    cur_data['pred'] = lgb_model.predict(cur_data[TRAIN_FEATURES])\n    cur_pred_df = cur_data[cur_data['pred'] > THRESHOLD][['id', 'match_id']]\n    pred_df = pd.concat([pred_df, cur_pred_df])\n    \n    start_row = end_row\n    count += len(cur_data)\n\n    del cur_data, cur_pred_df\n    gc.collect()\nprint(count)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T04:34:36.970684Z","iopub.execute_input":"2022-05-16T04:34:36.973276Z","iopub.status.idle":"2022-05-16T04:34:43.972917Z","shell.execute_reply.started":"2022-05-16T04:34:36.973224Z","shell.execute_reply":"2022-05-16T04:34:43.97236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Submission    \nout_df = pd.concat([out_df, pred_df])\nout_df = out_df.groupby('id')['match_id'].\\\n                        apply(list).reset_index()\nout_df['matches'] = out_df['match_id'].apply(lambda x: ' '.join(set(x)))\nout_df = post_process(out_df)\nprint('Unique id: %s' % len(out_df))\nprint(out_df.head())\n\nout_df[['id', 'matches']].to_csv('submission.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T04:34:43.973946Z","iopub.execute_input":"2022-05-16T04:34:43.974264Z","iopub.status.idle":"2022-05-16T04:34:43.999436Z","shell.execute_reply.started":"2022-05-16T04:34:43.974236Z","shell.execute_reply":"2022-05-16T04:34:43.998726Z"},"trusted":true},"execution_count":null,"outputs":[]}]}