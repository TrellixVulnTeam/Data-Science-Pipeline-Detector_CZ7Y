{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Installing h3 Lib","metadata":{}},{"cell_type":"code","source":"!pip install ../input/h3lib/h3-3.7.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl","metadata":{"_kg_hide-input":false,"_kg_hide-output":false,"execution":{"iopub.status.busy":"2022-05-18T07:18:45.156667Z","iopub.execute_input":"2022-05-18T07:18:45.157032Z","iopub.status.idle":"2022-05-18T07:19:15.308869Z","shell.execute_reply.started":"2022-05-18T07:18:45.15694Z","shell.execute_reply":"2022-05-18T07:19:15.307846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Limporting lib","metadata":{}},{"cell_type":"code","source":"import difflib\nfrom h3 import h3\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom unidecode import unidecode\nfrom tqdm.notebook import tqdm\n\ntqdm.pandas()\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-18T07:19:15.311019Z","iopub.execute_input":"2022-05-18T07:19:15.311255Z","iopub.status.idle":"2022-05-18T07:19:15.440118Z","shell.execute_reply.started":"2022-05-18T07:19:15.311228Z","shell.execute_reply":"2022-05-18T07:19:15.439257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data importing (train and test sets)","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv('../input/training-set-foursquare/training_set.csv')\ntraining_set_ = df_train.copy()\n\ndf_test = pd.read_csv('/kaggle/input/foursquare-location-matching/test.csv')\ntest_set_ = df_test.copy()","metadata":{"execution":{"iopub.status.busy":"2022-05-18T07:19:15.441214Z","iopub.execute_input":"2022-05-18T07:19:15.441431Z","iopub.status.idle":"2022-05-18T07:19:25.356099Z","shell.execute_reply.started":"2022-05-18T07:19:15.441402Z","shell.execute_reply":"2022-05-18T07:19:25.355062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data first check","metadata":{}},{"cell_type":"code","source":"training_set_.info()","metadata":{"execution":{"iopub.status.busy":"2022-05-18T07:19:25.357766Z","iopub.execute_input":"2022-05-18T07:19:25.359422Z","iopub.status.idle":"2022-05-18T07:19:26.021808Z","shell.execute_reply.started":"2022-05-18T07:19:25.359377Z","shell.execute_reply":"2022-05-18T07:19:26.020966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing = training_set_.isna().sum()\nmissing","metadata":{"execution":{"iopub.status.busy":"2022-05-18T07:19:26.02314Z","iopub.execute_input":"2022-05-18T07:19:26.023463Z","iopub.status.idle":"2022-05-18T07:19:26.659975Z","shell.execute_reply.started":"2022-05-18T07:19:26.023414Z","shell.execute_reply":"2022-05-18T07:19:26.659176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# normalizing name to avoid mismatch due to accents or punctuations\ntest_set_['name'] = test_set_['name'].astype(str)\ntest_set_['name'] = test_set_['name'].apply(unidecode)\ntest_set_['name'] = test_set_['name'].str.replace('[^\\w\\s]','')\ntest_set_['name'] = test_set_['name'].str.lower()\n\n# normalizing categories to avoid mismatch due to accents or punctuations\ntest_set_['categories'] = test_set_['categories'].astype(str)\ntest_set_['categories'] = test_set_['categories'].apply(unidecode)\ntest_set_['categories'] = test_set_['categories'].str.replace('[^\\w\\s]','')\ntest_set_['categories'] = test_set_['categories'].str.lower()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-18T07:19:26.661339Z","iopub.execute_input":"2022-05-18T07:19:26.661949Z","iopub.status.idle":"2022-05-18T07:19:26.676127Z","shell.execute_reply.started":"2022-05-18T07:19:26.661909Z","shell.execute_reply":"2022-05-18T07:19:26.675484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineer","metadata":{}},{"cell_type":"markdown","source":"## So, considering the number of categories available, it was unviable to classify all of them by myself. After some research, I found the LDA method that allows to cluster of words into categories. (check: https://towardsdatascience.com/topic-modelling-in-python-with-nltk-and-gensim-4ef03213cd21).\n\n","metadata":{}},{"cell_type":"code","source":"import re\nimport unidecode\n\ndef tokenization(text):\n    tokens = re.split(' ',text)\n    return tokens\n\ntest_set_['category_tokenied']= test_set_['categories'].apply(lambda x: tokenization(x))\ntraining_set_['category_tokenied']= training_set_['categories'].astype(str).apply(lambda x: tokenization(x))\n\ndata = training_set_.category_tokenied.values.tolist()","metadata":{"execution":{"iopub.status.busy":"2022-05-18T07:19:26.677317Z","iopub.execute_input":"2022-05-18T07:19:26.677562Z","iopub.status.idle":"2022-05-18T07:19:29.582024Z","shell.execute_reply.started":"2022-05-18T07:19:26.677532Z","shell.execute_reply":"2022-05-18T07:19:29.581083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gensim.corpora as corpora # Create Dictionary\n\nid2word = corpora.Dictionary(data) # Create Corpus\n\ntexts = data # Term Document Frequency\n\ncorpus = [id2word.doc2bow(text) for text in texts]# View\n","metadata":{"execution":{"iopub.status.busy":"2022-05-18T07:19:29.584481Z","iopub.execute_input":"2022-05-18T07:19:29.584727Z","iopub.status.idle":"2022-05-18T07:19:42.426063Z","shell.execute_reply.started":"2022-05-18T07:19:29.584701Z","shell.execute_reply":"2022-05-18T07:19:42.425091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gensim\nfrom gensim.utils import simple_preprocess\nfrom pprint import pprint# number of topics\n\nnum_topics = 4 # from 3, 4, 5, 10 ,15 - 4 clusters was the one that was the best coherence\n\nlda_model = gensim.models.LdaMulticore(corpus=corpus,\n                                       id2word=id2word,\n                                       num_topics=num_topics,\n                                       random_state=10)# Print the Keyword in the 10 topics\npprint(lda_model.print_topics())\ndoc_lda = lda_model[corpus]","metadata":{"execution":{"iopub.status.busy":"2022-05-18T07:19:42.42785Z","iopub.execute_input":"2022-05-18T07:19:42.428197Z","iopub.status.idle":"2022-05-18T07:20:50.132947Z","shell.execute_reply.started":"2022-05-18T07:19:42.42814Z","shell.execute_reply":"2022-05-18T07:20:50.132109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let's check how to get the cluster from this model","metadata":{}},{"cell_type":"code","source":"def prepare_text_for_lda(text):\n    tokens = tokenization(text)\n    tokens = [token for token in tokens if len(token) > 4]\n    return tokens\n","metadata":{"execution":{"iopub.status.busy":"2022-05-18T07:20:50.135716Z","iopub.execute_input":"2022-05-18T07:20:50.135968Z","iopub.status.idle":"2022-05-18T07:20:50.141531Z","shell.execute_reply.started":"2022-05-18T07:20:50.135936Z","shell.execute_reply":"2022-05-18T07:20:50.140541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## So get_document_topics returns a list of tuples with the percent of similarity with each cluster. Let's use it to apply for all the row by definig a function and them apply it. ","metadata":{}},{"cell_type":"code","source":"def get_lda_cluster(category):\n    new_doc = prepare_text_for_lda(category)\n    new_doc_bow = id2word.doc2bow(new_doc)\n    clusters = lda_model.get_document_topics(new_doc_bow)\n    \n    scores = []\n    for i in clusters:\n        scores.append(i[1])\n\n    max_index = scores.index(max(scores))\n    return clusters[max_index][0]\n    ","metadata":{"execution":{"iopub.status.busy":"2022-05-18T07:20:50.142834Z","iopub.execute_input":"2022-05-18T07:20:50.143172Z","iopub.status.idle":"2022-05-18T07:20:50.151042Z","shell.execute_reply.started":"2022-05-18T07:20:50.143097Z","shell.execute_reply":"2022-05-18T07:20:50.150431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_set_['category_cluster'] = test_set_.progress_apply(lambda x: get_lda_cluster(x['categories']),axis=1)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-18T07:20:50.152109Z","iopub.execute_input":"2022-05-18T07:20:50.152322Z","iopub.status.idle":"2022-05-18T07:20:50.201264Z","shell.execute_reply.started":"2022-05-18T07:20:50.152297Z","shell.execute_reply":"2022-05-18T07:20:50.200398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### My approach was using the h3 lib created by Uber (check documentation here: https://github.com/uber/h3) to cluster POIs into hex using the method h3.geo_to_h3 first. It will help us to check in a certain hex all the places.  \n### It allows to search new POIs according the hex_cell id which combined to others features to solve eventually conflits, can solve this bussiness problem. ","metadata":{}},{"cell_type":"code","source":"test_set_.rename(columns = {'longitude': 'lng', 'latitude': 'lat'}, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-05-18T07:20:50.202292Z","iopub.execute_input":"2022-05-18T07:20:50.202588Z","iopub.status.idle":"2022-05-18T07:20:50.208199Z","shell.execute_reply.started":"2022-05-18T07:20:50.202543Z","shell.execute_reply":"2022-05-18T07:20:50.20731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating the Hex_cell ids","metadata":{}},{"cell_type":"code","source":"H3_res = 9\ndef geo_to_h3(row):\n  return h3.geo_to_h3(lat=row.lat,lng=row.lng,resolution = H3_res)","metadata":{"execution":{"iopub.status.busy":"2022-05-18T07:20:50.209558Z","iopub.execute_input":"2022-05-18T07:20:50.210255Z","iopub.status.idle":"2022-05-18T07:20:50.218023Z","shell.execute_reply.started":"2022-05-18T07:20:50.210211Z","shell.execute_reply":"2022-05-18T07:20:50.217416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_set_['h3_cell'] = test_set_.progress_apply(geo_to_h3,axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-05-18T07:20:50.218846Z","iopub.execute_input":"2022-05-18T07:20:50.219447Z","iopub.status.idle":"2022-05-18T07:20:50.258186Z","shell.execute_reply.started":"2022-05-18T07:20:50.219417Z","shell.execute_reply":"2022-05-18T07:20:50.257394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Defining the search engine ","metadata":{}},{"cell_type":"markdown","source":"### Here I'm going to use all the features that I created to build a engine search. First i'm going to use the hex id to filter all the places of a certain area, than I will filter all of them by the category cluster. Than, to solve eventually conflits, I'm using the difflib.get_close_matches to get the most similar name str. ","metadata":{}},{"cell_type":"code","source":"def search_engine(id, name, h3_cell, cluster):\n    first_step = training_set_[training_set_['h3_cell'] == h3_cell]\n    second_step = first_step[training_set_['category_cluster'] == cluster]\n    \n    kw = difflib.get_close_matches(name, second_step['name'], n=3)\n    \n    result = second_step[second_step['name'].isin(kw)]\n    if result['id'].empty:\n        return id\n    else:\n        sep = ' '\n        match_formated = id\n        for match in result['id']:\n            match_formated += sep \n            match_formated += match\n        return match_formated","metadata":{"execution":{"iopub.status.busy":"2022-05-18T07:20:50.259329Z","iopub.execute_input":"2022-05-18T07:20:50.259535Z","iopub.status.idle":"2022-05-18T07:20:50.265465Z","shell.execute_reply.started":"2022-05-18T07:20:50.259511Z","shell.execute_reply":"2022-05-18T07:20:50.264702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_set_['matches'] = test_set_.apply(lambda x: search_engine(x['id'], x['name'], x['h3_cell'], x['category_cluster']),axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-05-18T07:20:50.266507Z","iopub.execute_input":"2022-05-18T07:20:50.266899Z","iopub.status.idle":"2022-05-18T07:20:50.684896Z","shell.execute_reply.started":"2022-05-18T07:20:50.26687Z","shell.execute_reply":"2022-05-18T07:20:50.684064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_set_","metadata":{"execution":{"iopub.status.busy":"2022-05-18T07:20:50.686242Z","iopub.execute_input":"2022-05-18T07:20:50.686472Z","iopub.status.idle":"2022-05-18T07:20:50.710824Z","shell.execute_reply.started":"2022-05-18T07:20:50.686445Z","shell.execute_reply":"2022-05-18T07:20:50.710036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_set_[training_set_['id'] == 'E_161706a74b5308']","metadata":{"execution":{"iopub.status.busy":"2022-05-18T07:20:50.712019Z","iopub.execute_input":"2022-05-18T07:20:50.712842Z","iopub.status.idle":"2022-05-18T07:20:50.829675Z","shell.execute_reply.started":"2022-05-18T07:20:50.712805Z","shell.execute_reply":"2022-05-18T07:20:50.828797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_set_[training_set_['id'] == 'E_6e477dd29c2dc2']","metadata":{"execution":{"iopub.status.busy":"2022-05-18T07:20:50.830779Z","iopub.execute_input":"2022-05-18T07:20:50.830999Z","iopub.status.idle":"2022-05-18T07:20:50.92193Z","shell.execute_reply.started":"2022-05-18T07:20:50.830973Z","shell.execute_reply":"2022-05-18T07:20:50.921132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## As we can see, it was enough to find a pretty good match for two of 5 rows of the sample","metadata":{}},{"cell_type":"code","source":"submission_sample = test_set_[['id','matches']]\n","metadata":{"execution":{"iopub.status.busy":"2022-05-18T07:20:50.923189Z","iopub.execute_input":"2022-05-18T07:20:50.923402Z","iopub.status.idle":"2022-05-18T07:20:50.928206Z","shell.execute_reply.started":"2022-05-18T07:20:50.923374Z","shell.execute_reply":"2022-05-18T07:20:50.927438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_sample","metadata":{"execution":{"iopub.status.busy":"2022-05-18T07:20:50.929355Z","iopub.execute_input":"2022-05-18T07:20:50.929805Z","iopub.status.idle":"2022-05-18T07:20:50.94378Z","shell.execute_reply.started":"2022-05-18T07:20:50.929777Z","shell.execute_reply":"2022-05-18T07:20:50.942893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sample Submittion","metadata":{}},{"cell_type":"code","source":"submission_sample.to_csv('/kaggle/working/submission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-05-18T07:20:50.945109Z","iopub.execute_input":"2022-05-18T07:20:50.945546Z","iopub.status.idle":"2022-05-18T07:20:50.952782Z","shell.execute_reply.started":"2022-05-18T07:20:50.945516Z","shell.execute_reply":"2022-05-18T07:20:50.952094Z"},"trusted":true},"execution_count":null,"outputs":[]}]}