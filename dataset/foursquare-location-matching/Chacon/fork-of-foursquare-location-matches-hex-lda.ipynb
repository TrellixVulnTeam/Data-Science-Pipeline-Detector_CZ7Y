{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Installing h3 Lib","metadata":{}},{"cell_type":"code","source":"!pip install ../input/h3lib/h3-3.7.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl","metadata":{"_kg_hide-input":false,"_kg_hide-output":false,"execution":{"iopub.status.busy":"2022-05-19T13:07:15.323945Z","iopub.execute_input":"2022-05-19T13:07:15.325066Z","iopub.status.idle":"2022-05-19T13:07:48.268381Z","shell.execute_reply.started":"2022-05-19T13:07:15.325002Z","shell.execute_reply":"2022-05-19T13:07:48.267014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Limporting lib","metadata":{}},{"cell_type":"code","source":"import difflib\nimport gc\nfrom h3 import h3\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom unidecode import unidecode\nfrom tqdm.notebook import tqdm\nimport re\nimport pickle\nimport gensim.corpora as corpora # Create Dictionary\nimport gensim\nfrom gensim.utils import simple_preprocess\nfrom pprint import pprint# number of topics\n\n\ntqdm.pandas()\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-19T13:07:48.271606Z","iopub.execute_input":"2022-05-19T13:07:48.272029Z","iopub.status.idle":"2022-05-19T13:07:50.138999Z","shell.execute_reply.started":"2022-05-19T13:07:48.271971Z","shell.execute_reply":"2022-05-19T13:07:50.13819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class tool_box:\n    def __init__(self):\n        self.H3_res = 9\n        \n    def tokenization(self, text):\n        if text:\n            tokens = re.split(' ',text)\n        return tokens\n\n    def get_lda_cluster(self, category):\n        if category:\n            new_doc = self.tokenization(str(category))\n            new_doc_bow = id2word.doc2bow(new_doc)\n            clusters = lda_model.get_document_topics(new_doc_bow)\n\n            scores = []\n            for i in clusters:\n                scores.append(i[1])\n\n            max_index = scores.index(max(scores))\n            return clusters[max_index][0]\n        else:\n            return 2\n\n    def geo_to_h3(self, row):\n        return h3.geo_to_h3(lat=row.latitude,lng=row.longitude,resolution = self.H3_res)\n\n    def search_engine(self, id_poi, name, h3_cell, cluster):\n        first_step = training_set_[training_set_['h3_cell'] == h3_cell]\n        second_step = first_step[training_set_['category_cluster'] == cluster]\n\n        kw = difflib.get_close_matches(name, second_step['name'], n=3)\n\n        result = second_step[second_step['name'].isin(kw)]\n        if result['id'].empty:\n            return id_poi\n        else:\n            sep = ' '\n            match_formated = id_poi\n            for match in result['id']:\n                match_formated += sep \n                match_formated += match\n            return match_formated\n","metadata":{"execution":{"iopub.status.busy":"2022-05-19T13:07:50.140653Z","iopub.execute_input":"2022-05-19T13:07:50.141388Z","iopub.status.idle":"2022-05-19T13:07:50.155535Z","shell.execute_reply.started":"2022-05-19T13:07:50.141347Z","shell.execute_reply":"2022-05-19T13:07:50.154217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tool = tool_box()","metadata":{"execution":{"iopub.status.busy":"2022-05-19T13:07:50.158404Z","iopub.execute_input":"2022-05-19T13:07:50.158946Z","iopub.status.idle":"2022-05-19T13:07:50.177173Z","shell.execute_reply.started":"2022-05-19T13:07:50.158881Z","shell.execute_reply":"2022-05-19T13:07:50.175642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data importing (train and test sets)","metadata":{}},{"cell_type":"code","source":"training_set_ = pd.read_csv('../input/training-set-foursquare/training_set.csv')\n\ntest_set_ = pd.read_csv('../input/foursquare-location-matching/test.csv')","metadata":{"execution":{"iopub.status.busy":"2022-05-19T13:07:50.179254Z","iopub.execute_input":"2022-05-19T13:07:50.179667Z","iopub.status.idle":"2022-05-19T13:08:03.775995Z","shell.execute_reply.started":"2022-05-19T13:07:50.179608Z","shell.execute_reply":"2022-05-19T13:08:03.774983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_set_.columns\n","metadata":{"execution":{"iopub.status.busy":"2022-05-19T13:08:03.777504Z","iopub.execute_input":"2022-05-19T13:08:03.777749Z","iopub.status.idle":"2022-05-19T13:08:03.786572Z","shell.execute_reply.started":"2022-05-19T13:08:03.777721Z","shell.execute_reply":"2022-05-19T13:08:03.785983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_set_.drop(['address', 'city', 'state','zip', 'country', 'url', 'phone'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T13:08:03.787663Z","iopub.execute_input":"2022-05-19T13:08:03.788447Z","iopub.status.idle":"2022-05-19T13:08:03.849544Z","shell.execute_reply.started":"2022-05-19T13:08:03.788407Z","shell.execute_reply":"2022-05-19T13:08:03.848864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data first check","metadata":{}},{"cell_type":"code","source":"# normalizing name to avoid mismatch due to accents or punctuations\ntest_set_['name'] = test_set_['name'].astype(str)\ntest_set_['name'] = test_set_['name'].apply(unidecode)\ntest_set_['name'] = test_set_['name'].str.replace('[^\\w\\s]','',regex=True)\ntest_set_['name'] = test_set_['name'].str.lower()\n\n# normalizing categories to avoid mismatch due to accents or punctuations\ntest_set_['categories'] = test_set_['categories'].astype(str)\ntest_set_['categories'] = test_set_['categories'].str.lower()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-19T13:08:03.850615Z","iopub.execute_input":"2022-05-19T13:08:03.851322Z","iopub.status.idle":"2022-05-19T13:08:03.865074Z","shell.execute_reply.started":"2022-05-19T13:08:03.851286Z","shell.execute_reply":"2022-05-19T13:08:03.863908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"collected = gc.collect() # or gc.collect(2)\nprint(\"Garbage collector: collected\",\n          \"%d objects.\" % collected)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T13:08:03.866483Z","iopub.execute_input":"2022-05-19T13:08:03.866745Z","iopub.status.idle":"2022-05-19T13:08:04.076967Z","shell.execute_reply.started":"2022-05-19T13:08:03.866714Z","shell.execute_reply":"2022-05-19T13:08:04.075955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineer","metadata":{}},{"cell_type":"markdown","source":"## So, considering the number of categories available, it was unviable to classify all of them by myself. After some research, I found the LDA method that allows to cluster of words into categories. (check: https://towardsdatascience.com/topic-modelling-in-python-with-nltk-and-gensim-4ef03213cd21).\n\n","metadata":{}},{"cell_type":"code","source":"test_set_['category_tokenied']= test_set_['categories'].apply(lambda x: tool.tokenization(x))","metadata":{"execution":{"iopub.status.busy":"2022-05-19T13:08:04.080278Z","iopub.execute_input":"2022-05-19T13:08:04.080596Z","iopub.status.idle":"2022-05-19T13:08:04.091803Z","shell.execute_reply.started":"2022-05-19T13:08:04.080563Z","shell.execute_reply":"2022-05-19T13:08:04.090715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"id2word = corpora.Dictionary.load('../input/training-set-foursquare/dictionary.gensim') # Create Disctionary\n\nloaded_model = pickle.load(open('../input/training-set-foursquare/corpus.pkl', 'rb'))\ncorpus = loaded_model # Create Corpus\n","metadata":{"execution":{"iopub.status.busy":"2022-05-19T13:08:04.093775Z","iopub.execute_input":"2022-05-19T13:08:04.094075Z","iopub.status.idle":"2022-05-19T13:08:08.824396Z","shell.execute_reply.started":"2022-05-19T13:08:04.094041Z","shell.execute_reply":"2022-05-19T13:08:08.823378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lda_model = gensim.models.LdaMulticore.load('../input/training-set-foursquare/model_trained', mmap='r')\ndoc_lda = lda_model[corpus]","metadata":{"execution":{"iopub.status.busy":"2022-05-19T13:08:08.825612Z","iopub.execute_input":"2022-05-19T13:08:08.826021Z","iopub.status.idle":"2022-05-19T13:08:08.851291Z","shell.execute_reply.started":"2022-05-19T13:08:08.825985Z","shell.execute_reply":"2022-05-19T13:08:08.850298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let's check how to get the cluster from this model","metadata":{}},{"cell_type":"markdown","source":"## So get_document_topics returns a list of tuples with the percent of similarity with each cluster. Let's use it to apply for all the row by definig a function and them apply it. ","metadata":{}},{"cell_type":"code","source":"test_set_['category_cluster'] = test_set_.progress_apply(lambda x: tool.get_lda_cluster(x['categories']),axis=1)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-19T13:08:08.853259Z","iopub.execute_input":"2022-05-19T13:08:08.85361Z","iopub.status.idle":"2022-05-19T13:08:08.908317Z","shell.execute_reply.started":"2022-05-19T13:08:08.853546Z","shell.execute_reply":"2022-05-19T13:08:08.907293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### My approach was using the h3 lib created by Uber (check documentation here: https://github.com/uber/h3) to cluster POIs into hex using the method h3.geo_to_h3 first. It will help us to check in a certain hex all the places.  \n### It allows to search new POIs according the hex_cell id which combined to others features to solve eventually conflits, can solve this bussiness problem. ","metadata":{}},{"cell_type":"code","source":"collected = gc.collect() # or gc.collect(2)\nprint(\"Garbage collector: collected\",\n          \"%d objects.\" % collected)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T13:08:08.909724Z","iopub.execute_input":"2022-05-19T13:08:08.910373Z","iopub.status.idle":"2022-05-19T13:08:09.67357Z","shell.execute_reply.started":"2022-05-19T13:08:08.910331Z","shell.execute_reply":"2022-05-19T13:08:09.672611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating the Hex_cell ids","metadata":{}},{"cell_type":"code","source":"test_set_['h3_cell'] = test_set_.progress_apply(tool.geo_to_h3,axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T13:08:09.675121Z","iopub.execute_input":"2022-05-19T13:08:09.675386Z","iopub.status.idle":"2022-05-19T13:08:09.730524Z","shell.execute_reply.started":"2022-05-19T13:08:09.67535Z","shell.execute_reply":"2022-05-19T13:08:09.72935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"collected = gc.collect() \nprint(\"Garbage collector: collected\",\n          \"%d objects.\" % collected)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-19T13:08:09.73263Z","iopub.execute_input":"2022-05-19T13:08:09.733008Z","iopub.status.idle":"2022-05-19T13:08:10.508569Z","shell.execute_reply.started":"2022-05-19T13:08:09.732959Z","shell.execute_reply":"2022-05-19T13:08:10.507183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Defining the search engine ","metadata":{}},{"cell_type":"markdown","source":"### Here I'm going to use all the features that I created to build a engine search. First i'm going to use the hex id to filter all the places of a certain area, than I will filter all of them by the category cluster. Than, to solve eventually conflits, I'm using the difflib.get_close_matches to get the most similar name str. ","metadata":{}},{"cell_type":"code","source":"test_set_['matches'] = test_set_.apply(lambda x: tool.search_engine(x['id'], x['name'], x['h3_cell'], x['category_cluster']),axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T13:08:10.51041Z","iopub.execute_input":"2022-05-19T13:08:10.510691Z","iopub.status.idle":"2022-05-19T13:08:11.485335Z","shell.execute_reply.started":"2022-05-19T13:08:10.510657Z","shell.execute_reply":"2022-05-19T13:08:11.484313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## As we can see, it was enough to find a pretty good match for two of 5 rows of the sample","metadata":{}},{"cell_type":"code","source":"submission_sample = test_set_[['id','matches']]\n","metadata":{"execution":{"iopub.status.busy":"2022-05-19T13:08:11.487215Z","iopub.execute_input":"2022-05-19T13:08:11.488058Z","iopub.status.idle":"2022-05-19T13:08:11.496433Z","shell.execute_reply.started":"2022-05-19T13:08:11.488Z","shell.execute_reply":"2022-05-19T13:08:11.495065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"collected = gc.collect() # or gc.collect(2)\nprint(\"Garbage collector: collected\",\n          \"%d objects.\" % collected)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-19T13:08:11.498478Z","iopub.execute_input":"2022-05-19T13:08:11.498762Z","iopub.status.idle":"2022-05-19T13:08:12.275272Z","shell.execute_reply.started":"2022-05-19T13:08:11.498726Z","shell.execute_reply":"2022-05-19T13:08:12.274113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_sample","metadata":{"execution":{"iopub.status.busy":"2022-05-19T13:08:12.27668Z","iopub.execute_input":"2022-05-19T13:08:12.277053Z","iopub.status.idle":"2022-05-19T13:08:12.292372Z","shell.execute_reply.started":"2022-05-19T13:08:12.277016Z","shell.execute_reply":"2022-05-19T13:08:12.29136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sample Submittion","metadata":{}},{"cell_type":"code","source":"submission_sample.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T13:08:12.294175Z","iopub.execute_input":"2022-05-19T13:08:12.294779Z","iopub.status.idle":"2022-05-19T13:08:12.308818Z","shell.execute_reply.started":"2022-05-19T13:08:12.29473Z","shell.execute_reply":"2022-05-19T13:08:12.30775Z"},"trusted":true},"execution_count":null,"outputs":[]}]}