{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Hello Fellow Kagglers,\n\nThis competition introduced me to the techniques of matching points-of-interest and I will publish my work in the coming days.\n\nThis first notebook shows how to make sentence embeddings from the names, which are in multiple languages and can consists of multiple words.\n\nWith a multilangual sentence encoder the names can be transformed to a vector of fixed size allowing for comparing names.\n\nIn the feature engineering notebook these vectors are used to quantify the similarity between names by computing the cosine similarity between the vectors.\n\nTwo multilangual sentence encoders are used\n\n1) [universal-sentence-encoder-multilingual](https://tfhub.dev/google/universal-sentence-encoder-multilingual/3)\n\n2) [sentence-transformers/paraphrase-multilingual-mpnet-base-v2](https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2)\n\nThe dataset generation and training/inference notebook will be (hopefully) published in the coming days","metadata":{}},{"cell_type":"code","source":"# Install Dependencies\n!pip install tensorflow-text==2.6.0 sentence_transformers","metadata":{"execution":{"iopub.status.busy":"2022-06-08T17:45:02.385432Z","iopub.execute_input":"2022-06-08T17:45:02.385714Z","iopub.status.idle":"2022-06-08T17:45:46.184908Z","shell.execute_reply.started":"2022-06-08T17:45:02.385684Z","shell.execute_reply":"2022-06-08T17:45:46.183716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow_hub as hub\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_text\n\nfrom transformers import AutoTokenizer, TFXLMRobertaModel\nfrom sentence_transformers import SentenceTransformer\nfrom tqdm.notebook import tqdm\n\nimport numba\nimport os\nimport pickle\n\n# Disable automatic allocation of all GPU memory to prevent OOM after first model is loaded\nfor gpu in tf.config.experimental.list_physical_devices('GPU'):\n    tf.config.experimental.set_memory_growth(gpu, True)","metadata":{"execution":{"iopub.status.busy":"2022-06-08T18:01:29.095066Z","iopub.execute_input":"2022-06-08T18:01:29.095689Z","iopub.status.idle":"2022-06-08T18:01:29.104022Z","shell.execute_reply.started":"2022-06-08T18:01:29.095654Z","shell.execute_reply":"2022-06-08T18:01:29.102306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train Dataset","metadata":{}},{"cell_type":"code","source":"# Cast columns to \"category\" to gratly reduce memory usage\ntrain_dtype = {\n    'city': 'category',\n    'state': 'category',\n    'zip': 'category',\n    'country': 'category',\n    'url': 'category',\n    'phone': 'category',\n    'latitude': np.float32,\n    'longitude': np.float32,\n}\ntrain = pd.read_csv('/kaggle/input/foursquare-location-matching/train.csv', dtype=train_dtype, skiprows=lambda i: i>10000 and False)\n# Display Train Dataset Stats/Sample\ndisplay(train.info(memory_usage=True))\ndisplay(train.head())\ndisplay(train.memory_usage(deep=True) / len(train))","metadata":{"execution":{"iopub.status.busy":"2022-06-08T17:45:56.977544Z","iopub.execute_input":"2022-06-08T17:45:56.977987Z","iopub.status.idle":"2022-06-08T17:46:15.432321Z","shell.execute_reply.started":"2022-06-08T17:45:56.977944Z","shell.execute_reply":"2022-06-08T17:46:15.431366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert name to lowercase\ntrain['name'] = train['name'].astype(str, copy=False).str.lower().replace('nan', '')","metadata":{"execution":{"iopub.status.busy":"2022-06-08T17:46:15.43449Z","iopub.execute_input":"2022-06-08T17:46:15.435346Z","iopub.status.idle":"2022-06-08T17:46:16.500965Z","shell.execute_reply.started":"2022-06-08T17:46:15.435302Z","shell.execute_reply":"2022-06-08T17:46:16.499958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Models","metadata":{}},{"cell_type":"code","source":"# universal-sentence-encoder-multilingual\nembed_use = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\")\n\n# sentence-transformers/paraphrase-multilingual-mpnet-base-v2\nembedder_mpnet = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')","metadata":{"execution":{"iopub.status.busy":"2022-06-08T17:49:56.328806Z","iopub.execute_input":"2022-06-08T17:49:56.329121Z","iopub.status.idle":"2022-06-08T17:51:04.382084Z","shell.execute_reply.started":"2022-06-08T17:49:56.329059Z","shell.execute_reply":"2022-06-08T17:51:04.380926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Embedding Generation","metadata":{}},{"cell_type":"code","source":"# Split names in 1000 batches to prevent OOM errors\nN_SPLITS = 1000\nNAMES_UNIQUE = train['name'].astype(str).unique()\nNAMES_CHUNKS = np.array_split(\n    NAMES_UNIQUE,\n    N_SPLITS,\n)","metadata":{"execution":{"iopub.status.busy":"2022-06-08T17:51:04.384571Z","iopub.execute_input":"2022-06-08T17:51:04.384859Z","iopub.status.idle":"2022-06-08T17:51:04.77651Z","shell.execute_reply.started":"2022-06-08T17:51:04.384818Z","shell.execute_reply":"2022-06-08T17:51:04.775546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Embedding Arrays\nNAMES_EMBEDDINGS_USE = np.zeros(shape=[len(NAMES_UNIQUE), 512], dtype=np.float32)\nNAMES_EMBEDDINGS_MPNET = np.zeros(shape=[len(NAMES_UNIQUE), 768], dtype=np.float32)\n\n# Generate Embeddings, will take about 10 minutes, take a cup of coffee\nOFFSET = 0\nfor chunk in tqdm(NAMES_CHUNKS):\n    n = len(chunk)\n    NAMES_EMBEDDINGS_USE[OFFSET:OFFSET + n] = embed_use(chunk)\n    NAMES_EMBEDDINGS_MPNET[OFFSET:OFFSET + n] = embedder_mpnet.encode(chunk, show_progress_bar=False)\n    OFFSET += n","metadata":{"execution":{"iopub.status.busy":"2022-06-08T17:51:04.778127Z","iopub.execute_input":"2022-06-08T17:51:04.778422Z","iopub.status.idle":"2022-06-08T18:00:40.658502Z","shell.execute_reply.started":"2022-06-08T17:51:04.778384Z","shell.execute_reply":"2022-06-08T18:00:40.657533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save embeddings\nnp.save('NAMES_EMBEDDINGS_USE.npy', NAMES_EMBEDDINGS_USE)\nnp.save('NAMES_EMBEDDINGS_MPNET.npy', NAMES_EMBEDDINGS_MPNET)","metadata":{"execution":{"iopub.status.busy":"2022-06-08T18:00:40.6609Z","iopub.execute_input":"2022-06-08T18:00:40.661816Z","iopub.status.idle":"2022-06-08T18:00:55.347515Z","shell.execute_reply.started":"2022-06-08T18:00:40.661757Z","shell.execute_reply":"2022-06-08T18:00:55.346286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Name to Embedding Index Dictionary","metadata":{}},{"cell_type":"code","source":"# Name to Embedding Index dictionary\nname2names_embedding_idx_dict  = dict([(a, b) for a, b in zip(NAMES_UNIQUE, np.arange(len(NAMES_UNIQUE)) )])\n\n# Save dictionary as pickle\nwith open('name2names_embedding_idx_dict.pkl', 'wb') as file:\n    pickle.dump(name2names_embedding_idx_dict, file)","metadata":{"execution":{"iopub.status.busy":"2022-06-08T18:01:51.49688Z","iopub.execute_input":"2022-06-08T18:01:51.497365Z","iopub.status.idle":"2022-06-08T18:01:56.230738Z","shell.execute_reply.started":"2022-06-08T18:01:51.497218Z","shell.execute_reply":"2022-06-08T18:01:56.229618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Embedding Index to Name Index dictionary\nnames_embedding_idx2name_dict  = dict([(a, b) for a, b in zip(np.arange(len(NAMES_UNIQUE)), NAMES_UNIQUE)])\n\n# Save dictionary as pickle\nwith open('names_embedding_idx2name_dict.pkl', 'wb') as file:\n    pickle.dump(names_embedding_idx2name_dict, file)","metadata":{"execution":{"iopub.status.busy":"2022-06-08T18:09:28.579547Z","iopub.execute_input":"2022-06-08T18:09:28.579822Z","iopub.status.idle":"2022-06-08T18:09:33.794861Z","shell.execute_reply.started":"2022-06-08T18:09:28.579791Z","shell.execute_reply":"2022-06-08T18:09:33.79386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Similarity Statistics\n\nThe goal of these statistics is to show similair names can be identified using the cosine similarity between embeddings","metadata":{}},{"cell_type":"code","source":"# Optimized cosine similarity function\n@numba.jit(nopython=True)\ndef cosine_similarity(a, b):\n    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))","metadata":{"execution":{"iopub.status.busy":"2022-06-08T18:20:54.652151Z","iopub.execute_input":"2022-06-08T18:20:54.652611Z","iopub.status.idle":"2022-06-08T18:20:54.663127Z","shell.execute_reply.started":"2022-06-08T18:20:54.652563Z","shell.execute_reply":"2022-06-08T18:20:54.662108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use 1 million random pairs of names to get some statistics on cosine similarities between names\nN = int(1e6)\n# Generate 1 million random pairs without \nnp.random.seed(11)\nidxs = np.random.randint(low=0, high=len(NAMES_UNIQUE), size=[N, 2])\nprint(f'Number of Duplicates: {(idxs[:,0] == idxs[:,1]).sum()}')\n# arrays to save similarities\nsimilarities_use = np.empty(shape=N, dtype=np.float32)\nsimilarities_mpnet = np.empty(shape=N, dtype=np.float32)\n\n# Compute cosine similarities\nfor idx, (a_idx, b_idx) in enumerate(tqdm(idxs)):\n    similarities_use[idx] = (\n            cosine_similarity(NAMES_EMBEDDINGS_USE[a_idx], NAMES_EMBEDDINGS_USE[b_idx])\n        )\n    \n    similarities_mpnet[idx] = (\n            cosine_similarity(NAMES_EMBEDDINGS_MPNET[a_idx], NAMES_EMBEDDINGS_MPNET[b_idx])\n        )","metadata":{"execution":{"iopub.status.busy":"2022-06-08T18:05:45.332383Z","iopub.execute_input":"2022-06-08T18:05:45.33266Z","iopub.status.idle":"2022-06-08T18:05:57.294514Z","shell.execute_reply.started":"2022-06-08T18:05:45.33263Z","shell.execute_reply":"2022-06-08T18:05:57.293447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Similarities for Universal Sentence Encoder embeddings\ndisplay(pd.Series(similarities_use).describe().to_frame(name='Value'))","metadata":{"execution":{"iopub.status.busy":"2022-06-08T18:07:33.991163Z","iopub.execute_input":"2022-06-08T18:07:33.991657Z","iopub.status.idle":"2022-06-08T18:07:34.048634Z","shell.execute_reply.started":"2022-06-08T18:07:33.991612Z","shell.execute_reply":"2022-06-08T18:07:34.047629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Similarities for MPNET embeddings\ndisplay(pd.Series(similarities_mpnet).describe().to_frame(name='Value'))","metadata":{"execution":{"iopub.status.busy":"2022-06-08T18:07:34.195877Z","iopub.execute_input":"2022-06-08T18:07:34.196241Z","iopub.status.idle":"2022-06-08T18:07:34.253938Z","shell.execute_reply.started":"2022-06-08T18:07:34.196197Z","shell.execute_reply":"2022-06-08T18:07:34.25296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Similar/Dissimilair Names\n\nGet an impression of the similar and dissimilar names according to the sentence embedders.\n\nVerification is quite hard, as many names are non-English.","metadata":{}},{"cell_type":"code","source":"# Similar names according to Universal Sentence Encoder\nuse_similar_rows = []\nfor idx in np.argwhere(similarities_use > 0.80).squeeze()[:10]:\n    use_similar_rows.append({\n        'name_1': names_embedding_idx2name_dict[idxs[idx][0]],\n        'name_2': names_embedding_idx2name_dict[idxs[idx][1]],\n        'cosine_similarity': similarities_use[idx],\n    })\n    \ndisplay(pd.DataFrame(use_similar_rows))","metadata":{"execution":{"iopub.status.busy":"2022-06-08T18:14:15.600085Z","iopub.execute_input":"2022-06-08T18:14:15.600375Z","iopub.status.idle":"2022-06-08T18:14:15.618467Z","shell.execute_reply.started":"2022-06-08T18:14:15.600345Z","shell.execute_reply":"2022-06-08T18:14:15.617268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dissimilar names according to Universal Sentence Encoder\nuse_dissimilar_rows = []\nfor idx in np.argwhere(similarities_use < 0.00).squeeze()[:10]:\n    use_dissimilar_rows.append({\n        'name_1': names_embedding_idx2name_dict[idxs[idx][0]],\n        'name_2': names_embedding_idx2name_dict[idxs[idx][1]],\n        'cosine_similarity_use': similarities_use[idx],\n    })\n    \ndisplay(pd.DataFrame(use_dissimilar_rows))","metadata":{"execution":{"iopub.status.busy":"2022-06-08T18:18:07.654657Z","iopub.execute_input":"2022-06-08T18:18:07.654969Z","iopub.status.idle":"2022-06-08T18:18:07.677217Z","shell.execute_reply.started":"2022-06-08T18:18:07.654938Z","shell.execute_reply":"2022-06-08T18:18:07.676179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Similar names according to MPNET\nmpnet_similar_rows = []\nfor idx in np.argwhere(similarities_mpnet > 0.90).squeeze()[:10]:\n    mpnet_similar_rows.append({\n        'name_1': names_embedding_idx2name_dict[idxs[idx][0]],\n        'name_2': names_embedding_idx2name_dict[idxs[idx][1]],\n        'cosine_similarity_mpnet': similarities_mpnet[idx],\n    })\n    \ndisplay(pd.DataFrame(mpnet_similar_rows))","metadata":{"execution":{"iopub.status.busy":"2022-06-08T18:17:49.762335Z","iopub.execute_input":"2022-06-08T18:17:49.762607Z","iopub.status.idle":"2022-06-08T18:17:49.778676Z","shell.execute_reply.started":"2022-06-08T18:17:49.762578Z","shell.execute_reply":"2022-06-08T18:17:49.777491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dissimilar names according to MPNET\nmpnet_dissimilar_rows = []\nfor idx in np.argwhere(similarities_mpnet < 0.00).squeeze()[:10]:\n    mpnet_dissimilar_rows.append({\n        'name_1': names_embedding_idx2name_dict[idxs[idx][0]],\n        'name_2': names_embedding_idx2name_dict[idxs[idx][1]],\n        'cosine_similarity_mpnet': similarities_mpnet[idx],\n    })\n    \ndisplay(pd.DataFrame(mpnet_dissimilar_rows))","metadata":{"execution":{"iopub.status.busy":"2022-06-08T18:17:55.557397Z","iopub.execute_input":"2022-06-08T18:17:55.557662Z","iopub.status.idle":"2022-06-08T18:17:55.57519Z","shell.execute_reply.started":"2022-06-08T18:17:55.557633Z","shell.execute_reply":"2022-06-08T18:17:55.574112Z"},"trusted":true},"execution_count":null,"outputs":[]}]}