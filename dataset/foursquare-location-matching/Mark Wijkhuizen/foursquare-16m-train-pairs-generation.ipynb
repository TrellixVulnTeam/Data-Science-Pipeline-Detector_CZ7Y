{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Hello Fellow Kagglers,\n\nThis is notebook demonstrates the generation of 16 million training pairs, using 15 neighbours.\n\nFor each point of interest all combination of pairs are added to the training data, as are the 15 nearest neighbours.\n\nThis training set is much larger than the 600K pairs provided in the competition dataset.\n\nMoreover, the provided pairs dataset is heavily skewed towards positive samples (68% matches), whereas the generated pairs dataset containing all pairs and all 15 nearest neighbours consists of just 11% positive samples, making the training dataset more representable for the expected pairs when performing inference on the 15 nearest neighbours.\n\n15 neighbours are chosen to make the training dataset as large as possible, while still fitting in the 16GB notebooks when computing features and training.\n\nA quick analysis is provided to demonstrate the marginal value of additional nearest neighbours.\n\nAs expected, the match ratio decreases as the number of neighbours increases.\n\nOther notebooks:\n\n[Foursquare USE/MPNET Name Embeddings](https://www.kaggle.com/code/markwijkhuizen/foursquare-use-mpnet-name-embeddings)\n\nTraining/Inference notebook coming soon!","metadata":{}},{"cell_type":"code","source":"# Used to deduce city/country from coordinates\n!pip install /kaggle/input/reversegeocode/reverse_geocode-1.4.1-py3-none-any.whl","metadata":{"execution":{"iopub.status.busy":"2022-06-09T06:50:01.335959Z","iopub.execute_input":"2022-06-09T06:50:01.336556Z","iopub.status.idle":"2022-06-09T06:50:32.629568Z","shell.execute_reply.started":"2022-06-09T06:50:01.336432Z","shell.execute_reply":"2022-06-09T06:50:32.628295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow_hub as hub\nimport tensorflow as tf\nimport tensorflow_addons as tfa\n\nfrom tensorflow.keras import backend as K\nfrom Levenshtein import distance as lev\nfrom tqdm.notebook import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom scipy.spatial import distance\nfrom sklearn import metrics\nfrom multiprocessing import cpu_count\nfrom sklearn.neighbors import BallTree\nfrom difflib import SequenceMatcher\n\nimport geopy.distance\nimport reverse_geocode\nimport math\nimport scipy\nimport numba\nimport warnings\nimport Levenshtein\nimport itertools\nimport gc\nimport psutil\nimport sys\n\n# Pandas Apply With Progress Bar\ntqdm.pandas()\n\n# Plot DPI\nimport matplotlib as mpl\nmpl.rcParams['figure.dpi'] = 150\n\n# Tensorflow Version\nprint(f'Tensorflow version {tf.__version__}')\n\n# Ignore Warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-06-09T06:50:32.632262Z","iopub.execute_input":"2022-06-09T06:50:32.632888Z","iopub.status.idle":"2022-06-09T06:50:42.894692Z","shell.execute_reply.started":"2022-06-09T06:50:32.632834Z","shell.execute_reply":"2022-06-09T06:50:42.893617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SEED = 42\nDEBUG = False\n# Earths Radius in KM\nEARTH_RADIUS = 6371","metadata":{"execution":{"iopub.status.busy":"2022-06-09T06:50:42.896154Z","iopub.execute_input":"2022-06-09T06:50:42.896861Z","iopub.status.idle":"2022-06-09T06:50:42.903227Z","shell.execute_reply.started":"2022-06-09T06:50:42.896829Z","shell.execute_reply":"2022-06-09T06:50:42.901469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Train/Test Data","metadata":{}},{"cell_type":"code","source":"%%time\n# Train\ntrain_dtype = {\n    'id': 'category',\n    'name': 'category',\n    'address': 'category',\n    'city': 'category',\n    'state': 'category',\n    'zip': 'category',\n    'country': 'category',\n    'url': 'category',\n    'phone': 'category',\n    'categories': 'category',\n    'latitude': np.float32,\n    'longitude': np.float32,\n}\ntrain = pd.read_csv('/kaggle/input/foursquare-location-matching/train.csv', index_col='id', dtype=train_dtype)\ntrain['id'] = train.index.values\ndisplay(train.info(memory_usage=True))\ndisplay(train.head())\ndisplay(train.memory_usage(deep=True) / len(train))\n\n# Test\ntest_usecols = [\n    'id',\n    'name',\n    'latitude',\n    'longitude',\n    'address',\n    'city',\n    'state',\n    'zip',\n    'country',\n    'url'\n    'categories',\n]\n\ntest = pd.read_csv('/kaggle/input/foursquare-location-matching/test.csv', dtype=train_dtype)\ndisplay(test.info())\ndisplay(test.head())","metadata":{"execution":{"iopub.status.busy":"2022-06-09T06:50:42.905397Z","iopub.execute_input":"2022-06-09T06:50:42.905722Z","iopub.status.idle":"2022-06-09T06:50:48.025234Z","shell.execute_reply.started":"2022-06-09T06:50:42.905694Z","shell.execute_reply":"2022-06-09T06:50:48.023785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Pairs","metadata":{}},{"cell_type":"code","source":"# Pairs\npairs_dtype = {\n    'id_1': 'category',\n    'id_2': 'category',\n    'name_1': 'category',\n    'name_2': 'category',\n    'address_1': 'category',\n    'address_1': 'category',\n    'city_1': 'category',\n    'city_2': 'category',\n    'state_1': 'category',\n    'state_2': 'category',\n    'zip_1': 'category',\n    'zip_2': 'category',\n    'country_1': 'category',\n    'country_2': 'category',\n    'url_1': 'category',\n    'url_2': 'category',\n    'phone_1': 'category',\n    'phone_2': 'category',\n    'categories_1': 'category',\n    'categories_2': 'category',\n    'latitude_1': np.float32,\n    'longitude_1': np.float32,\n    'latitude_2': np.float32,\n    'longitude_2': np.float32,\n}\npd.options.display.max_rows = 99\npd.options.display.max_columns = 99\n\npairs_sample = pd.read_csv('/kaggle/input/foursquare-location-matching/pairs.csv', dtype=pairs_dtype, skiprows=lambda idx: idx > 5)\ndisplay(pairs_sample.info())\ndisplay(pairs_sample.head())","metadata":{"execution":{"iopub.status.busy":"2022-06-09T06:50:48.026868Z","iopub.execute_input":"2022-06-09T06:50:48.027252Z","iopub.status.idle":"2022-06-09T06:50:52.031192Z","shell.execute_reply.started":"2022-06-09T06:50:48.027221Z","shell.execute_reply":"2022-06-09T06:50:52.030247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Sample Submission","metadata":{}},{"cell_type":"code","source":"# Sample Submission\nsample_submission = pd.read_csv('/kaggle/input/foursquare-location-matching/sample_submission.csv')\ndisplay(sample_submission.info())\ndisplay(sample_submission.head())","metadata":{"execution":{"iopub.status.busy":"2022-06-09T06:50:52.032697Z","iopub.execute_input":"2022-06-09T06:50:52.033141Z","iopub.status.idle":"2022-06-09T06:50:52.056609Z","shell.execute_reply.started":"2022-06-09T06:50:52.033099Z","shell.execute_reply":"2022-06-09T06:50:52.055613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# To Lower","metadata":{}},{"cell_type":"code","source":"# Cast columns to lower case to be case agnostic\nto_lower_columns = [\n    'name',\n    'state',\n    'country',\n    'city',\n    'address',\n    'zip',\n    'phone',\n    'url',\n    'categories',\n]\n\ndef to_lower(df):\n    f = lambda v: '' if v == 'NaN' else v.lower()\n    for col in to_lower_columns:\n        if f'{col}_1' in df and f'{col}_2' in df:\n            df[f'{col}_1'] = df[f'{col}_1'].astype(str, copy=False).str.lower().replace('nan', '').astype('category')\n            df[f'{col}_2'] = df[f'{col}_2'].astype(str, copy=False).str.lower().replace('nan', '').astype('category')\n        else:\n            df[col] = df[col].astype(str, copy=False).str.lower().replace('nan', '').astype('category')\n            \nto_lower(train)\nto_lower(pairs_sample)\nto_lower(test)","metadata":{"execution":{"iopub.status.busy":"2022-06-09T06:50:52.057929Z","iopub.execute_input":"2022-06-09T06:50:52.058226Z","iopub.status.idle":"2022-06-09T06:50:52.210785Z","shell.execute_reply.started":"2022-06-09T06:50:52.0582Z","shell.execute_reply":"2022-06-09T06:50:52.209708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(train.head())","metadata":{"execution":{"iopub.status.busy":"2022-06-09T06:50:52.212351Z","iopub.execute_input":"2022-06-09T06:50:52.212803Z","iopub.status.idle":"2022-06-09T06:50:52.233256Z","shell.execute_reply.started":"2022-06-09T06:50:52.21276Z","shell.execute_reply":"2022-06-09T06:50:52.232297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(pairs_sample.head())","metadata":{"execution":{"iopub.status.busy":"2022-06-09T06:50:52.234395Z","iopub.execute_input":"2022-06-09T06:50:52.235146Z","iopub.status.idle":"2022-06-09T06:50:52.268426Z","shell.execute_reply.started":"2022-06-09T06:50:52.235112Z","shell.execute_reply":"2022-06-09T06:50:52.26781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(test.head())","metadata":{"execution":{"iopub.status.busy":"2022-06-09T06:50:52.270123Z","iopub.execute_input":"2022-06-09T06:50:52.270558Z","iopub.status.idle":"2022-06-09T06:50:52.289819Z","shell.execute_reply.started":"2022-06-09T06:50:52.270526Z","shell.execute_reply":"2022-06-09T06:50:52.288669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Number of Neighbours Analysis","metadata":{}},{"cell_type":"code","source":"def plot_n_matches_by_n_neighbours():\n    # Mapping a point of interest to all corresponding ids\n    poi2ids = train.groupby('point_of_interest')['id'].apply(set).apply(list).apply(np.sort).to_dict()\n    # The famous nearest neighbours graph used in all notebooks\n    tree = BallTree(np.deg2rad(train[['latitude', 'longitude']].values), metric='haversine')\n    N_NEIGHBOURS_CHECK = 31\n\n    matches_count_dict = dict([(n,dict()) for n in range(1, N_NEIGHBOURS_CHECK)])\n    for n in range(1, N_NEIGHBOURS_CHECK):\n        matches_count_dict[n]['sum'] = 0\n        matches_count_dict[n]['count'] = 0\n\n    for row_idx, row in tqdm(train.iterrows(), total=len(train)):\n        row_dict = {}\n        distances, indices = tree.query(\n                    np.deg2rad([row['latitude'], row['longitude']]).reshape(1, -1),\n                    k=N_NEIGHBOURS_CHECK,\n                )\n        indices = indices[0]\n        match_ids = set(poi2ids[row['point_of_interest']])\n        query_ids = train.iloc[indices]['id'].values\n        for n in range(1, N_NEIGHBOURS_CHECK):\n            for ind in query_ids[:n]:\n                matches_count_dict[n]['count'] += 1\n                if ind in match_ids:\n                    matches_count_dict[n]['sum'] += 1\n\n    for n in range(1, N_NEIGHBOURS_CHECK):\n        matches_count_dict[n]['total_match_ratio (%)'] = matches_count_dict[n]['sum'] / matches_count_dict[n]['count'] * 100\n\n\n    matches_count_dict_df = pd.DataFrame(matches_count_dict).T.astype({'sum': np.int32, 'count': np.int32})\n    # Add Marginal Sum\n    matches_count_dict_df['sum_marginal'] = (\n            matches_count_dict_df['sum'] - matches_count_dict_df['sum'].shift(1)\n        ).fillna(matches_count_dict_df.loc[1, 'sum']).astype(np.int32)\n    \n    # Add Non Self Sum\n    matches_count_dict_df['sum_non_self'] = (matches_count_dict_df['sum'] - len(train)).astype(np.int32)\n    \n    # Add Percentage Ratio\n    matches_count_dict_df['match_ratio (%)'] = ((matches_count_dict_df['sum_marginal'] / len(train)) * 100 ).astype(np.float32)\n\n    pd.options.display.max_rows = 999\n    display(matches_count_dict_df)\n\n    plt.figure(figsize=(12,5))\n    plt.title('Number of Matches by Number of Neighbours', size=18)\n    plt.plot(matches_count_dict_df['sum'])\n    plt.xlabel('Number of Neighbours', size=16)\n    plt.xlabel('Number of Matches', size=16)\n    plt.xticks(size=12)\n    plt.yticks(size=12)\n    plt.ylim(0)\n    plt.grid()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-09T06:56:01.36135Z","iopub.execute_input":"2022-06-09T06:56:01.361771Z","iopub.status.idle":"2022-06-09T06:56:01.381004Z","shell.execute_reply.started":"2022-06-09T06:56:01.361741Z","shell.execute_reply":"2022-06-09T06:56:01.379898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nsum:                   total matches\ncount:                 total pairs\ntotal_match_ratio (%): global percentage of matches in pairs (positive ratio)\nsum_marginal:          pairs in current nearest neighbour\nsum_non_self:          total pairs excluding pairs referring to itself\nmatch_ratio (%) :      match ratio in current nearest neighbouir\n\"\"\"\n\n# As can be observed, the match ratio quickly decreases\nplot_n_matches_by_n_neighbours()","metadata":{"execution":{"iopub.status.busy":"2022-06-09T06:50:52.311576Z","iopub.execute_input":"2022-06-09T06:50:52.311881Z","iopub.status.idle":"2022-06-09T06:51:13.275097Z","shell.execute_reply.started":"2022-06-09T06:50:52.311853Z","shell.execute_reply":"2022-06-09T06:51:13.274163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Construct Pairs","metadata":{}},{"cell_type":"code","source":"# Source Columns\ncolumns = ['id']\n\n# Target Column 1\ncolumns_1 = ['id_1']\n\n# Target Column 2\ncolumns_2 = ['id_2']","metadata":{"execution":{"iopub.status.busy":"2022-06-09T06:57:47.541553Z","iopub.execute_input":"2022-06-09T06:57:47.541961Z","iopub.status.idle":"2022-06-09T06:57:47.546706Z","shell.execute_reply.started":"2022-06-09T06:57:47.541928Z","shell.execute_reply":"2022-06-09T06:57:47.545838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add a pair for 2 given ids in the training set\ndef add_train_sample(pairs_dict, id_1, id_2, match, count):\n    pairs_dict['match'][count] = match\n    pairs_dict['match'][count + 1] = match\n    row_1 = train.loc[id_1, columns]\n    row_2 = train.loc[id_2, columns]\n    for col_idx, (col, col_1, col_2) in enumerate(zip(columns, columns_1, columns_2)):\n        if col_idx == 1 or col_idx == 2:\n            pairs_dict[col_1][count] = row_1[col_idx]\n            pairs_dict[col_2][count] = row_2[col_idx]\n            pairs_dict[col_1][count + 1] = row_2[col_idx]\n            pairs_dict[col_2][count + 1] = row_1[col_idx]\n        else:\n            pairs_dict[col_1][count] = row_1[col_idx]\n            pairs_dict[col_2][count] = row_2[col_idx]\n            pairs_dict[col_1][count + 1] = row_2[col_idx]\n            pairs_dict[col_2][count + 1] = row_1[col_idx]","metadata":{"execution":{"iopub.status.busy":"2022-06-09T06:57:48.289902Z","iopub.execute_input":"2022-06-09T06:57:48.290666Z","iopub.status.idle":"2022-06-09T06:57:48.298212Z","shell.execute_reply.started":"2022-06-09T06:57:48.290618Z","shell.execute_reply":"2022-06-09T06:57:48.297547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate the pairs DataFrame\ndef get_pairs():\n    # point of interest to corresponding ids mapping\n    poi2ids = train.groupby('point_of_interest')['id'].apply(set).apply(list).apply(np.sort).to_dict()\n    # nearest neighbours graph\n    tree = BallTree(np.deg2rad(train[['latitude', 'longitude']].values), metric='haversine')\n    # point of interest cluster size count\n    display(pd.Series([len(v) for _, v in poi2ids.items()]).value_counts().head(10))\n    # As it is unknown how many pairs will be generated, set the target array size to a size value of 20M\n    N_TRAIN_SAMPLES = int(20e6)\n    # Number of neighbours to use\n    N_NEIGHBOURS = 15\n    NEIGHBOURS_IDXS = np.arange(1, N_NEIGHBOURS, dtype=np.int8)\n    # Set with pairs id hashes to prevent duplicate training samples\n    IDS_SET = set()\n    # Dictionary with dataframe columns\n    pairs_dict = {\n        'match': np.full(shape=N_TRAIN_SAMPLES, fill_value=np.nan, dtype=np.bool)\n    }\n\n    # Create empty array to fill up when generating pairs\n    for col in columns_1 + columns_2:\n        dtype = object if pd.api.types.is_categorical_dtype(pairs_sample[col]) else pairs_sample[col].dtype\n        pairs_dict[col] = np.full(shape=N_TRAIN_SAMPLES, fill_value=np.nan, dtype=dtype)\n\n    # Counter to keep track of the row to be filled\n    count = 0\n    # Iterate over all points of interest\n    for poi, ids in tqdm(poi2ids.items()):\n        # check if there are any pairs, thus more than 1 id in the point of interest\n        if len(ids) > 1:\n            # Generate training pair for all combinations\n            for id_1, id_2 in list(itertools.combinations(ids, 2)):\n                id_hash = hash(id_1 + id_2)\n                if id_hash not in IDS_SET:\n                    add_train_sample(pairs_dict, id_1, id_2, True, count)\n                    count += 2\n                    # Add hash of id1 and id2 to prevent duplicate training pairs\n                    IDS_SET.update([id_hash, hash(id_2 + id_1)])\n\n        # Get 15 nearest neighbours\n        distances, indices = tree.query(\n                np.deg2rad([train.loc[ids[0], 'latitude'], train.loc[ids[0], 'longitude']]).reshape(1, -1),\n                k=N_NEIGHBOURS,\n            )\n        # Only add neighbours within 10KM and neighbours which are not matches\n        indices = indices[0][ distances[0] < 10 / EARTH_RADIUS]\n        query_ids = train.iloc[indices]['id'].tolist()\n        query_ids = [e for e in query_ids if e not in ids]\n        # Add all negative examples\n        for id_2 in query_ids:\n            id_hash = hash(ids[0] + id_2)\n            if id_hash not in IDS_SET:\n                add_train_sample(pairs_dict, ids[0], id_2, False, count)\n                count += 2\n                IDS_SET.update([id_hash, hash(id_2 + ids[0])])\n    \n    print(f'Generated {count} Training Samples!')\n    return pd.DataFrame(dict([ (k, v[:count]) for k, v in pairs_dict.items()]))","metadata":{"execution":{"iopub.status.busy":"2022-06-09T07:06:23.372918Z","iopub.execute_input":"2022-06-09T07:06:23.373436Z","iopub.status.idle":"2022-06-09T07:06:23.392456Z","shell.execute_reply.started":"2022-06-09T07:06:23.373384Z","shell.execute_reply":"2022-06-09T07:06:23.391273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This is purely an dataframe with pairs of ids!\npairs = get_pairs()\n\n# Columns to fill\ncolumns = [\n    'name', 'latitude', 'longitude', 'address', 'city',\n    'state', 'zip', 'country', 'url', 'phone', 'categories',\n]\n\ncolumns_1 = [\n    'name_1', 'latitude_1', 'longitude_1', 'address_1', 'city_1',\n    'state_1', 'zip_1', 'country_1', 'url_1', 'phone_1', 'categories_1',\n]\n\ncolumns_2 = [\n    'name_2', 'latitude_2', 'longitude_2', 'address_2', 'city_2',\n    'state_2', 'zip_2', 'country_2', 'url_2', 'phone_2', 'categories_2'\n]\n\n# Fill pairs columns based on id\nfor col, col_1, col_2 in zip(tqdm(columns), columns_1, columns_2):\n    pairs[col_1] = train.loc[pairs['id_1'], col].values\n    pairs[col_2] = train.loc[pairs['id_2'], col].values\n\n# Add hash column of id_1 and id_2 to validate unique training samples\npairs['id_hash'] = pairs[['id_1', 'id_2']].apply(lambda t: hash(t[0] + t[1]), axis=1)\n\n# Cast ID to category\npairs['id_1'] = pairs['id_1'].astype('category')\npairs['id_2'] = pairs['id_2'].astype('category')\n\n# Display Pairs Data\ndisplay(pairs.head(25))\ndisplay(pairs.info())\n\n# Display Positive/Negative Sample Ratio's\ndisplay(pairs['match'].value_counts(normalize=True).to_frame())\n\n# Unique Names\ndisplay(pairs[['name_1', 'name_2']].nunique())","metadata":{"execution":{"iopub.status.busy":"2022-06-09T07:06:23.513248Z","iopub.execute_input":"2022-06-09T07:06:23.514048Z","iopub.status.idle":"2022-06-09T07:07:45.78532Z","shell.execute_reply.started":"2022-06-09T07:06:23.513998Z","shell.execute_reply":"2022-06-09T07:07:45.784244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Validate unique pairs\ndisplay(pairs['id_hash'].value_counts().value_counts(normalize=True) * 100)","metadata":{"execution":{"iopub.status.busy":"2022-06-09T07:07:45.787091Z","iopub.execute_input":"2022-06-09T07:07:45.787727Z","iopub.status.idle":"2022-06-09T07:07:45.80507Z","shell.execute_reply.started":"2022-06-09T07:07:45.787683Z","shell.execute_reply":"2022-06-09T07:07:45.804311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Memory Usage Analysis, category data type is highly efficient!\ndisplay(pairs.memory_usage(deep=True) / len(pairs))","metadata":{"execution":{"iopub.status.busy":"2022-06-09T07:07:45.806215Z","iopub.execute_input":"2022-06-09T07:07:45.806742Z","iopub.status.idle":"2022-06-09T07:07:45.826387Z","shell.execute_reply.started":"2022-06-09T07:07:45.80671Z","shell.execute_reply":"2022-06-09T07:07:45.825368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop id_hash\npairs.drop('id_hash', axis=1, inplace=True)\n\n# Save Training Pairs\npairs.to_pickle('pairs.pkl')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Haversine Distance\n\nTo finish with, a simple visualisation of random pairs and matching pairs. Matching pairs are almost always within 10KM of eachother, whereas non-matching pairs are not. The distance between pairs is thus an important indicator of pairs being a match.","metadata":{}},{"cell_type":"code","source":"@numba.jit(nopython=True)\ndef haversine_np(args):\n    \"\"\"\n    Calculate the great circle distance between two points\n    on the earth (specified in decimal degrees)\n\n    All args must be of equal length.    \n\n    \"\"\"\n    lon1, lat1, lon2, lat2 = args\n    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n\n    dlon = lon2 - lon1\n    dlat = lat2 - lat1\n\n    a = np.sin(dlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2.0)**2\n\n    c = 2 * np.arcsin(np.sqrt(a))\n    km = EARTH_RADIUS * c\n    return km","metadata":{"execution":{"iopub.status.busy":"2022-06-09T07:07:50.415589Z","iopub.execute_input":"2022-06-09T07:07:50.415976Z","iopub.status.idle":"2022-06-09T07:07:50.758705Z","shell.execute_reply.started":"2022-06-09T07:07:50.415944Z","shell.execute_reply":"2022-06-09T07:07:50.757737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_distances():\n    # Get 1 random pairs of points from train\n    N = int(1e6)\n    # Target array for train distances\n    TRAIN_DISTANCES = np.zeros(N, dtype=np.float32)\n    # get random train coordinates and compute distances\n    coords_a = train[['longitude', 'latitude']].sample(N, replace=True).values\n    coords_b = train[['longitude', 'latitude']].sample(N, replace=True).values\n    for idx, (a, b) in enumerate(tqdm(zip(coords_a, coords_b), total=N)):\n        TRAIN_DISTANCES[idx] = haversine_np((*a, *b))\n        \n    # plot distance distribution\n    plt.figure(figsize=(10,5))\n    plt.title('Train Haversine Distance in KM')\n    pd.Series(TRAIN_DISTANCES).plot(kind='hist', bins=10)\n    plt.show()\n    \n    # Compute Distances for matching pairs only\n    pairs_distance = pd.Series(\n            np.apply_along_axis(\n                haversine_np,\n                1,\n                pairs.loc[pairs['match'], ['longitude_1', 'latitude_1', 'longitude_2', 'latitude_2']].values.astype(np.float32)\n            )\n        )\n    \n    # Plot Distances between pairs\n    plt.figure(figsize=(10,5))\n    plt.title('Pairs Haversine Distance in KM')\n    pairs_distance.plot(kind='hist', bins=10)\n    plt.show()\n    \n    # Plot train/pairs/pairs_sample distance statistics\n    percentiles = [0.01, 0.05, 0.10, 0.25, 0.90, 0.95, 0.99]\n    display(\n        pd.concat([\n            pd.Series(TRAIN_DISTANCES).describe(percentiles=percentiles).to_frame(name='Train').T,\n            pairs_distance.describe(percentiles=percentiles).to_frame(name='Pairs').T,\n        ]).T\n    )","metadata":{"execution":{"iopub.status.busy":"2022-06-09T07:09:46.534747Z","iopub.execute_input":"2022-06-09T07:09:46.535103Z","iopub.status.idle":"2022-06-09T07:09:46.545475Z","shell.execute_reply.started":"2022-06-09T07:09:46.535073Z","shell.execute_reply":"2022-06-09T07:09:46.544599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_distances()","metadata":{"execution":{"iopub.status.busy":"2022-06-09T07:09:47.474732Z","iopub.execute_input":"2022-06-09T07:09:47.475116Z","iopub.status.idle":"2022-06-09T07:09:51.81074Z","shell.execute_reply.started":"2022-06-09T07:09:47.475086Z","shell.execute_reply":"2022-06-09T07:09:51.809767Z"},"trusted":true},"execution_count":null,"outputs":[]}]}