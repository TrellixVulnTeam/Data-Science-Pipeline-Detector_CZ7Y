{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!unzip ../input/foursquare-utils-checkpoints-v4/pykakasi_deps.dontopenthiskaggle -d .","metadata":{"execution":{"iopub.status.busy":"2022-06-13T20:53:15.342488Z","iopub.execute_input":"2022-06-13T20:53:15.34295Z","iopub.status.idle":"2022-06-13T20:53:15.721694Z","shell.execute_reply.started":"2022-06-13T20:53:15.342876Z","shell.execute_reply":"2022-06-13T20:53:15.720998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!conda install ./pykakasi_deps/offline_pykakasi.tar.bz2\n!conda install ./pykakasi_deps/offline_jaconv.tar.bz2\n!conda install ./pykakasi_deps/offline_deprecated.tar.bz2","metadata":{"execution":{"iopub.status.busy":"2022-06-13T20:53:15.72356Z","iopub.execute_input":"2022-06-13T20:53:15.724343Z","iopub.status.idle":"2022-06-13T20:53:48.560216Z","shell.execute_reply.started":"2022-06-13T20:53:15.724302Z","shell.execute_reply":"2022-06-13T20:53:48.559103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"UTIL_PATH = \"../input/foursquare-utils-checkpoints-v4\"\nDF_PATH = \"../input/foursquare-location-matching/test.csv\"\nSUB_PATH = \"../input/foursquare-location-matching/sample_submission.csv\"","metadata":{"execution":{"iopub.status.busy":"2022-06-13T20:53:48.561855Z","iopub.execute_input":"2022-06-13T20:53:48.563088Z","iopub.status.idle":"2022-06-13T20:53:48.568746Z","shell.execute_reply.started":"2022-06-13T20:53:48.563037Z","shell.execute_reply":"2022-06-13T20:53:48.567611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearnex import patch_sklearn\n\npatch_sklearn()\n\nfrom sklearn.neighbors import NearestNeighbors, BallTree\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nimport pandas as pd\nimport numpy as np\nimport cython\n\nimport Levenshtein\nimport difflib\nfrom tqdm.notebook import tqdm\n\ntqdm.pandas()\n\nimport gc\nimport multiprocessing\n\nimport pykakasi\n\nimport sys\nsys.path.insert(0, UTIL_PATH) \n\nfrom catboost import CatBoostClassifier","metadata":{"execution":{"iopub.status.busy":"2022-06-13T20:53:48.570661Z","iopub.execute_input":"2022-06-13T20:53:48.571151Z","iopub.status.idle":"2022-06-13T20:53:50.656546Z","shell.execute_reply.started":"2022-06-13T20:53:48.571123Z","shell.execute_reply":"2022-06-13T20:53:50.655511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Extracting Nearest Neighbors","metadata":{}},{"cell_type":"code","source":"def calc_dists_and_indices(df: pd.DataFrame,\n                           N: int,\n                           cols: list) -> (np.ndarray, np.ndarray):\n    \n    df[cols] = np.deg2rad(df[cols])\n    matcher = NearestNeighbors(n_neighbors=N,\n                               metric=\"haversine\",\n                               n_jobs=-1)\n    matcher.fit(df.loc[:, cols])\n    distances, indices = matcher.kneighbors(df.loc[:, cols])\n\n    return distances, indices","metadata":{"execution":{"iopub.status.busy":"2022-06-13T20:53:50.670015Z","iopub.execute_input":"2022-06-13T20:53:50.670642Z","iopub.status.idle":"2022-06-13T20:53:50.679076Z","shell.execute_reply.started":"2022-06-13T20:53:50.670608Z","shell.execute_reply":"2022-06-13T20:53:50.677151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(DF_PATH).reset_index()\ncoo_cols = [\"latitude\", \"longitude\"]\n\nN = 12 if len(df)>30000 else 2\n\ndistances, indices = calc_dists_and_indices(df=df,\n                                            N=N,\n                                            cols=coo_cols)","metadata":{"execution":{"iopub.status.busy":"2022-06-13T20:54:00.392041Z","iopub.execute_input":"2022-06-13T20:54:00.392404Z","iopub.status.idle":"2022-06-13T20:54:00.510148Z","shell.execute_reply.started":"2022-06-13T20:54:00.392378Z","shell.execute_reply":"2022-06-13T20:54:00.509235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Alphabet Conversion","metadata":{}},{"cell_type":"code","source":"def convert_japanese_alphabet(df: pd.DataFrame):\n    kakasi = pykakasi.kakasi()\n    kakasi.setMode('H', 'a')  # Convert Hiragana into alphabet\n    kakasi.setMode('K', 'a')  # Convert Katakana into alphabet\n    kakasi.setMode('J', 'a')  # Convert Kanji into alphabet\n    conversion = kakasi.getConverter()\n\n    def convert(row):\n        for column in [\"name\", \"address\", \"city\", \"state\"]:\n            try:\n                row[column] = conversion.do(row[column])\n            except:\n                pass\n        return row\n\n    df[df[\"country\"] == \"JP\"] = df[df[\"country\"] == \"JP\"].progress_apply(convert, axis=1)\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-06-13T20:54:07.194733Z","iopub.execute_input":"2022-06-13T20:54:07.195043Z","iopub.status.idle":"2022-06-13T20:54:07.201413Z","shell.execute_reply.started":"2022-06-13T20:54:07.195019Z","shell.execute_reply":"2022-06-13T20:54:07.200468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = convert_japanese_alphabet(df)","metadata":{"execution":{"iopub.status.busy":"2022-06-13T20:54:07.732628Z","iopub.execute_input":"2022-06-13T20:54:07.733199Z","iopub.status.idle":"2022-06-13T20:54:08.565118Z","shell.execute_reply.started":"2022-06-13T20:54:07.733167Z","shell.execute_reply":"2022-06-13T20:54:08.564292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Extracting Essential Similarities","metadata":{}},{"cell_type":"code","source":"def textcol_tfidf(df: pd.DataFrame,\n                  cols: list) -> np.ndarray:\n    V = dict()\n    for col in cols:\n        tfidf = TfidfVectorizer(ngram_range=(3, 3), analyzer=\"char_wb\", use_idf=False)\n        V[col] = tfidf.fit_transform(df[col].astype(str).fillna(f\"no{col}\").values)\n        print(col, V[col].shape)\n    return V\n\ndef cat_tfidf(df: pd.DataFrame) -> np.ndarray:\n    tfidf = TfidfVectorizer(use_idf=False)\n    V_cat = tfidf.fit_transform(df[\"categories\"].fillna(\"nocategory\").values)\n    print(\"categories\", V_cat.shape)\n    return V_cat\n","metadata":{"execution":{"iopub.status.busy":"2022-06-13T20:54:09.796089Z","iopub.execute_input":"2022-06-13T20:54:09.796402Z","iopub.status.idle":"2022-06-13T20:54:09.803068Z","shell.execute_reply.started":"2022-06-13T20:54:09.796378Z","shell.execute_reply":"2022-06-13T20:54:09.802205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calc_essential_feats(df: pd.DataFrame,\n                         indices,\n                         distances,\n                         textcol_tfidf,\n                         cat_tfidf,\n                         train_mode=True):\n    dfs = []\n\n    for i in tqdm(range(indices.shape[1])):\n        tmp_df = df[[\"id\"]].copy()\n\n        tmp_df[\"dist\"] = distances[:, i]\n        tmp_df[\"cat_sim\"] = cat_tfidf.multiply(cat_tfidf[indices[:, i]]).sum(axis=1).A1\n        for col in [\"name\", \"address\", \"url\", \"phone\"]:\n            tmp_df[f\"{col}_sim\"] = textcol_tfidf[col].multiply(textcol_tfidf[col][indices[:, i]]).sum(axis=1).A1\n\n        tmp_df[\"match_id\"] = df[\"id\"].values[indices[:, i]]\n\n        for col in [\"address\", \"url\", \"phone\", \"categories\"]:\n            tmp_df[f\"{col}_null\"] = df[col].isnull() * 1.0 + df[col].isnull().values[indices[:, i]]\n\n        if train_mode:\n            tmp_df[\"match\"] = df[\"point_of_interest\"] == df[\"point_of_interest\"].values[indices[:, i]]\n\n        dfs.append(tmp_df)\n\n    candidate_df = pd.concat(dfs)\n    candidate_df = candidate_df[candidate_df['id'] != candidate_df['match_id']]\n\n    candidate_df = pd.merge(candidate_df, df[['id',\n                                              'name',\n                                              'categories',\n                                              'phone',\n                                              'address']], on='id', how='left')\n\n    candidate_df = candidate_df.rename(columns={'id': 'id_x',\n                                                'name': 'name_x',\n                                                'categories': 'categories_x',\n                                                'phone': 'phone_x',\n                                                'address': 'address_x'})\n\n    candidate_df = pd.merge(candidate_df, df[['id',\n                                              'name',\n                                              'categories',\n                                              'phone',\n                                              'address']], left_on='match_id', right_on='id',\n                            how='left')\n    candidate_df = candidate_df.rename(columns={'match_id': 'id_y',\n                                                'name': 'name_y',\n                                                'categories': 'categories_y',\n                                                'phone': 'phone_y',\n                                                'address': 'address_y'})\n\n    candidate_df.drop(columns='id', axis=1, inplace=True)\n\n    ids = candidate_df['id_x']\n    match_ids = candidate_df['id_y']\n    candidate_df.drop(columns=['id_x', 'id_y'], axis=1, inplace=True)\n\n    return ids, match_ids, candidate_df","metadata":{"execution":{"iopub.status.busy":"2022-06-13T20:54:10.155935Z","iopub.execute_input":"2022-06-13T20:54:10.156408Z","iopub.status.idle":"2022-06-13T20:54:10.173176Z","shell.execute_reply.started":"2022-06-13T20:54:10.156376Z","shell.execute_reply":"2022-06-13T20:54:10.171878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_cols = [\"address\", \"url\", \"phone\", \"name\"]\nV_textcols = textcol_tfidf(df=df,\n                            cols=text_cols)\n\nV_cat = cat_tfidf(df=df)\n\nids, match_ids, candidate_df = calc_essential_feats(df=df,\n                                                     indices=indices,\n                                                     distances=distances,\n                                                     textcol_tfidf=V_textcols,\n                                                     cat_tfidf=V_cat,\n                                                     train_mode=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-13T20:54:10.361631Z","iopub.execute_input":"2022-06-13T20:54:10.362286Z","iopub.status.idle":"2022-06-13T20:54:10.46929Z","shell.execute_reply.started":"2022-06-13T20:54:10.362254Z","shell.execute_reply":"2022-06-13T20:54:10.467285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Extracting Word Similarity Stats","metadata":{}},{"cell_type":"code","source":"def reduce_memory(df):\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type != object:\n            cmin = df[col].min()\n            cmax = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if cmin > np.iinfo(np.int8).min and cmax < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif cmin > np.iinfo(np.int16).min and cmax < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif cmin > np.iinfo(np.int32).min and cmax < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif cmin > np.iinfo(np.int64).min and cmax < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if cmin > np.finfo(np.float16).min and cmax < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif cmin > np.finfo(np.float32).min and cmax < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-06-13T20:54:10.748841Z","iopub.execute_input":"2022-06-13T20:54:10.74923Z","iopub.status.idle":"2022-06-13T20:54:10.763391Z","shell.execute_reply.started":"2022-06-13T20:54:10.7492Z","shell.execute_reply":"2022-06-13T20:54:10.76277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def categorical_similarity(A, B):\n    if not A or not B:\n        return -1\n\n    A = set(str(A).split(\", \"))\n    B = set(str(B).split(\", \"))\n\n    # Find intersection of two sets\n    nominator = A.intersection(B)\n\n    similarity_1 = len(nominator) / len(A)\n    similarity_2 = len(nominator) / len(B)\n\n    return max(similarity_1, similarity_2)\n\n\n@cython.cfunc\ndef LCS(S: str, T: str):\n    i: cython.int\n    j: cython.int\n    dp: cython.list = [[0] * (len(T) + 1) for _ in range(len(S) + 1)]\n    for i in range(len(S)):\n        for j in range(len(T)):\n            dp[i + 1][j + 1] = max(dp[i][j] + (S[i] == T[j]), dp[i + 1][j], dp[i][j + 1], dp[i + 1][j + 1])\n    return dp[len(S)][len(T)]\n\n\ndef string_operation(s1, s2, op=\"seq_matcher\"):\n    if s1 and s2:\n        if op == \"seq_matcher\":\n            return difflib.SequenceMatcher(None, s1, s2).ratio()\n        elif op == \"lev_distance\":\n            return Levenshtein.distance(s1, s2)\n        elif op == \"jaro_winkler\":\n            return Levenshtein.jaro_winkler(s1, s2)\n        elif op == \"lcs\":\n            return LCS(str(s1), str(s2))\n    else:\n        return -1","metadata":{"execution":{"iopub.status.busy":"2022-06-13T20:54:10.93698Z","iopub.execute_input":"2022-06-13T20:54:10.93785Z","iopub.status.idle":"2022-06-13T20:54:10.955323Z","shell.execute_reply.started":"2022-06-13T20:54:10.9378Z","shell.execute_reply":"2022-06-13T20:54:10.954376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def _add_distance_features(args):\n    _, df = args\n\n    for c in [\"name\", \"categories\", \"phone\", \"address\"]:\n        df[c + \"_x\"] = df[c + \"_x\"].astype(str)\n        df[c + \"_y\"] = df[c + \"_y\"].astype(str)\n\n        df[f\"{c}_seqm\"] = df[[c + \"_x\", c + \"_y\"]].apply(lambda x: string_operation(x[c + \"_x\"], x[c + \"_y\"],\n                                                                                    op=\"seq_matcher\"),\n                                                         axis=1)\n        df[f\"{c}_leven\"] = df[[c + \"_x\", c + \"_y\"]].apply(lambda x: string_operation(x[c + \"_x\"], x[c + \"_y\"],\n                                                                                     op=\"lev_distance\"),\n                                                          axis=1)\n        df[f\"{c}_jaro\"] = df[[c + \"_x\", c + \"_y\"]].apply(lambda x: string_operation(x[c + \"_x\"], x[c + \"_y\"],\n                                                                                    op=\"jaro_winkler\"),\n                                                         axis=1)\n        df[f\"{c}_lcs\"] = df[[c + \"_x\", c + \"_y\"]].apply(lambda x: string_operation(x[c + \"_x\"], x[c + \"_y\"],\n                                                                                   op=\"lcs\"),\n                                                        axis=1)\n\n        df[f\"{c}_len\"] = df[f\"{c}_x\"].astype(str).map(len)\n        df[f\"{c}_len2\"] = df[f\"{c}_y\"].astype(str).map(len)\n        df[f'{c}_len_diff'] = np.abs(df[f\"{c}_len\"] - df[f\"{c}_len2\"])\n        df[f\"{c}_nleven\"] = df[f'{c}_leven'] / \\\n                            df[[f'{c}_len', f'{c}_len2']].max(axis=1)\n\n        df = df.drop(columns=f'{c}_len', axis=1)\n        df = df.drop(columns=f'{c}_len2', axis=1)\n\n    df[\"category_venn\"] = df[[\"categories_x\", \"categories_y\"]] \\\n        .progress_apply(lambda row: categorical_similarity(row.categories_x, row.categories_y),\n                        axis=1)\n\n    df = drop_unnecessary_cols(df, [\"name\", \"categories\", \"phone\", \"address\"])\n    gc_clear()\n\n    return df\n\ndef add_distance_features(df):\n    processes = multiprocessing.cpu_count()\n    with multiprocessing.Pool(processes=processes) as pool:\n        df[\"idx_group\"] = df.index // (len(df) / 256)\n        len_df_gby = len(df.groupby('idx_group'))\n        dfs = pool.imap(_add_distance_features, df.groupby('idx_group'))\n        dfs = tqdm(dfs, total=len_df_gby)\n        dfs = list(dfs)\n    df = pd.concat(dfs)\n    df.drop(columns=\"idx_group\", axis=1, inplace=True)\n    del dfs\n    return df\n\n\ndef drop_unnecessary_cols(df, PAIR_COLS):\n    for c in PAIR_COLS:\n        df.drop(columns=[c + \"_x\", c + \"_y\"], axis=1, inplace=True)\n    return df\n\n\ndef extract_features(df):\n    df = add_distance_features(df)\n    gc_clear()\n    df = reduce_memory(df)\n    gc_clear()\n    return df.reset_index(drop=True)\n\n\ndef gc_clear():\n    for i in range(5):\n        gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-06-13T20:54:11.146361Z","iopub.execute_input":"2022-06-13T20:54:11.14692Z","iopub.status.idle":"2022-06-13T20:54:11.170353Z","shell.execute_reply.started":"2022-06-13T20:54:11.146882Z","shell.execute_reply":"2022-06-13T20:54:11.169167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"candidate_df = extract_features(candidate_df)","metadata":{"execution":{"iopub.status.busy":"2022-06-13T20:54:11.332693Z","iopub.execute_input":"2022-06-13T20:54:11.33317Z","iopub.status.idle":"2022-06-13T20:54:16.187589Z","shell.execute_reply.started":"2022-06-13T20:54:11.333135Z","shell.execute_reply":"2022-06-13T20:54:16.186873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CV Prediction","metadata":{}},{"cell_type":"code","source":"def pred_w_model(foldnum, df_x):\n    model = CatBoostClassifier()\n    model.load_model(UTIL_PATH+'/cb_fold_'+str(foldnum))\n    pred = model.predict_proba(df_x)[:,1]\n    del model\n    gc_clear()\n    return pred\n\npreds = [pred_w_model(i, candidate_df) for i in tqdm(range(7))] \nthresholds = np.load(UTIL_PATH+\"/fold_threshs.npy\")\nthresholds","metadata":{"execution":{"iopub.status.busy":"2022-06-13T20:54:16.188959Z","iopub.execute_input":"2022-06-13T20:54:16.189397Z","iopub.status.idle":"2022-06-13T20:54:36.481414Z","shell.execute_reply.started":"2022-06-13T20:54:16.189365Z","shell.execute_reply":"2022-06-13T20:54:36.480452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Voting with Threshold-Tuning","metadata":{}},{"cell_type":"code","source":"candidate_df[\"pred\"] = (\n    np.mean(\n        [\n            (preds[pred_i] >= thresholds[pred_i]).astype(int)\n            for pred_i, pred in enumerate(preds)\n        ],\n        axis=0,\n    )\n    >= 0.5\n).astype(int)\ncandidate_df['ids']=ids\ncandidate_df['match_id']=match_ids","metadata":{"execution":{"iopub.status.busy":"2022-06-13T20:54:36.482591Z","iopub.execute_input":"2022-06-13T20:54:36.482922Z","iopub.status.idle":"2022-06-13T20:54:36.496141Z","shell.execute_reply.started":"2022-06-13T20:54:36.482885Z","shell.execute_reply":"2022-06-13T20:54:36.494875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating the submission","metadata":{}},{"cell_type":"code","source":"pred_match_mask = candidate_df.pred==1\nmatches_list = dict([(ids, list(groupby_df.match_id.values)) for ids, groupby_df in candidate_df[pred_match_mask].groupby(\"ids\")])","metadata":{"execution":{"iopub.status.busy":"2022-06-13T20:54:36.498108Z","iopub.execute_input":"2022-06-13T20:54:36.498564Z","iopub.status.idle":"2022-06-13T20:54:36.518093Z","shell.execute_reply.started":"2022-06-13T20:54:36.498514Z","shell.execute_reply":"2022-06-13T20:54:36.516811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"subm_df=pd.read_csv(SUB_PATH)\nsubm_df[\"matches\"] = np.nan","metadata":{"execution":{"iopub.status.busy":"2022-06-13T20:54:36.519578Z","iopub.execute_input":"2022-06-13T20:54:36.51993Z","iopub.status.idle":"2022-06-13T20:54:36.542505Z","shell.execute_reply.started":"2022-06-13T20:54:36.519896Z","shell.execute_reply":"2022-06-13T20:54:36.541315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_setter(selected_id):\n    try:\n        return \" \".join(matches_list[selected_id])\n    except:\n        return \"\"\n\nsubm_df['matches'] = subm_df['id'].progress_apply(lambda x: set_setter(x))","metadata":{"execution":{"iopub.status.busy":"2022-06-13T20:54:36.54386Z","iopub.execute_input":"2022-06-13T20:54:36.54444Z","iopub.status.idle":"2022-06-13T20:54:36.594444Z","shell.execute_reply.started":"2022-06-13T20:54:36.544399Z","shell.execute_reply":"2022-06-13T20:54:36.593137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Adding the self match","metadata":{}},{"cell_type":"code","source":"subm_df['matches'] = subm_df['id'] + \" \" + subm_df['matches']","metadata":{"execution":{"iopub.status.busy":"2022-06-13T20:54:36.595913Z","iopub.execute_input":"2022-06-13T20:54:36.596379Z","iopub.status.idle":"2022-06-13T20:54:36.602668Z","shell.execute_reply.started":"2022-06-13T20:54:36.596347Z","shell.execute_reply":"2022-06-13T20:54:36.602102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Post-Processing","metadata":{}},{"cell_type":"code","source":"def post_process(df):\n    id2match = dict(zip(df['id'].values, df['matches'].str.split()))\n\n    for base, match in tqdm(df[['id', 'matches']].values):\n        match = match.split()\n        if len(match) == 1:        \n            continue\n\n        for m in match:\n            if base not in id2match[m]:\n                id2match[m].append(base)\n    df['matches'] = df['id'].map(id2match).map(' '.join)\n    return df ","metadata":{"execution":{"iopub.status.busy":"2022-06-13T20:54:36.603787Z","iopub.execute_input":"2022-06-13T20:54:36.604553Z","iopub.status.idle":"2022-06-13T20:54:36.616275Z","shell.execute_reply.started":"2022-06-13T20:54:36.604522Z","shell.execute_reply":"2022-06-13T20:54:36.615455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"subm_df = post_process(subm_df)","metadata":{"execution":{"iopub.status.busy":"2022-06-13T20:54:36.617363Z","iopub.execute_input":"2022-06-13T20:54:36.618049Z","iopub.status.idle":"2022-06-13T20:54:36.6754Z","shell.execute_reply.started":"2022-06-13T20:54:36.617985Z","shell.execute_reply":"2022-06-13T20:54:36.673952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## The submission","metadata":{}},{"cell_type":"code","source":"subm_df","metadata":{"execution":{"iopub.status.busy":"2022-06-13T20:54:36.677683Z","iopub.execute_input":"2022-06-13T20:54:36.678679Z","iopub.status.idle":"2022-06-13T20:54:36.695011Z","shell.execute_reply.started":"2022-06-13T20:54:36.67864Z","shell.execute_reply":"2022-06-13T20:54:36.693912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"subm_df.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-13T20:54:36.696112Z","iopub.execute_input":"2022-06-13T20:54:36.696885Z","iopub.status.idle":"2022-06-13T20:54:36.705015Z","shell.execute_reply.started":"2022-06-13T20:54:36.696854Z","shell.execute_reply":"2022-06-13T20:54:36.704127Z"},"trusted":true},"execution_count":null,"outputs":[]}]}