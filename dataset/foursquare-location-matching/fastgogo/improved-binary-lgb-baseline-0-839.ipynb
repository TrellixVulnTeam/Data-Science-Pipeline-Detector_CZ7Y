{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### This notebook is the improved version of [binary_lgb_baseline](https://www.kaggle.com/code/guoyonfan/binary-lgb-baseline-0-834), and mainly benefits from the [simple_recall_method](https://www.kaggle.com/code/guoyonfan/simple-recall-method). This notebook improves the LB score of the binary_lgb_baseline from 0.834 to 0.839.\n\n### If you find these notebooks useful, please UPVOTE!","metadata":{}},{"cell_type":"code","source":"## Imports\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nimport gc\nimport time\nimport random\nimport Levenshtein\nimport difflib\nimport multiprocessing\nimport pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nfrom collections import Counter\nfrom tqdm.auto import tqdm\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.neighbors import NearestNeighbors","metadata":{"execution":{"iopub.status.busy":"2022-06-02T05:10:17.511779Z","iopub.execute_input":"2022-06-02T05:10:17.512264Z","iopub.status.idle":"2022-06-02T05:10:20.103734Z","shell.execute_reply.started":"2022-06-02T05:10:17.512137Z","shell.execute_reply":"2022-06-02T05:10:20.102552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAIN_FEATURES = ['kdist',\n                'kneighbors',\n                'kdist_country',\n                'kneighbors_country',\n                'latdiff',\n                'londiff',\n                'manhattan',\n                'euclidean',\n                'haversine',\n                'name_sim',\n                'name_gesh',\n                'name_leven',\n                'name_jaro',\n                'name_lcs',\n                'name_len_diff',\n                'name_nleven',\n                'name_nlcsk',\n                'name_nlcs',\n                'address_sim',\n                'address_gesh',\n                'address_leven',\n                'address_jaro',\n                'address_lcs',\n                'address_len_diff',\n                'address_nleven',\n                'address_nlcsk',\n                'address_nlcs',\n                'city_gesh',\n                'city_leven',\n                'city_jaro',\n                'city_lcs',\n                'city_len_diff',\n                'city_nleven',\n                'city_nlcsk',\n                'city_nlcs',\n                'state_sim',\n                'state_gesh',\n                'state_leven',\n                'state_jaro',\n                'state_lcs',\n                'state_len_diff',\n                'state_nleven',\n                'state_nlcsk',\n                'state_nlcs',\n                'zip_gesh',\n                'zip_leven',\n                'zip_jaro',\n                'zip_lcs',\n                'url_sim',\n                'url_gesh',\n                'url_leven',\n                'url_jaro',\n                'url_lcs',\n                'url_len_diff',\n                'url_nleven',\n                'url_nlcsk',\n                'url_nlcs',\n                'phone_gesh',\n                'phone_leven',\n                'phone_jaro',\n                'phone_lcs',\n                'categories_sim',\n                'categories_gesh',\n                'categories_leven',\n                'categories_jaro',\n                'categories_lcs',\n                'categories_len_diff',\n                'categories_nleven',\n                'categories_nlcsk',\n                'categories_nlcs',\n                'country_sim',\n                'country_gesh',\n                'country_leven',\n                'country_nleven',\n                'kdist_diff',\n                'kneighbors_mean',]","metadata":{"execution":{"iopub.status.busy":"2022-06-02T05:10:20.105985Z","iopub.execute_input":"2022-06-02T05:10:20.106288Z","iopub.status.idle":"2022-06-02T05:10:20.116255Z","shell.execute_reply.started":"2022-06-02T05:10:20.106256Z","shell.execute_reply":"2022-06-02T05:10:20.11536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Parameters\nNUM_NEIGHBOR = 20\nSEED = 2022\nTHRESHOLD = 0.5\nNUM_SPLIT = 10\nfeat_columns = ['dist', 'name', 'address', 'city', \n            'state', 'zip', 'url', \n           'phone', 'categories', 'country']\nvec_columns = ['name', 'categories', 'address', \n               'state', 'url', 'country']\nrec_columns = ['name', 'address', 'categories', 'address', 'phone']\n\ndef seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \nseed_everything(SEED)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T05:10:20.117602Z","iopub.execute_input":"2022-06-02T05:10:20.117871Z","iopub.status.idle":"2022-06-02T05:10:20.135332Z","shell.execute_reply.started":"2022-06-02T05:10:20.117841Z","shell.execute_reply":"2022-06-02T05:10:20.134561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%load_ext Cython","metadata":{"execution":{"iopub.status.busy":"2022-06-02T05:10:20.137248Z","iopub.execute_input":"2022-06-02T05:10:20.138309Z","iopub.status.idle":"2022-06-02T05:10:21.391428Z","shell.execute_reply.started":"2022-06-02T05:10:20.138263Z","shell.execute_reply":"2022-06-02T05:10:21.390275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%cython\ndef LCS(str S, str T):\n    cdef int i, j\n    cdef list dp = [[0] * (len(T) + 1) for _ in range(len(S) + 1)]\n    for i in range(len(S)):\n        for j in range(len(T)):\n            dp[i + 1][j + 1] = max(dp[i][j] + (S[i] == T[j]), dp[i + 1][j], dp[i][j + 1], dp[i + 1][j + 1])\n    return dp[len(S)][len(T)]","metadata":{"execution":{"iopub.status.busy":"2022-06-02T05:10:21.39357Z","iopub.execute_input":"2022-06-02T05:10:21.393915Z","iopub.status.idle":"2022-06-02T05:10:22.900581Z","shell.execute_reply.started":"2022-06-02T05:10:21.39387Z","shell.execute_reply":"2022-06-02T05:10:22.898283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def post_process(df):\n    id2match = dict(zip(df['id'].values, df['matches'].str.split()))\n\n    for base, match in df[['id', 'matches']].values:\n        match = match.split()\n        if len(match) == 1:        \n            continue\n\n        for m in match:\n            if base not in id2match[m]:\n                id2match[m].append(base)\n    df['matches'] = df['id'].map(id2match).map(' '.join)\n    return df \n\n# get manhattan distance\ndef manhattan(lat1, long1, lat2, long2):\n    return np.abs(lat2 - lat1) + np.abs(long2 - long1)\n\n# get haversine distance\ndef vectorized_haversine(lats1, lats2, longs1, longs2):\n    radius = 6371\n    dlat=np.radians(lats2 - lats1)\n    dlon=np.radians(longs2 - longs1)\n    a = np.sin(dlat/2) * np.sin(dlat/2) + np.cos(np.radians(lats1)) \\\n        * np.cos(np.radians(lats2)) * np.sin(dlon/2) * np.sin(dlon/2)\n    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n    d = radius * c\n    return d","metadata":{"execution":{"iopub.status.busy":"2022-06-02T05:10:22.903667Z","iopub.execute_input":"2022-06-02T05:10:22.904184Z","iopub.status.idle":"2022-06-02T05:10:22.916149Z","shell.execute_reply.started":"2022-06-02T05:10:22.904137Z","shell.execute_reply":"2022-06-02T05:10:22.915182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def recall_simple(df):\n    threshold = 2\n    \n    val2id_d = {}\n    for col in rec_columns:\n        temp_df = df[['id', col]]\n        temp_df[col] = temp_df[col].str.lower()\n        val2id = temp_df.groupby(col)['id'].apply(set).to_dict()\n        val2id_d[col] = val2id\n        del val2id\n    \n    cus_ids = []\n    match_ids = []\n    for vals in tqdm(df[rec_columns + ['id']].fillna('null').values):\n        cus_id = vals[-1]\n        match_id = []\n        \n        rec_match_count = []\n        for i in range(len(rec_columns)):\n            col = rec_columns[i]\n            \n            if vals[i] != 'null':\n                rec_match_count += list(val2id_d[col][vals[i].lower()])\n        rec_match_count = dict(Counter(rec_match_count))\n        \n        for k, v in rec_match_count.items():\n            if v > threshold:\n                match_id.append(k)\n        \n        cus_ids += [cus_id] * len(match_id)\n        match_ids += match_id\n    \n    train_df = pd.DataFrame()\n    train_df['id'] = cus_ids\n    train_df['match_id'] = match_ids\n    train_df = train_df.drop_duplicates()\n    del cus_ids, match_ids\n    \n    num_data = len(train_df)\n    num_data_per_id = num_data / train_df['id'].nunique()\n    print('Num of data: %s' % num_data)\n    print('Num of data per id: %s' % num_data_per_id)\n    \n    return train_df","metadata":{"execution":{"iopub.status.busy":"2022-06-02T05:10:22.917632Z","iopub.execute_input":"2022-06-02T05:10:22.91803Z","iopub.status.idle":"2022-06-02T05:10:22.944158Z","shell.execute_reply.started":"2022-06-02T05:10:22.917993Z","shell.execute_reply":"2022-06-02T05:10:22.943288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def recall_knn(df, Neighbors = 10):\n    print('Start knn grouped by country')\n    train_df_country = []\n    for country, country_df in tqdm(df.groupby('country')):\n        country_df = country_df.reset_index(drop = True)\n\n        neighbors = min(len(country_df), Neighbors)\n        knn = KNeighborsRegressor(n_neighbors = neighbors,\n                                    metric = 'haversine',\n                                    n_jobs = -1)\n        knn.fit(country_df[['latitude','longitude']], country_df.index)\n        dists, nears = knn.kneighbors(country_df[['latitude', 'longitude']], \n                                        return_distance = True)\n\n        for k in range(neighbors):            \n            cur_df = country_df[['id']]\n            cur_df['match_id'] = country_df['id'].values[nears[:, k]]\n            cur_df['kdist_country'] = dists[:, k]\n            cur_df['kneighbors_country'] = k\n            \n            train_df_country.append(cur_df)\n    train_df_country = pd.concat(train_df_country)\n    \n    print('Start knn')\n    train_df = []\n    knn = NearestNeighbors(n_neighbors = Neighbors)\n    knn.fit(df[['latitude','longitude']], df.index)\n    dists, nears = knn.kneighbors(df[['latitude','longitude']])\n    \n    for k in range(Neighbors):            \n        cur_df = df[['id']]\n        cur_df['match_id'] = df['id'].values[nears[:, k]]\n        cur_df['kdist'] = dists[:, k]\n        cur_df['kneighbors'] = k\n        train_df.append(cur_df)\n    \n    train_df = pd.concat(train_df)\n    train_df = train_df.merge(train_df_country,\n                                 on = ['id', 'match_id'],\n                                 how = 'outer')\n    del train_df_country\n    \n    return train_df","metadata":{"execution":{"iopub.status.busy":"2022-06-02T05:10:22.945402Z","iopub.execute_input":"2022-06-02T05:10:22.946023Z","iopub.status.idle":"2022-06-02T05:10:22.96565Z","shell.execute_reply.started":"2022-06-02T05:10:22.945986Z","shell.execute_reply":"2022-06-02T05:10:22.964742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def add_features(df):    \n    for col in tqdm(feat_columns):\n        if col == 'dist':\n            lat = data.loc[df['id']]['latitude'].values\n            match_lat = data.loc[df['match_id']]['latitude'].values\n            lon = data.loc[df['id']]['longitude'].values\n            match_lon = data.loc[df['match_id']]['longitude'].values\n            df['latdiff'] = (lat - match_lat)\n            df['londiff'] = (lon - match_lon)\n            df['manhattan'] = manhattan(lat, lon, match_lat, match_lon)\n            df['euclidean'] = (df['latdiff'] ** 2 + df['londiff'] ** 2) ** 0.5\n            df['haversine'] = vectorized_haversine(lat, match_lat, lon, match_lon)\n            continue\n        \n        col_values = data.loc[df['id']][col].values.astype(str)\n        matcol_values = data.loc[df['match_id']][col].values.astype(str)\n        \n        if col in vec_columns:\n            tv_fit = tfidf_d[col]\n            indexs = [id2index_d[i] for i in df['id']]\n            match_indexs = [id2index_d[i] for i in df['match_id']]                    \n            df[f'{col}_sim'] = tv_fit[indexs].multiply(tv_fit[match_indexs]).\\\n                                            sum(axis = 1).A.ravel()\n        \n        geshs = []\n        levens = []\n        jaros = []\n        lcss = []\n        for s, match_s in zip(col_values, matcol_values):\n            if s != 'nan' and match_s != 'nan':                    \n                geshs.append(difflib.SequenceMatcher(None, s, match_s).ratio())\n                levens.append(Levenshtein.distance(s, match_s))\n                jaros.append(Levenshtein.jaro_winkler(s, match_s))\n                lcss.append(LCS(str(s), str(match_s)))\n            else:\n                geshs.append(np.nan)\n                levens.append(np.nan)\n                jaros.append(np.nan)\n                lcss.append(np.nan)\n        \n        df[f'{col}_gesh'] = geshs\n        df[f'{col}_leven'] = levens\n        df[f'{col}_jaro'] = jaros\n        df[f'{col}_lcs'] = lcss\n        \n        if col not in ['phone', 'zip']:\n            df[f'{col}_len'] = list(map(len, col_values))\n            df[f'match_{col}_len'] = list(map(len, matcol_values)) \n            df[f'{col}_len_diff'] = np.abs(df[f'{col}_len'] - df[f'match_{col}_len']) /\\\n                                        df[f'{col}_len'] \n            df[f'{col}_nleven'] = df[f'{col}_leven'] / \\\n                                    df[[f'{col}_len', f'match_{col}_len']].max(axis = 1)\n            \n            df[f'{col}_nlcsk'] = df[f'{col}_lcs'] / df[f'match_{col}_len']\n            df[f'{col}_nlcs'] = df[f'{col}_lcs'] / df[f'{col}_len']\n            \n            df = df.drop(f'{col}_len', axis = 1)\n            df = df.drop(f'match_{col}_len', axis = 1)\n            gc.collect()\n            \n    return df","metadata":{"execution":{"iopub.status.busy":"2022-06-02T05:10:22.967207Z","iopub.execute_input":"2022-06-02T05:10:22.967996Z","iopub.status.idle":"2022-06-02T05:10:22.991621Z","shell.execute_reply.started":"2022-06-02T05:10:22.967956Z","shell.execute_reply":"2022-06-02T05:10:22.990818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Dada process\ndata = pd.read_csv('../input/foursquare-location-matching/test.csv')\n\nif len(data) < 20:\n    data = pd.read_csv('../input/foursquare-location-matching/train.csv',\n                      nrows = 100)\n    data = data.drop('point_of_interest', axis = 1)\n\n# special process\ndef get_lower(x):\n    try:\n        return x.lower()\n    except:\n        return x\n\nfor col in data.columns:\n    if data[col].dtype == object and col != 'id':\n        data[col] = data[col].apply(get_lower)\n        \nid2index_d = dict(zip(data['id'].values, data.index))\n\ntfidf_d = {}\nfor col in vec_columns:\n    tfidf = TfidfVectorizer()\n    tv_fit = tfidf.fit_transform(data[col].fillna('nan'))\n    tfidf_d[col] = tv_fit\n\nout_df = pd.DataFrame()\nout_df['id'] = data['id'].unique().tolist()\nout_df['match_id'] = out_df['id']\n\ntest_data_simple = recall_simple(data)\ntest_data = recall_knn(data, NUM_NEIGHBOR)\n\nprint('train data by knn: %s' % len(test_data))\ntest_data = test_data.merge(test_data_simple,\n                             on = ['id', 'match_id'],\n                             how = 'outer')\ndel test_data_simple\ngc.collect()\n\ndata = data.set_index('id')\nprint('Num of unique id: %s' % test_data['id'].nunique())\nprint('Num of test data: %s' % len(test_data))\nprint(test_data.sample(5))","metadata":{"execution":{"iopub.status.busy":"2022-06-02T05:10:22.99397Z","iopub.execute_input":"2022-06-02T05:10:22.994913Z","iopub.status.idle":"2022-06-02T05:10:27.233315Z","shell.execute_reply.started":"2022-06-02T05:10:22.994873Z","shell.execute_reply":"2022-06-02T05:10:27.232394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Model load\nlgb_model_path = '../input/binary-lgb-baseline/improved_lgb_baseline.lgb'\nlgb_model = lgb.Booster(model_file = lgb_model_path)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T05:10:27.234709Z","iopub.execute_input":"2022-06-02T05:10:27.234984Z","iopub.status.idle":"2022-06-02T05:10:27.349636Z","shell.execute_reply.started":"2022-06-02T05:10:27.23495Z","shell.execute_reply":"2022-06-02T05:10:27.348913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Prediction\ncount = 0\nstart_row = 0\npred_df = pd.DataFrame()\nunique_id = test_data['id'].unique().tolist()\nnum_split_id = len(unique_id) // NUM_SPLIT\nfor k in range(1, NUM_SPLIT + 1):\n    print('Current split: %s' % k)\n    end_row = start_row + num_split_id\n    if k < NUM_SPLIT:\n        cur_id = unique_id[start_row : end_row]\n        cur_data = test_data[test_data['id'].isin(cur_id)]\n    else:\n        cur_id = unique_id[start_row: ]\n        cur_data = test_data[test_data['id'].isin(cur_id)]\n    \n    # add features & model prediction\n    cur_data = add_features(cur_data)\n    cur_data['kdist_diff'] = (cur_data['kdist'] - cur_data['kdist_country']) /\\\n                                cur_data['kdist_country']\n    cur_data['kneighbors_mean'] = cur_data[['kneighbors', 'kneighbors_country']].mean(axis = 1)\n    cur_data['pred'] = lgb_model.predict(cur_data[TRAIN_FEATURES])\n    cur_pred_df = cur_data[cur_data['pred'] > THRESHOLD][['id', 'match_id']]\n    pred_df = pd.concat([pred_df, cur_pred_df])\n    \n    start_row = end_row\n    count += len(cur_data)\n\n    del cur_data, cur_pred_df\n    gc.collect()\nprint(count)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T05:10:27.354025Z","iopub.execute_input":"2022-06-02T05:10:27.356017Z","iopub.status.idle":"2022-06-02T05:10:41.735561Z","shell.execute_reply.started":"2022-06-02T05:10:27.355967Z","shell.execute_reply":"2022-06-02T05:10:41.734468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Submission    \nout_df = pd.concat([out_df, pred_df])\nout_df = out_df.groupby('id')['match_id'].\\\n                        apply(list).reset_index()\nout_df['matches'] = out_df['match_id'].apply(lambda x: ' '.join(set(x)))\nout_df = post_process(out_df)\nprint('Unique id: %s' % len(out_df))\nprint(out_df.head())\n\nout_df[['id', 'matches']].to_csv('submission.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T05:10:41.736874Z","iopub.execute_input":"2022-06-02T05:10:41.737131Z","iopub.status.idle":"2022-06-02T05:10:41.765162Z","shell.execute_reply.started":"2022-06-02T05:10:41.7371Z","shell.execute_reply":"2022-06-02T05:10:41.764174Z"},"trusted":true},"execution_count":null,"outputs":[]}]}