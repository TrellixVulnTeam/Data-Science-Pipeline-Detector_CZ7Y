{"metadata":{"kernelspec":{"display_name":"tf","language":"python","name":"gputest"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.11"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy  as np\nimport matplotlib.pyplot as plt\nimport seaborn \nimport tensorflow as tf\nimport tensorflow_data_validation as tfdv\nfrom sklearn.neighbors import NearestNeighbors\nfrom tqdm.notebook import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GroupKFold\nimport gc\nfrom tqdm import tqdm \n\ndef get_id2poi(input_df: pd.DataFrame) -> dict:\n    return dict(zip(input_df['id'], input_df['point_of_interest']))\n\ndef get_poi2ids(input_df: pd.DataFrame) -> dict:\n    return input_df.groupby('point_of_interest')['id'].apply(set).to_dict()\n\ndef get_score(input_df: pd.DataFrame):\n    scores = []\n    for id_str, matches in zip(input_df['id'].to_numpy(), input_df['matches'].to_numpy()):\n        targets = poi2ids[id2poi[id_str]]\n        preds = set(matches.split())\n        score = len((targets & preds)) / len((targets | preds))\n        scores.append(score)\n    scores = np.array(scores)\n    return scores.mean()\n\ndef reduce_memory(df):\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type != object:\n            cmin = df[col].min()\n            cmax = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if cmin > np.iinfo(np.int8).min and cmax < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif cmin > np.iinfo(np.int16).min and cmax < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif cmin > np.iinfo(np.int32).min and cmax < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif cmin > np.iinfo(np.int64).min and cmax < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if cmin > np.finfo(np.float16).min and cmax < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif cmin > np.finfo(np.float32).min and cmax < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    return df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"print('TF version:', tf.__version__)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ss = pd.read_csv('sample_submission.csv')\nss.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"serving = pd.read_csv('test.csv')\nserving.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"serving.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pairs = pd.read_csv('pairs.csv')\npairs = reduce_memory(pairs)\nprint(pairs.shape)\nprint(pairs.columns)\npairs.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pair_stats = tfdv.generate_statistics_from_dataframe(pairs)\ntfdv.visualize_statistics(pair_stats)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('train.csv')\ndf = reduce_memory(df)\nprint(df.shape)\ndf.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_stats = tfdv.generate_statistics_from_dataframe(df)\ntfdv.visualize_statistics(train_stats)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"schema = tfdv.infer_schema(train_stats)\ntfdv.display_schema(schema)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from matplotlib_venn import venn3\nfrom matplotlib_venn import venn2\nvenn3([set(pairs['id_1'].values.astype('str')),set(df['id'].values.astype('str')),set(pairs['id_2'].values.astype('str'))],('pairs1','train','pairs2'))\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"venn2([set(df['id'].values.astype('str')),set(pairs['id_2'].values.astype('str'))],('train','pairs2'))\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"venn2([set(df['id'].values.astype('str')),set(pairs['id_1'].values.astype('str'))],('train','pairs'))\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"set_of_no_match_ids  = set(df['id'].values.astype('str')) - set(pairs['id_1'].values.astype('str')).union(list(set(pairs['id_2'].values.astype('str'))))\nlen(set_of_no_match_ids)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DATA prep and modeling ","metadata":{}},{"cell_type":"code","source":"match_df = pd.merge(df, df, on=\"point_of_interest\", suffixes=('_1', '_2'))\nmatch_df = match_df[match_df[\"id_1\"]!=match_df[\"id_2\"]]\nmatch_df = match_df.drop([\"point_of_interest\"], axis=1)\nmatch_df[\"match\"] = True\nprint(match_df.shape)\nmatch_df.fillna(\"NA\",inplace = True)\nmatch_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"pair_stats = tfdv.generate_statistics_from_dataframe(match_df)\n","metadata":{}},{"cell_type":"raw","source":"tfdv.visualize_statistics(pair_stats)","metadata":{}},{"cell_type":"code","source":"from matplotlib_venn import venn2\nvenn2([set(match_df['id_1'].values.astype('str')),set(pairs['id_1'].values.astype('str'))],('train','pairs'))\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grouped_df = match_df.groupby('id_1')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dict_df = dict(list(grouped_df))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"set_keys = set(match_df['id_1'].values)\nkeys = list(set(match_df['id_1'].values))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dict_non_match = {}\ndef non_match(keys) :\n#     global counter\n#     print(counter)\n    temp  = set_keys.copy()\n    temp2 = dict_df[keys]['id_1'].iloc[0]\n    temp_nm = temp - set(dict_df[keys]['id_2'].values)\n    dict_non_match[temp2] = random.sample(list(temp_nm),40)\n    #counter+=1\n    #return dict_non_match","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(keys)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random \nrandom_keys = random.sample(keys,10000)\nrandom_keys[1]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp = list(map(non_match, random_keys)) # random_keys","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ind = list(df['id'].values)\ndef id_to_idx(id, ind = ind):\n    return ind.index(id)\n     ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from multiprocessing.dummy import Pool as ThreadPool \ndef create_non_match_tuples(i, keys, values):\n    list_non_match = []\n\n    values  = random.sample(values,40)\n    row_1 = df.iloc[id_to_idx(keys)]\n    for x in tqdm(values):\n        row_2 = df.iloc[id_to_idx(x)]\n        pair = (row_1,row_2)\n        list_non_match.append(pair)\n    return i,list_non_match\npool = ThreadPool(8)\n%timeit\nresults = pool.starmap(create_non_match_tuples, zip(range(len(dict_non_match)),list(dict_non_match.keys()),list(dict_non_match.values()))) \npool.close() \npool.join()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_non_match = [i[1] for i in results]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nm_df = pairs[pairs['match'] == 0]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nm_df.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"idx_temp = list(list_non_match[0][0][0].index)\nidx_1 = [i+'_1' for i in idx_temp]\nidx_2 = [i+'_2' for i in idx_temp]\nidx = idx_1 + idx_2\nprint(idx)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp = []\nfor y in list_non_match:\n    for x in y:\n        temp.append(list(x[0])+list(x[1]))\n    \n    \narr  = np.stack(temp)\n\nnm2 = pd.DataFrame(data = arr, columns = idx)\n\nnm2.drop(['point_of_interest_2', 'point_of_interest_1'],inplace= True, axis =1)\nnm2['match'] = [0 for i in range(nm2.shape[0])]\nprint(nm2.shape)\nnm2.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nm_df = pd.concat([nm_df,nm2])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nm_df.fillna(\"NA\", inplace = True)\nprint(nm_df.isnull().any().sum())\nprint(nm_df.shape)\nnm_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nm_df = pd.concat([nm_df,match_df.iloc[:600000]])\nnm_df.to_csv('full_train.csv',index  = False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nm_df = pd.read_csv('full_train.csv')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nm_df.fillna(\"NAN\",inplace  = True)\nnm_df.isna().sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Features to use \nX = nm_df[['latitude_1','longitude_1','latitude_2','longitude_2','categories_1','categories_2',]]\ny = nm_df['match']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# del X\n# del y\n# gc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"del X_train, X_test, y_train, y_test\ngc.collect()","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.33, random_state=42)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nsns.distplot([len(i.split()) for i in X['categories_1'] ])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_words = 100000\noov_token = '<UNK>'\npad_type = 'pre'\ntrunc_type = 'pre'\ntokenizer = Tokenizer(num_words=num_words, oov_token=oov_token)\ntokenizer.fit_on_texts(X_train['categories_1'] + X_train['categories_2'])\n\n# Get our training data word index\nword_index = tokenizer.word_index\n\n# # Encode training data sentences into sequences\n# train_sequences_1 = tokenizer.texts_to_sequences(X_train['categories_1'])\n# train_sequences_2 = tokenizer.texts_to_sequences(X_train['categories_2'])\n\n# # Pad the training sequences\n# train_padded_1 = pad_sequences(train_sequences_1, padding=pad_type, truncating=trunc_type, maxlen=12)\n# train_padded_2 = pad_sequences(train_sequences_1, padding=pad_type, truncating=trunc_type, maxlen=12)\n\n\n# print(\"Word index:\\n\", word_index)\n# print(\"\\nTraining sequences:\\n\", train_sequences_1)\n# print(\"\\nPadded training sequences:\\n\", train_padded_1)\n# print(\"\\nPadded training shape:\", train_padded_1.shape)\n# print(\"Training sequences data type:\", type(train_sequences_1))\n# print(\"Padded Training sequences data type:\", type(train_padded_1))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save tokenizer as a pickle\n\nimport pickle\n\ndict1 = {'foo': tokenizer}\n\n# Store data (serialize)\nwith open('tokenizer.pickle', 'wb') as handle:\n    pickle.dump(dict1, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n# Load data (deserialize)\nwith open('tokenizer.pickle', 'rb') as handle:\n    fq = pickle.load(handle)\n\nfq['foo'].word_index","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pre_process(X):\n    category,lat, lon = X[:,0],X[:,1],X[:,2]\n    seq =  tokenizer.texts_to_sequences(category)\n    seq = pad_sequences(seq, padding=pad_type, truncating=trunc_type, maxlen=12)\n    seq = np.concatenate([seq,np.reshape(lat,(-1,1))], axis = 1)\n    seq = np.concatenate([seq,np.reshape(lon,(-1,1))], axis = 1)\n    return seq\n#pre_processed = pre_process(nm_df[nm_df['match']==1][['categories_1','latitude_1','longitude_1']].values)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_doubelets(anchor, validation):\n    \"\"\"\n    Given the filenames corresponding to the three images, load and\n    preprocess them.\n    \"\"\"\n\n    return (\n        pre_process(anchor).astype('float32'),\n        pre_process(validation).astype('float32')\n      \n    )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"value_1 = nm_df[['categories_1','latitude_1','longitude_1']].values\nvalue_2 = nm_df[['categories_2','latitude_2','longitude_2']].values","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from itertools import starmap\npre = list(starmap(preprocess_doubelets,[(value_1,value_2)]))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pos  = tf.data.Dataset.from_tensor_slices(pre[0][0])\n# neg  = tf.data.Dataset.from_tensor_slices(pre[0][1])\n# y_true = tf.data.Dataset.from_tensor_slices(nm_df['match'].values)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dataset = tf.data.Dataset.zip((pos, neg,)\n# dataset = dataset.shuffle(buffer_size=1024)\n# dataset = dataset.map(preprocess_doubelets)\n\n# # Let's now split our dataset in train and validation.\n# train_dataset = dataset.take(round(image_count * 0.8))\n# val_dataset = dataset.skip(round(image_count * 0.8))\n\n# train_dataset = train_dataset.batch(32, drop_remainder=False)\n# train_dataset = train_dataset.prefetch(8)\n\n# val_dataset = val_dataset.batch(32, drop_remainder=False)\n# val_dataset = val_dataset.prefetch(8)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf \nfrom tensorflow.keras.layers import Dense,BatchNormalization,Dropout,Embedding,Input,Concatenate,Reshape, Multiply, Subtract, Add, Multiply, Dropout, Subtract, Add,Lambda\nfrom tensorflow.keras import Model\nfrom keras.regularizers import l2\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras import backend as K\n\ndef cosine_distance(vests):\n    x, y = vests\n    x = K.l2_normalize(x, axis=-1)\n    y = K.l2_normalize(y, axis=-1)\n    return -K.mean(x * y, axis=-1, keepdims=True)\n\ndef cos_dist_output_shape(shapes):\n    shape1, shape2 = shapes\n    return (shape1[0],1)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\n\ndef auroc(y_true, y_pred):\n    return tf.numpy_function(roc_auc_score, (y_true, y_pred), tf.double,stateful=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_1 = Input(shape = (14,))\n# input_12 = Input(shape = (2,))\n# input_2 = Input(shape = (12,))\n# input_22 = Input(shape = (2,))\n\nx = Embedding( len(word_index) ,100, input_length = 14)(input_1)\nx = Reshape((1400,))(x)\nx = Dense(128,activation = 'relu')(x)\nx = BatchNormalization()(x)\nx = Dropout(0.2)(x)\nx = Dense(256,activation = 'relu')(x)\nx = BatchNormalization()(x)\nx = Dropout(0.2)(x)\nx = Dense(512,activation = 'relu')(x)\n\n\nembeddings = Model(inputs = [input_1], outputs = x)\nembeddings.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_2 = Input(shape = (14,))\ninput_3 = Input(shape = (14,))\n\nembeddings_1 = embeddings(input_2)\nembeddings_2 = embeddings(input_3)\n\n\n\nx3 = Subtract()([embeddings_1, embeddings_2])\nx3 = Multiply()([x3, x3])\n\nx1_ = Multiply()([embeddings_1, embeddings_1])\nx2_ = Multiply()([embeddings_2, embeddings_2])\nx4 = Subtract()([x1_, x2_])\n    \n    #https://stackoverflow.com/a/51003359/10650182\nx5 = Lambda(cosine_distance, output_shape=cos_dist_output_shape)([embeddings_1, embeddings_2])\n    \nconc = Concatenate(axis=-1)([x5,x4, x3])\n\nx = Dense(100, activation=\"relu\", name='conc_layer')(conc)\nx = Dropout(0.01)(x)\nout = Dense(1, activation=\"sigmoid\", name = 'out')(x)\n\nmodel = Model([input_2, input_3], out)\nmodel.summary()\nmodel.compile(loss=\"binary_crossentropy\", metrics=['acc',auroc], optimizer=Adam(0.00001))","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tensorboard  =tf.keras.callbacks.TensorBoard(\n    log_dir='logs'\n)\nearly_stop  =tf.keras.callbacks.EarlyStopping(\n    monitor=\"val_loss\",\n    min_delta=.001,\n    patience=5,\n    verbose=1,\n    mode=\"auto\",\n    restore_best_weights=True,\n)\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" \nmodel.fit([pre[0][0][X_train],pre[0][1][X_train]],nm_df['match'][y_train], epochs = 100,\n       validation_data=([pre[0][0][X_test],pre[0][1][X_test]],nm_df['match'][y_test]),batch_size = 128,steps_per_epoch=len(X_test)//128,\n    validation_steps=len(X_test)//128,\n    validation_batch_size=128, callbacks = [tensorboard, early_stop])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('cosine.h5')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.load_weights('cosine.h5')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_model = Model(model.get_layer('model').inputs,model.get_layer('model').outputs)\nembedding_model.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"extracted_embeddings = []\n\nprev = 0\nfor f,i in tqdm(enumerate(range(2000,1780121,2000))):\n    j = range(prev,i)\n    extracted_embeddings.append(embedding_model.predict(pre[0][0][j],verbose=0))\n    prev = i\n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"extracted_embeddings  = np.vstack(extracted_embeddings)\nextracted_embeddings.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = nm_df[nm_df['match'] == 0].index[:10000]\nb = nm_df[nm_df['match'] == 1].index[:10000]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.manifold import TSNE\nX = np.vstack([extracted_embeddings[a] ,extracted_embeddings[b]])\nX_embedded = TSNE(n_components=2, learning_rate='auto',\n                   init='random').fit_transform(X)\nX_embedded.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To plot the embedding\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.scatter(X_embedded[:,0], X_embedded[:,1], c = np.vstack([nm_df['match'].values[a] ,nm_df['match'].values[b]]), s = 0.5)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"serving.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predict only on nearest neighbours","metadata":{}},{"cell_type":"code","source":"def generate_test_data(df, rounds = 3, n_neighbors = 5, features = ['id', 'latitude', 'longitude','categories']):\n    # Scale data for KNN\n#     scaler = StandardScaler()\n    scaled_data = df[features[1:3]]\n    print(df.shape)\n    # Fit KNN and predict indices\n    knn_model = NearestNeighbors(\n        n_neighbors = n_neighbors, \n        radius = 1.0, \n        algorithm = 'kd_tree', \n        leaf_size = 30, \n        metric = 'minkowski', \n        p = 2, \n        n_jobs = -1\n    )\n    knn_model.fit(scaled_data)\n    indices = knn_model.kneighbors(scaled_data, return_distance = False)\n    # Create a new dataframe to slice faster\n    df_features = df[features]\n    # Create a dataset to store final results\n    dataset = []\n    # Iterate through each round and get generated data\n    for j in range(rounds):\n        # Create temporal dataset to store round data\n        tmp_dataset = []\n        # Iterate through each row\n        for k in tqdm(range(len(df))):\n            neighbors = list(indices[k])\n            # Remove self from neighbors if exist\n            try:\n                neighbors.remove(k)\n            except:\n                pass\n            # Use iterator as first indices\n            ind1 = k\n            # Select from the neighbor list the second indices\n            ind2 = neighbors[j]\n            # Check if indices are the same, they should not be the same\n            if ind1 == ind2:\n                print('Indices are the same, error')\n            # Slice features dataframe\n            tmp1 = df_features.loc[ind1]\n            tmp2 = df_features.loc[ind2]\n            # Concatenate, don't add target, this is the test set\n            tmp = np.concatenate([tmp1, tmp2], axis = 0)\n            tmp_dataset.append(tmp)  \n        # Transform tmp_dataset to a pd.DataFrame\n        tmp_dataset = pd.DataFrame(tmp_dataset, columns = [i + '_1' for i in features] + [i + '_2' for i in features])\n        # Append round\n        dataset.append(tmp_dataset)\n    # Concatenate rounds to get final dataset\n    dataset = pd.concat(dataset, axis = 0)\n    # Remove duplicates\n    dataset.drop_duplicates(inplace = True)\n    # Reset index\n    dataset.reset_index(drop = True, inplace = True)\n    col_64 = list(dataset.dtypes[dataset.dtypes == np.float64].index)\n    for col in col_64:\n        dataset[col] = dataset[col].astype(np.float32)\n    return df, dataset","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df, dataset = generate_test_data(serving)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import load_model\nfrom sklearn.neighbors import NearestNeighbors\nfrom tqdm.notebook import tqdm\nimport re\nfrom itertools import starmap\nserving = pd.read_csv('test.csv')\nserving.head()\n\n\ndef build_model():\n    input_1 = Input(shape = (14,))\n# input_12 = Input(shape = (2,))\n# input_2 = Input(shape = (12,))\n# input_22 = Input(shape = (2,))\n\n    x = Embedding( len(word_index) ,100, input_length = 14)(input_1)\n    x = Reshape((1400,))(x)\n    x = Dense(128,activation = 'relu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.2)(x)\n    x = Dense(256,activation = 'relu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.2)(x)\n    x = Dense(512,activation = 'relu')(x)\n\n\n    embeddings = Model(inputs = [input_1], outputs = x)\n    input_2 = Input(shape = (14,))\n    input_3 = Input(shape = (14,))\n\n    embeddings_1 = embeddings(input_2)\n    embeddings_2 = embeddings(input_3)\n\n\n\n    x3 = Subtract()([embeddings_1, embeddings_2])\n    x3 = Multiply()([x3, x3])\n\n    x1_ = Multiply()([embeddings_1, embeddings_1])\n    x2_ = Multiply()([embeddings_2, embeddings_2])\n    x4 = Subtract()([x1_, x2_])\n\n        #https://stackoverflow.com/a/51003359/10650182\n    x5 = Lambda(cosine_distance, output_shape=cos_dist_output_shape)([embeddings_1, embeddings_2])\n\n    conc = Concatenate(axis=-1)([x5,x4, x3])\n\n    x = Dense(100, activation=\"relu\", name='conc_layer')(conc)\n    x = Dropout(0.01)(x)\n    out = Dense(1, activation=\"sigmoid\", name = 'out')(x)\n\n    model = Model([input_2, input_3], out)\n\n\n    return model\nmodel = build_model()\nmodel.load_weights('cosine.h5')\ndef pre_process(X):\n    lat, lon, category = X[:,0],X[:,1],X[:,2]\n    seq =  tokenizer.texts_to_sequences(category)\n    seq = pad_sequences(seq, padding=pad_type, truncating=trunc_type, maxlen=12)\n    seq = np.concatenate([seq,np.reshape(lat,(-1,1))], axis = 1)\n    seq = np.concatenate([seq,np.reshape(lon,(-1,1))], axis = 1)\n    return seq\n#pre_processed = pre_process(nm_df[nm_df['match']==1][['categories_1','latitude_1','longitude_1']].values)\n\ndef predict(df):\n    match_list = []\n    for values in tqdm(df.iterrows()):\n        anchor, validate = preprocess_doubelets(np.array(values[1][1:4]).reshape(1,-1),np.array(values[1][5:]).reshape(1,-1))\n        pred = model.predict([ anchor, validate])\n        if pred > 0.3:\n            match = 1\n            \n        else :\n            match = 0\n        match_list.append(match)\n    df['match'] = match_list\n    return df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = predict(dataset)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp = dict(list(df.groupby('id_1')))\ntemp2 = pd.DataFrame()\nfor k in temp.keys():\n    print(k)\n    eval_df = temp[k]\n    eval_df['match_id'] = eval_df[eval_df['match']==1]['id_2']\n    eval_df.fillna('NA',inplace = True)\n    temp2 = pd.concat([eval_df,temp2],axis = 0)\ntemp2.head(15)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eval_df = temp2.groupby('id_1')['match_id'].\\\n                        apply(list).reset_index()\neval_df\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction on test data set ","metadata":{}},{"cell_type":"code","source":"def matches(id1,list1):\n    str1 = ' '.join(set(list1))\n    str1 = str1.replace('NA','')\n    \n    str1 = re.sub(r' ','',str1)\n    if str1 == '':\n        str1 = id1\n    else:\n        str1 = id1+' '+str1\n    return str1\nmatches(eval_df['id_1'].iloc[4],eval_df['match_id'].iloc[4])\n\n\nl1 = list(starmap(matches,[(i,j) for i,j in zip(eval_df['id_1'].values,eval_df['match_id'].values)]))\neval_df['match'] = l1\neval_df = eval_df.drop(labels='match_id', axis=1)\neval_df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}