{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"> # 1. Read and Clean Data","metadata":{}},{"cell_type":"markdown","source":"Import neccessary modules:","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport time\nimport matplotlib as plt\nimport string\nimport re\nimport spacy\nimport random","metadata":{"execution":{"iopub.status.busy":"2022-05-26T01:40:27.480532Z","iopub.execute_input":"2022-05-26T01:40:27.480986Z","iopub.status.idle":"2022-05-26T01:40:40.246043Z","shell.execute_reply.started":"2022-05-26T01:40:27.480868Z","shell.execute_reply":"2022-05-26T01:40:40.245149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Read and clean train dataset:","metadata":{}},{"cell_type":"code","source":"train=pd.read_csv(\"../input/foursquare-location-matching/train.csv\")\n#ordering by longitude because two close points are highly possible to share the same PoI.\ntrain=train.sort_values(by=\"longitude\")","metadata":{"execution":{"iopub.status.busy":"2022-05-26T01:40:44.028006Z","iopub.execute_input":"2022-05-26T01:40:44.028341Z","iopub.status.idle":"2022-05-26T01:40:55.132577Z","shell.execute_reply.started":"2022-05-26T01:40:44.028302Z","shell.execute_reply":"2022-05-26T01:40:55.131249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_data(train):\n    col=[\"name\",\"address\",\"country\",\"categories\"]\n    for i in col:\n        #filling nan/blank values and removing punctuation\n        train[i]=train[i].str.replace(\"[{}]\".format(string.punctuation),'',regex=True).fillna(\"nan\")\n        train[i][train[i]==\"\"]=0\n        train[i][train[i]==\"ERROR\"]=0\n        train[i]=train[i].astype(\"string\")\n\n    phone=[]\n    for i in train[\"phone\"]:\n        #reserving only numbers\n        i=str(i)\n        i=re.sub(r\"[^0-9]\",\"\",i)\n        if i==\"\": i=\"0\"\n        phone.append(i)\n    phone=np.array(phone)\n    #deleting numbers that are too long and padding all up to 20 digits\n    too_long=np.where(np.array([len(i) for i in phone])>20)[0]\n    phone[too_long]=0\n    train[\"phone\"]=phone\n    train[\"phone\"]=train[\"phone\"].map(lambda x: x.rjust(20,\"0\"))\n    \n    #noise because addresses comprised of these words can be hardly finded in real map.\n    noise=train[train[\"address\"].str.contains(\"高仿|微信|精仿\")]\n    train=train[~train[\"address\"].str.contains(\"高仿|微信|精仿\")]\n    \n    return train, noise\n\ntrain, noise=clean_data(train)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-26T01:40:55.134361Z","iopub.execute_input":"2022-05-26T01:40:55.134645Z","iopub.status.idle":"2022-05-26T01:41:14.946022Z","shell.execute_reply.started":"2022-05-26T01:40:55.134607Z","shell.execute_reply":"2022-05-26T01:41:14.945109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#the desired data\nprint(\"train data shape:\",train.shape)\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-26T01:41:14.947212Z","iopub.execute_input":"2022-05-26T01:41:14.947438Z","iopub.status.idle":"2022-05-26T01:41:14.977702Z","shell.execute_reply.started":"2022-05-26T01:41:14.947409Z","shell.execute_reply":"2022-05-26T01:41:14.976911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"noise data shape:\",noise.shape)\nnoise.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-26T01:41:14.979567Z","iopub.execute_input":"2022-05-26T01:41:14.981045Z","iopub.status.idle":"2022-05-26T01:41:15.288656Z","shell.execute_reply.started":"2022-05-26T01:41:14.980983Z","shell.execute_reply":"2022-05-26T01:41:15.287385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Embed words:","metadata":{}},{"cell_type":"code","source":"nlp=spacy.load(\"en_core_web_lg\")\n#we use google spaCy to embed words(only English is considered in this case).\ndef embed_fun(i,train_data=train):       \n        x=train_data.iloc[i]\n        \n        feature=[]\n        #we choose \"name\",\"country\",\"categories\"\n        col_nlp=[\"name\",\"country\",\"categories\"]\n        col_numeric=[\"latitude\",\"longitude\"]\n        \n        feature.append(np.array(x[\"id\"]))\n        \n        for k in col_nlp:\n            word=nlp(x[k])\n            feature.append(np.array(word.vector))\n            \n        lat_long=np.array([float(i) for i in x[col_numeric]])\n        feature.append(lat_long)\n        \n        phone=np.array([[int(l)] for l in x[\"phone\"]])\n        feature.append(phone)\n        \n        feature.append(np.array(x[\"point_of_interest\"]))\n\n        return np.array(feature,dtype=\"object\")","metadata":{"execution":{"iopub.status.busy":"2022-05-26T01:41:15.290323Z","iopub.execute_input":"2022-05-26T01:41:15.290619Z","iopub.status.idle":"2022-05-26T01:41:20.958356Z","shell.execute_reply.started":"2022-05-26T01:41:15.290579Z","shell.execute_reply":"2022-05-26T01:41:20.957287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#writing them in momery so the program can run faster.\n#For saving time, we just pre-embed them and save it in hard drive.\nif False:\n    t0=time.time()\n\n    train_data=[]\n    for i in range(600000):\n        train_data.append(embed_fun(i,train_data=train))\n        if i>0 and i%20000==0:\n            print(\"train data-%s rows has finished\"%i)\n            print(\"spent time:\",\"{:.4f}\".format(time.time()-t0))\n\n    path=\"embed_train_data\"\n    train_data=np.array(train_data,dtype=\"object\")\n    np.save(path,train_data)\n    print(\"data is saved\")\n\n    print(\"total spent time:\",\"{:.4f}\".format(time.time()-t0))","metadata":{"execution":{"iopub.status.busy":"2022-05-25T11:47:27.719201Z","iopub.execute_input":"2022-05-25T11:47:27.719441Z","iopub.status.idle":"2022-05-25T11:47:27.72699Z","shell.execute_reply.started":"2022-05-25T11:47:27.719411Z","shell.execute_reply":"2022-05-25T11:47:27.726336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if False:\n    t0=time.time()\n\n    test_data=[]\n    for i in range(train.shape[0]-500000,train.shape[0]):\n        test_data.append(embed_fun(i,train_data=train))\n        if i>0 and i%20000==0:\n            print(\"test data-%s rows has finished\"%i)\n            print(\"spent time:\",\"{:.4f}\".format(time.time()-t0))\n\n    path=\"embed_test_data\"\n    test_data=np.array(test_data,dtype=\"object\")\n    np.save(path,test_data)\n    print(\"data is saved\")\n\n    print(\"total spent time:\",\"{:.4f}\".format(time.time()-t0))","metadata":{"execution":{"iopub.status.busy":"2022-05-25T11:47:27.728172Z","iopub.execute_input":"2022-05-25T11:47:27.729163Z","iopub.status.idle":"2022-05-25T11:47:27.746614Z","shell.execute_reply.started":"2022-05-25T11:47:27.729108Z","shell.execute_reply":"2022-05-25T11:47:27.746083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_date(j,embed_data,w=3):\n    m=max(0,j-w)\n    M=min(embed_data.shape[0],j+w+1)\n    \n#we set [\"name\",\"country\",\"categories\",\"latitude\",\"longitude\"] as features of a data point.\n#list \"features\" is a group of data points we take into account\n#anchor is the reference comparing with points in \"features\" so we can tell how similar those points are.\n\n#discarding phone feature because we find out that the model tend to overfit this feature for higher payback\n#if two data points have the same phone numbers, then they are almost surely the same.\n#However, if their numbers are different, they still have possibility to share the same PoI.\n    features=embed_data[m:M,1:5]\n    anchor=embed_data[j,1:5]\n        \n    ID=embed_data[m:M,0]\n    PoI=embed_data[m:M,-1]\n    labels=[]\n    for i in PoI:\n        #if two data points both share the same PoI, then the label is 1 or is 0.\n        if i==PoI[j-m]:labels.append([1])\n        else:labels.append([0])\n    labels=np.array(labels)\n    \n    return anchor, features, labels, ID","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-26T09:56:47.447167Z","iopub.execute_input":"2022-05-26T09:56:47.447519Z","iopub.status.idle":"2022-05-26T09:56:47.458744Z","shell.execute_reply.started":"2022-05-26T09:56:47.447482Z","shell.execute_reply":"2022-05-26T09:56:47.457412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> # 2. Filter Data (Optional)","metadata":{}},{"cell_type":"code","source":"t0=time.time()\n#If two points are distant, then they are less likely to have the same PoI.\n#This method can identify the solitary points and thus reduce needed computing resources and time.\ndef neighbor(x,window):\n    indices=[]\n    m=x[\"longitude\"]\n    n=x[\"latitude\"]\n    for i in range(x.shape[0]):\n        k1=1;k2=1;lst=[]\n        if (i-k1>0):\n            if (abs(m.iloc[i]-m.iloc[i-k1])<=window):\n                if (abs(n.iloc[i]-n.iloc[i-k1])<=window):\n                    lst.append(i-k1)\n                    k1=k1+1\n        if (i+k2<=(x.shape[0]-1)):\n            if (abs(m.iloc[i]-m.iloc[i+k2])<=window):\n                if (abs(n.iloc[i]-n.iloc[i+k2])<=window):\n                    lst.append(i+k2)\n                    k2=k2+1\n        if len(lst)>0:lst.append(i)\n        indices.append(lst)\n    return indices\n\n#When the difference of one point and its closest neighbors is higher than 0.1 longitude/latitude,\n#then it's a solitary point(a point with an unique PoI).\n#the tolerance is approximately 10~11 km in low latitude area and 6~8 km in high latitude area.\nindices=neighbor(x=train,window=0.1)\ncount=[len(i) for i in indices]\nindices=np.array(indices,dtype=\"object\")\ncount=np.array(count)\n\nprint(\"spent time:\",\"{:.4f}\".format(time.time()-t0))","metadata":{"execution":{"iopub.status.busy":"2022-05-25T11:47:27.765086Z","iopub.execute_input":"2022-05-25T11:47:27.765538Z","iopub.status.idle":"2022-05-25T11:48:40.185585Z","shell.execute_reply.started":"2022-05-25T11:47:27.765501Z","shell.execute_reply":"2022-05-25T11:48:40.184552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"the number of total sample:\",indices.shape[0])\nprint(\"the number of count:\",count.shape[0])\nprint(\"the number of none-0 count:\",count[count>0].shape[0])\nprint(\"max count:\",count.max())","metadata":{"execution":{"iopub.status.busy":"2022-05-25T11:48:40.186848Z","iopub.execute_input":"2022-05-25T11:48:40.18712Z","iopub.status.idle":"2022-05-25T11:48:40.201498Z","shell.execute_reply.started":"2022-05-25T11:48:40.18709Z","shell.execute_reply":"2022-05-25T11:48:40.200574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t0=time.time()\n#however, due to noice, some points actually refer to the same PoI with other points even though they are far away.\n#so we need to compute the false negitive rate and define a desired distance.\n\ndef FN_lst(x,count):\n    false_negative_lst=[]\n    for i in np.where(count==0)[0]:\n        x=train.iloc[i][\"point_of_interest\"]\n        x0=[]\n        \n        for j in [1,2]:\n            if i-j>=0:\n                x0.append(train.iloc[i-j][\"point_of_interest\"])\n            else:x0.append(\"0\")\n            if i+j<=(train.shape[0]-1):\n                x0.append(train.iloc[i+j][\"point_of_interest\"])\n            else:x0.append(\"0\")\n\n        if x in x0:false_negative_lst.append(1)\n        else:false_negative_lst.append(0)\n\n    false_negative_lst=np.array(false_negative_lst)\n    return false_negative_lst\n\nfalse_negative_lst=FN_lst(x=train,count=count)\nfalse_negative=false_negative_lst.sum()/false_negative_lst.shape[0]\n\nprint(\"spent time:\",\"{:.4f}\".format(time.time()-t0))","metadata":{"execution":{"iopub.status.busy":"2022-05-25T11:48:40.203873Z","iopub.execute_input":"2022-05-25T11:48:40.20406Z","iopub.status.idle":"2022-05-25T11:51:47.994428Z","shell.execute_reply.started":"2022-05-25T11:48:40.204035Z","shell.execute_reply":"2022-05-25T11:51:47.993575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#suspended, just for noting\nif False:\n    t0=time.time()\n\n    def IoU_lst(x,count,indices):\n        IoU=[]\n        for i in np.where(count>0)[0]:\n            x=train.iloc[i][\"point_of_interest\"]\n            length=len(indices[i])\n            count=0\n            for j in indices[i]:\n                if x==train.iloc[j][\"point_of_interest\"]:count=count+1\n            IoU.append(count/length)\n        IoU=np.array(IoU)\n        return IoU\n\n    IoU=IoU_lst(x=train,count=count,indices=indices)\n\n    print(\"spent time:\",\"{:.4f}\".format(time.time()-t0))","metadata":{"execution":{"iopub.status.busy":"2022-05-25T11:51:47.995457Z","iopub.execute_input":"2022-05-25T11:51:47.995631Z","iopub.status.idle":"2022-05-25T11:51:48.00192Z","shell.execute_reply.started":"2022-05-25T11:51:47.995609Z","shell.execute_reply":"2022-05-25T11:51:48.001198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The number of false negitive points is 22k. It's about 2% but filters more than a quarter of total dataset.\nIf we choose larger distance *(w)*, then we reduce both the false negitive and filtered data points; therefore, there is a tradeoff between the number of false negitive and dataset size.","metadata":{}},{"cell_type":"code","source":"print(\"false negative Rate:\",\"{:.2%}\".format(false_negative))\nprint(\"false negative Rate (in whole train data set):\",\"{:.2%}\".format(np.where(false_negative_lst==1)[0].shape[0]/train.shape[0]))\nprint(\"the number of false negitive:\",np.where(false_negative_lst==1)[0].shape[0])\n#print(\"average IoU:\",\"{:.2%}\".format(IoU.mean()))\n#print(\"IoU not less than 50%:\",np.where(IoU>=0.5)[0].shape[0])","metadata":{"execution":{"iopub.status.busy":"2022-05-25T11:51:48.002851Z","iopub.execute_input":"2022-05-25T11:51:48.003008Z","iopub.status.idle":"2022-05-25T11:51:48.017344Z","shell.execute_reply.started":"2022-05-25T11:51:48.002989Z","shell.execute_reply":"2022-05-25T11:51:48.016395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> # 3. Siamese Network","metadata":{}},{"cell_type":"markdown","source":"Beacuse PoIs are multiple and various, it's not a good idea to set all as a label vector. However, we don't need to specify each PoI for each data point. All we need is to tell whether two or more points share the same one.","metadata":{}},{"cell_type":"markdown","source":"But sometimes the data structure can be complicated like this one, we have NLP embedded words (name, counrty, categories) and also numeric features. The question for any two data points, how \"close\" is close and how to define the weights across different features.","metadata":{}},{"cell_type":"markdown","source":"Luckly, Siamese Network is the answer. It's a special structure of DNN; the main idea of Siamese Network is to tell how similar two data points are. When a data point (e.g., photos, sentences) went through a network and features were generated, if two data points have an identical label, then those feature must be very \"close\". So we can create a specific network that connect two (either different or identical) networks and then calculate the closeness of data points.","metadata":{}},{"cell_type":"code","source":"from IPython.display import Image\nprint(\"source:https://www.youtube.com/watch?v=4S-XDefSjTM\")\nImage(\"../input/images/2022-05-17 8.06.36.png\")","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-05-25T11:51:48.018562Z","iopub.execute_input":"2022-05-25T11:51:48.018797Z","iopub.status.idle":"2022-05-25T11:51:48.065735Z","shell.execute_reply.started":"2022-05-25T11:51:48.018766Z","shell.execute_reply":"2022-05-25T11:51:48.06509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* ## Baseline model\nThe first step is to create a network that can extract features from our row dataset. Because google spaCy has already finished the job of word embedding, we only need to process the phone numbers.","metadata":{}},{"cell_type":"code","source":"def BaseLine():\n    input_name=tf.keras.Input(shape=(300),name=\"name\")\n    output_name=input_name\n    \n    input_country=tf.keras.Input(shape=(300),name=\"country\")\n    output_country=input_country\n    \n    input_categories=tf.keras.Input(shape=(300),name=\"categories\")\n    output_categories=input_categories\n    \n    input_lat_long=tf.keras.Input(shape=(2),name=\"input_lat_long\")\n    output_lat_long=input_lat_long\n    \n    model=tf.keras.Model(inputs=[input_name,input_country,input_categories,input_lat_long],\n                         outputs=[output_name,output_country,output_categories,output_lat_long])\n    return model\n\nbaseline=BaseLine()\nbaseline.summary()","metadata":{"execution":{"iopub.status.busy":"2022-05-26T09:58:43.985144Z","iopub.execute_input":"2022-05-26T09:58:43.985475Z","iopub.status.idle":"2022-05-26T09:58:44.009815Z","shell.execute_reply.started":"2022-05-26T09:58:43.985439Z","shell.execute_reply":"2022-05-26T09:58:44.008549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The baseline model is a small network. We just simply pass feature name, country, categories and lat_long.","metadata":{}},{"cell_type":"code","source":"tf.keras.utils.plot_model(baseline)","metadata":{"execution":{"iopub.status.busy":"2022-05-26T09:58:47.417839Z","iopub.execute_input":"2022-05-26T09:58:47.41833Z","iopub.status.idle":"2022-05-26T09:58:47.822837Z","shell.execute_reply.started":"2022-05-26T09:58:47.418292Z","shell.execute_reply":"2022-05-26T09:58:47.821773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* ## Siamese Network\nThe final step of building our model is to create a lambda layer that connects two baseline models. Lambda layer is actually a square error function.","metadata":{}},{"cell_type":"code","source":"def siamese_model(baseline):\n    def difference_square(vec):\n        x, y=vec\n        output=(x-y)**2 #penalty function one can also try others like absolute error.\n        return output\n\n    anchor_name=tf.keras.Input(shape=(300),name=\"anchor_name\")\n    anchor_country=tf.keras.Input(shape=(300),name=\"anchor_country\")\n    anchor_categories=tf.keras.Input(shape=(300),name=\"anchor_categories\")\n    anchor_lat_long=tf.keras.Input(shape=(2),name=\"anchor_lat_long\")\n\n    input_name=tf.keras.Input(shape=(300),name=\"name\")\n    input_country=tf.keras.Input(shape=(300),name=\"country\")\n    input_categories=tf.keras.Input(shape=(300),name=\"categories\")\n    input_lat_long=tf.keras.Input(shape=(2),name=\"lat_long\")\n\n    inputs1=baseline([anchor_name,anchor_country,anchor_categories,anchor_lat_long])\n    inputs2=baseline([input_name,input_country,input_categories,input_lat_long])\n\n    x0=tf.keras.layers.Lambda(difference_square,name=\"diff_name\")([inputs1[0],inputs2[0]])\n    x1=tf.keras.layers.Lambda(difference_square,name=\"diff_country\")([inputs1[1],inputs2[1]])\n    x2=tf.keras.layers.Lambda(difference_square,name=\"diff_categories\")([inputs1[2],inputs2[2]])\n    x3=tf.keras.layers.Lambda(difference_square,name=\"diff_lat_long\")([inputs1[3],inputs2[3]])\n\n    #linear combination works best \n    #because we need the layers accurately respond the difference between two data points rather than \"twist\" it.\n    x0=tf.keras.layers.Dense(150)(x0) \n    x0=tf.keras.layers.Dense(1,name=\"score_name\")(x0)\n\n    x1=tf.keras.layers.Dense(150)(x1)\n    x1=tf.keras.layers.Dense(1,activation=\"tanh\",name=\"score_country\")(x1)\n\n    x2=tf.keras.layers.Dense(150)(x2)\n    x2=tf.keras.layers.Dense(1,name=\"score_categories\")(x2)\n    \n    #hyperbolic function is to suppress it so the model won't pay too much attention on.\n    x3=tf.keras.layers.Dense(1)(x3)\n    x3=tf.keras.layers.Dense(1,activation=\"tanh\",name=\"score_lat_long\")(x3)\n\n    x=tf.keras.layers.Concatenate()([x0,x1,x2,x3])\n    x=tf.keras.layers.Dense(4)(x)\n    #residual layer\n    res=tf.keras.layers.Dense(4,activation=\"relu\")(x)\n    x=tf.keras.layers.Add()([x,res])\n    x=tf.keras.layers.Dense(2)(x)\n    res=tf.keras.layers.Dense(2,activation=\"relu\")(x)\n    x=tf.keras.layers.Add()([x,res])\n    output=tf.keras.layers.Dense(1,activation=\"sigmoid\",name=\"score\")(x)\n    \n    model=tf.keras.Model(\n        inputs=[anchor_name,anchor_country,anchor_categories,anchor_lat_long,\n               input_name,input_country,input_categories,input_lat_long],\n        outputs=output\n    )\n    return model\n\nmodel=siamese_model(baseline)\n#model.load_weights(\"../input/siamese-network-weights/siamese_model\")\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-05-26T09:59:23.81243Z","iopub.execute_input":"2022-05-26T09:59:23.813763Z","iopub.status.idle":"2022-05-26T09:59:23.998095Z","shell.execute_reply.started":"2022-05-26T09:59:23.813651Z","shell.execute_reply":"2022-05-26T09:59:23.996412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As following, the complete network has 8 inputs (4 from anchor and the 4 from other samples) and 1 output. Selecting a sample each time and passing through a lambda layer with anchor, we therefore get the penalty score of two data points. As sending to dense layers, we can have sum-up scores of feature penalties and finally, get the similarity of two sample.","metadata":{}},{"cell_type":"code","source":"tf.keras.utils.plot_model(model)","metadata":{"execution":{"iopub.status.busy":"2022-05-26T09:59:29.061575Z","iopub.execute_input":"2022-05-26T09:59:29.061866Z","iopub.status.idle":"2022-05-26T09:59:29.633432Z","shell.execute_reply.started":"2022-05-26T09:59:29.06182Z","shell.execute_reply":"2022-05-26T09:59:29.631717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> # 4. Training and Testing Data","metadata":{}},{"cell_type":"markdown","source":"For two points that are distant, there is no need to compare and we only consider anchor's neighborhood. In following training, we select a data point as an anchor and its seven closest points (including itself) as a training group.","metadata":{}},{"cell_type":"code","source":"model.compile(\n    optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.0001),\n    loss=tf.keras.losses.BinaryCrossentropy(),\n    metrics=[tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n             tf.keras.metrics.Precision(name='precision'),\n             tf.keras.metrics.Recall(name='recall')]\n)\n\nclass myCallback(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs={}):\n        if(logs.get('accuracy')==1):\n            self.model.stop_training=True\ncallbacks = myCallback()\n\naccuracy=[]\nprecision=[]\nrecall=[]","metadata":{"execution":{"iopub.status.busy":"2022-05-26T09:59:44.385505Z","iopub.execute_input":"2022-05-26T09:59:44.38579Z","iopub.status.idle":"2022-05-26T09:59:44.441714Z","shell.execute_reply.started":"2022-05-26T09:59:44.385759Z","shell.execute_reply":"2022-05-26T09:59:44.440745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data=np.load(\"../input/foursquare-embedding-train-and-test-data/embed_train_data.npy\",allow_pickle=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-26T03:37:38.517557Z","iopub.execute_input":"2022-05-26T03:37:38.51821Z","iopub.status.idle":"2022-05-26T03:45:12.939415Z","shell.execute_reply.started":"2022-05-26T03:37:38.518162Z","shell.execute_reply":"2022-05-26T03:45:12.937031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lst=random.choices(range(train_data.shape[0]),k=40000) \n#it's just for saving time, we already had trained this model.\n\nt0=time.time()\nfor l in range(len(lst)):\n    anchor, features, labels, ID=create_date(lst[l],train_data,w=3)\n    k=len(labels)\n    #because the anchor itself is also in neighbor group, there must be at least one \"1\" label.\n    inputs=[\n            tf.constant([anchor[0]]*k),tf.constant([anchor[1]]*k),tf.constant([anchor[2]]*k),tf.constant([anchor[3]]*k),\n            tf.constant(list(features[:,0])),tf.constant(list(features[:,1])),tf.constant(list(features[:,2])),tf.constant(list(features[:,3]))\n        ]                                                        \n    history=model.fit(x=inputs,y=labels,epochs=20,verbose=0,callbacks=[callbacks])\n\n    accuracy.append(history.history[\"accuracy\"])\n    precision.append(history.history[\"precision\"])\n    recall.append(history.history[\"recall\"])\n    \n    if l>0 and l%2000==0:\n        print(\"%sth group is finished\"%l)\n        print(\"spent time:\",\"{:.4f}\".format(time.time()-t0))\n    if l>0 and l%10000==0:\n        model.save_weights(\"siamese_model\")\n\nprint(\"total spent time:\",\"{:.4f}\".format(time.time()-t0))\nmodel.save_weights(\"siamese_model\")\n#del train_data","metadata":{"execution":{"iopub.status.busy":"2022-05-26T11:13:56.237226Z","iopub.execute_input":"2022-05-26T11:13:56.23776Z","iopub.status.idle":"2022-05-26T11:55:22.708879Z","shell.execute_reply.started":"2022-05-26T11:13:56.237708Z","shell.execute_reply":"2022-05-26T11:55:22.707048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Acc=np.array([i[-1] for i in accuracy])\nPre=np.array([i[-1] for i in precision])\nRec=np.array([i[-1] for i in recall])\n\ndef moving_average(a, n=300):\n    ret = np.cumsum(a)\n    ret[n:] = ret[n:] - ret[:-n]\n    return ret[n-1:]/n","metadata":{"execution":{"iopub.status.busy":"2022-05-26T11:58:41.936346Z","iopub.execute_input":"2022-05-26T11:58:41.936826Z","iopub.status.idle":"2022-05-26T11:58:42.072251Z","shell.execute_reply.started":"2022-05-26T11:58:41.936782Z","shell.execute_reply":"2022-05-26T11:58:42.071123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"average accuracy:\",\"{:.4f}%\".format(Acc.mean()*100))\nprint(\"average precision:\",\"{:.4f}%\".format(Pre.mean()*100))\nprint(\"average recall:\",\"{:.4f}%\".format(Rec.mean()*100))\n\nfig, axe=plt.pyplot.subplots(3,1,figsize=(18,15))\n\naxe[0].set_title(\"moving average(300)-accuracy\")\naxe[0].set_xticks(np.arange(0,len(moving_average(Acc)),5000))\naxe[0].plot(moving_average(Acc))\n\naxe[1].set_title(\"moving average(300)-precision\")\naxe[1].set_xticks(np.arange(0,len(moving_average(Pre)),5000))\naxe[1].plot(moving_average(Pre),\"g\")\n\naxe[2].set_title(\"moving average(300)-recall\")\naxe[2].set_xticks(np.arange(0,len(moving_average(Rec)),5000))\naxe[2].plot(moving_average(Rec),\"m\")\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-26T11:58:44.740798Z","iopub.execute_input":"2022-05-26T11:58:44.741704Z","iopub.status.idle":"2022-05-26T11:58:45.435024Z","shell.execute_reply.started":"2022-05-26T11:58:44.741648Z","shell.execute_reply":"2022-05-26T11:58:45.43361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Test our model.","metadata":{}},{"cell_type":"code","source":"test_accuracy=[]\ntest_precision=[]\ntest_recall=[]\n\ntest_data=np.load(\"../input/foursquare-embedding-train-and-test-data/embed_test_data.npy\",allow_pickle=True)\n\nt0=time.time()\nfor l in range(test_data.shape[0]):\n    anchor, features, labels, ID=create_date(l,train_data,w=3)\n    k=len(labels)\n    inputs=[\n            tf.constant([anchor[0]]*k),tf.constant([anchor[1]]*k),tf.constant([anchor[2]]*k),tf.constant([anchor[3]]*k),\n            tf.constant(list(features[:,0])),tf.constant(list(features[:,1])),tf.constant(list(features[:,2])),tf.constant(list(features[:,3]))\n        ]  \n    lst=model(inputs).numpy()\n\n\n    BinaryAccuracy=tf.keras.metrics.BinaryAccuracy()\n    Precision=tf.keras.metrics.Precision()\n    Recall=tf.keras.metrics.Recall()\n\n    BinaryAccuracy.update_state(labels, lst)\n    test_accuracy.append(BinaryAccuracy.result().numpy())\n\n    Precision.update_state(labels, lst)\n    test_precision.append(Precision.result().numpy())\n\n    Recall.update_state(labels, lst)\n    test_recall.append(Recall.result().numpy())\n    \n    if l>0 and l%2000==0:\n        print(\"%sth group is finished\"%l)\n        print(\"spent time:\",\"{:.4f}\".format(time.time()-t0))\n\nprint(\"total spent time:\",\"{:.4f}\".format(time.time()-t0))\n\ndel test_data","metadata":{"execution":{"iopub.status.busy":"2022-05-26T06:19:43.322396Z","iopub.execute_input":"2022-05-26T06:19:43.322827Z","iopub.status.idle":"2022-05-26T07:13:53.430397Z","shell.execute_reply.started":"2022-05-26T06:19:43.322779Z","shell.execute_reply":"2022-05-26T07:13:53.428932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_accuracy=np.array(test_accuracy)\ntest_precision=np.array(test_precision)\ntest_recall=np.array(test_recall)\n\nprint(\"average accuracy:\",test_accuracy.mean())\nprint(\"average precision:\",test_precision.mean())\nprint(\"average recall:\",test_recall.mean())\n\nfig, axe=plt.pyplot.subplots(3,1,figsize=(18,15))\n\naxe[0].set_title(\"moving average(300)-test accuracy\")\naxe[0].set_xticks(np.arange(0,len(moving_average(test_accuracy,n=300)),10000))\naxe[0].plot(moving_average(test_accuracy))\n\naxe[1].set_title(\"moving average(300)-test precision\")\naxe[1].set_xticks(np.arange(0,len(moving_average(test_precision,n=300)),10000))\naxe[1].plot(moving_average(test_precision),\"g\")\n\naxe[2].set_title(\"moving average(300)-test recall\")\naxe[2].set_xticks(np.arange(0,len(moving_average(test_recall,n=300)),10000))\naxe[2].plot(moving_average(test_recall),\"m\")\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-26T07:14:22.769628Z","iopub.execute_input":"2022-05-26T07:14:22.769981Z","iopub.status.idle":"2022-05-26T07:14:23.53084Z","shell.execute_reply.started":"2022-05-26T07:14:22.769941Z","shell.execute_reply":"2022-05-26T07:14:23.529513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}