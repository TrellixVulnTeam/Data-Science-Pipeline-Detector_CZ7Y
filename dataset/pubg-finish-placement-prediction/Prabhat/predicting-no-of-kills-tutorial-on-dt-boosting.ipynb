{"cells":[{"metadata":{"_uuid":"32aba5a4cc5073b347fb7f8065aa91da931fa0ce"},"cell_type":"markdown","source":"<img src=\"pubg.jpg\">"},{"metadata":{"_uuid":"3d7b8d174444a97384429a2117a85cc47a3af661"},"cell_type":"markdown","source":"## Problem Statement"},{"metadata":{"_uuid":"3b4ee3a5ef101d2976c19654fecb0f9c85ebaf46"},"cell_type":"markdown","source":"PlayerUnknown’s BattleGround (PUBG) has taken the world by storm. 100 players are dropped onto an island empty-handed and must explore, scavenge, and eliminate other players until only one is left standing, all while the play zone continues to shrink.\nPUBG has enjoyed massive popularity. With over 50 million copies sold, it's the fifth best selling game of all time and has millions of active monthly players.\nThough we are pretty sure that all of you would have shown some great skills in playing PUBG, it’s time for the action outside “The Blue Circle”, but this time with the power of Machine Learning.\n\nThe task is to predict the number of kills made by a player by analyzing its other attributes like survival time, team size, assists, walking and riding time etc. given in the dataset ‘pubg_kills.csv’."},{"metadata":{"_uuid":"e0d16f6ae8c31edfcb01a43e13929e8d975a69a4"},"cell_type":"markdown","source":"## Data  Description"},{"metadata":{"_uuid":"7a6da2473afd0acd114ec9f1cb5c0fd5236a15b7"},"cell_type":"markdown","source":"The given dataset has the following variables:\n\n* match_id: The unique match id.\n* date: The date and time the match took place\n* game_size: The total number of teams that were in the game\n* match_mode: whether the game was played in first-person (FPP) or third-person (TPP)\n* party_size: The maximum number of players per team. e.g 2 implies it was a duo.\n* player_name: Name of the player\n* team_id: The team id that the player belonged to\n* team_placement: The final rank of the team within the match\n* player_dbno: Number of knockdowns the player has scored\n* player_assists: Number of assists the player has scored\n* player_dmg: Total Hitpoint that the player has dealt\n* player_dist_ride: Total distance that the player has traveled in a vehicle\n* player_dist_walk: Total distance that the player has traveled on foot\n* player_kills: Number of kills the player has scored ⇒ <b>To be predicted</b>\n"},{"metadata":{"_uuid":"da064d98a59d66a7793f9fb415072b3304199db1"},"cell_type":"markdown","source":"## Lets start Data Science Game"},{"metadata":{"_uuid":"c70470c52b4ddc4723130e46c66c166f2b98c9e7"},"cell_type":"markdown","source":"### Importing Standard Libraries"},{"metadata":{"trusted":false,"_uuid":"c0345dc93936dfdf0df8fadc15fe7a1b74b80ee6"},"cell_type":"code","source":"import numpy as np  # Library for array processing , Linear algebra\nimport pandas as pd  # Library for data processing, data manipulation\nimport matplotlib.pyplot as plt  # Library for data visualisation\nimport seaborn as sns  # Library for different plots\n\nfrom sklearn.model_selection import train_test_split  # To split data into training and validation data\nfrom sklearn.metrics import mean_squared_error  # Evaluation metric\n\nfrom subprocess import check_call # for running command line process\n\nsns.set(style=\"whitegrid\", color_codes=True) \nsns.set(font_scale=1)\n\nfrom IPython.display import display \npd.options.display.max_columns = None  # To display all columns in the notebook\nfrom IPython.display import Image as PImage # To display all images inside the notebook\n\n# Displaying graphs in the notebook itself\n%matplotlib inline \n\nimport warnings\nwarnings.filterwarnings('ignore')  # Doesn't display warnings","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6876480394f9dc1a1aa3a38907e07115d0fea82a"},"cell_type":"markdown","source":"### Loading Dataset"},{"metadata":{"trusted":false,"_uuid":"a5c6d4bc06e03ee8b20ed90d6423d58b31a0f41c"},"cell_type":"code","source":"### START CODE HERE ###\n# Read and store the data in a dataframe 'data' to be used for furthur processing (1 line of code)\ndata = pd.read_csv(\"pubg_kills.csv\")\n### END CODE HERE ###","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b2bb5f8ea6f7aa7da3e0dc7802e43ac6d9646864"},"cell_type":"code","source":"# Display first five rows of the dataset\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"142f2b06dbf621d78f5f8f224508312a74cb90bd"},"cell_type":"code","source":"# Similarily data.tail() shows last five rows of the data\n\n### START CODE HERE ###\n# Display the last five rows of the data (1 line of code)\ndata.tail()\n### END CODE HERE ###","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e082fc438c2301414bf509a47f39dff39f0f71e4"},"cell_type":"code","source":"# Dimensions of the data\n# Number of rows, Number of columns(features)\nprint(data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"794b9d63d6e99e4fc0f6f0bccc1b693b668796c2"},"cell_type":"code","source":"# print all the columns/features in the data","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a081788456f87c7a84d2230f4ce0315aaed05185"},"cell_type":"markdown","source":"#### Length of the dataset"},{"metadata":{"trusted":false,"_uuid":"2fccb7506fb9c216aa45618462fd99ab144397a8"},"cell_type":"code","source":"#length of dataset\nlen(data)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ad1ab9fc031b73496cb4c4e41ab7503e0558b198"},"cell_type":"markdown","source":"## Understanding Pandas DataFrame"},{"metadata":{"trusted":false,"_uuid":"24a4cf5ae52d37de2bf11b2329d762160723fccd"},"cell_type":"code","source":"#To access a column player_survive_time\ndata['player_survive_time'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"69bcf2ba4424048b3a85d9979d34715035660831"},"cell_type":"code","source":"#To access multiple columns\ndata[['party_size','player_kills']].head(4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"941c2ebb23876e0e096a5999a915a535d9266a26"},"cell_type":"code","source":"#To access a multiple rows\ndata.iloc[3:6]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b74ef7aedb8e7d1d4bef066d3ee4416cf007c944"},"cell_type":"markdown","source":"## Dealing with the 'date' feature"},{"metadata":{"trusted":false,"_uuid":"4c02f38a16853cdb86d8ee0c37d8178d517687f2"},"cell_type":"code","source":"#to change the date format\ndata['date'] =  pd.to_datetime(data['date'], format='%Y-%m-%dT%H:%M:%S+0000')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ebcdfd9cfdcdc2e56bbed855ab131e3816770f79"},"cell_type":"code","source":"#extracting the weekday from date\ndata['Day'] = pd.DatetimeIndex(data['date']).weekday\nweekday_map = {0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', 3: 'Thursday', 4: 'Friday', 5: 'Saturday', 6: 'Sunday'}","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"f90352535e5f6a4f8428081d2c867c8155e2e281"},"cell_type":"code","source":"# Extracting hour from time\n# creating new variable hour from the time variable \ndata['Hour'] = pd.DatetimeIndex(data['date']).hour","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"d1904327204034717fd74180a97dca4461e0c5ce"},"cell_type":"code","source":"# display first three rows of the data\ndata.head(3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4e87f6d4f970b09c474cd4cb1a651495a1bdf283"},"cell_type":"markdown","source":"### Getting Rid of Redundant Variables"},{"metadata":{"trusted":false,"_uuid":"fcde65489efde68cbcd3b0f1192b03861b3d42c2"},"cell_type":"code","source":"del data['date']  # As we have already extracted the useful info i.e. Weekday and Hour\ndel data['match_mode']  # Because all the matches were played in TPP (Third-Person Perspective) mode\ndel data['team_id']  # Because we already have match_id and player_name to uniquely identify an instance","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d63ff0b54d9e7feb146c2dd4955c55a16e768913"},"cell_type":"markdown","source":"## Steps\n*  Problem Identification \n*  Hypothesis Generation\n*  Variable Identification\n*  Univariate Analysis\n*  Bivariate Analysis\n*  Missing Values\n*  Outliers\n*  Feature Engineering/Variable Transformation\n*  Predictive Modeling\n*  Analysing the Model\n*  Final Model Selection"},{"metadata":{"_uuid":"8665d89c0bcc5ab770796bd7bf718b863764ad4c"},"cell_type":"markdown","source":"## Variable Identification & their datatypes\nIdentify the predictor and target variables & their data types along with the category of variables"},{"metadata":{"trusted":false,"_uuid":"5b6fb928e772ceeaff3627d07437a9e118353cf7"},"cell_type":"code","source":"# determining data types of the variable\ndata.dtypes","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0312bbefa7601f1dcf79c714cdac04506c6ed23d"},"cell_type":"markdown","source":"#### Normally, numeric columns in python are represented as \"int32\", \"float32\", \"int64\", \"float64\". Whereas character columns are represented as \"object\""},{"metadata":{"_uuid":"04aed67725d0adcb76ad33e96ae51581658ccc05"},"cell_type":"markdown","source":"## Univariate Analysis\nAnalysing the variables one at a time. Let's analyse coninuous and categorical variables separately."},{"metadata":{"_uuid":"a9df27a526a77d544705ee025ad34cf9f5977692"},"cell_type":"markdown","source":"### For Continuous Variables : We generally measure the central tendency of the variable such as Mean , Median , Mode , Std, variance ,etc.\n* Basic Statistics\n* Plotting Histogram\n* Plotting Boxplot"},{"metadata":{"trusted":false,"_uuid":"233db37763487c28e8685a884e79eb34fcf4b799"},"cell_type":"code","source":"# continious variable analysis\ndata.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"f6bce99dbc84a77caaa523e0b6340a99b5f8a2f3"},"cell_type":"code","source":"# plot given numerical variable with respect to other variables\ncont_vars = ['player_dbno', 'player_dist_walk', 'player_dmg', 'player_kills']\nsns.pairplot(data[cont_vars])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"d55f038750c43f2354bd63297bc79e215dde304c"},"cell_type":"code","source":"#Plotting histogram for 'player_kills' variable\nsns.distplot(data['player_kills'], color=\"purple\", kde=False)\nplt.title(\"Distribution of Number of Kills\")\nplt.ylabel(\"Number of Occurences\")\nplt.xlabel(\"Number of Kills\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"232950d515de5c69fdf78614384d5bc75a20ea32"},"cell_type":"code","source":"#frequency of each value in weekday column\nweekday_map = {0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', 3: 'Thursday', 4: 'Friday', 5: 'Saturday', 6: 'Sunday'}\ndict(data.Day.value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"cd8085d7def7efdb58d46bb2c1062bbd4873396e"},"cell_type":"code","source":"#Plotting histogram for 'Day' variable\nweek_data = {'Mon': 14155, 'Tue': 13860, 'Wed': 13183, 'Thu': 11611, 'Fri': 14458, 'Sat': 16443, 'Sun': 16290}\nnames = list(week_data.keys())\nvalues = list(week_data.values())\n\nfig, axs = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\naxs[0].bar(names, values)\naxs[1].plot(names, values)\nfig.suptitle('Categorical Plotting')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b772b22ea532251dca3130d638cd179b5c4fc549"},"cell_type":"code","source":"#for more information -> https://chartio.com/resources/tutorials/what-is-a-box-plot/\nsns.boxplot(\"game_size\", data=data, showfliers=False)\nplt.title(\"Distribution of game_size\")\nplt.xlabel(\"Number of Teams in Game\");","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"28873c735e10f46453d8d3d8acde010813c81e5e"},"cell_type":"markdown","source":"### For categorical variables: We generally measure the frequency of categories appearing in a particular categorical variable\n* Count/Frequency Table\n* Plotting Stacked Bar Graph"},{"metadata":{"trusted":false,"_uuid":"5084ce5de85b0bca9887078dd790f8f40a78ce37"},"cell_type":"code","source":"# selecting categorical variables from the data\ncategorical_variables = ['party_size', 'Day', 'Hour']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"fca70a7bb1fcc05b5d6a0c0cebbd52f9fceb8856"},"cell_type":"code","source":"#print categorical variables\nprint(categorical_variables)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"f853904b2810c722228f5c4c181c13acc480eac2"},"cell_type":"code","source":"# unique values count in each categorical variable\ndata[categorical_variables].apply(lambda x: len(x.unique()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"1db52380776a73043ddc5549c848b0989adda2f6"},"cell_type":"code","source":"#frequency count of each categorical variable\nfor var in categorical_variables:\n    print(var)\n    print(data[var].value_counts())\n    print('\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"d178d306536bbb4d1769e9f395aea0ea31779d3c"},"cell_type":"code","source":"#display in pie chart\nlabels = data['party_size'].unique()\nsizes = data['party_size'].value_counts().values\nexplode=[0.1,0,0]\nparcent = 100.*sizes/sizes.sum()\nlabels = ['{0} - {1:1.1f} %'.format(i,j) for i,j in zip(labels, parcent)]\n\ncolors = ['yellowgreen', 'gold', 'lightblue']\npatches, texts= plt.pie(sizes, colors=colors,explode=explode,\n                        shadow=True,startangle=90)\nplt.legend(patches, labels, loc=\"best\")\n\nplt.title(\"Party Size Classification\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d4e260211113ebb271af5c18d89613ff5ffa94b5"},"cell_type":"markdown","source":"## Bivariate Analysis\nBivariate analysis is used to find out the relationship between any 2 variables. It can be done for any combination of variables. The combinations are: \n* Continuous & Continuous\n* Categorical & Continuous\n* Categorical & Categorical"},{"metadata":{"_uuid":"a6ec768f1b6f5fb30a6eb37fe077b0b48dbb4e7f"},"cell_type":"markdown","source":"### Continuous & Continuous\nScatter Plots are used"},{"metadata":{"trusted":false,"_uuid":"52c3ca8274976330ba4b55578c7966a0f0d98f94"},"cell_type":"code","source":"#scatter plot\nplt.scatter(np.sqrt(data[\"player_dmg\"]), data[\"player_dbno\"])\n# to display title above the plot\nplt.title(\"Hitpoints Dealt Vs Down but not out \")\n# to label y-axis\nplt.ylabel(\"No. of DBNO's\")\n# to label x-axis\nplt.xlabel(\"Hitploints Dealt by the Player\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"901f2b43da74c18fad8b849305ed30783706f355"},"cell_type":"code","source":"# correlation between variables \n# heat map\ncorrMatrix = data[[\"game_size\", \"player_assists\", \"player_dbno\",\n                   \"player_dist_ride\", \"player_dist_walk\", \"player_dmg\",\n                   \"player_survive_time\", \"team_placement\", \"player_kills\"]].corr()\n\nsns.set(font_scale=1.10)\nplt.figure(figsize=(9, 9))\n\nsns.heatmap(corrMatrix, vmax=.8, linewidths=0.01,\n            square=True,annot=True,cmap='viridis',linecolor=\"white\")\nplt.title('Correlation between features');","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"88706e702210ff12875a00e464fd02cb7b918c07"},"cell_type":"markdown","source":"#### +1 : perfect postive correlation ; -1 : perfect negative correlation ; 0 : No correlation"},{"metadata":{"_uuid":"835d64d012b999fc9cc1865076b68edaffa57c6b"},"cell_type":"markdown","source":"### Categorical & Continuous\nBoxplots can be used"},{"metadata":{"trusted":false,"_uuid":"51cd95a76cb3f473335401e7004c3dd4db905761"},"cell_type":"code","source":"# sns.boxplot(x, y, argument to hide outliers)\nsns.boxplot(data[\"party_size\"], data[\"player_survive_time\"], showfliers=False)\n# title for the plot\nplt.title(\"Survival Time vs Team Size\")\nplt.ylabel(\"Survival Time\")\nplt.xlabel(\"Team Size\");","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ce202476fc3cb17a50249d3d22bac0ffe2b94ba1"},"cell_type":"markdown","source":"### Categorical and categorical\nCrosstable and stacked bar plots are used"},{"metadata":{"trusted":false,"_uuid":"6145a2240dafd8dc4c1f95ac395e686836bbd46a"},"cell_type":"code","source":"crosstable = pd.crosstab(data.Day, data.party_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"fccff3e1a152b3064487ba1cce006849078844b8"},"cell_type":"code","source":"crosstable","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"57068e3c930a22e92b802a52300e9f79a40c5e29"},"cell_type":"code","source":"# Plotting stacked bar plot\ncrosstable.plot(kind='bar',stacked='True')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3bfca6e6441bc7b162e0c3ca2d67e7c0370a78d0"},"cell_type":"markdown","source":"## Missing Values"},{"metadata":{"trusted":false,"_uuid":"5649e9b322087ad3cda7995a47b91058e4ad304b"},"cell_type":"code","source":"# Detecting missing values\ndata.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cfe6ec8a262cfbf74abbd9a3f34621ea5c052b45"},"cell_type":"markdown","source":"\n### Treating missing values:\n* For continuous variables impute with mean\n* For categorical variables impute with mode\n* For better results predict missing values in a variable by considering it target variable\n* If missing values are less then we can delete the observations having missing values\n"},{"metadata":{"_uuid":"6f06a353d5dce7d72cb0e36d95e1ab04fb85ddc3"},"cell_type":"markdown","source":"## Outliers\nOutliers are the data points showing out of the box behaviour or that appears far away from the overall trend."},{"metadata":{"trusted":false,"_uuid":"bd5957147e151c5ab186df3f6e8ad4fb36f60ed9"},"cell_type":"code","source":"#box plot\nsns.boxplot(\"player_survive_time\", data=data, showfliers=True)\nplt.title(\"Distribution of Survival Time\")\nplt.xlabel(\"Survival Time\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"5bc7b0faeb229f8afb1131b9a0a1ac6dcc70b09f"},"cell_type":"code","source":"#Treating outliers\n# Removing Outliers\nQ1 = data['player_survive_time'].quantile(.25)\nQ3 = data['player_survive_time'].quantile(.75)\nIQR = Q3-Q1\nlower_value = IQR-1.5*Q1\nupper_value = IQR+1.5*Q3","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"0a249607d796517da85127d237a2f9019fab7268"},"cell_type":"code","source":"# print range lower_value and upper_value\nlower_value, upper_value","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"65d3c972bafbf8121403037758e4e3d6dbd36874"},"cell_type":"code","source":"#replacing outlier with meadian value the data\ndef outlier_imputer(x):\n    if x < lower_value or x > upper_value:\n        return data['player_survive_time'].median()\n    else:\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"6504a3561c42bd90f96e245174be228a5d2549c6"},"cell_type":"code","source":"result = data['player_survive_time'].apply(outlier_imputer)  # This would take a lil bit time to run","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"838e3c6f0f5e3edcdef5b9f18a211b7402d12795"},"cell_type":"code","source":"sns.boxplot(result, showfliers=True)\nplt.title(\"Distribution of Survival Time\")\nplt.xlabel(\"Survival Time\");","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3eb85216e2715c8a4dc256d335a9efe01f740b48"},"cell_type":"markdown","source":"# Building the First Model"},{"metadata":{"_uuid":"d0861be143350cdffbbc9b07237a50c3802f250f"},"cell_type":"markdown","source":"#### After tightening seat-belt its time to takeoff"},{"metadata":{"trusted":false,"_uuid":"7b7f2684eedf0a9259504f1722878cd795892840"},"cell_type":"code","source":"#depenent_variable -> which we are going to predict\n#independent_variable -> helps in predicting dependent_variable\ndependent_variable = 'player_kills'\nindependent_variable = ['game_size', 'party_size', 'player_assists', 'player_dbno', 'player_dist_ride', 'Hour', \n                        'player_dist_walk', 'player_dmg', 'player_survive_time', 'team_placement', 'Day']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b30e717d0a371e2cd3314e2a1505bfb477ea95c2"},"cell_type":"code","source":"independent_variable","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d12034aead932a82c6290460615a3c02034e7767"},"cell_type":"markdown","source":"###  Splitting our data into training and testing(validation) data"},{"metadata":{"trusted":false,"_uuid":"4f206fbab8a4d6e860a51ab068fe0ed1e9171492"},"cell_type":"code","source":"#library to split data\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"bc9fb26a4861aade4c87e1571c4770e1ffc2f71d"},"cell_type":"code","source":"train, test = train_test_split(data, test_size=.2, shuffle=True, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"2680dcc2a97b20766d10097f65974e02ff4b8be5"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"5547caa42de696e647b2b27deeb362527575774d"},"cell_type":"code","source":"print(len(data))\nprint(len(train))\nprint(len(test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a3cfea1a09d8799a6b43c08e1fb630b80ec37f45"},"cell_type":"code","source":"# Predicting by using mode\nnp.round(train['player_kills'].mean())  # train['player_kills'].mean() = 0.887","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"89035c3ef077fd8d1f997b852d162beed665ddab"},"cell_type":"code","source":"test['prediction'] = 1.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"7342b5dd2a042d644f7e13d5236cf0330476d65e"},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"895a4fda7ca1d0517096797cb175b3e27663172e"},"cell_type":"code","source":"# Analysing the prediction\nfrom sklearn.metrics import mean_squared_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e764329058703c9e2bf4f6cf314eec988788a315"},"cell_type":"code","source":"RMSE = np.sqrt(mean_squared_error(test['prediction'], test[dependent_variable]))\nnp.round(RMSE)  # RMSE = 1.616","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6e345d2072a74d03103aacb662f2bb58fbedcbeb"},"cell_type":"markdown","source":"# Building Machine Learning Model"},{"metadata":{"_uuid":"5f4f25438bfeb3b75592990f5684eadaff924682"},"cell_type":"markdown","source":"# Linear Regression"},{"metadata":{"_uuid":"4b144621285f5a08390b369a806d0d0364cdc653"},"cell_type":"markdown","source":"## Simple Linear Regression\n\nSimple linear regression is an approach for predicting a **quantitative response** using a **single feature** (or \"predictor\" or \"input variable\"). It takes the following form:\n\n$y = \\beta_0 + \\beta_1x$\n\nWhat does each term represent?\n- $y$ is the response\n- $x$ is the feature\n- $\\beta_0$ is the intercept\n- $\\beta_1$ is the coefficient for x\n\nTogether, $\\beta_0$ and $\\beta_1$ are called the **model coefficients**. To create your model, you must \"learn\" the values of these coefficients. And once we've learned these coefficients, we can use the model to predict Sales!"},{"metadata":{"_uuid":"1a8063689789eddad75d9216ea8c174ea536da13"},"cell_type":"markdown","source":"## Estimating (\"Learning\") Model Coefficients\n\nGenerally speaking, coefficients are estimated using the **least squares criterion**, which means we are find the line (mathematically) which minimizes the **sum of squared residuals** (or \"sum of squared errors\"):"},{"metadata":{"_uuid":"51688099b20f02943ccb405ddc1d5eb1ae7ae548"},"cell_type":"markdown","source":"<img src=\"08_estimating_coefficients.png\">"},{"metadata":{"_uuid":"81b003d9140a4f3e82fd5f16ae5775c47088340e"},"cell_type":"markdown","source":"What elements are present in the diagram?\n- The black dots are the **observed values** of x and y.\n- The blue line is our **least squares line**.\n- The red lines are the **residuals**, which are the distances between the observed values and the least squares line.\n\nHow do the model coefficients relate to the least squares line?\n- $\\beta_0$ is the **intercept** (the value of $y$ when $x$=0)\n- $\\beta_1$ is the **slope** (the change in $y$ divided by change in $x$)\n\nHere is a graphical depiction of those calculations:"},{"metadata":{"_uuid":"d2fce06d0fdadd5483aa640b6f58a2f8336bfd93"},"cell_type":"markdown","source":"<img src=\"08_slope_intercept.png\">"},{"metadata":{"_uuid":"0ceb06e87605b740823bbe418f2bb6e814fce747"},"cell_type":"markdown","source":"### Using Linear Regression Algorithm"},{"metadata":{"trusted":false,"_uuid":"5cfe1e269af4b40acaaf12a2d17a59ecdc5e410b"},"cell_type":"code","source":"# Importing machine learning library\nfrom sklearn.linear_model import LinearRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a1d8af7895c5a6c889f3d2b594759dcd6c8489eb"},"cell_type":"code","source":"# Creating machine learning model\nmodel1 = LinearRegression()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"180bcc5b43392138908bd7cd17b1d36e21636e75"},"cell_type":"code","source":"# Training our model\nmodel1.fit(train[independent_variable], train[dependent_variable])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"21abb27e5cc0d17ad3154479f8ab14115f509e03"},"cell_type":"code","source":"# Get coeffecients\nmodel1.coef_","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c3f04b170f5df83cf63053cea6721003943b6c78"},"cell_type":"code","source":"# Get intercept\nmodel1.intercept_","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"f352ea5cf58eb29c73c87be6fcba5bda3332c4f4"},"cell_type":"code","source":"# Predicting on test data\nprediction = model1.predict(test[independent_variable])","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"a1b0a40cdafd52ef0d181ec5f1ef6fdc4213bd31"},"cell_type":"markdown","source":"#### Analysing our model"},{"metadata":{"trusted":false,"_uuid":"f882d5f5a505451e8f6ecf360f0896c9c80c543e"},"cell_type":"code","source":"# Accuracy on training dataset\nnp.sqrt(mean_squared_error(model1.predict(train[independent_variable]), train[dependent_variable]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"1ce7e08e9c36c33e7a44ca55fddb4949f658d13e"},"cell_type":"code","source":"# Accuracy on testing dataset\nnp.sqrt(mean_squared_error(model1.predict(test[independent_variable]), test[dependent_variable]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a10a761245ac2b1fd07d4522bf1660044e406f16"},"cell_type":"markdown","source":"# Introduction to Decision Trees\n\n\n||continuous|categorical|\n|---|---|---|\n|**supervised**|**regression**|**classification**|\n|**unsupervised**|dimension reduction|clustering|"},{"metadata":{"_uuid":"798a67bbc3ce4c4b02ea93af11f9f10a7583df69"},"cell_type":"markdown","source":"## Regression trees\n\nLet's look at a simple example to motivate our learning.\n\nOur goal is to **predict a baseball player's Salary** based on **Years** (number of years playing in the major leagues) and **Hits** (number of hits he made in the previous year). Here is the training data, represented visually (low salary is blue/green, high salary is red/yellow):"},{"metadata":{"_uuid":"ffb89bd2fb9c1a4cc625e1182dd7045c0341b5ca"},"cell_type":"markdown","source":"<img src=\"15_salary_color.png\">"},{"metadata":{"_uuid":"4a48e9e0293ae6ac3099c354a042c6aa5a549c3d"},"cell_type":"markdown","source":"**How might you \"stratify\" or \"segment\" the feature space into regions, based on salary?** Intuitively, you want to **maximize** the similarity (or \"homogeneity\") within a given region, and **minimize** the similarity between different regions.\n\nBelow is a regression tree that has been fit to the data by a computer. (We will talk later about how the fitting algorithm actually works.) Note that  Salary is measured in thousands and has been log-transformed."},{"metadata":{"_uuid":"5a5ffeac4ad2e3ffcf24a5f063608f83a8e6b68d"},"cell_type":"markdown","source":"<img src=\"15_salary_tree.png\">"},{"metadata":{"_uuid":"88001fc28a328fd1c49344b994882188918e8bed"},"cell_type":"markdown","source":"**How do we make Salary predictions (for out-of-sample data) using a decision tree?**\n\n- Start at the top, and examine the first \"splitting rule\" (Years < 4.5).\n- If the rule is True for a given player, follow the left branch. If the rule is False, follow the right branch.\n- Continue until reaching the bottom. The predicted Salary is the number in that particular \"bucket\".\n- *Side note:* Years and Hits are both integers, but the convention is to label these rules using the midpoint between adjacent values.\n\nExamples predictions:\n\n- Years=3, then predict 5.11 ($\\$1000 \\times e^{5.11} \\approx \\$166000$)\n- Years=5 and Hits=100, then predict 6.00 ($\\$1000 \\times e^{6.00} \\approx \\$403000$)\n- Years=8 and Hits=120, then predict 6.74 ($\\$1000 \\times e^{6.74} \\approx \\$846000$)\n\n**How did we come up with the numbers at the bottom of the tree?** Each number is just the **mean Salary in the training data** of players who fit that criteria. Here's the same diagram as before, split into the three regions:"},{"metadata":{"_uuid":"92833c465ba123b85658b68fcae538fb67afaea5"},"cell_type":"markdown","source":"<img src=\"15_salary_regions.png\">"},{"metadata":{"_uuid":"fcb61986d7e288719d85221eac7999cd15131a26"},"cell_type":"markdown","source":"This diagram is essentially a combination of the two previous diagrams (except that the observations are no longer color-coded). In $R_1$, the mean log Salary was 5.11. In $R_2$, the mean log Salary was 6.00. In $R_3$, the mean log Salary was 6.74. Thus, those values are used to predict out-of-sample data.\n\nLet's introduce some terminology:"},{"metadata":{"_uuid":"c780c12c29c284c856325660f004863798f58066"},"cell_type":"markdown","source":"<img src=\"15_salary_tree_annotated.png\">"},{"metadata":{"_uuid":"4899899a2efd0b208e4e91d797948e2b7834c32d"},"cell_type":"markdown","source":"**How might you interpret the \"meaning\" of this tree?**\n\n- Years is the most important factor determining Salary, with a lower number of Years corresponding to a lower Salary.\n- For a player with a lower number of Years, Hits is not an important factor determining Salary.\n- For a player with a higher number of Years, Hits is an important factor determining Salary, with a greater number of Hits corresponding to a higher Salary.\n\nWhat we have seen so far hints at the advantages and disadvantages of decision trees:\n\n**Advantages:**\n\n- Highly interpretable\n- Can be displayed graphically\n- Prediction is fast\n\n**Disadvantages:**\n\n- Predictive accuracy is not as high as some supervised learning methods\n- Can easily overfit the training data (high variance)"},{"metadata":{"_uuid":"b65b43e6ff69d220b72f261a6324290e6bb7de4f"},"cell_type":"markdown","source":"## How does a computer build a regression tree?\n\nThe ideal approach would be for the computer to consider every possible partition of the feature space. However, this is computationally infeasible, so instead an approach is used called **recursive binary splitting:**\n\n- Begin at the top of the tree.\n- For every single predictor, examine every possible cutpoint, and choose the predictor and cutpoint such that the resulting tree has the **lowest possible mean squared error (MSE)**. Make that split.\n- Repeat the examination for the two resulting regions, and again make a single split (in one of the regions) to minimize the MSE.\n- Keep repeating this process until a stopping criteria is met.\n\n**How does it know when to stop?**\n\n1. We could define a stopping criterion, such as a **maximum depth** of the tree or the **minimum number of samples in the leaf**.\n2. We could grow the tree deep, and then \"prune\" it back using a method such as \"cost complexity pruning\" (aka \"weakest link pruning\").\n\nMethod 2 involves setting a tuning parameter that penalizes the tree for having too many leaves. As the parameter is increased, branches automatically get pruned from the tree, resulting in smaller and smaller trees. The tuning parameter can be selected through cross-validation.\n\nNote: **Method 2 is not currently supported by scikit-learn**, and so we will use Method 1 instead.\n\nHere's an example of an **unpruned tree**, and a comparison of the training, test, and cross-validation errors for trees with different numbers of leaves:"},{"metadata":{"_uuid":"44ee8831474efc70d709859260f7b314c43503df"},"cell_type":"markdown","source":"<img src=\"15_salary_unpruned.png\">"},{"metadata":{"_uuid":"c23740084de96d4b60f703297c75d005ac62fd30"},"cell_type":"markdown","source":"As you can see, the **training error** continues to go down as the tree size increases, but the lowest **cross-validation error** occurs for a tree with 3 leaves."},{"metadata":{"_uuid":"c4de7a7445ca8a9f869d4d9b809a38f24729900b"},"cell_type":"markdown","source":"## Building a regression tree in scikit-learn"},{"metadata":{"trusted":false,"_uuid":"1cd2aa208bfcbe737f2674523cee88f7035537fa"},"cell_type":"code","source":"# Importing Decision Tree Classifier\nfrom sklearn.tree import DecisionTreeRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"4ac881a581e300518cf7e411cc8b5be32edbb6c0"},"cell_type":"code","source":"model2 = DecisionTreeRegressor()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"4d7103e977a3d304e78880607acdf2e501e53387"},"cell_type":"code","source":"# Training our model\nmodel2.fit(train[independent_variable], train[dependent_variable])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"f0978e09b25a8cce6a6823ce145aaa23691b5e2b"},"cell_type":"code","source":"# Get Predictions\nprediction = model2.predict(test[independent_variable])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b139941ed171a942dfb6de44e3bb38c59a922ac1"},"cell_type":"code","source":"# Accuracy on testing dataset\nnp.sqrt(mean_squared_error(prediction, test[dependent_variable]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"12345caead49a0b2a2bdc822614bf56dc0be0d35"},"cell_type":"code","source":"# create a Graphviz file\nfrom sklearn.tree import export_graphviz\nwith open(\"tree1.dot\", 'w') as f:\n    f = export_graphviz(model2, out_file=f, feature_names=independent_variable)\n    \n#Convert .dot to .png to allow display in web notebook\n#Please install graphviz before this conda install python-graphviz\n#check_call(['dot','-Tpng','tree.dot','-o','tree.png'])\n\n# Annotating chart with PIL\n#img = Image.open(\"tree.png\")\n#img.save('sample-out.png')\n#PImage(\"sample-out.png\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a05a78a694bd4c27a5926398dbcc0f64cb743a9d"},"cell_type":"markdown","source":"# Introduction to Boosting\n\n### Boosting is an ensemble technique in which the predictors are not made independently, but sequentially.\n\nThis technique employs the logic in which the subsequent predictors learn from the mistakes of the previous predictors. Therefore, the observations have an unequal probability of appearing in subsequent models and ones with the highest error appear most. (So the observations are not chosen based on the bootstrap process, but based on the error). The predictors can be chosen from a range of models like decision trees, regressors, classifiers etc."},{"metadata":{"_uuid":"263bfd4c423a1420ab66ed2b580f67e5b543bddd"},"cell_type":"markdown","source":"<img src=\"residuals_learning.png\">"},{"metadata":{"_uuid":"54b206087ef945664bc14f4468ac446b61a9860e"},"cell_type":"markdown","source":"<img src=\"residuals_learning2.png\">"},{"metadata":{"_uuid":"5f6406f53b7a2b78b434565d1d2a0552b48446c4"},"cell_type":"markdown","source":"### Using GradientBoostingRegressor"},{"metadata":{"trusted":false,"_uuid":"475109f9cd4a80ed6293aee8a3060142b4562240"},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"2e1ecdd70df1853908e947f83890adf081a43813"},"cell_type":"code","source":"model3 = GradientBoostingRegressor()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"fb279b96fc4d37b2c79d84157953f0258ffb3502"},"cell_type":"code","source":"# Training our model\nmodel3.fit(train[independent_variable], train[dependent_variable])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"abc6b42e9195af1849348fc302f8df8871b27614"},"cell_type":"code","source":"feat_importances = pd.Series(model3.feature_importances_, index=train[independent_variable].columns)\nfeat_importances.nsmallest(len(independent_variable)).plot(kind='barh')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"d6fd3ccb6434a59744d5f6d9f2e59be5de11222b"},"cell_type":"code","source":"# Get Predictions\nprediction = model3.predict(test[independent_variable])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a5cf6bbfc06290211a2055bed4e966ab39b7b2c5"},"cell_type":"code","source":"# Accuracy on testing dataset\nnp.sqrt(mean_squared_error(prediction, test[dependent_variable]))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"a0a3ec448d3392cfa3f6a2842dd5ee789f3f1b8f"},"cell_type":"markdown","source":"## What You Can Try Next on Your Own"},{"metadata":{"collapsed":true,"_uuid":"bc4299b2e52901bf0cb7f4e182e235c471630980"},"cell_type":"markdown","source":"We saw that LightGBM outperformed Linear Regression and Decision Trees by a little margin and clearly surpassed our baseline model by a huge amount. However, few more things can be tried to push RMSE:\n\n* HyperParameter Tuning using Hyperopt etc.\n* Better feature generation.\n* Trying ensembles of different models.\n* Better feature transformations."},{"metadata":{"collapsed":true,"_uuid":"d1f5413eadf278b79366fc7da2ff1e087a6a4ce5"},"cell_type":"markdown","source":"## Where to Go from Here"},{"metadata":{"collapsed":true,"_uuid":"bd04ceb50786260dbcf0c2836fc467b7629539dd"},"cell_type":"markdown","source":"Here are some resources and blogs that would help one to get started in Data Science and Machine Learning:\n\n* __[DSG Blog about How to Start Data Science](https://medium.com/data-science-group-iitr/stop-thinking-start-learning-cb74629bca3a)__\n* __[DSG Medium Handle](https://medium.com/data-science-group-iitr)__\n* __[3 Blue 1 Brown](https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw)__\n* __[Harvard Data Science Course (CS109)](http://cs109.github.io/2015/pages/videos.html)__\n* __[Andrew Ng Machine Learning Course](http://cs229.stanford.edu/)__\n* __[Analytics Vidhya](https://www.analyticsvidhya.com/blog/)__\n* __[Machine Learning Mastery](https://machinelearningmastery.com/)__\n* __[Kaggle (A Competitive Data Science Platform)](https://www.kaggle.com/)__"},{"metadata":{"trusted":false,"_uuid":"40f80e6ba8bfbfa5305acfc4ec791e76750f4706"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a68157b30a91c70917411ef00fcb4e8287553d17"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}