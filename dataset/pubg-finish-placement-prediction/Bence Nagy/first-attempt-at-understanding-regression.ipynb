{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# https://stackoverflow.com/questions/56283294/importerror-cannot-import-name-factorial\n!pip install statsmodels==0.10.0rc2","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I'm gonna need to familiarize myself with basic snippets like stuff for loading data; these I'll just be copy-pasting from other kernels."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_data = pd.read_csv('../input/train_V2.csv')\ntest_data = pd.read_csv('../input/test_V2.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.info()\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Okay, let's tackle the problem. Here's a couple starting points:\n\n- I took notes at PyCon US at a talk\n  called _\"When not to use neural networks and what to do instead\"_:\n  - I'll be checking that here:\n    https://github.com/underyx/conference-notes/blob/master/pycon-us-2019/when-not-to-use-deep-learning.md\n  - I don't quite grasp the underlying concepts\n    but at least know what terms to Google\n- The first time machine learning ever really clicked for me\n  was at Google I/O, during an intro talk\n  - The talk is available online here: https://www.youtube.com/watch?v=_RPHiqF2bSs\n    (I didn't take notes there)\n  - I'll probably try to adapt the code from the talk to this problem\n  - But I also suspect it would an unreasonably long time to train a machine learning model\n    so I'll try this approach after the more basic ones from the above PyCon talk\n- One thing I'm guessing now is that more `damageDealt` means a better finishing position,\n  so to keeps things simple, I'll start out by trying to make this work as the sole predicting feature\n  - The very simplest thing I'll try right now is **fitting a linear regression model**.\n    - For help with data transformation I found https://github.com/pandas-dev/pandas/blob/master/doc/cheatsheet/Pandas_Cheat_Sheet.pdf\n    - For help with linear regression I'll follow https://devarea.com/linear-regression-with-numpy/"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head(10)[[\"damageDealt\", \"winPlacePerc\"]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So let's try to visualize this,\nto see if a linear relationship even seems to be visible.\nThe article I linked above does this with `matplotlib`\nbut I remember seeing somewhere how much nicer `seaborn` looked\nso I'll try to use that first; it can't be that hard."},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"darkgrid\")\n\ndata_to_plot = (\n    train_data.sample(n=200)[[\"damageDealt\", \"winPlacePerc\"]]\n)\n\nsns.relplot(x=\"damageDealt\", y=\"winPlacePerc\", data=data_to_plot)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That's surpisingly all over the place.\nI'm guessing lots of these might be\ndue to less active players piggybacking on their teammates' high `damageDealt`.\nFor the sake of doing running any kind of prediction ASAP,\nlet me filter to solo players for now."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_to_plot = (\n    train_data[train_data.matchType.isin({\"solo\", \"solo-fpp\"})].sample(n=200)[[\"damageDealt\", \"winPlacePerc\"]]\n)\n\nsns.relplot(x=\"damageDealt\", y=\"winPlacePerc\", data=data_to_plot)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Alright, that's something.\nLet me see what I can do if I try to predict with linear regression on this.\nHopefully I'll get really low accuracy so I'll get to see lots of improvement once moving on to proper models."},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import stats\nprint(\"pure random\\n\", stats.linregress(np.random.random(200), np.random.random(200)).rvalue ** 2)\n\nsolo_train_data = train_data[train_data.matchType.isin({\"solo\", \"solo-fpp\"})]\nprint(\"20 samples\\n\", stats.linregress(solo_train_data.sample(n=20)[[\"damageDealt\", \"winPlacePerc\"]]).rvalue ** 2)\nprint(\"200 samples\\n\", stats.linregress(solo_train_data.sample(n=200)[[\"damageDealt\", \"winPlacePerc\"]]).rvalue ** 2)\nprint(\"2000 samples\\n\", stats.linregress(solo_train_data.sample(n=2000)[[\"damageDealt\", \"winPlacePerc\"]]).rvalue ** 2)\nprint(\"20000 samples\\n\", stats.linregress(solo_train_data.sample(n=20000)[[\"damageDealt\", \"winPlacePerc\"]]).rvalue ** 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I was checking R-squared values here\ncause people were writing how that's a useful metric for determining level of correlation.\nAnd since that number's higher than when trying to regress on random number series,\nI guess we did something, yay!\n\nWhen trying to look into how to visualize the results,\nI found out that seaborn has some cool tooling for calculating regression models on its own.\n\nThis tutorial is super useful for exploring my options:\nhttps://seaborn.pydata.org/tutorial/regression.html#regression-tutorial"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.regplot(x=\"damageDealt\", y=\"winPlacePerc\", x_jitter=5, data=solo_train_data.sample(n=500)[[\"damageDealt\", \"winPlacePerc\"]])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"One thing that bothers me is that this model doesn't seem to understand\nthat we're bound to results between 0.0 and 1.0.\n\nI have no idea what LOWESS/\"locally weighted scatterplot smoothing\" is,\nbut based on the above tutorial,\nit seems to return much more sensible results for datasets such as ours.\n\nLet me try it first, and if it truly looks good on our dataset,\nI'll read up on it.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.regplot(x=\"damageDealt\", y=\"winPlacePerc\", x_jitter=5, lowess=True, data=solo_train_data.sample(n=200)[[\"damageDealt\", \"winPlacePerc\"]])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Damn, that looks much better.\nUpon some reading, what I seem to have done with enabling `lowess`\nis that I stopped telling the computer that I expect a linear relationship here.\nThat's actually good: I have no reason to assume linear,\nor pretty much any kind of relationship.\n\nNormally I'd be worried about overfitting\nif I let the computer decide what kind of relationship to look for.\nBut looking at this regression line,\nit seems like what we have is still fairly smooth and inaccurate\n(there are no random spikes near outlier datapoints)\nso I'll go with it for now."},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.api as sm\nsample = solo_train_data.sample(n=10)\nprint(sample[[\"winPlacePerc\", \"damageDealt\"]])\nprint(sm.nonparametric.lowess(sample[\"winPlacePerc\"], sample[\"damageDealt\"]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Okay, this function seems to be smoothing existing data.\nThat's not what we want.\nThere's probably a way to use it to improve our models butâ€¦\nI don't want to go down too deep of a rabbit hole here.\n\nLet's just abandon this for now,\ncause I noticed that the LOWESS function fitted something\nthat looks like graphing `y = sqrt(x)` back in high school.\n\nSo I looked for some overview of regression functions\nand seems like polynomial is what I need here:\nhttps://towardsdatascience.com/machine-learning-polynomial-regression-with-python-5328e4e8a386\n\nI checked Seaborn's docs and the function I have above actually has an `order` parameter.\nLet's try it!"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.regplot(x=\"damageDealt\", y=\"winPlacePerc\", x_jitter=5, order=2, data=solo_train_data.sample(n=200)[[\"damageDealt\", \"winPlacePerc\"]])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well, that looks nice.\nBut I'm worried about the downward curve at the end.\nThat makes no logical sense.\nHow do I get rid of it?\n\nReading the other parameters of `seaborn.regplot`\nthere was one called `log`\nand the Wikipedia article on this looks very promising.\n\nFinally I found something that doesn't just looks right,\nbut makes logical sense, too."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.regplot(x=\"damageDealt\", y=\"winPlacePerc\", x_jitter=5, logistic=True, data=solo_train_data.sample(n=2000)[[\"damageDealt\", \"winPlacePerc\"]])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The only thing that doesn't make perfect logical sense is that\napparently the inputs on `y` should be just 0s and 1s.\n\nI can sort of justify passing in `winPlacePerc`\nas it correlates with win probability from 0.0 to 1.0.\n\nIt's really time to run some prediction now\nso I won't try finding even cooler models for now.\nLet's use https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nclean_train_data = train_data[[\"damageDealt\", \"winPlacePerc\"]].dropna()\n\nX_train, X_test, y_train, y_test = train_test_split(clean_train_data, clean_train_data.winPlacePerc, test_size=0.2)\n\nmodel = sm.Logit(y_train, X_train.damageDealt)\nresult = model.fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\n\npredictions = result.predict(X_test.damageDealt)\nsns.scatterplot(y_test[:200], predictions[:200])\n\nprint(mean_absolute_error(y_test, np.random.random(len(y_test))))\nprint(mean_absolute_error(y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Alright, we've got something that's better than random.\nAin't gonna win any awards, but I'm glad it seemed to work out.\n\nIt's a bit suspicious that 0.5 seems to be a hard lower bound for our predictions.\nJust for fun, let me check what happens\nif we expand the range of predictions by brute force."},{"metadata":{"trusted":true},"cell_type":"code","source":"garbled_predictions = (predictions - 0.5) * 2.0\n\nprint(mean_absolute_error(y_test, garbled_predictions))\nsns.scatterplot(y_test[:200], garbled_predictions[:200])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Worse than random.\nUnsurprising, but still, lesson learned.\nDon't mess with the models."},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_predictions = result.predict(test_data.damageDealt)\nsubmission = pd.DataFrame({\"Id\": test_data.Id, \"winPlacePerc\": submission_predictions})\nsubmission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}