{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold ,KFold, GridSearchCV\nfrom xgboost import XGBRFRegressor,XGBRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor, ExtraTreesRegressor, VotingRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression, RANSACRegressor, LogisticRegression, Lasso, Ridge, ElasticNet\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport gc\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df\n\ndef plot_learning_curves(model, X, y):\n    X_train, X_val, y_train, y_val = train_test_split(X,y, test_size = 0.2)\n    train_errors, val_errors = [], []\n    for m in range(1, len(X_train)):\n        model.fit(X_train[:m], y_train[:m])\n        y_train_predict = model.predict(X_train[:m])\n        y_val_predict = model.predict(X_val)\n        train_errors.append(mean_squared_error(y_train[:m], y_train_predict))\n        val_errors.append(mean_squared_error(y_val, y_val_predict))\n    plt.plot(np.sqrt(train_errors), \"r-+\", linewidth=2, label='훈련 세트')\n    plt.plot(np.sqrt(val_errors), \"b-\", linewidth=3, label='검증세트')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/pubg-finish-placement-prediction/train_V2.csv')\ntrain = reduce_mem_usage(train)\ntest = pd.read_csv('/kaggle/input/pubg-finish-placement-prediction/test_V2.csv')\ntest = reduce_mem_usage(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/pubg-finish-placement-prediction/test_V2.csv')\ntest = reduce_mem_usage(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"shape of before train drop null data : \"+ str(train.shape[0]) + \",\" + str(train.shape[1]))\ntrain = train.dropna()\nprint(\"shape of after train drop null data : \"+ str(train.shape[0]) + \",\" + str(train.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kfolds= KFold(n_splits=3)\n\nrid = Ridge()\nlasso = Lasso()\nela = ElasticNet()\nrfr = RandomForestRegressor(random_state = 42)\nxgr = XGBRegressor(random_state = 42)\nxgrf = XGBRFRegressor(random_state = 42)\ndtr = DecisionTreeRegressor(random_state = 42)\nadar = AdaBoostRegressor(random_state = 42)\ngrdr = GradientBoostingRegressor(random_state = 42)\nlinr = LinearRegression()\nlogr = LogisticRegression(random_state = 42)\nsvr = SVR()\nranc = RANSACRegressor()\nextr = ExtraTreesRegressor(random_state = 42)\n\ndef fillInf(df, val):\n    numcols = df.select_dtypes(include = 'number').columns\n    cols = numcols[numcols != 'winPlacePerc']\n    df[df == np.Inf] = np.NaN\n    df[df == np.NINF] = np.NaN\n    for c in data_cols:\n        df[c].fillna(val, inplace = True)\n\ndef pipline(data):\n    \n    def fillInf(df, val):\n        numcols = df.select_dtypes(include = 'number').columns\n        cols = numcols[numcols != 'winPlacePerc']\n        df[df == np.Inf] = np.NaN\n        df[df == np.NINF] = np.NaN\n        for c in data_cols:\n            df[c].fillna(val, inplace = True)\n    \n    col = ['assists', 'boosts', 'damageDealt', 'DBNOs',\n           'headshotKills','heals','killPlace','kills','killStreaks','longestKill',\n            'walkDistance']\n\n    cols = ['killPoints', 'maxPlace', 'numGroups', 'rankPoints', 'roadKills', 'teamKills','winPoints',\n           'matchDuration']\n    \n    group = data.groupby(['matchId','groupId','matchType'])\n    match = data.groupby('matchId')\n\n    match_data = pd.concat([\n        match.size().to_frame('mplayers'),\n        match[col].sum().rename(columns = lambda s : 'msum' + s),\n        match[col].max().rename(columns = lambda s : 'mmax' + s),\n        match[col].mean().rename(columns = lambda s : 'mmean' + s)\n    ], axis = 1).reset_index()\n\n    group_data = pd.concat([\n        group.size().to_frame('gplayer'),\n        group[col].sum().rename(columns = lambda x : 'gsum' + x),\n        group[col].max().rename(columns = lambda x : 'gmax' + x),\n        group[col].mean().rename(columns = lambda x : 'gmean' + x)\n    ], axis = 1).reset_index()\n\n    \n    data_one = pd.merge(match_data, group_data)\n\n    data_one = reduce_mem_usage(data_one)\n\n    data['DBNOsAndMP'] = data['DBNOs'] / data['maxPlace']\n    data['DBNOsAndNG'] = data['DBNOs'] / data['numGroups']\n    data['RDAndMD'] = data['rideDistance'] / data['matchDuration']\n    data['voidKills'] = data['teamKills'] + data['roadKills']\n    data['totalDistance'] = data['rideDistance'] + data['swimDistance'] + data['walkDistance']\n\n    test_data = pd.merge(data_one, data)\n\n\n    test_data['PlayerTime'] = test_data['mplayers'] / test_data['matchDuration']\n    test_data['enemyPlayer'] = test_data['mplayers'] - test_data['gplayer']\n    test_data['SavePlayer'] = test_data['enemyPlayer'] - test_data['kills']\n    \n    data_cols = test_data.columns\n    fillInf(test_data, 0)\n    \n    group_cols = group_data.columns\n    match_cols = match_data.columns\n    \n    for i in col:\n        test_data['msum' + i + 'avg'] = test_data['msum' + i] / test_data[i]\n        test_data['mmax' + i + 'avg'] = test_data['msum' + i] / test_data[i]\n        test_data['mmean' + i + 'avg'] = test_data['mmean' + i] / test_data[i]\n        test_data['gsum' + i + 'avg'] = test_data['gsum' + i] / test_data[i]\n        test_data['gmax' + i + 'avg'] = test_data['gmax' + i] / test_data[i]\n        test_data['gmean' + i + 'avg'] = test_data['gmean' + i] / test_data[i]\n        \n        test_data.drop('msum'+i, axis = 1, inplace = True)\n        test_data.drop('mmax'+i, axis = 1, inplace = True)\n        test_data.drop('mmean'+i, axis = 1, inplace = True)\n        test_data.drop('gsum'+i, axis = 1, inplace = True)\n        test_data.drop('gmax'+i, axis = 1, inplace = True)\n        test_data.drop('gmean'+i, axis = 1, inplace = True)\n    \n    for i in col:\n        test_data.drop(i, axis = 1, inplace = True)\n    for j in cols:\n        test_data.drop(j, axis = 1, inplace = True)\n\n    data_cols = test_data.columns\n    fillInf(test_data, 0)\n    \n    test_data = reduce_mem_usage(test_data)\n\n    test_data = pd.get_dummies(test_data, columns = ['matchType'])\n    test_data.drop({'groupId', 'matchId', 'Id'}, axis = 1, inplace = True)\n\n    \n        \n    return test_data\n\ntest = pipline(test)\nscaler = StandardScaler()\ntest = scaler.fit_transform(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pipline(train)\n\nscaler = StandardScaler()\nY = train['winPlacePerc']\nX = train.drop(['winPlacePerc'], axis = 1)\nX = reduce_mem_usage(X)\nX_scale = scaler.fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size = 0.2, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from lightgbm import LGBMRegressor\n\ngbm=LGBMRegressor()\n\ngbmreg = GridSearchCV(estimator = gbm,\n                     scoring = 'neg_mean_squared_error',\n                     param_grid = {'max_depth' : [2,4,6] },\n                     cv = kfolds)\ngbmreg.fit(X_train, y_train)\ny_predict_gbm = gbmreg.predict(X_test)\nprint('best mse : ',gbmreg.best_score_,' r2_score : ', r2_score(y_test, y_predict_gbm))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_test_y = gbmreg.predict(test)\ntest_raw = pd.read_csv('/kaggle/input/pubg-finish-placement-prediction/test_V2.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.DataFrame(columns=['Id', 'winPlacePerc'])\ntest['Id'] = test_raw['Id']\ntest['winPlacePerc'] = pred_test_y\n\ntest.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"file = './submission.csv'\n\nif os.path.isfile(file):\n  os.remove(file)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}