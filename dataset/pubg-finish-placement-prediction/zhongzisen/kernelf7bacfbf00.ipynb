{"cells":[{"metadata":{"trusted":true,"_uuid":"3f04024034508d332b4f50aa764c1efb35f95794"},"cell_type":"code","source":"import numpy as mp\nimport pandas as pd\ndef memory_reduce(df):\n    start_memory = df.memory_usage().sum()/1024**2\n#     print(\"start_memory is {:.2f}\".format(start_memory))\n    for col in df.columns:\n        col_type = df[col].dtype\n        if(col_type != object):\n            min_val = min(df[col])\n            max_val = max(df[col])\n            if(str(col_type)[:3] == 'int'):\n                if(min_val > np.iinfo(np.int8).min and max_val < np.iinfo(np.int8).max):\n                    df[col] = df[col].astype(np.int8)\n                elif(min_val > np.iinfo(np.int16).min and max_val < np.iinfo(np.int16).max):\n                    df[col] = df[col].astype(np.int16)\n                elif(min_val > np.iinfo(np.int32).min and max_val < np.iinfo(np.int32).max):\n                    df[col] = df[col].astype(np.int32)\n                elif(min_val > np.iinfo(np.int64).min and max_val < np.iinfo(np.int64).max):\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if(min_val > np.finfo(np.float16).min and max_val < np.finfo(np.float16).max):\n                    df[col] = df[col].astype(np.float16)\n                elif(min_val > np.finfo(np.float32).min and max_val < np.finfo(np.float32).max):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n#         else:\n#             print(col)\n#             df[col] = df[col].astype('category')\n    end_memory = df.memory_usage().sum()/1024**2\n#     print('end_memory is {:.2f}'.format(end_memory))\n#     print((start_memory - end_memory) / start_memory)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dc4c2a2f80397414c91f364035c808c36f58bf9f"},"cell_type":"code","source":"# import pandas as pd\n# import numpy as np\n# train = pd.read_csv('../input/train_V2.csv' , nrows = 1000000)\n# test = pd.read_csv('../input/test_V2.csv' , nrows = 1000000)\n# # print(train['winPlacePerc'] * train['numGroups'])\n# print((test['maxPlace'] == 1).sum())\n# print((test['maxPlace'] == 0).sum())\n# # print(train[['winPoints','winPlacePerc' , 'maxPlace' , 'rankPoints']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f76acf4ab781f451df3b0cf28def9c117b0844d7"},"cell_type":"code","source":"import time\nimport numpy as np\nimport pandas as pd\nimport gc\nstartTime = time.time()\ndef feature_engineering(is_train = True , debug = True):\n    test_Idx = None\n    if(is_train):\n        print('processing train data')\n        if(debug):\n            df = memory_reduce(pd.read_csv('../input/train_V2.csv' , nrows=10000))\n        else:\n            df = memory_reduce(pd.read_csv('../input/train_V2.csv'))\n            df = df[pd.notnull(df['winPlacePerc'])]\n    else:\n        print('processing test data')\n        if(debug):  \n            df = memory_reduce(pd.read_csv('../input/test_V2.csv' , nrows = 10000))\n        else:\n            df = memory_reduce(pd.read_csv('../input/test_V2.csv'))\n        test_Idx = df.Id\n    print('remove many feature')\n    target = 'winPlacePerc'\n    features = list(df.columns)\n    features.remove('Id')\n    features.remove('groupId')\n    features.remove('matchId')\n    features.remove('matchType')\n    y = None\n    if(is_train):\n        print('get target')\n        y = np.array(df.groupby(['matchId','groupId'])[target].agg('mean') , dtype=np.float64)\n        features.remove(target)\n        \n    print('get group mean featuers')\n    agg = df.groupby(['matchId' , 'groupId'])[features].agg('mean')\n    agg_rank = agg.groupby(['matchId'])[features].rank(pct = True).reset_index()\n    if(is_train):\n        df_out = agg.reset_index()[['matchId','groupId']]\n    else:\n        df_out = df[['matchId' , 'groupId']]\n        \n    df_out = df_out.merge(agg.reset_index() , suffixes = ['',''] , how = 'left' , on = ['matchId' , 'groupId'])\n    df_out = df_out.merge(agg_rank , suffixes = ['_mean','_mean_rank'] , how = 'left' , on = ['matchId' , 'groupId'])\n    \n    \n    \n    print('get group max features')\n    agg = df.groupby(['matchId' , 'groupId'])[features].agg('max')\n    agg_rank = agg.groupby(['matchId'])[features].rank(pct = True).reset_index()\n    df_out = df_out.merge(agg.reset_index() , suffixes = ['',''] , how = 'left' , on = ['matchId' , 'groupId'])\n    df_out = df_out.merge(agg_rank , suffixes = ['_max','_max_rank'] , how = 'left' , on = ['matchId' , 'groupId'])\n    \n    \n    print('get group min features')\n    agg = df.groupby(['matchId' , 'groupId'])[features].agg('min')\n    agg_rank = agg.groupby(['matchId'])[features].rank(pct = True).reset_index()\n    df_out = df_out.merge(agg.reset_index() , suffixes = ['',''] , how = 'left' , on = ['matchId' , 'groupId'])\n    df_out = df_out.merge(agg_rank , suffixes = ['_min','_min_rank'] , how = 'left' , on = ['matchId' , 'groupId'])\n    \n    print('get group size feature')\n    agg = df.groupby(['matchId','groupId']).size().reset_index(name = 'group_size')\n    df_out = df_out.merge(agg , how = 'left' , on = ['matchId','groupId'])\n    \n    \n    \n    print('get group matchId mean')\n    agg = df.groupby(['matchId'])[features].agg('mean').reset_index()\n    df_out = df_out.merge(agg , suffixes = ['','_match_mean'], how = 'left' , on = 'matchId')\n    \n    print('get match size feature')\n    agg = df.groupby(['matchId']).size().reset_index(name = 'matchSize')\n    df_out = df_out.merge(agg , on = 'matchId' , how = 'left')\n    \n    df_out.drop(['matchId' , 'groupId'] , axis = 1 ,inplace = True)\n#     df_out = df_out.merge(df[['matchType']] , left_index = True , right_index = True)\n    X = df_out\n    columnsName = list(df_out.columns)\n    del df , df_out , agg , agg_rank\n    \n    gc.collect()\n    return X , y , columnsName , test_Idx\nx_train , y_train  , train_columns , _ = feature_engineering(True , False)\nx_test , _ , _ , test_Idx = feature_engineering(False , False)\nprint('This time is {:.2f}' .format(time.time() - startTime))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e64c874a2835d4d1548686920913d179bca21c27"},"cell_type":"code","source":"import time\n\n# 移动总距离\nx_train['totalDistance'] = x_train['swimDistance'] + x_train['rideDistance'] + x_train['walkDistance']\nx_test['totalDistance'] = x_test['swimDistance'] + x_test['rideDistance'] + x_test['walkDistance']\n\n# 药品总用量\nx_train['healthItem'] = x_train['heals'] + x_train['boosts']\nx_test['healthItem'] = x_test['heals'] + x_test['boosts']\n\n# 爆头率\nx_train['headshotRate'] =  x_train['kills'] / x_train['headshotKills']\nx_test['headshotRate'] = x_test['kills'] / x_test['headshotKills']\n\n\n# 短时间内杀人数占总杀人数比例\nx_train['killStreakRate'] = x_train['killStreaks']/x_train['kills']\nx_test['killStreakRate'] = x_test['killStreaks']/x_test['kills']\n\n\n# 每分钟杀人数\nx_train['killMinute'] = x_train['kills'] / x_train['matchDuration']\nx_test['killMinute'] = x_test['kills'] / x_test['matchDuration']\n\n# 每分钟造成伤害\nx_train['damageDealtMinute'] = x_train['damageDealt'] / x_train['matchDuration']\nx_test['damageDealtMinute'] = x_test['damageDealt'] / x_test['matchDuration']\n\n# 总共参与的击杀人数\nx_train['participateKills'] = x_train['kills'] + x_train['assists'] + x_train['DBNOs']\nx_test['participateKills'] = x_test['kills'] + x_test['assists'] + x_test['DBNOs']\n\n# 每分钟摧毁车辆数\nx_train['vehicleDestroysMinute'] = x_train['vehicleDestroys'] / x_train['matchDuration']\nx_test['vehicleDestroysMinute'] = x_test['vehicleDestroys'] / x_test['matchDuration']\n\n# 在车上每米杀人数\nx_train['killsMiter'] = x_train['roadKills'] / x_train['rideDistance']\nx_test['killsMiter'] = x_test['roadKills'] / x_test['rideDistance']\n\n\n\ndel x_train['heals']\ndel x_test['heals']\n\ntrain_columns.append('totalDistance')\ntrain_columns.append('healthItems')\ntrain_columns.append('headshotRate')\ntrain_columns.append('killStreakRate')\ntrain_columns.append('killMinute')\ntrain_columns.append('damageDealtMinute')\ntrain_columns.append('participateKills')\ntrain_columns.append('vehicleDestroysMinute')\ntrain_columns.append('killsMiter')\ntrain_columns.remove('heals')\n\nprint(x_train.shape)\nprint('This time is {:.2f}'.format(time.time() - startTime))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2b159c6908523f4e41f7ae808ed50d820b829b94"},"cell_type":"code","source":"import time\nstartTime = time.time()\nx_train = memory_reduce(x_train)\nx_test = memory_reduce(x_test)\nprint(time.time() - startTime)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"59f47fb79e350c7618e7aa7284b0a797f9812977"},"cell_type":"code","source":"# import lightgbm as lgb\n# from sklearn.model_selection import KFold\n# from lightgbm.sklearn import LGBMRegressor\n# import pandas as pd\n# import numpy as np\n# import time\n# import warnings\n# warnings.filterwarnings('ignore')\n# startTime = time.time()\n# folds = KFold(n_splits = 3 , random_state = 6)\n# oof_preds = np.zeros(x_train.shape[0])\n# sub_preds = np.zeros(x_test.shape[0])\n# valid_score = 0\n# feature_importance_df = pd.DataFrame()\n# for nfold , (trn_idx , val_idx) in enumerate(folds.split(x_train , y_train)):\n#     trn_x , trn_y = x_train.iloc[trn_idx] , y_train[trn_idx]\n    \n#     val_x , val_y = x_train.iloc[val_idx] , y_train[val_idx]\n    \n    \n#     train_data = lgb.Dataset(data = trn_x , label = trn_y)\n#     valid_data = lgb.Dataset(data = val_x , label = val_y)\n#     params = {'objective' : 'regression',\n#               'metric' : 'mae',\n#               'n_estimators' : 15000,\n#               'early_stopping_rounds' : 100,\n#               'num_leaves' : 31,\n#               'learning_rate' : 0.05, \n#                \"bagging_fraction\" : 0.9,\n#                \"bagging_seed\" : 0, \n#                \"num_threads\" : 4,\n#                \"colsample_bytree\" : 0.7\n#     }\n#     lgb_model = lgb.train(params , train_data , valid_sets=[train_data , valid_data] , verbose_eval=100) \n#     lgb_pre = lgb_model.predict(val_x , num_iteration=lgb_model.best_iteration)\n#     oof_preds[val_idx] = lgb_pre\n#     oof_preds[oof_preds > 1] = 1\n#     oof_preds[oof_preds < 0] = 0\n#     sub_pred = lgb_model.predict(x_test , num_iteration=lgb_model.best_iteration)\n#     sub_pred[sub_pred > 1] = 1\n#     sub_pred[sub_pred < 0] = 0\n#     sub_preds += sub_pred * 1.0 / folds.n_splits\n#     fold_important_df = pd.DataFrame()\n#     fold_important_df['feature'] = train_columns\n#     fold_important_df['importance'] = lgb_model.feature_importance()\n#     fold_important_df['fold'] = nfold + 1\n#     feature_importance_df = pd.concat([feature_importance_df , fold_important_df] , axis = 0)\n    \n#     gc.collect()\n    \n    \n    \n#     print('The best_iteration {}'.format(lgb_model.best_iteration))\n\n    \n    \n# print(time.time() - startTime)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6ccc0cc9aa4da98d510add41a7d03afeb3e3f0fc"},"cell_type":"code","source":"import lightgbm as lgb\nfrom sklearn.model_selection import KFold\nfrom lightgbm.sklearn import LGBMRegressor\nimport pandas as pd\nimport numpy as np\nimport time\nimport warnings\nwarnings.filterwarnings('ignore')\nstartTime = time.time()\nprint(startTime)\ntrain_index = round(int(x_train.shape[0]*0.8))\ndev_X = x_train[:train_index] \nval_X = x_train[train_index:]\ndev_y = y_train[:train_index] \nval_y = y_train[train_index:] \ngc.collect();\n\n# custom function to run light gbm model\ndef run_lgb(train_X, train_y, val_X, val_y, x_test):\n    params = {\"objective\" : \"regression\", \"metric\" : \"mae\", 'n_estimators':20000, 'early_stopping_rounds':200,\n              \"num_leaves\" : 31, \"learning_rate\" : 0.05, \"bagging_fraction\" : 0.7,\n               \"bagging_seed\" : 0, \"num_threads\" : 4,\"colsample_bytree\" : 0.7\n             }\n    \n    lgtrain = lgb.Dataset(train_X, label=train_y)\n    lgval = lgb.Dataset(val_X, label=val_y)\n    model = lgb.train(params, lgtrain, valid_sets=[lgtrain, lgval], early_stopping_rounds=200, verbose_eval=1000)\n    \n    pred_test_y = model.predict(x_test, num_iteration=model.best_iteration)\n    return pred_test_y, model\n\n# Training the model #\npred_test, model = run_lgb(dev_X, dev_y, val_X, val_y, x_test)\n\nprint(time.time() - startTime)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9ec32243c212b214cc589fca071efc62f9e93ba6"},"cell_type":"code","source":"# import seaborn as sns\n# import matplotlib.pyplot as plt\n# import time\n# startTime = time.time()\n# cols = fold_important_df[['feature','importance']].groupby('feature').agg('mean').sort_values('importance',ascending = False)[:50]\n# fig , ax = plt.subplots(figsize = (15,15))\n# sns.barplot(x = cols.importance , y = cols.index)\n# print(time.time() - startTime)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5f23efb916b793d305b2fbe02fc4676cfa86c04d"},"cell_type":"code","source":"import time\nstartTime = time.time()\ndf_sub = pd.read_csv(\"../input/sample_submission_V2.csv\")\ndf_test = pd.read_csv(\"../input/test_V2.csv\")\ndf_sub['winPlacePerc'] = pred_test\n# Restore some columns\ndf_sub = df_sub.merge(df_test[[\"Id\", \"matchId\", \"groupId\", \"maxPlace\", \"numGroups\"]], on=\"Id\", how=\"left\")\n\n# Sort, rank, and assign adjusted ratio\ndf_sub_group = df_sub.groupby([\"matchId\", \"groupId\"]).first().reset_index()\ndf_sub_group[\"rank\"] = df_sub_group.groupby([\"matchId\"])[\"winPlacePerc\"].rank()\ndf_sub_group = df_sub_group.merge(\n    df_sub_group.groupby(\"matchId\")[\"rank\"].max().to_frame(\"max_rank\").reset_index(), \n    on=\"matchId\", how=\"left\")\ndf_sub_group[\"adjusted_perc\"] = (df_sub_group[\"rank\"] - 1) / (df_sub_group[\"numGroups\"] - 1)\n\ndf_sub = df_sub.merge(df_sub_group[[\"adjusted_perc\", \"matchId\", \"groupId\"]], on=[\"matchId\", \"groupId\"], how=\"left\")\ndf_sub[\"winPlacePerc\"] = df_sub[\"adjusted_perc\"]\n\n# Deal with edge cases\ndf_sub.loc[df_sub.maxPlace == 0, \"winPlacePerc\"] = 0\ndf_sub.loc[df_sub.maxPlace == 1, \"winPlacePerc\"] = 1\n\n# Align with maxPlace\n# Credit: https://www.kaggle.com/anycode/simple-nn-baseline-4\nsubset = df_sub.loc[df_sub.maxPlace > 1]\ngap = 1.0 / (subset.maxPlace.values - 1)\nnew_perc = np.around(subset.winPlacePerc.values / gap) * gap\ndf_sub.loc[df_sub.maxPlace > 1, \"winPlacePerc\"] = new_perc\n\n# Edge case\ndf_sub.loc[(df_sub.maxPlace > 1) & (df_sub.numGroups == 1), \"winPlacePerc\"] = 0\nassert df_sub[\"winPlacePerc\"].isnull().sum() == 0\n\n# print(df_sub[[\"Id\", \"winPlacePerc\"]])\n\ndf_sub[[\"Id\", \"winPlacePerc\"]].to_csv(\"submission.csv\", index=False)\nprint(time.time() - startTime)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5ea2d6f8fdb321a15a194e357407fa15e2e3933b"},"cell_type":"code","source":"# import time\n# startTime = time.time()\n# df_test = pd.read_csv('../input/' + 'test_V2.csv')\n# pred = sub_preds\n# for i in range(len(df_test)):\n#     winPlacePerc = pred[i]\n#     maxPlace = int(df_test.loc[i]['maxPlace'])\n#     if(maxPlace == 0):\n#         winPlacePerc = 0.0\n#     elif(maxPlace == 1):\n#         winPlacePerc = 1.0\n#     else:\n#         gap = 1.0 / (maxPlace - 1)\n#         winPlacePerc = round(winPlacePerc / gap) * gap\n#     if winPlacePerc < 0: winPlacePerc = 0.0\n#     if winPlacePerc > 1: winPlacePerc = 1.0    \n#     pred[i] = winPlacePerc\n\n#     if (i + 1) % 100000 == 0:\n#         print(i, flush=True, end=\" \")\n\n# df_test['winPlacePerc'] = pred\n# submission = df_test[['Id' , 'winPlacePerc']]\n# submission.to_csv('submission.csv' , index = False)\n# print(time.time() - startTime)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1cbe0eb43307021c9b3f91dbcd6ab1a810ded2ad"},"cell_type":"code","source":"# import matplotlib.pyplot as plt\n# import seaborn as sns\n# import sklearn\n# import time\n# from sklearn.preprocessing import LabelEncoder\n# start = time.time()\n# # 处理train中killPlace一个异常值\n# train = train[train.killPlace <= 100]\n# # 将rideDistance、walkDistance 、swimDistance相加求得移动总距离\n# train['totalDistance'] = train['rideDistance'] + train['walkDistance'] + train['swimDistance']\n# test['totalDistance'] = test['rideDistance'] + test['walkDistance'] + test['swimDistance']\n# train['totalDistance']  = train['totalDistance'].apply(lambda dist : dist if dist >= 10 else 0)\n# test['totalDistance']  = test['totalDistance'].apply(lambda dist : dist if dist >= 10 else 0)\n# # 添加其他方式打死敌人数量\n# train['otherKill'] = train['kills'] - (train['headshotKills'] + train['roadKills']) \n# test['otherKill'] = test['kills'] - (test['headshotKills'] + test['roadKills']) \n# # 删除游戏时长低于50秒的场次\n# train = train[train.matchDuration > 50]\n# test = test[test.matchDuration > 50]\n# # 将杀人超过20的玩家改为20\n# train['kills']  = train['kills'].apply(lambda kill : kill if kill <= 20 else 20)\n# test['kills']  = test['kills'].apply(lambda kill : kill if kill <= 20 else 20)\n# # 将杀死队友数量限制在0-5，其实最大只能杀死四个队友\n# train['teamKills']  = train['teamKills'].apply(lambda teamKill : teamKill if teamKill <= 4 else 4)\n# test['teamKills']  = test['teamKills'].apply(lambda teamKill : teamKill if teamKill <= 4 else 4)\n\n# # 控制爆头数量\n# train['headshotKills']  = train['headshotKills'].apply(lambda headshotKill : headshotKill if headshotKill <= 7 else 8)\n# test['headshotKills']  = test['headshotKills'].apply(lambda headshotKill : headshotKill if headshotKill <= 7 else 8)\n\n# # 对boosts数量控制\n# train['boosts']  = train['boosts'].apply(lambda x : x if x <= 10 else 10)\n# test['boosts']  = test['boosts'].apply(lambda x : x if x <= 10 else 10)\n\n# # # 对使用药品数量进行合并 \n# train['heals']  = train['heals'].apply(lambda heal : heal if heal <= 10 else 11)\n# test['heals']  = test['heals'].apply(lambda heal : heal if heal <= 10 else 11)\n\n# # 添加每分钟击杀敌人数量\n# train['killMinute'] = train['kills'] / (train['matchDuration'] / 60)\n# test['killMinute'] = test['kills'] / (test['matchDuration'] / 60)\n# # 添加每分钟造成的伤害量\n# train['damageMinute'] = train['damageDealt'] / (train['matchDuration'] / 60)\n# test['damageMinute'] = test['damageDealt'] / (test['matchDuration'] / 60)\n# # 对助攻数量进行合并\n# train['assists']  = train['assists'].apply(lambda assist : assist if assist <= 14 else 14)\n# test['assists']  = test['assists'].apply(lambda assist : assist if assist <= 14 else 14)\n# # 对damageDealt正态化处理\n# train['damageDealt'] = np.log1p(train['damageDealt'])\n\n\n\n# # 添加是第一视角还是第三视角\n# fpp_tpp = {'squad-fpp' : 'fpp',  \n#                'duo' : 'tpp',\n#                'solo-fpp' : 'fpp',\n#                'squad' : 'tpp',\n#                'duo-fpp' : 'fpp',\n#                'solo' : 'tpp',\n#                'normal-squad-fpp' : 'fpp',\n#                'normal-solo-fpp' : 'fpp',\n#                'normal-duo-fpp' : 'fpp',\n#                'normal-duo' : 'tpp',\n#                'normal-squad' : 'tpp',\n#                'normal-solo' : 'tpp',\n#                'crashfpp' : 'fpp',\n#                'flaretpp' : 'tpp',\n#                'flarefpp' : 'fpp',\n#                'crashtpp' : 'tpp'}\n# train['fpp_tpp'] = train['matchType'].replace(fpp_tpp)\n# test['fpp_tpp'] = test['matchType'].replace(fpp_tpp)\n# le = LabelEncoder().fit(['fpp','tpp'])\n# train['fpp_tpp'] = le.transform(train['fpp_tpp'])\n# test['fpp_tpp'] = le.transform(test['fpp_tpp'])\n# # 添加游戏模式\n# mode = {'squad-fpp' : 'nothing',  \n#                'duo' : 'nothing',\n#                'solo-fpp' : 'nothing',\n#                'squad' : 'nothing',\n#                'duo-fpp' : 'nothing',\n#                'solo' : 'nothing',\n#                'normal-squad-fpp' : 'normal',\n#                'normal-solo-fpp' : 'normal',\n#                'normal-duo-fpp' : 'normal',\n#                'normal-duo' : 'normal',\n#                'normal-squad' : 'normal',\n#                'normal-solo' : 'normal',\n#                'crashfpp' : 'crash',\n#                'flaretpp' : 'flare',\n#                'flarefpp' : 'flare',\n#                'crashtpp' : 'crash'}\n# train['playerMode'] = train['matchType'].replace(mode)\n# test['playerMode'] = test['matchType'].replace(mode)\n# le = LabelEncoder().fit(['nothing' , 'normal' , 'crash' , 'flare'])\n# train['playerMode'] = le.transform(train['playerMode'])\n# test['playerMode'] = le.transform(test['playerMode'])\n# # 填充winPlacePerc的缺失值\n# train['winPlacePerc'] = train['winPlacePerc'].fillna(train['winPlacePerc'].mean())\n# label = train['winPlacePerc']\n\n\n\n# del train['winPlacePerc']\n\n\n# # 删除某些特征\n# del train['Id']\n# del train['groupId']\n# del train['matchId']\n# del train['matchType']\n\n# del test['Id']\n# del test['groupId']\n# del test['matchId']\n# del test['matchType']\n\n# print(time.time() - start)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2521d7184fa59b873a253f8402452bbffbeb8cc0"},"cell_type":"code","source":"# import time\n# startTime = time.time()\n# # 求出一队中击杀敌人最多的数量      \n# maxKills = train[['groupId','kills']].groupby(['groupId']).max()\n# maxKills.rename(columns=lambda x : 'maxKills' , inplace=True)\n# train = pd.merge(left = train , right = maxKills , left_on = 'groupId' , right_index=True)\n# maxKills = test[['groupId','kills']].groupby(['groupId']).max()\n# maxKills.rename(columns=lambda x : 'maxKills' , inplace=True)\n# test = pd.merge(left = test , right = maxKills , left_on = 'groupId' , right_index=True)\n# print(train.columns)\n# print(time.time() - startTime)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3bf628a15fda7fcedf55860badc52236134c20e8"},"cell_type":"code","source":"# import time\n# startTime = time.time()\n# # 添加每队总计杀敌人数\n# totalKills = train[['groupId','kills']].groupby(['groupId']).sum()\n# totalKills.rename(columns=lambda x : 'totalKills' , inplace=True)\n# train = pd.merge(left = train , right = totalKills , left_on = 'groupId' , right_index=True)\n# totalKills = test[['groupId','kills']].groupby(['groupId']).sum()\n# totalKills.rename(columns=lambda x : 'totalKills' , inplace=True)\n# test = pd.merge(left = test , right = totalKills , left_on = 'groupId' , right_index=True)\n# print(train.columns)\n# print(time.time() - startTime)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"11365cc8ad5941530e2daa18ee1a737a83264342"},"cell_type":"code","source":"# import time\n# startTime = time.time()\n# # 添加组内有几个人列\n# size = train.groupby('groupId').size()\n# size = size.to_frame()\n# size.rename(columns = lambda x : 'groupSize' , inplace = True)\n# train = pd.merge(left = train , right = size , left_on='groupId' , right_index=True)\n# size = test.groupby('groupId').size()\n# size = size.to_frame()\n# size.rename(columns = lambda x : 'groupSize' , inplace = True)\n# test = pd.merge(left = test , right = size , left_on='groupId' , right_index=True)\n# print(train.columns)\n# print(time.time() - startTime)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"21365fae705c1ca17f281208541a0241baeb70ee"},"cell_type":"code","source":"# import time\n# startTime = time.time()\n# # 添加每队平均杀敌人数\n# # print(train[['groupSize_x','groupSize_y']])\n# train['averageKills'] = round(train['totalKills'] / train['groupSize_x'])\n# test['averageKills'] = round(test['totalKills'] / train['groupSize_x'])\n# print(time.time() - startTime)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"793bff6bde2f8f069d87b87bfdf90f82d42c6ecd"},"cell_type":"code","source":"# # 将杀死队友数量限制在0-5，其实最大只能杀死四个队友\n# train['weaponsAcquired']  = train['weaponsAcquired'].apply(lambda x : x if x <= 40 else 41)\n# test['weaponsAcquired']  = test['weaponsAcquired'].apply(lambda x : x if x <= 40 else 41)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"218f872a05a9ee821a12c5e10cf67798a70aca2f"},"cell_type":"code","source":"# # 将杀死队友数量限制在0-5，其实最大只能杀死四个队友\n# train['boosts']  = train['boosts'].apply(lambda x : x if x <= 10 else 10)\n# test['boosts']  = test['boosts'].apply(lambda x : x if x <= 10 else 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9cc5233ac7ae8dba05e301c5158d09bbaf64b7fd"},"cell_type":"code","source":"# import numpy as np\n# import pandas as pd\n# import seaborn as sns\n# import matplotlib.pyplot as plt\n\n# train_solo = train[train['playerGroupby'] == 'solo']\n# train_duo = train[train['playerGroupby'] == 'duo']\n# train_squad = train[train['playerGroupby'] == 'squad']\n# train_random = train[train['playerGroupby'] == 'random']\n# print((train_random['assists'] == 4).sum())\n# print(train[(train['matchType'] == 'crashfpp')].groupby('numGroups').size())\n# f , ax = plt.subplots(figsize = (15,15))\n# sns.countplot(train['matchType'])\n# plt.xticks(rotation = 90)\n\n# f , ax = plt.subplots(figsize = (15,15))\n# plt.pie(train.groupby(['matchType']).size() , labels=train.groupby(['matchType']).size().index)\n\n\n# print((train['matchType'] == 'flashfpp').sum())\n# train['k'] = train['DBNOs'] + train['kills']\n# print(len(train) - len(train['Id'].unique()))\n# print(train[['numGroups']])\n# print(train[train['matchId'] == 'a10357fd1a4a91'].sort_values(['killPlace'])[['kills','killPlace','killPoints','winPlacePerc']])\n# print(train[train['matchId'] == 'a10357fd1a4a91'].sort_values(['maxPlace']))\n\n# print(train.sort_values('') [['maxPlace','numGroups']])\n\n# corr = train.corr()\n# corr = round(corr , 2)\n# f, ax = plt.subplots(figsize=(15, 15))\n# sns.heatmap(corr , annot=True , annot_kws={'size':9 , 'color':'black'})\n# print(np.log1p(train['damageDealt']).skew())\n# f, ax = plt.subplots(figsize=(12, 10))\n# sns.distplot(train['matchType'])\n# sns.distplot(train['totalDistance'])\n# sns.distplot(np.log1p(train['totalDistance']))\n\n# print(train['matchType'].unique())\n# print((train['boosts'] == 20).sum())\n# print(len(train[train['killPoints'] < 100]))\n# print(min(train['killPoints'].unique()))\n# print((train['boosts'] > 19).sum())\n# print(train.groupby('boosts').size())\n\n\n# trainRoad = train[train['matchType'] == 'normal-squad-fpp']\n# trainCrashfpp = train[train['matchType'] == 'normal-squad-fpp']\n# trainNotCrash = train[(train['matchType'] == 'squad-fpp') | (train['matchType'] == 'squad') | (train['matchType'] == 'duo-fpp') | (train['matchType'] == 'duo') | (train['matchType'] == 'solo-fpp') | (train['matchType'] == 'solo')]\n# trainCrashtpp = train[(train['matchType'] == 'normal-squad-fpp') | (train['matchType'] == 'normal-squad') | (train['matchType'] == 'normal-duo-fpp') | (train['matchType'] == 'normal-duo') | (train['matchType'] == 'normal-solo-fpp') | (train['matchType'] == 'normal-solo')]\n# print(len(trainCrash))\n# f, ax = plt.subplots(figsize=(14, 14) , ncols=2 ,nrows=2)\n# sns.boxplot(x=trainNotCrash['assists'] , y=trainNotCrash[\"winPlacePerc\"] , ax=ax[0,0])\n# sns.boxplot(x=trainCrashtpp['assists'] , y=trainCrashtpp[\"winPlacePerc\"] , ax=ax[0,1])\n# sns.boxplot(x=trainCrashfpp['revives'] , y=trainCrashfpp[\"winPlacePerc\"] , ax=ax[1,0])\n# sns.boxplot(x=train['assists'] , y=train[\"winPlacePerc\"] , ax=ax[1,1])\n\n# sns.scatterplot(x=trainNotCrash['matchDuration'] , y=trainNotCrash[\"winPlacePerc\"] , ax=ax[0,0])\n# sns.scatterplot(x=trainCrashtpp['matchDuration'] , y=trainCrashtpp[\"winPlacePerc\"] , ax=ax[0,1])\n# sns.scatterplot(x=trainCrashfpp['matchDuration'] , y=trainCrashfpp[\"winPlacePerc\"] , ax=ax[1,0])\n# sns.scatterplot(x=train['matchDuration'] , y=train[\"winPlacePerc\"] , ax=ax[1,1])\n\n# print((trainCrash['headshotKills'] == 2).sum())\n\n# fig.axis(ymin=0, ymax=1)\n# print(train['kills'].unique())\n# print(len(train[train['assists']>=15]))\n# print(train.groupby('assists').size())\n# f , ax = plt.subplots(figsize = (20,20))\n# f , ax = plt.subplots(ncols=2 , nrows=2, figsize = (20,20))\n# sns.heatmap(train_solo.corr() , ax = ax[0,0] , cbar = False)\n# sns.heatmap(train_duo.corr() , ax = ax[0,1] , cbar = False)\n# sns.heatmap(train_squad.corr() , ax = ax[1,0] , cbar = False)\n# sns.heatmap(train_random.corr() , ax = ax[1,1] , cbar = False)\n# sns.boxplot(x = train_solo['assists'] , y = train_solo['winPlacePerc'] , ax=ax[0,0])\n# sns.boxplot(x = train_duo['assists'] , y = train_duo['winPlacePerc'] , ax=ax[0,1])\n# sns.boxplot(x = train_squad['assists'] , y = train_squad['winPlacePerc'] , ax=ax[1,0])\n# print(train['Id'].head(10))\n# train['firstName'] = train['groupId'].apply(lambda x : x[-1])\n# sns.boxplot(x = train['playerGroupby'], y = train['winPlacePerc'])\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"210673ace0cbd0a545b21a69c177e29ca26f295e"},"cell_type":"code","source":"# 利用xgboost\n# import numpy as np\n# import pandas as pd\n# from xgboost.sklearn import XGBRegressor\n# import xgboost as xgb\n# import time\n# from sklearn.model_selection import train_test_split\n# from sklearn.model_selection import GridSearchCV\n# startTime = time.time()\n# train_X , test_X , train_Y , test_Y = train_test_split(train , label , test_size = 0.3)\n# xgbRegressor = XGBRegressor(n_estimators = 500,\n#                            max_depth = 6,\n#                            learning_rate = 0.1,\n#                            subsample = 0.5,\n#                            colsample_bytree = 0.8,\n#                            min_child_weight = 1)\n# res = xgb.cv(params = xgbRegressor.get_xgb_params() , \n#             dtrain = xgb.DMatrix(train_X , train_Y),\n#              num_boost_round=xgbRegressor.get_xgb_params()['n_estimators'],\n#             early_stopping_rounds = 50)\n\n# xgbRegressor = XGBRegressor(n_estimators = 600,\n#                            max_depth = 6,\n#                            learning_rate = 0.1,\n#                            subsample = 0.8,\n#                            colsample_bytree = 0.8,\n#                            min_child_weight = 1)\n# xgbRegressor.fit(train_X , train_Y)\n# prediction = xgbRegressor.predict(test)\n\n# sample = pd.read_csv('../input/sample_submission_V2.csv')\n# sample['winPlacePerc'] = prediction\n# sample.to_csv('sample_submission.csv',index = False)\n# params = {\"n_estimators\" : [100,200]}\n# grid = GridSearchCV(xgbRegressor , param_grid = params , cv = 3)\n# grid = grid.fit(train_X , train_Y)\n# print(grid.best_params_)\n# print(res.shape[0])\n# print(time.time() - startTime)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a01c6cb065394e82365094b1cf4e9d39fef67178"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f2ecba673b5f28dd974005f34eff4a6b4fbd568c"},"cell_type":"code","source":"\n# import pandas as pd\n# import numpy as np\n# import seaborn as sns\n# import matplotlib.pyplot as plt\n# import warnings\n# import sklearn\n# from sklearn.model_selection import train_test_split\n# from sklearn.model_selection import GridSearchCV\n# from sklearn.linear_model import RidgeCV\n# from sklearn.ensemble import RandomForestRegressor\n# import time\n# warnings.filterwarnings(\"ignore\")\n# train_x , test_x, train_y , test_y = train_test_split(train , label , test_size = 0.3)\n\n# # randomTree = RandomForestRegressor(max_features = 'auto'  , min_samples_leaf = 1  , min_samples_split = 4,\n# #                                   n_estimators = 5)\n# randomTree = RandomForestRegressor(max_features = 0.8 , n_estimators = 100)\n# params = {'min_samples_leaf' : [1,3],\n#          'min_samples_split' : [2,4]}\n# grid = GridSearchCV(estimator=randomTree , param_grid=params)\n# grid = grid.fit(train_x , train_y)\n# print(grid.best_params_)\n# randomTree = RandomForestRegressor(max_features = 0.8  \n#                                    , min_samples_leaf = grid.best_params_['min_samples_leaf'] \n#                                    , min_samples_split = grid.best_params_['min_samples_split']\n#                                    , n_estimators = grid.best_params_['n_estimators'])\n\n\n# randomTree.fit(train , label)\n\n# prediction = randomTree.predict(test)\n\n# sample = pd.read_csv('../input/sample_submission_V2.csv')\n# sample['winPlacePerc'] = prediction\n# sample.to_csv('sample_submission.csv',index = False)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# import matplotlib.pyplot as plt\n# import seaborn as sns\n# import sklearn\n# # 处理train中killPlace一个异常值\n# train = train[train.killPlace <= 100]\n# # 将rideDistance、walkDistance 、swimDistance相加求得移动总距离\n# train['totalDistance'] = train['rideDistance'] + train['walkDistance'] + train['swimDistance']\n# test['totalDistance'] = test['rideDistance'] + test['walkDistance'] + test['swimDistance']\n# # 添加其他方式打死敌人数量\n# train['otherKill'] = train['kills'] - (train['headshotKills'] + train['roadKills']) \n# test['otherKill'] = test['kills'] - (test['headshotKills'] + test['roadKills']) \n# # 删除游戏时长低于50秒的场次\n# train = train[train.matchDuration > 50]\n# test = test[test.matchDuration > 50]\n# # 添加每分钟击杀敌人数量\n# train['killMinute'] = train['kills'] / (train['matchDuration'] / 60)\n# test['killMinute'] = test['kills'] / (test['matchDuration'] / 60)\n# # 添加每分钟造成的伤害量\n# train['damageMinute'] = train['damageDealt'] / (train['matchDuration'] / 60)\n# test['damageMinute'] = test['damageDealt'] / (test['matchDuration'] / 60)\n# # 添加组内有几个人列\n# size = train.groupby('groupId').size()\n# size = size.to_frame()\n# size.rename(columns = lambda x : 'groupSize' , inplace = True)\n# train = pd.merge(left = train , right = size , left_on='groupId' , right_index=True)\n# size = test.groupby('groupId').size()\n# size = size.to_frame()\n# size.rename(columns = lambda x : 'groupSize' , inplace = True)\n# test = pd.merge(left = test , right = size , left_on='groupId' , right_index=True)\n# # 填充winPlacePerc的缺失值\n# train['winPlacePerc'] = train['winPlacePerc'].fillna(train['winPlacePerc'].mean())\n\n\n\n# # 提取Id第一个字母\n# # train['IdFirst'] = train['Id'].apply(lambda x : x[0])\n# # test['IdFirst'] = test['Id'].apply(lambda x : x[0])\n\n\n\n# print(len(train.columns))\n# print(len(test.columns))\n# print('stop')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"efc4928941707055044f5382681f3f4073aef829"},"cell_type":"code","source":"        # 对同一队的数据进行分析\n\n# 求出一队中击杀敌人最多的数量      \n# maxKills = train[['groupId','kills']].groupby(['groupId']).max()\n# maxKills.rename(columns=lambda x : 'maxKills' , inplace=True)\n# train = pd.merge(left = train , right = maxKills , left_on = 'groupId' , right_index=True)\n# maxKills = test[['groupId','kills']].groupby(['groupId']).max()\n# maxKills.rename(columns=lambda x : 'maxKills' , inplace=True)\n# test = pd.merge(left = test , right = maxKills , left_on = 'groupId' , right_index=True)\n# print(train.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f5c6c88a07d05e50b3fb6ae3fc68d1b4abce2128"},"cell_type":"code","source":"# 添加每队总计杀敌人数\n# totalKills = train[['groupId','kills']].groupby(['groupId']).sum()\n# totalKills.rename(columns=lambda x : 'totalKills' , inplace=True)\n# train = pd.merge(left = train , right = totalKills , left_on = 'groupId' , right_index=True)\n# totalKills = test[['groupId','kills']].groupby(['groupId']).sum()\n# totalKills.rename(columns=lambda x : 'totalKills' , inplace=True)\n# test = pd.merge(left = test , right = totalKills , left_on = 'groupId' , right_index=True)\n# print(train.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2131a0a9efcb4e9a369ca3ebb048be4c5967551f"},"cell_type":"code","source":"# 添加每队平均杀敌人数\n# train['averageKills'] = train['totalKills'] / train['groupSize']\n# test['averageKills'] = test['totalKills'] / train['groupSize']\n# print(train.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0381e0b5572077a06c85b8bd19606f28bbf6b4e3"},"cell_type":"code","source":"# 一队中造成伤害最多\n# maxDamageDealt = train[['groupId','damageDealt']].groupby(['groupId']).max()\n# maxDamageDealt.rename(columns=lambda x : 'maxDamageDealt' , inplace=True)\n# train = pd.merge(left = train , right = maxDamageDealt , left_on = 'groupId' , right_index=True)\n# maxDamageDealt = test[['groupId','damageDealt']].groupby(['groupId']).max()\n# maxDamageDealt.rename(columns=lambda x : 'maxKills' , inplace=True)\n# test = pd.merge(left = test , right = maxDamageDealt , left_on = 'groupId' , right_index=True)\n# print(train.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6e5fa43be261505b6bb216866be8752e2de1c66e"},"cell_type":"code","source":"# 添加每队总伤害\n# totalDamageDealt = train[['groupId','damageDealt']].groupby(['groupId']).sum()\n# totalDamageDealt.rename(columns=lambda x : 'totalDamageDealt' , inplace=True)\n# train = pd.merge(left = train , right = totalDamageDealt , left_on = 'groupId' , right_index=True)\n# totalDamageDealt = test[['groupId','damageDealt']].groupby(['groupId']).sum()\n# totalDamageDealt.rename(columns=lambda x : 'totalDamageDealt' , inplace=True)\n# test = pd.merge(left = test , right = totalDamageDealt , left_on = 'groupId' , right_index=True)\n# print(train.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"844fede9a3b9fefd4112280ee2bf852437fd6555"},"cell_type":"code","source":"# 添加每队平均伤害\n# train['averageDamageDealt'] = train['totalDamageDealt'] / train['groupSize']\n# test['averageDamageDealt'] = test['totalDamageDealt'] / train['groupSize']\n\n# print(train.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"de713719d61f7769efdaafc0e525caa9149eae2f"},"cell_type":"code","source":"# 添加全队一共行进距离\n# groupDistance = train[['groupId','totalDistance']].groupby(['groupId']).sum()\n# groupDistance.rename(columns=lambda x : 'groupDistance' , inplace=True)\n# train = pd.merge(left = train , right = groupDistance , left_on = 'groupId' , right_index=True)\n# groupDistance = test[['groupId','totalDistance']].groupby(['groupId']).sum()\n# groupDistance.rename(columns=lambda x : 'groupDistance' , inplace=True)\n# test = pd.merge(left = test , right = groupDistance , left_on = 'groupId' , right_index=True)\n# print(train.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f354c794846178d5c1ae9c6fa2ad3467cf52c62c"},"cell_type":"code","source":"# 添加全队行进的最远距离\n# maxDistance = train[['groupId','totalDistance']].groupby(['groupId']).max()\n# maxDistance.rename(columns=lambda x : 'maxDistance' , inplace=True)\n# train = pd.merge(left = train , right = maxDistance , left_on = 'groupId' , right_index=True)\n# maxDistance = test[['groupId','totalDistance']].groupby(['groupId']).max()\n# maxDistance.rename(columns=lambda x : 'maxDistance' , inplace=True)\n# test = pd.merge(left = test , right = maxDistance , left_on = 'groupId' , right_index=True)\n# print(train.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0aec2ad11dd6914e4d7f4c74b4ff38aa1d91d1c8"},"cell_type":"code","source":"# 添加全队行进的最近距离\n# minDistance = train[['groupId','totalDistance']].groupby(['groupId']).min()\n# minDistance.rename(columns=lambda x : 'minDistance' , inplace=True)\n# train = pd.merge(left = train , right = minDistance , left_on = 'groupId' , right_index=True)\n# minDistance = test[['groupId','totalDistance']].groupby(['groupId']).min()\n# minDistance.rename(columns=lambda x : 'minDistance' , inplace=True)\n# test = pd.merge(left = test , right = minDistance , left_on = 'groupId' , right_index=True)\n# print(train.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"825b620f651b32ef16d1c0ce2288e09a08abce14"},"cell_type":"code","source":"# 添加全队平均行进距离\n# train['meanDistance'] = train['totalDistance'] / train['groupSize']\n# test['meanDistance'] = test['totalDistance'] / train['groupSize']\n\n# print(train.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b3c2f7e6ce86e10aafb79455eb0d96860f098986"},"cell_type":"code","source":"# 助攻总数\n# sumAssists = train[['groupId','assists']].groupby(['groupId']).sum()\n# sumAssists.rename(columns=lambda x : 'sumAssists' , inplace=True)\n# train = pd.merge(left = train , right = sumAssists , left_on = 'groupId' , right_index=True)\n# sumAssists = test[['groupId','assists']].groupby(['groupId']).sum()\n# sumAssists.rename(columns=lambda x : 'sumAssists' , inplace=True)\n# test = pd.merge(left = test , right = sumAssists , left_on = 'groupId' , right_index=True)\n# print(train.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"318b1d162b7b27349b4a3091d8fa0b5d0d412ea7"},"cell_type":"code","source":"# 助攻最大值\n# maxAssists = train[['groupId','assists']].groupby(['groupId']).max()\n# maxAssists.rename(columns=lambda x : 'maxAssists' , inplace=True)\n# train = pd.merge(left = train , right = maxAssists , left_on = 'groupId' , right_index=True)\n# maxAssists = test[['groupId','assists']].groupby(['groupId']).max()\n# maxAssists.rename(columns=lambda x : 'maxAssists' , inplace=True)\n# test = pd.merge(left = test , right = maxAssists , left_on = 'groupId' , right_index=True)\n# print(train.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8514418c440418ea025e95de74cef935d353f05a"},"cell_type":"code","source":"# 助攻平均数\n# train['meanAssists'] = train['sumAssists'] / train['groupSize']\n# test['meanAssists'] = test['sumAssists'] / train['groupSize']\n# print(train.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7f7e4a24cae6b4ef46274869ebe9d0d781cbf26e"},"cell_type":"code","source":"# 击倒总数\n# sumDBNOs = train[['groupId','DBNOs']].groupby(['groupId']).sum()\n# sumDBNOs.rename(columns=lambda x : 'sumDBNOs' , inplace=True)\n# train = pd.merge(left = train , right = sumDBNOs , left_on = 'groupId' , right_index=True)\n# sumDBNOs = test[['groupId','DBNOs']].groupby(['groupId']).sum()\n# sumDBNOs.rename(columns=lambda x : 'sumDBNOs' , inplace=True)\n# test = pd.merge(left = test , right = sumDBNOs , left_on = 'groupId' , right_index=True)\n# print(train.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"35d77015ddd1520e9d0046869ea489d3ac15d246"},"cell_type":"code","source":"# 击倒平均数\n# train['meanDBNOs'] = train['sumDBNOs'] / train['groupSize']\n# test['meanDBNOs'] = test['sumDBNOs'] / train['groupSize']\n# print(train.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"89b49adcb7ad4ab3845501ff1736c5021f4d9395"},"cell_type":"code","source":"# 最大击倒数\n# maxDBNOs = train[['groupId','DBNOs']].groupby(['groupId']).max()\n# maxDBNOs.rename(columns=lambda x : 'maxDBNOs' , inplace=True)\n# train = pd.merge(left = train , right = maxDBNOs , left_on = 'groupId' , right_index=True)\n# maxDBNOs = test[['groupId','DBNOs']].groupby(['groupId']).max()\n# maxDBNOs.rename(columns=lambda x : 'maxDBNOs' , inplace=True)\n# test = pd.merge(left = test , right = maxDBNOs , left_on = 'groupId' , right_index=True)\n# print(train.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"64d625b0eba0923637d4cd97316006d58237a1ea"},"cell_type":"code","source":"# 删除某些特征\n# del train['Id']\n# del train['matchId']\n# del train['matchType']\n# del train['DBNOs']\n# del train['assists']\n# del train['damageDealt']\n# del train['heals']\n# del train['kills']\n# del train['totalDistance']\n\n\n# del test['Id']\n# del test['matchId']\n# del test['matchType']\n# del test['DBNOs']\n# del test['assists']\n# del test['damageDealt']\n# del test['heals']\n# del test['kills']\n# del test['totalDistance']\n# print(len(train.columns))\n# print(len(test.columns))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4f5e4e30cf20e4b737dad5fa1fa8cbce4d71e323"},"cell_type":"code","source":"# train.drop_duplicates(subset=['groupId'] , inplace = True)\n# testFlag = test.drop_duplicates(subset=['groupId'])\n# label = train['winPlacePerc']\n# dropGroupId = testFlag['groupId'].to_frame()\n# del train['winPlacePerc']\n# del train['groupId']\n# del testFlag['groupId']\n# print(len(train.columns))\n# print(len(testFlag.columns))\n# print('stop')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0cc244039591299e89d5fb0a30ce64d01d5cc790"},"cell_type":"code","source":"# import pandas as pd\n# import numpy as np\n# import seaborn as sns\n# import matplotlib.pyplot as plt\n# import warnings\n# import sklearn\n# from sklearn.model_selection import train_test_split\n# from sklearn.model_selection import GridSearchCV\n# from sklearn.linear_model import RidgeCV\n# from sklearn.ensemble import RandomForestRegressor\n# import time\n# warnings.filterwarnings(\"ignore\")\n# # train_x , test_x, train_y , test_y = train_test_split(train , label , test_size = 0.3)\n# # 岭回归，效果奇差\n# # ridge = RidgeCV(alphas=[0.01,0.03,0.09,0.3,1,3,10,30,60])\n# # ridge.fit(train_x , train_y)\n# # alpha = ridge.alpha_\n\n# # ridge = RidgeCV(alphas = [alpha * 0.6 , alpha * 0.65 ,alpha * 0.7 ,alpha * 0.75 ,alpha * 0.8 ,\n# #                                               alpha * 0.85 ,alpha * 0.9 ,alpha * 0.95 ,alpha * 1 ,\n# #                                               alpha * 1.05 ,alpha * 1.1 ,alpha * 1.15 ,alpha * 1.2 ,alpha * 1.25 ])\n# # # ridge = RidgeCV(alphas = 0.375)\n# # ridge.fit(train_x , train_y)\n# # alpha = ridge.alpha_\n# # prediction = ridge.predict(test)\n# # ti = time.time()\n# # print(ti)\n\n# randomTree = RandomForestRegressor(max_features = 0.7  , min_samples_leaf = 1  , min_samples_split = 4,\n#                                   n_estimators = 20)\n# # params = {'min_samples_leaf' : [1,3],\n# #          'min_samples_split' : [2,4],\n# #          'n_estimators' : [200,400]}\n# # grid = GridSearchCV(estimator=randomTree , param_grid=params)\n# # print(1)\n# # grid = grid.fit(train , label)\n# # print(2)\n# # print(grid.best_score_)\n# # print(grid.best_params_)\n# randomTree.fit(train , label)\n\n# prediction = randomTree.predict(testFlag)\n# dropGroupId['winPlacePerc'] = prediction\n# prediction = pd.merge(test , dropGroupId , left_on = 'groupId' , right_on = 'groupId')['winPlacePerc']\n# sample = pd.read_csv('../input/sample_submission_V2.csv')\n# sample['winPlacePerc'] = prediction\n# sample.to_csv('sample_submission.csv',index = False)\n# print('stop')\n# # print(sample)\n# print(np.sqrt(sklearn.metrics.mean_absolute_error(ridge.predict(test_x) , test_y)))\n\n\n\n# train['normal'] = train['matchType'].apply(lambda x : 0 if (x.find('flare') >=0 or x.find('crash') >=0) else 1)\n# train['normal'] = train['matchType'].apply(lambda x : 0 if (x.find('solo') >=0) else 1)\n# print(train.groupby(['normal']).mean()['winPlacePerc'])\n\n\n# trainNO = train[train['normal'] == 0]\n# print(trainNO['winPlacePerc'].mean())\n# print(train['winPlacePerc'].mean())\n# print(trainNO[['matchType','revives','teamKills','assists']])\n\n\n# print(train[['matchType','killPlace']])\n# print(time.time() - ti)\n# print('stop')\n# plt.scatter(train.totalDistance , train.winPlacePerc)\n# plt.show()\n# train['totalDistance'] = train['rideDistance'] + train['walkDistance'] + train['swimDistance']\n\n# print(train['heals'].sort_values(ascending = False))\n# f , ax = plt.subplots(figsize = (12,12))\n# sns.countplot(x = 'kills' , data = test)\n# plt.show()\n# print(train['matchDuration'])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}