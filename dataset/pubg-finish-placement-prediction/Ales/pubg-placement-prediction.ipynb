{"cells":[{"metadata":{"_uuid":"0273b22caf8c995094be6bf8b557bb3283c0a591"},"cell_type":"markdown","source":"**File descriptions**\n* train_V2.csv - the training set\n* test_V2.csv - the test set\n* sample_submission_V2.csv - a sample submission file in the correct format\n\n**Y:**\n* winPlacePerc = (maxPlace-winPlace)/(maxPlace-1)\n* winPlacePerc - The target of prediction. This is a percentile winning placement, where 1 corresponds to 1st place, and 0 corresponds to last place in the match. It is calculated off of maxPlace, not numGroups, so it is possible to have missing chunks in a match.\n\n**X:**\n* DBNOs - Number of enemy players knocked.\n* assists - Number of enemy players this player damaged that were killed by teammates.\n* boosts - Number of boost items used.\n* damageDealt - Total damage dealt. Note: Self inflicted damage is subtracted.\n* headshotKills - Number of enemy players killed with headshots.\n* heals - Number of healing items used.\n* Id - Player’s Id\n* killPlace - Ranking in match of number of enemy players killed.\n* killPoints - Kills-based external ranking of player. (Think of this as an Elo ranking where only kills matter.) If there is a value other than -1 in rankPoints, then any 0 in killPoints should be treated as a “None”.\n* killStreaks - Max number of enemy players killed in a short amount of time.\n* kills - Number of enemy players killed.\n* longestKill - Longest distance between player and player killed at time of death. This may be misleading, as downing a player and driving away may lead to a large longestKill stat.\n* matchDuration - Duration of match in seconds.\n* matchId - ID to identify match. There are no matches that are in both the training and testing set.\n* matchType - String identifying the game mode that the data comes from. The standard modes are “solo”, “duo”, “squad”, “solo-fpp”, “duo-fpp”, and “squad-fpp”; other modes are from events or custom matches.\n* rankPoints - Elo-like ranking of player. This ranking is inconsistent and is being deprecated in the API’s next version, so use with caution. Value of -1 takes place of “None”.\n* revives - Number of times this player revived teammates.\n* rideDistance - Total distance traveled in vehicles measured in meters.\n* roadKills - Number of kills while in a vehicle.\n* swimDistance - Total distance traveled by swimming measured in meters.\n* teamKills - Number of times this player killed a teammate.\n* vehicleDestroys - Number of vehicles destroyed.\n* walkDistance - Total distance traveled on foot measured in meters.\n* weaponsAcquired - Number of weapons picked up.\n* winPoints - Win-based external ranking of player. (Think of this as an Elo ranking where only winning matters.) If there is a value other than -1 in rankPoints, then any 0 in winPoints should be treated as a “None”.\n* groupId - ID to identify a group within a match. If the same group of players plays in different matches, they will have a different groupId each time.\n* numGroups - Number of groups we have data for in the match.\n* maxPlace - Worst placement we have data for in the match. This may not match with numGroups, as sometimes the data skips over placements.\n\n\n**Thoughts:**\n* group features by group(match and team)\n* group features by match\n* sum total player distance (walk, swim, ride)\n* sum points\n* create sevral models for each game mode\n\n* add headshot_rate train['headshotKills'] / train['kills']\n\nnormilize features by:\ntrain['playersJoined'] = train.groupby('matchId')['matchId'].transform('count')\ntrain['killsNorm'] = train['kills']*((100-train['playersJoined'])/100 + 1)\n\n* train['healsandboosts'] = train['heals'] + train['boosts']"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport sys\nimport numpy as np\nnp.random.seed(1)\n\nimport pandas as pd\npd.set_option(\"display.max_rows\", 300)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport keras\nfrom keras.models import Model\nfrom keras.layers import Dense, Input, LeakyReLU, BatchNormalization\nfrom keras.optimizers import Adam\nfrom keras.losses import MAE\nfrom keras.callbacks import Callback, LearningRateScheduler, EarlyStopping\nfrom keras import backend as K\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nimport gc\ngc.enable()\n\ny_label = 'winPlacePerc'\nx_categorical = ['matchType']\nstandard_modes = ['solo', 'duo', 'squad', 'solo-fpp', 'duo-fpp', 'squad-fpp']\nx_numeric_columns = [\n    'DBNOs', 'assists', 'boosts', 'damageDealt', 'headshotKills', 'heals', 'killPlace', 'killPoints',\n    'killStreaks', 'kills', 'longestKill', 'matchDuration', 'rankPoints', 'revives', 'rideDistance',\n    'roadKills', 'swimDistance', 'teamKills', 'vehicleDestroys', 'walkDistance', 'weaponsAcquired',\n    'winPoints', 'numGroups', 'maxPlace'\n    \n]\nidx_columns = ['Id', 'matchId', 'groupId']\n\nepochs = 50\nbatch_size = 2 ** 13\ntrain_nrows = None\nvalidation_split=0.0\n\nprint(\"batch size: {}\".format(batch_size))\nprint(\"Input dir ../input: {}\".format(os.listdir(\"../input\")))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"def optimize_memory(df):\n    before_mem_mb = df.memory_usage().sum() / 1048576\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        if  col_type != object and str(col_type) != 'category':\n            c_min = df[col].min()\n            c_max = df[col].max()\n            \n            if str(col_type)[:3] == 'int' and col not in x_numeric_columns:\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            elif c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                df[col] = df[col].astype(np.float16)\n            elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                df[col] = df[col].astype(np.float32)\n            else:\n                df[col] = df[col].astype(np.float64)       \n        elif str(col_type) != 'category':\n            df[col] = df[col].astype('category')\n    \n    after_mem_mb = df.memory_usage().sum() / 1048576\n    print(\"Memory size before: {:.2f} mb, after: {:.2f} mb\".format(before_mem_mb, after_mem_mb))\n    gc.collect()\n    \n    return df\n\ndef load_csv(path, nrows=None, dtype=None):\n    df = pd.read_csv(path, dtype=dtype, nrows=nrows)\n    df = optimize_memory(df)\n    return df\n\nstr_to_int_dict = {}\nint_to_str_dict = {}\n\ndef str_to_int(string):\n    if string in str_to_int_dict:\n        return str_to_int_dict[string]\n    else:\n        index = len(str_to_int_dict) + 1\n        str_to_int_dict[string] = index\n        int_to_str_dict[index]  = string\n        return index\n\n\ndef optimize_data(data):\n    for col in idx_columns:\n        data[col] = data[col].map(lambda x: str_to_int(x)).astype(np.int64, inplace=True)\n\n    data['matchType'].cat.add_categories(['custom'], inplace=True)\n    data.loc[~data['matchType'].isin(standard_modes), ['matchType']] = 'custom'\n    data['matchType'].cat.remove_unused_categories(inplace=True)\n\n\nsource_dtypes = { \n    'Id': 'category', 'groupId': 'category', 'matchId': 'category', 'assists': 'float16',\n    'boosts': 'float16', 'damageDealt': 'float16', 'DBNOs': 'float16', 'headshotKills': 'float16',\n    'heals': 'float16', 'killPlace': 'float16', 'killPoints': 'float16', 'kills': 'float16',\n    'killStreaks': 'float16', 'longestKill': 'float16', 'matchDuration': 'float16', 'matchType': 'category',\n    'maxPlace': 'float16', 'numGroups': 'float16', 'rankPoints': 'float16', 'revives': 'float16',\n    'rideDistance': 'float16', 'roadKills': 'float16', 'swimDistance': 'float16', 'teamKills': 'float16',\n    'vehicleDestroys': 'float16', 'walkDistance': 'float16', 'weaponsAcquired': 'float16', 'winPoints': 'float16',\n    'winPlacePerc': 'float16' \n}\nsource_data    = load_csv('../input/train_V2.csv', nrows=train_nrows) \nif train_nrows is None or train_nrows > 2744603:\n    source_data.drop(2744604, inplace=True)\n\noptimize_data(source_data)\nsource_y = source_data[y_label]\nprint(\"Data sizes: train {}\".format(len(source_data)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9d432d7fd2dc2b609f523169b694dfb94c3506fd"},"cell_type":"code","source":"print(sys.getsizeof(source_data) * 1e-6)\ndisplay(source_data.sample(10))\ndisplay(source_data.info())\ndisplay(source_data.describe())\ndisplay(source_data.isna().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"83250e56df0f093b66a5da5837302177f0ad24f1"},"cell_type":"code","source":"def add_features(df):\n    df = df.assign(totalDistance=lambda x: x['rideDistance'] + x['walkDistance'] + x['swimDistance'])\n    df = df.assign(totalPoints=lambda x: x['killPoints'] + x['winPoints'])\n    df = df.assign(totalMedicine=lambda x: x['heals'] + x['boosts'])\n    df = df.assign(headshotRate=lambda x: x['headshotKills'] / (x['kills'] + 0.00001 ))\n    \n    \n    for new_column in ['totalDistance', 'totalPoints', 'totalMedicine', 'headshotRate']:\n        if new_column not in x_numeric_columns:\n            x_numeric_columns.append(new_column)\n    \n    print('New columns added')\n    match_group_by = df.groupby(['matchId', 'groupId'])[x_numeric_columns]   \n    print('match_group_by done')\n    \n    match_group_mean = match_group_by.mean()\n    print('match_group_mean done')\n    match_group_mean_rank = match_group_mean.groupby('matchId')[x_numeric_columns].rank(pct=True)\n    df = pd.merge(df, match_group_mean.reset_index(), on=['matchId', 'groupId'], how='left', suffixes=[\"\", \"_group_mean\"], copy=False)\n    df = pd.merge(df, match_group_mean_rank, on=['matchId', 'groupId'], how='left', suffixes=[\"\", \"_group_mean_rank\"], copy=False)\n    del match_group_mean, match_group_mean_rank; gc.collect()\n    print('all match_group_mean done')\n    \n    match_group_median = match_group_by.median()\n    match_group_median_rank = match_group_median.groupby('matchId')[x_numeric_columns].rank(pct=True)\n    df = pd.merge(df, match_group_median.reset_index(), on=['matchId', 'groupId'], how='left', suffixes=[\"\", \"_group_median\"], copy=False)\n    df = pd.merge(df, match_group_median_rank, on=['matchId', 'groupId'], how='left', suffixes=[\"\", \"_group_median_rank\"], copy=False)\n    del match_group_median, match_group_median_rank; gc.collect()\n    print('match_group_median done')\n    \n    df = optimize_memory(df)\n    \n#     match_group_std  = match_group_by.std()  \n#     match_group_std.replace([np.inf, -np.inf], np.nan, inplace=True)\n#     match_group_std.fillna(0, inplace=True)    \n#     match_group_std_rank = match_group_std.groupby('matchId')[x_numeric_columns].rank(pct=True)\n#     df = pd.merge(df, match_group_std.reset_index(), on=['matchId', 'groupId'], how='left', suffixes=[\"\", \"_group_std\"], copy=False)\n#     df = pd.merge(df, match_group_std_rank, on=['matchId', 'groupId'], how='left', suffixes=[\"\", \"_group_std_rank\"], copy=False)\n#     del match_group_std, match_group_std_rank; gc.collect()\n    \n    match_group_size = match_group_by.size().reset_index(name='group_size')  \n    df = pd.merge(df, match_group_size, how='left', on=['matchId', 'groupId'], copy=False)\n    del match_group_size; gc.collect()\n    \n    match_group_max  = match_group_by.max()\n    match_group_max_rank = match_group_max.groupby('matchId')[x_numeric_columns].rank(pct=True)\n    df = pd.merge(df, match_group_max.reset_index(), on=['matchId', 'groupId'], how='left', suffixes=[\"\", \"_group_max\"], copy=False)\n    df = pd.merge(df, match_group_max_rank, on=['matchId', 'groupId'], how='left', suffixes=[\"\", \"_group_max_rank\"], copy=False)\n    del match_group_max, match_group_max_rank; gc.collect()\n    \n    match_group_min  = match_group_by.min()\n    match_group_min_rank = match_group_min.groupby('matchId')[x_numeric_columns].rank(pct=True)\n    df = pd.merge(df, match_group_min.reset_index(), on=['matchId', 'groupId'], how='left', suffixes=[\"\", \"_group_min\"], copy=False)\n    df = pd.merge(df, match_group_min_rank, on=['matchId', 'groupId'], how='left', suffixes=[\"\", \"_group_min_rank\"], copy=False)\n    del match_group_min, match_group_min_rank; gc.collect()\n    \n    df = optimize_memory(df)\n    \n    match_group_sum  = match_group_by.sum()\n    match_group_sum.replace([np.inf, -np.inf], np.nan, inplace=True)\n    match_group_sum.fillna(0, inplace=True) \n    match_group_sum_rank = match_group_sum.groupby('matchId')[x_numeric_columns].rank(pct=True)\n    df = pd.merge(df, match_group_sum.reset_index(), on=['matchId', 'groupId'], how='left', suffixes=[\"\", \"_group_sum\"], copy=False)\n    df = pd.merge(df, match_group_sum_rank, on=['matchId', 'groupId'], how='left', suffixes=[\"\", \"_group_sum_rank\"], copy=False)\n    del match_group_sum, match_group_sum_rank, match_group_by; gc.collect()\n    \n    df = optimize_memory(df)\n    print('Group matches done')\n    match_by = df.groupby(['matchId'])[x_numeric_columns]\n\n    match_mean = match_by.mean().reset_index()\n    df = pd.merge(df, match_mean, on=['matchId'], how='left', suffixes=[\"\", \"_match_mean\"], copy=False)\n    del match_mean; gc.collect()\n    print('Matches mean done')\n    \n    match_median = match_by.median().reset_index()\n    df = pd.merge(df, match_median, on=['matchId'], how='left', suffixes=[\"\", \"_match_median\"], copy=False)\n    del match_median; gc.collect()\n    print('Matches median done')\n    \n#     match_std  = match_by.std().reset_index()\n#     match_std.replace([np.inf, -np.inf], np.nan, inplace=True)\n#     match_std.fillna(0, inplace=True)    \n#     df = pd.merge(df, match_std, on=['matchId'], how='left', suffixes=[\"\", \"_match_std\"], copy=False)\n#     del match_std; gc.collect()\n#     print('Matches std done')\n    \n    match_max  = match_by.max().reset_index()\n    df = pd.merge(df, match_max, on=['matchId'], how='left', suffixes=[\"\", \"_match_max\"], copy=False)\n    del match_max; gc.collect()\n    print('Matches max done')\n    \n    match_min  = match_by.min().reset_index()\n    df = pd.merge(df, match_min, on=['matchId'], how='left', suffixes=[\"\", \"_match_min\"], copy=False)\n    del match_min, match_by; gc.collect()\n    print('Matches min done')\n    \n    return df\n\n\ndef prepare_data(data, scaler=None):\n    data = add_features(data)\n    print(\"Added more features\")\n    columns = [col for col in data.columns if col.split('_')[0] in x_numeric_columns] + ['group_size']\n        \n    drop_columns = list(set(data.columns) - set(columns))\n    print(\"Columns to drop: {}\".format(drop_columns))\n        \n    match_type_cats = pd.get_dummies(data[x_categorical])\n    data = pd.merge(data, match_type_cats, left_index=True, right_index=True, how='left', suffixes=[\"\", \"\"], copy=False)\n    del match_type_cats\n    \n    data.drop(columns=drop_columns, inplace=True)\n    gc.collect()\n    \n    print('Start scaler')\n    \n    data_len = len(data)\n    step = 50000\n    batches = np.arange(0, data_len, step)\n    \n    if scaler is None:\n        scaler = StandardScaler(copy=False)\n        print('start scaler fitting')\n        for batch in batches:\n            print(\"scaler fit batch {}\".format(batch))\n            scaler.partial_fit(data.loc[batch:batch+step, columns].astype(np.float32))\n\n    gc.collect()\n    print('Start transform')\n    for batch in batches:\n        print(\"scaler transform batch {}\".format(batch))\n        data.loc[batch:batch+step, columns] = scaler.transform(data.loc[batch:batch+step, columns]) \n        gc.collect()   \n\n    data = optimize_memory(data)\n    return (data, scaler)\n\n\ndef build_model(input_shape):\n    input_layer = Input(shape=input_shape)\n    dense_1 = Dense(512, activation='relu')(input_layer)\n    dense_2 = Dense(256, activation='relu')(dense_1)\n    dense_3 = Dense(256, activation='relu')(dense_2)\n    output_layer = Dense(1, activation='sigmoid')(dense_3)\n    model = Model(inputs=input_layer, outputs=output_layer)\n    model.compile(optimizer=keras.optimizers.Adam(lr=0.001), loss='mae')\n    return model\n\n\ndef normalize_predictions(predicted, max_place):\n    predicted_place = max_place - predicted * (max_place - 1)\n    predicted_place = predicted_place.round()\n    norm_predicted  = (max_place - predicted_place) / ( max_place - 1 + 0.0001 )\n    return norm_predicted\n\n\nclass LRSchedulerByLoss(Callback):    \n    def __init__(self, decay=0.95, tolerans=5, verbose=0):\n        super(LRSchedulerByLoss, self).__init__()\n        self.decay = decay\n        self.tolerans = tolerans\n        self.verbose = verbose\n        self.loss_not_decrease_epochs = 0\n        self.min_lost  = float('inf')\n    \n    def on_train_begin(self, logs=None):\n         self.current_lr = float(K.get_value(self.model.optimizer.lr))\n\n    def on_epoch_end(self, epoch, logs={}):\n        current_loss = logs.get('loss')\n\n        if self.min_lost - current_loss > 0.0001:\n            self.min_lost = current_loss\n            self.loss_not_decrease_epochs = 0\n            print('Improved')\n        else:\n            self.loss_not_decrease_epochs += 1\n            print('Not improved')\n\n        \n        if self.loss_not_decrease_epochs > self.tolerans:\n            self.loss_not_decrease_epochs = self.loss_not_decrease_epochs // 2\n            self.current_lr = self.current_lr * self.decay\n            K.set_value(self.model.optimizer.lr, self.current_lr)\n            if self.verbose > 0:\n                print('\\nEpoch %05d: LRSchedulerByLoss setting lr to %s.' % (epoch + 1, self.current_lr))\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"24c211da19fb6a3ba9a11e0a29f381ada485f333"},"cell_type":"code","source":"train_x, scaler = prepare_data(source_data)\ninput_shape = (train_x.shape[1],)\nprint(\"Input shape {}\".format(input_shape))\ndisplay(train_x.head())\ndisplay(train_x.info())\nmodel = build_model(input_shape)\ngc.collect()\n\nearly_stopping = EarlyStopping(monitor='loss', min_delta=0.0001, patience=10, verbose=1, mode='auto', restore_best_weights=True)\n\nlr_scheduler_by_loss = LRSchedulerByLoss(verbose=1)\nmodel.fit(train_x.values, source_y, batch_size=batch_size, epochs=epochs, validation_split=validation_split, callbacks=[lr_scheduler_by_loss, early_stopping])\n\n# 180000/180000 [==============================] - 3s 16us/step - loss: 0.0187 - val_loss: 0.0766\n# 350 180000/180000 [==============================] - 4s 21us/step - loss: 0.0144 - val_loss: 0.0739","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4ec7a86748113126be38811218839cc82d955ab6"},"cell_type":"code","source":"# train_solos = train_data[train_data['numGroups']>50]\n# train_y_solos = train_y[train_solos.index]\n# dev_solos = dev_data[dev_data['numGroups']>50]\n# dev_y_solos = dev_y[dev_solos.index]\n\n# solos = train_data[train_data['numGroups']>50]\n# duos = train_data[(train_data['numGroups']>25) & (train_data['numGroups']<=50)]\n# squads = train_data[train_data['numGroups']<=25]\n# print(\"There are {} ({:.2f}%) solo games, {} ({:.2f}%) duo games and {} ({:.2f}%) squad games.\".format(\n#     len(solos), 100*len(solos)/len(train_data), len(duos), 100*len(duos)/len(train_data), \n#     len(squads), 100*len(squads)/len(train_data),))\n\n# train_x_solos, dev_x_solos = prepare_data(train_solos, dev_solos)\n# input_shape = (train_x_solos.shape[1],)\n# model_solos = build_model(input_shape)\n# model_solos.fit(train_x_solos, train_y_solos, batch_size=4096, epochs=25, validation_data=(dev_x_solos, dev_y_solos))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"72f553ecae239f630c064a64168047190ae702c7"},"cell_type":"code","source":"# train_y_solos_predicted = model.predict(train_x_solos)\n# train_y_solos_predicted = pd.Series(train_y_solos_predicted.reshape(-1), index=train_y_solos.index, name='winPlacePerc')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5a20d7df57f70c73cd9e500b2b4f8cffc2e9d46f"},"cell_type":"code","source":"# display(train_y_solos.describe())\n# display(train_y_solos_predicted.describe())\n# plt.figure()\n# diff = train_y_solos.subtract(train_y_solos_predicted)\n# sns.distplot(diff)\n# diff.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8bf3bb3b4c85b94cace92926fc0e6e04cf880636"},"cell_type":"code","source":"del source_data, train_x\ngc.collect()\n\nsubmit_data = load_csv('../input/test_V2.csv')\noptimize_data(submit_data)\n\n\nsubmit_data_x, _  = prepare_data(submit_data, scaler)\nsubmit_data['winPlacePercPred'] = model.predict(submit_data_x, batch_size=batch_size)\nsubmit_data['winPlacePercPred'] = np.clip(submit_data['winPlacePercPred'], a_min=0, a_max=1)\n\nresults = submit_data.groupby(['matchId', 'groupId'])['winPlacePercPred'].mean().groupby('matchId').rank(pct=True).reset_index()\nresults.columns = ['matchId','groupId', y_label]\nsubmit_data = submit_data.merge(results, how='left', on=['matchId','groupId'])\n\n#submit_data[y_label] = normalize_predictions(submit_data[y_label], submit_data['maxPlace'])\nsubmission['Id'] = submission['Id'].map(lambda x: int_to_str_dict[x])\nsubmission = submit_data[['Id', y_label]]\ndisplay(submission.head())\nsubmission.to_csv('./submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}