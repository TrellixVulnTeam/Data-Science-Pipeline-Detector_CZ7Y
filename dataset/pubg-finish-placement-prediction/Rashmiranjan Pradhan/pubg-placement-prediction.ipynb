{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"## Something went wrong when importing fastai.structured.\n## We fixed this by put the whole source code of fastai.structured in the notebook.\n## This was copied from: https://github.com/anandsaha/fastai.part1.v2/blob/master/fastai/structured.py\n\nfrom sklearn_pandas import DataFrameMapper\nfrom sklearn.preprocessing import LabelEncoder, Imputer, StandardScaler\nfrom pandas.api.types import is_string_dtype, is_numeric_dtype\nfrom sklearn.ensemble import forest\nfrom sklearn.tree import export_graphviz\n\ndef get_sample(df,n):\n    \"\"\" Gets a random sample of n rows from df, without replacement.\n    Parameters:\n    -----------\n    df: A pandas data frame, that you wish to sample from.\n    n: The number of rows you wish to sample.\n    Returns:\n    --------\n    return value: A random sample of n rows of df.\n    Examples:\n    ---------\n    >>> df = pd.DataFrame({'col1' : [1, 2, 3], 'col2' : ['a', 'b', 'a']})\n    >>> df\n       col1 col2\n    0     1    a\n    1     2    b\n    2     3    a\n    >>> get_sample(df, 2)\n       col1 col2\n    2     3    a\n    1     2    b\n    \"\"\"\n    idxs = sorted(np.random.permutation(len(df))[:n])\n    return df.iloc[idxs].copy()\n\ndef proc_df(df, y_fld, skip_flds=None, do_scale=False, na_dict=None,\n            preproc_fn=None, max_n_cat=None, subset=None, mapper=None):\n\n    \"\"\" proc_df takes a data frame df and splits off the response variable, and\n    changes the df into an entirely numeric dataframe.\n    Parameters:\n    -----------\n    df: The data frame you wish to process.\n    y_fld: The name of the response variable\n    skip_flds: A list of fields that dropped from df.\n    do_scale: Standardizes each column in df,Takes Boolean Values(True,False)\n    na_dict: a dictionary of na columns to add. Na columns are also added if there\n        are any missing values.\n    preproc_fn: A function that gets applied to df.\n    max_n_cat: The maximum number of categories to break into dummy values, instead\n        of integer codes.\n    subset: Takes a random subset of size subset from df.\n    mapper: If do_scale is set as True, the mapper variable\n        calculates the values used for scaling of variables during training time(mean and standard deviation).\n    Returns:\n    --------\n    [x, y, nas, mapper(optional)]:\n        x: x is the transformed version of df. x will not have the response variable\n            and is entirely numeric.\n        y: y is the response variable\n        nas: returns a dictionary of which nas it created, and the associated median.\n        mapper: A DataFrameMapper which stores the mean and standard deviation of the corresponding continous\n        variables which is then used for scaling of during test-time.\n    Examples:\n    ---------\n    >>> df = pd.DataFrame({'col1' : [1, 2, 3], 'col2' : ['a', 'b', 'a']})\n    >>> df\n       col1 col2\n    0     1    a\n    1     2    b\n    2     3    a\n    note the type of col2 is string\n    >>> train_cats(df)\n    >>> df\n       col1 col2\n    0     1    a\n    1     2    b\n    2     3    a\n    now the type of col2 is category { a : 1, b : 2}\n    >>> x, y, nas = proc_df(df, 'col1')\n    >>> x\n       col2\n    0     1\n    1     2\n    2     1\n    >>> data = DataFrame(pet=[\"cat\", \"dog\", \"dog\", \"fish\", \"cat\", \"dog\", \"cat\", \"fish\"],\n                 children=[4., 6, 3, 3, 2, 3, 5, 4],\n                 salary=[90, 24, 44, 27, 32, 59, 36, 27])\n    >>> mapper = DataFrameMapper([(:pet, LabelBinarizer()),\n                          ([:children], StandardScaler())])\n    >>>round(fit_transform!(mapper, copy(data)), 2)\n    8x4 Array{Float64,2}:\n    1.0  0.0  0.0   0.21\n    0.0  1.0  0.0   1.88\n    0.0  1.0  0.0  -0.63\n    0.0  0.0  1.0  -0.63\n    1.0  0.0  0.0  -1.46\n    0.0  1.0  0.0  -0.63\n    1.0  0.0  0.0   1.04\n    0.0  0.0  1.0   0.21\n    \"\"\"\n    if not skip_flds: skip_flds=[]\n    if subset: df = get_sample(df,subset)\n    df = df.copy()\n    if preproc_fn: preproc_fn(df)\n    y = df[y_fld].values\n    df.drop(skip_flds+[y_fld], axis=1, inplace=True)\n\n    if na_dict is None: na_dict = {}\n    for n,c in df.items(): na_dict = fix_missing(df, c, n, na_dict)\n    if do_scale: mapper = scale_vars(df, mapper)\n    for n,c in df.items(): numericalize(df, c, n, max_n_cat)\n    res = [pd.get_dummies(df, dummy_na=True), y, na_dict]\n    if do_scale: res = res + [mapper]\n    return res\n\ndef rf_feat_importance(m, df):\n    return pd.DataFrame({'cols':df.columns, 'imp':m.feature_importances_}\n                       ).sort_values('imp', ascending=False)\n\ndef set_rf_samples(n):\n    \"\"\" Changes Scikit learn's random forests to give each tree a random sample of\n    n random rows.\n    \"\"\"\n    forest._generate_sample_indices = (lambda rs, n_samples:\n        forest.check_random_state(rs).randint(0, n_samples, n))\n\ndef reset_rf_samples():\n    \"\"\" Undoes the changes produced by set_rf_samples.\n    \"\"\"\n    forest._generate_sample_indices = (lambda rs, n_samples:\n        forest.check_random_state(rs).randint(0, n_samples, n_samples))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For autoreloading modules\n%load_ext autoreload\n%autoreload 2\n# For notebook plotting\n%matplotlib inline\n\n# Standard libraries\nimport os\nimport numpy as np\nimport pandas as pd\n\n# Visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nmpl.rc('axes', labelsize=14)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)\n\nimport seaborn as sns\nfrom pdpbox import pdp\nfrom plotnine import *\nfrom pandas_summary import DataFrameSummary\nfrom IPython.display import display\nimport pickle\n\n# Machine Learning\nimport xgboost as xgb\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\nfrom scipy.cluster import hierarchy as hc\nfrom fastai.imports import *","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/pubg-finish-placement-prediction/train_V2.csv\")\ntest = pd.read_csv(\"../input/pubg-finish-placement-prediction/test_V2.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Illegal Match\n\n#### There is one particular player with a 'winPlacePerc' of NaN. We can delete that row."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check row with NaN value\ntrain[train['winPlacePerc'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's drop this entity\ntrain.drop(2744604, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check row with NaN value\ntrain[train['winPlacePerc'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The Killers"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The average person kills {:.4f} players, 99% of people have {} kills or less, while the most kills ever recorded is {}.\".format(train['kills'].mean(),train['kills'].quantile(0.99), train['kills'].max()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Let's plot kill counts"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_copy = train.copy()\ntrain_copy.loc[train_copy['kills'] > train_copy['kills'].quantile(0.99)] = '8+'\nsns.countplot(train_copy['kills'].astype('str').sort_values())\nplt.title(\"Kill Count\", fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Most people can't make a single kill. At least do they do damage?"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_copy = train.copy()\ntrain_copy = train_copy[train_copy['kills'] == 0]\nplt.title(\"Damage Dealt by 0 killers\", fontsize=15)\nsns.distplot(train_copy['damageDealt'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['totalDistance'] = train['walkDistance'] + train['rideDistance'] + train['swimDistance']\ntrain['healsAndBoosts'] = train['heals']+train['boosts']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# A column for team\ntrain['team'] = [1 if i>50 else 2 if (i>25 & i<=50) else 4 for i in train['numGroups']]\ntrain['playersJoined'] = train.groupby('matchId')['matchId'].transform('count')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create normalized features\ntrain['killsNorm'] = train['kills']*((100-train['playersJoined'])/100 + 1)\ntrain['damageDealtNorm'] = train['damageDealt']*((100-train['playersJoined'])/100 + 1)\ntrain['maxPlaceNorm'] = train['maxPlace']*((100-train['playersJoined'])/100 + 1)\ntrain['matchDurationNorm'] = train['matchDuration']*((100-train['playersJoined'])/100 + 1)\n# Compare standard features and normalized features\nto_show = ['Id', 'kills','killsNorm','damageDealt', 'damageDealtNorm', 'maxPlace', 'maxPlaceNorm', 'matchDuration', 'matchDurationNorm']\ntrain[to_show][0:11]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create headshot_rate feature\ntrain['headshot_rate'] = train['headshotKills'] / train['kills']\ntrain['headshot_rate'] = train['headshot_rate'].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### This helps to find out cheaters."},{"metadata":{"trusted":true},"cell_type":"code","source":"train['killsWithoutMoving'] = ((train['kills'] > 0) & (train['totalDistance'] == 0))\n# Got those cheaters\ntrain.drop(train[train['killsWithoutMoving'] == True].index, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Anomalies in roadkills"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Players who got more than 10 roadkills\ntrain[train['roadKills'] > 10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop roadkill 'cheaters'\ntrain.drop(train[train['roadKills'] > 10].index, inplace=True)\n\n#Note that player c3e444f7d1289d drove 5 meters but killed 14 people with it. Sounds insane doesn't it?","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Anomalies in aim(More than 45 kills)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's plot the total kills for every player first. It doesn't look like there are too many outliers\n# Plot the distribution of kills\nplt.figure(figsize=(12,4))\nsns.countplot(data=train, x=train['kills']).set_title('Kills')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Players who got more than 30 kills\ndisplay(train[train['kills'] > 30].shape)\ntrain[train['kills'] > 30].head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove outliers\ntrain.drop(train[train['kills'] > 30].index, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Anomalies in aim part 2 (100% headshot rate)"},{"metadata":{},"cell_type":"markdown","source":"Again, we first take a look at the whole dataset and create a new feature 'headshot_rate'. We see that the most players score in the 0 to 10% region. However, there are a few anomalies that have a headshot_rate of 100% percent with more than 9 kills!"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the distribution of headshot_rate\nplt.figure(figsize=(12,4))\nsns.distplot(train['headshot_rate'], bins=10)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Players who made a minimum of 10 kills and have a headshot_rate of 100%\ndisplay(train[(train['headshot_rate'] == 1) & (train['kills'] > 9)].shape)\ntrain[(train['headshot_rate'] == 1) & (train['kills'] > 9)].head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Anomalies in aim part 3 (Longest kill)"},{"metadata":{},"cell_type":"markdown","source":"Most kills are made from a distance of 100 meters or closer. There are however some outliers who make a kill from more than 1km away. This is probably done by cheaters"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the distribution of longestKill\nplt.figure(figsize=(12,4))\nsns.distplot(train['longestKill'], bins=10)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check out players who made kills with a distance of more than 1 km\ndisplay(train[train['longestKill'] >= 1000].shape)\ntrain[train['longestKill'] >= 1000].head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove outliers\ntrain.drop(train[train['longestKill'] >= 1000].index, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is something fishy going on with these players. We are probably better off removing them from our dataset.\n\n"},{"metadata":{},"cell_type":"markdown","source":"#### Anomalies in travelling (rideDistance, walkDistance and swimDistance)"},{"metadata":{},"cell_type":"markdown","source":"#### Walk distance"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Summary statistics for the Distance features\ntrain[['walkDistance', 'rideDistance', 'swimDistance', 'totalDistance']].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# walkDistance anomalies\ndisplay(train[train['walkDistance'] >= 10000].shape)\ntrain[train['walkDistance'] >= 10000].head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove outliers\ntrain.drop(train[train['walkDistance'] >= 10000].index, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### rideDistance"},{"metadata":{"trusted":true},"cell_type":"code","source":"# rideDistance anomalies\ndisplay(train[train['rideDistance'] >= 20000].shape)\ntrain[train['rideDistance'] >= 20000].head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove outliers\ntrain.drop(train[train['rideDistance'] >= 20000].index, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### swimDistance"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Players who swam more than 2 km\ntrain[train['swimDistance'] >= 2000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove outliers\ntrain.drop(train[train['swimDistance'] >= 2000].index, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Anomalies in supplies (weaponsAcquired)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the distribution of weaponsAcquired\nplt.figure(figsize=(12,4))\nsns.distplot(train['weaponsAcquired'], bins=100)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Players who acquired more than 80 weapons\ndisplay(train[train['weaponsAcquired'] >= 80].shape)\ntrain[train['weaponsAcquired'] >= 80].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We should probably remove these outliers from our model. Do you agree?\n\nNote that player 3f2bcf53b108c4 acquired 236 weapons in one game!"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove outliers\ntrain.drop(train[train['weaponsAcquired'] >= 80].index, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Anomalies in supplies part 2 (heals)\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of heals\nplt.figure(figsize=(12,4))\nsns.distplot(train['heals'], bins=10)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 40 or more healing items used\ndisplay(train[train['heals'] >= 40].shape)\ntrain[train['heals'] >= 40].head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most players us 5 healing items or less. We can again recognize some weird anomalies"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove outliers\ntrain.drop(train[train['heals'] >= 40].index, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We removed about 2000 players from our dataset. Do you think this is too much? Please let us know in the comments"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remaining players in the training set\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Categorical Variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('There are {} different Match types in the dataset.'.format(train['matchType'].nunique()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# One hot encode matchType\ntrain = pd.get_dummies(train, columns=['matchType'])\n\n# Take a look at the encoding\nmatchType_encoding = train.filter(regex='matchType')\nmatchType_encoding.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are a lot of groupId's and matchId's so one-hot encoding them is computational suicide. We will turn them into category codes. That way we can still benefit from correlations between groups and matches in our Random Forest algorithm"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Turn groupId and match Id into categorical types\ntrain['groupId'] = train['groupId'].astype('category')\ntrain['matchId'] = train['matchId'].astype('category')\n\n# Get category coding for groupId and matchID\ntrain['groupId_cat'] = train['groupId'].cat.codes\ntrain['matchId_cat'] = train['matchId'].cat.codes\n\n# Get rid of old columns\ntrain.drop(columns=['groupId', 'matchId'], inplace=True)\n\n# Lets take a look at our newly created features\ntrain[['groupId_cat', 'matchId_cat']].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop Id column, because it probably won't be useful for our Machine Learning algorithm,\n# because the test set contains different Id's\ntrain.drop(columns = ['Id'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Test set engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add engineered features to the test set\ntest['totalDistance'] = test['rideDistance'] + test['walkDistance'] + test['swimDistance']\ntest['healsAndBoosts'] = test['heals'] + test['boosts']\ntest['team'] = [1 if i>50 else 2 if (i>25 & i<=50) else 4 for i in test['numGroups']]\ntest['playersJoined'] = test.groupby('matchId')['matchId'].transform('count')\ntest['killsNorm'] = test['kills']*((100-test['playersJoined'])/100 + 1)\ntest['damageDealtNorm'] = test['damageDealt']*((100-test['playersJoined'])/100 + 1)\ntest['maxPlaceNorm'] = test['maxPlace']*((100-train['playersJoined'])/100 + 1)\ntest['matchDurationNorm'] = test['matchDuration']*((100-test['playersJoined'])/100 + 1)\ntest['headshot_rate'] = test['headshotKills'] / test['kills']\ntest['headshot_rate'] = test['headshot_rate'].fillna(0)\ntest['killsWithoutMoving'] = ((test['kills'] > 0) & (test['totalDistance'] == 0))\n\n# One hot encode matchType\ntest = pd.get_dummies(test, columns=['matchType'])\n\n# Turn groupId and match Id into categorical types\ntest['groupId'] = test['groupId'].astype('category')\ntest['matchId'] = test['matchId'].astype('category')\n\n# Get category coding for groupId and matchID\ntest['groupId_cat'] = test['groupId'].cat.codes\ntest['matchId_cat'] = test['matchId'].cat.codes\n\n# Get rid of old columns\ntest.drop(columns=['groupId', 'matchId'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preparation for Machine Learning\n### Sampling\nWe will take a sample of 500000 rows from our training set for easy debugging and exploration."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Take sample for debugging and exploration\nsample = 500000\ntrain_sample = train.sample(sample)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Split target variable, validation data, etc."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split sample into training data and target variable\nX = train_sample.drop(columns=['winPlacePerc']) # All columns except target\ny = train_sample['winPlacePerc'] # Only target variable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function for splitting training and validation data\ndef split_vals(a, n : int):\n    return a[:n].copy(), a[n:].copy()\nval_perc = 0.12 # % to use for validation set\nn_valid = int(val_perc * sample)\nn_trn = len(X) - n_valid\n\n# Split data\nraw_train, raw_valid = split_vals(train_sample, n_trn)\nX_train, X_valid = split_vals(X, n_trn)\ny_train, y_valid = split_vals(y, n_trn)\n\n# Check dimensions of samples\nprint('Sample train shape: ', X_train.shape, \n      'Sample target shape: ', y_train.shape, \n      'Sample validation shape: ', X_valid.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to print the MAE (Mean Absolute Error) score\n# This is the metric used by Kaggle in this competition\ndef print_score(m : xgb):\n    res = ['mae train: ', mean_absolute_error(m.predict(X_train), y_train), \n           'mae val: ', mean_absolute_error(m.predict(X_valid), y_valid)]\n    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)\n    print(res)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XgBoost Or Random Forest\n\nI tried with XgBoost and i didn't get significant result.\nThen I tried random Forest with random search, but with 5M datas it cannot get result within kernel time.\nFinally I used Random forest and fastai to find best features and best result."},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Building DMatrix for XGboost, because it does not work on numpy matrix\n# dtrain = xgb.DMatrix(X_train, label=y_train)\n# dvalid = xgb.DMatrix(X_valid, label=y_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # \"Learn\" the mean from the training data\n# mean_train = np.mean(y_train)\n# # Get predictions on the test set\n# baseline_predictions = np.ones(y_valid.shape) * mean_train\n# # Compute MAE\n# mae_baseline = mean_absolute_error(y_valid, baseline_predictions)\n# print(\"Baseline MAE is {:.4f}\".format(mae_baseline))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# params = {'min_child_weight':[4,5], \n#           'gamma':[i/10.0 for i in range(3,6)],  \n#           'subsample':[i/10.0 for i in range(6,11)], \n#           'colsample_bytree':[i/10.0 for i in range(6,11)], \n#           'max_depth': [2,3,4]\n#          }\n\n# # Number of trees in Random forest\n# n_estimators = [int(x) for x in np.linspace(start=200, stop=2000, num=10)]\n# # Number of features to consider at every split\n# max_features = ['auto', 'sqrt']\n# # Maximum number of levels in tree\n# max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n# max_depth.append(None)\n# # Minimum number of samples required at each split node\n# min_samples_split = [2, 5, 10]\n# # Minimum number of samples required at each leaf node\n# min_samples_leaf = [1, 2, 3]\n# # Method for selecting samples for training each tree\n# bootstrap = [True, False]\n\n# param_dist_rf = {\n#     'n_estimators': n_estimators,\n#     'max_features': max_features,\n#     'max_depth': max_depth,\n#     'min_samples_split': min_samples_split,\n#     'min_samples_leaf' : min_samples_leaf,\n#     'bootstrap' : bootstrap\n# }\n\n# print(param_dist_rf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# xgb_model = xgb.XGBRegressor(learning_rate=0.02, n_estimators=600, objective='binary:logistic', silent=True, nthread=1)\nrnd_mod_1 = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features='sqrt', n_jobs=-1)\n# rnd_reg_1 = RandomForestRegressor()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# random_rf = RandomizedSearchCV(rnd_reg_1, param_distributions=param_dist_rf,\n#                               n_iter=25,\n#                               cv=5,\n#                               scoring='neg_mean_absolute_error',\n#                               error_score=0,\n#                               n_jobs=-1,\n#                               verbose=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# xgb_model.fit(X_train, y_train)\n# print_score(xgb_model)\n\nrnd_mod_1.fit(X_train, y_train)\nprint_score(rnd_mod_1)\n\n# random_rf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(random_rf.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pkl_rnd_rf = \"pickle_rnd_rf.pkl\"\n# with open(pkl_rnd_rf, 'wb') as file:\n#     pickle.dump(random_rf, file)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"# fi = rf_feat_importance(xgb_model, X); fi[:10]\nfi = rf_feat_importance(rnd_mod_1, X); fi[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot a feature importance graph for the 20 most important features\nplot1 = fi[:20].plot('cols', 'imp', figsize=(14,6), legend=False, kind = 'barh')\nplot1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Keep only significant features\nto_keep = fi[fi.imp>0.005].cols\nprint('Significant features: ', len(to_keep))\nto_keep","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make a DataFrame with only significant features\nX_keep = X[to_keep].copy()\nX_train, X_valid = split_vals(X_keep, n_trn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# xgb_model_1 = xgb.XGBRegressor(learning_rate=0.02, n_estimators=800, objective='binary:logistic', silent=True, nthread=1)\nrnd_mod_2 = RandomForestRegressor(n_estimators=80, min_samples_leaf=3, max_features='sqrt', n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# xgb_model_1.fit(X_train, y_train)\n# print_score(xgb_model_1)\nrnd_mod_2.fit(X_train, y_train)\nprint_score(rnd_mod_2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get feature importances of our top features\nfi_to_keep = rf_feat_importance(rnd_mod_2, X_keep)\nplot2 = fi_to_keep.plot('cols', 'imp', figsize=(14,6), legend=False, kind = 'barh')\nplot2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correlation Heat map\ncorr = X_keep.corr()\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Create heatmap\nheatmap = sns.heatmap(corr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Final Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prepare data\nval_perc_full = 0.12 # % to use for validation set\nn_valid_full = int(val_perc_full * len(train)) \nn_trn_full = len(train)-n_valid_full\nX_full = train.drop(columns = ['winPlacePerc']) # all columns except target\ny = train['winPlacePerc'] # target variable\nX_full = X_full[to_keep] # Keep only relevant features\nX_train, X_valid = split_vals(X_full, n_trn_full)\ny_train, y_valid = split_vals(y, n_trn_full)\n\n# Check dimensions of data\nprint('Sample train shape: ', X_train.shape, \n      'Sample target shape: ', y_train.shape, \n      'Sample validation shape: ', X_valid.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# xgb_model_2 = xgb.XGBRegressor(learning_rate=0.02, n_estimators=999, objective='binary:logistic', silent=True, nthread=1)\nrnd_mod_3 = RandomForestRegressor(n_estimators=70, min_samples_leaf=3, max_features=0.5, n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# xgb_model_2.fit(X_train, y_train)\n# print_score(xgb_model_2)\nrnd_mod_3.fit(X_train, y_train)\nprint_score(rnd_mod_3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Remove irrelevant features from the test set\ntest_pred = test[to_keep].copy()\n\n# Fill NaN with 0 (temporary)\ntest_pred.fillna(0, inplace=True)\ntest_pred.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make submission ready for Kaggle\n# We use our final Random Forest model (rnd_mod_3) to get the predictions\npredictions = np.clip(a = rnd_mod_3.predict(test_pred), a_min = 0.0, a_max = 1.0)\npred_df = pd.DataFrame({'Id' : test['Id'], 'winPlacePerc' : predictions})\n\n# Create submission file\npred_df.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}