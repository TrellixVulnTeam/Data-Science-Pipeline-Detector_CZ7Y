{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport gc\nimport lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 减小内存\ndef reduce_men_usage(df):\n    start_mem = df.memory_usage().sum() / 1024 ** 2\n    print(f\"Begin Memory usage of dataframe is {start_mem} MB\")\n\n    for col in df.columns:\n        col_type = df[col].dtype\n\n        if col_type != object:\n            col_min = df[col].min()\n            col_max = df[col].max()\n\n            if str(col_type)[:3] == 'int':\n                if col_min > np.iinfo(np.int8).min and col_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif col_min > np.iinfo(np.int16).min and col_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif col_min > np.iinfo(np.int32).min and col_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif col_min < np.iinfo(np.int64).min and col_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            elif str(col_type)[:5] == 'float':\n                if col_min > np.finfo(np.float16).min and col_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                if col_min > np.finfo(np.float32).min and col_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() / 1024 ** 2\n\n    print(f\"End Memory usage of dataframe is {end_mem} MB\")\n\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#按照比赛id划分数据集\ndef split_train_validation(data, fraction):\n    matchIds = data['matchId'].unique().reshape([-1])\n    train_size = int(len(matchIds) * fraction)\n    random_index = np.random.RandomState(seed=2).permutation(len(matchIds))\n\n    train_matchIds = matchIds[random_index[:train_size]]\n    validation_matchIds = matchIds[random_index[train_size:]]\n\n    x_train = data.loc[data['matchId'].isin(train_matchIds)]\n    x_validation = data.loc[data['matchId'].isin(validation_matchIds)]\n    return x_train, x_validation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def feature_engineering(is_train, debug):\n    if is_train:\n        data = pd.read_csv('../input/pubg-finish-placement-prediction/train_V2.csv')\n        data = reduce_men_usage(data)\n        if debug:\n            data = data[data['matchId'].isin(data['matchId'].unique()[:2000])]\n    else:\n        data = pd.read_csv('../input/pubg-finish-placement-prediction/test_V2.csv')\n        data = reduce_men_usage(data)\n\n    initial = data[['matchId','groupId','Id']]\n\n    data.drop(columns=['killPoints', 'rankPoints', 'winPoints', 'matchType',\n                       'maxPlace', 'Id'], inplace=True)\n\n    data['matchSize'] = data.groupby('matchId')['matchId'].transform('count')\n    data['groupSize'] = data.groupby(['matchId', 'groupId'])['groupId'].transform('count')\n\n    data['killPlacePer'] = data['killPlace'] / data['matchSize']\n    data['killPerDamage'] = data['kills'] / (0.01 * data['damageDealt'])\n    data['killPlacePerWalk'] = (10 * data['killPlacePer']) / (0.001 * data['walkDistance'])\n    data['walkPerBoost'] = data['boosts'] / (0.001 * data['walkDistance'])\n    data['walkPerWeapon'] = data['weaponsAcquired'] / (0.001 * data['walkDistance'])\n    data['DBNOsPerDamage'] = data['DBNOs'] / (0.01 * data['damageDealt'])\n    data['healthItem'] = data['boosts'] + data['heals']\n    data['killsPerStreaks'] = data['kills'] / data['killStreaks']\n    data['killsPerDBNOs'] = data['kills'] / data['DBNOs']\n    data['longestPerkill'] = data['kills'] / (0.1 * data['longestKill'])\n    data['placePerStreak'] = data['killStreaks'] / (0.1 * data['killPlace'])\n    data[data == np.Inf] = np.NaN\n    data[data == np.NINF] = np.NaN\n    data.fillna(0, inplace=True)\n    data = reduce_men_usage(data)\n\n    print('新建特征完成...')\n\n    team_features = {\n        'walkDistance': [np.var, np.mean],\n        'killPlacePer': [sum, min, max, np.var, np.mean],\n        'boosts': [sum, np.var, np.mean],\n        'weaponsAcquired': [np.mean],\n        'damageDealt': [np.var, min, max, np.mean],\n        'heals': [sum, np.var, np.mean],\n        'kills': [sum, max, np.var, np.mean],\n        'longestKill': [max, np.mean, np.var],\n        'killStreaks': [max, np.var, np.mean],\n        'assists': [sum, np.mean, np.size],\n        'DBNOs': [np.var, max, np.mean],\n        'headshotKills': [max, np.mean],\n        'rideDistance': [sum, np.mean, np.var],\n        'killPerDamage': [np.var, max, np.mean],\n        'killPlacePerWalk': [np.mean],\n        'walkPerBoost': [np.mean],\n        'walkPerWeapon': [np.mean],\n        'DBNOsPerDamage': [np.mean],\n        'healthItem': [np.var, np.mean],\n        'killsPerStreaks': [min, max, np.mean],\n        'killsPerDBNOs': [min, max, np.mean],\n        'longestPerkill': [min, max, np.mean],\n        'placePerStreak': [min, max, np.mean],\n        'revives': [sum],\n        'vehicleDestroys': [sum],\n        'swimDistance': [np.var],\n        'roadKills': [sum],\n        'teamKills': [sum],\n        'matchDuration': [np.mean],\n        'numGroups': [np.mean]\n    }\n\n    if is_train:\n        team_features['winPlacePerc'] = max\n\n    final_x = data.groupby(['matchId', 'groupId']).agg(team_features)\n    final_x.fillna(0, inplace=True)\n    final_x.replace(np.inf, 1000000, inplace=True)\n    final_x = reduce_men_usage(final_x)\n\n    del data\n    gc.collect()\n\n    final_y = np.NaN\n    if is_train:\n        final_y = pd.DataFrame(final_x[('winPlacePerc', 'max')])\n        final_y.columns = [na1 for na1, na2 in final_y.columns]\n        final_x.drop(columns=[('winPlacePerc', 'max')], inplace=True)\n\n    final_rank = final_x.groupby('matchId').rank(pct=True)\n    final_x = final_x.reset_index()[['matchId', 'groupId']].merge(final_rank,\n                                                                  suffixes=['', '_rank'],\n                                                                  how='left',\n                                                                  on=['matchId', 'groupId'])\n\n    final_x.columns = [na1 + '_' + na2 for na1, na2 in final_x.columns]\n\n    final_x.rename(columns={ 'matchId_': 'matchId', 'groupId_': 'groupId' }, inplace=True)\n\n    final_x = reduce_men_usage(final_x)\n    print('特征操作完成...')\n\n    features = final_x.columns.tolist()\n    features.remove('matchId')\n    features.remove('groupId')\n\n    after = final_x[['matchId','groupId']]\n\n    return final_x, final_y, initial, after, features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def LGB_model(x_train, y_train, x_validation, y_validation):\n    params = {\n        #回归问题\n        \"objective\": \"regression\",\n        #度量参数\n        \"metric\": \"mae\",\n        #一棵树上的叶子树\n        \"num_leaves\": 100,\n        #学习率\n        \"learning_rate\": 0.03,\n        #在不进行重采样的情况下随机选择部分数据\n        \"bagging_fraction\": 0.9,\n        \"bagging_seed\": 0,\n        #线程数\n        \"num_threads\": 4,\n        #在建立树时对特征随机采样的比例\n        \"colsample_bytree\": 0.5,\n        #一个叶子上数据的最小数量. 可以用来处理过拟合\n        'min_data_in_leaf': 500,\n        #分裂的最小增益阈值\n        'min_split_gain': 0.00011,\n        #L2 正则\n        'lambda_l2': 9\n    }\n\n    train_set = lgb.Dataset(x_train, label=y_train)\n    validation_set = lgb.Dataset(x_validation, label=y_validation)\n\n    model = lgb.train(params,\n                      #训练集\n                      train_set=train_set,\n                      #迭代次数\n                      num_boost_round=9400,\n                      #早停\n                      early_stopping_rounds=200,\n                      #每200轮打印一次\n                      verbose_eval=200,\n                      valid_sets=[train_set, validation_set]\n                      )\n    \n#     model.save_model('.../output/kaggle/working/model.txt')\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X,Y,_,_,features = feature_engineering(True, False)\nX = X.merge(Y, how='left', on=['matchId', 'groupId'])\n\n'''划分训练集、验证集'''\nx_train, x_validation = split_train_validation(X, 0.9)\ny_train = x_train['winPlacePerc']\n\ny_validation = x_validation['winPlacePerc']\nx_train = x_train.drop(columns=['matchId', 'groupId', 'winPlacePerc'])\nx_validation = x_validation.drop(columns=['matchId', 'groupId', 'winPlacePerc'])\n\nx_train = np.array(x_train)\ny_train = np.array(y_train)\nx_validation = np.array(x_validation)\ny_validation = np.array(y_validation)\n\ndel X,Y\ngc.collect()\n\nmodel = LGB_model(x_train, y_train, x_validation, y_validation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# '''打印特征重要性'''\n# featureImp = list(model.feature_importance())\n# featureImp, features = zip(*sorted(zip(featureImp, features)))\n# with open(\"FeatureImportance.txt\", \"w\") as text_file:\n#     for i in range(len(featureImp)):\n#         print(f\"{features[i]} =  {featureImp[i]}\", file=text_file)\n\n# print(\"特征重要性输出结束...\")\n# del featureImp, features\n# gc.collect()\n\nX,_,initial,after,_ = feature_engineering(False,False)\nX.drop(['matchId','groupId'], axis = 1, inplace = True)\nX = np.array(X)\npred = model.predict(X, num_iteration=model.best_iteration)\n\ndel X\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"after['winPlacePerc'] = pred\nresult = initial.merge(after,how = 'left', on = ['matchId','groupId'])\n\n# Any results you write to the current directory are saved as output.\ndf_sub = pd.DataFrame()\ndf_test = pd.read_csv(\"../input/pubg-finish-placement-prediction/test_V2.csv\")\ndf_test = reduce_men_usage(df_test)\n\n# Restore some columns\ndf_sub['Id'] = df_test['Id']\ndf_sub = df_sub.merge(df_test[[\"Id\", \"matchId\", \"groupId\", \"maxPlace\", \"numGroups\"]], on=\"Id\", how=\"left\")\ndf_sub = df_sub.merge(result,how = 'left', on = ['Id','matchId','groupId'])\n\n# Sort, rank, and assign adjusted ratio\ndf_sub_group = df_sub.groupby([\"matchId\", \"groupId\"]).first().reset_index()\ndf_sub_group[\"rank\"] = df_sub_group.groupby([\"matchId\"])[\"winPlacePerc\"].rank()\ndf_sub_group = df_sub_group.merge(\n    df_sub_group.groupby(\"matchId\")[\"rank\"].max().to_frame(\"max_rank\").reset_index(),\n    on=\"matchId\", how=\"left\")\ndf_sub_group[\"adjusted_perc\"] = (df_sub_group[\"rank\"] - 1) / (df_sub_group[\"numGroups\"] - 1)\n\ndf_sub = df_sub.merge(df_sub_group[[\"adjusted_perc\", \"matchId\", \"groupId\"]], on=[\"matchId\", \"groupId\"], how=\"left\")\ndf_sub[\"winPlacePerc\"] = df_sub[\"adjusted_perc\"]\n\n# Deal with edge cases\ndf_sub.loc[df_sub.maxPlace == 0, \"winPlacePerc\"] = 0\ndf_sub.loc[df_sub.maxPlace == 1, \"winPlacePerc\"] = 1\n\nsubset = df_sub.loc[df_sub.maxPlace > 1]\ngap = 1.0 / (subset.maxPlace.values - 1)\nnew_perc = np.around(subset.winPlacePerc.values / gap) * gap\ndf_sub.loc[df_sub.maxPlace > 1, \"winPlacePerc\"] = new_perc\n\n# Edge case\ndf_sub.loc[(df_sub.maxPlace > 1) & (df_sub.numGroups == 1), \"winPlacePerc\"] = 0\nassert df_sub[\"winPlacePerc\"].isnull().sum() == 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sub[[\"Id\", \"winPlacePerc\"]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sub[[\"Id\", \"winPlacePerc\"]].to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}