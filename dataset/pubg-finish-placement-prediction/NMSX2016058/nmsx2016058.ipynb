{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport warnings\nimport torch\nfrom torch import nn\nfrom IPython import display\nimport itertools\nimport gc\nimport sys\nimport os\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.\n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                #if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                #    df[col] = df[col].astype(np.float16)\n                #el\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        #else:\n            #df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB --> {:.2f} MB (Decreased by {:.1f}%)'.format(\n        start_mem, end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 数据属性\ntrain_data = pd.read_csv('/kaggle/input/pubg-finish-placement-prediction/train_V2.csv')\ntest_data = pd.read_csv('/kaggle/input/pubg-finish-placement-prediction/test_V2.csv')\ntrain_data.describe().drop('count').T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 数据分布显示\ndef Variable_distribution(data, feature, type, figsize=(15, 6), rot=90, saveimg=False):\n    if type == 'int64':\n        feature_proc = feature + '_proc'\n        data[feature_proc] = data[feature]\n        data.loc[data[feature_proc] > data[feature_proc].quantile(0.99)] = 'larger'\n        plt.figure(figsize=(10, 6))\n        sns.countplot(data[feature_proc].astype('str').sort_values())\n        plt.title(feature)\n        plt.show()\n    elif type == 'float64':\n        plt.figure(figsize=(10, 6))\n        plt.title(feature)\n        sns.distplot(data[feature])  # distplot直方图\n        plt.show()\n    else:\n        print('data type error')\n    if saveimg == True:\n        figname = feature + \".png\"\n        plt.savefig(figname, dpi=75)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#热力图分析\nf,ax = plt.subplots(figsize=(15, 15))\nsns.set(font_scale=1)\nsns.heatmap(train_data.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 单个数据与winPlacePerc相关性折线图\ndef data_coefficient_process(data, feature):\n    if data[feature].dtypes == 'int64' | data[feature].dtypes == 'float64':\n        data_new = data[[feature, 'winPlacePerc']]\n        sns.set(style=\"darkgrid\")\n        g = sns.relplot(x=feature, y=\"winPlacePerc\", height=4, linewidth=2, aspect=1.3, kind=\"line\", data=data_new)\n        g.fig.autofmt_xdate()  # Rotate coordinates\n        plt.show()\n    else:\n        print(\"data type error!\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 匹配模式直方图\ndata_mathType = train_data[['matchType', 'winPlacePerc']]\nfeatures_mathType = ['squad-fpp','duo-fpp','squad','solo-fpp','duo','solo','normal-squad-fpp','crashfpp','normal-duo-fpp','flaretpp','normal-solo-fpp','flarefpp','normal-squad','crashtpp','normal-solo','normal-duo']\nmean_winPlacePerc = []\nfor feature in features_mathType:\n    test = data_mathType.loc[data_mathType['matchType'] == feature, ['matchType', 'winPlacePerc']]\n    mean_winPlacePerc.append(test.winPlacePerc.mean())\n\nplt.figure(figsize=(20,10))\nplt.ylabel('winPlacePerc')\nplt.xlabel('matchType')\nplt.bar(features_mathType,mean_winPlacePerc ,color=sns.color_palette(\"cubehelix\", len(features_mathType)),tick_label=features_mathType)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# matchType数量统计\ndef feature_barplot(data,feature, figsize=(15,6), rot = 90, saveimg = False):\n    feat_train = data[feature].value_counts()\n    fig_feature, axis1, = plt.subplots(1,1,sharex=True, sharey = True, figsize = figsize)\n    sns.barplot(feat_train.index.values, feat_train.values, ax = axis1)\n    axis1.set_xticklabels(axis1.xaxis.get_majorticklabels(), rotation = rot)\n    axis1.set_title(feature + ' of training dataset')\n    axis1.set_ylabel('Counts')\n    plt.tight_layout()\n    plt.show()\n    if saveimg == True:\n        figname = feature + \".png\"\n        fig_feature.savefig(figname, dpi = 75)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data[train_data['winPlacePerc'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nsolo  <-- solo,solo-fpp,normal-solo,normal-solo-fpp\nduo   <-- duo,duo-fpp,normal-duo,normal-duo-fpp,crashfpp,crashtpp\nsquad <-- squad,squad-fpp,normal-squad,normal-squad-fpp,flarefpp,flaretpp\n'''\nmapper = lambda x: 'solo' if('solo'in x) else 'duo' if('duo' in x)or('crash'in x) else 'squad'\ntrain_data['matchType'] = train_data['matchType'].apply(mapper)\nmatch_type_counts=train_data.groupby('matchId')['matchType'].first().value_counts().sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data = train_data.append(test_data, sort=False).reset_index(drop=True)\ndel train_data, test_data\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"match = all_data.groupby('matchId')\nall_data['killsPerc'] = match['kills'].rank(pct=True).values\nall_data['killPlacePerc'] = match['killPlace'].rank(pct=True).values\nall_data['walkDistancePerc'] = match['walkDistance'].rank(pct=True).values\n#all_data['damageDealtPerc'] = match['damageDealt'].rank(pct=True).values\nall_data['walkPerc_killsPerc'] = all_data['walkDistancePerc'] / all_data['killsPerc']\nall_data['_totalDistance'] = all_data['rideDistance'] + all_data['walkDistance'] + all_data['swimDistance']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fillInf(df, val):\n    numcols = df.select_dtypes(include='number').columns\n    cols = numcols[numcols != 'winPlacePerc']\n    df[df == np.Inf] = np.NaN\n    df[df == np.NINF] = np.NaN\n    for c in cols: df[c].fillna(val, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data['_healthItems'] = all_data['heals'] + all_data['boosts']\nall_data['_headshotKillRate'] = all_data['headshotKills'] / all_data['kills']\nall_data['_killPlaceOverMaxPlace'] = all_data['killPlace'] / all_data['maxPlace']\nall_data['_killsOverWalkDistance'] = all_data['kills'] / all_data['walkDistance']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"match = all_data.groupby(['matchId'])\ngroup = all_data.groupby(['matchId','groupId','matchType'])\n\n# target feature (max, min)\nagg_col = list(all_data.columns)\nexclude_agg_col = ['Id','matchId','groupId','matchType','maxPlace','numGroups','winPlacePerc']\nfor c in exclude_agg_col:\n    agg_col.remove(c)\n\n# target feature (sum)\nsum_col = ['kills','killPlace','damageDealt','walkDistance','_healthItems']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"match_data = pd.concat([\n    match.size().to_frame('m.players'), \n    match[sum_col].sum().rename(columns=lambda s: 'm.sum.' + s), \n    match[sum_col].max().rename(columns=lambda s: 'm.max.' + s),\n    match[sum_col].mean().rename(columns=lambda s: 'm.mean.' + s)\n    ], axis=1).reset_index()\nmatch_data = pd.merge(match_data, \n    group[sum_col].sum().rename(columns=lambda s: 'sum.' + s).reset_index())\nmatch_data = reduce_mem_usage(match_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"minKills = all_data.sort_values(['matchId','groupId','kills','killPlace']).groupby(\n    ['matchId','groupId','kills']).first().reset_index().copy()\nfor n in np.arange(4):\n    c = 'kills_' + str(n) + '_Place'\n    nKills = (minKills['kills'] == n)\n    minKills.loc[nKills, c] = minKills[nKills].groupby(['matchId'])['killPlace'].rank().values\n    match_data = pd.merge(match_data, minKills[nKills][['matchId','groupId',c]], how='left')\n    #match_data[c].fillna(0, inplace=True)\ndel minKills, nKills\nmatch_data = reduce_mem_usage(match_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"''' group mean, max, min\n'''\nall_data = pd.concat([\n    group.size().to_frame('players'),\n    group.mean(),\n    group[agg_col].max().rename(columns=lambda s: 'max.' + s),\n    group[agg_col].min().rename(columns=lambda s: 'min.' + s),\n    ], axis=1).reset_index()\nall_data = reduce_mem_usage(all_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numcols = all_data.select_dtypes(include='number').columns.values\nnumcols = numcols[numcols != 'winPlacePerc']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"''' match summary, max\n'''\nall_data = pd.merge(all_data, match_data)\ndel match_data\ngc.collect()\n\nall_data['enemy.players'] = all_data['m.players'] - all_data['players']\nfor c in sum_col:\n    #all_data['enemy.' + c] = (all_data['m.sum.' + c] - all_data['sum.' + c]) / all_data['enemy.players']\n    #all_data['p.sum_msum.' + c] = all_data['sum.' + c] / all_data['m.sum.' + c]\n    #all_data['p.max_mmean.' + c] = all_data['max.' + c] / all_data['m.mean.' + c]\n    all_data['p.max_msum.' + c] = all_data['max.' + c] / all_data['m.sum.' + c]\n    all_data['p.max_mmax.' + c] = all_data['max.' + c] / all_data['m.max.' + c]\n    all_data.drop(['m.sum.' + c, 'm.max.' + c], axis=1, inplace=True)\n    \nfillInf(all_data, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"match = all_data.groupby('matchId')\nmatchRank = match[numcols].rank(pct=True).rename(columns=lambda s: 'rank.' + s)\nall_data = reduce_mem_usage(pd.concat([all_data, matchRank], axis=1))\nrank_col = matchRank.columns\ndel matchRank\ngc.collect()\n\n# instead of rank(pct=True, method='dense')\nmatch = all_data.groupby('matchId')\nmatchRank = match[rank_col].max().rename(columns=lambda s: 'max.' + s).reset_index()\nall_data = pd.merge(all_data, matchRank)\nfor c in numcols:\n    all_data['rank.' + c] = all_data['rank.' + c] / all_data['max.rank.' + c]\n    all_data.drop(['max.rank.' + c], axis=1, inplace=True)\ndel matchRank\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"killMinorRank = all_data[['matchId','min.kills','max.killPlace']].copy()\ngroup = killMinorRank.groupby(['matchId','min.kills'])\nkillMinorRank['rank.minor.maxKillPlace'] = group.rank(pct=True).values\nall_data = pd.merge(all_data, killMinorRank)\n\nkillMinorRank = all_data[['matchId','max.kills','min.killPlace']].copy()\ngroup = killMinorRank.groupby(['matchId','max.kills'])\nkillMinorRank['rank.minor.minKillPlace'] = group.rank(pct=True).values\nall_data = pd.merge(all_data, killMinorRank)\n\ndel killMinorRank\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop constant column\nconstant_column = [col for col in all_data.columns if all_data[col].nunique() == 1]\n# print('drop columns:', constant_column)\nall_data.drop(constant_column, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nsolo  <-- solo,solo-fpp,normal-solo,normal-solo-fpp\nduo   <-- duo,duo-fpp,normal-duo,normal-duo-fpp,crashfpp,crashtpp\nsquad <-- squad,squad-fpp,normal-squad,normal-squad-fpp,flarefpp,flaretpp\n'''\nall_data['matchType'] = all_data['matchType'].apply(mapper)\n\nall_data = pd.concat([all_data, pd.get_dummies(all_data['matchType'])], axis=1)\nall_data.drop(['matchType'], axis=1, inplace=True)\n\nall_data['matchId'] = all_data['matchId'].apply(lambda x: int(x,16))\nall_data['groupId'] = all_data['groupId'].apply(lambda x: int(x,16))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"null_cnt = all_data.isnull().sum().sort_values()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = [col for col in all_data.columns if col not in ['Id','matchId','groupId']]\nfor i, t in all_data.loc[:, cols].dtypes.iteritems():\n    if t == object:\n        all_data[i] = pd.factorize(all_data[i])[0]\n\nall_data = reduce_mem_usage(all_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = all_data[all_data['winPlacePerc'].notnull()].reset_index(drop=True)\nX_test = all_data[all_data['winPlacePerc'].isnull()].drop(['winPlacePerc'], axis=1).reset_index(drop=True)\ndel all_data\ngc.collect()\n\nY_train = X_train.pop('winPlacePerc')\nX_test_grp = X_test[['matchId','groupId']].copy()\ntrain_matchId = X_train['matchId']\n\n# drop matchId,groupId\nX_train.drop(['matchId','groupId'], axis=1, inplace=True)\nX_test.drop(['matchId','groupId'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GroupKFold\nfrom sklearn.preprocessing import minmax_scale\nimport lightgbm as lgb\n\nparams={'learning_rate': 0.05,\n        'objective':'mae',\n        'metric':'mae',\n        'num_leaves': 128,\n        'verbose': 1,\n        'random_state':42,\n        'bagging_fraction': 0.7,\n        'feature_fraction': 0.7\n       }\n\nreg = lgb.LGBMRegressor(**params, n_estimators=10000)\nreg.fit(X_train, Y_train)\npred = reg.predict(X_test, num_iteration=reg.best_iteration_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_grp['_nofit.winPlacePerc'] = pred\n\ngroup = X_test_grp.groupby(['matchId'])\nX_test_grp['winPlacePerc'] = pred\nX_test_grp['_rank.winPlacePerc'] = group['winPlacePerc'].rank(method='min')\nX_test = pd.concat([X_test, X_test_grp], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fullgroup = (X_test['numGroups'] == X_test['maxPlace'])\n\n# full group (201366) --> calculate from rank\nsubset = X_test.loc[fullgroup]\nX_test.loc[fullgroup, 'winPlacePerc'] = (subset['_rank.winPlacePerc'].values - 1) / (subset['maxPlace'].values - 1)\n\n# not full group (684872) --> align with maxPlace\nsubset = X_test.loc[~fullgroup]\ngap = 1.0 / (subset['maxPlace'].values - 1)\nnew_perc = np.around(subset['winPlacePerc'].values / gap) * gap  # half&up\nX_test.loc[~fullgroup, 'winPlacePerc'] = new_perc\n\nX_test['winPlacePerc'] = X_test['winPlacePerc'].clip(lower=0,upper=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# edge cases\nX_test.loc[X_test['maxPlace'] == 0, 'winPlacePerc'] = 0\nX_test.loc[X_test['maxPlace'] == 1, 'winPlacePerc'] = 1  # nothing\nX_test.loc[(X_test['maxPlace'] > 1) & (X_test['numGroups'] == 1), 'winPlacePerc'] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/pubg-finish-placement-prediction/test_V2.csv')\ntest['matchId'] = test['matchId'].apply(lambda x: int(x,16))\ntest['groupId'] = test['groupId'].apply(lambda x: int(x,16))\n\nsubmission = pd.merge(test, X_test[['matchId','groupId','winPlacePerc']])\nsubmission = submission[['Id','winPlacePerc']]\nsubmission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}