{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Final Notebook for BIM801 Project**\n## *Kaggle competition / 22 Spring*\n### PUBG Finish Placement Prediction\n- Including feature engineering, model, predict\n- Applied Neural Netwok Model","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport itertools\nimport sys\nimport time\nimport matplotlib.pyplot as plt\nimport seaborn as sns\npd.options.display.float_format = '{:.5g}'.format\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\nimport warnings\nwarnings.filterwarnings(action='ignore')\nfrom tqdm import tqdm \nimport gc\nfrom timeit import default_timer as timer\n\n# Train * Test * Model \nfrom sklearn import metrics\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn import preprocessing\n# FOR MODEL\n# from ultimate.mlp import MLP \nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.callbacks import ModelCheckpoint","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-03T14:23:54.330628Z","iopub.execute_input":"2022-06-03T14:23:54.331541Z","iopub.status.idle":"2022-06-03T14:24:02.157616Z","shell.execute_reply.started":"2022-06-03T14:23:54.33146Z","shell.execute_reply":"2022-06-03T14:24:02.156671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\" UDF for Reduce memory use of dataframe by change data type\n\"\"\"\n# Thanks and credited to GUILLAUME MARTIN\n# https://www.kaggle.com/code/gemartin/load-data-reduce-memory-usage\ndef reduce_mem_usage(df):\n    start_mem = df.memory_usage().sum() / 1024**2\n    for col in df.columns:\n        col_type = df[col].dtype\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() / 1024**2\n#     print('Memory usage of dataframe is {:.2f} MB --> {:.2f} MB (Decreased by {:.1f}%)'.format(\n#         start_mem, end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-06-03T14:24:02.163257Z","iopub.execute_input":"2022-06-03T14:24:02.163895Z","iopub.status.idle":"2022-06-03T14:24:02.17793Z","shell.execute_reply.started":"2022-06-03T14:24:02.163862Z","shell.execute_reply":"2022-06-03T14:24:02.176745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"INPUT_DIR = \"../input/pubg-finish-placement-prediction/\"","metadata":{"execution":{"iopub.status.busy":"2022-06-03T14:24:02.17917Z","iopub.execute_input":"2022-06-03T14:24:02.179657Z","iopub.status.idle":"2022-06-03T14:24:02.208214Z","shell.execute_reply.started":"2022-06-03T14:24:02.179609Z","shell.execute_reply":"2022-06-03T14:24:02.207147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\" UDF for transform inf values into certain value\n\"\"\"\ndef fillInf(df, val):\n    numcols = df.select_dtypes(include='number').columns\n    cols = numcols[numcols != 'winPlacePerc']\n    df[df == np.Inf] = np.NaN\n    df[df == np.NINF] = np.NaN\n    for c in cols: df[c].fillna(val, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-03T14:24:02.210971Z","iopub.execute_input":"2022-06-03T14:24:02.211493Z","iopub.status.idle":"2022-06-03T14:24:02.223222Z","shell.execute_reply.started":"2022-06-03T14:24:02.211445Z","shell.execute_reply":"2022-06-03T14:24:02.222141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\" UDF for generate Group aggregated features \n\"\"\"\n# Need to predict the order of places for groups within each match.\n# Train on group-level instead of the user-level\ndef grouping(df, agg_col, sum_col):\n    group = df.groupby(['matchId','groupId','matchType'])\n    # group size, mean, sum, max, min\n    gSize = group.size().to_frame('gSize') # players\n    gMean = group.mean()\n    gSum = group[sum_col].sum().rename(columns=lambda s: '_gSum.' + s)\n    gMax = group[agg_col].max().rename(columns=lambda s: '_gMax.' + s)\n    gMin = group[agg_col].min().rename(columns=lambda s: '_gMin.' + s)\n    return pd.concat([gSize, gMean, gSum, gMax, gMin], axis=1).reset_index()","metadata":{"execution":{"iopub.status.busy":"2022-06-03T14:24:02.224666Z","iopub.execute_input":"2022-06-03T14:24:02.225321Z","iopub.status.idle":"2022-06-03T14:24:02.237759Z","shell.execute_reply.started":"2022-06-03T14:24:02.225284Z","shell.execute_reply":"2022-06-03T14:24:02.236451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Engineering","metadata":{}},{"cell_type":"code","source":"def feature_engineering(is_train=True):\n    if is_train:\n        print(\"processing TRAIN set\")\n        df = reduce_mem_usage(pd.read_csv(INPUT_DIR + 'train_V2.csv'))\n        # Take the matches that have more than 1 player\n        df = df[df['maxPlace'] > 1]\n        # Anormal data row drop\n        df.drop(df.query('rideDistance == 0 and roadKills > 0').index, inplace=True)\n    else:\n        print(\"processing TEST set\")\n        df = reduce_mem_usage(pd.read_csv(INPUT_DIR + 'test_V2.csv'))\n    \n# LOG Transform features\n    log_target = ['assists', 'boosts', 'DBNOs', 'headshotKills', 'heals', \n              'kills', 'revives', 'roadKills', 'teamKills', 'vehicleDestroys', \n              'weaponsAcquired', 'walkDistance']\n    for col in log_target:\n        df[col] = df[col].apply(lambda x: np.log1p(x))\n    \n# Rank to percentile\n    match = df.groupby('matchId')\n    df['killPlacePerc'] = match['kills'].rank(pct=True).values\n    df['walkDistancePerc'] = match['walkDistance'].rank(pct=True).values\n    df['damageDealtPerc'] = match['damageDealt'].rank(pct=True).values\n    del match\n    gc.collect()\n\n# Drop external point features\n    df.drop(['rankPoints','killPoints','winPoints'], axis=1, inplace=True)\n\n# Linear combination features\n    print(\"i am doing lcf\")\n    df['_totalDistance'] = (df['rideDistance']*0.5 + df[\"walkDistance\"]*0.2 + df[\"swimDistance\"]*0.3) / df['matchDuration']\n    df['_healthItems'] = df['heals'] + df['boosts']\n    df['_teamWork'] = df['revives'] + df['assists']\n    df['_over1km'] = df['longestKill'].apply(lambda x: 1 if x > 1000 else 0)\n    df['_headshotKillRate'] = df['headshotKills'] / df['kills']\n    df['_killsOverWalkDistance'] = df['kills'] / df['walkDistance']\n    df['_killsOverDistance'] = df['kills'] / df['_totalDistance']\n    df['_killPlacePerc'] = df['killPlace'] / df['maxPlace']\n#     df['killStreakrate'] = df['killStreaks'] / df['kills']\n#     df['headshotKills_over_kills'] = df['headshotKills'] / df['kills']\n#     df['distance_over_weapons'] = df['_totalDistance'] / df['weaponsAcquired']\n#     df['walkDistance_over_heals'] = df['walkDistance'] / df['heals']\n#     df[\"skill\"] = df[\"headshotKills\"] + df[\"roadKills\"]\n    fillInf(df, 0)\n    \n    \n    y = None\n    target = 'winPlacePerc'\n    \n# Grouping features (size, mean, max, min)\n    sum_col = ['kills','assists','teamKills','revives','damageDealt','walkDistance', '_totalDistance', '_healthItems']\n    agg_col = list(df.columns)\n    exclude_agg_col = ['Id','matchId','groupId','matchType','matchDuration','maxPlace','numGroups']\n    for c in exclude_agg_col:\n        agg_col.remove(c)\n        \n    if is_train:\n        y = pd.DataFrame(np.array(df.groupby(['matchId','groupId'])[target].agg('mean'), dtype=np.float64))\n        df.drop(target, axis=1, inplace=True)\n        agg_col.remove(target)\n    \n    df = reduce_mem_usage(grouping(df, agg_col, sum_col))\n    for c in sum_col:\n        df['_perc.gMean_gMax.' + c] = df[c] / df['_gMax.' + c]\n        \n# Match feature - NumCols\n    numcols = df.select_dtypes(include='number').columns.values\n    numcols = numcols[numcols != target]\n    cols = np.r_[numcols,['matchId']]   \n    \n    # Match Rank\n    match = df[cols].groupby('matchId')\n    matchRank = match.rank(pct=True).rename(columns=lambda s: '_rank.' + s)\n    df = reduce_mem_usage(pd.concat([df, matchRank], axis=1))\n    del matchRank\n    gc.collect()\n\n    # Match Sum\n    cols = np.r_[agg_col,['matchId','gSize']]\n    match = df[cols].groupby('matchId')\n    matchSum = match.sum().rename(columns=lambda s: '_mSum.' + s).reset_index()\n    df = reduce_mem_usage(pd.merge(df, matchSum))\n    del matchSum\n    gc.collect()\n    \n    # Ranking of Kills & killPlace in each match\n    minKills = df.sort_values(['matchId','groupId','kills','killPlace']).groupby(['matchId','groupId','kills']).first().reset_index().copy()\n    for n in np.arange(4):\n        c = 'kills_' + str(n) + '_Place'\n        nKills = (minKills['kills'] == n)\n        minKills.loc[nKills, c] = minKills[nKills].groupby(['matchId'])['killPlace'].rank().values\n        df = pd.merge(df, minKills[nKills][['matchId','groupId',c]], how='left')\n        df[c].fillna(0, inplace=True)\n    df = reduce_mem_usage(df)\n    del minKills, nKills\n\n    \n# Enemy info\n    df['_enemy.sum.gSize'] = df['_mSum.gSize'] - df['gSize'] # 해당 매치에서 우리팀을 뺀 플레이어 수\n    df['_enemy.kills'] = (df['_mSum.kills'] - df['_gSum.kills']) / df['_enemy.sum.gSize'] # 해당 매치에서 에너미 한 명이 평균적으로 몇 명을 죽였는지\n    df['_enemy.damageDealt'] = (df['_mSum.damageDealt'] - df['_gSum.damageDealt']) / df['_enemy.sum.gSize'] # 해당 매치에서 에너미 한 명이 평균적으로 넣은 딜량\n    for c in agg_col:\n        df['_perc.gMax_mSum.' + c] = df['_gMax.' + c] / df['_mSum.' + c]  # 그룹 맥스 / 매치 총량  (for agg_col)\n        if c in sum_col:\n            df['_perc.gSum_mSum.' + c] = df['_gSum.' + c] / df['_mSum.' + c]  #그룹 총 / 매치 총량 (for sum_col)\n    fillInf(df, 0)\n    \n# Match Max\n    matchMax = match.max().rename(columns=lambda s: '_mMax.' + s).reset_index()\n    df = reduce_mem_usage(pd.merge(df, matchMax))\n    del matchMax\n    gc.collect()\n    for c in agg_col:\n        df['_perc.gMax_mMax.' + c] = df['_gMax.' + c] / df['_mMax.' + c] # 그룹 맥스 / 매치 맥스 (for agg_col)\n        df.drop(['_mMax.' + c], axis=1, inplace=True)\n    fillInf(df, 0)\n    \n# Rank of Top / bottom player of each group in match\n    killBottomPlayer = df[['matchId','_gMin.kills','_gMax.killPlace']].copy()\n    group = killBottomPlayer.groupby(['matchId','_gMin.kills'])\n    killBottomPlayer['_rank.bottomPlayer'] = group.rank().values\n    df = pd.merge(df, killBottomPlayer)\n\n    killTopPlayer = df[['matchId','_gMax.kills','_gMin.killPlace']].copy()\n    group = killTopPlayer.groupby(['matchId','_gMax.kills'])\n    killTopPlayer['_rank.topPlayer'] = group.rank().values\n    df = pd.merge(df, killTopPlayer)\n\n    del killBottomPlayer, killTopPlayer\n    gc.collect()\n\n# killPlace rank of group and kills\n# MatchType mapping\n    mapper = lambda x: 'solo' if ('solo' in x) else 'duo' if ('duo' in x) or ('crash' in x) else 'squad'\n    df['matchTypeCat'] = df['matchType'].map(mapper)\n    \n# Drop constant feature\n    const_column = [col for col in df.columns if df[col].nunique() == 1]\n\n# Label Encoding\n    cols = [col for col in df.columns if col not in ['Id','matchId','groupId']]\n    for i, t in df.loc[:, cols].dtypes.iteritems():\n        if t == object:\n            df[i] = pd.factorize(df[i])[0]\n    print('Final df shape', df.shape)\n    return df, y, const_column","metadata":{"execution":{"iopub.status.busy":"2022-06-03T14:24:02.239759Z","iopub.execute_input":"2022-06-03T14:24:02.240151Z","iopub.status.idle":"2022-06-03T14:24:02.27951Z","shell.execute_reply.started":"2022-06-03T14:24:02.240115Z","shell.execute_reply":"2022-06-03T14:24:02.278604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, y, trn_cc = feature_engineering(True)\nX_test, _, tst_cc = feature_engineering(False)","metadata":{"execution":{"iopub.status.busy":"2022-06-03T14:24:02.280994Z","iopub.execute_input":"2022-06-03T14:24:02.2816Z","iopub.status.idle":"2022-06-03T14:36:33.334409Z","shell.execute_reply.started":"2022-06-03T14:24:02.281539Z","shell.execute_reply":"2022-06-03T14:36:33.333229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"drop_cols = ['matchId', 'groupId', '_min._over1km']\nX_train.drop(drop_cols, axis=1, inplace=True)\nX_test.drop(drop_cols, axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-03T14:39:28.649064Z","iopub.execute_input":"2022-06-03T14:39:28.649443Z","iopub.status.idle":"2022-06-03T14:39:30.264588Z","shell.execute_reply.started":"2022-06-03T14:39:28.649413Z","shell.execute_reply":"2022-06-03T14:39:30.263517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scaler - Train\nscaler = preprocessing.MinMaxScaler(feature_range=(-1, 1), copy=False).fit(X_train)\nX_train_scaled = scaler.transform(X_train)\ny = y * 2 - 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scaler - Test\nX_test_scaled = scaler.transform(X_test)\nnp.clip(X_test_scaled, out=X_test_scaled, a_min=-1, a_max=1)","metadata":{"execution":{"iopub.status.busy":"2022-06-03T14:40:47.328271Z","iopub.execute_input":"2022-06-03T14:40:47.328772Z","iopub.status.idle":"2022-06-03T14:40:47.354731Z","shell.execute_reply.started":"2022-06-03T14:40:47.328731Z","shell.execute_reply":"2022-06-03T14:40:47.353586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# create NN_model\nfCount = X_train.shape[1]\nNN_model = Sequential()\nNN_model.add(Dense(fCount,  input_dim = fCount, activation='relu'))\nNN_model.add(Dense(fCount*2, activation='relu'))\nNN_model.add(Dense(fCount*2, activation='relu'))\nNN_model.add(Dense(fCount*2, activation='relu'))\nNN_model.add(Dense(fCount*2, activation='relu'))\n\n# output Layer\nNN_model.add(Dense(1, activation='linear'))\n\n# Compile the network :\nNN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\nNN_model.summary()\n\ncheckpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \ncheckpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\ncallbacks_list = [checkpoint]","metadata":{"execution":{"iopub.status.busy":"2022-06-03T14:41:00.593822Z","iopub.execute_input":"2022-06-03T14:41:00.594314Z","iopub.status.idle":"2022-06-03T14:41:00.831965Z","shell.execute_reply.started":"2022-06-03T14:41:00.594273Z","shell.execute_reply":"2022-06-03T14:41:00.830813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"%%time\nhistory = NN_model.fit(x=X_train_scaled, y=y, batch_size=1000,\n             epochs=40, verbose=1, callbacks=callbacks_list,\n             validation_split=0.15, validation_data=None, shuffle=True,\n             class_weight=None, sample_weight=None, initial_epoch=0,\n             steps_per_epoch=None, validation_steps=None)\ndel X_train, y\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-06-03T14:41:31.263523Z","iopub.execute_input":"2022-06-03T14:41:31.263934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# list all data in history\nprint(history.history.keys())\n# summarize history for mean_absolute_error\nplt.plot(history.history['mean_absolute_error'])\nplt.plot(history.history['val_mean_absolute_error'])\nplt.title('model mean_absolute_error')\nplt.ylabel('MAE')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predict","metadata":{}},{"cell_type":"code","source":"%%time\npred = NN_model.predict(X_test_scaled)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = pred.reshape(-1)\npred = (pred + 1) / 2","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test_id['winPlacePerc'] = pred\n\ngroup = X_test_id.groupby(['matchId'])\n# X_test_id['winPlacePerc'] = pred\nX_test_id['_rank.winPlacePerc'] = group['winPlacePerc'].rank(method='min')\nX_test = pd.concat([X_test, X_test_id], axis=1)\n\n# sub_match = X_test_id[['matchId','_rank.winPlacePerc']].groupby(['matchId']) # for what???\nsub_group = group.count().reset_index()['matchId'].to_frame()\n\nX_test = pd.merge(X_test, sub_group)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Post processing \n### WinPlacePerc for adjust by the scoring rule of the winPlacePerc","metadata":{}},{"cell_type":"code","source":"# wpp 값을 더 정확한 (매치 상황에서 나올 수 있는) 값으로 보정해주는 과정\n\nfullgroup = (X_test['numGroups'] == X_test['maxPlace'])\n\n# full group (201,366 개) --> calculate from rank\nsubset = X_test.loc[fullgroup]\nX_test.loc[fullgroup, 'winPlacePerc'] = (subset['_rank.winPlacePerc'].values - 1) / (subset['maxPlace'].values - 1)\n\n# not full group (684,872 개) --> align with maxPlace\nsubset = X_test.loc[~fullgroup]\ngap = 1.0 / (subset['maxPlace'].values - 1)\nnew_perc = np.around(subset['winPlacePerc'].values / gap) * gap  # half&up\nX_test.loc[~fullgroup, 'winPlacePerc'] = new_perc","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 위의 작업이 잘 작동하는지 확인 \nX_test.loc[~fullgroup, '_pred.winPlace'] = np.around(X_test.loc[~fullgroup, 'winPlacePerc'].values / gap) + 1\nX_test.loc[~fullgroup & (X_test['matchId'] == '000b598b79aa5e'),\n           ['matchId','groupId','winPlacePerc','maxPlace','numGroups','_pred.winPlace','_rank.winPlacePerc']\n          ].sort_values(['matchId','_pred.winPlace'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# edge cases\nX_test.loc[X_test['maxPlace'] == 0, 'winPlacePerc'] = 0\nX_test.loc[X_test['maxPlace'] == 1, 'winPlacePerc'] = 1  # nothing\nX_test.loc[(X_test['maxPlace'] > 1) & (X_test['numGroups'] == 1), 'winPlacePerc'] = 0\nX_test['winPlacePerc'].describe()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv('../input/test_V2.csv')\n\nsubmission = pd.merge(test, X_test[['matchId','groupId','winPlacePerc']])\nsubmission = submission[['Id','winPlacePerc']]\nsub_file_name = \"nn_0609_ep50\"\nsubmission.to_csv(\"../build/{}.csv\".format(sub_file_name), index=False)","metadata":{},"execution_count":null,"outputs":[]}]}