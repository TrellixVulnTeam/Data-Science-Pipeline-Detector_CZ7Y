{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The real reason behind doing all the hard work in the two previous notebooks is to actually get better results. Otherwise whats the point in writing tons of fancy code that does nothing. I attempted this competition before and got quite ordinnary results. Now in this notebook lets try to improve our score with all the fancy code that we learnt. Now Due to a bug in kaggle kernels we will use keras to train a models. In future we will also demonstrate GCP related operation where we will use distributed training."},{"metadata":{},"cell_type":"markdown","source":"Thank fully the dataset is already split into test and train so there is not much effor required in that part. So lets take a look at the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%bash\ncd ../input/pubg-finish-placement-prediction\nls","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dataset is huge. This makes it challenging for using large datasets in memory with pandas. Although we have plenty of ram but still will use it cautiously and so we will write a very simple apache beam job to pre process the data and write it down to at location as the eval set. So we have installed beam. Beam is an unified framework for defining batch and stream processing pipelines and it can run on top of a variety for backends. So using this simple pipeline we will create our eval set instead of using in memory pandas."},{"metadata":{},"cell_type":"markdown","source":"Since we have plenty of data about 4 million plenty , it won't hurt us much to take out a few datapoints randomly out of the train set. But this split has to be reproducable to get consistent results. In the previous notebooks we discussed about a technique that would randomly split the data yet make the split reproducible. In the previous dataset we used a random seed but here we will hash the data to generate random numbers. So lets get to work and make our train_test_split method"},{"metadata":{},"cell_type":"markdown","source":"# Apache beam approach to train test split"},{"metadata":{},"cell_type":"markdown","source":"In order to get a repeatable set of train and evaluation data which is randomly distributed we cannot rely on a random seed. So we use the dataset itself to generate the seed. The splitting algorithm calculates a numeric hash from an unused column in the dataset like the match ID. Then we take the absolute value of this hash and do a modulus with a values like 10000. Depending on the size of the dataset and the level of granularity required for the splits we can increase this number to account for more granularity on large numbers and dataset. Now the remainder is compared with a threshold value which is typically a float like 0.2 multiplied by the number we have selected. Hence if the remainder is less than the threshold it goes to the eval set , else it goes to the train set."},{"metadata":{},"cell_type":"markdown","source":"Our beam pipeline exploits this algorithm. It reads the main train file and after this it , splits the pipe into two p collections. These are then separately written down into 2 separate files. This method might seem more time consuming as compared to pandas train test split. But this method allows us for a distributed approach and hence we can even perform these preprocessing techniques of huge datasets without compromising efficient system memory usage , which is not always possible in case of pandas"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%bash\npip3 install apache-beam","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Defining Pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"import apache_beam as beam\n\ndef eval_filter(data):\n    eval_split = 0.1\n    if(data != None and 'Id' not in data):\n        if(abs(hash(data[0])) % 1000000 < (1000000 * eval_split)):\n            return True\n        else:\n            return False\n    else:\n        return False\n    \n\ndef train_filter(data):\n    eval_split = 0.1\n    if(data != None and 'Id' not in data):\n        if(abs(hash(data[0])) % 1000000 >= (1000000 * eval_split)):\n            return True\n        else:\n            return False\n    else:\n        return False\n       \n\ncols = ['Id', 'groupId', 'matchId', 'assists', 'boosts', 'damageDealt', 'DBNOs',\n       'headshotKills', 'heals', 'killPlace', 'killPoints', 'kills',\n       'killStreaks', 'longestKill', 'matchDuration', 'matchType', 'maxPlace',\n       'numGroups', 'rankPoints', 'revives', 'rideDistance', 'roadKills',\n       'swimDistance', 'teamKills', 'vehicleDestroys', 'walkDistance',\n       'weaponsAcquired', 'winPoints', 'winPlacePerc']\n\nclass SplitWords(beam.DoFn):\n    def __init__(self, delimiter=','):\n        self.delimiter = delimiter\n\n    def process(self, text):\n        yield text.split(self.delimiter)\n        \nclass ConvertToCsv(beam.DoFn):\n    def process(self, text):\n        yield ','.join(text)\n      \n\n    \nwith beam.Pipeline() as pipeline:\n    out = (pipeline\n           | \"Read Data\" >> beam.io.ReadFromText('../input/pubg-finish-placement-prediction/train_V2.csv')\n           | \"Split to array\" >> beam.ParDo(SplitWords(','))\n    )\n    \n    train = (out\n             | \"Filter train\" >> beam.Filter(train_filter)\n             | \"convert to array train\" >> beam.ParDo(ConvertToCsv())\n             | \"Write train\" >> beam.io.WriteToText(\n                     header = ','.join(cols),\n                    file_path_prefix = 'train',\n                    file_name_suffix = '.csv',\n                    shard_name_template = ''\n                )\n    )\n    \n    eval = (out\n             | \"Filter eval\" >> beam.Filter(eval_filter)\n             | \"convert to array eval\" >> beam.ParDo(ConvertToCsv())\n             | \"Write eval\" >> beam.io.WriteToText(\n                     header = ','.join(cols),\n                    file_path_prefix = 'eval',\n                    file_name_suffix = '.csv',\n                    shard_name_template = ''\n                )\n    )\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our beam pipeline will generate two datasets train and eval . We will then use a data generator to iterate over them in a memory efficiennt manner"},{"metadata":{},"cell_type":"markdown","source":"# Generating feature columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for item in x_train:\n    print({item : \"{} : {}\".format(x_train[item].unique(),len(x_train[item].unique()))})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def gennerate_feature_columns():\n    return [\n        tf.feature_column.numeric_column('assists'),\n        tf.feature_column.numeric_column('boosts'),\n        tf.feature_column.numeric_column('damageDealt'),\n        #tf.feature_column.bucketized_column(tf.feature_column.numeric_column('DBNOs'),[0,1,2,3,5,10,15,20,25,30]),\n        tf.feature_column.numeric_column('headshotKills'),\n        tf.feature_column.numeric_column('heals'),\n        tf.feature_column.bucketized_column(tf.feature_column.numeric_column('killPlace'),[0,10,20]),\n        tf.feature_column.bucketized_column(tf.feature_column.numeric_column('killPoints'),[50,750,1000,1100]),\n        tf.feature_column.bucketized_column(tf.feature_column.numeric_column('killStreaks'),[1,3,5,10]),\n        #tf.feature_column.bucketized_column(tf.feature_column.numeric_column('longestKill'),[0,20,40,60,80,100,150,200]),\n        tf.feature_column.numeric_column('matchDuration'),\n        #tf.feature_column.indicator_column(tf.feature_column.categorical_column_with_vocabulary_list('matchType',['squad-fpp','duo-fpp','squad','solo-fpp','duo'])),\n        #tf.feature_column.bucketized_column(tf.feature_column.numeric_column('numGroups'),[0,10,20,22,24,26,28,30,40,42,44,46,48,50,60,70,80,82,84,86,88,90,92,94,96,98,100]),\n        tf.feature_column.bucketized_column(tf.feature_column.numeric_column('revives'),[0,5,10,15,20,25,30,35,40,45,50]),\n        tf.feature_column.bucketized_column(tf.feature_column.numeric_column('walkDistance'),[0,1000,3000]),\n        tf.feature_column.bucketized_column(tf.feature_column.numeric_column('weaponsAcquired'),[0,5,10,15,20]),\n    ]  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These feature columns as defined in previous notebooks will map the corresponding pandas column with the required preprocessed format and convert it into a tensor to feed to the model. This allows us to do things like one hot encoding as part of the model graph itself , thereby reducing our stress on system resources and makinng it part of the automated training pipeline."},{"metadata":{},"cell_type":"markdown","source":"# Model Definition"},{"metadata":{"trusted":true},"cell_type":"code","source":"# def model_fn(features, labels, mode):\n#     model = tf.keras.Sequential([\n#       tf.keras.layers.DenseFeatures(gennerate_feature_columns()),\n#       tf.keras.layers.Dense(1,activation = 'relu'),\n#       tf.keras.layers.Dense(1,activation = 'softmax')\n#     ])\n    \n#     logits = model(features, training=False)\n    \n#     if mode == tf.estimator.ModeKeys.PREDICT:\n#         predictions = {'logits': logits}\n#         return tf.estimator.EstimatorSpec(labels=labels, predictions=predictions)\n    \n#     optimizer = tf.compat.v1.train.AdamOptimizer()\n#     loss = tf.keras.losses.MAE(labels, logits)\n    \n#     if mode == tf.estimator.ModeKeys.EVAL:\n#         return tf.estimator.EstimatorSpec(mode = mode, loss=loss)\n\n#     return tf.estimator.EstimatorSpec(\n#           mode=mode,\n#           loss=loss,\n#           train_op=optimizer.minimize(\n#           loss, tf.compat.v1.train.get_or_create_global_step()))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above code is commented out because it doesnot really demonstrate the capabilities of the estimator api and also due to some bug proper logs are not printed in kaggle. So for the time being we will use keras to train our models , but keep using tensor flow as much as possible. "},{"metadata":{},"cell_type":"markdown","source":"# Dataset Generation"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_batch_size = 100\neval_batch_size = 10\n\ntrain_dataset = tf.data.experimental.make_csv_dataset(\n    ['train.csv'],\n    train_batch_size,\n    label_name='winPlacePerc',\n    num_epochs=3)\n\neval_dataset = tf.data.experimental.make_csv_dataset(\n    ['eval.csv'],\n    train_batch_size,\n    label_name='winPlacePerc',\n    num_epochs=3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above code will generate a dataset which is loaded lazily while training. This dataset will be fed directly to the keras model."},{"metadata":{},"cell_type":"markdown","source":"# Model Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"# loss_object = tf.keras.losses.MeanAbsoluteError()\n\n# import time\n\n# def loss(model, x, y, training):\n#     y_ = model(x, training=training)\n#     return loss_object(y_true=y, y_pred=y_)\n\n# def grad(model, inputs, targets):\n#     with tf.GradientTape() as tape:\n#         loss_value = loss(model, inputs, targets, training=True)\n#     return loss_value, tape.gradient(loss_value, model.trainable_variables)\n\n# optimizer = tf.keras.optimizers.Adam(learning_rate=0.1)\n\n# model = tf.keras.Sequential([\n#       tf.keras.layers.DenseFeatures(gennerate_feature_columns()),\n#       tf.keras.layers.Dense(100,activation = 'relu'),\n#       tf.keras.layers.Dense(10,activation = 'relu'),\n#       tf.keras.layers.Dense(1,activation = 'softmax')\n#     ])\n\n# train_loss_results = []\n# train_accuracy_results = []\n\n# num_epochs = 1\n# start_time = 0\n# counnt = 1\n# for epoch in range(num_epochs):\n#     epoch_loss_avg = tf.keras.metrics.Mean()\n#     epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n#     start_time = time.time()\n#     print(\"Epoch:- \",counnt)\n#     for x, y in train_dataset:\n#         loss_value, grads = grad(model, x, y)\n#         optimizer.apply_gradients(zip(grads, model.trainable_variables))\n#         # Track progress\n#         epoch_loss_avg.update_state(loss_value)  # Add current batch loss\n#         # Compare predicted label to actual label\n#         # training=True is needed only if there are layers with different\n#         # behavior during training versus inference (e.g. Dropout).\n#         epoch_accuracy.update_state(tf.reshape(y,(100,1)), model(x, training=True))\n#         print(counnt)\n#         counnt+=1\n#      # End epoch\n    \n#     train_loss_results.append(epoch_loss_avg.result())\n#     train_accuracy_results.append(epoch_accuracy.result())\n#     print(\"Epoch {:03d}: Loss: {:.3f}, Accuracy: {:.3%}\".format(epoch,\n#                                                                 epoch_loss_avg.result(),\n#                                                                 epoch_accuracy.result()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above code is a custom training loop which we can use to write a custom training job . Lets use the vanilla keras trainer for the timebeing"},{"metadata":{"trusted":true},"cell_type":"code","source":"next(iter(train_dataset))[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = tf.keras.Sequential([\n      tf.keras.layers.DenseFeatures(gennerate_feature_columns()),\n      tf.keras.layers.Dense(2048,activation = tf.keras.activations.relu),\n     tf.keras.layers.Dense(1024,activation = tf.keras.activations.relu),\n    tf.keras.layers.Dropout(0.2),\n     tf.keras.layers.Dense(2048,activation = tf.keras.activations.relu),\n     tf.keras.layers.Dense(1024,activation = tf.keras.activations.relu),\n    tf.keras.layers.Dropout(0.2),\n     tf.keras.layers.Dense(2048,activation = tf.keras.activations.relu),\n     tf.keras.layers.Dense(1024,activation = tf.keras.activations.relu),\n    tf.keras.layers.Dropout(0.2),\n     tf.keras.layers.Dense(2048,activation = tf.keras.activations.relu),\n     tf.keras.layers.Dense(1024,activation = tf.keras.activations.relu),\n    tf.keras.layers.Dropout(0.2),\n     tf.keras.layers.Dense(2048,activation = tf.keras.activations.relu),\n     tf.keras.layers.Dense(1024,activation = tf.keras.activations.relu),\n    tf.keras.layers.Dense(1,activation = 'softmax')\n    ])\n\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = 0.01), loss = tf.keras.losses.mean_squared_error, metrics=[\"acc\"])\nmodel.fit(train_dataset ,epochs=3,verbose = 1,validation_data = eval_dataset,workers=-1,batch_size = 100)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see that we no longer have to tweak the input layer every time we change the shape of the input data and it gives us a lot more freedom. Also when we are dealing with a large number of colummns which contain different types of data , it becomes difficult to calculate te exact input shape of the models. Thus by using feature columns we can very easily elliminate all these problems."},{"metadata":{},"cell_type":"markdown","source":"We can play around with the model architecture to improve the accuracy a lot more , and we have achieved better results than befor. But the intension of this notebook is to demonstrate how we can efficiently use our system resources to train our models. These techniques will help us later to handle even larger datasets "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}