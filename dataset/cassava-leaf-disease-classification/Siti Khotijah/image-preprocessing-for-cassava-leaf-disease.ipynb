{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# What is Computer Vision?\n\n<img src=\"https://analyticsinsight.b-cdn.net/wp-content/uploads/2020/07/Computer-Vision.jpg\" width=\"400\">\n\nComputer vision is an interdisciplinary scientific field that deals with how computers can gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to understand and automate tasks that the human visual system can do[[1](https://en.wikipedia.org/wiki/Computer_vision)]. Computer vision tasks include methods for acquiring, processing, analyzing and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g. in the forms of decisions ( Reinhard Klette (2014))\n\n\n","metadata":{}},{"cell_type":"markdown","source":"# **What is image processing?**\n\nIn machine learning projects in general, you usually go through a data preprocessing or cleaning step. The goal of this step is to make your data ready for the ML model to make it easier to analyze and process computationally, as it is with images. Based on the problem you’re solving and the dataset in hand, there’s some data massaging required before you feed your images to the ML model.\n\nImage processing could be simple tasks like image resizing.In order to feed a dataset of images to a convolutional network, they must all be the same size. Other processing tasks can take place like geometric and color transformation or converting color to grayscale and many more.","metadata":{}},{"cell_type":"markdown","source":"# **Why image preprocessing?**\n\nThe acquired data are usually messy and come from different sources. To feed them to the Machine Learning model (or neural network), they need to be standardized and cleaned up.\n","metadata":{}},{"cell_type":"markdown","source":"# **Data preprocessing techniques might include:**\n\nin certain problems you’ll find it useful to lose unnecessary information from your images to reduce space or computational complexity.\n\nFor example, converting your colored images to grayscale images. This is because in many objects, color isn’t necessary to recognize and interpret an image. Grayscale can be good enough for recognizing certain objects. Because color images contain more information than black and white images, they can add unnecessary complexity and take up more space in memory (Remember how color images are represented in three channels, which means that converting it to grayscale reduces the number of pixels that need to be processed).","metadata":{}},{"cell_type":"markdown","source":"Practical example","metadata":{}},{"cell_type":"code","source":"import os\nimport json\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sn\nimport matplotlib.pyplot as plt\nimport cv2\nimport albumentations as A\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('fivethirtyeight')\nsns.set_style('whitegrid')\n\n","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Cassava_dir = \"../input/cassava-leaf-disease-classification/\"\nwith open(os.path.join(Cassava_dir, \"label_num_to_disease_map.json\")) as file:\n    map_classes = json.loads(file.read())\n    \nprint(json.dumps(map_classes, indent=4))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_files = os.listdir(os.path.join(Cassava_dir, \"train_images\"))\nprint(f\"Number of train images: {len(input_files)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_shapes = {}\nfor image_name in os.listdir(os.path.join(Cassava_dir, \"train_images\"))[:300]:\n    image = cv2.imread(os.path.join(Cassava_dir, \"train_images\", image_name))\n    img_shapes[image.shape] = img_shapes.get(image.shape, 0) + 1\n\nprint(img_shapes)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(os.path.join(Cassava_dir, \"train.csv\"))\n\ndf[\"class_name\"] = df[\"label\"].astype(str).map(map_classes)\n\ndf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Read the image from the disk and convert it from the BGR color space to the RGB color space\nFor historical reasons, OpenCV reads an image in BGR format (so color channels of the image have the following order: Blue, Green, Red). Albumentations uses the most common and popular RGB image format. So when using OpenCV, we need to convert the image format to RGB explicitly.","metadata":{}},{"cell_type":"code","source":"def show_image(image_ids, labels):\n    plt.figure(figsize=(16, 12))\n    \n    for ind, (image_id, label) in enumerate(zip(image_ids, labels)):\n        plt.subplot(3, 3, ind + 1)\n        image = cv2.imread(os.path.join(Cassava_dir, \"train_images\", image_id))\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        plt.imshow(image)\n        plt.title(f\"Class: {label}\", fontsize=12)\n        plt.axis(\"off\")\n    \n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tmp_df = df.sample(9)\nimage_ids = tmp_df[\"image_id\"].values\nlabels = tmp_df[\"class_name\"].values\n\nshow_image(image_ids, labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n# 1. Image Transform\n\nThis work tried to transform images from one color-space to another. The transformation you can see below:\n* mask\n* segment\n* deskew\n* gray\n* thresh\n* rnoise\n* canny\n* sharpen\n\nAccording to [2] while the color image can be treated arbitrary vector value functions or collections of independent bands, it usually makes sense to think about them as highly correlated signals with strong connections to the image formation process, sensor design, and Human perception. Consider example brightening picture by adding a constant value to all three channels. In fact, adding the same value to each color channel not only increases the apparent intensity of each pixel, but it cal also affects the picture hue and saturation.\n\n","metadata":{}},{"cell_type":"code","source":"#masking function\ndef create_mask_for_image(image):\n    image_hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n\n    lower_hsv = np.array([0,0,250])\n    upper_hsv = np.array([250,255,255])\n    \n    mask = cv2.inRange(image_hsv, lower_hsv, upper_hsv)\n    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11,11))\n    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n    return mask\n\n#image  deskew function\ndef  deskew_image(image):\n    mask = create_mask_for_image(image)\n    output = cv2.bitwise_and(image, image, mask = mask)\n    return output/255\n\n#image  gray  function\ndef  gray_image(image):\n    #mask = create_mask_for_image(image)\n    output = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n    return output/255\n\n#image  thresh  function\ndef  thresh_image(image):\n    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n    output = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV) #+cv.THRESH_OTSU)\n    return output\n\n\n#image  rnoise  function\ndef  rnoise_image(image):\n    mask = create_mask_for_image(image)\n    output = cv2.bitwise_and(image, image, mask = mask)\n    return output/255\n\n#image  dilate  function\ndef  dilate_image(image):\n    mask = create_mask_for_image(image)\n    output = cv2.bitwise_and(image, image, mask = mask)\n    return output/255\n\n\n#image  erode  function\ndef  erode_image(image):\n    mask = create_mask_for_image(image)\n    output = cv2.bitwise_and(image, image, mask = mask)\n    return output/255\n\n\n\n#image  opening  function\ndef  opening_image(image):\n    mask = create_mask_for_image(image)\n    output = cv2.bitwise_and(image, image, mask = mask)\n    return output/255\n\n#image canny function\ndef  canny_image(image):\n    mask = create_mask_for_image(image)\n    output = cv2.bitwise_and(image, image, mask = mask)\n    return output/255\n\n\n#image segmentation function\ndef segment_image(image):\n    mask = create_mask_for_image(image)\n    output = cv2.bitwise_and(image, image, mask = mask)\n    return output/255\n\n\n#sharpen the image\ndef sharpen_image(image):\n    image_blurred = cv2.GaussianBlur(image, (0, 0), 3)\n    image_sharp = cv2.addWeighted(image, 1.5, image_blurred, -0.5, 0)\n    return image_sharp\n\n\n# function to get an image\ndef read_img(image_id, labels):\n    plt.figure(figsize=(16, 12))\n    img = cv2.imread(os.path.join(Cassava_dir, \"train_images\", image_id))\n    #convert image to array\n    #img = image.img_to_array(img)\n    return img","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"SHOW SAMPLE IMAGES","metadata":{}},{"cell_type":"code","source":"nb_rows = 2\nnb_cols = 5\nfig, axs = plt.subplots(nb_rows, nb_cols, figsize=(15, 5));\nplt.suptitle('SAMPLE IMAGES');\nfor i in range(0, nb_rows):\n    for j in range(0, nb_cols):\n        axs[i, j].xaxis.set_ticklabels([]);\n        axs[i, j].yaxis.set_ticklabels([]);\n        axs[i, j].imshow((read_img(df['image_id'][np.random.randint(1400)], (255,255)))/255.);\nplt.show();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Show Result","metadata":{}},{"cell_type":"markdown","source":"# mask","metadata":{}},{"cell_type":"code","source":"#get an image\nimg = read_img(df['image_id'][13],(255,255))\n\n#mask\nimage_mask = create_mask_for_image(img)\n\nfig, ax = plt.subplots(1, 2, figsize=(15, 10));\nplt.suptitle('RESULT', x=0.5, y=0.8)\nplt.tight_layout(1)\n\nax[0].set_title('ORIGINAL', fontsize=12)\nax[1].set_title('MASK', fontsize=12)\n\nax[0].imshow(img/255);\nax[1].imshow(image_mask);\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#get an image\nimg = read_img(df['image_id'][15],(255,255))\n\n#mask\nimage_mask = create_mask_for_image(img)\n\nfig, ax = plt.subplots(1, 2, figsize=(15, 10));\nplt.suptitle('RESULT', x=0.5, y=0.8)\nplt.tight_layout(1)\n\nax[0].set_title('ORIGINAL', fontsize=12)\nax[1].set_title('MASK', fontsize=12)\n\nax[0].imshow(img/255);\nax[1].imshow(image_mask);\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#get an image\nimg = read_img(df['image_id'][110],(255,255))\n\n#mask\nimage_mask = create_mask_for_image(img)\n\nfig, ax = plt.subplots(1, 2, figsize=(15, 10));\nplt.suptitle('RESULT', x=0.5, y=0.8)\nplt.tight_layout(1)\n\nax[0].set_title('ORIGINAL', fontsize=12)\nax[1].set_title('MASK', fontsize=12)\n\nax[0].imshow(img/255);\nax[1].imshow(image_mask);\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#get an image\nimg = read_img(df['image_id'][1112],(255,255))\n\n#mask\nimage_mask = create_mask_for_image(img)\n\nfig, ax = plt.subplots(1, 2, figsize=(15, 10));\nplt.suptitle('RESULT', x=0.5, y=0.8)\nplt.tight_layout(1)\n\nax[0].set_title('ORIGINAL', fontsize=12)\nax[1].set_title('MASK', fontsize=12)\n\nax[0].imshow(img/255);\nax[1].imshow(image_mask);\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# segmented","metadata":{}},{"cell_type":"code","source":"#get an image\nimg = read_img(df['image_id'][13],(255,255))\n\n#segmentation\nimage_segmented = segment_image(img)\n\nfig, ax = plt.subplots(1, 2, figsize=(15, 5));\nplt.suptitle('RESULT', x=0.5, y=0.8)\nplt.tight_layout(1)\n\nax[0].set_title('ORIGINAL', fontsize=12)\nax[1].set_title('SEGMENTED', fontsize=12)\n\nax[0].imshow(img/255);\nax[1].imshow(image_segmented);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#get an image\nimg = read_img(df['image_id'][15],(255,255))\n\n#segmentation\nimage_segmented = segment_image(img)\n\nfig, ax = plt.subplots(1, 2, figsize=(15, 5));\nplt.suptitle('RESULT', x=0.5, y=0.8)\nplt.tight_layout(1)\n\nax[0].set_title('ORIGINAL', fontsize=12)\nax[1].set_title('SEGMENTED', fontsize=12)\n\nax[0].imshow(img/255);\nax[1].imshow(image_segmented);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#get an image\nimg = read_img(df['image_id'][5920],(255,255))\n\n#segmentation\nimage_segmented = segment_image(img)\n\nfig, ax = plt.subplots(1, 2, figsize=(15, 5));\nplt.suptitle('RESULT', x=0.5, y=0.8)\nplt.tight_layout(1)\n\nax[0].set_title('ORIGINAL', fontsize=12)\nax[1].set_title('SEGMENTED', fontsize=12)\n\nax[0].imshow(img/255);\nax[1].imshow(image_segmented);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# deskew","metadata":{}},{"cell_type":"code","source":"#get an image\nimg = read_img(df['image_id'][13],(255,255))\n\n#deskew\nimage_deskew = deskew_image(img)\n\nfig, ax = plt.subplots(1, 2, figsize=(15, 5));\nplt.suptitle('RESULT', x=0.5, y=0.8)\nplt.tight_layout(1)\n\nax[0].set_title('ORIGINAL', fontsize=12)\nax[1].set_title('DESKEW', fontsize=12)\n\nax[0].imshow(img/255);\nax[1].imshow(image_deskew);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#get an image\nimg = read_img(df['image_id'][15],(255,255))\n\n#deskew\nimage_deskew = deskew_image(img)\n\nfig, ax = plt.subplots(1, 2, figsize=(15, 5));\nplt.suptitle('RESULT', x=0.5, y=0.8)\nplt.tight_layout(1)\n\nax[0].set_title('ORIGINAL', fontsize=12)\nax[1].set_title('DESKEW', fontsize=12)\n\nax[0].imshow(img/255);\nax[1].imshow(image_deskew);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#get an image\nimg = read_img(df['image_id'][21367],(255,255))\n\n#deskew\nimage_deskew = deskew_image(img)\n\nfig, ax = plt.subplots(1, 2, figsize=(15, 5));\nplt.suptitle('RESULT', x=0.5, y=0.8)\nplt.tight_layout(1)\n\nax[0].set_title('ORIGINAL', fontsize=12)\nax[1].set_title('DESKEW', fontsize=12)\n\nax[0].imshow(img/255);\nax[1].imshow(image_deskew);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# gray","metadata":{}},{"cell_type":"code","source":"#get an image\nimg = read_img(df['image_id'][13],(255,255))\n\n#gray\nimage_gray = gray_image(img)\n\nfig, ax = plt.subplots(1, 2, figsize=(15, 5));\nplt.suptitle('RESULT', x=0.5, y=0.8)\nplt.tight_layout(1)\n\nax[0].set_title('ORIGINAL', fontsize=12)\nax[1].set_title('GRAY', fontsize=12)\n\nax[0].imshow(img/255);\nax[1].imshow(image_gray);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#get an image\nimg = read_img(df['image_id'][15],(255,255))\n\n#gray\nimage_gray = gray_image(img)\n\nfig, ax = plt.subplots(1, 2, figsize=(15, 5));\nplt.suptitle('RESULT', x=0.5, y=0.8)\nplt.tight_layout(1)\n\nax[0].set_title('ORIGINAL', fontsize=12)\nax[1].set_title('GRAY', fontsize=12)\n\nax[0].imshow(img/255);\nax[1].imshow(image_gray);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#get an image\nimg = read_img(df['image_id'][110],(255,255))\n\n#gray\nimage_gray = gray_image(img)\n\nfig, ax = plt.subplots(1, 2, figsize=(15, 5));\nplt.suptitle('RESULT', x=0.5, y=0.8)\nplt.tight_layout(1)\n\nax[0].set_title('ORIGINAL', fontsize=12)\nax[1].set_title('GRAY', fontsize=12)\n\nax[0].imshow(img/255);\nax[1].imshow(image_gray);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# thresh","metadata":{}},{"cell_type":"code","source":"#get an image\nimg = read_img(df['image_id'][13],(255,255))\n\n#thresh\ngray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\nret, thresh = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV +cv2.THRESH_OTSU)\nfig, ax = plt.subplots(1, 2, figsize=(15, 5));\nplt.suptitle('RESULT', x=0.5, y=0.8)\nplt.tight_layout(1)\n\n\nax[0].set_title('ORIGINAL', fontsize=12)\nax[1].set_title('THRESH', fontsize=12)\n\nax[0].imshow(img/255);\nax[1].imshow(thresh);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#get an image\nimg = read_img(df['image_id'][15],(255,255))\n\n#thresh\ngray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\nret, thresh = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV +cv2.THRESH_OTSU)\nfig, ax = plt.subplots(1, 2, figsize=(15, 5));\nplt.suptitle('RESULT', x=0.5, y=0.8)\nplt.tight_layout(1)\n\nax[0].set_title('ORIGINAL', fontsize=12)\nax[1].set_title('THRESH', fontsize=12)\n\nax[0].imshow(img/255);\nax[1].imshow(thresh);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#get an image\nimg = read_img(df['image_id'][105],(255,255))\n\n#thresh\ngray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\nret, thresh = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV +cv2.THRESH_OTSU)\nfig, ax = plt.subplots(1, 2, figsize=(15, 5));\nplt.suptitle('RESULT', x=0.5, y=0.8)\nplt.tight_layout(1)\n\nax[0].set_title('ORIGINAL', fontsize=12)\nax[1].set_title('THRESH', fontsize=12)\n\nax[0].imshow(img/255);\nax[1].imshow(thresh);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# rnoise","metadata":{}},{"cell_type":"code","source":"#get an image\nimg = read_img(df['image_id'][13],(255,255))\n\n#rnoise\nimage_rnoise = rnoise_image(img)\n\nfig, ax = plt.subplots(1, 2, figsize=(15, 5));\nplt.suptitle('RESULT', x=0.5, y=0.8)\nplt.tight_layout(1)\n\nax[0].set_title('ORIGINAL', fontsize=12)\nax[1].set_title('RNOISE', fontsize=12)\n\nax[0].imshow(img/255);\nax[1].imshow(image_rnoise);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#get an image\nimg = read_img(df['image_id'][15],(255,255))\n\n#rnoise\nimage_rnoise = rnoise_image(img)\n\nfig, ax = plt.subplots(1, 2, figsize=(15, 5));\nplt.suptitle('RESULT', x=0.5, y=0.8)\nplt.tight_layout(1)\n\nax[0].set_title('ORIGINAL', fontsize=12)\nax[1].set_title('RNOISE', fontsize=12)\n\nax[0].imshow(img/255);\nax[1].imshow(image_rnoise);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#get an image\nimg = read_img(df['image_id'][19025],(255,255))\n\n#rnoise\nimage_rnoise = rnoise_image(img)\n\nfig, ax = plt.subplots(1, 2, figsize=(15, 5));\nplt.suptitle('RESULT', x=0.5, y=0.8)\nplt.tight_layout(1)\n\nax[0].set_title('ORIGINAL', fontsize=12)\nax[1].set_title('RNOISE', fontsize=12)\n\nax[0].imshow(img/255);\nax[1].imshow(image_rnoise);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# canny ","metadata":{}},{"cell_type":"code","source":"#get an image\nimg = read_img(df['image_id'][13],(255,255))\n\n#canny\nimage_canny = canny_image(img)\n\nfig, ax = plt.subplots(1, 2, figsize=(15, 5));\nplt.suptitle('RESULT', x=0.5, y=0.8)\nplt.tight_layout(1)\n\nax[0].set_title('ORIGINAL', fontsize=12)\nax[1].set_title('CANNY', fontsize=12)\n\nax[0].imshow(img/255);\nax[1].imshow(image_canny);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#get an image\nimg = read_img(df['image_id'][15],(255,255))\n\n#canny\nimage_canny = canny_image(img)\n\nfig, ax = plt.subplots(1, 2, figsize=(15, 5));\nplt.suptitle('RESULT', x=0.5, y=0.8)\nplt.tight_layout(1)\n\nax[0].set_title('ORIGINAL', fontsize=12)\nax[1].set_title('CANNY', fontsize=12)\n\nax[0].imshow(img/255);\nax[1].imshow(image_canny);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#get an image\nimg = read_img(df['image_id'][6900],(255,255))\n\n#canny\nimage_canny = canny_image(img)\n\nfig, ax = plt.subplots(1, 2, figsize=(15, 5));\nplt.suptitle('RESULT', x=0.5, y=0.8)\nplt.tight_layout(1)\n\nax[0].set_title('ORIGINAL', fontsize=12)\nax[1].set_title('CANNY', fontsize=12)\n\nax[0].imshow(img/255);\nax[1].imshow(image_canny);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# sharpen","metadata":{}},{"cell_type":"code","source":"#get an image\nimg = read_img(df['image_id'][13],(255,255))\n\n#sharpen the image\nimage_sharpen = sharpen_image(img)\n\nfig, ax = plt.subplots(1, 2, figsize=(15, 5));\nplt.suptitle('RESULT', x=0.5, y=0.8)\nplt.tight_layout(1)\n\nax[0].set_title('ORIGINAL', fontsize=12)\nax[1].set_title('SHARPEN', fontsize=12)\n\nax[0].imshow(img/255);\nax[1].imshow(image_sharpen);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#get an image\nimg = read_img(df['image_id'][15],(255,255))\n\n#sharpen the image\nimage_sharpen = sharpen_image(img)\n\nfig, ax = plt.subplots(1, 2, figsize=(15, 5));\nplt.suptitle('RESULT', x=0.5, y=0.8)\nplt.tight_layout(1)\n\nax[0].set_title('ORIGINAL', fontsize=12)\nax[1].set_title('SHARPEN', fontsize=12)\n\nax[0].imshow(img/255);\nax[1].imshow(image_sharpen);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#get an image\nimg = read_img(df['image_id'][2090],(255,255))\n\n#sharpen the image\nimage_sharpen = sharpen_image(img)\n\nfig, ax = plt.subplots(1, 2, figsize=(15, 5));\nplt.suptitle('RESULT', x=0.5, y=0.8)\nplt.tight_layout(1)\n\nax[0].set_title('ORIGINAL', fontsize=12)\nax[1].set_title('SHARPEN', fontsize=12)\n\nax[0].imshow(img/255);\nax[1].imshow(image_sharpen);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Comparison of color space transformation","metadata":{}},{"cell_type":"code","source":"#get an image\nimg = read_img(df['image_id'][13],(255,255))\n\n#mask\nimage_mask = create_mask_for_image(img)\n\n#segmentation\nimage_segmented = segment_image(img)\n\n\n#deskew\nimage_deskew = deskew_image(img)\n\n#gray\nimage_gray = gray_image(img)\n\n\n\nfig, ax = plt.subplots(1, 5, figsize=(15, 6));\nplt.suptitle('RESULT', x=0.5, y=0.8)\nplt.tight_layout(2)\n\nax[0].set_title('ORIGINAL', fontsize=9)\nax[1].set_title('MASK', fontsize=9)\nax[2].set_title('SEGMENTED', fontsize=9)\nax[3].set_title('DESKEW', fontsize=9)\nax[4].set_title('GRAY', fontsize=9)\n\n\nax[0].imshow(img/255);\nax[1].imshow(image_mask);\nax[2].imshow(image_segmented);\nax[3].imshow(image_deskew);\nax[4].imshow(image_gray );\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 5, figsize=(15, 6));\nplt.tight_layout(2)\n\n#thresh\nimage_thresh = thresh_image(img)\n\n#rnoise\nimage_rnoise = rnoise_image(img)\n\n#canny\nimage_canny = canny_image(img)\n\n#sharpen the image\nimage_sharpen = sharpen_image(img)\n\n\nax[0].set_title('ORIGINAL', fontsize=9)\nax[1].set_title('THREST', fontsize=9)\nax[2].set_title('RNOISE', fontsize=9)\nax[3].set_title('CANNY', fontsize=9)\nax[4].set_title('SHARPEN', fontsize=9)\n\n\nax[0].imshow(img/255);\nax[1].imshow(thresh);\nax[2].imshow(image_rnoise);\nax[3].imshow(image_canny);\nax[4].imshow(image_sharpen);\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Morphological Transformations","metadata":{}},{"cell_type":"markdown","source":"# erosion\n\nThe basic idea of erosion is just like soil erosion only, it erodes away the boundaries of foreground object (Always try to keep foreground in white)","metadata":{}},{"cell_type":"code","source":"#get an image\nimg = read_img(df['image_id'][13],(255,255))\n\n#erode\nimage_erode = erode_image(img)\n\nfig, ax = plt.subplots(1, 2, figsize=(15, 5));\nplt.suptitle('RESULT', x=0.5, y=0.8)\nplt.tight_layout(1)\n\nax[0].set_title('ORIGINAL', fontsize=12)\nax[1].set_title('ERODE', fontsize=12)\n\nax[0].imshow(img/255);\nax[1].imshow(image_erode);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#get an image\nimg = read_img(df['image_id'][15],(255,255))\n\n#erode\nimage_erode = erode_image(img)\n\nfig, ax = plt.subplots(1, 2, figsize=(15, 5));\nplt.suptitle('RESULT', x=0.5, y=0.8)\nplt.tight_layout(1)\n\nax[0].set_title('ORIGINAL', fontsize=12)\nax[1].set_title('ERODE', fontsize=12)\n\nax[0].imshow(img/255);\nax[1].imshow(image_erode);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#get an image\nimg = read_img(df['image_id'][110],(255,255))\n\n#erode\nimage_erode = erode_image(img)\n\nfig, ax = plt.subplots(1, 2, figsize=(15, 5));\nplt.suptitle('RESULT', x=0.5, y=0.8)\nplt.tight_layout(1)\n\nax[0].set_title('ORIGINAL', fontsize=12)\nax[1].set_title('ERODE', fontsize=12)\n\nax[0].imshow(img/255);\nax[1].imshow(image_erode);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# dilate\n\nIt is just opposite of erosion. Here, a pixel element is ‘1’ if atleast one pixel under the kernel is ‘1’. So it increases the white region in the image or size of foreground object increases. Normally, in cases like noise removal, erosion is followed by dilation.","metadata":{}},{"cell_type":"code","source":"#get an image\nimg = read_img(df['image_id'][13],(255,255))\n\n#dilate\nimage_dilate = dilate_image(img)\n\nfig, ax = plt.subplots(1, 2, figsize=(15, 5));\nplt.suptitle('RESULT', x=0.5, y=0.8)\nplt.tight_layout(1)\n\nax[0].set_title('ORIGINAL', fontsize=12)\nax[1].set_title('DILATE', fontsize=12)\n\nax[0].imshow(img/255);\nax[1].imshow(image_dilate);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#get an image\nimg = read_img(df['image_id'][15],(255,255))\n\n#dilate\nimage_dilate = dilate_image(img)\n\nfig, ax = plt.subplots(1, 2, figsize=(15, 5));\nplt.suptitle('RESULT', x=0.5, y=0.8)\nplt.tight_layout(1)\n\nax[0].set_title('ORIGINAL', fontsize=12)\nax[1].set_title('DILATE', fontsize=12)\n\nax[0].imshow(img/255);\nax[1].imshow(image_dilate);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#get an image\nimg = read_img(df['image_id'][110],(255,255))\n\n#dilate\nimage_dilate = dilate_image(img)\n\nfig, ax = plt.subplots(1, 2, figsize=(15, 5));\nplt.suptitle('RESULT', x=0.5, y=0.8)\nplt.tight_layout(1)\n\nax[0].set_title('ORIGINAL', fontsize=12)\nax[1].set_title('DILATE', fontsize=12)\n\nax[0].imshow(img/255);\nax[1].imshow(image_dilate);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Opening\n\nOpening is just another name of erosion followed by dilation. It is useful in removing noise","metadata":{}},{"cell_type":"code","source":"#get an image\nimg = read_img(df['image_id'][13],(255,255))\n\n#opening\nimage_opening = opening_image(img)\n\nfig, ax = plt.subplots(1, 2, figsize=(15, 5));\nplt.suptitle('RESULT', x=0.5, y=0.8)\nplt.tight_layout(1)\n\nax[0].set_title('ORIGINAL', fontsize=12)\nax[1].set_title('OPENING', fontsize=12)\n\nax[0].imshow(img/255);\nax[1].imshow(image_opening);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#get an image\nimg = read_img(df['image_id'][15],(255,255))\n\n#opening\nimage_opening = opening_image(img)\n\nfig, ax = plt.subplots(1, 2, figsize=(15, 5));\nplt.suptitle('RESULT', x=0.5, y=0.8)\nplt.tight_layout(1)\n\nax[0].set_title('ORIGINAL', fontsize=12)\nax[1].set_title('OPENING', fontsize=12)\n\nax[0].imshow(img/255);\nax[1].imshow(image_opening);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#get an image\nimg = read_img(df['image_id'][110],(255,255))\n\n#opening\nimage_opening = opening_image(img)\n\nfig, ax = plt.subplots(1, 2, figsize=(15, 5));\nplt.suptitle('RESULT', x=0.5, y=0.8)\nplt.tight_layout(1)\n\nax[0].set_title('ORIGINAL', fontsize=12)\nax[1].set_title('OPENING', fontsize=12)\n\nax[0].imshow(img/255);\nax[1].imshow(image_opening);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Comparison of Morphological Transformations","metadata":{}},{"cell_type":"code","source":"#get an image\nimg = read_img(df['image_id'][13],(255,255))\n\n#dilate\nimage_dilate = dilate_image(img)\n\n#erode\nimage_erode = erode_image(img)\n\n#opening\nimage_opening = opening_image(img)\n\nfig, ax = plt.subplots(1, 4, figsize=(15, 6));\nplt.suptitle('RESULT', x=0.5, y=0.8)\nplt.tight_layout(1)\n\nax[0].set_title('ORIGINAL', fontsize=9)\nax[1].set_title('DILATE', fontsize=9)\nax[2].set_title('ERODE', fontsize=9)\nax[3].set_title('OPENING', fontsize=9)\n\n\nax[0].imshow(img/255);\nax[1].imshow(image_dilate);\nax[2].imshow(image_erode);\nax[3].imshow(image_opening);\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Image Blurring (Image Smoothing)","metadata":{}},{"cell_type":"markdown","source":"Image blurring is achieved by convolving the image with a low-pass filter kernel. It is useful for removing noise. It actually removes high frequency content (e.g: noise, edges) from the image resulting in edges being blurred when this is filter is applied. (Well, there are blurring techniques which do not blur edges). OpenCV provides mainly four types of blurring techniques.","metadata":{}},{"cell_type":"markdown","source":"# Averaging","metadata":{}},{"cell_type":"markdown","source":"This is done by convolving the image with a normalized box filter. It simply takes the average of all the pixels under kernel area and replaces the central element with this average. This is done by the function cv2.blur() or cv2.boxFilter(). ","metadata":{}},{"cell_type":"code","source":"\n#get an image\nimg = read_img(df['image_id'][13],(255,255))\n\n#Blur\nblur = cv2.blur(img,(5,5))\n\nfig, ax = plt.subplots(1, 2, figsize=(15, 5));\nplt.suptitle('RESULT', x=0.5, y=0.8)\nplt.tight_layout(1)\n\nax[0].set_title('ORIGINAL', fontsize=12)\nax[1].set_title('BLUR IMAGE', fontsize=12)\n\nax[0].imshow(img/255);\nax[1].imshow(blur);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#get an image\nimg = read_img(df['image_id'][15],(255,255))\n\n#Blur\nblur = cv2.blur(img,(5,5))\n\nfig, ax = plt.subplots(1, 2, figsize=(15, 5));\nplt.suptitle('RESULT', x=0.5, y=0.8)\nplt.tight_layout(1)\n\nax[0].set_title('ORIGINAL', fontsize=12)\nax[1].set_title('BLUR IMAGE', fontsize=12)\n\nax[0].imshow(img/255);\nax[1].imshow(blur);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Gaussian Filtering\nIt is done with the function, cv2.GaussianBlur(). ","metadata":{}},{"cell_type":"code","source":"#get an image\nimg = read_img(df['image_id'][13],(255,255))\n\n#GaussianBlur\nGblur = cv2.GaussianBlur(img,(5,5),0)\n\nfig, ax = plt.subplots(1, 2, figsize=(15, 5));\nplt.suptitle('RESULT', x=0.5, y=0.8)\nplt.tight_layout(1)\n\nax[0].set_title('ORIGINAL', fontsize=12)\nax[1].set_title('GAUSSIAN BLUR', fontsize=12)\n\nax[0].imshow(img/255);\nax[1].imshow(Gblur);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#get an image\nimg = read_img(df['image_id'][15],(255,255))\n\n#GaussianBlur\nGblur = cv2.GaussianBlur(img,(5,5),0)\n\nfig, ax = plt.subplots(1, 2, figsize=(15, 5));\nplt.suptitle('RESULT', x=0.5, y=0.8)\nplt.tight_layout(1)\n\nax[0].set_title('ORIGINAL', fontsize=12)\nax[1].set_title('GAUSSIAN BLUR', fontsize=12)\n\nax[0].imshow(img/255);\nax[1].imshow(Gblur);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Median Filtering\n\nHere, the function cv2.medianBlur() computes the median of all the pixels under the kernel window and the central pixel is replaced with this median value. This is highly effective in removing salt-and-pepper noise.","metadata":{}},{"cell_type":"code","source":"#get an image\nimg = read_img(df['image_id'][13],(255,255))\n\n#medianBlur\nblur_image_median = cv2.medianBlur(img,5)\n\nfig, ax = plt.subplots(1, 2, figsize=(15, 5));\nplt.suptitle('RESULT', x=0.5, y=0.8)\nplt.tight_layout(1)\n\nax[0].set_title('ORIGINAL', fontsize=12)\nax[1].set_title('MEDIAN BLUR', fontsize=12)\n\nax[0].imshow(img/255);\nax[1].imshow(blur_image_median);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#get an image\nimg = read_img(df['image_id'][15],(255,255))\n\n#medianBlur\nblur_image_median = cv2.medianBlur(img,5)\n\nfig, ax = plt.subplots(1, 2, figsize=(15, 5));\nplt.suptitle('RESULT', x=0.5, y=0.8)\nplt.tight_layout(1)\n\nax[0].set_title('ORIGINAL', fontsize=12)\nax[1].set_title('MEDIAN BLUR', fontsize=12)\n\nax[0].imshow(img/255);\nax[1].imshow(blur_image_median);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Bilateral Filtering\n\nAccording to [3] cv2.bilateralFilter() is highly effective at noise removal while preserving edges","metadata":{}},{"cell_type":"code","source":"\n#get an image\nimg = read_img(df['image_id'][13],(255,255))\n\n#BILATERAL FILTER\nbilblur = cv2.bilateralFilter(img,9,75,75)\n\nfig, ax = plt.subplots(1, 2, figsize=(15, 5));\nplt.suptitle('RESULT', x=0.5, y=0.8)\nplt.tight_layout(1)\n\nax[0].set_title('ORIGINAL', fontsize=12)\nax[1].set_title('BILATERAL FILTER', fontsize=12)\n\nax[0].imshow(img/255);\nax[1].imshow(bilblur);\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#get an image\nimg = read_img(df['image_id'][15],(255,255))\n\n#BILATERAL FILTER\nbilblur = cv2.bilateralFilter(img,9,75,75)\n\nfig, ax = plt.subplots(1, 2, figsize=(15, 5));\nplt.suptitle('RESULT', x=0.5, y=0.8)\nplt.tight_layout(1)\n\nax[0].set_title('ORIGINAL', fontsize=12)\nax[1].set_title('BILATERAL FILTER', fontsize=12)\n\nax[0].imshow(img/255);\nax[1].imshow(bilblur);\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Comparison of Image Blurring","metadata":{}},{"cell_type":"code","source":"#get an image\nimg = read_img(df['image_id'][13],(255,255))\n\nfig, ax = plt.subplots(1, 5, figsize=(15, 6));\nplt.suptitle('RESULT', x=0.5, y=0.8)\nplt.tight_layout(1)\n\nax[0].set_title('ORIGINAL', fontsize=9)\nax[1].set_title('BLUR IMAGE', fontsize=9)\nax[2].set_title('GAUSSIAN BLUR', fontsize=9)\nax[3].set_title('MEDIAN BLUR', fontsize=9)\nax[4].set_title('BILATERAL FILTER', fontsize=9)\n\n\nax[0].imshow(img/255);\nax[1].imshow(blur);\nax[2].imshow(Gblur);\nax[3].imshow(blur_image_median);\nax[4].imshow(bilblur);\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Rotating image based on a specific angle\n\n\nIn the code below, the image is rotated in increments of 60-degrees using rotate() of imutils\n","metadata":{}},{"cell_type":"code","source":"!pip install imutils\n","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import imutils\nimport numpy as np\nimport cv2\n\nimage = image= cv2.imread(r'../input/cassava-leaf-disease-classification/train_images/1000015157.jpg')\n\n# loop over the rotation angles\nfor angle in np.arange(0, 360, 60):\n    cv2.namedWindow(\"Rotated\", cv2.WINDOW_NORMAL)\n    rotated = imutils.rotate(image, angle)\n    cv2.imshow(\"Rotated\", rotated)\n    cv2.waitKey(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Rotating image using PIL\n\nHere the image is rotated by 110 degrees using PIL","metadata":{}},{"cell_type":"code","source":"import cv2\n\n# Rotate image using PIL\n#image= cv2.imread(r'../input/cassava-leaf-disease-classification/train_images/1000015157.jpg')\npil_image= cv2.imread(r'../input/cassava-leaf-disease-classification/train_images/1000015157.jpg')\nrotate_img_pil=pil_image.rotate(110)\nrotate_img_pil.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# References\n\n[1] https://en.wikipedia.org/wiki/Computer_vision\n\n[2] http://szeliski.org/Book/\n\n[3] https://opencv-python-tutroals.readthedocs.io/\n\n\n","metadata":{}}]}