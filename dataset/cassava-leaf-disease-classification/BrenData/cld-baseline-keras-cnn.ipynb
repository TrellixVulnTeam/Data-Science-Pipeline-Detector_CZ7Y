{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os, json, cv2\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\n\n#model imports (keras/tensorflow)\nimport tensorflow as tf\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.preprocessing import image\nfrom keras import layers, models\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom keras.optimizers import Adam\nfrom keras.layers import Conv2D, MaxPooling2D, BatchNormalization, GlobalAveragePooling2D\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Notebooks that helped me, and taught me some awesome things along the way.\n\n[Maksym Shkliarevskyis Notebook](https://www.kaggle.com/maksymshkliarevskyi/cassava-leaf-disease-best-keras-cnn)"},{"metadata":{},"cell_type":"markdown","source":"# Load Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"WORK_DIR = '../input/cassava-leaf-disease-classification'\n\n#counting number of images in the train_images folder\nprint('Train images: %d' %len(os.listdir(os.path.join(WORK_DIR, \"train_images\"))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#reading labels from .json file\nwith open(os.path.join(WORK_DIR, \"label_num_to_disease_map.json\")) as file:\n    print(json.dumps(json.loads(file.read()), indent=4))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The labels for each image are stored in a csv file. We can read these in to a pandas dataframe and view the top five rows of this dataFrame."},{"metadata":{"trusted":true},"cell_type":"code","source":"#reading in the training image labels \nall_train_labels_df = pd.read_csv('../input/cassava-leaf-disease-classification/train.csv')\nall_train_labels_df.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualizing Data"},{"metadata":{},"cell_type":"markdown","source":"Although I find it is quite difficult for the human eye to determine any major differnces between the leaves in this dataset, I feel it is important to plot a few images to get a feel the nature of the dataset. \n\n- I used .sample() to take three random images from each group. This is handy as everytime the notebook is ran we see a different set of images."},{"metadata":{},"cell_type":"markdown","source":"###  \"0\" : Cassava Bacterial Blight (CBB)"},{"metadata":{"trusted":true},"cell_type":"code","source":"i=1\nplt.figure(figsize=(40, 40))\nfor image_id in all_train_labels_df.loc[all_train_labels_df.label==0].sample(3).image_id.values:\n    img = cv2.imread(os.path.join(WORK_DIR, \"train_images\", image_id)) #using cv2 library to read imgs\n    plt.subplot(1, 3, i)\n    plt.imshow(img)\n    plt.title(image_id, fontsize=30)\n    i+=1\n\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  \"1\" : Cassava Brown Streak Disease (CBSD)"},{"metadata":{"trusted":true},"cell_type":"code","source":"i=1\nplt.figure(figsize=(40, 40))\nfor image_id in all_train_labels_df.loc[all_train_labels_df.label==1].sample(3).image_id.values:\n    img = cv2.imread(os.path.join(WORK_DIR, \"train_images\", image_id)) #using cv2 library to read imgs\n    plt.subplot(1, 3, i)\n    plt.imshow(img)\n    plt.title(image_id, fontsize=30)\n    i+=1\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### \"2\" : Cassava Green Mottle (CGM)"},{"metadata":{"trusted":true},"cell_type":"code","source":"i=1\nplt.figure(figsize=(40, 40))\nfor image_id in all_train_labels_df.loc[all_train_labels_df.label==2].sample(3).image_id.values:\n    img = cv2.imread(os.path.join(WORK_DIR, \"train_images\", image_id)) #using cv2 library to read imgs\n    plt.subplot(1, 3, i)\n    plt.imshow(img)\n    plt.title(image_id, fontsize=30)\n    i+=1\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### \"3\" : Cassava Mosaic Disease (CMD)"},{"metadata":{"trusted":true},"cell_type":"code","source":"i=1\nplt.figure(figsize=(40, 40))\nfor image_id in all_train_labels_df.loc[all_train_labels_df.label==3].sample(3).image_id.values:\n    img = cv2.imread(os.path.join(WORK_DIR, \"train_images\", image_id)) #using cv2 library to read imgs\n    plt.subplot(1, 3, i)\n    plt.imshow(img)\n    plt.title(image_id, fontsize=30)\n    i+=1\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### \"4\" : Healthy"},{"metadata":{"trusted":true},"cell_type":"code","source":"i=1\nplt.figure(figsize=(40, 40))\nfor image_id in all_train_labels_df.loc[all_train_labels_df.label==4].sample(3).image_id.values:\n    img = cv2.imread(os.path.join(WORK_DIR, \"train_images\", image_id)) #using cv2 library to read imgs\n    plt.subplot(1, 3, i)\n    plt.imshow(img)\n    plt.title(image_id, fontsize=30)\n    i+=1\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Augmentation\n\nSTEPS_PER_EPOCH - great parameter to user when making augmented data on the fly. This is basically the number of batch iterations before the epoch is considered finished.\n\nVALIDATION_STEPS - Same as STEPS_PER_EPOCH but is used when we are testing the model on the validation dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"#model parameters values used to aid in readability and debugging etc.\nBATCH_SIZE = 16\nEPOCHS = 20\nTARGET_SIZE = 224 #parameter that specifies the image dimensions\nSTEPS_PER_EPOCH = len(all_train_labels_df)*0.8 / BATCH_SIZE #number of batch iterations before epoch is considered done\nVALIDATION_STEPS = len(all_train_labels_df)*0.2 / BATCH_SIZE","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I would like to do a stratified test split on the data so that I have both a training and validation dataset that is representative of the entire dataset. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#setting train_labels to string before train_test_split\nall_train_labels_df.label = all_train_labels_df.label.astype('str')\n\n#stratified train_test_split\ntrain_labels_df, valid_labels_df = train_test_split(all_train_labels_df, random_state=314,\n                                      test_size=0.2, stratify = all_train_labels_df.label)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Just making sure we are getting an even distribution of labels across the train and test datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels_df.label.value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_labels_df.label.value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using very intuituive keras image preprocessing framework called ImageDataGenerator.\n\n- documentation and parameter guide found here -> [link](https://keras.io/api/preprocessing/image/#imagedatagenerator-class)"},{"metadata":{"trusted":true},"cell_type":"code","source":"my_data_augmentor = ImageDataGenerator(preprocessing_function = None,\n                    rotation_range = 45,\n                    zoom_range = 0.2,\n                    horizontal_flip = True,\n                    vertical_flip = True,\n                    fill_mode = 'nearest', #set to default val\n                    shear_range = 0.1, #shear_range is distortion along axis, which changes the angle of the image\n                    height_shift_range = 0.1,\n                    width_shift_range = 0.1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is how we read through different images in the work directory and apply the imagedatagenerator as they are read in.\n\nIt is also important to note that the typical train_test_split of the data is occuring in the imagedatagenerator. We "},{"metadata":{"trusted":true},"cell_type":"code","source":"training_generator = my_data_augmentor.flow_from_dataframe(train_labels_df,\n                         directory = os.path.join(WORK_DIR, \"train_images\"),\n                         x_col = \"image_id\",\n                         y_col = \"label\",\n                         target_size = (TARGET_SIZE, TARGET_SIZE),\n                         batch_size = BATCH_SIZE,\n                         class_mode = \"sparse\")\n\nvalidation_generator = my_data_augmentor.flow_from_dataframe(valid_labels_df,\n                         directory = os.path.join(WORK_DIR, \"train_images\"),\n                         x_col = \"image_id\",\n                         y_col = \"label\",\n                         target_size = (TARGET_SIZE, TARGET_SIZE),\n                         batch_size = BATCH_SIZE,\n                         class_mode = \"sparse\") #Determines the type of label arrays that are returned","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualizing Augmented Data\n\nThe following sub section is pretty cool, it enables me to alter the parameters of the ImageDataGnerator, and visualize how this affects an image before committing those parameters to a model to train."},{"metadata":{"trusted":true},"cell_type":"code","source":"generator = my_data_augmentor.flow_from_dataframe(train_labels_df.iloc[153:154],\n                         directory = os.path.join(WORK_DIR, \"train_images\"),\n                         x_col = \"image_id\",\n                         y_col = \"label\",\n                         target_size = (TARGET_SIZE, TARGET_SIZE),\n                         batch_size = BATCH_SIZE,\n                         class_mode = \"sparse\")\n\naug_images = [generator[0][0][0]/255 for i in range(10)]\nfig, axes = plt.subplots(2, 5, figsize = (20, 10))\naxes = axes.flatten()\nfor img, ax in zip(aug_images, axes):\n    ax.imshow(img)\n    ax.axis('off')\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating the CNN model\n\nTo start of I am going to be building off one of tensorflows pretrained models called efficientnetB0. I will be trying out models using different base models ie VGG, Inception, and ResNet. Im not entirely sure which model is best for this situation, but thats what the internet is for. I am looking into this topic!\n\nThe following cell builds and saves the model below. This has previously been saved to a dataset and we can loads this back in with its optimal weights."},{"metadata":{"trusted":true},"cell_type":"code","source":"#make sure internet is enabled in notebook so we can access efficientNetB0 model from google.storage.api\ndef create_model():\n    model = models.Sequential()\n\n    model.add(EfficientNetB0(include_top = False, weights = 'imagenet', \n                             input_shape = (TARGET_SIZE, TARGET_SIZE, 3)))\n    #I would like to experiment making the pretrained base so that we cant train it further\n    \n    model.add(layers.GlobalAveragePooling2D())\n    model.add(layers.Dense(5, activation = \"softmax\"))\n\n    model.compile(optimizer = Adam(lr = 0.001),\n                  loss = \"sparse_categorical_crossentropy\",\n                  metrics = [\"acc\"])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = create_model()\nprint('CNN has {} layers'.format(len(model.layers)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('./EfNetB0_untrained_baseline_model.h5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training Model"},{"metadata":{},"cell_type":"markdown","source":"ModelCheckpoint docs - https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint\n\nReduceLROnPlateau docs - https://keras.io/api/callbacks/reduce_lr_on_plateau/"},{"metadata":{"trusted":true},"cell_type":"code","source":"#callbacks\nmodel_save = ModelCheckpoint('./EffNetB0_best_baseline_model.h5', \n                             save_best_only = True, \n                             save_weights_only = False,\n                             monitor = 'val_loss', \n                             mode = 'min', verbose = 1)\n\nmy_early_stopper = EarlyStopping(monitor = 'val_loss', min_delta = 0.001, \n                           patience = 5, mode = 'min', verbose = 1,\n                           restore_best_weights = True)\n\nreduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.3, \n                              patience = 2, min_delta = 0.001, \n                              mode = 'min', verbose = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":" history = model.fit(\n     training_generator,\n     steps_per_epoch = STEPS_PER_EPOCH,\n     epochs = EPOCHS,\n     validation_data = validation_generator,\n     validation_steps = VALIDATION_STEPS,\n     callbacks = [model_save, my_early_stopper, reduce_lr]\n )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('./EffNetB0_trained_baseline_model.h5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prediction\n\nMaking predictions in a seperate notebook, just keeping code here as a guide."},{"metadata":{"trusted":true},"cell_type":"code","source":"# sample_submission = pd.read_csv('../input/cassava-leaf-disease-classification/sample_submission.csv')\n# sample_submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#making preds on all the test_images\n# preds = []\n\n# for image_id in sample_submission.image_id:\n#     image = Image.open(os.path.join(WORK_DIR,  \"test_images\", image_id))\n#     image = image.resize((TARGET_SIZE, TARGET_SIZE))\n#     image = np.expand_dims(image, axis = 0)\n#     preds.append(np.argmax(model.predict(image)))\n\n# sample_submission['label'] = preds\n# sample_submission","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}