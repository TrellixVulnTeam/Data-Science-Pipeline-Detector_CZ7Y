{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Visualizing Augmentations\nIn this notebook, I am trying to visualize various augmentation techniques available on Albumentations. Albumentations is a Python library for image augmentation. Image augmentation is used in deep learning and computer vision tasks to increase the quality of trained models. The purpose of image augmentation is to create new training samples from the existing data.\n\nType of augmentations:\n1. Pixel-level transforms: Pixel-level transforms will change just an input image and will leave any additional targets such as masks, bounding boxes, and keypoints unchanged. \n2. Spatial-level transforms: Spatial-level transforms will simultaneously change both an input image as well as additional targets such as masks, bounding boxes, and keypoints. "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport os\nimport cv2\nimport albumentations as A\nimport random\nimport torch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\ndef visualize(image):\n    plt.figure(figsize=(10, 10))\n    plt.axis('off')\n    plt.imshow(image)\n    \ndef visualize_multiple(nrows, ncols, img, transform):\n    fig, axes = plt.subplots(nrows,ncols)\n    fig.set_figheight(15)\n    fig.set_figwidth(15)\n    num_iter = 0\n    for row in range(nrows):\n        for col in range(ncols):\n            augmented_img = transform[num_iter](image=img)['image']\n            axes[row,col].imshow(augmented_img)\n            axes[row,col].grid(False)\n            axes[row,col].set_xticks([])\n            axes[row,col].set_yticks([])\n            num_iter += 1\n    return fig, axes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Sample Image\nI pick one image from the Cassava dataset for as a sample."},{"metadata":{"trusted":true},"cell_type":"code","source":"seed_everything(100)\nimg = cv2.imread('../input/cassava-leaf-disease-classification/train_images/100042118.jpg')\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\nvisualize(img)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1.0. Pixel-Level Transforms\nPixel-level transforms will change just an input image and will leave any additional targets such as masks, bounding boxes, and keypoints unchanged."},{"metadata":{},"cell_type":"markdown","source":"## 1.1. Blur\nBlur the input image using a random-sized kernel."},{"metadata":{"trusted":true},"cell_type":"code","source":"blur_limits = np.arange(3,39,4)\ntransform = [A.Blur(p=1, blur_limit=[limit,limit], always_apply=True) for limit in blur_limits]\nfig, axes = visualize_multiple(3,3,img,transform)\n\nnum_iter = 0\nfor row in range(3):\n    for col in range(3):\n        text = 'Blur kernel size: ({}, {})'.format(blur_limits[num_iter], blur_limits[num_iter])\n        axes[row, col].set_title(text)\n        num_iter += 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.2. CLAHE\nApply Contrast Limited Adaptive Histogram Equalization to the input image."},{"metadata":{"trusted":true},"cell_type":"code","source":"params = np.arange(3,30,3)\ntransform = [A.CLAHE(clip_limit=[param, param], tile_grid_size=(param, param), always_apply=True) for param in params]\nfig, axes = visualize_multiple(3,3,img,transform)\n\nnum_iter = 0\nfor row in range(3):\n    for col in range(3):\n        text = 'Clip limit: ({}, {}), tile grid size: ({}, {})'.format(params[num_iter], params[num_iter], params[num_iter], params[num_iter])\n        axes[row, col].set_title(text)\n        num_iter += 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.3. Channel Dropout\nRandomly Drop Channels in the input Image."},{"metadata":{"trusted":true},"cell_type":"code","source":"transform = [A.ChannelDropout(channel_drop_range=(1,2), always_apply=True) for _ in range(9)]\nfig, axes = visualize_multiple(3,3,img,transform)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.4. Channel Shuffle\nRandomly rearrange channels of the input RGB image."},{"metadata":{"trusted":true},"cell_type":"code","source":"transform = [A.ChannelShuffle(p=1) for _ in range(9)]\nfig, axes = visualize_multiple(3,3,img,transform)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.5. Color Jitter\nRandomly changes the brightness, contrast, and saturation of an image. Compared to ColorJitter from torchvision, this transform gives a little bit different results because Pillow (used in torchvision) and OpenCV (used in Albumentations) transform an image to HSV format by different formulas."},{"metadata":{"trusted":true},"cell_type":"code","source":"transform = [A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2, always_apply=True) for _ in range(9)]\nfig, axes = visualize_multiple(3,3,img,transform)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.6. Downscale\nDecreases image quality by downscaling and upscaling back."},{"metadata":{"trusted":true},"cell_type":"code","source":"transform = [A.Downscale(scale_min=0.25, scale_max=0.25, always_apply=True) for _ in range(9)]\nfig, axes = visualize_multiple(3,3,img,transform)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.7. Equalize\nEqualize the image histogram."},{"metadata":{"trusted":true},"cell_type":"code","source":"params = [['cv', True], ['cv', False], ['pil', True], ['pil', False]]\ntransform = [A.Equalize(mode=param[0], by_channels=param[1], always_apply=True) for param in params]\nfig, axes = visualize_multiple(2,2,img,transform)\n\nnum_iter = 0\nfor row in range(2):\n    for col in range(2):\n        text = 'Mode: {}, By channels: {}'.format(params[num_iter][0], params[num_iter][1])\n        axes[row, col].set_title(text)\n        num_iter += 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.8. Fancy PCA\nAugment RGB image using FancyPCA from Krizhevsky's paper \"ImageNet Classification with Deep Convolutional Neural Networks\""},{"metadata":{"trusted":true},"cell_type":"code","source":"params = np.arange(0.1,1.0,0.1)\ntransform = [A.FancyPCA(alpha=param, always_apply=True) for param in params]\nfig, axes = visualize_multiple(3,3,img,transform)\n\nnum_iter = 0\nfor row in range(3):\n    for col in range(3):\n        text = 'Alpha: {:.1f}'.format(params[num_iter])\n        axes[row, col].set_title(text)\n        num_iter += 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.9. Gaussian Noise\nApply gaussian noise to the input image."},{"metadata":{"trusted":true},"cell_type":"code","source":"params = np.arange(1000, 10000, 1000)\ntransform = [A.GaussNoise(var_limit=(param-500, param), mean=0, always_apply=True) for param in params]\nfig, axes = visualize_multiple(3,3,img,transform)\n\nnum_iter = 0\nfor row in range(3):\n    for col in range(3):\n        text = 'Variance range: ({}, {})'.format(params[num_iter]-500, params[num_iter])\n        axes[row, col].set_title(text)\n        num_iter += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## 1.10. Gaussian Blur\nBlur the input image using a Gaussian filter with a random kernel size."},{"metadata":{"trusted":true},"cell_type":"code","source":"blur_limits = np.arange(3,39,4)\ntransform = [A.GaussianBlur(blur_limit=(limit, limit), always_apply=True) for limit in blur_limits]\nfig, axes = visualize_multiple(3,3,img,transform)\n\nnum_iter = 0\nfor row in range(3):\n    for col in range(3):\n        text = 'Kernel size: ({}, {})'.format(blur_limits[num_iter], blur_limits[num_iter])\n        axes[row, col].set_title(text)\n        num_iter += 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.11. Glass Blur\nApply glass noise to the input image."},{"metadata":{"trusted":true},"cell_type":"code","source":"sigmas = np.arange(0.5, 5.0, 0.5)\ntransform = [A.GlassBlur(sigma=sigma, max_delta=4, iterations=2, always_apply=True) for sigma in sigmas]\nfig, axes = visualize_multiple(3,3,img,transform)\n\nnum_iter = 0\nfor row in range(3):\n    for col in range(3):\n        text = 'Sigma: {}'.format(sigmas[num_iter])\n        axes[row, col].set_title(text)\n        num_iter += 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.12. Hue Saturation Value\nRandomly change hue, saturation and value of the input image."},{"metadata":{"trusted":true},"cell_type":"code","source":"transform = [A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, always_apply=True) for sigma in sigmas]\nfig, axes = visualize_multiple(3,3,img,transform)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.13. IAA Additive Gaussian Noise\nAdd gaussian noise to the input image."},{"metadata":{"trusted":true},"cell_type":"code","source":"locs = np.arange(10,100,10)\ntransform = [A.IAAAdditiveGaussianNoise(loc=loc, scale=(2.5500000000000003, 12.75), always_apply=True) for loc in locs]\nfig, axes = visualize_multiple(3,3,img,transform)\n\nnum_iter = 0\nfor row in range(3):\n    for col in range(3):\n        text = 'Mean: {}'.format(locs[num_iter])\n        axes[row, col].set_title(text)\n        num_iter += 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.14. IAA Sharpen\nSharpen the input image and overlays the result with the original image."},{"metadata":{"trusted":true},"cell_type":"code","source":"alphas = np.arange(0.1, 1.0, 0.1)\nlightness = np.arange(0.1, 1.0, 0.1)\ntransform = [A.IAASharpen(alpha=(alpha, alpha), lightness=(light, light), always_apply=True) for alpha, light in zip(alphas, lightness)]\nfig, axes = visualize_multiple(3,3,img,transform)\n\nnum_iter = 0\nfor row in range(3):\n    for col in range(3):\n        text = 'Alpha: {:.1f}, Lightness: {:.1f}'.format(alphas[num_iter], lightness[num_iter])\n        axes[row, col].set_title(text)\n        num_iter += 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.15. IAA Superpixels\nCompletely or partially transform the input image to its superpixel representation. Uses skimage's version of the SLIC algorithm. May be slow."},{"metadata":{"trusted":true},"cell_type":"code","source":"transform = [A.IAASuperpixels(p_replace=0.1, n_segments=100, always_apply=True) for _ in range(9)]\nfig, axes = visualize_multiple(3,3,img,transform)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.16. ISO Noise\nApply camera sensor noise."},{"metadata":{"trusted":true},"cell_type":"code","source":"color_shifts = np.arange(0.01, 0.1, 0.01)\nintensities = np.arange(0.1, 1.0, 0.1)\ntransform = [A.ISONoise(color_shift=(shift, shift), intensity=(intensity, intensity), always_apply=True) for shift, intensity in zip(color_shifts, intensities)]\nfig, axes = visualize_multiple(3,3,img,transform)\n\nnum_iter = 0\nfor row in range(3):\n    for col in range(3):\n        text = 'Color shift: {:.2f}, Intensity: {:.2f}'.format(color_shifts[num_iter], intensities[num_iter])\n        axes[row, col].set_title(text)\n        num_iter += 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.17. Invert Image\nInvert the input image by subtracting pixel values from 255."},{"metadata":{"trusted":true},"cell_type":"code","source":"transform = [A.InvertImg(p=1.0), A.InvertImg(p=0.0), A.InvertImg(p=0.0), A.InvertImg(p=1.0)]\nfig, axes = visualize_multiple(2,2,img,transform)\n\nnum_iter = 0\nfor row in range(2):\n    for col in range(2):\n        text = 'Inverted' if num_iter==0 or num_iter==3 else 'Normal'\n        axes[row, col].set_title(text)\n        num_iter += 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.18. Median Blur\nBlur the input image using a median filter with a random aperture linear size."},{"metadata":{"trusted":true},"cell_type":"code","source":"blur_limits = np.arange(5, 30, 2)\ntransform = [A.MedianBlur(blur_limit=(limit,limit), always_apply=True) for limit in blur_limits]\nfig, axes = visualize_multiple(3,3,img,transform)\n\nnum_iter = 0\nfor row in range(3):\n    for col in range(3):\n        text = 'Blur limit: {}'.format(blur_limits[num_iter])\n        axes[row, col].set_title(text)\n        num_iter += 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.19. Motion Blur\nApply motion blur to the input image using a random-sized kernel."},{"metadata":{"trusted":true},"cell_type":"code","source":"blur_limits = np.arange(7, 30, 2)\ntransform = [A.MotionBlur(blur_limit=(limit,limit), always_apply=True) for limit in blur_limits]\nfig, axes = visualize_multiple(3,3,img,transform)\n\nnum_iter = 0\nfor row in range(3):\n    for col in range(3):\n        text = 'Blur limit: {}'.format(blur_limits[num_iter])\n        axes[row, col].set_title(text)\n        num_iter += 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.20. Multiplicative Noise\nMultiply image to random number or array of numbers."},{"metadata":{"trusted":true},"cell_type":"code","source":"multipliers = np.arange(0.5, 1.5, 0.1)\ntransform = [A.MultiplicativeNoise(multiplier=plier, always_apply=True) for plier in multipliers]\nfig, axes = visualize_multiple(3,3,img,transform)\n\nnum_iter = 0\nfor row in range(3):\n    for col in range(3):\n        text = 'Multiplier: {:.1f}'.format(multipliers[num_iter])\n        axes[row, col].set_title(text)\n        num_iter += 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.21. Posterize\nReduce the number of bits for each color channel."},{"metadata":{"trusted":true},"cell_type":"code","source":"list_num_bits = np.arange(0,9,1)\ntransform = [A.Posterize(num_bits=int(num_bits), always_apply=True) for num_bits in list_num_bits]\nfig, axes = visualize_multiple(3,3,img,transform)\n\nnum_iter = 0\nfor row in range(3):\n    for col in range(3):\n        text = 'Num bits: {}'.format(list_num_bits[num_iter])\n        axes[row, col].set_title(text)\n        num_iter += 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.22. RGB Shift\nRandomly shift values for each channel of the input RGB image."},{"metadata":{"trusted":true},"cell_type":"code","source":"transform = [A.RGBShift(always_apply=True) for _ in range(9)]\nfig, axes = visualize_multiple(3,3,img,transform)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.23. Random Brightness Contrast\nRandomly change brightness and contrast of the input image."},{"metadata":{"trusted":true},"cell_type":"code","source":"transform = [A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, always_apply=True) for _ in range(9)]\nfig, axes = visualize_multiple(3,3,img,transform)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.24. Random Fog\nSimulates fog for the image"},{"metadata":{"trusted":true},"cell_type":"code","source":"transform = [A.RandomFog(fog_coef_lower=0.3, fog_coef_upper=1, alpha_coef=0.08, always_apply=True) for _ in range(9)]\nfig, axes = visualize_multiple(3,3,img,transform)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.25. Random Gamma"},{"metadata":{"trusted":true},"cell_type":"code","source":"transform = [A.RandomGamma(gamma_limit=(50, 250), always_apply=True) for _ in range(9)]\nfig, axes = visualize_multiple(3,3,img,transform)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.26. Random Rain\nAdds rain effects."},{"metadata":{"trusted":true},"cell_type":"code","source":"rain_types = [None, 'drizzle', 'heavy', 'torrential']\ntransform = [A.RandomRain(slant_lower=-10, slant_upper=10, \n                          drop_length=20, drop_width=1, drop_color=(200, 200, 200), \n                          blur_value=7, brightness_coefficient=0.7, \n                          rain_type=rain_type, always_apply=True) for rain_type in rain_types]\nfig, axes = visualize_multiple(2,2,img,transform)\n\nnum_iter = 0\nfor row in range(2):\n    for col in range(2):\n        text = 'Rain type: {}'.format(rain_types[num_iter])\n        axes[row, col].set_title(text)\n        num_iter += 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.27. Random Shadow\nSimulates shadows for the image"},{"metadata":{"trusted":true},"cell_type":"code","source":"transform = [A.RandomShadow(shadow_roi=(0, 0.5, 1, 1), \n                            num_shadows_lower=1, num_shadows_upper=4, \n                            shadow_dimension=5, always_apply=True) for _ in range(9)]\nfig, axes = visualize_multiple(3,3,img,transform)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.28. Random Snow \nBleach out some pixel values simulating snow."},{"metadata":{"trusted":true},"cell_type":"code","source":"transform = [A.RandomSnow(snow_point_lower=0.1, \n                          snow_point_upper=0.3, \n                          brightness_coeff=2.5, always_apply=True) for _ in range(9)]\nfig, axes = visualize_multiple(3,3,img,transform)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.29. Random Sun Flare\nSimulates Sun Flare for the image"},{"metadata":{"trusted":true},"cell_type":"code","source":"transform = [A.RandomSunFlare(flare_roi=(0, 0, 1, 0.5), \n                              angle_lower=0, angle_upper=1, \n                              num_flare_circles_lower=6, \n                              num_flare_circles_upper=10, \n                              src_radius=400, src_color=(255, 255, 255),\n                              always_apply=True) for _ in range(9)]\nfig, axes = visualize_multiple(3,3,img,transform)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.30. Solarize\nInvert all pixel values above a threshold."},{"metadata":{"trusted":true},"cell_type":"code","source":"thresholds = np.arange(10, 255, 20)\ntransform = [A.Solarize(threshold=int(thresh), always_apply=True) for thresh in thresholds]\nfig, axes = visualize_multiple(3,3,img,transform)\n\nnum_iter = 0\nfor row in range(3):\n    for col in range(3):\n        text = 'Threshold: {}'.format(thresholds[num_iter])\n        axes[row, col].set_title(text)\n        num_iter += 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2.0. Spatial-Level Transform\nSpatial-level transforms will simultaneously change both an input image as well as additional targets such as masks, bounding boxes, and keypoints."},{"metadata":{},"cell_type":"markdown","source":"## 2.1. Center Crop\nCrop the central part of the input."},{"metadata":{"trusted":true},"cell_type":"code","source":"crop_sizes = np.arange(50, 500, 50)\ntransform = [A.CenterCrop(height=crop_size, width=crop_size, always_apply=True) for crop_size in crop_sizes]\nfig, axes = visualize_multiple(3,3,img,transform)\n\nnum_iter = 0\nfor row in range(3):\n    for col in range(3):\n        text = 'Crop size: ({}, {})'.format(crop_sizes[num_iter], crop_sizes[num_iter])\n        axes[row, col].set_title(text)\n        num_iter += 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.2. Coarse Dropout\nCoarseDropout of the rectangular regions in the image."},{"metadata":{"trusted":true},"cell_type":"code","source":"hole_sizes = np.arange(10, 100, 10)\ntransform = [A.CoarseDropout(max_holes=8, max_height=hole_size, max_width=hole_size, \n                             min_holes=None, min_height=hole_size, min_width=hole_size,\n                             always_apply=True) for hole_size in hole_sizes]\nfig, axes = visualize_multiple(3,3,img,transform)\n\nnum_iter = 0\nfor row in range(3):\n    for col in range(3):\n        text = 'Hole size: ({}, {})'.format(hole_sizes[num_iter], hole_sizes[num_iter])\n        axes[row, col].set_title(text)\n        num_iter += 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.3. Crop\nCrop region from image.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"transform = [A.Crop(x_min=random.randint(0, 100), \n                    y_min=random.randint(0, 75), \n                    x_max=random.randint(200, img.shape[1]), \n                    y_max=random.randint(200, img.shape[0]), \n                    always_apply=True) for _ in range(9)]\nfig, axes = visualize_multiple(3,3,img,transform)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.4. Flip\nFlip the input either horizontally, vertically or both horizontally and vertically."},{"metadata":{"trusted":true},"cell_type":"code","source":"transform = [A.Flip(p=0.75) for _ in range(9)]\nfig, axes = visualize_multiple(3,3,img,transform)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.5. Grid Distortion"},{"metadata":{"trusted":true},"cell_type":"code","source":"transform = [A.GridDistortion(always_apply=True) for _ in range(9)]\nfig, axes = visualize_multiple(3,3,img,transform)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.6. Grid Dropout\nGridDropout, drops out rectangular regions of an image and the corresponding mask in a grid fashion."},{"metadata":{"trusted":true},"cell_type":"code","source":"transform = [A.GridDropout(ratio=0.5, \n                           unit_size_min=random.randint(0, 50), \n                           unit_size_max=random.randint(60, 100), \n                           holes_number_x=random.randint(0, 5), \n                           holes_number_y=random.randint(0, 5), \n                           always_apply=True) for _ in range(9)]\nfig, axes = visualize_multiple(3,3,img,transform)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.7. IAA Affine\nPlace a regular grid of points on the input and randomly move the neighbourhood of these point around via affine transformations."},{"metadata":{"trusted":true},"cell_type":"code","source":"transform = [A.IAAAffine(scale=1.0, \n                         translate_percent=random.randint(0,20), \n                         translate_px=None, \n                         rotate=random.randint(0,360), \n                         shear=random.randint(0,10), \n                         order=1, \n                         cval=0, \n                         mode='reflect',\n                         always_apply=True) for _ in range(9)]\nfig, axes = visualize_multiple(3,3,img,transform)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.8. Optical Distortion"},{"metadata":{"trusted":true},"cell_type":"code","source":"transform = [A.OpticalDistortion(distort_limit=0.75, shift_limit=0.75, always_apply=True) for _ in range(9)]\nfig, axes = visualize_multiple(3,3,img,transform)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.9. Random Grid Shuffle\nRandom shuffle grid's cells on image."},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_sizes = np.arange(2, 11, 1)\ntransform = [A.RandomGridShuffle(grid=(size,size), always_apply=True) for size in grid_sizes]\nfig, axes = visualize_multiple(3,3,img,transform)\n\nnum_iter = 0\nfor row in range(3):\n    for col in range(3):\n        text = 'Grid size: ({}, {})'.format(grid_sizes[num_iter], grid_sizes[num_iter])\n        axes[row, col].set_title(text)\n        num_iter += 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.10. Random Resized Crop\nTorchvision's variant of crop a random part of the input and rescale it to some size."},{"metadata":{"trusted":true},"cell_type":"code","source":"transform = [A.RandomResizedCrop(height=300, width=400, always_apply=True) for _ in range(9)]\nfig, axes = visualize_multiple(3,3,img,transform)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.11. Random Rotate 90\nRandomly rotate the input by 90 degrees zero or more times."},{"metadata":{"trusted":true},"cell_type":"code","source":"transform = [A.RandomRotate90(p=0.75) for _ in range(9)]\nfig, axes = visualize_multiple(3,3,img,transform)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.12. Random Scale\nRandomly resize the input. Output image size is different from the input image size."},{"metadata":{"trusted":true},"cell_type":"code","source":"transform = [A.RandomScale(scale_limit=0.5, always_apply=True) for _ in range(9)]\nfig, axes = visualize_multiple(3,3,img,transform)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.13. Transpose\nTranspose the input by swapping rows and columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"transform = [A.Transpose(p=0.5) for _ in range(9)]\nfig, axes = visualize_multiple(2,2,img,transform)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}