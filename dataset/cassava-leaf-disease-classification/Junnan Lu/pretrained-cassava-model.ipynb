{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport os\n# using python csv library for writing results\nimport csv\n# loading data in windows environment\nimport pathlib\nimport re\nprint (tf.__version__)\nfrom kaggle_datasets import KaggleDatasets\nimport PIL.Image\nimport cv2\n\n# Hardware platform: You may want to scale your training onto multiple GPUs on one machine, \n# or multiple machines in a network (with 0 or more GPUs each), or on Cloud TPUs.\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() \n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\n\nif tpu:\n    # Passing in the name of the CloudTPU\n    tf.config.experimental_connect_to_cluster(tpu)\n    # The TPU initialization code has to be at the beginning of the program\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    # A distribution strategy is an abstraction that can be used to drive models on CPU, GPUs or TPUs\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.experimental.CentralStorageStrategy()\n    print (\"GPU VERSION\")\n\n\n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\nGCS_DS_PATH = KaggleDatasets().get_gcs_path('cassava-leaf-disease-classification')\nGCS_PATH = GCS_DS_PATH + '/train_tfrecords'\n#TEST_GCS_PATH = GCS_DS_PATH + '/test_tfrecords'\nAUTO = tf.data.experimental.AUTOTUNE\n\n#TRAINING_FILENAMES = tf.io.gfile.glob(\"../input/cassava-leaf-disease-classification/\" + 'train_tfrecords/*.tfrec')\nTRAINING_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/*.tfrec')\n\n# Image height after resizing\nIMG_HEIGHT=386\n# Image width after resizing\nIMG_WIDTH=386\n\n# Helper Function \ndef mapping_image(dat):\n    # Extracting image content from tfrecords\n    TFREC_MAP = {\"image\": tf.io.FixedLenFeature([], tf.string)}\n    # Parsing the extracted content\n    parsed_dat = tf.io.parse_single_example(dat, TFREC_MAP)\n    # Decoding the image\n    image = tf.io.decode_image(parsed_dat['image'], channels=3)\n    # Normalizing the image\n    image = tf.cast(image/255, dtype=tf.float32)\n    return image\n\n# Helper Function\ndef mapping_id(dat):\n    # Extracting targeting label from tfrecords\n    TFREC_MAP = {\"target\": tf.io.FixedLenFeature([], tf.int64)}\n    # Parsing the targeting label\n    parsed_dat = tf.io.parse_single_example(dat, TFREC_MAP)\n    # Loading image label\n    image_id = parsed_dat['target']\n    # Casting image label into integer\n    image_id = tf.cast(image_id, dtype=tf.int32)\n    return image_id\n\n# Helper Function\ndef mapping_name(dat):\n    # Extracting targeting name from tfrecords\n    TFREC_MAP = {\"image_name\": tf.io.FixedLenFeature([], tf.string)}\n    # Parsing the image name\n    parsed_dat = tf.io.parse_single_example(dat, TFREC_MAP)\n    # Loading the image name\n    image_name = parsed_dat['image_name']\n    return image_name\n\n# Helper Function\ndef mapping_image_id(dat):\n    # Extracting image and targeting label from tfrecords\n    TFREC_MAP = {\"image\": tf.io.FixedLenFeature([], tf.string),\n                 \"target\": tf.io.FixedLenFeature([], tf.int64),\n                }\n    # Parsing the extracted data\n    parsed_dat = tf.io.parse_single_example(dat, TFREC_MAP)\n    # Decoding the image data\n    image = tf.io.decode_jpeg(parsed_dat['image'], channels=3)\n    # Normalizing image data\n    image = tf.cast(image/255, dtype=tf.float32)\n    # Loading the predicting label\n    target = parsed_dat['target']\n    # Casting the label into integer\n    target = tf.cast(target, dtype=tf.int32)\n    return image, target\n\n# Processing Input Training Data\ntrain_data_set = tf.data.TFRecordDataset(TRAINING_FILENAMES, num_parallel_reads=AUTO)\n# Extracting the Training Data\ntrain_data = train_data_set.map(mapping_image_id)\n# Shuffling the Training Data\ntrain_data = train_data.shuffle(1000, reshuffle_each_iteration=False)\n\n# TPU Training\nSTEPS_PER_EPOCH = 32\n\n# Number of iteration steps\nEPOCHS = 10\n\n# Image list\nIMG=[]\n# Label list\nLAB=[]\n\n# Generating Image and Labeling for Training\nL=0\n# Scaning through the training dataset\nfor dat, lab in train_data:\n    # Loading the first 16000 images for training\n    if (L>1000):\n        break\n    # Resizing the training image\n    re_image = tf.image.resize_with_pad(dat, IMG_HEIGHT,IMG_WIDTH, method=tf.image.ResizeMethod.BICUBIC,antialias=True)\n    # Generating Input Image\n    IMG.append(re_image)\n    # Generating Input Label\n    LAB.append(lab)\n    #print (L)\n    L=L+1\n\n# Inspecting the training image\n# Plotting the figure of 8X8\nfig = plt.figure(figsize=(8,8))\nax = fig.add_subplot()\nax.imshow(IMG[0])\nplt.show()\n\n# Converting Image Data into Tensor Object\nIMG = tf.convert_to_tensor(IMG)\n# Converting Label Data into Tensor Object\nLAB = tf.convert_to_tensor(LAB)\n\n# TPU training loop\nwith strategy.scope():\n    # Sequential Model\n    model = tf.keras.Sequential()\n    # Loading pretrained model\n    tensor_model = tf.keras.applications.DenseNet201(input_shape=[IMG_HEIGHT, IMG_WIDTH, 3], include_top=False, weights='imagenet')\n    # Freeze the layers\n    tensor_model.trainable = False\n    # Getting number of model layers\n    print (\"TOTAL LAYER {}\".format(len(tensor_model.layers)))\n    # Adding the pretrained model to the stack\n    model.add(tensor_model)\n    # Global Average Pooling\n    model.add(tf.keras.layers.GlobalAveragePooling2D())\n    # Final decision layer\n    model.add(tf.keras.layers.Dense(5))\n    # Model summary\n    model.summary()\n    # Applying the Sparse Categorical Cross Entropy as the loss function\n    model_loss_func = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n    # Gradiant Descent Optimizer \n    model_opt_func = tf.keras.optimizers.SGD(lr=0.00001)\n    # Computing the weighted mean value\n    metrics = tf.keras.metrics.Mean()\n    # Computes the crossentropy metric between the labels and predictions.\n    accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n    # Compiling the model\n    model.compile(optimizer=model_opt_func,loss=model_loss_func,metrics=[accuracy])\n\n\n# Train the Model\n# Saving the model\n# Storing in Dataset Kaggle\nhistory = model.fit(x=IMG,y=LAB, epochs=EPOCHS, steps_per_epoch=STEPS_PER_EPOCH)\nmodel.save('kaggle_cassava_model_Dense201.h5')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}