{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport cv2\nfrom matplotlib import pyplot as plt\nimport albumentations\nimport seaborn as sns\nfrom matplotlib import rcParams","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# List of Augmentations available"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":" temp = dir(albumentations)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/cassava-leaf-disease-classification/train.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"disease\"] = df.label.map({0:\"Cassava Bacterial Blight (CBB)\",\n1:\"Cassava Brown Streak Disease (CBSD)\",\n2:\"Cassava Green Mottle (CGM)\",\n3:\"Cassava Mosaic Disease (CMD)\",\n4:\"Healthy\"})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rcParams[\"figure.figsize\"] = 30,10\nsns.countplot(data = df,x=\"disease\",hue=\"label\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image = cv2.imread('../input/cassava-leaf-disease-classification/train_images/1000723321.jpg', 1)  # BGR\nprint(image.shape)\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Images are of size of**\n* Height - 600\n* Width - 400\n* Channel - 3"},{"metadata":{},"cell_type":"markdown","source":"Initial Image"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.title(\"Original Image without any Augmentations\")\nplt.imshow(image)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## RandomResizedCrop - Torchvision's variant of crop a random part of the input and rescale it to some size."},{"metadata":{"trusted":true},"cell_type":"code","source":"randomResizeCrop = albumentations.RandomResizedCrop(height=512,width=512)\nval = randomResizeCrop(image=image)\nimg1 =val[\"image\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.title(\"After Random Resized Crop\")\nplt.imshow(img1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Blur - Blur the input image using a random-sized kernel."},{"metadata":{"trusted":true},"cell_type":"code","source":"Blur = albumentations.Blur(blur_limit=10,p=0.3)\nval = Blur(image=image)\nimg2 = val[\"image\"]\nplt.title(\"After Blur\")\nplt.imshow(img2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## CenterCrop - Crop the central part of the input."},{"metadata":{"trusted":true},"cell_type":"code","source":"centerCrop = albumentations.CenterCrop(height=512,width=512,p=0.9,always_apply=False)\nval = centerCrop(image=image)\nimg3 = val[\"image\"]\nplt.title(\"After CenterCrop\")\nplt.imshow(img3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ChannelShuffle - Randomly rearrange channels of the input RGB image."},{"metadata":{"trusted":true},"cell_type":"code","source":"channelShuffle = albumentations.ChannelShuffle(p=0.8,always_apply=False)\nval4 = channelShuffle(image=image)\nimg4 = val4[\"image\"]\nplt.title(\"After Channel Shuffle\")\nplt.imshow(img4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ColorJitter - Randomly changes the brightness, contrast, and saturation of an image."},{"metadata":{"trusted":true},"cell_type":"code","source":"colorJitter = albumentations.ColorJitter(brightness=0.4,contrast=0.5,saturation=0.5,hue=0.3,p=0.7,always_apply=False)\nval5 = colorJitter(image=image)\nimg5 = val5[\"image\"]\nplt.title(\"After Color Jitter\")\nplt.imshow(img5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Flip - Flip the input either horizontally, vertically or both horizontally and vertically."},{"metadata":{"trusted":true},"cell_type":"code","source":"flip = albumentations.Flip(always_apply=False,p=0.9)\nval6 = flip(image=image)\nimg6 = val6[\"image\"]\nplt.title(\"After Flipping\")\nplt.imshow(img6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## GaussianBlur - Blur the input image using a Gaussian filter with a random kernel size."},{"metadata":{},"cell_type":"markdown","source":"* Blur Limit \n        maximum Gaussian kernel size for blurring the input image.Must be zero or odd and in range [0, inf) If set to 0 it will be computed from sigma as `round(sigma * (3 if img.dtype == np.uint8 else 4) * 2 + 1) + 1`.If set single value `blur_limit` will be in range (0, blur_limit) Default: (3, 7).\n* Sigma Limit\n        Gaussian kernel standard deviation. Must be greater in range [0, inf).If set single value `sigma_limit` will be in range (0, sigma_limit).If set to 0 sigma will be computed as `sigma = 0.3*((ksize-1)*0.5 - 1) + 0.8`. Default: 0."},{"metadata":{"trusted":true},"cell_type":"code","source":"gaussianBlur = albumentations.GaussianBlur(blur_limit=(5,9),sigma_limit=3,always_apply=False,p=0.7)\nval7 = gaussianBlur(image=image)\nimg7 = val7[\"image\"]\nplt.title(\"After Gaussian Blur\")\nplt.imshow(img7)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## MedianBlur -  Blur the input image using a median filter with a random aperture linear size."},{"metadata":{},"cell_type":"markdown","source":"* blur_limit\n        maximum aperture linear size for blurring the input image.Must be odd and in range [3, inf). Default: (3, 7)."},{"metadata":{"trusted":true},"cell_type":"code","source":"medianBlur = albumentations.MedianBlur(blur_limit=15,\n                                       always_apply=False,p=0.9)\nval8 = medianBlur(image=image)\nimg8 = val8[\"image\"]\nplt.title(\"After Median Blur\")\nplt.imshow(img8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## RandomFog - Simulates fog for the image"},{"metadata":{},"cell_type":"markdown","source":"* fog_coef_lower \n        fog_coef_lower (float): lower limit for fog intensity coefficient. Should be in [0, 1] range.\n* fog_coef_upper\n         fog_coef_upper (float): upper limit for fog intensity coefficient. Should be in [0, 1] range.\n* alpha_coef\n         alpha_coef (float): transparency of the fog circles. Should be in [0, 1] range.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"randomFog = albumentations.RandomFog(fog_coef_lower=0.7,fog_coef_upper=1,alpha_coef=0.8,always_apply=False,p=0.7)\nval9 = randomFog(image=image)\nimg9 = val9[\"image\"]\nplt.title(\"After Random Fog\")\nplt.imshow(img9)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## RandomSizedBBoxSafeCrop -  Crop a random part of the input and rescale it to some size without loss of bboxes."},{"metadata":{},"cell_type":"markdown","source":"* height (int): \n        height after crop and resize.\n* width (int): \n        width after crop and resize.\n* erosion_rate (float): \n        erosion rate applied on input image height before crop.\n* interpolation (OpenCV flag): \n        flag that is used to specify the interpolation algorithm. Should be one of:\n        cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n        Default: cv2.INTER_LINEAR.\n* p (float): \n        probability of applying the transform. Default: 1."},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# randomSizedBBox = albumentations.RandomSizedBBoxSafeCrop(height=512,width=512,erosion_rate=0.3,interpolation=1,p=0.7)\n# val10 = randomSizedBBox(image=image)\n# img10 = val10[\"image\"]\n# plt.title(\"After Random Sized BBoxSafeCrop\")\n# plt.imshow(img10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Solarize - Invert all pixel values above a threshold."},{"metadata":{},"cell_type":"markdown","source":"* Threshold - \n        range for solarizing threshold\n        ((int, int) or int, or (float, float) or float)"},{"metadata":{"trusted":true},"cell_type":"code","source":"solarize = albumentations.Solarize(threshold=140,always_apply=False,p=0.7)\nval11 = solarize(image=image)\nimg11 = val11[\"image\"]\nplt.title(\"After Solarize\")\nplt.imshow(img11)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ShiftScaleRotate - Randomly apply affine transforms: translate, scale and rotate the input."},{"metadata":{},"cell_type":"markdown","source":"*  shift_limit ((float, float) or float): \n        shift factor range for both height and width. If shift_limit\n        is a single float value, the range will be (-shift_limit, shift_limit). Absolute values for lower and\n        upper bounds should lie in range [0, 1]. Default: (-0.0625, 0.0625).\n*  scale_limit ((float, float) or float): \n        scaling factor range. If scale_limit is a single float value, the\n        range will be (-scale_limit, scale_limit). Default: (-0.1, 0.1).\n*  rotate_limit ((int, int) or int): \n        rotation range. If rotate_limit is a single int value, the\n        range will be (-rotate_limit, rotate_limit). Default: (-45, 45).\n*  interpolation (OpenCV flag): \n        flag that is used to specify the interpolation algorithm. Should be one of:\n        cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n        Default: cv2.INTER_LINEAR.\n*  border_mode (OpenCV flag): \n        flag that is used to specify the pixel extrapolation method. Should be one of:\n        cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101.\n        Default: cv2.BORDER_REFLECT_101\n* value (int, float, list of int, list of float): \n        padding value if border_mode is cv2.BORDER_CONSTANT.\n*  mask_value (int, float,\n                list of int,\n                list of float): padding value if border_mode is cv2.BORDER_CONSTANT applied for masks.\n*  shift_limit_x ((float, float) or float): shift factor range for width. If it is set then this value\n        instead of shift_limit will be used for shifting width.  If shift_limit_x is a single float value,\n        the range will be (-shift_limit_x, shift_limit_x). Absolute values for lower and upper bounds should lie in\n        the range [0, 1]. Default: None.\n*  shift_limit_y ((float, float) or float): shift factor range for height. If it is set then this value\n        instead of shift_limit will be used for shifting height.  If shift_limit_y is a single float value,\n        the range will be (-shift_limit_y, shift_limit_y). Absolute values for lower and upper bounds should lie\n        in the range [0, 1]. Default: None.\n*  p (float): probability of applying the transform. Default: 0.5."},{"metadata":{"trusted":true},"cell_type":"code","source":"shiftScaleRotate = albumentations.ShiftScaleRotate(shift_limit=0.0625,scale_limit=0.1,rotate_limit=90,interpolation=1,border_mode=4,value=None,mask_value=None,\n                                                       shift_limit_x=None,shift_limit_y=None,always_apply=False,p=0.5)\nval12 = shiftScaleRotate(image=image)\nimg12 = val12[\"image\"]\nplt.title(\"After Shift Scale Rotate\")\nplt.imshow(img12)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## HueSaturationValue - Randomly change hue, saturation and value of the input image."},{"metadata":{},"cell_type":"markdown","source":"* hue_shift_limit ((int, int) or int): range for changing hue. If hue_shift_limit is a single int, the range\n        will be (-hue_shift_limit, hue_shift_limit). Default: (-20, 20).\n*  sat_shift_limit ((int, int) or int): range for changing saturation. If sat_shift_limit is a single int,\n        the range will be (-sat_shift_limit, sat_shift_limit). Default: (-30, 30).\n*  val_shift_limit ((int, int) or int): range for changing value. If val_shift_limit is a single int, the range\n        will be (-val_shift_limit, val_shift_limit). Default: (-20, 20).\n*  p (float): probability of applying the transform. Default: 0.5."},{"metadata":{"trusted":true},"cell_type":"code","source":"hueSaturationValue = albumentations.HueSaturationValue(hue_shift_limit=40,sat_shift_limit=50,val_shift_limit=70,always_apply=False,p=0.5,)\nval13 = hueSaturationValue(image=image)\nimg13 = val13[\"image\"]\nplt.title(\"After Hue Saturation Value\")\nplt.imshow(img13)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## RandomBrightnessContrast - Randomly change brightness and contrast of the input image."},{"metadata":{},"cell_type":"markdown","source":"* brightness_limit ((float, float) or float): \n        factor range for changing brightness.\n        If limit is a single float, the range will be (-limit, limit). Default: (-0.2, 0.2).\n* contrast_limit ((float, float) or float):\n        factor range for changing contrast.\n        If limit is a single float, the range will be (-limit, limit). Default: (-0.2, 0.2).\n* brightness_by_max (Boolean): \n        If True adjust contrast by image dtype maximum,\n        else adjust contrast by image mean.\n* p (float):\n        probability of applying the transform. Default: 0.5."},{"metadata":{"trusted":true},"cell_type":"code","source":"randomBrightnessContrast = albumentations.RandomBrightnessContrast(brightness_limit=0.5,contrast_limit=0.7,brightness_by_max=True,always_apply=False,p=0.5)\nval14 = randomBrightnessContrast(image=image)\nimg14 = val14[\"image\"]\nplt.title(\"After Random Brightness Contrast\")\nplt.imshow(img14)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Normalize - Divide pixel values by 255 = 2**8 - 1, subtract mean per channel and divide by std per channel."},{"metadata":{},"cell_type":"markdown","source":"* mean (float, list of float): \n        mean values\n* std  (float, list of float): \n        std values\n* max_pixel_value (float):\n        maximum possible pixel value"},{"metadata":{"trusted":true},"cell_type":"code","source":"normalize = albumentations.Normalize(mean=(0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225),max_pixel_value=255.0,always_apply=False,p=1.0,)\nval15 = normalize(image=image)\nimg15 = val15[\"image\"]\nplt.title(\"After Normalize\")\nplt.imshow(img15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Coarse DropOut -  CoarseDropout of the rectangular regions in the image."},{"metadata":{},"cell_type":"markdown","source":"* max_holes (int): \n        Maximum number of regions to zero out.\n* max_height (int):\n        Maximum height of the hole.\n* max_width (int): \n        Maximum width of the hole.\n* min_holes (int):\n        Minimum number of regions to zero out. If `None`,\n        `min_holes` is be set to `max_holes`. Default: `None`.\n* min_height (int): \n        Minimum height of the hole. Default: None. If `None`,\n        `min_height` is set to `max_height`. Default: `None`.\n* min_width (int): \n        Minimum width of the hole. If `None`, `min_height` is\n        set to `max_width`. Default: `None`.\n* fill_value (int, float, lisf of int, list of float): \n        value for dropped pixels.\n* mask_fill_value (int, float, lisf of int, list of float): \n        fill value for dropped pixels\n        in mask. If None - mask is not affected."},{"metadata":{"trusted":true},"cell_type":"code","source":"coarseDropout = albumentations.CoarseDropout(max_holes=16,max_height=6,max_width=20,min_holes=None,min_height=None,min_width=None,fill_value=0,mask_fill_value=None,always_apply=False,p=0.5,)\nval16 = coarseDropout(image=image)\nimg16 = val16[\"image\"]\nplt.title(\"After Coarse DropOut\")\nplt.imshow(img16)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## CutOut - CoarseDropout of the square regions in the image."},{"metadata":{},"cell_type":"markdown","source":"* num_holes (int): \n        number of regions to zero out\n* max_h_size (int):\n        maximum height of the hole\n* max_w_size (int):\n        maximum width of the hole\n* fill_value (int, float, lisf of int, list of float):\n        value for dropped pixels."},{"metadata":{"trusted":true},"cell_type":"code","source":"cutOut = albumentations.Cutout(num_holes=16,max_h_size=10,max_w_size=4,fill_value=0,always_apply=False,p=0.5,)\nval17 = cutOut(image=image)\nimg17 = val17[\"image\"]\nplt.title(\"After Cut Out\")\nplt.imshow(img17)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## DownScale"},{"metadata":{},"cell_type":"markdown","source":"* scale_min (float):\n        lower bound on the image scale. Should be < 1.\n* scale_max (float):  \n        lower bound on the image scale. Should be .\n* interpolation: \n        cv2 interpolation method. cv2.INTER_NEAREST by default"},{"metadata":{"trusted":true},"cell_type":"code","source":"downscale = albumentations.Downscale(scale_min=0.15,scale_max=0.25,interpolation=0,always_apply=False,p=0.5,)\nval18 = downscale(image=image)\nimg18 = val18[\"image\"]\nplt.title(\"After Down Scale\")\nplt.imshow(img18)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## CLAHE - Apply Contrast Limited Adaptive Histogram Equalization to the input image."},{"metadata":{},"cell_type":"markdown","source":"* clip_limit (float or (float, float)): \n        upper threshold value for contrast limiting.\n        If clip_limit is a single float value, the range will be (1, clip_limit). Default: (1, 4).\n* tile_grid_size ((int, int)): \n        size of grid for histogram equalization. Default: (8, 8).\n* p (float): \n        probability of applying the transform. Default: 0.5."},{"metadata":{"trusted":true},"cell_type":"code","source":"clahe = albumentations.CLAHE(clip_limit=4.0,tile_grid_size=(8, 8),always_apply=False,p=0.5,)\nval19 = clahe(image=image)\nimg19 = val19[\"image\"]\nplt.title(\"After Clahe\")\nplt.imshow(img19)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## IAAAffine - Place a regular grid of points on the input and randomly move the neighbourhood of these point around via affine transformations."},{"metadata":{"trusted":true},"cell_type":"code","source":"iaaaffine = albumentations.IAAAffine(scale=3.0,translate_percent=None,translate_px=None,rotate=0.45,shear=0.0,order=1,cval=0,mode='reflect',always_apply=False,p=0.5,)\nval20 = iaaaffine(image=image)\nimg20 = val20[\"image\"]\nplt.title(\"After AIIIffine\")\nplt.imshow(img20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## GaussNoise -  Apply gaussian noise to the input image."},{"metadata":{},"cell_type":"markdown","source":"* var_limit ((float, float) or float): \n        variance range for noise. If var_limit is a single float, the range\n        will be (0, var_limit). Default: (10.0, 50.0).\n* mean (float): \n        mean of the noise. Default: 0\n* p (float): \n        probability of applying the transform. Default: 0.5."},{"metadata":{"trusted":true},"cell_type":"code","source":"gaussNoise = albumentations.GaussNoise(var_limit=(50.0, 110.0),mean=4,always_apply=False,p=0.5,)\nval21 = gaussNoise(image = image)\nimg21 = val21[\"image\"]\nplt.title(\"After  Gauss Noise\")\nplt.imshow(img21)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## HistogramMatching -  Apply histogram matching. It manipulates the pixels of an input image so that its histogram matches the histogram of the reference image. If the images have multiple channels, the matching is done independently for each channel, as long as the number of channels is equal in the input image and the reference.\n"},{"metadata":{},"cell_type":"markdown","source":"* reference_images (List[str] or List(np.ndarray)): \n        List of file paths for reference images\n        or list of reference images.\n* blend_ratio (float, float): \n        Tuple of min and max blend ratio. Matched image will be blended with original\n        with random blend factor for increased diversity of generated images.\n* read_fn (Callable): \n        Used-defined function to read image. Function should get image path and return numpy\n        array of image pixels.\n* p (float): \n        probability of applying the transform. Default: 1.0."},{"metadata":{},"cell_type":"markdown","source":"### some random images"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"refImag = [\"/kaggle/input/cassava-leaf-disease-classification/train_images/3459977804.jpg\",\n            \"/kaggle/input/cassava-leaf-disease-classification/train_images/1258625916.jpg\",\n            \"/kaggle/input/cassava-leaf-disease-classification/train_images/2174460518.jpg\",\n            \"/kaggle/input/cassava-leaf-disease-classification/train_images/4054194563.jpg\"]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def red_img(path):\n    image = cv2.imread(path, 1)  # BGR\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    return image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"histogramMatching = albumentations.HistogramMatching(reference_images= refImag,read_fn = red_img,blend_ratio=(0.5, 1.0),always_apply=False,p=0.5,)\nval22 = histogramMatching(image=image)\nimg22 = val22[\"image\"]\nplt.title(\"After Histogram Matching\")\nplt.imshow(img22)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ChannelDropout"},{"metadata":{},"cell_type":"markdown","source":"* channel_drop_range (int, int): \n        range from which we choose the number of channels to drop.\n* fill_value (int, float):\n        pixel value for the dropped channel.\n* p (float):\n        probability of applying the transform. Default: 0.5."},{"metadata":{"trusted":true},"cell_type":"code","source":"channelDropout = albumentations.ChannelDropout(channel_drop_range=(5,9),fill_value=2,always_apply=False,p=0.5,)\nval23 = channelDropout(image=image)\nimg23 = val23[\"image\"]\nplt.title(\"After Channel Dropout\")\nplt.imshow(img23)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ElasticTransform"},{"metadata":{},"cell_type":"markdown","source":"* alpha (float):\n* sigma (float): \n        Gaussian filter parameter.\n* alpha_affine (float): \n            The range will be (-alpha_affine, alpha_affine)\n* interpolation (OpenCV flag): \n        flag that is used to specify the interpolation algorithm. Should be one of:\n        cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4. Default: cv2.INTER_LINEAR.\n* border_mode (OpenCV flag): \n        flag that is used to specify the pixel extrapolation method. Should be one of:\n        cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101.\n        Default: cv2.BORDER_REFLECT_101\n* value (int, float, list of ints, list of float): \n        padding value if border_mode is cv2.BORDER_CONSTANT.\n* mask_value (int, float,\n        list of ints,\n        list of float): padding value if border_mode is cv2.BORDER_CONSTANT applied for masks.\n* approximate (boolean): Whether to smooth displacement map with fixed kernel size.\n        Enabling this option gives ~2X speedup on large images."},{"metadata":{"trusted":true},"cell_type":"code","source":"elasticTransform = albumentations.ElasticTransform(alpha=4,sigma=100,alpha_affine=50,interpolation=1,  border_mode=4,   value=None,\n                mask_value=None,always_apply=False,approximate=False,p=0.5,)\nval24 = elasticTransform(image=image)\nimg24 = val24[\"image\"]\nplt.title(\"After elasticTransform\")\nplt.imshow(img24)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Equalize -  Equalize the image histogram."},{"metadata":{},"cell_type":"markdown","source":"* mode (str): {'cv', 'pil'}. \n        Use OpenCV or Pillow equalization method.\n* by_channels (bool): \n        If True, use equalization by channels separately,\n        else convert image to YCbCr representation and use equalization by `Y` channel.\n* mask (np.ndarray, callable): \n        If given, only the pixels selected by\n        the mask are included in the analysis. Maybe 1 channel or 3 channel array or callable.\n        Function signature must include `image` argument.\n* mask_params (list of str): \n        Params for mask function."},{"metadata":{"trusted":true},"cell_type":"code","source":"equalize = albumentations.Equalize(mode='cv',by_channels=True,mask=None,mask_params=(),always_apply=False,p=0.9,)\nval25 = equalize(image=image)\nimg25 = val25[\"image\"]\nplt.title(\"After equalize\")\nplt.imshow(img25)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## FDA -  Fourier Domain Adaptation from https://github.com/YanchaoYang/FDA Simple \"style transfer\"."},{"metadata":{},"cell_type":"markdown","source":"* reference_images (List[str] or List(np.ndarray)): \n        List of file paths for reference images or list of reference images.\n* beta_limit (float or tuple of float): \n        coefficient beta from paper. Recommended less 0.3.\n* read_fn (Callable): \n        Used-defined function to read image. Function should get image path and return numpyarray of image pixels."},{"metadata":{"trusted":true},"cell_type":"code","source":"fda = albumentations.FDA(reference_images=refImag,read_fn=red_img)\nval26 = fda(image=image)\nimg26 = val26[\"image\"]\nplt.title(\"After FDA\")\nplt.imshow(img26)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## FancyPCA -  Augment RGB image using FancyPCA from Krizhevsky's paper\n\"ImageNet Classification with Deep Convolutional Neural Networks\""},{"metadata":{},"cell_type":"markdown","source":"* alpha (float):  \n        how much to perturb/scale the eigen vecs and vals.\n* scale is samples from gaussian distribution (mu=0, sigma=alpha)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"fancyPCA = albumentations.FancyPCA(alpha=2.3, always_apply=False, p=0.5)\nval27 = fancyPCA(image=image)\nimg27 = val27[\"image\"]\nplt.title(\"After Fancy PCA\")\nplt.imshow(img27)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## RandomSunFlare"},{"metadata":{},"cell_type":"markdown","source":"* flare_roi (float, float, float, float): \n        region of the image where flare will\n* appear (x_min, y_min, x_max, y_max).\n        All values should be in range [0, 1].\n* angle_lower (float): \n        should be in range [0, `angle_upper`].\n* angle_upper (float):\n        should be in range [`angle_lower`, 1].\n* num_flare_circles_lower (int): \n        lower limit for the number of flare circles.Should be in range [0, `num_flare_circles_upper`].\n* num_flare_circles_upper (int): \n        upper limit for the number of flare circles.Should be in range [`num_flare_circles_lower`, inf].\n* src_radius (int):\n        src_color ((int, int, int)): color of the flare"},{"metadata":{"trusted":true},"cell_type":"code","source":"randomFlare = albumentations.RandomSunFlare(flare_roi=(0, 0, 1, 0.5),angle_lower=0,angle_upper=1,num_flare_circles_lower=6,num_flare_circles_upper=10,\n                                            src_radius=600,src_color=(255, 255, 255),always_apply=False,p=0.5,)\nval28 = randomFlare(image=image)\nimg28 = val28[\"image\"]\nplt.title(\"After RandomSunFlare\")\nplt.imshow(img28)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}