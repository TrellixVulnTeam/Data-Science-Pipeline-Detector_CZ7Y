{"cells":[{"metadata":{},"cell_type":"markdown","source":"This notebook explores the data (TFRecord format) using a subsample of the YouTube-8M `frame-level` and `validate` data. \nTo work with the entire dataset, please refer to the Starter code on the [YouTube-8M github repo](https://github.com/google/youtube-8m)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\n\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"print(os.listdir(\"../input/frame-sample/frame\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading libraries & datasets\nimport tensorflow as tf\nimport numpy as np\nfrom IPython.display import YouTubeVideo\n\nframe_lvl_record = \"../input/frame-sample/frame/train00.tfrecord\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Video-level information (extracted from the frame-level files)"},{"metadata":{"trusted":true},"cell_type":"code","source":"vid_ids = []\nlabels = []\n\nfor example in tf.python_io.tf_record_iterator(frame_lvl_record):\n    tf_example = tf.train.Example.FromString(example)\n    vid_ids.append(tf_example.features.feature['id']\n                   .bytes_list.value[0].decode(encoding='UTF-8'))\n    labels.append(tf_example.features.feature['labels'].int64_list.value)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Number of videos in this tfrecord: ',len(vid_ids))\nprint('Picking a youtube video id:',vid_ids[13])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As described on the YouTube8M download page, for privacy reasons, the video id has been randomly generated and does not directly correspond to the actual YouTube video id. To convert the id into the actua YouTube video id, we follow link: http://data.yt8m.org/2/j/i/UL/UL00.js"},{"metadata":{"trusted":true},"cell_type":"code","source":"# With that video id, we can play the video\nYouTubeVideo('UzXQaOLQVCU')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Read the frame-level data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# due to execution time, we're only going to read the first video\n\nfeat_rgb = []\nfeat_audio = []\n\nfor example in tf.python_io.tf_record_iterator(frame_lvl_record):  \n    tf_seq_example = tf.train.SequenceExample.FromString(example)\n    n_frames = len(tf_seq_example.feature_lists.feature_list['audio'].feature)\n    sess = tf.InteractiveSession()\n    rgb_frame = []\n    audio_frame = []\n    # iterate through frames\n    for i in range(n_frames):\n        rgb_frame.append(tf.cast(tf.decode_raw(\n                tf_seq_example.feature_lists.feature_list['rgb']\n                  .feature[i].bytes_list.value[0],tf.uint8)\n                       ,tf.float32).eval())\n        audio_frame.append(tf.cast(tf.decode_raw(\n                tf_seq_example.feature_lists.feature_list['audio']\n                  .feature[i].bytes_list.value[0],tf.uint8)\n                       ,tf.float32).eval())\n        \n        \n    sess.close()\n    \n    feat_audio.append(audio_frame)\n    feat_rgb.append(rgb_frame)\n    break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The first video has %d frames' %len(feat_rgb[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Now let's explore the labels\nFirst, we'll find the most commonly used labels..."},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib import pyplot as plt\n%matplotlib inline\nfrom sklearn.manifold import TSNE\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab = pd.read_csv('../input/vocabulary.csv')\nprint(\"we have {} unique labels in the dataset\".format(len(vocab['Index'].unique())))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n = 30 # although, we'll only show those that appear in the 1,000 for this competition\nfrom collections import Counter\n\nlabel_mapping =  vocab[['Index', 'Name']].set_index('Index', drop=True).to_dict()['Name']\n\ntop_n = Counter([item for sublist in labels for item in sublist]).most_common(n)\ntop_n_labels = [int(i[0]) for i in top_n]\ntop_n_label_names = [label_mapping[x] for x in top_n_labels if x in label_mapping] # filter out the labels that aren't in the 1,000 used for this competition\nprint(top_n_label_names)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plot the relationships between each of these top labels...\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import networkx as nx\nfrom itertools import combinations\n\nG = nx.Graph()\n\nG.clear()\nfor list_of_nodes in labels:\n    filtered_nodes = set(list_of_nodes).intersection(set(top_n_labels) & \n                                                     set(vocab['Index'].unique()))  \n    for node1,node2 in list(combinations(filtered_nodes,2)): \n        node1_name = label_mapping[node1]\n        node2_name = label_mapping[node2]\n        G.add_node(node1_name)\n        G.add_node(node2_name)\n        G.add_edge(node1_name, node2_name)\n\nplt.figure(figsize=(9,9))\nnx.draw_networkx(G, font_size=\"12\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Read the validate data"},{"metadata":{"trusted":true},"cell_type":"code","source":"validate_record = \"../input/validate-sample/validate/validate00.tfrecord\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_vid_ids = []\nval_vid_labels = []\nsegment_start_times = []\nsegment_end_times = []\nsegment_labels = []\nsegment_scores = []\n\nfor example in tf.python_io.tf_record_iterator(validate_record):\n    tf_example = tf.train.Example.FromString(example)\n    val_vid_ids.append(tf_example.features.feature['id']\n                   .bytes_list.value[0].decode(encoding='UTF-8'))\n    val_vid_labels.append(tf_example.features.feature['labels'].int64_list.value)\n    segment_start_times.append(tf_example.features.feature['segment_start_times'].int64_list.value)\n    segment_end_times.append(tf_example.features.feature['segment_end_times'].int64_list.value)\n    segment_labels.append(tf_example.features.feature['segment_labels'].int64_list.value)\n    segment_scores.append(tf_example.features.feature['segment_scores'].float_list.value)\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By inspecting the labels and segment score data, you can see that label 1036 was in the segment 135:140, but _not_ in the other segments!"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(val_vid_ids[0])\nprint(val_vid_labels[0])\nprint(segment_start_times[0])\nprint(segment_end_times[0])\nprint(segment_labels[0])\nprint(segment_scores[0])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}