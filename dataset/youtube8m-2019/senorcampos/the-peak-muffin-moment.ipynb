{"cells":[{"metadata":{},"cell_type":"markdown","source":"This kernel takes a trip through the **the life-journey of a batch of chocolate muffins**.  Starting from the time that they were not even recognizable as eggs in a mixing bowl, to being spooned into a baking pan, the oven, and finally the big \n\n# Peak Muffin\n\nmoment when they are presented in unmistakable muffin wrappers.  By the end, they they look extremely muffin-like.\n\nThis kernel borrows from https://www.kaggle.com/jesucristo/analysis-youtube8m-2019, which itself seems to borrow from https://www.kaggle.com/inversion/starter-kernel-yt8m-2019-sample-data  "},{"metadata":{},"cell_type":"markdown","source":"### Load packages"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport csv\nimport networkx as nx\nfrom subprocess import check_output\nfrom wordcloud import WordCloud, STOPWORDS\nimport tensorflow as tf\nfrom IPython.display import YouTubeVideo\nplt.style.use('ggplot')\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n#import os\n#print(os.listdir(\"../input\"))\n#import warnings\n#warnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### read in a sample validation file"},{"metadata":{"trusted":true},"cell_type":"code","source":"# first, let's use validation data instead of train data.\n# only the validation data contains the segment start and times\n# and segment labels\n\n#frame_lvl_record = \"../input/frame-sample/frame/train00.tfrecord\"\nframe_lvl_record = \"../input/validate-sample/validate/validate00.tfrecord\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"# there are two tensor flow records of each\nprint(os.listdir(\"../input/frame-sample/frame\"))\nprint(os.listdir(\"../input/validate-sample/validate\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### extract tensor flow records into a human-readable format"},{"metadata":{"trusted":true},"cell_type":"code","source":"vid_ids = []   # each video has an id\nlabels = []    # these labels refer to the entire video (a number 0 - 3861)\nseg_start = [] # segment start times (appear to start only at multiples of 5 sec)\nseg_end = []   # segment end times (end 5 seconds after the corresponding start)\nseg_label = [] # the label for each start segment (same length as start, end)\nseg_scores = [] # the score for that label.  Scores are 1 = present, 0 = not present\nfor example in tf.python_io.tf_record_iterator(frame_lvl_record):\n    tf_example = tf.train.Example.FromString(example)\n    # thanks to the original kernel authors for figuring out how to do this:\n    vid_ids.append(tf_example.features.feature['id']\n                   .bytes_list.value[0].decode(encoding='UTF-8'))\n    labels.append(tf_example.features.feature['labels'].int64_list.value)\n    seg_start.append(tf_example.features.feature['segment_start_times'].int64_list.value)\n    seg_end.append(tf_example.features.feature['segment_end_times'].int64_list.value)\n    seg_label.append(tf_example.features.feature['segment_labels'].int64_list.value)\n    seg_scores.append(tf_example.features.feature['segment_scores'].float_list.value)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Find a the HERO of our story"},{"metadata":{"trusted":true},"cell_type":"code","source":"# my technique:\n# manually go through a few videos until I find a video for which \n# the seg_scores are not uniformly 1.0\n# in rec_id = 1, all of the seq_scores are 1.0, so this video will not \n# give much insight as to how a video could have precise event start time\n# rec_id = 5 has 0, 1, 1, 1, 0, which looks more promising\nrec_id = 5\nprint(labels[rec_id])\nprint(seg_start[rec_id])\nprint(seg_label[rec_id])\nprint(seg_scores[rec_id])\n\nprint('Picking a youtube video id:',vid_ids[rec_id])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note that only one of the video level labels, 1672, is used for the segment labels.  \nLooking at the vocabulary.csv, we can find that 1672 = \"**Muffins**\""},{"metadata":{"trusted":true},"cell_type":"code","source":"# quick verification of the segment label\nvocabulary = pd.read_csv('../input/vocabulary.csv')\nvocabulary[vocabulary['Index']==1672]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# When is a Muffin Born?"},{"metadata":{"trusted":true},"cell_type":"code","source":"# With that video id, we can play the video\n# apparently if you take the videoID, let's call it vvDD, \n# then you can put the following in a browser:\n# http://data.yt8m.org/2/j/i/vv/vvDD.js\n# so for rec_id = 5 of the first validation example, vvDD = Kt00,\n# so pointing a browser here: http://data.yt8m.org/2/j/i/Kt/Kt00.js\n# yields this: i(\"Kt00\",\"MZYaCFJogqo\");\n# I'd be interested in a more pythonic way of doing that.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# let's watch the video:\n\n#YouTubeVideo('FBQ00Vk7Obs')  #op00\n#YouTubeVideo('1Cb84yXZgZs')   #O900\nYouTubeVideo('MZYaCFJogqo')  #Kt00","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nWait for it to load.  You'll see that it is definately a video about muffins!  \n\nIt might be more interesting if you don't watch the whole video now.  I'll refer back to it in a minute...  \n\nNow let's see if we can get the **video features** to tell us when the muffins are born."},{"metadata":{},"cell_type":"markdown","source":"## Look to the video features..."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# let's read the video features (feat_rgb), \n# and the audio features while we're at it \n# (thanks again to earlier kernel providers)\n\nfeat_rgb = []\nfeat_audio = []\n\n# the example that I started with read the first record and then did a break\n# the continue statements at the beginning get us to the record of interest\n# but then do the same thing - break after reading in the video feature data.\ncur_rec = -1\nfor example in tf.python_io.tf_record_iterator(frame_lvl_record):\n    cur_rec += 1\n    if cur_rec < rec_id:\n        continue\n    tf_seq_example = tf.train.SequenceExample.FromString(example)\n    n_frames = len(tf_seq_example.feature_lists.feature_list['audio'].feature)\n    sess = tf.InteractiveSession()\n    rgb_frame = []\n    audio_frame = []\n    # iterate through frames\n    for i in range(n_frames):\n        rgb_frame.append(tf.cast(tf.decode_raw(\n                tf_seq_example.feature_lists.feature_list['rgb']\n                  .feature[i].bytes_list.value[0],tf.uint8)\n                       ,tf.float32).eval())\n        audio_frame.append(tf.cast(tf.decode_raw(\n                tf_seq_example.feature_lists.feature_list['audio']\n                  .feature[i].bytes_list.value[0],tf.uint8)\n                       ,tf.float32).eval())\n        \n        \n    sess.close()\n    \n    feat_audio.append(audio_frame)\n    feat_rgb.append(rgb_frame)\n    break\nprint('The video has %d frames' %len(feat_rgb[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualize the Feature Vectors"},{"metadata":{"trusted":true},"cell_type":"code","source":"# from here on out we'll switch to exploration in numpy\n# which I am more familiar with\n# first, we'll convert the rgb_frame values into a numpy array\nrgb_frame = np.array(rgb_frame)\n# the numpy array has the shape: (num frames, 1024)\n# where 1024 is the number of abstract \"features\" output by a neural network\nrgb_frame.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's take a look at these abstracted features:\n# the x-axis is time in seconds, the y-axis is the list of features\nplt.imshow(rgb_frame.T, aspect='auto');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"One thing is immediately obvious from just doing this image plot of the feature vectors.  There is an obviously different, and more fuzzy, vertical band corresponding to about 65 to 120 on the Y-axis.  \n\nNow would be a good time to actually watch the video.  \n\n67 seconds (1:07) corresponds to when the video transitions from cute opening video of  muffins and muffin ingredients, and to the harsh, grim, reality of breaking eggs.  Go ahead and look.\n\nOther interesting times below:  \n\n100 seconds (1:40): seg_labels says - no muffins  \n110 seconds (1:50): seg_labels says - still no muffins  \n\n121 seconds (2:01): batter being spooned into the muffin tray \nNote the transition in the image plot above\n\n145 seconds (2:25): seg_labels says - Muffins!  \n150 seconds (2:30): seg_labels says - Muffins!  \n175 seconds (2:55): seg_labels says - Muffins!  \n\nSo, according to the segment labels, the muffins were born some time between 110 seconds and 145 seconds. (1:50 - 2:25)  \n\nWhy did they choose 145 seconds as the time they would provide the label?  \n\nTo answer that, we'll do a bit more analysis\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# the following is a dot product of the feature vector with itself\n# it will yield a matrix of the size (num seconds, num seconds)\n# the value of the matrix will correspond to self-similarity of different seconds  \ncross_t = np.dot(rgb_frame,rgb_frame.T)\ncross_t.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Find Self-Similar blocks in the video"},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's look at it\nplt.imshow(cross_t, aspect = 'auto');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on the dot products, it looks like the video entered into a whole different phase, right around 120 seconds   \n\nThe block in the bottom right corner suggests that the video images from 120 seconds until about 190 seconds are all pretty similar with each other.  They are also a little bit similar to first 45 seconds or so.  \n\nBut the 120 - 190 block is very different from the 60 - 120 block.\n\nAnd coincidentally, the two segment labels corresponding to:\n\"no muffins\" came from very different 60 - 120 block, when there were indeed no muffins.\n\nThe actually chosen segments for \"muffins\" (== 1.0) correspond to periods of high self-similarity and extreme muffinness.  \n\nThis last point makes me think that the most winning answers for this competition might have more to do with finding the peak \"muffin\" momement, and not so much the exact moment that the muffin was born.  \n  "},{"metadata":{},"cell_type":"markdown","source":"I hope that this has been helpful.  I would appreciate comments below."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}