{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Import Packages\n\nimport numpy as np\nimport pandas as pd\nimport os\nimport math\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import linear_model\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Set up the paths of the files\n\ntrain_path = '../input/new-york-city-taxi-fare-prediction/train.csv'\ntest_path = '../input/new-york-city-taxi-fare-prediction/test.csv'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Input the Training Data and Evaluate. Only going to take 1M rows out of the 55M to reduce processing time\n\ntrain_data = pd.read_csv(train_path, nrows=5000000)\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Convert Pickup Datetime to a proper Datetime format\n\ntrain_data['pickup_datetime'] =  pd.to_datetime(train_data['pickup_datetime'],format='%Y-%m-%d %H:%M:%S %Z')\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Build out date values into separate columns\n\ntrain_data['pickup_year'] = train_data['pickup_datetime'].dt.year\ntrain_data['pickup_quarter'] = train_data['pickup_datetime'].dt.quarter\ntrain_data['pickup_month'] = train_data['pickup_datetime'].dt.month\ntrain_data['pickup_day'] = train_data['pickup_datetime'].dt.day\ntrain_data['pickup_hour'] = train_data['pickup_datetime'].dt.hour\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Drop columns we don't require\n\ntrain_data.drop(['key', 'pickup_datetime'], axis=1, inplace=True)\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create Cyclical Date Values\n\ntrain_data['pickup_month_cos']=np.cos((train_data['pickup_month']-1)*(2*(np.pi/12)))\ntrain_data['pickup_month_sin']=np.sin((train_data['pickup_month']-1)*(2*(np.pi/12)))\ntrain_data['pickup_day_cos']=np.cos((train_data['pickup_day']-1)*(2*(np.pi/30)))\ntrain_data['pickup_day_sin']=np.sin((train_data['pickup_day']-1)*(2*(np.pi/30)))\ntrain_data['pickup_quarter_cos']=np.cos((train_data['pickup_quarter']-1)*(2*(np.pi/4)))\ntrain_data['pickup_quarter_sin']=np.sin((train_data['pickup_quarter']-1)*(2*(np.pi/4)))\ntrain_data['pickup_hour_cos']=np.cos((train_data['pickup_hour']-1)*(2*(np.pi/24)))\ntrain_data['pickup_hour_sin']=np.sin((train_data['pickup_hour']-1)*(2*(np.pi/24)))\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Convert Year into Number of Years Historically from 2020.\n\ntrain_data['pickup_year_age']=2020-train_data['pickup_year']\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Drop existing pickup date fields and use just cyclical ones going forward\n\ntrain_data.drop(['pickup_year','pickup_quarter','pickup_month','pickup_day','pickup_hour'], axis=1, inplace=True)\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now that most of the data is ready for modelling, let's confirm that the fare amount is distributed okay\n\ntrain_data.hist(bins=10,column='fare_amount',figsize=(15,6))\n\n#Will not scale this as it looks quite good as-is already. However there are negative values we must remove those","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Filter based on analysis re-fares\n\ntrain_data.dropna(inplace=True) #Modelling resulted in NaN errors, dropping these\ntrain_data = train_data[(train_data.fare_amount > 0)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Evaluate the Passenger Count\n\ntrain_data.hist(column='passenger_count',figsize=(15,6))\n\n#It looks like this is having values over 10 people, not sure how that is possible for a taxi, will remove","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Ensure no negative passengers or where there are more than 10 passengers as most limos would have a 10 person maximum\n\ntrain_data = train_data[(train_data.passenger_count > 0) & (train_data.passenger_count < 10)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Next step is to build a distance function between the start and end points. Will use the Haversine distance calculation\n\ndef distance(pickup_latitude, pickup_longitude, dropoff_latitude, dropoff_longitude):\n    radius = 6371\n    pickup_latitude, pickup_longitude, dropoff_latitude, dropoff_longitude = map(np.radians,[pickup_latitude, pickup_longitude, dropoff_latitude, dropoff_longitude])\n    distance_latitude = dropoff_latitude - pickup_latitude\n    distance_longitude = dropoff_longitude - pickup_longitude\n    calculation = np.sin(distance_latitude/2.0)**2 + np.cos(pickup_latitude) * np.cos(dropoff_latitude) * np.sin(distance_longitude/2.0)**2\n    \n    return 2 * radius * np.arcsin(np.sqrt(calculation))\n\ntrain_data['distance'] = distance(train_data['pickup_latitude'], train_data['pickup_longitude'], train_data['dropoff_latitude'], train_data['dropoff_longitude'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Workbook from https://www.kaggle.com/gunbl4d3/xgboost-ing-taxi-fares mentioned to filter out inappropriate locations outside of the range in NYC\ntrain_data = train_data[(train_data.pickup_longitude > -80) & (train_data.pickup_longitude < -70) & (train_data.pickup_latitude > 35) & (train_data.pickup_latitude < 45) & (train_data.dropoff_longitude > -80) & (train_data.dropoff_longitude < -70) &\n        (train_data.dropoff_latitude > 35) & (train_data.dropoff_latitude < 45)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Split training data into training records and validation records. We cannot use test set as we do not know the outcome\n\nX_train, X_val, y_train, y_val = train_test_split(train_data.iloc[:, 1:], train_data['fare_amount'], test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fit Model(s) here and create model shells/parameters\n\n#linear_regression = linear_model.LinearRegression()\n#ridge_regression = linear_model.Ridge(alpha=0.5)\n#lasso_regression = linear_model.Lasso(alpha=0.1)\n#random_forest = RandomForestRegressor(max_depth=4,n_estimators=250, random_state=0)\n#gradient_boost = GradientBoostingRegressor()\nxg_boost = XGBRegressor(objective='reg:squarederror')\n#light_gbm = LGBMRegressor()\n\n\n#linear_regression.fit(X_train,y_train)\n#ridge_regression.fit(X_train,y_train)\n#lasso_regression.fit(X_train,y_train)\n#random_forest.fit(X_train,y_train)\n#gradient_boost.fit(X_train,y_train)\nxg_boost.fit(X_train,y_train)\n#light_gbm.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Predict validation result and measure it against actual\n\n#y_pred_linear_regression = linear_regression.predict(X_val)\n#y_pred_ridge_regression = ridge_regression.predict(X_val)\n#y_pred_lasso_regression = lasso_regression.predict(X_val)\n#y_pred_random_forest = random_forest.predict(X_val)\n#y_pred_gradient_boost = gradient_boost.predict(X_val)\ny_pred_xg_boost = xg_boost.predict(X_val)\n#y_pred_light_gbm = light_gbm.predict(X_val)\n\n#print('Linear Regression - Root Mean Squared Error: %.2f'\n#      % math.sqrt(mean_squared_error(y_val, y_pred_linear_regression)))\n#print('Ridge Regression - Root Mean Squared Error: %.2f'\n#      % math.sqrt(mean_squared_error(y_val, y_pred_ridge_regression)))\n#print('Lasso Regression - Root Mean Squared Error: %.2f'\n#      % math.sqrt(mean_squared_error(y_val, y_pred_lasso_regression)))\n#print('Random Forest - Root Mean Squared Error: %.2f'\n#      % math.sqrt(mean_squared_error(y_val, y_pred_random_forest)))\n#print('Gradient Boost - Root Mean Squared Error: %.2f'\n#      % math.sqrt(mean_squared_error(y_val, y_pred_gradient_boost)))\nprint('XG Boost - Root Mean Squared Error: %.2f'\n      % math.sqrt(mean_squared_error(y_val, y_pred_xg_boost)))\n#print('Light GBM - Root Mean Squared Error: %.2f'\n#      % math.sqrt(mean_squared_error(y_val, y_pred_light_gbm)))\n\n#Result on 100k record sample shows XG Boost is the strongest performer by a good margin\n#Result on 1M record sample shows XG Boost is still the winner and the RMSE is very similar to the 100k sample.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Bring in Test Data now and transform it appropriately\n\ntest_data = pd.read_csv(test_path)\ntest_data['pickup_datetime'] =  pd.to_datetime(test_data['pickup_datetime'],format='%Y-%m-%d %H:%M:%S %Z')\ntest_data['pickup_year'] = test_data['pickup_datetime'].dt.year\ntest_data['pickup_quarter'] = test_data['pickup_datetime'].dt.quarter\ntest_data['pickup_month'] = test_data['pickup_datetime'].dt.month\ntest_data['pickup_day'] = test_data['pickup_datetime'].dt.day\ntest_data['pickup_hour'] = test_data['pickup_datetime'].dt.hour\ntest_data['pickup_month_cos']=np.cos((test_data['pickup_month']-1)*(2*(np.pi/12)))\ntest_data['pickup_month_sin']=np.sin((test_data['pickup_month']-1)*(2*(np.pi/12)))\ntest_data['pickup_day_cos']=np.cos((test_data['pickup_day']-1)*(2*(np.pi/30)))\ntest_data['pickup_day_sin']=np.sin((test_data['pickup_day']-1)*(2*(np.pi/30)))\ntest_data['pickup_quarter_cos']=np.cos((test_data['pickup_quarter']-1)*(2*(np.pi/4)))\ntest_data['pickup_quarter_sin']=np.sin((test_data['pickup_quarter']-1)*(2*(np.pi/4)))\ntest_data['pickup_hour_cos']=np.cos((test_data['pickup_hour']-1)*(2*(np.pi/24)))\ntest_data['pickup_hour_sin']=np.sin((test_data['pickup_hour']-1)*(2*(np.pi/24)))\ntest_data['pickup_year_age']=2020-test_data['pickup_year']\ntest_data.drop(['pickup_year','pickup_quarter','pickup_month','pickup_day','pickup_hour','pickup_datetime'], axis=1, inplace=True)\ntest_data['distance'] = distance(test_data['pickup_latitude'], test_data['pickup_longitude'], test_data['dropoff_latitude'], test_data['dropoff_longitude'])\ntest_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Make predictions on Test Set\n\ny_predictions = xg_boost.predict(test_data.iloc[:, 1:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Send Test Set predictions to file for upload\n\nsubmission = pd.DataFrame({'key': test_data['key'], 'fare_amount': y_predictions},columns = ['key', 'fare_amount'])\nsubmission.to_csv('submission.csv', index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}