{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('dark_background')\nsns.set_style(\"darkgrid\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7fca7d8cd4e325ece65b6da483fa54d05223d55b"},"cell_type":"code","source":"%%time\n# Reading File\ntrain_path  = '../input/train.csv'\n\n# Set columns to most suitable type to optimize for memory usage, default is float 64 but we just need float 32, it will save a lot of RAM\ntraintypes = {'fare_amount': 'float32',\n              'pickup_datetime': 'str', \n              'pickup_longitude': 'float32',\n              'pickup_latitude': 'float32',\n              'dropoff_longitude': 'float32',\n              'dropoff_latitude': 'float32',\n              'passenger_count': 'uint8'}\n\ncols = list(traintypes.keys())\n# I used 2.000.000 rows to test and 10.000.000 to commit\ntrain_df = pd.read_csv(train_path, usecols=cols, dtype=traintypes, nrows=2_000_000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b5b1c9edcdba79e27521a84c146948732edc9c76"},"cell_type":"code","source":"%%time\n# Save into feather format, it will be faster for the next time \ntrain_df.to_feather('nyc_taxi_data_raw.feather')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"44ad8450ec48d1acdd03d60ef5893757c86c5db5"},"cell_type":"code","source":"# load the same dataframe next time directly, without reading the csv file again!\n\ndf_train = pd.read_feather('nyc_taxi_data_raw.feather')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bb3457c3c81185d9a2c893b6ca543424d635035a"},"cell_type":"code","source":"# check datatypes\ndf_train.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9c223c2ea72086d6b94f77f56b7c6493e7f8be81"},"cell_type":"code","source":"# check statistics of the features\ndf_train.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ce9a0f0212dbbbaf74aefd2cce066d0d0157fc21"},"cell_type":"markdown","source":"**We need to look at data with fare_amount < 0 ( It looks like a refund when passenger_count = 0). If it's too many we need to concern and try to fill them with some numbers but they are a few number, we can just drop them.**","execution_count":null},{"metadata":{"trusted":true,"_uuid":"3e712f6dcaf7dfce4220d3bae52d9d7acc748896"},"cell_type":"code","source":"len(df_train[df_train.fare_amount > 0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c814d910ea376c60ba6a3843800779e2880dc619"},"cell_type":"code","source":"# Since they are less than 300 records so we will drop them ( for 2.000.000 rows)\ndf_train = df_train[df_train.fare_amount>=0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2bb78424c1a6baf2da5b372fe50e4bf838a13005"},"cell_type":"code","source":"# IQR is 3.5 so we can see most data less than 20 but we will plot data to 100 to see some outliers\nsns.distplot(df_train[df_train.fare_amount < 100].fare_amount, bins=50);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"371797ddc5d9d8662eab168af00b58448f49d400"},"cell_type":"code","source":"# Count the number of null value\ndf_train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4dc17711ecac07c560c9fb5e24bde3b9cb943ed4"},"cell_type":"code","source":"# it's not too much so we will drop all row with null value\ndf_train = df_train.dropna(how = 'any', axis = 'rows')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ba0244092fe208bc0cfe02bfa10d86ea036b9341"},"cell_type":"code","source":"# read test data\ndf_test =  pd.read_csv('../input/test.csv')\ndf_test.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cef2208da2d95a50469ce33a7b51053acb30f840"},"cell_type":"code","source":"df_test.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c6d4be7a9b3ba4f366a5898665d9580d17ea2495"},"cell_type":"code","source":"# We will paste pickup date to date type\ndf_train['pickup_datetime'] = pd.to_datetime(df_train['pickup_datetime'],format=\"%Y-%m-%d %H:%M:%S UTC\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"193c11a08931538f39f7518f62abd628783f281d"},"cell_type":"code","source":"df_train['pickup_datetime']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e37cc3518542741711fdd7748d284582fb87dffe"},"cell_type":"code","source":"df_test['pickup_datetime'] = pd.to_datetime(df_test['pickup_datetime'],format=\"%Y-%m-%d %H:%M:%S UTC\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2c9bc0ee7cb27f418bf689566499bd77653a84ba"},"cell_type":"markdown","source":"We will extract more features from Pickup Datetime to hour, day, month, year, day_of_week","execution_count":null},{"metadata":{"trusted":true,"_uuid":"0b29486939c52067078063d0e48f09e64bd88e15"},"cell_type":"code","source":"def add_new_date_time_features(dataset):\n    dataset['hour'] = dataset.pickup_datetime.dt.hour\n    dataset['day'] = dataset.pickup_datetime.dt.day\n    dataset['month'] = dataset.pickup_datetime.dt.month\n    dataset['year'] = dataset.pickup_datetime.dt.year\n    dataset['day_of_week'] = dataset.pickup_datetime.dt.dayofweek\n    \n    return dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"03cdd78176cdb6c20ba4741de757555d8d0e7404"},"cell_type":"code","source":"df_train = add_new_date_time_features(df_train)\ndf_test = add_new_date_time_features(df_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cd9ecebef181011052271368a667a69c2995c01d"},"cell_type":"code","source":"df_train.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7f42b92e52b61bbe27da31563b529d9c941435cd"},"cell_type":"markdown","source":"<h1> Long & Lat</h1>\nFor longitude and lattitude, we will have some stategies:\n* We can use the max and min in testing set to make a limit to our training data\n* We can check boundaries for New York on map and filter our data base on min and max of long, lat  \n* We drop invalid long, lat in our data\n-------------------------------------------------------------------------------------------------------------------------------------------------\nIn this situation, we will set the max and min with our data test set because our target is get the best result for test dataset but in real life, I prefer check it more carefuly and make a boundary around area we want to predict ( for example 1000 miles from NYC).\n","execution_count":null},{"metadata":{"trusted":true,"_uuid":"b1169dd3b9f2349864b15fd1918f4f54a0e660c9"},"cell_type":"code","source":"\ndf_train = df_train[df_train.pickup_longitude.between(df_test.pickup_longitude.min(), df_test.pickup_longitude.max())]\ndf_train = df_train[df_train.pickup_latitude.between(df_test.pickup_latitude.min(), df_test.pickup_latitude.max())]\ndf_train = df_train[df_train.dropoff_longitude.between(df_test.dropoff_longitude.min(), df_test.dropoff_longitude.max())]\ndf_train = df_train[df_train.dropoff_latitude.between(df_test.dropoff_latitude.min(), df_test.dropoff_latitude.max())]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e83835f00fafb8b671fb695872d5f459ff3aa7d4"},"cell_type":"code","source":"df_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d31a177a8a8ca2ee0cdd0e1d4e5a022ec7d10a7b"},"cell_type":"markdown","source":"<h1>Distance</h1>\n\nIt is not too much meaning if we use coordinates for our model, so we will extract distance, it will be more important for our model. <br>\nWe will 3 ways to calculate the distance: Euclidean Distance, Haversine and Manhattan Distance<br>\n1. Euclidean Distance, it's the easiest way to calculate distance between 2 points but it is \"bird fly\" distance so we will can get a big different between our estimate and real life distance\n2. Haversine determines the great-circle distance between two points on a sphere given their longitudes and latitudes so it will give us the better estimate when we use radius of Earth to calculate\n3. Manhattan Distance will calculate distance between 2 point base on blocks so its estimate will be good for us (https://en.wikipedia.org/wiki/Taxicab_geometry)\n\n","execution_count":null},{"metadata":{"trusted":true,"_uuid":"d5ad0659bfb27e490a59a681820841217e3a3255"},"cell_type":"code","source":"def calculate_abs_different(df):\n    df['abs_diff_longitude'] = (df.dropoff_longitude - df.pickup_longitude).abs()\n    df['abs_diff_latitude'] = (df.dropoff_latitude - df.pickup_latitude).abs()\ncalculate_abs_different(df_train)\ncalculate_abs_different(df_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"20b155b9c72285ae30208dbe079c8f6172549d28"},"cell_type":"code","source":"# Since we are calculating this at New York, we can assign a constant, rather than using a formula\n# longitude = degrees of latitude in radians * 69.172\n#1 degree of longitude = 50 miles\ndef convert_different_miles(df):\n    df['abs_diff_longitude'] = df.abs_diff_longitude*50\n    df['abs_diff_latitude'] = df.abs_diff_latitude*69\nconvert_different_miles(df_train)\nconvert_different_miles(df_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fef17e6ad5d787aa7fedf4cc9225476a1d1036c2"},"cell_type":"code","source":"### Angle difference between north, and manhattan roadways\nmeas_ang = 0.506 # 29 degrees = 0.506 radians (https://en.wikipedia.org/wiki/Commissioners%27_Plan_of_1811)\nimport math\n\n\n## adding extra features\ndef add_distance(df):\n    df['Euclidean'] = (df.abs_diff_latitude**2 + df.abs_diff_longitude**2)**0.5 ### as the crow flies  \n    df['delta_manh_long'] = (df.Euclidean*np.sin(np.arctan(df.abs_diff_longitude / df.abs_diff_latitude)-meas_ang)).abs()\n    df['delta_manh_lat'] = (df.Euclidean*np.cos(np.arctan(df.abs_diff_longitude / df.abs_diff_latitude)-meas_ang)).abs()\n    df['distance'] = df.delta_manh_long + df.delta_manh_lat\n    df.drop(['abs_diff_longitude', 'abs_diff_latitude','Euclidean', 'delta_manh_long', 'delta_manh_lat'], axis=1, inplace=True)\nadd_distance(df_train)\nadd_distance(df_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7f67cc21acacb22c4cdbb414106d53ac7833ff4e"},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ece4cd114463df0e8876a16269e10cb883c4d286"},"cell_type":"markdown","source":"<h1> We will extract 1 more feature, it's direction</h1>\n* When we extract feture distance, it's estimate and it's not exactly the distance in reality. \n* Direction wil give us more information, if the angle is 45*, it means the distance in real life will be longer because it needs to travel though more blocks in the city.\n![image.png](attachment:image.png)","attachments":{},"execution_count":null},{"metadata":{"trusted":true,"_uuid":"7e077a8930f34f48bbf2f071c626f9383c2aa79b"},"cell_type":"code","source":"def calculate_direction(pickup_lat, pickup_lon, dropoff_lat, dropoff_lon):\n    \"\"\"\n    Return distance along great radius between pickup and dropoff coordinates.\n    \"\"\"\n    #Define earth radius (km)\n    R_earth = 6371\n    #Convert degrees to radians\n    pickup_lat, pickup_lon, dropoff_lat, dropoff_lon = map(np.radians,\n                                                             [pickup_lat, pickup_lon, \n                                                              dropoff_lat, dropoff_lon])\n    #Compute distances along lat, lon dimensions\n    dlat = dropoff_lat - pickup_lat\n    dlon = pickup_lon - dropoff_lon\n    \n    #Compute bearing distance\n    a = np.arctan2(np.sin(dlon * np.cos(dropoff_lat)),np.cos(pickup_lat) * np.sin(dropoff_lat) - np.sin(pickup_lat) * np.cos(dropoff_lat) * np.cos(dlon))\n    return a","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"89c1d509af30996dcb3cb0d6f3d130bf5b02eac7"},"cell_type":"code","source":"# We will convert pandas to numpy to get the best performance\ntrain_df['direction'] = calculate_direction(train_df['pickup_latitude'].values, train_df['pickup_longitude'].values, \n                                   train_df['dropoff_latitude'].values , train_df['dropoff_longitude'].values) \ndf_test['direction'] = calculate_direction(df_test['pickup_latitude'].values, df_test['pickup_longitude'].values, \n                                   df_test['dropoff_latitude'].values , df_test['dropoff_longitude'].values) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ca9460bfb674874739bdbd52b2d96f6772982bf7"},"cell_type":"markdown","source":"<h1>Normalize lat and long</h1>\n* After we extract more feature from long and lat, they are less important so we will normalize them to make sure they don't impact too much to our model.\n* We will convert them to radian so their range will shink from range of degrees ( 1 -> 360) to range of radians. ","execution_count":null},{"metadata":{"trusted":true,"_uuid":"d43adb9c0fb9e430e69e5eb7dda259914122dfff"},"cell_type":"code","source":"train_df['pickup_latitude'].apply(lambda x: np.radians(x))\ntrain_df['pickup_longitude'].apply(lambda x: np.radians(x))\ntrain_df['dropoff_latitude'].apply(lambda x: np.radians(x))\ntrain_df['dropoff_longitude'].apply(lambda x: np.radians(x))\n\ndf_test['pickup_latitude'].apply(lambda x: np.radians(x))\ndf_test['pickup_longitude'].apply(lambda x: np.radians(x))\ndf_test['dropoff_latitude'].apply(lambda x: np.radians(x))\ndf_test['dropoff_longitude'].apply(lambda x: np.radians(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4c8c63392c915da2b81da83025e9829eeefa77f0"},"cell_type":"code","source":"sns.jointplot(x='distance', y='fare_amount', data=df_train)\n# We can see that distance is less than 100 so it makes sense and we can use it","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e1a561f9274eb4e08d9a80f5bcf5c7b6056baefb"},"cell_type":"code","source":"# We extracted feature with day, week, month, year so we can remove pickup_datetime\ndf_train.drop(columns=['pickup_datetime'], inplace=True)\n\ny = df_train['fare_amount']\ndf_train = df_train.drop(columns=['fare_amount'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b022acd76415464f938c85b1c06f20f926d1480f"},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"42eae92690a9aec8c0f870ce3dc4bd7b0550b9c4"},"cell_type":"markdown","source":"<h1>Modeling</h1>\n* Since I don't have time and machine so I just choose lightgbm to build model\n* It is fast and good for train a lot of data.\n* If I have time and machine I will try with Random Forest, XGBoost, do something like grid search, random search to get the best model but for this homework I just run it 1 time\n\n","execution_count":null},{"metadata":{"trusted":true,"_uuid":"c7640dcbb77aaa19ca0257f0719d9f6483d5378a"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport lightgbm as lgbm\nx_train,x_test,y_train,y_test = train_test_split(df_train,y,random_state=123,test_size=0.1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"353416f1be61fa66c6be92edaaacec5ba7a3480c"},"cell_type":"markdown","source":"<h1>Params</h1>\nI will choose some default value and modify it later if necessary, for example \n* if I see it overfitting, I will use smaller max_bin and num_leaves\n*  if I see it is low accuracy, I will use larger max_bin and num_leaves\n*  The best way is run it in a grid search with combine of paramenters but it will be slow so I will make it simple .","execution_count":null},{"metadata":{"trusted":true,"_uuid":"7408884d40ce5799ff455fb443186f42369b6bec"},"cell_type":"code","source":"params = {\n        'boosting_type':'gbdt',\n        'objective': 'regression',\n        'nthread': 4,\n        'num_leaves': 31,\n        'learning_rate': 0.1,\n        'max_depth': -1,\n        'subsample': 0.8,\n        'bagging_fraction' : 1,\n        'max_bin' : 10000 ,\n        'bagging_freq': 10,\n        'metric': 'rmse',  \n        'zero_as_missing': True,\n        'num_rounds':50000\n    }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ace37fb0c98cdbbe8019b53770f547bf68717029"},"cell_type":"code","source":"train_set = lgbm.Dataset(x_train, y_train, silent=False,categorical_feature=['year','month','day','day_of_week'])\nvalid_set = lgbm.Dataset(x_test, y_test, silent=False,categorical_feature=['year','month','day','day_of_week'])\nmodel = lgbm.train(params, train_set = train_set, num_boost_round=10000,early_stopping_rounds=1000,verbose_eval=500, valid_sets=valid_set)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c35ff7bdff39337c0dd542ec466eed6aee35bc69"},"cell_type":"code","source":"df_train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a2a3c021e62fa19b5888c5d4c94cc00a9a1f26d3"},"cell_type":"code","source":"test_key = df_test['key']\n\ndf_test.drop(columns=[\"pickup_datetime\",'key'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"59ac187e77b42e4516197cec6b9b7e1be7ad2e01"},"cell_type":"code","source":"prediction = model.predict(df_test, num_iteration = model.best_iteration)      \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0a4f76b11a8bb8620c7a5d70fdaf2cde2b654fdc"},"cell_type":"code","source":"submission = pd.DataFrame({\n        \"key\": test_key,\n        \"fare_amount\": prediction\n})\n\nsubmission.to_csv('taxi_fare_submission.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}