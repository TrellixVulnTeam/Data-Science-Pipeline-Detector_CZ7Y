{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# TAXI Fare Prediction - EDA Parts","metadata":{}},{"cell_type":"markdown","source":"## 0. Data Load","metadata":{}},{"cell_type":"markdown","source":"#### Import librairies","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nimport geopandas as gpd\nfrom shapely.geometry import Point, Polygon\n\nimport ssl\n\ncontext = ssl._create_unverified_context()\nplt.style.use('fivethirtyeight')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load data\nAs the entire dataset is about 55M rows, only part of the dataset is used for EDA","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('../input/new-york-city-taxi-fare-prediction/train.csv', nrows=500_000)\ntest_df = pd.read_csv('../input/new-york-city-taxi-fare-prediction/test.csv', nrows=500_000)\n\nprint('Number of train: {}'.format(train_df.shape))\nprint('Number of test: {}'.format(test_df.shape))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Match the datetime format\nThis process is time-consuming. Therefore matching the datetime format after data cleaning is better.<br>\nHowever, I perform it previously to check the datetime format during the EDA and claning.","metadata":{}},{"cell_type":"code","source":"train_df['key'] = pd.to_datetime(train_df['key'])\ntrain_df['pickup_datetime'] = pd.to_datetime(train_df['pickup_datetime'])\n\ntest_df['key'] = pd.to_datetime(test_df['key'])\ntest_df['pickup_datetime'] = pd.to_datetime(test_df['pickup_datetime'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Check missing value\nBefore checking the data, the basic level of data cleaning is performed. <br>\nFirst of all, we should check the missing value.","metadata":{}},{"cell_type":"code","source":"train_df.isnull().sum().sort_values(ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.isnull().sum().sort_values(ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The number of missing value can be changed according to the data size which is selected on the data loading step. <br>\nPortion of the missing value is quite small to neglectable. </br>\nTherefore just <code>dropna</code> the missing values.","metadata":{}},{"cell_type":"code","source":"train_df = train_df.dropna()\ntest_df = test_df.dropna() # Although there is no missing value in test dataset, perform the dropna","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Number of Missing values in train: {}'.format(train_df.isnull().sum().sum()))\nprint('Number of Missing values in test: {}'.format(test_df.isnull().sum().sum()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In conclusion, there is no missing values or null values in the dataset.","metadata":{}},{"cell_type":"markdown","source":"## 1. Data Cleaning\nBasic level of EDA is performed using the train dataset above.","metadata":{}},{"cell_type":"markdown","source":"### Fare_amount","metadata":{}},{"cell_type":"code","source":"print('Previous dataset: {}'.format(len(train_df)))\ntrain_df = train_df[train_df.fare_amount>0]\nprint('Corrected dataset: {}'.format(len(train_df)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(1, 2, figsize=(10,4))\nsns.kdeplot(train_df['fare_amount'].values, ax=axs[0]).set_title(\"distribution of fare amount\")\nsns.kdeplot(np.log(train_df['fare_amount'].values), ax=axs[1]).set_title(\"Distribution of log-scaled fare_amount\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Passenger_count","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(figsize=(8,4))\nplt.hist(train_df['passenger_count'].values)\nplt.title(\"distribution of passenger\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Previous dataset: {}'.format(len(train_df)))\ntrain_df = train_df[train_df.passenger_count>0]\ntrain_df = train_df[train_df.passenger_count<13]\nprint('Corrected dataset: {}'.format(len(train_df)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(figsize=(8,4))\nplt.hist(train_df['passenger_count'].values)\nplt.title(\"Corrected distribution of passenger\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Location of pickup and dropoff\nThe location of New York city is -74.0063889 (longitude) and 40.7141667 (latitude).","metadata":{}},{"cell_type":"code","source":"print('Max and Min pickup longitude: {} and {}'.format(max(train_df.pickup_longitude), min(train_df.pickup_longitude)))\nprint('Max and Min dropout longitude: {} and {}'.format(max(train_df.dropoff_longitude), min(train_df.dropoff_longitude)))\n\nprint('Max and Min pickup latitude: {} and {}'.format(max(train_df.pickup_latitude), min(train_df.pickup_latitude)))\nprint('Max and Min dropout latitude: {} and {}'.format(max(train_df.dropoff_latitude), min(train_df.dropoff_latitude)))\n\nprint('Mean pickup latitude: {}'.format(np.mean(train_df.pickup_latitude)))\nprint('Mean dropout latitude: {}'.format(np.mean(train_df.dropoff_latitude)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots()\nax.scatter(x = train_df.pickup_longitude, y = train_df.pickup_latitude, color='blue')\nax.scatter(train_df.dropoff_longitude, train_df.dropoff_longitude, color='red')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since there are some outliers, we should remove them.<br>\n##### Reference -  [NYC Taxi Fare - Data Exploration](https://www.kaggle.com/breemen/nyc-taxi-fare-data-exploration)","metadata":{}},{"cell_type":"code","source":"def select_within_boundingbox(df, BB):\n    return (df.pickup_longitude >= BB[0]) & (df.pickup_longitude <= BB[1]) & \\\n           (df.pickup_latitude >= BB[2]) & (df.pickup_latitude <= BB[3]) & \\\n           (df.dropoff_longitude >= BB[0]) & (df.dropoff_longitude <= BB[1]) & \\\n           (df.dropoff_latitude >= BB[2]) & (df.dropoff_latitude <= BB[3])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on the coordinate of the test dataset, bounding box can be created. </br>\nMax and Min <b>longitude</b> of test dataset","metadata":{}},{"cell_type":"code","source":"min(test_df.pickup_longitude.min(), test_df.dropoff_longitude.min()), \\\nmax(test_df.pickup_longitude.max(), test_df.dropoff_longitude.max())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Max and Min <b>latitude</b> of test dataset","metadata":{}},{"cell_type":"code","source":"min(test_df.pickup_latitude.min(), test_df.dropoff_latitude.min()), \\\nmax(test_df.pickup_latitude.max(), test_df.dropoff_latitude.max())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Load NYC map","metadata":{}},{"cell_type":"code","source":"BB = (-74.3, -72.9, 40.5, 41.7)\nBB_zoom = (-74.1, -73.75, 40.6, 40.9)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Old size: %d' % len(train_df))\ntrain_df = train_df[select_within_boundingbox(train_df, BB)]\nprint('New size: %d' % len(train_df))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_on_map(df, BB, s=10, alpha=0.2):\n    fig, axs = plt.subplots(1, 3, figsize=(20,5))\n    axs[0].scatter(df.pickup_longitude, df.pickup_latitude, zorder=1, alpha=alpha, c='r', s=s)\n    axs[0].set_xlim((BB[0], BB[1]))\n    axs[0].set_ylim((BB[2], BB[3]))\n    axs[0].set_title('Pickup locations')\n    #axs[0].imshow(extend=BB)\n\n    axs[1].scatter(df.dropoff_longitude, df.dropoff_latitude, zorder=1, alpha=alpha, c='b', s=s)\n    axs[1].set_xlim((BB[0], BB[1]))\n    axs[1].set_ylim((BB[2], BB[3]))\n    axs[1].set_title('Dropoff locations')\n    #axs[1].imshow()\n    \n    axs[2].scatter(df.pickup_longitude, df.pickup_latitude, zorder=1, alpha=alpha, c='r', s=s, label='pickup')\n    axs[2].scatter(df.dropoff_longitude, df.dropoff_latitude, zorder=1, alpha=alpha, c='b', s=s, label='dropoff')\n    axs[2].set_xlim((BB[0], BB[1]))\n    axs[2].set_ylim((BB[2], BB[3]))\n    plt.legend(loc='upper left')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_on_map(train_df, BB, s=1, alpha=0.3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_on_map(train_df, BB_zoom, s=1, alpha=0.3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Location Data cleaning","metadata":{}},{"cell_type":"code","source":"print('Previous dataset: {}'.format(len(train_df)))\ntrain_df = train_df[train_df.pickup_longitude>-75]\ntrain_df = train_df[train_df.pickup_longitude<-73]\ntrain_df = train_df[train_df.pickup_latitude>40]\ntrain_df = train_df[train_df.pickup_latitude<42]\n\ntrain_df = train_df[train_df.dropoff_longitude>-75]\ntrain_df = train_df[train_df.dropoff_longitude<-73]\ntrain_df = train_df[train_df.dropoff_latitude>40]\ntrain_df = train_df[train_df.dropoff_latitude<42]\nprint('Corrected dataset: {}'.format(len(train_df)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Data creating","metadata":{}},{"cell_type":"markdown","source":"### Distance parameter","metadata":{}},{"cell_type":"code","source":"# Accurate form\ndef cal_dist(plo, pla, dlo, dla):\n    # plo = pickup longtitude\n    # pla = pickup latitude\n    # dlo = dropoff longitude\n    # dla = dropoff latitude\n    data = [train_df, test_df]\n    R = 6373.0\n    \n    for i in data:\n    \n        lat1 = np.radians(i[pla])\n        lat2 = np.radians(i[dla])\n        lon1 = np.radians(i[plo])\n        lon2 = np.radians(i[dlo])\n    \n        dlon = abs(lon2-lon1)\n        dlat = abs(lat2-lat1)\n    \n        a = np.sin(dlat / 2) **2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2) **2\n        c = 2*np.arctan2(np.sqrt(a), np.sqrt(1-a))\n    \n        dist = R * c\n        i['dist'] = dist\n    return dist\n\n# Simple form\ndef dist(plo, pla, dlo, dla):\n    dist = np.abs(dlo - plo) + np.abs(dla - pla)\n    return dist","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['dist'] = 0.0\ncal_dist('pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude')\ncal_dist('pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(1,2, figsize=(15,4))\nsns.kdeplot(train_df['dist'].values, ax=axs[0]).set_title(\"Distance distribution of Train data\")\nsns.kdeplot(test_df['dist'].values, ax=axs[1]).set_title(\"Distance distribution of Test data\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Distance from Airports\nMain airports in NYC. <br>\n1. NYC - city center\n2. JFK\n3. EWR\n4. LGR","metadata":{}},{"cell_type":"code","source":"nyc = (-74.0063889, 40.7141667)\njfk = (-73.7822222222, 40.6441666667)\newr = (-74.175, 40.69)\nlgr = (-73.87, 40.77)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#def dist(plo, pla, dlo, dla):\n# pla plo dla dlo\ndef cal_airport(plo, pla, dlo, dla):\n    data = [train_df, test_df]\n    for i in data:\n        i['pickup_nyc'] = dist(nyc[0], nyc[1], i['pickup_longitude'], i['pickup_latitude'])\n        i['dropoff_nyc'] = dist(nyc[0], nyc[1], i['dropoff_longitude'], i['dropoff_latitude'])\n        \n        i['pickup_jfk'] = dist(jfk[0], jfk[1], i['pickup_longitude'], i['pickup_latitude'])\n        i['dropoff_jfk'] = dist(jfk[0], jfk[1], i['dropoff_longitude'], i['dropoff_latitude'])\n        \n        i['pickup_ewr'] = dist(ewr[0], ewr[1], i['pickup_longitude'], i['pickup_latitude'])\n        i['dropoff_ewr'] = dist(ewr[0], ewr[1], i['dropoff_longitude'], i['dropoff_latitude'])\n        \n        i['pickup_lgr'] = dist(lgr[0], lgr[1], i['pickup_longitude'], i['pickup_latitude'])\n        i['dropoff_lgr'] = dist(lgr[0], lgr[1], i['dropoff_longitude'], i['dropoff_latitude'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df[['pickup_nyc', 'dropoff_nyc', 'pickup_jfk', 'dropoff_jfk', 'pickup_ewr', 'dropoff_ewr', 'pickup_lgr', 'dropoff_lgr']] = 0.0\ntest_df[['pickup_nyc', 'dropoff_nyc', 'pickup_jfk', 'dropoff_jfk', 'pickup_ewr', 'dropoff_ewr', 'pickup_lgr', 'dropoff_lgr']] = 0.0\n\ncal_airport('pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude')\ntrain_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(1,4, figsize=(20,4))\nsns.kdeplot(train_df['pickup_nyc'].values, ax=axs[0]).set_title(\"Pickup from NYC\")\nsns.kdeplot(train_df['pickup_jfk'].values, ax=axs[1]).set_title(\"Pickup from JFK\")\nsns.kdeplot(train_df['pickup_ewr'].values, ax=axs[2]).set_title(\"Pickup from EWR\")\nsns.kdeplot(train_df['pickup_lgr'].values, ax=axs[3]).set_title(\"Pickup from LGR\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(1,4, figsize=(20,4))\nsns.kdeplot(np.log(train_df['pickup_nyc'].values), ax=axs[0]).set_title(\"Pickup from NYC(log)\")\nsns.kdeplot(np.log(train_df['pickup_jfk'].values), ax=axs[1]).set_title(\"Pickup from JFK(log)\")\nsns.kdeplot(np.log(train_df['pickup_ewr'].values), ax=axs[2]).set_title(\"Pickup from EWR(log)\")\nsns.kdeplot(np.log(train_df['pickup_lgr'].values), ax=axs[3]).set_title(\"Pickup from LGR(log)\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Datetime","metadata":{}},{"cell_type":"code","source":"train_df['hour'] = train_df['pickup_datetime'].dt.hour\ntrain_df['day'] = train_df['pickup_datetime'].dt.day\ntrain_df['month'] = train_df['pickup_datetime'].dt.month\ntrain_df['year'] = train_df['pickup_datetime'].dt.year","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df['hour'] = test_df['pickup_datetime'].dt.hour\ntest_df['day'] = test_df['pickup_datetime'].dt.day\ntest_df['month'] = test_df['pickup_datetime'].dt.month\ntest_df['year'] = test_df['pickup_datetime'].dt.year","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Modeling\n#### Reference: [Stacked Regressions: Top 4% on LeaderBoard - House Prices](https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard)","metadata":{}},{"cell_type":"code","source":"import lightgbm as lgbm\nimport xgboost as xgb\nfrom xgboost import XGBRegressor\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train_df.drop(['key','pickup_datetime'], axis=1)\ntest = test_df.drop(['key', 'pickup_datetime'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(train.drop('fare_amount', axis=1), train['fare_amount'], test_size=0.3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cross validate model","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Validation function\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, X_train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### LASSO Regression","metadata":{}},{"cell_type":"code","source":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Elastic Net Regression","metadata":{}},{"cell_type":"code","source":"ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### XGBoost","metadata":{}},{"cell_type":"code","source":"model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =42, nthread = -1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### LightGBM","metadata":{}},{"cell_type":"code","source":"model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.005, n_estimators=100,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Base models scores","metadata":{}},{"cell_type":"code","source":"score = rmsle_cv(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std())) # mean & the standard deviation","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score = rmsle_cv(ENet)\nprint(\"\\nENet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std())) # mean & the standard deviation","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#score = rmsle_cv(KRR)\n#print(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#score = rmsle_cv(GBoost)\n#print(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score = rmsle_cv(model_xgb)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score = rmsle_cv(model_lgb)\nprint(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Stacking models","metadata":{}},{"cell_type":"code","source":"class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"averaged_models = AveragingModels(models = (ENet,lasso))\n\nscore = rmsle_cv(averaged_models)\nprint(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"averaged_models.fit(X_train.values, y_train)\nstacked_train_pred = averaged_models.predict(X_train.values)\nstacked_pred = np.expm1(averaged_models.predict(test.values))\nprint(rmsle(y_train, stacked_train_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### NEW XGB and LGBM Models for Ensemble\nThe parameters for model are refered from [this notebook](https://www.kaggle.com/madhurisivalenka/cleansing-eda-modelling-lgbm-xgboost-starters))","metadata":{}},{"cell_type":"code","source":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = {'max_depth':7,\n          'eta':1,\n          'objective':'reg:linear',\n          'eval_metric':'rmse',\n          'learning_rate':0.05\n         }\nnum_rounds = 50","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dtrain = xgb.DMatrix(X_train, label=y_train)\ndtest = xgb.DMatrix(test)\nmodel_xgbm = xgb.train(params, dtrain, num_rounds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_train_pred = model_xgbm.predict(dtest)\nxgb_pred = np.expm1(model_xgbm.predict(dtest))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = {\n    'boosting_type':'gbdt',\n    'objective': 'regression',\n    'nthread': -1,\n    'verbose': 0,\n    'num_leaves': 31,\n    'learning_rate': 0.05,\n    'max_depth': -1,\n    'subsample': 0.8,\n    'subsample_freq': 1,\n    'colsample_bytree': 0.6,\n    'reg_lambda': 0.001,\n    'metric': 'rmse',\n    'min_split_gain': 0.5,\n    'min_child_weight': 1,\n    'min_child_samples': 10,\n    'scale_pos_weight':1,\n    'force_col_wise':True\n    }\ntrain_set = lgb.Dataset(X_train, y_train, silent=True)\nmodel_lgbm = lgb.train(params, train_set = train_set)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgb_train_pred = model_lgbm.predict(X_train)\nlgb_pred = np.expm1(model_lgbm.predict(test.values))\nprint(rmsle(y_train, lgb_train_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Number of test: {}'.format(lgb_pred.shape))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Ensemble","metadata":{}},{"cell_type":"code","source":"'''RMSE on the entire Train data when averaging'''\n\n#print('RMSLE score on train data:')\n#print(rmsle(y_train,stacked_train_pred*0.70 +\n#               xgb_train_pred*0.15 + lgb_train_pred*0.15 ))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ensemble = stacked_pred*0.70 + xgb_pred*0.15 + lgb_pred*0.15\nprint(len(ensemble))\nensemble","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Submission","metadata":{}},{"cell_type":"code","source":"sub=pd.read_csv('../input/new-york-city-taxi-fare-prediction/sample_submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub['fare_amount'] = ensemble\nsub.to_csv('submission.csv',index=False)\nsub.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}