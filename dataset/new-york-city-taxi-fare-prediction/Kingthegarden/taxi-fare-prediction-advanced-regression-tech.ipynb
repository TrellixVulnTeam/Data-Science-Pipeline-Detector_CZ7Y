{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Import necessary librairies**","metadata":{}},{"cell_type":"code","source":"import time\nnotebookstart= time.time()\n\nimport numpy as np\nimport pandas as pd\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew\n# Simply works with Numpy, Matplotlib, Pandas, Sympy etc. \n# SciPy provides numerical integral routines and differential equations interpreters, algorithms to root out equations, standard continuous/differentiated probability distributions, and various statistical tools.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Collecting Data**","metadata":{}},{"cell_type":"code","source":"df_train =  pd.read_csv('../input/new-york-city-taxi-fare-prediction/train.csv', nrows = 100000, parse_dates=[\"pickup_datetime\"])\n\ndf_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test =  pd.read_csv('../input/new-york-city-taxi-fare-prediction/test.csv', parse_dates=[\"pickup_datetime\"])\n\ndf_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Exploratory data analysis**","metadata":{}},{"cell_type":"code","source":"df_train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.shape, df_test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **Target Variable**\n\n**Fare amount** is the variable we need to predict. So let's do some analysis on this variable first.","metadata":{}},{"cell_type":"code","source":"sns.distplot(df_train['fare_amount'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(df_train['fare_amount'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('Fare amount distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(df_train['fare_amount'], plot=plt)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The target variable is skewed. As (linear) models love normally distributed data , we need to transform this variable and make it more normally distributed.","metadata":{}},{"cell_type":"markdown","source":"* **Log-transformation of the target variable**","metadata":{}},{"cell_type":"code","source":"#We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\ndf_train[\"fare_amount\"] = np.log1p(df_train[\"fare_amount\"])\n\n#Check the new distribution \nsns.distplot(df_train['fare_amount'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(df_train['fare_amount'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('Fare amount distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(df_train['fare_amount'], plot=plt)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **Missing values**","metadata":{}},{"cell_type":"code","source":"df_train.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Missing values are very small, so it is believed to have a small impact on predictions. Therefore, it seems safe to remove it from the dataset.","metadata":{}},{"cell_type":"code","source":"df_train = df_train.dropna()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **Fare amount** : dollar amount of the cost of the taxi ride. \n* **Passenger count** :  indicating the number of passengers in the taxi ride.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots()\nax.scatter(x = df_train['passenger_count'], y = df_train['fare_amount'])\nplt.ylabel('fare_amount', fontsize=13)\nplt.xlabel('passenger_count', fontsize=13)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['passenger_count'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train[df_train['fare_amount'] > 600]['fare_amount'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train[df_train['fare_amount'] < 0]['fare_amount'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can find the outliers.\n1. Passenger_count == 200 in the bottom right can be judged by the outlier value.\n2. The high fare in the upper left is far from the distribution, so it is judged to be outlier.\n3. Fare determines that a rate below zero cannot exist as an outlier.\n\n\n**Note** :\n\nEliminating outliers is stable for creating robust models. Therefore, we will remove the abnormalities found above later.","metadata":{}},{"cell_type":"markdown","source":"* **Fare amount**\n* **pickup datetime** : value indicating when the taxi ride started.","metadata":{}},{"cell_type":"markdown","source":"Let's find out the rate for each time zone!\nData will be divided by year, month, date, time, and day.","metadata":{}},{"cell_type":"code","source":"df_train[\"year\"] = df_train[\"pickup_datetime\"].dt.year\ndf_train[\"month\"] = df_train[\"pickup_datetime\"].dt.month\ndf_train[\"day\"] = df_train[\"pickup_datetime\"].dt.day\ndf_train[\"hour\"] = df_train[\"pickup_datetime\"].dt.hour\ndf_train[\"dayofweek\"] = df_train[\"pickup_datetime\"].dt.dayofweek","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(nrows=5)\nfig.set_size_inches(18,14)\n\nplt.sca(axes[0])\nplt.xticks(rotation=30, ha='right')\naxes[0].set(ylabel='fare_amount',title=\"Annual Fare\")\nsns.pointplot(data = df_train, x=\"year\", y=\"fare_amount\", ax=axes[0])\n\nplt.sca(axes[1])\nplt.xticks(rotation=30, ha='right')\naxes[1].set(ylabel='fare_amount',title=\"Monthly Fare\")\nsns.pointplot(data = df_train, x=\"month\", y=\"fare_amount\", ax=axes[1])\n\nplt.sca(axes[2])\nplt.xticks(rotation=30, ha='right')\naxes[2].set(ylabel='fare_amount',title=\"Daily Fare\")\nsns.pointplot(data = df_train, x=\"day\", y=\"fare_amount\", ax=axes[2])\n\nplt.sca(axes[3])\nplt.xticks(rotation=30, ha='right')\naxes[3].set(ylabel='fare_amount',title=\"hourly Fare\")\nsns.pointplot(data = df_train, x=\"hour\", y=\"fare_amount\", ax=axes[3])\n\nplt.sca(axes[4])\nplt.xticks(rotation=30, ha='right')\naxes[4].set(ylabel='fare_amount',title=\"Fare by Day\")\nsns.pointplot(data = df_train, x=\"dayofweek\", y=\"fare_amount\", ax=axes[4])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's interpret the results!\n\n1. Fare increases over the years.\n2. Fare is high at the beginning of the month (4th to 6th)\n\nPlease let me know if you have any other information you can find out.","metadata":{}},{"cell_type":"markdown","source":"* **pickup_longitude** - float for longitude coordinate of where the taxi ride started.\n* **pickup_latitude** - float for latitude coordinate of where the taxi ride started.\n* **dropoff_longitude** - float for longitude coordinate of where the taxi ride ended.\n* **dropoff_latitude** - float for latitude coordinate of where the taxi ride ended.","metadata":{}},{"cell_type":"markdown","source":"**Note** :\n\nLatitude and longitude are a pair of numbers (coordinates) used to describe a position on the plane of a geographic coordinate system. The numbers are in decimal degrees format and range from -90 to 90 for latitude and -180 to 180 for longitude.","metadata":{}},{"cell_type":"code","source":"df_train[(df_train['pickup_longitude'] > 180) | (df_train['pickup_longitude'] < -180)]['pickup_longitude'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train[(df_train['pickup_latitude'] > 90) | (df_train['pickup_latitude'] < -90)]['pickup_latitude'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train[(df_train['dropoff_longitude'] > 180) | (df_train['dropoff_longitude'] < -180)]['dropoff_longitude'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train[(df_train['dropoff_latitude'] > 90) | (df_train['dropoff_latitude'] < -90)]['dropoff_latitude'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Values outside the latitude and longitude range can be determined as outliers, so we decided to remove them.","metadata":{}},{"cell_type":"markdown","source":"# **Data Cleaning and Feature Engineering**","metadata":{}},{"cell_type":"markdown","source":"* Reference : https://www.kaggle.com/nicapotato/taxi-rides-time-analysis-and-oof-lgbm\n\nThis work has already referred to well-organized kernels.","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/new-york-city-taxi-fare-prediction/train.csv', nrows = 5000, index_col = \"key\")\ntrain = train.dropna()\ntest_df = pd.read_csv('../input/new-york-city-taxi-fare-prediction/test.csv', index_col = \"key\")\ntestdex = test_df.index","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_distance_features(df):\n    # Distance is expected to have an impact on the fare\n    df['longitude_distance'] = abs(df['pickup_longitude'] - df['dropoff_longitude'])\n    df['latitude_distance'] = abs(df['pickup_latitude'] - df['dropoff_latitude'])\n\n    # Straight distance\n    df['distance_travelled'] = (df['longitude_distance'] ** 2 + df['latitude_distance'] ** 2) ** .5\n    df['distance_travelled_sin'] = np.sin((df['longitude_distance'] ** 2 * df['latitude_distance'] ** 2) ** .5)\n    df['distance_travelled_cos'] = np.cos((df['longitude_distance'] ** 2 * df['latitude_distance'] ** 2) ** .5)\n    df['distance_travelled_sin_sqrd'] = np.sin((df['longitude_distance'] ** 2 * df['latitude_distance'] ** 2) ** .5) ** 2\n    df['distance_travelled_cos_sqrd'] = np.cos((df['longitude_distance'] ** 2 * df['latitude_distance'] ** 2) ** .5) ** 2\n\n    # Haversine formula for distance\n    # Haversine formula:\ta = sin²(Δφ/2) + cos φ1 ⋅ cos φ2 ⋅ sin²(Δλ/2)\n    R = 6371e3 # Metres\n    phi1 = np.radians(df['pickup_latitude'])\n    phi2 = np.radians(df['dropoff_latitude'])\n    phi_chg = np.radians(df['pickup_latitude'] - df['dropoff_latitude'])\n    delta_chg = np.radians(df['pickup_longitude'] - df['dropoff_longitude'])\n    a = np.sin(phi_chg / 2) + np.cos(phi1) * np.cos(phi2) * np.sin(delta_chg / 2)\n    c = 2 * np.arctan2(a ** .5, (1-a) ** .5)\n    d = R * c\n    df['haversine'] = d\n\n    # Bearing\n    # Formula:\tθ = atan2( sin Δλ ⋅ cos φ2 , cos φ1 ⋅ sin φ2 − sin φ1 ⋅ cos φ2 ⋅ cos Δλ )\n    y = np.sin(delta_chg * np.cos(phi2))\n    x = np.cos(phi1) * np.sin(phi2) - np.sin(phi1) * np.cos(phi2) * np.cos(delta_chg)\n    df['bearing'] = np.arctan2(y, x)\n\n    return df\n\ndef prepare_time_features(df):\n    df['pickup_datetime'] = df['pickup_datetime'].str.replace(\" UTC\", \"\")\n    df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'], format='%Y-%m-%d %H:%M:%S')\n    df['hour_of_day'] = df.pickup_datetime.dt.hour\n    df['week'] = df.pickup_datetime.dt.week\n    df['month'] = df.pickup_datetime.dt.month\n    df[\"year\"] = df.pickup_datetime.dt.year\n    df['day_of_year'] = df.pickup_datetime.dt.dayofyear\n    df['week_of_year'] = df.pickup_datetime.dt.weekofyear\n    df[\"weekday\"] = df.pickup_datetime.dt.weekday\n    df[\"quarter\"] = df.pickup_datetime.dt.quarter\n    df[\"day_of_month\"] = df.pickup_datetime.dt.day\n    \n    return df\n\n# Airport Features - By Albert van Breenmen\n# https://www.kaggle.com/breemen/nyc-taxi-fare-data-exploration\ndef dist(pickup_lat, pickup_long, dropoff_lat, dropoff_long):  \n    distance = np.abs(dropoff_lat - pickup_lat) + np.abs(dropoff_long - pickup_long)\n    \n    return distance\n\ndef airport_feats(train,test_df):\n    for data in [train,test_df]:\n        nyc = (-74.0063889, 40.7141667)\n        jfk = (-73.7822222222, 40.6441666667)\n        ewr = (-74.175, 40.69)\n        lgr = (-73.87, 40.77)\n        data['distance_to_center'] = dist(nyc[1], nyc[0],\n                                          data['pickup_latitude'], data['pickup_longitude'])\n        data['pickup_distance_to_jfk'] = dist(jfk[1], jfk[0],\n                                             data['pickup_latitude'], data['pickup_longitude'])\n        data['dropoff_distance_to_jfk'] = dist(jfk[1], jfk[0],\n                                               data['dropoff_latitude'], data['dropoff_longitude'])\n        data['pickup_distance_to_ewr'] = dist(ewr[1], ewr[0], \n                                              data['pickup_latitude'], data['pickup_longitude'])\n        data['dropoff_distance_to_ewr'] = dist(ewr[1], ewr[0],\n                                               data['dropoff_latitude'], data['dropoff_longitude'])\n        data['pickup_distance_to_lgr'] = dist(lgr[1], lgr[0],\n                                              data['pickup_latitude'], data['pickup_longitude'])\n        data['dropoff_distance_to_lgr'] = dist(lgr[1], lgr[0],\n                                               data['dropoff_latitude'], data['dropoff_longitude'])\n    return train, test_df\n\n# Percentile\ndef percentile(n):\n    def percentile_(x):\n        return np.percentile(x, n)\n    percentile_.__name__ = 'percentile_%s' % n\n    return percentile_\n\n# Build ime Aggregate Features\ndef time_agg(train, test_df, vars_to_agg, vars_be_agg):\n    for var in vars_to_agg:\n        agg = train.groupby(var)[vars_be_agg].agg([\"sum\",\"mean\",\"std\",\"skew\",percentile(80),percentile(20)])\n        if isinstance(var, list):\n            agg.columns = pd.Index([\"fare_by_\" + \"_\".join(var) + \"_\" + str(e) for e in agg.columns.tolist()])\n        else:\n            agg.columns = pd.Index([\"fare_by_\" + var + \"_\" + str(e) for e in agg.columns.tolist()]) \n        train = pd.merge(train,agg, on=var, how= \"left\")\n        test_df = pd.merge(test_df,agg, on=var, how= \"left\")\n    \n    return train, test_df\n\n# Clean dataset from https://www.kaggle.com/gunbl4d3/xgboost-ing-taxi-fares\ndef clean_df(df):\n    return df[(df.fare_amount > 0) & \n            (df.pickup_longitude > -80) & (df.pickup_longitude < -70) &\n            (df.pickup_latitude > 35) & (df.pickup_latitude < 45) &\n            (df.dropoff_longitude > -80) & (df.dropoff_longitude < -70) &\n            (df.dropoff_latitude > 35) & (df.dropoff_latitude < 45)]\nprint(\"Cleaning Functions Defined..\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Percent of Training Set with Zero and Below Fair: \", round(((train.loc[train[\"fare_amount\"] <= 0, \"fare_amount\"].shape[0]/train.shape[0]) * 100),5))\nprint(\"Percent of Training Set 200 and Above Fair: \", round((train.loc[train[\"fare_amount\"] >= 200, \"fare_amount\"].shape[0]/train.shape[0]) * 100,5))\ntrain = train.loc[(train[\"fare_amount\"] > 0) & (train[\"fare_amount\"] <= 200),:]\nprint(\"\\nPercent of Training Set with Zero and Below Passenger Count: \", round((train.loc[train[\"passenger_count\"] <= 0, \"passenger_count\"].shape[0]/train.shape[0]) * 100,5))\nprint(\"Percent of Training Set with Nine and Above Passenger Count: \", round((train.loc[train[\"passenger_count\"] >= 9, \"passenger_count\"].shape[0]/train.shape[0]) * 100,5))\ntrain = train.loc[(train[\"passenger_count\"] > 0) & (train[\"passenger_count\"] <= 9),:]\n\n# Clean Training Set\ntrain = clean_df(train)\n\n# Distance Features\ntrain = prepare_distance_features(train)\ntest_df = prepare_distance_features(test_df)\ntrain,test_df = airport_feats(train,test_df)\n\n# Time Features\ntrain = prepare_time_features(train)\ntest_df = prepare_time_features(test_df)\n\n# Ratios\ntrain[\"fare_to_dist_ratio\"] = train[\"fare_amount\"] / ( train[\"distance_travelled\"]+0.0001)\ntrain[\"fare_npassenger_to_dist_ratio\"] = (train[\"fare_amount\"] / train[\"passenger_count\"]) /( train[\"distance_travelled\"]+0.0001)\n\n# Time Aggregate Features\ntrain, test_df = time_agg(train, test_df,\n                          vars_to_agg  = [\"passenger_count\", \"weekday\", \"quarter\", \"month\", \"year\", \"hour_of_day\",\n                                          [\"weekday\", \"month\", \"year\"], [\"hour_of_day\", \"weekday\", \"month\", \"year\"]],\n                          vars_be_agg = \"fare_amount\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **Time Range**","metadata":{}},{"cell_type":"code","source":"train_time_start = train.pickup_datetime.min()\ntrain_time_end = train.pickup_datetime.max()\nprint(\"Train Time Starts: {}, Ends {}\".format(train_time_start,train_time_end))\ntest_time_start = test_df.pickup_datetime.min()\ntest_time_end = test_df.pickup_datetime.max()\nprint(\"Test Time Starts: {}, Ends {}\".format(test_time_start,test_time_end))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **missing values**","metadata":{}},{"cell_type":"code","source":"train_data_na = (train.isnull().sum() / len(train)) * 100\ntrain_data_na = train_data_na.drop(train_data_na[train_data_na == 0].index).sort_values(ascending=False)[:30]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_data = pd.DataFrame({'Missing Ratio' :train_data_na})\nmissing_data.head(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(figsize=(15, 12))\nplt.xticks(rotation='45')\nsns.barplot(x=train_data_na.index, y=train_data_na)\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data_na = (test_df.isnull().sum() / len(test_df)) * 100\ntest_data_na = test_data_na.drop(test_data_na[test_data_na == 0].index).sort_values(ascending=False)[:30]\n\nmissing_data = pd.DataFrame({'Missing Ratio' :test_data_na})\nmissing_data.head(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **Imputing missing values**","metadata":{}},{"cell_type":"code","source":"# df.mode() : The value that occurs most frequently\n\n# train['haversine'] = train['haversine'].fillna(train['haversine'].mode()[0])\n# test_df['haversine'] = test_df['haversine'].fillna(test_df['haversine'].mode()[0])\n\ntrain['fare_by_weekday_month_year_skew'] = train['fare_by_weekday_month_year_skew'].fillna(train['fare_by_weekday_month_year_skew'].mode()[0])\ntest_df['fare_by_weekday_month_year_skew'] = test_df['fare_by_weekday_month_year_skew'].fillna(test_df['fare_by_weekday_month_year_skew'].mode()[0])\n\ntrain['fare_by_weekday_month_year_std'] = train['fare_by_weekday_month_year_std'].fillna(train['fare_by_weekday_month_year_std'].mode()[0])\ntest_df['fare_by_weekday_month_year_std'] = test_df['fare_by_weekday_month_year_std'].fillna(test_df['fare_by_weekday_month_year_std'].mode()[0])\n\n# train['fare_by_hour_of_day_weekday_month_year_skew'] = train['fare_by_hour_of_day_weekday_month_year_skew'].fillna(train['fare_by_hour_of_day_weekday_month_year_skew'].mode()[0])\n# test_df['fare_by_hour_of_day_weekday_month_year_skew'] = test_df['fare_by_hour_of_day_weekday_month_year_skew'].fillna(test_df['fare_by_hour_of_day_weekday_month_year_skew'].mode()[0])\n\n# train['fare_by_hour_of_day_weekday_month_year_std'] = train['fare_by_hour_of_day_weekday_month_year_std'].fillna(train['fare_by_hour_of_day_weekday_month_year_std'].mode()[0])\n# test_df['fare_by_hour_of_day_weekday_month_year_std'] = test_df['fare_by_hour_of_day_weekday_month_year_std'].fillna(test_df['fare_by_hour_of_day_weekday_month_year_std'].mode()[0])\n\n# test_df['fare_by_hour_of_day_weekday_month_year_sum'] = test_df['fare_by_hour_of_day_weekday_month_year_sum'].fillna(test_df['fare_by_hour_of_day_weekday_month_year_sum'].mode()[0])\n\n# test_df['fare_by_hour_of_day_weekday_month_year_mean'] = test_df['fare_by_hour_of_day_weekday_month_year_mean'].fillna(test_df['fare_by_hour_of_day_weekday_month_year_mean'].mode()[0])\n\n# test_df['fare_by_hour_of_day_weekday_month_year_percentile_80'] = test_df['fare_by_hour_of_day_weekday_month_year_percentile_80'].fillna(test_df['fare_by_hour_of_day_weekday_month_year_percentile_80'].mode()[0])\n\n# test_df['fare_by_hour_of_day_weekday_month_year_percentile_20'] = test_df['fare_by_hour_of_day_weekday_month_year_percentile_20'].fillna(test_df['fare_by_hour_of_day_weekday_month_year_percentile_20'].mode()[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data_na = (test_df.isnull().sum() / len(test_df)) * 100\ntest_data_na = test_data_na.drop(test_data_na[test_data_na == 0].index).sort_values(ascending=False)[:30]\n\nmissing_data = pd.DataFrame({'Missing Ratio' :test_data_na})\nmissing_data.head(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data_na = (train.isnull().sum() / len(train)) * 100\ntrain_data_na = train_data_na.drop(train_data_na[train_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :train_data_na})\nmissing_data.head(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Target Variable","metadata":{}},{"cell_type":"code","source":"sns.distplot(train['fare_amount'] , fit=norm);\n\n(mu, sigma) = norm.fit(train['fare_amount'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('Fare amount distribution')\n\nfig = plt.figure()\nres = stats.probplot(train['fare_amount'], plot=plt)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[\"fare_amount\"] = np.log1p(train[\"fare_amount\"])\n\nsns.distplot(train['fare_amount'] , fit=norm);\n\n(mu, sigma) = norm.fit(train['fare_amount'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('Fare amount distribution')\n\nfig = plt.figure()\nres = stats.probplot(train['fare_amount'], plot=plt)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **Keep Relevant Variables**","metadata":{}},{"cell_type":"code","source":"y_train = train.fare_amount.values\nfeatures_drop = ['pickup_datetime','fare_by_hour_of_day_weekday_month_year_skew', 'fare_by_hour_of_day_weekday_month_year_std', 'fare_by_hour_of_day_weekday_month_year_sum', 'fare_by_hour_of_day_weekday_month_year_mean','fare_by_hour_of_day_weekday_month_year_percentile_80','fare_by_hour_of_day_weekday_month_year_percentile_20','haversine']\ntest_df.drop(features_drop, axis = 1, inplace=True)\ntrain = train[test_df.columns]\nprint(\"Does Train feature equal test feature?: \", all(train.columns == test_df.columns))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape, test_df.shape, len(y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Mdelling**","metadata":{}},{"cell_type":"markdown","source":"I thank the kernel for allowing me to study improved modeling.\n(https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard)","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Define a cross validation strategy**\n\nWe use the cross_val_score function of Sklearn. However this function has not a shuffle attribut, we add then one line of code, in order to shuffle the dataset prior to cross-validation","metadata":{}},{"cell_type":"code","source":"#Validation function\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Base models**\n\n* **LASSO Regression**\n\nThis model may be very sensitive to outliers. So we need to made it more robust on them. For that we use the sklearn's Robustscaler() method on pipeline","metadata":{}},{"cell_type":"code","source":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=45))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **Elastic Net Regression**","metadata":{}},{"cell_type":"code","source":"ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **Kernel Ridge Regression**","metadata":{}},{"cell_type":"code","source":"KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **Gradient Boosting Regression**","metadata":{}},{"cell_type":"code","source":"GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **XGBoost**","metadata":{}},{"cell_type":"code","source":"model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **LightGBM**","metadata":{}},{"cell_type":"code","source":"model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **Base models scores**","metadata":{}},{"cell_type":"code","source":"score = rmsle_cv(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score = rmsle_cv(ENet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score = rmsle_cv(KRR)\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score = rmsle_cv(GBoost)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score = rmsle_cv(model_xgb)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Stacking models**\n\nSimplest Stacking approach : Averaging base models\n\nWe begin with this simple approach of averaging base models. We build a new class to extend scikit-learn with our model and also to laverage encapsulation and code reuse (inheritance)","metadata":{}},{"cell_type":"markdown","source":"* **Averaged base models class**","metadata":{}},{"cell_type":"code","source":"class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"averaged_models = AveragingModels(models = (ENet, GBoost, KRR, lasso))\n\nscore = rmsle_cv(averaged_models)\nprint(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **Stacking averaged Models Class**","metadata":{}},{"cell_type":"code","source":"class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Stacking Averaged models Score**\n\nTo make the two approaches comparable (by using the same number of models) , we just average Enet KRR and Gboost, then we add lasso as meta-model.","metadata":{}},{"cell_type":"code","source":"stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, KRR),\n                                                 meta_model = lasso)\n\nscore = rmsle_cv(stacked_averaged_models)\nprint(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **Ensembling StackedRegressor, XGBoost and LightGBM**","metadata":{}},{"cell_type":"code","source":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Final Training and Prediction**","metadata":{}},{"cell_type":"markdown","source":"* **StackedRegressor**","metadata":{}},{"cell_type":"code","source":"stacked_averaged_models.fit(train.values, y_train)\nstacked_train_pred = stacked_averaged_models.predict(train.values)\nstacked_pred = np.expm1(stacked_averaged_models.predict(test_df.values))\nprint(rmsle(y_train, stacked_train_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **XGBoost**","metadata":{}},{"cell_type":"code","source":"model_xgb.fit(train, y_train)\nxgb_train_pred = model_xgb.predict(train)\nxgb_pred = np.expm1(model_xgb.predict(test_df))\nprint(rmsle(y_train, xgb_train_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **LightGBM**","metadata":{}},{"cell_type":"code","source":"model_lgb.fit(train, y_train)\nlgb_train_pred = model_lgb.predict(train)\nlgb_pred = np.expm1(model_lgb.predict(test_df.values))\nprint(rmsle(y_train, lgb_train_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''RMSE on the entire Train data when averaging'''\n\nprint('RMSLE score on train data:')\nprint(rmsle(y_train,stacked_train_pred*0.70 +\n               xgb_train_pred*0.15 + lgb_train_pred*0.15 ))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **Ensemble prediction**","metadata":{}},{"cell_type":"code","source":"ensemble = stacked_train_pred*0.70 + xgb_train_pred*0.15 + lgb_train_pred*0.15","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.read_csv('../input/new-york-city-taxi-fare-prediction/sample_submission.csv')\nsubmission = pd.DataFrame()\nsubmission['key'] = sub['key']\nsubmission['fare_amount'] = ensemble\nsubmission.to_csv('submission_ensemble_1.csv',index=False)","metadata":{},"execution_count":null,"outputs":[]}]}