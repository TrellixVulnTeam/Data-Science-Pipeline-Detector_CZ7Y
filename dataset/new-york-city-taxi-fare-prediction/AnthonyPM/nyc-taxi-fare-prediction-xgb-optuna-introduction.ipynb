{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# New York Taxi Fare Prediction Notebook\r\n\r\nThis notebook has been created as a basic entry into the [New York City Taxi Fare Prediction](https://www.kaggle.com/c/new-york-city-taxi-fare-prediction/overview) competition on Kaggle. This is a good starting notebook which will go through data cleaning and visualisation, feature selection, hyperparameter tuning and finally training a model to generate predictions for submission to the competition. I wrote a post that goes through this notebook in detail which can be [found here.](https://www.apmarkham.com/posts/nyc-taxi-kaggle/)","metadata":{"_cell_guid":"bc861aa3-34d9-4373-9114-f01174e715d8","_uuid":"9a050aea-517a-42db-b0e2-770cd5e85dcc","editable":true}},{"cell_type":"markdown","source":"## Part 1 - Data Cleaning and Feature Selection","metadata":{}},{"cell_type":"code","source":"# Import the packages that we will be using\r\nimport pandas as pd\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport seaborn as sns\r\nimport geopandas as gpd\r\nimport optuna\r\nimport xgboost as xgb\r\nfrom sklearn.metrics import mean_squared_error\r\nfrom sklearn.model_selection import train_test_split","metadata":{"_cell_guid":"ea2347fc-f286-4937-aff0-e47a548c3c45","_uuid":"99ffa4da-f36a-4825-8b1a-520c7bb179f3","collapsed":false,"editable":true,"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After importing the required packages we import the train and test data. Note that we aren't loading the entire train data here - only 2 million rows. The entire data file contains over 54 million rows and loading all this would take a significant amount of time to clean, visualise and train. However with a powerful system you could load more data, just keep in mind the extra time this will take to run.\n\nI have published this notebook using a Kaggle kernel with just 2 million rows of train data loaded, but have also trained this model using the full data on my local system and saw some improvement in my final score, so if possible, run this in an environment where you can take advantage of the full data.","metadata":{}},{"cell_type":"code","source":"# Load in our train and test data\ntrain = pd.read_csv(\"../input/new-york-city-taxi-fare-prediction/train.csv\", nrows=2000000)  # 2 million rows loaded initially\ntest = pd.read_csv(\"../input/new-york-city-taxi-fare-prediction/test.csv\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's inspect the shape of the data and take a look at the columns in each set.","metadata":{"_cell_guid":"3a64af1d-37d9-4953-a27d-337318cb9c7d","_uuid":"ecbe8bfa-19c4-4e47-80a1-a1a6f2c255d8","editable":true}},{"cell_type":"code","source":"print(\"Data loaded.\")\r\nprint(\"Train shape:\", train.shape)\r\nprint(\"Test shape:\", test.shape)\r\n\r\nprint('Train columns: ', train.columns.tolist())\r\nprint('Test columns:  ', test.columns.tolist())\r\n\r\n# Let's take a quick initial look at an overview of our train data\r\ntrain.describe()","metadata":{"_cell_guid":"88948bd8-6957-4539-906b-90fb7b271796","_uuid":"62a4fade-7538-4c3b-afd6-11a1a279574b","collapsed":false,"editable":true,"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that the fare_amount is what we're looking to predict using our test data.\n\nWe will begin by looking at cleaning this data. First we remove any rows in the train data that have missing data. Next, if the fare amount is zero, or the amount of passengers in the taxi is zero, it doesn't make much sense, so lets drop those observations as well.\n\nWe will also constrain the passenger count - looking at our summary statistics above we can see the maximum passenger count is 208, which isn't very realistic for a taxi. We will constrain the passenger count to a maximum of 6 passengers. We have 54 million total observations (with 2m loaded), so dropping any observations that don't make sense isn't a problem.","metadata":{}},{"cell_type":"code","source":"# Perform cleaning here, drop NaN rows (rows with empty cells)\r\ntrain.dropna(inplace=True)\r\n\r\n# Drop the <=0 fares from the dataset, keeping only fares >0\r\ntrain = train[train[\"fare_amount\"] > 0]\r\n\r\n# Now we want to look at the passenger count. If there is zero passengers, drop the observation\r\ntrain = train[train[\"passenger_count\"] > 0]\r\n# If there is more than 6 passengers, also drop the observation\r\ntrain = train[train[\"passenger_count\"] <= 6]","metadata":{"_cell_guid":"eb86781b-d2cb-4688-824f-1fa6e0f1e08b","_uuid":"f7c031ae-c7ca-41ff-8946-e827d90850a4","collapsed":false,"editable":true,"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The next thing to think about is the location data. Looking at the columns above, we know that we have the pickup and dropoff latitude and longitude for each observation. However the maximum and minimum values in our summary statistics table above reveals a lot of inaccuracy. We know that this dataset focuses on NYC, so we should be dropping records where either the onboard GPS has potentially failed and is returning values at null island (0, 0) or values that are incorrect (i.e. in the ocean/outside of NY state).\n\nLet us first visualise the pickup and dropoff locations on a map of the world. It is important to note that using Geopandas and shapefiles is certainly not the most efficient way to visualise this - normal plots are faster but provide less detail. Finding a balance between these two can be difficult and a good exercise for the learner is to try visualise this in a different way!","metadata":{}},{"cell_type":"code","source":"street_map = gpd.read_file(\"../input/worldmap/WB_Land_10m.shp\")\n\n# Create a dataframe using Geopandas to hold the pickup longitude and latitude values from the train dataset\ngeo_df = gpd.GeoDataFrame(train, geometry=gpd.points_from_xy(x=train[\"pickup_longitude\"], y=train[\"pickup_latitude\"]))\n\n# Now we create our figure and axes and assign these to out plot\nfig, ax = plt.subplots(figsize=(15, 15))\nstreet_map.plot(ax=ax, alpha=0.4, color=\"grey\")\ngeo_df.plot(ax=ax, alpha=0.5, legend=True, markersize=10)\n\n# Give our plot a title\nplt.title(\"Pickup Locations\", fontsize=15, fontweight=\"bold\")\n\n# Set latitude and longitude boundaries for the large overview of the world map\nplt.xlim(-180, 180)\nplt.ylim(-90, 90)\n\n# These lines very roughly represent where our cutoff point should be for NY state\nplt.axvline(x=-70)\nplt.axvline(x=-83)\nplt.axhline(y=36)\nplt.axhline(y=46)\n\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a dataframe using Geopandas to hold the dropoff longitude and latitude values from the train dataset\r\ngeo_df = gpd.GeoDataFrame(train, geometry=gpd.points_from_xy(x=train[\"dropoff_longitude\"], y=train[\"dropoff_latitude\"]))\r\n\r\n# Now we create our figure and axes and assign these to out plot\r\nfig, ax = plt.subplots(figsize=(15, 15))\r\nstreet_map.plot(ax=ax, alpha=0.4, color=\"grey\")\r\ngeo_df.plot(ax=ax, alpha=0.5, legend=True, markersize=10)\r\n\r\n# Give our plot a title\r\nplt.title(\"Dropoff Locations\", fontsize=15, fontweight=\"bold\")\r\n\r\n# Set latitude and longitude boundaries for the large overview of the world map\r\nplt.xlim(-180, 180)\r\nplt.ylim(-90, 90)\r\n\r\n# These lines very roughly represent where our cutoff point should be for NY state\r\nplt.axvline(x=-70)\r\nplt.axvline(x=-83)\r\nplt.axhline(y=36)\r\nplt.axhline(y=46)\r\n\r\nplt.show()","metadata":{"_cell_guid":"8df4b0b1-383c-4978-a4b8-94cff99a67d5","_uuid":"fa9e8a66-6dfd-410d-8769-a00c3ce11d3e","collapsed":false,"editable":true,"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we can see our outliers visually for both the pickup and dropoff locations. They are scattered all over the globe, with points at null island (0, 0), as well as in Europe, South America, and even Antarctica! Let's get rid of these outliers, constraining the trips to those within NY.\n\nAs a side note, both these plots could be merged into one, but the overlap makes it hard to see some outliers and to identify whether they are pickup or dropoff outliers. It's far better to have two seperate plots to look at.\n\nIt's also worth noting that the longitude/latitude chop below is a rough one. There are ways to do this accurately but we will not go into this in an introductory submission.","metadata":{}},{"cell_type":"code","source":"# Drop any records that have zero longitude/latitude, or long/lats that are outside bounds. Rembmer our longitude for NYC is always negative.\r\n# This is going to be a rough chop, there is a significant amount of outliers to discuss\r\ntrain = train[(train[\"pickup_longitude\"] < -70) & (train[\"pickup_longitude\"] > -83)]\r\ntrain = train[(train[\"pickup_latitude\"] > 36) & (train[\"pickup_latitude\"] < 46 )]\r\ntrain = train[(train[\"dropoff_longitude\"] < -70) & (train[\"dropoff_longitude\"] > -83)]\r\ntrain = train[(train[\"dropoff_latitude\"] > 36) & (train[\"dropoff_latitude\"] < 46 )]","metadata":{"_cell_guid":"c0b34b1f-6c52-4f11-88a8-2eccd6e36fc0","_uuid":"92f81b55-9c45-4de3-8219-ca6d4b2dba5f","collapsed":false,"editable":true,"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Confirm that all our longitude and latitude values now fall within acceptable bounds\r\ntrain.describe()","metadata":{"_cell_guid":"6b09da5f-e613-4bae-b684-985d68e4db36","_uuid":"c3729b62-41b3-419e-84c8-63bdadaa0886","collapsed":false,"editable":true,"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# See the new shape of our data after dropping the outlier coordinates\r\nprint(\"Train data shape: \", train.shape)","metadata":{"_cell_guid":"d976c8a1-1b8e-4a76-8304-a946324cffe1","_uuid":"fd77dda7-34b3-49ec-af94-5cd7ebdcb5ed","collapsed":false,"editable":true,"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have now dropped outliers from our dataset and we can see from the initial 2 million observations that we loaded from the train data, we now have 1.95 million. This means we've dropped around 50,000 observations or only 2.5% of our total - not bad at all.\n\nBefore we move on let's confirm that we've actually dropped all NaN values from the train dataset.","metadata":{}},{"cell_type":"code","source":"# Finally, search the train dataframe for any NaN values to confirm they have all been dropped successfully\r\nprint(\"NaN values:\")\r\nprint(format(train.isna().sum()))","metadata":{"_cell_guid":"dc6fbf40-f577-4566-8219-aafb6a6956e8","_uuid":"b2142649-f4c2-4046-9d8e-082f685dc17a","collapsed":false,"editable":true,"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Great, all the NaN values are gone.\n\nThe best way to confirm that the latitude and longitude data has been cleaned is visually, with a plot. First, we will use the same shapefile as before, the world map, comparing the before and after images.\n\nNext we plot the latitude and longitude for pickup location on a shapefile map of the east coast of the United States. A third map is also provided, a map of NYC itself. The east coast map is useful for looking at outliers, while the NYC map is less useful but enjoyable to look at.","metadata":{}},{"cell_type":"code","source":"# Let us now visualise pickup and dropoff coordinates on two seperate maps so we can confirm that there are no outliers in our coordinates\ngeo_df = gpd.GeoDataFrame(train, geometry=gpd.points_from_xy(x=train[\"pickup_longitude\"], y=train[\"pickup_latitude\"]))\n\nfig, ax = plt.subplots(figsize=(15, 15))\nstreet_map.plot(ax=ax, alpha=0.4, color=\"grey\")\ngeo_df.plot(ax=ax, alpha=0.5, legend=True, markersize=10)\n\nplt.title(\"Cleaned Pickup Locations\", fontsize=15, fontweight=\"bold\")\n\n# Set latitude and longitude boundaries\nplt.xlim(-180, 180)\nplt.ylim(-90, 90)\n\nplt.axvline(x=-70)\nplt.axvline(x=-83)\nplt.axhline(y=36)\nplt.axhline(y=46)\n\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"geo_df = gpd.GeoDataFrame(train, geometry=gpd.points_from_xy(x=train[\"dropoff_longitude\"], y=train[\"dropoff_latitude\"]))\n\nfig, ax = plt.subplots(figsize=(15, 15))\nstreet_map.plot(ax=ax, alpha=0.4, color=\"grey\")\ngeo_df.plot(ax=ax, alpha=0.5, legend=True, markersize=10)\n\nplt.title(\"Cleaned Dropoff Locations\", fontsize=15, fontweight=\"bold\")\n\n# Set latitude and longitude boundaries\nplt.xlim(-180, 180)\nplt.ylim(-90, 90)\n\nplt.axvline(x=-70)\nplt.axvline(x=-83)\nplt.axhline(y=36)\nplt.axhline(y=46)\n\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's also look at a quick map of the east coast of the United States.","metadata":{"_cell_guid":"50dcc6c6-2d24-43ea-bc0d-a0d036c7f4b8","_uuid":"4bd0abc4-286b-4b50-8387-bd08d1794f03","editable":true}},{"cell_type":"code","source":"street_map = gpd.read_file(\"../input/eastus/ne_110m_land.shp\")\n\n# Create a dataframe using Geopandas to hold the pickup longitude and latitude values from the train dataset\ngeo_df = gpd.GeoDataFrame(train, geometry=gpd.points_from_xy(x=train[\"pickup_longitude\"], y=train[\"pickup_latitude\"]))\n\n# Now we create our figure and axes and assign these to out plot\nfig, ax = plt.subplots(figsize=(15, 15))\nstreet_map.plot(ax=ax, alpha=0.4, color=\"grey\")\ngeo_df.plot(ax=ax, alpha=0.5, legend=True, markersize=10)\n\n# Give our plot a title\nplt.title(\"Pickup Locations - East Coast Map\", fontsize=15, fontweight=\"bold\")\n\n# Set latitude and longitude boundaries for the large overview of the world map\nplt.xlim(-90, -50)\nplt.ylim(20.496103, 55.01585)\n\n# These lines very roughly represent where our cutoff point should be for NY state\nplt.axvline(x=-70)\nplt.axvline(x=-83)\nplt.axhline(y=36)\nplt.axhline(y=46)\n\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"street_map = gpd.read_file(\"../input/nycity/geo_export_21a50042-3764-4df5-b711-bf69e0736421.shp\")\n\n# Now we create our figure and axes and assign these to out plot\nfig, ax = plt.subplots(figsize=(15, 15))\nstreet_map.plot(ax=ax, alpha=0.4, color=\"grey\")\ngeo_df.plot(ax=ax, alpha=0.5, legend=True, markersize=10)\n\n# Give our plot a title\nplt.title(\"Pickup Locations - NYC Map\", fontsize=15, fontweight=\"bold\")\n\n# Set latitude and longitude boundaries for the \nplt.xlim(-74.02, -73.925)\nplt.ylim(40.7, 40.8)\n\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Great, we can see the vast majority of the points now lie within NY, on land. Now our mapping is complete, we can look at introducing new features into our dataset.\r\n\r\nAgain, it's important to think about a dataset like taxi fares from a logical point of view. For example, what sort of information would impact the fare of a taxi? The distance of the trip of course, but what else? The time of day? The day of the week? All these are features that could prove useful in our model, so let's introduce some of them into our dataset.\r\n\r\nWe will create 6 new features in total from our datetime object - the day of the week, the day of the month, the hour, the week of the year, the month, and the year.","metadata":{"_cell_guid":"82b12f9b-8f62-41fc-a2ab-294867dc260d","_uuid":"8413c4d7-7e13-47e8-a404-1089d0542c9a"}},{"cell_type":"code","source":"# Convert pickup date to datetime object\r\ntrain['pickup_datetime'] = pd.to_datetime(train['pickup_datetime'], format=\"%Y-%m-%d %H:%M:%S UTC\")\r\ntest['pickup_datetime'] = pd.to_datetime(test['pickup_datetime'], format=\"%Y-%m-%d %H:%M:%S UTC\")\r\nprint(\"Datetime object created.\")\r\n\r\n# Create an hour feature\r\ntrain['hour'] = train['pickup_datetime'].dt.hour\r\ntest['hour'] = test['pickup_datetime'].dt.hour\r\nprint(\"Hour created.\")\r\n\r\n# Create a day of week feature, map days from 0 to 6\r\ntrain['dayofweek'] = train['pickup_datetime'].dt.dayofweek\r\ntest['dayofweek'] = test['pickup_datetime'].dt.dayofweek\r\nprint(\"DayOfWeek column created.\")\r\n\r\n# Create a day of the month feature\r\ntrain['dayofmonth'] = train.pickup_datetime.dt.day\r\ntest['dayofmonth'] = test.pickup_datetime.dt.day\r\n\r\n# Create a week of the year feature\r\ntrain['week'] = train.pickup_datetime.dt.week\r\ntest['week'] = test.pickup_datetime.dt.week\r\n\r\n# Create a month feature\r\ntrain['month'] = train.pickup_datetime.dt.month\r\ntest['month'] = test.pickup_datetime.dt.month\r\n\r\n# Create a year feature\r\ntrain['year'] = train.pickup_datetime.dt.year\r\ntest['year'] = test.pickup_datetime.dt.year","metadata":{"_cell_guid":"704f5b61-f225-49f6-ab5e-f3308aa6d510","_uuid":"e34ad42c-3d43-4059-8053-da1f65d5f5fe","collapsed":false,"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Great, we've created six new features based on our datetime object. Now that we have added features based on the datetime column, we can look at using the latitude and longitude columns to add new features.\r\n\r\nThe first and most obvious one we will add is the distance of the trip in kilometres. This is done by using the Haversine distance formula to take in our latitude and longitude values, translating those into a distance in kilometers. First we define the function, and then apply this function to both the train and test data.","metadata":{"_cell_guid":"95d0ecdc-c78d-41da-81ae-29f2648c1616","_uuid":"2a7c435a-d31a-439d-8d84-6e520e50ac5b"}},{"cell_type":"code","source":"# Define the Haversine distance formula to calculate the distance between two points on the Earth\r\ndef haversine_distance(lat1, lon1, lat2, lon2):\r\n   r = 6371  # Radius of the earth in km\r\n   phi1 = np.radians(lat1)\r\n   phi2 = np.radians(lat2)\r\n   delta_phi = np.radians(lat2 - lat1)\r\n   delta_lambda = np.radians(lon2 - lon1)\r\n   a = np.sin(delta_phi / 2)**2 + np.cos(phi1) * np.cos(phi2) * np.sin(delta_lambda / 2)**2\r\n   res = r * (2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a)))\r\n   return np.round(res, 2)\r\n\r\n# Calculate the ride distance and add it as a feature to our train and test sets\r\ntrain['distance_km'] = haversine_distance(train[\"pickup_latitude\"], train[\"pickup_longitude\"], train[\"dropoff_latitude\"], train[\"dropoff_longitude\"])\r\ntest['distance_km'] = haversine_distance(test[\"pickup_latitude\"], test[\"pickup_longitude\"], test[\"dropoff_latitude\"], test[\"dropoff_longitude\"])","metadata":{"_cell_guid":"80b35c24-c46b-4850-ae9d-0c19c9cf1449","_uuid":"d731cacd-21b5-4c96-afbf-7744fc1d0e2f","collapsed":false,"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we've added the distance from point A to point B of the trip. However something that we can notice on our map plots above is that a lot of trips both originate and terminate at the three major airports in NY - JFK, EWR, and LGA. Often these trips will have a flat fee, and so adding features for these trips may help our model. Let's add one feature for pickup and one feature for dropoff, for each of the three major commercial airports in NY.\r\n\r\nFirst we define the coordinates of the airports and then apply our haversine distance function that we used before to again calculate the airport distance.","metadata":{}},{"cell_type":"code","source":"def from_airport_distance(dataset):\n\n    # Define the coordinates for each airport\n    jfk_coords = (40.639722, -73.778889)\n    ewr_coords = (40.6925, -74.168611)\n    lga_coords = (40.77725, -73.872611)\n\n    dataset['pickup_jfk_distance'] = haversine_distance(jfk_coords[0], jfk_coords[1], dataset.pickup_latitude, dataset.pickup_longitude)\n    dataset['dropof_jfk_distance'] = haversine_distance(jfk_coords[0], jfk_coords[1], dataset.dropoff_latitude, dataset.dropoff_longitude)\n    \n    dataset['pickup_ewr_distance'] = haversine_distance(ewr_coords[0], ewr_coords[1], dataset.pickup_latitude, dataset.pickup_longitude)\n    dataset['dropof_ewr_distance'] = haversine_distance(ewr_coords[0], ewr_coords[1], dataset.dropoff_latitude, dataset.dropoff_longitude)\n    \n    dataset['pickup_lga_distance'] = haversine_distance(lga_coords[0], lga_coords[1], dataset.pickup_latitude, dataset.pickup_longitude)\n    dataset['dropof_lga_distance'] = haversine_distance(lga_coords[0], lga_coords[1], dataset.dropoff_latitude, dataset.dropoff_longitude)\n    \n    return dataset\n\ntrain = from_airport_distance(train)\ntest = from_airport_distance(test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Another feature we can modify relates to our pickup and dropoff latitude and longitude coordinates. We won't actually be creating a new feature here, just converting the degrees to radians for the pickup and dropoff longitude and latitude values. Why do we do this? There's a great article [here](https://towardsdatascience.com/machine-learning-tip-using-rotational-data-b67ded0a33ad) explaining why, the summary is that radians make far more sense than degrees for a model to interpret.","metadata":{}},{"cell_type":"code","source":"def degrees_to_radians(degree):\n    return  np.radians(degree)    \n\ntrain['pickup_latitude'] = degrees_to_radians(train['pickup_latitude'])\ntrain['pickup_longitude'] = degrees_to_radians(train['pickup_longitude'])\ntrain['dropoff_latitude'] = degrees_to_radians(train['dropoff_latitude'])\ntrain['dropoff_longitude'] = degrees_to_radians(train['dropoff_longitude'])\n\ntest['pickup_latitude'] = degrees_to_radians(test['pickup_latitude'])\ntest['pickup_longitude'] = degrees_to_radians(test['pickup_longitude'])\ntest['dropoff_latitude'] = degrees_to_radians(test['dropoff_latitude'])\ntest['dropoff_longitude'] = degrees_to_radians(test['dropoff_longitude'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Perfect. We've now added a number of new features. It's worth noting that this is only some of the features that can be added - there are many more that you could add!\n\nNow let's try visualise our new features as well as the original features in the data. As we said previously, we expect the fare amount to change based on these new features. Remember that we have only loaded 2m rows of data for these visualisations - loading the entire dataset will give us a better look at what's going on!\n\nFirst, let's take a look at the distance of our trip and see how the fare amount varies with increasing distance.","metadata":{}},{"cell_type":"code","source":"# How does the distance affect the fare price?\r\nplt.figure(figsize=(12, 8))\r\nsns.scatterplot(data=train, x=\"fare_amount\", y=\"distance_km\", marker=\".\", alpha=0.6)\r\nplt.xlabel('Fare amount')\r\nplt.ylabel('Distance, km')\r\nplt.xlim(0, 500)\r\nplt.ylim(0, 500)\r\nplt.title('Fare amount based on the distance')\r\nplt.show()","metadata":{"_cell_guid":"8715648f-5d00-4f6f-8799-68566ba27c50","_uuid":"16b33646-edb9-4887-a108-709df488a4cf","collapsed":false,"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This plot is very interesting. We can see a very high concentration of trips between 0 and 40km in distance, and the fare for these trips seems to follow a relatively linear relationship - as the distance increases, so does the fare. However, this isn't the case for every trip visualised on this plot.\r\n\r\nSomething that could potentially improve this model is dropping trips above a certain distance. A trip above even 100km in a taxi would be incredibly rare in practice, and the fares certainly don't make a lot of sense for these trips. As an example, on this plot, at the 150km distance, most of these trips cost less than $50! This isn't realistic and really fails a logical check, so we could potentially improve our model by dropping these odd trips from our train data.\r\n\r\nNext let's look at the passenger count and how it affects the price, as well as the distribution of the passenger count in each trip.","metadata":{}},{"cell_type":"code","source":"# How does the passenger count affect the fare price?\nplt.figure(figsize=(12, 8))\nsns.scatterplot(data=train, x=\"passenger_count\", y=\"fare_amount\", s=10)\nplt.xlabel('Number of Passengers')\nplt.ylabel('Fare')\nplt.show()\n\n# How many passengers are in each trip?\nplt.figure(figsize=(12, 8))\nsns.countplot(data=train, x=\"passenger_count\", color=(89/255, 117/255, 164/255, 1))\nplt.xlabel('Number of Passengers')\nplt.ylabel('Observations')\nplt.yticks([200000, 400000, 600000, 800000, 1000000, 1200000, 1400000], [\"0.2M\", \"0.4M\", \"0.6M\", \"0.8M\", \"1M\", \"1.2M\", \"1.4M\"])\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see there isn't a significant relationship between passenger count and price. This is further apparent when we look at the distribution of passengers across all trips - the vast majority of trips have only one passenger on board.\r\n\r\nWe will now look at the hour of each trip and how this affects the fare price.","metadata":{}},{"cell_type":"code","source":"# How does the hour affect the fare price?\nhour_price = train.groupby('hour', as_index=False)['fare_amount'].median()\n\n# Generate a line plot of the median fare for each hour\nplt.figure(figsize=(12, 8))\nsns.lineplot(data=hour_price, x=\"hour\", y=\"fare_amount\", marker=\"o\")\nplt.xlabel('Hour of the day')\nplt.ylabel('Median fare amount')\nplt.title('Fare amount based on day time')\nplt.xticks(range(24))\nplt.show()\n\n# Let's also look at amount of trips by hour\nplt.figure(figsize=(12, 8))\nsns.countplot(data=train, x=\"hour\", color=(89/255, 117/255, 164/255, 1))\nplt.xlabel('Hour')\nplt.ylabel('Observations')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see the highest median fare price comes in at 4am, and this makes a lot of sense. At such an early hour there is unlikely to be many customers looking for a taxi, and as such fares are increased. As we move into peak hours and during the day, the median fare price drops dramatically. It is between the hours of 10pm and 5am that the median fare price is higher.\r\n\r\nThis assumption is supported by our bar plot as well. The amount of trips is at a minimum at 5am, and starts to drop off at around 10pm. Less trips in the evening and early hours of the morning, leading to higher fares during those hours. Interesting to note is that the busiest hour is 7pm, likely a combination of some people returning home from work, and others on their way out for dinner.\r\n\r\nWe'll now look at the day of the week and how this affects the fare price, as well as the number of trips per day.","metadata":{}},{"cell_type":"code","source":"# How does the day of the week affect the fare price?\n# Note that Monday is 0 and Sunday is 6 before labels are changed\nday_price = train.groupby('dayofweek', as_index=False)['fare_amount'].median()\ndays = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n\nplt.figure(figsize=(12, 8))\nsns.lineplot(data=day_price, x=\"dayofweek\", y=\"fare_amount\", marker=\"o\")\nplt.xlabel('Day of Week')\nplt.ylabel('Median fare amount')\nplt.title('Fare amount based on day of the week')\nplt.xticks([0, 1, 2, 3, 4, 5, 6], days)\nplt.show()\n\n# How many trips are in each day?\nplt.figure(figsize=(12, 8))\nsns.countplot(data=train, x=\"dayofweek\", color=(89/255, 117/255, 164/255, 1))\nplt.xlabel('Day of Week')\nplt.ylabel('Observations')\nplt.xticks([0, 1, 2, 3, 4, 5, 6], days)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It's interesting to see that there isn't much of a relationship here. The fare amount remains constant except on a Monday, where the median fare is 40 cents lower than the rest of the days. The distribution of trips per day of the week doesn't show any significant variation either - Monday has the lowest amount of trips, but not by a significant margin. Friday has the most trips which makes some sense, a combination of people still going to and from work (unlike on weekends), but also has the added spike of trips for people going out for dinner on the Friday evening.\n\nWe can now look at the week of the year and see how this might affect our fare price.","metadata":{}},{"cell_type":"code","source":"# How does the week of the year affect the price?\r\nweek_price = train.groupby('week', as_index=False)['fare_amount'].median()\r\n\r\nplt.figure(figsize=(12, 8))\r\nsns.lineplot(data=week_price, x=\"week\", y=\"fare_amount\", marker=\"o\")\r\nplt.xlabel('Week')\r\nplt.ylabel('Median fare amount')\r\nplt.xticks(range(1, 54))\r\nplt.title('Fare amount based on the week')\r\nplt.show()\r\n\r\n# What is the frequency of trips in each week of the year?\r\nplt.figure(figsize=(12, 8))\r\nsns.countplot(data=train, x=\"week\", color=(89/255, 117/255, 164/255, 1))\r\nplt.xlabel('Week')\r\nplt.ylabel('Observations')\r\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The first thing you might notice here is that there's 53 weeks on these figures! This is because the year 2012 was a year with 53 weeks, a phenomenon that occurs every 5-6 years. Our data includes the years 2009-2015.\r\n\r\nIt is interesting to note that the number of trips is significantly higher in the first 26 weeks of the year, at which point we see a significant decline in week 27, and then a steady trend for the remainder of the year. The number of trips also drops off significantly in the final weeks of the year (51 and 52), likely due to Christmas and New Year celebrations. There isn't any other significant variation in the number of trips per week except in week 53, as there is only one week 53 in our data of 2009-2015.\r\n\r\nWe will now look at how the fare price changes through the months of the year.","metadata":{}},{"cell_type":"code","source":"# Get the median fare for each month\r\nmonth_price = train.groupby('month', as_index=False)['fare_amount'].median()\r\n\r\n# Plot the median fare amount each month in the year\r\nplt.figure(figsize=(12, 8))\r\nmonths = [\"J\", \"F\", \"M\", \"A\", \"M\", \"J\", \"J\", \"A\", \"S\", \"O\", \"N\", \"D\"]\r\nsns.lineplot(data=month_price, x=\"month\", y=\"fare_amount\", marker=\"o\")\r\nplt.xlabel('Month')\r\nplt.ylabel('Median fare amount')\r\nplt.xticks(range(1, 13), months)\r\nplt.title('Fare amount based on month of the year')\r\nplt.show()\r\n\r\n# How many trips are there each month?\r\nplt.figure(figsize=(12, 8))\r\nsns.countplot(data=train, x=\"month\", color=(89/255, 117/255, 164/255, 1))\r\nplt.xlabel(\"Month\")\r\nplt.ylabel(\"Observations\")\r\nplt.xticks(range(0, 12), months)\r\nplt.show()\r\n\r\n# How does the day of the month affect the price?\r\nplt.figure(figsize=(15, 7))\r\nsns.scatterplot(data=train, x=\"dayofmonth\", y=\"fare_amount\", s=10)\r\nplt.xlabel('Date')\r\nplt.ylabel('Fare Amount')\r\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"These plots are similar to our previous ones where we investigated the weeks of the year, but instead now we are looking at a different level of abstraction. The highest median fare amount occurs in December, and then a significant drop occurs to the lowest median fare amount in January. July and August also see a large dip in the median fare. When we look at our next plot of the number of trips, we can see the least amount of trips occur in July and August.\r\n\r\nOther than these small variations there isn't anything significant to point out one month being far busier than any other.\r\n\r\nWe will now move to the highest level of abstraction that we have - the year feature.","metadata":{}},{"cell_type":"code","source":"# Again we find the median fare price, but this time for each year\nyear_price = train.groupby('year', as_index=False)['fare_amount'].median()\n\n# Now plot the median fare price for each year\nplt.figure(figsize=(12, 8))\nsns.lineplot(data=year_price, x=\"year\", y=\"fare_amount\", marker=\"o\")\nplt.xlabel('Year')\nplt.ylabel('Median fare amount')\nplt.title('Fare amount based on the year')\nplt.show()\n\n# Now plot the number of trips taken each year in our data\nplt.figure(figsize=(12, 8))\nsns.countplot(data=train, x=\"year\", color=(89/255, 117/255, 164/255, 1))\nplt.xlabel(\"Year\")\nplt.ylabel(\"Observations\")\nplt.title(\"Number of trips per Year\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It's interesting to see that the median fare sees no change in 2009, 2010, and 2011, but in 2012 we see a relatively sharp increase of \\\\$0.75 in the median fare, and in 2013 we see an increase of \\\\$1.00. The median fare then remains the same through 2013, 2014, and 2015.\n\nThe number of trips per year doesn't provide much insight into this, as it remains relatively steady with only mild fluctiations in the number of trips per year. The drop in 2015 is not important as we previously mentioned that this dataset simply doesn't contain data for the entirety of 2015.\n\nThis is our visualisation complete. It's always important to note that this is just a basic overview of each feature - you could go far further into depth with these, especially with the maps of NYC above. I would encourage any learners to take advantage of the ability to create interactive maps using Bokeh and other packages which can help visualise the trips in the state in an even better way.","metadata":{}},{"cell_type":"markdown","source":"## Part 2 - Model and Predictions\r\n\r\nNow that we've done our preliminary data cleaning and feature selection, let's look at creating multiple models that will allow us to predict the fare amount on our test data for the competition.\r\n\r\nThe first thing that I always like to do is look at the columns in the train and test data. We've added a number of new features to our data so it's important to verify one last time that all the columns are as we expect before we begin to create a model.","metadata":{"_cell_guid":"00fd9b78-b40f-4447-99b4-d5ce0f2f3335","_uuid":"bc7286c7-8d79-42bc-8205-a508d9612a23"}},{"cell_type":"code","source":"# Take a look at our columns before we pass anything into our model to make sure we have the correct structure\nprint('Train columns:', train.columns.tolist())\nprint('\\nTest columns:', test.columns.tolist())\n\n# Remove the geometry column added for creating the plots earlier on the train data\ntrain = train.drop(\"geometry\", axis=1)\n\n# Select only numeric features to use for our model training and predictions\n# Note no key or pickup_datetime here\nfeatures = [\n'pickup_longitude',\n'pickup_latitude',\n'dropoff_longitude',\n'dropoff_latitude',\n'distance_km',\n'passenger_count',\n'hour',\n'dayofweek',\n'dayofmonth',\n'week',\n'month',\n'year',\n'pickup_jfk_distance',\n'dropof_jfk_distance',\n'pickup_ewr_distance',\n'dropof_ewr_distance',\n'pickup_lga_distance',\n'dropof_lga_distance']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see above that all the columns are correct. Note the absence of the fare amount column in the test data, as this is what we are trying to predict. Other than that, the datasets have the same columns. We've also defined the features we want to use in our model, ignoring the key and the fare amount in the train data.\r\n\r\nFor this notebook I'm only going to show one approach, using a package called Optuna.\r\n\r\nOptuna is a package that will allow us to tune the hyperparameters of an XGBoost model by providing a set of hyperparameters to iterate through. It's efficient and far faster than other methods such as GridSearchCV which I haven't demonstrated here.\r\n\r\nThe first thing we do is define our objective function, specifying our train data and our train target. We then use train_test_split to split our train data into multiple sets, and then we define our hyperparameter grid, just as we would with any other hyperparameter searching package.\r\n\r\nWe then create an XGB Regression model which we will use to run through our hyperparameter grid.","metadata":{}},{"cell_type":"code","source":"# A list of hyperparameters available for tuning can be found at https://xgboost.readthedocs.io/en/latest/parameter.html\n\ndef objective(trial, data=train[features], target=train.fare_amount):\n    \n    x_train, x_test, y_train, y_test = train_test_split(train[features], train.fare_amount, test_size=0.2, random_state=1)\n    \n    param = {\n        'tree_method':'gpu_hist',  # Use GPU acceleration\n        'lambda': trial.suggest_loguniform(\n            'lambda', 1e-3, 10.0\n        ),\n        'alpha': trial.suggest_loguniform(\n            'alpha', 1e-3, 10.0\n        ),\n        'colsample_bytree': trial.suggest_categorical(\n            'colsample_bytree', [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n        ),\n        'subsample': trial.suggest_categorical(\n            'subsample', [0.6, 0.7, 0.8, 1.0]\n        ),\n        'learning_rate': trial.suggest_categorical(\n            'learning_rate', [0.008, 0.009, 0.01, 0.012, 0.014, 0.016, 0.018, 0.02]\n        ),\n        'n_estimators': trial.suggest_categorical(\n            \"n_estimators\", [4000, 5000, 6000]\n        ),\n        'max_depth': trial.suggest_categorical(\n            'max_depth', [7, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n        ),\n        'random_state': 1,\n        'min_child_weight': trial.suggest_int(\n            'min_child_weight', 1, 300\n        ),\n    }\n    \n    model = xgb.XGBRegressor(**param)  \n    \n    model.fit(x_train, y_train, eval_set=[(x_test, y_test)], early_stopping_rounds=100, verbose=False)\n    \n    predictions = model.predict(x_test)\n    \n    rmse = mean_squared_error(y_test, predictions, squared=False)\n    \n    return rmse","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we create an Optuna study with 20 trials. We also generate some visualisations which will appear in the local notebook environment. To have these plots display in the browser instead (if running on a local jupyter kernel), add renderer=\"browser\" in the show method.","metadata":{}},{"cell_type":"code","source":"study = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=20)\n\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)\n\n# Plot the optimisation history, showing the scores from all the trials\noptuna.visualization.plot_optimization_history(study).show()\n\n# Create a parallel coordinate plot\noptuna.visualization.plot_parallel_coordinate(study).show()\n\n# Plot the feature importance of each individual hyperparameter being tuned\noptuna.visualization.plot_param_importances(study).show()\n\n# Finally, plot the best parameters\nstudy.best_params\n\n# Now store the best hyperparameters that we obtained, as well as maintaining the random state and gpu acceleration for the final model\nbest_params = study.best_params\nbest_params['tree_method'] = 'gpu_hist'\nbest_params['random_state'] = 1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we have obtained the best hyperparameters from our grid, we create a final XGB Regressor with these parameters and then fit the new model to our train data once again. We then make predictions on the provided test data and export these predictions as a CSV file.","metadata":{}},{"cell_type":"code","source":"clf = xgb.XGBRegressor(**(best_params))\r\n\r\nclf.fit(train[features], train.fare_amount)\r\n\r\npredictions = clf.predict(test[features])\r\n\r\n# Confirm that our predictions are the same shape as the test data\r\nprint(predictions.shape)\r\nprint(test.shape)\r\n\r\nholdout = pd.DataFrame({'key': test['key'], 'fare_amount': predictions})\r\nholdout.to_csv('predictions.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"One final visualisation we generate is the feature importances for each feature in the model - note that this is different from the Optuna feature importance which is for the importance of each hyperparameter, not each feature in the model.","metadata":{}},{"cell_type":"code","source":"# Create a horizontal bar chart of the model feature importances, note that we have 18 features\nfeature_importances = pd.Series(clf.feature_importances_, index=train[features].columns)\nfeature_importances.nlargest(20).plot(kind='barh')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This simple submission using one model and hyperparameter tuning using Optuna gives a top 10% ranking in the competition. To further improve I would encourage the reader to think about the following topics:\n* Data cleaning thoroughly, i.e. dropping outliers in a more accurate way than simply just chopping at a given latitude and longitude\n* Further feature selection, either through dropping features that aren't significant and creating further new features that may help model performance\n* Multiple models (LGBM/RF are good starters), varied hyperparameter grids, and model ensembling\n\nThat's everything for this notebook. I hope you've been able to learn something and if you have any feedback feel free to get in touch.","metadata":{}}]}