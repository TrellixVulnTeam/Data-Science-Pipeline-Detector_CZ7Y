{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nimport glob\nimport time\nimport datetime\nfrom sklearn.linear_model import LinearRegression \n#import xgboost as xgb #XGBoost classifier\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom math import sin, cos, sqrt, atan2, radians\nfrom sklearn import metrics #evaluating models\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom datetime import datetime\nfrom sklearn.externals import joblib\nfrom functools import wraps\nimport xgboost as xgb\nimport scipy\nfrom sklearn.metrics import mean_squared_error\nfrom bayes_opt import BayesianOptimization\n\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"def fn_timer(function):\n    @wraps(function)\n    def function_timer(*args, **kwargs):\n        t0 = time.time()\n        #print(\"function type: {}\".format(type(function)))\n        result = function(*args, **kwargs)\n        t1 = time.time()\n        print (\"***************************Total time running << %s >>: %s seconds\" %\n               (function.__name__, str(np.round((t1-t0),2)))\n               )\n        return result\n    return function_timer\n\n# Any results you write to the current directory are saved as output.\n@fn_timer\ndef read_file_to_df(fileName,rows):\n    # Set columns to most suitable type to optimize for memory usage\n    start_time = time.time()\n    traintypes = {'fare_amount': 'float32',\n                  'pickup_datetime': 'str', \n                  'pickup_longitude': 'float32',\n                  'pickup_latitude': 'float32',\n                  'dropoff_longitude': 'float32',\n                  'dropoff_latitude': 'float32',\n                  'passenger_count': 'uint8'}\n    \n    cols = list(traintypes.keys())\n    \n\n#    if (rows> 0):\n#        df = pd.read_csv(fileName, usecols=cols, dtype=traintypes,nrows=rows)\n#    else:\n#        df = pd.read_csv(fileName, usecols=cols, dtype=traintypes)\n#    df.head()\n#    print(df.shape)\n#    return df\n\n    from pathlib import Path\n    \n    my_file = Path(fileName + \".feather\")\n    if my_file.is_file():\n        df = pd.read_feather(fileName + \".feather\")\n    else:    \n        if (rows> 0):\n            df = pd.read_csv(fileName, usecols=cols, dtype=traintypes)\n        else:\n            df = pd.read_csv(fileName, usecols=cols, dtype=traintypes)\n        #df = pd.read_csv(\"../input/train.csv\", usecols=cols, dtype=traintypes)\n        df.to_feather(fileName + \".feather\")\n        #df.head()\n    print(\"--- Read Files: %s secs ---\" % np.round((time.time() - start_time),2))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"138388b2060c437b9430cd894de6fbb9264b507e","collapsed":true},"cell_type":"code","source":"def read_test_file_to_df(fileName):\n    df = pd.read_csv(fileName)\n    return df\n\n\n\ndef drop_rows_with_nan(df):\n    print(\"Before dropna\")\n    df.dropna(how = 'any', axis = 'rows',inplace=True)\n    print(df.shape)\n\n@fn_timer\ndef data_cleanup(tmp_df):\n    print(\"Before clearing outliers\")\n    tmp_df = tmp_df[tmp_df['fare_amount'] > 0]\n    tmp_df = tmp_df[tmp_df['pickup_longitude'] < -72]\n    tmp_df = tmp_df[(tmp_df['pickup_latitude'] > 40) & (tmp_df['pickup_latitude'] < 44)]\n    tmp_df = tmp_df[tmp_df['dropoff_longitude'] < -72]\n    tmp_df = tmp_df[(tmp_df['dropoff_latitude'] > 40) & (tmp_df['dropoff_latitude'] < 44)]\n    tmp_df = tmp_df[(tmp_df['passenger_count'] > 0) & (tmp_df['passenger_count'] < 10)]\n    print(tmp_df.shape)\n    return tmp_df\n\n\ndef reformat_pickup_datetime(df):\n    df['pickup_datetime'] = df['pickup_datetime'].str.slice(0, 13)\n    df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'],utc=True,format='%Y-%m-%d %H')\n    df['pickup_datetime'] = df['pickup_datetime'].apply(lambda x: x.astimezone('EST'))\n    return df\n\ndef add_day_of_week_feature(tmp_df):\n    tmp_df['dayOfWeek'] = tmp_df['pickup_datetime'].dt.dayofweek.astype('uint8')\n\n\ndef add_time_of_day_feature(tmp_df):\n    #val = tmp_df['pickup_datetime'].dt.hour + (1 if tmp_df['pickup_datetime'].dt.minute > 30 else 0)\n    #tmp_df['timeOfDay'] = tmp_df['pickup_datetime'].dt.hour.astype('uint8')\n    #val = 0 if val > 23 else val\n    tmp_df['timeOfDay'] = tmp_df['pickup_datetime'].dt.hour.astype('uint8')\n\ndef add_month_feature(tmp_df):\n    #tmp_df['month'] = (pd.to_datetime(tmp_df['pickup_datetime'],utc=True,format='%Y-%m-%d %H')).dt.month\n    tmp_df['month'] = tmp_df['pickup_datetime'].dt.month.astype('uint8')\n\ndef add_week_of_year_feature(tmp_df):\n    tmp_df['weekOfYear'] = tmp_df['pickup_datetime'].dt.weekofyear.astype('uint8')\n\ndef add_year_feature(tmp_df):\n    tmp_df['year'] = tmp_df['pickup_datetime'].dt.year.astype('uint16')\n\n\ndef distance_between_two_points(row):\n    # approximate radius of earth in km\n    R = 6373.0\n\n    lat1 = radians(row['pickup_latitude'])\n    lon1 = radians(row['pickup_longitude'])\n    lat2 = radians(row['dropoff_latitude'])\n    lon2 = radians(row['dropoff_longitude'])\n\n    dlon = lon2 - lon1\n    dlat = lat2 - lat1\n\n    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n\n    distance = R * c\n    return \"{0:.3f}\".format(distance)\n\n#calculate distance between two cordinates \n@fn_timer    \ndef add_distance_feature(tmp_df):\n    start_time = time.time()\n    tmp_df['distance'] = tmp_df.apply(distance_between_two_points,axis=1).apply(pd.to_numeric).astype('float32')\n    #tmp_df['distance'] = tmp_df.apply(distance_between_two_points,axis=1)\n    print(\"--- Add distance: %s secs ---\" % np.round((time.time() - start_time),2))\n\n\n@fn_timer\ndef filter_based_on_distance(df,distance):\n    df = df[(df['distance'] < 100)]    \n    \n@fn_timer\ndef data_one_hot_encoding(tmp_df):\n    print(\"Before one hot encoding\")\n    start_time = time.time()\n    tmp_df = pd.concat([tmp_df,pd.get_dummies(tmp_df['timeOfDay'], prefix='timeOfDay')],axis=1)\n    tmp_df.drop(['timeOfDay'],axis=1, inplace=True)\n    \n    tmp_df = pd.concat([tmp_df,pd.get_dummies(tmp_df['dayOfWeek'], prefix='dayOfWeek')],axis=1)\n    tmp_df.drop(['dayOfWeek'],axis=1, inplace=True)\n    \n    tmp_df = pd.concat([tmp_df,pd.get_dummies(tmp_df['month'], prefix='month')],axis=1)\n    tmp_df.drop(['month'],axis=1, inplace=True)\n    \n    #tmp_df = pd.concat([tmp_df,pd.get_dummies(tmp_df['weekOfYear'], prefix='weekOfYear')],axis=1)\n    print(tmp_df.columns)\n    print(\"--- One hot encoding: %s secs ---\" % np.round((time.time() - start_time),2))\n    return tmp_df\n\n@fn_timer\ndef prepare_data_split(tmp_df):\n    #tmp_X = tmp_df.drop(['pickup_datetime','fare_amount','key'],axis=1)\n    tmp_X = tmp_df.drop(['pickup_datetime','fare_amount','key'],axis=1)\n    tmp_y = tmp_df['fare_amount']\n    print(\"Before train test split\")\n    tmp_X_train, tmp_X_test, tmp_y_train, tmp_y_test = train_test_split(tmp_X,tmp_y, test_size=0.20)\n    print(\"df.shape:\" + str(tmp_df.shape))\n    print(\"tmp_X.shape:\" + str(tmp_X.shape))\n    print(\"tmp_y.shape:\" + str(tmp_y.shape))\n    return tmp_X,tmp_y,tmp_X_train, tmp_X_test, tmp_y_train, tmp_y_test\n\n\n\n\n\n\n@fn_timer\ndef get_rmse(model,data,output):\n    dtrain = xgb.DMatrix(data, label=output)\n    y_pred = model.predict(dtrain)\n    rmse = np.sqrt(metrics.mean_squared_error(y_pred, output))\n    #print(\"rmse:\" + str(rmse))\n    return rmse\n@fn_timer\n\ndef xgb_evaluate(max_depth, gamma, colsample_bytree):\n    params = {'eval_metric': 'rmse',\n              'max_depth': int(max_depth),\n              'subsample': 0.8,\n              'eta': 0.1,\n              'gamma': gamma,\n              'colsample_bytree': colsample_bytree}\n    # Used around 1000 boosting rounds in the full model\n    cv_result = xgb.cv(params, dtrain, num_boost_round=100, nfold=3,verbose_eval=None,early_stopping_rounds=10)    \n    \n    # Bayesian optimization only knows how to maximize, not minimize, so return the negative RMSE\n    return -1.0 * cv_result['test-rmse-mean'].iloc[-1]\n\ndtrain =[]\n@fn_timer\ndef fit_xgboost_model(X_train,X_test,y_train,y_test):\n    dtrain = xgb.DMatrix(X_train, label=y_train)\n    del(X_train)\n    dtest = xgb.DMatrix(X_test)\n    del(X_test)\n    params = {'eval_metric': 'rmse',\n              'max_depth': 7,\n              'subsample': 0.8,\n              'eta': 0.03,\n              'gamma': 1,\n              'colsample_bytree': 0.9}\n    print(\"fit_xgboost_model\")\n    model = xgb.train(params, dtrain, num_boost_round=1000,early_stopping_rounds=10)\n    \n    # Predict on testing and training set\n    y_pred = model.predict(dtest)\n    y_train_pred = model.predict(dtrain)\n    \n    # Report testing and training RMSE\n    print(np.sqrt(mean_squared_error(y_test, y_pred)))\n    print(np.sqrt(mean_squared_error(y_train, y_train_pred)))\n    #model.fit(X_train,y_train)\n    #print(\"XGB Score on train:\" + str(model.score(X_train,y_train)))\n    #print(\"XGB score on test:\" + str(model.score(X_test,y_test)))\n    return model\n\n@fn_timer\ndef bayes_optimization(X_train,X_test,y_train,y_test):\n    global dtrain\n    dtrain = xgb.DMatrix(X_train, label=y_train)\n    del(X_train)\n    dtest = xgb.DMatrix(X_test)\n    del(X_test)\n    print(\"fit_xgboost_model\")\n    xgb_bo = BayesianOptimization(xgb_evaluate, {'max_depth': (3,6, 7), \n                                             'gamma': (0,0.1,1),\n                                             'colsample_bytree': (0.3,0.6, 0.9)})\n    # Use the expected improvement acquisition function to handle negative numbers\n    # Optimally needs quite a few more initiation points and number of iterations\n    xgb_bo.maximize(init_points=2, n_iter=5, acq='ei')\n\n\n    \n@fn_timer\ndef output_submission(model,df_test,test_X):\n    start_time = time.time()\n    test_pred = model.predict(test_X)\n    print(type(test_pred))\n    print(\"Shape of test_pred:\" + str(test_pred.shape))\n    test_pred = np.round(test_pred,2)\n    print(df_test.shape)\n    print(test_pred.shape)\n    # Write the predictions to a CSV file which we can submit to the competition.\n    submission = pd.DataFrame(\n        {'key': df_test.key, 'fare_amount': test_pred},\n        columns = ['key', 'fare_amount'])\n    submission.to_csv('submission.csv', index = False)\n    print(\"--- Output submission: %s secs ---\" % np.round((time.time() - start_time),2))\n\n@fn_timer\ndef output_submission_stacking(df_test,test_d):\n    #files = glob.glob(\"./model/*.compressed\") \n    test_X = xgb.DMatrix(test_d)\n    rmse_df = pd.DataFrame()\n    for model in models:\n    #for f in files:        \n        #model = joblib.load(f)\n        test_pred = model.predict(test_X)\n        print(type(test_pred))\n        print(\"Shape of test_pred:\" + str(test_pred.shape))\n        test_pred = np.round(test_pred,2)\n        rmse_df = pd.concat([rmse_df,pd.DataFrame(test_pred)],axis=1)\n        print(df_test.shape)\n        print(test_pred.shape)\n        del model\n        \n        # Write the predictions to a CSV file which we can submit to the competition.\n        #    submission = pd.DataFrame(\n        #        {'key': df_test.key, 'fare_amount': np.round(np.array(rmse_df.mean(axis=1)),2)},\n        #        columns = ['key', 'fare_amount'])\n    submission = pd.DataFrame(\n        {'key': df_test.key, 'fare_amount': np.round(scipy.stats.mstats.gmean(rmse_df,axis=1),2)},\n        columns = ['key', 'fare_amount'])\n    # scipy.stats.mstats.gmean(rmse_df,axis=1)\n    submission.to_csv('submission.csv', index = False)\n    \n\n@fn_timer\ndef preprocess_df(df,ifTest):\n    start_time = time.time()\n    \n    print(\"Preprocessing data start\")\n    if(ifTest == False):\n        drop_rows_with_nan(df)\n        df = data_cleanup(df)\n    df = reformat_pickup_datetime(df)\n    print(\"Adding day,time and month feature\")\n    add_day_of_week_feature(df)\n    add_time_of_day_feature(df)\n    add_month_feature(df)\n    add_year_feature(df)\n    #add_week_of_year_feature(df)\n    print(df.shape)\n    print(df.dtypes)\n    df.head()\n    print(\"Adding distance feature\")\n    add_distance_feature(df)\n    #df.describe()\n    print(\"One hot encoding features\")\n    #print(df.isnull().sum())\n    if(ifTest == False):\n        filter_based_on_distance(df,100)\n    df = data_one_hot_encoding(df)    \n    if(ifTest == False):\n        drop_rows_with_nan(df)\n    print(\"Preprocessing data end\")\n    print(\"--- preprocess_df: %s secs ---\" % np.round((time.time() - start_time),2))\n    return df    \n\n\n@fn_timer\ndef split_and_fit_model(df):\n\n    tmp_df = preprocess_df(df,False)\n    print(\"Shape of tmp_df: {}\".format(tmp_df.shape))\n    print(\"Info of tmp_df: {}\".format(tmp_df.info()))\n    \n    X,y,X_train,X_test,y_train,y_test = prepare_data_split(tmp_df)\n    del tmp_df\n     \n    bayes_optimization(X_train,X_test,y_train,y_test)   \n    #model =  fit_xgboost_model(X_train,X_test,y_train,y_test)\n    #rmse = get_rmse(model,X,y)\n    #print(\"rmse:\" + str(rmse))\n    del X,y,X_train,X_test,y_train,y_test\n    return model,rmse        \n\ndef dump_model(model,fileName):\n    joblib.dump(model, fileName + \".compressed\",compress=True)\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"217a45fea360705115dea91fa465f72a4e71f84f"},"cell_type":"code","source":"#df = read_file_to_df(\"../input/train.csv\",0)\nreader = pd.read_csv(\"../input/train.csv\", header=0, iterator=True)\n    \n#print(\"Shape of df: {}\".format(df.shape))\n#print(\"Info of df: {}\".format(df.info()))\nmodels = []\nrmses = []\npklFileName = \"./model/modelFile\"\nfor i in range(0,5):\n    print(\"##################### Batch start num {} #####################\".format(i))\n    batchSize = 5_00_000\n    df = reader.get_chunk(batchSize)\n    startRow = batchSize * i\n    endRow = startRow + batchSize\n    model,rmse = split_and_fit_model(df)\n    #dump_model(model,pklFileName+ str(i) )\n    models.append(model)\n    del model\n    rmses.append(rmse)\n    del df\n    print(\"##################### Batch end #####################\")\n    \nreader.close()    \ndel reader\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3e028bd37f5a4af0bfd6fac5637e2ced39d115a3","collapsed":true},"cell_type":"code","source":"df_test = read_test_file_to_df(\"../input/test.csv\")\ndf_test = preprocess_df(df_test,True)    \ntest_X = df_test.drop(['pickup_datetime','key'],axis=1)\n#output_submission_stacking(df_test,test_X)\ndel df_test\ndel test_X","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}