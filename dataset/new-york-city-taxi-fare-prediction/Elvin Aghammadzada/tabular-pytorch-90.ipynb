{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df =  pd.read_csv('../input/new-york-city-taxi-fare-prediction/train.csv', nrows = 2_000_000, parse_dates=[\"pickup_datetime\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['fare_amount'].describe()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from numpy import radians, cos, sin, arcsin, sqrt\n\ndef haversine(lon1, lat1, lon2, lat2):\n    \"\"\"\n    Calculate the great circle distance between two points \n    on the earth (specified in decimal degrees)\n    \"\"\"\n\n    #Convert decimal degrees to Radians:\n    lon1 = np.radians(lon1.values)\n    lat1 = np.radians(lat1.values)\n    lon2 = np.radians(lon2.values)\n    lat2 = np.radians(lat2.values)\n\n    #Implementing Haversine Formula: \n    dlon = np.subtract(lon2, lon1)\n    dlat = np.subtract(lat2, lat1)\n\n    a = np.add(np.power(np.sin(np.divide(dlat, 2)), 2),  \n                          np.multiply(np.cos(lat1), \n                                      np.multiply(np.cos(lat2), \n                                                  np.power(np.sin(np.divide(dlon, 2)), 2))))\n    c = np.multiply(2, np.arcsin(np.sqrt(a)))\n    r = 6371\n\n    return c*r\ndef distance(s_lat, s_lng, e_lat, e_lng):\n\n   # approximate radius of earth in km\n   R = 6373.0\n\n   s_lat = s_lat*np.pi/180.0                      \n   s_lng = np.deg2rad(s_lng)     \n   e_lat = np.deg2rad(e_lat)                       \n   e_lng = np.deg2rad(e_lng)  \n\n   d = np.sin((e_lat - s_lat)/2)**2 + np.cos(s_lat)*np.cos(e_lat) * np.sin((e_lng - s_lng)/2)**2\n\n   return 2 * R * np.arcsin(np.sqrt(d))\n\n# from haversine import haversine","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['dist_kmm'] = haversine(df['pickup_latitude'], df['pickup_longitude'], df['dropoff_latitude'], df['dropoff_longitude'])\ndf['dist_km'] = distance(df['pickup_latitude'], df['pickup_longitude'], df['dropoff_latitude'], df['dropoff_longitude'])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def plot_on_map(df, BB, nyc_map, s=10, alpha=0.2):\n#     fig, axs = plt.subplots(1, 2, figsize=(16,10))\n#     axs[0].scatter(df.pickup_longitude, df.pickup_latitude, zorder=1, alpha=alpha, c='r', s=s)\n#     axs[0].set_xlim((BB[0], BB[1]))\n#     axs[0].set_ylim((BB[2], BB[3]))\n#     axs[0].set_title('Pickup locations')\n#     axs[0].imshow(nyc_map, zorder=0, extent=BB)\n\n#     axs[1].scatter(df.dropoff_longitude, df.dropoff_latitude, zorder=1, alpha=alpha, c='r', s=s)\n#     axs[1].set_xlim((BB[0], BB[1]))\n#     axs[1].set_ylim((BB[2], BB[3]))\n#     axs[1].set_title('Dropoff locations')\n#     axs[1].imshow(nyc_map, zorder=0, extent=BB)\n# BB = (-74.5, -72.8, 40.5, 41.8)\n# nyc_map = plt.imread('https://aiblog.nl/download/nyc_-74.5_-72.8_40.5_41.8.png')\n# plot_on_map(df, BB, nyc_map, s=1, alpha=0.3)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"one_time = df['pickup_datetime'][0]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['EDTdate'] = df['pickup_datetime'] - pd.Timedelta(hours=4)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Hour'] = df['EDTdate'].dt.hour\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['AMPM'] = np.where(df['Hour']<12, 'am', 'pm')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Weekday'] = df['EDTdate'].dt.strftime(\"%a\")\ndf['DoW'] = df['EDTdate'].dt.dayofweek","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_cols = ['Hour', 'AMPM', 'Weekday', 'DoW']\ncont_cols = ['pickup_longitude','pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'passenger_count', 'dist_km']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_col = ['fare_amount']\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for cat in cat_cols:\n    df[cat] = df[cat].astype('category')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['AMPM'].cat.categories\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hr = df['Hour'].cat.codes.values\nampm = df['AMPM'].cat.codes.values\nwd = df['Weekday'].cat.codes.values\ndw = df['DoW'].cat.codes.values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cats = np.stack([hr, ampm, wd, dw], axis=1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cats = torch.tensor(cats, dtype=torch.int64)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"conts = np.stack([df[col].values for col in cont_cols], axis=1)\nconts = torch.tensor(conts, dtype=torch.float)\nconts","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = torch.tensor(df[y_col].values, dtype=torch.float).reshape(-1, 1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cats_size = [len(df[col].cat.categories) for col in cat_cols]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_size = [(size, min(50, (size+1)//2)) for size in cats_size]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"catz = cats[:2]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"selfembeds = nn.ModuleList([nn.Embedding(ni, nf) for ni, nf in embedding_size])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# forward\nembedding_z = []\n\nfor i, e in enumerate(selfembeds):\n    embedding_z.append(e(catz[:,i]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"z = torch.cat(embedding_z, 1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"selfembeddingdrop = nn.Dropout(0.4)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"z = selfembeddingdrop(z)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TabularModel(nn.Module):\n    \n#     u can define the number of layers in this manner of build - flexibility\n    def __init__(self, emb_size, n_cont, out_size, layers, p=0.5):\n        \n        super().__init__()\n        self.embeds = nn.ModuleList([nn.Embedding(ni, nf) for ni, nf in embedding_size])\n        self.emb_drop = nn.Dropout(p)\n        self.bn_cont = nn.BatchNorm1d(n_cont)\n        \n        layer_list = []\n        n_emb = sum([nf for ni, nf in emb_size])\n        n_in = n_emb + n_cont\n        \n        for i  in layers:\n            layer_list.append(nn.Linear(n_in, i))\n            layer_list.append(nn.ReLU(inplace=True))\n            layer_list.append(nn.BatchNorm1d(i))\n            layer_list.append(nn.Dropout(p))\n            n_in = i\n            \n        layer_list.append(nn.Linear(layers[-1], out_size))\n        self.layers = nn.Sequential(*layer_list)\n    \n    def forward(self, x_cat, x_cont):\n        embeddings = []\n        \n        for i, e in enumerate(self.embeds):\n            embeddings.append(e(x_cat[:,i]))\n        \n        x = torch.cat(embeddings, 1)\n        x = self.emb_drop(x)\n        \n        x_cont = self.bn_cont(x_cont)\n        x = torch.cat([x, x_cont], 1)\n        x = self.layers(x)\n        return x\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.manual_seed(33)\nmodel = TabularModel(embedding_size, conts.shape[1], 1, [200, 100], p=0.4)\n# for classification problem, use class size 2 instead of 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion = nn.MSELoss()\n# for classification problem: use nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_test_split\nbatch_size = 60000\ntest_size = int(batch_size*0.2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_train = cats[:batch_size-test_size] \ncat_test = cats[batch_size - test_size:batch_size]\n\ncon_train = conts[:batch_size-test_size]\ncon_test = conts[batch_size - test_size:batch_size]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = y[:batch_size-test_size]\ny_test = y[batch_size - test_size:batch_size]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nstart_time = time.time()\n\nepochs = 250\n\nlosses = []\n\nfor i in range(epochs):\n    i+=1\n    \n    y_pred = model(cat_train, con_train)\n    loss = torch.sqrt(criterion(y_pred, y_train))\n    losses.append(loss)\n    if i%25 == 1:\n        print(f\"epoch:{i} loss: {loss}\")\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\nduration = time.time() - start_time\nprint(f\"training time: {duration/60}min\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\nplt.plot(losses)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with torch.no_grad():\n    y_val = model(cat_test, con_test)\n    loss = torch.sqrt(criterion(y_val,y_test))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(10):\n    diff = np.abs(y_val[i].item()-y_test[i].item())\n    print(f\"{i}predicted {y_val[i].item():8.2f} True:{y_test[i].item():8.2f} DIFF: {diff:8.2f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict(), 'taxi_model_kaggle_pytorch.pt')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntest = pd.read_csv(r'../input/titanic/test.csv')\nresult = pd.read_csv(r'../input/titanic/gender_submission.csv')\n\nX_test = test.drop(columns=['PassengerId','Name','Ticket'])\nX_test['Cabin'] = X_test.Cabin.fillna('NA')\nX_test['Cabin'] = X_test.Cabin.apply(lambda x : 'NA' if x == 'No' else 'Yes')\n\ny_test = result['Survived'].copy()\n\nX_test_prepared = full_pipeline.transform(X_test)\nfinal_predictions = final_model.predict(X_test_prepared)\n\nfinal_mse = mean_squared_error(y_test, final_predictions)\nfinal_rmse = np.sqrt(final_mse)\nfinal_rmse\n0.30487833502771516\npredictions = np.abs(np.around(final_predictions))\npredictions = predictions.astype(int)\npassenger_id = list(test['PassengerId'])\nprediction_submission = list(zip(passenger_id,predictions))\nprediction_submission = pd.DataFrame(prediction_submission, columns = ('PassengerId','Survived'))\nprediction_submission\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}