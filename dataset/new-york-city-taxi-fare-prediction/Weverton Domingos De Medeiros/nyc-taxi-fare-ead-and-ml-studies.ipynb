{"cells":[{"metadata":{},"cell_type":"markdown","source":"Weverton Domingos de Medeiros - weverton.medeiros@ee.ufcg.edu.br\n![](https://www.peoplesworld.org/wp-content/uploads/2017/08/taxisinnewyork960.jpg)"},{"metadata":{},"cell_type":"markdown","source":"In this project is proposed the use all my Machine Learning techniques and data hadling skills. For this, it is necessary to apply pre-processing techniques and feature engineering in the dataset to have a good model performance and then submit for the competition.\n\n# Dataset description\n\n### Name: \n* New York City Taxi Fare Prediction\n\n### File descriptions:\n* train.csv - Input features and target fare_amount values for the training set (about 55M rows).\n* test.csv - Input features for the test set (about 10K rows). Your goal is to predict fare_amount for each row.\n* sample_submission.csv - a sample submission file in the correct format (columns key and fare_amount). This file 'predicts' fare_amount to be $11.35 for all rows, which is the mean fare_amount from the training set."},{"metadata":{},"cell_type":"markdown","source":"### Data Field\n#### ID\n* key - Unique string identifying each row in both the training and test sets. Comprised of pickup_datetime plus a unique integer, but this doesn't matter, it should just be used as a unique ID field. Required in your submission CSV. Not necessarily needed in the training set, but could be useful to simulate a 'submission file' while doing cross-validation within the training set.\n\n#### Features\n* pickup_datetime - timestamp value indicating when the taxi ride started.\n* pickup_longitude - float for longitude coordinate of where the taxi ride started.\n* pickup_latitude - float for latitude coordinate of where the taxi ride started.\n* dropoff_longitude - float for longitude coordinate of where the taxi ride ended.\n* dropoff_latitude - float for latitude coordinate of where the taxi ride ended.\n* passenger_count - integer indicating the number of passengers in the taxi ride.\n\n#### Target\n* fare_amount - float dollar amount of the cost of the taxi ride. This value is only in the training set; this is what you are predicting in the test set and it is required in your submission CSV.\n\n#### Support - This analysis was made with support on the next notebook: https://www.kaggle.com/breemen/nyc-taxi-fare-data-exploration"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Libraries applied to the development of the EDA, preprocessing and model evaluation."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# from matplotlib.gridspec import GridSpec\nfrom sklearn.model_selection import train_test_split\n# from sklearn.compose import ColumnTransformer\n# from sklearn.pipeline import Pipeline\n# from sklearn.impute import SimpleImputer\n# from sklearn.preprocessing import OneHotEncoder\n# from sklearn.ensemble import RandomForestRegressor\n# from sklearn.ensemble import RandomForestClassifier\n# from sklearn.ensemble import GradientBoostingClassifier\n# from sklearn.model_selection import cross_val_score\n# import matplotlib.pyplot as plt\nimport lightgbm as lgbm\nfrom sklearn import metrics\n# from sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nimport category_encoders as ce\n# import plotly.express as px\nimport itertools\n# import time\nimport math\nimport datetime","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Functions developed for the project in order to facilitate the feature engineering and pre-processing process.\n\n* **remove_geog_outliers(row), remove_passenger_outliers(row), remove_fare_outliers(row):** functions that return the updated values without outliers.\n\n* **haversine_dist(long_pickup, long_dropoff, lat_pickup, lat_dropoff):** a function that calculate the haversine distance between two geolocation points.\n\n* **datetime_features(row):** function that helps to get the individual values from datetime feature, as *year*, *month*, *hour*, etc.\n\n* **rush_hour(row):** a function to define the rush hour. It was created in order to help the model to improve the predictions.\n\n* **is_weekend(row):** a function to define if the day is on weekend on dyring the week. It was created in order to help the model to improve the predictions.\n\n* **define_airport(row):** a function to define if the location is from an airport or not. It was created in order to help the model to improve the predictions.\n\nData collect from dataset."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"X = pd.read_csv('../input/new-york-city-taxi-fare-prediction/train.csv', nrows = 5_000_000)\nX_test = pd.read_csv('../input/new-york-city-taxi-fare-prediction/test.csv')\n\nX.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Import and initial processing of data\nDataset overview about the characteristics of its variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"X.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Analyzing the output of the *describe()* method, it can be seen that some values present unrealistic values, like the minimum value of *fare_amount* feature, which is negative, as the maximum value of this same variable is really high. Using this approach, it is easy to find this outliers values in other features. Therefore, the outliers will be removed immediately at processing stage."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_correlation = X.corr()\nsns.heatmap(X_correlation, annot = True, cmap = 'viridis')\nplt.title('Correlation among features')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From this correlation map there is no conclusion it can be made about the relation among *fare_amount* and other *features*, since these are not strongly correlated. But what can be seen is some variables, besides *fare_amount*, are correlated, such as the variables *pickup_latitude* and *dropoff_latitude* and *pickup_longitude* and *dropoff_longitude*, what demonstrate that, after a good feature engineering the model can be improved in order to predict the *fare_amount* values with less error."},{"metadata":{},"cell_type":"markdown","source":"# Pre-processing and Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Size before removal of negative values: ', len(X))\n\nX = X[X.fare_amount >= 0]\nprint('Size after removal of negative values: ', len(X))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s = (X.dtypes == 'object')\nobject_cols = list(s[s].index)\n\nprint(\"Categorical variables:\")\nprint(object_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# minimum and maximum longitude test\nmin(X.pickup_longitude.min(), X.dropoff_longitude.min()), \\\nmax(X.pickup_longitude.max(), X.dropoff_longitude.max())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# minimum and maximum latitude test\nmin(X.pickup_latitude.min(), X.dropoff_latitude.min()), \\\nmax(X.pickup_latitude.max(), X.dropoff_latitude.max())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Just as described by the method *describe()* used above, some variables present outlier values, and some of those were just desmontrated on the cells above."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax1,ax2) = plt.subplots(figsize = [12,12], nrows = 1, ncols = 2)\nplt.subplots_adjust(left = 0, bottom = None, right = 1, top = 0.5, \n                    wspace = 0.2, hspace = None)\n\nplt.figure\n\nax1.boxplot(X['fare_amount'])\nax1.set_title('Fare amount')\nax1.set_ylabel('Fare amount',fontsize = 10)\n\nax2.boxplot(X['passenger_count'])\nax2.set_title('Passenger count')\nax2.set_ylabel('Passenger count',fontsize = 10)\n\nfig, (ax3,ax4) = plt.subplots(figsize = [12,12], nrows = 1, ncols = 2)\nplt.subplots_adjust(left = 0, bottom = None, right = 1, top = 0.5, \n                    wspace = 0.2, hspace = None)\n\nplt.figure\n\nax3.boxplot(X['pickup_longitude'])\nax3.set_title('Pickup longitude')\nax3.set_ylabel('Pickup longitude',fontsize = 10)\n\nax4.boxplot(X['pickup_latitude'])\nax4.set_title('Pickup latitude')\nax4.set_ylabel('Pickup latitude',fontsize = 10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the views above, one can see the importance of, in addition to treating missing data, also removing disproportionate and unrealistic data. Thus, knowing this, below will be done the removal of these values that can hinder the performance of the model.\n\nIt is known that the city New York is at latitude 40.730610 and longitude -73.935242."},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_geog_outliers(df):\n    return df[(df.pickup_latitude < 49.7) & (df.pickup_latitude > 31.77) &\n             (df.dropoff_latitude < 49.7) & (df.dropoff_latitude > 31.77) &\n             (df.pickup_longitude > -77) & (df.pickup_longitude < -71) &\n             (df.dropoff_longitude > -77) & (df.dropoff_longitude < -71)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"init_amount, _ = X.shape\nprint('Removing outliers from geographic data features')\nX = remove_geog_outliers(X)\nfinal_amount, _ = X.shape\nprint('Amount of data removed in (%): {:.03f}%'.format((1 - final_amount/init_amount)*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_fare_outliers(df):\n    return df[(df.fare_amount >= 2.5) & (df.fare_amount <= 250)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"init_amount, _ = X.shape\nprint('Removing outliers from fare amount feature')\nX = remove_fare_outliers(X)\nfinal_amount, _ = X.shape\nprint('Amount of data removed in (%): {:.03f}%'.format((1 - final_amount/init_amount)*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_passenger_outliers(df):\n    return df[(df.passenger_count > 0) & (df.passenger_count <= 7)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"init_amount, _ = X.shape\nprint('Removing outliers from passenger count feature')\nX = remove_passenger_outliers(X)\nfinal_amount, _ = X.shape\nprint('Amount of data removed in (%): {:.03f}%'.format((1 - final_amount/init_amount)*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The distance traveled and the time taken by the Taxi to take the passenger to the destination is also one of the reasons for the increase in the fare amount, but this information is not described in the data set, thus, through a feature engineering treatment, by adding new variables, the relationship between the route and its final value can become clearer."},{"metadata":{"trusted":true},"cell_type":"code","source":"def haversine_dist(long_pickup, long_dropoff, lat_pickup, lat_dropoff):\n    \n    distance = []\n    \n    for i in range(len(long_pickup)):\n        long1, long2, lat1, lat2 = map(math.radians, \n                                       (long_pickup[i], long_dropoff[i], \n                                        lat_pickup[i], lat_dropoff[i]))\n        dlat = (lat2 - lat1)\n        dlong = (long2 - long1)    \n        a = math.sin(dlat/2)**2 + math.cos(lat1) * math.cos(lat2) * (math.sin(dlong/2)**2)\n\n        distance.append(2 * math.asin(math.sqrt(a)) * 6371)\n\n    return distance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X['dist_km'] = haversine_dist(X['pickup_longitude'].to_numpy(),X['dropoff_longitude'].to_numpy(),X['pickup_latitude'].to_numpy(),X['dropoff_latitude'].to_numpy())\nX_test['dist_km'] = haversine_dist(X_test['pickup_longitude'].to_numpy(),X_test['dropoff_longitude'].to_numpy(),X_test['pickup_latitude'].to_numpy(),X_test['dropoff_latitude'].to_numpy())\n\nX = X[X.dist_km <=130]\nX_test = X_test[X_test.dist_km <= 130]\n\nX.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To understand how the fare amount change according with the timeof the day and the day of the week the function *datetime_features* was implemented below."},{"metadata":{"trusted":true},"cell_type":"code","source":"def datetime_features(df):\n    df['date'] = df['pickup_datetime'].str.replace('UTC', '')\n    df['date'] = pd.to_datetime(df['date'], format = '%Y-%m-%d %H:%M:%S')\n    df['hour_of_day'] = df.date.dt.hour\n    df['week'] = df.date.dt.week\n    df['day_of_week'] = df.date.dt.weekday\n    df['month'] = df.date.dt.month\n    df['year'] = df.date.dt.year\n    \n    df.drop('date', inplace = True, axis = 1)\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = datetime_features(X)\nX_test = datetime_features(X_test)\n\nX.drop(columns = ['key', 'pickup_datetime'], axis = 1, \n       inplace = True, errors = 'ignore')\nX_test.drop(columns = ['key', 'pickup_datetime'], axis = 1, \n            inplace = True, errors = 'ignore')\n\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rush_time = X.groupby(['hour_of_day']).size()\n\nprint('Fare by hour of the day:')\nprint(rush_time)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is noticed, therefore, that there are two hour intervals during the day that stand out due to the continuous increase in the number of runs: 06-09am and 04-08pm. That is the reason for the creation of the feature *rush_hour*. For the same reason, try to find new relations with the *fare_amount* feature, the features *is_weekend* and *airport* were created."},{"metadata":{"trusted":true},"cell_type":"code","source":"def rush_hour(row):\n    if ((row.hour_of_day >= 6 and row.hour_of_day <= 9) or \n    (row.hour_of_day >= 16 and row.hour_of_day <= 20)):\n        return 1\n    else:\n        return 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X['rush_hour'] = X.apply(lambda row: rush_hour(row), axis = 1)\nX_test['rush_hour'] = X_test.apply(lambda row: rush_hour(row), axis = 1)\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def is_weekend(row):\n    if (row.day_of_week == 5 or row.day_of_week == 6):\n        return 1\n    else:\n        return 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X['is_weekend'] = X.apply(lambda row: is_weekend(row), axis = 1)\nX_test['is_weekend'] = X_test.apply(lambda row: is_weekend(row), axis = 1)\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def define_airport(row):\n    if ((row.pickup_latitude >= 40.63 and row.pickup_latitude <= 40.68)\n       and (row.pickup_longitude >= -73.79 and row.pickup_longitude <= -73.75) \n       or (row.dropoff_latitude >= 40.63 and row.dropoff_latitude <= 40.68)\n       and (row.dropoff_longitude >= -73.79 and row.dropoff_longitude <= -73.75)):\n        return 1 #JFK airport\n    \n    elif ((row.pickup_latitude >= 40.76 and row.pickup_latitude <= 40.79)\n       and (row.pickup_longitude >= -73.89 and row.pickup_longitude <= -73.85)\n         or (row.dropoff_latitude >= 40.76 and row.dropoff_latitude <= 40.79)\n       and (row.dropoff_longitude >= -73.89 and row.dropoff_longitude <= -73.85)):\n        return 2 #LGA airport\n    else:\n        return 0 #None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X['airport'] = X.apply(lambda row: define_airport(row), axis = 1)\nX_test['airport'] = X_test.apply(lambda row: define_airport(row), axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis (EDA)\nDataset overview about the characteristics of its variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,8))\nsns.set_style(\"whitegrid\")\nplt.title('Distance distribution (in km)')\nplt.xlabel('Distance (km)')\nplt.xticks(range(0,400,5))\n\nsns.kdeplot(X[X.dist_km < 200].dist_km, shade=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As can be seen above, most part of the trips are between aproximately 5 km to 25 km. All the taxi trips that have their total distance (in km) too long may be outliers values and were removed to make the feature *dist_km* more correlated to the target variable. The distance of the trips are quite reasonable since from the centre New York city extends in a radius around 130 km."},{"metadata":{"trusted":true},"cell_type":"code","source":"X.pivot_table('fare_amount', index='hour_of_day', columns='year').plot(figsize = (14,6))\nplt.ylabel('Fare amount $USD');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the view above it can be see that over the years the taxi fare mean grew and it can be justified by the increase in the number of vehicles circulating on the streets, which leads to an increase in the time the taxi remains on the street during the trip."},{"metadata":{"trusted":true},"cell_type":"code","source":"hour_of_day = X.groupby('hour_of_day').fare_amount.agg(['mean'])\nday_of_week = X.groupby('day_of_week').fare_amount.agg(['mean'])\nmonth = X.groupby('month').fare_amount.agg(['mean'])\n\n\nfig, (ax1,ax2, ax3) = plt.subplots(figsize = [14,12], nrows = 3, ncols = 1)\n\nax1.plot(hour_of_day, 'b')\nax2.plot(day_of_week,'g')\nax3.plot(month, 'r')\n\nax1.set_title('Fare by hour', fontsize = 18)\nax2.set_title('Fare by day', fontsize = 18)\nax3.set_title('Fare by month', fontsize = 18)\n\n# plt.style.use('seaborn')\nsns.set_style(\"white\")\n# plt.grid()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The hour of the day with highest fare amount is in the beggining of the day. Maybe because of traffic and people moving to their jobs/classes/chores. During the week the values of fare amount remain close but from Saturday to Sunday there is a notable grow, which can be explained because usually people get out with alone or with their family/friends to enjoy touristic places and others, so there is a high demand and maybe and increase on the traffics and the chosen of specific places."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (12,6))\nplt.title('Fare taxis during the weekend (or not)')\nsns.set_style(\"white\")\nsns.countplot(x = 'day_of_week', hue = 'is_weekend', data = X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(figsize = [14,10], nrows = 2, ncols = 1)\n\n\nsns.set_style(\"white\")\nsns.countplot(x = 'hour_of_day', data = X[X.day_of_week <= 4], ax = ax1, palette = 'viridis')\nax1.set_title('Number of taxi trips by the hour during the week')\n\nsns.set_style(\"white\")\nsns.countplot(x = 'hour_of_day', data = X[X.day_of_week >= 5], ax = ax2, palette = 'viridis')\nax2.set_title('Number of taxi trips by the hour during the weekend')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is a slight difference on the number of taxi trips when compering the days during the week and the weekend. People used to get out to parties and events during the weekend, what explain the high number of taxi trips during dawn, what is the opposite in what happens during the week days, that this number is low during dawn and higher at convetional hours."},{"metadata":{"trusted":true},"cell_type":"code","source":"X[X.fare_amount < 150].fare_amount.hist(bins = 100, figsize=(14,5))\nplt.xlabel('Fare amount $USD')\nplt.title('Histogram')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (14,10))\n\n# zoom in on part of data\nidx = (X.dist_km <= 30) & (X.fare_amount <= 150)\nplt.scatter(X[idx].dist_km, X[idx].fare_amount, alpha = 0.2)\nplt.xlabel('distance km')\nplt.ylabel('fare amount $USD')\nplt.title('Zoom in on distance <= 30 km, fare amount <= $150');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As there are a few fixed values and greater than the values that are in the distribution between $ USD 0 ~ 30, it can be said that these are fixed values for trips to selected places. The horizontal lines in the right plot might indicate again the fixed fare trips to/from an airport or other place that can indicate a fixed value for the taxi fare."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_correlation2 = X.corr()\nplt.figure(figsize = (18,18))\nsns.heatmap(X_correlation2, annot = True, cmap = 'viridis')\nplt.title('Correlation among features after feature engineering')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Due to the pre-processing and feature engineering work, now there are lots of features that are correlated with *fare_amount* feature, which leads the model to a better performance and lower error. Still, it can be seen that other some features, even if are not well correlated with *fare_amount* started to be correlated with features that are well correlated with the target."},{"metadata":{},"cell_type":"markdown","source":"# Model\n\n### Light GBM Classifier\n\nLightGBM is a gradient boosting framework that uses tree based learning algorithm. In this model is used leaf-wise tree growth technique. For a small number of nodes, leaf-wise will probably out-perform level-wise, and another great advantage is its high speed.\n\nInitially the param was defined and all the parameters were defined as suggested. It was made a initial study to see a good n_estimators that returned the best evaluation metrics."},{"metadata":{"trusted":true},"cell_type":"code","source":"y = X['fare_amount']\nX.drop(columns = ['fare_amount'], axis = 1, \n       inplace = True, errors = 'ignore')\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size = 0.8, \n                                                           test_size = 0.2, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n        'boosting_type':'gbdt',\n        'objective': 'regression',\n        'num_leaves': 38,\n        'learning_rate': 0.08,\n        'max_depth': -1,\n        'subsample': 0.8,\n        'bagging_fraction' : 1,\n        'max_bin' : 5000 ,\n        'bagging_freq': 20,\n        'colsample_bytree': 0.6,\n        'metric': 'rmse',\n        'min_split_gain': 0.5,\n        'min_child_weight': 1,\n        'min_child_samples': 10,\n        'scale_pos_weight': 1,\n        'zero_as_missing': True,\n        'seed': 0,\n        'num_rounds': 500\n    }\n\n\nmodel_train = lgbm.Dataset(X_train, y_train)\nmodel_test = lgbm.Dataset(X_valid, y_valid)\n\nmodel = lgbm.train(params = params, train_set = model_train, num_boost_round = 1000, \n                   early_stopping_rounds = 250, verbose_eval = 100, \n                   valid_sets = model_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction = model.predict(X_valid)\nrmse = math.sqrt(mean_squared_error(y_valid, prediction))\n\nprint(rmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm.plot_importance(model)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_prediction = model.predict(X_test)\n\nsubmission = pd.read_csv('../input/new-york-city-taxi-fare-prediction/sample_submission.csv')\nsubmission['fare_amount'] = y_prediction\n\nsubmission.to_csv('submission_taxifare_lgbm.csv', index = False)\nsubmission.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusions\n\n**As a result of previous activities and analyzes, the LGBM model was chosen due to the better performance in the evaluation metrics for previous tabular data and is faster than other Boosting techniques, as XGBoost model. The LGBM model design and submission of the test dataset achieved a RMSE performance of 3.61, being reaching a XXX place on leaderboard with score 3.12518. The competition has been closed, but from the leaderboard now I'd reach the position 372.**\n\nAs it was seen on EDA, some features were, even no having a strong correlation with the target, were used to create new features in order to improve the model performance. In this sense, some variable even after processing, didn't add any relevant information.\n\nMaybe a better pre-processing technique could make the result better, such as instead of using the harvesine distance, calculate the real distance through the geolocalization points obeying the streets, to know how long the taxi trip lasted, if this information was given on the dataset, or even can to say for sure the time of the trip and the distance from the traffic in a specific hour. *dist_km* stands as the main feature.\n\nAfter the feature engineering and processing stages, some variables had their correlation parameter improved, and so on the model performance."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}