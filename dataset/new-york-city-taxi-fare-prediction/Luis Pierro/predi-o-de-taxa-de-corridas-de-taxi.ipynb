{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom keras import regularizers, optimizers\nfrom keras.layers.normalization import BatchNormalization\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Analisando as especificações dos datasets de treinamento e teste, foi possível notar que eles contam com mais de 55 Milhões de Linhas. Com o intuito de reduzir o tamanho e melhorar o processamento do Kernel, vamos definir nosso conjunto de treinamento com menos linhas.\n\nAlém disso, vamos verificar quais são os tipos de features que encontramos nesse data set. "},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df =  pd.read_csv('../input/new-york-city-taxi-fare-prediction/train.csv', nrows = 1_000_000)\ntrain_df.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"É interessante observar quatro campos especificos:\n\n* pickup_longitude\n* pickup_latitude\n* dropoff_longitude\n* dropoff_latitude\n\nCom eles criaremos duas novas features traçarmos a rota da corrida do taxi."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def add_travel_vector_features(df):\n    df['abs_diff_longitude'] = (df.dropoff_longitude - df.pickup_longitude).abs()\n    df['abs_diff_latitude'] = (df.dropoff_latitude - df.pickup_latitude).abs()\n\nadd_travel_vector_features(train_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Outro ponto que devemos tomar cuidado ao trabalhar com os datasets é sobre os dados nulos."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_df.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Analisando nosso resultado, percebemos que em comparação com o todo, há apenas 69 registros nulos que representa uma porcentagem quase insignificante. Por isso, iremos apenas ignoraremos esses dados."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Tamanho antes: %d' % len(train_df))\ntrain_df = train_df.dropna(how = 'any', axis = 'rows')\nprint('Após exclusão: %d' % len(train_df))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Outro ponto interessante de se notar, é que podem existir casos em que as corridas não terminaram ou iniciaram em New York. E como podemos perceber isso? Bom, olhando para os graus, a diferença de 1 grau representa o percurso de 69 milhas. Por isso, ignoraremos dados em que a diferença seja maior que 5. \n\nOutro ponto que vamos ignorar no dataset são aqueles que começaram e terminaram no mesmo lugar.\n\nComo nossa feature de distância já está criada, vamos agora dropar as colunas extras de latitude e longitude.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Tamanho antes do tratamento: %d' % len(train_df))\ntrain_df = train_df[(train_df.abs_diff_longitude < 5.0) & (train_df.abs_diff_latitude < 5.0)]\ntrain_df = train_df[(train_df.abs_diff_longitude != 0) & (train_df.abs_diff_latitude != 0)]\ndropped_columns = ['pickup_longitude', 'pickup_latitude', \n                   'dropoff_longitude', 'dropoff_latitude']\ntrain = train_df.drop(dropped_columns, axis=1)\nprint('Tratamento após o tratamento: %d' % len(train))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Agora faremos um pouco de feature engineering e processamento dos dados.\n\nSerão criadas três features:\n    * Year, Month, Day, Hour, Weekday\n    * Night (between 16h and 20h, from monday to friday)\n    * Late night (between 20h and and 6h)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def late_night (row):\n    if (row['hour'] <= 6) or (row['hour'] >= 20):\n        return 1\n    else:\n        return 0\n\n\ndef night (row):\n    if ((row['hour'] <= 20) and (row['hour'] >= 16)) and (row['weekday'] < 5):\n        return 1\n    else:\n        return 0\n    \n    \ndef manhattan(pickup_lat, pickup_long, dropoff_lat, dropoff_long):\n    return np.abs(dropoff_lat - pickup_lat) + np.abs(dropoff_long - pickup_long)\n\n\ndef add_time_features(df):\n    df['pickup_datetime'] =  pd.to_datetime(df['pickup_datetime'], format='%Y-%m-%d %H:%M:%S %Z')\n    df['year'] = df['pickup_datetime'].apply(lambda x: x.year)\n    df['month'] = df['pickup_datetime'].apply(lambda x: x.month)\n    df['day'] = df['pickup_datetime'].apply(lambda x: x.day)\n    df['hour'] = df['pickup_datetime'].apply(lambda x: x.hour)\n    df['weekday'] = df['pickup_datetime'].apply(lambda x: x.weekday())\n    df['pickup_datetime'] =  df['pickup_datetime'].apply(lambda x: str(x))\n    df['night'] = df.apply (lambda x: night(x), axis=1)\n    df['late_night'] = df.apply (lambda x: late_night(x), axis=1)\n    # Drop 'pickup_datetime' as we won't need it anymore\n    df = df.drop('pickup_datetime', axis=1)\n    \n    return df\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = add_time_features(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Para continuar o trabalho, vamos droppar duas colunas que não vão nos ajudar muito daqui pra frente: Key e Passenger_Count "},{"metadata":{"trusted":true},"cell_type":"code","source":"dropped_columns2 = ['key', 'passenger_count']\ntrain = train.drop(dropped_columns2, axis=1)\ntrain.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Vamos Splitar o dataset de treino na proporção Treino 90% e Validação 10%."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df, validation_df = train_test_split(train, test_size=0.10, random_state=1)\n\n# Get labels\ntrain_labels = train_df['fare_amount'].values\nvalidation_labels = validation_df['fare_amount'].values\ntrain_df = train_df.drop(['fare_amount'], axis=1)\nvalidation_df = validation_df.drop(['fare_amount'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(train_df))\nprint(len(validation_df))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = preprocessing.MinMaxScaler()\ntrain_df_scaled = scaler.fit_transform(train_df)\nvalidation_df_scaled = scaler.transform(validation_df)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Com os nossos datasets de treino e validação prontos, partiremos para preparar o nosso dataset de test"},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('../input/new-york-city-taxi-fare-prediction/test.csv')\nprint(len(test))\n#vamos limpar um pouco do dataset\nadd_travel_vector_features(test)\ntest = test.drop(dropped_columns, axis=1)\n\ntest_aux = test.drop(['passenger_count'], axis=1)\ntest = test.drop(dropped_columns2, axis=1)\n\n# vamos agora adicionar a feature criada para datas\ntest = add_time_features(test)\n\ntest_df_scaled = scaler.transform(test)\n\ntest.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"O nosso modelo sera sequencial e os parametros serão:\n\n* Função de Ativação: ReLu\n* Otimização: Adam\n* Função de Perda: MSE\n* Epocas: 20\n* Learnin Rate: 0.1\n* Batch Size: 256\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model parameters\nBATCH_SIZE = 512\nEPOCHS = 20\nLEARNING_RATE = 0.001\nDATASET_SIZE = 1000000\n\nmodel = Sequential()\nmodel.add(Dense(256, activation='relu', input_dim=train_df_scaled.shape[1], activity_regularizer=regularizers.l1(0.01)))\nmodel.add(BatchNormalization())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(64, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(32, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(1))\n\nsgd = optimizers.SGD(lr=LEARNING_RATE, clipvalue=0.5)\n#adam = optimizers.adam(lr=LEARNING_RATE)\nmodel.compile(loss='mse', optimizer=sgd, metrics=['mae'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(x=train_df_scaled, y=train_labels, batch_size=BATCH_SIZE, epochs=EPOCHS, \n                    verbose=1, validation_data=(validation_df_scaled, validation_labels), \n                    shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction = model.predict(test_df_scaled, batch_size=128, verbose=1)\n\nprint(prediction)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}