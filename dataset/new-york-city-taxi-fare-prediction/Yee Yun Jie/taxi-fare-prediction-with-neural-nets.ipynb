{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# New York Taxi Fare ","metadata":{}},{"cell_type":"markdown","source":"## Executive Summary","metadata":{}},{"cell_type":"markdown","source":"Always wondered about the costs of transportation when planning your budget? This model aims to provide accurate estimations of transport expenditure when taking taxis in New York. In addition, it might provide a good gauge as to how much a taxi fare should be so as to not be fooled! Data was cleaned, analyzed using feature engineering and finally modelled. Specifically, we are leveraging on the\nqualities of ANN model to help us predict the **fare_amount** due to its comparatively lower MSE on both train and test set, which exhibits the best bias-variance tradeoff.","metadata":{}},{"cell_type":"markdown","source":"### Import libraries","metadata":{}},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom keras.models import Sequential\nfrom keras.layers import Dense,LSTM, TimeDistributed, Flatten, MaxPooling1D,Conv1D,Dropout\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso,ElasticNet,HuberRegressor,PassiveAggressiveRegressor,SGDRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.tree import DecisionTreeRegressor,ExtraTreeRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import AdaBoostRegressor,BaggingRegressor,RandomForestRegressor,ExtraTreesRegressor,GradientBoostingRegressor\nfrom sklearn.model_selection import train_test_split,GridSearchCV,RandomizedSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom math import radians, cos, sin, asin, sqrt\npd.set_option('display.float_format', lambda x: '%.3f' % x)\n\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Visualization","metadata":{}},{"cell_type":"code","source":"# loading train data\n# will only be including 1,000,000 rows in this notebook due to size constraints\ndf = pd.read_csv('/kaggle/input/new-york-city-taxi-fare-prediction/train.csv',nrows = 1000000)\n\n# loading test data\ntest_df = pd.read_csv('/kaggle/input/new-york-city-taxi-fare-prediction/test.csv')\n\n# loading sample submissions\nsample = pd.read_csv('/kaggle/input/new-york-city-taxi-fare-prediction/sample_submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# number of rows and columns\nprint(f'Number of records: {df.shape[0]}')\nprint(f'Number of columns: {df.shape[1]}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data types\ndf.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data dictionary","metadata":{}},{"cell_type":"markdown","source":"|Feature|Type|Dataset|Description|\n|:---|:---|:---|:---|\n|`key`|object|train/test|Unique ID field - pickup_datetime + unique integer|\n|`fare_amount`|float|train|Cost of taxi fare|\n|`pickup_datetime`|object|train/test|Date and time of pick up|\n|`pickup_longitude`|float|train/test|Longitude coordinate of where taxi ride started|\n|`pickup_latitutde`|float|train/test|Latitude coordinate of where taxi ride started|\n|`dropoff_longitude`|float|train/test|Longitude coordinate of where taxi ride ended|\n|`dropoff_latitude`|float|train/test|Latitude coordinate of where taxi ride ended|\n|`passenger_count`|float|train/test|Indicating number of passengers in the taxi|","metadata":{}},{"cell_type":"code","source":"# Statistical summary of data\ndf.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1) Negative fare amounts? Max fare amount over $1000?! <br>\n2) 0 passengers yet its in the records?","metadata":{}},{"cell_type":"markdown","source":"### Exploratory Data Analysis \nCleaning dataset","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 69 rows with at least 1 null value\ndf[df.isnull().any(1)]","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# columns with null values\ndf.columns[df.isnull().any()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# drop these record since they are impt in determining prices\ndf1 = df[~df.isnull().any(1)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Dropping unrealistic longitudes/latitudes\n1) Longitudes should be negative and latitudes should be positive","metadata":{}},{"cell_type":"code","source":"# swap these values\nincorrect_location = df1[((df1['dropoff_latitude'] < 0) | (df1['pickup_latitude'] < 0)) & ((df1['dropoff_longitude'] > 0) | (df1['pickup_longitude'] > 0))]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# swap columns\nincorrect_location.columns = ['key','fare_amount',\"pickup_datetime\",\"pickup_latitude\",\"pickup_longitude\",\n                              \"dropoff_latitude\",\"dropoff_longitude\",\"passenger_count\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# merge these values back into original df\ndf1.loc[df1.index.isin(incorrect_location.index),[\"pickup_latitude\",\"pickup_longitude\",\"dropoff_latitude\",\"dropoff_longitude\"]] = incorrect_location[[\"pickup_latitude\",\"pickup_longitude\",\"dropoff_latitude\",\"dropoff_longitude\"]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remaining odd coordinates, drop them\ndf1[((df1['dropoff_latitude'] < 0) | (df1['pickup_latitude'] < 0)) & ((df1['dropoff_longitude'] > 0) | (df1['pickup_longitude'] > 0))]","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# drop the remaining 77 rows\ntodrop = df1[((df1['dropoff_latitude'] < 0) | (df1['pickup_latitude'] < 0)) & ((df1['dropoff_longitude'] > 0) | (df1['pickup_longitude'] > 0))]\ndf1 = df1[~df1.index.isin(todrop.index)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Dropping unrealistic longitudes/latitudes\n2) Coordinates should fall within USA","metadata":{}},{"cell_type":"code","source":"# dropping these records that sit in the ATLANTIC OCEAN:\n# train:\ndf1 = df1.drop(df1[(df1['dropoff_latitude'] == 0) & (df1['dropoff_longitude'] == 0) & (df1['pickup_latitude'] == 0) & (df1['pickup_longitude'] ==0)].index)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since range of longitudes and latitudes for cities in USA is between -125 & -67 and 24 & 50 respectively, remove all the other records that fall outside of these ranges","metadata":{}},{"cell_type":"code","source":"df1.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# drop these records: Odd latitudes\ndf1[((df1[\"pickup_latitude\"] < 24) & (df1[\"pickup_latitude\"] > 50)) | (df1[\"dropoff_latitude\"]) < 24 & (df1[\"dropoff_latitude\"] > 50)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# drop these records: Odd longitudes\ndf1.loc[((df1[\"pickup_longitude\"] < -125) & (df1[\"pickup_longitude\"]  > -67)) | (df1[\"dropoff_longitude\"])  < -125 & (df1[\"dropoff_longitude\"] > -67)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"todrop = df1.loc[((df1[\"pickup_longitude\"] < -125) & (df1[\"pickup_longitude\"]  > -67)) | (df1[\"dropoff_longitude\"])  < -125 & (df1[\"dropoff_longitude\"] > -67)]\ndf1 = df1[~df1.index.isin(todrop.index)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Drop records that fall outside of `test_df`'s coordinates","metadata":{}},{"cell_type":"code","source":"# looking at range of pickup latitude and longitude in test set\nfig,ax = plt.subplots(2,figsize = (12,8))\nsns.boxplot(test_df['pickup_latitude'],ax = ax[0])\nsns.boxplot(test_df['pickup_longitude'],ax = ax[1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1 = df1[((df1['pickup_longitude'] > -75) & (df1['pickup_longitude'] < -72)) & ((df1['pickup_latitude'] > 40) & (df1['pickup_latitude'] < 42)) & ((df1['dropoff_longitude'] > -75) & (df1['dropoff_longitude'] < -72)) & ((df1['dropoff_latitude'] > 40) & (df1['dropoff_latitude'] < 42))]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# looking at range of pickup latitude and longitude in test set\nfig,ax = plt.subplots(2,figsize = (12,8))\nsns.boxplot(df1['pickup_latitude'],ax = ax[0])\nsns.boxplot(df1['pickup_longitude'],ax = ax[1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Drop unrealistic cab fares\nDrop negative cab fares","metadata":{}},{"cell_type":"code","source":"df1 = df1.drop(df1[df1['fare_amount'] <= 0].index)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Drop unrealistic passenger count","metadata":{}},{"cell_type":"code","source":"fig,ax = plt.subplots(figsize = (12,8))\nsns.boxplot(df1['passenger_count'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1[df1['passenger_count'] >50]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# drop the 2 extreme values\ndf1 = df1.drop(df1[df1['passenger_count'] > 50].index)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Drop unrealitstic fare amount","metadata":{}},{"cell_type":"code","source":"fig,ax = plt.subplots(figsize = (12,8))\nsns.boxplot(df1['fare_amount'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assumption: cab fares are all below $200\ndf1 = df1.drop(df1[df1['fare_amount'] > 200].index)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature Engineering\nChanging datatypes and creating new fields","metadata":{}},{"cell_type":"code","source":"pd.to_datetime(pd.to_datetime(df1.head()['pickup_datetime']).dt.strftime(\"%Y-%m-%d %H:%M\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# changing date column to datetime(ns) \ndf1['pickup_datetime'] = pd.to_datetime(pd.to_datetime(df1['pickup_datetime']).dt.strftime(\"%Y-%m-%d %H:%M\"))\ntest_df['pickup_datetime'] = pd.to_datetime(pd.to_datetime(test_df['pickup_datetime']).dt.strftime(\"%Y-%m-%d %H:%M\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating separate fields for year, month, weekday and hour\n# train set:\ndf1['year'] = df1['pickup_datetime'].dt.year\ndf1['month'] = df1['pickup_datetime'].dt.month\ndf1['day'] = df1['pickup_datetime'].dt.day\ndf1['weekday'] = df1['pickup_datetime'].dt.weekday\ndf1['hour'] = df1['pickup_datetime'].dt.hour\ndf1['min'] = df1['pickup_datetime'].dt.minute\n\n# test set:\ntest_df['year'] = test_df['pickup_datetime'].dt.year\ntest_df['month'] = test_df['pickup_datetime'].dt.month\ntest_df['day'] = test_df['pickup_datetime'].dt.day\ntest_df['weekday'] = test_df['pickup_datetime'].dt.weekday\ntest_df['hour'] = test_df['pickup_datetime'].dt.hour\ntest_df['min'] = test_df['pickup_datetime'].dt.minute","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### `distance`\nConverting longitudes, latitudes into distance in km ","metadata":{}},{"cell_type":"code","source":"# define haversine formula to convert points to distance in km\ndef haversine(df2):\n    \"\"\"\n    Calculate the great circle distance between two points \n    on the earth (specified in decimal degrees)\n    \"\"\"\n    # convert decimal degrees to radians \n    lon1 = df2['pickup_longitude']\n    lon2 = df2['dropoff_longitude']\n    lat1 = df2['pickup_latitude']\n    lat2 = df2['dropoff_latitude']\n    \n    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n\n    # haversine formula \n    dlon = lon2 - lon1 \n    dlat = lat2 - lat1 \n    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n    c = 2 * asin(sqrt(a)) \n    r = 6371 # Radius of earth in kilometers.\n    return c * r","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# apply formula to get distance column\ndf1['distance'] = df1.apply(haversine,axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since `distance` is enough to encapsulate the relationship between distance travelled and cab fare, other dependent features eg longitude and latitude features can be dropped.","metadata":{}},{"cell_type":"code","source":"# Dropping correlated and redundant columns\ndf2 = df1.copy()\ndf2 = df2.drop(columns = ['pickup_latitude','pickup_longitude','dropoff_latitude','dropoff_longitude'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# doing the same for the test set\ntest_df['distance'] = test_df.apply(haversine,axis = 1)\n\n# Dropping correlated and redudant columns\ntest_df = test_df.drop(columns = ['pickup_latitude','pickup_longitude','dropoff_latitude','dropoff_longitude'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Visualization","metadata":{}},{"cell_type":"code","source":"# to visualize correlation betwen variables\nmask = np.triu(np.ones_like(df2.corr(),dtype = bool))\nfig,ax = plt.subplots(figsize = (12,8))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nsns.heatmap(df2.corr(),ax = ax,annot = True,cmap = cmap,mask = mask)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Spread of variables\nfig,ax = plt.subplots(2,figsize = (12,8))\nsns.violinplot(y = df2['fare_amount'],x = df2['year'],ax = ax[0])\nsns.violinplot(y = df2['fare_amount'],x = df2['month'],ax = ax[1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visualize number of trips\nfig,ax = plt.subplots(2,figsize = (12,8))\nsns.barplot(y = df2['fare_amount'],x = df2['year'],ax = ax[0],palette = 'Set2')\nsns.barplot(y = df2['fare_amount'],x = df2['month'],ax = ax[1],palette = 'Set2')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save cleaned data as a separate csv\n\n# df2.to_csv('../data/distanced_train.csv',index = False)\n# test_df.to_csv('../data/distanced_test.csv',index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df2 = pd.read_csv('../data/distanced_train.csv')\n# test_df = pd.read_csv('../data/distanced_test.csv')","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data modeling ","metadata":{}},{"cell_type":"code","source":"# Separating predictor variables and target variable\nX = df2.drop(columns = ['fare_amount','key','pickup_datetime'])\ny = df2['fare_amount']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train,X_test,y_train,y_test = train_test_split(X,y,random_state = 42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# looking at rows, columns for train and validation set\nprint(f'train: {X_train.shape}')\nprint(f'test: {y_train.shape}')\nprint(f'val train: {X_test.shape}')\nprint(f'val test: {y_test.shape}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### StandardScaler\nNormalize scales of features to improve accuracy of predictions especially if our variables are on different scales/magnitudes. This is because this would affect the performances of models that specifically rely on distance metrics(k-NN, PCA) as well as to speed up gradient descent convergence for deep neural networks during backpropagation. Mainly to ensure that every feature contributes equally to the models! ","metadata":{}},{"cell_type":"code","source":"X_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# scale data\nss = StandardScaler()\nss.fit(X_train)\nX_train_ss = ss.transform(X_train)\nX_test_ss = ss.transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_models(models=dict()):\n# linear models\n    models['lr'] = LinearRegression()\n    models['lasso'] = Lasso()\n    models['ridge'] = Ridge()\n    models['en'] = ElasticNet()\n    models['huber'] = HuberRegressor()\n    models['pa'] = PassiveAggressiveRegressor(max_iter=1000, tol=1e-3)\n   \n    return models\n\ndef get_models_nl(models=dict()):\n# non-linear models\n    models['svr'] = SVR()\n# ensemble models\n    n_trees = 100\n    models['ada'] = AdaBoostRegressor(n_estimators=n_trees)\n    models['bag'] = BaggingRegressor(n_estimators=n_trees)\n    models['rf'] = RandomForestRegressor(n_estimators=n_trees)\n    models['et'] = ExtraTreesRegressor(n_estimators=n_trees)\n    models['gbm'] = GradientBoostingRegressor(n_estimators=n_trees)\n    return models\n\ndef evaluate_models(models, X_train_ss,y_train,X_test_ss,y_test):\n    for name, model in models.items():\n    # fit models\n        model_fit = model.fit(X_train_ss,y_train)\n        # make predictions\n        train_preds = model_fit.predict(X_train_ss)\n        test_preds = model_fit.predict(X_test_ss)\n        # evaluate forecast\n        train_mse = mean_squared_error(y_train,train_preds)\n        test_mse = mean_squared_error(y_test,test_preds)\n        print(f'{name}:')\n        print(f'----')\n        print(f'Train MAE: {round(train_mse,2)}')\n        print(f'Test MAE: {round(test_mse,2)}')\n        print(f'\\n')\n        \ndef pipeline(model):\n    pipe = Pipeline([(model, model_dict[model])])\n    return pipe\n\ndef params(model):\n    \n\n    if model == 'lasso':\n        return {\"alpha\":[0.01,0.1,1,2,5,10],\n               }\n    \n    \n    elif model == 'ridge':\n        return {\n            \"alpha\":[0.01,0.1,1,2,5,10],\n            }\n    \n    elif model == 'en':\n        return {\n            'alpha':[0.01,0.1,1,10],\n            'l1_ratio':[0.2,0.3,0.4,0.5,0.6]\n            }\n    elif model == 'knn':\n        return {\n            'n_neighbors':[4,5,6,7]}\n\n    elif model == 'dt':\n        return {\n            'max_depth':[3,4,5],\n            'min_samples_split':[2,3,4],\n            'min_samples_leaf':[2,3,4]\n        }\n    elif model == 'bag':\n        return {\n            'max_features':[100, 150]\n        }\n        \n    elif model == 'rf':\n        return {\n            'n_estimators':[100,150],\n            'max_depth':[4],\n            'min_samples_leaf':[2,3,4]\n        }\n    elif model == 'et':\n        return {\n            'n_estimators':[50,100,150,200],\n            'max_depth':[1000,2000,3000],\n            'min_samples_leaf':[10000,20000,30000],\n        }\n    elif model == 'abc':\n        return {\n            'n_estimators':[50,100,150,200],\n            'learning_rate':[0.3,0.6,1]\n        }\n    elif model == 'gbc':\n        return {\n            'learning_rate':[0.2],\n            'max_depth':[1000,2000,3000],\n            'min_samples_split':[10000,20000,30000]\n            \n        }\n    elif model == 'xgb':\n        return {\n            'eval_metric' : ['auc'],\n            'subsample' : [0.8], \n            'colsample_bytree' : [0.5], \n            'learning_rate' : [0.1],\n            'max_depth' : [5], \n            'scale_pos_weight': [5], \n            'n_estimators' : [100,200],\n            'reg_alpha' : [0, 0.05],\n            'reg_lambda' : [2,3],\n            'gamma' : [0.01]\n                             \n        }\n    elif model == 'svr':\n        return {\n            'kernel': ['rbf', 'linear','poly'], \n            'C': [1,20,50,100],\n            'gamma':['scale','auto'],\n            'epsilon':[0.1,1,10]\n        }\n    elif model == 'ada':\n        return {\n            'n_estimators':[50,100,150],\n            'learning_rate':[0.01,0.1,1],\n            \n        }\n    elif model == 'bag':\n        return {\n            'n_estimators':[20,50,100,150],\n            'max_features':[2,4,6],\n            'max_samples':[0.1,0.2,0.3,0.5,0.7],\n            'bootstrap':[True]\n            \n        }\n    elif model == 'rf':\n        return {\n             'bootstrap': [True],\n             'max_depth': [5,10,15],\n             'max_features': [\"auto\", \"sqrt\", \"log2\"],\n             'min_samples_leaf': [10000,20000,30000],\n             'min_samples_split': [10000,20000,30000],\n             'n_estimators': [50,200,300,400],\n             'random_state': 42,\n             }\n    elif model == 'et':\n        return {\n             'bootstrap': [True],\n             'max_depth': [5,10,15],\n             'max_features': [\"auto\", \"sqrt\", \"log2\"],\n             'min_samples_leaf': [10000,20000,30000],\n             'min_samples_split': [10000,20000,30000],\n             'n_estimators': [50,200,300,400],\n             'random_state': 42,\n        }\n            \n    elif model == 'gbm':\n        return {\n            'learning_rate' : [0.1,0.3,0.6,1], \n            'min_samples_split':[10000,20000,30000],\n            'min_samples_leaf': [10000,20000,30000],\n            'max_depth' : [8,10,20]\n       }\n\n\n\n# grid search with randomizedsearchcv\ndef grid_search_rs(model,models,X_train = X_train_ss,y_train = y_train,X_test = X_test_ss,y_test=y_test):\n    pipe_params = params(model)\n    model = models[model]\n    gs = RandomizedSearchCV(model,param_distributions = pipe_params,cv = 5,scoring = 'neg_mean_squared_error', verbose=True, n_jobs=8)\n    gs.fit(X_train_ss,y_train)\n    train_score = gs.score(X_train_ss,y_train)\n    test_score = gs.score(X_test_ss,y_test)\n    \n    print(f'Results from: {model}')\n    print(f'-----------------------------------')\n    print(f'Best Hyperparameters: {gs.best_params_}')\n    print(f'Mean MSE: {-round(gs.best_score_,4)}')\n    print(f'Train Score: {-round(train_score,4)}')\n    print(f'Test Score: {-round(test_score,4)}')\n    print(' ')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Linear Models","metadata":{}},{"cell_type":"code","source":"models = get_models()\nevaluate_models(models,X_train_ss,y_train,X_test_ss,y_test)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%time grid_search_rs(\"ridge\",models)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Neural Nets","metadata":{}},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Dense(64,activation = 'relu',kernel_initializer = 'normal',input_dim = X_train_ss.shape[1]))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(32,activation = 'relu'))\nmodel.add(Dense(1))\nmodel.compile(loss = 'mse',optimizer = 'adam',metrics = 'mae')\nhistory_model = model.fit(X_train_ss,y_train, epochs = 100, batch_size = 50000, validation_data = (X_test_ss,y_test),verbose = 2)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model1 = Sequential()\nmodel1.add(Dense(128,activation = 'relu',kernel_initializer = 'normal',input_dim = X_train_ss.shape[1]))\nmodel1.add(Dropout(0.3))\nmodel1.add(Dense(64,activation = 'relu'))\nmodel1.add(Dense(1))\nmodel1.compile(loss = 'mse',optimizer = 'adam',metrics = 'mean_squared_error')\nhistory_model1 = model1.fit(X_train_ss,y_train, epochs = 30, batch_size = 50000, validation_data = (X_test_ss,y_test),verbose = 2)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig,ax = plt.subplots(figsize = (20,10))\nax.plot(history_model.history['loss'],label = 'Train Loss')\nax.plot(history_model.history['val_loss'],label = 'Val Loss')\nax.plot(history_model1.history['loss'],label = 'Train Loss - More Layers')\nax.plot(history_model1.history['val_loss'],label = 'Val Loss - More Layers')\nplt.legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Evaluating ANN's(Vanilla) performance:\")\nprint('------')\nprint(f'Train Score:{mean_squared_error(y_train,model.predict(X_train_ss))}')\nprint(f'Train Score:{mean_squared_error(y_test,model.predict(X_test_ss))}')\nprint(f'\\n')\nprint(f\"Evaluating ANN's(extra layer) performance:\")\nprint('------')\nprint(f'Train Score: {mean_squared_error(y_train,model1.predict(X_train_ss))}')\nprint(f'Test Score: {mean_squared_error(y_test,model1.predict(X_test_ss))}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Final predictions\nFit the model with entire train set now","metadata":{}},{"cell_type":"code","source":"ss = StandardScaler()\nss.fit(X)\nX_ss = ss.transform(X)\ntest_df_ss = ss.transform(test_df.iloc[:,2:])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Shape of X: {X_ss.shape}')\nprint(f'Shape of y: {y.shape}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Linear Model","metadata":{}},{"cell_type":"code","source":"ridge = Ridge(alpha = 10)\nridge.fit(X_ss,y)\nridge_preds = pd.DataFrame({\"key\":test_df[\"key\"],\"fare_amount\":ridge.predict(test_df_ss)})\n# ridge_preds.to_csv('../submissions/my_submissions_ridge.csv',index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### ANN","metadata":{}},{"cell_type":"code","source":"ann_preds = pd.DataFrame({\"key\": test_df['key'], \"fare_amount\":finalmodel.predict(test_df_ss).flatten()})\nann_preds.to_csv(\"my_final_submission\", index=False)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"finalmodel = Sequential()\nfinalmodel.add(Dense(64,activation = 'relu',kernel_initializer = 'normal',input_dim = X_ss.shape[1]))\nfinalmodel.add(Dropout(0.3))\nfinalmodel.add(Dense(3,activation = 'relu'))\nfinalmodel.add(Dense(1))\nfinalmodel.compile(loss = 'mse',optimizer = 'adam',metrics = 'mae')\nhistory_finalmodel = finalmodel.fit(X_ss,y, epochs = 100, batch_size = 50000,verbose = 2)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ann_preds = pd.DataFrame({\"key\": test_df['key'], \"fare_amount\":finalmodel.predict(test_df_ss).flatten()})\nann_preds.to_csv(\"my_final_submission\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}