{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data= pd.read_csv('../input/train.csv',nrows=1000000,parse_dates=[\"pickup_datetime\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train= data.copy(deep=True)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Key is not useful for us so we should drop it."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop('key', axis=1, inplace=True)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. We see negative fare amounts which needs to be removed.\n2. Passenger count of 208 doesn't make sense for a taxi.\n3. If there are any null values then we should remove them as well."},{"metadata":{"trusted":true},"cell_type":"code","source":"#remove fare amounts less than 2\ntrain= train[train['fare_amount']>2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check for null values\ntrain.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#remove nulls\ntrain = train.dropna(how='any',axis=0) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets check again\ntrain.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets see the passenger count\ntrain.passenger_count.unique()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that 208 is an outlier so we should remove it."},{"metadata":{"trusted":true},"cell_type":"code","source":"train= train[train['passenger_count']<10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets see the passenger count\ntrain.passenger_count.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets see the distribution of fare amount."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nsns.distplot(train['fare_amount'])\nplt.title('Fare distribution')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. There are some outliers considering the longitude and latitude information.\n2. It can be verified that New York City, NY, USA Latitude and longitude coordinates are: 40.730610, -73.935242. \nhttps://www.latlong.net/place/new-york-city-ny-usa-1848.html\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.loc[train['pickup_latitude'].between(40, 42)]\ntrain = train.loc[train['pickup_longitude'].between(-75, -72)]\ntrain = train.loc[train['dropoff_latitude'].between(40, 45)]\ntrain = train.loc[train['dropoff_longitude'].between(-75, -72)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Initially we had 1 million rows\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. It seems we have less no of features so we can generate some derived attributes to improve accuracy.\n2. We can have the absolute difference between the latitude and longitude of pickup and dropoff locations.\n3. We can also have a L2 and L1 distance between the pickup and dropoff locations as features."},{"metadata":{"trusted":true},"cell_type":"code","source":"train['latitude_diff']= (train['pickup_latitude']-train['dropoff_latitude']).abs()\ntrain['longitude_diff']= (train['pickup_longitude']- train['dropoff_longitude']).abs()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets see how many of the rows have 0 absolute difference of latitude and longitude.\nX=train[(train['latitude_diff'] == 0) & (train['longitude_diff'] == 0)]\nX.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets add L2 and L1 distance as features in our data\ntrain['L2']=  ((train['dropoff_latitude']-train['pickup_latitude'])**2 +\n(train['dropoff_longitude']-train['pickup_longitude'])**2)**1/2\n\ntrain['L1']= ((train['dropoff_latitude']-train['pickup_latitude']) +\n(train['dropoff_longitude']-train['pickup_longitude'])).abs()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets see the correlation between features created\ncorr_mat = train.corr()\ncorr_mat.style.background_gradient(cmap='coolwarm')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can see latitude_diff, longitude_diff, L1 dist are correlated with fare_amount which we want to predict."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data= pd.read_csv('../input/test.csv',parse_dates=[\"pickup_datetime\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## We need to create the same features for Test data as well."},{"metadata":{"trusted":true},"cell_type":"code","source":"test= test_data.copy(deep=True)\ntest.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that the Test data doesn't have outliers and we just need to add the additional features."},{"metadata":{"trusted":true},"cell_type":"code","source":"test['latitude_diff']= (test['pickup_latitude']-test['dropoff_latitude']).abs()\ntest['longitude_diff']= (test['pickup_longitude']- test['dropoff_longitude']).abs()\n\ntest['L2']=  ((test['dropoff_latitude']-test['pickup_latitude'])**2 +\n(test['dropoff_longitude']-test['pickup_longitude'])**2)**1/2\n\ntest['L1']= ((test['dropoff_latitude']-test['pickup_latitude']) +\n(test['dropoff_longitude']-test['pickup_longitude'])).abs()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.describe()\n#train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training and Testing"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x = train.drop(['pickup_datetime','fare_amount'],axis=1)\ntrain_y = train['fare_amount'].values\n\ntest_x = test.drop(columns=['pickup_datetime','key'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x.head()\n#test_x.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\nLR = LinearRegression()\n\nLR.fit(train_x, train_y)\n\n#making predictions\nlr_prediction= LR.predict(test_x)\n\n\nsubmission = pd.read_csv('../input/sample_submission.csv')\nsubmission['fare_amount'] = lr_prediction\nsubmission.to_csv('submission_LR.csv', index=False)\nsubmission.head(20)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom sklearn.ensemble import RandomForestRegressor\nRF = RandomForestRegressor()\nRF.fit(train_x, train_y)\nRF_predict = RF.predict(test_x)\n\nsubmission = pd.read_csv('../input/sample_submission.csv')\nsubmission['fare_amount'] = RF_predict\nsubmission.to_csv('submission_RF.csv', index=False)\nsubmission.head(20)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Lets test LightGBM"},{"metadata":{"trusted":true},"cell_type":"code","source":"parameters = {\n        'boosting_type':'gbdt',\n        'objective': 'regression',\n        'nthread': -1,\n        'num_leaves': 25,\n        'learning_rate': 0.02,\n        'max_depth': -1,\n        'subsample': 0.8,\n        'subsample_freq': 1,\n        'colsample_bytree': 0.6,\n        'reg_aplha': 1,\n        'reg_lambda': 0.001,\n        'metric': 'rmse',\n        'min_split_gain': 0.5,\n        'min_child_weight': 1,\n        'min_child_samples': 10,\n        'scale_pos_weight':1,\n        'verbose':0\n    \n    }\n\n\nimport lightgbm as lgbm\n\ntrain_lgbm = lgbm.Dataset(train_x, train_y, silent=True)\n\nlgbm_model= lgbm.train(parameters, train_lgbm, num_boost_round=500)\n\nlgbm_prediction= lgbm_model.predict(test_x)\n\nsubmission = pd.read_csv('../input/sample_submission.csv')\nsubmission['fare_amount'] = lgbm_prediction\nsubmission.to_csv('submission_LGBM.csv', index=False)\nsubmission.head(20)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets try XGboost method"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport xgboost as xgb\n\nxgb_train = xgb.DMatrix(train_x, label=train_y)\nxgb_test = xgb.DMatrix(test_x)\n\n\nparams = {'max_depth':7,\n          'eta':1,\n          'silent':1,\n          'objective':'reg:linear',\n          'eval_metric':'rmse',\n          'learning_rate':0.05\n         }\n\nxgb_model= xgb.train(params, xgb_train,50 )\n\nxgb_prediction = xgb_model.predict(xgb_test)\n\nsubmission = pd.read_csv('../input/sample_submission.csv')\nsubmission['fare_amount'] = xgb_prediction\nsubmission.to_csv('submission_XGB.csv', index=False)\nsubmission.head(20)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Including time features\n\nI have ignored time features initially and getting RMSE of around 3.7  We can do more feature engineering and add time features."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x['year'] =train['pickup_datetime'].dt.year\ntrain_x['month'] = train['pickup_datetime'].dt.month\ntrain_x['day']=train['pickup_datetime'].dt.day\ntrain_x['day_of_week']=train['pickup_datetime'].dt.dayofweek\ntrain_x['hour']=pd.to_datetime(train['pickup_datetime'], format='%H:%M').dt.hour\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Doing same thing for test data\ntest_x['year'] =test['pickup_datetime'].dt.year\ntest_x['month'] = test['pickup_datetime'].dt.month\ntest_x['day']=test['pickup_datetime'].dt.day\ntest_x['day_of_week']=test['pickup_datetime'].dt.dayofweek\ntest_x['hour']=pd.to_datetime(test['pickup_datetime'], format='%H:%M').dt.hour","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_x.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets see the correlation between features created\ntrain_x['fare']= train['fare_amount']\ncorr_mat_new = train_x.corr()\ncorr_mat_new.style.background_gradient(cmap='coolwarm')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We don't need this row anymore\ntrain_x= train_x.drop(['fare'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# We need to run all the models on this new data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\nLR = LinearRegression()\n\nLR.fit(train_x, train_y)\n\n#making predictions\nlr_prediction= LR.predict(test_x)\n\n\nsubmission = pd.read_csv('../input/sample_submission.csv')\nsubmission['fare_amount'] = lr_prediction\nsubmission.to_csv('submission_LR_new.csv', index=False)\nsubmission.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nRF = RandomForestRegressor()\nRF.fit(train_x, train_y)\n\nRF_predict = RF.predict(test_x)\n\nsubmission = pd.read_csv('../input/sample_submission.csv')\nsubmission['fare_amount'] = RF_predict\nsubmission.to_csv('submission_RF_new.csv', index=False)\nsubmission.head(20)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parameters = {\n        'boosting_type':'gbdt',\n        'objective': 'regression',\n        'nthread': -1,\n        'num_leaves': 25,\n        'learning_rate': 0.02,\n        'max_depth': -1,\n        'subsample': 0.8,\n        'subsample_freq': 1,\n        'colsample_bytree': 0.6,\n        'reg_aplha': 1,\n        'reg_lambda': 0.001,\n        'metric': 'rmse',\n        'min_split_gain': 0.5,\n        'min_child_weight': 1,\n        'min_child_samples': 10,\n        'scale_pos_weight':1,\n        'verbose':0\n    \n    }\n\n\nimport lightgbm as lgbm\n\ntrain_lgbm = lgbm.Dataset(train_x, train_y, silent=True)\n\nlgbm_model= lgbm.train(parameters, train_lgbm, num_boost_round=500)\n\n#lgbm prediction\nlgbm_prediction= lgbm_model.predict(test_x)\n\nsubmission = pd.read_csv('../input/sample_submission.csv')\nsubmission['fare_amount'] = lgbm_prediction\nsubmission.to_csv('submission_LGBM_new.csv', index=False)\nsubmission.head(20)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\n\nxgb_train = xgb.DMatrix(train_x, label=train_y)\nxgb_test = xgb.DMatrix(test_x)\n\n\nparams = {'max_depth':7,\n          'eta':1,\n          'silent':1,\n          'objective':'reg:linear',\n          'eval_metric':'rmse',\n          'learning_rate':0.05\n         }\n\nxgb_model= xgb.train(params, xgb_train,50 )\n\nxgb_prediction = xgb_model.predict(xgb_test)\n\nsubmission = pd.read_csv('../input/sample_submission.csv')\nsubmission['fare_amount'] = xgb_prediction\nsubmission.to_csv('submission_XGB_new.csv', index=False)\nsubmission.head(20)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}