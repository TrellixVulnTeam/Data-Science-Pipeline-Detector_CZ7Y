{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Initial Python environment setup...\n# import numpy as np # linear algebra\n# import pandas as pd # CSV file I/O (e.g. pd.read_csv)\n# import os # reading the input files we have access to\n\n# print(os.listdir('../input'))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Read train data\ntaxi_train = pd.read_csv('../input/new-york-city-taxi-fare-prediction/train.csv', nrows = 10_000_000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"taxi_train.columns.to_list()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"taxi_train.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n# Plot a histogram\ntaxi_train.fare_amount.hist(bins=30, alpha=0.5)\nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Given a dataframe, add two new features 'abs_diff_longitude' and\n# 'abs_diff_latitude' reprensenting the \"Manhattan vector\" from\n# the pickup location to the dropoff location.\ndef add_travel_vector_features(df):\n    df['abs_diff_longitude'] = (df.dropoff_longitude - df.pickup_longitude).abs()\n    df['abs_diff_latitude'] = (df.dropoff_latitude - df.pickup_latitude).abs()\n\nadd_travel_vector_features(taxi_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(taxi_train.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Old size: %d' % len(taxi_train))\ntaxi_train = taxi_train.dropna(how = 'any', axis = 'rows')\nprint('New size: %d' % len(taxi_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot = taxi_train.iloc[:2000].plot.scatter('abs_diff_longitude', 'abs_diff_latitude')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Old size: %d' % len(taxi_train))\ntaxi_train = taxi_train[(taxi_train.abs_diff_longitude < 5.0) & (taxi_train.abs_diff_latitude < 5.0)]\nprint('New size: %d' % len(taxi_train))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train our model\nOur model will take the form  X⋅w=y  where  X  is a matrix of input features, and  y  is a column of the target variable, fare_amount, for each row. The weight column  w  is what we will \"learn\".\n\nFirst let's setup our input matrix  X  and target column  y  from our training set. The matrix  X  should consist of the two GPS coordinate differences, plus a third term of 1 to allow the model to learn a constant bias term. The column  y  should consist of the target fare_amount values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Construct and return an Nx3 input matrix for our linear model\n# using the travel vector, plus a 1.0 for a constant bias term.\ndef get_input_matrix(df):\n    return np.column_stack((df.abs_diff_longitude, df.abs_diff_latitude, np.ones(len(df))))\n\ntaxi_train_X = get_input_matrix(taxi_train)\ntaxi_train_y = np.array(taxi_train['fare_amount'])\n\nprint(taxi_train_X.shape)\nprint(taxi_train_y.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's use numpy's lstsq library function to find the optimal weight column  w .","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# The lstsq function returns several things, and we only care about the actual weight vector w.\n(w, _, _, _) = np.linalg.lstsq(taxi_train_X, taxi_train_y, rcond = None)\nprint(w)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These weights pass a quick sanity check, since we'd expect the first two values -- the weights for the absolute longitude and latitude differences -- to be positive, as more distance should imply a higher fare, and we'd expect the bias term to loosely represent the cost of a very short ride.\n\nSidenote: we can actually calculate the weight column  w  directly using the Ordinary Least Squares method:  w=(XT⋅X)−1⋅XT⋅y","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"w_OLS = np.matmul(np.matmul(np.linalg.inv(np.matmul(taxi_train_X.T, taxi_train_X)), taxi_train_X.T), taxi_train_y)\nprint(w_OLS)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Make predictions on the test set\nNow let's load up our test inputs and predict the fare_amounts for them using our learned weights!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read test data\ntaxi_test = pd.read_csv('../input/new-york-city-taxi-fare-prediction/test.csv')\ntaxi_test.columns.to_list()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reuse the above helper functions to add our features and generate the input matrix.\nadd_travel_vector_features(taxi_test)\ntaxi_test_X = get_input_matrix(taxi_test)\n# Predict fare_amount on the test set using our model (w) trained on the training set.\ntaxi_test_y_predictions = np.matmul(taxi_test_X, w).round(decimals = 2)\n\n# Write the predictions to a CSV file which we can submit to the competition.\nsubmission = pd.DataFrame(\n    {'key': taxi_test.key, 'fare_amount': taxi_test_y_predictions},\n    columns = ['key', 'fare_amount'])\nsubmission.to_csv('submission.csv', index = False)\n\nprint(os.listdir('.'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ideas for Improvement\nThe output here will score an RMSE of $5.74, but you can do better than that! Here are some suggestions:\n\n* Use more columns from the input data. Here we're only using the start/end GPS points from columns [pickup|dropoff]_[latitude|longitude]. Try to see if the other columns -- pickup_datetime and passenger_count -- can help improve your results.\n* Use absolute location data rather than relative. Here we're only looking at the difference between the start and end points, but maybe the actual values -- indicating where in NYC the taxi is traveling -- would be useful.\n* Use a non-linear model to capture more intricacies within the data.\n* Try to find more outliers to prune, or construct useful feature crosses.\n* Use the entire dataset -- here we're only using about 20% of the training data!\n\nSpecial thanks to Dan Becker, Will Cukierski, and Julia Elliot for reviewing this Kernel and providing suggestions!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n# Create a LinearRegression object\nlr = LinearRegression()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit the model on the train data\nlr.fit(X=taxi_train[['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'passenger_count']],\ny=taxi_train['fare_amount']) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Select features\nfeatures = ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'passenger_count']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make predictions on the test data\ntaxi_test['fare_amount'] = lr.predict(taxi_test[features]) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read sample submission\ntaxi_sample_sub = pd.read_csv('../input/new-york-city-taxi-fare-prediction/sample_submission.csv')\ntaxi_sample_sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read a sample submission file\ntaxi_sample_sub = pd.read_csv('taxi_sample_submission.csv')\ntaxi_sample_sub.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prepare a submission file\ntaxi_submission = taxi_test[['key', 'fare_amount']]\n# Save the submission file as .csv\ntaxi_submission.to_csv('first_sub.csv', index=False) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Write a submission file to the disk\n# submission[['id', 'target']].to_csv('submission_1.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Some classification and regression metrics\nfrom sklearn.metrics import roc_auc_score, f1_score, mean_squared_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\ndef rmsle(y_true, y_pred):\n    diffs = np.log(y_true + 1) - np.log(y_pred + 1)\n    squares = np.power(diffs, 2)\n    err = np.sqrt(np.mean(squares))\n    return err","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Another Prediction","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport datetime as dt\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\nimport os\n\nprint(os.listdir(\"../input\"))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Load dataset\n\nFirst we will load the train.csv dataset. Since this dataset has 55M rows, we will only use the first 6M to build our model to prevent memory issues and speed up preprocessing and model building.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df =  pd.read_csv('../input/new-york-city-taxi-fare-prediction/train.csv', nrows = 6_000_000) #1M to test models\ntrain_df.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data exploration\n\nNow we will explore the loaded data to identify outliers and other problems that might need fixing such as null values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Identify null values\nprint(train_df.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have a few rows with null values so it is safe to remove them.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Drop rows with null values\ntrain_df = train_df.dropna(how = 'any', axis = 'rows')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's explore the variables in the dataset. First we will look at the first rows to get an idea of the format of the values and then we will plot them to get a sense of their distribution and identify outliers.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Look at the first rows\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot variables using only 1000 rows for efficiency\ntrain_df.iloc[:1000].plot.scatter('pickup_longitude', 'pickup_latitude')\ntrain_df.iloc[:1000].plot.scatter('dropoff_longitude', 'dropoff_latitude')\n\n#Get distribution of values\ntrain_df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Okay, that was interesting. We learned a few things about the dataset:\n\n* Fare_amount has negative values. We will remove those.\n* Latitudes and longitudes have values near 0 that cannot be correct since NYC is at (40,-74) aprox. We will remove points not near these coordinates.\n* Passenger_count has values of 0 and as high as 200, which are also unrealistic. We will remove those.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Clean dataset\ndef clean_df(df):\n    return df[(df.fare_amount > 0) & \n            (df.pickup_longitude > -80) & (df.pickup_longitude < -70) &\n            (df.pickup_latitude > 35) & (df.pickup_latitude < 45) &\n            (df.dropoff_longitude > -80) & (df.dropoff_longitude < -70) &\n            (df.dropoff_latitude > 35) & (df.dropoff_latitude < 45) &\n            (df.passenger_count > 0) & (df.passenger_count < 10)]\n\ntrain_df = clean_df(train_df)\nprint(len(train_df))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Feature engineering\n\nNow that we have cleaned some extreme values, we will add some interesting features in the dataset.\n\n* total_distance: distance from pickup to dropoff. The longer the trip, the more expensive.\n* Extract information from datetime (day of week, month, hour, day). Taxi fares change day/night or on weekdays/holidays.\n* Add columns indicating distance from pickup or dropoff coordinates to airports. Trips from/to an airport have a fixed fee.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def sphere_dist(pickup_lat, pickup_lon, dropoff_lat, dropoff_lon):\n    \"\"\"\n    Return distance along great radius between pickup and dropoff coordinates.\n    \"\"\"\n    #Define earth radius (km)\n    R_earth = 6371\n    #Convert degrees to radians\n    pickup_lat, pickup_lon, dropoff_lat, dropoff_lon = map(np.radians,\n                                                             [pickup_lat, pickup_lon, \n                                                              dropoff_lat, dropoff_lon])\n    #Compute distances along lat, lon dimensions\n    dlat = dropoff_lat - pickup_lat\n    dlon = dropoff_lon - pickup_lon\n    \n    #Compute haversine distance\n    a = np.sin(dlat/2.0)**2 + np.cos(pickup_lat) * np.cos(dropoff_lat) * np.sin(dlon/2.0)**2\n    \n    return 2 * R_earth * np.arcsin(np.sqrt(a))\n\ndef add_airport_dist(dataset):\n    \"\"\"\n    Return minumum distance from pickup or dropoff coordinates to each airport.\n    JFK: John F. Kennedy International Airport\n    EWR: Newark Liberty International Airport\n    LGA: LaGuardia Airport\n    \"\"\"\n    jfk_coord = (40.639722, -73.778889)\n    ewr_coord = (40.6925, -74.168611)\n    lga_coord = (40.77725, -73.872611)\n    \n    pickup_lat = dataset['pickup_latitude']\n    dropoff_lat = dataset['dropoff_latitude']\n    pickup_lon = dataset['pickup_longitude']\n    dropoff_lon = dataset['dropoff_longitude']\n    \n    pickup_jfk = sphere_dist(pickup_lat, pickup_lon, jfk_coord[0], jfk_coord[1]) \n    dropoff_jfk = sphere_dist(jfk_coord[0], jfk_coord[1], dropoff_lat, dropoff_lon) \n    pickup_ewr = sphere_dist(pickup_lat, pickup_lon, ewr_coord[0], ewr_coord[1])\n    dropoff_ewr = sphere_dist(ewr_coord[0], ewr_coord[1], dropoff_lat, dropoff_lon) \n    pickup_lga = sphere_dist(pickup_lat, pickup_lon, lga_coord[0], lga_coord[1]) \n    dropoff_lga = sphere_dist(lga_coord[0], lga_coord[1], dropoff_lat, dropoff_lon) \n    \n    dataset['jfk_dist'] = pd.concat([pickup_jfk, dropoff_jfk], axis=1).min(axis=1)\n    dataset['ewr_dist'] = pd.concat([pickup_ewr, dropoff_ewr], axis=1).min(axis=1)\n    dataset['lga_dist'] = pd.concat([pickup_lga, dropoff_lga], axis=1).min(axis=1)\n    \n    return dataset\n    \ndef add_datetime_info(dataset):\n    #Convert to datetime format\n    dataset['pickup_datetime'] = pd.to_datetime(dataset['pickup_datetime'],format=\"%Y-%m-%d %H:%M:%S UTC\")\n    \n    dataset['hour'] = dataset.pickup_datetime.dt.hour\n    dataset['day'] = dataset.pickup_datetime.dt.day\n    dataset['month'] = dataset.pickup_datetime.dt.month\n    dataset['weekday'] = dataset.pickup_datetime.dt.weekday\n    dataset['year'] = dataset.pickup_datetime.dt.year\n    \n    return dataset\n\ntrain_df = add_datetime_info(train_df)\ntrain_df = add_airport_dist(train_df)\ntrain_df['distance'] = sphere_dist(train_df['pickup_latitude'], train_df['pickup_longitude'], \n                                   train_df['dropoff_latitude'] , train_df['dropoff_longitude'])\n\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we need to drop the columns that we will not use to train our model.\n\n* key\n* pickup_datetime","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.drop(columns=['key', 'pickup_datetime'], inplace=True)\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model training\n\nNow that we have the dataframe that we wanted we can start to train the XGBoost model. First we will split the dataset into train (99%) and test (1%). With this amount of data 1% should be enough to test performance.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train_df['fare_amount']\ntrain = train_df.drop(columns=['fare_amount'])\n\nx_train,x_test,y_train,y_test = train_test_split(train,y,random_state=0,test_size=0.01)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Cross-validation\nparams = {\n    # Parameters that we are going to tune.\n    'max_depth': 8, #Result of tuning with CV\n    'eta':.03, #Result of tuning with CV\n    'subsample': 1, #Result of tuning with CV\n    'colsample_bytree': 0.8, #Result of tuning with CV\n    # Other parameters\n    'objective':'reg:linear',\n    'eval_metric':'rmse',\n    'silent': 1\n}\n\n#Block of code used for hypertuning parameters. Adapt to each round of parameter tuning.\n#Turn off CV in submission\nCV=False\nif CV:\n    dtrain = xgb.DMatrix(train,label=y)\n    gridsearch_params = [\n        (eta)\n        for eta in np.arange(.04, 0.12, .02)\n    ]\n\n    # Define initial best params and RMSE\n    min_rmse = float(\"Inf\")\n    best_params = None\n    for (eta) in gridsearch_params:\n        print(\"CV with eta={} \".format(\n                                 eta))\n\n        # Update our parameters\n        params['eta'] = eta\n\n        # Run CV\n        cv_results = xgb.cv(\n            params,\n            dtrain,\n            num_boost_round=1000,\n            nfold=3,\n            metrics={'rmse'},\n            early_stopping_rounds=10\n        )\n\n        # Update best RMSE\n        mean_rmse = cv_results['test-rmse-mean'].min()\n        boost_rounds = cv_results['test-rmse-mean'].argmin()\n        print(\"\\tRMSE {} for {} rounds\".format(mean_rmse, boost_rounds))\n        if mean_rmse < min_rmse:\n            min_rmse = mean_rmse\n            best_params = (eta)\n\n    print(\"Best params: {}, RMSE: {}\".format(best_params, min_rmse))\nelse:\n    #Print final params to use for the model\n    params['silent'] = 0 #Turn on output\n    print(params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def XGBmodel(x_train,x_test,y_train,y_test,params):\n    matrix_train = xgb.DMatrix(x_train,label=y_train)\n    matrix_test = xgb.DMatrix(x_test,label=y_test)\n    model=xgb.train(params=params,\n                    dtrain=matrix_train,num_boost_round=5000, \n                    early_stopping_rounds=10,evals=[(matrix_test,'test')])\n    return model\n\nmodel = XGBmodel(x_train,x_test,y_train,y_test,params)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Prediction\n\nFinally we can use our trained model to predict the submission. First we will need to load and preprocess the test dataset just like we did for the training dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Read and preprocess test set\ntest_df =  pd.read_csv('../input/test.csv')\ntest_df = add_datetime_info(test_df)\ntest_df = add_airport_dist(test_df)\ntest_df['distance'] = sphere_dist(test_df['pickup_latitude'], test_df['pickup_longitude'], \n                                   test_df['dropoff_latitude'] , test_df['dropoff_longitude'])\n\ntest_key = test_df['key']\nx_pred = test_df.drop(columns=['key', 'pickup_datetime'])\n\n#Predict from test set\nprediction = model.predict(xgb.DMatrix(x_pred), ntree_limit = model.best_ntree_limit)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create submission file","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({\n        \"key\": test_key,\n        \"fare_amount\": prediction.round(2)\n})\n\nsubmission.to_csv('taxi_fare_submission.csv',index=False)\nsubmission.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}