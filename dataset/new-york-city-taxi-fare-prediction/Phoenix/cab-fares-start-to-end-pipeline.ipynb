{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_original=pd.read_csv('/kaggle/input/new-york-city-taxi-fare-prediction/train.csv',nrows=1000000,parse_dates=['pickup_datetime'])\ntest_original=pd.read_csv('/kaggle/input/new-york-city-taxi-fare-prediction/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=train_original.copy()\ntest=test_original.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['pickup_datetime']=pd.to_datetime(train['pickup_datetime'])\ntest['pickup_datetime']=pd.to_datetime(test['pickup_datetime'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_original.shape,test_original.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum()#Very few Missing value, so will delete the rows that have it","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.isnull().sum() #good , No missing in test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.dropna(axis=0,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape## only 10 were deleted. the same rows had NAs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe() #Negative fare,Latitudes range from -90 to 90, and longitudes range from -180 to 80.\n#Passeneger count max is 208? Is it a Cab or train?? :D\n# A lot of cleaning will be needed here","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.describe()#data looks good for the test data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sum(train['fare_amount']<0)) #only a few values are negative, Will delete them\n#Also from Kaggle found that the NYC cabs charge a min of $2.50(2019), so will delete records below $2(adjusting for inflation)\n\nprint(sum(train.fare_amount<2)) #very few records, so wouldnt matter anyways","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=train[(train['fare_amount']>=2)]\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['passenger_count'].value_counts()#passenger count 208?? Passenger count 0, lets see the fares for these.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#delete passenger count 208 will be deleted. passenger count 0 will also be deleted as  it doesnot make sense + test doesnot have this\ntrain=train[(train['passenger_count']<7)&(train['passenger_count']>0)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16, 5))\nplt.hist(train['fare_amount'],bins=100);\nplt.title(\"Fare Amount\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets Zoom into the above figure. Be mindful of the Y axis\nplt.figure(figsize=(15, 5))\nplt.subplot(1, 2, 1)\nplt.title(\"Low/Med Fare Amount\")\nplt.hist(train[train['fare_amount']<100]['fare_amount'],bins=100);\nplt.subplot(1, 2, 2)\nplt.title(\"High Fare Amount\")\nplt.hist(train[train['fare_amount']>=100]['fare_amount'],bins=100);\n#looks like there are many rides having fixed charge of 350,400,450 and 500. But these are very few in numbers <100 total","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x='passenger_count', y='fare_amount', data=train);\nplt.title('Fare wrt Total Passengers');#so fare not very much dependent on the # of Passenegrs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create Day of Week, Month, Year, Time of Day etc variables from the pickup_datetime variable\ndef process_date(df,colname):\n    date_parts = [\"year\", \"weekday\", \"month\", 'weekofyear', 'day', 'quarter','hour']\n    for part in date_parts:\n        part_col = colname.split('_')[0] + \"_\" + part\n        df[part_col] = getattr(df[colname].dt, part).astype(int)\n    \n    return df\n\ntrain = process_date(train,'pickup_datetime')\ntest = process_date(test,'pickup_datetime')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x='pickup_year', y='fare_amount', data=train);\nplt.title('Fare wrt pickup_year');#definitely see some outliers in each year","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='pickup_year', y='fare_amount', data=train);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x='pickup_weekday', y='fare_amount', data=train);\nplt.title('Fare wrt pickup_dayofweek');#not much of a difference except  afew outliers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x='pickup_month', y='fare_amount', data=train);\nplt.title('Fare wrt pickup_month');#not much of a difference except  afew outliers but in Summer months","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x='pickup_hour', y='fare_amount', data=train);\nplt.title('Fare wrt pickup_day');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Find min/max longitude and latitude in the data\nprint('Train data')\nprint('Min Pickup Longitude: {}, Max Pickup Longitude {}'.format(max(train['pickup_longitude']),min(train['pickup_longitude'])))\nprint('Min Drop Off Longitude: {}, Max Drop Off Longitude {}'.format(max(train['dropoff_longitude']),min(train['dropoff_longitude'])))\nprint('Min Pickup Latitude: {}, Max Pickup Latitude {}'.format(max(train['pickup_latitude']),min(train['pickup_latitude'])))\nprint('Min Drop Off Latitude: {}, Max Drop Off Latitude {}'.format(max(train['dropoff_latitude']),min(train['dropoff_latitude'])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets correct the lat/long values. Note 1degree is approx 100 kms for both Lat & long (at this place on earth)\nprint(np.quantile(train['pickup_latitude'],[0.025,0.05,0.95,0.975]))\nprint(np.quantile(train['dropoff_latitude'],[0.025,0.05,0.95,0.975]))\n#Analysing these results, lets say we will take threshold as 40.50 and 40.90( after rounding off in decimal places)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.quantile(train['pickup_longitude'],[0.025,0.05,0.95,0.975]))\nprint(np.quantile(train['dropoff_longitude'],[0.025,0.05,0.95,0.975]))\n#Analysing these results, lets say we will take threshold as -74.10 and -73.60( after rounding off in decimal places)\n#Note we may be tempted to use the limits from test data but that is IMHO cheating","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Test data')\nprint('Min Pickup Longitude: {}, Max Pickup Longitude {}'.format(max(test['pickup_longitude']),min(test['pickup_longitude'])))\nprint('Min Drop Off Longitude: {}, Max Drop Off Longitude {}'.format(max(test['dropoff_longitude']),min(test['dropoff_longitude'])))\nprint('Min Pickup Latitude: {}, Max Pickup Latitude {}'.format(max(test['pickup_latitude']),min(test['pickup_latitude'])))\nprint('Min Drop Off Latitude: {}, Max Drop Off Latitude {}'.format(max(test['dropoff_latitude']),min(test['dropoff_latitude'])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#we see a huge difference between train & test. The extreme co-ordinates of train are not even present in the US not even feasible\n#I will just remove the rows based on the max/min co-ordinates I see in the test data. As the test data seems very clean\n#with the max/min values of lat-long\nboundary=(-74.10,-73.60,40.50,40.90)\ntrain=train[(train['pickup_longitude']>boundary[0])&(train['pickup_longitude']<boundary[1])&\\\n            (train['pickup_latitude']>boundary[2])&(train['pickup_latitude']<boundary[3])]\n\ntrain=train[(train['dropoff_longitude']>boundary[0])&(train['dropoff_longitude']<boundary[1])&\\\n            (train['dropoff_latitude']>boundary[2])&(train['dropoff_latitude']<boundary[3])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://en.wikipedia.org/wiki/Haversine_formula\ndef distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295 # Pi/180 , since 2pi radians=260degrees\n    a = 0.5 - np.cos((lat2 - lat1) * p)/2 + np.cos(lat1 * p) * np.cos(lat2 * p) * (1 - np.cos((lon2 - lon1) * p)) / 2\n    return 12742 * np.arcsin(np.sqrt(a)) # 2*R*asin... #multiply this by 0.62137 for miles","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Haversine_distance']=distance(train['pickup_latitude'],train['pickup_longitude'],\\\n                                     train['dropoff_latitude'],train['dropoff_longitude'])\ntest['Haversine_distance']=distance(test['pickup_latitude'],test['pickup_longitude'],\\\n                                     test['dropoff_latitude'],test['dropoff_longitude'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lest see the distance \ntrain['Haversine_distance'].describe()#max distance is 35kms","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.quantile(test['Haversine_distance'],[0.95,0.99,1])#for test the max is 100 kms but i believe that such high values are outliers\n#So, though the train seems a bit different from test, it okay i guess as test has a few outliers.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train[train['Haversine_distance']==0]['fare_amount'].describe())\nprint(train[train['Haversine_distance']==0].shape)\n#Many, 10k records have the same Pickup& Drop location but still a positive fare. \n#May be a genuine ride as we have a few of these in test too","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test[test['Haversine_distance']==0].shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15, 5))\nplt.subplot(1, 2, 1)\nplt.title(\"Low/Med distance\")\nplt.hist(train[train['Haversine_distance']<20]['Haversine_distance'],bins=100);\nplt.subplot(1, 2, 2)\nplt.title(\"High distance\")\nplt.hist(train[train['Haversine_distance']>=20]['Haversine_distance'],bins=100);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot Fare vs ditance.\nplt.figure(figsize=(20, 8))\nplt.subplot(1, 2, 1)\nplt.scatter(train['Haversine_distance'],train['fare_amount']);\nplt.xlabel('distance kms')\nplt.ylabel('Fare USD')\nplt.title('all data')\n\nplt.subplot(1, 2, 2)\nplt.scatter(train[(train['Haversine_distance']<25)&(train['fare_amount']<100)]['Haversine_distance'],\\\n            train[(train['Haversine_distance']<25)&(train['fare_amount']<100)]['fare_amount']);\nplt.xlabel('distance kms')\nplt.ylabel('Fare USD')\nplt.title('Zoom in on Fare<100 and Distance<25 kms');\n#we can see that as the distance increases Fare amount increases","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['Haversine_distance'].describe(),train['Haversine_distance'].describe()\n#we can see there are a lot of outliers in the training data. ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# What have we done so far?\n# 1) Applied filtering on lat/long based on percentiles\n# 2) Removed <$2 fare\n# 3) Removed >6 Passenger_count\n# 4) Calculated Haversine Distance, that comes to max 35 kms in train but 99 kms in test\n# 5) the test max HD is actually an outlier (just one/2 record with 99kms), all other are in fact below 25 kms","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Count of train rows with 0 haversine distance is {}'.format(sum(train['Haversine_distance']==0)))\n#lets get drop these rows where distance is 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Count of test rows with 0 haversine distance is {}'.format(sum(test['Haversine_distance']==0)))\n#since test also has a few HD==0, i will not delete these observations from train data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Latitude: 1 deg = 110.574 km\n#Unlike latitude, the distance between degrees of longitude varies greatly depending upon your \n#location on the planet. They are farthest apart at the equator and converge at the poles.\n#A degree of longitude is widest at the equator with a distance of 110kms\n#np.digitize([2,11,21,33],bins=[1,5,10,15,20])#the bins are 0(0-1),1(1-5),2(5-10),3(10-15),4(15-20),5(20-)\n#above gives array([1, 3, 5, 5])\n#boundary=(-74.50,-72.80,40.50,41.80)Long/lat \n#Delta long is 1.7 and delta lat is 1.29, so around 1.7*90=153(lesser than it is at equator) kms lat and 1.29*110=142 kms long\n#distance(lat1, lon1, lat2, lon2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Some trips, like to/from an airport, are fixed fee. To prce this see the plot below\n# JFK airport coordinates, see https://www.travelmath.com/airport/JFK\njfk = (-73.7822222222, 40.6441666667) #airport\nnyc = (-74.0063889, 40.7141667)#city centre\n\ndef plot_location_fare(loc, name, range=2): #within range kms of the location\n    # select all datapoints with dropoff location within range\n    fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n    idx = (distance(train.pickup_latitude, train.pickup_longitude, loc[1], loc[0]) < range)\n    train[idx].fare_amount.hist(bins=100, ax=axs[0])\n    axs[0].set_xlabel('fare $USD')\n    axs[0].set_title('Histogram pickup location within {} KMS of {}'.format(range, name))\n\n    idx = (distance(train.dropoff_latitude, train.dropoff_longitude, loc[1], loc[0]) < range)\n    train[idx].fare_amount.hist(bins=100, ax=axs[1])\n    axs[1].set_xlabel('fare $USD')\n    axs[1].set_title('Histogram dropoff location within {} KMS of {}'.format(range, name));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_location_fare(jfk,'JFK airport',3)#looks like it is true. Fare is the same for most rides within 5kms from jfk airport","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ewr = (-74.175, 40.69) # Newark Liberty International Airport, see https://www.travelmath.com/airport/EWR\nlgr = (-73.87, 40.77) # LaGuardia Airport, see https://www.travelmath.com/airport/LGA\nplot_location_fare(ewr, 'Newark Airport',3)\nplot_location_fare(lgr, 'LaGuardia Airport',3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#So,lets add a binary variable which is 1 if the pick up is from JFK and another if the drop is at JFk\n##TRAIN\n#This did not help ( got to know by looking at the feature importance plot from XGB) so wont take this nomore\n# train.loc[:,'Pick_up_jfk']=np.where(distance(train.pickup_latitude,\\\n#                                                             train.pickup_longitude, jfk[1], jfk[0])<3,1,0)\n\n# train.loc[:,'dropoff_jfk']=np.where(distance(train.dropoff_latitude,\\\n#                                                             train.dropoff_longitude, jfk[1], jfk[0])<3,1,0)\n\n# train.loc[:,'Pick_up_ewr']=np.where(distance(train.pickup_latitude,\\\n#                                                             train.pickup_longitude, ewr[1], ewr[0])<3,1,0)\n\n# train.loc[:,'dropoff_ewr']=np.where(distance(train.dropoff_latitude,\\\n#                                                             train.dropoff_longitude, ewr[1], ewr[0])<3,1,0)\n\n\n# train.loc[:,'Pick_up_lgr']=np.where(distance(train.pickup_latitude,\\\n#                                                             train.pickup_longitude, lgr[1], lgr[0])<3,1,0)\n\n# train.loc[:,'dropoff_lgr']=np.where(distance(train.dropoff_latitude,\\\n#                                                             train.dropoff_longitude, lgr[1], lgr[0])<3,1,0)\n\n# ##Test\n\n# test.loc[:,'Pick_up_jfk']=np.where(distance(test.pickup_latitude,\\\n#                                                             test.pickup_longitude, jfk[1], jfk[0])<3,1,0)\n\n# test.loc[:,'dropoff_jfk']=np.where(distance(test.dropoff_latitude,\\\n#                                                             test.dropoff_longitude, jfk[1], jfk[0])<3,1,0)\n\n# test.loc[:,'Pick_up_ewr']=np.where(distance(test.pickup_latitude,\\\n#                                                             test.pickup_longitude, ewr[1], ewr[0])<3,1,0)\n\n# test.loc[:,'dropoff_ewr']=np.where(distance(test.dropoff_latitude,\\\n#                                                             test.dropoff_longitude, ewr[1], ewr[0])<3,1,0)\n\n\n# test.loc[:,'Pick_up_lgr']=np.where(distance(test.pickup_latitude,\\\n#                                                             test.pickup_longitude, lgr[1], lgr[0])<3,1,0)\n\n# test.loc[:,'dropoff_lgr']=np.where(distance(test.dropoff_latitude,\\\n#                                                             test.dropoff_longitude, lgr[1], lgr[0])<3,1,0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# display pivot table\ntrain.pivot_table('fare_amount', index='pickup_hour', columns='pickup_year').plot(figsize=(14,6))\nplt.ylabel('Fare $USD');\n#we can see that the average Fare vs time of day has been increasing with year. #Inflation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def select_within_boundingbox(df, BB):\n    '''\n    returns a Boolean series\n    '''\n    return (df.pickup_longitude >= BB[0]) & (df.pickup_longitude <= BB[1]) & \\\n           (df.pickup_latitude >= BB[2]) & (df.pickup_latitude <= BB[3]) & \\\n           (df.dropoff_longitude >= BB[0]) & (df.dropoff_longitude <= BB[1]) & \\\n           (df.dropoff_latitude >= BB[2]) & (df.dropoff_latitude <= BB[3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Relevance of direction for fare amount\n#How do we find out of the direction influences the fare_amount?\n\n#Remember the co-ordinates are the LAt/Long value. from this we calculate DELTA Lat \n#and DELTA LONG and plot that wrt Fare to see if there is any\n#Difference\ntrain['delta_lat']=train['pickup_latitude']-train['dropoff_latitude']\ntrain['delta_long']=train['pickup_longitude']-train['dropoff_longitude']\n\nplt.figure(figsize=(14,8))\n# Select only the trips in Manhattan\nBB_manhattan = (-74.025, -73.925, 40.7, 40.8)#found from Google\nwithin_manhattan_train=select_within_boundingbox(train,BB_manhattan)\n\n\nplt.scatter(train[within_manhattan_train]['delta_long'],train[within_manhattan_train]['delta_lat'],\\\n            s=0.5, alpha=1.0, \n            c=np.log1p(train[within_manhattan_train]['fare_amount']), cmap='magma')\nplt.colorbar()\nplt.xlabel('pickup_longitude - dropoff_longitude')\nplt.ylabel('pickup_latitude - dropoff_latidue')\nplt.title('log1p(fare_amount)');\n#Fare seems to be lesser in the center and more around perimeter nad i can see a star here.(slightly tilted). ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('total {} records out of {} are in manhattan. So {}'.format(sum(within_manhattan_train),train.shape[0],\\\n                                                                 sum(within_manhattan_train)*100/train.shape[0]))\n#Since most of the records are from Manhattan, If my model predicts well for these, my overall \n#performance will be quite good","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import Image\nImage(filename = \"../input/manhattan/manhattan.JPG\", width = 400, height = 350)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#if you see the map of Manhattan, the streets are at 60 degrees and -30 degrees with horizontal. \n#hence, 2 location along this angle are very close to each other. hence you see the star.\n#lets get a new variable that is the actual angle with the horizontal. This variable will help \n#whatever model we build later","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Image(filename = \"../input/astc-picjpg/astc.JPG\", width = 150, height = 120)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#From the triangle that is formed above for a given street, we know the base ( delta long), perpen(delta lat)\n#and can calculate the Hypotenuse ( l2 distance)\n\n#So we can also calculate the angle of the route with the horizontal.\n#TAN (Theta)= P/base\n#So, Theta in degrees is tan-1 of p/base\n# direction of a trip, from 180 to -180 degrees. Horizontal axes = 0 degrees.\ndef calculate_direction(d_lon, d_lat):\n    result = np.zeros(len(d_lon))\n    l = np.sqrt(d_lon**2 + d_lat**2)\n    result[d_lon>0] = (180/np.pi)*np.arcsin(d_lat[d_lon>0]/l[d_lon>0])\n    idx = (d_lon<0) & (d_lat>0)\n    result[idx] = 180 - (180/np.pi)*np.arcsin(d_lat[idx]/l[idx])\n    idx = (d_lon<0) & (d_lat<0)\n    result[idx] = -180 - (180/np.pi)*np.arcsin(d_lat[idx]/l[idx])\n    return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#I am calculatig this for all records but this will be applicate to only the records in Manhattan\n#I will create a Binary variable saying if a record is within Manhattan or not\ntrain['direction'] = calculate_direction(train['delta_long'],train['delta_lat'])\ntrain['direction'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#prepare the Delta variables for test data\ntest['delta_lat']=test['pickup_latitude']-test['dropoff_latitude']\ntest['delta_long']=test['pickup_longitude']-test['dropoff_longitude']\ntest['direction'] = calculate_direction(test['delta_long'],test['delta_lat'])\ntest['direction'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot direction vs average fare amount\nfig, ax = plt.subplots(1, 1, figsize=(14,6))\ndirec = pd.cut(train[within_manhattan_train]['direction'], np.linspace(-180, 180, 37))\ntrain[within_manhattan_train].pivot_table('fare_amount', index=[direc], columns='pickup_year', aggfunc='mean').plot(ax=ax)\nplt.xlabel('direction (degrees)')\nplt.xticks(range(36), np.arange(-170, 190, 10))\nplt.ylabel('average fare amount $USD');\n#Clearly avg Fare is Lower in 60 degree and -120 degrees as Manhattan road are very straight in that direction(so less total distance taken)\n#also google maps show me that the 60 degree roads are broader than others/hence less traffic\n#also avg fare is lowest in -20degree as there is hardly any land in this direction. Mostly water. So the distances along this must be less","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#but for the same Haversine distance, the total actual distance and hence FARE along 60& 120 degrees must be lesser than at other angles\nwithin_manhattan_train_and_around_5kms_trip_HD=within_manhattan_train &(train['Haversine_distance']>4.5)&(train['Haversine_distance']<5.5)\nidx2=within_manhattan_train_and_around_5kms_trip_HD\n# plot direction vs average fare amount\nfig, ax = plt.subplots(1, 1, figsize=(14,6))\ndirec = pd.cut(train[idx2]['direction'], np.linspace(-180, 180, 37))\ntrain[idx2].pivot_table('fare_amount', index=[direc], columns='pickup_year', aggfunc='mean').plot(ax=ax)\nplt.xlabel('direction (degrees)')\nplt.xticks(range(36), np.arange(-170, 190, 10))\nplt.ylabel('average fare amount $USD');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#The above should be observed at any particular total Haversine distance\nwithin_manhattan_train_and_around_3kms_trip_HD=within_manhattan_train &(train['Haversine_distance']>2.5)&(train['Haversine_distance']<3.5)\nidx3=within_manhattan_train_and_around_3kms_trip_HD\n# plot direction vs average fare amount\nfig, ax = plt.subplots(1, 1, figsize=(14,6))\ndirec = pd.cut(train[idx3]['direction'], np.linspace(-180, 180, 37))\ntrain[idx3].pivot_table('fare_amount', index=[direc], columns='pickup_year', aggfunc='mean').plot(ax=ax)\nplt.xlabel('direction (degrees)')\nplt.xticks(range(36), np.arange(-170, 190, 10))\nplt.ylabel('average fare amount $USD');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## add the binary column for Manhatttan or not\n\ntrain['manhattan']=within_manhattan_train.map(lambda x: int(x))\n\nwithin_manhattan_test=select_within_boundingbox(test,BB_manhattan)\ntest['manhattan']=within_manhattan_test.map(lambda x: int(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Empirical Cumulative Distribution Function Plot for fare_amount\ndef ecdf(x):\n    \"\"\"Empirical cumulative distribution function of a variable\"\"\"\n    # Sort in ascending order\n    x = np.sort(x)\n    n = len(x)\n    \n    # Go from 1/n to 1\n    y = np.arange(1, n + 1, 1) / n\n    \n    return x, y\n\nxs, ys = ecdf(train['fare_amount'])\nplt.figure(figsize = (8, 6))\nplt.plot(xs, ys, '.')\nplt.ylabel('Percentile'); plt.title('ECDF of Fare Amount'); plt.xlabel('Fare Amount ($)');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.corrcoef(train['Haversine_distance'],train['fare_amount'])#there is a good co-relation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Add a column that gives the pickup and dropoff distance from the 3 airports and NYC centre. Thsi may help us capture the fixed charge \n#from these the airports and prime location charges\newr = (-74.175, 40.69) # Newark Liberty International Airport, see https://www.travelmath.com/airport/EWR\nlgr = (-73.87, 40.77) # LaGuardia Airport, see https://www.travelmath.com/airport/LGA\njfk = (-73.7822222222, 40.6441666667) #airport\nnyc = (-74.0063889, 40.7141667)#city centre\n#distance(lat1, lon1, lat2, lon2) use this previously built function to calculate distance\ntrain['dis_pickup_from_ewr']=distance(train['pickup_latitude'],train['pickup_longitude'],ewr[1],ewr[0])\ntrain['dis_dropoff_from_ewr']=distance(train['dropoff_latitude'],train['dropoff_longitude'],ewr[1],ewr[0])\n\ntrain['dis_pickup_from_lgr']=distance(train['pickup_latitude'],train['pickup_longitude'],lgr[1],lgr[0])\ntrain['dis_dropoff_from_lgr']=distance(train['dropoff_latitude'],train['dropoff_longitude'],lgr[1],lgr[0])\n\ntrain['dis_pickup_from_jfk']=distance(train['pickup_latitude'],train['pickup_longitude'],jfk[1],jfk[0])\ntrain['dis_dropoff_from_jfk']=distance(train['dropoff_latitude'],train['dropoff_longitude'],jfk[1],jfk[0])\n\ntrain['dis_pickup_from_nyc']=distance(train['pickup_latitude'],train['pickup_longitude'],nyc[1],nyc[0])\ntrain['dis_dropoff_from_nyc']=distance(train['dropoff_latitude'],train['dropoff_longitude'],nyc[1],nyc[0])\n\n#For test\n\ntest['dis_pickup_from_ewr']=distance(test['pickup_latitude'],test['pickup_longitude'],ewr[1],ewr[0])\ntest['dis_dropoff_from_ewr']=distance(test['dropoff_latitude'],test['dropoff_longitude'],ewr[1],ewr[0])\n\ntest['dis_pickup_from_lgr']=distance(test['pickup_latitude'],test['pickup_longitude'],lgr[1],lgr[0])\ntest['dis_dropoff_from_lgr']=distance(test['dropoff_latitude'],test['dropoff_longitude'],lgr[1],lgr[0])\n\ntest['dis_pickup_from_jfk']=distance(test['pickup_latitude'],test['pickup_longitude'],jfk[1],jfk[0])\ntest['dis_dropoff_from_jfk']=distance(test['dropoff_latitude'],test['dropoff_longitude'],jfk[1],jfk[0])\n\ntest['dis_pickup_from_nyc']=distance(test['pickup_latitude'],test['pickup_longitude'],nyc[1],nyc[0])\ntest['dis_dropoff_from_nyc']=distance(test['dropoff_latitude'],test['dropoff_longitude'],nyc[1],nyc[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.sample(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#COlumns that i plan to use for modelling\nmodel_cols=['pickup_longitude',\n       'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude',\n        'pickup_year', 'pickup_weekday', 'pickup_month',\n       'pickup_day', 'pickup_hour',\n       'Haversine_distance', 'direction','manhattan', 'dis_pickup_from_ewr', 'dis_dropoff_from_ewr',\n       'dis_pickup_from_lgr', 'dis_dropoff_from_lgr', 'dis_pickup_from_jfk',\n       'dis_dropoff_from_jfk', 'dis_pickup_from_nyc', 'dis_dropoff_from_nyc']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modelling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error,make_scorer\nfrom sklearn.model_selection import GridSearchCV,StratifiedShuffleSplit,StratifiedKFold,RandomizedSearchCV\ndef rmse(y_true, y_pred):\n    diff = mean_squared_error(y_true, y_pred)\n    return diff**0.5\nmy_scorer = make_scorer(rmse,greater_is_better=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Linear Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#for Linear regression since they do not have High variance, i will simply divide Train into train & valdation\n#and use this to finally predict on the test\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\nX=train[model_cols]\ny=train['fare_amount']\n\nX_train, X_valid, y_train, y_valid =train_test_split(X, y, test_size=0.30, random_state=2020)\n\nmodel_lin = Pipeline((\n        (\"standard_scaler\", StandardScaler()),\n        (\"lin_reg\", LinearRegression()),\n    ))\n\nmodel_lin.fit(X_train, y_train)\ny_train_pred = model_lin.predict(X_train)\ny_valid_pred = model_lin.predict(X_valid)\ny_test_pred = model_lin.predict(test[model_cols])\n\nprint('RMSE on the Validation data is {} and on train is {}'.format(rmse(y_valid,y_valid_pred),\\\n                                                                    rmse(y_train,y_train_pred)))\n#This scores 5.4 on Kaggle LB. So we have overfit the data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check if there is Variance in Linear Regression\n#or check if the selection of train & validation matters for the MODEL\n\ndef variance_linreg(train,n=50):#will try 50 different splits\n    X=train[model_cols]\n    y=train['fare_amount']\n    rmse_train=[]\n    rmse_valid=[]\n    for i in range(n):\n        X_train, X_valid, y_train, y_valid =train_test_split(X, y, test_size=0.30, random_state=i)\n        model_lin = Pipeline((\n        (\"standard_scaler\", StandardScaler()),\n        (\"lin_reg\", LinearRegression()),\n    ))\n        model_lin.fit(X_train, y_train)\n        y_train_pred = model_lin.predict(X_train)\n        rmse_train.append(rmse(y_train,y_train_pred))\n        y_valid_pred = model_lin.predict(X_valid)\n        rmse_valid.append(rmse(y_valid,y_valid_pred))\n    \n    return (np.mean(rmse_valid),np.std(rmse_valid))\n\nvariance_linreg(train,n=20)#Not much deviation from mean. So No Variance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Hylper parameter tuning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class Hyper_param_tuning():\n    def __init__(self,train_x,train_y,folds=5,n_estimators=500):\n        self.train_x=train_x\n        self.train_y=train_y\n        #self.test_x=test_x\n        self.folds=folds\n        self.n_estimators=n_estimators\n        #self.skf = StratifiedKFold(n_splits=self.folds, shuffle = True, random_state = 2017)\n        \n        \n    def Tree_Model(self,params,model_name,param_comb=0):\n        \"\"\"\n        model_name should be xgb or lgbm or rf in smalls letters\n        \"\"\"\n        \n        if model_name=='xgb':\n            model=XGBRegressor(learning_rate=0.04,n_estimators=self.n_estimators ,objective='reg:squarederror')\n        elif model_name=='lgbm':\n            model=lgb.LGBMRegressor(learning_rate=0.02,n_estimators=self.n_estimators ,objective='regression')\n        else:\n            print(\"Running a RF Model\")\n            model=RandomForestRegressor(random_state=2,criterion='mse')\n        \n        search_obj = GridSearchCV(estimator=model, param_grid=params,\\\n                                scoring=my_scorer, n_jobs=-1, cv=self.folds, verbose=3)      \n        \n        search_obj.fit(self.train_x,self.train_y)\n        print('\\n Best estimator:')\n        print(search_obj.best_estimator_) #gives values of all hyperparameters\n        print('\\n Best hyperparameters for {} Model are:'.format(model_name))\n        print(search_obj.best_params_) #gives the best out of the parameter search space\n        print('\\n Best Score for {}-fold search is {}'.format(self.folds,search_obj.best_score_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\nNumber_of_folds = 5\n#We have to make sure same K fold splits are used for all Models. This avoids Overfitting and Leakage\nfolds = KFold(n_splits=Number_of_folds, shuffle=True, random_state=2017)\n\nX_train=train[model_cols]\ny_train=train['fare_amount']\nX_test=test[model_cols]\n\ntune=Hyper_param_tuning(X_train,y_train,folds=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier,RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV,StratifiedShuffleSplit,StratifiedKFold,RandomizedSearchCV\nfrom xgboost import XGBRegressor\nimport lightgbm as lgb\nimport xgboost as xgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n#RF\n#parameter grid for RF\nparams = {'n_estimators': [100],\n              'max_features': ['sqrt'], #, 'sqrt','auto'\n             # 'criterion':  ['gini'], #'entropy',#gini is for clssification\n              'max_depth': [30,40,50,80],\n              'min_samples_leaf': [40,15,50]\n            # 'min_samples_split':5,\n            }\n#xgb=tune.Tree_Model(params=params,model_name='rf')\n\n#10 trees, 160 fits, 1 hour, -2.821685185125628,{'max_depth': 30, 'max_features': 'auto', 'min_samples_leaf': 15, 'n_estimators': 10}\n# 30 trees,60 fits, 28 minutes,-2.808, {'max_depth': 40, 'max_features': 'sqrt', 'min_samples_leaf': 15, 'n_estimators': 30}\n# 60 trees,60 fits,55min, -2.801,{'max_depth': 40, 'max_features': 'sqrt', 'min_samples_leaf': 15, 'n_estimators': 60}\n# 100 trees,60 fits,90 min,-2.798,{'max_depth': 40, 'max_features': 'sqrt', 'min_samples_leaf': 15, 'n_estimators': 100}\n# I notice hardly  any change in the metric with increasing number of trees. So, lets keep it at 60\n#Good thing is 'max_depth': 40, 'min_samples_leaf': 15 for all values of n_estimators","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n## XGB\nparams = {\n        'min_child_weight': [ 5], #[ 5, 10]\n        'gamma': [1.5], #[1.5, 5]\n        'subsample': [0.6, 1.0],\n        'colsample_bytree': [0.6], #[0.6, 1.0]\n        'max_depth': [3, 10],\n        'alpha': [1],#[5,1]\n        'lambda': [5] #[5,15]\n            }\n#xgb=tune.Tree_Model(params=params,model_name='xgb')\n#1h 25min\n#-2.6689, {'alpha': 1, 'colsample_bytree': 0.6, 'gamma': 1.5, 'lambda': 5, 'max_depth': 10, 'min_child_weight': 5, 'subsample': 1.0}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n## LGBM\nlgbm_params= {#\"max_depth\": 5,          #max depth for tree model\n              #\"num_leaves\": 25,        #max number of leaves in one tree\n              # 'feature_fraction':0.6,  #LightGBM will randomly select part of features on each tree node\n               'bagging_fraction':[0.8],    #randomly select part of data without resampling\n              # 'max_drop': 5,         #used only in dart,max number of dropped trees during one boosting iteration\n              'lambda_l1': [5],#[1,5]\n              'lambda_l2':[ 0.01,0.5], #[ 0.01,0.5,10]\n              'min_child_samples':[400,600],  #minimal number of data in one leaf\n                'max_bin':[15,20], #max number of bins that feature values will be bucketed in. Higher value--> Overfitting\n               # 'subsample':[0.6,0.8],  #randomly select part of data without resampling\n                'colsample_bytree':[0.8], #same as feature_fraction\n               'boosting_type': ['dart']   #options are gbdt(gradientboosting decision trees), rf,dart,goss\n                }  #weight of labels with positive class\n\n#lgbm=tune.Tree_Model(params=lgbm_params,model_name='lgbm')\n\n#-3.291, {'bagging_fraction': 0.8, 'boosting_type': 'dart', 'colsample_bytree': 0.8, 'lambda_l1': 5,\n#'lambda_l2': 0.5, 'max_bin': 20, 'min_child_samples': 400}, 2 hours","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Bayesian Optimization with XGBoost/Alternative to parameter tuning for XGB/others","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from bayes_opt import BayesianOptimization\nimport xgboost as xgb\n# params = {\n#         'min_child_weight': [ 5], #[ 5, 10]\n#         'gamma': [1.5,5], #[1.5, 5]\n#         'subsample': [0.6, 1.0],\n#         'colsample_bytree': [0.6,1.0], #[0.6, 1.0]\n#         'max_depth': [3, 10],\n#         'alpha': [1],#[5,1]\n#         'lambda': [5] #[5,15]\n#             }\n\ndef xgb_evaluate(max_depth, gamma, colsample_bytree):\n    params = {'eval_metric': 'rmse',\n              'max_depth': int(max_depth),\n              'subsample': 1.0,\n              'eta': 0.1,\n              'gamma': gamma,\n              'colsample_bytree': colsample_bytree}\n    # Used around 1000 boosting rounds in the full model\n    cv_result = xgb.cv(params, dtrain, num_boost_round=100, nfold=3)    \n    \n    # Bayesian optimization only knows how to maximize, not minimize, so return the negative RMSE\n    return -1.0 * cv_result['test-rmse-mean'].iloc[-1]\n\n#X_train, X_valid, y_train, y_valid taking these from the linear regression model\ndtrain = xgb.DMatrix(X_train, label=y_train)\ndtest = xgb.DMatrix(X_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_bo = BayesianOptimization(xgb_evaluate, {'max_depth': (3, 10), \n                                             'gamma': (0, 1),\n                                             'colsample_bytree': (0.6, 1.0)})\n# Use the expected improvement acquisition function to handle negative numbers\n# Optimally needs quite a few more initiation points and number of iterations\n#xgb_bo.maximize(init_points=3, n_iter=5, acq='ei') #commenting for now","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Extract the parameters of the best model.\n# params = xgb_bo.max['params']\n# params['max_depth'] = int(xgb_bo.max['params']['max_depth'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Final Modelling- Level 1","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Provide a K-fold function that generate out-of-fold predictions for train data.\nclass Modelling():\n    def __init__(self,X,y,test_X,folds,N):\n        self.X=X\n        self.y=y\n        self.test_X=test_X\n        self.folds=folds\n        self.N=N\n     \n    def Single_Model(self,Regressor): #for all other Models like LInear,NB ,KNN etc\n        stacker_train = np.zeros((self.X.shape[0], 1))\n        stacker_test  = np.zeros(self.test_X.shape[0])        \n        for index, (trn_idx,val_idx) in enumerate(self.folds.split(self.X)):\n            trn_x, val_x = self.X[trn_idx], self.X[val_idx]\n            trn_y, val_y = self.y[trn_idx], self.y[val_idx]\n            print('Train model in fold {}'.format(index+1))           \n            Regressor.fit(trn_x,(trn_y))#if passing log then take np.log1p(trn_y)\n            val_pred = (Regressor.predict(val_x))#np.expm1(Regressor.predict(val_x))\n            stacker_train[val_idx,0]=val_pred\n            val_rmse=rmse(val_y, val_pred)            \n            print('fold {} RMSE score on VAL is {:.6f}'.format(index+1, val_rmse))\n            #for test\n            pred_test= (Regressor.predict(self.test_X))#np.expm1(Regressor.predict(self.test_X))\n            stacker_test+=(pred_test/self.N)\n            \n        #evaluate for entire train data (oof)\n        train_rmse=rmse(self.y,stacker_train)\n        print(\"CV score on TRAIN (OOF) is RMSE: {}\".format(train_rmse))   \n        return stacker_test,stacker_train        \n        \n        \n        \n    def SingleRF_oof(self,params):\n        clf_rf=RandomForestRegressor(**rf_params)\n        stacker_train = np.zeros((self.X.shape[0], 1))\n        stacker_test  = np.zeros(self.test_X.shape[0])\n        for index, (trn_idx,val_idx) in enumerate(self.folds.split(self.X,self.y)):\n            trn_x, val_x = self.X[trn_idx], self.X[val_idx]\n            trn_y, val_y = self.y[trn_idx], self.y[val_idx]\n            print('Train model in fold {}'.format(index+1))         \n            clf_rf.fit(trn_x,trn_y)\n            val_pred = clf_rf.predict(val_x)\n            stacker_train[val_idx,0]=val_pred\n            val_rmse=rmse(val_y, val_pred)    \n                        \n            print('fold {} RMSE score on VAL is {:.6f}'.format(index+1,val_rmse))\n            #for test\n            pred_test= clf_rf.predict(self.test_X)\n            stacker_test+=(pred_test/self.N)\n            print('OOB Score: {}'.format(clf_rf.oob_score_)) #R2 by default for regression\n        #evaluate for entire train data (oof)\n        train_rmse=rmse(self.y,stacker_train)\n        print(\"CV score on TRAIN (OOF) is RMSE: {}\".format(train_rmse))   \n        return stacker_test,stacker_train    \n\n    \n    def SingleXGB_oof(self,params,num_boost_round):\n        stacker_train = np.zeros((self.X.shape[0], 1))\n        stacker_test=np.zeros(self.test_X.shape[0])\n        dtest=xgb.DMatrix(self.test_X)\n        for index, (trn_idx,val_idx) in enumerate(self.folds.split(self.X)):\n            trn_x, val_x = self.X[trn_idx], self.X[val_idx]\n            trn_y, val_y = self.y[trn_idx], self.y[val_idx]\n            dtrn = xgb.DMatrix(data=trn_x, label=(trn_y))#np.log1p(trn_y)\n            dval = xgb.DMatrix(data=val_x, label=(val_y))#np.log1p(val_y))\n            print('Train model in fold {}'.format(index+1)) \n            cv_model = xgb.train(params=params,dtrain=dtrn,num_boost_round=num_boost_round\\\n                                 ,evals=[(dtrn, 'train'), (dval, 'val')],verbose_eval=10,early_stopping_rounds=200)\n                        \n            pred_test = (cv_model.predict(dtest, ntree_limit=cv_model.best_ntree_limit))#np.expm1\n            stacker_test+=(pred_test/self.N)\n            val_pred=(cv_model.predict(dval, ntree_limit=cv_model.best_ntree_limit))#np.expm1\n            stacker_train[val_idx,0]=val_pred\n            val_rmse=rmse(val_y, val_pred)\n            \n            print('fold {} RMSE score on VAL is {:.6f}'.format(index+1, val_rmse))\n            \n        #evaluate for entire train data (oof)\n        train_rmse=rmse(self.y,stacker_train)\n        print(\"CV score on TRAIN (OOF) is RMSE: {}\".format(train_rmse))   \n        return stacker_test,stacker_train\n    \n    \n    def SingleLGBM_oof(self,params,num_boost_round,colnames,importance_plot=False): #passing the col names to print the Feature imp\n        stacker_train = np.zeros((self.X.shape[0], 1))\n        stacker_test=np.zeros(self.test_X.shape[0])\n        feature_importance =pd.DataFrame()\n        for index, (trn_idx,val_idx) in enumerate(self.folds.split(self.X,self.y)):\n            trn_x, val_x = self.X[trn_idx], self.X[val_idx]\n            trn_y, val_y = self.y[trn_idx], self.y[val_idx]\n\n            print('Train model in fold {}'.format(index+1)) \n            lgb_train = lgb.Dataset(trn_x,(trn_y)) #np.log1p\n            lgb_val = lgb.Dataset(val_x, (val_y), reference=lgb_train)#np.log1p\n            \n            lgb_model = lgb.train(params,\n                        lgb_train,\n                        num_boost_round=num_boost_round,\n                        valid_sets=lgb_val,\n                        early_stopping_rounds=200,\n                        verbose_eval=10)\n            \n            val_pred=(lgb_model.predict(val_x))#np.expm1\n            val_rmse=rmse(val_y, val_pred)\n            print('fold {} RMSE score on VAL is {:.6f}'.format(index+1, val_rmse))\n            stacker_train[val_idx,0]=val_pred\n\n            pred_test = (lgb_model.predict(self.test_X))#np.expm1\n            stacker_test+=(pred_test/self.N)\n            #feature importance\n            fold_importance = pd.DataFrame()\n            \n            fold_importance[\"feature\"] = colnames\n            fold_importance[\"importance\"] = lgb_model.feature_importance()\n            fold_importance[\"fold\"] = index+1\n            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n        \n        if importance_plot:\n            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n                by=\"importance\", ascending=False)[:30].index\n            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n            plt.figure(figsize=(12, 9));\n            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n            plt.title('LGBM Features (avg over folds,Top Few)');\n                \n        \n        #evaluate for entire train data (oof)\n        train_rmse=rmse(self.y,stacker_train)\n        print(\"CV score on TRAIN (OOF) is RMSE: {}\".format(train_rmse))   \n        return stacker_test,stacker_train\n    \n    \n    def SingleCatBoost_oof(self,params): #simple catboost without the cat columns\n        stacker_train = np.zeros((self.X.shape[0], 1))\n        stacker_test=np.zeros(self.test_X.shape[0])\n        \n        for index, (trn_idx,val_idx) in enumerate(self.folds.split(self.X)):\n            trn_x, val_x = self.X[trn_idx], self.X[val_idx]\n            trn_y, val_y = self.y[trn_idx], self.y[val_idx]\n            print('Train model in fold {}'.format(index+1))              \n                \n            cat_model = CatBoostRegressor(**params)\n            cat_model.fit(trn_x,(trn_y),eval_set=(val_x,(val_y)),use_best_model=True,verbose=False)# np.log1p\n            val_pred = (cat_model.predict(val_x))#np.expm1\n            stacker_train[val_idx,0]=val_pred\n            val_rmse=rmse(val_y, val_pred)            \n            print('fold {} RMSE score on VAL is {:.6f}'.format(index+1, val_rmse))\n            #for test\n            pred_test=(cat_model.predict(self.test_X))\n            stacker_test+=(pred_test/self.N)\n            \n        #evaluate for entire train data (oof)\n        train_rmse=rmse(self.y,stacker_train)\n        print(\"CV score on TRAIN (OOF) is RMSE: {}\".format(train_rmse))   \n        return stacker_test,stacker_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\nNumber_of_folds = 5\n#We have to make sure same K fold splits are used for all Models. This avoids Overfitting and Leakage\nfolds = KFold(n_splits=Number_of_folds, shuffle=True, random_state=2017)\n\nX_train=train[model_cols]\ny_train=train['fare_amount']\nX_test=test[model_cols]\n\nmodelling_object = Modelling(X=X_train.values, y=y_train.values, test_X=X_test.values, folds=folds, N=Number_of_folds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Level 1 RF","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nrf_params = {'n_estimators': 200,\n              'max_features': 'sqrt', #, 'sqrt','auto'\n              #'criterion':  'gini', #'entropy',\n              'max_depth': 40,\n              'min_samples_leaf': 15,\n            # 'min_samples_split':5,\n            # 'class_weight':'balanced',\n             'random_state':0,\n             'n_jobs': -1,\n             'oob_score': True\n            }\n\ntest_pred_stacked_rf,stacker_train_rf=modelling_object.SingleRF_oof(params=rf_params)\n#All validation scores (for each folds) come 3.34-3.92\n#TRAIN (OOF) is RMSE: 3.61\n#LB score 3.46\n#Wall time: 39min 21s","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results=pd.DataFrame({'key':test['key'],'fare_amount':test_pred_stacked_rf})\nresults.to_csv('/kaggle/working/test_pred_stacked_rf.csv',index=False)\n\nresults_train=pd.DataFrame({'Model_fare_amount':stacker_train_rf[:,0]})\nresults_train.to_csv('/kaggle/working/stacker_train_rf.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XGB Level 1","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n#Call XGB\nparams_for_xgb = {\n    'objective': 'reg:squarederror',  #the learning task and the corresponding learning objective\n    'eval_metric': 'rmse',            #Evaluation metrics for validation data\n    'eta': 0.04,          #learning_rate          \n    'max_depth': 10,       #Maximum depth of a tree. High will make the model more complex and more likely to overfit.\n    'min_child_weight': 5, #[0,inf] Higher the value,lesser the number of splits\n    'gamma': 0.0,       #Minimum loss reduction required to make a further partition on a leaf node of the tree    \n    'colsample_bytree': 0.6,  #subsample ratio of columns when constructing each tree\n    'alpha': 1,  #L1 regularization term on weights\n    'lambda': 5,  \n    'subsample':1.0, #'subsample': 0.8,    #Subsample ratio of the training instances\n    'seed': 2017}\n\ntest_pred_stacked_xgb,stacker_train_xgb=modelling_object.SingleXGB_oof(params=params_for_xgb,num_boost_round=1000)\n\n#All validation scores (for each folds) come between 3.18-3.80 \n#OOF score on train is 3.48\n#LB score 3.93\n#1h 35min 58s","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results=pd.DataFrame({'key':test['key'],'fare_amount':test_pred_stacked_xgb})\nresults.to_csv('/kaggle/working/test_pred_stacked_xgb.csv',index=False)\n\nresults_train=pd.DataFrame({'Model_fare_amount':stacker_train_xgb[:,0]})\nresults_train.to_csv('/kaggle/working/stacker_train_xgb.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LGBM","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nlgbm_params= {#\"max_depth\": 5,          #max depth for tree model\n              \"learning_rate\" : 0.02,\n    'eval_metric': 'rmse', \n    'objective': 'regression',\n              #\"num_leaves\": 25,        #max number of leaves in one tree\n              # 'feature_fraction':0.6,  #LightGBM will randomly select part of features on each tree node\n               'bagging_fraction':0.8,    #randomly select part of data without resampling\n              # 'max_drop': 5,         #used only in dart,max number of dropped trees during one boosting iteration\n               'lambda_l1': 5,\n               'lambda_l2': 0.5,\n              'min_child_samples':400,  #minimal number of data in one leaf\n                'max_bin':20, #max number of bins that feature values will be bucketed in. Higher value--> Overfitting\n                'subsample':0.6,  #randomly select part of data without resampling\n                'colsample_bytree':0.8, #same as feature_fraction\n               'boosting_type': 'gbdt',   #options are dart,gbdt(gradientboosting decision trees), rf,dart,goss\n               'task': 'train'}  #weight of labels with positive class\n\ntest_pred_stacked_lgbm,stacker_train_lgbm=\\\nmodelling_object.SingleLGBM_oof(params=lgbm_params,num_boost_round=1000,colnames=X_train.columns,importance_plot=True)\n#All validation scores (for each folds) come between 3.40-3.97\n#LB score around 3.35\n# time 4 minutes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results=pd.DataFrame({'key':test['key'],'fare_amount':test_pred_stacked_lgbm})\nresults.to_csv('/kaggle/working/test_pred_stacked_lgbm.csv',index=False)\n\nresults_train=pd.DataFrame({'Model_fare_amount':stacker_train_lgbm[:,0]})\nresults_train.to_csv('/kaggle/working/stacker_train_lgbm.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n#Catboost\nimport catboost\nfrom catboost import CatBoostRegressor\ncat_params= {\n    'iterations':1000,\n    'learning_rate':0.004,\n   'depth':5,\n    'eval_metric':'RMSE',\n    'colsample_bylevel':0.8,\n    'random_seed' : 2017,\n    'bagging_temperature' : 0.2,\n    'early_stopping_rounds':200\n} \ntest_pred_stacked_cat,stacker_train_cat=\\\nmodelling_object.SingleCatBoost_oof(params=cat_params)\n#All validation scores (for each folds) come around 3.72-4.24\n#LB score 3.72\n#Wall time: 10min 29s","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results=pd.DataFrame({'key':test['key'],'fare_amount':test_pred_stacked_cat})\nresults.to_csv('/kaggle/working/test_pred_stacked_cat.csv',index=False)\n\nresults_train=pd.DataFrame({'Model_fare_amount':stacker_train_cat[:,0]})\nresults_train.to_csv('/kaggle/working/stacker_train_cat.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Level 2 Stacking","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"columns=['catboost','xgb','lgbm','rf']\ntrain_pred_df_list=[stacker_train_cat,stacker_train_xgb, stacker_train_lgbm, stacker_train_rf]\ntest_pred_df_list=[test_pred_stacked_cat,test_pred_stacked_xgb,test_pred_stacked_lgbm,test_pred_stacked_rf]\nlv1_train_df=pd.DataFrame(columns=columns)\nlv1_test_df=pd.DataFrame(columns=columns)\nfor i in range(len(columns)):\n    lv1_train_df[columns[i]]=train_pred_df_list[i][:,0]\n    lv1_test_df[columns[i]]=test_pred_df_list[i]\n    \nlv1_train_df['Y']=y_train.values #add the dependendt variable to training","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lv1_train_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lv1_train_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#LGBM Level 2\nl2_modelling_object = Modelling(X=lv1_train_df.drop('Y',axis=1).values, y=lv1_train_df['Y'].values, \\\n                                test_X=lv1_test_df.values, folds=folds, N=5)\n\ntest_pred_stacked_lgbm_L2,stacker_train_lgbm_L2=\\\nl2_modelling_object.SingleLGBM_oof(params=lgbm_params,num_boost_round=10000,colnames=columns,importance_plot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results=pd.DataFrame({'key':test['key'],'fare_amount':test_pred_stacked_lgbm_L2})\nresults.to_csv('/kaggle/working/test_pred_stacked_lgbm_L2.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#XGB L2\ntest_pred_stacked_xgb_L2,stacker_train_xgb_L2=l2_modelling_object.SingleXGB_oof(params_for_xgb,1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results=pd.DataFrame({'key':test['key'],'fare_amount':test_pred_stacked_xgb_L2})\nresults.to_csv('/kaggle/working/test_pred_stacked_xgb_L2_final.csv',index=False)\n#from kaggle\n# from IPython.display import FileLinks\n# FileLinks('.')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}