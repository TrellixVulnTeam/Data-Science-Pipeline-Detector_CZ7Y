{"cells":[{"metadata":{},"cell_type":"markdown","source":"## New York Taxi fare Prediction\n\n- Decision Tree, Random Forest and XGBoost Regression technique to predict the taxi fare\n- Train data is huge so we will be taking only sample of data (100k records) for building the model\n- Result will improve if increase the train size and do some Hyperparameter tuning with cross validation. Due to memory issue in Kaggle environment, chose to go with only 100k records.\n\n## Steps Taken to build the model\n- Load the data / Cleanup the data\n- Feature Engineering\n- Exploratory Data Anaysis\n- Univariate and Bivariate Anaysis\n- Distribution of data\n- Decision Tree for Predict the taxi fare\n- Random Forest for Predicting the taxi fare\n- XGBoost for Predicting the taxi fare\n\n## Result\n- Descision Tree: 77.9 % accuracy\n- Random Forest: 78.4 % accuracy\n- XGBoost: 85.46% accuracy\n\n## Final Result: XGBoost accuracy is 85.46% is much higher than Random Forest and Decision Tree which proves that XGBoost is the best in predicting the New York Taxi fares\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.tree import DecisionTreeRegressor\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import GridSearchCV\n\nimport xgboost as xgb\nfrom sklearn.metrics import mean_squared_error\n\nfrom sklearn.ensemble import StackingRegressor","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"## Load 100k rows only\ndata = pd.read_csv(\"/kaggle/input/new-york-city-taxi-fare-prediction/train.csv\", nrows=100_000, parse_dates=['pickup_datetime'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data.shape)\nprint(data.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fare Amount Distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"data[data.fare_amount<100].fare_amount.hist(bins=100, figsize=(14,3))\nplt.xlabel('fare $USD')\nplt.title('Histogram');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Calculate the distance between two GPS location\n- actual lat long are not useful for modeling\n- we will calculate the distance between two points"},{"metadata":{"trusted":true},"cell_type":"code","source":"from math import sin, cos, sqrt, atan2, radians\n\ndef calculateDistance(lt1, ln1, lt2, ln2):\n\n    # approximate radius of earth in km\n    R = 6373.0\n\n    lat1 = radians(lt1)\n    lon1 = radians(ln1)\n    lat2 = radians(lt2)\n    lon2 = radians(ln2)\n\n    dlon = lon2 - lon1\n    dlat = lat2 - lat1\n\n    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n\n    distance = R * c * 1000\n    \n    return distance","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering\n- pickup_datetime will not help much in feature selection\n- We can extract weekday and pickup_time from the pickup_datetime which will be very good feature for prediction\n- Weekday will tell which day has peak day in the month\n- Pickup Time will tell which is a peak hour in a day\n\n## Data Cleanup\n- Remove the rows which have fare amount as negative which doesn't make sense\n- Remove the rows which have distance as <=0\n- Also we will remove all the rows which have nan values"},{"metadata":{"trusted":true},"cell_type":"code","source":"def featureCleanup(dfOrig, train = True):\n    if(train):\n        df = dfOrig[dfOrig['fare_amount'] >= 0]\n    else:\n        df = dfOrig.copy()\n        \n    df['weekday'] = df['pickup_datetime'].dt.day_name()\n    df['pickup_hour'] = df['pickup_datetime'].dt.hour\n    df['pickup_time'] = df['pickup_datetime'].dt.hour + df['pickup_datetime'].dt.minute/60\n    \n    df['distance'] = df.apply(lambda x: \n                              calculateDistance(x['pickup_latitude'], \n                                                x['pickup_longitude'],\n                                                x['dropoff_latitude'],\n                                                x['dropoff_longitude']), \n                              axis=1)\n    \n    df.drop(columns = ['pickup_latitude','pickup_longitude','dropoff_latitude','dropoff_longitude','pickup_datetime','key'], \n          inplace = True)\n    \n    if(train):\n        df.dropna(\n            axis=0,\n            how='any',\n            thresh=None,\n            subset=None,\n            inplace=True\n        )\n\n        df = df[df['distance'] > 0]\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainData = featureCleanup(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainData.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plotChart(df, x, y, title, num):\n    plt.subplot(5, 2, num)\n    sns.lineplot(data = df, x= x, y = y)\n    plt.title(title)\n    #plt.xticks(rotation = 90)\n    plt.legend(loc='upper right')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"## Bivariate Analysis\n- Generally taxi fares are expensive on Sundays \n- Generally people are travelling on Sundays or Wednesday (wednesday has max distance because of an outlier)\n- Taxi fare is maximum during 2AM - 4AM. Midnight Charges ?\n- Outlier causing issue with distance vs fare distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize  = (15,30))\nplotChart(trainData.groupby(by=\"weekday\").mean().reset_index(), 'weekday', 'fare_amount', 'weekday vs fare', 1)\nplotChart(trainData.groupby(by=\"weekday\").mean().reset_index(), 'weekday', 'distance', 'weekday vs distance', 2)\nplotChart(trainData.groupby(by=\"pickup_hour\").mean().reset_index(), 'pickup_hour', 'fare_amount', 'hour vs fare', 3)\nplotChart(trainData.groupby(by=\"distance\").mean().reset_index(), 'distance', 'fare_amount', 'distance vs fare', 4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Univariate Analysis\n- Single passenger Taxi hire has maximum trend, hiring taxi from office to home ?\n- Thurday, Friday and Saturday has maximum taxi hiring count\n- Moderate hour is from 9:00 AM to 5:00 PM\n- Peak hour is from 6:00PM to 9:00PM, leaving from office to home ?"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize  = (20,40))\nfor i in enumerate(trainData.columns.drop(['fare_amount', 'distance', 'pickup_time'])):\n    plt.subplot(10, 2, i[0]+1)\n    sns.countplot(trainData[i[1]])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Outliers Detection\n- There are outliers in the dataset but it will not impact on ML models based on decision tree.\n- Outlier detection and treatment are not required here."},{"metadata":{},"cell_type":"markdown","source":"### Convert Weekday names with numeric numbers\n- ML models always look for numbers not String values so converting weeknames to weeknumber."},{"metadata":{"trusted":true},"cell_type":"code","source":"trainData.drop(columns=['pickup_hour'], inplace=True)\ntrainData['weekday'] = trainData['weekday'].map({\"Monday\": 1, \"Tuesday\": 2, \"Wednesday\": 3, \"Thursday\": 4, \"Friday\": 5, \"Saturday\": 6, \"Sunday\": 7})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = trainData.pop('fare_amount')\nX_train = trainData","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Decision Tree"},{"metadata":{},"cell_type":"markdown","source":"## Hyperparameter Tuning for Decision Trees\n- Max depth need to be set in order to avoid over fitting\n- select multiple max_depth from 4 to 10 to identity the best max_depth\n- Other parameters like max_sample_split etc can also be set, but its taking a lot of time to fit the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n    'max_depth': [4,5,6,7,8,9,10]\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Grid Search Cross validation Technique\n- We don't have to split the data (train, test) into two parts because test data is provided seperately\n- So I am using Cross validation technique to validate the model with random validation set\n- cv=4 means 3 part will be used for traning and 1 part will be used for cross validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Instantiate the grid search model\n\ndt = DecisionTreeRegressor(random_state=100)\n\ngrid_search = GridSearchCV(estimator=dt, param_grid = params, \n                          cv=4, n_jobs=-1, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search.fit(X_train, y_train)\ngrid_search.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Decision Tree Result\n- Decision tree has predicted the data with 77.9% accuracy"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_predict = grid_search.predict(X_train)\nprint(\"Decision Tree Accuracy:\", round(r2_score(y_train, y_train_predict)*100, 2), \"%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest"},{"metadata":{},"cell_type":"markdown","source":"## Hyperparameter Tuning for Random Forest\n- Number of estimator used 50\n- max_depth used 6 to 8 to identify the best depth of the trees\n- max_feature used from 2 to 4 to identify best number of features\n- We can iterate this based on the results and tune the hyperparameter futher to get the optimal values"},{"metadata":{"trusted":true},"cell_type":"code","source":"rfEstimator = RandomForestRegressor(random_state=42)\npara_grids = {\n            \"n_estimators\" : [50],\n            \"max_depth\": [6,7,8],\n            'max_features': [2,3,4]\n        }","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random forest will create 45 different trees for training the model and will use best tree for prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_rf = GridSearchCV(rfEstimator, para_grids, verbose=1, n_jobs=-1, cv=5)\ngrid_rf.fit(X_train, y_train)\ngrid_rf.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_rf = grid_rf.predict(X_train)\nprint(\"Random Forest Accuracy:\", round(r2_score(y_train, y_train_pred_rf)*100, 2), \"%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random forest accuracy is 78.41 slighly better than Decision Tree Regressor 77.9%\n"},{"metadata":{},"cell_type":"markdown","source":"# XGBoost (Extreme Gradient Boosting)\n- This is the best Machine learning Algorithm in today's world\n- The concept of using 100s of weak learner to create a strong learner which makes it special\n- Also it is much much faster than Random Forest and Decision Tree because it is leveraging parallel computations"},{"metadata":{"trusted":true},"cell_type":"code","source":"xg_reg = xgb.XGBRegressor(n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xg_reg.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\n\ny_train_pred_xg = xg_reg.predict(X_train)\ny_train_pred_xg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"XGBoost Accuracy:\", round(r2_score(y_train, y_train_pred_xg)*100, 2), \"%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XGBoost accuracy 85.46% is much higher than Random Forest which proves that XGBoost is the best in predicting the New York Taxi fares"},{"metadata":{},"cell_type":"markdown","source":"# Hyperparameter tuning for XGBoost\n- Note: XGBoost already has inbuilt hyperparameter tuning but we can test it further with cross validation\n- we will try to see if we tune different parameter, do we get the better results or not\n- This Hyperparameter tuning might take around 6-7 mins because it is training with around 72 XGBoost Trees to find the best estimator"},{"metadata":{"trusted":true},"cell_type":"code","source":"para_grids = {\n            \"n_estimators\": [100,200],\n            \"learning_rate\": [0.3,0.4,0.5],\n            \"max_depth\": [6,7,8]\n        }\n\ngrid_xg = GridSearchCV(xg_reg, para_grids, verbose=1, n_jobs=-1, cv=4)\ngrid_xg.fit(X_train, y_train)\ngrid_xg.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_xg_cv = grid_xg.predict(X_train)\ny_train_pred_xg_cv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"XGBoost Accuracy after Hyperparameter tuning:\", round(r2_score(y_train, y_train_pred_xg_cv)*100, 2), \"%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Accuracy didn't change even after Hyperparameter tuning, that means XGBoost is really predicting well with high level of accuracy on its own."},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb.plot_tree(grid_xg.best_estimator_,num_trees=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Using Stacking Regressor to check if it improves the accracy\n- use Random Forest and XGBoost together to predict"},{"metadata":{"trusted":true},"cell_type":"code","source":"base_learners = [\n                 ('es1', xg_reg),\n                 ('es2', grid_rf.best_estimator_)     \n                ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stregr = StackingRegressor(estimators=base_learners, cv=4,n_jobs=1,verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stregr.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_predict_stack_reg = stregr.predict(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Accuracy:\", round(r2_score(y_train, y_predict_stack_reg)*100, 2), \"%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Accuracy goes little down if Random Forest and XGBoost stacked together."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predict the taxi fare for Test Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(\"/kaggle/input/new-york-city-taxi-fare-prediction/test.csv\", parse_dates=['pickup_datetime'])\n\ntestData = featureCleanup(test, False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testData.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testData.drop(columns=['pickup_hour'], inplace=True)\ntestData['weekday'] = testData['weekday'].map({\"Monday\": 1, \"Tuesday\": 2, \"Wednesday\": 3, \"Thursday\": 4, \"Friday\": 5, \"Saturday\": 6, \"Sunday\": 7})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_pred_xg_cv = grid_xg.predict(testData)\ny_test_pred_xg_cv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['fare_amount_predicted'] = y_test_pred_xg_cv\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}