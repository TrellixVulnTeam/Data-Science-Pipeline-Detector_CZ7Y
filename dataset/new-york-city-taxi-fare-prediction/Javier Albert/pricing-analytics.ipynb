{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Before we look into the data, what do we know about taxi fares in NYC?"},{"metadata":{},"cell_type":"markdown","source":"We can get the fare rules from here http://home.nyc.gov/html/tlc/html/passenger/taxicab_rate.shtml\n\nThere was a change in the fare rules on the 30/09/2012"},{"metadata":{},"cell_type":"markdown","source":"## Before 05/09/2012 (in USD)"},{"metadata":{},"cell_type":"markdown","source":"**Normal Trip:**\nInitial charge: 2  \nSurcharge Improvement: 0.30  \nSurcharge from 8pm to 6am every day: 0.50  \nSurcharge from 4pm to 8pm on weekdays (exc. holidays): 1.00  \nSurcharge for dropoff location (NYC, Nassau, Suffolk, Westchester, Rockland, Dutchess, Orange, Putnam): 0.50  \n\nNo surcharge for extra pasengers or bags.  \nReceipt includes payments for bridges and tunnels.  \n\nMeter: 0.40 per 1/5 mile or 0.40 per 60 seconds when vehicle is stopped.  \n\n**Airport Trip:**  \nTo-From La Guardia: Normal Trip  \nTo-From JFK and any location in NYC excluding Manhattan: Normal Trip To/From JFK and any location in Manhattan: 45 + 4.5 if from 4pm to 8pm on weekdays (exc. holidays) + tolls  \nTo Newark: Normal trip + 15  "},{"metadata":{},"cell_type":"markdown","source":"## After 05/09/2012 (in USD)"},{"metadata":{},"cell_type":"markdown","source":"**Normal Trip:**  \nInitial charge: 2.50  \nSurcharge Improvement: 0.30  \nSurcharge from 8pm to 6am every day: 0.50  \nSurcharge from 4pm to 8pm on weekdays (exc. holidays): 1.00  \nSurcharge for dropoff location (NYC, Nassau, Suffolk, Westchester, Rockland, Dutchess, Orange, Putnam): 0.50  \n\nNo surcharge for extra pasengers or bags.  \nReceipt includes payments for bridges and tunnels.  \n\nMeter: 0.50 per 1/5 mile or 0.50 per 60 seconds when vehicle is stopped.  \n\n**Airport Trip:**  \nTo-From La Guardia: Normal Trip  \nTo-From JFK and any location in NYC excluding Manhattan: Normal Trip To/From JFK and any location in Manhattan: 52.8 + 4.5 if from 4pm to 8pm on weekdays (exc. holidays) + tolls  \nTo Newark: Normal trip + 17.5  "},{"metadata":{},"cell_type":"markdown","source":"# Import stuff"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport itertools\nimport math\nfrom math import radians\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#Visualization\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use('seaborn-whitegrid')\nimport plotly.graph_objs as go\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport seaborn as sns\nsns.set_style(\"darkgrid\")\nimport folium\nimport folium.plugins\nfrom folium.plugins import MarkerCluster\nfrom folium.plugins import FastMarkerCluster\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n\n# Using datashader\n#Import Libraries\nfrom bokeh.models import BoxZoomTool\nfrom bokeh.plotting import figure, output_notebook, show\nimport datashader as ds\nfrom datashader.bokeh_ext import InteractiveImage\nfrom functools import partial\nfrom datashader.utils import export_image\nfrom datashader.colors import colormap_select, Hot, inferno, Elevation\nfrom datashader import transfer_functions as tf\noutput_notebook()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fae3e4b66cb7d1d8a4364bb707efcae4e10ffd10"},"cell_type":"markdown","source":"# Step 1: Data reading and enrichment"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":false,"_kg_hide-output":false},"cell_type":"code","source":"def readData(path, types, chunksize, chunks):\n\n    df_list = []\n    counter = 1\n    \n    for df_chunk in tqdm(pd.read_csv(path, usecols=list(types.keys()), dtype=types, chunksize=chunksize)):\n\n        # The counter helps us stop whenever we want instead of reading the entire data\n        if counter == chunks+1:\n            break\n        counter = counter+1\n\n        # Neat trick from https://www.kaggle.com/btyuhas/bayesian-optimization-with-xgboost\n        # Using parse_dates would be much slower!\n        df_chunk['date'] = pd.to_datetime(df_chunk['pickup_datetime'].str.slice(0,10))\n        df_chunk['pickup_datetime'] = df_chunk['pickup_datetime'].str.slice(0, 16)\n        df_chunk['pickup_datetime'] = pd.to_datetime(df_chunk['pickup_datetime'])\n\n        # Process the datetime and get hour of day and day of week\n        # After Price Reform - Before Price Reform ('newRate')\n        df_chunk['hour'] = df_chunk['pickup_datetime'].apply(lambda x: x.hour)\n        df_chunk['weekday'] = df_chunk['pickup_datetime'].apply(lambda x: x.weekday())\n        df_chunk['newRate'] = df_chunk['pickup_datetime'].apply(lambda x: True if x > pd.Timestamp(2012, 9, 30, 10) else False)\n        \n        # Aappend the chunk to list\n        df_list.append(df_chunk) \n\n    # Merge all dataframes into one dataframe\n    df = pd.concat(df_list)\n\n    # Delete the dataframe list to release memory\n    del df_list\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We run the function with \"chunksnumberTrain = 1\" meaning that we only read 1 million datapoints. This is just to make it faster to run, we can use 5 or 10 to make the models more accurate as they have more data but it takes much longer to run."},{"metadata":{"_kg_hide-input":false,"trusted":true,"_uuid":"1bea35bcbdf212511c2d046e9ea28a74897a3751"},"cell_type":"code","source":" # The path where the Training set is\nTRAIN_PATH = '../input/train.csv'\n\n# The datatypes we want to pass the reading function\ntraintypes = {'fare_amount': 'float32',\n              'pickup_datetime': 'str', \n              'pickup_longitude': 'float32',\n              'pickup_latitude': 'float32',\n              'dropoff_longitude': 'float32',\n              'dropoff_latitude': 'float32',\n              'passenger_count': 'float32'}\n\n# The size of the chunk for each iteration\nchunksizeTrain = 1_000_000\n\n# The number of chunks we want to read\nchunksnumberTrain = 3\n\ndf = readData(TRAIN_PATH, traintypes, chunksizeTrain, chunksnumberTrain)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"993d922e247011cd4880d7c6c929387adea7403c"},"cell_type":"markdown","source":"# Step 2: Data cleaning\n\n1) Drop the rows with null values  \n2) Drop rows with fare_amount less than 2.5 USD  \n3) Drop rows with fare_amount above 400 USD  \n4) Drop rows with passenger_count outside the range 1 to 6  \n5) Drop rows with geolocations not close to NYC    "},{"metadata":{"trusted":true,"_uuid":"d1b7aa39cac155e015bbc1feaa60f3b641a53ed1","_kg_hide-input":false},"cell_type":"code","source":"# 1) Drop NaN\ndf.dropna(how = 'any', axis = 'rows', inplace = True)\n\n# 2) 3) Drop fares below 2.5 USD or above 400 USD\ndf = df[df['fare_amount']>=2.5]\ndf = df[df['fare_amount']<400]\n    \n# 4) Drop passenger count below 1 or above 6\ndf = df[(df['passenger_count']>=1) & (df['passenger_count']<=6)] \n    \n# 5) Drop rides outside NYC\nminLon = -74.3\nmaxLon = -73.7\nminLat = 40.5\nmaxLat = 41\n\ndf = df[df['pickup_latitude'] < maxLat]\ndf = df[df['pickup_latitude'] > minLat]\ndf = df[df['pickup_longitude'] < maxLon]\ndf = df[df['pickup_longitude'] > minLon]\n\ndf = df[df['dropoff_latitude'] < maxLat]\ndf = df[df['dropoff_latitude'] > minLat]\ndf = df[df['dropoff_longitude'] < maxLon]\ndf = df[df['dropoff_longitude'] > minLon]\n\n# Reset Index\ndf.reset_index(inplace=True, drop=True)\n\n# Convert datatype to categorical\ndf['hour'] = pd.Categorical(df['hour'])\ndf['weekday'] = pd.Categorical(df['weekday'])\ndf['passenger_count'] = pd.Categorical(df['passenger_count'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cdaf7daf9488dafab8eba367e646db85df2f822b"},"cell_type":"markdown","source":"Let's have a quick look at how many data we loss while cleaning"},{"metadata":{"trusted":true,"_uuid":"071c45348025339dbe03a683f99b26bf0a92583e","_kg_hide-input":false},"cell_type":"code","source":"trace = go.Pie(values = [df.shape[0],chunksizeTrain*chunksnumberTrain - df.shape[0]],\n               labels = [\"Useful data\" , \"Data loss due to missing values or other reasons\"],\n               marker = dict(colors = ['skyblue' ,'yellow'], line = dict(color = \"black\", width =  1.5)),\n               rotation  = 60,\n               hoverinfo = 'label+percent',\n              )\n\nlayout = go.Layout(dict(title = 'Data Cleaning (percentage of data loss)',\n                        plot_bgcolor  = \"rgb(243,243,243)\",\n                        paper_bgcolor = \"rgb(243,243,243)\",\n                        showlegend=False\n                       )\n                  )\n\nfig = go.Figure(data=[trace],layout=layout)\npy.iplot(fig)\nfig = go.Figure(data=[trace],layout=layout)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5fb0f59eb279de436651c789bc5847f9d93613ac"},"cell_type":"markdown","source":"# Step 3: Data Exploration"},{"metadata":{},"cell_type":"markdown","source":"- Draw all pickup locations using Bokeh and Folium"},{"metadata":{"_kg_hide-input":false,"trusted":true,"_uuid":"1497301fc16838c7a6a7b86559f27d9e3bffb9b8"},"cell_type":"code","source":"# Define plotting function using Datashader\ndef plot_data_points(longitude,latitude,data_frame) :\n    export  = partial(export_image, export_path=\"export\", background=\"black\")\n    fig = figure(background_fill_color = \"black\")    \n    cvs = ds.Canvas(plot_width=800, \n                    plot_height=600,\n                    x_range=(-74.15,-73.75), \n                    y_range=(40.6,40.9))\n    agg = cvs.points(data_frame,longitude,latitude)\n    #img = tf.shade(agg, cmap=Hot, how='eq_hist')\n    img = tf.shade(agg)   \n    image_xpt = tf.dynspread(img, threshold=0.5, max_px=4)\n    return export(image_xpt,'map')\n\n# Call function and plot\nplot_data_points('pickup_longitude', 'pickup_latitude', df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1bbe3814a25b3bd9bde9cbcc8c05929e7d4e6125"},"cell_type":"code","source":"# Let's look at some clusters with Folium (20000 points)\nsamples = df.sample(n=min(20000,df.shape[0]))\nm = folium.Map(location=[np.mean(samples['pickup_latitude']), np.mean(samples['pickup_longitude'])], zoom_start=11)\nFastMarkerCluster(data=list(zip(samples['pickup_latitude'], samples['pickup_longitude']))).add_to(m)\nfolium.LayerControl().add_to(m)\nm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Draw pickup and dropoff locations on airport"},{"metadata":{"trusted":true},"cell_type":"code","source":"# One end has to be JFK\njfk_lat_min = 40.626777\njfk_lat_max = 40.665599\njfk_lon_min = -73.823964\njfk_lon_max = -73.743085\n\n# Filter trips originating on JFK\ndf_fromJFK = df[(df['pickup_latitude']<jfk_lat_max)&\n            (df['pickup_latitude']>jfk_lat_min)&\n            (df['pickup_longitude']<jfk_lon_max)&\n            (df['pickup_longitude']>jfk_lon_min)]\n\n# Filter trips ending on JFK\ndf_toJFK = df[(df['dropoff_latitude']<jfk_lat_max)&\n           (df['dropoff_latitude']>jfk_lat_min)&\n           (df['dropoff_longitude']<jfk_lon_max)&\n           (df['dropoff_longitude']>jfk_lon_min)]\n\nm1 = folium.Map(location=[40.645580, -73.785115], zoom_start=16)\nsamples = df_fromJFK.sample(n=min(500,df_fromJFK.shape[0]))\nfor lt, ln in zip(samples['pickup_latitude'], samples['pickup_longitude']):\n            folium.Circle(location = [lt,ln] ,radius = 2, color = 'blue').add_to(m1)\n            \nsamples = df_toJFK.sample(n=min(500,df_toJFK.shape[0]))\nfor lt, ln in zip(samples['dropoff_latitude'], samples['dropoff_longitude']):\n            folium.Circle(location = [lt,ln] ,radius = 2, color = 'red').add_to(m1)\n        \nm1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Filter rides inside Manhattan using polygons"},{"metadata":{"trusted":true},"cell_type":"code","source":"from shapely.geometry import Point\nfrom shapely.geometry.polygon import Polygon\n\n# Define polygon using coordinates (Just took them from Google Maps by clicking on the map)\nlats_vect = [40.851638, 40.763022, 40.691262, 40.713380, 40.743944, 40.794344, 40.846332]\nlons_vect = [-73.952423, -74.010418, -74.026685, -73.972200, -73.962051, -73.924073, -73.926454]\nlons_lats_vect = np.column_stack((lons_vect, lats_vect))\npolygon = Polygon(lons_lats_vect)\n\n# Plot the polygon using Folium\nman_map = folium.Map(location=[40.7631, -73.9712], zoom_start=12)\nfor i in range(0,6):\n    folium.PolyLine(locations=[[lats_vect[i],lons_vect[i]], [lats_vect[i+1],lons_vect[i+1]]], color='blue').add_to(man_map)\nfolium.PolyLine(locations=[[lats_vect[6],lons_vect[6]], [lats_vect[0],lons_vect[0]]], color='blue').add_to(man_map)\nman_map","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for every point on df_train if it belongs to polygon or not\nmanhattanRides = df[df[['pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude']]\n                          .apply(lambda row: ((polygon.contains(Point(row['pickup_longitude'],row['pickup_latitude']))) &\n                                              (polygon.contains(Point(row['dropoff_longitude'],row['dropoff_latitude'])))), axis=1)]\n\n# Plot the remaining dataset 'manhattanRides'\nplot_data_points('pickup_longitude', 'pickup_latitude', manhattanRides)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Percentage of trips that happen inside Manhattan: ' + str(np.around(100*(manhattanRides.shape[0])/df.shape[0],2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Calculate distance and add it as a feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Simple Euclidean Distance calculator \ndef quickDist(lat1, lng1, lat2, lng2):\n    lng1, lat1, lng2, lat2 = map(radians, [lng1, lat1, lng2, lat2])\n    R = 6371\n    x = (lng2 - lng1) * np.cos(0.5*(lat2+lat1))\n    y = lat2 - lat1\n    d = R * np.sqrt(x*x + y*y)\n    return d\n\n# Longitude distance (use same Euclidean distance function with fixed latitude)\ndef latDist(lat1, lng1, lat2, lng2):\n    uno = quickDist((lat1+lat2)/2, lng1, (lat1+lat2)/2, lng2)\n    return uno\n\n# Calculate real distance (Manhattan distance with 29 degrees to north)\ndef realManDist(lat1, lng1, lat2, lng2):\n    flightDist = quickDist(lat1, lng1, lat2, lng2)\n    latDistance = latDist(lat1, lng1, lat2, lng2)\n    if flightDist == 0:\n        ret = np.nan\n    else:\n        th = np.arccos(latDistance/flightDist)\n        ata = flightDist*np.cos(th-0.506) + flightDist*np.sin(th-0.506)\n        bta = flightDist*np.cos(th+0.506) + flightDist*np.sin(th+0.506)\n        ret = max(ata,bta)\n    return ret","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate distance for every ride on manhattanRides\nmanhattanRides['distance'] = manhattanRides.apply(lambda row: realManDist(row['pickup_latitude'], \n                                                                          row['pickup_longitude'], \n                                                                          row['dropoff_latitude'], \n                                                                          row['dropoff_longitude']), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop rows with distance = NaN\nmanhattanRides.dropna(inplace=True)\n\n# Drop rows with distance below 0.3 (300 meters)\nmanhattanRides = manhattanRides[manhattanRides['distance']>=0.3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Download manhattanRides dataset as csv\nmanhattanRides.to_csv('manhattanRides.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From now on we use the dataset that contains the Manhattan rides only"},{"metadata":{},"cell_type":"markdown","source":"- Plot histogram for the fare_amount, devided in 3 sections so it's easier to see what is going on\n\n1) Up to 40 USD (most of the rides)  \n2) From 40 USD to 70 USD (rides including airports)  \n3) Above 70 USD (expensive rides)"},{"metadata":{"trusted":true,"_uuid":"aae571c9cee11f17a5b7b0ffebce1a17e209ebe8","_kg_hide-input":false},"cell_type":"code","source":"# Define how we want to split the data into sections based on 'fare_amount'\na = 40\nb = 70\nc = 300\n\n# Plot normalized histogram for each section\nplt.figure(figsize = (25,7))\nplt.subplot(1,3,1)\nplt.title('Below ' + str(a) + ' USD',color = \"b\")\nplt.ylabel('Normalized Density')\nsns.distplot(manhattanRides[manhattanRides['fare_amount']<=a]['fare_amount'], norm_hist=True, bins=np.arange(0,a))\nplt.subplot(1,3,2)\nplt.title('From ' + str(a) + ' USD to ' + str(b) + ' USD',color = \"b\")\nplt.ylabel('Normalized Density')\nsns.distplot(manhattanRides[(manhattanRides['fare_amount']>a)&(df['fare_amount']<=b)]['fare_amount'], norm_hist=True, bins=np.arange(a,b))\nplt.subplot(1,3,3)\nplt.title('From ' + str(b) + ' USD to ' + str(c) + ' USD',color = \"b\")\nplt.ylabel('Normalized Density')\nsns.distplot(manhattanRides[(manhattanRides['fare_amount']>b)&(df['fare_amount']<=c)]['fare_amount'], norm_hist=True, bins=np.arange(b,c));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" - Calculate mean and median of the fare_amount before and after the fare change"},{"metadata":{"trusted":true,"_uuid":"fbf40edf9d4843cca710f51b08bfbe40b4e2f57d","_kg_hide-input":false},"cell_type":"code","source":"# Split df_train into a dataset of the rides before the fare rules change and after the fare rules change\ndf_before = manhattanRides[manhattanRides['newRate']==False]\ndf_after = manhattanRides[manhattanRides['newRate']==True]\nprint('Mean fare BEFORE rate change: ' + str(np.around(df_before['fare_amount'].mean(),2)))\nprint('Mean fare AFTER rate change: ' + str(np.around(df_after['fare_amount'].mean(),2)))\nprint('Median fare BEFORE rate change: ' + str(np.around(df_before['fare_amount'].median(),2)))\nprint('Median fare AFTER rate change: ' + str(np.around(df_after['fare_amount'].median(),2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Plot mean fare_amount grouped by hour\n- Plot mean fare_amount grouped by weekday\n- Plot mean fare_amount grouped by passenger_count"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (30,10))\nplt.subplot(1,3,1)\nplt.title('Mean fare_amount by hour of the day',color = \"b\")\nax = sns.barplot(x='hour',y='fare_amount', data = manhattanRides, edgecolor=\".1\", errcolor = 'red')\nplt.subplot(1,3,2)\nplt.title('Mean fare_amount by day of the week',color = \"b\")\nax = sns.barplot(x='weekday',y='fare_amount', data = manhattanRides, edgecolor=\".1\", errcolor = 'red')\nplt.subplot(1,3,3)\nplt.title('Mean fare_amount by passenger_count',color = \"b\")\nax = sns.barplot(x='passenger_count',y='fare_amount', data = manhattanRides, edgecolor=\".1\", errcolor = 'red')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ca4790f4c0932576295518cecff012eac2c73cec"},"cell_type":"markdown","source":"- Plot time series for the fare_amount with moving average"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sorted = manhattanRides.sort_values('pickup_datetime')\ndf_sorted.reset_index(inplace=True, drop=True)\nmvavg = df_sorted.rolling(window=2000, on='pickup_datetime')['fare_amount'].mean()\nmvavg_pd = pd.DataFrame(columns=['avg', 'pickup_datetime'])\nmvavg_pd['avg'] = mvavg\nmvavg_pd['pickup_datetime'] = df_sorted['pickup_datetime']\n\ndf_sorted.plot('pickup_datetime','fare_amount', figsize=(30,10), title='fare_amount time series')\nmvavg_pd.plot('pickup_datetime','avg', figsize=(30,10), title='fare_amount moving average');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Analyze the cents value distribution before and after the fare change (notice how after the fare change only .00 or .50 is possible)"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (20,7))\nplt.subplot(1,2,1)\nplt.title('Old Rate, cents value',color = \"b\")\nsns.distplot(np.mod(df_before['fare_amount'],1)) \nplt.subplot(1,2,2)\nplt.title('New Rate, cents value',color = \"g\")\nsns.distplot(np.mod(df_after['fare_amount'],1), color='green');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 4: fare_amount VS distance, regression models"},{"metadata":{},"cell_type":"markdown","source":"- Let's first plot the fare_amount as a function of the distance, we see that of course the fare_amount increases as the distance increases but it doesn't look like we have a very good linear relationship. There are also \"weird lines\" at specific fare amounts, at the same values that we saw on the histogram."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the 'fare_amount' against the distance of the trip\nplt.figure(figsize = (20,15))\nplt.title('Manhattan Rides', color = \"b\")\nplt.ylabel('Fare in USD')\nplt.xlabel('Distance in Km')\nplt.scatter(manhattanRides['distance'], manhattanRides['fare_amount'], alpha=0.5);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Anyway we will try to fit a linear model and see what happens. We will first do a train/test split and then we will also split the dataset into \"before the fare change\" and \"after the fare change\" in order to fit a linear model to each of them."},{"metadata":{},"cell_type":"markdown","source":"What do we expect? Based on the fare rules the relationship between the fare and the distance is 0.40 USD per 1/5 mile before the change and 0.50 USD per 1/5 mile after the change. If we translate that to kilometers we get that the relationship is 1.243 USD per kilometer before the change and 1.553 USD per kilometer after the change. Let's see what our linear regression gives us (we should look at the slope of the regression): "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split train/test\nfrom sklearn.model_selection import train_test_split\nmanhattanRides_train, manhattanRides_test = train_test_split(manhattanRides, test_size=0.2, random_state=42)\n\n# Split before and after\nmanhattanRides_train_before = manhattanRides_train[manhattanRides_train['newRate']==0]\nmanhattanRides_train_after = manhattanRides_train[manhattanRides_train['newRate']==1]\nmanhattanRides_test_before = manhattanRides_test[manhattanRides_test['newRate']==0]\nmanhattanRides_test_after = manhattanRides_test[manhattanRides_test['newRate']==1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting a linear model and measuring the MSE\nimport statsmodels.api as sm\n# Define function that makes regression and returns params\ndef measureMSE(df_train, df_test):\n    regression = sm.OLS(df_train['fare_amount'], sm.add_constant(df_train['distance'])).fit()\n    farepred = regression.predict(sm.add_constant(df_test['distance'])) \n    mse = np.around(np.sqrt((((df_test['fare_amount']-farepred)**2).sum())/(df_test.shape[0])),4)\n    return [regression.params[1], regression.params[0], mse]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply function on manhattanRides_train\nreg_before = measureMSE(manhattanRides_train_before, manhattanRides_test_before)\nreg_after = measureMSE(manhattanRides_train_after, manhattanRides_test_after)\n\nprint('Before fare rule change:')\nprint ('Slope: ' + str(np.around(reg_before[0],2)))\nprint ('Intercept: ' + str(np.around(reg_before[1],2)))\nprint ('RMSE: ' + str(np.around(reg_before[2],4)))\n\nprint(' ')\n\nprint('After fare rule change:')\nprint ('Slope: ' + str(np.around(reg_after[0],2)))\nprint ('Intercept: ' + str(np.around(reg_after[1],2)))\nprint ('RMSE: ' + str(np.around(reg_after[2],4)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That is really great. By doing this simple regression we manage to overcome the noise, the traffic, the overcharges and all the other variables that affect the final fare and get a slope (fare dependency on distance) that is pretty close to the fare rules."},{"metadata":{},"cell_type":"markdown","source":"# Step 5: Use the other features to make a better model"},{"metadata":{"_uuid":"126125320e3b28fdff47dda7db531a71bba591b8"},"cell_type":"markdown","source":"Our features to make the model:\n- hour (categorical)\n- weekday (categorical)\n- passenger_count (categorical)\n- distance (continuous)\n\nOur target:\n- fare_amount (continuous)"},{"metadata":{},"cell_type":"markdown","source":"We will just run a Gradient Boosting Machine for each of the datasets (before and after) without too much tuning or optimization. We see that in both cases the mean RMSE is better than the one we obtained using the linear regression. This could mean that Gradient Boosting performs better than linear regression for this task and/or that the other features contain information and that they help us predict the fare."},{"metadata":{"trusted":true},"cell_type":"code","source":"import h2o\nh2o.init();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"h20_train_before = h2o.H2OFrame(manhattanRides_train_before)\nh20_test_before = h2o.H2OFrame(manhattanRides_test_before)\nh20_train_after = h2o.H2OFrame(manhattanRides_train_after)\nh20_test_after = h2o.H2OFrame(manhattanRides_test_after)\n\n# Define feature space and label space\nmyCat = ['hour', 'weekday']\nmyNum = ['distance', 'passenger_count']\nmyResponse = 'fare_amount'\n\nfor i in myCat:\n    h20_train_before[i] = h20_train_before[i].asfactor()\n    h20_test_before[i] = h20_test_before[i].asfactor()\n    h20_train_after[i] = h20_train_after[i].asfactor()\n    h20_test_after[i] = h20_test_after[i].asfactor()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from h2o.estimators.gbm import H2OGradientBoostingEstimator\nmodel_before = H2OGradientBoostingEstimator(nfolds=5, seed=42, stopping_metric = \"MSE\")\nmodel_after = H2OGradientBoostingEstimator(nfolds=5, seed=42, stopping_metric = \"MSE\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_before.train(x=myNum+myCat,y=myResponse, training_frame=h20_train_before, validation_frame=h20_test_before)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_after.train(x=myNum+myCat,y=myResponse, training_frame=h20_train_after, validation_frame=h20_test_after)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Before fare rule change:')\nprint ('RMSE: ' + model_before.cross_validation_metrics_summary().cell_values[5][1])\n\nprint(' ')\n\nprint('After fare rule change:')\nprint ('RMSE: ' + model_after.cross_validation_metrics_summary().cell_values[5][1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's now take a look at the feature importance to see what besides the distance is helping us with the predictions. We see that the distance is the most important feature by far, accounting for >95% of the information. The next important feature in both models is the hour of the day, after that the day of the week and finally the number of passengers, which makes total sense with our intuition."},{"metadata":{"trusted":true},"cell_type":"code","source":"model_before.varimp_plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_after.varimp_plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 6: Demand model"},{"metadata":{},"cell_type":"markdown","source":"Formulate the demand function model. To do this we group our data by day, and we compute the average price per kilometer paid on that day and the total amount of kilometers sold that day. That way we have a product called “a kilometer of taxi ride” where the price of the product changes every day and the total amount of units sold also changes everyday. \n\nThere is an important difference to what we’ve done so far until now, since before this step we computed the “price per kilometer” by fitting a regression and allowing for a constant (intercept of the regression) to absorb all the other things that the final price of the ride is dependent on (initial charge, waiting time, etc). We just looked at the coefficient of the price per kilometer feature and saw that it’s consistent with the fare rules.\n\nHere instead, we look at every taxi ride as if the ONLY thing that determines the final price is the amount of kilometers driven. We can picture this as if the owner of the taxi company is evaluating a new price policy were only the distance traveled determines the fare of the ride, nothing else. He wants to know what the price per kilometer should be under this policy, and what is the elasticity of his customers to changes on the price per kilometer."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_demand = manhattanRides[['fare_amount', 'date', 'hour', 'distance']]\ndf_demand['fare/distance'] = df_demand['fare_amount']/df_demand['distance']\ndf_demand = df_demand[['date', 'distance', 'fare/distance']]\ndf_demand = df_demand.groupby('date').agg({'fare/distance':'mean', 'distance':'sum'})\ndf_demand.reset_index(inplace=True)  \ndf_demand.columns = ['date', 'mean', 'sum']\ndf_demand.plot('date', 'mean', figsize=(30,10), alpha=0.5, title='Average price per kilometer per day');\ndf_demand.plot('date', 'sum', figsize=(30,10), alpha=0.5, title='Amount of kilometers sold per day')\ndf_demand.plot.scatter('mean', 'sum', figsize=(30,10), alpha=0.5, title='Amount of kilometers sold VS price per kilometer');\nsns.regplot(x=\"mean\", y=\"sum\", data=df_demand)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The regression coefficients are:\nprint('Slope: ' + str(np.around(sm.OLS(df_demand['sum'], sm.add_constant(df_demand['mean'])).fit().params[1], 4)))\nprint('Intersect: ' + str(np.around(sm.OLS(df_demand['sum'], sm.add_constant(df_demand['mean'])).fit().params[0], 4)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The first graph is the prices of our pruduct. We as taxi company owners changed the price per kilometer a little bit every day to see what happens. Also, on a specific point in time we started to be more expensive but still changing the price a little bit every day.\n\nThe second graph is the amount of kilometers we sold every day. It's quite noisy and not much can be appreciated with the nude eye. It seems to be that after we decided to increase the price per kilometer the number of kilometers sold started to go down a bit.\n\nThe third graph is the relationship between the number of kilometers sold and the price of the kilometer. We see that there is a slight trend down, meaning that the more expensive our product is the less we sell. But it also seems like customers have a lot of eleasticity, the demand doesn't drop that hard when increasing prices. We can see this by looking on how small the slope is compared to the intersect.\n\nWe see that most of the datapoints are around 3 to 4 USD, meaning that \"the taxi company owner\" guessed that if he were to charge based on travelled distance ONLY he should charge about 3 or 4 USD per kilometer, and he is doing this experiment to see how does it affect the demand. *** This is a nice story to convince myself, haha ***"},{"metadata":{},"cell_type":"markdown","source":"We know that given a linear model, the price were revenue is maximized (the optimal price) can be obtained by doing  \np_opt = - inters****ect/(2*slope)"},{"metadata":{},"cell_type":"markdown","source":"In our case that would give p_opt around 14 USD... It doesn't make sense of course to charge 14 USD per kilometer for a taxi ride, and that is because it doesn't make sense to fit our demand model to a linear regression. Maybe it's linear on some segment of the price, but expecting it to stay linear while we keep raising and raising the price doesn't make sense."},{"metadata":{},"cell_type":"markdown","source":"### Online learning from data generated by the linear model and random noise"},{"metadata":{},"cell_type":"markdown","source":"First we will run the algorithm we used for the homework. We know the \"true\" values of alpha and beta based on the regression that we did before so we can run and see what happens (it works, we get the optimal price and the values of alpha and beta as expected). "},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\n\n# Initial Parameters\nalpha = sm.OLS(df_demand['sum'], sm.add_constant(df_demand['mean'])).fit().params[0]\nbeta = -sm.OLS(df_demand['sum'], sm.add_constant(df_demand['mean'])).fit().params[1]\ninitial_p = 2\nsigma = 1\n\n# Model\ndef model(p):\n    val = alpha - beta*p + np.random.normal(0,1)\n    return val\n\n# Regression function\ndef regression(p_list, D_list):\n    reg = LinearRegression(fit_intercept=False).fit(p_list, D_list)\n    return reg.coef_\n\n# Performance accummulator\ndef performance(perf_last,p_val):\n    return perf_last - (beta-p_val*(alpha-beta*p_val))\n\nfinal_p = []\nfinal_alpha = []\nfinal_beta = []\nperf_record = []\nnum_iterations = 1500\nnum_expriments = 100\n\nfor j in range(num_expriments):\n\n    # Initialize\n    p_list = [[1,initial_p], [1,2*initial_p]]\n    D_list = [model(initial_p), model(2*initial_p)]\n    coef = regression(p_list, D_list)\n    alpha_list = [coef[0]]\n    beta_list = [-coef[1]]\n    perf_metric = [0]\n  \n    # Run\n    for i in range(num_iterations):\n        p_list.append([1,alpha_list[-1]/(2*beta_list[-1])])\n        D_list.append(model(alpha_list[-1]/(2*beta_list[-1])))\n        coef = regression(p_list, D_list)\n        alpha_list.append(coef[0])\n        beta_list.append(-coef[1])\n        if i>=2:\n            perf_metric.append(performance(perf_metric[-1],p_list[-1][1])) \n \n    # Save final price, alpha and beta of iteration\n    final_p.append(p_list[-1][1])\n    final_alpha.append(alpha_list[-1])\n    final_beta.append(beta_list[-1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Price path\ndata=pd.DataFrame([x[1] for x in p_list[3:1000]])\ndata.plot(figsize=(10,5), alpha=0.5, title='Price path');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Alpha path\ndata=pd.DataFrame([x for x in alpha_list[3:1000]])\ndata.plot(figsize=(10,5), alpha=0.5, title='Alpha path');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Beta path\ndata=pd.DataFrame([x for x in beta_list[3:1000]])\ndata.plot(figsize=(10,5), alpha=0.5, title='Beta path');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Price histogram\nax = sns.distplot(final_p)\nax.set(xlabel='price', ylabel='count', title='Final Price histogram');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Alpha histogram\nax = sns.distplot(final_alpha)\nax.set(xlabel='alpha', ylabel='count', title='Final Alpha histogram');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Beta histogram\nax = sns.distplot(final_beta)\nax.set(xlabel='beta', ylabel='count', title='Final Beta histogram');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Performance metric\ndata=pd.DataFrame(perf_metric)\ndata.plot(figsize=(10,5), alpha=0.5, title='Performance Metric - commulative error');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Online learning by querying from real dataset"},{"metadata":{},"cell_type":"markdown","source":"Now we can do something more interesting. In the last algorithm everytime we want to generate a new {price,demand} pair to recalculate our alpha and beta we ask a linear model with noise to generate that pair (we give the price we want and it returns the demand). But that seems strange when we have a realistic dataset, why would we run an online algorithm on simulated data coming from a linear model that was fitted on the original data? Why don't we run an online algorithm that uses {price,demand} pairs from the original dataset?  \n\nThat seems more like a real life situation. At every time step the online algorithm decides that he needs the demand value of a speficic price, we then as business owners change the price to the price the algorithm told us and let the market tell us what is the demand. That way we construct {price,demand} pairs in a real online learning fashion.  \n\nAlso, we waw that the algorithm above decides that the optimal price is about 14 per kilometer. It seems strange, we don't even have a single point of data above 6 USD to check if it makes sense. This happens because the algorithm runs on simulated data from a linear model that has no boundaries, it can return a demand value for any price that you give it, even negative prices. If we instead use the real dataset to get the pairs and set the necesarry boundaries we can force the algorithm to find the optimal price inside the range of the experiment. That also makes more sense as business owners.\n\nSo how can we do that? Everytime the algorithm asks us for the demand of a given price that he decides we go the the original dataset (equivalent to \"go check the market\" in real life) and find the demand of the price that is closest to the requested price. For example if the algorithm asks us what is the demand for price 2.265 USD we might not have that information, so we return the closest pair {2.27,1245}. That way the algorithm has the most similar information to what he needed. In the case that the algorithm ask us for the demand of a price that is below the minimum price or above the maximum price we just return him a random {price,demand} pair.  \n\nThis way the algorithm should arrive to the same alpha and beta conclusions without going through a linear model that looks at all the data first. Also, it should arrive to the optimal price INSIDE the range of the dataset, not some extreme value like 14 USD per kilometer. Disclaimer: The fact that we get 14 USD and not something more reasonable is because our dataset doesn't really fit a linear model, and even when we force it to fit it we get a super inelastic demand function."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\nimport random\n\n# Initial Parameters\nalpha = sm.OLS(df_demand['sum'], sm.add_constant(df_demand['mean'])).fit().params[0]\nbeta = -sm.OLS(df_demand['sum'], sm.add_constant(df_demand['mean'])).fit().params[1]\ninitial_p = 2\n\ndf_demand_sorted = df_demand.sort_values(by='mean', ascending=True)\nminPrice = df_demand_sorted['mean'].min()\nmaxPrice = df_demand_sorted['mean'].max()\n\n# Function that returns the closes price to the queryed one and the demand for that query \ndef model(price):\n    if ((price < maxPrice) & (price > minPrice)):\n        idx = df_demand_sorted['mean'].sub(price).abs().idxmin()\n        return [df_demand_sorted.loc[idx]['mean'], df_demand_sorted.loc[idx]['sum']]\n    else:\n        rand_price = random.uniform(minPrice, maxPrice)\n        return model(rand_price)\n\n# Regression function\ndef regression(p_list, D_list):\n    reg = LinearRegression(fit_intercept=False).fit(p_list, D_list)\n    return reg.coef_\n\n# Performance accummulator\ndef performance(perf_last,p_val):\n    return perf_last - (beta-p_val*(alpha-beta*p_val))\n\nfinal_p = []\nfinal_alpha = []\nfinal_beta = []\nperf_record = []\n\nnum_iterations = 1500\nnum_expriments = 100\n\nfor j in range(num_expriments):\n  \n    # Initialize\n    p_list = [[1,initial_p], [1,2*initial_p]]\n    D_list = [model(initial_p)[1], model(2*initial_p)[1]]\n    coef = regression(p_list, D_list)\n    alpha_list = [coef[0]]\n    beta_list = [-coef[1]]\n    perf_metric = [0]\n\n    # Run\n    for i in range(num_iterations):\n        query = alpha_list[-1]/(2*beta_list[-1])\n        vals = model(query)\n        p_list.append([1,vals[0]])\n        D_list.append(vals[1])\n        coef = regression(p_list, D_list)\n        alpha_list.append(coef[0])\n        beta_list.append(-coef[1])\n        if i>=2:\n            perf_metric.append(performance(perf_metric[-1],p_list[-1][1]))\n      \n    # Save final price, alpha and beta of iteration\n    final_p.append(p_list[-1][1])\n    final_alpha.append(alpha_list[-1])\n    final_beta.append(beta_list[-1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Price path\ndata=pd.DataFrame([x[1] for x in p_list[3:1000]])\ndata.plot(figsize=(10,5), alpha=0.5, title='Price path');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Alpha path\ndata=pd.DataFrame([x for x in alpha_list[3:1000]])\ndata.plot(figsize=(10,5), alpha=0.5, title='Alpha path');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Beta path\ndata=pd.DataFrame([x for x in beta_list[3:1000]])\ndata.plot(figsize=(10,5), alpha=0.5, title='Beta path');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Price histogram\nax = sns.distplot(final_p)\nax.set(xlabel='price', ylabel='count', title='Final Price histogram');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Alpha histogram\nax = sns.distplot(final_alpha)\nax.set(xlabel='alpha', ylabel='count', title='Final Alpha histogram');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Beta histogram\nax = sns.distplot(final_beta)\nax.set(xlabel='beta', ylabel='count', title='Final Beta histogram');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Performance metric\ndata=pd.DataFrame(perf_metric)\ndata.plot(figsize=(10,5), alpha=0.5, title='Performance Metric - commulative error');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It may seem like the plots of the online algorithm based on the data from the linear model simulation are much better and smoother than the ones from online algorithm from real data. But of course, in the first case we had to go though the entire dataset to get the linea model coefficients and then we simulate data coming from a linear model with perfect gaussian noise with mean 0 and variance 1, a very unrealistic situation. On the second case instead we never went though the entire dataset, with just a well chosen sample we get to similar values of alpha and beta."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}