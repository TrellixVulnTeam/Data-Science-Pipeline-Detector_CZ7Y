{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport datetime as dt\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\nimport os","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df =  pd.read_csv('/kaggle/input/new-york-city-taxi-fare-prediction/train.csv', nrows = 6_000_000) #1M to test models\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cleaning\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Clean dataset\ndef clean_df(df):\n    return df[(df.fare_amount > 0) & \n            (df.pickup_longitude > -80) & (df.pickup_longitude < -70) &\n            (df.pickup_latitude > 35) & (df.pickup_latitude < 45) &\n            (df.dropoff_longitude > -80) & (df.dropoff_longitude < -70) &\n            (df.dropoff_latitude > 35) & (df.dropoff_latitude < 45) &\n            (df.passenger_count > 0) & (df.passenger_count < 10)]\n\ntrain_df = clean_df(train_df)\nprint(len(train_df))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Feature engineering","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def sphere_dist(pickup_lat, pickup_lon, dropoff_lat, dropoff_lon):\n    \n\n    R_earth = 6371\n\n    pickup_lat, pickup_lon, dropoff_lat, dropoff_lon = map(np.radians,\n                                                             [pickup_lat, pickup_lon, \n                                                              dropoff_lat, dropoff_lon])\n    #Compute distances along lat, lon dimensions\n    dlat = dropoff_lat - pickup_lat\n    dlon = dropoff_lon - pickup_lon\n    \n    #Compute haversine distance\n    a = np.sin(dlat/2.0)**2 + np.cos(pickup_lat) * np.cos(dropoff_lat) * np.sin(dlon/2.0)**2\n    \n    return 2 * R_earth * np.arcsin(np.sqrt(a))\n\ndef add_airport_dist(dataset):\n\n    jfk_coord = (40.639722, -73.778889)\n    ewr_coord = (40.6925, -74.168611)\n    lga_coord = (40.77725, -73.872611)\n    \n    pickup_lat = dataset['pickup_latitude']\n    dropoff_lat = dataset['dropoff_latitude']\n    pickup_lon = dataset['pickup_longitude']\n    dropoff_lon = dataset['dropoff_longitude']\n    \n    pickup_jfk = sphere_dist(pickup_lat, pickup_lon, jfk_coord[0], jfk_coord[1]) \n    dropoff_jfk = sphere_dist(jfk_coord[0], jfk_coord[1], dropoff_lat, dropoff_lon) \n    pickup_ewr = sphere_dist(pickup_lat, pickup_lon, ewr_coord[0], ewr_coord[1])\n    dropoff_ewr = sphere_dist(ewr_coord[0], ewr_coord[1], dropoff_lat, dropoff_lon) \n    pickup_lga = sphere_dist(pickup_lat, pickup_lon, lga_coord[0], lga_coord[1]) \n    dropoff_lga = sphere_dist(lga_coord[0], lga_coord[1], dropoff_lat, dropoff_lon) \n    \n    dataset['jfk_dist'] = pd.concat([pickup_jfk, dropoff_jfk], axis=1).min(axis=1)\n    dataset['ewr_dist'] = pd.concat([pickup_ewr, dropoff_ewr], axis=1).min(axis=1)\n    dataset['lga_dist'] = pd.concat([pickup_lga, dropoff_lga], axis=1).min(axis=1)\n    \n    return dataset\n    \ndef add_datetime_info(dataset):\n    #Convert to datetime format\n    dataset['pickup_datetime'] = pd.to_datetime(dataset['pickup_datetime'],format=\"%Y-%m-%d %H:%M:%S UTC\")\n    \n    dataset['hour'] = dataset.pickup_datetime.dt.hour\n    dataset['day'] = dataset.pickup_datetime.dt.day\n    dataset['month'] = dataset.pickup_datetime.dt.month\n    dataset['weekday'] = dataset.pickup_datetime.dt.weekday\n    dataset['year'] = dataset.pickup_datetime.dt.year\n    \n    return dataset\n\ntrain_df = add_datetime_info(train_df)\ntrain_df = add_airport_dist(train_df)\ntrain_df['distance'] = sphere_dist(train_df['pickup_latitude'], train_df['pickup_longitude'], \n                                   train_df['dropoff_latitude'] , train_df['dropoff_longitude'])\n\ntrain_df.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.drop(columns=['key', 'pickup_datetime'], inplace=True)\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train_df['fare_amount']\ntrain = train_df.drop(columns=['fare_amount'])\n\nx_train,x_test,y_train,y_test = train_test_split(train,y,random_state=0,test_size=0.01)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Cross-validation\nparams = {\n    # Parameters that we are going to tune.\n    'max_depth': 8, #Result of tuning with CV\n    'eta':.03, #Result of tuning with CV\n    'subsample': 1, #Result of tuning with CV\n    'colsample_bytree': 0.8, #Result of tuning with CV\n    # Other parameters\n    'objective':'reg:linear',\n    'eval_metric':'rmse',\n    'silent': 1\n}\n\n#Block of code used for hypertuning parameters. Adapt to each round of parameter tuning.\n#Turn off CV in submission\nCV=False\nif CV:\n    dtrain = xgb.DMatrix(train,label=y)\n    gridsearch_params = [\n        (eta)\n        for eta in np.arange(.04, 0.12, .02)\n    ]\n\n    # Define initial best params and RMSE\n    min_rmse = float(\"Inf\")\n    best_params = None\n    for (eta) in gridsearch_params:\n        print(\"CV with eta={} \".format(\n                                 eta))\n\n        # Update our parameters\n        params['eta'] = eta\n\n        # Run CV\n        cv_results = xgb.cv(\n            params,\n            dtrain,\n            num_boost_round=1000,\n            nfold=3,\n            metrics={'rmse'},\n            early_stopping_rounds=10\n        )\n\n        # Update best RMSE\n        mean_rmse = cv_results['test-rmse-mean'].min()\n        boost_rounds = cv_results['test-rmse-mean'].argmin()\n        #print(\"\\tRMSE {} for {} rounds\".format(mean_rmse, boost_rounds))\n        if mean_rmse < min_rmse:\n            min_rmse = mean_rmse\n            best_params = (eta)\n\n    print(\"Best params: {}, RMSE: {}\".format(best_params, min_rmse))\nelse:\n    #Print final params to use for the model\n    params['silent'] = 0 #Turn on output\n    print(params)\n    \n    \n    \ndef XGBmodel(x_train,x_test,y_train,y_test,params):\n    matrix_train = xgb.DMatrix(x_train,label=y_train)\n    matrix_test = xgb.DMatrix(x_test,label=y_test)\n    model=xgb.train(params=params,\n                    dtrain=matrix_train,num_boost_round=5000, \n                    early_stopping_rounds=10,evals=[(matrix_test,'test')])\n    return model\n\nmodel = XGBmodel(x_train,x_test,y_train,y_test,params)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Preprocess test data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Read and preprocess test set\ntest_df =  pd.read_csv('/kaggle/input/new-york-city-taxi-fare-prediction/test.csv')\ntest_df = add_datetime_info(test_df)\ntest_df = add_airport_dist(test_df)\ntest_df['distance'] = sphere_dist(test_df['pickup_latitude'], test_df['pickup_longitude'], \n                                   test_df['dropoff_latitude'] , test_df['dropoff_longitude'])\n\ntest_key = test_df['key']\nx_pred = test_df.drop(columns=['key', 'pickup_datetime'])\n\nprediction = model.predict(xgb.DMatrix(x_pred), ntree_limit = model.best_ntree_limit)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Output.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsubmission = pd.DataFrame({\n        \"key\": test_key,\n        \"fare_amount\": prediction.round(2)\n})\n\nsubmission.to_csv('taxi_fare_submission.csv',index=False)\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}