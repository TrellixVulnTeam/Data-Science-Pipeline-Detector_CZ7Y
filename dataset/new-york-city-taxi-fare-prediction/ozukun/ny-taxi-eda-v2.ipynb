{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nfrom sklearn.metrics import r2_score\nimport xgboost as xgb\nimport featuretools as ft\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# PANDA LIBRARY'SI  UZERINDEN   DATAFRAME OBJESI YARATARAK ILK 1M ROWU OKUYORUM\ndf_train=pd.read_csv(\"/kaggle/input/new-york-city-taxi-fare-prediction/train.csv\", sep=',', lineterminator='\\n',nrows=1000000)\ndf_train.rename(columns={'passenger_count\\r':'passenger_count'}, inplace=True)\nprint(\"df_train size \",df_train.info()) \ndf_train.info()\n\nprint( df_train.head() )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# IS NULL CHECK\nprint(\"NULL CHECK\")\nprint( df_train.columns[df_train.isnull().any()].tolist() )\n\nprint(\"UNIQUE CHECK\")\n# UNIQUE CHECK\nfor  d in df_train.columns:\n    print(d+\"  \"+str( df_train[d].is_unique ) )\n    \n    \nprint(\"LIST OF UNIQUE VALUES\")\nprint( \"passenger_count \"+str( df_train.passenger_count.unique() ) )\nprint( \"pickup_longitude  \"+str( df_train.pickup_longitude.unique() ) )\n\n\nprint(\"GET INDEX\")\nprint(df_train.index)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# general info\n\ndf_train[['fare_amount','passenger_count']].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# CHECK INVALID  DATA\nsns.relplot(x=\"passenger_count\", y=\"fare_amount\", data=df_train);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  FILTER AND RECHECK\nsns.relplot(x=\"passenger_count\", y=\"fare_amount\", data=df_train[(df_train.passenger_count <20) & (df_train.passenger_count>0)]);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#FARE AMOUNT KONTOLLERI\nsns.boxplot(x=df_train['fare_amount'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#FARE AMOUNT KONTOLLERI\ndf_train_2=df_train[(df_train.fare_amount<15) & (df_train.fare_amount>0)]\nsns.boxplot(x=df_train_2['fare_amount'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# HEAT MAP CORRELATION ILE ILGILI\nplt.figure(figsize=(15,7))\nsns.heatmap(df_train.corr(),annot=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# feauture engineering\n# ENLEM VE BOYLAM KONTROLLERI\ndef add_travel_vector_features(df):\n    df['abs_diff_longitude'] = (df.dropoff_longitude - df.pickup_longitude).abs()\n    df['abs_diff_latitude'] = (df.dropoff_latitude - df.pickup_latitude).abs()\n\nadd_travel_vector_features(df_train)\n\nsns.relplot(x=\"abs_diff_latitude\", y=\"abs_diff_longitude\", data=df_train);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#CLEAN DATA  \"abs_diff_latitude\", y=\"abs_diff_longitude\",\n\ndef clean_df(df):\n    return df[\n              (df.fare_amount<15) & (df.fare_amount>0) & \n              (df.abs_diff_latitude<5) & (df.abs_diff_longitude<5) &\n              (df.passenger_count > 0) & (df.passenger_count < 10)\n             ]\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_2 = clean_df(df_train).dropna()\nprint(\"Clean data completed\")\ndf_train_2.info()\n# HEAT MAP CORRELATION ILE ILGILI\nplt.figure(figsize=(15,7))\nsns.heatmap(df_train_2.corr(),annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# feauture engineering\n# calculate distance\ndef  getDistance(lat1,lon1,lat2,lon2):\n    R = 6373.0\n    \n    dlon = lon2 - lon1\n    dlat = lat2 - lat1\n    a = (np.sin(dlat/2))**2 + np.cos(lat1) * np.cos(lat2) * (np.sin(dlon/2))**2\n    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n    distance = R * c\n    return distance\n\ndf_train_2[\"distance\"]=getDistance(df_train_2.pickup_latitude,df_train_2.pickup_longitude,df_train_2.dropoff_latitude,df_train_2.dropoff_longitude)\n\n\n# HEAT MAP CORRELATION ILE ILGILI\nplt.figure(figsize=(15,7))\nsns.heatmap(df_train_2.corr(),annot=True)\n\n\n#create pickupdate workday or weekend\ndf_train_2['date_pickup']=pd.to_datetime(df_train_2['pickup_datetime'])\nWeekValue_dict = {0:0,1: 0, 2: 0, 3: 0, 4: 0,5: 1, 6: 1}\ndf_train_2['WeekValue'] = df_train_2['date_pickup'].dt.dayofweek.map(WeekValue_dict)\n \n# seperate month - day - time values\ndf_train_2['year'] = df_train_2['date_pickup'].dt.year\ndf_train_2['month'] = df_train_2['date_pickup'].dt.month\ndf_train_2['day'] = df_train_2['date_pickup'].dt.day\ndf_train_2['hour'] = df_train_2['date_pickup'].dt.hour\n\n\nday_int_dict = {6:0,7:0,8:0,9:0,10:0,11:0,12:1,13:1,14:2,15:2,16:2,17:2,18:2,0:4,1:4,2:4,3:4,4:4,5:4,19:3,\n20:3,21:3,22:3,23:3}\ndf_train_2['day_int'] = df_train_2['hour'].map(day_int_dict)\ndf_train_2['day_name'] = df_train_2['date_pickup'].dt.day_name()   \n\n\nseason_dict = {12:0,1: 0, 2: 0, 3: 1, 4: 1,5: 1, 6: 2, 7: 2, 8: 2,9: 3, 10: 3, 11: 3}\ndf_train_2['season'] = df_train_2['month'].map(season_dict)\n\n\n# encoding day_name char --> int\n\nlb_make = LabelEncoder()\ndf_train_2[\"day_name_num\"] = lb_make.fit_transform(df_train_2[\"day_name\"])\n\ndf_train_2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create  column to define day interval ( morning:0, noon:1 , afternoon:2 , evening:3 ,midnight:4)\n# create  column to define week value 0: workday 1:weekend\ng=sns.lineplot(x=\"day_int\", y=\"fare_amount\",hue='WeekValue',ci=None, data=df_train_2, estimator=np.sum)\n#put legends outside of graph\ng.legend(loc='center left', bbox_to_anchor=(1.25, 0.5), ncol=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create  column to define week value 0: workday 1:weekend\nsns.barplot(x=\"WeekValue\", y=\"fare_amount\", data=df_train_2, estimator=sum)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#day_int ( morning:0, noon:1 , afternoon:2 , evening:3 ,midnight:4)\ng=sns.barplot(x=\"day_int\", y=\"fare_amount\",hue='day_name',hue_order=['Monday', 'Tuesday', 'Wednesday','Thursday','Friday','Saturday','Sunday'],ci=None,data=df_train_2,estimator=np.sum)\n#put legends outside of graph\ng.legend(loc='center left', bbox_to_anchor=(1.25, 0.5), ncol=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#0 winter ,1 april , 2 summer,3 Autumn\ng=sns.barplot(x=\"season\", y=\"fare_amount\",hue='day_name',hue_order=['Monday', 'Tuesday', 'Wednesday','Thursday','Friday','Saturday','Sunday'],ci=None,data=df_train_2,estimator=np.sum)\n#put legends outside of graph\ng.legend(loc='center left', bbox_to_anchor=(1.25, 0.5), ncol=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# PREDICTION ALGORITHM\n\n\nchunk_list = [] \ndf_train_4 = pd.DataFrame(columns=['passenger_count', 'distance', 'WeekValue', 'year', 'day_int', 'season', 'day_name_num',\n        'abs_diff_longitude','abs_diff_latitude','fare_amount'])\n#Cross-validation \nparams ={\n    # Parameters that we are going to tune.\n    'n_estimators':4,\n    'max_depth':6, #Result of tuning with CV\n    'eta':0.05, #Result of tuning with CV\n    #'subsample': 1, #Result of tuning with CV\n    #'colsample_bytree': 0.8, #Result of tuning with CV\n    # Other parameters\n    #'objective':'reg:linear',\n    #'eval_metric':'rmse',\n    #'silent': 1\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef index_marks(nrows, chunk_size):\n    return range(1 * chunk_size, (nrows // chunk_size + 1) * chunk_size, chunk_size)\n\n\ndef split(dfm, chunk_size):\n    indices = index_marks(dfm.shape[0], chunk_size)\n    return np.split(dfm, indices)\n\nmodel = xgb.XGBRegressor(params=params)\nchunk_idx = 0\ndf_train_3 = pd.DataFrame()\nchunks = split(df_train_2, 50000)\nfor c in chunks:\n    \n    if chunk_idx > 0:\n        df_train_3 =  df_train_3.append(c)\n    else:\n        df_train_3=c\n        \n    print (len(df_train_3))\n    lister_4=['passenger_count', 'distance', 'WeekValue', 'year', 'day_int', 'season', 'day_name_num','abs_diff_longitude',\n       'abs_diff_latitude']\n    lister_5=['fare_amount']    \n    \n    X_train, X_test, y_train, y_test= train_test_split(df_train_3[lister_4],df_train_3[lister_5], test_size=0.1,\n    random_state=42)\n    \n    if chunk_idx > 0: # not load in first run\n        model.fit(X_train, y_train, xgb_model='model_1.model')\n        model.save_model('model_1.model')\n    else:\n        model.fit(X_train, y_train)\n        model.save_model('model_1.model')\n    chunk_idx = chunk_idx + 1\n    rmse = sqrt(mean_squared_error(y_test, model.predict(X_test)))\n    print(\"RMSE RESULT \",rmse)\n    # evaluate predictions\n    accuracy = r2_score(y_test, model.predict(X_test))\n    print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#AUTO FEATURE ENG.\ndf_train_2_1=df_train_2[:1000]\n# creating and entity set 'es'\nes = ft.EntitySet(id = 'All_taxi_fare')\n\n# adding a dataframe \nes.entity_from_dataframe(entity_id = 'taxi_all', dataframe = df_train_2_1, index = 'key')\n\n\nes.normalize_entity(base_entity_id='taxi_all', new_entity_id='tax_dates', index = 'pickup_datetime', \nadditional_variables = ['pickup_longitude', 'pickup_latitude','dropoff_longitude', 'dropoff_latitude', 'passenger_count'])\n\nprint(es)\n\n\nfeature_matrix, feature_names = ft.dfs(entityset=es, \ntarget_entity = 'taxi_all', \nmax_depth = 2, \nverbose = 1, \nn_jobs = 3)\n\n\nprint(feature_matrix.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_matrix.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# HEAT MAP CORRELATION ILE ILGILI\nplt.figure(figsize=(300,5))\nsns.heatmap(feature_matrix.corr().loc[['fare_amount'],:],annot=True)\n\n\n\nc_matrix=feature_matrix.corr()[\"fare_amount\"]\nfor  indexer,f in enumerate(c_matrix):\n    if f!=1 and f>=0.3:\n        print(c_matrix.index[indexer])\n        print(f)\n                ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# NORMAL DISTRIBUTION\n\nplt.hist(df_train.fare_amount, bins=10)\nplt.ylabel('frequency')\nplt.show()\n\n\nplt.hist(df_train_2.fare_amount, bins=10)\nplt.ylabel('frequency')\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}