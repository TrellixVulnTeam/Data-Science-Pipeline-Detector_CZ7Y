{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport pickle\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans, MiniBatchKMeans\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.model_selection import train_test_split\nimport lightgbm as lgb\nimport os\nfrom tqdm import tqdm\nfrom scipy.sparse import csr_matrix, hstack, vstack\nfrom sklearn.impute import SimpleImputer\nfrom sklearn import metrics\nfrom sklearn.preprocessing import LabelEncoder\n\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"814880d3ca25c4b465597a70f959bb262495c031"},"cell_type":"markdown","source":"Add weather data from: ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/daily/ \nFrom this, use daily maximum and minimum temperatures (encoded as binary 'extreme' days), average wind speed, precipitation, and snow (all as numeric)."},{"metadata":{"trusted":true,"_uuid":"a53258524d9d2596ac075d0568f06364af5e70f9"},"cell_type":"code","source":"nyc_weather = pd.read_csv('../input/nyc-weather/nyc_weather.csv')\nweather_cols = ['DATE','AWND','PRCP','SNOW','TMAX','TMIN']\nnyc_weather = nyc_weather[weather_cols].copy()\nnyc_weather['DATE'] = pd.to_datetime(nyc_weather['DATE'], utc=True, format='%m/%d/%Y') \nnyc_weather.head()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3507c1ddd6673bc27429c1144eccd29dfa484fc9"},"cell_type":"markdown","source":"The plot below shows the distribution of daily high and low temperatures. This highlights the temperatures that will be considered extreme. "},{"metadata":{"trusted":true,"_uuid":"c752156f88013fdda8b7dec3265ea317f098a957","_kg_hide-input":true},"cell_type":"code","source":"plt.rc('figure', figsize=(15, 8))\nplt.subplot(1,2,1)\nplt.hist(nyc_weather.TMAX, bins =  30)\nplt.xlabel('Temperature (C)')\nplt.ylabel('Frequency Count')\nplt.title('Max Daily Temperature')\nplt.subplot(1,2,2)\nplt.hist(nyc_weather.TMIN, bins =  30)\nplt.xlabel('Temperature (C)')\nplt.ylabel('Frequency Count')\nplt.title('Min Daily Temperature')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1bd8888a58eae0701a94f13b5d65521cd83448a2"},"cell_type":"code","source":"nyc_weather.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a545217025c4b896040ca67e64d9bd8e0f199b00"},"cell_type":"code","source":"holidays = pd.read_csv('../input/us-bank-holidays-20092018/US Bank Holidays 2012-2018.csv')\nholidays['Date'] = pd.to_datetime(holidays['Date'], utc=True, format='%m/%d/%y') \nholidays.head(12)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"caeea8f330ad6f2abc93364456ca48504242c143"},"cell_type":"markdown","source":"The training set has ~55M rows so I take a sample of 12M.  After cleaning up some formatting and changing the data types to improve efficiency, it's time to begin feature engineering.  I break the pickup times down into categorical features, including year, month, and day/hour combinations (e.g. Friday 5pm, Saturday 7am, etc.).  I join the weather data and use wind speed, precipitation and snow as numeric features. I encode the extreme temperature days as binary features. I calculate the distance between pickup and dropoff using the Haversine. Lastly, I add the various bank holidays as categorical features. \n\n*Hiding this code for ease of reading the kernel. "},{"metadata":{"trusted":true,"_uuid":"18367bc3b22817d1481854e58d179d5fb023ea09"},"cell_type":"code","source":"%%time\n#import sample of train and full test\nimport random\n\nn = sum(1 for line in open('../input/new-york-city-taxi-fare-prediction/train.csv')) - 1 #number of records in file (excludes header)\ns = 10000000 #desired sample size\nskip = sorted(random.sample(range(1,n+1),n-s)) #the 0-indexed header will not be included in the skip list\n\ntrain_full = pd.read_csv('../input/new-york-city-taxi-fare-prediction/train.csv', skiprows=skip) \ntest = pd.read_csv('../input/new-york-city-taxi-fare-prediction/test.csv') \ntest_id = test.key.values #set this value for final submission\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c58595c04be110d35aa83ca166b2ce0dfbacbf01"},"cell_type":"code","source":"train_one = train_full.iloc[:2500000,:]\ntrain_two = train_full.iloc[2500000:5000000,:]\ntrain_three = train_full.iloc[5000000:7500000,:]\ntrain_four = train_full.iloc[7500000:,:]\n\ntrain_one.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f59b6a8da69136f7cbd0afefb7c11360616664b4","_kg_hide-input":true},"cell_type":"code","source":"%%time\n\ntrain_sample = train_one.dropna()\n\n#truncate datetime string for efficiency converting to datetime format\ntrain_sample['pickup_datetime'] = train_sample.pickup_datetime.str.slice(0, 16)\ntest['pickup_datetime'] = test.pickup_datetime.str.slice(0, 16)\n\ntrain_sample['pickup_datetime'] = pd.to_datetime(train_sample.pickup_datetime, utc=True, format='%Y-%m-%d %H:%M') \ntest['pickup_datetime'] = pd.to_datetime(test.pickup_datetime, utc=True,format='%Y-%m-%d %H:%M') \n\n#get rid of unnecessary memory consuming column\ntrain_sample.drop(labels='key', axis=1, inplace=True)\ntest.drop(labels='key', axis=1, inplace=True)\n\n#convert data to less memory intensive types\ntrain_sample.loc[:,'passenger_count'] = train_sample.passenger_count.astype(dtype = 'uint8')\ntrain_sample['pickup_longitude'] = train_sample.pickup_longitude.astype(dtype = 'float32')\ntrain_sample['pickup_latitude'] = train_sample.pickup_latitude.astype(dtype = 'float32')\ntrain_sample['dropoff_longitude'] = train_sample.dropoff_longitude.astype(dtype = 'float32')\ntrain_sample['dropoff_latitude'] = train_sample.dropoff_latitude.astype(dtype = 'float32')\ntrain_sample['fare_amount'] = train_sample.fare_amount.astype(dtype = 'float32')\n\ntest['pickup_longitude'] = test.pickup_longitude.astype(dtype = 'float32')\ntest['pickup_latitude'] = test.pickup_latitude.astype(dtype = 'float32')\ntest['dropoff_longitude'] = test.dropoff_longitude.astype(dtype = 'float32')\ntest['dropoff_latitude'] = test.dropoff_latitude.astype(dtype = 'float32')\n\n#filter training set to be within full range of test set\ntrain_sample = train_sample.loc[train_sample.pickup_longitude.between(test.pickup_longitude.min(), test.pickup_longitude.max())]\ntrain_sample = train_sample.loc[train_sample.pickup_latitude.between(test.pickup_latitude.min(), test.pickup_latitude.max())]\ntrain_sample = train_sample.loc[train_sample.dropoff_longitude.between(test.dropoff_longitude.min(), test.dropoff_longitude.max())]\ntrain_sample = train_sample.loc[train_sample.dropoff_latitude.between(test.dropoff_latitude.min(), test.dropoff_latitude.max())]\n\n#convert timestamp to features to be used as categorial\ntrain_sample['hour'] = train_sample['pickup_datetime'].apply(lambda time: time.hour)\ntrain_sample['month'] = train_sample['pickup_datetime'].apply(lambda time: time.month)\ntrain_sample['day_of_week'] = train_sample['pickup_datetime'].apply(lambda time: time.dayofweek)\ntrain_sample['year'] = train_sample['pickup_datetime'].apply(lambda t: t.year)\n\n\ntest['hour'] = test['pickup_datetime'].apply(lambda time: time.hour)\ntest['month'] = test['pickup_datetime'].apply(lambda time: time.month)\ntest['day_of_week'] = test['pickup_datetime'].apply(lambda time: time.dayofweek)\ntest['year'] = test['pickup_datetime'].apply(lambda t: t.year)\n\n#reduce memory by converting datatypes\ntrain_sample['hour'] = train_sample.hour.astype(dtype = 'uint8')\ntrain_sample['month'] = train_sample.month.astype(dtype = 'uint8')\ntrain_sample['day_of_week'] = train_sample.day_of_week.astype(dtype = 'uint8')\ntrain_sample['year'] = train_sample.year.astype(dtype = 'uint16')\n\n\ntest['hour'] = test.hour.astype(dtype = 'uint8')\ntest['month'] = test.month.astype(dtype = 'uint8')\ntest['day_of_week'] = test.day_of_week.astype(dtype = 'uint8')\ntest['year'] = test.year.astype(dtype = 'uint16')\n\n\n# Join Weather data\ntrain_sample['pickup_day'] = train_sample.pickup_datetime.dt.floor('d')\ntrain_sample = train_sample.merge(nyc_weather, how = 'left', left_on ='pickup_day', right_on = 'DATE')\ntrain_sample.drop(columns = ['pickup_day','DATE'], axis = 0, inplace = True)\n\ntest['pickup_day'] = test.pickup_datetime.dt.floor('d')\ntest = test.merge(nyc_weather, how = 'left', left_on ='pickup_day', right_on = 'DATE')\ntest.drop(columns = ['pickup_day','DATE'], axis = 0, inplace = True)\n\ntrain_sample['AWND'] = train_sample.AWND.astype(dtype = 'float16')\ntrain_sample['PRCP'] = train_sample.PRCP.astype(dtype = 'float16')\ntrain_sample['SNOW'] = train_sample.day_of_week.astype(dtype = 'float16')\ntrain_sample['TMAX'] = train_sample.TMAX.astype(dtype = 'float16')\ntrain_sample['TMIN'] = train_sample.TMAX.astype(dtype = 'float16')\n\ntest['AWND'] = test.AWND.astype(dtype = 'float16')\ntest['PRCP'] = test.PRCP.astype(dtype = 'float16')\ntest['SNOW'] = test.day_of_week.astype(dtype = 'float16')\ntest['TMAX'] = test.TMAX.astype(dtype = 'float16')\ntest['TMIN'] = test.TMAX.astype(dtype = 'float16')\n\n#create weather features\n#extreme temps\ntrain_sample['hot_day'] = np.where(train_sample.TMAX >= 30,1,0)\ntrain_sample['cold_day'] = np.where(train_sample.TMIN <= 0,1,0)\ntest['hot_day'] =  np.where(test.TMAX >= 30,1,0)\ntest['cold_day'] = np.where(test.TMIN <= 0,1,0)\ntrain_sample['hot_day'] = train_sample.hot_day.astype(dtype = 'uint8')\ntrain_sample['cold_day'] = train_sample.cold_day.astype(dtype = 'uint8')\ntest['hot_day'] = test.hot_day.astype(dtype = 'uint8')\ntest['cold_day'] = test.cold_day.astype(dtype = 'uint8')\n\n#rain and snow\ntrain_sample['rainy_day'] = np.where(train_sample.PRCP >= 0,1,0)\ntrain_sample['snowy_day'] = np.where(train_sample.SNOW <= 0,1,0)\ntest['rainy_day'] =  np.where(test.PRCP >= 0,1,0)\ntest['snowy_day'] = np.where(test.SNOW <= 0,1,0)\ntrain_sample['rainy_day'] = train_sample.rainy_day.astype(dtype = 'uint8')\ntrain_sample['snowy_day'] = train_sample.snowy_day.astype(dtype = 'uint8')\ntest['rainy_day'] = test.rainy_day.astype(dtype = 'uint8')\ntest['snowy_day'] = test.snowy_day.astype(dtype = 'uint8')\n\n#windy days\ntrain_sample['windy_day'] = np.where(train_sample.AWND >= 0,1,0)\ntest['windy_day'] =  np.where(test.AWND >= 0,1,0)\ntrain_sample['windy_day'] = train_sample.windy_day.astype(dtype = 'uint8')\ntest['windy_day'] = test.windy_day.astype(dtype = 'uint8')\n\n\n#calculate distance between pickup and dropoff\ndef degree_to_radion(degree):\n    return degree*(np.pi/180)\n\ndef calculate_distance(pickup_latitude, pickup_longitude, dropoff_latitude, dropoff_longitude):\n    \n    from_lat = degree_to_radion(pickup_latitude)\n    from_long = degree_to_radion(pickup_longitude)\n    to_lat = degree_to_radion(dropoff_latitude)\n    to_long = degree_to_radion(dropoff_longitude)\n    \n    radius = 6371.01\n    \n    lat_diff = to_lat - from_lat\n    long_diff = to_long - from_long\n\n    a = np.sin(lat_diff / 2)**2 + np.cos(degree_to_radion(from_lat)) * np.cos(degree_to_radion(to_lat)) * np.sin(long_diff / 2)**2\n    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n    \n    return radius * c\n\ntrain_sample['distance'] = calculate_distance(train_sample.pickup_latitude, train_sample.pickup_longitude, train_sample.dropoff_latitude, train_sample.dropoff_longitude)\ntest['distance'] = calculate_distance(test.pickup_latitude, test.pickup_longitude, test.dropoff_latitude, test.dropoff_longitude)\n\ntrain_sample['distance'] = train_sample.distance.astype(dtype = 'float32')\ntest['distance'] = test.distance.astype(dtype = 'float32')\n\n\n#combine day and hour to make every hour of the week a binary feature\ntrain_sample['day_hour'] = train_sample.day_of_week.astype(str) + \"_\" + train_sample.hour.astype(str)\ntrain_sample['day_hour'] = train_sample['day_hour'].astype('category')\n\ntest['day_hour'] = test.day_of_week.astype(str) + test.hour.astype(str)\ntest['day_hour'] = test['day_hour'].astype('category')\n\n#filter out negative fares\ntrain_sample = train_sample[train_sample.fare_amount > 0]\n\n#holidays\ntrain_sample['pickup_day'] = train_sample.pickup_datetime.dt.floor('d')\ntrain_sample = train_sample.merge(holidays, left_on = 'pickup_day', right_on = 'Date', how = 'left')\ntrain_sample['Holiday'] =train_sample.Holiday.fillna('None')\n\nle = LabelEncoder()\ntrain_sample['holiday'] = le.fit_transform(train_sample.Holiday.values)\ntrain_sample.drop(['Holiday','Date','pickup_day'], axis = 1, inplace = True)\n\ntest['pickup_day'] = test.pickup_datetime.dt.floor('d')\ntest = test.merge(holidays, left_on = 'pickup_day', right_on = 'Date', how = 'left')\ntest['Holiday'] =test.Holiday.fillna('None')\n\ntest['holiday'] = le.fit_transform(test.Holiday.values)\ntest.drop(['Holiday','Date','pickup_day'], axis = 1, inplace = True)\n\n\ntrain_sample['holiday'] = train_sample.holiday.astype(dtype = 'uint8')\ntest['holiday'] = test.holiday.astype(dtype = 'uint8')\n\ntrain_one = train_sample.copy()\ndel train_sample\ntrain_one.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3630816ea609ad94b4e823489ca2ab5a19f5ee52"},"cell_type":"code","source":"%%time\n\ntrain_sample = train_two.dropna()\n\n#truncate datetime string for efficiency converting to datetime format\ntrain_sample['pickup_datetime'] = train_sample.pickup_datetime.str.slice(0, 16)\n\ntrain_sample['pickup_datetime'] = pd.to_datetime(train_sample.pickup_datetime, utc=True, format='%Y-%m-%d %H:%M') \n\n#get rid of unnecessary memory consuming column\ntrain_sample.drop(labels='key', axis=1, inplace=True)\n\n#convert data to less memory intensive types\ntrain_sample.loc[:,'passenger_count'] = train_sample.passenger_count.astype(dtype = 'uint8')\ntrain_sample['pickup_longitude'] = train_sample.pickup_longitude.astype(dtype = 'float32')\ntrain_sample['pickup_latitude'] = train_sample.pickup_latitude.astype(dtype = 'float32')\ntrain_sample['dropoff_longitude'] = train_sample.dropoff_longitude.astype(dtype = 'float32')\ntrain_sample['dropoff_latitude'] = train_sample.dropoff_latitude.astype(dtype = 'float32')\ntrain_sample['fare_amount'] = train_sample.fare_amount.astype(dtype = 'float32')\n\n#filter training set to be within full range of test set\ntrain_sample = train_sample.loc[train_sample.pickup_longitude.between(test.pickup_longitude.min(), test.pickup_longitude.max())]\ntrain_sample = train_sample.loc[train_sample.pickup_latitude.between(test.pickup_latitude.min(), test.pickup_latitude.max())]\ntrain_sample = train_sample.loc[train_sample.dropoff_longitude.between(test.dropoff_longitude.min(), test.dropoff_longitude.max())]\ntrain_sample = train_sample.loc[train_sample.dropoff_latitude.between(test.dropoff_latitude.min(), test.dropoff_latitude.max())]\n\n#convert timestamp to features to be used as categorial\ntrain_sample['hour'] = train_sample['pickup_datetime'].apply(lambda time: time.hour)\ntrain_sample['month'] = train_sample['pickup_datetime'].apply(lambda time: time.month)\ntrain_sample['day_of_week'] = train_sample['pickup_datetime'].apply(lambda time: time.dayofweek)\ntrain_sample['year'] = train_sample['pickup_datetime'].apply(lambda t: t.year)\n\n#reduce memory by converting datatypes\ntrain_sample['hour'] = train_sample.hour.astype(dtype = 'uint8')\ntrain_sample['month'] = train_sample.month.astype(dtype = 'uint8')\ntrain_sample['day_of_week'] = train_sample.day_of_week.astype(dtype = 'uint8')\ntrain_sample['year'] = train_sample.year.astype(dtype = 'uint16')\n\n# Join Weather data\ntrain_sample['pickup_day'] = train_sample.pickup_datetime.dt.floor('d')\ntrain_sample = train_sample.merge(nyc_weather, how = 'left', left_on ='pickup_day', right_on = 'DATE')\ntrain_sample.drop(columns = ['pickup_day','DATE'], axis = 0, inplace = True)\n\ntrain_sample['AWND'] = train_sample.AWND.astype(dtype = 'float16')\ntrain_sample['PRCP'] = train_sample.PRCP.astype(dtype = 'float16')\ntrain_sample['SNOW'] = train_sample.day_of_week.astype(dtype = 'float16')\ntrain_sample['TMAX'] = train_sample.TMAX.astype(dtype = 'float16')\ntrain_sample['TMIN'] = train_sample.TMAX.astype(dtype = 'float16')\n\n#create weather features\n#extreme temps\ntrain_sample['hot_day'] = np.where(train_sample.TMAX >= 30,1,0)\ntrain_sample['cold_day'] = np.where(train_sample.TMIN <= 0,1,0)\ntrain_sample['hot_day'] = train_sample.hot_day.astype(dtype = 'uint8')\ntrain_sample['cold_day'] = train_sample.cold_day.astype(dtype = 'uint8')\n#rain and snow\ntrain_sample['rainy_day'] = np.where(train_sample.PRCP >= 0,1,0)\ntrain_sample['snowy_day'] = np.where(train_sample.SNOW <= 0,1,0)\ntrain_sample['rainy_day'] = train_sample.rainy_day.astype(dtype = 'uint8')\ntrain_sample['snowy_day'] = train_sample.snowy_day.astype(dtype = 'uint8')\n\n#windy days\ntrain_sample['windy_day'] = np.where(train_sample.AWND >= 0,1,0)\ntrain_sample['windy_day'] = train_sample.windy_day.astype(dtype = 'uint8')\n\n#calculate distance\ntrain_sample['distance'] = calculate_distance(train_sample.pickup_latitude, train_sample.pickup_longitude, train_sample.dropoff_latitude, train_sample.dropoff_longitude)\ntrain_sample['distance'] = train_sample.distance.astype(dtype = 'float32')\n\n#combine day and hour to make every hour of the week a binary feature\ntrain_sample['day_hour'] = train_sample.day_of_week.astype(str) + \"_\" + train_sample.hour.astype(str)\ntrain_sample['day_hour'] = train_sample['day_hour'].astype('category')\n\n#filter out negative fares\ntrain_sample = train_sample[train_sample.fare_amount > 0]\n\n#holidays\ntrain_sample['pickup_day'] = train_sample.pickup_datetime.dt.floor('d')\ntrain_sample = train_sample.merge(holidays, left_on = 'pickup_day', right_on = 'Date', how = 'left')\ntrain_sample['Holiday'] =train_sample.Holiday.fillna('None')\n\nle = LabelEncoder()\ntrain_sample['holiday'] = le.fit_transform(train_sample.Holiday.values)\ntrain_sample.drop(['Holiday','Date','pickup_day'], axis = 1, inplace = True)\ntrain_sample['holiday'] = train_sample.holiday.astype(dtype = 'uint8')\n\ntrain_two = train_sample.copy()\ndel train_sample\ntrain_two.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"56bf546091eadcad984a0689346e0c4586cae4d5"},"cell_type":"code","source":"%%time\n\ntrain_sample = train_three.dropna()\n\n#truncate datetime string for efficiency converting to datetime format\ntrain_sample['pickup_datetime'] = train_sample.pickup_datetime.str.slice(0, 16)\n\ntrain_sample['pickup_datetime'] = pd.to_datetime(train_sample.pickup_datetime, utc=True, format='%Y-%m-%d %H:%M') \n\n#get rid of unnecessary memory consuming column\ntrain_sample.drop(labels='key', axis=1, inplace=True)\n\n#convert data to less memory intensive types\ntrain_sample.loc[:,'passenger_count'] = train_sample.passenger_count.astype(dtype = 'uint8')\ntrain_sample['pickup_longitude'] = train_sample.pickup_longitude.astype(dtype = 'float32')\ntrain_sample['pickup_latitude'] = train_sample.pickup_latitude.astype(dtype = 'float32')\ntrain_sample['dropoff_longitude'] = train_sample.dropoff_longitude.astype(dtype = 'float32')\ntrain_sample['dropoff_latitude'] = train_sample.dropoff_latitude.astype(dtype = 'float32')\ntrain_sample['fare_amount'] = train_sample.fare_amount.astype(dtype = 'float32')\n\n#filter training set to be within full range of test set\ntrain_sample = train_sample.loc[train_sample.pickup_longitude.between(test.pickup_longitude.min(), test.pickup_longitude.max())]\ntrain_sample = train_sample.loc[train_sample.pickup_latitude.between(test.pickup_latitude.min(), test.pickup_latitude.max())]\ntrain_sample = train_sample.loc[train_sample.dropoff_longitude.between(test.dropoff_longitude.min(), test.dropoff_longitude.max())]\ntrain_sample = train_sample.loc[train_sample.dropoff_latitude.between(test.dropoff_latitude.min(), test.dropoff_latitude.max())]\n\n#convert timestamp to features to be used as categorial\ntrain_sample['hour'] = train_sample['pickup_datetime'].apply(lambda time: time.hour)\ntrain_sample['month'] = train_sample['pickup_datetime'].apply(lambda time: time.month)\ntrain_sample['day_of_week'] = train_sample['pickup_datetime'].apply(lambda time: time.dayofweek)\ntrain_sample['year'] = train_sample['pickup_datetime'].apply(lambda t: t.year)\n\n#reduce memory by converting datatypes\ntrain_sample['hour'] = train_sample.hour.astype(dtype = 'uint8')\ntrain_sample['month'] = train_sample.month.astype(dtype = 'uint8')\ntrain_sample['day_of_week'] = train_sample.day_of_week.astype(dtype = 'uint8')\ntrain_sample['year'] = train_sample.year.astype(dtype = 'uint16')\n\n# Join Weather data\ntrain_sample['pickup_day'] = train_sample.pickup_datetime.dt.floor('d')\ntrain_sample = train_sample.merge(nyc_weather, how = 'left', left_on ='pickup_day', right_on = 'DATE')\ntrain_sample.drop(columns = ['pickup_day','DATE'], axis = 0, inplace = True)\n\ntrain_sample['AWND'] = train_sample.AWND.astype(dtype = 'float16')\ntrain_sample['PRCP'] = train_sample.PRCP.astype(dtype = 'float16')\ntrain_sample['SNOW'] = train_sample.day_of_week.astype(dtype = 'float16')\ntrain_sample['TMAX'] = train_sample.TMAX.astype(dtype = 'float16')\ntrain_sample['TMIN'] = train_sample.TMAX.astype(dtype = 'float16')\n\n#create weather features\n#extreme temps\ntrain_sample['hot_day'] = np.where(train_sample.TMAX >= 30,1,0)\ntrain_sample['cold_day'] = np.where(train_sample.TMIN <= 0,1,0)\ntrain_sample['hot_day'] = train_sample.hot_day.astype(dtype = 'uint8')\ntrain_sample['cold_day'] = train_sample.cold_day.astype(dtype = 'uint8')\n#rain and snow\ntrain_sample['rainy_day'] = np.where(train_sample.PRCP >= 0,1,0)\ntrain_sample['snowy_day'] = np.where(train_sample.SNOW <= 0,1,0)\ntrain_sample['rainy_day'] = train_sample.rainy_day.astype(dtype = 'uint8')\ntrain_sample['snowy_day'] = train_sample.snowy_day.astype(dtype = 'uint8')\n\n#windy days\ntrain_sample['windy_day'] = np.where(train_sample.AWND >= 0,1,0)\ntrain_sample['windy_day'] = train_sample.windy_day.astype(dtype = 'uint8')\n\n#calculate distance\ntrain_sample['distance'] = calculate_distance(train_sample.pickup_latitude, train_sample.pickup_longitude, train_sample.dropoff_latitude, train_sample.dropoff_longitude)\ntrain_sample['distance'] = train_sample.distance.astype(dtype = 'float32')\n\n#combine day and hour to make every hour of the week a binary feature\ntrain_sample['day_hour'] = train_sample.day_of_week.astype(str) + \"_\" + train_sample.hour.astype(str)\ntrain_sample['day_hour'] = train_sample['day_hour'].astype('category')\n\n#filter out negative fares\ntrain_sample = train_sample[train_sample.fare_amount > 0]\n\n#holidays\ntrain_sample['pickup_day'] = train_sample.pickup_datetime.dt.floor('d')\ntrain_sample = train_sample.merge(holidays, left_on = 'pickup_day', right_on = 'Date', how = 'left')\ntrain_sample['Holiday'] =train_sample.Holiday.fillna('None')\n\nle = LabelEncoder()\ntrain_sample['holiday'] = le.fit_transform(train_sample.Holiday.values)\ntrain_sample.drop(['Holiday','Date','pickup_day'], axis = 1, inplace = True)\ntrain_sample['holiday'] = train_sample.holiday.astype(dtype = 'uint8')\n\ntrain_three = train_sample.copy()\ndel train_sample\ntrain_three.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"328778dcf66fb825eb0c287594ebdc6e3a77c54b"},"cell_type":"code","source":"%%time\n\ntrain_sample = train_four.dropna()\n\n#truncate datetime string for efficiency converting to datetime format\ntrain_sample['pickup_datetime'] = train_sample.pickup_datetime.str.slice(0, 16)\n\ntrain_sample['pickup_datetime'] = pd.to_datetime(train_sample.pickup_datetime, utc=True, format='%Y-%m-%d %H:%M') \n\n#get rid of unnecessary memory consuming column\ntrain_sample.drop(labels='key', axis=1, inplace=True)\n\n#convert data to less memory intensive types\ntrain_sample.loc[:,'passenger_count'] = train_sample.passenger_count.astype(dtype = 'uint8')\ntrain_sample['pickup_longitude'] = train_sample.pickup_longitude.astype(dtype = 'float32')\ntrain_sample['pickup_latitude'] = train_sample.pickup_latitude.astype(dtype = 'float32')\ntrain_sample['dropoff_longitude'] = train_sample.dropoff_longitude.astype(dtype = 'float32')\ntrain_sample['dropoff_latitude'] = train_sample.dropoff_latitude.astype(dtype = 'float32')\ntrain_sample['fare_amount'] = train_sample.fare_amount.astype(dtype = 'float32')\n\n#filter training set to be within full range of test set\ntrain_sample = train_sample.loc[train_sample.pickup_longitude.between(test.pickup_longitude.min(), test.pickup_longitude.max())]\ntrain_sample = train_sample.loc[train_sample.pickup_latitude.between(test.pickup_latitude.min(), test.pickup_latitude.max())]\ntrain_sample = train_sample.loc[train_sample.dropoff_longitude.between(test.dropoff_longitude.min(), test.dropoff_longitude.max())]\ntrain_sample = train_sample.loc[train_sample.dropoff_latitude.between(test.dropoff_latitude.min(), test.dropoff_latitude.max())]\n\n#convert timestamp to features to be used as categorial\ntrain_sample['hour'] = train_sample['pickup_datetime'].apply(lambda time: time.hour)\ntrain_sample['month'] = train_sample['pickup_datetime'].apply(lambda time: time.month)\ntrain_sample['day_of_week'] = train_sample['pickup_datetime'].apply(lambda time: time.dayofweek)\ntrain_sample['year'] = train_sample['pickup_datetime'].apply(lambda t: t.year)\n\n#reduce memory by converting datatypes\ntrain_sample['hour'] = train_sample.hour.astype(dtype = 'uint8')\ntrain_sample['month'] = train_sample.month.astype(dtype = 'uint8')\ntrain_sample['day_of_week'] = train_sample.day_of_week.astype(dtype = 'uint8')\ntrain_sample['year'] = train_sample.year.astype(dtype = 'uint16')\n\n# Join Weather data\ntrain_sample['pickup_day'] = train_sample.pickup_datetime.dt.floor('d')\ntrain_sample = train_sample.merge(nyc_weather, how = 'left', left_on ='pickup_day', right_on = 'DATE')\ntrain_sample.drop(columns = ['pickup_day','DATE'], axis = 0, inplace = True)\n\ntrain_sample['AWND'] = train_sample.AWND.astype(dtype = 'float16')\ntrain_sample['PRCP'] = train_sample.PRCP.astype(dtype = 'float16')\ntrain_sample['SNOW'] = train_sample.day_of_week.astype(dtype = 'float16')\ntrain_sample['TMAX'] = train_sample.TMAX.astype(dtype = 'float16')\ntrain_sample['TMIN'] = train_sample.TMAX.astype(dtype = 'float16')\n\n#create weather features\n#extreme temps\ntrain_sample['hot_day'] = np.where(train_sample.TMAX >= 30,1,0)\ntrain_sample['cold_day'] = np.where(train_sample.TMIN <= 0,1,0)\ntrain_sample['hot_day'] = train_sample.hot_day.astype(dtype = 'uint8')\ntrain_sample['cold_day'] = train_sample.cold_day.astype(dtype = 'uint8')\n#rain and snow\ntrain_sample['rainy_day'] = np.where(train_sample.PRCP >= 0,1,0)\ntrain_sample['snowy_day'] = np.where(train_sample.SNOW <= 0,1,0)\ntrain_sample['rainy_day'] = train_sample.rainy_day.astype(dtype = 'uint8')\ntrain_sample['snowy_day'] = train_sample.snowy_day.astype(dtype = 'uint8')\n\n#windy days\ntrain_sample['windy_day'] = np.where(train_sample.AWND >= 0,1,0)\ntrain_sample['windy_day'] = train_sample.windy_day.astype(dtype = 'uint8')\n\n#calculate distance\ntrain_sample['distance'] = calculate_distance(train_sample.pickup_latitude, train_sample.pickup_longitude, train_sample.dropoff_latitude, train_sample.dropoff_longitude)\ntrain_sample['distance'] = train_sample.distance.astype(dtype = 'float32')\n\n#combine day and hour to make every hour of the week a binary feature\ntrain_sample['day_hour'] = train_sample.day_of_week.astype(str) + \"_\" + train_sample.hour.astype(str)\ntrain_sample['day_hour'] = train_sample['day_hour'].astype('category')\n\n#filter out negative fares\ntrain_sample = train_sample[train_sample.fare_amount > 0]\n\n#holidays\ntrain_sample['pickup_day'] = train_sample.pickup_datetime.dt.floor('d')\ntrain_sample = train_sample.merge(holidays, left_on = 'pickup_day', right_on = 'Date', how = 'left')\ntrain_sample['Holiday'] =train_sample.Holiday.fillna('None')\n\nle = LabelEncoder()\ntrain_sample['holiday'] = le.fit_transform(train_sample.Holiday.values)\ntrain_sample.drop(['Holiday','Date','pickup_day'], axis = 1, inplace = True)\ntrain_sample['holiday'] = train_sample.holiday.astype(dtype = 'uint8')\n\ntrain_four = train_sample.copy()\ndel train_sample\ntrain_four.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b08cb1091157955464e38f81556a15537c943bfe"},"cell_type":"code","source":"train_full_pre = pd.concat([train_one, train_two, train_three,train_four])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"82e17993b4c2f5601252091af0853d1e6efbbb66"},"cell_type":"code","source":"#dir()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"97591c6e8079038f60b6070c6fed6bf0214084fc"},"cell_type":"code","source":"del train_full","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c33259e06e349c3084186adef96b31949f459387"},"cell_type":"markdown","source":"Here, I identify 200 'neighborhoods' throughout the city using Kmeans clustering. I include all possible coordinate pairs from both the training sample and the test set, rounded to 4 decimal places, and take the unique values (note: I am loading a model from an earlier kernel, which was trained on the full test set and a sample of 25M rows from the training set) Rounding and taking unique values makes the neighborhoods more evenly distributed, instead of being concentrated in the most frequent pickup/dropoff points. Without doing this, the cluster centers are much more densely located in Manhattan and even more sparse in the outer boroughs. Rounding to 4 decimal places performed best on the test set. \n\nWith the trained model, I label each pickup/dropoff 'neighborhood' on the training and test set, which I will encode as categorical features."},{"metadata":{"trusted":true,"_uuid":"ad31710bbff94deb192ac6df294fb0dd2cf655be","_kg_hide-input":false},"cell_type":"code","source":"# Create set of unique locations rounded to 4 decimal places. This prevents the model from biasing towards more frequently used pickup/dropoff spots\ntrain_sample = train_full_pre.copy() #.sample(15000000)\n\nfull_pickups = pd.concat([train_sample[['pickup_longitude','pickup_latitude']],test[['pickup_longitude','pickup_latitude']]], axis = 0)\nfull_pickups.columns = ['x','y']\nfull_dropoffs = pd.concat([train_sample[['dropoff_longitude','dropoff_latitude']],test[['dropoff_longitude','dropoff_latitude']]], axis = 0)\nfull_dropoffs.columns = ['x','y']\nfull_locs = pd.concat([full_pickups,full_dropoffs], axis = 0)\n#full_locs = full_locs.sample(10000000)\n#full_locs['x'] = full_locs.x.round(4)\n#full_locs['y'] = full_locs.y.round(4)\n\nfull_locs = full_locs.groupby(['x','y']).count().reset_index()\n\ndel full_pickups, full_dropoffs, train_sample, train_full_pre\n\nfull_locs.info()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bf3f4b916b73fe35764f84d897cb18df28c13eca"},"cell_type":"markdown","source":"The plots below show the resulting clusters of locations and their centers. These are the rounded/unique values, not the training set. "},{"metadata":{"trusted":true,"_uuid":"a7437bc26a1d3e94e9a1da79c9080ef77b18f2a9","_kg_hide-input":true},"cell_type":"code","source":"%%time\n\nX_df = full_locs.copy()\nX_kmeans = full_locs.values\ndel full_locs\n\nnum_clusters = 200\n\n#fit the model (done in a previous kernel on a larger set, since the number of rows gets reduced with rounding and taking unique values)\n#kmeans = MiniBatchKMeans(n_clusters=num_clusters)\n#kmeans.fit(X_kmeans)\n\n#load model from previous kernel trained on 25M observations\nwith open('../input/taxi-weather-holidays-kmeans-neighborhoods/kmeans_200_round4.pkl', 'rb') as fid:\n    kmeans = pickle.load(fid)\n\n#create labels for graph below\nz = kmeans.predict(X_kmeans)\n\ncenters = kmeans.cluster_centers_\n\nx_centers = [pair[0] for pair in centers]\ny_centers = [pair[1] for pair in centers]\nz_centers = np.arange(num_clusters)\n\n#locations plotted with clusters as different shades\nplt.subplot(1,2,1)\nplt.scatter(X_df['x'], X_df['y'], c=z)\nplt.gray()\nplt.xlabel('Pickup/Dropoff Longitude')\nplt.ylabel('Pickup/Dropoff Latitude')\nplt.title('Clusters of NYC locations')\nplt.subplot(1,2,2)\n#plot of cluster center locations\nplt.scatter(x_centers, y_centers, c=z_centers)\nplt.gray()\nplt.xlabel('Pickup/Dropoff Longitude')\nplt.ylabel('Pickup/Dropoff Latitude')\nplt.title('Cluster Centers of NYC locations')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"19d7d3daa7aee4580eb664c9df43856edb2c1d61","_kg_hide-input":true},"cell_type":"code","source":"#add cluster labels as new features\ndel X_kmeans, X_df\n#train_sample = train_sample.sample(10000000)\n\ntrain_one['pickup_neighborhood'] = kmeans.predict(np.column_stack([train_one.pickup_longitude.values,train_one.pickup_latitude.values]))\ntrain_one['dropoff_neighborhood'] = kmeans.predict(np.column_stack([train_one.dropoff_longitude.values,train_one.dropoff_latitude.values]))\ntrain_one['pickup_neighborhood'] = train_one.pickup_neighborhood.astype(dtype = 'uint8')\ntrain_one['dropoff_neighborhood'] = train_one.dropoff_neighborhood.astype(dtype = 'uint8')\n\ntrain_two['pickup_neighborhood'] = kmeans.predict(np.column_stack([train_two.pickup_longitude.values,train_two.pickup_latitude.values]))\ntrain_two['dropoff_neighborhood'] = kmeans.predict(np.column_stack([train_two.dropoff_longitude.values,train_two.dropoff_latitude.values]))\ntrain_two['pickup_neighborhood'] = train_two.pickup_neighborhood.astype(dtype = 'uint8')\ntrain_two['dropoff_neighborhood'] = train_two.dropoff_neighborhood.astype(dtype = 'uint8')\n\ntrain_three['pickup_neighborhood'] = kmeans.predict(np.column_stack([train_three.pickup_longitude.values,train_three.pickup_latitude.values]))\ntrain_three['dropoff_neighborhood'] = kmeans.predict(np.column_stack([train_three.dropoff_longitude.values,train_three.dropoff_latitude.values]))\ntrain_three['pickup_neighborhood'] = train_three.pickup_neighborhood.astype(dtype = 'uint8')\ntrain_three['dropoff_neighborhood'] = train_three.dropoff_neighborhood.astype(dtype = 'uint8')\n\ntrain_four['pickup_neighborhood'] = kmeans.predict(np.column_stack([train_four.pickup_longitude.values,train_four.pickup_latitude.values]))\ntrain_four['dropoff_neighborhood'] = kmeans.predict(np.column_stack([train_four.dropoff_longitude.values,train_four.dropoff_latitude.values]))\ntrain_four['pickup_neighborhood'] = train_four.pickup_neighborhood.astype(dtype = 'uint8')\ntrain_four['dropoff_neighborhood'] = train_four.dropoff_neighborhood.astype(dtype = 'uint8')\n\ntest['pickup_neighborhood'] =  kmeans.predict(np.column_stack([test.pickup_longitude.values, test.pickup_latitude.values]))\ntest['dropoff_neighborhood'] = kmeans.predict(np.column_stack([test.dropoff_longitude.values,test.dropoff_latitude.values]))\n\ntest['pickup_neighborhood'] = test.pickup_neighborhood.astype(dtype = 'uint8')\ntest['dropoff_neighborhood'] = test.dropoff_neighborhood.astype(dtype = 'uint8')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3dc5b8a1daa7c094b194153148d22aaf285b46a1","_kg_hide-input":true},"cell_type":"code","source":"#save kmeans model for future use\nwith open('kmeans_minibatch_200_round4_v2.pkl', 'wb') as fid:\n    pickle.dump(kmeans, fid)    \n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4a171105b0c357bb4ae663a8939b226e9fb22f84"},"cell_type":"markdown","source":"Before I train the final model, I need to convert the training set into the proper format. Since there are a large number of binary features resulting from the neighborhoods and day/hour combinations, I use sparse matricies.  I impute any missing values with the mean, then split the set 90/10 for cross validation, in this case just to see how the test score compares to the submission. "},{"metadata":{"trusted":true,"_uuid":"fd613d56856e3ee2406e00abdf46ac20e4bc88d8"},"cell_type":"code","source":"del  KMeans, le_dict, MiniBatchKMeans,StandardScaler, calculate_distance, centers, degree_to_radion, holidays, kmeans,nyc_weather, pickle, z, z_centers, x_centers, y_centers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8cb506f73500912235673cd7b1c445c5fc1a67b6"},"cell_type":"code","source":"import gc\ngc.collect()\ndir()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8e3983130322c087a670d142f2610ca2151d9a05"},"cell_type":"code","source":"train_full = pd.concat([train_one, train_two, train_three, train_four])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"78602288e50151b9178a1db1f615eceb511d6ff5"},"cell_type":"code","source":"#create final array for model\ncategorical_cols = ['pickup_neighborhood','dropoff_neighborhood','day_hour','month','year','passenger_count','holiday','hot_day','cold_day','rainy_day','snowy_day','windy_day'] #hot_day','cold_day','jfk_pickup','jfk_dropoff','lga_pickup','lga_dropoff','ewr_pickup','ewr_dropoff'     'hour','day_of_week', 'pickup_lat_round','pickup_long_round'\nnumerical_cols = ['distance'] # 'delta_lat','delta_long', , 'pickup_latitude','pickup_longitude','AWND','PRCP','SNOW','TMAX','TMIN'\n\ndf_cats = train_full[categorical_cols].copy()\n\nle_dict = {}\nfor col in categorical_cols:\n    le_dict[col] = LabelEncoder().fit(df_cats[col])\n    df_cats[col] = le_dict[col].transform(df_cats[col])\n    \nX_cats_full = df_cats.values\n\nohe = OneHotEncoder(categories = 'auto', drop = 'first')\nX_onehot = ohe.fit_transform(X_cats_full)\n#del df_cats, X_cats_full\n\nX_nums_full =  train_full[numerical_cols].values\nX_nums_sparse = csr_matrix(X_nums_full)\n\n#del df, X_nums_full\n\nX_full = hstack([X_onehot, X_nums_sparse])\nX_full = X_full.tocsr()\n\n#impute any missing data\nsi = SimpleImputer()\nX = si.fit_transform(X_full)\n\ny = train_full.fare_amount.values\ndel X_onehot, X_nums_sparse, X_full, train_full\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d6faf84c2959a51658ab81197f897ce6c87916b6"},"cell_type":"code","source":"\ndf = test.copy()\ndf_cats = df[categorical_cols].copy()\n\nle_dict = {}\nfor col in categorical_cols:\n    le_dict[col] = LabelEncoder().fit(df_cats[col])\n    df_cats[col] = le_dict[col].transform(df_cats[col])\n    \nX_cats_full = df_cats.values\n\nX_onehot = ohe.transform(X_cats_full)\ndel df_cats, X_cats_full\n\nX_nums_full =  df[numerical_cols].values\nX_nums_sparse = csr_matrix(X_nums_full)\n\ndel df, X_nums_full\n\nX_full = hstack([X_onehot, X_nums_sparse])\nX_full = X_full.tocsr()\n\n#impute any missing data\nsi = SimpleImputer()\nX_public = si.fit_transform(X_full)\n\ndel X_onehot, X_nums_sparse, X_full\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c14c4cbeacf5a785f22b6f4c7962ba0c52a523b0"},"cell_type":"code","source":"\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size = .1)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"43ba71adb991542fad2f121c5e89dfab80a60114"},"cell_type":"markdown","source":"Finally, train the Light GBM. "},{"metadata":{"trusted":true,"_uuid":"3d84a9835457b0c808db8149f983c24ce2f1463a"},"cell_type":"code","source":"%%time\n\nparams = {'objective': 'regression',\n          'boosting': 'gbdt',\n          'metric': 'rmse',\n          'num_leaves': 50,\n          'max_depth': 8,\n          'learning_rate': 0.5,\n          'bagging_fraction': 0.8,\n          'feature_fraction': 0.8,\n          'min_split_gain': 0.02,\n          'min_child_samples': 10, \n          'min_child_weight': 0.02, \n          'lambda_l2': 0.0475,\n          'verbosity': -1,\n          'data_random_seed': 17,\n          'early_stop': 100,\n          'verbose_eval': 100,\n          'num_rounds': 100} \n\nd_train = lgb.Dataset(X_train, label=y_train)\nd_test = lgb.Dataset(X_test, label=y_test)\nwatchlist = [d_train, d_test]\nnum_rounds = 100\nverbose_eval = 100\nearly_stop = 100\nmodel_lgb = lgb.train(params,\n                      train_set=d_train,\n                      num_boost_round=num_rounds,\n                      valid_sets=watchlist,\n                      verbose_eval=verbose_eval,\n                      early_stopping_rounds=early_stop)\n    \npred_test_y_lgb = model_lgb.predict(X_test, num_iteration=model_lgb.best_iteration)\n\nprint(\"LGB Loss = \" + str(sqrt(mean_squared_error(y_test,pred_test_y_lgb))))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0effb19c2e28d60c7eddc08a8618440e95f0ffbe"},"cell_type":"markdown","source":"Format the final predictions on the public set for submission, then plot a distribution of the fare amount predictions. "},{"metadata":{"trusted":true,"_uuid":"5a63dcb4390722de089b505da0df34c9c4d9fa9f"},"cell_type":"code","source":"lgb_public= model_lgb.predict(X_public, num_iteration=model_lgb.best_iteration)\n\nfinal_pred_public =lgb_public.flatten()\n\n#clean and format final submission\ntest_predictions_lgb = [float(np.asscalar(x)) for x in final_pred_public]\ntest_predictions_lgb = [x if x>0 else 0 for x in test_predictions_lgb]\nsample = pd.DataFrame({'key': test_id,'fare_amount':test_predictions_lgb})\nsample = sample.reindex(['key', 'fare_amount'], axis=1)\nsample.to_csv('submission_lgb.csv', index=False)\nsample.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c9a37b1198d21a0995ae13c9e5d71ecc7fb641d6"},"cell_type":"code","source":"plt.rc('figure', figsize=(10, 10))\nplt.hist(test_predictions_lgb, bins = 100)\nplt.xlabel('Predicticted Price')\nplt.ylabel('Frequency')\nplt.title('Predictions from LGB')\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}