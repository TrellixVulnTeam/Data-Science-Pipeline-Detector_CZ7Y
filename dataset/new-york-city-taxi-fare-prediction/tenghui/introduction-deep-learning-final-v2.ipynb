{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import seaborn as sns\nimport numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n#plt.style.use('fivethirtyeight');\n#plt.rcParams['font.size'] = 14;\n#plt.figure(figsize=(12,5));\npalette = sns.color_palette('Paired', 10);\nimport folium\nfrom folium.plugins import HeatMap\nfrom folium.plugins import HeatMapWithTime","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-30T09:03:48.194497Z","iopub.execute_input":"2021-06-30T09:03:48.194901Z","iopub.status.idle":"2021-06-30T09:03:48.200817Z","shell.execute_reply.started":"2021-06-30T09:03:48.194864Z","shell.execute_reply":"2021-06-30T09:03:48.199933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" # **Load data:**","metadata":{}},{"cell_type":"code","source":"train_1= pd.read_csv('../input/taxi-data/taxi_data/train_gps_points.csv')\ntrain_2= pd.read_csv('../input/taxi-data/taxi_data/train_hire_stats.csv')\ntest_df= pd.read_csv('../input/taxi-data/taxi_data/test_hire_stats.csv')\nzone_df= pd.read_csv('../input/taxi-data/taxi_data/zones.csv')\ndata_time= train_2['Date']","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:03:48.273616Z","iopub.execute_input":"2021-06-30T09:03:48.274197Z","iopub.status.idle":"2021-06-30T09:03:52.732469Z","shell.execute_reply.started":"2021-06-30T09:03:48.274142Z","shell.execute_reply":"2021-06-30T09:03:52.731713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**correlation of Hour_slot&Hire count**","metadata":{}},{"cell_type":"code","source":"## Look Data Distribution\nsns.set(font_scale=1.5)\nsns.jointplot(train_2['Hour_slot'], train_2['Hire_count'], kind='resid')","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:03:52.733964Z","iopub.execute_input":"2021-06-30T09:03:52.734542Z","iopub.status.idle":"2021-06-30T09:03:54.588552Z","shell.execute_reply.started":"2021-06-30T09:03:52.734497Z","shell.execute_reply":"2021-06-30T09:03:54.587624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.hist2d(train_2['Hour_slot'], train_2['Hire_count'], bins=(50, 50), vmax=1200)\nplt.colorbar()\nplt.xlabel('Hour slot')\nplt.ylabel('Hire Count')","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:03:54.590179Z","iopub.execute_input":"2021-06-30T09:03:54.59049Z","iopub.status.idle":"2021-06-30T09:03:54.852058Z","shell.execute_reply.started":"2021-06-30T09:03:54.590459Z","shell.execute_reply":"2021-06-30T09:03:54.851088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Turn \"Date\" to other form**","metadata":{}},{"cell_type":"code","source":"time_df= pd.to_datetime(train_2['Date'], format='%Y-%m-%d')\n# Time data preparation\ntrain=pd.DataFrame()\ntrain[\"year\"] = time_df.dt.year\ntrain[\"month\"] = time_df.dt.month\ntrain[\"day\"] = time_df.dt.day\ntrain[\"weekday\"] = time_df.dt.weekday\ntrain.head(200)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:03:54.853803Z","iopub.execute_input":"2021-06-30T09:03:54.85413Z","iopub.status.idle":"2021-06-30T09:03:55.022204Z","shell.execute_reply.started":"2021-06-30T09:03:54.854076Z","shell.execute_reply":"2021-06-30T09:03:55.021198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**correlation of weekday&Hire count**","metadata":{}},{"cell_type":"code","source":"plt.hist2d(train['weekday'], train_2['Hire_count'], bins=(50, 50), vmax=1000)\nplt.colorbar()\nplt.xlabel('WeekDay')\nplt.ylabel('Hire Count')","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:03:55.023607Z","iopub.execute_input":"2021-06-30T09:03:55.024213Z","iopub.status.idle":"2021-06-30T09:03:55.268656Z","shell.execute_reply.started":"2021-06-30T09:03:55.024164Z","shell.execute_reply":"2021-06-30T09:03:55.26742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.jointplot(train['weekday'], train_2['Hire_count'], kind='resid',)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:03:55.270373Z","iopub.execute_input":"2021-06-30T09:03:55.270829Z","iopub.status.idle":"2021-06-30T09:03:57.130349Z","shell.execute_reply.started":"2021-06-30T09:03:55.270782Z","shell.execute_reply":"2021-06-30T09:03:57.12932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**correlation of Month&Hire count**","metadata":{}},{"cell_type":"code","source":"sns.jointplot(train['month'], train_2['Hire_count'], kind='resid',)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:03:57.131592Z","iopub.execute_input":"2021-06-30T09:03:57.131906Z","iopub.status.idle":"2021-06-30T09:03:59.010763Z","shell.execute_reply.started":"2021-06-30T09:03:57.131874Z","shell.execute_reply":"2021-06-30T09:03:59.009979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.hist2d(train['month'], train_2['Hire_count'], bins=(50, 50), vmax=1000)\nplt.colorbar()\nplt.xlabel('Month')\nplt.ylabel('Hire Count')","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:03:59.012962Z","iopub.execute_input":"2021-06-30T09:03:59.013439Z","iopub.status.idle":"2021-06-30T09:03:59.262424Z","shell.execute_reply.started":"2021-06-30T09:03:59.013404Z","shell.execute_reply":"2021-06-30T09:03:59.261304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"''' Working extract time Feature'''\ndef extract_time_feature(df): \n  time_colum=\"Datetime\"\n  df.index= pd.to_datatime(df[time_colum])\n  df.index= df.index.tz_convert()\n# Data preprocessing\nimport datetime\n# Time data preprocessing\ntime_df_gps=pd.to_datetime(train_1['Datetime'],utc=True ,infer_datetime_format=True, format='%Y-%m-%d %H-%M-%S +UTC')\n# Time data preparation\ntrain=pd.DataFrame()\ntrain[\"year\"] = time_df_gps.dt.year\ntrain[\"month\"] = time_df_gps.dt.month\ntrain[\"day\"] = time_df_gps.dt.day\ntrain[\"weekday\"] = time_df_gps.dt.weekday\ntrain[\"hour\"] = time_df_gps.dt.hour","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:03:59.264323Z","iopub.execute_input":"2021-06-30T09:03:59.264628Z","iopub.status.idle":"2021-06-30T09:04:49.162313Z","shell.execute_reply.started":"2021-06-30T09:03:59.264597Z","shell.execute_reply":"2021-06-30T09:04:49.16124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**correlation of Zone_ID&Hire count**","metadata":{}},{"cell_type":"code","source":"sns.jointplot(train_1['Zone_ID'], train_2['Hire_count'], kind='hex')","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:04:49.163822Z","iopub.execute_input":"2021-06-30T09:04:49.164141Z","iopub.status.idle":"2021-06-30T09:04:51.404394Z","shell.execute_reply.started":"2021-06-30T09:04:49.164097Z","shell.execute_reply":"2021-06-30T09:04:51.403415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**correlation of hour &Hire count**","metadata":{}},{"cell_type":"code","source":"gps_extract_df=pd.DataFrame()\ngps_extract_df['hour']= train['hour']\ngps_extract_df['Zone_ID']=train_1['Zone_ID']\ngps_extract_df['day']= train['day']\ngps_extract_df['month']= train['month']\ngps_extract_df['year']=train['year']\ngps_extract_df\n# generate a table of those culprit rows which are duplicated:\ndf=gps_extract_df\ndups = df.groupby(df.columns.tolist()).size().reset_index().rename(columns={0:'count'})\ndups\n## Look Data Distribution\nsns.jointplot(dups['hour'], dups['count'], kind='hex', ylim=(0,200))","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:04:51.405611Z","iopub.execute_input":"2021-06-30T09:04:51.405909Z","iopub.status.idle":"2021-06-30T09:04:53.438817Z","shell.execute_reply.started":"2021-06-30T09:04:51.405879Z","shell.execute_reply":"2021-06-30T09:04:53.437914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" # **HeatMap of Hire count**","metadata":{}},{"cell_type":"code","source":"df=train_1.copy()\ndf=df.sample(frac=1).reset_index(drop=True)\ndf=df[:100000]\nfor col in [\"Longitude_X\", \"Latitude_Y\"]:\n    MIN = df[col].min()\n    MAX = df[col].max()\n    print(col, MIN, MAX)\ndf = df[df[\"Longitude_X\"].between(left = 121.5, right = 121.7 )]\ndf = df[df[\"Latitude_Y\"].between(left = 24.0, right = 25.2 )] \ncenter_location = [25.0838005, 121.590033]\nm = folium.Map(location=center_location, control_scale=True,zoom_start=13)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:04:53.44033Z","iopub.execute_input":"2021-06-30T09:04:53.440684Z","iopub.status.idle":"2021-06-30T09:04:55.791954Z","shell.execute_reply.started":"2021-06-30T09:04:53.440652Z","shell.execute_reply":"2021-06-30T09:04:55.790934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Heat_data = [[row['Latitude_Y'], row['Longitude_X']] for index, row in df.iterrows()]\nHeatMap(Heat_data,radius=18).add_to(m)\nfrom folium.plugins import MarkerCluster\nmarker_cluster = MarkerCluster().add_to(m)\nfor index, row in zone_df.iterrows():\n    information = 'Zone_ID: '+ str(row['Zone_ID'])\n    folium.Marker(location=[(row['top']+row['bottom'])/2, (row['left']+row['right'])/2], \n                  popup=folium.Popup(information, max_width=500), max_width='100').add_to(marker_cluster)\nm","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:04:55.793369Z","iopub.execute_input":"2021-06-30T09:04:55.793674Z","iopub.status.idle":"2021-06-30T09:05:06.30228Z","shell.execute_reply.started":"2021-06-30T09:04:55.793634Z","shell.execute_reply":"2021-06-30T09:05:06.301209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" # **CircleMap of Hire count**","metadata":{}},{"cell_type":"code","source":"df= pd.read_csv('../input/taxi-data/taxi_data/train_hire_stats.csv')\ndf['Zone_ID']=df['Zone_ID']-1\ndf=df.groupby(by=['Zone_ID']).sum()\ndf['Latitude_Y']=(zone_df['top']+zone_df['bottom'])/2\ndf['Longitude_X']=(zone_df['left']+zone_df['right'])/2\n#df=df[:24]\ndf\nm2 = folium.Map(location=center_location, control_scale=True,zoom_start=13)\n#for i in range(0,len(df)):\nfor i in range(len(df)):    \n    folium.Circle(\n        location=[df.iloc[i]['Latitude_Y'], df.iloc[i]['Longitude_X']],\n        tooltip = \"<h5 style='text-align:center;font-weight: bold'>\"+str(i+1)+\"</h5>\",\n        radius=(int((np.log(df.iloc[i,-3]+1.00001)))+0.2)*100,\n        color='#ff6600',\n        fill_color='#ff8533',\n        fill=True).add_to(m2)\n\nm2","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:05:06.303664Z","iopub.execute_input":"2021-06-30T09:05:06.303978Z","iopub.status.idle":"2021-06-30T09:05:06.457079Z","shell.execute_reply.started":"2021-06-30T09:05:06.303946Z","shell.execute_reply":"2021-06-30T09:05:06.456158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" # **Trend of Hire count**","metadata":{}},{"cell_type":"code","source":"df= pd.read_csv('../input/taxi-data/taxi_data/train_hire_stats.csv')\ndf=df.groupby(by=['Date']).sum()\ndf=df.drop([\"Zone_ID\"], axis=1)\ndf=df.drop([\"Hour_slot\"], axis=1)\n\n# Plot\nf = plt.figure(figsize=(15,10))\nax = f.add_subplot(111)\n\ndate = np.arange(0,len(df))\nmarker_style = dict(linewidth=2, linestyle='-', marker='o',markersize=5)\nplt.plot(date,df,\"-.\",color=\"red\",**marker_style)\nax.tick_params(which='both', width=1,labelsize=12)\nax.tick_params(which='major', length=6)\nax.tick_params(which='minor', length=3, color='0.8')\n#plt.xticks(list(np.arange(0,len(total),int(len(total)/5))),total[:-1:int(len(total)/5)]+[total[-1]])\n# Grid\nplt.grid(lw = 1, ls = '-', c = \"0.85\", which = 'major')\nplt.grid(lw = 1, ls = '-', c = \"0.95\", which = 'minor')\n\n# Axis Lable\nplt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%m/%d'))\nplt.xlabel(\"Day\",fontsize =18)\nplt.ylabel(\"hire_count\",fontsize =18)\n\n# plt.yscale(\"log\")\nplt.tick_params(labelsize = 13) \n#plt.savefig(out+\"daily confirmed cases global.png\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:05:06.459833Z","iopub.execute_input":"2021-06-30T09:05:06.460161Z","iopub.status.idle":"2021-06-30T09:05:06.884911Z","shell.execute_reply.started":"2021-06-30T09:05:06.460127Z","shell.execute_reply":"2021-06-30T09:05:06.881775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **NORMALIZE DATA TURN DATA INTO FORMAT MODEL UNDERSTAND WINDOWWINDOW_STRIDE**","metadata":{}},{"cell_type":"markdown","source":"**Time Processing turn into Sine and Cosine Frequency**","metadata":{}},{"cell_type":"code","source":"'''Removing the Repeat in Time '''\ntime_df=pd.DataFrame()\ntime_df['date']= train_2['Date']\ntime_df['zone_id']=train_2['Zone_ID']\n# print('Unique values of ZoneID:', time_df.zone_id.unique())\ntime_df_rep = time_df.groupby(time_df.columns.tolist()).size().reset_index().rename(columns={0:'count'})\nprint(time_df_rep)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:06:03.180897Z","iopub.execute_input":"2021-06-30T09:06:03.181293Z","iopub.status.idle":"2021-06-30T09:06:03.248852Z","shell.execute_reply.started":"2021-06-30T09:06:03.18126Z","shell.execute_reply":"2021-06-30T09:06:03.247858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**This Time Cosine Fequ for 1 YEAR DATA WITHOUT ZONE_ID**","metadata":{}},{"cell_type":"code","source":"'''Removing the Repeat in Time '''\ntime_df=pd.DataFrame()\ntime_df['date']= train_2['Date']\ntime_df['zone_id']=train_2['Zone_ID']\n# print('Unique values of ZoneID:', time_df.zone_id.unique())\ntime_df_rep = time_df.groupby(time_df.columns.tolist()).size().reset_index().rename(columns={0:'count'})\nprint(time_df_rep)\n\n''' Get Training Data time '''\nholiday=['2016-02-06','2016-02-07','2016-02-08','2016-02-09','2016-02-10','2016-02-11','2016-02-12','2016-02-13','2016-02-14','2016-02-27','2016-02-28','2016-02-29',\n        '2016-04-02','2016-04-03','2016-04-04','2016-04-05','2016-06-09','2016-06-10','2016-06-11','2016-06-12',\n        '2016-09-15','2016-09-16','2016-09-17','2016-09-18',\n         '2016-10-09', '2016-10-10', '2016-10-11','2016-12-31','2017-01-01','2017-01-02',\n         '2017-01-27','2017-01-28','2017-01-29','2017-01-30','2017-01-31',\n         '2017-02-01','2017-02-25','2017-02-26','2017-02-27','2017-02-28'\n        ]\ntime_df= pd.to_datetime(time_df_rep['date'], format='%Y-%m-%d')\ntrain_time_df=pd.DataFrame()\ntrain_time_df[\"year\"] = time_df.dt.year\ntrain_time_df[\"month\"] = time_df.dt.month\ntrain_time_df[\"day\"] = time_df.dt.day\ntrain_time_df[\"weekday\"] = time_df.dt.weekday\ntrain_time_df[\"workday\"] = time_df.dt.weekday\nfor i in train_time_df.index: \n     if train_time_df[\"weekday\"][i]==5 or  train_time_df[\"weekday\"][i]==6 or str(time_df.iloc[i]).split()[0] in holiday :\n        train_time_df[\"workday\"][i]=0\n     else:\n        train_time_df[\"workday\"][i]=1\n#train[\"hour\"]=train_2['Hour_slot']\n\n'''Convert Date Time into the Frequency Sine and COSINE '''\n\nfreq_df=pd.DataFrame()\nfreq_df['day_sin'] = np.sin(train_time_df.day*(2.*np.pi/30*25))\nfreq_df['day_cos'] = np.cos(train_time_df.day*(2.*np.pi/30*25))\n# Hour\nfreq_df['hour_sin'] = np.sin(train_time_df.day*(2.*np.pi/24*25))\nfreq_df['hour_cos'] = np.cos(train_time_df.day*(2.*np.pi/24*25))\n\nplt.plot(np.array(freq_df['hour_sin'][0:720]), scaley=True, scalex=True)\nplt.plot(np.array(freq_df['hour_cos'][0:720]))\nplt.xlabel('Time [h]')\nplt.title('Time of day signal')","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:06:03.250356Z","iopub.execute_input":"2021-06-30T09:06:03.250664Z","iopub.status.idle":"2021-06-30T09:06:08.031613Z","shell.execute_reply.started":"2021-06-30T09:06:03.250632Z","shell.execute_reply":"2021-06-30T09:06:08.030576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"time_df=pd.DataFrame()\ntime_df['date']= train_2['Date']\n\ntime_df= pd.to_datetime(time_df['date'], format='%Y-%m-%d')\ntrain_time_df=pd.DataFrame()\nholiday=['2016-02-06','2016-02-07','2016-02-08','2016-02-09','2016-02-10','2016-02-11','2016-02-12','2016-02-13','2016-02-14','2016-02-27','2016-02-28','2016-02-29',\n        '2016-04-02','2016-04-03','2016-04-04','2016-04-05','2016-06-09','2016-06-10','2016-06-11','2016-06-12',\n        '2016-09-15','2016-09-16','2016-09-17','2016-09-18',\n         '2016-10-09', '2016-10-10', '2016-10-11','2016-12-31','2017-01-01','2017-01-02',\n         '2017-01-27','2017-01-28','2017-01-29','2017-01-30','2017-01-31',\n         '2017-02-01','2017-02-25','2017-02-26','2017-02-27','2017-02-28'\n        ]\ntrain_time_df[\"weekday\"] = time_df.dt.weekday\ntrain_time_df[\"workday\"] = 0\nfor i in train_time_df.index: \n     if train_time_df[\"weekday\"][i]==5 or  train_time_df[\"weekday\"][i]==6 or str(time_df.iloc[i]).split()[0] in holiday :\n        train_time_df[\"workday\"][i]=0\n     else:\n        train_time_df[\"workday\"][i]=1\ntrain_time_df[\"month\"] = time_df.dt.month\ntrain_time_df[\"zone_id\"] = train_2['Zone_ID']\ndf_month_zone=pd.DataFrame()\ndf_month_zone['month']=train_time_df[\"month\"] \ndf_month_zone['zone']=train_time_df[\"zone_id\"] ","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:06:08.033375Z","iopub.execute_input":"2021-06-30T09:06:08.033682Z","iopub.status.idle":"2021-06-30T09:07:54.063401Z","shell.execute_reply.started":"2021-06-30T09:06:08.033649Z","shell.execute_reply":"2021-06-30T09:07:54.062664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_month_zone_rep = df_month_zone.groupby(df_month_zone.columns.tolist()).size().reset_index().rename(columns={0:'count'})\ndf_month_zone_rep","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:07:54.064599Z","iopub.execute_input":"2021-06-30T09:07:54.064999Z","iopub.status.idle":"2021-06-30T09:07:54.089778Z","shell.execute_reply.started":"2021-06-30T09:07:54.06497Z","shell.execute_reply":"2021-06-30T09:07:54.08913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Create Cosine Value for 1 DAY Hour value**","metadata":{}},{"cell_type":"code","source":"df=[int(i) for i in np.linspace(0, 23, 24)]\ndf=pd.DataFrame(df)\ndf\nseconds_in_day =24\ndf['sin_time'] = np.sin(2*np.pi*df[0]/seconds_in_day)\ndf['cos_time'] = np.cos(2*np.pi*df[0]/seconds_in_day)\n\nplt.plot(np.array(df['sin_time']), scaley=True, scalex=True)\nplt.plot(np.array(df['cos_time']))\nplt.xlabel('Time [h]')\nplt.title('Time of day signal')","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:07:54.090728Z","iopub.execute_input":"2021-06-30T09:07:54.091098Z","iopub.status.idle":"2021-06-30T09:07:54.276793Z","shell.execute_reply.started":"2021-06-30T09:07:54.091069Z","shell.execute_reply":"2021-06-30T09:07:54.27597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hour_df_freq=pd.DataFrame()\nhour_df_freq['hour_sine'] = pd.concat([df['sin_time']]*9150, ignore_index=True)\nhour_df_freq['hour_cosine'] = pd.concat([df['cos_time']]*9150, ignore_index=True)\nhour_df_freq","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:07:54.277966Z","iopub.execute_input":"2021-06-30T09:07:54.278296Z","iopub.status.idle":"2021-06-30T09:07:54.48041Z","shell.execute_reply.started":"2021-06-30T09:07:54.278264Z","shell.execute_reply":"2021-06-30T09:07:54.479419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"day_fre_df=pd.DataFrame()\nday_fre_df['day_sine'] = pd.concat([freq_df['hour_sin']]*24, ignore_index=True)\nday_fre_df['day_cosine'] = pd.concat([freq_df['hour_cos']]*24, ignore_index=True)\nday_fre_df","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:07:54.482616Z","iopub.execute_input":"2021-06-30T09:07:54.482916Z","iopub.status.idle":"2021-06-30T09:07:54.528174Z","shell.execute_reply.started":"2021-06-30T09:07:54.482886Z","shell.execute_reply":"2021-06-30T09:07:54.527215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_2[\"workday\"]=train_time_df[\"workday\"] ","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:07:54.530377Z","iopub.execute_input":"2021-06-30T09:07:54.530813Z","iopub.status.idle":"2021-06-30T09:07:54.536515Z","shell.execute_reply.started":"2021-06-30T09:07:54.530767Z","shell.execute_reply":"2021-06-30T09:07:54.535787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntrain_df=train_2.drop(['Hour_slot','Date'], axis=1)\ntrain_df= pd.concat([train_df, day_fre_df], axis=1)\ntrain_df=pd.concat([train_df, hour_df_freq], axis=1)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:07:54.537446Z","iopub.execute_input":"2021-06-30T09:07:54.537737Z","iopub.status.idle":"2021-06-30T09:07:54.567006Z","shell.execute_reply.started":"2021-06-30T09:07:54.537707Z","shell.execute_reply":"2021-06-30T09:07:54.566151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df=train_df[['day_cosine', 'day_sine','hour_sine', 'hour_cosine','Zone_ID','workday','Hire_count']]\ntrain_df.to_csv('processing_train_df.csv')","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:07:54.568278Z","iopub.execute_input":"2021-06-30T09:07:54.568605Z","iopub.status.idle":"2021-06-30T09:07:56.684656Z","shell.execute_reply.started":"2021-06-30T09:07:54.568575Z","shell.execute_reply":"2021-06-30T09:07:56.683663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:07:56.686233Z","iopub.execute_input":"2021-06-30T09:07:56.686657Z","iopub.status.idle":"2021-06-30T09:07:56.705035Z","shell.execute_reply.started":"2021-06-30T09:07:56.686611Z","shell.execute_reply":"2021-06-30T09:07:56.704138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Normalize Splitting Data INTO Training and Testing DATA**\nWe will using 80% data for training data, 20% for validation\n\n1. Ensure that chopping data into windows of consecutive sample is still possible\n2.Ensure that Training/Validation data will not shuffle because due to the time sereis data --> have time and Position is matter Most","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import minmax_scale\nimport pandas as pd\n##Normalize Data \ntrain_df[['Hire_count', 'Zone_ID']]= minmax_scale(train_df[['Hire_count', 'Zone_ID']],feature_range=(0, 1))\n#label_df['Hire_count']=train_df['Hire_count']\n#train_df=train_df.drop(['Hire_count'], axis=1)\n#label_df","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:07:56.706859Z","iopub.execute_input":"2021-06-30T09:07:56.707333Z","iopub.status.idle":"2021-06-30T09:07:56.972529Z","shell.execute_reply.started":"2021-06-30T09:07:56.707289Z","shell.execute_reply":"2021-06-30T09:07:56.971556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:07:56.973678Z","iopub.execute_input":"2021-06-30T09:07:56.973958Z","iopub.status.idle":"2021-06-30T09:07:56.999851Z","shell.execute_reply.started":"2021-06-30T09:07:56.973929Z","shell.execute_reply":"2021-06-30T09:07:56.998844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n=len(train_df)\ntraining_df= train_df[0:int(n*0.8)]\nval_df= train_df[int(n*0.8):]\nprint(len(training_df))\nprint(len(val_df))","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:07:57.001153Z","iopub.execute_input":"2021-06-30T09:07:57.00145Z","iopub.status.idle":"2021-06-30T09:07:57.007566Z","shell.execute_reply.started":"2021-06-30T09:07:57.001422Z","shell.execute_reply":"2021-06-30T09:07:57.006444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_df","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:07:57.009084Z","iopub.execute_input":"2021-06-30T09:07:57.009546Z","iopub.status.idle":"2021-06-30T09:07:57.038215Z","shell.execute_reply.started":"2021-06-30T09:07:57.009499Z","shell.execute_reply":"2021-06-30T09:07:57.037339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Normalize and Preprocessing for Test data**","metadata":{}},{"cell_type":"code","source":"test_df","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:07:57.039519Z","iopub.execute_input":"2021-06-30T09:07:57.03992Z","iopub.status.idle":"2021-06-30T09:07:57.060841Z","shell.execute_reply.started":"2021-06-30T09:07:57.039875Z","shell.execute_reply":"2021-06-30T09:07:57.05958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''Removing the Repeat in Time '''\ntest_time_df=pd.DataFrame()\ntest_time_df['date']= test_df['Date']\ntest_time_df['zone_id']=test_df['Zone_ID']\n# print('Unique values of ZoneID:', time_df.zone_id.unique())\ntest_time_df_rep = test_time_df.groupby(test_time_df.columns.tolist()).size().reset_index().rename(columns={0:'count'})\nprint(test_time_df_rep)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:07:57.062404Z","iopub.execute_input":"2021-06-30T09:07:57.062758Z","iopub.status.idle":"2021-06-30T09:07:57.085423Z","shell.execute_reply.started":"2021-06-30T09:07:57.062726Z","shell.execute_reply":"2021-06-30T09:07:57.084672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# '''Removing the Repeat in Time '''\n# time_df=pd.DataFrame()\n# time_df['date']= train_2['Date']\n# time_df['zone_id']=train_2['Zone_ID']\n# # print('Unique values of ZoneID:', time_df.zone_id.unique())\n# time_df_rep = time_df.groupby(time_df.columns.tolist()).size().reset_index().rename(columns={0:'count'})\n# print(time_df_rep)\n\n''' Get Training Data time '''\ntime_df_for= pd.to_datetime(test_time_df_rep['date'], format='%Y-%m-%d')\ntime_df=pd.DataFrame()\ntime_df[\"year\"] = time_df_for.dt.year\ntime_df[\"month\"] = time_df_for.dt.month\ntime_df[\"day\"] = time_df_for.dt.day\ntime_df[\"weekday\"] = time_df_for.dt.weekday\n#train[\"hour\"]=train_2['Hour_slot']\n\n'''Convert Date Time into the Frequency Sine and COSINE '''\n\nfreq_df=pd.DataFrame()\nfreq_df['day_sin'] = np.sin(time_df.day*(2.*np.pi/30*25))\nfreq_df['day_cos'] = np.cos(time_df.day*(2.*np.pi/30*25))\n# Hour\nfreq_df['hour_sin'] = np.sin(time_df.day*(2.*np.pi/24*25))\nfreq_df['hour_cos'] = np.cos(time_df.day*(2.*np.pi/24*25))\n\nplt.plot(np.array(freq_df['hour_sin'][0:720]), scaley=True, scalex=True)\nplt.plot(np.array(freq_df['hour_cos'][0:720]))\nplt.xlabel('Time [h]')\nplt.title('Time of day signal')","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:07:57.086451Z","iopub.execute_input":"2021-06-30T09:07:57.086734Z","iopub.status.idle":"2021-06-30T09:07:57.29563Z","shell.execute_reply.started":"2021-06-30T09:07:57.086706Z","shell.execute_reply":"2021-06-30T09:07:57.294684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Building DATA Windowing for Training Model**\n\n**Main feature of Windows input**\n\n1.The width (number of time steps) the input -label Windows\n\n2.The time offset between them\n\n3.Which features are used as inputs, labels, or both\n\n4.Single-Output and Mult-output predictions\n\n5.Single-time-Step and Multi-time-Step predictions\n","metadata":{}},{"cell_type":"code","source":"class WindowGenerator():\n  def __init__(self, input_width, label_width, shift,\n               train_df=train_df, val_df=val_df, \n               label_columns=None):\n    # Store the raw data.\n    self.train_df = train_df\n    self.val_df = val_df\n\n\n    # Work out the label column indices.\n    self.label_columns = label_columns\n    if label_columns is not None:\n      self.label_columns_indices = {name: i for i, name in\n                                    enumerate(label_columns)}\n    self.column_indices = {name: i for i, name in\n                           enumerate(train_df.columns)}\n\n    # Work out the window parameters.\n    self.input_width = input_width\n    self.label_width = label_width\n    self.shift = shift\n\n    self.total_window_size = input_width + shift\n\n    self.input_slice = slice(0, input_width)\n    self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n\n    self.label_start = self.total_window_size - self.label_width\n    self.labels_slice = slice(self.label_start, None)\n    self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n\n  def __repr__(self):\n    return '\\n'.join([\n        f'Total window size: {self.total_window_size}',\n        f'Input indices: {self.input_indices}',\n        f'Label indices: {self.label_indices}',\n        f'Label column name(s): {self.label_columns}'])","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:07:57.298931Z","iopub.execute_input":"2021-06-30T09:07:57.299254Z","iopub.status.idle":"2021-06-30T09:07:57.308871Z","shell.execute_reply.started":"2021-06-30T09:07:57.299221Z","shell.execute_reply":"2021-06-30T09:07:57.307739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w1 = WindowGenerator(input_width=6, label_width=1, shift=1,train_df=training_df, val_df=val_df, \n                     label_columns='Hire_count')\nw1","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:07:57.311508Z","iopub.execute_input":"2021-06-30T09:07:57.311958Z","iopub.status.idle":"2021-06-30T09:07:57.328018Z","shell.execute_reply.started":"2021-06-30T09:07:57.311914Z","shell.execute_reply":"2021-06-30T09:07:57.326939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**1.SPLIT_WINDOW_METHOD Convert All the data into the window input and Label**\n\n**2.PLOT function Visualize Data**","metadata":{}},{"cell_type":"markdown","source":"1_Spliting Data\n\nSolving model for different step input and Output Data","metadata":{}},{"cell_type":"code","source":"## Spliting the Window as we feed the input \ndef split_window(self, features): \n  inputs = features[:, self.input_slice, :]\n  labels = features[:, self.labels_slice, :]\n  if self.label_columns is not None:\n    labels = tf.stack(\n        [labels[:, :, self.column_indices[self.label_columns]]],axis=-1)\n    # labels = tf.stack(\n    #     [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n    #     axis=-1)\n  # Slicing doesn't preserve static shape information, so set the shapes\n  # manually. This way the `tf.data.Datasets` are easier to inspect\n  inputs.set_shape([None, self.input_width, None])\n  labels.set_shape([None, self.label_width, None])\n\n  return inputs, labels\n\nimport tensorflow as tf\n'''IMPORTANT OF IMPROVING YOUR CODE '''\n## Learning cool way to Extend your Objects \nWindowGenerator.split_window= split_window\n\n# Stack three slices, the length of the total window:\nexample_window = tf.stack([np.array(train_df[:w1.total_window_size]),\n                           np.array(train_df[100:100+w1.total_window_size]),\n                           np.array(train_df[200:200+w1.total_window_size])])\n\n## Example input and Output. \nexample_inputs, example_labels= w1.split_window(example_window)\nprint('All shapes are: (batch, time, features)')\nprint(f'Window shape: {example_window.shape}')\nprint(f'Inputs shape: {example_inputs.shape}')\nprint(f'labels shape: {example_labels.shape}')","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:07:57.329404Z","iopub.execute_input":"2021-06-30T09:07:57.329792Z","iopub.status.idle":"2021-06-30T09:08:02.821415Z","shell.execute_reply.started":"2021-06-30T09:07:57.32976Z","shell.execute_reply":"2021-06-30T09:08:02.820552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2.PLOT_Function","metadata":{}},{"cell_type":"code","source":"def plot(self, model=None, plot_col='Hire_count', max_subplots=3):\n  inputs, labels = self.example\n  print(inputs.shape)\n  plt.figure(figsize=(12, 8))\n  plot_col_index = self.column_indices[plot_col]\n  max_n = min(max_subplots, len(inputs))\n  for n in range(max_n):\n    plt.subplot(max_n, 1, n+1)\n    plt.ylabel(f'{plot_col} [normed]')\n    plt.plot(self.input_indices, inputs[n, :, plot_col_index],\n             label='Inputs', marker='.', zorder=-10)\n\n    if self.label_columns:\n      label_col_index = self.label_columns_indices.get(plot_col, None)\n    else:\n      label_col_index = plot_col_index\n\n    if label_col_index is None:\n      continue\n\n    plt.scatter(self.label_indices, labels[n, :, label_col_index],\n                edgecolors='k', label='Labels', c='#2ca02c', s=64)\n    \n    if model is not None:\n      predictions = model.predict(inputs)\n      #print(Predictions.shape)\n      plt.scatter(self.label_indices, predictions[n, :, label_col_index],\n                  marker='X', edgecolors='k', label='Predictions',\n                  c='#ff7f0e', s=64)\n\n    if n == 0:\n      plt.legend()\n\n  plt.xlabel('Time [h]')\n\nWindowGenerator.plot = plot","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:08:02.822622Z","iopub.execute_input":"2021-06-30T09:08:02.823154Z","iopub.status.idle":"2021-06-30T09:08:02.833264Z","shell.execute_reply.started":"2021-06-30T09:08:02.823098Z","shell.execute_reply":"2021-06-30T09:08:02.832499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## this line code set attribute example for Windowgenerator \nw1.example = example_inputs, example_labels","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:08:02.834801Z","iopub.execute_input":"2021-06-30T09:08:02.835298Z","iopub.status.idle":"2021-06-30T09:08:02.852813Z","shell.execute_reply.started":"2021-06-30T09:08:02.835265Z","shell.execute_reply":"2021-06-30T09:08:02.851751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot(self, model=None, plot_col='Hire_count', max_suplots=2):\n  inputs, labels= self.example\n  #print(inputs)\n\n  plt.figure(figsize=(12, 8))\n  plot_col_index= self.column_indices[plot_col]\n  print(plot_col_index)\n  max_n= min(max_suplots, len(inputs))\n  \n  for n in range(max_n):\n    plt.subplot(max_n, 1, n+1)\n    plt.ylabel(f'{plot_col}: [Norm]')\n\n    plt.plot(self.input_indices, inputs[n, :, plot_col_index], \n             label='Inputs', marker=\".\", zorder=-10)\n    \n    if self.label_columns: \n      #label_col_index = self.label_columns_indices.get(plot_col, None)\n      label_col_index = self.label_columns_indices.get(plot_col)\n      print(label_col_index)\n    else: \n      label_col_index= plot_col_index\n      \n    if label_col_index is None: \n      continue\n    \n    plt.scatter(self.label_indices, labels[n, :, label_col_index],\n                edgecolors='k', label='Labels', c='#2ca02c', s=64)\n    #if model is not None:\n    predictions_, predictions= model.predict(inputs)## Consider adding predict??\n    print(Predictions)\n    print(loss)\n    plt.scatter(self.label_indices, predictions[n, :, label_col_index],\n                marker='X', edgecolors='k', label='Predictions',\n                c='#ff7f0e', s=64)\n\n    if n == 0:\n      plt.legend()\n\n  plt.xlabel('Time [h]')\n\n  plt.xlabel('Time [h]')\n  \nWindowGenerator.plot=plot","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:08:02.854319Z","iopub.execute_input":"2021-06-30T09:08:02.855022Z","iopub.status.idle":"2021-06-30T09:08:02.866863Z","shell.execute_reply.started":"2021-06-30T09:08:02.854982Z","shell.execute_reply":"2021-06-30T09:08:02.865851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w1.plot()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:08:02.867964Z","iopub.execute_input":"2021-06-30T09:08:02.868406Z","iopub.status.idle":"2021-06-30T09:08:03.602815Z","shell.execute_reply.started":"2021-06-30T09:08:02.868375Z","shell.execute_reply":"2021-06-30T09:08:03.601614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Create TF.data.Dataset -- Keras Processing Time Series Data**\n**Purpose TURN Dataframe to tf.data.Dataset (input_window, label_window)**\n\n**tf.data.Dataset you can easily iterate over the Data --> Dataset.element_spec property tell you the structure, dtypes and shape to the dataset elements**\n","metadata":{}},{"cell_type":"code","source":"'''Convert data to tensor with certain batch_size'''\ndef make_dataset(self, data): \n  ## Convert data to Numpy array\n  data= np.array(data, dtype= np.float32)\n\n  ds= tf.keras.preprocessing.timeseries_dataset_from_array(\n      data=data, \n      targets=None, \n      sequence_length= self.total_window_size, \n      sequence_stride=1, \n      shuffle= True, \n      batch_size=32, )\n  ds= ds.map(self.split_window)\n  return ds\n\nWindowGenerator.make_dataset= make_dataset\n'''The WindowGenerator adding property to access training - val datset'''\n## Traindata\n@property\ndef train(self): \n  return self.make_dataset(self.train_df)\n#Valdata\n@property\ndef val(self): \n  return self.make_dataset(self.val_df)\n@property\ndef example(self): \n  \"\"\"Get and Cache an example batch of 'Input', 'labels' for plotting\"\"\"\n  result= getattr(self, '_example', None)\n  if result is None: \n    #No example batch was found, so get from the train dataset\n    result= next(iter(self.train))\n    #cache it for the next time\n    self._example= result\n  return result\n\nWindowGenerator.train=train\nWindowGenerator.val=val\nWindowGenerator.example=example","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:08:03.604491Z","iopub.execute_input":"2021-06-30T09:08:03.605098Z","iopub.status.idle":"2021-06-30T09:08:03.614969Z","shell.execute_reply.started":"2021-06-30T09:08:03.605047Z","shell.execute_reply":"2021-06-30T09:08:03.614207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w1.plot()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:08:03.61657Z","iopub.execute_input":"2021-06-30T09:08:03.616927Z","iopub.status.idle":"2021-06-30T09:08:04.334001Z","shell.execute_reply.started":"2021-06-30T09:08:03.616893Z","shell.execute_reply":"2021-06-30T09:08:04.332937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#1 Inspecting data \nprint(w1.train.element_spec)\n#2 Iterate over dataset and Yield concrete batches\nfor example_inputs, example_labels in w1.train.take(1): \n  print(f'Inputs shape (batch, time, features): {example_inputs.shape}')\n  print(f'Labels shape(batch, time, features): {example_labels.shape}')","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:08:04.335407Z","iopub.execute_input":"2021-06-30T09:08:04.335982Z","iopub.status.idle":"2021-06-30T09:08:04.493882Z","shell.execute_reply.started":"2021-06-30T09:08:04.335936Z","shell.execute_reply":"2021-06-30T09:08:04.492521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Model window Generator will Base On Two Approach**\n**1.Single Steps model**\n\n**2.Multi-steps input and outputs models**\n\n**3.Multiple steps Mode**","metadata":{}},{"cell_type":"markdown","source":"![](http://)","metadata":{}},{"cell_type":"code","source":"def compile_and_fit(model, window, patience=20):\n  #early_stopping= tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n  #                                                 patience= patience, mode='min')\n  model.compile(loss=tf.losses.MeanSquaredError(), \n                optimizer=tf.optimizers.Adam(), \n                metrics=[tf.metrics.MeanAbsoluteError()])\n  history= model.fit_generator(window.train, epochs=EPOCHS, \n                     validation_data=window.val, callbacks=[early_stopping]\n                     )\n  return history","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:08:04.495453Z","iopub.execute_input":"2021-06-30T09:08:04.495883Z","iopub.status.idle":"2021-06-30T09:08:04.502296Z","shell.execute_reply.started":"2021-06-30T09:08:04.495845Z","shell.execute_reply":"2021-06-30T09:08:04.500931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nDense = tf.keras.Sequential([\n    tf.keras.layers.Dense(units=64, activation='relu'),\n    tf.keras.layers.Dense(units=64, activation='relu'),\n    tf.keras.layers.Dense(units=1)\n])\n\nmultiple_steps_window = WindowGenerator(\n    input_width=24, label_width=24, shift=1,\n    train_df=training_df, val_df=val_df, \n                     label_columns='Hire_count')\n\nprint('Input shape', multiple_steps_window.example[0].shape)\nprint('Output shape', Dense(multiple_steps_window.example[0]).shape)\n\n\ndef compile_and_fit(model, window, patience=20):\n  #early_stopping= tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n#                                                   patience= patience, mode='min')\n  model.compile(loss=tf.losses.MeanSquaredError(), \n                optimizer=tf.optimizers.Adam(), \n                metrics=[tf.metrics.MeanAbsoluteError()])\n # history= model.fit_generator(window.train, epochs=EPOCHS, \n  #                   validation_data=window.val, callbacks=[early_stopping])\n  history= model.fit_generator(window.train, epochs=EPOCHS, \n                     validation_data=window.val)\n                     \n  return history, model\nEPOCHS=50\nval_performance = {}\nhistory, model= compile_and_fit(Dense, multiple_steps_window)\nval_performance['Dense_multiple_steps'] = Dense.evaluate(multiple_steps_window.val)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:08:04.504052Z","iopub.execute_input":"2021-06-30T09:08:04.504531Z","iopub.status.idle":"2021-06-30T09:32:39.489747Z","shell.execute_reply.started":"2021-06-30T09:08:04.504484Z","shell.execute_reply":"2021-06-30T09:32:39.488883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Dense.save_weights('Dense_multiple_steps_ouputs.h5')\nmultiple_steps_window.plot(model= Dense)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:32:39.4911Z","iopub.execute_input":"2021-06-30T09:32:39.491432Z","iopub.status.idle":"2021-06-30T09:32:39.859423Z","shell.execute_reply.started":"2021-06-30T09:32:39.491403Z","shell.execute_reply":"2021-06-30T09:32:39.858326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## We need to Connect all Three Multiple timestep together FLATTEN layer\nDense_single = tf.keras.Sequential([\n    # Connect all multiple steps flatten()                     \n    tf.keras.layers.Flatten(),               \n    tf.keras.layers.Dense(units=64, activation='relu'),\n    tf.keras.layers.Dense(units=64, activation='relu'),\n    tf.keras.layers.Dense(units=1),\n    ## there also need to reshape the output layers \n    tf.keras.layers.Reshape([1, -1])\n])\n\n## We Using 6hours to Predict 1 hours Outputs \nsing_step_out_window = WindowGenerator(\n    input_width=4, label_width=1, shift=1,\n    train_df=training_df, val_df=val_df, \n                     label_columns='Hire_count')\n\nprint('Input shape', sing_step_out_window.example[0].shape)\nprint('Output shape', Dense(sing_step_out_window.example[0]).shape)\n\nEPOCHS=50\n#val_performance = {}\nhistory= compile_and_fit(Dense_single, sing_step_out_window)\nval_performance['Dense_Single_step'] = Dense_single.evaluate(sing_step_out_window.val)\nDense_single.save_weights('Dense_Single_steps_output.h5')","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:32:39.860838Z","iopub.execute_input":"2021-06-30T09:32:39.861173Z","iopub.status.idle":"2021-06-30T09:49:26.644925Z","shell.execute_reply.started":"2021-06-30T09:32:39.86114Z","shell.execute_reply":"2021-06-30T09:49:26.643958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## We Using 6hours to Predict 1 hours Outputs \nsing_step_out_window = WindowGenerator(\n    input_width=4, label_width=1, shift=1,\n    train_df=training_df, val_df=val_df, \n                     label_columns='Hire_count')\nsing_step_out_window.plot(model=Dense)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:49:26.646037Z","iopub.execute_input":"2021-06-30T09:49:26.646365Z","iopub.status.idle":"2021-06-30T09:49:27.095936Z","shell.execute_reply.started":"2021-06-30T09:49:26.646333Z","shell.execute_reply":"2021-06-30T09:49:27.095041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''Create Window Generator for Conv1D model'''\n## 3 input steps and 1 output steps\nconv_width=4\nconv_window=WindowGenerator(input_width=conv_width, label_width=1, shift=1,\n    train_df=training_df, val_df=val_df, \n                     label_columns='Hire_count')\nprint(conv_window)\nprint(\"Conv model on conv Window\")\nprint(\"input_shape: \", conv_window.example[0].shape)\n\nConv1D_model= tf.keras.Sequential([\n                                   tf.keras.layers.Conv1D(filters=64, kernel_size=(conv_width,), activation='relu' ),\n                                   tf.keras.layers.Dense(units=64, activation='relu'), \n                                   tf.keras.layers.Dense(units=32, activation='relu'), \n                                   tf.keras.layers.Dense(units=1), \n])\nprint(\"Output_shape\", Conv1D_model(conv_window.example[0]).shape)\n\nEPOCHS=50\n#val_performance = {}\nhistory= compile_and_fit(Conv1D_model, conv_window)\nval_performance['Conv1_Single_step'] = Conv1D_model.evaluate(conv_window.val)\nConv1D_model.save_weights('Conv1D_single_steps_output_2hidden_Dense.h5')","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:49:27.097224Z","iopub.execute_input":"2021-06-30T09:49:27.097498Z","iopub.status.idle":"2021-06-30T10:05:34.729541Z","shell.execute_reply.started":"2021-06-30T09:49:27.09747Z","shell.execute_reply":"2021-06-30T10:05:34.728594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"conv_window.plot(Conv1D_model)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T10:05:34.731178Z","iopub.execute_input":"2021-06-30T10:05:34.731631Z","iopub.status.idle":"2021-06-30T10:05:35.101347Z","shell.execute_reply.started":"2021-06-30T10:05:34.731585Z","shell.execute_reply":"2021-06-30T10:05:35.100383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"conv_width=4\nLABEL_WIDTH = 24\nINPUT_WIDTH = LABEL_WIDTH + (conv_width - 1)\n\nwide_conv_window=WindowGenerator(input_width=INPUT_WIDTH, label_width=LABEL_WIDTH, shift=1,\n    train_df=training_df, val_df=val_df, \n                     label_columns='Hire_count')\nprint(wide_conv_window)\nprint(\"Conv model on conv Window\")\nprint(\"input_shape: \", wide_conv_window.example[0].shape)\nprint(\"Output_shape\", Conv1D_model(wide_conv_window.example[0]).shape)\n\nEPOCHS=50\nval_performance = {}\nhistory= compile_and_fit(Conv1D_model, wide_conv_window)\nval_performance['Conv1D_Multiple_steps'] = Conv1D_model.evaluate(wide_conv_window.val)\nConv1D_model.save_weights('Conv1D_multiple_steps_output_2hidden.h5')","metadata":{"execution":{"iopub.status.busy":"2021-06-30T10:05:35.103063Z","iopub.execute_input":"2021-06-30T10:05:35.103522Z","iopub.status.idle":"2021-06-30T10:32:40.24656Z","shell.execute_reply.started":"2021-06-30T10:05:35.103468Z","shell.execute_reply":"2021-06-30T10:32:40.245821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"''' LSTM model will training on the same \n1 Conv Window // 4 hours Input -- predict 1 hour Output \n2 wide Conv Window // 4 hours input -- 4 Hours outputs\n'''\n'''SECTION 1 Train on Multiple inputs steps 1 outputs steps'''\n# [[Model can improve by adding more layers and Neuron units]]\n# lstm_model= tf.keras.Sequential([\n#                                  tf.keras.layers.LSTM(64, return_sequences=True),\n#                                  tf.keras.layers.Dense(units=1)\n# ])\n# print('Input shape:', conv_window.example[0].shape)\n# print('Output shape:', lstm_model(conv_window.example[0]).shape)\n# ## this Mod\n# EPOCHS=50\n# val_performance = {}\n# history= compile_and_fit(lstm_model, conv_window)\n# val_performance['LSTM'] = lstm_model.evaluate(conv_window.val)\n# lstm_model.save_weights('LSTM_Single_steps_output.h5')\n\n'''SECTION 1 Train on Multiple inputs steps and Multiple outputs steps'''\nnum_features=6\nlstm_model= tf.keras.Sequential([\n                          tf.keras.layers.LSTM(32, return_sequences=True),\n                           tf.keras.layers.LSTM(32, return_sequences=True),\n                          # Shape => [batch, time, features]\n                          tf.keras.layers.Dense(units=num_features)\n])\nprint('Input shape:', wide_conv_window.example[0].shape)\nprint('Output shape:', lstm_model(wide_conv_window.example[0]).shape)\n## this Mod\n# EPOCHS=50\n# val_performance = {}\n# history= compile_and_fit(lstm_model, wide_conv_window)\n# val_performance['LSTM'] = lstm_model.evaluate(wide_conv_window.val)\n# lstm_model.save_weights('LSTM_Multiple_steps_output.h5')","metadata":{"execution":{"iopub.status.busy":"2021-06-30T10:32:40.247703Z","iopub.execute_input":"2021-06-30T10:32:40.248141Z","iopub.status.idle":"2021-06-30T10:32:40.768997Z","shell.execute_reply.started":"2021-06-30T10:32:40.248097Z","shell.execute_reply":"2021-06-30T10:32:40.767922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Define multiple steps OUTPUT \nOUT_STEPS= 24\nhour_input= 24\nnum_features=6\nmulti_window= WindowGenerator(input_width =hour_input, label_width= OUT_STEPS, train_df=training_df,\n                              val_df= val_df, shift= OUT_STEPS)\nmulti_window.plot()\n\n### Model Architecture will Train on Dense -- CNN -- LSTM model \n## Dense Model \nmulti_dense_model= tf.keras.Sequential([\n                                  #Take the last time steps \n                                  #shape [Batch_size,  time, features] => [batch, 1, features]\n                                  tf.keras.layers.Lambda(lambda x: x[:, -1:, :]),\n                                  #shape => > [batch, 1, dense_units]\n                                  tf.keras.layers.Dense(512, activation='relu'), \n                                  tf.keras.layers.Dense(214, activation='relu'), \n                                  #Shape ==> [batch, 1, out_steps*features]\n                                  tf.keras.layers.Dense(OUT_STEPS*num_features, \n                                                        kernel_initializer= tf.initializers.zeros()), \n                                  #shape=> [batch, out_steps, features]\n                                  tf.keras.layers.Reshape([OUT_STEPS, num_features])\n                                  \n])\nhistory= compile_and_fit(multi_dense_model, multi_window)\nEPOCHS=50\nval_performance_mutliple_steps = {}\nval_performance_mutliple_steps['Multi_Dense'] = multi_dense_model.evaluate(multi_window.val)\nmulti_dense_model.save('Multi_Dense_steps_output.h5')","metadata":{"execution":{"iopub.status.busy":"2021-06-30T10:32:40.770428Z","iopub.execute_input":"2021-06-30T10:32:40.770823Z","iopub.status.idle":"2021-06-30T10:32:41.084073Z","shell.execute_reply.started":"2021-06-30T10:32:40.77078Z","shell.execute_reply":"2021-06-30T10:32:41.082423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Model 1 layer CONV1D and 1 Layer Hidden \n\nCONV_WIDTH=3\nmulti_conv_model= tf.keras.Sequential([\n                                       #shape [batch, time, conv_Units] => [batch, CONV_WIDTH, features]\n                                       tf.keras.layers.Lambda(lambda x:  x[:, -CONV_WIDTH:, :]), \n                                       #Shape => [batch, 1, conv_units]\n                                       tf.keras.layers.Conv1D(256, activation='relu', kernel_size=(CONV_WIDTH)), \n                                       #Shape => [batch, 1, out_steps*features ]\n                                       tf.keras.layers.Dense(units=256,  activation='relu'),\n                                       tf.keras.layers.Dense(OUT_STEPS*num_features, kernel_initializer=tf.initializers.zeros()), \n                                       #shape => [batch, out_step, features]\n                                       tf.keras.layers.Reshape([OUT_STEPS, num_features])    \n])\n\nEPOCHS=50\nhistory= compile_and_fit(multi_dense_model, multi_window)\nval_performance_mutliple_steps = {}\nval_performance_mutliple_steps['Multi_Conv_Output'] = multi_dense_model.evaluate(multi_window.val)\nmulti_dense_model.save('Multi_Conv_steps_output.h5')\n","metadata":{"execution":{"iopub.status.busy":"2021-06-30T10:32:41.085027Z","iopub.status.idle":"2021-06-30T10:32:41.085483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"multi_lstm_model = tf.keras.Sequential([\n    # Shape [batch, time, features] => [batch, lstm_units]\n    # Adding more `lstm_units` just overfits more quickly.\n    tf.keras.layers.LSTM(64, return_sequences=False),\n    tf.keras.layers.Dense(units=256, activation='relu'), \n    # Shape => [batch, out_steps*features]\n    tf.keras.layers.Dense(OUT_STEPS*num_features,\n                          kernel_initializer=tf.initializers.zeros()),\n    # Shape => [batch, out_steps, features]\n    tf.keras.layers.Reshape([OUT_STEPS, num_features])\n])\n\nhistory = compile_and_fit(multi_lstm_model, multi_window)\n# IPython.display.clear_output()\n\nmulti_val_performance['LSTM'] = multi_lstm_model.evaluate(multi_window.val)\nmulti_performance['LSTM'] = multi_lstm_model.evaluate(multi_window.test, verbose=0)\nmulti_window.plot(multi_lstm_model)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T10:32:41.087138Z","iopub.status.idle":"2021-06-30T10:32:41.087592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_features=6\nclass FeedBack(tf.keras.Model):\n  def __init__(self, units, out_steps):\n    super().__init__()\n    self.out_steps = out_steps\n    self.units = units\n    self.lstm_cell = tf.keras.layers.LSTMCell(units)\n    # Also wrap the LSTMCell in an RNN to simplify the `warmup` method.\n    self.lstm_rnn = tf.keras.layers.RNN(self.lstm_cell, return_state=True)\n    self.dense = tf.keras.layers.Dense(num_features)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T10:32:41.088741Z","iopub.status.idle":"2021-06-30T10:32:41.089175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feedback_model= FeedBack(units=32, out_steps=OUT_STEPS)\ndef warmup(self, inputs): \n  #inputs.shape => (batch, time, features)\n  #x.shape => (batch, lstm_units)\n  x, *state= self.lstm_rnn(inputs)\n\n  #predictions.shape => (batch, features)\n  prediction= self.dense(x)\n  return prediction, state\nFeedBack.warmup= warmup\n\nprediction, state= feedback_model.warmup(multi_window.example[0])\nprint(prediction.shape)\n\ndef call(self, inputs, training=None):\n  # Use a TensorArray to capture dynamically unrolled outputs.\n  predictions = []\n  # Initialize the lstm state\n  prediction, state = self.warmup(inputs)\n  \n  # Insert the first prediction\n  predictions.append(prediction)\n\n  # Run the rest of the prediction steps\n  for n in range(1, self.out_steps):\n    # Use the last prediction as input.\n    x = prediction\n    # Execute one lstm step.\n    x, state = self.lstm_cell(x, states=state,\n                              training=training)\n    # Convert the lstm output to a prediction.\n    prediction = self.dense(x)\n    # Add the prediction to the output\n    predictions.append(prediction)\n\n  # predictions.shape => (time, batch, features)\n  predictions = tf.stack(predictions)\n  # predictions.shape => (batch, time, features)\n  predictions = tf.transpose(predictions, [1, 0, 2])\n  return predictions\n\nFeedBack.call = call","metadata":{"execution":{"iopub.status.busy":"2021-06-30T10:32:41.089986Z","iopub.status.idle":"2021-06-30T10:32:41.090437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for n in range (1,self.out_step):\n    x= prediction\n    # Use the last prediction \n    x , state = self.lstm_cell(x,states=state,\n                               training=training)\n    predictions = tf.stack(predictions)\n    predictions = tf.stack(predictions)\n    # Convert the lstm output to a prediction.\n    predictions.append(prediction)\n    # Add the prediction to the output\n    predictions.append(prediction)\n# prediction.shape =>(time, batch, features)\npredictions = tf.transpose(prediction, [1, 0, 2])\nreturn predictions\n\nFeedBack.call = call\n ","metadata":{"execution":{"iopub.status.busy":"2021-06-30T10:32:41.091544Z","iopub.status.idle":"2021-06-30T10:32:41.091977Z"},"trusted":true},"execution_count":null,"outputs":[]}]}