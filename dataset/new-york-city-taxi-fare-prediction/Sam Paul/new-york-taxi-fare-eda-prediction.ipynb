{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"# Data processing\nimport numpy as np\nimport pandas as pd\nimport datetime as dt\nfrom datetime import date\n\n\n# Visualization libaries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.figure_factory as ff\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\nimport plotly.plotly as py\nfrom plotly import tools\n%matplotlib inline\n\n# Machine Learning\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"test = pd.read_csv('../input/test.csv')\ntrain = pd.read_csv('../input/train.csv', nrows = 1_000_000)\n\nprint (\"Train Dataset: Rows, Columns: \", train.shape)\nprint (\"Test Dataset: Rows, Columns: \", test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0e1cce1cdedb17b54c97df7fff9410cf2ad45921","collapsed":true},"cell_type":"code","source":"print (\"Glimpse of Train Dataset: \")\ntrain.head(n=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d234358f7b5c735e6c791dfe2684f66172bf642c","collapsed":true},"cell_type":"code","source":"test.head(n=3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"26c4f5af807f4119cce9f3a02e6b68ceca819851"},"cell_type":"markdown","source":"Seems like there are several outliers in the selected train dataset!!\nLets arrange the dataset for use."},{"metadata":{"trusted":true,"_uuid":"9321b5d249bcd89cd022454cee69ed747836b356","collapsed":true},"cell_type":"code","source":"# Lets Check out the missing value columns\nprint (\"Top Columns having missing values\")\nmissmap = train.isnull().sum().to_frame().sort_values(0, ascending = False)\nmissmap.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"70a5fcaa96b4e35e4d2858a6e2cb75e17af8ae96"},"cell_type":"markdown","source":"There seems to have 10 missing values. Lets get rid of them first, since they are only a very tiny portion of data."},{"metadata":{"trusted":true,"_uuid":"7b67aa826fd72989ef0c419c4c5e802eb521d1d3","collapsed":true},"cell_type":"code","source":"train = train.dropna() # dropping the NAN values\nprint (\"Number of missing values after dropping NaNs\")\nmissmap = train.isnull().sum().to_frame().sort_values(0, ascending = False)\nmissmap.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1a1688325522f7cf3e394e27ee214818ed40cf51"},"cell_type":"markdown","source":"Looks like we have cleaned the 'Nan' Rows. Yaay! Lets check the outliers now (if any, by plotting!)"},{"metadata":{"trusted":true,"_uuid":"928eaf2378a964f1d4b008328fc7236009ce5469","collapsed":true},"cell_type":"code","source":"print (\"Summary of Train Dataset: \")\ntrain.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"af57bae9244bff4020c5699f0f9f51b20dfe4d07","collapsed":true},"cell_type":"code","source":"# Plotting the histogram of counts of passengers,\n# binning it into 208 parts from the fact that the maximum value is 208\nx = train['passenger_count']\nplt.hist(x, bins=208, color = 'yellow', edgecolor = 'black')\nplt.ylabel('Counts')\nplt.xlabel('Number of Passengers per trip')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4372ee2f9eb6dbc668e483c9504f4cf862a3b453"},"cell_type":"markdown","source":"Clearly there are hardly any substantial count greater than 5 passengers per trip! lets eliminate thos extreme outlier rows."},{"metadata":{"trusted":true,"_uuid":"035b9918d0fe99abce1956cb0ea239c9a1615878","collapsed":true},"cell_type":"code","source":"# selecting rows with 5 passengers or less only\ntrain = train.loc[train['passenger_count'] <= 5]\ntrain.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dedfa4fa6d3a4dd15f84d77efa93979cc4084ebc"},"cell_type":"markdown","source":"The Max Latitude and Longitude Looks way out of place, consideing this as a data input eroor. Lets get rid of this now!"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"eaa1b8cf2c14892998616c30396eb34583393963"},"cell_type":"code","source":"# Statistically Outliers are considered mean values - 3 times the standard deviation,\n# but considering the Latitude and longitude to be decimal point sensitive, \n#I would personally stringent the factor to 2 \n\ncolumns_to_select = [ 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']\n\nfor column in columns_to_select:\n    train = train.loc[(train[column] > (train[column].mean() - train[column].std() * 2 )) & (train[column] < (train[column].mean() + train[column].std() * 2 ))]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8e93abf2470f81053887d61a9d453979690eafe3","collapsed":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f17966c7ff98f6bcbeefa56defab2aaec422da7f"},"cell_type":"markdown","source":"Still We can see that there are lots of negetive fare amount. Lets eliminate those columns!"},{"metadata":{"trusted":true,"_uuid":"d255609da637a3ca489f91a835be307f64bede3d","collapsed":true},"cell_type":"code","source":"train = train.loc[train['fare_amount'] >= 0]\ntrain.describe()\n# 33 data points have been eliminated","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"56964c5f2a06a5dd9b436340190d74db05e5eff0"},"cell_type":"code","source":"# renaming our Cleaned Dataset\ntrain_clean = train","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"24447ae3f0868a81a79664d4c0e8441ad2487d7c"},"cell_type":"markdown","source":"Now lets create a colum for distance travelled to from latitude and longitude positions. For this we generally need haversine's Formulae."},{"metadata":{"trusted":true,"_uuid":"bcd078ac4ca7c3d64123677a24c363c2f02d9703","collapsed":true},"cell_type":"code","source":"   # Haversine formula:\ta = sin²(Δφ/2) + cos φ1 ⋅ cos φ2 ⋅ sin²(Δλ/2)\nR = 6371e3 # Metres\nphi1 = np.radians( train_clean['pickup_latitude'])\nphi2 = np.radians( train_clean['dropoff_latitude'])\n\ndelta_phi = np.radians( train_clean['pickup_latitude'] - train_clean['dropoff_latitude'])\ndelta_lambda = np.radians( train_clean['pickup_longitude'] - train_clean['dropoff_longitude'])\na = np.sin(delta_phi / 2)**2 + np.cos(phi1) * np.cos(phi2) * np.sin(delta_lambda / 2)**2\nc = 2 * np.arctan2(a ** .5, (1-a) ** .5)\nd = R * c\ntrain_clean['haversine'] = 0.000621371 *d # converting Meters to miles\ntrain_clean.head(n=10)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c2363b40de7b6df3da7f0dfda29b350fce4560f9","collapsed":true},"cell_type":"code","source":"train_clean['haversine'].describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f318fee392e6bbf6f1e2e92d8247db68ac81b1c6"},"cell_type":"markdown","source":"We can see a mean of 2.08 miles are travelled and a maximum value of 1064 miles has been travelled too!!"},{"metadata":{"trusted":true,"_uuid":"1cb5aa6f0f1cb1092db8bcd4dab4ca4a81c37a3a","collapsed":true},"cell_type":"code","source":"   # Lets de-construct the date, time, weekday and weekend factors.\ntrain_clean['pickup_datetime'] = pd.to_datetime(train_clean['pickup_datetime'])\ntrain_clean['hour_of_day'] = train_clean.pickup_datetime.dt.hour\ntrain_clean['day'] = train_clean.pickup_datetime.dt.day\ntrain_clean['week'] = train_clean.pickup_datetime.dt.weekday\ntrain_clean['month'] = train_clean.pickup_datetime.dt.month\ntrain_clean['day_of_year'] = train_clean.pickup_datetime.dt.dayofyear\ntrain_clean['week_of_year'] = train_clean.pickup_datetime.dt.weekofyear","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"be8ff54924e14ffd6868191af83c029b9c35da46","collapsed":true},"cell_type":"code","source":"train_clean.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"86bd35627685bfa1bb9559452e037d7ca9f90d81"},"cell_type":"markdown","source":"Lets plot Some Histogram plots with time Stamp trips"},{"metadata":{"_uuid":"7d334fdd857db5489a424b6a62823b52cb92138e"},"cell_type":"markdown","source":"**Yearly Trips**"},{"metadata":{"trusted":true,"_uuid":"2f093da42c75417081bd9a9560e3de8f46d88fcf","collapsed":true},"cell_type":"code","source":"\ng_yr = train_clean.groupby('day_of_year')\nyr = g_yr.count()\nyr = yr.reset_index()\n\nax =  yr.set_index('day_of_year')[['key']].plot.bar(figsize=(200, 40), legend=True, fontsize=12, \n                 edgecolor = 'black', \n                 alpha=0.49, \n                 color = 'hotpink') \n\nax.set_xlabel(\"Days of Year\", fontsize=30)\nax.set_ylabel(\"Counts of Trips\", fontsize=30)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"30613300800c35cce1d70628be08deeb9eff56db"},"cell_type":"markdown","source":"**Trips in Weeks of the year**"},{"metadata":{"trusted":true,"_uuid":"550907228d0c516ce7364d2ca87187af512a0abe","collapsed":true},"cell_type":"code","source":"g_wk = train_clean.groupby('week_of_year')\nwk = g_wk.count()\nwk = wk.reset_index()\n\nax =  wk.set_index('week_of_year')[['key']].plot.bar(figsize=(40, 20), legend=True, fontsize=24, \n                 edgecolor = 'black', \n                 alpha=0.79, \n                 color = 'purple') \n\nax.set_xlabel(\"Weeks of Year\", fontsize=30)\nax.set_ylabel(\"Counts of Trips\", fontsize=30)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c2d93d63a2748279f66fdc1e4bce8ddbb330530e"},"cell_type":"markdown","source":"**Monthly Trips**"},{"metadata":{"trusted":true,"_uuid":"86cd7f072c3773e2aec02e71d8b1ca1855c743ed","collapsed":true},"cell_type":"code","source":"g_mn = train_clean.groupby('month')\nmn = g_mn.count()\nmn = mn.reset_index()\n\nax =  mn.set_index('month')[['key']].plot.bar(figsize=(40, 20), legend=True, fontsize=24, \n                 edgecolor = 'black', \n                 alpha=0.79, \n                 color = 'darkcyan') \n\nax.set_xlabel(\"Month of Year\", fontsize=30)\nax.set_ylabel(\"Counts of Trips\", fontsize=30)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6db92a1d91920c3b70a373abf6f227879716c863"},"cell_type":"markdown","source":"**Trips in Hour of Day**"},{"metadata":{"trusted":true,"_uuid":"0483ca9b28489933d7a3b2f4f5ee81a0193b4c07","collapsed":true},"cell_type":"code","source":"g_hr = train_clean.groupby('hour_of_day')\nhr = g_hr.count()\nhr = hr.reset_index()\n\nax =  hr.set_index('hour_of_day')[['key']].plot.bar(figsize=(40, 20), legend=True, fontsize=24, \n                 edgecolor = 'black', \n                 alpha=0.79, \n                 color = 'orange') \n\nax.set_xlabel(\"Hour of Day\", fontsize=30)\nax.set_ylabel(\"Counts of Trips\", fontsize=30)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7305bd548e1adcd5ac5b29a2393c8277eb382148"},"cell_type":"markdown","source":"**Trips in Weekdays**"},{"metadata":{"trusted":true,"_uuid":"38e18b3fb7df562c6c3574b79c68b3af491e6184","collapsed":true},"cell_type":"code","source":"g_wk_ = train_clean.groupby('week')\nwk_= g_wk_.count()\nwk_= wk_.reset_index()\n\nax =  wk_.set_index('week')[['key']].plot.bar(figsize=(40, 20), legend=True, fontsize=24, \n                 edgecolor = 'black', \n                 alpha=0.79, \n                 color = 'salmon') \n\nax.set_xlabel(\"Weekday\", fontsize=30)\nax.set_ylabel(\"Counts of Trips\", fontsize=30)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"583ec8b2a7b8540c0cd915b13c3fc35f7a7ff4db"},"cell_type":"markdown","source":"**Co-Relation Plot**"},{"metadata":{"trusted":true,"_uuid":"4fe0110af08bee2b39422cc46f2f88ab43d06dd0","collapsed":true},"cell_type":"code","source":"d = train_clean[[ 'fare_amount', 'haversine', 'passenger_count', 'hour_of_day', 'week', 'month' ]]\ncorr = d.corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\ncolormap = plt.cm.RdBu\nplt.figure(figsize=(8,8))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(d.corr(),linewidths=0.5,vmax=1.0, mask=mask,\n            square=True, cmap=colormap, linecolor='white', annot=True, cbar_kws={\"shrink\": .5})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ee2d28d828cdf90909660be64d0e5f1e23dcc914"},"cell_type":"markdown","source":"Lets convert the test sets"},{"metadata":{"trusted":true,"_uuid":"f9a3c7b79ec3c63ab0fa9d427b8a500aa08b7c58","collapsed":true},"cell_type":"code","source":"test.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8a08cbdf9daa5ca8c42ee9a3ff6a556b8b5ee6e7","collapsed":true},"cell_type":"code","source":"\n   # Haversine formula:\ta = sin²(Δφ/2) + cos φ1 ⋅ cos φ2 ⋅ sin²(Δλ/2)\nR = 6371e3 # Metres\nphi1 = np.radians( test['pickup_latitude'])\nphi2 = np.radians( test['dropoff_latitude'])\n\ndelta_phi = np.radians( test['pickup_latitude'] - test['dropoff_latitude'])\ndelta_lambda = np.radians( test['pickup_longitude'] - test['dropoff_longitude'])\na = np.sin(delta_phi / 2)**2 + np.cos(phi1) * np.cos(phi2) * np.sin(delta_lambda / 2)**2\nc = 2 * np.arctan2(a ** .5, (1-a) ** .5)\nd = R * c\ntest['haversine'] = 0.000621371 *d # converting Meters to miles\n\n\n# generating Similar columns for Test set too\ntest['pickup_datetime'] = pd.to_datetime(test['pickup_datetime'])\ntest['hour_of_day'] = test.pickup_datetime.dt.hour\ntest['day'] = test.pickup_datetime.dt.day\ntest['week'] = test.pickup_datetime.dt.weekday\ntest['month'] = test.pickup_datetime.dt.month\ntest['day_of_year'] = test.pickup_datetime.dt.dayofyear\ntest['week_of_year'] = test.pickup_datetime.dt.weekofyear\n\ntest.head(n=3)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6cdc3269ef6ce3f4b589ac166ca7b80bb6c049d4","collapsed":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6876a7a2729f09541b3f44ee9ddb38cba879da9a","collapsed":true},"cell_type":"code","source":"# Let's drop all the irrelevant features\n# train_features_to_keep = ['haversine', 'hour_of_day', 'fare_amount']\n#  train_clean.drop(train_clean.columns.difference(train_features_to_keep), 1, inplace=True)\ntrain_model_ = train_clean\ntest_model_ = test\n# test_features_to_keep = ['haversine', 'hour_of_day', 'key']\n# test.drop(test.columns.difference(test_features_to_keep), 1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9d8e90c299f3588d58850defbafc70afcf66742f","collapsed":true},"cell_type":"code","source":"test_model_.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"268ee05341c027218b885ce4406a39bc4707d9a7","collapsed":true},"cell_type":"code","source":"train_model= train_model_[['haversine', 'hour_of_day', 'fare_amount']]\ntest_model = test_model_[['haversine', 'hour_of_day', 'key']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"91da7906df0ebe7c9dade387e6d6a6f36a874a50","collapsed":true},"cell_type":"code","source":"train_model.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3098e90240a46514dbdf9362af9593ee2cb9a6e1"},"cell_type":"markdown","source":"**XG_Boost Model**"},{"metadata":{"trusted":true,"_uuid":"ad1295b6bd7cbef33c9f64868c2ff08c039e29d5","collapsed":true},"cell_type":"code","source":"x_pred = test_model.drop('key', axis=1)\n\n# Let's run XGBoost and predict those fares!\nx_train,x_test,y_train,y_test = train_test_split(train_model.drop('fare_amount',axis=1),train_model.pop('fare_amount'),random_state=123,test_size=0.2)\n\ndef XGBmodel(x_train,x_test,y_train,y_test):\n    matrix_train = xgb.DMatrix(x_train,label=y_train)\n    matrix_test = xgb.DMatrix(x_test,label=y_test)\n    model=xgb.train(params={'objective':'reg:linear','eval_metric':'rmse'}\n                    ,dtrain=matrix_train,num_boost_round=200, \n                    early_stopping_rounds=10,evals=[(matrix_test,'test')],)\n    return model\n\nmodel=XGBmodel(x_train,x_test,y_train,y_test)\nprediction = model.predict(xgb.DMatrix(x_pred), ntree_limit = model.best_ntree_limit)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7b5beed5041e561fba0f17135317b19d96a32ff7"},"cell_type":"code","source":"# Add to submission\nsubmission = pd.DataFrame({\n        \"key\": test['key'],\n        \"fare_amount\": prediction.round(2)\n})\n\nsubmission.to_csv('predicted_fare.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"daa9820f147bdf80da0b9785e46adbb42e99ce9e","collapsed":true},"cell_type":"code","source":"submission","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"72d6ffded40409916b0768a3a14ab922de53df3d"},"cell_type":"markdown","source":"**SVR Model**"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c3950fd3cfde68e22fc5eeff43995eaa7acc4efc"},"cell_type":"code","source":"# from sklearn.svm import SVR\n# from sklearn.model_selection import train_test_split\n# from sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e7735c4c1cb8c3ae87a0be649e99ca185f85852d","collapsed":true},"cell_type":"code","source":"# train_model= train_model_[['haversine', 'hour_of_day', 'fare_amount']]\n# test_model = test_model_[['haversine', 'hour_of_day', 'key']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"225ecd3d68a8d204189e0b7d7d988fe5ece64bbf","collapsed":true},"cell_type":"code","source":"# test_model.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"01899b4449c17ec9a76013062092c7729aaa92d4","collapsed":true},"cell_type":"code","source":"# X_pred = test_model.drop('key', axis=1)\n\n# # Let's run XGBoost and predict those fares!\n# X = train_model.drop('fare_amount',axis=1)\n# y = train_model.pop('fare_amount')\n\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n\n# # Scale the data to be between -1 and 1\n# scaler = StandardScaler()\n# scaler.fit(X_train)\n# X_train = scaler.transform(X_train)\n# X_test = scaler.transform(X_test)\n\n# # Establish a model\n# model = SVR(C=1, cache_size=500, epsilon=1, kernel='rbf')\n\n# # Train the model - this will take a minute\n# m_fit = model.fit(X_train, y_train)\n# pre = m_fit.predict(X_pred)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c5b721275a851089e4324fcdab6e5764feb84a32"},"cell_type":"markdown","source":"**TO BE CONTINUED!!...**"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"cb0ce128837c9c4a9209210bbcfad26bab839f42"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}