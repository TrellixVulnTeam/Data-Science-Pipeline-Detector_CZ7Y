{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Taxi Fare Predictions\n\nHere we use the New York Taxi Fare Prediction dataset to evaluate the performance of different ML models for taxi fare predictions.\n\nFirst of all, let us import the libraries we are going to use.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.neural_network import MLPRegressor\n\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error","metadata":{"execution":{"iopub.status.busy":"2021-10-01T18:04:53.113052Z","iopub.execute_input":"2021-10-01T18:04:53.114969Z","iopub.status.idle":"2021-10-01T18:04:54.413722Z","shell.execute_reply.started":"2021-10-01T18:04:53.114646Z","shell.execute_reply":"2021-10-01T18:04:54.412893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Import data\n\nImport the train dataset, with a number of rows equal to 500.000.","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('../input/new-york-city-taxi-fare-prediction/train.csv', nrows=500_000)\n\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-01T18:04:54.415314Z","iopub.execute_input":"2021-10-01T18:04:54.415542Z","iopub.status.idle":"2021-10-01T18:04:56.17064Z","shell.execute_reply.started":"2021-10-01T18:04:54.415517Z","shell.execute_reply":"2021-10-01T18:04:56.169895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This dataset contains information on taxi rides, including the taxi fare, the pickup datetime, the pickup and dropoff locations, and the number of passenger per ride. The taxi fare is the target variable and we have the others to build the predictors.","metadata":{}},{"cell_type":"markdown","source":"### Remove NaN values\n\nFirst, remove nan values\n","metadata":{}},{"cell_type":"code","source":"def drop_nan_values(df): return df.dropna()\n\ntrain_df = drop_nan_values(train_df)\nprint(train_df.isnull().sum())\nprint('shape:', train_df.shape)","metadata":{"execution":{"iopub.status.busy":"2021-10-01T18:04:56.171781Z","iopub.execute_input":"2021-10-01T18:04:56.171969Z","iopub.status.idle":"2021-10-01T18:04:56.470681Z","shell.execute_reply.started":"2021-10-01T18:04:56.171946Z","shell.execute_reply":"2021-10-01T18:04:56.469925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Remove outliers\n\n\n\n\n\nNow, we detect and remove outliers based on computing percentiles.\n\nFirst, review the statistics of data","metadata":{}},{"cell_type":"code","source":"train_df.describe()","metadata":{"execution":{"iopub.status.busy":"2021-10-01T18:04:56.474122Z","iopub.execute_input":"2021-10-01T18:04:56.474319Z","iopub.status.idle":"2021-10-01T18:04:56.626536Z","shell.execute_reply.started":"2021-10-01T18:04:56.474295Z","shell.execute_reply":"2021-10-01T18:04:56.625593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Second, define a functions to produce scatterplots of the target variable against the input variables, so that we can inspect the cuts in the data volume.","metadata":{}},{"cell_type":"code","source":"# Function to produce multiple scatter plots\ndef multi_scatter_plot(df, features, target):\n    \n    def single_plot(df, xlabel, ylabel):\n        x = df[xlabel]\n        fig, axes = plt.subplots(figsize=(6, 4))\n        axes.scatter(x, y, alpha=.3)\n        axes.set(xlabel=xlabel, ylabel=ylabel)\n        axes.legend()\n        plt.tight_layout()\n        plt.show()\n    \n    def double_plot(df, x_labels, ylabel):\n        fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n        for i, xlabel in enumerate(x_labels):\n            x = df[xlabel]\n            axes[i].scatter(x, y, alpha=.3)\n            axes[i].set(xlabel=xlabel, ylabel=ylabel)\n            axes[i].legend()\n        plt.tight_layout()\n        plt.show()\n    \n    def multi_plot(df, x_labels, ylabel):\n        n = len(x_labels)//2\n        fig, axes = plt.subplots(n, 2, figsize=(12, 4*n))\n        count=0\n        for i in range(n):\n            for j in range(2):\n                xlabel = x_labels[count]\n                x = df[xlabel]\n                axes[i][j].scatter(x, y, alpha=.3)\n                axes[i][j].set(xlabel=xlabel, ylabel=ylabel)\n                axes[i][j].legend()\n                count += 1\n        plt.tight_layout()\n        plt.show()\n    \n    y = df[target]\n    ylabel = target\n    x_labels = features\n    \n    if len(x_labels)==1:\n        xlabel = x_labels[0]\n        single_plot(df, xlabel, ylabel)\n        \n    elif len(x_labels)==2:\n        double_plot(df, x_labels, ylabel)\n        \n    elif len(x_labels)==3:\n        double_plot(df, x_labels[:2], ylabel)\n        single_plot(df, x_labels[-1], ylabel)\n        \n    else:\n        multi_plot(df, x_labels, ylabel)\n        if len(x_labels)%2!=0:           \n            single_plot(df, x_labels[-1], ylabel)\n            \n\nfeatures = ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'passenger_count']\ntarget = 'fare_amount'\n#multi_scatter_plot(train_df, features, target)","metadata":{"execution":{"iopub.status.busy":"2021-10-01T18:04:56.627736Z","iopub.execute_input":"2021-10-01T18:04:56.62807Z","iopub.status.idle":"2021-10-01T18:04:56.651063Z","shell.execute_reply.started":"2021-10-01T18:04:56.628042Z","shell.execute_reply":"2021-10-01T18:04:56.649218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By computing percentiles, we drop data in the outermost region of the data volume as well as zero points.","metadata":{}},{"cell_type":"code","source":"percentiles = {'fare_amount': (.001,.9995),\n               'pickup_longitude': (0.0005, 0.98),\n               'pickup_latitude': (0.02, 0.999),\n               'dropoff_longitude': (0.001, 0.98),\n               'dropoff_latitude': (0.02, 0.98),\n               'passenger_count':(0.001, 1.),}","metadata":{"execution":{"iopub.status.busy":"2021-10-01T18:04:56.652572Z","iopub.execute_input":"2021-10-01T18:04:56.652863Z","iopub.status.idle":"2021-10-01T18:04:56.666246Z","shell.execute_reply.started":"2021-10-01T18:04:56.65283Z","shell.execute_reply":"2021-10-01T18:04:56.665323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def drop_percentiles(df, percentiles):\n    for key in percentiles.keys():\n        per1, per2 = percentiles[key]\n        low, high = train_df[key].quantile(per1), train_df[key].quantile(per2)\n        df = df[(df[key] > low) & (df[key] < high)]\n    return df\n\nprint('Old size: %d' % len(train_df))\ntrain_df = drop_percentiles(train_df, percentiles)\nprint('New size: %d' % len(train_df))","metadata":{"execution":{"iopub.status.busy":"2021-10-01T18:04:56.66746Z","iopub.execute_input":"2021-10-01T18:04:56.667886Z","iopub.status.idle":"2021-10-01T18:04:56.989504Z","shell.execute_reply.started":"2021-10-01T18:04:56.667847Z","shell.execute_reply":"2021-10-01T18:04:56.988531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"multi_scatter_plot(train_df, features, target)","metadata":{"execution":{"iopub.status.busy":"2021-10-01T18:04:56.990798Z","iopub.execute_input":"2021-10-01T18:04:56.991106Z","iopub.status.idle":"2021-10-01T18:05:21.69626Z","shell.execute_reply.started":"2021-10-01T18:04:56.991066Z","shell.execute_reply":"2021-10-01T18:05:21.695454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Absolute distance\n\nAs critical information, we use the pickup and dropoff locations to build new columns with the absolute longitudinal distances and the absolute distance traveled.","metadata":{}},{"cell_type":"code","source":"def add_abs_distances(df):\n    df['abs_diff_longitude'] = (df.dropoff_longitude - df.pickup_longitude).abs()\n    df['abs_diff_latitude'] = (df.dropoff_latitude - df.pickup_latitude).abs()\n    df['abs_distance'] = np.sqrt(np.square(df.abs_diff_longitude) + np.square(df.abs_diff_latitude))\n    return df\n\ntrain_df = add_abs_distances(train_df)","metadata":{"execution":{"iopub.status.busy":"2021-10-01T18:05:21.697398Z","iopub.execute_input":"2021-10-01T18:05:21.697637Z","iopub.status.idle":"2021-10-01T18:05:21.714353Z","shell.execute_reply.started":"2021-10-01T18:05:21.697608Z","shell.execute_reply":"2021-10-01T18:05:21.713363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Pickup date and time\n\nLet us extract from the \"pickup_datetime\" the year, month, day and hour and place them in new columns.","metadata":{}},{"cell_type":"code","source":"def add_date_time(df):\n    # extract information from 'pickup_datetime'\n    df[['year', 'month', 'day']] = df.pickup_datetime.str.split(' ', expand=True).iloc[:,0].str.split('-', expand=True).astype('int64')\n    df['hour'] = df.pickup_datetime.str.split(' ', expand=True).iloc[:,1].str.split(':', expand=True).iloc[:,0].astype('int64')\n    # removing 'key' and 'pickup_datetime' columns\n    df = df.drop(columns = ['key', 'pickup_datetime'])#, inplace=True)\n    return df\n    \ntrain_df = add_date_time(train_df)","metadata":{"execution":{"iopub.status.busy":"2021-10-01T18:05:21.716438Z","iopub.execute_input":"2021-10-01T18:05:21.716677Z","iopub.status.idle":"2021-10-01T18:05:27.722792Z","shell.execute_reply.started":"2021-10-01T18:05:21.71665Z","shell.execute_reply":"2021-10-01T18:05:27.722107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, plot these new columns.","metadata":{}},{"cell_type":"code","source":"features = ['abs_diff_longitude', 'abs_diff_latitude', 'abs_distance', 'passenger_count']\ntarget = 'fare_amount'\nmulti_scatter_plot(train_df, features, target)","metadata":{"execution":{"iopub.status.busy":"2021-10-01T18:05:27.723726Z","iopub.execute_input":"2021-10-01T18:05:27.724381Z","iopub.status.idle":"2021-10-01T18:05:46.809337Z","shell.execute_reply.started":"2021-10-01T18:05:27.724348Z","shell.execute_reply":"2021-10-01T18:05:46.808455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature selection\n\nFirst, split the train dataset into train and test datasets","metadata":{}},{"cell_type":"code","source":"train_df, test_df = train_test_split(train_df, test_size=.2, random_state=1)\n\nprint(train_df.shape)\nprint(test_df.shape)","metadata":{"execution":{"iopub.status.busy":"2021-10-01T18:05:46.810526Z","iopub.execute_input":"2021-10-01T18:05:46.810757Z","iopub.status.idle":"2021-10-01T18:05:46.884978Z","shell.execute_reply.started":"2021-10-01T18:05:46.810731Z","shell.execute_reply":"2021-10-01T18:05:46.884168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Linear correlation\n\nNext, let's compute the correlation matrix to identify those features that are most correlated with the target and those that are most correlated to each other. The former type corresponds to the best predictors and the latter type those that introduce multicollinearity. For two variables, $x$ and $y$, the formula is as follows\n\n$\\sum_{i,j}(x_i-\\bar{x}))(y_j-\\bar{y})/\\sigma_i \\sigma_j$\n\nWe do this just after separating a test data set, and use the train data set only, so as not to involve the test data set in any feature selection procedures.","metadata":{}},{"cell_type":"code","source":"def corr_heatmap(df):\n    corr_data = df.corr()\n    fig, ax = plt.subplots(figsize=(10,8))\n    # Add title\n    #plt.title(title, fontsize=12)\n    # Heatmap showing the amount of genomes with the same MIC for each MIC, by antibiotic\n    sns.heatmap(corr_data, annot=corr_data, cmap='Blues', cbar=True, fmt='.2f')\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=45, ha='right', rotation_mode='anchor')\n    plt.show()\n    \ncorr_heatmap(train_df)","metadata":{"execution":{"iopub.status.busy":"2021-10-01T18:05:46.886091Z","iopub.execute_input":"2021-10-01T18:05:46.886322Z","iopub.status.idle":"2021-10-01T18:05:48.117205Z","shell.execute_reply.started":"2021-10-01T18:05:46.886295Z","shell.execute_reply":"2021-10-01T18:05:48.116397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The correlation heatmap shows that there are sizeable correlation between the target and some of the features. High correlation is reached between the target and the absolute distances traveled, median correlation between the target and the pickup and dropoff longitudes, and low correlation with the pickup and dropoff latitudes, passenger counts and time variables. \n\nHowever, high correlation is also found between feature variables. \"The performance of some algorithms can deteriorate if two or more variables are tightly related, called multicollinearity\".\n\nThus, we can safely reduce the number of columns, using only the features with high correlation with the target but uncorrelated with each other.","metadata":{}},{"cell_type":"markdown","source":"## Modeling and Validation\n\nWe will do the model evaluation in two different ways:\n- i) by using the train and test datasets, \n- ii) by cross-validation, using the whole train dataset reloaded again\n\nFirst of all, define the models","metadata":{}},{"cell_type":"code","source":"# specify models custom models\ndef NNRegressor():\n    return MLPRegressor(hidden_layer_sizes=(4,), activation='tanh', solver='sgd', tol=0.01, n_iter_no_change=50, verbose=False)\n\n\nmodels = {'DTR': DecisionTreeRegressor(),\n          'RFR': RandomForestRegressor(),\n          'LR': LinearRegression(),\n          'SGDR': SGDRegressor(),\n          'NNR': NNRegressor()}","metadata":{"execution":{"iopub.status.busy":"2021-10-01T18:05:48.118327Z","iopub.execute_input":"2021-10-01T18:05:48.11857Z","iopub.status.idle":"2021-10-01T18:05:48.124877Z","shell.execute_reply.started":"2021-10-01T18:05:48.118542Z","shell.execute_reply":"2021-10-01T18:05:48.124026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then, define the target and the feature variables.","metadata":{}},{"cell_type":"code","source":"# Define the target and the feature variables in decreasing level of importance\nsel_features = ['abs_distance', 'abs_diff_latitude', 'pickup_longitude', 'dropoff_longitude']\nX_train, y_train = train_df[sel_features], train_df.fare_amount\nX_test, y_test = test_df[sel_features], test_df.fare_amount","metadata":{"execution":{"iopub.status.busy":"2021-10-01T18:05:48.125997Z","iopub.execute_input":"2021-10-01T18:05:48.126298Z","iopub.status.idle":"2021-10-01T18:05:48.14085Z","shell.execute_reply.started":"2021-10-01T18:05:48.126269Z","shell.execute_reply":"2021-10-01T18:05:48.139951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Generalization or test error\n\n\"Test error, also referred to as generalization error, is the prediction error\nover an independent test sample\". Let us compute here the root mean squared error.","metadata":{}},{"cell_type":"code","source":"starttime = time.time()\n\n# Define an empty list to save the root mean squared error as the test error\nrmse_errt = []\nfor model in models.values():\n    # Always scale the input. The most convenient way is to use a pipeline.\n    model = make_pipeline(StandardScaler(), model)\n    # fit the model\n    model.fit(X_train, y_train)\n    # make predictions and compute the root mean squared error\n    predictions = model.predict(X_test)\n    rmse_errt.append(mean_squared_error(y_test, predictions, squared=False))\n\nfor i, model in enumerate(models):\n    print('%s RMSE = %.2f' % (model, rmse_errt[i]))\n\nprint('Time: {:0.2f} seconds'.format(time.time() - starttime))","metadata":{"execution":{"iopub.status.busy":"2021-10-01T18:05:48.141848Z","iopub.execute_input":"2021-10-01T18:05:48.142654Z","iopub.status.idle":"2021-10-01T18:11:52.930182Z","shell.execute_reply.started":"2021-10-01T18:05:48.142594Z","shell.execute_reply":"2021-10-01T18:11:52.929036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Cross-validation\n\nNow we validate the models by means of cross-validation. Here, we use the entire training dataset","metadata":{}},{"cell_type":"code","source":"# define a function that includes all previous preprocessing steps\ndef preprocessing(df):\n    df = drop_nan_values(df)\n    df = drop_percentiles(df, percentiles)\n    df = add_abs_distances(df)\n    df = add_date_time(df)\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-10-01T18:11:52.932083Z","iopub.execute_input":"2021-10-01T18:11:52.932381Z","iopub.status.idle":"2021-10-01T18:11:52.943904Z","shell.execute_reply.started":"2021-10-01T18:11:52.932342Z","shell.execute_reply":"2021-10-01T18:11:52.942676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reload the training data, preprocess it, and define the input and output variables\ntrain_df = pd.read_csv('../input/new-york-city-taxi-fare-prediction/train.csv', nrows=500_000)\ntrain_df = preprocessing(train_df)\nX_train, y_train = train_df[sel_features], train_df.fare_amount\n\n# define model evaluation method (n_splits = 1/test_size)\ncv = RepeatedKFold(n_splits=5, n_repeats=1, random_state=0)\n\nstarttime = time.time()\n\n# evaluate the models\nrmse_per_model = []\nfor model in models.values():\n    model = make_pipeline(StandardScaler(), model)\n    rmse = cross_val_score(model, X_train, y_train, scoring='neg_root_mean_squared_error', cv=cv, n_jobs=-1)\n    rmse = np.abs(rmse)\n    rmse_per_model.append(rmse)\n    \nprint('Time: {:0.2f} seconds'.format(time.time() - starttime))","metadata":{"execution":{"iopub.status.busy":"2021-10-01T18:11:52.945847Z","iopub.execute_input":"2021-10-01T18:11:52.946548Z","iopub.status.idle":"2021-10-01T18:28:01.485864Z","shell.execute_reply.started":"2021-10-01T18:11:52.946482Z","shell.execute_reply":"2021-10-01T18:28:01.485103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Results\n\nLet us show the results of error measurements in a box whisker plot","metadata":{}},{"cell_type":"code","source":"# define a custom box whisker plot\ndef box_whisker_plot(data, labels, user_errt):\n    mean = np.mean(data, axis=1)\n    y = np.array(range(1,len(mean)+1))\n    fig, ax = plt.subplots(figsize=(6,4))\n    ax.set_title('RMSE per model')\n    ax.set_xlabel('RMSE')\n    ax.set_ylabel('Model')\n    ax.boxplot(data, labels=labels, vert=False, whis=(0,100))\n    ax.scatter(user_errt, y,  marker='^', label='Test error')\n    ax.scatter(mean, y,  marker='^', label='C-V error')\n    plt.legend()\n    plt.show()\n\nbox_whisker_plot(rmse_per_model, models.keys(), rmse_errt)","metadata":{"execution":{"iopub.status.busy":"2021-10-01T18:28:01.487636Z","iopub.execute_input":"2021-10-01T18:28:01.48787Z","iopub.status.idle":"2021-10-01T18:28:02.056138Z","shell.execute_reply.started":"2021-10-01T18:28:01.487842Z","shell.execute_reply":"2021-10-01T18:28:02.055211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As final remarks:\n\n- Cross-validation estimates are in agreement with the generalization error.\n- The Random Forest Regressor performs better in both validation schemes.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}