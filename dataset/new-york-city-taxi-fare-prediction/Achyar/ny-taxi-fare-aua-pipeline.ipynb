{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# Pandas and numpy for data manipulation\nimport pandas as pd\nimport numpy as np\n# Pandas display options\npd.set_option('display.float_format', lambda x: '%.3f' % x)\n# Set random seed \nRSEED = 2020\n# Visualizations\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use('fivethirtyeight')\nplt.rcParams['font.size'] = 12\nimport seaborn as sns\npalette = sns.color_palette('Paired', 10)\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import accuracy_score, f1_score, make_scorer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.compose import ColumnTransformer, TransformedTargetRegressor\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures, PowerTransformer, PowerTransformer\nfrom sklearn.linear_model import LinearRegression, ElasticNet, Ridge, Lasso\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVR\n# from sklearn.ensemble import StackingRegressor\nfrom mlxtend.regressor import StackingCVRegressor\nfrom pandas.tseries.holiday import USFederalHolidayCalendar as calendar","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/train.csv', nrows = 5_000_00, \n                   parse_dates = ['pickup_datetime']).drop(columns = 'key')\n\n# Remove na\ndata = data.dropna()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Findings :\n\n1. Fare amount ada yang minus dan max harganya 500\n2. nilai longitute dan latitude aneh karena nilainya ribuan padahal harusnya:\n   Latitudes range from -90 ke 90, and longitudes range from -180 ke 180, bahkan NewYork Latitude 40 ke 42 dan Longitudes -75 ke -72\n3. passanger count ada yg 0 dan max 6 padahal kapasitas taxi max <6"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isna().sum()/data.shape[0]*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#distribution of target (fare amount)\nsns.distplot(data['fare_amount'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ada harga 0 ke bawah"},{"metadata":{"trusted":true},"cell_type":"code","source":"def ecdf(x):\n    \"\"\"Empirical cumulative distribution function of a variable\"\"\"\n    # Sort in ascending order\n    x = np.sort(x)\n    n = len(x)\n    # Go from 1/n to 1\n    y = np.arange(1, n + 1, 1) / n\n    return x, y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xs, ys = ecdf(data['fare_amount'])\nplt.figure(figsize = (8, 6))\nplt.plot(xs, ys, '.')\nplt.ylabel('Percentile'); plt.title('ECDF of Fare Amount'); plt.xlabel('Fare Amount ($)');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fare Amount diata 100 itu outlier"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['passenger_count'].value_counts().plot.bar(color = 'b', edgecolor = 'k');\nplt.title('Passenger Counts'); plt.xlabel('Number of Passengers'); plt.ylabel('Count');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('ada '+str(data[data['passenger_count']==0].shape[0])+'transaksi dengan 0 passangger')\nprint('ada '+str(data[data['passenger_count']==6].shape[0])+'transaksi dengan 6 passangger')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Asumsi :\n\n1. 0 passanger terjadi karena cancel booking\n2. 6 passanger terjadi karena melebihi kapasita, kalo taxi harusnya 4 max, taxicab 5\n\n   https://www1.nyc.gov/site/tlc/passengers/passenger-frequently-asked-questions.page#:~:text=The%20maximum%20amount%20of%20passengers,of%20an%20adult%20passenger%20seated"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.loc[data['pickup_latitude'].between(40, 42)]\ndata = data.loc[data['pickup_longitude'].between(-75, -72)]\ndata = data.loc[data['dropoff_latitude'].between(40, 42)]\ndata = data.loc[data['dropoff_longitude'].between(-75, -72)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\n\nkmeans = KMeans(n_clusters=5)\ncluster_pickup = kmeans.fit_predict(data[['pickup_longitude','pickup_latitude']])\ncluster_dropoff = kmeans.fit_predict(data[['dropoff_longitude','dropoff_latitude']])\ndata['cluster_pickup']=cluster_pickup\ndata['cluster_dropoff']=cluster_dropoff","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# this function will also be used with the test set below\ndef select_within_boundingbox(df, BB):\n    return (df.pickup_longitude >= BB[0]) & (df.pickup_longitude <= BB[1]) & \\\n           (df.pickup_latitude >= BB[2]) & (df.pickup_latitude <= BB[3]) & \\\n           (df.dropoff_longitude >= BB[0]) & (df.dropoff_longitude <= BB[1]) & \\\n           (df.dropoff_latitude >= BB[2]) & (df.dropoff_latitude <= BB[3])\n\n# load extra image to zoom in on NYC\nBB_zoom = (-74.1, -73.7, 40.6, 40.85)\nnyc_map_zoom = plt.imread('https://github.com/WillKoehrsen/Machine-Learning-Projects/blob/master/images/nyc_-74.1_-73.7_40.6_40.85.PNG?raw=true')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# this function will be used more often to plot data on the NYC map\ndef plot_on_map(df, BB, nyc_map, s=10, alpha=0.2, color = False):\n    fig, axs = plt.subplots(1, 2, figsize=(18, 22))\n    axs[0].scatter(df.pickup_longitude, df.pickup_latitude, zorder=1, alpha=alpha, c='r', s=s)\n    axs[0].set_xlim((BB[0], BB[1]))\n    axs[0].set_ylim((BB[2], BB[3]))\n    axs[0].set_title('Pickup locations')\n    axs[0].axis('off')\n    axs[0].imshow(nyc_map, zorder=0, extent=BB)\n\n    axs[1].scatter(df.dropoff_longitude, df.dropoff_latitude, zorder=1, alpha=alpha, c='b', s=s)\n    axs[1].set_xlim((BB[0], BB[1]))\n    axs[1].set_ylim((BB[2], BB[3]))\n    axs[1].set_title('Dropoff locations')\n    axs[1].axis('off')\n    axs[1].imshow(nyc_map, zorder=0, extent=BB)\n    \n# plot training data on map zoomed in\nplot_on_map(data.sample(4_000_00, random_state = RSEED), \n            BB_zoom, nyc_map_zoom, s=0.05, alpha=0.05)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a color mapping based on fare bins\ncolor_mapping = {cluster_pickup: palette[i] for i, cluster_pickup in enumerate(data['cluster_pickup'].unique())}\ndata['color'] = data['cluster_pickup'].map(color_mapping)\nplot_data = data.sample(4_000_00, random_state = RSEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BB = BB_zoom\n\nfig, axs = plt.subplots(1, 1, figsize=(20, 18))\n\n# Plot the pickups\nfor b, df in plot_data.groupby('cluster_pickup'):\n    # Set the zorder to 1 to plot on top of map\n    axs.scatter(df.pickup_longitude, df.pickup_latitude, zorder=1, alpha=0.2, c=df.color, s=30, label = f'{b}')\n    axs.set_xlim((BB[0], BB[1]))\n    axs.set_ylim((BB[2], BB[3]))\n    axs.set_title('Pickup locations', size = 32)\n    axs.axis('off')\n    \n# Legend\nleg = axs.legend(fontsize = 28, markerscale = 3)\n\n# Adjust alpha of legend markers\nfor lh in leg.legendHandles: \n    lh.set_alpha(1)\n\nleg.set_title('Cluster Pickup', prop = {'size': 28})\n\n# Show map in background (zorder = 0)\naxs.imshow(nyc_map_zoom, zorder=0, extent=BB_zoom);\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a color mapping based on fare bins\ncolor_mapping = {cluster_pickup: palette[i] for i, cluster_pickup in enumerate(data['cluster_dropoff'].unique())}\ndata['color'] = data['cluster_dropoff'].map(color_mapping)\nplot_data = data.sample(4_000_00, random_state = RSEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(1, 1, figsize=(20, 18))\n\n# Plot the pickups\nfor b, df in plot_data.groupby('cluster_dropoff'):\n    axs.scatter(df.dropoff_longitude, df.dropoff_latitude, zorder=1, \n                alpha=0.2, c=df.color, s=30, label = f'{b}')\n    axs.set_xlim((BB[0], BB[1]))\n    axs.set_ylim((BB[2], BB[3]))\n    axs.set_title('cluster_dropoff', size = 32)\n    axs.axis('off')\n    \n# Legend\nleg = axs.legend(fontsize = 28, markerscale = 3)\n\n# Adjust alpha of legend markers\nfor lh in leg.legendHandles: \n    lh.set_alpha(1)\n\nleg.set_title('Cluster Dropoff', prop = {'size': 28})\n\n# Show map in background (zorder = 0)\naxs.imshow(nyc_map_zoom, zorder=0, extent=BB_zoom);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def minkowski_distance(x1, x2, y1, y2, p):\n    return ((abs(x2 - x1) ** p) + (abs(y2 - y1)) ** p) ** (1 / p)\n                                                           \nR = 6378\n\ndef haversine_np(lon1, lat1, lon2, lat2):\n    \"\"\"\n    Calculate the great circle distance between two points\n    on the earth (specified in decimal degrees)\n\n    All args must be of equal length.    \n    \n    source: https://stackoverflow.com/a/29546836\n\n    \"\"\"\n    # Convert latitude and longitude to radians\n    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n\n    # Find the differences\n    dlon = lon2 - lon1\n    dlat = lat2 - lat1\n\n    # Apply the formula \n    a = np.sin(dlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2.0)**2\n    # Calculate the angle (in radians)\n    c = 2 * np.arcsin(np.sqrt(a))\n    # Convert to kilometers\n    km = R * c\n    \n    return km\n                                                           \n                                                           \nplace = pd.DataFrame({'loc' : ['jfk','nyc','ewr','lgr'], 'long' : [-73.7822222222,-74.0063889,-74.175,-73.87], 'lat' : [40.6441666667,40.7141667,40.69,40.77]})\n\ndef distance_to_place(df,location,source_long,source_lat):\n    selected_place = place[place['loc']==location]\n    selected_place = selected_place.reset_index()\n    xx = haversine_np(df[source_long], df[source_lat], selected_place['long'][0], selected_place['lat'][0])\n    \n    return xx\n                                                           \n\ndef calculate_direction(df):\n    d_lon = df['pickup_longitude'] - df['dropoff_longitude']\n    d_lat = df['pickup_latitude'] - df['dropoff_latitude']\n    result = np.zeros(len(d_lon))\n    l = np.sqrt(d_lon**2 + d_lat**2)\n    result[d_lon>0] = (180/np.pi)*np.arcsin(d_lat[d_lon>0]/l[d_lon>0])\n    idx = (d_lon<0) & (d_lat>0)\n    result[idx] = 180 - (180/np.pi)*np.arcsin(d_lat[idx]/l[idx])\n    idx = (d_lon<0) & (d_lat<0)\n    result[idx] = -180 - (180/np.pi)*np.arcsin(d_lat[idx]/l[idx])\n    return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['abs_lat_diff'] = (data['dropoff_latitude'] - data['pickup_latitude']).abs()\ndata['abs_lon_diff'] = (data['dropoff_longitude'] - data['pickup_longitude']).abs()\n\ndata['manhattan'] = minkowski_distance(data['pickup_longitude'], data['dropoff_longitude'],\n                                       data['pickup_latitude'], data['dropoff_latitude'], 1)\n\ndata['euclidean'] = minkowski_distance(data['pickup_longitude'], data['dropoff_longitude'],\n                                       data['pickup_latitude'], data['dropoff_latitude'], 2)\n\n\ndata['haversine'] =  haversine_np(data['pickup_longitude'], data['pickup_latitude'],\n                         data['dropoff_longitude'], data['dropoff_latitude']) \n\nfor i in place['loc'].tolist():\n    for j in ['pickup','dropoff']:\n        data[str(j)+'_distance_to'+str(i)] = distance_to_place(data,i,str(j)+'_longitude',str(j)+'_latitude')\n\ndata['direction'] = calculate_direction(data)\n\ndata = extract_dateinfo(data, 'pickup_datetime', drop = False, \n                         time = True, start_ref = data['pickup_datetime'].min())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['haversine'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cek = data[data['haversine']<200]\nsns.distplot(cek['haversine'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(data['haversine']>25).sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# scatter plot distance - fare\nfig, axs = plt.subplots(1, 2, figsize=(16,6))\naxs[0].scatter(data.haversine, data.fare_amount, alpha=0.2)\naxs[0].set_xlabel('distance km')\naxs[0].set_ylabel('fare $USD')\naxs[0].set_title('All data')\n\n# zoom in on part of data\nidx = (data.haversine <= 25) & (data.fare_amount < 100)\naxs[1].scatter(data[idx].haversine, data[idx].fare_amount, alpha=0.2)\naxs[1].set_xlabel('distance km')\naxs[1].set_ylabel('fare $USD')\naxs[1].set_title('Zoom in on distance < 15 km, fare < $100');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"kalau outlier haversine di exclude kita dapat pola linear antara pertambahan jarak dan harga"},{"metadata":{"trusted":true},"cell_type":"code","source":"data[(data['abs_lat_diff']==0)&(data['abs_lon_diff']==0)].shape[0], data.shape[0], data[(data['abs_lat_diff']==0)&(data['abs_lon_diff']==0)].shape[0]/data.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(data[(data['abs_lat_diff']==0)&(data['abs_lon_diff']==0)]['passenger_count'])\nplt.show()\nsns.distplot(data[(data['abs_lat_diff']==0)&(data['abs_lon_diff']==0)]['fare_amount'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ada 14250 transaksi yang tidak ada perubahan longituted dan latitude tapi kalau dilihat lagi ini tidak ada hubungan ngsung dengan fare amount <=0 (assumsi cancle booking) ataupun jumlah passanger. jadi kemungkinan gps nya mati selama perjalanan"},{"metadata":{"trusted":true},"cell_type":"code","source":"corrs = data.corr()\ncorrs = corrs.drop('fare_amount',axis=0)\ncorrs['fare_amount'].plot.bar(color = 'b');\nplt.title('Correlation with Fare Amount');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cek_cols = corrs.columns.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cek = data.copy()\nfor i in cek_cols:\n    cek[i] = np.log(cek[i])\ncorrs = cek.corr()\ncorrs = corrs.drop('fare_amount',axis=0)\ncorrs['fare_amount'].plot.bar(color = 'b');\nplt.title('Correlation with Fare Amount');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#time\nimport re\ndef extract_dateinfo(df, date_col, drop=True, time=False, \n                     start_ref = pd.datetime(1900, 1, 1),\n                     extra_attr = False):\n    \"\"\"\n    Extract Date (and time) Information from a DataFrame\n    Adapted from: https://github.com/fastai/fastai/blob/master/fastai/structured.py\n    \"\"\"\n    df = df.copy()\n    \n    # Extract the field\n    fld = df[date_col]\n    \n    # Check the time\n    fld_dtype = fld.dtype\n    if isinstance(fld_dtype, pd.core.dtypes.dtypes.DatetimeTZDtype):\n        fld_dtype = np.datetime64\n\n    # Convert to datetime if not already\n    if not np.issubdtype(fld_dtype, np.datetime64):\n        df[date_col] = fld = pd.to_datetime(fld, infer_datetime_format=True)\n    \n\n    # Prefix for new columns\n    pre = re.sub('[Dd]ate', '', date_col)\n    pre = re.sub('[Tt]ime', '', pre)\n    \n    # Basic attributes\n    attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear', 'Days_in_month', 'is_leap_year']\n    \n    # Additional attributes\n    if extra_attr:\n        attr = attr + ['Is_month_end', 'Is_month_start', 'Is_quarter_end', \n                       'Is_quarter_start', 'Is_year_end', 'Is_year_start']\n    \n    # If time is specified, extract time information\n    if time: \n        attr = attr + ['Hour', 'Minute', 'Second']\n        \n    # Iterate through each attribute\n    for n in attr: \n        df[pre + n] = getattr(fld.dt, n.lower())\n        \n    # Calculate days in year\n    df[pre + 'Days_in_year'] = df[pre + 'is_leap_year'] + 365\n        \n    if time:\n        # Add fractional time of day (0 - 1) units of day\n        df[pre + 'frac_day'] = ((df[pre + 'Hour']) + (df[pre + 'Minute'] / 60) + (df[pre + 'Second'] / 60 / 60)) / 24\n        \n        # Add fractional time of week (0 - 1) units of week\n        df[pre + 'frac_week'] = (df[pre + 'Dayofweek'] + df[pre + 'frac_day']) / 7\n    \n        # Add fractional time of month (0 - 1) units of month\n        df[pre + 'frac_month'] = (df[pre + 'Day'] + (df[pre + 'frac_day'])) / (df[pre + 'Days_in_month'] +  1)\n        \n        # Add fractional time of year (0 - 1) units of year\n        df[pre + 'frac_year'] = (df[pre + 'Dayofyear'] + df[pre + 'frac_day']) / (df[pre + 'Days_in_year'] + 1)\n        \n    # Add seconds since start of reference\n    df[pre + 'Elapsed'] = (fld - start_ref).dt.total_seconds()\n    \n    if drop: \n        df = df.drop(date_col, axis=1)\n        \n    return df\n\ndata = extract_dateinfo(data, 'pickup_datetime', drop = False,time = True, start_ref = df['pickup_datetime'].min())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def time_slicer(df, timeframes, value, color=\"purple\"):\n    \"\"\"\n    Function to count observation occurrence through different lenses of time.\n    \"\"\"\n    f, ax = plt.subplots(len(timeframes), figsize = [12,12])\n    for i,x in enumerate(timeframes):\n        df.loc[:,[x,value]].groupby([x]).mean().plot(ax=ax[i],color=color)\n        ax[i].set_ylabel(value.replace(\"_\", \" \").title())\n        ax[i].set_title(\"{} by {}\".format(value.replace(\"_\", \" \").title(), x.replace(\"_\", \" \").title()))\n        ax[i].set_xlabel(\"\")\n    ax[len(timeframes)-1].set_xlabel(\"Time Frame\")\n    plt.tight_layout(pad=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"time_slicer(df=data, timeframes=['pickup_Year', 'pickup_Month', 'pickup_Day','pickup_Hour'], value = \"fare_amount\", color=\"blue\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"findings :\n1. Secara tahun semakin tinggi tahun ongkos semakin naik\n2. Secara bulan naik dengan puncak di bulan 5 kemduian  turun sampe paling parah di bulan 7 & 8 terus naik lagi peak dibulan 9 ters turun sampe akhir tahun\n3. secara tangal pola fluktuatif\n4. paling peak jam 5 pagi dan jam 3 sore agak meningkat"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\nholidays = calendar().holidays()\ndata[\"usFedHoliday\"] =  data.pickup_datetime.dt.date.astype('datetime64').isin(holidays)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cek =  data[(data.haversine <= 25) & (data.fare_amount <= 50) & (data.usFedHoliday == True)]\nsns.distplot(cek['fare_amount'])\ncek1 =  data[(data.haversine <= 25) & (data.fare_amount <= 50) & (data.usFedHoliday == False)]\nsns.distplot(cek['fare_amount'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modelling"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/train.csv', nrows = 5_000_0, \n                   parse_dates = ['pickup_datetime']).drop(columns = 'key')\n\n# Remove na\ndata = data.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def minkowski_distance(x1, x2, y1, y2, p):\n    return ((abs(x2 - x1) ** p) + (abs(y2 - y1)) ** p) ** (1 / p)\n                                                           \nR = 6378\n\ndef haversine_np(lon1, lat1, lon2, lat2):\n    \"\"\"\n    Calculate the great circle distance between two points\n    on the earth (specified in decimal degrees)\n\n    All args must be of equal length.    \n    \n    source: https://stackoverflow.com/a/29546836\n\n    \"\"\"\n    # Convert latitude and longitude to radians\n    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n\n    # Find the differences\n    dlon = lon2 - lon1\n    dlat = lat2 - lat1\n\n    # Apply the formula \n    a = np.sin(dlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2.0)**2\n    # Calculate the angle (in radians)\n    c = 2 * np.arcsin(np.sqrt(a))\n    # Convert to kilometers\n    km = R * c\n    \n    return km\n                                                           \n                                                           \nplace = pd.DataFrame({'loc' : ['jfk','nyc','ewr','lgr'], 'long' : [-73.7822222222,-74.0063889,-74.175,-73.87], 'lat' : [40.6441666667,40.7141667,40.69,40.77]})\n\ndef distance_to_place(df,location,source_long,source_lat):\n    selected_place = place[place['loc']==location]\n    selected_place = selected_place.reset_index()\n    xx = haversine_np(df[source_long], df[source_lat], selected_place['long'][0], selected_place['lat'][0])\n    \n    return xx\n                                                           \n\ndef calculate_direction(df):\n    d_lon = df['pickup_longitude'] - df['dropoff_longitude']\n    d_lat = df['pickup_latitude'] - df['dropoff_latitude']\n    result = np.zeros(len(d_lon))\n    l = np.sqrt(d_lon**2 + d_lat**2)\n    result[d_lon>0] = (180/np.pi)*np.arcsin(d_lat[d_lon>0]/l[d_lon>0])\n    idx = (d_lon<0) & (d_lat>0)\n    result[idx] = 180 - (180/np.pi)*np.arcsin(d_lat[idx]/l[idx])\n    idx = (d_lon<0) & (d_lat<0)\n    result[idx] = -180 - (180/np.pi)*np.arcsin(d_lat[idx]/l[idx])\n    return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#time\nimport re\ndef extract_dateinfo(df, date_col, drop=True, time=False, \n                     start_ref = pd.datetime(1900, 1, 1),\n                     extra_attr = False):\n    \"\"\"\n    Extract Date (and time) Information from a DataFrame\n    Adapted from: https://github.com/fastai/fastai/blob/master/fastai/structured.py\n    \"\"\"\n    df = df.copy()\n    \n    # Extract the field\n    fld = df[date_col]\n    \n    # Check the time\n    fld_dtype = fld.dtype\n    if isinstance(fld_dtype, pd.core.dtypes.dtypes.DatetimeTZDtype):\n        fld_dtype = np.datetime64\n\n    # Convert to datetime if not already\n    if not np.issubdtype(fld_dtype, np.datetime64):\n        df[date_col] = fld = pd.to_datetime(fld, infer_datetime_format=True)\n    \n\n    # Prefix for new columns\n    pre = re.sub('[Dd]ate', '', date_col)\n    pre = re.sub('[Tt]ime', '', pre)\n    \n    # Basic attributes\n    attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear', 'Days_in_month', 'is_leap_year']\n    \n    # Additional attributes\n    if extra_attr:\n        attr = attr + ['Is_month_end', 'Is_month_start', 'Is_quarter_end', \n                       'Is_quarter_start', 'Is_year_end', 'Is_year_start']\n    \n    # If time is specified, extract time information\n    if time: \n        attr = attr + ['Hour', 'Minute', 'Second']\n        \n    # Iterate through each attribute\n    for n in attr: \n        df[pre + n] = getattr(fld.dt, n.lower())\n        \n    # Calculate days in year\n    df[pre + 'Days_in_year'] = df[pre + 'is_leap_year'] + 365\n        \n    if time:\n        # Add fractional time of day (0 - 1) units of day\n        df[pre + 'frac_day'] = ((df[pre + 'Hour']) + (df[pre + 'Minute'] / 60) + (df[pre + 'Second'] / 60 / 60)) / 24\n        \n        # Add fractional time of week (0 - 1) units of week\n        df[pre + 'frac_week'] = (df[pre + 'Dayofweek'] + df[pre + 'frac_day']) / 7\n    \n        # Add fractional time of month (0 - 1) units of month\n        df[pre + 'frac_month'] = (df[pre + 'Day'] + (df[pre + 'frac_day'])) / (df[pre + 'Days_in_month'] +  1)\n        \n        # Add fractional time of year (0 - 1) units of year\n        df[pre + 'frac_year'] = (df[pre + 'Dayofyear'] + df[pre + 'frac_day']) / (df[pre + 'Days_in_year'] + 1)\n        \n    # Add seconds since start of reference\n    df[pre + 'Elapsed'] = (fld - start_ref).dt.total_seconds()\n    \n    if drop: \n        df = df.drop(date_col, axis=1)\n        \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class data_transform():\n    \n    \n    def __init__(self, num, cat, is_cat):\n        self.num = num\n        self.cat = cat\n        self.is_cat = is_cat\n        \n    def fit(self, X):\n        # do not do anything\n        return self\n    \n    def transform(self, X, y = None):\n        num_cols = self.num\n        cat_cols = self.cat\n        df = X.copy()\n        df['passenger_count'] = np.where(df['passenger_count']<1,1,df['passenger_count'])\n        df['passenger_count'] = np.where(df['passenger_count']>6,5,df['passenger_count'])\n        df['pickup_latitude'] = np.where(df['pickup_latitude']<40, 40,df['pickup_latitude'])\n        df['pickup_latitude'] = np.where(df['pickup_latitude']>42,42,df['pickup_latitude'])\n        df['dropoff_latitude'] = np.where(df['dropoff_latitude']<40, 40,df['dropoff_latitude'])\n        df['dropoff_latitude'] = np.where(df['dropoff_latitude']>42, 42,df['dropoff_latitude'])\n        df['pickup_longitude'] = np.where(df['pickup_longitude']<-75, -75,df['pickup_longitude'])\n        df['pickup_longitude'] = np.where(df['pickup_longitude']>-72, -72,df['pickup_longitude'])\n        df['dropoff_longitude'] = np.where(df['dropoff_longitude']<-75, -75,df['dropoff_longitude'])\n        df['dropoff_longitude'] = np.where(df['dropoff_longitude']>-72, -72,df['dropoff_longitude'])\n\n        kmeans = KMeans(n_clusters=5)\n        cluster_pickup = kmeans.fit_predict(data[['pickup_longitude','pickup_latitude']])\n        cluster_dropoff = kmeans.fit_predict(data[['dropoff_longitude','dropoff_latitude']])\n        data['cluster_pickup']=cluster_pickup\n        data['cluster_dropoff']=cluster_dropoff\n        \n        df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n        # Absolute difference in latitude and longitude\n        df['abs_lat_diff'] = (df['dropoff_latitude'] - df['pickup_latitude']).abs()\n        df['abs_lon_diff'] = (df['dropoff_longitude'] - df['pickup_longitude']).abs()\n\n        df['manhattan'] = minkowski_distance(df['pickup_longitude'], df['dropoff_longitude'],\n                                               df['pickup_latitude'], df['dropoff_latitude'], 1)\n\n        df['euclidean'] = minkowski_distance(df['pickup_longitude'], df['dropoff_longitude'],\n                                               df['pickup_latitude'], df['dropoff_latitude'], 2)\n\n        df['haversine'] =  haversine_np(df['pickup_longitude'], df['pickup_latitude'],\n                                 df['dropoff_longitude'], df['dropoff_latitude']) \n\n        for i in place['loc'].tolist():\n            for j in ['pickup','dropoff']:\n                df[str(j)+'_distance_to'+str(i)] = distance_to_place(df,i,str(j)+'_longitude',str(j)+'_latitude')\n\n        df['direction'] = calculate_direction(df)\n\n        df = extract_dateinfo(df, 'pickup_datetime', drop = False, \n                                 time = True, start_ref = df['pickup_datetime'].min())\n        \n        holidays = calendar().holidays()\n        data[\"usFedHoliday\"] =  data.pickup_datetime.dt.date.astype('datetime64').isin(holidays)\n\n        df[cat_cols] = df[cat_cols].astype(str)\n        \n        \n        if self.is_cat==1:\n            df = df[cat_cols]\n        elif self.is_cat==0:\n            df = df[num_cols] \n        else:\n            df = df\n            \n        return df\n    \n    def fit_transform(self, X, y = None):\n        self.fit(X)\n        return self.transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\ndef rmse(y_true, y_pred):\n    result = np.sqrt(mean_squared_error(y_true, y_pred))\n    return result\n\n# def mape(y_true, y_pred):\n#     result = np.abs(y_true-y_pred) / y_true\n#     result = np.mean(result)\n#     return result\n\n# def my_scorer1():\n#     return make_scorer(rmse, greater_is_better=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ori_cols = ['pickup_datetime', 'pickup_longitude', 'pickup_latitude',\n       'dropoff_longitude', 'dropoff_latitude', 'passenger_count']\nnum_cols = ['pickup_longitude', 'pickup_latitude',\n       'dropoff_longitude', 'dropoff_latitude', 'passenger_count',\n       'abs_lat_diff', 'abs_lon_diff', 'manhattan', 'euclidean', 'haversine',\n       'pickup_distance_tojfk', 'dropoff_distance_tojfk',\n       'pickup_distance_tonyc', 'dropoff_distance_tonyc',\n       'pickup_distance_toewr', 'dropoff_distance_toewr',\n       'pickup_distance_tolgr', 'dropoff_distance_tolgr', 'direction',\n       'pickup_Year', 'pickup_Month', 'pickup_Week', 'pickup_Day',\n       'pickup_Dayofweek', 'pickup_Dayofyear', 'pickup_Days_in_month',\n       'pickup_Hour', 'pickup_Minute', 'pickup_Second',\n       'pickup_Days_in_year', 'pickup_frac_day', 'pickup_frac_week',\n       'pickup_frac_month', 'pickup_frac_year', 'pickup_Elapsed']\ncat_cols = ['pickup_is_leap_year','usFedHoliday']\ntarget = 'fare_amount'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain, val = train_test_split(data,test_size=0.2, random_state=RSEED)\ndel data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_transformer = Pipeline(steps=[\n                                ('dataprep', data_transform(num_cols,cat_cols,is_cat=0)),\n                                ('imputer', SimpleImputer(strategy = \"mean\")),\n                                ('scaler', PowerTransformer())  #PowerTransformer\n                                ])\n\ncat_transformer = Pipeline(steps=[\n                                ('dataprep', data_transform(num_cols,cat_cols,is_cat=1)),\n                                ('imputer', SimpleImputer(strategy='most_frequent')),\n                                ('onehot', OneHotEncoder(handle_unknown='error')) #, drop = \"if_binary\"\n                                ])\n\ntransformer = ColumnTransformer(\n    transformers=[\n        ('num', num_transformer, ori_cols),\n        ('cat', cat_transformer, ori_cols)\n    ])\n\nknn = KNeighborsRegressor(n_neighbors=3)\ndt = DecisionTreeRegressor(random_state=123)\nrf = DecisionTreeRegressor(random_state=123)\neln = ElasticNet(alpha=1.0, l1_ratio=0.5, random_state=2020)\nrg = Ridge(alpha=1.0, random_state=2020)\nls = Lasso(alpha=1.0, random_state=2020)\nxgb = XGBRegressor(random_state=2020, booster='gbtree',n_estimators=20, tree_method='hist')\nlgb = LGBMRegressor(objective='regression',random_states=2020, metric = 'rmse', num_leaves = 31, boosting_type='gbdt', max_depth=5, learning_rate=0.034)\ncatb = CatBoostRegressor(iterations=2,learning_rate=0.5,depth=3, silent=True)\n# lgb_goss = lgb.LGBMRegressor(objective='regression',random_states=2020,  boosting_type = 'goss', metric = 'rmse', learning_rate=0.034, num_leaves = 31)\nlr = LinearRegression()\n\n\nstack = StackingCVRegressor(regressors=(knn, lgb, xgb, catb, dt, rf, eln, rg, ls), meta_regressor=lr, cv=3)\n\nstack_pipeline = Pipeline(steps=[('transformer', transformer),\n                      ('stack', stack)\n                      ])\n\nmain_pipeline = TransformedTargetRegressor(stack_pipeline,\n                                    transformer = PowerTransformer())\n\nparams = {  \n#           'regressor__stack__lgbmregressor__boosting_type': ['gbdt','dart'],\n#           'regressor__stack__lgbmregressor__max_depth': [3,5,7],\n#           'regressor__stack__lgbmregressor__learning_rate': [0.05,0.5,1],\n    \n#           'regressor__stack__xgbregressor__n_estimators': [10,20,30],\n#           'regressor__stack__xgbregressor__tree_method: ['exact','approx','hist'],\n           \n#           'regressor__stack__catboostregressor__n_estimators': [10,20,30],\n#           'regressor__stack__catboostregressor__learning_rate': [0.05,0.5,1]\n    \n          }\n\n\nmodel = GridSearchCV(estimator=main_pipeline, param_grid=params,  cv=3, n_jobs=-1, scoring='neg_mean_squared_error' ,refit=True)\n\n\ntrain_X = train.drop(target,axis=1)\ntrain_y = train[target]\n\nmodel.fit(train_X, train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nimport warnings\nwarnings.filterwarnings('ignore', category = RuntimeWarning)\n\ndef metrics(train_pred, valid_pred, y_train, y_valid):\n    \"\"\"Calculate metrics:\n       Root mean squared error and mean absolute percentage error\"\"\"\n    \n    # Root mean squared error\n    train_rmse = np.sqrt(mean_squared_error(y_train, train_pred))\n    valid_rmse = np.sqrt(mean_squared_error(y_valid, valid_pred))\n    \n    # Calculate absolute percentage error\n    train_ape = abs((y_train - train_pred) / y_train)\n    valid_ape = abs((y_valid - valid_pred) / y_valid)\n    \n    # Account for y values of 0\n    train_ape[train_ape == np.inf] = 0\n    train_ape[train_ape == -np.inf] = 0\n    valid_ape[valid_ape == np.inf] = 0\n    valid_ape[valid_ape == -np.inf] = 0\n    \n    train_mape = 100 * np.mean(train_ape)\n    valid_mape = 100 * np.mean(valid_ape)\n    \n    return train_rmse, valid_rmse, train_mape, valid_mape\n\ndef evaluate(model, train,val, label):\n    \"\"\"Mean absolute percentage error\"\"\"\n    train_X = data_transform(num_cols,cat_cols,is_cat=0).transform(train)\n    train_y = train[label]\n    \n    val_X = data_transform(num_cols,cat_cols,is_cat=0).transform(val)\n    val_y = val[label]\n    # Make predictions\n    train_pred = model.predict(train_X)\n    valid_pred = model.predict(val_X)\n    \n    # Get metrics\n    train_rmse, valid_rmse, train_mape, valid_mape = metrics(train_pred, valid_pred,\n                                                             train_y, val_y)\n    \n    print(f'Training:   rmse = {round(train_rmse, 2)} \\t mape = {round(train_mape, 2)}')\n    print(f'Validation: rmse = {round(valid_rmse, 2)} \\t mape = {round(valid_mape, 2)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Model Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# model_lr_tyj = model #rmse val 200\n#model_lr = model0 #rmse val 6.93","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluate(model, train, val, target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X = data_transform(num_cols,cat_cols,is_cat=0).transform(train)\ntrain_y = train[target]\ntrain_pred = model.predict(train_X)\n\nplt.figure(figsize = (10, 6))\nsns.kdeplot(train_y, label = 'Actual')\nsns.kdeplot(train_pred, label = 'Predicted')\nplt.legend(prop = {'size': 30})\nplt.title(\"Distribution of Train Data Fares\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_X = data_transform(num_cols,cat_cols,is_cat=0).transform(val)\nval_y = val[target]\nval_pred = model.predict(val_X)\n\nplt.figure(figsize = (10, 6))\nsns.kdeplot(val_y, label = 'Actual')\nsns.kdeplot(val_pred, label = 'Predicted')\nplt.legend(prop = {'size': 30})\nplt.title(\"Distribution of val Data Fares\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import joblib\n# joblib.dump(model, f'nyk_taxi_lgbm.pkl')\n# joblib.dump(model, f'nyk_taxi_lgbm_v2.pkl')\n# joblib.dump(model, f'nyk_taxi_rf.pkl')\n# joblib.dump(model, f'nyk_taxi_rf_yjt.pkl')\n#joblib.dump(model, f'nyk_taxi_lgb_yjt_700k.pkl')\n#joblib.dump(model, f'nyk_taxi_lgb_yjt_700k_tune1.pkl')\n# joblib.dump(model, f'nyk_taxi_lgb_yjt_700k_tune2.pkl')\n#joblib.dump(model, f'nyk_taxi_lgb_yjt_3mio_tune3.pkl')\n#joblib.dump(model, f'nyk_taxi_lgb_yjt_3mio_goss.pkl')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_gbdt = joblib.load(f'nyk_taxi_lgb_yjt_3mio_tune3.pkl')\nmodel_goss = joblib.load(f'nyk_taxi_lgb_yjt_3mio_goss.pkl')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Bulk Prediction & Kaggle Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('../input/test.csv', parse_dates = ['pickup_datetime'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_X = test.drop('key',axis=1)\n# Use the model to make predictions\npredicted_fare = model_gbdt.predict(test_X)*0.95553+ model_goss.predict(test_X)*0.00007\n# We will look at the predicted prices to ensure we have something sensible.\nprint(predicted_fare)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_submission = pd.DataFrame({'key': test.key, 'fare_amount': predicted_fare})\n# you could use any filename. We choose submission here\nmy_submission.to_csv('submission_v10.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_submission.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Single Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"dt    = '2019-06-20 12:26:21'\nplong =  -73.844\nplat  =  60.721\ndlong =  123.842\ndlat  =  60.712\npc    =  5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cek = pd.DataFrame({'pickup_datetime' : [dt], 'pickup_longitude' : [plong], 'pickup_latitude' : [plat], 'dropoff_longitude' : [dlong], 'dropoff_latitude' : [dlat], 'passenger_count' : [pc]})\ncek = data_transform(num_cols,cat_cols,is_cat=0).transform(cek)\n# model.predict(cek)\nmodel_gbdt.predict(cek)*0.95553+ model_goss.predict(cek)*0.00007","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### =================================================================================="},{"metadata":{"trusted":true},"cell_type":"code","source":"class data_transform_v0():\n    \n    \n    def __init__(self, num, cat):\n        self.num = num\n        self.cat = cat\n        \n    def fit(self, X):\n        # do not do anything\n        return self\n    \n    def transform(self, X, y = None):\n        num_cols = self.num\n        cat_cols = self.cat\n        df = X\n        df['passenger_count'] = np.where(df['passenger_count']<1,1,df['passenger_count'])\n        df['passenger_count'] = np.where(df['passenger_count']>6,5,df['passenger_count'])\n        df['pickup_latitude'] = np.where(df['pickup_latitude']<40, 40,df['pickup_latitude'])\n        df['pickup_latitude'] = np.where(df['pickup_latitude']>42,42,df['pickup_latitude'])\n        df['dropoff_latitude'] = np.where(df['dropoff_latitude']<40, 40,df['dropoff_latitude'])\n        df['dropoff_latitude'] = np.where(df['dropoff_latitude']>42, 42,df['dropoff_latitude'])\n        df['pickup_longitude'] = np.where(df['pickup_longitude']<-75, -75,df['pickup_longitude'])\n        df['pickup_longitude'] = np.where(df['pickup_longitude']>-72, -72,df['pickup_longitude'])\n        df['dropoff_longitude'] = np.where(df['dropoff_longitude']<-75, -75,df['dropoff_longitude'])\n        df['dropoff_longitude'] = np.where(df['dropoff_longitude']>-72, -72,df['dropoff_longitude'])\n\n        \n        df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n        # Absolute difference in latitude and longitude\n        df['abs_lat_diff'] = (df['dropoff_latitude'] - df['pickup_latitude']).abs()\n        df['abs_lon_diff'] = (df['dropoff_longitude'] - df['pickup_longitude']).abs()\n\n        df['manhattan'] = minkowski_distance(df['pickup_longitude'], df['dropoff_longitude'],\n                                               df['pickup_latitude'], df['dropoff_latitude'], 1)\n\n        df['euclidean'] = minkowski_distance(df['pickup_longitude'], df['dropoff_longitude'],\n                                               df['pickup_latitude'], df['dropoff_latitude'], 2)\n\n        df['haversine'] =  haversine_np(df['pickup_longitude'], df['pickup_latitude'],\n                                 df['dropoff_longitude'], df['dropoff_latitude']) \n\n        for i in place['loc'].tolist():\n            for j in ['pickup','dropoff']:\n                df[str(j)+'_distance_to'+str(i)] = distance_to_place(df,i,str(j)+'_longitude',str(j)+'_latitude')\n\n        df['direction'] = calculate_direction(df)\n\n        df = extract_dateinfo(df, 'pickup_datetime', drop = False, \n                                 time = True, start_ref = df['pickup_datetime'].min())\n\n        df = df[num_cols+cat_cols]\n        df[cat_cols] = df[cat_cols].astype(str)\n            \n        return df\n    \n    def fit_transform(self, X, y = None):\n        self.fit(X)\n        return self.transform(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Alasan Tidak menggunakan Transformer Log pada Label Y dan Mengganti jadi Yoe Jhonson"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_transformer = Pipeline(steps=[\n                                ('imputer', SimpleImputer(strategy = \"mean\")),\n                                ('scaler', PowerTransformer())  #PowerTransformer\n                                ])\n\ncat_transformer = Pipeline(steps=[\n                                ('imputer', SimpleImputer(strategy='most_frequent')),\n                                ('onehot', OneHotEncoder(handle_unknown='error')) #, drop = \"if_binary\"\n                                ])\ntransformer = ColumnTransformer(\n    transformers=[\n        ('num', num_transformer, num_cols),\n#        ('cat', cat_transformer, cat_cols)\n    ])\n\n# main_pipeline = Pipeline(steps=[('transformer', transformer),\n#                       ('lgbm', lgb.LGBMRegressor(objective='regression',random_states=2020))])\n\n# param_grid = {\n# #                 'lgbm__num_leaves': [30, 60, 90],\n# #                 'lgbm__max_depth': [3, 5, 7],\n# #                 'lgbm__n_estimators': [200, 400, 500],\n#                 'lgbm__boosting' : ['goss'] #,'dart','gbdt'\n#             }\n\n\n# model = GridSearchCV(main_pipeline, param_grid, n_jobs=-1,scoring = 'neg_mean_absolute_error', cv=3)\n\nmodel0 = Pipeline(steps=[('transformer', transformer),\n                      ('lr', LinearRegression())])\n\nmodel0_log = Pipeline(steps=[('transformer', transformer),\n                      ('lr', LinearRegression())])\n\n# model = TransformedTargetRegressor(model0,\n#                                   func=np.log, \n#                                   inverse_func=np.exp)\n\n\ntrain = train[train['fare_amount'].between(left = 2.5, right = 100)]\ntrain = train.loc[(train['passenger_count'] > 0)&(train['passenger_count'] < 6)]\ntrain = train.loc[train['pickup_latitude'].between(40, 42)]\ntrain = train.loc[train['pickup_longitude'].between(-75, -72)]\ntrain = train.loc[train['dropoff_latitude'].between(40, 42)]\ntrain = train.loc[train['dropoff_longitude'].between(-75, -72)]\n\ntrain_X = data_transform_v0(num_cols,cat_cols).transform(train)\ntrain_y = train[target]\n\nmodel0_log.fit(train_X, np.log(train_y))\nmodel0.fit(train_X, train_y)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X = data_transform_v0(num_cols,cat_cols).transform(train)\ntrain_y = train[target]\ntrain_pred = model0_log.predict(train_X)\ntrain_pred = np.exp(train_pred)\n\nplt.figure(figsize = (10, 6))\nsns.kdeplot(train_y, label = 'Actual')\nsns.kdeplot(train_pred, label = 'Predicted')\nplt.legend(prop = {'size': 30})\nplt.title(\"Distribution of Train Data Fares\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X = data_transform_v0(num_cols,cat_cols).transform(train)\ntrain_y = train[target]\ntrain_pred = model0.predict(train_X)\n\nplt.figure(figsize = (10, 6))\nsns.kdeplot(train_y, label = 'Actual')\nsns.kdeplot(train_pred, label = 'Predicted')\nplt.legend(prop = {'size': 30})\nplt.title(\"Distribution of Train Data Fares\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.log(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in num_cols:\n    print(i)\n    sns.distplot(train[i])\n    sns.distplot(np.log(train[i]))\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fnidings:\n1. tranformasi label dengan log evaluasinya tidak bagus\n2. setelah di cek kenapa nya, kasus ini disebabkan ada value 0, seingga waktu di transformasi pake log hasilnya -inf\n3. perlu EDA ada passanger count 0 dan ada kasus tidak ada perubahan longitude latitude"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_transformer = Pipeline(steps=[\n                                ('imputer', SimpleImputer(strategy = \"mean\")),\n                                ('scaler', PowerTransformer())  #PowerTransformer\n                                ])\n\ncat_transformer = Pipeline(steps=[\n                                ('imputer', SimpleImputer(strategy='most_frequent')),\n                                ('onehot', OneHotEncoder(handle_unknown='error')) #, drop = \"if_binary\"\n                                ])\ntransformer = ColumnTransformer(\n    transformers=[\n        ('num', num_transformer, num_cols),\n#        ('cat', cat_transformer, cat_cols)\n    ])\n\n# main_pipeline = Pipeline(steps=[('transformer', transformer),\n#                       ('lgbm', lgb.LGBMRegressor(objective='regression',random_states=2020))])\n\n# param_grid = {\n# #                 'lgbm__num_leaves': [30, 60, 90],\n# #                 'lgbm__max_depth': [3, 5, 7],\n# #                 'lgbm__n_estimators': [200, 400, 500],\n#                 'lgbm__boosting' : ['goss'] #,'dart','gbdt'\n#             }\n\n\n# model = GridSearchCV(main_pipeline, param_grid, n_jobs=-1,scoring = 'neg_mean_absolute_error', cv=3)\n\nmodel0 = Pipeline(steps=[('transformer', transformer),\n                      ('lr', LinearRegression())])\n\nmodel = TransformedTargetRegressor(model0,\n                                   transformer = PowerTransformer())\n\n\ntrain = train[train['fare_amount'].between(left = 2.5, right = 100)]\ntrain = train.loc[train['passenger_count'] < 6]\ntrain = train.loc[train['pickup_latitude'].between(40, 42)]\ntrain = train.loc[train['pickup_longitude'].between(-75, -72)]\ntrain = train.loc[train['dropoff_latitude'].between(40, 42)]\ntrain = train.loc[train['dropoff_longitude'].between(-75, -72)]\n\ntrain_X = data_transform_v0(num_cols,cat_cols).transform(train)\ntrain_y = np.array(train[target]).reshape(-1,1)\n\n\n# model0.fit(train_X, np.log(train_y))\nmodel.fit(train_X, train_y)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['result_tyj_pipe'] =model.predict(train_X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train['result_tyj_pipe']>100]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cek1 = train #[train['result_tyj_pipe']<=100]\nplt.figure(figsize = (10, 6))\nsns.kdeplot(cek1['fare_amount'], label = 'Actual')\nsns.kdeplot(cek1['result_tyj_pipe'], label = 'Predicted')\nplt.legend(prop = {'size': 30})\nplt.title(\"Distribution of Train Data Fares\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train1 = train.copy()\ntrain1['fare_amount']\npt = PowerTransformer()\npt.fit(train1[['fare_amount']])\n\npt.transform(train1[['fare_amount']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pt.inverse_transform(pt.transform(train1[['fare_amount']]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pt.get_params(deep=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_transformer = Pipeline(steps=[\n                                ('imputer', SimpleImputer(strategy = \"mean\")),\n                                ('scaler', PowerTransformer())  #PowerTransformer\n                                ])\n\ncat_transformer = Pipeline(steps=[\n                                ('imputer', SimpleImputer(strategy='most_frequent')),\n                                ('onehot', OneHotEncoder(handle_unknown='error')) #, drop = \"if_binary\"\n                                ])\ntransformer = ColumnTransformer(\n    transformers=[\n        ('num', num_transformer, num_cols),\n#        ('cat', cat_transformer, cat_cols)\n    ])\n\n# main_pipeline = Pipeline(steps=[('transformer', transformer),\n#                       ('lgbm', lgb.LGBMRegressor(objective='regression',random_states=2020))])\n\n# param_grid = {\n# #                 'lgbm__num_leaves': [30, 60, 90],\n# #                 'lgbm__max_depth': [3, 5, 7],\n# #                 'lgbm__n_estimators': [200, 400, 500],\n#                 'lgbm__boosting' : ['goss'] #,'dart','gbdt'\n#             }\n\n\n# model = GridSearchCV(main_pipeline, param_grid, n_jobs=-1,scoring = 'neg_mean_absolute_error', cv=3)\n\nmodel = Pipeline(steps=[('transformer', transformer),\n                      ('lr', LinearRegression())])\n\n\ntrain = train[train['fare_amount'].between(left = 2.5, right = 100)]\ntrain = train.loc[train['passenger_count'] < 6]\ntrain = train.loc[train['pickup_latitude'].between(40, 42)]\ntrain = train.loc[train['pickup_longitude'].between(-75, -72)]\ntrain = train.loc[train['dropoff_latitude'].between(40, 42)]\ntrain = train.loc[train['dropoff_longitude'].between(-75, -72)]\n\ntrain_X = data_transform_v0(num_cols,cat_cols).transform(train)\ntrain_y = train[[target]]\n\npt = PowerTransformer()\npt.fit(train_y)\ntrain_yt = pt.transform(train_y)\n\n# model0.fit(train_X, np.log(train_y))\nmodel.fit(train_X, train_yt)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cek= model.predict(train_X)\npt.inverse_transform(cek).max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(pt.inverse_transform(cek)>100).sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['result_tyj'] = pt.inverse_transform(cek)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train['result_tyj'] >100]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cek1 = train#[train['result_tyj'] <=100]\nsns.distplot(cek1['result_tyj'],color='r')\nsns.distplot(cek1['fare_amount'],color='b')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}