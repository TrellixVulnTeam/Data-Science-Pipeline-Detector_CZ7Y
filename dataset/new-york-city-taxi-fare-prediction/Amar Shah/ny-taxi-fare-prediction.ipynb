{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1> New York Taxi Fare Prediction </h1>\n<h2> Introduction </h2>\nThis project was about learning how to spot poor or incorrect data, as well as learning how to deal with large data sets where I only train on a subsection of the data.\nI used the project below extensively for guidance and inspiration.\n\nhttps://www.kaggle.com/btyuhas/bayesian-optimization-with-xgboost\n\nThe notebook linked below was invaluable in generating the haversine distance feature as well as the Airport distance feature, which was published in a popular notebook for this competition.\n\nhttps://www.kaggle.com/jsylas/python-version-of-top-ten-rank-r-22-m-2-88\n","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn import preprocessing\nfrom sklearn.cluster import KMeans\n\nfrom xgboost import XGBRegressor\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\ntrain = pd.read_csv('/kaggle/input/new-york-city-taxi-fare-prediction/train.csv', nrows = 20000, parse_dates = ['pickup_datetime'])\ntest = pd.read_csv('/kaggle/input/new-york-city-taxi-fare-prediction/test.csv', parse_dates = ['pickup_datetime'])","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2> Preprocessing the data </h2>\n\nInitially I dropped the 0 values from columns which should not have any zeroes","metadata":{}},{"cell_type":"code","source":"nonzerocols = ['pickup_latitude', 'pickup_longitude', 'dropoff_longitude', 'dropoff_latitude', 'passenger_count']\nfor col in nonzerocols:\n    train.drop(train[train[col] == 0].index, inplace = True)\ntrain.reset_index(inplace=True, drop = True)\n","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Whilst looking at plots of the data, I discovered this interesting fact - some of the positional values seem to have been switched.\n\nNew York is at a latitude of 40.7 degrees, and a longitude of -74 degrees. \n\nHere we have a small minority of journies that appear to have their longitude and latitude switched! When examining the histogram plots, it was clear this only affected a few entries. (This is becasuse I found 3 errors in the first 10,000 values of pickup_latitude).\n\nIt is possible to fix this in order to use all the data, however given that we have 5Gb of data already, I will opt to drop the incorrect rows instead.\n\nIn addition to the switching, some of these values are incorrect, for example one value is 0.34 for latitude. Since New York is nowhere near zero latitude I will drop latitude values outside of the range (38,42) and longitude values outside of the range (-76, -72).\n\nFortunately the test data does indeed have the correct values inputted for latitude and longitude.\n\nI also dropped all negative fares.","metadata":{}},{"cell_type":"code","source":"train.dropna(how='any', axis='rows', inplace=True)\ntrain.drop(train[(train['pickup_latitude'] < 38) | (train['pickup_latitude'] > 42)].index, inplace = True)\ntrain.drop(train[(train['dropoff_latitude'] < 38) | (train['dropoff_latitude'] > 42)].index, inplace = True)\ntrain.drop(train[(train['pickup_longitude'] < -76) | (train['pickup_longitude'] > -72)].index, inplace = True)\ntrain.drop(train[(train['dropoff_longitude'] < -76) | (train['dropoff_longitude'] > -72)].index, inplace = True)\ntrain.drop(train[(train['fare_amount'] < 0) | (train['fare_amount'] > 500)].index, inplace = True)\n\ntrain.reset_index(inplace=True, drop = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now I combined the data to make preprocessing slightly easier.","metadata":{}},{"cell_type":"code","source":"train[\"is_train\"] = 1\ntest[\"is_train\"] = 0\ndata = pd.concat([train, test.drop(['key'], axis = 1)])\n\ndata[\"lat_diff_squared\"] = np.power(data.dropoff_latitude.subtract(data.pickup_latitude),2)\ndata[\"long_diff_squared\"] = np.power(data.dropoff_longitude.subtract(data.pickup_longitude),2)\n\ndata['day'] = data['pickup_datetime'].dt.day\ndata['month'] = data['pickup_datetime'].dt.month\ndata['year'] = data['pickup_datetime'].dt.year\ndata['hour'] = data['pickup_datetime'].dt.hour\n\ndata = pd.concat([data, pd.get_dummies(data.hour, prefix = 'hour')], axis = 1)\ndata = pd.concat([data, pd.get_dummies(data.day, prefix = 'day')], axis = 1)\ndata = pd.concat([data, pd.get_dummies(data.month, prefix = 'month')], axis = 1)\n\ndata = data.drop(['hour', 'day','month'], axis = 1)\n\ndata.drop(['key', 'pickup_datetime'], axis = 1, inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As an example, the distribution of one of the coordinate features is shown below.","metadata":{}},{"cell_type":"code","source":"plt.hist(data.dropoff_longitude, range = (-74.1, -73.7), bins = 70)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2> Feature Engineering </h2>\n\nInitially I used a K-means clustering algorithm on the geographical data to generate points of interest as features. (see end of notebook for code)\n\nUltimately this proved unsuccessful, and so I instead opted to use the features given in [this](https://www.kaggle.com/jsylas/python-version-of-top-ten-rank-r-22-m-2-88) notebook, namely the haversine distance and airport distances.","metadata":{}},{"cell_type":"code","source":"def sphere_dist(pickup_lat, pickup_lon, dropoff_lat, dropoff_lon):\n    \"\"\"\n    Return distance along great radius between pickup and dropoff coordinates.\n    \"\"\"\n    #Define earth radius (km)\n    R_earth = 6371\n    #Convert degrees to radians\n    pickup_lat, pickup_lon, dropoff_lat, dropoff_lon = map(np.radians,\n                                                             [pickup_lat, pickup_lon, \n                                                              dropoff_lat, dropoff_lon])\n    #Compute distances along lat, lon dimensions\n    dlat = dropoff_lat - pickup_lat\n    dlon = dropoff_lon - pickup_lon\n    \n    #Compute haversine distance\n    a = np.sin(dlat/2.0)**2 + np.cos(pickup_lat) * np.cos(dropoff_lat) * np.sin(dlon/2.0)**2\n    return 2 * R_earth * np.arcsin(np.sqrt(a))\n    \ndata['haversine_dist'] = sphere_dist(data.pickup_latitude, data.pickup_longitude, data.dropoff_latitude, data.dropoff_longitude)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nReturn minumum distance from pickup or dropoff coordinates to each airport.\nJFK: John F. Kennedy International Airport\nEWR: Newark Liberty International Airport\nLGA: LaGuardia Airport\nSOL: Statue of Liberty \nNYC: Newyork Central\n\"\"\"\njfk_coord = (40.639722, -73.778889)\newr_coord = (40.6925, -74.168611)\nlga_coord = (40.77725, -73.872611)\nsol_coord = (40.6892,-74.0445) # Statue of Liberty\nnyc_coord = (40.7141667,-74.0063889) \n\n\npickup_lat = data['pickup_latitude']\ndropoff_lat = data['dropoff_latitude']\npickup_lon = data['pickup_longitude']\ndropoff_lon = data['dropoff_longitude']\n\npickup_jfk = sphere_dist(pickup_lat, pickup_lon, jfk_coord[0], jfk_coord[1]) \ndropoff_jfk = sphere_dist(jfk_coord[0], jfk_coord[1], dropoff_lat, dropoff_lon) \npickup_ewr = sphere_dist(pickup_lat, pickup_lon, ewr_coord[0], ewr_coord[1])\ndropoff_ewr = sphere_dist(ewr_coord[0], ewr_coord[1], dropoff_lat, dropoff_lon) \npickup_lga = sphere_dist(pickup_lat, pickup_lon, lga_coord[0], lga_coord[1]) \ndropoff_lga = sphere_dist(lga_coord[0], lga_coord[1], dropoff_lat, dropoff_lon)\npickup_sol = sphere_dist(pickup_lat, pickup_lon, sol_coord[0], sol_coord[1]) \ndropoff_sol = sphere_dist(sol_coord[0], sol_coord[1], dropoff_lat, dropoff_lon)\npickup_nyc = sphere_dist(pickup_lat, pickup_lon, nyc_coord[0], nyc_coord[1]) \ndropoff_nyc = sphere_dist(nyc_coord[0], nyc_coord[1], dropoff_lat, dropoff_lon)\n\n\n\ndata['jfk_dist'] = pickup_jfk + dropoff_jfk\ndata['ewr_dist'] = pickup_ewr + dropoff_ewr\ndata['lga_dist'] = pickup_lga + dropoff_lga\ndata['sol_dist'] = pickup_sol + dropoff_sol\ndata['nyc_dist'] = pickup_nyc + dropoff_nyc\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that the features are made I can split the data and begin modelling.","metadata":{}},{"cell_type":"code","source":"train = data.loc[data[\"is_train\"] == 1]\ntest = data.loc[data[\"is_train\"] == 0]\nX = train.drop(['fare_amount', 'is_train'], axis = 1)\ny = train.fare_amount\nX_test = test.drop(['fare_amount','is_train'], axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2> Modelling </h2>\n\nI initially wanted to use an ensemble of tree methods, however I decided to instead train on 1.5m data points using a simple xgbr. \n\nI attempted to optimise hyperparameters whilst only using 20000 data points, however I found this to be very variable and actually made the model perform worse when training on all 1.5m data points (see appendix for code). This is a problem I wish to learn how to solve in the future - how do you optimise hyperparameters under the constraint that you cannot test them on your full training data set?","metadata":{}},{"cell_type":"code","source":"xgbr = XGBRegressor()\nxgbr.fit(X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2> Conclusions </h2>\n\nInterestingly my best result came by using a feature given by the square difference of the dropoff and pickup coordinates. I am unsure how to optimise the use of these valuable other features I have found. \n\nThe plots below show the feature importance is completely dominated by Haversine distance, and I have not yet learned how to prevent the tree from using this feature so heavily.\n\nIn future projects I would like to address this problem, as well as the hyperparameter question mentioned previously. My best score with all the features produced a mean average error of $3.15 using default xgbr parameters and only 1m data points. With 1.5m data points the score got worse, suggesting an overfitting issue due to lack of any regularisation from non-default hyperparameters. \n\nThis project should probably have been tackled with neural networks, and this is an approach I would enjoy attempting in future.","metadata":{}},{"cell_type":"code","source":"num_feat = 10\nplt.xticks(range(1,len(X.columns[-num_feat:])+1), X.columns[-num_feat:], rotation=70, fontsize = 15)\nplt.bar(x = range(1,len(X.columns[-num_feat:])+1), height = xgbr.feature_importances_.tolist()[-num_feat:])","metadata":{"_kg_hide-output":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2> Appendix </h2>\n\nBelow shows the code I initially used to generate 5 cluster points on the map.","metadata":{}},{"cell_type":"code","source":"# kmeans = KMeans(n_clusters = 5)\n# train['cluster_label_pickup'] = kmeans.fit_predict(train[['pickup_latitude', 'pickup_longitude']])\n# test['cluster_label_pickup'] = kmeans.predict(test[['pickup_latitude', 'pickup_longitude']])\n\n# # plt.figure(figsize = (7,7))\n# # plt.scatter(x = train['pickup_latitude'], y = train['pickup_longitude'], c=train.cluster_label_pickup, cmap='viridis')\n# # plt.scatter(kmeans.cluster_centers_[:, 0],kmeans.cluster_centers_[:,1], s = 100, c = 'red')\n\n\n# train = pd.concat([train, pd.get_dummies(train.cluster_label_pickup, prefix = 'clust_pick')], axis = 1)\n# train = train.drop(['cluster_label_pickup'], axis = 1)\n\n# test = pd.concat([test, pd.get_dummies(test.cluster_label_pickup, prefix = 'clust_pick')], axis = 1)\n# test = test.drop(['cluster_label_pickup'], axis = 1)\n\n# train['cluster_label_dropoff'] = kmeans.fit_predict(train[['dropoff_latitude', 'dropoff_longitude']])\n# test['cluster_label_dropoff'] = kmeans.predict(test[['dropoff_latitude', 'dropoff_longitude']])\n\n# train = pd.concat([train, pd.get_dummies(train.cluster_label_dropoff, prefix = 'clust_drop')], axis = 1)\n# train = train.drop(['cluster_label_dropoff'], axis = 1)\n\n# test = pd.concat([test, pd.get_dummies(test.cluster_label_dropoff, prefix = 'clust_drop')], axis = 1)\n# test = test.drop(['cluster_label_dropoff'], axis = 1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This code I used to try to optimise the number of clusters, however this proved to do more harm than good and requires a more thoughtful approach.","metadata":{}},{"cell_type":"code","source":"#tested both xgbr and rf, basically no difference in cross val scores, maybe slightly better for about 6 clusters. However test data less spread so 6 clusters too many\n# for i in range(2,11):\n#     kmeans = KMeans(n_clusters = i)\n    \n#     train['cluster_label_pickup'] = kmeans.fit_predict(train[['pickup_latitude', 'pickup_longitude']])\n\n#     train = pd.concat([train, pd.get_dummies(train.cluster_label_pickup, prefix = 'clust_pick')], axis = 1)\n#     train = train.drop(['cluster_label_pickup'], axis = 1)\n\n#     train['cluster_label_dropoff'] = kmeans.fit_predict(train[['dropoff_latitude', 'dropoff_longitude']])\n    \n\n#     train = pd.concat([train, pd.get_dummies(train.cluster_label_dropoff, prefix = 'clust_drop')], axis = 1)\n#     train = train.drop(['cluster_label_dropoff'], axis = 1)\n\n    \n#     print(cross_val_score(rf, train.drop(['fare_amount'], axis = 1), train.fare_amount, cv = 3, scoring = 'neg_mean_absolute_error').mean(), i)\n    \n#     #reset training data so we can remake columns with new kmeans cluster number\n#     train = train.loc[:,:'long_diff_squared']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Below is the code for hyperparameter optimisation.","metadata":{}},{"cell_type":"code","source":"# parameter_grid_xgbr = {'max_depth':[1,2,3],'min_child_weight':[0.01],\n#                         'gamma':[0, 0.1, 0.2]}\n# best_xgbr = GridSearchCV(XGBRegressor(), param_grid = parameter_grid_xgbr,cv=3, verbose = 1,n_jobs=-1, scoring = 'neg_mean_absolute_error')\n# best_xgbr.fit(X,y)\n# print(best_xgbr.best_params_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Prediction submission code is given below.","metadata":{}},{"cell_type":"code","source":"# preds = xgbr.predict(X_test)\n# test_submission = pd.DataFrame({'key':pd.read_csv('/kaggle/input/new-york-city-taxi-fare-prediction/test.csv', parse_dates = ['pickup_datetime']).key, 'fare_amount':preds})\n# test_submission.to_csv('submission27.csv', index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}