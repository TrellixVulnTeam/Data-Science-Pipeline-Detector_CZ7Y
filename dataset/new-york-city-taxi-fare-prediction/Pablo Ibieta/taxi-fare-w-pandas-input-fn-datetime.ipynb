{"cells":[{"metadata":{"_uuid":"8f5b8cff15323d4109c2edb8aa4e4141ac8b670f"},"cell_type":"markdown","source":"# What's New?\n## In this version we implement more feature engineering to the model. In particular, we include information extracted from the pickup_datetime column which was ignored so far"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport tensorflow as tf\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport shutil\nimport os\nprint('../', os.listdir('../'))\nprint('../input', os.listdir(\"../input\"))\nprint('tf version: ', tf.__version__)\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df =  pd.read_csv('../input/train.csv', nrows= 200000, parse_dates=['pickup_datetime'])\ntest = pd.read_csv('../input/test.csv',parse_dates=['pickup_datetime'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4853f5e9fab8fa54dd6afd10994e1f6489f6e512"},"cell_type":"code","source":"df.pickup_datetime.dt.weekday_name","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a3a11afbfe33e403d0d16d014cf80a867a757ab3"},"cell_type":"code","source":"#calculating the distance between the two points (pickup and dropoff) in km\nfrom math import cos, asin, sqrt\ndef distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295     #Pi/180\n    a = 0.5 - cos((lat2 - lat1) * p)/2 + cos(lat1 * p) * cos(lat2 * p) * (1 - cos((lon2 - lon1) * p)) / 2\n    return 12742 * asin(sqrt(a)) #2*R*asin...","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"38c06725ed84e26b9faa4ae47cf93f21ba380e40"},"cell_type":"code","source":"# to include the daytime features\ndef add_feats(df):\n    df['distance'] = pd.concat([pd.DataFrame([distance(df['pickup_latitude'][i],df['pickup_longitude'][i],df['dropoff_latitude'][i],df['dropoff_longitude'][i])], columns=['distance']) for i in range(len(df))], ignore_index=True)\n    df['hour'] = df.pickup_datetime.dt.hour\n   # df['day'] = df.pickup_datetime.dt.day\n   # df['month'] = df.pickup_datetime.dt.month\n    df['weekday'] = df.pickup_datetime.dt.weekday\n   # df['weekday'] = df.pickup_datetime.dt.weekday_name\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"468d2373685fa84da29bcebacfba531cd294b1d8"},"cell_type":"code","source":"add_feats(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"759614f68a757bded70ab85ae8e842d964666da1"},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"35e5676a7c5d53367778050afc3fd5ff9bb97405"},"cell_type":"code","source":"df.pickup_datetime.isnull().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"96f9901a8f2bb02cd6de319dcf43e11c44aa7546"},"cell_type":"markdown","source":"# Perform the cut\n## maybe it can be optimized"},{"metadata":{"trusted":true,"_uuid":"0698b989981f5fdeb351aab8146d62cd9c500649"},"cell_type":"code","source":"dfc = df[((df.pickup_longitude >= -75.0) & (df.pickup_longitude <= -72)) \n         & ((df.pickup_latitude >= 38) & (df.pickup_latitude <= 42)) \n         & ((df.dropoff_longitude >= -75.0) & (df.dropoff_longitude <= -72)) \n         & ((df.dropoff_latitude >= 38) & (df.dropoff_latitude <= 42)) \n         & (df.fare_amount > 2.5) & (df.passenger_count > 0) & (df.passenger_count < 7) & (df.distance > 0.2)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"66effb068e02e959795af7daef9f5a3324c76fcc"},"cell_type":"markdown","source":"# Split the data into traindf and evaldf \n"},{"metadata":{"trusted":true,"_uuid":"3ae77569f061671aa5710c0d1b8dda3f24dec70a"},"cell_type":"code","source":"# now, we split the data into the train and validation sets\nnp.random.seed(seed=1) #makes result reproducible\nmsk = np.random.rand(len(dfc)) < 0.8\ntraindf = dfc[msk].drop(['key', 'pickup_datetime'], axis=1)\nevaldf = dfc[~msk].drop(['key', 'pickup_datetime'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9bad84ba35a37268c0ed59776b302dee43a344b4"},"cell_type":"code","source":"testdf = add_feats(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3605326c901db394ee9c1d210f183848350de46d"},"cell_type":"code","source":"testdf = test.drop(['key', 'pickup_datetime'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bed470e9a94e87ffe124d72bbfd6bbfeb03cbca9"},"cell_type":"code","source":"traindf.weekday.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"86952066d37828fcd8043a3f35da27737d06900d"},"cell_type":"code","source":"def build_model_columns(nbuckets = 10):\n    \"\"\"Builds a set of wide and deep feature columns.\"\"\"\n    # Numeric columns\n    plon = tf.feature_column.numeric_column('pickup_longitude')\n    plat = tf.feature_column.numeric_column('pickup_latitude')\n    dlon = tf.feature_column.numeric_column('dropoff_longitude')\n    dlat = tf.feature_column.numeric_column('dropoff_latitude')\n    pcount = tf.feature_column.numeric_column('passenger_count')\n    dist = tf.feature_column.numeric_column('distance') # this should be an engineered feature for the final model\n    \n    #Categorical columns for weekday and hour, make it numerical... it is not working because of that (I think)\n    wday = tf.feature_column.numeric_column('weekday')\n    wday_b = tf.feature_column.categorical_column_with_identity('weekday', num_buckets= 7)    \n    hour = tf.feature_column.numeric_column('hour')\n    # then bucketize to make the feature cross later and then feed the day_hr feat cross into the N.N.\n     # hour_b = tf.feature_column.bucketized_column(hour, boundaries = [-1, 5, 7, 10 ,16, 21, 23])\n    hour_b = tf.feature_column.categorical_column_with_identity('hour',num_buckets= 24)\n\n   # Bucketized columns for pickup and dropoff coordinates  \n    latbuckets = np.linspace(38.0, 42.0, nbuckets).tolist()\n    lonbuckets = np.linspace(-75.0, -72.0, nbuckets).tolist()\n    b_plat = tf.feature_column.bucketized_column(plat, latbuckets)\n    b_dlat = tf.feature_column.bucketized_column(dlat, latbuckets)\n    b_plon = tf.feature_column.bucketized_column(plon, lonbuckets)\n    b_dlon = tf.feature_column.bucketized_column(dlon, lonbuckets)\n    \n\n    # Feature cross\n    ploc = tf.feature_column.crossed_column([b_plat, b_plon], nbuckets * nbuckets)\n    dloc = tf.feature_column.crossed_column([b_dlat, b_dlon], nbuckets * nbuckets)\n    pd_pair = tf.feature_column.crossed_column([ploc, dloc], nbuckets ** 4 )\n    day_hr =  tf.feature_column.crossed_column([hour_b, wday_b], 24 * 7)\n    \n    \n    # Wide columns\n    wide_columns = [\n        # crossed columns go here\n        #dloc, ploc,\n        dloc, ploc, pd_pair,\n        day_hr,\n\n        # Sparse columns\n        wday, hour,\n\n        # Anything with a linear relationship\n        pcount \n    ]\n    \n    #Deep columns\n    deep_columns = [\n        # Embedding_column to \"group\" together ...\n        tf.feature_column.embedding_column(pd_pair, 10),\n        tf.feature_column.embedding_column(day_hr, 10),\n\n        # Numeric columns\n        plat, plon, dlat, dlon, dist\n        #latdiff, londiff, euclidean\n    ]\n    return wide_columns, deep_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"00d76d8d7a0027951a50ba85d05e74c1fa300350"},"cell_type":"code","source":"def build_estimator(model_dir, nbuckets = 10):\n    \"\"\"Build an estimator appropriate for the given model type.\"\"\"\n    wide_columns, deep_columns = build_model_columns()\n    hidden_units = [512, 256, 128, 64, 32, 4]\n    \n  # Create a tf.estimator.RunConfig to ensure the model is run on CPU, which\n  # trains faster than GPU for this model.\n    run_config = tf.estimator.RunConfig().replace(\n    session_config=tf.ConfigProto(device_count={'GPU': 0}))\n    return tf.estimator.DNNLinearCombinedRegressor(\n        model_dir=model_dir,\n        dnn_activation_fn=tf.nn.relu,\n        linear_feature_columns=wide_columns,\n        dnn_feature_columns=deep_columns,\n        dnn_hidden_units=hidden_units,\n        config=run_config)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"20f7b58a83859dbfd07fadf55765d4baac52c6e7"},"cell_type":"code","source":"OUTDIR = './taxi_trained'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ae816ababe54bf2204abb409cd343e49a0ed1dda"},"cell_type":"code","source":"BATCH_SIZE = 512\ntrain_input_fn = tf.estimator.inputs.pandas_input_fn(x = traindf[list(traindf.drop(['fare_amount'], axis=1).keys())],\n                                                    y = traindf['fare_amount'],\n                                                    num_epochs = 100,\n                                                    batch_size = BATCH_SIZE,\n                                                    shuffle = True)\neval_input_fn = tf.estimator.inputs.pandas_input_fn(x = evaldf[list(traindf.drop(['fare_amount'], axis=1).keys())],\n                                                    y = evaldf[\"fare_amount\"],\n                                                    num_epochs = 1, \n                                                    batch_size = len(evaldf), \n                                                    shuffle=False)\npredict_input_fn = tf.estimator.inputs.pandas_input_fn(x = testdf[list(testdf.keys())],\n                                                    y = None,\n                                                    num_epochs = 1, \n                                                    batch_size = len(testdf), \n                                                    shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8c9e6926145d647205a5d328b99ec9bbc5b38117"},"cell_type":"code","source":"shutil.rmtree(OUTDIR, ignore_errors = True) # start fresh each time\nnum_train_steps = (100 * len(traindf)) / BATCH_SIZE\n  #myopt = tf.train.FtrlOptimizer(learning_rate = 0.01) # note the learning rate\nestimator = build_estimator(OUTDIR)\ndef rmse(labels, predictions):\n    pred_values = tf.cast(predictions['predictions'],tf.float64)\n    return {'rmse': tf.metrics.root_mean_squared_error(labels, pred_values)}\nestimator = tf.contrib.estimator.add_metrics(estimator,rmse)\ntrain_spec=tf.estimator.TrainSpec(\n                                input_fn = train_input_fn,\n                                max_steps = num_train_steps)\neval_spec=tf.estimator.EvalSpec(\n                   input_fn = eval_input_fn,\n                   steps = None,\n                   start_delay_secs = 1, # start evaluating after N seconds\n                   throttle_secs = 10,  # evaluate every N seconds\n                   )\ntf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"64c2bd02573475367de3fb5af796b3f05a62d8dd"},"cell_type":"code","source":"predictions = estimator.predict(input_fn= predict_input_fn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"39a19f0ac850629f54ad78eeea57ff819b90ce9a"},"cell_type":"code","source":"predlist = list(predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d2537399cd652e0bd7e8e4248453e105385cda03"},"cell_type":"code","source":"predlist[0].get('predictions')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"659cb131d3b2a9356158424cdd10fc86c17dd5f1"},"cell_type":"code","source":"predval = [predlist[i].get('predictions') for i in range(len(predlist))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8e799551bd7a3164b0ff61e08011a89b8d4b424e"},"cell_type":"code","source":"pconc = np.concatenate(predval)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e5c76b59770d764b00f8f9017e2339f57da8cd95"},"cell_type":"code","source":"pconc.reshape(-1,1) #made it!","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2d80ab038e02d1f6dbb08b86da1380ed8752dacf"},"cell_type":"code","source":"test['key'].values.reshape(-1,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d404ff6a105c1e7401cefea08b5f570c8c966442"},"cell_type":"code","source":"output = np.hstack((test['key'].values.reshape(-1,1),pconc.reshape(-1,1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"59685bcc3d10fafad6bb4d5145ae52092a923813"},"cell_type":"code","source":"output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f5d97c0c557c5d28372d9bd044e4946e093647b5"},"cell_type":"code","source":"dataset_output = pd.DataFrame({'key':output[:,0],'fare_amount':output[:,1]})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ecac71cd228c36f46d43f3a5f7ce6c908275b5df"},"cell_type":"code","source":"dataset_output.to_csv('submission_file.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9f20763d094b72150f5c3aed311ee08293ad2164"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}