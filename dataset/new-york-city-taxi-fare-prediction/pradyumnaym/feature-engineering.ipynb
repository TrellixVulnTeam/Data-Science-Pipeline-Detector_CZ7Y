{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#!pip install modin[ray] #parallelized pandas..\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport pandas as pd \nimport dask.dataframe as dd\nimport os\nfrom tqdm import tqdm\nimport gc\nimport holidays\nimport matplotlib.pyplot as plt\nimport datetime as dt\n\n#check the available input data\nimport os\nfor dirname, _, filenames in os.walk(\"/kaggle\"):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As the dataset size is very large, we need to read and process it in chunks. We chose a chunk size of 10^6 rows. All of the below features have been carefully chosen after \nexamining the data. Please refer to the cells below and in the Vizualization notebook for more information."},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"markdown","source":"\ndf_list = [] # list to hold the batch dataframe\nchunksize = 1000000  #load the data in chunks of chunksize\n#datatypes of the columns. using float32 to reduce memory needed\ntraintypes = {'fare_amount': 'float32',\n              'pickup_datetime': 'str', \n              'pickup_longitude': 'float32',\n              'pickup_latitude': 'float32',\n              'dropoff_longitude': 'float32',\n              'dropoff_latitude': 'float32',\n              'passenger_count': 'uint8'}\n\nTRAIN_PATH = '../input/new-york-city-taxi-fare-prediction/train.csv'\n\n#haversine distance to calculate ditance between two points\ndef haversine_distance(lat1, long1, lat2, long2):\n    R = 6371  #radius of earth in kilometers\n    phi1 = np.radians(lat1)\n    phi2 = np.radians(lat2)\n\n    delta_phi = np.radians(lat2-lat1)\n    delta_lambda = np.radians(long2-long1)\n\n    #a = sin²((φB - φA)/2) + cos φA . cos φB . sin²((λB - λA)/2)\n    a = np.sin(delta_phi / 2.0) ** 2 + np.cos(phi1) * np.cos(phi2) * np.sin(delta_lambda / 2.0) ** 2\n\n    #c = 2 * atan2( √a, √(1−a) )\n    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n\n    #d = R*c\n    d = (R * c) #in kilometers\n\n    return d\nfor df_chunk in tqdm(pd.read_csv(TRAIN_PATH, usecols=cols, dtype=traintypes, chunksize=chunksize)):\n     \n    # Neat trick from https://www.kaggle.com/btyuhas/bayesian-optimization-with-xgboost\n    # Using parse_dates would be much slower!\n    df_chunk['pickup_datetime'] = df_chunk['pickup_datetime'].str.slice(0, 16)\n    df_chunk['pickup_datetime'] = pd.to_datetime(df_chunk['pickup_datetime'], utc=True, format='%Y-%m-%d %H:%M')\n    \n    # Can process each chunk of dataframe here\n    # clean_data(), feature_engineer(),fit()\n    #print(\"Before: \",len(df_chunk))\n    df_chunk.dropna()\n    #remove the rows that have coordinates outside the bounding box of the city and its nearby areas.\n    mask = df_chunk['pickup_longitude'].between(-75, -73)\n    mask &= df_chunk['dropoff_longitude'].between(-75, -73)\n    mask &= df_chunk['pickup_latitude'].between(40, 42)\n    mask &= df_chunk['dropoff_latitude'].between(40, 42)\n    #remove the rows that have wrong number of passengers(negative or more than 8 passsengers)\n    mask &= df_chunk['passenger_count'].between(0, 8)\n    #remove rows with wrong fares(negative fares and grater than 250 USD..) and rows with fare amount = 0\n    mask &= df_chunk['fare_amount'].between(0, 250)\n    mask &= df_chunk['fare_amount'].gt(0)\n    \n    #apply this mask, which will remove all the inconsistent rows\n    df_chunk = df_chunk[mask]\n    #print(\"After: \",len(df_chunk))\n    df_chunk = df_chunk.reset_index()  #make it featherable again. masking messes with the index. reset index helps remove this problem.\n    mask = 0\n    #recover memory!\n    gc.collect()\n    #add a new attribute to the dataset, the haversine distance between pickup and dropodd coordinates\n    df_chunk[\"distance\"] = pd.to_numeric(haversine_distance(df_chunk['pickup_latitude'], df_chunk['pickup_longitude'], df_chunk['dropoff_latitude'], df_chunk['dropoff_longitude']), downcast = 'float')\n    gc.collect()\n    #add a new column called time, which is the number of minutes since 12:00 am that day\n    df_chunk[\"time\"] = pd.to_numeric(df_chunk.apply(lambda r: r.pickup_datetime.hour*60 + r.pickup_datetime.minute, axis = 1), downcast = \"unsigned\")\n    gc.collect()\n    #print(\"time\")\n    \n    #get a list of all US holidays\n    us_holidays = holidays.US()\n    #add a new column to the dataset, which is 1 if the row is on a holiday, or else, 0.\n    df_chunk[\"holiday\"] = pd.to_numeric(df_chunk.apply(lambda x: 1 if x.pickup_datetime.strftime('%d-%m-%y')in us_holidays else 0, axis =1), downcast = \"unsigned\")\n    gc.collect()\n    #add a column named year to the dataset\n    df_chunk[\"year\"] = df_chunk[\"pickup_datetime\"].dt.year\n    #add the weekday attribute to the dataset.\n    df_chunk[\"weekday\"] = pd.to_numeric(df_chunk[\"pickup_datetime\"].dt.weekday, downcast = \"unsigned\")\n    gc.collect()\n    # append the chunk to list and merge all\n    df_list.append(df_chunk)\n\n#concatenate all the cunks to make the whole dataset into a single data frame\ntrain_df = pd.concat(df_list)\n#remove rows with distance = 0\nmask = train_df['distance'].gt(0)\ntrain_df = train_df[mask]\n# Delete the dataframe list to release memory\ndel df_list\ngc.collect()\n# See what we have loaded\ntrain_df.info()\n#reset index, as mask messes with the index, which prevents it from being serializable\ntrain_df = train_df.reset_index()\ntrain_df.index = range(len(train_df.fare_amount))\n#save to output.\ntrain_df.to_feather('nyc_taxi_data_raw.feather')"},{"metadata":{"trusted":true},"cell_type":"code","source":"'''df_list = [] # list to hold the batch dataframe\nchunksize = 1000000\ntrain_df = pd.read_feather('../input/kernel318ff03a29/nyc_taxi_data_raw.feather')\ngc.collect()'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''#as we see, all the NaN values have been removed successfully.\ntrain_df.isna().sum()'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''#the summary statistics\ntrain_df.describe()'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''train_df.head()'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''train_df = 0\ngc.collect()\n#read only the 2 required columns from the dataset\ntrain_df = pd.read_feather('../input/kernel318ff03a29/nyc_taxi_data_raw.feather', columns = ['holiday', 'fare_amount'])\n#selcet all the rows on public holidays in one dataframe\nholiday_df = train_df[train_df['holiday']==1]\n#select all the rows from normla days in another dataframe\nnot_holiday_df = train_df[train_df['holiday']==0]\ntrain_df = 0\ngc.collect()\n\n#find the mean fare amount for both of these sets\nholiday_average = np.mean(holiday_df['fare_amount'])\nnot_holiday_average = np.mean(not_holiday_df['fare_amount'])'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''print(holiday_average, not_holiday_average)'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''#as we can see, the fare amounts are noticably higher on public holidays than regular days. So, the Holiday attribute is useful...\nlabel  = [\"Holiday\", \"Not Holiday\"]\nx = [holiday_average, not_holiday_average]\ndef plot_bar_x():\n    # this is for plotting purpose\n    index = np.arange(len(label))\n    plt.figure(figsize = (10,10))\n    plt.bar(index, x)\n    plt.xlabel('Holiday')\n    plt.ylabel('Average Fare Amount')\n    plt.xticks(index, label, rotation=0)\n    plt.title('Average fare amounts on holidays and normal days')\n    plt.show()\nplot_bar_x()'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''holiday_df = 0\nnot_holiday_df = 0\ngc.collect()'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''train_df = pd.read_feather('../input/kernel318ff03a29/nyc_taxi_data_raw.feather', columns = ['distance', 'fare_amount'])\nsample_df = train_df.sample(n = 5000)\nplt.figure(figsize = (10,10))\nplt.scatter(sample_df[\"distance\"], sample_df[\"fare_amount\"])\nplt.title('Scatter plot of distance vs fare_amount')\nplt.xlabel('distance')\nplt.ylabel('fare_amount')\nplt.show()\nprint(len(train_df[train_df['distance']==0]))\ntrain_df = 0\ngc.collect()'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def haversine_distance(lat1, long1, lat2, long2):\n    R = 6371  #radius of earth in kilometers\n    phi1 = np.radians(lat1)\n    phi2 = np.radians(lat2)\n    delta_phi = np.radians(lat2-lat1)\n    delta_lambda = np.radians(long2-long1)\n    #a = sin²((φB - φA)/2) + cos φA . cos φB . sin²((λB - λA)/2)\n    a = np.sin(delta_phi / 2.0) ** 2 + np.cos(phi1) * np.cos(phi2) * np.sin(delta_lambda / 2.0) ** 2\n    #c = 2 * atan2( √a, √(1−a) )\n    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n    #d = R*c\n    d = (R * c) #in kilometers\n    return d\n\ndf_chunk = pd.read_csv('/kaggle/input/new-york-city-taxi-fare-prediction/test.csv')\ndf_chunk['pickup_datetime'] = df_chunk['pickup_datetime'].str.slice(0, 16)\ndf_chunk['pickup_datetime'] = pd.to_datetime(df_chunk['pickup_datetime'], utc=True, format='%Y-%m-%d %H:%M')\n\n'''df_chunk.dropna()\ndf_chunk = df_chunk.drop(df_chunk[df_chunk['passenger_count']==208].index, axis = 0)\n#remove the rows that have coordinates outside the bounding box of the city and its nearby areas.\nmask = df_chunk['pickup_longitude'].between(-75, -73)\nmask &= df_chunk['dropoff_longitude'].between(-75, -73)\nmask &= df_chunk['pickup_latitude'].between(40, 42)\nmask &= df_chunk['dropoff_latitude'].between(40, 42)\n#remove the rows that have wrong number of passengers(negative or more than 8 passsengers)\nmask &= df_chunk['passenger_count'].between(0, 8)\n#remove rows with wrong fares(negative fares and grater than 250 USD..)\ndf_chunk = df_chunk[mask]\n#print(\"After: \",len(df_chunk))\ndf_chunk = df_chunk.reset_index()  #make it featherable again.\nmask = 0'''\n\ngc.collect()\ndf_chunk[\"time\"] = pd.to_numeric(df_chunk.apply(lambda r: r.pickup_datetime.hour*60 + r.pickup_datetime.minute, axis = 1), downcast = \"unsigned\")\ngc.collect()\n#print(\"time\")\nus_holidays = holidays.US()\ndf_chunk[\"holiday\"] = pd.to_numeric(df_chunk.apply(lambda x: 1 if x.pickup_datetime.strftime('%d-%m-%y')in us_holidays else 0, axis =1), downcast = \"unsigned\")\ngc.collect()\n\nManhattan = (-73.9712,40.7831)[::-1]\nJFK_airport = (-73.7781,40.6413)[::-1]\nLaguardia_airport = (-73.8740,40.7769)[::-1]\ndf_chunk[\"distance\"] = pd.to_numeric(haversine_distance(df_chunk['pickup_latitude'], df_chunk['pickup_longitude'], df_chunk['dropoff_latitude'], df_chunk['dropoff_longitude']), downcast = 'float')\ndf_chunk[\"year\"] = df_chunk[\"pickup_datetime\"].dt.year\ndf_chunk[\"weekday\"] = pd.to_numeric(df_chunk[\"pickup_datetime\"].dt.weekday, downcast= \"unsigned\")\ndf_chunk['pickup_distance_Mtn'] = pd.to_numeric(haversine_distance(Manhattan[0],Manhattan[1],df_chunk['pickup_latitude'],df_chunk['pickup_longitude']), downcast = 'float')\ndf_chunk['dropoff_distance_Mtn'] = pd.to_numeric(haversine_distance(Manhattan[0],Manhattan[1],df_chunk['dropoff_latitude'],df_chunk['dropoff_longitude']), downcast = 'float')\ndf_chunk['dropoff_distance_jfk'] = pd.to_numeric(haversine_distance(JFK_airport[0],JFK_airport[1],df_chunk['dropoff_latitude'],df_chunk['dropoff_longitude']), downcast = 'float')\ndf_chunk['pickup_distance_jfk'] = pd.to_numeric(haversine_distance(JFK_airport[0],JFK_airport[1],df_chunk['pickup_latitude'],df_chunk['pickup_longitude']), downcast = 'float')\ndf_chunk['pickup_distance_lg'] = pd.to_numeric(haversine_distance(Laguardia_airport[0],Laguardia_airport[1],df_chunk['dropoff_latitude'],df_chunk['dropoff_longitude']), downcast = 'float')\ndf_chunk['dropoff_distance_lg'] = pd.to_numeric(haversine_distance(Laguardia_airport[0],Laguardia_airport[1],df_chunk['pickup_latitude'],df_chunk['pickup_longitude']), downcast = 'float')\n\nprint(\"before\", len(df_chunk))\nprint(\"after\", len(df_chunk))\nprint(df_chunk.head())\ndf_chunk.index = range(len(df_chunk.pickup_datetime)) #fix thr index\nprint(\"after\", len(df_chunk))\ndf_chunk.to_feather('test_feature.feather')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''df_chunk.head()'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''train_df = pd.read_feather('../input/kernel318ff03a29/nyc_taxi_data_raw.feather')\n#mask = train_df['fare_amount'].gt(0)\n#train_df = train_df[mask]\n#train_df.reset_index(inplace = True)\ntrain_df[\"weekday\"] = pd.to_numeric(train_df[\"weekday\"], downcast = \"unsigned\")\ntrain_df.to_feather('nyc_taxi_data_raw.feather')'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_feather('../input/kernel318ff03a29/nyc_taxi_data_raw.feather')\nManhattan = (-73.9712,40.7831)[::-1]\nJFK_airport = (-73.7781,40.6413)[::-1]\nLaguardia_airport = (-73.8740,40.7769)[::-1]\ndef haversine_distance(lat1, long1, lat2, long2):\n    R = 6371  #radius of earth in kilometers\n    phi1 = np.radians(lat1)\n    phi2 = np.radians(lat2)\n    delta_phi = np.radians(lat2-lat1)\n    delta_lambda = np.radians(long2-long1)\n    #a = sin²((φB - φA)/2) + cos φA . cos φB . sin²((λB - λA)/2)\n    a = np.sin(delta_phi / 2.0) ** 2 + np.cos(phi1) * np.cos(phi2) * np.sin(delta_lambda / 2.0) ** 2\n    #c = 2 * atan2( √a, √(1−a) )\n    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n    #d = R*c\n    d = (R * c) #in kilometers\n    return d\ntrain_df['pickup_distance_Mtn'] = pd.to_numeric(haversine_distance(Manhattan[0],Manhattan[1],train_df['pickup_latitude'],train_df['pickup_longitude']), downcast = 'float')\ntrain_df['dropoff_distance_Mtn'] = pd.to_numeric(haversine_distance(Manhattan[0],Manhattan[1],train_df['dropoff_latitude'],train_df['dropoff_longitude']), downcast = 'float')\ntrain_df['dropoff_distance_jfk'] = pd.to_numeric(haversine_distance(JFK_airport[0],JFK_airport[1],train_df['dropoff_latitude'],train_df['dropoff_longitude']), downcast = 'float')\ntrain_df['pickup_distance_jfk'] = pd.to_numeric(haversine_distance(JFK_airport[0],JFK_airport[1],train_df['pickup_latitude'],train_df['pickup_longitude']), downcast = 'float')\ntrain_df['pickup_distance_lg'] = pd.to_numeric(haversine_distance(Laguardia_airport[0],Laguardia_airport[1],train_df['dropoff_latitude'],train_df['dropoff_longitude']), downcast = 'float')\ntrain_df['dropoff_distance_lg'] = pd.to_numeric(haversine_distance(Laguardia_airport[0],Laguardia_airport[1],train_df['pickup_latitude'],train_df['pickup_longitude']), downcast = 'float')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()\ntrain_df.to_feather('nyc_taxi_data_raw.feather')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}