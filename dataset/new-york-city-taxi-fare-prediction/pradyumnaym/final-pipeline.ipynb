{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.|\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!pip install holidays\n\n#package imports\nimport pandas as pd\nfrom sklearn import preprocessing\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error\nfrom bayes_opt import BayesianOptimization\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\n\n\nimport gc\nimport os\nfrom tqdm import tqdm\nimport holidays\nimport datetime as dt\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"us_holidays = holidays.US()\ndef haversine_distance(lat1, long1, lat2, long2):\n    R = 6371  #radius of earth in kilometers\n    phi1 = np.radians(lat1)\n    phi2 = np.radians(lat2)\n    delta_phi = np.radians(lat2-lat1)\n    delta_lambda = np.radians(long2-long1)\n    #a = sin²((φB - φA)/2) + cos φA . cos φB . sin²((λB - λA)/2)\n    a = np.sin(delta_phi / 2.0) ** 2 + np.cos(phi1) * np.cos(phi2) * np.sin(delta_lambda / 2.0) ** 2\n    #c = 2 * atan2( √a, √(1−a) )\n    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n    #d = R*c\n    d = (R * c) #in kilometers\n    return d\n\ndef readData(nrows = 15000000):\n    \n    #kaggle kernels: https://www.kaggle.com/pradyu99914/data-feature-engineering?scriptVersionId=21782913\n    #feature engineering: https://www.kaggle.com/anushkini/nyc-taxi-fare-graphs\n    \n    try:\n        test_df = pd.read_feather(\"/kaggle/input/final-pipeline/test_df.feather\")\n        df_chunk = pd.read_feather(\"/kaggle/input/final-pipeline/df_chunk.feather\")\n    except Exception:\n        #read the test and train sets\n        gc.collect()\n        df_chunk = pd.read_csv('../input/new-york-city-taxi-fare-prediction/train.csv', nrows = 15_000_000)\n        test_df = pd.read_feather('../input/data-feature-engineering/test_feature.feather')\n        gc.collect()\n\n\n        df_chunk['pickup_datetime'] = df_chunk['pickup_datetime'].str.slice(0, 16)\n        df_chunk['pickup_datetime'] = pd.to_datetime(df_chunk['pickup_datetime'], utc=True, format='%Y-%m-%d %H:%M')\n        df_chunk.dropna()\n        #remove the rows that have coordinates outside the bounding box of the city and its nearby areas.\n        mask = df_chunk['pickup_longitude'].between(-75, -73)\n        mask &= df_chunk['dropoff_longitude'].between(-75, -73)\n        mask &= df_chunk['pickup_latitude'].between(40, 42)\n        mask &= df_chunk['dropoff_latitude'].between(40, 42)\n        #remove the rows that have wrong number of passengers(negative or more than 8 passsengers)\n        mask &= df_chunk['passenger_count'].between(0, 8)\n        #remove rows with wrong fares(negative fares and grater than 250 USD..) and rows with fare amount = 0\n        mask &= df_chunk['fare_amount'].between(0, 250)\n        mask &= df_chunk['fare_amount'].gt(0)\n\n        #apply this mask, which will remove all the inconsistent rows\n        df_chunk = df_chunk[mask]\n        #print(\"After: \",len(df_chunk))\n        df_chunk = df_chunk.reset_index()  #make it featherable again. masking messes with the index. reset index helps remove this problem.\n        mask = 0\n        #recover memory!\n        gc.collect()\n        \n        #add time and holiday features\n        df_chunk[\"time\"] = pd.to_numeric(df_chunk.apply(lambda r: r.pickup_datetime.hour*60 + r.pickup_datetime.minute, axis = 1), downcast = \"unsigned\")\n        gc.collect()\n        df_chunk[\"holiday\"] = pd.to_numeric(df_chunk.apply(lambda x: 1 if x.pickup_datetime.strftime('%d-%m-%y')in us_holidays else 0, axis =1), downcast = \"unsigned\")\n        gc.collect()\n        \n        #coordinates for important places in the city\n        Manhattan = (-73.9712,40.7831)[::-1]\n        JFK_airport = (-73.7781,40.6413)[::-1]\n        Laguardia_airport = (-73.8740,40.7769)[::-1]\n        statue_of_liberty = (-74.0445,40.6892)[::-1]\n        central_park = (-73.9654,40.7829)[::-1]\n        time_square = (-73.9855,40.7580)[::-1]\n        brooklyn_bridge = (-73.9969,40.7061)[::-1]\n        rockerfeller = (-73.9787,40.7587)[::-1]\n\n        #more features\n        df_chunk[\"distance\"] = pd.to_numeric(haversine_distance(df_chunk['pickup_latitude'], df_chunk['pickup_longitude'], df_chunk['dropoff_latitude'], df_chunk['dropoff_longitude']), downcast = 'float')\n        df_chunk[\"year\"] = df_chunk[\"pickup_datetime\"].dt.year\n        df_chunk[\"weekday\"] = pd.to_numeric(df_chunk[\"pickup_datetime\"].dt.weekday, downcast= \"unsigned\")\n\n        #distance from tourist spots\n        df_chunk['pickup_distance_Mtn'] = pd.to_numeric(haversine_distance(Manhattan[0],Manhattan[1],df_chunk['pickup_latitude'],df_chunk['pickup_longitude']), downcast = 'float')\n        df_chunk['dropoff_distance_Mtn'] = pd.to_numeric(haversine_distance(Manhattan[0],Manhattan[1],df_chunk['dropoff_latitude'],df_chunk['dropoff_longitude']), downcast = 'float')\n        df_chunk['dropoff_distance_jfk'] = pd.to_numeric(haversine_distance(JFK_airport[0],JFK_airport[1],df_chunk['dropoff_latitude'],df_chunk['dropoff_longitude']), downcast = 'float')\n        df_chunk['pickup_distance_jfk'] = pd.to_numeric(haversine_distance(JFK_airport[0],JFK_airport[1],df_chunk['pickup_latitude'],df_chunk['pickup_longitude']), downcast = 'float')\n        df_chunk['pickup_distance_lg'] = pd.to_numeric(haversine_distance(Laguardia_airport[0],Laguardia_airport[1],df_chunk['dropoff_latitude'],df_chunk['dropoff_longitude']), downcast = 'float')\n        df_chunk['dropoff_distance_lg'] = pd.to_numeric(haversine_distance(Laguardia_airport[0],Laguardia_airport[1],df_chunk['pickup_latitude'],df_chunk['pickup_longitude']), downcast = 'float')\n\n        #add the date and month features.\n        df_chunk['day'] = df_chunk['pickup_datetime'].dt.day\n        df_chunk['month'] = df_chunk['pickup_datetime'].dt.month\n\n        test_df['day'] = test_df['pickup_datetime'].dt.day\n        test_df['month'] = test_df['pickup_datetime'].dt.month\n\n        #add more distances from tourist spots\n        df_chunk['pickup_distance_sol'] = pd.to_numeric(haversine_distance(statue_of_liberty[0],statue_of_liberty[1],df_chunk['pickup_latitude'],df_chunk['pickup_longitude']), downcast = 'float')\n        df_chunk['dropoff_distance_sol'] = pd.to_numeric(haversine_distance(statue_of_liberty[0],statue_of_liberty[1],df_chunk['dropoff_latitude'],df_chunk['dropoff_longitude']), downcast = 'float')\n        df_chunk['pickup_distance_cp'] = pd.to_numeric(haversine_distance(central_park[0],central_park[1],df_chunk['pickup_latitude'],df_chunk['pickup_longitude']), downcast = 'float')\n        df_chunk['dropoff_distance_cp'] = pd.to_numeric(haversine_distance(central_park[0],central_park[1],df_chunk['dropoff_latitude'],df_chunk['dropoff_longitude']), downcast = 'float')\n        df_chunk['pickup_distance_ts'] = pd.to_numeric(haversine_distance(time_square[0],time_square[1],df_chunk['pickup_latitude'],df_chunk['pickup_longitude']), downcast = 'float')\n        df_chunk['dropoff_distance_ts'] = pd.to_numeric(haversine_distance(time_square[0],time_square[1],df_chunk['dropoff_latitude'],df_chunk['dropoff_longitude']), downcast = 'float')\n        df_chunk['pickup_distance_bb'] = pd.to_numeric(haversine_distance(brooklyn_bridge[0],brooklyn_bridge[1],df_chunk['pickup_latitude'],df_chunk['pickup_longitude']), downcast = 'float')\n        df_chunk['dropoff_distance_bb'] = pd.to_numeric(haversine_distance(brooklyn_bridge[0],brooklyn_bridge[1],df_chunk['dropoff_latitude'],df_chunk['dropoff_longitude']), downcast = 'float')\n        df_chunk['pickup_distance_r'] = pd.to_numeric(haversine_distance(rockerfeller[0],rockerfeller[1],df_chunk['pickup_latitude'],df_chunk['pickup_longitude']), downcast = 'float')\n        df_chunk['dropoff_distance_r'] = pd.to_numeric(haversine_distance(rockerfeller[0],rockerfeller[1],df_chunk['dropoff_latitude'],df_chunk['dropoff_longitude']), downcast = 'float')\n\n        test_df['pickup_distance_sol'] = pd.to_numeric(haversine_distance(statue_of_liberty[0],statue_of_liberty[1],test_df['pickup_latitude'],test_df['pickup_longitude']), downcast = 'float')\n        test_df['dropoff_distance_sol'] = pd.to_numeric(haversine_distance(statue_of_liberty[0],statue_of_liberty[1],test_df['dropoff_latitude'],test_df['dropoff_longitude']), downcast = 'float')\n        test_df['pickup_distance_cp'] = pd.to_numeric(haversine_distance(central_park[0],central_park[1],test_df['pickup_latitude'],test_df['pickup_longitude']), downcast = 'float')\n        test_df['dropoff_distance_cp'] = pd.to_numeric(haversine_distance(central_park[0],central_park[1],test_df['dropoff_latitude'],test_df['dropoff_longitude']), downcast = 'float')\n        test_df['pickup_distance_ts'] = pd.to_numeric(haversine_distance(time_square[0],time_square[1],test_df['pickup_latitude'],test_df['pickup_longitude']), downcast = 'float')\n        test_df['dropoff_distance_ts'] = pd.to_numeric(haversine_distance(time_square[0],time_square[1],test_df['dropoff_latitude'],test_df['dropoff_longitude']), downcast = 'float')\n        test_df['pickup_distance_bb'] = pd.to_numeric(haversine_distance(brooklyn_bridge[0],brooklyn_bridge[1],test_df['pickup_latitude'],test_df['pickup_longitude']), downcast = 'float')\n        test_df['dropoff_distance_bb'] = pd.to_numeric(haversine_distance(brooklyn_bridge[0],brooklyn_bridge[1],test_df['dropoff_latitude'],test_df['dropoff_longitude']), downcast = 'float')\n        test_df['pickup_distance_r'] = pd.to_numeric(haversine_distance(rockerfeller[0],rockerfeller[1],test_df['pickup_latitude'],test_df['pickup_longitude']), downcast = 'float')\n        test_df['dropoff_distance_r'] = pd.to_numeric(haversine_distance(rockerfeller[0],rockerfeller[1],test_df['dropoff_latitude'],test_df['dropoff_longitude']), downcast = 'float')\n\n        df_chunk['pickup_longitude'] = np.radians(df_chunk['pickup_longitude'])\n        df_chunk['pickup_latitude'] = np.radians(df_chunk['pickup_latitude'])\n        df_chunk['dropoff_latitude'] = np.radians(df_chunk['dropoff_latitude'])\n        df_chunk['dropoff_longitude'] = np.radians(df_chunk['dropoff_longitude'])\n\n        test_df['pickup_longitude'] = np.radians(test_df['pickup_longitude'])\n        test_df['pickup_latitude'] = np.radians(test_df['pickup_latitude'])\n        test_df['dropoff_latitude'] = np.radians(test_df['dropoff_latitude'])\n        test_df['dropoff_longitude'] = np.radians(test_df['dropoff_longitude'])\n        \n        \n    #write this back so that it will be availablwe after the next commit\n    test_df.to_feather(\"test_df.feather\")\n    df_chunk.to_feather(\"df_chunk.feather\")\n    y = df_chunk['fare_amount']\n    \n    #drop the unwanted columns\n    df_chunk = df_chunk.drop(['key','pickup_datetime','fare_amount'],axis = 1)\n    X_train,X_val,y_train,y_val = train_test_split(df_chunk,y,test_size = 0.1)\n    del X_train['index']\n    test_df = test_df[X_train.columns]\n    del(df_chunk)\n    del(y)\n    gc.collect()\n\n    if nrows!=15000000: #reading lesser number of rows\n        X_train = X_train[:nrows]\n        y_train = y_train[:nrows]\n        gc.collect()\n\n    return X_train, y_train, X_val, y_val, test_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# transform the data into lgbm-compatible form\nX_train, y_train, X_val, y_val, test_df = readData(1500000) #number of data points to read, please make sure this is atleast 50k as some models reserve some data for grid search","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check if the model is already present, if not train it again.\ndef getLGB():\n    #kaggle kernel - https://www.kaggle.com/anushkini/taxi-lightgbm?scriptVersionId=23609067\n    try:\n        #try to read the model\n        model = lgb.Booster(model_file = \"/kaggle/input/trained-model/model.txt\" )\n    except Exception:\n        #if the trained model is not present, train it again\n        lgbm_params =  {\n            'task': 'train',\n            'boosting_type': 'gbdt',\n            'objective': 'regression',\n            'metric': 'rmse',\n            'nthread': 4,\n            'learning_rate': 0.05,\n            'bagging_fraction': 1,\n            'num_rounds':50000\n            }\n        model = lgb.train(lgbm_params, train_set = dtrain, num_boost_round=10000,early_stopping_rounds=500,verbose_eval=500, valid_sets=dval)\n        del(X_train)\n        del(y_train)\n        del(X_val)\n        del(y_val)\n        gc.collect()\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def getkNNpredictions(X_train, y_train, X_test):\n    #kaggle kernel - https://www.kaggle.com/pradyu99914/nyc-taxi-fare-models-knn?scriptVersionId=22570753\n    from sklearn.neighbors import KNeighborsRegressor\n    #this will store the predictions for each of the knn regressors\n    knnregressoroutputs = []\n    #go through chunks of 1M\n    for i in tqdm(range(len(X_train)//1000000)):\n        neigh = KNeighborsRegressor(n_neighbors=2)\n        #extract the required sample of the data\n        X = X_train.iloc[i*10**6:(i+1)*10**6, :]\n        #target variable\n        y = y_train[i*10**6:(i+1)*10**6]\n        neigh.fit(X,y)\n        #take the predictions\n        y_test = neigh.predict(X_test)\n        #save the predictions\n        knnregressoroutputs.append(y_test)\n        neigh = 0\n        gc.collect()\n        \n    #average all the predictions\n    res = knnregressoroutputs[0]\n    for i in knnregressoroutputs[1:]:\n        res+=i\n    res/=len(knnregressoroutputs)\n    return res\n\ndef getLassoPredictions(X, y, X_test):\n    #kaggle kernel - https://www.kaggle.com/pradyu99914/nyc-taxi-fare-models-latest\n    from sklearn import linear_model\n    from sklearn.metrics import mean_squared_error\n    from math import sqrt\n    from tqdm import tqdm\n    import matplotlib.pyplot as plt\n    \n    test_df1 = X.iloc[-10000:,:]\n    y_test_actual = y.iloc[-10000:]\n    \n    X=X.iloc[:len(X)-10000,:]\n    y = y.iloc[:len(y)-10000]\n    \n    #variables needed for grid search\n    minrms = float('inf')\n    minrmsalpha = -1\n    rmserrs = []\n    miny = pd.DataFrame()\n    #values of alpha\n    for i in tqdm(range(0, 5)):\n        gc.collect()\n        model = linear_model.Lasso(normalize = True, alpha = 10**(-i))\n        gc.collect()\n        model.fit(X,y)\n        y_test = model.predict(X_test)\n        \n        y_test1 = model.predict(test_df1)\n        rms = sqrt(mean_squared_error(y_test_actual, y_test1))\n        rmserrs.append(rms)\n        del model\n        if rms<minrms:\n            minrms = rms\n            minrmsalpha = i\n            miny = y_test\n    plt.plot(range(0,5),rmserrs)\n    plt.title(\"Grid search for lasso regression\")\n    plt.xlabel(\"alpha (10^-x)\")\n    plt.ylabel(\"RMSE\")\n    return miny\n\ndef getRidgePredictions(X, y, X_test):\n    #kaggle kernel - https://www.kaggle.com/pradyu99914/nyc-taxi-fare-models-latest\n    from sklearn import linear_model\n    from sklearn.metrics import mean_squared_error\n    from math import sqrt\n    from tqdm import tqdm\n    import matplotlib.pyplot as plt\n    \n    test_df1 = X.iloc[-10000:,:]\n    y_test_actual = y.iloc[-10000:]\n    \n    X=X.iloc[:len(X)-10000,:]\n    y = y.iloc[:len(y)-10000]\n        \n    #variables needed for grid search\n    minrms = float('inf')\n    minrmsalpha = -1\n    rmserrs = []\n    miny = pd.DataFrame()\n    #values of alpha\n    for i in tqdm(range(0, 5)):\n        gc.collect()\n        model = linear_model.Ridge(normalize = True, alpha = 10**(-i))\n        gc.collect()\n        model.fit(X,y)\n        y_test = model.predict(X_test)\n\n        y_test1 = model.predict(test_df1)\n        rms = sqrt(mean_squared_error(y_test_actual, y_test1))\n        rmserrs.append(rms)\n        del model\n        if rms<minrms:\n            minrms = rms\n            minrmsalpha = i\n            miny = y_test\n    plt.plot(range(0,5),rmserrs)\n    plt.title(\"Grid search for ridge regression\")\n    plt.xlabel(\"alpha (10^-x)\")\n    plt.ylabel(\"RMSE\")\n    return miny\n\ndef getLRPredictions(X, y, X_test):\n    #kaggle kernel - https://www.kaggle.com/pradyu99914/nyc-taxi-fare-models-latest\n    from sklearn import linear_model\n    #Note: this library uses the closed form expression for the parameters and not gradient descent\n    model = linear_model.LinearRegression()\n    model.fit(X,y)\n    y_test = model.predict(X_test)\n    return y_test\n\ndef getRFPredictions(X, y, X_test):\n    #kaggle kernel: https://www.kaggle.com/pradyu99914/nyc-taxi-fare-models-latest\n    from sklearn.ensemble import RandomForestRegressor\n    model = RandomForestRegressor(max_depth=4, random_state=0, n_estimators=100)\n    gc.collect()\n    model.fit(X,y)\n    y_test = model.predict(X_test)\n    return y_test\n\ndef getLGBMPredictions(X_train, y_train, X_val, y_val, test_df):\n    #get the model\n    dtrain = lgb.Dataset(X_train,y_train,silent=False,categorical_feature=['year','month','day','weekday'])\n    dval = lgb.Dataset(X_val,y_val,silent=False,categorical_feature=['year','month','day','weekday'])\n    model = getLGB()\n    pred = model.predict(test_df)\n    feature_imp = pd.DataFrame({'Value':model.feature_importance(),'Feature':test_df.columns})\n\n    #plt.figure(figsize=(20, 10))\n    #sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\n    #plt.title('LightGBM Features (avg over folds)')\n    #plt.xlabel(\"Split Gain\")\n    #plt.tight_layout()\n    #plt.show()\n    return pred\n\ndef getDNNPredictions(X, y, test_df):\n    \n    #kaggle kernel - https://www.kaggle.com/pradyu99914/fork-of-fork-of-nyc-taxi-fare-models-dl-model?scriptVersionId=23712985\n    from keras.models import Sequential\n    from keras.layers import Dense,Dropout\n    from keras.models import load_model\n    try:\n        #try to read the model if it is already present\n        model = load_model('/kaggle/input/final-pipeline/model.h5')\n    except Exception:\n        #create and train a new model in case it is not already pesent\n        model = Sequential()\n        model.add(Dense(2048, input_dim = 28, activation = 'relu'))\n        model.add(Dropout(0.2))\n        model.add(Dense(1024, activation = 'relu'))\n        model.add(Dropout(0.2))\n        model.add(Dense(512, activation = 'relu'))\n        model.add(Dropout(0.2))\n        model.add(Dense(256,  activation = 'tanh'))\n        model.add(Dropout(0.2))\n        model.add(Dense(128,  activation = 'tanh'))\n        model.add(Dense(1, activation = \"linear\"))\n        model.compile(loss='mse', optimizer='adam', metrics=['mse','mae'])\n        history = model.fit(X,y, batch_size=2048, epochs = 30)\n        import matplotlib.pyplot as plt\n        plt.plot(history.history['loss'])\n        plt.title('model loss')\n        plt.ylabel('loss')\n        plt.xlabel('epoch')\n        plt.show()\n    #save the model for future commits\n    model.save(\"model.h5\")\n    y_test = model.predict(test_df)\n    return y_test.reshape(len(test_df))\n\ndef getXGBpredictions(X_train, y_train, test_df):\n    '''import xgboost as xgb\n    from bayes_opt import BayesianOptimization\n    from sklearn import preprocessing\n    from sklearn.metrics import mean_squared_error\n    from sklearn.model_selection import train_test_split\n    import joblib\n    #save model\n    try:\n        model = joblib.load('/kaggle/input/final-pipeline/xgb.pkl')\n    except Exception:\n        dtrain = xgb.DMatrix(X_train, label=y_train)\n        dtest = xgb.DMatrix(test_df)\n        gc.collect()\n        def xgb_evaluate(max_depth, gamma, colsample_bytree):\n            params = {'eval_metric': 'rmse',\n                      'max_depth': int(max_depth),\n                      'subsample': 0.8,\n                      'eta': 0.1,\n                      'gamma': gamma,\n                      'verbose_eval':False,\n                      'silent':1,\n                      'colsample_bytree': colsample_bytree}\n            cv_result = xgb.cv(params, dtrain, num_boost_round=100, nfold=3)    \n            return -1.0 * cv_result['test-rmse-mean'].iloc[-1]\n        xgb_bo = BayesianOptimization(xgb_evaluate, {'max_depth': (7, 12), \n                                                 'gamma': (0, 1),\n                                                 'colsample_bytree': (0.5, 0.9)})\n        xgb_bo.maximize(init_points=5, n_iter=10, acq='ei')\n        sorted_res = sorted(xgb_bo.res,key = lambda x: x['target'])\n        params = sorted_res[-1]\n        params['params']['max_depth'] = int(params['params']['max_depth']) \n        model = xgb.train(params, dtrain, num_boost_round=1000, silent = 1)\n\n    joblib.dump(model, \"xgb.pkl\") \n    # Predict on testing and training set\n    y_pred = model.predict(dtest)\n    y_train_pred = model.predict(dtrain)\n    \n    return y_pred'''\n    #Please refer to this kernel : https://www.kaggle.com/anushkini/taxi-xgboost\n    return None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#getLGBMPredictions(X_train, y_train, X_val, y_val, test_df) #lightgbm\n#getkNNpredictions(X_train, y_train, test_df)  #knn - must be atleast 1 million data points for this to work\n#getLRPredictions(X_train, y_train, test_df) #linear regression\n#getLassoPredictions(X_train, y_train, test_df) #lasso regression\n#getRidgePredictions(X_train, y_train, test_df) #ridge regression\n#getRFPredictions(X_train, y_train, test_df) random forest regressor\ngetDNNPredictions(X_train, y_train, test_df) #dnn","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>Please uncomment this in edit mode in order to take a demo of the predictor.</h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"'''import requests\nimport json\nfrom datetime import datetime\n\n#PLEASE TURN INTERNET ON FOR THIS TO WORK... ------->\n\n#read the details\nprint(\"Enter the source: \")\nplace = input()\nprint(\"Enter the destination: \")\ndest = input()\nprint(\"Enter the approximate time in number of hours: \")\ntime = int(input())\nprint(\"Please enter the number of passengers\")\npsngcnt = int(input())\nprint(\"Please enter the date (dd/mm/yyyy)\")\ndate = input().strip()\n\n#coordinates of important places\nManhattan = (-73.9712,40.7831)[::-1]\nJFK_airport = (-73.7781,40.6413)[::-1]\nLaguardia_airport = (-73.8740,40.7769)[::-1]\nstatue_of_liberty = (-74.0445,40.6892)[::-1]\ncentral_park = (-73.9654,40.7829)[::-1]\ntime_square = (-73.9855,40.7580)[::-1]\nbrooklyn_bridge = (-73.9969,40.7061)[::-1]\nrockerfeller = (-73.9787,40.7587)[::-1]\n\n#create a datetime object for the given day\ndatetime_object = datetime.strptime(date, '%d/%m/%Y')\n\n#perform an api request in order to get the coordinates of the source and destination\nresponse = requests.get(\"https://api.opencagedata.com/geocode/v1/geojson?q=\"+place.replace(' ', '+') +\"&key=c2f9d990b75444389382e38f107441b0&pretty=1\")\nsrccoords = json.loads(response.text)[\"features\"][0][\"geometry\"][\"coordinates\"]\nresponse = requests.get(\"https://api.opencagedata.com/geocode/v1/geojson?q=\"+dest.replace(' ', '+') +\"&key=c2f9d990b75444389382e38f107441b0&pretty=1\")\ndstcoords = json.loads(response.text)[\"features\"][0][\"geometry\"][\"coordinates\"]\n\n#create a new dataframe for the data point\nnewdf = pd.DataFrame(columns = test_df.columns)\n#create a new row with the extra feature\nrow = [srccoords[0], \n       srccoords[1],\n       dstcoords[0],\n       dstcoords[1],\n       psngcnt,\n       time*60,\n       1 if datetime_object.strftime('%d-%m-%y')in us_holidays else 0,\n       haversine_distance(srccoords[1], srccoords[0], dstcoords[1], dstcoords[0]),\n       datetime_object.year,\n       datetime_object.weekday(),\n       haversine_distance(Manhattan[0],Manhattan[1],srccoords[1],srccoords[0]),\n       haversine_distance(Manhattan[0],Manhattan[1],dstcoords[1],dstcoords[0]),\n       haversine_distance(JFK_airport[0],JFK_airport[1],dstcoords[1],dstcoords[0]),\n       haversine_distance(JFK_airport[0],JFK_airport[1],srccoords[1],srccoords[0]),\n       haversine_distance(Laguardia_airport[0],Laguardia_airport[1],srccoords[1],srccoords[0]),\n       haversine_distance(Laguardia_airport[0],Laguardia_airport[1],dstcoords[1],dstcoords[0]),\n       datetime_object.day,\n       datetime_object.month,\n       haversine_distance(statue_of_liberty[0],statue_of_liberty[1],srccoords[1],srccoords[0]),\n       haversine_distance(statue_of_liberty[0],statue_of_liberty[1],dstcoords[1],dstcoords[0]),\n       haversine_distance(central_park[0],central_park[1],srccoords[1],srccoords[0]),\n       haversine_distance(central_park[0],central_park[1],dstcoords[1],dstcoords[0]),\n       haversine_distance(time_square[0],time_square[1],srccoords[1],srccoords[0]),\n       haversine_distance(time_square[0],time_square[1],dstcoords[1],dstcoords[0]),\n       haversine_distance(brooklyn_bridge[0],brooklyn_bridge[1],srccoords[1],srccoords[0]),\n       haversine_distance(brooklyn_bridge[0],brooklyn_bridge[1],dstcoords[1],dstcoords[0]),\n       haversine_distance(rockerfeller[0],rockerfeller[1],srccoords[1],srccoords[0]),\n       haversine_distance(rockerfeller[0],rockerfeller[1],dstcoords[1],dstcoords[0])\n      ]\n\n#add the row to the dataframe\nnewdf.loc[len(newdf)] = row\nprint(newdf)\ntime*=60\n\nmincost = float('inf')\nmaxcost = 0\nmintime = 0\n\nmodel = getLGB()\n\ndef hours_and_minutes(time):\n    hours = (time//60)\n    minutes = time - hours*60\n    return str(hours)+\":\"+str(minutes)\n\n#find the best time\nfor i in range(120):\n    newtime = min((max((time-60+i, 0)), 1339)) #taking care of exceptions\n    newdf.loc[0, \"time\"] = newtime\n    cost = model.predict(newdf)[0]\n    if cost >maxcost:\n        maxcost = cost\n    if cost < mincost:\n        mincost = cost\n        mintime = newtime\nprint(\"The best time to leave is \", hours_and_minutes(mintime))\nprint(\"It will cost you: \", mincost, \"USD\")\nprint(\"savings(best case) in USD:\", maxcost-mincost) '''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}