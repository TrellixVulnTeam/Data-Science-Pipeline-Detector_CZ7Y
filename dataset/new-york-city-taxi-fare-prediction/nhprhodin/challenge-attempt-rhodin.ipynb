{"cells":[{"metadata":{},"cell_type":"markdown","source":"**This python notebook** summarises my approach to the challenge of predicting taxifare prices in New York. Due to the iterative nature of the analysis, latter analysis steps may reveal flaws and justify reexaminations of earlier assumptions and steps. These earlier steps have then been updated. As such, this notebook reflects my final (organised) approach to the challenge rather than a linear sequence of events. Briefly the code is structued as follows:\n\n- Libraries are imported\n- Functions are defined\n- Input and output options are specified\n- Data is processed\n- Data is summarised in plots\n- Intermediate plots imporant for the analysis are provided at the very end\n\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n\n# Import libraries\nimport numpy as np\nimport pandas as pd\nimport math as math\nimport geopy.distance\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as colors\nfrom scipy import stats\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import mean_squared_error\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Having imported the libraries, I proceed to set up functions to clean and process the input data. These will be called during the analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_data(dataframe, drop_columns, percentage_clip=2.5):\n    \n    \"\"\"\n    \n    This function cleans the input dataframe. This cleaning process includes removal of \n    unneccesary/redundant columns, removal or rows with empty fields, and removal of \n    statistical outliers. Here, I define statistical outliers as anything beyond the \n    percentiles set by the input parameter \"percentage_clip\". I will only clean input\n    data this way. Any features introduced in the latter analysis, i.e., during feature \n    engineering, will only be cut based on physical reasoning and *not* on statistical \n    anomalies.\n    \n    Inputs:\n        \n        - dataframe:       Pandas dataframe structure. The input will be copied, and the \n                           copy will be edited before output.\n        \n        - drop_columns:    Columns present in input dataframe to be dropped. Should be \n                           a list of column names.\n    \n        - percentage_clip: Data beyond the value of this perccentile (in each feature \n                           column) will be removed, on either side of its distribution.\n                           A standard 3-sigma for a gaussian distribution would imply\n                           that 99.87% of data should be kept, and hence 0.13% of the \n                           extreme values (0.065% on either side) in each variable \n                           should be removed. The final value of 2.5% on either side\n                           was decided on iteratively, as it allowed a relatively good\n                           linear regression analysis (see below).\n\n    \"\"\"\n    \n    # Copy input structure\n    data = dataframe.copy()\n    \n    # Drop columns\n    data.drop(drop_columns, axis='columns', inplace=True)\n\n    # Remove rows with missing values\n    data.dropna(inplace=True)\n    \n    # Remove caps with zero passenger counts\n    data = data[(data['passenger_count'] > 0)]\n    \n    # Remove statistical outliers\n    qlow  = data.quantile(percentage_clip / 100.)        # Division to convert % to frac.\n    qhigh = data.quantile(1. - (percentage_clip / 100.)) # Mirrored quantile\n    distributions = [xx for xx in data.columns if data[xx].dtype!='datetime64[ns]']\n\n    for ii in distributions:\n        data = data[(data[ii] >= qlow[ii]) & (data[ii] <= qhigh[ii])]\n        \n    return data","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"def feature_engineering(dataframe):\n    \n    \"\"\"\n    Construct new features based on clean data.\n    \n    Inputs:\n        \n        - dataframe:  Pandas dataframe structure. New features will be added to a copy \n                      of this structure.\n    \n    Added Features:\n        \n        - dr_deg:     Calculate coordinate separation based on gps initial- and final \n                      positions, read from input dataframe as 'pickup_longitude', \n                      'pickup_latitude', 'dropoff_longitude' and 'dropoff_latitude'.\n                      Note that this feature is a proxy, and only reflects the shortest\n                      linear path between initial and end point. It is *NOT* equivalent to\n                      the physical, travelled distance on roads. In addition, for large\n                      distances, one should account for Earth's curvature. However, as we\n                      are dealing with taxt-fare prices within a city, curvature is\n                      negligable. In addition, this effect is expected to minor relative\n                      to the (ignored) perturbations from infrastructure, road-planning and\n                      road choices. Hence, this justifies its removal.\n                      \n        - year:       Extract year information from input dataframe's datetime column. We \n                      expect a dependence based on inflation and raised taxi prices.\n        \n        - day:        Extracted same as year. Expect that taxi fare price may vary with the \n                      day of the week. E.g. between work days and weekend.\n        \n        - hour:       Extracted same as year. Expect short-term temporal variability in price \n                      to emerge from standard work-hours, evenings, and over-pay/night-time.\n    \n    \"\"\"\n    \n    # Feature to approximate distance traveled\n    \n    # Instantiate empty dataframe and populate it with gps positions\n    df_tmp = pd.DataFrame()\n    df_tmp['pickup_position']=list(zip(dataframe['pickup_latitude'],dataframe['pickup_longitude']))\n    df_tmp['dropoff_position'] =list(zip(dataframe['dropoff_latitude'],dataframe['dropoff_longitude']))\n    \n    # Copy input structure\n    data = dataframe.copy()\n    \n    # Add feature dr to approximate distance travelled. Calculated from absolute difference in lat. & long.\n    dy                 = (data['dropoff_latitude'] - data['pickup_latitude']).abs()\n    dx                 = (data['dropoff_longitude'] - data['pickup_longitude']).abs()\n    data['dr_deg']     = np.sqrt(dx**2 + dy**2)\n        \n    # Add features to model long- and short-scale temporal variations\n    data['year'] = data['pickup_datetime'].dt.year\n    data['day']  = data['pickup_datetime'].dt.dayofweek\n    data['hour'] = data['pickup_datetime'].dt.hour\n    \n    # Ensure taxi drives a positive distance greater than zero\n    data = data[data['dr_deg'] > 0]\n    \n    return data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The functions above include parameters and limits determined from an inital exploratory analysis. I now proceed to define input- and outpout options. Essentially, the following piece of code collects the most important parameters that can be tweaked by the user. Note that I do not read all 55 million rows. At a trade-off between time and computing power, I settled for performing most of the analysis on the first 5E5 rows of data. I arrived at this number after testing input sizes of: 1E3; 5E3; 5E4; 5E5 and 5E6 rows.\n\nIf limited by time and computing power, a nice follow-up to this approach would be to draw the 5E5 rows randomly from the complete datset rather than using the first 5E5 rows. I will leave this for a potential future implementation."},{"metadata":{"trusted":true},"cell_type":"code","source":"#  --------- User inputs\n\n# Set paths\npath_train = '../input/train.csv'\npath_test  = '../input/test.csv'\n\n# Drop 'key' column: it is just a dummy-variable, similar to 'pickup_datetime'\ndrop_columns = ['key']\n\n# Optional outputs, see end of notebook\ntune_hyperparams             = False  # Allows me to optimize random forest (RF) model parameters\nshow_parameter_distributions = True   # Distributions of each parameter\nshow_feature_rankings        = True   # Calculates the feature importance, based on RF \n\n# Import raw data\ndf = pd.read_csv(path_train, nrows=int(5E5), header=0, parse_dates=['pickup_datetime'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Clean data\ndf_c = clean_data(df, drop_columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add features\ndf_cf = feature_engineering(df_c)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set up parameters to fit cleaned data\ny_c = df_c['fare_amount']\nX_c = df_c.drop(['fare_amount', 'pickup_datetime'], axis='columns')\nX_c_train, X_c_valid, y_c_train, y_c_valid = train_test_split(X_c, y_c, random_state=47, test_size = int(len(y_c)/3.))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set up parameters to fit cleaned + featured data\ny_cf = df_cf['fare_amount']\nX_cf = df_cf.drop(['fare_amount', 'pickup_datetime'], axis='columns')\nX_cf_train, X_cf_valid, y_cf_train, y_cf_valid = train_test_split(X_cf, y_cf, random_state=47, test_size = int(len(y_cf)/3.))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make linear regression models\nlr_c = LinearRegression()\nlr_c.fit(X_c_train, y_c_train)\nlr_c_prediction = lr_c.predict(X_c_valid)\n\nlr_cf = LinearRegression()\nlr_cf.fit(X_cf_train, y_cf_train)\nlr_cf_prediction = lr_cf.predict(X_cf_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make random forest models\nrf_c = RandomForestRegressor(n_estimators=100, max_depth=10, min_samples_leaf=5)\nrf_c.fit(X_c_train, y_c_train)\nrf_c_prediction = rf_c.predict(X_c_valid)\n\nrf_cf = RandomForestRegressor(n_estimators=100, max_depth=10, min_samples_leaf=5)\nrf_cf.fit(X_cf_train, y_cf_train)\nrf_cf_prediction = rf_cf.predict(X_cf_valid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At this point, I have performed the main work: the data has been cleaned and processed; features have been added; and the final input has been passed to models. In the following sequence of figures, I summarise how well I am able to retrieve the taxi prices based on the input. For a complete description, see markdown text below plots."},{"metadata":{"trusted":true},"cell_type":"code","source":"# --------------- Test how well we recover known prices\n\n# Define figure dimensions\nplt.figure(figsize=(10, 10), dpi= 80, facecolor='w', edgecolor='k')\nplt.rcParams['figure.figsize'] = [10, 10]\n\n# Evaluate a gaussian kernel density estimate (kde) on data, following python documentation on:\n# https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gaussian_kde.html\nnbins     = int(50)\nnlevels   = int(5E2)\n\n# --------------- LINEAR REGRESSION\n# On Cleaned data\nplt.figure(1)\nxx,yy     = np.mgrid[y_c_valid.min():y_c_valid.max():nbins*1j, lr_c_prediction.min():lr_c_prediction.max():nbins*1j]\npositions = np.vstack([xx.ravel(), yy.ravel()])\nvalues    = np.vstack([y_c_valid, lr_c_prediction])\nkernel    = stats.gaussian_kde(values)\nzz        = np.reshape(kernel(positions).T, xx.shape)\nplt.contour(xx, yy, zz, nlevels)\nplt.scatter(y_c_valid, lr_c_prediction, s=1, label='LR_C')\nplt.xlim(0, 80)\nplt.ylim(0, 80)\nplt.plot([0, 80], [0, 80], ls=\"--\", c=\".3\")\nplt.xlabel(\"Known Prices (validation data)\")\nplt.ylabel(\"Predicted Prices (validation data)\")\n\n# Make sure to write associated scores and test statistics\nscore_lr_c_train = lr_c.score(X_c_train, y_c_train)\nplt.text(50., 40., r'Score: %5.2f (training)' %score_lr_c_train, fontsize=15.)\nscore_lr_c_valid = lr_c.score(X_c_valid, y_c_valid)\nplt.text(50., 35., r'Score: %5.2f (validation)' %score_lr_c_valid, fontsize=15.)\nRMSE_lr_c_train = np.sqrt(mean_squared_error(y_c_train, lr_c.predict(X_c_train)))\nplt.text(50., 25., r'RMSE: %5.2f (training)' %RMSE_lr_c_train, fontsize=15.)\nRMSE_lr_c_valid = np.sqrt(mean_squared_error(y_c_valid, lr_c.predict(X_c_valid)))\nplt.text(50., 20., r'RMSE: %5.2f (validation)' %RMSE_lr_c_valid, fontsize=15.)\n\nplt.legend()\n\n# On Cleaned + featured data\nplt.figure(2)\nxx,yy     = np.mgrid[y_cf_valid.min():y_cf_valid.max():nbins*1j, lr_cf_prediction.min():lr_cf_prediction.max():nbins*1j]\npositions = np.vstack([xx.ravel(), yy.ravel()])\nvalues    = np.vstack([y_cf_valid, lr_cf_prediction])\nkernel    = stats.gaussian_kde(values)\nzz        = np.reshape(kernel(positions).T, xx.shape)\nplt.contour(xx, yy, zz, nlevels)\nplt.scatter(y_cf_valid, lr_cf_prediction, s=1, label='LR_CF')\nplt.xlim(0, 80)\nplt.ylim(0, 80)\nplt.plot([0, 80], [0, 80], ls=\"--\", c=\".3\")\nplt.xlabel(\"Known Prices (validation data)\")\nplt.ylabel(\"Predicted Prices (validation data)\")\n\n# Make sure to write associated scores and test statistics\nscore_lr_cf_train = lr_cf.score(X_cf_train, y_cf_train)\nplt.text(50., 40., r'Score: %5.2f (training)' %score_lr_cf_train, fontsize=15.)\nscore_lr_cf_valid = lr_cf.score(X_cf_valid, y_cf_valid)\nplt.text(50., 35., r'Score: %5.2f (validation)' %score_lr_cf_valid, fontsize=15.)\nRMSE_lr_cf_train = np.sqrt(mean_squared_error(y_cf_train, lr_cf.predict(X_cf_train)))\nplt.text(50., 25., r'RMSE: %5.2f (training)' %RMSE_lr_cf_train, fontsize=15.)\nRMSE_lr_cf_valid = np.sqrt(mean_squared_error(y_cf_valid, lr_cf.predict(X_cf_valid)))\nplt.text(50., 20., r'RMSE: %5.2f (validation)' %RMSE_lr_cf_valid, fontsize=15.)\n\nplt.legend()\n\n# --------------- RANDOM FORREST\n# On Cleaned data\nplt.figure(3)\nxx,yy     = np.mgrid[y_c_valid.min():y_c_valid.max():nbins*1j, rf_c_prediction.min():rf_c_prediction.max():nbins*1j]\npositions = np.vstack([xx.ravel(), yy.ravel()])\nvalues    = np.vstack([y_c_valid, rf_c_prediction])\nkernel    = stats.gaussian_kde(values)\nzz        = np.reshape(kernel(positions).T, xx.shape)\nplt.contour(xx, yy, zz, nlevels)\nplt.scatter(y_c_valid, rf_c_prediction, s=1, label='RF_C')\nplt.xlim(0, 80)\nplt.ylim(0, 80)\nplt.plot([0, 80], [0, 80], ls=\"--\", c=\".3\")\nplt.xlabel(\"Known Prices (validation data)\")\nplt.ylabel(\"Predicted Prices (validation data)\")\n\n# Make sure to write associated scores and test statistics\nscore_rf_c_train = rf_c.score(X_c_train, y_c_train)\nplt.text(50., 40., r'Score: %5.2f (training)' %score_rf_c_train, fontsize=15.)\nscore_rf_c_valid = rf_c.score(X_c_valid, y_c_valid)\nplt.text(50., 35., r'Score: %5.2f (validation)' %score_rf_c_valid, fontsize=15.)\nRMSE_rf_c_train = np.sqrt(mean_squared_error(y_c_train, rf_c.predict(X_c_train)))\nplt.text(50., 25., r'RMSE: %5.2f (training)' %RMSE_rf_c_train, fontsize=15.)\nRMSE_rf_c_valid = np.sqrt(mean_squared_error(y_c_valid, rf_c.predict(X_c_valid)))\nplt.text(50., 20., r'RMSE: %5.2f (validation)' %RMSE_rf_c_valid, fontsize=15.)\n\nplt.legend()\n\n# On Cleaned + featured data\nplt.figure(4)\nxx,yy     = np.mgrid[y_cf_valid.min():y_cf_valid.max():nbins*1j, rf_cf_prediction.min():rf_cf_prediction.max():nbins*1j]\npositions = np.vstack([xx.ravel(), yy.ravel()])\nvalues    = np.vstack([y_cf_valid, rf_cf_prediction])\nkernel    = stats.gaussian_kde(values)\nzz        = np.reshape(kernel(positions).T, xx.shape)\nplt.contour(xx, yy, zz, nlevels)\nplt.scatter(y_cf_valid, rf_cf_prediction, s=1, label='RF_CF')\nplt.xlim(0, 80)\nplt.ylim(0, 80)\nplt.plot([0, 80], [0, 80], ls=\"--\", c=\".3\")\nplt.xlabel(\"Known Prices (validation data)\")\nplt.ylabel(\"Predicted Prices (validation data)\")\n\n# Make sure to write associated scores and test statistics\nscore_rf_cf_train = rf_cf.score(X_cf_train, y_cf_train)\nplt.text(50., 40., r'Score: %5.2f (training)' %score_rf_cf_train, fontsize=15.)\nscore_rf_cf_valid = rf_cf.score(X_cf_valid, y_cf_valid)\nplt.text(50., 35., r'Score: %5.2f (validation)' %score_rf_cf_valid, fontsize=15.)\nRMSE_rf_cf_train = np.sqrt(mean_squared_error(y_cf_train, rf_cf.predict(X_cf_train)))\nplt.text(50., 25., r'RMSE: %5.2f (training)' %RMSE_rf_cf_train, fontsize=15.)\nRMSE_rf_cf_valid = np.sqrt(mean_squared_error(y_cf_valid, rf_cf.predict(X_cf_valid)))\nplt.text(50., 20., r'RMSE: %5.2f (validation)' %RMSE_rf_cf_valid, fontsize=15.)\n\nplt.legend()\n\n# Show all plots\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Figure explanation:** In the above plots, I show how the recovered taxi-fare estimates (y-axis) compare with the known prices of the validation set. From top to bottom, I show LR_C (Linear Regression, Cleaned); LR_CF (Linear Regression, Cleaned+Feature Engineered); RF_C (Random Forest, Cleaned); and RF_CF (Random Forest, Cleaned+Engineered). For perfect recovery, we expect that the data should lie on the one-to-one correlation (the diagonal grey dashed line). Because of the vast amount of data and scatter present, even with small symbol-sizes, it is hard to quantify how well we match the true price. In addition, it is hard to resolve individual points -and if on top of each other, such scatter plots become saturated and cannot be used to quantify the match visually. Therefore, I overplot density contours. A high density of data produces yellow, saptially close contours; low-density data regions produce purple, largely separated contours. These contours allow us to verify that (1) feature engineering makes a difference; (2) the random forest algorithm is superior to the linear regression; and (3) most of the data is centered on the lower price-range. \n\n**Reflections**: It is interesting to note that (2) can be verified directy by varying the \"percentage_clip\" parameter in the clean_data function. Reducing it from 2.5% to 0.1%- or as low as 0.05% will, on its own, lead to a worse match in the linear regression models, whereas the random forest models remain largely unaffected. This further demonstrates that Linear Regression is more sensitive to outliers. Finally, I show the scores and RMSE values for each model for both training and validation samples. These statistics confirm the observed correlations. In all cases, it seems that training- and validation statistics agree. This indicates a good match, and I am not significantly overfitting the data.\n\nFinally, we can note that a consistent RMSE of 2.3-ish for RF in training and validation set alike nicely improve on the values reported in the kernel https://www.kaggle.com/willkoehrsen/a-walkthrough-and-a-challenge. Of course, one difference is that the strict statistical rejection has removed all data with prices exceeding approximately $40. Although the density of data towards such high prices is low, this should perhaps be considered. A combination of large input data, and a lower rejection cut will enable this range to be analysed.\n"},{"metadata":{},"cell_type":"markdown","source":"**Model hyper-parameters**\n\nWith these results in place, I have proceeded to fine-tune the random forrest hyperparameters. This is computationally expensive, and the grid is currently running for the complete input (5E5 rows). Once these results are done, I will apply the best-fit model - possibly on an even larger dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"if tune_hyperparams:\n    \n    \"\"\"\n    This option allows us to identify the best model parameter set. This is done by\n    defining a dictionary with the relevant hyper-parameters and the ranges to be\n    explored. This dictionary, together with the model, is then passed into a grid\n    search algorithm which performs the heavy lifting.\n    \"\"\"\n    \n    model = RandomForestRegressor()\n    param_grid = {\"n_estimators\" : np.arange(50, 1100, 50),\n                  \"max_depth\" : np.arange(1,51, 5),\n                  \"criterion\" : [\"mse\"],\n                  \"min_samples_leaf\" : [3],\n                  \"min_samples_split\" : [3],\n                  \"bootstrap\" : [True]\n                  }\n\n    random_cv = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, n_jobs=-1, verbose=1)\n    random_cv = random_cv.fit(X_cf_train, y_cf_train)\n\n    print(random_cv.best_score_)\n    print(random_cv.best_params_)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I now proceed to show optional intermediate outputs used during the data exploration phase. "},{"metadata":{"trusted":true},"cell_type":"code","source":"if show_parameter_distributions:\n    \n    \"\"\"\n    This option allows us to view individual parameter distributions. I originally used it to explore each\n    parameter of the input before, and the effect of cleaning and adding features.  \n    \"\"\"\n    \n    dist_list = [xx for xx in df_c.columns if df_c[xx].dtype!='datetime64[ns]']\n    \n    for ii in dist_list:\n        \n        plt.figure()\n        df[ii].plot(kind='hist', bins=200, range=(math.floor(df_cf[ii].min()),math.ceil(df_cf[ii].max())), alpha=0.2, density=True, label=\"df\")\n        df_c[ii].plot(kind='hist', bins=200, range=(math.floor(df_cf[ii].min()),math.ceil(df_cf[ii].max())), alpha=0.2, density=True, label=\"df_c\")\n        df_cf[ii].plot(kind='hist', bins=200, range=(math.floor(df_cf[ii].min()),math.ceil(df_cf[ii].max())), alpha=0.2, density=True, label=\"df_cf\")\n        plt.legend()\n        plt.xlabel(ii)\n    \n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if show_feature_rankings:\n    \n    \"\"\"\n    This option allows us to look at the relative importance of each feature to\n    the final taxi fare prediction. The values are based on the random forest\n    output.\n    \"\"\"\n    \n    # On clean data\n    plt.figure()\n    features = X_c_train.columns\n    importances = rf_c.feature_importances_\n    indicies = np.argsort(importances)\n    plt.barh(range(len(indicies)), importances[indicies], color='b', alpha=0.6, align='center')\n    plt.title('Clean Data')\n    plt.xlabel('Relative Importance')\n    plt.yticks(range(len(indicies)), [features[i] for i in indicies])\n    \n    # On clean, feature-engineered data\n    plt.figure()\n    features = X_cf_train.columns\n    importances = rf_cf.feature_importances_\n    indicies = np.argsort(importances)\n    plt.barh(range(len(indicies)), importances[indicies], color='g', alpha=0.6, align='center')\n    plt.title('Clean + Feature Engineered Data')\n    plt.xlabel('Relative Importance')\n    plt.yticks(range(len(indicies)), [features[i] for i in indicies])\n    \n    plt.show()\n\n\n    \n    \n    \n    ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}