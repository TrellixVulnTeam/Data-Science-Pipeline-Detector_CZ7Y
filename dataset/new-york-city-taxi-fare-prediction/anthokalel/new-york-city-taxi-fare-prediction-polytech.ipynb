{"cells":[{"metadata":{"_uuid":"c695013dc07da4a2b4b53276611641712892a59e"},"cell_type":"markdown","source":"# New York City Taxi Fare Prediction"},{"metadata":{"_uuid":"5c3d9214f6bafea8f2f55c9ff818016bca7aa51d"},"cell_type":"markdown","source":"## Can we predict a rider's taxi fare?"},{"metadata":{"_uuid":"5b2f1299adad364b1c716078f3647ed431c649fe"},"cell_type":"markdown","source":"## Import data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# load some default Python modules\n%matplotlib inline\n\nimport time\n\nfrom sklearn.metrics import mean_squared_error\nimport plotly.offline as py\nimport plotly.graph_objs as go\npy.init_notebook_mode()\n\nfrom math import radians, cos, sin, asin, sqrt\nimport re\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom fbprophet import Prophet\nplt.style.use('seaborn-whitegrid')\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8a923cdda1574801f214c7593b75534b41bd4266"},"cell_type":"markdown","source":"We start by importing the data. The original file train.csv contains more than 55 millions rows. Because we use a Kaggle kernel we take only 22 millions rows."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/train.csv\", nrows = 2_000_000)\n\nprint(\"shape of train data\", train.shape)\ntrain.head()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7fd25b63a2df708d8f31f5d626a4edf9f1f72b25"},"cell_type":"markdown","source":"We look what is the type of each feature"},{"metadata":{"trusted":true,"_uuid":"4511b51a9a54c135d01c38f10a6a6fb4fce05887"},"cell_type":"code","source":"# datatypes\ntrain.dtypes","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"74d303377208f22f58966366ffa90ea340ed11b8"},"cell_type":"markdown","source":"We look more closely at the data."},{"metadata":{"trusted":true,"_uuid":"2c4edba874fe782a562d77bb0e7d72c59906f94f"},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"134dcec1f9d844969e46f3ce29aa49a2582a7aea"},"cell_type":"markdown","source":"## Data cleaning"},{"metadata":{"_uuid":"c73624e5692da4aa4870cfa0b5db57c136b4ec34"},"cell_type":"markdown","source":"We can see that there are some outliers in the dataset.\n\nFor example : \n<p> \n    <ul>\n        <li> The minimum of fare amount is negative and the maximum is more than 60,000 USD</li>\n        <li> The maximum of passenger count is 208 and minimum is 0 </li>\n        <li> Some latitude and longitude are very high </li>\n    </ul>\n</p>"},{"metadata":{"_uuid":"c5a89cc623c1d83b5e1555ad0ccc260dacc35969"},"cell_type":"markdown","source":"In New York City, minimum taxi fare is 2.5 USD. We remove data where fare_amount is less than 2.5 USD"},{"metadata":{"trusted":true,"_uuid":"08f932e82eaee4934d45f8ba44c559db09f360a8"},"cell_type":"code","source":"# Checking for valid fare amount\nprint('Old size: %d' % len(train))\ntrain = train.drop(train[train['fare_amount']<2.5].index, axis=0)\nprint('New size after dropping invalid fare amount: %d' % len(train))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4be78509b054fb4039e0d2619091f168439b3cbf"},"cell_type":"markdown","source":"We look if there is some missing data."},{"metadata":{"trusted":true,"_uuid":"99e1911f64f085545deaf2627d1668563d95c1f9"},"cell_type":"code","source":"# check missing data\ntrain.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"925f391eb10c80a785b393034cafb9ad525da2ab"},"cell_type":"markdown","source":"We remove it"},{"metadata":{"trusted":true,"_uuid":"6c75f656d46995908719d143af3c977121f3820a"},"cell_type":"code","source":"print(\"old size: %d\" % len(train))\ntrain = train.dropna(how='any', axis=0)\nprint(\"New size after dropping missing value: %d\" % len(train))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1d7e45d9f0089bf1a471446e5f94785525b4421c"},"cell_type":"markdown","source":"Now we look closer at passanger count"},{"metadata":{"trusted":true,"_uuid":"1dc320018e3252ee2c06e9ff13418efb56a1bc07"},"cell_type":"code","source":"# checking for passanger count\ntrain.passenger_count.hist(bins=10, figsize = (16,8))\nplt.xlabel(\"Passanger Count\")\nplt.ylabel(\"Frequency\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"da88921269bb1a5687bc5c1be7c65b83c2ebc229"},"cell_type":"markdown","source":"It seems that there are taxi with more than 200 passanger"},{"metadata":{"trusted":true,"_uuid":"b7f50c73d0c4dfd5d3b8b223b965d1076c164f34"},"cell_type":"code","source":"# checking for passanger count greater than 7\ntrain[train.passenger_count >7].passenger_count.hist(bins=10, figsize = (16,8))\nplt.xlabel(\"Passanger Count\")\nplt.ylabel(\"Frequency\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1e57072a1c12d0ebc57a27a780451f466dbbb8d1"},"cell_type":"markdown","source":"The maximum capacity for taxi is 7 so we remove data above "},{"metadata":{"trusted":true,"_uuid":"fa115dd6c9ae8d20f2d0e3aaa1a70cc254779857"},"cell_type":"code","source":"print('Old size: %d' % len(train))\ntrain = train.drop(train[train['passenger_count']>7].index, axis = 0)\ntrain = train.drop(train[train['passenger_count']<1].index, axis = 0)\nprint('New size: %d' % len(train))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"18bdb94abe1e71f90416d626b124fb0a465c2961"},"cell_type":"markdown","source":"Now we look for outliers on taxi fare"},{"metadata":{"trusted":true,"_uuid":"3d89aa9e037d4954b565f61ef0f494b6e7ebffe0"},"cell_type":"code","source":"# checking for taxi fare\ntrain.fare_amount.hist(bins=10, figsize = (16,8))\nplt.xlabel(\"Taxi Fare\")\nplt.ylabel(\"Frequency\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3eb2ad82fa61dfc45082ecb9210cdb665ffde014"},"cell_type":"code","source":"# checking for taxi fare more than 250 USD\ntrain[train.fare_amount >250].fare_amount.hist(bins=10, figsize = (16,8))\nplt.xlabel(\"Taxi Fare\")\nplt.ylabel(\"Frequency\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"916413fb11bf812da890f19d94ee101f76a88e28"},"cell_type":"code","source":"print('Old size: %d' % len(train))\ntrain = train.drop(train[train['fare_amount']>250].index, axis = 0)\nprint('New size: %d' % len(train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b8414e196e0a2028c60f9e462c063483c105d791"},"cell_type":"code","source":"# Lets see the distribution of fare amount less than 100\ntrain[train.fare_amount <100 ].fare_amount.hist(bins=100, figsize = (16,8))\nplt.xlabel(\"Fare Amount\")\nplt.ylabel(\"Number of courses\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8be974e8fedf5a3dc6b4853e5008647cb301f651"},"cell_type":"markdown","source":"As we can see,  majority of taxi rides cost around 7 USD that means people use it on short distances. "},{"metadata":{"_uuid":"8db89fa3de89d7e1c9941fec1e88c8e939f3ae5f"},"cell_type":"markdown","source":"The bounding box around New York city is :\n<p>\n    <ul>\n        <li>North Latitude: 40.917577</li>\n        <li>South Latitude: 40.477399</li> \n        <li>East Longitude: -73.700272 </li>\n        <li>West Longitude: -74.259090</li>\n    </ul>       \n</p>\nWe remove ride out of New York city:"},{"metadata":{"trusted":true,"_uuid":"e0ae012e2c3820c32f04ecb1fa68604081a07fac"},"cell_type":"code","source":"print('Old size: %d' % len(train))\ntrain = train[(train['pickup_longitude'] >= -74.259090) & (train['pickup_longitude'] <= -73.700272)]\ntrain = train[(train['dropoff_longitude'] >= -74.259090) & (train['dropoff_longitude'] <= -73.700272)]\ntrain = train[(train['pickup_latitude'] >= 40.477399) & (train['pickup_latitude'] <= 40.917577)]\ntrain = train[(train['dropoff_latitude'] >= 40.477399) & (train['dropoff_latitude'] <= 40.917577)]\nprint('New size: %d' % len(train))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"af41a45203f8b410a39da8c2c0a3dcc481e3d26f"},"cell_type":"markdown","source":"We also remove rides where pickup and dropoff location are exactly the same:"},{"metadata":{"trusted":true,"_uuid":"9eeaa98d124c15f4f03db30af31a26a1cb411836"},"cell_type":"code","source":"print('Old size: %d' % len(train))\ntrain = train[(train['pickup_longitude'] != train['dropoff_longitude']) | (train['pickup_latitude'] != train['dropoff_latitude'])]\nprint('New size: %d' % len(train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d43f4752a63a453b9075012296e683ace28a308"},"cell_type":"markdown","source":"Let's check if it is better !"},{"metadata":{"trusted":true,"_uuid":"e2441e984536b01ded78f53ba04c8654ab4a2b88"},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e638f47a16d0f29cec23635b632ac0767ef08e15"},"cell_type":"markdown","source":"We look the test set:"},{"metadata":{"trusted":true,"_uuid":"6bb54a26e4b34d0c2072a3ea899b1231aba0a4d8"},"cell_type":"code","source":"test = pd.read_csv(\"../input/test.csv\")\nprint(\"shape of test data\", test.shape)\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"49cd61889759820ac4988b1027378c4d4f4bfb66"},"cell_type":"code","source":"#check for missing value\ntest.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1b0346c3137c77c077b3d92dde75afb57ad5d237"},"cell_type":"code","source":"# checking for basic stats\ntest.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4e7e17037681496b8c51d6f6502475e1bbe36972"},"cell_type":"markdown","source":"It seems find !"},{"metadata":{"_uuid":"e5c7ce02c7b4c19a374a3e0571cf1f912066259a"},"cell_type":"markdown","source":"## Feature engineering"},{"metadata":{"_uuid":"d7a789897f009dd17ba5bc08dc040f1aace9879f"},"cell_type":"markdown","source":"<p>\n    <ol>\n        <li>First we add a distance in kilometers</li>\n         <li>Second we add time feature</li>\n     </ol>\n </p>\n        "},{"metadata":{"trusted":true,"_uuid":"e12d71be69580893e9918aea654b3b7272520b5d"},"cell_type":"code","source":"# For XGBoost, later\ndef time_features(dataframe):\n    dataframe['pickup_datetime'] = dataframe['pickup_datetime'].astype(str).str.slice(0, 16)\n    dataframe['pickup_datetime'] = pd.to_datetime(dataframe['pickup_datetime'], utc=True, format='%Y-%m-%d %H:%M')\n    dataframe['hour_of_day'] = dataframe.pickup_datetime.dt.hour\n    dataframe['month'] = dataframe.pickup_datetime.dt.month\n    dataframe[\"year\"] = dataframe.pickup_datetime.dt.year\n    dataframe[\"weekday\"] = dataframe.pickup_datetime.dt.weekday    \n    return dataframe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8dff60202bdb237e33514e8a3d033fedcf5ed9c1"},"cell_type":"code","source":"# calculate distance between two latitude longitude points haversine formula \n# Returns distance in kilometers\ndef distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295 # Pi/180\n    a = 0.5 - np.cos((lat2 - lat1) * p)/2 + np.cos(lat1 * p) * np.cos(lat2 * p) * (1 - np.cos((lon2 - lon1) * p)) / 2\n    return 12742 * np.arcsin(np.sqrt(a))   # 2*R*asin...","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"83cfad3573b395089a5fbc83b3b4797f6d98ed64"},"cell_type":"code","source":"train['distance_miles'] = distance(train.pickup_latitude, train.pickup_longitude, \\\n                                      train.dropoff_latitude, train.dropoff_longitude)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fd68cc9d57b52b83e0d42f872e950186168ca043"},"cell_type":"code","source":"test['distance_miles'] = distance(test.pickup_latitude, test.pickup_longitude, \\\n                                      test.dropoff_latitude, test.dropoff_longitude)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f2dbb78b6ffcf47f428c84eecf9e64a6ad47d2bd"},"cell_type":"code","source":"print(\"Average $USD/Km : {:0.2f}\".format(train.fare_amount.sum()/train.distance_miles.sum()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"90e57a1a7c44832f97c0ae8fedda9dce97c7962c"},"cell_type":"code","source":"# scatter plot distance - fare\nplt.scatter(train.distance_miles, train.fare_amount, alpha=0.2)\nplt.xlabel('distance mile')\nplt.ylabel('fare $USD')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"523a0df4d80299041208378ce88703244237c470"},"cell_type":"markdown","source":"It seems that the relation is linear between the distance and the fare amount"},{"metadata":{"_uuid":"674fcac32a517118760fff0dbcc91f63a96b0b11"},"cell_type":"markdown","source":"## FbProphet model\n\nProphet is a procedure for forecasting time series data based on an additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects. It works best with time series that have strong seasonal effects and several seasons of historical data.\nProphet is open source software released by Facebook.\n\nFbProphet is a very ressource consuming algorithm, so we need to split the train data set. \nWe take 1 million sample to process FbProphet"},{"metadata":{"trusted":true,"_uuid":"6b6a64c74e2c0ded78b8dfee3b54ca96c9c3904a"},"cell_type":"code","source":"prophet_df = train.iloc[:1000000]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e85e21f400b885cc233b1c814fb0770a343886ea"},"cell_type":"markdown","source":"In FbProphet library we must use 'ds' and 'y' as column names. So we rename the existing columns."},{"metadata":{"trusted":true,"_uuid":"c69b842c6de36cce91a2e7222d478d89bb13e7aa"},"cell_type":"code","source":"prophet_df = prophet_df.reset_index()[[\"pickup_datetime\", \"fare_amount\"]]\nprophet_df.columns = [\"ds\", \"y\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7134ead62971c993510c2a5f425b2134fdfae7e4"},"cell_type":"code","source":"prophet_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"df9633ac095393f3fe63b3725095723f57107536"},"cell_type":"markdown","source":"We convert 'ds' column to datastamp and sort the values."},{"metadata":{"trusted":true,"_uuid":"1f80340569a472a1ff2a030a768ac49f8b6bd9e7"},"cell_type":"code","source":"prophet_df['ds'] = pd.to_datetime(prophet_df['ds'].sort_values())\nprophet_df['y'] = pd.to_numeric(prophet_df['y'],errors='ignore')\nprophet_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"57f3c12bd59369ae2dbe8120beaa453ab0c68d60"},"cell_type":"markdown","source":"### Split train/test\n\nThe train set will be the 80% firsts values, and the test set the 20% last values."},{"metadata":{"trusted":true,"_uuid":"4552b8ad60dc37e5ec87484e0e1bbfa1091b8267"},"cell_type":"code","source":"df_train = prophet_df.iloc[:round(len(prophet_df)*0.8)]\ndf_test = prophet_df.iloc[round(len(prophet_df)*0.8):]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d3caa3ca006846e08338b584160fe867cb822e01"},"cell_type":"markdown","source":"### Fitting the model"},{"metadata":{"trusted":true,"_uuid":"153c770b11b8b622cff702248b0d6cf7ce8c2539"},"cell_type":"code","source":"model = Prophet(changepoint_prior_scale=2.5, daily_seasonality=True)\n\nstart = time.time()\nmodel.fit(df_train)\nprint(\"Fitting duration : {:.3f}s\".format(time.time() - start) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"73aef9bf12fc6b95d29bfe3ff844233a38a4b48e"},"cell_type":"code","source":"future_data = df_test.drop(\"y\", axis=1)\nstart = time.time()\nforecast_data = model.predict(future_data)\nprint(\"Predict duration : {:.3f}s\".format(time.time() - start) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fc7aa30298761ef0d7c45344d0ec6083f3a47c04"},"cell_type":"code","source":"forecast_data[\"y\"] = df_test[\"y\"].values\nforecast_data[['ds', 'y', 'yhat', 'yhat_lower', 'yhat_upper']].tail()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5f7606ba48070a4d9cb305615f4a8c6a781d1a21"},"cell_type":"markdown","source":"Now let's compare the prediction and confidence to the real data (you can zoom, pan on the plot)"},{"metadata":{"trusted":true,"_uuid":"c560697c565af29b5e4d5c6a017f9c2db4418867"},"cell_type":"code","source":"py.iplot([\n    go.Scatter(x=df_test['ds'], y=df_test['y'], name='y'),\n    go.Scatter(x=forecast_data['ds'], y=forecast_data['yhat'], name='yhat'),\n    go.Scatter(x=forecast_data['ds'], y=forecast_data['yhat_upper'], fill='tonexty', mode='none', name='upper'),\n    go.Scatter(x=forecast_data['ds'], y=forecast_data['yhat_lower'], fill='tonexty', mode='none', name='lower'),\n    go.Scatter(x=forecast_data['ds'], y=forecast_data['trend'], name='Trend')\n])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"821199c36b017611e7e15fd5908d50d87352824d"},"cell_type":"markdown","source":"### Metric\n\nFor the metric, I'll compute the MSE for every 2 days (48 hours of data) to check how it evolves during the complete year"},{"metadata":{"trusted":true,"_uuid":"ec78cda90a577a3e9f4dd09ae7c4eff1e5f19040"},"cell_type":"code","source":"mse = []\nfor i in range(0, len(forecast_data), 48):\n    mse.append(mean_squared_error(\n                    forecast_data.loc[i:i+48, \"y\"],\n                    forecast_data.loc[i:i+48, \"yhat\"]\n                ))\n\nplt.figure(figsize=(20,12))\nplt.plot(mse) # mse per day during 2 years\nplt.title(\"Evolution of MSE during year 2016 - 2017\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d71ee78ab4f14fd1b98380025444e4dd9eb0aa11"},"cell_type":"markdown","source":"\n\nWe can see that we have a correct MSE during the summer but every winter have a lot more error. This can be explained with weather. We have peaks of error which may be explained by the lack of wind. This is more difficult to predict. As a result, the pollution generated by heating system stacked over the city and decrease quickly when wind is back. The rest of the time we have a quite good approximation\n"},{"metadata":{"trusted":true,"_uuid":"2b38e58ff7d4672523f3f76e2e265598d41c3fea"},"cell_type":"code","source":"model.plot_components(forecast_data)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"36306940f2205f1e26a596d7a2aaf8f45a735879"},"cell_type":"markdown","source":"We have now 4 analysis.\n\n  <ul>\n    <li>A global trend which is increasing during years. This is logical as we predict over 6 years of data, and it is known that taxi fare increases (a little over years).\n    </li> \n    <li>A yearly trend that shows that the prices are quite equal from may to december and starts to decrease because it is the least touristic periods</li>\n    <li> A weekly trend which shows that the taxi fare are high especially the week end </li>\n    <li> A daily trend that show that the highest rates fare are near to 4 a.m</li>\n\n"},{"metadata":{"trusted":true,"_uuid":"8673a4807e29c6328b35102b47f84b7e18cdb3c3"},"cell_type":"code","source":"# Calculate root mean squared error.\nprint('RMSE: %f' % np.sqrt(np.mean((forecast_data.loc[:800, 'yhat']-prophet_df['y'])**2)) )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bb492dfe45d0a743da30ea96c86b800d7b4323ad"},"cell_type":"markdown","source":"The result seems predictable because FbProphet take only timestamp parameters so that it could be interesting to take also distance and number of passengers. "},{"metadata":{"_uuid":"5113637518f320914a3ec33375558a1ec2c65eb0"},"cell_type":"markdown","source":"## Train a linear model"},{"metadata":{"_uuid":"1c1582d1f8525825c0beb19fb3c2fed2f32fd8d3"},"cell_type":"markdown","source":"Our model will take the form  $X⋅w=y$  where  $X$  is a matrix of input features, and  $y$ is a column of the target variable, fare_amount, for each row. The weight column  $w$  is what we will \"learn\".\n\nFirst let's setup our input matrix $X$  and target column  $y$  from our training set. The matrix  $X$  should consist of the two GPS coordinate differences, plus a third term of 1 to allow the model to learn a constant bias term. \n\nIn a way, the column of 1s is a hack to extend the model to support a bias term.\n\nA simpler example is in 2D space, where $x∈ℝ$ is your \"input\" and $y∈ℝ$ is your \"target\". If you try to capture this relationship with a linear model of form $y=ax$, where $a∈ℝ$, your model could only be lines that pass through the origin (0,0) and you could not effectively capture most 2D relationships.\n\nHowever if you extend the model to have a second variable $b$ -- sometimes called the bias term -- say $y=ax+b$, then your model can be (almost) any 2D line, and the model can now capture most 2D linear relationships.\n\nNote that if we write $\\vec{x}=\\begin{pmatrix} x & 1 \\end{pmatrix}$ and $\\vec{w}=\\begin{pmatrix} a\\\\b \\end{pmatrix}$ then the following two models are equivalent:\n\n$$y=ax+b$$\n$$y=\\vec{x} \\cdot \\vec{w} $$\nSo adding the column of 1s to our inputs $\\vec{x}$  allows us to write the model in a more concise way (just $\\vec{w}$  instead of a and b), while still allowing the model (encoded by the $\\vec{x}$  column) to learn the additional bias term. The column  $y$  should consist of the target fare_amount values."},{"metadata":{"trusted":true,"_uuid":"bb10a31b5fdaaa08c0fceaf03205fdf1a9c2792e"},"cell_type":"code","source":"X = train[['distance_miles']]\ny = train[['fare_amount']]\nX['default'] = 1\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0bc85f125738be8c3aa90c4df235d310190aa1e6"},"cell_type":"markdown","source":"Training of Linear model using sklearn library : "},{"metadata":{"trusted":true,"_uuid":"4842b2c578b1a39c27867187e71191e6b6006084"},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nmodelRegression = LinearRegression(normalize=True)\nmodelRegression.fit(X,y)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c03ac347dc1e832074bd8e6643ea1bdd0b0d642a"},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import r2_score\npredictions = modelRegression.predict(X)\nprint(\"RMSE : \" + str(sqrt(mean_absolute_error(predictions, y))))\nprint(\"R2 : \" + str(r2_score(predictions, y)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7720a5bb12ab97bf2c883706bba083a47467f5cd"},"cell_type":"markdown","source":"The RMSE shows the strong correlation between distance and price. \n\nThe R-squared score is relatively close to 1. The nearer, the more correlated. This indicator shows the variation in relation to the regression line. Yet, we see the good relation between distance and price. "},{"metadata":{"_uuid":"d0aa582165b143e784879fb5c8825a7c7804f7d6"},"cell_type":"markdown","source":"Plot of our prediction : "},{"metadata":{"trusted":true,"_uuid":"a77d4e7eb52473e75f03b180497d3c067bf26b19"},"cell_type":"code","source":"plt.plot(X[['distance_miles']], predictions, 'r')\nplt.scatter(train.distance_miles, train.fare_amount, alpha=0.2)\nplt.xlabel('distance mile')\nplt.ylabel('fare $USD')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"48cdaa764a305cce5c4cf3e89a08cb0be0bc6c41"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"6418ac22b54e63c0b5639680916c5977e31ea10b"},"cell_type":"markdown","source":"## XGBoost model"},{"metadata":{"_uuid":"06d1e73e8f07054fc03e966f7c097df2f4e9580f"},"cell_type":"markdown","source":"XGBoost model is a well-known model on Kaggle competitions. It is composed both of decision trees and boosting and can give good accuracy. What we want do with this model is to predict the price of the course according to our variables. Before, we split the model within the training set and testing set. X are the data in  which we do the prediction and y what we want to predict (the fare amount)."},{"metadata":{"trusted":true,"_uuid":"3e212e3638dd93c6598a36e0d0c381632e8c5150"},"cell_type":"code","source":"train = time_features(train)\n\n\nfrom sklearn.model_selection import train_test_split\ny = train.fare_amount\nX = train.drop(['fare_amount', 'key', 'pickup_datetime'], axis=1)\ntrain_X, test_X, train_y, test_y = train_test_split(X.as_matrix(), y.as_matrix(), test_size=0.2)\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"608f6ff3c567af6b8f32d5eb537b97bbd8005f09"},"cell_type":"markdown","source":"For Kaggle competition : "},{"metadata":{"trusted":true,"_uuid":"6844140a0ed6134d8b11fabc87fdb978bf33ce12"},"cell_type":"code","source":"#train_y = train.fare_amount\n#train_X = train.drop(['fare_amount', 'key', 'pickup_datetime'], axis=1)\n#sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n#test_X = test.drop(['key', 'pickup_datetime' ], axis=1)\n#test_y = sample_submission.drop(['key'], axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9f7e5e72e69824208f350f0250fad6efd6e4399a"},"cell_type":"markdown","source":"Then, We created two fonctions. The first function is the creation of the model with specific parameters : \n* max_depth : the depth of the tree ;\n* nb_estimators is the numbers of trees ;\n* learning_rate is speed learning, it means mutiply the prediction of model before we add them together. It can reduce overfit ;\n* early_stopping_rounds : the number of rounds maximum before the error rise. It means if we have the minimum error for a round and after, the error rises, this parameter will stop the training to come back at the step when we had the minimum error.\n\nThe second function is the error returned by the first model, in which the mean square error is calculated. We want to reduce this value to the maximum. \n"},{"metadata":{"trusted":true,"_uuid":"d1811650a16372528c05c4ea619f5e40e2efe19f"},"cell_type":"code","source":"from xgboost import XGBRegressor\nfrom math import sqrt\n\ndef XGBoost(train_X, test_X, train_y, test_y, max_depth, nb_estimators, learning_rate, early_stopping_rounds):\n    model = XGBRegressor(max_depth = max_depth, nb_estimators = nb_estimators, learning_rate = learning_rate)\n    model.fit(train_X, train_y, early_stopping_rounds = early_stopping_rounds , eval_set=[(test_X, test_y)], verbose=False)\n    return model\n\ndef errorXGBoost(model, test_X, test_y):\n    predictions = model.predict(test_X)\n    return str(mean_absolute_error(predictions, test_y))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3cc8e7631e245f60e28e184bb4c82ba105da85c9"},"cell_type":"markdown","source":"Mean square error has been chosen because it gives score in the square fare amount.\nSome tests has been done with different parameters. Max depth and the number of estimators are the most important parameters in XGBoost. It has been changed many times to approximate a good score, be robust, trying to not be in overfitting. "},{"metadata":{"trusted":true,"_uuid":"7b000391ad7878639d21a8f99125522072e9f0c8"},"cell_type":"code","source":"\nmodel = XGBoost(train_X, test_X, train_y, test_y, 5, 500, 0.05, 5)\nprint(errorXGBoost(model, test_X, test_y))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"85c95a2d46dd760508f140f6afe6af3ba6a5437d"},"cell_type":"markdown","source":"Submission to Kaggle competition : "},{"metadata":{"trusted":true,"_uuid":"99e0aa4375b548d7508555c9336d584ca3f57299"},"cell_type":"code","source":"test = pd.read_csv(\"../input/test.csv\")\ntest['distance_miles'] = distance(test.pickup_latitude, test.pickup_longitude, \\\n                                      test.dropoff_latitude, test.dropoff_longitude)\n\ntest = time_features(test)\ntest = test.drop(['key', 'pickup_datetime' ], axis=1)\ntest= test.as_matrix()\n\nprediction = model.predict(test)\ntest = pd.read_csv(\"../input/test.csv\")\nholdout = pd.DataFrame({'key': test['key'], 'fare_amount': prediction})\nholdout.to_csv('predictionTest.csv', index=False)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"298a8321db81ad3731a1012a8c2603f8e5f69d2d"},"cell_type":"markdown","source":"According to Kaggle competition, we have a RSE of 6.05 $. "}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}