{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style=\"color:blue;\"> Problem Definition: </h2>\n\n**Predicting the fare amount for a taxi ride in New York City with given the pickup and dropoff locations details.**\n\n![image.png](https://storage.googleapis.com/kaggle-competitions/kaggle/10170/logos/header.png?t=2018-07-12-22-07-30)\n\n<h2 style=\"color:blue;\"> Data Fields: </h2>\n\n##### <u>ID:</u>\n**key** - Unique string identifying each row in both the training and test sets. Comprised of pickup_datetime plus a unique integer, but this doesn't matter, it should just be used as a unique ID field. \n\n##### <u>Features:</u>\n**pickup_datetime** - timestamp value indicating when the taxi ride started.<br>\n**pickup_longitude** - float for longitude coordinate of where the taxi ride started.<br>\n**pickup_latitude** - float for latitude coordinate of where the taxi ride started.<br>\n**dropoff_longitude** - float for longitude coordinate of where the taxi ride ended.<br>\n**dropoff_latitude** - float for latitude coordinate of where the taxi ride ended.<br>\n**passenger_count** - integer indicating the number of passengers in the taxi ride.<br>\n\n##### <u>Target:</u>\n**fare_amount** - float dollar amount of the cost of the taxi ride. This value is only in the training set; this is what you are predicting in the test set and it is required in your submission CSV.\n\n<h2 style=\"color:blue;\">Problem type: </h2>\n\n**This a supervised regression problem**. We can try following most popular Machine learning regression algorithm to solve our usecase.\n\n1. Linear Regression\n2. Ridge Regression\n3. Neural Network Regression \n4. Lasso Regression \n5. Decision Tree Regression \n6. Random Forest\n7. KNN Model \n8. Support Vector Machines (SVM)\n\n<h2 style=\"color:blue;\">Hypothesis: </h2>\n\n**Let's consider following hypothesis test case, which is impacting fare amount of a taxi trip in New York**\n+ **Distance:** There will be a <u>linear relationship between Fare amount & trip distance</u> (Positive correlation).\n+ **Location Specific:** Based on Pickup and Dropoff location the fare amount will vary.\n+ **Booking time:** The fare amount will be high during the pick hours compare to normal hours.\n+ **Late Night charge:** There will be additional fare changing for late night trip bookings.\n+ **Day of travel:** Fare amount will be differ on weekdays & weekends for the same distance.\n+ **Weather condition:** Based on season the cabs booking count will be differ.\n+ **Hotsport location:** The number of booking for hotsport locations like Hotel, Airpot will be high.\n+ **Passenger count:** Passenger count deside the car size and fare amount.\n+ **Demand of bookings:** Based on demand of taxi availability in perticular area, the fare amount will also increase.","metadata":{}},{"cell_type":"code","source":"# Including the necessary python libraries\n\n# Data manipulation\nimport pandas as pd\nimport calendar\n\n# Math calculations\nimport numpy as np\n\n# Visualization \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Geograpical visualization\nimport folium\nfrom folium import plugins\nfrom folium.plugins import MeasureControl\nfrom folium.plugins import HeatMap\n\n# For math calculations\nfrom math import sin, cos, sqrt, atan2, radians\n\n# Stats\nfrom scipy import stats\nfrom scipy.stats import norm, skew\n\n# Warnings\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style=\"color:black;\"><b>1. Reading data: </b></h2>","metadata":{}},{"cell_type":"code","source":"# Reading the train dataset from local memory using pandas read csv menthod and store them in the form of dataframe\n\ndataset = pd.read_csv('../input/new-york-city-taxi-fare-prediction/train.csv', nrows = 500000, parse_dates = [\"pickup_datetime\"])\ndataset.sample(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv('../input/new-york-city-taxi-fare-prediction/test.csv')\nprint(\"Number of datapoints in test file\", test.shape[0])\ntest.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style=\"color:black;\"><b>2. Dataset Investigation: </b></h2>","metadata":{}},{"cell_type":"code","source":"# Total observation in dataset\n\ndataset.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Insights:\nWe have taken 500000 taxi booking details as a sample data from population and each booking data represented with 8 features.","metadata":{}},{"cell_type":"code","source":"# Dataset Features Information\n\ndataset.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Insights:** <br>\n+ We have **8 features in our dataset**, In which 7 are Indipendent feature and 1 Dependent feature.\n+ **Independent features:** key, pickup_datetime, pickup_longitude, pickup_latitude, dropoff_longitude, dropoff_latitude, passenger_count.\n+ **Dependent feature:** fare_amount.\n+ The features are aleady in proper datatype. So we don't need to do any datatype conversion in data cleaning phase.\n+ There are **5 missing values in dropoff geo coodinates**.","metadata":{}},{"cell_type":"code","source":"# Basic statistics about numerical features in the dataset\n\ndataset.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Insights:\n+ The **average taxi fare amount is $11 Dollers**.\n+ There are few datapoints contain negative fare amounts, This could be outliers.\n+ Fare amount data distribution is **right skewed**.\n+ We cannot infer more details from latitude & longitude coordinates. But we can say there are few outliers in it.\n+ The maximum Pickup longitude is **2140.6011** & minimum longitude is **-2986.242495**, Ideally the valid longitude range between **-180 <= longitude <= 180**. \n+ The maximum Pickup latitude is **1703.092772** & minimum longitude is **-3116.285383**, Ideally the valid latitude range between **-90 <= latitude <= 90**.  \n+ The maximum Dropoff latitude is **404.616667** & minimum longitude is **-2559.748913**, Ideally the valid latitude range between **-90 <= latitude <= 90**. \n+ There are few datapoints with **zero passenger count**. In sometime we use to book taxi for goods transfer, So we cannot directly say these are outliers. But we can check the test datapoints with zero passenger count or not.\n+ More number of booking is done for single passenger. ","metadata":{}},{"cell_type":"code","source":"# Basic statistics about categorical features in the dataset\n\ndataset.describe( include = np.object )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Insights:\nKey feature will **identify unique datapoint in the dataset**, becuase the frequency count is 1.","metadata":{}},{"cell_type":"markdown","source":"<h2 style=\"color:black;\"><b>3. Data cleaning & preprocessing: </b></h2>","metadata":{}},{"cell_type":"code","source":"# Replicate the dataset and make our changes only in copied dataset\n\ndf1 = dataset.copy( deep = True )\ndf1.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Rounding the Geographical Coorinate upto 4 decimal place\n\ndf1.pickup_longitude  = round(df1.pickup_longitude.astype(float),4)\ndf1.pickup_latitude   = round(df1.pickup_latitude.astype(float),4)\ndf1.dropoff_longitude = round(df1.dropoff_longitude.astype(float),4)\ndf1.dropoff_latitude  = round(df1.dropoff_latitude.astype(float),4)\ndf1.sample()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reduce our latitude & longitude scope with respect to test dataset.\n\n# First we will check the max & min geographical coodinates in test dataset.\n# Test: Latitude Minimum & Maximum\ntest_lat_min = min(test.pickup_latitude.min(), test.dropoff_latitude.min())\ntest_lat_max = max(test.pickup_latitude.max(), test.dropoff_latitude.max())\n\n# Train: Latitude Minimum & Maximum\ntrain_lat_min = min(dataset.pickup_latitude.min(), dataset.dropoff_latitude.min())\ntrain_lat_max = max(dataset.pickup_latitude.max(), dataset.dropoff_latitude.max())\n\nprint(\">>> Minimum Latitude in test: {}, Maximum Latitude in test: {}\".format(test_lat_min, test_lat_max))\nprint(\">>> Minimum Latitude in train: {}, Maximum Latitude in test: {}\".format(train_lat_min, train_lat_max))\n\n# Test: Longitude Minimum & Maximum\ntest_lon_min = min(test.pickup_longitude.min(), test.dropoff_longitude.min())\ntest_lon_max = min(test.pickup_longitude.max(), test.dropoff_longitude.max())\n\n# Train: Longitude Minimum & Maximum\ntrain_lon_min = min(dataset.pickup_longitude.min(), dataset.dropoff_longitude.min())\ntrain_lon_max = max(dataset.pickup_longitude.max(), dataset.dropoff_longitude.max())\n\nprint(\">>> Minimum Longitude in test: {}, Maximum Longitude in test: {}\".format(test_lon_min, test_lon_max))\nprint(\">>> Minimum Longitude in train: {}, Maximum Longitude in test: {}\".format(train_lon_min, train_lon_max))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Insights:\n+ There is huge difference in train & test Geogrphical coordinate points.\n+ So we can focus more on test region boundary in train datasets. \n+ **Boundary box is defined by test datapoints and focusing only those geographical coordinates in train data points.**","metadata":{}},{"cell_type":"code","source":"# Eliminate the datapoints from train dataset, In which geographical coordinates out of boundary \n# Boundary is decided based on test dataset\n\n# Defining method \ndef geographical_boundary(data):\n    return (data[ (data.pickup_latitude  >= test_lat_min)  & (data.pickup_latitude <= test_lat_max) &\n                  (data.dropoff_latitude >= test_lat_min)  & (data.dropoff_latitude <= test_lat_max) &\n                  (data.pickup_longitude >= test_lon_min)  & (data.pickup_longitude <= test_lon_max) & \n                  (data.dropoff_longitude >= test_lon_min) & (data.dropoff_longitude <= test_lon_max) ])\n\n# Invoking method\ndf1 = geographical_boundary(df1)\nprint(\"Number of datapoint remaining after deletion : \",df1.shape[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Statistical analysis for pickup geographical coordinate outlier detection \n# Checking both pickup latitude & longitude  \n\n#--> Statistics using describe method\nprint(\"------------------------------------------------------\\n| Statistical Data about pickup latitude & longitude |\\n------------------------------------------------------\")\nprint(df1[['pickup_latitude', 'pickup_longitude']].describe(percentiles = [.25,.50,.75,.95]))\n\n# Finding IQR and to check number of outliers with respect Latitude\nP_Q1 = df1.pickup_latitude.quantile(0.25)\nP_Q3 = df1.pickup_latitude.quantile(0.75)\nP_IQR = P_Q3 - P_Q1\nlat_out = df1[(df1.pickup_latitude < (P_Q1 - 1.5 * P_IQR)) | (df1.pickup_latitude > (P_Q3 + 1.5 * P_IQR))].shape[0]\nprint(\"\\n>>> Number of outlier records only in pickup latitude: \",lat_out)\n\n# Finding IQR and to check number of outliers with respect Longitude\np_q1 = df1.pickup_longitude.quantile(0.25)\np_q3 = df1.pickup_longitude.quantile(0.75)\np_iqr = p_q3 - p_q1\nlon_out = df1[(df1.pickup_longitude < (p_q1 - 1.5 * p_iqr)) | (df1.pickup_longitude > (p_q3 + 1.5 * p_iqr))].shape[0]\nprint(\">>> Number of outlier records only in pickup longitude: \",lon_out)\n\n# Finding list of records for outlier either in latitude or longitude \noutlier = df1[(df1.pickup_latitude < (P_Q1 - 1.5 * P_IQR)) | \n              (df1.pickup_latitude > (P_Q3 + 1.5 * P_IQR)) |\n              (df1.pickup_longitude < (p_q1 - 1.5 * p_iqr))|\n              (df1.pickup_longitude > (p_q3 + 1.5 * p_iqr)) ]\n\nprint(\">>> Number of pickup geographical coordinate outlier records comparing Latitude & Longitude: \",outlier.shape[0])\n\nfig = plt.figure(figsize=(16,2))\n# Histogram\nplt.subplot(121)\nsns.boxplot(df1.pickup_latitude).set_title(\"Boxplot for Pickup latetude outlier detection\", size = 11)\n# Boxplot\nplt.subplot(122)\nsns.boxplot(df1.pickup_longitude).set_title(\"Boxplot for Pickup longitude outlier detection\", size = 11)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Insights:\n+ Most of the Pickup latitude location cooridinates between **40.73 to 40.83**. There are **15063 outlier entries** only with respect to Pickup latitude.\n+ Most of the Pickup longitude location coordinates between **-74.05 to -73.96**. There are **24295 outlier entries** only with respect to Pickup longitude.\n+ When we **join both Pickup latitude & longitude outlier datapoints** then it leeds to **30716 outlier datapoints**.","metadata":{}},{"cell_type":"code","source":"# Statistical analysis for Dropoff geographical coordinate outlier detection \n# Checking both dropoff latitude & longitude  \n\n#--> Statistics using describe method\nprint(\"-------------------------------------------------------\\n| Statistical Data about dropoff latitude & longitude |\\n-------------------------------------------------------\")\nprint(df1[['dropoff_latitude', 'dropoff_longitude']].describe(percentiles = [.25,.50,.75,.95]))\n\n# Finding IQR and to check number of outliers with respect Latitude\nD_Q1 = df1.dropoff_latitude.quantile(0.25)\nD_Q3 = df1.dropoff_latitude.quantile(0.75)\nD_IQR = D_Q3 - D_Q1\nlat_out = df1[(df1.dropoff_latitude < (D_Q1 - 1.5 * D_IQR)) | (df1.dropoff_latitude > (D_Q3 + 1.5 * D_IQR))].shape[0]\nprint(\"\\n>>> Number of outlier records only in dropoff latitude: \",lat_out)\n\n# Finding IQR and to check number of outliers with respect Longitude\nd_q1 = df1.dropoff_longitude.quantile(0.25)\nd_q3 = df1.dropoff_longitude.quantile(0.75)\nd_iqr = d_q3 - d_q1\nlon_out = df1[(df1.dropoff_longitude < (d_q1 - 1.5 * d_iqr)) | (df1.dropoff_longitude > (d_q3 + 1.5 * d_iqr))].shape[0]\nprint(\">>> Number of outlier records only in dropoff longitude: \",lon_out)\n\n# Finding list of records for outlier either in latitude or longitude \noutlier = df1[(df1.dropoff_latitude < (D_Q1 - 1.5 * D_IQR)) | \n              (df1.dropoff_latitude > (D_Q3 + 1.5 * D_IQR)) |\n              (df1.dropoff_longitude < (d_q1 - 1.5 * d_iqr))|\n              (df1.dropoff_longitude > (d_q3 + 1.5 * d_iqr)) ]\n\nprint(\">>> Number of dropoff geo coordinate outlier records: \",outlier.shape[0])\n\nfig = plt.figure(figsize=(16,2))\n# Histogram\nplt.subplot(121)\nsns.boxplot(df1.dropoff_latitude).set_title(\"Boxplot for dropoff latetude outlier detection\", size = 11)\n# Boxplot\nplt.subplot(122)\nsns.boxplot(df1.dropoff_longitude).set_title(\"Boxplot for dropoff longitude outlier detection\", size = 11)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Insights:\n+ Most of the Dropoff latitude location cooridinates between **40.73 to 40.83**. There are **22487 outlier entries** only with respect to Dropoff latitude.\n+ Most of the Dropoff longitude location coordinates between **-74.08 to -73.95**. There are **27540 outlier entries** only with respect to Dropoff longitude.\n+ When we **join both Dropoff latitude & longitude outlier datapoints** then it leeds to **41907 outlier datapoints**.","metadata":{}},{"cell_type":"code","source":"# Scatter Plot for trip location spread in Test & Train\n\n# Boundary Box\nboundary = (test_lon_min, test_lon_max, test_lat_min, test_lat_max)\n\n# Method for create scatter plot \ndef Scatter_plot(train, test, boundary):\n    fig, axis = plt.subplots(2, 2, figsize = (16, 10))\n    # Pickup outlier condition in train\n    ptrain_condition = [ (train.pickup_latitude < (P_Q1 - 1.5 * P_IQR)) | \n                         (train.pickup_latitude > (P_Q3 + 1.5 * P_IQR)) |\n                         (train.pickup_longitude < (p_q1 - 1.5 * p_iqr))|\n                         (train.pickup_longitude > (p_q3 + 1.5 * p_iqr))]\n    # Dropoff outlier condition in train\n    dtrain_condition = [ (train.dropoff_latitude < (D_Q1 - 1.5 * D_IQR))  | \n                         (train.dropoff_latitude > (D_Q3 + 1.5 * D_IQR))  |\n                         (train.dropoff_longitude < (d_q1 - 1.5 * d_iqr)) |\n                         (train.dropoff_longitude > (d_q3 + 1.5 * d_iqr)) ]\n    # Pickup outlier condition in test\n    ptest_condition = [ (test.pickup_latitude < (P_Q1 - 1.5 * P_IQR)) | \n                        (test.pickup_latitude > (P_Q3 + 1.5 * P_IQR)) |\n                        (test.pickup_longitude < (p_q1 - 1.5 * p_iqr))|\n                        (test.pickup_longitude > (p_q3 + 1.5 * p_iqr))]\n    # Dropoff outlier condition in test\n    dtest_condition = [ (test.dropoff_latitude < (D_Q1 - 1.5 * D_IQR))  | \n                        (test.dropoff_latitude > (D_Q3 + 1.5 * D_IQR))  |\n                        (test.dropoff_longitude < (d_q1 - 1.5 * d_iqr)) |\n                        (test.dropoff_longitude > (d_q3 + 1.5 * d_iqr)) ]\n    # Pickup location in train dataset\n    plt.subplot(221)\n    train['out_detection'] = np.select(ptrain_condition, ['outlier'], default = 'non-outlier')\n    sns.scatterplot(train.pickup_longitude, train.pickup_latitude, hue = train.out_detection).set_title(\"Pickup datapoints in Train\")\n    \n    # Dropoff location in train dataset\n    plt.subplot(222)\n    train['out_detection'] = np.select(dtrain_condition, ['outlier'], default = 'non-outlier')\n    sns.scatterplot(train.dropoff_longitude, train.dropoff_latitude, hue = train.out_detection).set_title(\"Dropoff datapoints in Train\")\n    \n    # Pickup location in test dataset\n    plt.subplot(223)\n    test['out_detection'] = np.select(ptest_condition, ['outlier'], default = 'non-outlier')\n    sns.scatterplot(test.pickup_longitude, test.pickup_latitude, hue = test.out_detection).set_title(\"Pickup datapoints in Test\")\n    \n    # Pickup location in train dataset\n    plt.subplot(224)\n    test['out_detection'] = np.select(dtest_condition, ['outlier'], default = 'non-outlier')\n    sns.scatterplot(test.dropoff_longitude, test.dropoff_latitude, hue = test.out_detection).set_title(\"Dropoff datapoints in Test\")\n    plt.show()\n\n# Invoking the method call \nScatter_plot(df1, test, boundary)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Insights:\n+ The **test dataset also has outliers**.\n+ The IQR value of train dataset is used to find the outliers of test dataset as well.\n+ When we **remove the outliers from train dataset further then the model will not be more generalized one for new datapoints**.\n+ For initial model building we can keep outlier, base on model accuracy we can decide, whether we have to reimpute or not.","metadata":{}},{"cell_type":"code","source":"# Statistics for fare amount feature\n# Finding IQR and to check number of outliers with respect to fare amount\nprint(\"--------------------------------------\\n| Statistical data about Fare amount |\\n--------------------------------------\")\nprint(df1.fare_amount.describe(percentiles = [.25, .50, .75, .95]))\n\n#--> IQR calculation\nQ1 = df1.fare_amount.quantile(0.25)\nQ3 = df1.fare_amount.quantile(0.75)\nIQR = Q3 - Q1\n\n#--> Checking outliers\nout_fare = df1[(df1.fare_amount < (Q1 - 1.5 * IQR)) | (df1.fare_amount > (Q3 + 1.5 * IQR))]\nprint(\"\\n=> Number of outlier records: \",out_fare.shape[0])\n\nfar_condition = [(df1.fare_amount < (Q1 - 1.5 * IQR)) | (df1.fare_amount > (Q3 + 1.5 * IQR))]\ndf1['fare_out'] = np.select(far_condition, ['outlier'], default = 'non-outlier')\n\nfig = plt.figure(figsize=(15,10))\n# Histogram\nplt.subplot(211)\nsns.histplot(df1.fare_amount, kde = True).set_title('fare_amount data distribution', size = 11)\n# Boxplot\nplt.subplot(212)\nsns.boxplot(df1.fare_amount).set_title(\"Boxplot for fare_amount outlier detection\", size = 11)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Insights:\n+ The dataset contain **42202 outlier datapoints with respect to fare amount**.\n+ There are **few negative fare amount datapoints in train dataset**. We have to check those entries and remove them from dataset.\n+ The average taxi fare amount is **$11.3 Dollars**.\n+ We can **treat the fare amount outliers by considering the trip distance**. But this will be done only after calculating the distance feature in feature engineering phase.","metadata":{}},{"cell_type":"code","source":"# Checking the negative fare amount datapoints\n\nnegative_fare = df1[ df1.fare_amount <= 0 ]\nprint(\"The number of datapoints contain negative fare amount: \",len(negative_fare[negative_fare.fare_amount < 0]))\nprint(\"The number of datapoints contain zero fare amount: \",len(negative_fare[negative_fare.fare_amount == 0]))\nnegative_fare.sample(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Insights:\n+ We have around **20 negative fare amount datapoints & 13 zero fare amount in train dataset**.\n+ Few entries fare amount is zero, this could be due to some special offer given to the customer.\n+ We can either simply remove here datapoints or based on trip distance we can apply mean fare amount. But we will recalculate the fare amount from trip distance.# Checking outlier in Passenger count feature","metadata":{}},{"cell_type":"code","source":"# Checking unique pasenger count with its frequency in dataset\n\ndf1.passenger_count.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Assemption:\n+ **Most of the booking is done for 1 passenger and maximum passenger count is 6**.\n+ Surprisingly there are **1754 datapoints with zero passenger count**. But this case is possible, because we can book taxi for goods transfer. \n+ We can check value count in test dataset, If we have any entry with zero passenger then we have to consider those datapoints as well. Otherwise we can delete 1754 datapoints.","metadata":{}},{"cell_type":"code","source":"# Checking the test dataset passenger count\n\ntest.passenger_count.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Insights:\n+ Since we don't have any datapoints with zero passenger count.\n+ Not it is completly optional for us to keep the datapoints with zero passenger count or delete it.","metadata":{}},{"cell_type":"code","source":"# Filter the zero passenger count first and check the following conditions:\n# we have to focus on location coordinate geographical outliers compare to fare amount. \n# Because we easly recalculate the fare amount with proper non outlier location coordinate points.\n# we are considering location coordinate outlier for deletion.\n\ndf1.drop(df1[(df1.passenger_count == 0) & ( df1.out_detection == 'outlier')].index, inplace = True)\n\nprint(\"Number of datapoint remaining after deletion : \",df1.shape[0])\n\n# Assigning passenger count as 1 for remaining non outlier entries. Because one is most frequent in passsenger count\ndf1['passenger_count'] = df1['passenger_count'].apply( lambda x : 1 if x == 0 else x )\nprint(df1.passenger_count.value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Validating whether duplicate entries present or not \n\nduplicate = df1[df1.duplicated()]\nduplicate","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Insights:\nThere is **no duplicate entries** in train dataset.","metadata":{}},{"cell_type":"code","source":"# Checking is there any null values or not\n\ndf1.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Insights:\nThere is **no missing values** in train dataset.","metadata":{}},{"cell_type":"code","source":"# Seperate the timestamp into date, day, hour, month, year \n# There new features will be treated as dummay and this seperated features will be helpful in EDA\n\n# Date\ndf1['pickup_date']  = df1['pickup_datetime'].dt.date\n# Day\ndf1['pickup_day']   = df1['pickup_datetime'].apply(lambda x : calendar.day_name[x.weekday()])\n# Hour\ndf1['pickup_hour']  = df1['pickup_datetime'].apply(lambda x : x.hour).astype(int)\n# Month\ndf1['pickup_month'] = df1['pickup_datetime'].apply(lambda x : x.month).astype(int)\n# Year\ndf1['pickup_year']  = df1['pickup_datetime'].apply(lambda x : x.year).astype(int)\n# Weekend or Weekday\ndf1['pickup_on']    = df1['pickup_day'].apply(lambda x : 'Weekend' if x == 'Saturday' or x == 'Sunday' else 'Weekday')           \n\ndf1.sample(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Insights:\n**We have added following new feature from datetime feature**\n+ **pickup_date:** Pickup date in the form of [YYYY-MM-DD].\n+ **pickup_day:** Calender day of the pickup date.\n+ **pickup_hour:** Pickup hour.\n+ **pickup_month:** Pickup month.\n+ **pickup_on:** Booked in weekdays or weekend.","metadata":{}},{"cell_type":"markdown","source":"<h2 style=\"color:black;\"><b>4. Exploratory Data Analysis: </b></h2>","metadata":{}},{"cell_type":"code","source":"# Booking location density plot using Folium Heatmap \n# Reference: https://towardsdatascience.com/data-101s-spatial-visualizations-and-analysis-in-python-with-folium-39730da2adf\n\n# Data preparation: Combining pickup & drop details into single column\ndf_pickup = df1[['pickup_latitude', 'pickup_longitude']].copy().rename(columns = {'pickup_latitude' : 'latitude', \n                                                                                  'pickup_longitude' : 'longitude'})\ndf_dropoff = df1[['dropoff_latitude', 'dropoff_longitude']].copy().rename(columns = {'dropoff_latitude' : 'latitude',\n                                                                                     'dropoff_longitude' : 'longitude'})\ndf_pickup = df_pickup.append(df_dropoff)        \ndf_pickup['count'] = 1\n\n# Map instance creation\nnew_york = folium.Map(location=[40.693943, -73.985880], control_scale=True, zoom_start=9)\nnew_york.add_child(MeasureControl())\n# Apply heatmap on top of map instance\nHeatMap(data=df_pickup[['latitude', 'longitude', 'count']].groupby(['latitude', 'longitude']).sum().reset_index().values.tolist(), radius=8, max_zoom=13).add_to(new_york)\nnew_york","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Insights:\n+ Most of the trips are surrounded by **Manhattan City**.\n+ There are few trips pointing in ocean geo space. These must be outliers.","metadata":{}},{"cell_type":"code","source":"# Checking data distribution for fare_data feature\n\nfig = plt.figure(figsize=(15, 5))\nsns.histplot(df1.fare_amount, kde = True).set_title('fare_amount data distribution', size = 15)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Insights:\n+ The dataset contain **42202 outlier datapoints with respect to fare amount**.\n+ There are **few negative fare amount datapoints in train dataset**. We have to check those entries and remove them from dataset.\n+ The average taxi fare amount is **$11.3 Dollars**.\n+ We can **treat the fare amount outliers by considering the trip distance**. But this will be done only after calculating the distance feature in feature engineering phase.","metadata":{}},{"cell_type":"code","source":"# Checking data distribution for passenger_count data\n\nfig, ax = plt.subplots(figsize = (10,5))\nsns.countplot(df1.passenger_count, ax = ax)\nax.set_title('passenger_count data analysis', size = 16)\nax.set_xlabel('Passenger count', size = 12)\nax.set_ylabel('Count', size = 12)\nax.grid(axis='y')\nfor p in ax.patches:\n    ax.annotate('{:.1f}%'.format( (p.get_height() / df1.shape[0]) * 100 ), (p.get_x()+0.2, p.get_height()+55))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Insights:\n+ Around **70% of booking is done for single passenger**.\n+ Passenger count five and six might have booked large seat capacity cabs, so it is obvious to have high fare.\n+ We can prove this after computing distance because this is one of our hypothesis test case.","metadata":{}},{"cell_type":"code","source":"# Finding average fare amount with respect to passenger count\n\ndf1.groupby('passenger_count')['fare_amount'].mean().plot(kind='bar')\nplt.title(\"Average fare amount VS Passenger count\")\nplt.xlabel(\"Passenger count\")\nplt.ylabel(\"Avg. fare amount\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Assemption:\n+ **Hypothesis:** Based on passenger count the fare will increase..\n+ But **average fare amount of passenger count 6 is high compare to all others**. This is strong evdient but in other hand average fare amount will not that much high comparitively for passenger count 3,4 & 5.\n+ After Calculating distance feature, we can again test this hypothesis.","metadata":{}},{"cell_type":"code","source":"# Year wise taxi booking count\n\nfig, ax = plt.subplots(figsize = (10,5))\nsns.countplot(df1.pickup_year, ax = ax)\nax.set_title('Year wise taxi booking count', size = 16)\nax.set_xlabel('Year', size = 12)\nax.set_ylabel('Count', size = 12)\nfor p in ax.patches:\n    ax.annotate('{:}'.format(p.get_height()) , (p.get_x()+0.2, p.get_height()+55))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Insights:\n+ Till 2012 the taxi booking rate per year is linearly increasing except 2010 year.\n+ Suddenly the taxi booking rate is decrease in 2013 & 2014 years.\n+ Surprisingly in **2015 the rate of booking is reduced half the rate**. \n+ This is because of dataset generated middle of 2015 or actul number of booking itself half the rate compare to previous year.","metadata":{}},{"cell_type":"code","source":"# Explore further why we have only 3389 bookings in 2015\n\n#--> Fetch the booking details of 2015 and check we have observation for all the moth or not ?\ndf1.query(\"pickup_year == 2015\")['pickup_month'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Insights:\n+ So it is clear that, **dataset contain 2015's booking details only for first 6 months**. \n+ Because of this we have very less booking details compare to previous year.","metadata":{}},{"cell_type":"code","source":"# Moth wise booking count with respect to year\n\nfig, ax = plt.subplots(figsize = (20,5))\nsns.countplot(df1.pickup_month, hue = df1.pickup_year, ax = ax)\nax.set_title('Month wise taxi booking count', size = 16)\nax.set_xlabel('Month', size = 15)\nax.set_ylabel('Count', size = 15)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Insights:\n+ Except few months, **the booking count is uniform** for all of the months of different year.\n+ Year of 2015 contain very less data points, due to which booking count will be very low.","metadata":{}},{"cell_type":"code","source":"# Grouping monthly booking count with respect to year wise\n\n\n#--> Creating group by table\ndf1['count'] = 1\nmonth_group = pd.DataFrame(df1.groupby(['pickup_month', 'pickup_year'])['count'].count()).reset_index()\nmonth_group = month_group.pivot(\"pickup_month\", \"pickup_year\", \"count\")\n\n#--> Ploting \nfig, ax = plt.subplots(figsize = (18,7))\nsns.lineplot(data = month_group, markers = True, dashes=False, ax = ax)\nax.set_title('Month wise taxi booking count with respect to year', size = 16)\nax.set_xlabel('Month', size = 15)\nax.set_ylabel('Count', size = 15)\nax.grid(axis='x')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Insights:\n+ The **maximum trips booked at March 2012** and **minimum trips booked at Augest 2010**. \n+ **Monthwise booking count distribution is more or less following same distribution**. The **reason could be weather condition**, so we can explore booking count average with weather season.","metadata":{}},{"cell_type":"code","source":"# Reading season details \n# Reference: https://www.nyc.com/visitor_guide/weather_facts.75835/\n\n# Creating Dataframe for Season details\ndata = [['September','FallSeason'], ['October','FallSeason'], ['November','FallSeason'], \n        ['December','WinterSeason'], ['January','WinterSeason'], ['February','WinterSeason'],\n        ['March','SpringSeason'], ['April','SpringSeason'],  ['May','SpringSeason'],\n        ['June','SummerSeason'], ['July','SummerSeason'], ['August','SummerSeason']]\n\n# Dataframe\nseason_tab = pd.DataFrame(data, columns = ['month', 'season'])\n\n# First preprocessing the month name to respective month number in season data\nseason_tab['month'] = season_tab.apply(lambda x : list(calendar.month_name).index(x.month), axis =1)\n\n# Adding new column for Season detail in our dataframe\ndf1['season'] = df1.pickup_month.replace(dict(zip(season_tab['month'],season_tab['season'])))\ndf1.season.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Insights:\n+ **FallSeason Months:** September, October & November.\n+ **WinterSeason:** December, January & Februray\n+ **SpringSeason:** March, April & May\n+ **SummarSeason:** June, July & August","metadata":{}},{"cell_type":"code","source":"# Grouping Season booking count with respect to year wise\n\n#--> Creating group by table\n# Since we have very few entries in 2015, we are eliminating for time being\ntemp = df1[~ ( df1.pickup_year == 2015 )]\ntemp['count'] = 1\nmonth_group = pd.DataFrame(temp.groupby(['season', 'pickup_year'])['count'].count()).reset_index()\nmonth_group = month_group.pivot(\"season\", \"pickup_year\", \"count\")\n\n#--> Ploting \nfig, ax = plt.subplots(figsize = (18,7))\nsns.lineplot(data = month_group, markers = True, dashes=False, ax = ax)\nax.set_title('Season wise taxi booking count with respect to year', size = 16)\nax.set_xlabel('Season', size = 15)\nax.set_ylabel('Count', size = 15)\nax.grid(axis='x')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Insights:\n+ **In Summer season, the booking count will decrease** as compare to previous season booking count in all the years.\n+ The **more number of booking is done during the spring seasons**.\n+ We can cannot predict the Winter season, because every year it is getting vary.\n+ 2012 is a best year, becuase booking count is very high compare to all other years.","metadata":{}},{"cell_type":"code","source":"# Hour wise booking count \n\nfig, ax = plt.subplots(figsize = (18,5))\nsns.countplot(df1.pickup_hour, ax = ax)\nax.set_title('Hour wise taxi booking count', size = 16)\nax.set_xlabel('Hour', size = 12)\nax.set_ylabel('Count', size = 12)\nfor p in ax.patches:\n    ax.annotate('{:}'.format(p.get_height()) , (p.get_x()+0.09, p.get_height()+55))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Insights:\n+ From mid night to **early Morning** (12 AM to 7AM) the **booking rate will be gardually reducing** and reaching very low value.\n+ From 8 AM to 4 PM the the booking rate will be average.\n+ **The maximum booking are done between 6 PM to 8 PM**.\n+ late Evening to mind Night the booking count will be above average.","metadata":{}},{"cell_type":"code","source":"# Grouping hourly booking count with respect to year wise\n\n#--> Creating Groupby table\nhour_group = pd.DataFrame(df1.groupby(['pickup_hour', 'pickup_year'])['count'].count()).reset_index()\nhour_group = hour_group.pivot(\"pickup_hour\", \"pickup_year\", \"count\")\n\n#--> Ploting\nfig, ax = plt.subplots(figsize = (18,5))\nsns.lineplot(data = hour_group, dashes=False, ax = ax)\nax.set_title('Hour wise taxi booking count with respect to year', size = 16)\nax.set_xlabel('Hour', size = 15)\nax.set_ylabel('Count', size = 15)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Insights:\n+ The **hour wise booking count distribution will be more or less same for all the years**.\n+ Early Morning the number of booking will be very low.\n+ There are large number of bookings between 18 to 20 Hours.\n+ Since we don't have sufficient datapoints for 2015, it is looks differnt from all other years.","metadata":{}},{"cell_type":"code","source":"# Exploring time based heatmap for taxi booking \n\n# Creating map instance\nnew_york = folium.Map(location=[40.693943, -73.985880], control_scale=True, zoom_start=12)\nheat_df = df1[['pickup_latitude', 'pickup_longitude']]\n\n# Create weight column, using date\nheat_df['Weight'] = df1['pickup_hour']\nheat_df = heat_df.dropna(axis=0, subset=['pickup_latitude','pickup_longitude', 'Weight'])\n\n# List comprehension to make out list of lists\nheat_data = [[[row['pickup_latitude'],row['pickup_longitude']] for index, row in heat_df[heat_df['Weight'] == i].iterrows()] for i in range(0,24)]\n\n#create superhero markers and add them to map object\nfolium.Marker([40.6441666667, -73.7822222222], tooltip=\"John F. Kennedy International Airport (YFK)\").add_to(new_york)\nfolium.Marker([40.7769271, -73.87396590000003], tooltip=\"LaGuardia Airport (LGA)\").add_to(new_york)\nfolium.Marker([40.6895314, -74.17446239999998], tooltip=\"Newark Liberty International Airport (EWR)\").add_to(new_york)\n\n# Plot it on the map\nhm = plugins.HeatMapWithTime(heat_data,auto_play=True,max_opacity=0.8)\nhm.add_to(new_york)\n# Display the map\nnew_york","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Insights:\n+ **Most of the trips within Manhattan city through out the day**. \n+ Most of the **bookings near to Airport and hotsport place**.\n+ **YFK and LGA Airport is most booking density place**.\n+ Mostly the **long trips upto Norwalk, stford, ossining, Huntington, paterson & Cedar Grove Cities**.","metadata":{}},{"cell_type":"markdown","source":"<h2 style=\"color:black;\"><b>5. Feature Engineering: </b></h2>\n","metadata":{}},{"cell_type":"code","source":"# Replicate the dataset and make our changes in copied dataset\n\ndf2 = df1.copy( deep = True )\ndf2.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**According to the official Wikipedia Page, the haversine formula determines the great-circle distance between two points on a sphere given their longitudes and latitudes**<br>\n![image.png](http://ttarnawski.usermd.net/wp-content/uploads/2017/08/Bez-nazwy.png)\n","metadata":{}},{"cell_type":"code","source":"# Creating new column for trip distance, we can find this details using trip Latitude & longitude details\n\ndef haversine_distance( lon1, lat1, lon2, lat2 ):\n    # approximate radius of earth in km\n    R = 6373.0 \n    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n    dlon = lon2 - lon1\n    dlat = lat2 - lat1\n    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n    return round(R * c, 2)\n\ndf2['distance'] = df2.apply( lambda row : haversine_distance( row['pickup_longitude'],\n                                                              row['pickup_latitude'],\n                                                              row['dropoff_longitude'],\n                                                              row['dropoff_latitude'] ), axis = 1 )\ndf2.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Assemption:\n+ The distacnce is calculated between pickup(latitude, longitude) & dropoff(latitude, longitude) using **Haversine distance** formula.\n+ We can calculate either by manual formula or geographical API like geodics. But API will take long time compare to manual calculation.\n+ This **distance feature will play important role in model building**.\n+ we can replace the outlier values in fare amount using the distance.","metadata":{}},{"cell_type":"code","source":"# Ensuring the relationtionship between fare_amount and distance using scatter plot\n\nfig, ax = plt.subplots(figsize = (18,5))\nsns.regplot( x = df2.distance, y = df2.fare_amount)\nax.set_title('Distacne VS Fare amount', size = 16)\nax.set_xlabel('Distance', size = 15)\nax.set_ylabel('Fare amount', size = 15)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Insights:\n+ There are some datapoints has **long distance value but very low fare amount**. These datapoints could be **outliers**.\n+ As we know already we have few datapoints with negative fare.\n+ There are few datapoint with **high fare amount for very low distance**. These datapoints could be **outliers**.\n+ We can calculate average value for removing outliers from fare amount with respect to fare amount.\n","metadata":{}},{"cell_type":"code","source":"# Outlier detection for distance feature\n\n#--> IQR calculation\nQ1 = df2.distance.quantile(0.25)\nQ2 = df2.distance.quantile(0.50)\nQ3 = df2.distance.quantile(0.75)\nQ4 = df2.distance.quantile(0.95)\nIQR = Q3 - Q1\n\n#--> Removing outliers from distacnce\nprint(\"Statistical Data about Distance\")\nprint(\"----------------------------------\")\nprint(\"=> 25th Quantile: {} \\n=> 50th Quantile: {} \\n=> 75th Quantile: {} \\n=> 95th Quantile: {}\".format(Q1, Q2, Q3, Q4))\nprint(\"=> Min distance: {} \\n=> Max distance: {} \".format(df2.distance.min(), df2.distance.max()))\nlength = df2[(df2.distance < (Q1 - 1.5 * IQR)) | (df2.distance > (Q3 + 1.5 * IQR))].shape[0]\nprint(\"=> Number of outlier records: \",length)\n\nfig = plt.figure(figsize=(15,10))\n# Histogram\nplt.subplot(211)\nsns.histplot(df2.distance, kde = True).set_title('Distance data distribution', size = 11)\n# Boxplot\nplt.subplot(212)\nsns.boxplot(df2.distance).set_title(\"Boxplot for Distance outlier detection\", size = 11)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Insights:\n+ There are **40632 datapoints** considered as a **outliers based on distance feature**.\n+ Few data points contain **distance value as zero. We have to further analyze these**.\n+ Since the outlier count is very large, If we delete the data then we will lose some informations. \n+ So we have to find better way to handle this outliers.","metadata":{}},{"cell_type":"code","source":"# Fetching the zero fare amount datapoints\n\nprint(\"=> Number of datapoints with distance value as zero: \",len(df2[df2.distance == 0]))\nprint(\"=> Number of datapoints with pickup & drop at same location: \",len(df2[(df2.pickup_latitude == df2.dropoff_latitude) & \n                                                                            (df2.pickup_longitude == df2.dropoff_longitude)]))\n\nprint(\"=> Number of datapoints with distance & fare value as zero: \",len(df2[(df2.distance == 0) & (df2.fare_amount == 0)]))\n\ndf2.drop( df2[ df2.distance == 0 ].index, inplace = True )\nprint(\"Number of datapoint remaining after deletion : \",df2.shape[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Insights:\n+ Number of datapoints with **distance value as zero is equal to Number of datapoints with pickup & drop at same location** for **6013 datapoints**.\n+ The distance feature is going be a important feature for model building, because coorelation between **fare amount & distance is very high**.\n+ There are two datapoints with **zero fare amount and distance is also zero**. So these **8** data is completly not useful.\n+ We have to remove these datapoints from our dataset for better accuracy model. And 739 is small amount compare to 50k orginal datapoints, So it will not cause big issue.\n+ After deletion we have **remaining 483255 datapoints**.","metadata":{}},{"cell_type":"code","source":"# Removing the outlier datapoints with respect to distance\n\n#--> IQR calculation\nQ1 = df2.distance.quantile(0.25)\nQ3 = df2.distance.quantile(0.75)\nIQR = Q3 - Q1\n\n#--> Filtering the outlier datapoints\ndf2 = df2[~((df2.distance < (Q1 - 1.5 * IQR)) | (df2.distance > (Q3 + 1.5 * IQR)))]\nprint(\"Number of datapoint remaining after distance outlier deletion : \",df2.shape[0])\nfig, ax = plt.subplots(figsize = (15,5))\nsns.regplot( x = df2.distance, y = df2.fare_amount , marker = '+').set_title(\"Distance VS Fare amount\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Insights:\n+ After removing the most extreme distance outliers we are getting **proper liner relationship between distance & fare amount**.\n+ Based on distance we can rework on fare amount outlier by taking **average fare amount for same distance**.","metadata":{}},{"cell_type":"code","source":"# Handling fare amount outliers with respect to distance:\n# The strong assemption is fare amount is linearly dependented on distance. \n# So we can apply average fare amount with same distance for outlier datapoints.\n\n# IQR Calculation\nQ1 = df2.fare_amount.quantile(0.25)\nQ3 = df2.fare_amount.quantile(0.75)\nIQR = Q3 - Q1\n\n# Method for outlier treatment on fare amount\ndef remove_outlier(distance, fare):\n    if fare <= 0:\n        # Negative fare amount\n        res = df2[(df2['distance'] == distance)]['fare_amount'].mean()\n    elif fare < ( Q1 - 1.5 * IQR ) or fare > ( Q3 + 1.5 * IQR ):\n        # Outlier fare amount\n        res = df2[(df2['distance'] == distance)]['fare_amount'].mean()\n    else:\n        # Default as input fare amount\n        res = fare\n    return res\n\n# Outlier removal function call\ndf2['fare_amount'] = df2.apply(lambda x : remove_outlier(x['distance'], x['fare_amount']) \n                                                      if x['fare_amount'] <= 0 or \n                                                         x['fare_amount'] < ( Q1 - 1.5 * IQR ) or\n                                                         x['fare_amount'] > ( Q3 + 1.5 * IQR )\n                                                    else x['fare_amount'] , axis = 1 )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Insights: \n+ The strong assemption is **fare amount is linearly dependented on distance**.\n+ Using this assemption we can recalculate the oulier fare amount with distance.\n+ **The average fare amount is calculated from dataset with exact distance and replaced for outlier entry**.","metadata":{}},{"cell_type":"code","source":"# Regplot after fare amount outlier value replacement\n\nfig, ax = plt.subplots(figsize = (15,5))\nsns.scatterplot( x = df2.distance, y = df2.fare_amount , marker = '+').set_title(\"Distance VS Fare amount\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Removing the unwanted columns from Dataframe\n\ndf2.drop( ['key', 'count', 'out_detection', 'fare_out'], axis = 'columns', inplace = True)\ndf2.sample(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}