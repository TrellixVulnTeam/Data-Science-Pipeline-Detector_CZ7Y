{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"# Data processing\nimport numpy as np\nimport pandas as pd\nimport datetime as dt\n\n# Visualization libaries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Machine Learning\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\n\n# Read data\n#n = 55000000 # Number of total rows\n#s = 100000 # Desired sample size\n#skip = sorted(np.random.choice(range(n), n-s, replace=False))\n#skip[0] = 1\n#train = pd.read_csv('../input/train.csv', skiprows=skip, header=0)\ntrain = pd.read_csv('../input/train.csv', nrows=10_000_000)\ntest = pd.read_csv('../input/test.csv')\n\ntrain.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0b91c74370b1efe0324a39459fdb18f0e10ab106","scrolled":false,"collapsed":true},"cell_type":"code","source":"# Let's start by checking for NaN values\nprint('Sum of NaN values for each column')\nprint(train.isnull().sum())\n\n# It seems like we lost some data for the dropoff. There are several ways of handling this, but I just go with removing the rows.\ntrain = train.dropna()\nprint('Sum of NaN values for each column after dropping NaN')\nprint(train.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"bc99fa3c922ae88681ab269d8303b81253509c35","collapsed":true},"cell_type":"code","source":"# Let's have a look at the data\ntrain.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"ee71ff8644f0fc938167562e1a3a2960674fd7d7","collapsed":true},"cell_type":"markdown","source":"# Seems like we have a few outliers. Let's visualize the data and see if we can spot the outliers.\ncolumns_to_plot = ['fare_amount', 'passenger_count']\nsns.pairplot(train.loc[:, train.columns != 'pickup_datetime'])"},{"metadata":{"trusted":true,"_uuid":"64a26f6003e700786451c62398864a0bd0e4e3ad","scrolled":false,"collapsed":true},"cell_type":"code","source":"# So there seem to be a lot of outliers.\n\n# Manually picking reasonable levels until I find a smarter way\ntrain = train.loc[(train['fare_amount'] > 0) & (train['fare_amount'] < 200)]\ntrain = train.loc[(train['pickup_longitude'] > -300) & (train['pickup_longitude'] < 300)]\ntrain = train.loc[(train['pickup_latitude'] > -300) & (train['pickup_latitude'] < 300)]\ntrain = train.loc[(train['dropoff_longitude'] > -300) & (train['dropoff_longitude'] < 300)]\ntrain = train.loc[(train['dropoff_latitude'] > -300) & (train['dropoff_latitude'] < 300)]\n#train = train.loc[train[columns_to_select] < ]\n# Let's assume taxa's can be mini-busses as well, so we select up to 8 passengers.\ntrain = train.loc[train['passenger_count'] <= 8]\ntrain.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9a22d719f8836600b50734ecb5bf28d55b2c0bb9","collapsed":true},"cell_type":"code","source":"print('Sum of NaN values for each column')\nprint(test.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"scrolled":false,"collapsed":true},"cell_type":"code","source":"combine = [test, train]\nfor dataset in combine:\n    # Distance is expected to have an impact on the fare\n    dataset['longitude_distance'] = dataset['pickup_longitude'] - dataset['dropoff_longitude']\n    dataset['latitude_distance'] = dataset['pickup_latitude'] - dataset['dropoff_latitude']\n    \n    # Straight distance\n    dataset['distance_travelled'] = (dataset['longitude_distance'] ** 2 + dataset['latitude_distance'] ** 2) ** .5\n    dataset['distance_travelled_sin'] = np.sin((dataset['longitude_distance'] ** 2 * dataset['latitude_distance'] ** 2) ** .5)\n    dataset['distance_travelled_cos'] = np.cos((dataset['longitude_distance'] ** 2 * dataset['latitude_distance'] ** 2) ** .5)\n    dataset['distance_travelled_sin_sqrd'] = np.sin((dataset['longitude_distance'] ** 2 * dataset['latitude_distance'] ** 2) ** .5) ** 2\n    dataset['distance_travelled_cos_sqrd'] = np.cos((dataset['longitude_distance'] ** 2 * dataset['latitude_distance'] ** 2) ** .5) ** 2\n    \n    # Haversine formula for distance\n    # Haversine formula:\ta = sin²(Δφ/2) + cos φ1 ⋅ cos φ2 ⋅ sin²(Δλ/2)\n    R = 6371e3 # Metres\n    phi1 = np.radians(dataset['pickup_latitude'])\n    phi2 = np.radians(dataset['dropoff_latitude'])\n    phi_chg = np.radians(dataset['pickup_latitude'] - dataset['dropoff_latitude'])\n    delta_chg = np.radians(dataset['pickup_longitude'] - dataset['dropoff_longitude'])\n    a = np.sin(phi_chg / 2) ** .5 + np.cos(phi1) * np.cos(phi2) * np.sin(delta_chg / 2) ** .5\n    c = 2 * np.arctan2(a ** .5, (1-a) ** .5)\n    d = R * c\n    dataset['haversine'] = d\n    \n    # Bearing\n    # Formula:\tθ = atan2( sin Δλ ⋅ cos φ2 , cos φ1 ⋅ sin φ2 − sin φ1 ⋅ cos φ2 ⋅ cos Δλ )\n    y = np.sin(delta_chg * np.cos(phi2))\n    x = np.cos(phi1) * np.sin(phi2) - np.sin(phi1) * np.cos(phi2) * np.cos(delta_chg)\n    dataset['bearing'] = np.arctan2(y, x)\n    \n    # Maybe time of day matters? Obviously duration is a factor, but there is no data for time arrival\n    # Features: hour of day (night vs day), month (some months may be in higher demand) \n    dataset['pickup_datetime'] = pd.to_datetime(dataset['pickup_datetime'])\n    dataset['hour_of_day'] = dataset.pickup_datetime.dt.hour\n    dataset['day'] = dataset.pickup_datetime.dt.day\n    dataset['week'] = dataset.pickup_datetime.dt.week\n    dataset['month'] = dataset.pickup_datetime.dt.month\n    dataset['day_of_year'] = dataset.pickup_datetime.dt.dayofyear\n    dataset['week_of_year'] = dataset.pickup_datetime.dt.weekofyear\n    \n\n# Remove rows with zero distance from training set\ntrain = train.loc[train['haversine'] != 0]\ntrain = train.dropna()\n\n    \ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ccc43bdc443ac5213e74fb841f2d2a01a91d4285","scrolled":false,"collapsed":true},"cell_type":"code","source":"print('Train data: Sum of NaN values for each column')\nprint(train.isnull().sum())\nprint('Test data: Sum of NaN values for each column')\nprint(test.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"c5357b9c3ff46b8472b423fb6e9db2e922902ddc","collapsed":true},"cell_type":"code","source":"# So we have a lot of NaN values for the Haversine for the test set.\n# This is probably because python cannot work with to short distances.\n# This is not good, since there's a lot more NaN values than actual values.\n# So maybe Haversine isn't such a good feature afterall?\n# If used, it should be fixed somehow. Ideas?\n# One way could be by using the median or mean. However, this is not accurate.\nmedian = test['haversine'].median()\ntest['haversine'] = test['haversine'].fillna(median)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"39def2ffb283d945d6d1287c91cbe489ef1040d5","collapsed":true},"cell_type":"code","source":"# Let's check how the features correlate\ncolormap = plt.cm.RdBu\nplt.figure(figsize=(20,20))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(train.corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4cec893be34e814522c8945ba0e9959a712536d6","collapsed":true},"cell_type":"code","source":"# Let's drop all the irrelevant features\ntrain_features_to_keep = ['haversine', 'fare_amount']\ntrain.drop(train.columns.difference(train_features_to_keep), 1, inplace=True)\n\ntest_features_to_keep = ['haversine', 'key']\ntest.drop(test.columns.difference(test_features_to_keep), 1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"24f9379fd1b113a754dc4c689610275238372281","collapsed":true},"cell_type":"code","source":"# Let's experiment with different models.\n# Process:\n# 1. Get predictions using Linear Regression, Random Forest and XGBoost.\n# 2. Check how prediction correlate.\n# 3. Take a weighted average of predictions\n# 4. Submit and fingers crossed \\X/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a206e89e8f24cad232b5e0f6e6fea508578a1017","collapsed":true},"cell_type":"code","source":"# Step 1:\n# Let's combine the training set again\nx_train = train.drop('fare_amount', axis=1)\ny_train = train['fare_amount']\nx_test = test.drop('key', axis=1)\n\n# Set up the models.\n# Linear Regression Model\nfrom sklearn.linear_model import LinearRegression\nregr = LinearRegression()\nregr.fit(x_train, y_train)\nregr_pred = regr.predict(x_test)\n\n# Random Forest Regressor\nfrom sklearn.ensemble import RandomForestRegressor\nrfr = RandomForestRegressor()\nrfr.fit(x_train, y_train)\nrfr_pred = rfr.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b5eb984e34364949f052e7a603be5cb428fe2222","collapsed":true},"cell_type":"code","source":"# Let's prepare the test set\nx_pred = test.drop('key', axis=1)\n\n# Let's run XGBoost and predict those fares!\nx_train,x_test,y_train,y_test = train_test_split(train.drop('fare_amount',axis=1),train.pop('fare_amount'),random_state=123,test_size=0.2)\n\ndef XGBmodel(x_train,x_test,y_train,y_test):\n    matrix_train = xgb.DMatrix(x_train,label=y_train)\n    matrix_test = xgb.DMatrix(x_test,label=y_test)\n    model=xgb.train(params={'objective':'reg:linear','eval_metric':'rmse'}\n                    ,dtrain=matrix_train,num_boost_round=200, \n                    early_stopping_rounds=20,evals=[(matrix_test,'test')],)\n    return model\n\nmodel=XGBmodel(x_train,x_test,y_train,y_test)\nxgb_pred = model.predict(xgb.DMatrix(x_pred), ntree_limit = model.best_ntree_limit)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aaa90463c6912d6f2ea464c9bcbf982c77150ed2","collapsed":true},"cell_type":"code","source":"regr_pred, rfr_pred, xgb_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d303eee0bcf3278f850c53870e017c1c8ec956be","collapsed":true},"cell_type":"code","source":"# Assigning weights. More precise models gets higher weight.\nregr_weight = 1\nrfr_weight = 1\nxgb_weight = 3\nprediction = (regr_pred * regr_weight + rfr_pred * rfr_weight + xgb_pred * xgb_weight) / (regr_weight + rfr_weight + xgb_weight)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"45871c14be6368b7034caf2cbc4c6d3525c603ef","collapsed":true},"cell_type":"code","source":"prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c9728f5c59bc7a50a93a1d7b2d5e9e17914785d3"},"cell_type":"code","source":"# Add to submission\nsubmission = pd.DataFrame({\n        \"key\": test['key'],\n        \"fare_amount\": prediction.round(2)\n})\n\nsubmission.to_csv('sub_fare.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7f375bd4ce358439c382a14594d8f3b79e211140","scrolled":true,"collapsed":true},"cell_type":"code","source":"submission","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3f3ae7753c6dc1c095cc578767a74752c93e356e"},"cell_type":"markdown","source":"# Things to do:\n- Check if whether outliers are correctly removed\n- Come up with new features\n- Increase or randomize the train loading"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"909c4955b5a9eac9ed176657735531271a5133bb"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}