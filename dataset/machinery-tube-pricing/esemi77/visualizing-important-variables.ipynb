{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"\"\"\"\nCaterpillar @ Kaggle\nAdapted from arnaud demytt's R script\nAND\nGilberto Titericz Junior's python scripts\n__author__ = saihttam\n\"\"\"\n\n\nimport os\nimport pandas as pd\nimport numpy as np\nfrom sklearn.utils import shuffle\nfrom sklearn.ensemble import ExtraTreesClassifier\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.cross_validation import ShuffleSplit\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble.partial_dependence import plot_partial_dependence\n\nnp.random.seed(42)\nnumerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n\n# load training datasets\ntrain = pd.read_csv(os.path.join('..', 'input', 'train_set.csv'), parse_dates=[2,])\ntube_data = pd.read_csv(os.path.join('..', 'input', 'tube.csv'))\n\ntrain = pd.merge(train, tube_data, on='tube_assembly_id')\n\n# create some new features\ntrain['year'] = train.quote_date.dt.year\ntrain['month'] = train.quote_date.dt.month\ntrain['week'] = train.quote_date.dt.dayofyear % 52\n\ntrain = train.drop(['quote_date', 'tube_assembly_id'], axis=1)\nrs = ShuffleSplit(train.shape[0], n_iter=3, train_size=.2, test_size=.8, random_state=0)\nfor train_index, _ in rs:\n    pass\n\ntrain = train.iloc[train_index]\nprint(train.shape)\n# Adapted from R script, only use top {threshold features from categorical columns}\nnewdf = train.select_dtypes(include=numerics)\nnumcolumns = newdf.columns.values\n\nallcolumns = train.columns.values\nnonnumcolumns = list(set(allcolumns) - set(numcolumns))\nprint(\"Numcolumns %s \" % numcolumns)\nprint(\"Nonnumcolumns %s \" % nonnumcolumns)\n\nprint(\"Nans before processing: \\n {0}\".format(train.isnull().sum()))\ntrain[numcolumns] = train[numcolumns].fillna(-999999)\ntrain[nonnumcolumns] = train[nonnumcolumns].fillna(\"NAvalue\")\nprint(\"Nans after processing: \\n {0}\".format(train.isnull().sum()))\n\nfor col in nonnumcolumns:\n    ser = train[col]\n    counts = ser.value_counts().keys()\n    # print \"%s has %d different values before\" % (col, len(counts))\n    threshold = 5\n    if len(counts) > threshold:\n        ser[~ser.isin(counts[:threshold])] = \"rareValue\"\n    if len(counts) <= 1:\n        print(\"Dropping Column %s with %d values\" % (col, len(counts)))\n        train = train.drop(col, axis=1)\n    else:\n        train[col] = ser.astype('category')\n\ntrain = pd.get_dummies(train)\nprint(\"Size after dummies {0}\".format(train.shape))\n\n# Use log for some variables for better visualization\ntrain[\"logquantity\"] = np.log(train['quantity'])\ntrain[\"log1usage\"] = np.log1p(train['annual_usage'])\ntrain[\"log1radius\"] = np.log1p(train['bend_radius'])\ntrain[\"log1length\"] = np.log1p(train['length'])\ntrain = train.drop(['quantity', 'annual_usage', 'bend_radius', 'length'], axis=1)\n\nlabels = train.cost.values\nXtrain = train.drop(['cost'], axis=1)\nnames = list(Xtrain.columns.values)\nXtrain = np.array(Xtrain)\n\nlabel_log = np.log1p(labels)\nXtrain, label_log = shuffle(Xtrain, label_log, random_state=666)\n\n"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"names"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"model = ExtraTreesClassifier(n_estimators=50, max_depth=15)\nmodel.fit(Xtrain, label_log)\nfeatures = []\n\n# display the relative importance of each attribute\nimportances = model.feature_importances_\nindices = np.argsort(importances)[::-1]\n\nfor f in range(len(importances)):\n    print(\"%d. feature %d (%f), %s\" % (f + 1, indices[f], importances[indices[f]], names[indices[f]]))\n    features.append(indices[f])\n    # Print only first 5 most important variables\n    if len(features) >= 5:\n        break\n\nq = pd.qcut(train[\"cost\"], 5)\nprint(\"Bins are {0}\".format(q))\ntrain['cost_5'] = q\n\nfig = plt.figure()\nfeaturenames = [names[feature] for feature in features]\nfeaturenames.append('cost_5')\npg = sns.pairplot(train[featurenames], hue='cost_5', size=2.5)\npg.savefig('pairplotquintile.png')\n\n\nprint(\"Training GBRT...\")\nclf = GradientBoostingRegressor(n_estimators=100, max_depth=4,\n                                learning_rate=0.1, loss='huber',\n                                random_state=1)\nclf.fit(Xtrain, label_log)\nprint('Convenience plot with ``partial_dependence_plots``')\n\n# 2-D dependence plot\ntarget_feature = (features[0], features[1])\nfeatures.append(target_feature)\nfig, axs = plot_partial_dependence(clf, Xtrain, features, feature_names=names,\n                                   n_jobs=3, grid_resolution=50)\nfig.savefig('partial.png')"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":0}