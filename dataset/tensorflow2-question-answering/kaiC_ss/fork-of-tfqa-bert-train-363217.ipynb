{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport sys\nsys.path.extend([#'../input/tf2_0_baseline_w_bert/',#'../input/bert_modeling/',\n                 '../input/bert-joint-baseline/'])\nimport bert_utils\nimport modeling \n#import bert_optimization as optimization\nimport tokenization\nimport json\n\n#tf.compat.v1.disable_eager_execution()\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n# In this case, we've got some extra BERT model files under `/kaggle/input/bertjointbaseline`\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"on_kaggle_server = os.path.exists('/kaggle')\nnq_test_file = '../input/tensorflow2-question-answering/simplified-nq-test.jsonl' \npublic_dataset = os.path.getsize(nq_test_file)<20_000_000\nprivate_dataset = os.path.getsize(nq_test_file)>=20_000_000\n\nprint(on_kaggle_server,public_dataset,private_dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if True:\n    import importlib\n    importlib.reload(bert_utils)","execution_count":null,"outputs":[]},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"with open('../input/bert-joint-baseline/bert_config.json','r') as f:\n    config = json.load(f)\nprint(json.dumps(config,indent=4))\n","execution_count":null,"outputs":[]},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"class TDense(tf.keras.layers.Layer):\n    def __init__(self,\n                 output_size,\n                 kernel_initializer=None,\n                 bias_initializer=\"zeros\",\n                **kwargs):\n        super().__init__(**kwargs)\n        self.output_size = output_size\n        self.kernel_initializer = kernel_initializer\n        self.bias_initializer = bias_initializer\n\n    def build(self,input_shape):\n        dtype = tf.as_dtype(self.dtype or tf.keras.backend.floatx())\n        if not (dtype.is_floating or dtype.is_complex):\n          raise TypeError(\"Unable to build `TDense` layer with \"\n                          \"non-floating point (and non-complex) \"\n                          \"dtype %s\" % (dtype,))\n        input_shape = tf.TensorShape(input_shape)\n        if tf.compat.dimension_value(input_shape[-1]) is None:\n          raise ValueError(\"The last dimension of the inputs to \"\n                           \"`TDense` should be defined. \"\n                           \"Found `None`.\")\n        last_dim = tf.compat.dimension_value(input_shape[-1])\n        self.input_spec = tf.keras.layers.InputSpec(min_ndim=2, axes={-1: last_dim})\n        self.kernel = self.add_weight(\n            \"kernel\",\n            shape=[self.output_size,last_dim],\n            initializer=self.kernel_initializer,\n            dtype=self.dtype,\n            trainable=True)\n        self.bias = self.add_weight(\n            \"bias\",\n            shape=[self.output_size],\n            initializer=self.bias_initializer,\n            dtype=self.dtype,\n            trainable=True)\n        super(TDense, self).build(input_shape)\n    def call(self,x):\n        return tf.matmul(x,self.kernel,transpose_b=True)+self.bias\n    \n    def get_config(self):\n\n        cfg = super().get_config().copy()\n        cfg.update({\n            'output_size': self.output_size,\n            'kernel_initializer': self.kernel_initializer,\n            'bias_initializer': self.bias_initializer,\n        })\n        return cfg\ndef mk_model(config):\n    seq_len = config['max_position_embeddings']\n    unique_id  = tf.keras.Input(shape=(1,),dtype=tf.int64,name='unique_id')\n    input_ids   = tf.keras.Input(shape=(seq_len,),dtype=tf.int32,name='input_ids')\n    input_mask  = tf.keras.Input(shape=(seq_len,),dtype=tf.int32,name='input_mask')\n    segment_ids = tf.keras.Input(shape=(seq_len,),dtype=tf.int32,name='segment_ids')\n    BERT = modeling.BertModel(config=config,name='bert')\n    pooled_output, sequence_output = BERT(input_word_ids=input_ids,\n                                          input_mask=input_mask,\n                                          input_type_ids=segment_ids)\n    \n    logits = TDense(2,name='logits')(sequence_output)\n    start_logits,end_logits = tf.split(logits,axis=-1,num_or_size_splits= 2,name='split')\n    start_logits = tf.squeeze(start_logits,axis=-1,name='start_squeeze')\n    end_logits   = tf.squeeze(end_logits,  axis=-1,name='end_squeeze')\n    \n    ans_type      = TDense(5,name='ans_type')(pooled_output)\n    return tf.keras.Model([input_ for input_ in [unique_id,input_ids,input_mask,segment_ids] \n                           if input_ is not None],\n                          [unique_id,start_logits,end_logits,ans_type],\n                          name='bert-baseline')","execution_count":null,"outputs":[]},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"model= mk_model(config)","execution_count":null,"outputs":[]},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"model.load_weights('../input/unk0201128w/weights')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tqdm\neval_records = \"../input/bert-joint-baseline/nq-test.tfrecords\"\n#nq_test_file = '../input/tensorflow2-question-answering/simplified-nq-test.jsonl'\nif private_dataset:\n    eval_records='nq-test.tfrecords'\nif not os.path.exists(eval_records):\n    # tf2baseline.FLAGS.max_seq_length = 512\n    eval_writer = bert_utils.FeatureWriter(\n        filename=os.path.join(eval_records),\n        is_training=False)\n\n    tokenizer = tokenization.FullTokenizer(vocab_file='../input/bert-joint-baseline/vocab-nq.txt', \n                                           do_lower_case=True)\n\n    features = []\n    convert = bert_utils.ConvertExamples2Features(tokenizer=tokenizer,\n                                                   is_training=False,\n                                                   output_fn=eval_writer.process_feature,\n                                                   collect_stat=False)\n\n    n_examples = 0\n    tqdm_notebook= tqdm.tqdm_notebook if not on_kaggle_server else None\n    for examples in bert_utils.nq_examples_iter(input_file=nq_test_file, \n                                           is_training=False,\n                                           tqdm=tqdm_notebook):\n        for example in examples:\n            n_examples += convert(example)\n\n    eval_writer.close()\n    print('number of test examples: %d, written to file: %d' % (n_examples,eval_writer.num_features))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seq_length = bert_utils.FLAGS.max_seq_length #config['max_position_embeddings']\nname_to_features = {\n      \"unique_id\": tf.io.FixedLenFeature([], tf.int64),\n      \"input_ids\": tf.io.FixedLenFeature([seq_length], tf.int64),\n      \"input_mask\": tf.io.FixedLenFeature([seq_length], tf.int64),\n      \"segment_ids\": tf.io.FixedLenFeature([seq_length], tf.int64),\n  }\n\ndef _decode_record(record, name_to_features=name_to_features):\n    \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n    example = tf.io.parse_single_example(serialized=record, features=name_to_features)\n\n    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n    # So cast all int64 to int32.\n    for name in list(example.keys()):\n        t = example[name]\n        if name != 'unique_id': #t.dtype == tf.int64:\n            t = tf.cast(t, dtype=tf.int32)\n        example[name] = t\n\n    return example\n\ndef _decode_tokens(record):\n    return tf.io.parse_single_example(serialized=record, \n                                      features={\n                                          \"unique_id\": tf.io.FixedLenFeature([], tf.int64),\n                                          \"token_map\" :  tf.io.FixedLenFeature([seq_length], tf.int64)\n                                      })","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_ds = tf.data.TFRecordDataset(eval_records)\ntoken_map_ds = raw_ds.map(_decode_tokens)\ndecoded_ds = raw_ds.map(_decode_record)\nds = decoded_ds.batch(batch_size=16,drop_remainder=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time result=model.predict_generator(ds,verbose=1 if not on_kaggle_server else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('model.h5')  # 存檔","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import collections\nSpan = collections.namedtuple(\"Span\", [\"start_token_idx\", \"end_token_idx\"])\n\n\nclass EvalExample(object):\n  \"\"\"Eval data available for a single example.\"\"\"\n\n  def __init__(self, example_id, candidates):\n    self.example_id = example_id\n    self.candidates = candidates\n    self.results = {}\n    self.features = {}\n\n\nclass ScoreSummary(object):\n\n  def __init__(self):\n    self.predicted_label = None\n    self.short_span_score = None\n    self.cls_token_score = None\n    self.answer_type_logits = None\n\ndef get_best_indexes(logits, n_best_size):\n  \"\"\"Get the n-best logits from a list.\"\"\"\n  index_and_score = sorted(\n      enumerate(logits[1:], 1), key=lambda x: x[1], reverse=True)\n  best_indexes = []\n  for i in range(len(index_and_score)):\n    if i >= n_best_size:\n      break\n    best_indexes.append(index_and_score[i][0])\n  return best_indexes\n\n\ndef compute_predictions(example):\n  \"\"\"Converts an example into an NQEval object for evaluation.\"\"\"\n  predictions = []\n  n_best_size = 10\n  max_answer_length = 30\n\n  i = 0\n  for unique_id, result in example.results.items():\n    if unique_id not in example.features:\n      raise ValueError(\"No feature found with unique_id:\", unique_id)\n    token_map = np.array(example.features[unique_id][\"token_map\"])\n    start_indexes = get_best_indexes(result[\"start_logits\"], n_best_size)\n    end_indexes = get_best_indexes(result[\"end_logits\"], n_best_size)\n    for start_index in start_indexes:\n      for end_index in end_indexes:\n        if end_index < start_index:\n          continue\n        if token_map[start_index] == -1:\n          continue\n        if token_map[end_index] == -1:\n          continue\n        length = end_index - start_index + 1\n        if length > max_answer_length:\n          continue\n        summary = ScoreSummary()\n        summary.short_span_score = (\n            result[\"start_logits\"][start_index] +\n            result[\"end_logits\"][end_index])\n        summary.cls_token_score = (\n            result[\"start_logits\"][0] + result[\"end_logits\"][0])\n        summary.answer_type_logits = result[\"answer_type_logits\"]\n        \n        # additional scores \n        summary.start_score = result[\"start_logits\"][start_index] \n        summary.end_score = result[\"end_logits\"][end_index] \n        summary.cls_start_score = result[\"start_logits\"][0]\n        summary.cls_end_score = result[\"end_logits\"][0]\n        \n        start_span = token_map[start_index]\n        end_span = token_map[end_index] + 1\n\n        # Span logits minus the cls logits seems to be close to the best.\n        score = summary.short_span_score - summary.cls_token_score\n        predictions.append((score, i, summary, start_span, end_span))\n        i += 1\n\n  short_span = Span(-1, -1)\n  long_span = Span(-1, -1)\n\n  if predictions:\n    score, _, summary, start_span, end_span = sorted(predictions, reverse=True)[0]\n    short_span = Span(start_span, end_span)\n    \n    matching_candidates = []\n    start = short_span.start_token_idx\n    end = short_span.end_token_idx\n    \n    c_corr = None\n    for c in example.candidates:\n\n      if c[\"start_token\"] <= start and c[\"end_token\"] >= end:\n        long_span = Span(c[\"start_token\"], c[\"end_token\"])\n        matching_candidates.append(c)\n        \n    if len(matching_candidates) == 1:\n        c_corr = matching_candidates[0]\n        if not c_corr['top_level']:\n            c_corr = None\n        \n    if len(matching_candidates) > 2:\n        for c in matching_candidates:\n          if c['top_level']:\n            c_corr = c\n            break\n            \n    if len(matching_candidates) == 2:\n        for c in matching_candidates:\n          if not c['top_level'] and c['start_token_value'] == '<Li>':\n            c_corr = c\n        if not c_corr:\n          for c in matching_candidates:\n            if c['top_level'] :\n                c_corr = c\n    \n    if c_corr:\n        long_span = Span(c_corr[\"start_token\"], c_corr[\"end_token\"])\n    \n    \n  else:\n      summary = ScoreSummary()\n\n      summary.predicted_label = {\n      \"example_id\": int(example.example_id),\n      \"long_answer\": {\n          \"start_token\": int(-1),\n          \"end_token\": int(-1),\n          \"start_byte\": int(-1),\n          \"end_byte\": int(-1)\n      },\n      \"long_answer_score\": float(-10),\n      \"short_answers\": [],\n      \"short_answers_score\": float(-10),\n      \"yes_no_answer\": \"NONE\",\n      \"answer_type_logits\": [10, -10, -10, -10, -10],\n      \"answer_type\": int(0),\n          \n      # additional scores\n      \"start_score\": float(-100), \n      \"end_score\": float(-100),\n      \"cls_start_score\": float(100),\n      \"cls_end_score\": float(100)\n      }\n      return summary\n        \n  summary.predicted_label = {\n      \"example_id\": int(example.example_id),\n      \"long_answer\": {\n          \"start_token\": int(long_span.start_token_idx),\n          \"end_token\": int(long_span.end_token_idx),\n          \"start_byte\": -1,\n          \"end_byte\": -1\n      },\n      \"long_answer_score\": float(summary.start_score * 0.3 + summary.end_score * 0.15 - 0.4 * summary.cls_start_score - 0.4 * summary.cls_end_score),\n      \"short_answers\": [{\n          \"start_token\": int(short_span.start_token_idx),\n          \"end_token\": int(short_span.end_token_idx),\n          \"start_byte\": -1,\n          \"end_byte\": -1\n      }],\n      \"short_answers_score\": float(score * 0.22 -0.1 * summary.answer_type_logits[0] + 0.5 *  summary.answer_type_logits[3] - 0.3 * summary.answer_type_logits[4]),\n      \"yes_no_answer\": \"NONE\",\n      \"answer_type_logits\": summary.answer_type_logits.tolist(),\n      \"answer_type\": int(np.argmax(summary.answer_type_logits)),\n      # additional scores\n      \"start_score\": float(summary.start_score), \n      \"end_score\": float(summary.end_score),\n      \"cls_start_score\": float(summary.cls_start_score),\n      \"cls_end_score\": float(summary.cls_end_score)\n  }\n\n  return summary\n\ndef compute_pred_dict(candidates_dict, dev_features, raw_results,tqdm=None):\n    \"\"\"Computes official answer key from raw logits.\"\"\"\n    raw_results_by_id = [(int(res['unique_id']),1, res) for res in raw_results]\n\n    examples_by_id = [(int(k),0,v) for k, v in candidates_dict.items()]\n  \n    features_by_id = [(int(d['unique_id']),2,d) for d in dev_features] \n  \n    # Join examples with features and raw results.\n    examples = []\n    print('merging examples...')\n    merged = sorted(examples_by_id + raw_results_by_id + features_by_id)\n    print('done.')\n    for idx, type_, datum in merged:\n        if type_==0: #isinstance(datum, list):\n            examples.append(EvalExample(idx, datum))\n        elif type_==2: #\"token_map\" in datum:\n            examples[-1].features[idx] = datum\n        else:\n            examples[-1].results[idx] = datum\n\n    # Construct prediction objects.\n    print('Computing predictions...')\n   \n    nq_pred_dict = {}\n    if tqdm is not None:\n        examples = tqdm(examples)\n    for e in examples:\n        summary = compute_predictions(e)\n        nq_pred_dict[e.example_id] = summary.predicted_label\n\n    return nq_pred_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_candidates_from_one_split(input_path):\n  \"\"\"Read candidates from a single jsonl file.\"\"\"\n  candidates_dict = {}\n  print(\"Reading examples from: %s\" % input_path)\n  if input_path.endswith(\".gz\"):\n    with gzip.GzipFile(fileobj=tf.io.gfile.GFile(input_path, \"rb\")) as input_file:\n      for index, line in enumerate(input_file):\n        e = json.loads(line)\n        candidates = e[\"long_answer_candidates\"]\n        tokens = e['document_text'].split()\n        add_candidates = []\n        for c in candidates:\n            c_start_token = tokens[c['start_token']]\n            c['start_token_value'] = c_start_token\n            add_candidates.append(c)\n        candidates_dict[e[\"example_id\"]] = add_candidates\n        \n  else:\n    with tf.io.gfile.GFile(input_path, \"r\") as input_file:\n      for index, line in enumerate(input_file):\n        e = json.loads(line)\n        candidates = e[\"long_answer_candidates\"]\n        tokens = e['document_text'].split()\n        add_candidates = []\n        for c in candidates:\n            c_start_token = tokens[c['start_token']]\n            c['start_token_value'] = c_start_token\n            add_candidates.append(c)\n        candidates_dict[e[\"example_id\"]] = add_candidates\n  return candidates_dict\n\n\ndef read_candidates(input_pattern):\n  \"\"\"Read candidates with real multiple processes.\"\"\"\n  input_paths = tf.io.gfile.glob(input_pattern)\n  final_dict = {}\n  for input_path in input_paths:\n    final_dict.update(read_candidates_from_one_split(input_path))\n  return final_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_results = [bert_utils.RawResult(*x) for x in zip(*result)]\n    \nprint (\"Going to candidates file\")\n\ncandidates_dict = read_candidates('../input/tensorflow2-question-answering/simplified-nq-test.jsonl')\n\nprint (\"setting up eval features\")\n\neval_features = list(token_map_ds)\n\nprint (\"compute_pred_dict\")\n\ntqdm_notebook= tqdm.tqdm_notebook if not on_kaggle_server else None\nnq_pred_dict = compute_pred_dict(candidates_dict, \n                                       eval_features,\n                                       [r._asdict() for r in all_results])\n\npredictions_json = {\"predictions\": list(nq_pred_dict.values())}\n\nprint (\"writing json\")\n\nwith tf.io.gfile.GFile('predictions.json', \"w\") as f:\n    json.dump(predictions_json, f, indent=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_short_answer(entry):\n    \n    if entry['answer_type'] == 1:\n        return 'YES'\n        \n    if entry['answer_type'] == 2:\n        return 'NO'\n    \n    \n    if entry[\"short_answers_score\"] < 1.75:\n        return \"\"\n\n    \n    if entry['answer_type'] == 4 : #or single_prediction['answer_type'] == 0 :\n        return \"\"\n    \n    \n    answer = []    \n    for short_answer in entry[\"short_answers\"]:\n        if short_answer[\"start_token\"] > -1:\n            answer.append(str(short_answer[\"start_token\"]) + \":\" + str(short_answer[\"end_token\"]))\n    if entry[\"yes_no_answer\"] != \"NONE\":\n        answer.append(entry[\"yes_no_answer\"])\n    return \" \".join(answer)\n\ndef create_long_answer(entry):\n    if entry[\"long_answer_score\"] < -2.85:\n        return \"\"\n\n    answer = []\n    if entry[\"long_answer\"][\"start_token\"] > -1:\n        answer.append(str(entry[\"long_answer\"][\"start_token\"]) + \":\" + str(entry[\"long_answer\"][\"end_token\"]))\n    return \" \".join(answer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_answers_df = pd.read_json(\"../working/predictions.json\")\nfor var_name in ['long_answer_score','short_answers_score','answer_type']:\n    test_answers_df[var_name] = test_answers_df['predictions'].apply(lambda q: q[var_name])\ntest_answers_df[\"long_answer\"] = test_answers_df[\"predictions\"].apply(create_long_answer)\ntest_answers_df[\"short_answer\"] = test_answers_df[\"predictions\"].apply(create_short_answer)\ntest_answers_df[\"example_id\"] = test_answers_df[\"predictions\"].apply(lambda q: str(q[\"example_id\"]))\n\nlong_answers = dict(zip(test_answers_df[\"example_id\"], test_answers_df[\"long_answer\"]))\nshort_answers = dict(zip(test_answers_df[\"example_id\"], test_answers_df[\"short_answer\"]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission = pd.read_csv(\"../input/tensorflow2-question-answering/sample_submission.csv\")\n\nlong_prediction_strings = sample_submission[sample_submission[\"example_id\"].str.contains(\"_long\")].apply(lambda q: long_answers[q[\"example_id\"].replace(\"_long\", \"\")], axis=1)\nshort_prediction_strings = sample_submission[sample_submission[\"example_id\"].str.contains(\"_short\")].apply(lambda q: short_answers[q[\"example_id\"].replace(\"_short\", \"\")], axis=1)\n\nsample_submission.loc[sample_submission[\"example_id\"].str.contains(\"_long\"), \"PredictionString\"] = long_prediction_strings\nsample_submission.loc[sample_submission[\"example_id\"].str.contains(\"_short\"), \"PredictionString\"] = short_prediction_strings","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if public_dataset:\n    print(test_answers_df[\"long_answer_score\"].describe())\n    print(\"-------------------\")\n    print(test_answers_df[\"short_answers_score\"].describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if public_dataset:\n    print(np.bincount(test_answers_df['answer_type'].values))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if public_dataset:\n    print(test_answers_df[test_answers_df['answer_type']==0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if public_dataset:\n    print(test_answers_df.predictions.values[-4])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if public_dataset:\n    print(sample_submission.head(10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ShowPrediction:\n    def __init__(self,jsonl_file):\n        self._data = {}\n        with open(jsonl_file,'r') as f:\n            for line in f.readlines():\n                d = json.loads(line)\n                #print(d.keys())\n                self._data[int(d['example_id'])]={\n                    'text': d['document_text'],\n                    'question': d['question_text']\n                }\n    def __call__(self,prediction,include_full_text=True):\n        data = self._data[prediction['example_id']]\n        res = {'question': data['question']}\n        if include_full_text:\n            res['text'] = data['text']\n        for type_ in ['long_answer','short_answers']:\n            ans = prediction[type_]\n            if isinstance(ans,list):\n                ans = ans[0]\n            start,end = ans['start_token'],ans['end_token']\n            res[type_] = ' '.join(data['text'].split()[start:end])\n        return res","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if public_dataset:\n    show_pred = ShowPrediction('../input/tensorflow2-question-answering/simplified-nq-test.jsonl')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if public_dataset:\n    for pred in test_answers_df.predictions[test_answers_df.answer_type==0]:\n        print(json.dumps(show_pred(pred,include_full_text=True),indent=4))\n        sys.exit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if public_dataset:\n    for pred in np.random.choice(predictions_json['predictions'],10):\n        print(json.dumps(show_pred(pred,include_full_text=False),indent=4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}