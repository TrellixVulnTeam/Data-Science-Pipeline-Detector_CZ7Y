{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# basuc library\nimport numpy as np\nimport pandas as pd\n\n# #import my kaggle_utiles file that has all the custom funcitons i want.\n# import sys\n# sys.path.append(\"/home/pavithra/Pictures/learning/ML/kaggle/\")\n# sys.path\nimport kaggle_utils_py as kaggle_utils\n\n\n# data visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n# models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# evaluation metrics\nfrom sklearn.metrics import accuracy_score, roc_auc_score","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:06:43.720821Z","iopub.execute_input":"2022-05-08T16:06:43.721176Z","iopub.status.idle":"2022-05-08T16:06:43.728582Z","shell.execute_reply.started":"2022-05-08T16:06:43.721124Z","shell.execute_reply":"2022-05-08T16:06:43.727504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_columns', None)\n\n# set the warning off\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:06:43.730554Z","iopub.execute_input":"2022-05-08T16:06:43.730905Z","iopub.status.idle":"2022-05-08T16:06:43.745699Z","shell.execute_reply.started":"2022-05-08T16:06:43.730863Z","shell.execute_reply":"2022-05-08T16:06:43.744839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Index\n- [1. Describtive analysis (Univariant Analysis)](#1)\n    - [1.1 Common Data](#1.1)\n        - [1.1.1 Data type and Missing values Analysis](#1.1.1)\n        - [1.1.2 Zero variance Features](#1.1.2)\n    - [1.2 Numerical Data ](#1.2)\n        - [1.2.1 Quantile statistics](#1.2)\n        - [1.2.2 Descriptive analysis](#1.2)\n        - [1.2.3 Distribution Analysis](#1.2)\n            - [1.2.3.2 Outlier Detection](#1.2.3.2)\n    - [1.3 Categorical Data](#1.3)\n        - [1.3.1 Cordinality/ Unnique count ](#1.3.1)\n        - [1.3.2 Target Class balance check(only for classification)](#1.3.3)\n- [2. Correlation Analysis (Bivariant Analysis)](#2)\n- [3. Data Cleaning](#3)\n- [4. Hyper parameter tuning](#4)\n- [5. Modeling](#5)","metadata":{}},{"cell_type":"code","source":"# read the data\ndata = pd.read_csv(\"../input/tabular-playground-series-may-2022/train.csv\", index_col=0)\ntest = pd.read_csv(\"../input/tabular-playground-series-may-2022/test.csv\", index_col=0)\nsub = pd.read_csv(\"../input/tabular-playground-series-may-2022/sample_submission.csv\")\n\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:06:43.747121Z","iopub.execute_input":"2022-05-08T16:06:43.747526Z","iopub.status.idle":"2022-05-08T16:07:01.451772Z","shell.execute_reply.started":"2022-05-08T16:06:43.747484Z","shell.execute_reply":"2022-05-08T16:07:01.450875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:07:01.453743Z","iopub.execute_input":"2022-05-08T16:07:01.454066Z","iopub.status.idle":"2022-05-08T16:07:01.487334Z","shell.execute_reply.started":"2022-05-08T16:07:01.454023Z","shell.execute_reply":"2022-05-08T16:07:01.486652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"shape of the data --->\", data.shape)\nprint(\"shape of the test data --->\", test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:07:01.488628Z","iopub.execute_input":"2022-05-08T16:07:01.489084Z","iopub.status.idle":"2022-05-08T16:07:01.494511Z","shell.execute_reply.started":"2022-05-08T16:07:01.489051Z","shell.execute_reply":"2022-05-08T16:07:01.493708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"1\"></a>\n# <p style=\"background-color:#5811D3;font-family:newtimeroman;color:#FEDFA0;font-size:120%;text-align:center;border-radius:10px 10px;\">Descriptive Analysis</p>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"1.1\"></a>\n# <p style=\"background-color:#5811D3;font-family:newtimeroman;color:#FEDFA0;font-size:100%;text-align:left;border-radius:10px 10px;\">1.1 | Common data Analysis</p>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"1.1.1\"></a>\n# <p style=\"font-family:newtimeroman;color:#5811D3;font-size:100%;text-align:left;border-radius:10px 10px;\">1.1.1 | Data type and Missing values Analysis</p>","metadata":{}},{"cell_type":"code","source":"columns, categorical_col, numerical_col,missing_value_df = kaggle_utils.Common_data_analysis(data, missing_value_highlight_threshold=5.0, display_df = True,\n                                                                                         only_show_missing=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:07:01.496944Z","iopub.execute_input":"2022-05-08T16:07:01.497299Z","iopub.status.idle":"2022-05-08T16:07:07.352721Z","shell.execute_reply.started":"2022-05-08T16:07:01.497256Z","shell.execute_reply":"2022-05-08T16:07:07.351343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target = \"target\"\nnumerical_col.remove(target)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:07:07.355386Z","iopub.execute_input":"2022-05-08T16:07:07.357014Z","iopub.status.idle":"2022-05-08T16:07:07.363557Z","shell.execute_reply.started":"2022-05-08T16:07:07.356963Z","shell.execute_reply":"2022-05-08T16:07:07.362335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## observation\n- All features are numberical columns except f_27\n- No missing values :) ","metadata":{}},{"cell_type":"markdown","source":"<a id=\"1.1.2\"></a>\n# <p style=\"font-family:newtimeroman;color:#5811D3;font-size:100%;text-align:left;border-radius:10px 10px;\">1.1.2 | Zero variance Features</p>","metadata":{}},{"cell_type":"code","source":"zero_var_col = []\nfor col in data.columns:\n    if len(data[col].unique()) == 1:\n        print(data[col].unique(), end= \"\")\n        zero_var_col.append(col)\nprint()\nprint(zero_var_col, end=\"\")\nprint()\nprint(\"Total number of features having zero variance -->\", len(zero_var_col))","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:07:07.365272Z","iopub.execute_input":"2022-05-08T16:07:07.366002Z","iopub.status.idle":"2022-05-08T16:07:09.186187Z","shell.execute_reply.started":"2022-05-08T16:07:07.365965Z","shell.execute_reply":"2022-05-08T16:07:09.18522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"1.2\"></a>\n# <p style=\"background-color:#5811D3;font-family:newtimeroman;color:#FEDFA0;font-size:100%;text-align:left;border-radius:10px 10px;\">1.2 | Numerical Data -- descriptive, distribution, Quantitative</p>","metadata":{}},{"cell_type":"code","source":"kaggle_utils.numerical_data_analysis(data[numerical_col], numerical_col)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:07:09.187866Z","iopub.execute_input":"2022-05-08T16:07:09.18828Z","iopub.status.idle":"2022-05-08T16:07:11.66285Z","shell.execute_reply.started":"2022-05-08T16:07:09.18824Z","shell.execute_reply":"2022-05-08T16:07:11.661792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## observation\n- All features are within some range, example f_00 - f_06 are with the range of [-5, +5] -- continuous features\n- f_07 - f_18 seems to be a descrete numerical features with some range -- mostly 0 - 14 (may be a ordinal value)\n- Again f_19 - f_28 are within some range -- continuous values\n- f_29 and f_30 are ordinal values\n\n- **target** values has median zero -- so we have more than 50% of data is zero (may be a balanced dataset)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"1.2.3.2\"></a>\n# <p style=\"font-family:newtimeroman;color:#5811D3;font-size:100%;text-align:left;border-radius:10px 10px;\">1.2.3.2 | Outlier detection</p>","metadata":{}},{"cell_type":"code","source":"outlier_df = kaggle_utils.find_outliers_iqr_method(data)\nprint(\"Number of feature having outliers ---> \",len(outlier_df[outlier_df[\"Number of outliers\"] > 0]))\noutlier_df[outlier_df[\"Number of outliers\"] > 0].T","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:07:11.664642Z","iopub.execute_input":"2022-05-08T16:07:11.664985Z","iopub.status.idle":"2022-05-08T16:07:13.692349Z","shell.execute_reply.started":"2022-05-08T16:07:11.664944Z","shell.execute_reply":"2022-05-08T16:07:13.691226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_hist(data, columns, nrow, ncol, figsize, hue_value=None):\n    # find the distubution of the data. ( visualization would be so good)\n    fig, ax = plt.subplots(nrow,ncol, figsize=figsize)\n    # we have 9 numerical values.\n    col, row = ncol,nrow\n    col_count = 0\n    if row<=1:\n        for c in range(col):\n            if hue_value:\n                sns.histplot(data=data, x=columns[col_count],kde=True, hue=hue_value, ax=ax[c])\n            else:\n                sns.histplot(data=data, x=columns[col_count], kde=True, ax=ax[c])\n    else:\n        for r in range(row):\n            for c in range(col):\n                if col_count >= len(columns):\n                    ax[r,c].text(0.5, 0.5, \"no data\")\n                else:\n                    if hue_value:\n                        sns.histplot(data=data, x=columns[col_count],kde=True, hue=hue_value, ax=ax[r,c])\n                    else:\n                        sns.histplot(data=data, x=columns[col_count],kde=True,  ax=ax[r,c])\n                col_count +=1\ndef plot_boxplot(data, columns, nrow, ncol, figsize, hue_value=None):\n    # find the distubution of the data. ( visualization would be so good)\n    fig, ax = plt.subplots(nrow,ncol, figsize=figsize)\n    # we have 9 numerical values.\n    col, row = ncol,nrow\n    col_count = 0\n\n    for r in range(row):\n        for c in range(col):\n            if col_count >= len(columns):\n                ax[r,c].text(0.5, 0.5, \"no data\")\n            else:\n                if hue_value:\n                    sns.boxplot(data=data, x=columns[col_count],hue=hue_value, ax=ax[r,c])\n                else:\n                    sns.boxplot(data=data, x=columns[col_count], ax=ax[r,c])\n            col_count +=1\n\ndef plot_scatter(data,common_y, columns, nrow, ncol, figsize):\n    # find the distubution of the data. ( visualization would be so good)\n    fig, ax = plt.subplots(nrow,ncol, figsize=figsize)\n    # we have 9 numerical values.\n    col, row = ncol,nrow\n    col_count = 0\n\n    for r in range(row):\n        for c in range(col):\n            if col_count >= len(columns):\n                ax[r,c].text(0.5, 0.5, \"no data\")\n            else:\n                \n                sns.scatterplot(data=data, x=columns[col_count],y=common_y, ax=ax[r,c])\n            col_count +=1\n","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:07:13.693774Z","iopub.execute_input":"2022-05-08T16:07:13.694123Z","iopub.status.idle":"2022-05-08T16:07:13.71344Z","shell.execute_reply.started":"2022-05-08T16:07:13.693983Z","shell.execute_reply":"2022-05-08T16:07:13.712206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot the histogram\nplot_hist(data, numerical_col, 5,6, (30,30))","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:07:13.716876Z","iopub.execute_input":"2022-05-08T16:07:13.717438Z","iopub.status.idle":"2022-05-08T16:09:21.442863Z","shell.execute_reply.started":"2022-05-08T16:07:13.717389Z","shell.execute_reply":"2022-05-08T16:09:21.442032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_boxplot(data, numerical_col, 5, 6, figsize=(30,30))","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:09:21.444041Z","iopub.execute_input":"2022-05-08T16:09:21.444351Z","iopub.status.idle":"2022-05-08T16:09:26.175378Z","shell.execute_reply.started":"2022-05-08T16:09:21.444313Z","shell.execute_reply":"2022-05-08T16:09:26.174208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## observation\n- All cotinuoue features has very decent normal distribution centered zero. Having few outliers (will decide after the base model about outlier)\n- All Descrete features have large number of values in starting categories (say 1 -5) and very few data in other categories.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"1.3\"></a>\n# <p style=\"background-color:#5811D3;font-family:newtimeroman;color:#FEDFA0;font-size:100%;text-align:left;border-radius:10px 10px;\">1.3 | Categorical Data Analysis</p>","metadata":{}},{"cell_type":"code","source":"# number of categorical features\nprint(\"Categorical col --> \", categorical_col )","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:09:26.176959Z","iopub.execute_input":"2022-05-08T16:09:26.177596Z","iopub.status.idle":"2022-05-08T16:09:26.182775Z","shell.execute_reply.started":"2022-05-08T16:09:26.177535Z","shell.execute_reply":"2022-05-08T16:09:26.182004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"1.3.1\"></a>\n# <p style=\"font-family:newtimeroman;color:#5811D3;font-size:100%;text-align:left;border-radius:10px 10px;\">1.3.1 | Cordinality/ Unnique count</p>","metadata":{}},{"cell_type":"code","source":"data[categorical_col[0]].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:09:26.183769Z","iopub.execute_input":"2022-05-08T16:09:26.184292Z","iopub.status.idle":"2022-05-08T16:09:27.151059Z","shell.execute_reply.started":"2022-05-08T16:09:26.184256Z","shell.execute_reply":"2022-05-08T16:09:27.150214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nunique = data[categorical_col[0]].nunique()\ndata_len = data.shape[0]\n\nprint(\"we have {} unique values in our dataset having {} samples ----> {}% different categories\".format(nunique, data_len, (nunique/data_len)*100))","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:09:27.152599Z","iopub.execute_input":"2022-05-08T16:09:27.152927Z","iopub.status.idle":"2022-05-08T16:09:27.441436Z","shell.execute_reply.started":"2022-05-08T16:09:27.152882Z","shell.execute_reply":"2022-05-08T16:09:27.440302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## observation \n- We can't use onhot encoding --  we can't use ordinal encoding -- Fequency encoding is also not a better idea since most of the categories having single sample.\n- Seems like each values has some pattern, we can make use of that pattern to generate some meaningful feature.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"1.3.2\"></a>\n# <p style=\"font-family:newtimeroman;color:#5811D3;font-size:100%;text-align:left;border-radius:10px 10px;\">1.3.2 | Target Class balance check(only for classification)</p>","metadata":{}},{"cell_type":"code","source":"# mostly has equal number of samples in both classes\nsns.countplot(data=data, y=target)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:09:27.44303Z","iopub.execute_input":"2022-05-08T16:09:27.443498Z","iopub.status.idle":"2022-05-08T16:09:27.669117Z","shell.execute_reply.started":"2022-05-08T16:09:27.443449Z","shell.execute_reply":"2022-05-08T16:09:27.668119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count = data[target].value_counts()\nprint(count)\nprint(\"percentage of first class --- >\",count[0]/data.shape[0])\nprint(\"percentage of second class --->\", count[1]/data.shape[0])","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:09:27.670596Z","iopub.execute_input":"2022-05-08T16:09:27.670817Z","iopub.status.idle":"2022-05-08T16:09:27.683321Z","shell.execute_reply.started":"2022-05-08T16:09:27.67079Z","shell.execute_reply":"2022-05-08T16:09:27.682342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2\"></a>\n# <p style=\"background-color:#5811D3;font-family:newtimeroman;color:#FEDFA0;font-size:100%;text-align:left;border-radius:10px 10px;\">2 ) Correlation Analysis (Bivariant Analysis)</p>","metadata":{}},{"cell_type":"markdown","source":"# <p style=\"font-family:newtimeroman;color:#5811D3;font-size:100%;text-align:left;border-radius:10px 10px;\"> 2.1 | Corrilation between variables</p>","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(25,8))\nsns.heatmap(data.corr(), annot=True, cbar=True, cmap=\"YlGnBu\")","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:09:27.684748Z","iopub.execute_input":"2022-05-08T16:09:27.684997Z","iopub.status.idle":"2022-05-08T16:09:35.246567Z","shell.execute_reply.started":"2022-05-08T16:09:27.684966Z","shell.execute_reply":"2022-05-08T16:09:35.24552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_df = data.corr()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:09:35.248546Z","iopub.execute_input":"2022-05-08T16:09:35.248861Z","iopub.status.idle":"2022-05-08T16:09:37.759371Z","shell.execute_reply.started":"2022-05-08T16:09:35.248823Z","shell.execute_reply":"2022-05-08T16:09:37.758346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def find_high_correlation(corr_df, threshold):\n    col_corr_dict = {}\n    for row in range(len(corr_df.columns)):\n        for col in range(row): # the upper half and lower half is same -- so checking to half way is enough\n            if corr_df.iloc[row, col] >= threshold or corr_df.iloc[row, col] <= -threshold:\n                #print(corr_df.columns[col], corr_df.index[row])\n                col_corr_dict[corr_df.index[row]] = [corr_df.columns[col], corr_df.iloc[row, col]] # these two values are highly correlated\n    return col_corr_dict\n\nthreshold = 0.40\ncol_corr_dict = find_high_correlation(corr_df, threshold)\n\nprint(col_corr_dict)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:09:37.760907Z","iopub.execute_input":"2022-05-08T16:09:37.761213Z","iopub.status.idle":"2022-05-08T16:09:37.805094Z","shell.execute_reply.started":"2022-05-08T16:09:37.761178Z","shell.execute_reply":"2022-05-08T16:09:37.804173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_df","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:09:37.806434Z","iopub.execute_input":"2022-05-08T16:09:37.806688Z","iopub.status.idle":"2022-05-08T16:09:37.87595Z","shell.execute_reply.started":"2022-05-08T16:09:37.806649Z","shell.execute_reply":"2022-05-08T16:09:37.87486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"font-family:newtimeroman;color:#5811D3;font-size:100%;text-align:left;border-radius:10px 10px;\"> 2.2 | correlation with target</p>","metadata":{}},{"cell_type":"code","source":"corr_with_target = corr_df[\"target\"]\ncorr_with_target.to_frame().T","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:09:37.877606Z","iopub.execute_input":"2022-05-08T16:09:37.877948Z","iopub.status.idle":"2022-05-08T16:09:37.909857Z","shell.execute_reply.started":"2022-05-08T16:09:37.877903Z","shell.execute_reply":"2022-05-08T16:09:37.908886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_with_target = corr_with_target[:-1]","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:09:37.911089Z","iopub.execute_input":"2022-05-08T16:09:37.911316Z","iopub.status.idle":"2022-05-08T16:09:37.920336Z","shell.execute_reply.started":"2022-05-08T16:09:37.911291Z","shell.execute_reply":"2022-05-08T16:09:37.919543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# distribution of correlation\n# distirbution of the correlation with the target value\nplt.title(\"Correlation distribution between the variables and target\")\nplt.xlabel(\"Correlation with target\")\nplt.ylabel(\"Number of features\")\nsns.histplot(corr_with_target)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:09:37.923617Z","iopub.execute_input":"2022-05-08T16:09:37.923897Z","iopub.status.idle":"2022-05-08T16:09:38.17607Z","shell.execute_reply.started":"2022-05-08T16:09:37.923857Z","shell.execute_reply":"2022-05-08T16:09:38.175018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# observation | from whole EDA\n- **No null** values :)\n- **No zero variance** feature :)\n- **One categorical** value -- Need some custom handling method for this.\n- **Not so much correlated** features :)\n- All features are **almost normally** distributed :)\n- All features has **outliers** :( --> but since they are normally distributed we can ignore this for now.\n\n- No datetime features :)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3\"></a>\n# <p style=\"background-color:#5811D3;font-family:newtimeroman;color:#FEDFA0;font-size:100%;text-align:left;border-radius:10px 10px;\"> 3 ) Data Cleaning</p>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"1.3.1\"></a>\n# <p style=\"font-family:newtimeroman;color:#5811D3;font-size:100%;text-align:left;border-radius:10px 10px;\">Decoding f_27</p>","metadata":{}},{"cell_type":"code","source":"data[\"f_27\"] # this is a length 10 string ","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:09:38.177089Z","iopub.execute_input":"2022-05-08T16:09:38.177479Z","iopub.status.idle":"2022-05-08T16:09:38.185591Z","shell.execute_reply.started":"2022-05-08T16:09:38.177442Z","shell.execute_reply":"2022-05-08T16:09:38.184749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# distibution of letter in this feature --  from \"Exploring the string feature notebook\"\nunique_letters = set(())\nfrom collections import Counter\nc = Counter()\ncps = [Counter() for _ in range(10)]\nfor i,astr in enumerate(data['f_27']):\n   \n    letters = [str(x) for x in astr]\n    unique_letters.update(letters)\n    c.update(letters)\n    for i in range(10):\n        cps[i].update([letters[i],])\n    if i == 10**12:\n        break\n\nletter_freqs = pd.DataFrame(c.items())\nletter_freqs.columns = ['letter', 'count']\n_=sns.barplot(data=letter_freqs.sort_values(by=['letter']), x='letter', y='count')","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:09:38.187Z","iopub.execute_input":"2022-05-08T16:09:38.18777Z","iopub.status.idle":"2022-05-08T16:10:01.958496Z","shell.execute_reply.started":"2022-05-08T16:09:38.187724Z","shell.execute_reply":"2022-05-08T16:10:01.957412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This has values from A - T . what we are going to decide this \n- Going to crete 10 features representing which letter present in each position \n- Gonna add 'unique_feature_count' -- which will contain the unhique features in that string\n- Gonna add 'sum_of_string\" -- will contain the summ of all letter in the string","metadata":{}},{"cell_type":"code","source":"for i in range(10):\n    data[\"position_\"+ str(i)] = data.f_27.str.get(i).apply(ord) - ord('A') + 1\n    test[\"position_\"+ str(i)] = test.f_27.str.get(i).apply(ord) - ord('A') + 1\n# add unique_feature_count -- this feature from https://www.kaggle.com/code/cabaxiom/tps-may-22-eda-lgbm-model\ndata[\"unique_feature_count\"] = data.f_27.apply(lambda s: len(set(s)))\ntest[\"unique_feature_count\"] = test.f_27.apply(lambda s: len(set(s)))\n\n# # add sum_of_string\ndata[\"sum_of_string\"] = data.iloc[:,32:42].sum(axis=1)\ntest[\"sum_of_string\"] = test.iloc[:,32:42].sum(axis=1)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:10:01.959954Z","iopub.execute_input":"2022-05-08T16:10:01.960685Z","iopub.status.idle":"2022-05-08T16:10:23.078797Z","shell.execute_reply.started":"2022-05-08T16:10:01.960641Z","shell.execute_reply":"2022-05-08T16:10:23.078053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.drop(\"f_27\", axis= 1, inplace=True)\ntest.drop(\"f_27\", axis =1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:10:23.079912Z","iopub.execute_input":"2022-05-08T16:10:23.080974Z","iopub.status.idle":"2022-05-08T16:10:23.545249Z","shell.execute_reply.started":"2022-05-08T16:10:23.080926Z","shell.execute_reply":"2022-05-08T16:10:23.54419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"1.3.1\"></a>\n# <p style=\"font-family:newtimeroman;color:#5811D3;font-size:100%;text-align:left;border-radius:10px 10px;\"> Scaling data</p>","metadata":{}},{"cell_type":"code","source":"data1 = data.copy()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:10:23.55045Z","iopub.execute_input":"2022-05-08T16:10:23.550722Z","iopub.status.idle":"2022-05-08T16:10:23.686415Z","shell.execute_reply.started":"2022-05-08T16:10:23.550692Z","shell.execute_reply":"2022-05-08T16:10:23.685407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = data1.copy()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:10:23.687891Z","iopub.execute_input":"2022-05-08T16:10:23.68816Z","iopub.status.idle":"2022-05-08T16:10:23.853781Z","shell.execute_reply.started":"2022-05-08T16:10:23.688112Z","shell.execute_reply":"2022-05-08T16:10:23.85273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = data.drop(\"target\", axis=1)\n\nstand_scal = StandardScaler()\nstand_scal.fit(data)\ndata = pd.DataFrame(stand_scal.transform(data), columns=data.columns)\ntest = pd.DataFrame(stand_scal.transform(test), columns=test.columns)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:10:23.85536Z","iopub.execute_input":"2022-05-08T16:10:23.855717Z","iopub.status.idle":"2022-05-08T16:10:25.050346Z","shell.execute_reply.started":"2022-05-08T16:10:23.855668Z","shell.execute_reply":"2022-05-08T16:10:25.049158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:10:25.051593Z","iopub.execute_input":"2022-05-08T16:10:25.05182Z","iopub.status.idle":"2022-05-08T16:10:25.107719Z","shell.execute_reply.started":"2022-05-08T16:10:25.051791Z","shell.execute_reply":"2022-05-08T16:10:25.10675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4\"></a>\n# <p style=\"background-color:#5811D3;font-family:newtimeroman;color:#FEDFA0;font-size:100%;text-align:left;border-radius:10px 10px;\"> 3 | Base models</p>","metadata":{}},{"cell_type":"code","source":"trainable_features = data.columns.to_list()\n#trainable_features.remove(\"target\")\n\ndata_features = data[trainable_features].copy()\ntarget_feature = data1[\"target\"].copy()\nX_train, X_val, y_train, y_val = train_test_split(data_features, target_feature, train_size=0.2, random_state=24)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:10:25.108934Z","iopub.execute_input":"2022-05-08T16:10:25.109179Z","iopub.status.idle":"2022-05-08T16:10:25.837907Z","shell.execute_reply.started":"2022-05-08T16:10:25.109149Z","shell.execute_reply":"2022-05-08T16:10:25.836995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# logestic regression\nlog_reg = LogisticRegression()\nlog_reg.fit(X_train,y_train)\ny_pred = log_reg.predict(X_val)\nprint(\"Accuracy --->\", accuracy_score(y_true=y_val,y_pred=y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:10:25.839632Z","iopub.execute_input":"2022-05-08T16:10:25.83996Z","iopub.status.idle":"2022-05-08T16:10:26.523406Z","shell.execute_reply.started":"2022-05-08T16:10:25.839919Z","shell.execute_reply":"2022-05-08T16:10:26.522368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nran_forest = RandomForestClassifier()\nran_forest.fit(X_train,y_train)\ny_pred = ran_forest.predict(X_val)\nprint(\"Accuracy --->\", accuracy_score(y_true=y_val,y_pred=y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:10:26.524746Z","iopub.execute_input":"2022-05-08T16:10:26.524996Z","iopub.status.idle":"2022-05-08T16:12:42.852687Z","shell.execute_reply.started":"2022-05-08T16:10:26.524957Z","shell.execute_reply":"2022-05-08T16:12:42.851853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # running more than one hour without any results :( -- leave this algo for this dataset\n# svc = SVC()\n# svc.fit(X_train,y_train)\n# y_pred = svc.predict(X_val)\n# print(\"Accuracy --->\", accuracy_score(y_true=y_val,y_pred=y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:12:42.854411Z","iopub.execute_input":"2022-05-08T16:12:42.854914Z","iopub.status.idle":"2022-05-08T16:12:42.858853Z","shell.execute_reply.started":"2022-05-08T16:12:42.854864Z","shell.execute_reply":"2022-05-08T16:12:42.85822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# knn = KNeighborsClassifier()\n# knn.fit(X_train,y_train)\n# y_pred = knn.predict(X_val)\n# print(\"Accuracy --->\", accuracy_score(y_true=y_val,y_pred=y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:12:42.859921Z","iopub.execute_input":"2022-05-08T16:12:42.860533Z","iopub.status.idle":"2022-05-08T16:12:42.876746Z","shell.execute_reply.started":"2022-05-08T16:12:42.8605Z","shell.execute_reply":"2022-05-08T16:12:42.875743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Logestic regression -- Checking Assumption, Why it is not a best model here ","metadata":{}},{"cell_type":"markdown","source":"## Assumptions of logestic regerssion\n- 1. Appropriate Outcome Type\n- 2. Sufficiently large sample size\n- 3. Absence of multicollinearity\n- 4. No strongly influential outliers\n- 5. Linearity of independent variables and log-odds\n- 6. Independent of Abservation","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"### ASSUMPTION 1 -- you should have a binary classification problem -- logostic regression only works with binary class problems \n- THIS PROBLEM -- ASSUMPTION 1 --- SATISFIED :) :)","metadata":{}},{"cell_type":"code","source":"\ndata1[target].nunique()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:12:42.878544Z","iopub.execute_input":"2022-05-08T16:12:42.879231Z","iopub.status.idle":"2022-05-08T16:12:42.899023Z","shell.execute_reply.started":"2022-05-08T16:12:42.87918Z","shell.execute_reply":"2022-05-08T16:12:42.898327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"### ASSUMPTION 2\n- evry categorical feature must have >=10 samples per each target class  or   have atleast 500 samples in total\n\n- THIS PROBLEM -- ASSUMPTION 2 --- SATISFIED :) :)","metadata":{}},{"cell_type":"code","source":"\nprint(data.shape[0])\nfor i in data.columns.to_list():\n    if data.dtypes[i] == 'uint8': # keep only col values\n        data[i].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:12:42.900555Z","iopub.execute_input":"2022-05-08T16:12:42.90101Z","iopub.status.idle":"2022-05-08T16:12:42.910523Z","shell.execute_reply.started":"2022-05-08T16:12:42.900969Z","shell.execute_reply":"2022-05-08T16:12:42.90982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"### ASSUMPTION 3 -- Absence of multicollinearity\n- there should not be any features has correlated with any other features (no two/three / more featres linearly correlated)\n- can be found by correlation matrix -- Correlation matrix can be difficult to interpret when there are many independent variables\n- Furthermore, not all collinearity problems can be detected by inspection of the correlation matrix: it is possible for collinearity to exist between three or more variables even if no pair of variables has a particularly high correlation.\n\n- VIF Variance Inflation Factor can be used\n(Include constant in VIF calculation. Reference: https://stackoverflow.com/questions/42658379/variance-inflation-factor-in-python)\n\n- VIF\n    - The threshold for VIF is usually 5 (i.e. values above 5 means there is presence of multicollinearity)\n    - Since all the variables have VIF <5, it means that there is no multicollinearity, and this assumption is satisfied\n    - Let's have a look at the situation where we did not drop the first variable upon getting dummies:\n","metadata":{}},{"cell_type":"code","source":"\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n# Use variance inflation factor to identify any significant multi-collinearity\ndef calc_vif(df):\n    vif = pd.DataFrame()\n    vif[\"variables\"] = df.columns\n    vif[\"VIF\"] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]\n    return(vif)\n\ncalc_vif(data)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:12:42.911688Z","iopub.execute_input":"2022-05-08T16:12:42.912284Z","iopub.status.idle":"2022-05-08T16:16:50.18424Z","shell.execute_reply.started":"2022-05-08T16:12:42.912247Z","shell.execute_reply":"2022-05-08T16:16:50.183003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- If there is perfect correlation, then VIF = infinity. This shows a perfect correlation between two independent variables. In the case of perfect correlation, we get R2 =1, which lead to 1/(1-R2) infinity. To solve this problem we need to drop one of the variables from the dataset which is causing this perfect multicollinearity.","metadata":{}},{"cell_type":"code","source":"keep_col = [col for col in data.columns if col.startswith(\"f_\")]\ndata_without_f_27 = data[keep_col]\ndata_without_f_27","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:16:50.189311Z","iopub.execute_input":"2022-05-08T16:16:50.193589Z","iopub.status.idle":"2022-05-08T16:16:50.340014Z","shell.execute_reply.started":"2022-05-08T16:16:50.193511Z","shell.execute_reply":"2022-05-08T16:16:50.338854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"calc_vif(data_without_f_27)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:16:50.342004Z","iopub.execute_input":"2022-05-08T16:16:50.342356Z","iopub.status.idle":"2022-05-08T16:18:50.415941Z","shell.execute_reply.started":"2022-05-08T16:16:50.342312Z","shell.execute_reply":"2022-05-08T16:18:50.415027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# try to fit the logestic regresion model here\ndata_features = data_without_f_27\ntarget_feature = data1[\"target\"].copy()\nX_train, X_val, y_train, y_val = train_test_split(data_features, target_feature, train_size=0.2, random_state=24)\n\n\n# logestic regression\nlog_reg = LogisticRegression()\nlog_reg.fit(X_train,y_train)\ny_pred = log_reg.predict(X_val)\nprint(\"Accuracy --->\", accuracy_score(y_true=y_val,y_pred=y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:19:37.220054Z","iopub.execute_input":"2022-05-08T16:19:37.220529Z","iopub.status.idle":"2022-05-08T16:19:38.13775Z","shell.execute_reply.started":"2022-05-08T16:19:37.220487Z","shell.execute_reply":"2022-05-08T16:19:38.136689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"### ASSUMPTION  4  -- No strongly influential outliers\n- Influential values are extreme individual data points that can alter the quality of the logistic regression model.\n- Cook’s Distance is an estimate of the influence of a data point. It takes into account both the leverage and residual of each observation. Cook’s Distance is a summary of how much a regression model changes when the ith observation is removed.\n- A general rule of thumb is that any observation with a Cook’s distance greater than 4/n (where n = total observations) is considered to be influential (https://www.statology.org/cooks-distance-python/ and https://www.scikit-yb.org/en/latest/api/regressor/influence.html?highlight=cook#module-yellowbrick.regressor.influence), though there are even more generic cutoff values of >0.5-1.0.\n\n- For outliers, we can use the absolute standardized residuals to identify them (std resid > 3) ( which are all the data has standarnd deviation above +/- 3 is called outliers -- only applies for nor mal distribution)","metadata":{}},{"cell_type":"markdown","source":"#### ","metadata":{}},{"cell_type":"code","source":"# from statsmodels.genmod.generalized_linear_model import GLM\n# from statsmodels.genmod import families\n\n# # Use GLM method for logreg here so that we can retrieve the influence measures\n# logit_model = GLM(target_feature, data, family=families.Binomial())\n# logit_results = logit_model.fit()\n# print(logit_results.summary())\n","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:18:50.564989Z","iopub.status.idle":"2022-05-08T16:18:50.565548Z","shell.execute_reply.started":"2022-05-08T16:18:50.565272Z","shell.execute_reply":"2022-05-08T16:18:50.565302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from scipy import stats\n\n# # Get influence measures\n# influence = logit_results.get_influence()\n\n# # Obtain summary df of influence measures\n# summ_df = influence.summary_frame()\n\n# # Filter summary df to Cook distance\n# diagnosis_df = summ_df.loc[:,['cooks_d']]\n\n# # Append absolute standardized residual values\n# diagnosis_df['std_resid'] = stats.zscore(logit_results.resid_pearson)\n# diagnosis_df['std_resid'] = diagnosis_df.loc[:,'std_resid'].apply(lambda x: np.abs(x))\n\n# # Sort by Cook's Distance\n# diagnosis_df.sort_values(\"cooks_d\", ascending=False)\n# diagnosis_df","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:18:50.567091Z","iopub.status.idle":"2022-05-08T16:18:50.567574Z","shell.execute_reply.started":"2022-05-08T16:18:50.567335Z","shell.execute_reply":"2022-05-08T16:18:50.567362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Set Cook's distance threshold\n# cook_threshold = 4 / len(X)\n# print(f\"Threshold for Cook Distance = {cook_threshold}\")\n# Plot influence measures (Cook's distance)\n# fig = influence.plot_index(y_var=\"cooks\", threshold=cook_threshold)\n# plt.axhline(y=cook_threshold, ls=\"--\", color='red')\n# fig.tight_layout(pad=2)\n# # Find number of observations that exceed Cook's distance threshold\n# outliers = diagnosis_df[diagnosis_df['cooks_d'] > cook_threshold]\n# prop_outliers = round(100*(len(outliers) / len(X)),1)\n# print(f'Proportion of data points that are highly influential = {prop_outliers}%')\n# # Find number of observations which are BOTH outlier (std dev > 3) and highly influential\n# extreme = diagnosis_df[(diagnosis_df['cooks_d'] > cook_threshold) & \n#                        (diagnosis_df['std_resid'] > 3)]\n# prop_extreme = round(100*(len(extreme) / len(X)),1)\n# print(f'Proportion of highly influential outliers = {prop_extreme}%')\n\n\n\n\n\n# remove the outliers or not is decided by the domain expert -- here we don't know what the features are first of all so we can't say a datapoint is a outlier or not","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:18:50.56948Z","iopub.status.idle":"2022-05-08T16:18:50.569976Z","shell.execute_reply.started":"2022-05-08T16:18:50.569715Z","shell.execute_reply":"2022-05-08T16:18:50.569741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"### ASSUMPTION  5  -- Linearity of independent variables and log odds\nBox-Tidwell Test\n- One of the important assumptions of logistic regression is the linearity of the logit over the continuous covariates. This assumption means that relationships between the continuous predictors and the logit (log odds) is linear.\n- The Box-Tidwell transformation (test) can be used to test the linearity in the logit assumption when performing logistic regression.\n- It checks whether the logit transform is a linear function of the predictor, effectively adding the non-linear transform of the original predictor as an interaction term to test if this addition made no better prediction.\n- A statistically significant p-value of the interaction term in the Box-Tidwell transformation means that the linearity assumption is violated\n- If one variable is indeed found to be non-linear, then we can resolve it by incorporating higher order polynomial terms for that variable in the regression analysis to capture the non-linearity (e.g. x^2) .- Another solution to this problem is the - categorization of the independent variables. That is transforming metric variables to ordinal level and then including them in the model.\n\n- I am going to use visual check here . \n- that is plot between features and log(p / 1-p ) shold be linear -- p is probability of a sample being positve","metadata":{}},{"cell_type":"code","source":"data_features = data_without_f_27\ntarget_feature = data1[\"target\"].copy()\n#X_train, X_val, y_train, y_val = train_test_split(data_features, target_feature, train_size=0.2, random_state=24)\n\n\n# logestic regression\nlog_reg = LogisticRegression()\nlog_reg.fit(data_features,target_feature)\ny_pred = log_reg.predict_proba(data_features)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:19:53.203087Z","iopub.execute_input":"2022-05-08T16:19:53.203395Z","iopub.status.idle":"2022-05-08T16:19:54.836789Z","shell.execute_reply.started":"2022-05-08T16:19:53.203364Z","shell.execute_reply":"2022-05-08T16:19:54.835734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred # first element is probability of sample being +ve -- second is probability of sample being -ve\n","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:19:54.839119Z","iopub.execute_input":"2022-05-08T16:19:54.839752Z","iopub.status.idle":"2022-05-08T16:19:54.849639Z","shell.execute_reply.started":"2022-05-08T16:19:54.839704Z","shell.execute_reply":"2022-05-08T16:19:54.848729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get log odds values\nlog_odds = np.log(y_pred[:,0] / (1 - y_pred[:,0]))\n\n# Visualize predictor continuous variable vs logit values (Age)\nplot_scatter(data_without_f_27, log_odds, data_without_f_27.columns.to_list(), 5,6, figsize=(30,30))","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:19:54.854162Z","iopub.execute_input":"2022-05-08T16:19:54.858727Z","iopub.status.idle":"2022-05-08T16:21:34.298936Z","shell.execute_reply.started":"2022-05-08T16:19:54.858652Z","shell.execute_reply":"2022-05-08T16:21:34.297404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Can't say all the featres are linearly related to logodds -- better to tranform the data or  try some complex model ","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"### ASSUMPTIONS 6 -- Independence of observations\n- Error terms need to be independent. That is that the data-points should not be from any dependent samples design, e.g., before-after measurements, or matched pairings\n\n- Here the samples are indepentant -- so assumption satisfied","metadata":{}},{"cell_type":"code","source":"# # can be check by\n# # Setup logistic regression model (using GLM method so that we can retrieve residuals)\n# logit_model = GLM(y, X_constant, family=families.Binomial())\n# logit_results = logit_model.fit()\n# print(logit_results.summary())\n# # Generate residual series plot\n# fig = plt.figure(figsize=(8,5))\n# ax = fig.add_subplot(111, title=\"Residual Series Plot\",\n#                     xlabel=\"Index Number\", ylabel=\"Deviance Residuals\")\n\n# # ax.plot(X.index.tolist(), stats.zscore(logit_results.resid_pearson))\n# ax.plot(X.index.tolist(), stats.zscore(logit_results.resid_deviance))\n# plt.axhline(y=0, ls=\"--\", color='red');\n\n\n# credits = https://towardsdatascience.com/assumptions-of-logistic-regression-clearly-explained-44d85a22b290\n# https://github.com/kennethleungty/Logistic-Regression-Assumptions/blob/main/Logistic_Regression_Assumptions.ipynb","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:18:50.577064Z","iopub.status.idle":"2022-05-08T16:18:50.577552Z","shell.execute_reply.started":"2022-05-08T16:18:50.577298Z","shell.execute_reply":"2022-05-08T16:18:50.577323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Assumption 1 -- YES\n- Assumption 2 -- YES\n- Assumption 3 -- YES\n- Assumption 4 -- NO\n- Assumption 5 -- NO\n- Assumption 6 -- YES","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"- Overfitting / Underfitting -- CURRECT FIT","metadata":{}},{"cell_type":"code","source":"trainable_features = data.columns.to_list()\n#trainable_features.remove(\"target\")\n\ndata_features = data[trainable_features].copy()\ntarget_feature = data1[\"target\"].copy()\nX_train, X_val, y_train, y_val = train_test_split(data_features, target_feature, train_size=0.2, random_state=24)\n# logestic regression\nlog_reg = LogisticRegression()\nlog_reg.fit(X_train,y_train)\ny_pred_train = log_reg.predict(X_train)\ny_pred_val = log_reg.predict(X_val)\nprint(\"Accuracy for training data--->\", accuracy_score(y_true=y_train,y_pred=y_pred_train))\nprint(\"Accuracy for validation data--->\", accuracy_score(y_true=y_val,y_pred=y_pred_val))","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:22:53.548413Z","iopub.execute_input":"2022-05-08T16:22:53.548713Z","iopub.status.idle":"2022-05-08T16:22:54.97226Z","shell.execute_reply.started":"2022-05-08T16:22:53.548683Z","shell.execute_reply":"2022-05-08T16:22:54.971262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"### Try removing outliers\n","metadata":{}},{"cell_type":"code","source":"continuous_features = [col for col in data.columns.to_list() if data[col].nunique() > 15]\nprint(continuous_features)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:23:02.891454Z","iopub.execute_input":"2022-05-08T16:23:02.891734Z","iopub.status.idle":"2022-05-08T16:23:04.139553Z","shell.execute_reply.started":"2022-05-08T16:23:02.891705Z","shell.execute_reply":"2022-05-08T16:23:04.138521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# i am gonna use un scaled feature \nlower_limit , upper_limit = kaggle_utils.find_outlier_z_score_method(data1[continuous_features], return_limits=True)\nlower_limit[\"upper_limit\"] = upper_limit[\"upper limit\"]\nlower_limit","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:23:04.141394Z","iopub.execute_input":"2022-05-08T16:23:04.141651Z","iopub.status.idle":"2022-05-08T16:23:04.615857Z","shell.execute_reply.started":"2022-05-08T16:23:04.14162Z","shell.execute_reply":"2022-05-08T16:23:04.614807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(data1.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:23:04.617066Z","iopub.execute_input":"2022-05-08T16:23:04.617319Z","iopub.status.idle":"2022-05-08T16:23:04.621971Z","shell.execute_reply.started":"2022-05-08T16:23:04.617284Z","shell.execute_reply":"2022-05-08T16:23:04.621404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lower_limit.loc[2, \"lower limit\"]","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:23:04.62356Z","iopub.execute_input":"2022-05-08T16:23:04.624078Z","iopub.status.idle":"2022-05-08T16:23:04.641922Z","shell.execute_reply.started":"2022-05-08T16:23:04.624043Z","shell.execute_reply":"2022-05-08T16:23:04.641071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"drop_index = set()\nfor index,col in enumerate(continuous_features):\n    for sample_index,sample in enumerate(data1[col]):\n        if sample < lower_limit.loc[index, \"lower limit\"] or  sample > lower_limit.loc[index, \"upper_limit\"]:\n            drop_index.add(sample_index)\ndrop_index","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:23:04.643403Z","iopub.execute_input":"2022-05-08T16:23:04.644325Z","iopub.status.idle":"2022-05-08T16:31:56.18794Z","shell.execute_reply.started":"2022-05-08T16:23:04.644282Z","shell.execute_reply":"2022-05-08T16:31:56.18693Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(drop_index)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:31:56.189315Z","iopub.execute_input":"2022-05-08T16:31:56.189584Z","iopub.status.idle":"2022-05-08T16:31:56.196304Z","shell.execute_reply.started":"2022-05-08T16:31:56.189554Z","shell.execute_reply":"2022-05-08T16:31:56.195363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(drop_index) / data1.shape[0] * 100 # i decided to loss this 8% of data","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:31:56.198558Z","iopub.execute_input":"2022-05-08T16:31:56.198802Z","iopub.status.idle":"2022-05-08T16:31:56.215203Z","shell.execute_reply.started":"2022-05-08T16:31:56.198773Z","shell.execute_reply":"2022-05-08T16:31:56.214443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data2 = data1.copy()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:31:56.217007Z","iopub.execute_input":"2022-05-08T16:31:56.21798Z","iopub.status.idle":"2022-05-08T16:31:56.229517Z","shell.execute_reply.started":"2022-05-08T16:31:56.217934Z","shell.execute_reply":"2022-05-08T16:31:56.228244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data1.drop(drop_index, axis=0, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:31:56.232749Z","iopub.execute_input":"2022-05-08T16:31:56.233532Z","iopub.status.idle":"2022-05-08T16:31:56.506085Z","shell.execute_reply.started":"2022-05-08T16:31:56.233481Z","shell.execute_reply":"2022-05-08T16:31:56.505184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data1.shape","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:31:56.507535Z","iopub.execute_input":"2022-05-08T16:31:56.508165Z","iopub.status.idle":"2022-05-08T16:31:56.516577Z","shell.execute_reply.started":"2022-05-08T16:31:56.508093Z","shell.execute_reply":"2022-05-08T16:31:56.515588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# build the model with this data\ndata_features = data1\ntarget_feature = data1[\"target\"].copy()\ndata_features = data1.drop(\"target\",axis=1)\nX_train, X_val, y_train, y_val = train_test_split(data_features, target_feature, train_size=0.2, random_state=24)\n\n\n# logestic regression\nlog_reg = LogisticRegression()\nlog_reg.fit(X_train,y_train)\ny_pred = log_reg.predict(X_val)\nprint(\"Accuracy -->\", accuracy_score(y_val, y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-05-08T16:31:56.517837Z","iopub.execute_input":"2022-05-08T16:31:56.518046Z","iopub.status.idle":"2022-05-08T16:32:00.5038Z","shell.execute_reply.started":"2022-05-08T16:31:56.518019Z","shell.execute_reply":"2022-05-08T16:32:00.502707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### No use -- better to go with some complex model\n### resons\n- the target label has no linear correlation with the features\n- Having  outliers\n- others things are very hard to understand for me .. since i am starting this theory checklist now -- need to learn lot of statistical and mathematical terms to describe more\n","metadata":{}},{"cell_type":"markdown","source":"# SVM - loading ..... :)","metadata":{}},{"cell_type":"markdown","source":"# Conclusion\n- Why i am trying some basic models? \nI know Deep learning/ XGBoost will give the top scores...... but, I wanna learn how all( alomost know) models working from this ..... ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}