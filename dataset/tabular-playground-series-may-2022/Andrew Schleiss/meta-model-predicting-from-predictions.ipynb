{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import Libraries ","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\nimport seaborn as sns\n\nfrom IPython.display import display\nimport datetime\nimport scipy.stats\nimport math\nimport random\n\nimport lightgbm as lgb\n\nfrom sklearn.model_selection import KFold, StratifiedKFold, train_test_split\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.calibration import CalibrationDisplay\nfrom sklearn.preprocessing import StandardScaler, QuantileTransformer,LabelEncoder\nfrom sklearn.linear_model import LinearRegression\n\nfrom sklearn.calibration import calibration_curve, CalibratedClassifierCV\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, LearningRateScheduler, EarlyStopping\nfrom tensorflow.keras.layers import Dense, Input, InputLayer, Add\nfrom tensorflow.keras.utils import plot_model","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-27T08:04:17.410839Z","iopub.execute_input":"2022-05-27T08:04:17.411656Z","iopub.status.idle":"2022-05-27T08:04:17.41962Z","shell.execute_reply.started":"2022-05-27T08:04:17.411613Z","shell.execute_reply":"2022-05-27T08:04:17.41909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CALIBRATION = True\nSCALING = True\n\nEPOCHS = 2000\nEARLY_STOP = 30 ","metadata":{"execution":{"iopub.status.busy":"2022-05-27T08:04:17.484149Z","iopub.execute_input":"2022-05-27T08:04:17.484772Z","iopub.status.idle":"2022-05-27T08:04:17.489603Z","shell.execute_reply.started":"2022-05-27T08:04:17.484721Z","shell.execute_reply":"2022-05-27T08:04:17.488996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Get competition data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/tabular-playground-series-may-2022/train.csv')\ntest = pd.read_csv('../input/tabular-playground-series-may-2022/test.csv')\nsub = pd.read_csv(\"../input/tabular-playground-series-may-2022/sample_submission.csv\",index_col = 0)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T08:04:17.52999Z","iopub.execute_input":"2022-05-27T08:04:17.530472Z","iopub.status.idle":"2022-05-27T08:04:26.587318Z","shell.execute_reply.started":"2022-05-27T08:04:17.530423Z","shell.execute_reply":"2022-05-27T08:04:26.586573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering \nThis below model is modified version of public notebooks of @ambrosm. Consider upvoting the original work. <br>\nTwo additional features are introduced (Feature interaction between f_00, f_01 with f_26 ) <br>","metadata":{}},{"cell_type":"code","source":"features = [f for f in test.columns if f != 'id' and f != 'f_27']\nfloat_features = [f for f in features if test[f].dtype == float]\nfor df in [train, test]:\n    # Extract the 10 letters of f_27 into individual features\n    for i in range(10):\n        df[f'ch{i}'] = df.f_27.str.get(i).apply(ord) - ord('A')\n        \n    # unique_characters feature is from https://www.kaggle.com/code/cabaxiom/tps-may-22-eda-lgbm-model\n    df[\"unique_characters\"] = df.f_27.apply(lambda s: len(set(s)))\n    \n    # Feature interactions: create three ternary features\n    # Every ternary feature can have the values -1, 0 and +1\n    df['i_02_21'] = (df.f_21 + df.f_02 > 5.2).astype(int) - (df.f_21 + df.f_02 < -5.3).astype(int)\n    df['i_05_22'] = (df.f_22 + df.f_05 > 5.1).astype(int) - (df.f_22 + df.f_05 < -5.4).astype(int)\n    df['i_00_26'] = (df.f_00 + df.f_26 > 5.1).astype(int) - (df.f_00 + df.f_26 < -5.4).astype(int)  #addtional feature\n    df['i_01_26'] = (df.f_01 + df.f_26 > 5.1).astype(int) - (df.f_01 + df.f_26 < -5.4).astype(int)  #addtional feature\n    i_00_01_26 = df.f_00 + df.f_01 + df.f_26\n    df['i_00_01_26'] = (i_00_01_26 > 5.0).astype(int) - (i_00_01_26 < -5.0).astype(int)\n    \nfeatures = [f for f in test.columns if f != 'id' and f != 'f_27']\nfloat_features = [f for f in features if test[f].dtype == float]\nint_features = [f for f in features if test[f].dtype == int and f.startswith('f')]\nch_features = [f for f in features if f.startswith('ch')]","metadata":{"execution":{"iopub.status.busy":"2022-05-27T08:04:26.588824Z","iopub.execute_input":"2022-05-27T08:04:26.589032Z","iopub.status.idle":"2022-05-27T08:04:46.90302Z","shell.execute_reply.started":"2022-05-27T08:04:26.589007Z","shell.execute_reply":"2022-05-27T08:04:46.902246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# It has been noticed that in addtion to various features extracted in f_27, by label encoding the F_27 the results are improved slightly.\n# there are several unique lables are noticed in f_27 column of test data set. Hence both train and test data is merged for label encoding.\n\nle = LabelEncoder()\ntemp_df =(pd.concat([train.f_27, test.f_27],ignore_index=True)).to_frame() # dataframe co\ntemp_df['f_27']  = le.fit_transform(temp_df['f_27'])\ntest['f_27']  = le.transform(test['f_27'])\ntrain['f_27'] = le.transform(train['f_27'])","metadata":{"execution":{"iopub.status.busy":"2022-05-27T08:04:46.904147Z","iopub.execute_input":"2022-05-27T08:04:46.904384Z","iopub.status.idle":"2022-05-27T08:04:57.920298Z","shell.execute_reply.started":"2022-05-27T08:04:46.904354Z","shell.execute_reply":"2022-05-27T08:04:57.919497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Get prediction data ","metadata":{}},{"cell_type":"markdown","source":"## Neural network data\nThis below model is modified version of public notebooks of @ambrosm. Consider upvoting the original work. The main differnce is number of Epchos and features ","metadata":{}},{"cell_type":"code","source":"X = train.drop([\"target\",\"f_27\"],axis =1)\ny= train[\"target\"]","metadata":{"execution":{"iopub.status.busy":"2022-05-27T08:04:57.922176Z","iopub.execute_input":"2022-05-27T08:04:57.922391Z","iopub.status.idle":"2022-05-27T08:04:58.197733Z","shell.execute_reply.started":"2022-05-27T08:04:57.922367Z","shell.execute_reply":"2022-05-27T08:04:58.196749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def my_model():\n    \"\"\"Simple sequential neural network with four hidden layers.\n    \n    Returns a (not yet compiled) instance of tensorflow.keras.models.Model.\n    \"\"\"\n    activation = 'swish'\n    inputs = Input(shape=(len(features)))\n    x = Dense(64, kernel_regularizer=tf.keras.regularizers.l2(40e-6),\n              activation=activation,\n             )(inputs)\n    x = Dense(64, kernel_regularizer=tf.keras.regularizers.l2(40e-6),\n              activation=activation,\n             )(x)\n    x = Dense(64, kernel_regularizer=tf.keras.regularizers.l2(40e-6),\n              activation=activation,\n             )(x)\n    x = Dense(16, kernel_regularizer=tf.keras.regularizers.l2(40e-6),\n              activation=activation,\n             )(x)\n    x = Dense(1, #kernel_regularizer=tf.keras.regularizers.l2(1e-6),\n              activation='sigmoid',\n             )(x)\n    model = Model(inputs, x)\n    return model\n\nplot_model(my_model(), show_layer_names=False, show_shapes=True)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-27T08:04:58.199022Z","iopub.execute_input":"2022-05-27T08:04:58.199347Z","iopub.status.idle":"2022-05-27T08:04:59.551766Z","shell.execute_reply.started":"2022-05-27T08:04:58.199303Z","shell.execute_reply":"2022-05-27T08:04:59.550755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NN_EPOCHS =500\nEPOCHS_COSINEDECAY = 150\nCYCLES = 1\nVERBOSE = 0 # set to 0 for less output, or to 2 for more output\nDIAGRAMS = True\nUSE_PLATEAU = False\nBATCH_SIZE = 2048\nONLY_FIRST_FOLD = False\n\n# see https://keras.io/getting_started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development\nnp.random.seed(1)\nrandom.seed(1)\ntf.random.set_seed(1)\nfeatures = [f for f in test.columns if f != 'id']\nprint(features)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-27T08:04:59.553572Z","iopub.execute_input":"2022-05-27T08:04:59.553939Z","iopub.status.idle":"2022-05-27T08:04:59.562549Z","shell.execute_reply.started":"2022-05-27T08:04:59.553895Z","shell.execute_reply":"2022-05-27T08:04:59.561633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fit_model(X_tr, y_tr, X_va=None, y_va=None, run=0):\n\n\n    global y_va_pred\n    start_time = datetime.datetime.now()\n    \n    scaler = StandardScaler()\n    X_tr = scaler.fit_transform(X_tr)\n    \n    if X_va is not None:\n        X_va = scaler.transform(X_va)\n        validation_data = (X_va, y_va)\n    else:\n        validation_data = None\n\n    # Define the learning rate schedule and EarlyStopping\n    lr_start=0.01\n    if USE_PLATEAU and X_va is not None: # use early stopping\n        epochs = NN_EPOCHS\n        lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.7, \n                               patience=4, verbose=VERBOSE)\n        es = EarlyStopping(monitor=\"val_loss\",\n                           patience=12, \n                           verbose=1,\n                           mode=\"min\", \n                           restore_best_weights=True)\n        callbacks = [lr, es, tf.keras.callbacks.TerminateOnNaN()]\n\n    else: # use cosine learning rate decay rather than early stopping\n        epochs = EPOCHS_COSINEDECAY\n        lr_end = 0.0002\n        def cosine_decay(epoch):\n            # w decays from 1 to 0 in every cycle\n            # epoch == 0                  -> w = 1 (first epoch of cycle)\n            # epoch == epochs_per_cycle-1 -> w = 0 (last epoch of cycle)\n            epochs_per_cycle = epochs // CYCLES\n            epoch_in_cycle = epoch % epochs_per_cycle\n            if epochs_per_cycle > 1:\n                w = (1 + math.cos(epoch_in_cycle / (epochs_per_cycle-1) * math.pi)) / 2\n            else:\n                w = 1\n            return w * lr_start + (1 - w) * lr_end\n\n        lr = LearningRateScheduler(cosine_decay, verbose=0)\n        callbacks = [lr, tf.keras.callbacks.TerminateOnNaN()]\n        \n    # Construct and compile the model\n    model = my_model()\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_start),\n                  metrics='AUC',\n                  loss=tf.keras.losses.BinaryCrossentropy())\n\n    # Train the model\n    history = model.fit(X_tr, y_tr, \n                        validation_data=validation_data, \n                        epochs=epochs,\n                        verbose=VERBOSE,\n                        batch_size=BATCH_SIZE,\n                        shuffle=True,\n                        callbacks=callbacks)\n\n    history_list.append(history.history)\n    callbacks, es, lr, history = None, None, None, None\n    \n    if X_va is None:\n        print(f\"Training loss: {history_list[-1]['loss'][-1]:.4f}\")\n    else:\n        lastloss = f\"Training loss: {history_list[-1]['loss'][-1]:.4f} | Val loss: {history_list[-1]['val_loss'][-1]:.4f}\"\n        \n        # Inference for validation\n        y_va_pred = model.predict(X_va, batch_size=len(X_va), verbose=0)\n        #oof_list[run][val_idx] = y_va_pred\n        test_preds = model.predict(scaler.transform(test[features]), batch_size=len(X_va), verbose=0)\n        \n        # Evaluation: Execution time, loss and AUC\n        score = roc_auc_score(y_va, y_va_pred)\n        \n        print(f\"Fold {run}.{fold} | {str(datetime.datetime.now() - start_time)[-12:-7]}\"\n              f\" | {lastloss} | AUC: {score:.5f}\")\n        score_list.append(score)\n    return model, scaler, y_va_pred, test_preds","metadata":{"execution":{"iopub.status.busy":"2022-05-27T08:04:59.564356Z","iopub.execute_input":"2022-05-27T08:04:59.565198Z","iopub.status.idle":"2022-05-27T08:04:59.583561Z","shell.execute_reply.started":"2022-05-27T08:04:59.565163Z","shell.execute_reply":"2022-05-27T08:04:59.582569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"{len(features)} features\")\nhistory_list = []\nscore_list = []\nval_preds = []\ntest_preds = []\nkf = KFold(n_splits=5)\nfor fold, (idx_tr, idx_va) in enumerate(kf.split(X,y)):\n    X_tr = train.iloc[idx_tr][features]\n    X_va = train.iloc[idx_va][features]\n    y_tr = train.iloc[idx_tr].target\n    y_va = train.iloc[idx_va].target\n    \n    model, scaler, y_va_pred, test_pred = fit_model(X_tr, y_tr, X_va, y_va)\n    val_preds.extend(y_va_pred)\n    test_preds.append(test_pred)\n    \n    if ONLY_FIRST_FOLD: \n        break \n\nprint(f\"OOF AUC:                       {np.mean(score_list):.5f}\")","metadata":{"execution":{"iopub.status.busy":"2022-05-27T08:04:59.584754Z","iopub.execute_input":"2022-05-27T08:04:59.584969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"neural_test = pd.DataFrame(np.array(test_preds).mean(axis =0),columns = [\"target\"])\nneural_test.to_csv(\"neural_network_test.csv\")\nneural_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"neural_vals = pd.DataFrame(val_preds,columns = [\"target\"])\nneural_vals.to_csv(\"neural_network_train.csv\")\nneural_vals","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"roc_auc_score(train[\"target\"],neural_vals )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LightGBM data\nTaken from my other https://www.kaggle.com/code/slythe/relative-features-w-lightgbm  <br>","metadata":{}},{"cell_type":"code","source":"lgb_vals = pd.read_csv(\"../input/relative-features-w-lightgbm/lgb_vals.csv\",index_col = 0)\nlgb_test = pd.read_csv(\"../input/relative-features-w-lightgbm/submission_csv.csv\",index_col = 0)\nlgb_vals","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"roc_auc_score(train[\"target\"],lgb_vals )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pytorch Data","metadata":{}},{"cell_type":"code","source":"py_vals = pd.read_csv(\"../input/tps-may-pytorch-with-gpu/pytorch_vals.csv\",index_col = 0)\npy_test = pd.read_csv(\"../input/tps-may-pytorch-with-gpu/submission.csv\",index_col = 0)\npy_vals","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"roc_auc_score(train[\"target\"],py_vals )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Meta modelling ","metadata":{}},{"cell_type":"code","source":"#merging all the Level-0 prediction of the traing data\nMeta_train =pd.DataFrame()\nMeta_train.index=train.index\nMeta_train['target']=train['target'].values\nMeta_train['LGBM_train'] = lgb_vals[\"target\"].values\nMeta_train['neural_train'] = neural_vals[\"target\"].values\nMeta_train['pytorch_train'] = py_vals[\"target\"].values\nMeta_train.head(50)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"roc_auc_score(train[\"target\"],Meta_train['LGBM_train'] )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Meta_train.round()[Meta_train.round()['LGBM_train'] != Meta_train.round()['pytorch_train']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Meta_test =pd.DataFrame()\nMeta_test.index=test.index\nMeta_test['LGBM_test'] = lgb_test[\"target\"].values\nMeta_test['neural_test'] = neural_test[\"target\"].values\nMeta_test['pytorch_test'] = py_test[\"target\"].values\nMeta_test.head(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Meta_test.round()[Meta_test.round()['LGBM_test'] != Meta_test.round()['neural_test']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Split and show","metadata":{}},{"cell_type":"code","source":"meta_features = Meta_test.columns \n\nX = Meta_train.drop(\"target\",axis =1)\ny = Meta_train['target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state = 42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (20,8))\nsns.histplot(Meta_train['LGBM_train'] ,color = \"blue\" , alpha = 0.8,label = \"LGB\")\nsns.histplot(Meta_train['neural_train'],color = \"red\" , alpha = 0.5,label = \"NN\")\nplt.title(\"Train meta data\")\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (20,8))\nsns.histplot(Meta_test['LGBM_test'] ,color = \"blue\" , alpha = 0.8,label = \"LGB\")\nsns.histplot(Meta_test['neural_test'],color = \"red\" , alpha = 0.5,label = \"NN\")\nplt.title(\"Test meta data\")\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Linear Regression Meta Model","metadata":{}},{"cell_type":"code","source":"cv = KFold(n_splits = 5)\n\nval_preds = []\nlin_preds= []\nauc_cv = []\nfor fold, (idx_train, idx_val) in enumerate(cv.split(X,y)):\n    print(\"\\n\")\n    print(\"#\"*10, f\"Fold: {fold}\",\"#\"*10)\n    X_train , X_test = X.iloc[idx_train] , X.iloc[idx_val]\n    y_train , y_test = y[idx_train] , y[idx_val]\n\n    #scaling\n    if SCALING:\n        print(\"Scaling\")\n        qt = StandardScaler()\n        X_train = qt.fit_transform(X_train)\n        X_test = qt.transform(X_test)\n        \n        test_s = Meta_test.copy(deep = True)\n        test_s = qt.transform(test_s)\n    else:\n        test_s = test.copy(deep = True)\n\n    model = LinearRegression()        \n    model.fit(X_train,y_train)\n\n    val_pred = model.predict(X_test)\n    val_preds.extend(val_pred)\n\n    auc = roc_auc_score(y_test, val_pred)\n    print(\"\\n Validation AUC:\" , auc)\n    lin_preds.append(model.predict(test_s))\n\n    auc_cv.append(auc)\n\nprint(\"FINAL AUC: \", np.mean(auc_cv))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_lin = sub.copy(deep=True)\nsub_lin[\"target\"] = np.array(lin_preds).mean(axis =0)\nsub_lin.to_csv(\"submission_lin.csv\")\nsub_lin","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (20,8))\nsns.histplot(sub_lin[\"target\"])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LightGBM Meta Model","metadata":{}},{"cell_type":"code","source":"def build_model( epochs = EPOCHS):\n    \n    model = lgb.LGBMClassifier(\n    objective= 'binary',\n    metric= \"auc, binary_logloss, binary_error\",\n    num_iterations = epochs,\n    num_threads= -1,\n    learning_rate= 0.18319492258552644,\n    boosting= 'gbdt',\n    lambda_l1= 0.00028648667113792726,\n    lambda_l2= 0.00026863027834978876,\n    num_leaves= 229,\n    max_depth= 0,\n    min_child_samples=80,\n    device = 'cpu',\n    max_bins=511, \n    random_state=42 \n    )\n        \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ncv = KFold(n_splits = 5, shuffle = True,random_state=42)\n\nval_preds = []\nlgb_preds= []\nauc_cv = []\nfor fold, (idx_train, idx_val) in enumerate(cv.split(X,y)):\n    print(\"\\n\")\n    print(\"#\"*10, f\"Fold: {fold}\",\"#\"*10)\n    X_train , X_test = X.iloc[idx_train] , X.iloc[idx_val]\n    y_train , y_test = y[idx_train] , y[idx_val]\n\n    model = build_model()\n\n    #scaling\n    if SCALING:\n        qt = QuantileTransformer(n_quantiles=1000, \n                         output_distribution='normal', \n                         random_state=42).fit(X_train)\n        X_train = qt.transform(X_train)\n        X_test = qt.transform(X_test)\n        test_s = Meta_test.copy(deep = True)\n        test_s = qt.transform(test_s)\n    else:\n        test_s = test.copy(deep = True)\n\n    model.fit(X_train,y_train, eval_set=[(X_test,y_test)], callbacks = [lgb.early_stopping(EARLY_STOP)],eval_metric=\"auc\")\n\n    if CALIBRATION:\n        calibrator = CalibratedClassifierCV(model, method = \"isotonic\", cv='prefit')\n        calibrator.fit(X_test, y_test)\n\n        val_pred = calibrator.predict_proba(X_test)[:, 1]\n        val_preds.extend(val_pred)\n\n        auc = roc_auc_score(y_test, val_pred)\n        print(\"\\n Calibration AUC:\" , auc)\n        lgb_preds.append(calibrator.predict_proba(test_s)[:, 1])\n    else:\n\n        val_pred = model.predict_proba(X_test)[:, 1]\n        val_preds.extend(val_pred)\n\n        auc = roc_auc_score(y_test, val_pred)\n        print(\"\\n Validation AUC:\" , auc)\n        lgb_preds.append(model.predict_proba(test_s)[:, 1])\n\n    auc_cv.append(auc)\n\nprint(\"FINAL AUC: \", np.mean(auc_cv))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feat_importance = pd.DataFrame(model.feature_importances_, index  = meta_features, columns=[\"importance\"])\nfeat_importance.plot(kind = 'bar', figsize = (20,7))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_lgb = sub.copy(deep=True)\nsub_lgb[\"target\"] = np.array(lgb_preds).mean(axis =0)\nsub_lgb.to_csv(\"submission_lgb.csv\")\nsub_lgb","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (20,8))\nsns.histplot(sub_lgb[\"target\"])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# NN prediction ","metadata":{}},{"cell_type":"code","source":"print(f\"{len(features)} features\")\n\nhistory_list = []\nscore_list = []\nval_preds = []\ntest_preds = []\nkf = KFold(n_splits=5)\n\nfor fold, (idx_tr, idx_va) in enumerate(kf.split(X,y)):\n    X_tr = train.iloc[idx_tr][features]\n    X_va = train.iloc[idx_va][features]\n    y_tr = train.iloc[idx_tr].target\n    y_va = train.iloc[idx_va].target\n    \n    model, scaler, y_va_pred, test_pred = fit_model(X_tr, y_tr, X_va, y_va)\n    val_preds.extend(y_va_pred)\n    test_preds.append(test_pred)\n\nprint(f\"OOF AUC:                       {np.mean(score_list):.5f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_nn = sub.copy(deep=True)\nsub_nn[\"target\"] = np.array(test_preds).mean(axis =0)\nsub_nn.to_csv(\"submission_nn.csv\")\nsub_nn","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (20,8))\nsns.histplot(sub_nn[\"target\"])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ensemble Meta predictions ","metadata":{}},{"cell_type":"code","source":"ensemble_df = pd.DataFrame()\nensemble_df[\"target1\"] = sub_nn[\"target\"]\nensemble_df[\"target2\"] = sub_lgb[\"target\"]\nensemble_df[\"target3\"] = sub_lin[\"target\"]\nensemble_df","metadata":{"execution":{"iopub.status.busy":"2022-05-27T09:49:45.605106Z","iopub.execute_input":"2022-05-27T09:49:45.605436Z","iopub.status.idle":"2022-05-27T09:49:45.697791Z","shell.execute_reply.started":"2022-05-27T09:49:45.605346Z","shell.execute_reply":"2022-05-27T09:49:45.696639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_final = sub.copy(deep = True)\nsub_final[\"target\"] = ensemble_df.mean(axis =1).values\nsub_final.to_csv(\"submission_ensemble.csv\")\nsub_final","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (20,8))\nsns.histplot(sub_final[\"target\"])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-27T09:51:27.192105Z","iopub.execute_input":"2022-05-27T09:51:27.192401Z","iopub.status.idle":"2022-05-27T09:51:27.209574Z","shell.execute_reply.started":"2022-05-27T09:51:27.192369Z","shell.execute_reply":"2022-05-27T09:51:27.208429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}